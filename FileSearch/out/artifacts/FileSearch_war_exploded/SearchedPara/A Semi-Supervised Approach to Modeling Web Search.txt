 Web search is an interactive process that involves actions from Web search users and responses from the search en-gine. Many research efforts have been made to address the problem of understanding search behavior in general. Some of this work focused on predicting whether a particular user has succeeded in achieving her search goal or not. Most of these studies have faced the problem of the lack of reliable labeled data to learn from. Unlike labeled data, unlabeled data recording behavioral signals in Web search is widely available in search logs. In this work, we study the plau-sibility of using labeled and unlabeled data to learn better models of user behavior that can be used to predict search success more effectively. We present a semi-supervised ap-proach to modeling Web search satisfaction. The proposed approach can use either labeled data only or both labeled and unlabeled data. We show that the proposed model out-performs previous methods for modeling search success using labeled data. We also show that adding unlabeled data im-proves the effectiveness of the proposed models and that the proposed method outperforms other strong semi-supervised baselines.
 H.3.3 [ Information Search and Retrieval ]: Search Pro-cess search engine evaluation, user behavior models, semi-supervised learning
Finding effective evaluation metrics for information re-trieval systems has always received plenty of attention. The need for good evaluation metrics increases as information re-trieval systems advance leaving smaller rooms for improve-ments. Traditionally, per-query metrics such as Mean Av-erage Precision (MAP), Precision @k, and Normalized Dis-counted Cumulative Gain (NDCG) have been widely used. Recently, researchers started using behavioral signals for evaluating information retrieval systems. Some of this work still considered query-level evaluation [25], while other looked at goal success [12]. Goal level success prediction is related to another body of work that tries to understand different patterns of user behavior in Web search like understand-ing the behavioral differences between novice and expert users [27], and studying how behavior changes in difficult search tasks [4].

A major problem facing these research efforts is the lack of labeled data to train effective learning algorithms. Hence, most of this work has either been limited to small-scale stud-ies [8], or large-scale analysis of unlabeled data [27, 4]. La-beled data consists of all behavioral signals collected from a user in a search goal and a label indicating whether the user X  X  information need was satisfied or not.

Some approaches have been proposed to overcome the lim-ited availability of labeled data by building frameworks for collecting realistic labeled data. Hassan et al. [13] created a toolbar that monitors search behavior and collects explicit satisfaction ratings from users when they normally use Web search. The toolbar asks users to rate their search experi-ence after every search goal. The main problem with this approach is the low adoption rates of users and the difficulty of keeping users engaged with the toolbar. Ageev et al. [1] suggested using a game like strategy where study partici-pants are competing to find answers to questions using Web search in a given amount of time. Like the first approach, this approach suffers from the low adoption rates. It also creates artificial information needs, rather than monitoring the information needs of real user; which may affect user behavior. Human judges were used to collect labeled search goals in [12]. This process is noisy because judges have no information about the actual intent underlying the informa-tion need. Moreover, it is expensive and time consuming. Hence, it does not scale when a large amount of annotated data is needed.

Unlike labeled data, unlabeled data of behavioral signals is readily available in search logs. In unlabeled data, we have access to several signal describing the user behavior, but we do not know whether the user succeeded in satisfying her information goal or not. This suggests that algorithms that can learn from both labeled and unlabeled data could be very effective in modeling search success.

In this work, we look into ways of predicting success in search by modeling user behavior. We introduce a new ap-proach that can use either labeled data only, or labeled and unlabeled data in a semi-supervised setting. We compare the proposed method to previous work on Web search suc-cess prediction using labeled data. We also compare it to strong baselines that apply other semi-supervised learning frameworks to the same problem. We discuss the advan-tages and limitations of the proposed method, and highlight the performance gains the proposed method achieves both in the supervised and semi-supervised settings. We also show empirically that adding unlabeled data improves prediction accuracy.

The rest of this paper will proceed as follows. In Section 2, we discuss related work. We provide a formal definition of our problem is Section 3. In Section 4, we describe a generative model for predicting success and show how it can be extended to the semi-supervised case. Finally we present experiments in Section 6, discussions in Section 7 and we conclude in Section 8.
Evaluating information retrieval is a challenging problem that received a lot of attention. Traditionally, search en-gines have been evaluated using classical methodologies that use query sets and relevance judgments. One of the most frequently used metrics for evaluating ad hoc information retrieval is Mean Average Precision (MAP). MAP is opti-mized to cover both recall and precision while taking the entire search result ranking into consideration. State of the art measurements of query result relevance use discounted cumulative gain (DCG) [18]. The intuition behind DCG is that users are only interested in the first few results and hence high early precision is desirable [14]. DCG metrics lend themselves to a user model of scanning a ranked list of results to some depth. DCG can be calculated using manual query URL relevance judgments, or estimated query URL relevance [2, 21]. Later, DCG evolved into the Normalized Discounted Cumulative Gain (NDCG) [17]. NDCG normal-izes DCG across queries by sorting documents of a result set by relevance, computing the ideal DCG, and normal-izing DCG with with the ideal DCG. DCG is additive in nature and assumes that documents are independent. Ex-pected Reciprocal Rank [6] is another metric inspired by the cascade model that alleviates these problems. It is defined as the expected reciprocal length of time that the user will take to find a relevant document. Other metrics have been also used like Precision @k.

All those methods are query based metrics. They evalu-ate an information retrieval system by evaluating individual queries. However, a single user information need may result in one or more queries, and the same query may have differ-ent underlying information needs. Hence, individual query URL relevance may not always correlate with user satis-faction. Moreover, all these metrics use human relevance judgments which is expensive to collect and may be noisy because human judges do not know the exact intent behind an information need.

Mining search logs has been extensively studied by various researchers. For example, Jansen et al. [16] studied user query distribution, information needs, and sessions. Beitzel et al. [5] studied the change in popularity of topically cat-egorized queries. White et al. [28] studied search trails including queries and page navigation from toolbar logs.
More specifically, other research has looked at mining search logs for measuring search quality. Fox et al. [9] found that a strong correlation exists between search log features and user satisfaction. They used Bayesian modeling and deci-sion trees to model explicit satisfaction ratings using sev-eral kinds of features. They used clickthrough rate features, dwell time features, and features describing how users end their search sessions.

Huffman and Hochster [15] studied the correlation be-tween user satisfaction and simple relevance metrics. They reported a relatively strong correlation between user sat-isfaction and linear models encompassing the query URL relevance of the first three results for the first query in the search task.

Radlinski et al. [26] showed that interleaving the results of two ranking functions and presenting the interleaved re-sults to users is a good predictor of relative search engine performance. They also looked at using metrics including abandonment rate, reformulation rate, and time to first click to predict relative performance. They found out that those metrics do not perform as well as interleaving results. Using aggregated features extracted from user behavior is certainly correlated with satisfaction. However, it has been shown that modeling the transitions between actions performed by users during search is a stronger predictor of satisfaction [12].
White and Morris [27] tried to understand the behavioral differences between novice and expert users. They define expert users as those who use advanced operators. They show that the behavior of expert users is different compared to non-expert ones. Aula et al. [4] studied how user behav-ior changes in difficult search tasks. They performed a user study to understand how users behave with difficult search tasks. They found out that when faced with a difficult search tasks, users tend to use more diverse queries, use advanced operators, and spend more time on the search results page. These studies provide many interesting observations about user behavior. However, they do not try to model or pre-dict success due to the lack of labeled data about goals and success.

Hassan et al. [12] showed that modeling action sequences representing user behavior is better than models based on the query URL relevance of the first three results for the first query. The main reasons are that different information needs sometimes underlie the same query, and that the first query does not tell the complete picture in terms of the entire search task. A user study where firsthand satisfaction ratings were directly collected from users, who opted in to use a special toolbar, was presented in [13].
 Piwowarski et al. [24] presented a model that uses Bayesian Networks to predict document relevance in the absence of document content models. They presented a Bayesian Net-work approach to holistically model user behavior interac-tions, and used it to identify distinct patterns of search be-haviors. These patterns, along with a list of custom features, were used to predict the relevance of a set of query document pairs.

Ageev et al. [1] proposes an extension of the Markov Model approach, described in [12], by augmenting the Markov Model with additional search features. They also used a game like strategy for collecting labeled data where they ask partici-pants to find answers to questions using Web search. None of those methods has considered using unlabeled data to improve the effectiveness of the success prediction models. They all faced the problem of the lack of labeled data and came up with different solutions: human judges [12], tool-bars [13], and games [1]. The amount of data collected is still relatively small and does not allow further developments targeting specific types of information needs, verticals, or users.

Semi-supervised learning is a special form of machine learn-ing where both labeled and unlabeled data are used in the learning process. Labeled data is usually expensive, and hard to obtain. This resulted in a huge interest in learning techniques that can use unlabeled data as well as labeled data. Semi-supervised learning is a very well established area. Some of the earliest semi-supervised methods in the literature is the self training wrapper algorithm which re-trains a supervised model using unlabeled data that have been labeled by the same model with high confidence [3, 10]. The closely related concept of transductive inference has been studied in [11]. A very good introduction and sur-vey of semi-supervised learning techniques could be found in [31, 7, 29]. We will show in this work how semi-supervised learning techniques can improve the effectiveness of success prediction algorithms by using labeled and unlabeled data.
We start by defining some terms that will be used through-out the paper:
Definition : A search goal/task is a single information need that may result in one or more queries [20, 12].
Definition : A search trail is an ordered sequence of ac-tions performed by the user during a search goal.

Definition : A labeled dataset is a collection of search goals associated with success labels. Every goal in the la-beled data is labeled as successful or unsuccessful either by a human judge or by collecting firsthand labels from users conducting the search.

Definition : An unlabeled dataset is a collection of search goals without success labels. This data is easier to collect and does not need any human judges , or any special software to collect labels (i.e. browser plugins).

Assume we have a stream of queries submitted to a search engine. In response to each query, the engine returns a search results page. The user may decide to interact with this page by clicking on results, other search engine fea-tures, or by reformulating the query. Every query is a part of a search goal with the objective of satisfying a particu-lar information need. Every search goal is represented with a search trail. A search trail is represented by an ordered sequence of user actions. Actions include all types of interac-tions performed by the users while using the search engine. Following [12], we use the set of actions described in Ta-ble 1. Examples of a successful and an unsuccessful goals are shown in Tables 2 and 3 respectively. In the successful goal, the user started with the query  X  van gogh self portrait  X , and quickly decided to click on an image results. She spent some time examining the result then she successfully ended her search. In the unsuccessful goal, the user typed a query, and briefly clicked on two search results. After that, she de-cided to switch to the images vertical, but the image vertical still did not have any relevant results so she reformulated her query. Apparently, that still did not satisfy her information need, and hence she gave up and ended her search.

The labeled data was collected during a user study where search behavioral signals and explicit success ratings were collected directly from users. This will be described in de-tails in Section 5. This is better than asking human annota-tors to judge whether a user was successful with his search or not because judges do not have access to information about the actual user intent.

So given a search goal, our objective is to predict whether that goal ended up being successful or not. We adopt a semi-supervised learning approach to address this problem. Our training data X can be divided into X l , for which labels Y are provided, and X u , for which no labels are provided.
Generative models specify a joint probability distribution over observations and labels. Generative models are often used in machine learning for classification. A conditional distribution is formed from a generative model by use of Baye X  X  rule. In this section, we define a generative model for user behavior and describe how it can be used for both supervised and semi-supervised learning.
We assume that search trails are generated by a mixture model. The mixture model has two mixture components corresponding to success and failure . Every trail is generated using a probability distribution defined by the parameters of the mixture mode, denoted  X  . Let there be a set of classes C = [ c s ,c f ] corresponding to the success and failure cases.
Actions in a search trail x i are generated using the selected mixture component. Every trail x i is generated according to a distribution P ( x i | c, X  ). Hence the likelihood of seeing a trail x i is the sum of its probability over the success and fail mixture components:
Every search trail x i consists of a set of actions. For ev-ery action we chose the next action by drawing from the mixture distributions. Assuming every trail consists of a set of actions: x = { a 1 ,a 2 ,...,a n } and that every action is de-pendent only on the previous action and independent of the trail length, then we can write:
This leaves us with a set of parameters  X  a,a 0 ,c  X  P ( a for every pair of actions a and a 0 , and every class c . In addi-tion we have two more parameters  X  c s and  X  c f corresponding to the prior class probabilities P ( c s |  X  ) and P ( c f the probability of generating any search trail x is: where x is a search trail consisting of a set of actions x =  X  a 1 ,a 2 ,...,a n ,  X  , n is the number of actions in trail x , and a j is action number j in x . In our experiments, we model the trail length by introducing one multinomial distribution for each possible length. Alternatively, we can assume that trail length is identically distributed to reduce the number of parameters.
To use this model for classification, we need to learn an es-timate  X   X  of the model parameters. To estimate these param-eters, we may use maximum likelihood estimation (MLE) or maximum a posterior (MAP) estimates. One potential problem with MLE is that it may result in zero estimates of some parameters if the corresponding transition has not been seen in the training data. This problem is usually solved by smoothing. Alternatively, we can use MAP esti-mates with Dirichlet priors, which is identical to MLE with Laplace smoothing. Hence, the action transition parameters are estimated as: where N a i ,a j ,c is the empirical count of transitions from a to a j in trails in class c , N a i ,c is the empirical count of a in the same trails, and | A | is the total number of actions. Similarly, the class probabilities are estimated as: where N c is the fraction of trails in the training data with class c , and N is the size of the training set.

Given these parameters we can derive a conditional prob-ability distribution to classify new instances given the gen-erative model: To classify a new trail we just need to compute:
This model has three advantages compared the one pre-sented in [12]: (1) the model proposes a more principled way of modeling user behavior, (2) the Markov model presented in [12] makes decisions based on trail likelihood ( P ( x | c ;  X  )), while the model we present here uses posterior class prob-abilities instead ( P ( c | x, X  )). We will show later the advan-tages of taking the prior distribution into consideration, and (3) this model can be extended to the semi-supervised case as will be shown in the next sections.
We tried two different approaches to modeling time. In the first approach, we treat transition time as a continuous variable drawn from a Gamma distribution with parameters k , and  X  . We compute the distribution parameters for every class (i.e. success or failure), and every transition. Then, the probability of some value given a class, P ( t = v | c ), can be computed by plugging v into the equation for the Gamma distribution parameterized by the parameters we computed earlier.

In the second approach, we replace every click action (e.g. algorithmic result click, sponsored result click, etc.) with one of three different actions based on the click dwell time. For example instead of the action  X  SR  X , which denote a search result click, we have  X  SR-short  X  ,  X  SR  X , and  X  SR-long  X . Long clicks are defined to have a dwell time for longer than 30 seconds and have been shown to correlate with satisfac-tion, as in previous work [9]. Similarly, short clicks are clicks with a quick bounce back to the SERP and have dwell time on the landing page of under 15 seconds [9]. SERP view actions (e.g. query submission, related search click, etc.) are replaced with one of two actions: short if dwell time is less than 20 seconds, and long otherwise. Short time to first click, which is the difference between the timestamp from serving up the page and the timestamp of the first user click on the page, has been previously shown to be correlated with satisfaction [26].
The model we described in the previous subsections uses labeled data only to estimate the model parameters. Due to the high cost of obtaining labeled data for this task, and the wide availability of unlabeled data, we would like to ex-tend the model such that it uses labeled and unlabeled data for learning. The model parameters,  X   X  have been estimated using MAP estimations. This cannot directly handle unla-beled data. However, we can do so by using the expectation maximization (EM) algorithm. Semi-supervised classifica-tion with EM has been successfully applied to different do-mains before, including text categorization [23].

The EM algorithm is an iterative algorithms which can be used to find the MAP estimates of parameters in sta-tistical modeling. The algorithm has two main steps: the Expectation step and the Maximization step. The Expecta-tion (E) step of the algorithm computes the expectation of the missing values; in this case the class membership distri-bution of the unlabeled data. The Maximization (M) step re-estimates the parameters by maximizing the likelihood of the parameters using the previously computed expectations. The algorithm works as follows:
To take the component membership of the unlabeled data into consideration during training, we modify Equation 4 to:  X   X  where X is the set of training data, N ( x i ,a,a 0 ) is the number of transitions from a to a 0 in x i , and N ( x the number of occurrences of a in x i . For the labeled data,  X  ( x i ,c ) is 1 if x i is in class c and 0 otherwise. For the unla-beled data,  X  ( x i ,c ) is the component membership probabil-ities calculated in the previous E step.
Our data includes search behavioral signals and explicit success ratings collected directly from users. This data was collected during a user study where firsthand success rat-ings were collected from users. This is better than tradi-tional ways of collecting data that use annotators to judge whether a user has been successful or not [12]. The latter method introduces more noise in the labels because judges lack information about the actual intent behind the search goal.
 Study participants were instructed to install a toolbar. The toolbar detects when a user submits a query to Google, Bing, or Yahoo search engines. It then monitors and records all actions performed by the user during search. Participants were instructed to submit a binary success rating at the end of every search goal. Participants were also allowed to ignore a search goal, in which case, no data was collected about that goal.

Every log entry contained a user identifier, a time-stamp for every page view, and the URL of the visited page. Page views included query submission, search result clicks, navi-gation beyond the search results page originating from clicks on links in a search result, and clicks on other search engine features (e.g. spelling corrections, related searches, etc.). Secure (https) URLs have not been collected and any iden-tifiable information was removed from the data prior to any analysis.

More than 10k unique searches were collected a long with a label indicating whether the search was successful or not. Every record included a search trail, and a success label. A search trail originates with the submission of a query to a search engine and contains all queries and post-query navi-gation trails [27]. A search trail always begins with a query and ends when the information seeking activity stops. A search trail is represented by an ordered sequence of user actions as described earlier.
We compare the proposed approach to three baselines in the supervised setting. The first baseline poses the problem as a classic machine learning problem where a set of features adopted from previous work, [26, 12], are used to train a logistic regression classifier . The set of features we used are summarized in Table 4
The second baseline is the Markov model approach de-scribed in [12]. In this approach, two Markov models are trained to characterize the behavior of users in successful and failed goals. Features are described by a sequence of actions as described earlier. A new goal x is classified as successful if P ( x | success ) &gt; P ( x | failure ).
The last baseline uses Conditional Random Fields to model success [1]. We ran our approach on the data used in [1]; which was made publicly avaiable. We will report the results from [1] and the results obtained by our approach using the same data.
We perform experiments using the data described in Sec-tion 5. We used four supervised methods; the generative model presented in this paper and three baselines. To com-pare supervised methods, we report accuracy using 10-fold cross-validation. Folds were created based on user ids. We used the entire dataset described in Section 5. Users re-ported success in approximately 80% of the goals in this dataset and the rest reported failure. A different dataset was used for the comparison against the CRF method, pre-sented in [1], as will be described later.
We start by comparing the two approaches we described for modeling time and found that the difference in perfor-mance was not statistically significant at the 0.05 level ac-cording to the Wicoxon p-value . We will use the approach based on time discretization in all following experiments.
We then compared the performance of all supervised meth-ods using 10-fold cross validation over the entire dataset. Figure 1 shows the performance of the proposed method and the two baselines. We notice that the proposed ap-proach outperforms the two baselines and the difference is statistically significant at the 0.05 level. We notice that the static features baseline has the worst performance. This agrees with findings reported in previous work because the static features describe an aggregate view of user behavior and misses many important pieces of information. We also see performance gains compared to the Markov model ap-proach. Modeling the posterior probability of search goals ( P ( c | x, X  ) yields better results that modeling the likelihood ( P ( x | c, X  ) especially because the dataset is unbalanced with respect to the proportion of successful and failed goals This imbalance is typical in any random sample of query impres-sions because most users achieve their search goals and dis-satisfaction is usually the exception. For example, Ageev et al. [1] performed a study where users were asked to find answers to specific questions using Web search. In 87% of the tasks, users reported success by finding and reporting an answer. Hassan et al. [12] asked judges to label search goals as successful or not and reported that more than 70% of the goals were labeled as successful. In our dataset, where first-hand success labels were collected from users, approximately 80% of the goals ended with users reporting that their in-formation need was satisfied. When we had a closer look at the performance of the proposed generative model and the Markov model of [12], we noticed that the performance slightly improved for the majority class (satisfied goals) and significantly improved by over for the minority class (dissat-isfied goals).
 We also compared the performance of our approach to the CRF approach presented in [1]. We used the same dataset and the same experimental setup described in [1]. They used a game like strategy where study participants are com-peting to find answers to questions using Web search in a given amount of time. The data was collected in 4 different game rounds. We used 4-fold cross-validation where the data from three games was used for training and the data from the fourth was used for testing. Ageev et al. [1] defined four different success criteria: 1) the user submitted an answer and the answer was correct, 2) the user submitted an answer that could be correct or not, 3) the user visited a good URL that has relevant information to the question, and 4) the user visited a good URL and it was last in the search ses-sion. Like [1], we measured accuracy and macro-averaged F-measure over the successful and unsuccessful results. For the Markov model and the generative model approaches. We modeled time using the discrete approach described in Sec-tion 4.3. The results are shown in Table 5. The table shows that both the CRF model and the generative model outper-form the Markov model. The gap between the performance of the generative model and the Markov model increases as the data imbalance increases as expected. The performance of the CRF and the generative model depends on the success definition. The former is better for some definitions and the latter is better for others. The difference in performance is very small in all cases.
We compare the proposed approach to two baselines. The first baseline is a transductive discriminative model trained on features describing the transition between different ac-tions. The second baseline is a graph based algorithm based on random fields and harmonic functions. In the rest of this section, we describe the baselines, experimental setup and results.
In this baseline, we can use the action transitions (i.e. transition from a to a 0 in search trails) as features and train a discriminative model. The objective of this model is to classify each search trail into one of two categories (success or failure). The first step in this process is to transform search trails into a representation suitable for the classifica-tion task. One way to approach this problem is to compute a set of aggregate features to describe the user behavior from the search trail (e.g. number of clicks, number of queries, average dwell time, etc.). Hassan et al. [12] looked at this approach and showed that directly modeling transitions is better than using aggregate features. They also showed that transitions subsume information in aggregate features.
Instead of using aggregate features, we can derive features that directly represents the search trail without losing much information. We can use the transition between every two consecutive actions as a feature. For example, given the search trail:  X  X  Q SR END X  where Q , SR , and END cor-respond to query submission, search result click, and goal termination respectively, we can extract the following three features: Q-Q , Q-SR , and SR-END .

Generally, we will have | A | 2 features where A is the set of possible actions. Every feature corresponds to a possible transition between two states. Every trail will be repre-sented using those features. Feature values are simply the number of times every transition has been observed in the trail. The feature vector is expected to be sparse with most of the features having a zero value. Transductive support vector machines [19] extend SVMs in that they could han-dle both labeled and unlabeled data in a semi-supervised setting. SVMs use a set of labeled data, X l , and a set of corresponding labels Y l . SVM training algorithm builds a model that assigns new examples into one category or the other. In that, SVMs try to induce a general decision func-tion for the learning task. Transductive SVMs use an un-labeled set X u along with X l , and Y l . TSVMs take into account a particular set of unlabeled data, X u , and try to minimize misclassification of this set of unlabeled data.
Joachims [19] used TSVMs for text classification and pro-posed an algorithm for training TSVMs efficiently. Joachims [19] argues that TSVMs are suitable for text classification because of the high dimensional input space and the feature vector sparseness. Although our feature space is smaller than typical feature spaces in text classification, we believe TSVMs is a strong baseline for this problem.
Another popular class of semi-supervised learning algo-rithms is the graph based methods. Graph based learning algorithms rely on building a graph representation of labeled and unlabeled data points. Edges in the graph encode pair-wise similarities between points. Graph based algorithms have been successfully applied to a range of problems in natural language processing, computer vision, and compu-tational biology. Graph based methods are discriminative and transductive in nature.

We construct a graph G ( V,E ), where V is the list of la-beled and unlabeled trails. E is a set of edges connecting similar points. Inspired by the bag of words model for rep-resenting text documents, we represent every trail using a bag of transitions. Every transition has a count associated with it. Each transition corresponds to a dimension in high dimensional space, and every trail is a vector in that space. The frequency of every transition is used as its weight. Let A be the set of possible user actions, | A | be the number of such possible actions, and N ( x i ,a,a 0 ) be the number of transitions from a to a 0 in x i . Then every trail can be rep-resented as vector as follows
We use the popular Cosine similarity measure to com-pute similarity between user behavior in different goals. The vector representation of trails allows us to use the Cosine similarity measure to compute similarity between any two given trails. The Cosine metric measures the similarity by computing the cosine of the angle between the two vectors representing the search trails. Formally, this can be written as:
To construct the graph, we create a node for every search trail. Every pair of nodes is connected by an edge. The weight of edges comes from one of the similarity measure described above. The graph could be also represented by an n  X  n symmetric matrix, where n is the number of search trails. Every element M ( i,j ) represents the similarity be-tween trail x i and trail x j .

Zhu et al. [30] presented a semi-supervised graph learning approach based on Gaussian random fields and harmonic functions. The similarity graph we described earlier is used to represent both labeled and unlabeled data. The learning problem is then formulated using the Gaussian random field on this graph, with the mean of this field characterized using harmonic functions [30].

The method assumes n data points, l of which are labeled, and the rest u are unlabeled. Consider the graph G = ( V,E ) built as described above. We represent the graph using a symmetric n  X  n matrix W . The objective of this method is to compute a real valued function f : V  X  R . The values of f are fixed for labeled data and equal the class label. The value of f for unlabeled data will be computed and used to assign labels to unlabeled data. Zhu et al. shows that the function f that minimizes the eneregy function E ( f ) is harmonic [30]. It is intuitive that the function f is expected to have similar labels to points that are close in the graph. Hence, they used the quadratic energy function:
The harmonic solution is computed explicitly as follows:
We perform experiments using the data described in Sec-tion 5. We compare the performance of three semi-supervised methods. The first is the proposed approach that uses expec-tation maximization to extend a generative mode to handle labeled and unlabeled data. The second uses Transductive Support Vector Machines (TSVMs), and the third is a graph based model that uses Gaussian random fields and harmonic functions. We carried out the experiments in the transduc-tive settings, following [7]. In this setting, the test set co-incides with the set of unlabeled points (50% of the data). This has two main advantages. First, it is economical in terms of the required data. Second, it allows us to com-pare different methods including those that are naturally transductive. We compare every semi-supervised method to a corresponding supervised method to assess the benefit of adding the unlabeled data. We also compare the semi-supervised methods to one another. To make sure that the reported results are independent of labeled points selection, we select 10 different sets of labeled data for every number of labeled data points at random and report the average of the 10 runs.
In this section, we assess the benefit of using unlabeled data for every model. We compare every semi-supervised method (the proposed method and the baselines) to a cor-responding supervised method. We compare the supervised and the semi-supervised versions of the proposed approach. We compare the Trasductive SVM baseline to an inductive SVM classifier, and the harmonic functions baseline to a k-neaerest neighbor method using the same graph. The ob-jective of this experiment is to estimate the benefit of using unlabeled data regardless of the semi-supervised model used. We start with SVM and TSVM. Figure 2 compares the per-formance of both methods while varying the amount of la-beled data. We notice a consistent improvement over using labeled data only when unlabeled data is introduced. The improvement is bigger when the number of labeled points is small and gets smaller quickly as more labeled data is introduced.

We notice similar behavior when we compare semi-supervised learning using harmonic functions to the K-nearest neighbor method in Figure 3. Interestingly, we notice that adding the unlabeled data hurts the performance when the number of labeled points is very small. Otherwise, adding labeled data improves performance. Like TSVM, the benefit of adding unlabeled data decreases as more labeled data is introduced. The benefit of using harmonic functions with labeled and un-labeled data over using KNN is considerably bigger than the benefits of using TSVM over SVM.

Figure 4 compares the performance of the semi-supervised generative model that uses EM and the supervised genera-tive mode. We notice similar behavior to the two previous cases where using EM and unlabeled data improves perfor-mance. Unlike the graph based model, the improvement is consistent regardless of the amount of labeled data. The im-provement is also considerably larger than the TSVM case.
We also compare the performance of the three semi-supervised learning algorithm in Figure 5. The figure compares the performance of the generative model with EM, the Trans-ductive Support Vector Machines (TSVMs) model, and the graph based model that uses Gaussian random fields and harmonic functions using a Cosine similarity graph. We no-tice that the graph based model performs poorly when the amount of labeled data is limited. The two other methods Table 5: The Majority Baseline (MB) vs. the Markov Model (MM) approach vs. the Generative Model (GM) approach vs. the CRF approach using data from [1].
 Figure 1: Performance of all supervised methods using 10 fold cross validation. perform better with the the generative model with EM out-performing all other models with a large margin. As more labeled data is introduced the improvement margin starts to decrease.

The experiments show that adding unlabeled data consid-erably increases the prediction power of the models. This gain is achieved regardless of which semi-supervised model is used. When we compare the three semi-supervised mode, we notice that both the EM model and the TSVM model per-form better than the other baseline and their performance is pretty close to one another. The EM model has an ad-vantage over the TSVM and the graph based models which are transductive in nature. Transductive models need to be retrained for every new test set. The generative EM model does not suffer from this limitation.

Adding more unlabeled data points is shown to improve performance in Figure 6. In this experiment, we fixed the number of labeled points at 250, varied the number of un-labeled points and observed the performance. The figure indicates that adding more unlabeled data improves perfor-mance for all methods.

In summary, we have shown that using a semi-supervised framework improves performance regardless of the algorithm. We have also shown that the EM model and the TSVM model perform better than the graph based model, and that the EM model has an advantage over the other models be-cause it does not have to operate in a transductive setting. We studied the problem of Web search success prediction. Our study is different from previous work in that the meth-ods we proposed can learn from both labeled and unlabeled data. The area of using behavioral signals to model success in Web search is rather new. All previous work has posed the problem as a supervised learning problem where labeled data is used to train a model that can be used later for pre-Figure 2: Performance of SVM vs. TSVM for dif-ferent numbers of labeled data points.
 Figure 3: Performance of Harmonic Functions vs. KNN for different numbers of labeled data points. dictions. We have discussed the problems associated with collecting labeled data for search success prediction and how the process is time consuming, expensive, and sometimes not very reliable. In the meantime, unlabeled data could be easily and quickly collected. We showed how learning from labeled and unlabeled data can improve the perfor-mance of search success prediction models. We proposed different methods for incorporating unlabeled data in the learning process. We showed that adding unlabeled data significantly improves performance. We also compared the performance of different methodologies for learning from la-beled and unlabeled data and highlighted the strength and weakness of every method. In addition, we also proposed a novel supervised learning model for search success predic-tion. We showed that this model outperforms or achieves a comparable perfromance compared to the state of the art methods for predicting search success.

We performed several experiments comparing the perfor-mance of several supervised and semi-supervised models for predicting success in search. Now, we try to summarize our finding for both the supervised and the semi-supervised cases. For the supervised case, the proposed model, and the Markov model [12] perform much better than a clas-sifier that used a set of static features. The static features classifier aggregates features describing the user behavior. It performs poorly compared to the other models because the other models have a more accurate picture of user behavior by modeling action sequences. The proposed model is more Figure 4: Performance of Generative Model vs. Generative Model with Expectation Maximization for different numbers of labeled data points.
 Figure 5: Performance of all semi-supervised models for different numbers of labeled data points. principled and performs better than the Markov model [12]. The Markov model is intuitive and powerful, but it uses the likelihood of the action sequence representing a search goal, rather than the posterior probability as is the case with the proposed method. The posterior probability takes the prior into consideration which was shown to be important especially because of the inherent imbalance in the data dis-tribution.
 In the semi-supervised case, the generative model with EM performs best when the the labeled data is scarce. This is consistent with [22] that states that generative models have superior performance when only a few training exam-ples are available. When the number of labeled data is in-creased, the performance of all algorithms is similar. One advantage for the generative model with EM over TSVM is that TSVMs are very slow to train and cannot handle large amounts of data. The graph based model is also pretty slow because it involves computing a pairwise similarities for all data points. Another advantage for the generative model is that it is inductive by nature. However, other models may be used in an inductive setting as well but possibly with some performance loss.

Our results show that incorporating unlabeled data in the user satisfaction prediction task improves classification. Pre-vious work on user satisfaction prediction and other related tasks have faced the problem of the limited avaiability of la-beled data for supervised learning. Our results suggest that Figure 6: Performance of All Semi-supervised meth-ods for different numbers of unlabeled data points. semi-supervised methods in general, not only the proposed one, should be seriously considered in the future to predict satisfaction and other similar tasks.

Our findings suggest several lines of future directions for this work. The fact that adding unlabeled data can improve the performance will allow us to build more specific success prediction models that target specific verticals or specific types of queries. Moreover, this may allow us to build per-sonalized success models tailored to the behavior of different users. We also plan to compare the predictive power of dif-ferent transitions along the search trail. A preliminary study showed that looking at the entire trail is more informative that any parts of it, but we believe that we can develop better models by further studying this problem. One other direction of future research is to try to incorporate other query and SERP features like the ones presented in [1] in the semi-supervised models.
We presented novel methodologies for predicting success in Web search by analyzing user behavior. Collecting large amounts of labeled data for search success prediction is time consuming and sometimes not reliable. We showed that us-ing semi-supervised methods, where both labeled and unla-beled data are used, improves the prediction acuracy. We presented several methods for incorporating unlabeled data and we discussed the advantages nad disadvantages of each. The results suggest that semi-supervised methods in general should be considered in the future for this task and other similar tasks. [1] M. Ageev, Q. Guo, D. Lagun, and E. Agichtein. Find [2] E. Agichtein, E. Brill, and S. T. Dumais. Improving [3] A. Agrawala. Learning with a probabilistic teacher. [4] A. Aula, R. M. Khan, and Z. Guan. How does search [5] S. M. Beitzel, E. C. Jensen, A. Chowdhury, [6] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. [7] O. Chapelle, B. Sch  X  olkopf, and A. Zien, editors. [8] H. A. Feild, J. Allan, and R. Jones. Predicting [9] S. Fox, K. Karnawat, M. Mydland, S. Dumais, and [10] S. Fralick. Learning to recognize patterns without a [11] A. Gammerman, V. Vovk, and V. Vapnik. Learning by [12] A. Hassan, R. Jones, and K. L. Klinkner. Beyond dcg: [13] A. Hassan, Y. Song, and L.-w. He. A task level metric [14] D. Hawking, N. Craswell, P. Thistlewaite, and [15] S. B. Huffman and M. Hochster. How well does result [16] B. J. Jansen, A. Spink, and T. Saracevic. Real life, [17] K. J  X  arvelin and J. Kek  X  al  X  ainen. Ir evaluation methods [18] K. J  X  arvelin and J. Kek  X  al  X  ainen. Cumulated gain-based [19] T. Joachims. Making large-scale support vector [20] R. Jones and K. Klinkner. Beyond the session [21] S. Jung, J. L. Herlocker, and J. Webster. Click data as [22] T. M. Mitchell. Machine Learning . McGraw-Hill, Inc., [23] K. Nigam, A. Mccallum, and T. M. Mitchell.
 [24] B. Piwowarski, G. Dupret, and R. Jones. Mining user [25] F. Radlinski, M. Kurup, and T. Joachims. How does [26] F. Radlinski, M. Kurup, and T. Joachims. How does [27] R. W. White and S. M. Drucker. Investigating [28] R. W. White and J. Huang. Assessing the scenic [29] X. Zhu. Semi-supervised learning literature survey. [30] X. Zhu, Z. Ghahramani, and J. D. Lafferty.
 [31] X. Zhu and A. B. Goldberg. Introduction to
