 Manifold Ranking (MR), a graph-based ranking algorithm, has been widely applied in information retrieval and shown to have excellent performance and feasibility on a variety of data types. Particularly, it has been successfully applied to content-based image retrieval, because of its outstanding ability to discover underlying geometrical structure of the given image database. However, manifold ranking is com-putationally very expensive, both in graph construction and ranking computation stages, which significantly limits its ap-plicability to very large data sets. In this paper, we extend the original manifold ranking algorithm and propose a new framework named Efficient Manifold Ranking (EMR). We aim to address the shortcomings of MR from two perspec-tives: scalable graph construction and efficient computation. Specifically, we build an anchor graph on the data set instead of the traditional k-nearest neighbor graph, and design a new form of adjacency matrix utilized to speed up the ranking computation. The experimental results on a real world im-age database demonstrate the effectiveness and efficiency of our proposed method. With a comparable performance to the original manifold ranking, our method significantly re-duces the computational time, makes it a promising method to large scale real world retrieval problems.
 H.3.3 [ Information Search and Retrieval ]: Retrieval models; H.2.8 [ Database Applications ]: Image databases Algorithm, Performance Efficient manifold ranking, image retrieval, graph-based al-gorithm, out-of-sample
Traditional image retrieval systems are based on keyword search, such as Google and Yahoo image search. In these systems, user keyword is matched with the context around an image including the title, manual annotation, web doc-ument, etc. These systems don X  X  utilize information from images. However these systems suffer many problems, such as shortage of the text information and inconsistency of the meaning of the text and image.

Content-based image retrieval (CBIR) is a considerable choice to overcome these difficulties. CBIR has drawed a great attention in the past two decades [5, 19, 27]. Differ-ent from traditional keyword search systems, CBIR systems utilize the low-level features (color, texture, shape, etc.), automatically extracted from images. This is a vital char-acter for efficient management and search in a large image database. But the low-level features used in CBIR systems are often visually characterized, and with no direct connec-tion with semantic concepts of the images. How to narrow the semantic gap has been the main challenge for CBIR.
Many data sets have underlying cluster or manifold struc-ture. Under such circumstances, the assumption of label con-sistency is reasonable [23, 36]. It means that those nearby data points, or points belong to the same cluster or mani-fold, are very likely to share the same semantic label. This phenomenon is extremely important to explore the seman-tic relevance when the label information is unknown. Thus, a good CBIR method should consider low-level features as well as intrinsic structure of the data.

Manifold Ranking (MR) [36,37], a semi-supervised graph-based ranking algorithm, has been widely applied in infor-mation retrieval, and shown to have excellent performance and feasibility on a variety of data types, such as the text [28], image [9], and video [34]. The core idea of manifold ranking is to rank the data with respect to the intrinsic structure collectively revealed by a large number of data. By taking the underlying structure into account, manifold ranking assigns each data point a relative ranking score, in-stead of an absolute pairwise similarity as traditional ways. The score is treated as a distance metric defined on the man-ifold, which is more meaningful to capturing the semantic relevance degree. He et al. [9] firstly applied manifold rank-ing to CBIR, and significantly improved image retrieval per-formance compared with state-of-the-art algorithms.
However, manifold ranking has its own drawbacks to han-dle large scale data sets  X  it has expensive computational cost, both in graph construction and ranking computation stages. Particularly, it is costly to handle an out-of-sample query (a new sample). That means original manifold rank-ing is inadequate for a real world CBIR system, in which the user provided query is always an out-of-sample. It X  X  in-tolerable for a user to wait a long time to get returns.
In this paper, we extend the original manifold ranking and propose a novel framework named Efficient Manifold Rank-ing (EMR). We try to address the shortcomings of manifold ranking from two perspectives: the first is scalable graph construction; and the second is efficient computation, espe-cially for out-of-sample retrieval. The main contributions of this paper are as follows. (1) Based on the anchor graph construction [18,35], we design a new form of adjacency ma-trix and give it an intuitive explanation. (2) By the new form, our method EMR achieves a comparable performance to original manifold ranking but significantly reduces the computational time.

The rest of this paper is organized as follows. In section 2, we briefly discuss some related works and in section 3, we review the manifold ranking algorithm and make a detailed analysis. The proposed approach EMR is described in sec-tion 4. In section 5, we present the experiment results on a real world image database. Finally we provide an extension analysis in section 6 and conclusions in section 7.
In this section, we discuss two most relevant topics to our work: ranking model and content-based image retrieval.
The problem of ranking has recently gained great atten-tions in both information retrieval and machine learning areas. Conventional ranking models can be content based models, like the Vector Space Model, BM25, and the lan-guage modeling [22]; or link structure based models, like the famous PageRank [2] and HITS [15]; or cross media mod-els [13]. Another important category is the learning to rank model, which aims to optimize a ranking function that incor-porates relevance features and avoids tuning a large number of parameters empirically [7, 26]. However, many conven-tional models ignore the important issue of efficiency, which is crucial for a real-time systems, such as a web application. In [29], the authors present a unified framework for jointly optimizing effectiveness and efficiency.
In this paper, we focus on a particular kind of ranking model  X  graph-based ranking. It has been successfully ap-plied in link-structure analysis of the web [2,15] and social networks research [3, 8, 17]. Generally, a graph can be de-noted as G =( V,E,W ), where V is a set of vertices in which each vertex represents a data point, E  X  V  X  V is a set of edges connecting related vertices, and W is a adjacency ma-trix recording the pairwise weights between vertices. The object of a graph-based ranking model is to decide the im-portance of a vertex in a graph, based on local or global information draw from the graph.

Agarwal [1] proposed to model the data by a weighted graph, and incorporated this graph structure into the rank-ing function as a regularizer. Guan et al. [8] proposed a graph-based ranking algorithm for interrelated multi-type resources to generate personalized tag recommendation. Liu et al. [17] proposed an automatically tag ranking scheme by performing a random walk over a tag similarity graph. In [3], the authors made the music recommendation by ranking on a unified hypergraph, combining with rich social information and music content. Recently, there have been some papers on speeding up manifold ranking. In [11], the authors parti-tioned the data into several parts and computed the ranking function by a block-wise way.
In many cases, we have no more information than the data itself. Without label information, capturing semantic rela-tionship between images is quite difficult. A great amount of researches have been performed for designing more informa-tive low-level features (e.g., SIFT features [20]) to represent images, or better metrics (e.g., DPF [16]) to measure the perceptual similarity, but their performance is restricted by many conditions and is sensitive to the data. Relevance feedback [24] is a powerful tool for interactive CBIR. User X  X  high level perception is captured by dynamically updated weights based on the user X  X  feedback.

Many traditional image retrieval methods focus on the data features too much, and they ignore the underlying structure information, which is of great importance for se-mantic discovery, especially when the label information is unknown. A good CBIR method should consider the image features as well as the intrinsic structure of the data, in our opinion. Manifold ranking [36,37] ranks data with respect to the intrinsic geometrical structure, which is exactly in line with our consideration.
In this section, we briefly review the manifold ranking algorithm and make a detailed analysis about its drawbacks. We start form the description of notations.
Given a set of data  X  = { x 1 ,x 2 ,...,x n } X  R m and build a graph on the data (e.g., k NN graph). W  X  R n  X  n denotes the adjacency matrix with element w ij saving the weight of the edge between point i and j . Normally the weight can be defined by the heat kernel w ij =exp[  X  d 2 ( x i ,x j there is an edge linking x i and x j ,otherwise w ij = 0. Func-tion d ( x i ,x j ) is a distance metric of x i and x j defined on  X  , such as the Euclidean distance. Let r :  X   X  R be a rank-ing function which assigns to each point x i a ranking score r . Finally, we define an initial vector y =[ y 1 ,...,y n ] which y i =1if x i is a query and y i =0otherwise. The cost function associated with r is defined to be O ( r )= 1 where  X &gt; 0 is the regularization parameter and D is a diagonal matrix with D ii = n j =1 w ij .

The first term in the cost function is a smoothness con-straint, which makes the nearby points in the space have close ranking scores. The second term is a fitting constraint, which means the ranking result should fit to the initial label assignment.Ifwehavemorepriorknowledgeabouttherel-evance or confidence of each query, we can assign different initial scores to the queries. Minimizing the cost function O ( r ), we get the optimal r by the following closed form where  X  = 1 1+  X  , I n is an identity matrix with n  X  n ,and S is the symmetrical normalization of W , S = D  X  1 / 2 WD  X  1 In large scale problems, we prefer to use the iteration scheme:
During each iteration, each point receives information from its neighbors (first term), and retains its initial informa-tion (second term). The iteration process is repeated until convergence. When manifold ranking is applied to retrieval (such as image retrieval), after specifying a query by the user, we can use the closed form or iteration scheme to com-pute the ranking score of each point. The ranking score can be viewed as a metric of the manifold distance which is more meaningful to measure the semantic relevance.
Although manifold ranking has been widely used in many applications and proved effective for multiple resources, it has its own drawbacks to handle large scale data sets, which significantly limits its applicability.

The first is its graph construction method. The k NN graph is quite appropriate for manifold ranking because of its good ability to capture local structure of the data. But the construction cost for k NN graph is O ( kn 2 ), which is expen-sive in large scale situations. Moreover, manifold ranking, as well as many other graph-based algorithms directly use the adjacency matrix W in their computation. In some cases, it is impossible to keep matrix W as large as n  X  n in memory, especially for very large data sets or memory-short environ-ment applications. Thus, we need to find a way to build a graph in both low construction cost and small storage space, as well as good ability to capture underlying structure of the given data set.

The second, manifold ranking has very expensive compu-tational cost because of the matrix inversion operation in equation (2). This has been the main bottleneck to apply manifold ranking in large scale applications. Although we can use the iteration algorithm in equation (3), it is still inefficient in large scale cases and may arrive at a local con-vergence. Thus, original manifold ranking is inadequate for a real-time retrieval system.
We try to address the shortcomings of original manifold ranking from two main perspectives: scalable graph con-struction and efficient ranking computation. Particularly, our method can handle the out-of-sample retrieval problem, which is crucial for a real world retrieval system.
To handle scalable data sets, we want the graph construc-tion cost to be linear or near linear with the graph size. That means, for each data point, we can X  X  search the whole graph, like k NN strategy does. To achieve this requirement, we construct an anchor graph [18, 35] and propose a new design of adjacency matrix W .

The definitions of anchor points and anchor graph have appeared in some other works. For instance, in [33], the au-thors proposed that each data point on the manifold can be locally approximated by a linear combination of its nearby anchor points, and the linear weights become its local co-ordinate coding. Liu et al. [18] designed the adjacency ma-trix in a probabilistic measure and used it for scalable semi-supervised learning. This work inspires us much.
Now we introduce how to use anchor graph to model the data [18,35]. Suppose we have a data set  X  = { x 1 ,...,x R m with n samples in m dimensions, and U = { u 1 ,...,u d } X  R m denotes a set of anchors sharing the same space with the data set. Let f :  X   X  R be a real value function which as-signs each data point in  X  a semantic label. We aim to find a weight matrix Z  X  R d  X  n that measures the potential rela-tionships between data points in  X  and anchors in U .Then we estimate f ( x ) for each data point as a weighted average of the labels on anchors with constraints d k =1 z ki =1and z ki  X  0. Element z ki resents the weight between data point x i and anchor u k .We adopt the well known Nadaraya-Watson kernel regression to assign weights smoothly with the Epanechnikov quadratic kernel
The smoothing parameter  X  determines the size of the local region in which anchors can affect the target point. It is reasonable to consider that one data point has the same semantic label with its nearby anchors in a high probability. There are many ways to determine the parameter  X  .For example, it can be a constant selected by cross-validation from a set of training data. In this paper we use a more robust way to get  X  , which uses the nearest neighborhood size s to replace  X  ,thatis where u [ s ] is the s th closest anchor of x i .
Specifically, to build the anchor graph, we connect each data point to its s nearest anchors and then assign weights to each connection by the kernel function. So the construction has a total computational complexity O ( sdn ), where d is the number of anchors. Thus, the number of anchors determines the efficiency of the anchor graph construction. If d n , the construction is very fast.

How can we get the anchors? Active learning [25, 32] or clustering methods are considerable choices. For example, we use k-means algorithm and select the centers as anchors. Some fast k-means algorithms [14] can speed up the compu-tation. Random selection is a competitive method which has extremely low selection cost and acceptable performance. Later in our experiments, we compare the performance of using random anchors, fast k-means anchors and normal k-means anchors.

The main feature, also the main advantage of building an anchor graph is separating the graph construction into two stages  X  an off-line anchor selection stage and an on-line graph construction stage. That means, we can adopt any useful method to select or build the anchors with little concern for their time complexity, while the graph construc-tion is always efficient since it has linear complexity to the date size. Note that we don X  X  have to update the anchors frequently, because informative anchors for a large data set are relatively stable (e.g., the cluster centers), even if a few new samples are added.
We present a new approach to design the adjacency ma-trix W and make an intuitive explanation for it. The weight matrix Z  X  R d  X  n canbeseenasa d dimensional represen-tation of the data X  X  R m  X  n , d is the number of anchor points. That is to say, data points can be represented in the new space, no matter what the original features are. This is a big advantage to handle some high dimensional data. Then, with the inner product as the metric to measure the adjacent weight between data points, we design the adja-cency matrix to be a low-rank form which means that if two data points are correlative ( W ij 0), they share at least one common anchor point, other-wise W ij = 0. By sharing the same anchors, data points have similar semantic concepts in a high probability as our consideration. Thus, our design is helpful to explore the semantic relationships in the data.
 This formula naturally preserves some good properties of W : sparseness and nonnegativeness. The highly sparse ma-trix Z makes W sparse, which is consistent with the ob-servation that most of the points in a graph have only a small amount of edges with other points. The nonnega-tive property makes the adjacent weight more meaningful: in real world data, the relationship between two items is always positive or zero, but not negative. Moreover, non-negative W guarantees the positive semidefinite property of the graph Laplacian in many graph-based algorithms [18].
In large scale cases, it is expensive to keep the matrix W as large as n  X  n in memory, while in our construction, we just need to save the d  X  n matrix Z .If d n ,wecan dramatically reduce the memory cost.
After graph construction, the main computational cost for manifold ranking is the matrix inversion in equation (2), whose complexity is O ( n 3 ). So the data size n can not be too large. Although we can use the iteration algorithm, it is still inefficient for large scale cases.

One may argue that the matrix inversion can be done off-line, then it is not a problem for on-line search. However, off-line calculation can only handle the case when the query is already in the graph (an in-sample). If the query is not in the graph (an out-of-sample), for exact graph structure, we have to update the whole graph to add the new query and compute the matrix inversion in equation (2) again. Thus, the off-line computation doesn X  X  work for an out-of-sample query. Actually, for a real CBIR system, user X  X  query is always an out-of-sample.

With the form of W = Z T Z ,wecanrewritetheequa-tion (2), the main step of manifold ranking, by Woodbury formula as follows. Let H = ZD  X  1 2 ,and S = H T H ,then the final ranking function r can be directly computed by r  X  =( I Proof. Just check that ( I n  X   X H T H )times( I n  X  H T ( HH
I ( I n  X   X H T H )( I n  X  H T ( HH T  X  1  X  I d )  X  1 H ) = I n  X   X H T H +  X H T (  X  1  X  I d + HH T )( HH T  X  1  X  I d = I n  X   X H T H +  X H T H = I n
By equation (9), the inversion part (taking the most com-putational cost) changes from a n  X  n matrix to a d  X  d matrix. If d n , this change can significantly speed up the calculation of manifold ranking. Thus, applying our proposed method to a real-time retrieval system is viable, which is a big shortage for original manifold ranking.
During the computation process, we never use the adja-cency matrix W . So we don X  X  save the matrix W in memory, but save matrix Z instead. In equation (9), D is a diagonal matrix with D ii = n j =1 w ij .When W = Z T Z , where z i is the i th column of Z and v = n j =1 z j .Thuswe get the matrix D without using W .

A useful trick for computing r  X  in equation (9) is running it from right to left. So every time we multiply a matrix by a vector, avoiding the matrix -matrix multiplication. As a result, to compute the ranking function, EMR has a complexity O ( dn + d 3 ).
In this part, we make a brief summary of EMR applied to pure content-based image retrieval. To add more informa-tion, we just extend the data features.

First of all, we extract the low-level features of images in the database, and use them as coordinates of data points in the graph. We will further discuss the low-level features in section 5. Secondly, we select representative points as anchors and construct the weight matrix Z by kernel regres-sion with a small neighborhood size s . Anchors are selected off-line and does not affect the on-line process. For a stable data set, we don X  X  frequently update the anchors. At last, after the user specifying or uploading an image as a query, we get or extract its low-level features, update the weight matrix Z , and directly compute the ranking scores by equa-tion (9). Images with highest ranking scores are considered as the most relevant and return to the user.
In this section, we show several experimental results and comparisons to evaluate the effectiveness and efficiency of our proposed method EMR on a real world image database. All algorithms in our experiments are implemented in MAT-LAB 9.0 and run on a computer with 2.0 GHZ(  X  2) CPU, 8GB RAM.
The image database consisting of 7,700 images from COREL image database. COREL is widely used in many CBIR works [12, 19, 31]. All of the images are from 77 different categories, with 100 images per category. Images in the same category belong to the same semantic concept, such as beach, bird, elephant and so on. That is to say, images from the same category are judged relevant and otherwise irrelevant. In Figure 1, we randomly select and show nine image samples from three different categories.
 Figure 1: COREL image samples randomly selected from semantic concept beach, bird and elephant.
There are many measures to evaluate the retrieval results such as precision , recall , Fmeasure , MAP and NDCG [21]. But for a real CBIR application, especially for a web application, not all the measures are meaningful.
Generally, the image retrieval engine present at most 20 images in a screen without scrolling. Too many images in a screen will confuse the user and drop the experience evi-dently. Images in the top pages attract the most interests and attentions from the user. So the precision at K metric is significant to evaluate the image retrieval performance.
MAP (Mean Average Precision) provides a single-figure measure of quality across recall levels. Among evaluation measures, MAP has been shown to have especially good dis-crimination and stability. For a single query, Average Pre-cision is the average of the precision value obtained for the set of top k items existing after each relevant item is re-trieved, and this value is then averaged over all queries [21]. That is, if the set of relevant items for a query q j  X  Q is { d 1 ,...,d m j } and R jk is the set of ranked retrieval results from the top result until you get to item d k ,then
NDCG is designed for situations of non-binary notions of relevance. It is not suitable here.
To show EMR X  X  effectiveness, several methods are com-pared: MR, FMR [11] and SVM for image retrieval (base-line) [25]. Given that the goal of our paper is to improve MR, so the main comparative method is the original MR.
FMR firstly partitions the data into several parts (clus-tering) and computes the matrix inversion by a block-wise way. It uses the SVD technique which is time consuming. So its computational bottleneck is transformed to SVD. When SVD is accurately solved, FMR equals MR. But FMR uses the approximate solution to speed up the computation. We use 10 clusters and calculate the approximation of SVD with 10 singular values. Higher accuracy requires much more computational time.

SVM can be easily transformed to a query-based ranking algorithm, and has been successfully applied in image re-trieval. With the specified relevant/irrelevant (to the query) information, a maximal margin hyperplane is build to sepa-rate the relevant from irrelevant images, and then the most relevant images are the ones farthest from the SVM bound-ary. We use the well known LIBSVM toolbox [4] and select the RBF kernel. The parameters C and g in LIBSVM are 50 and 0.5 respectively.
Here we discuss some important implementation issues for our experiments: low-level features of images and out-of-sample retrieval. Low-level features representation of images is crucial for CBIR. If the low-level features can accurately measure the semantic distance between images, there is no need to design any new retrieval method. Unfortunately, low-level features always fail to capture the high-level semantic concepts. In our opinion, more than the low-level features, a good CBIR method should take the underlying structure of the data into account. In our experiments, we use a 64-dimensional color histogram and a 64-dimensional Color Texture Moment [12, 31] to represent the images.
For in-sample data retrieval, we can construct the graph and compute the matrix inversion part of equation (2) off-line. But for out-of-sample data, the situation is totally different. In [10], the authors solve the out-of-sample prob-lem by finding the nearest neighbors of the query and using the neighbors as query points. They don X  X  add the query into the graph, therefore their database is static. Moreover, their method may change the query X  X  initial semantic mean-ing. In contrast, we efficiently update the graph structure and use the new sample as a query for retrieval. The image database can be dynamically updated.

For one instant retrieval, it is unwise to update the whole graph or rebuild the anchors, especially on a large data set. We can adopt fast updating strategies as follows. Figure 2: Extend matrix W (MR and FMR) and Z (EMR) in the gray regions for an out-of-sample.
A fast but inexact strategyforMRandFMRisleaving the original graph unchanged and adding a new row and a new column to W .But k nearest neighbor relationship is not symmetric, therefore the query X  X  neighbors would lose some local information  X  its original neighbors X  weights decrease relatively and they may have k +1 nearest neighbors. While for EMR, each data point is independently computed. We just assign weights between the new query and its nearby anchors. That forms a new column of Z .Onepointhas little effect to the stable anchors in a large data set (e.g., cluster centers). Figure 2 shows the updating strategies. We firstly compare our proposed method EMR with MR, FMR and baseline on COREL database without relevance feedback. Relevance feedback asks users to label some re-trieved samples, making the retrieval procedure inconve-nient. So if possible, we prefer an algorithm having good performance without relevance feedback. For our method EMR, 1000 k-means anchors are used. Later in the model se-lection part, we find that using 800 or fewer anchors achieves acloseperformance.

An important issue needs to be emphasized: although we have the image labels (categories), we don X  X  use them in our algorithm, since in real world applications, labeling is very expensive. The label information can only be used to evaluation and relevance feedback.

At the beginning of the retrieval, we don X  X  have the user specified relevant/irrelevant information. To apply SVM, the pseudo relevance feedback [21] strategy, also known as blind relevance feedback is adopted. We rank the images in the database according to their euclidean distances to the query, and then the nearest ten images are considered as relevant and the farthest ten are irrelevant. With such an approach, we run SVM as normal at the beginning, and the user gets images without any additional interaction.
Each image is used as a query and the retrieval perfor-mance is averaged. Figure 3 prints the average precision (at 10 to 80) of each method and Table 1 records the average values of recall, F1 score (at 10 to 80), and the values of MAP with all relevant images. It is easy to find that the performance of MR and EMR are very close, while FMR lose a little precision. All the three algorithms are better than the baseline method in the whole range.
 Figure 3: Retrieval precision at top 10 to 80 returns of MR (left), EMR, FMR and Baseline.
 Table 1: The average values of Recall, F1 on top 10 to 80, and MAP.

The anchor points are computed off-line and do not affect the current on-line retrieval system. To speed up the selec-tion, we can use fast k-means strategy or just select random anchors. The fast k-means strategy we used is very simple and effective: setting the number of maximum iterations of k-means to 10 (default 100). Experiments results in Fig-ure 4 show that the performance of fast k-means or random anchors is close to normal k-means anchors. Using random anchors dramatically decreases the anchor points selection cost when the data size is extremely large. Thus, cost and performance are trade-offs in many situations. Figure 4: Performance of EMR with random an-chors vs. k-means (fast and normal) anchors. Note that using random anchors achieves a close perfor-mance to k-means anchors.

To see the performance distribution in the whole data set more concretely, we plot the retrieval precision at top 10 returns for all 77 categories in Figure 5 without relevance feedback. As can be seen, the performance of each algo-rithm varies with different categories. We find that EMR is fairly close to MR in almost every category, and on some categories, EMR is better than MR. But for FMR, the dis-tribution is totally different. In this section, we compare the response time of MR, FMR and EMR when handling a new sample. We use a serial number of COREL samples, from 1,500 to 7,500, to build the graph and test 100 out-of-samples. The response time (in seconds) for each sample is averaged and showed in Table 2. To test EMR X  X  efficiency in very large data sets, we use 50,000 to 200,000 synthetic data in 128 dimensions. Since EMR has a closed form solution, so the response time is reliable and comparable  X  no matter the data is real or synthetic.
 Table 2: Response time in seconds, MR (closed form in MR c and iteration scheme in MR it ), FMR, and EMR (1000 anchors in EMR 1 k and 800 anchors in EMR 800 ) with different graph size. The last three lines use synthetic data.

Size MR c MR it FMR EMR 1 k EMR 800 1500 1.283 1.685 0.821 0.462 0.264 2500 5.416 5.463 3.014 0.517 0.308 3500 14.233 12.437 7.507 0.576 0.358 4500 29.712 23.355 15.169 0.638 0.402 5500 53.854 39.559 26.866 0.710 0.447 6500 85.319 59.296 42.944 0.769 0.495 7500 129.715 85.916 65.124 0.833 0.542 50k ---3.869 2.700 100k ---7.375 5.201 200k ---14.534 10.183
When the data size is large, it is extremely costly to save matrix W  X  R n  X  n in memory for MR and FMR. Even if we could save W in memory, the computational time is too long. But for EMR, we just save the smaller matrix Z  X  R d  X  n , and the time is acceptable.

With different graph size, we fix the number of anchors to 1000 and 800 for convenience. Actually we could use fewer anchors to achieve a good performance in many cases. For MR, we use both the closed form and the iteration scheme. We record the response time of MR by closed form in MR c and by iteration scheme in MR it . The stop condition for MR it is defined as r t +1  X  r t &lt; ,where t is the iteration step and is a threshold. We use a very loose threshold 10  X  4 . With smaller threshold, MR converge. We find that, the computational time of MR or FMR increases very fast, while our method increases slowly. Note that the time of extracting low-level features from the query image is not included in the response time, since it is not the focus of our ranking method.
Model selection plays a key role to many machine learning methods. In some cases, the performance of an algorithm may drastically vary by different choices of the parameters, thus we have to estimate the quality of the parameters. In this subsection, we evaluate the performance of our method EMR with different values of the parameters. For each single value, EMR is run independently 10 times and the results are averaged.

There are three parameters in our method EMR: s ,  X  , and N . Parameter s is the neighborhood size in the anchor graph. Small value of s makes the weight matrix Z very sparse. Parameter  X  is the tradeoff parameter in EMR and MR. Parameter N is the number of anchor points. For con-venience, the parameter  X  is fixed at 0.99, consistent with the experiments performed in [9,36,37]
Figure 6 shows the performance of EMR (Precision at 10) by k-means anchors at different values of s . We find that the performance of EMR is not sensitive to the selection of s when s&gt; 3. With small s , we can guarantee the matrix Z highly sparse, which is helpful to efficient computation. In our experiments, we just select s =5. Figure 6: Retrieval precision versus different values of parameter s . The dotted line represents the base-line.

Figure 7(a) shows the performance of EMR (Precision at 10) versus different number of anchors in the whole data set. From the figure, we find that the performance increases very slowly when the number of anchors is larger than 400 (ap-proximately). In previous experiments, we fix the number of anchors to 1000. Actually, a smaller number of anchors, like 800 or 600 anchors, can achieve a close performance. With fewer anchors, the graph construction and out-of-sample re-trieval cost will be further reduced. To further investigate how many anchors are required, we evaluate EMR on half of the data set (77 categories, 50 images per category, 3850 images in total) in Figure 7(b). The number of categories is the same and the data size is reduced by half. However Figure 7: Retrieval precision versus different num-ber of anchor points in (a) 7700 images and (b) 3850 images. The dotted lines represent baseline perfor-manceineachcase. the result is similar  X  the performance increases very slowly when the number of anchors is larger than 400. This demon-strates that the number of required anchors is affected by the underlying structure, but not easily proportional to the size of the data set.
Relevance Feedback [24] is a powerful interactive tech-nique used to improve the performance of image retrieval systems. With user provided relevant/irrelevant informa-tion on the retrieved images, The system can capture the semantic concept of the query more correctly and gradually improve the retrieval precision.

Applying relevance feedback to EMR is extremely sim-ple. We update the initial vector y and recompute the rank-ing scores. We use an automatic labeling strategy to simu-late relevance feedback: for each query, the top 20 returns X  ground truth labels (relevant or irrelevant to the query) are used as relevance feedbacks. It is performed for two rounds, since the users have no patience to do more. Images have been labeled in the first round are excluded in the second round. The retrieval performance are plotted in Figure 8. To avoid changing the original query X  X  semantic meaning, the initial scores of the query and the feedbacks are 10 and  X  1 respectively.

By relevance feedback, original MR receives higher im-provement than any other methods. But, relevance feed-back is designed for real interactive systems in which the data size is always large. Original MR is too slow for these systems. See Table 2, when the data size is 7500, for one out-of-sample retrieval, MR c needs more than 100 seconds. The meaning of applying relevance feedback to original MR is limited. Our method is a little worse than MR, but it is much faster than MR and better than the rest two. We leave the question of how to make further efforts to EMR X  X  performance by relevance feedback in our future works. In this paper, our focus is to speed up manifold ranking, but not the relevance feedback. Figure 8: Average precision-scope curves for each methods after two relevance feedbacks.
We show two real image retrieval cases in this part to see the performance of EMR intuitively. The first case in Fig-ure 9 succeeds to retrieve relevant images by EMR without relevance feedback. The second case in Figure 10(a) is failed to capture the query X  X  concept at first, but improved greatly after two feedback rounds in Figure 10(b).
 An interesting thing is that the distance metric defined by EMR (we name it manifold distance) is very different with traditional metrics (e.g., Euclidean distance) used in many other retrieval methods. Euclidean distance only considers the data similarity, but manifold distance tries to capture the semantic relevance by the underlying structure of the data set. For example, the first retrieved image in the first case is the 34th image retrieved by Euclidean distance.
In the second case, the query is confused with many other concepts, including the oil painting and the elephant. The similar color and texture features make images in those con-cepts receiving higher ranking scores. At the same time, this case indicates that EMR is confused by the low-level features and misses the dominant concept antelope of the query, when there is no manual help. Fortunately, EMR is easy to integrate the label information  X  after two relevance feedbacks, the concept is captured successfully by EMR.
Many real world applications have highly complex graph structure, like a social network or a web link graph. In such circumstances, the graph is existing and has very large size. If we have rich information about each node, we use EMR for efficient retrieval. But if we have no more information except the graph, how can we accelerate manifold ranking? image with red box is the query, and the rest are the top 10 returns. The first image with red box is the query.

Adjacency matrix W is symmetric. Its SVD decomposi-tion is W = U  X  U T ,where U  X  R n  X  k and  X   X  R k  X  k .Let which is very efficient if k n . But computing SVD of ma-trix W requires O ( n 3 ) operations, which is the same with the original matrix inversion. If we can fast decompose W , we save the computational time. Fortunately, W is highly sparse and may have a very low rank. Thus, some fast low-rank matrix approximation methods may help. We can ar-range matrix W to be where G is a l  X  l symmetric matrix from W , l n .The Nystr  X  om method [30] computes SVD of G ( G = U G  X  G U and approximates singular values and singular vectors of W by  X  = ( n l ) X  G and U = l n CU G  X   X  1 G .When k singular vectors are used, the cost of this algorithm is O ( l 3 + nlk ). Another alternative method is the Column-sampling method [6]. It approximates the spectral decomposition of W by using the SVD of C directly ( C = U C  X  C V T C ). That is,  X = n l  X  C and U = U C .Itscostis O ( nl 2 ). These two methods both can be used to speed up the computation.
In this paper, we propose the Efficient Manifold Rank-ing algorithm which extends the original manifold ranking to handle scalable data sets. We apply EMR to a content-based image retrieval application based on a real world im-age database. EMR tries to address the shortcomings of original manifold ranking from two perspectives: the first is scalable graph construction; and the second is efficient computation, especially for out-of-sample retrieval. Exper-imental results demonstrate that EMR is feasible to large scale real world image retrieval systems  X  it significantly re-duces the computational time, as well as the storage space. Actually, our new design of the adjacency matrix can be used to many other graph-based algorithms, and EMR is also feasible to other types of information resources.
In our future work, we will test more visual features and evaluate our method on other databases. Moreover, many social network sites like Flickr allow users to annotate im-ages with free tags, which significantly help us to under-stand semantic concepts of images. Thus, for images having tag information, we can combine the image features and tag information to improve retrieval performance. Another extension of our work is the distributed computation, an enterprise solution for web scale retrieval systems.
This work is supported by Program for New Century Ex-cellent Talents in University (NCET-09-0685). [1] S. Agarwal. Ranking on graph data. In Proceedings of [2] S. Brin and L. Page. The anatomy of a large-scale [3] J. Bu, S. Tan, C. Chen, C. Wang, H. Wu, L. Zhang, [4] C.-C. Chang and C.-J. Lin. LIBSVM: a library for [5] R. Datta, D. Joshi, J. Li, and J. Wang. Image [6] A. Frieze, R. Kannan, and S. Vempala. Fast Monte [7] W. Gao, P. Cai, K. Wong, and A. Zhou. Learning to [8] Z. Guan, J. Bu, Q. Mei, C. Chen, and C. Wang. [9] J.He,M.Li,H.Zhang,H.Tong,andC.Zhang.
 [10] J. He, M. Li, H. Zhang, H. Tong, and C. Zhang. [11] R. He, Y. Zhu, and W. Zhan. Fast Manifold-Ranking [12] X. He, D. Cai, and J. Han. Learning a maximum [13] J. Jeon, V. Lavrenko, and R. Manmatha. Automatic [14] T. Kanungo, D. Mount, N. Netanyahu, C. Piatko, [15] J. Kleinberg. Authoritative sources in a hyperlinked [16] B.Li,E.Chang,andC.Wu.DPF-aperceptual [17] D. Liu, X. Hua, L. Yang, M. Wang, and H. Zhang. [18] W. Liu, J. He, and S. Chang. Large graph [19] Y. Liu, D. Zhang, G. Lu, and W. Ma. A survey of [20] D. Lowe. Object recognition from local scale-invariant [21] C. Manning, P. Raghavan, and H. Sch  X  utze. [22] J. Ponte and W. Croft. A language modeling approach [23] S. Roweis and L. Saul. Nonlinear dimensionality [24] Y. Rui, T. Huang, M. Ortega, and S. Mehrotra. [25] S. Tong and E. Chang. Support vector machine active [26] M. Tsai, T. Liu, T. Qin, H. Chen, and W. Ma. FRank: [27] R. Veltkamp and M. Tanase. Content-Based Image [28] X. Wan, J. Yang, and J. Xiao. Manifold-ranking based [29] L. Wang, J. Lin, and D. Metzler. Learning to [30] C. Williams and M. Seeger. Using the Nystr  X  om [31] H. Yu, M. Li, H. Zhang, and J. Feng. Color texture [32] K. Yu, J. Bi, and V. Tresp. Active learning via [33] K. Yu, T. Zhang, and Y. Gong. Nonlinear learning [34] X. Yuan, X. Hua, M. Wang, and X. Wu.
 [35] K. Zhang, J. Kwok, and B. Parvin. Prototype vector [36] D. Zhou, O. Bousquet, T. Lal, J. Weston, and [37] D. Zhou, J. Weston, A. Gretton, O. Bousquet, and
