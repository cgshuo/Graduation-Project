 Predicting the values of continuous variable as a function of sev-eral independent variables is one of the most important problems for data mining. A very large number of regression methods, both parametric and nonparametric, have been proposed in the past. How-ever, since the list is quite extensive and many of these models make rather explicit, strong yet different assumptions about the type of applicable problems and involve a lot of parameters and options, choosing the appropriate regression methodology and then specifying the parameter values is a none-trivial, sometimes frus-trating, task for data mining pr actitioners. Choosing the inappro-priate methodology can have rather disappointing results. This issue is against the general utility of data mining software. For example, linear regression methods are straightforward and well-understood. However, since the linear assumption is very strong, its performance is compromised for complicated non-linear problems. Kernel-based methods perform quite well if the kernel functions are selected correctly. In this paper, we propose a straightforward ap-proach based on summarizing the training data using an ensemble of random decisions trees. It re quires very little knowledge from the user, yet is applicable to every type of regression problem that we are currently aware of. We have experimented on a wide range of problems including those that parametric methods perform well, a large selection of benchmark datasets for nonparametric regres-sion, as well as highly non-linear stochastic problems. Our results are either significantly better than or identical to many approaches that are known to perform well on these problems.
 H.2.8 [ Database Management ]: Database Applications -Data Min-ing Algorithms regression, random, decision trees Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00.
Regression modeling has been an integral part of data mining and statistical analysis. With n observations between the depen-dent variable y and a vector x =( x 1 ,...,x k ) of covariants (or independent variables), regression relationship can be modeled as y = m ( x i )+ i where m is the unknown regression model and  X  X  are the observation errors. There are extensive use of regression analysis in countless number of applications in biometry, econo-metrics, financial services, engineering, health sciences, mathemat-ics, and etc. In this paper, we propose a remarkably simple and fully automatic, but highly accurate and efficient regression tech-nique that is applicable to all known areas of regression applica-tions that we are aware of. Compared with many state-of-the-art re-gression algorithms reviewed below, the proposed method requires little next to none knowledge from the user and fully automates the model construction process, but returns either signifcantly better or similar results as more sophisticated approaches in a large number of problems. It works particularly well for highly non-linear high dimensional problems.
Since the first documented work on regression in 1898 by Schus-ter, a large number of regression methods have been proposed. However, this is still a very active topic due to its importance in science as well as the fact that none of the existing approaches are completely general and satisfactory enough.

Parametric Models There are two main subdivision of regres-sion models, although many methods cross both categories. In  X  X arametric regression X  (sometimes also called  X  X enerative X  mod-els), the form of the functional relationship between the dependent and independent variables is assumed to be known, but contain pa-rameters whose value is unknown and capable of being estimated from the training data. For example, fitting a straight-line to a num-ber of points. Any function essentially can be used as a paramet-ric model. The simplest form of parametric models is linear re-gression which assumes that the expected value of y given the de-pendent variables x is normal and linearly associated with each dependent variable, E ( y | x )= b 0 + lihood estimates or MLE of b j  X  X  can be found by least squares. In reality, however, not every event is normal and not every asso-ciation has to be linear, and more sophisticated parametric func-tions are needed. In practice, we normally decompose the para-metric model into several subcomponents of basic functions, then simplify the construction of sophisticated functions by combining these basic functions in some ways and under some assumptions. In generalized linear model (GLM or GLIM) [Nelder and Wed-derburn, 1972, McCullagh and Nelder, 1989], a dependent vari-able y is linearly associated with values on the x variables while the functional relationship is assumed to be nonlinear or formally, E ( y | x )= G ( b 0 + G  X  1 () , is called the link function, and is known and pre-specified. In GLM, the distribution of the dependent variable is no longer re-stricted to be Gaussian as in linear regressions, and in theory, the link function can be any form. Widely used forms of GLM include normal, Gama, binomial and Poisson distributions, as well as in-verse, log, and identify link functions.

After a most suitable parametric model is chosen, usually sub-jectively, parametric regression is transformed into an MLE prob-lem to select a set of parameter values to minimize some given criteria, e.g., least square for linear regression and weighed least square for GLM. For problems with known and visually observ-able behaviors, for example, normal or Poisson distribution, linear dependency, periodical smoother functions, parametric modeling can be quite powerful. However, given a new problem with little knowledge about the behavior, especially those with large number of independent variables, choosing the most parametric model is a none-trivial task and cannot be easily automated. The datasets in Section 4.1 are two such examples.

Parametric methods work very well when the selected parametric model is the correct model to explain the physical event. Success-ful use usually requires extensive knowledge of the properties of available parametric methods as well as a good understanding of the physical laws behind the monitored phenomenon (i.e., how dif-ferent parameters interact with each other), but the exhaustive list of available methods is still not guaranteed to cover every possi-ble aspect of physical events. When the parametric model happens to be the wrong model, the result will be unsatisfactory. As dis-cussed earlier, linear regression methods are straightforward and well-understood. However, since the linear assumption of the in-teractions among independent variables is very strong, its perfor-mance is compromised for complicated non-linear problems. Sim-ilarly, kernel-based methods can perform quite well if the kernel functions are chosen appropriately.

The central issue for parametric regression is  X  X oodness of fit X  or the  X  X ias X  of the model, which quantifies the systematic error to use the model to represent a physical event. The standard MSE bias and variance decomposition as well as residual plots are com-monly adopted experime ntal approaches. In addition, information theoretic-based standards, such as Aikaike information criterion (AIC) and Bayesian information criterion (BIC), quantifies the gen-erality of a model by weighting its error and the number of parame-ters. Under the assumption of  X  X ormally distributed errors X , AIC = 2 p + n ln ( RSS ric model, and RSS is residual squared error. Similar to AIC, BIC = p ln ( n )+ n ln ( RSS n ) . Both AIC and BIC are suitable criterion to measure the generality on the same family of functions. They balance model complexity with generalization error. There is still much active work being done on this topic, for example, Neyman test based approaches [Fan and Huang, 2001].

Another practical restriction is that most parametric models do not handle categorical features directly. The simple approach is to assign an integer value to each unique category, but this imposes an order on the categorical values and alternative encoding may run into different results. To reduce the amount of arbitrariness, it is helpful to use a vector to encode a categorical feature, such as (0 , 1 , 0) to represent  X  X us X  when the complete list of values are car, bus, and train. When there are a large number of unique categorical values, the tranformed feature set tends to be huge.

For the above reasons and others, parametric models are very well known in statistics community but comparatively less known in machine learning and data mining community. Data mining ap-plications require algorithms that are simple and automatic, but still return satisfactory results.

Nonparametric Models Compared with parametric models, the family of nonparametric methods (also called  X  X escriptive X  models by some researchers) do not clearly impose an exact form. The dis-tinguishing feature is that there is no (or very little) a priori knowl-edge about the form of the true function which is being estimated. The function is still modeled using an equation containing free pa-rameters but in a way which allows the class of functions which the model can represent to be very broad. Typically this involves using many free parameters which have no physical meaning in re-lation to the problem. In parametric regression there is typically a small number of parameters and often they have physical inter-pretations. Estimating values for parameters is never the primary goal in nonparametric modeling, but the primary goal is to estimate the underlying function (or at least to estimate its output at certain desired values of the input). On the other hand, the main goal of parametric regression can be, and often is, the estimation of param-eter values because of their intrinsic meaning.

There is an extensive list of nonparametric methods differing in the choice of function familities and combination of function fam-ilies. One of the simplest nonparametric regression methods is re-gression trees, pioneered by CART [Breiman et al., 1984]. When constructing the regression tree, it chooses one independent vari-able as the splitting feature that can minimize the variance of pre-dictions after the split. CART always predicts with the average de-pendent variable value of all examples from the training set sorted at the leaf, since this minimizes squared error. Although quite sim-ple and straightforward, CART usually returns okay results, but there is usually much room for improvement. One of these signifi-cant improvements is GUIDE as discussed below.

Using decision trees to group examples with similar indepen-dent feature values, GUIDE replaces the average value prediction in the leaf node of CART by some parametric models [Loh, 2002]. The current release of GUIDE includes linear, piece-wise linear, quantile, Poisson, and proporti onal hazard regressions. Each leaf node has its own parametric model (same kind for every leaf node but different parameter values) estimated with a pre-specified sub-set of typically non-categorical independent features or converted 0 , 1 vectors for categorical features, using the subset of training examples sorted into the leaf node. GUIDE offers three levels of tree pruning prior to fitting par ametric models in the leaf nodes, no-pruning, pruning by cross-validation or pruning by a hold-out dataset. All these options, i.e, parametric model at leaf nodes, in-dependent variable, and pruning level, need to be specified by the user prior to constructing the tree. For the datasets we have studied, the regression results are quite sensitive to the choice of paramet-ric model at the leaf nodes (i.e, linear, Poisson or the others), and less sensitive on the choice of feature subsets. We find that GUIDE with linear regression leaf nodes constructed from the complete set of independent variables return the most stable and decent results.
The splitting criteria that CART and GUIDE use is a recursive partitioning regression (RPR) procedure that splits along each co-ordinate or independent variable. The instance space is split into disjoint hyper-rectangles along each coordinate. The obvious limi-tation is that the split is only parallel to each coordinate projection. Regression problems that is piece-wise constant but in a rotated coordinate will be not correctly approximated, for example, x This motivates the use of univariate ridge functions g to project the coordinates, m ( x )= linear combination of coordinates need to be chosen.

CART belongs to a large family of  X  X dditive models X  that relax linear regression by allowing the dependency on each independent variable to be functional, in other words, E ( y | x )= b F ( x i ) where each F i ( x ) is nonparametric  X  X moother X  function. Many nonparametric regression can be described in this form, at least asymptotically. Their main difference lies in the choice of smoothers. Examples of additive models include, but is not limited to, Kernel-smoothing, RBF, neural network, nearest neighbor, or-thogonal series estimator, spline smoothing. For an extensive treat-ment of this subject, please refer to [Hardle, 1990].

Several work combines parametric and nonparametric regres-sion. Combining the notions of both GLM and additive models, generalized additive model or GAM relaxes the linear combination of x i in GLM to be functional [Hastie and Tibshirani, 1986]. In other words, G  X  1 ( y )= b 0 + models maximize the quality of prediction of a dependent variable y from various distributions, by estimating nonparametric functions of the independent variables which are  X  X onnected X  to the depen-dent variable via a link function. Other approaches include partial linear modeling and shape-invariant modeling.

Since some nonparametric models, esp. CART and GUIDE, do not require much specialized knowledge about either the applica-tion or the properties of modeling techniques, they receive more interests in the data mining community where ease of use and au-tomation is one of the highest priorities. In all the commercial data mining software that we are aware of, regression tree can always be found. Compared with parametric modeling, the main concern for nonparametric model is possibly high variance rather than bias.
To solve the difficult and sometimes frustrating process to choose among many parametric and nonparametric regression methods, then either determine the component functions or select the right options and parameter values for a chosen methodology, we pro-pose the use of random decision trees as a general framework for regression. Our goal is high accuracy, simplicity and efficiency.
Based on the acceptance of regression trees in the data mining community and the recent results of randomized decision trees or RDT for classification and conditional probability estimation prob-lems [Fan et al., 2003, Fan et al., 2005, Zhang et al., 2005, Liu, 2005], we propose an extension to use random decision trees for general-purpose regression problems.

The procedure to generate random trees for regressions is ex-actly the same as for classification and posterior probability esti-mation problems. It summarizes the training data into k = trees randomly. At each step, a feature is selected randomly with-out testing any splitting criterion, such as the variance reduction criterion employed by CART. A categorical feature can be used only once on any decision path starting from the root of the tree to the current node. However, a continuous variable can be used multiple times, but each time a random threshold to split the data is chosen. The tree stops growing if the number of examples in the node is less than or equal to a minimal threshold. For regres-sion problems, each tree outputs the average value of the dependent variable of all examples in the classifying leaf node, just as CART. Then the outputs from multiple random trees are averaged as the final regression output. Our claim is that this straightforward pro-cedure is general enough to solve a large family of regression prob-lems including those highly non-linear problems, accurately and efficiently. Since the precise definition of  X  X raining X  implies  X  X n-struction or discipline X  according to Webster X  X , and the procedure to construct random trees is less goal-oriented and less disciplined than traditional decision trees, a more appropriate term suggests data  X  X ummarization X  with random decision trees. Indeed, deci-sion tree itself is used just as a  X  X ata structure X  to group examples with similar values.

There is no difference in how the data is summarized by  X  X andom decision trees X  for regression, classification, probability estimation problems, and possibly other problems (such as multi-class multi-label prediction). The only difference is in the prediction stage, i.e., predicting the average dependent variable value in leaf node for regression problems, and predicting the ratio of examples per class for classification and posterior probability estimation problems. In each case, however, the output from multiple trees are averaged as the final output.

An argument that splitting criterion, such as variance-reduction, information gain, etc, is more a preference and belief rather than law of nature can be found in [Fan et al., 2003]. In particular, it is argued that when perfect model that makes no mistakes are impossible, there are many many optimal models that can equally minimize a given loss function. Using splitting criterion to grow trees increases the probability to find such an optimal model, how-ever, there is no guarantee. Importantly, each random decision tree is only random in its structure. But all the informations inside the tree still matches or fits the training data just like a traditional de-cision tree. Each random decision tree makes predictions based on the same data as the traditional single decision tree.

Randomized decision tree is not the only approach for random-ization, however it is probably one of the  X  X ost random X  that we are aware of. Other approaches for randomization independently proposed by various researchers are reviewed and studied in [Fan et al., 2005, Liu, 2005]. In particular, it is worthwhile to mention bagging and random forest [Breiman, 2001], feature subset ran-domization [Amit and Geman, 1997] among others. For regression problem, random forest (RF) for regression is the closest work to random decision trees [Breiman, 2001, Segal, 2004]. However, there are a few important distinctions. First, each tree in random forest is trained from one bootstrap sample of the training set. But each tree in RDT is trained from the same original training set. In random forest, at each node, it chooses a feature to minimize the variance among a subset of features randomly selected at the node. The size of the subset need to be pre-specified by the user and Breiman suggests 80% of the number of available features at the node. When the subset contains just 1 feature, RF is still differ-ent from RDT in two ways, 1) the decision threshold by RF is not random, but chosen to minimize th e variance after the split, 2) each tree is still computed from bootstraps.
Previously, several explanations were given to explain why RDT works for both classification problems and posterior probability es-timation [Fan et al., 2005, Liu, 2005]. As a summary, each random decision tree is used more as a data structure to group and summa-rize the training data rather than a clearly motivated hypothesis in traditional decision tree learning to maximize/minimize some split-ting criterion, such as information gain, gini index, variance, etc. The randomization procedure itself reflects the fact that the same dataset can be summarized and partitioned in many many ways. Since randomization do not specify any particular ways and orders on how the training data should be summarized, it imposes less or weaker  X  X nductive or hypothesis bias. X  Statistically, each indepen-dently constructed decision tree is quite uncorrelated, and it is well known that the variance can be significantly reduced when these trees are combined. That is also one of the key arguments given by Figure 1: 100 training examples for f ( x )=1+ x 2  X  50 x sin Figure 2: Series of RDT X  X  to approximate a univariate function 1+ x 2  X  50 x sin ( x Breiman for random forest [Breiman, 2001]. Many of these argu-ments for classification can be made similarly for regression prob-lems. In this paper, we instead choose to use a univariate function and a multi-variate function with one hidden variable to illustrate how RDTs actually approximate to the true function as more trees are being constructed.
We choose a highly nonlinear sinusoidal univariate function, f 1+ x 2  X  50 x sin ( x any monotone approximation systematically imperfect. We allow x to be continuous within [0 . 0 , 100 . 0] . The 100 randomly sam-pled training examples are plotted in Figure 1. It is not an artifact that there appear to be more examples towards the lower end of the curve. The sample procedure is random on ( x ,y ) , but not random just on the feature vector x . A separate testing data set contains an-other 10000 randomly sampled data points. Figure 2 illustrates the results of 1, 2, 10, and 30 random decision trees, and Figure 3 plots the results of CART and GUIDE with linear regression. The mean square error results for each method are summarized in Table 1.
The hyper-rectangular projection along axis by single decision trees, both RDT=1 and CART, partition the univariate independent variable x into multiple disjoint ranges. Each range correspond to one leaf node in the tree. Within each range, the decision tree pre-Figure 3: CART and GUIDE approximating to the same uni-variate function in Figure 2 Figure 4: Number of unique range bars for up to 30 RDTs mea-sured from the testing data dicts with the average dependent variable y value from all training examples in the same range. Visually, we observe the  X  X orizontal bars X  or h  X  X . The use of  X  X orizontal X  distinguishes from  X  X lanted bars X  by linear regression of GUIDE, as shown in the right plot of Figure 3. By comparing each horizontal bar with the training examples in Figure 1, it is interesting to examine which training examples are grouped together by the single decision trees into the same leaf nodes or horizontal bars, and from which examples the average values used for prediction are calculated.

For RDT, it is pre-specified that a node with  X  3 examples will not split anymore. Then the 100 training examples result in ap-proximately 33 to 34 horizontal bars on average. The RDT=1 in Figure 2 happens to contain 36 bars. Since each decision threshold is made by choosing an example in the node randomly, the  X  X idth X  | h | of a horizontal bar is at most the largest difference of any 3 con-secutive examples sorted by x or formally | h | X  max ( | x and can be as small as | h | X  min ( | x i +1  X  x i | ) . Comparing between RDT=1 and CART, the obvious difference is that CART concen-trates on each of 16 turning points or  X  X oints X  on the true function. In the same time, RDT=1 places horizontal bars around only 6 of these joints. This phenomenon is expected since splitting on exam-ples around joints reduces variance. GUIDE divides the range into fewer number of ranges since it needs significant number of ex-amples to compute the linear fit. As expected, many of its cutting points are somehow included by CART since both of them mean to reduce variance at the split.

Next, we study how RDT approximates the true function closer as more random decision trees are being constructed and added into the ensemble. When two RDT X  X  are averaged, as shown in the top right RDT=2 plot in Figure 2, the number of horizontal bars in-creases from 36 to 67. As suggested intuitively by its meaning, this is the number of  X  X nique values X  that can be predicted by RDT=2 1505.57 1205.54 999.24 817.38 864.77 866.03 on these 10000 testing examples. 1 With 10 and 30 random decision trees, these numbers are 317 and 816 respectively. As a summary, we plot the number of horizontal bars for 1 to 30 trees in Figure 4. Apparently, the plot is a perfectly linear fit. Approximately, there is an increase of 30 bars as one random tree is constructed.
The predicted value by each horizontal bar may not exist in the training data. With RDT=30, it makes 816 unique predictions on the 10000 testing examples, while the training data contains as small as 100 examples. Each RDT is randomly and independently constructed, and there is little correlation among multiple decision trees. For a set of examples with close x values, as more trees are added into the ensemble, it is becoming increasing unlikely that these examples will be predicted by exactly the same leaf node for each tree. Averaging their predictions provides extremely refined and continuous predictions. Both CART and GUIDE X  X  prediction are discontinuous due to their specific choices. CART uses limited number of horizontal bars, and it cannot be more than the number of training examples. Our choice to run GUIDE use linear func-tion to replace horizontal bars that can hardly connect continuously across neighboring bar . Table 1 summarizes mean squared error of RDTs, CART and GUIDE. The single random decision tree ap-proximately doubles the MSE of CART and GUIDE. As more trees are being constructed and added into the ensemble, the error start to decrease. With 30 random decision trees, RDT X  X  MSE is about 817, while CART and GUIDE each hold around 865.
We study a multi-variate function with one hidden variable. In particular, we will show how random decision tree and CART can discover the hidden variable that GUIDE appears to ignore. The true function is defined as f ( v )=1+ v 2 +5 v sin ( v 2 hidden variable and related to the 5 dimensional feature vector via v = x 1 + x 2 + ... + x 5 . The component v 2 = difficulty for along-axis projection by hyper-rectangles. Both the training and test sets contain 10000 randomly sampled examples. In Figure 5, we plot how random decision tree approximates the true function dependent on the hidden variable, as more trees are being constructed and included into the ensemble. Similarly, we plot the results of CART and GUIDE in Figure 6.

Apparently, both RDTs and CART can discover the hidden vari-able and relate the hidden variable to the dependent variable, but GUIDE appears to decompose the function into three disjoint types of components. Two types of linear components approximate the ups and downs of sinusoidal characteristic, however, there is a third type of components that attempts to fit a monotonic function. Both RDTs and CART X  X  prediction, particularly that of RDTs, are rather continuous as a function of the hidden variable. However, the prediction by GUIDE is rather discontinuous or quite  X  X umpy and pulse-like X  for close values of the hidden variable. For example, in the range of 60-70, it alternates among 5 thick lines to predict the value as v increases. If we have experimented with the other
For RDT=2 and more random decision trees. if two horizontal bars (discontinuous of course) have the same predicted value, the number of unique value would be be counted twice. Figure 5: Series of RDT X  X  to approximate a multi-variate func-tion with univariate hidden variable f ( v )=1+ v 2  X  5 v sin where v = Figure 6: CART and GUIDE approximating the same multi-variate function with hidden variable in Figure 5 choices of GUIDE, such as fitting a different parametric regression model at the leaf node (Poisson reg ression, quantile, piece-wise lin-ear, etc), use a different subset of features to train these models, or different ways of pruning, we might have obtained better results. However, in reality, w ithout information about the true function, an exhaustive search will be rather inefficient. In conclusion, the mean square error for each method is summarized in Table 2. RDT=10,30 and CART are in similar range with RDT=30 being the least, while GUIDE X  X  error is significantly higher than any other methods.
We apply the standard approach for bias and variance decompo-sition for mean square errors to study how the bias and variance changes as more random decision trees are constructed. We use the multivariate function. We use the fixed test dataset of 10000 ex-amples and sample multiple training sets of 100 examples each for 100 times, train the 100 models and test each one on the same test set of 10000 examples. The results on bias and variance decompo-sition is summarized in Table 3. A s more random deci sion trees are constructed and appended into the ensemble. both its bias and vari-ance decrease. The reduction in variance is particularly significant when comparing RDT=2 with RDT=1. From the plots in Figure 5, it is clear that as the number of trees increase, the averaged predic-tion of multiple random trees gets cl oser to the true function. To be specific, when more trees are added. the curve of RDT gets thin-ner, implying a reduction in variance, and gets closer to the true function, implying a reduction in bias or systematic error. GUIDE Table 2: MSE on the multi-variate function with one hidden variable
RDT=1 2 10 30 CART GUIDE 1126.35 946.62 678.97 616.69 641.53 1633.77 has the lowest variance among all methods, however, its bias is sig-nifcantly higher than both RDT=30 and CART. For this particular dataset, it implies that using linear regression at the leaf node may not be a proper inductive bias.
To study the performance of RDT as a general regression meth-ods for a large variety of problems, we first compared RDT with parametric models on datasets that parametric methods are known to work well. We then compared with random forest, CART and GUIDE on twelve bench mark datasets used previously for regres-sion trees and other regression methods. In addition, we used a multi-variate, stochastic and hi ghly non-linear synthetic dataset. We also discuss some of the details on a storage server application.
We choose two examples that parametric modeling return satis-factory results and run RDT on the same datasets for comparison.
We use a time-series water evaporation problem to show the gen-erality of random decision tree as compared to parametric mod-els. We consider a time series of monthly evapotranspiration rates X ,t =1 ,..., 96 , obtained during the period of 1960 to 1970 at Yotvata, Souther Israel. The problem appears in [Kedem and Fokianos, 2002]. The data is analyzed by clipping the seasonal or else Y t  X  12 =0 for t =13 ,..., 96 . The clipped time se-ries Y t ,t =1 ,..., 84 , can be found on page 39 in [Kedem and Fokianos, 2002]. It is decided that either logistic or probit models are the most appropriate models for binary time series problems. Besides the value of the parameters, it yet needs to determine the number of parameters p or the number of previous Y t s per model, formally, Y t = m ( b 0 + model and 30 random decision trees with minimal number of ex-amples to split a node set at 3 are summarized in Table 4. Since RDT might overfit the training data, we also include the leave-one-out (or lv-1-out) results, since it is well know that leave-one-out is identical to AIC to measure model generality. The improvement over both logit and probit models are consistent for every choice on the number of independent variables. The reduction in MSE is particular obvious when the number of parameters p is large. Table 4: Logit, Probit and RDT on water evaporation problem
The second example is to fit a model to the monthly number of tourist arrivals in Cyprus from January 1979 to December 2000, a total of 264 observations in thousands. The data is available on page 167 of [Kedem and Fokianos, 2002], and is plotted in Fig-ure 7. After an extensive discussion of the best possible para-metric model to fit this trend data, the chosen parametric model is a Poisson regression with a few sinusoidal components as fol-lows: log  X  t ( b )= b 0 + b 1 cos ( 2  X t 12 )+ b 2 sin ( 2  X t b b log ( Y t  X  2 )+ b 10 log ( Y t  X  5 ) ,where I t =1 if t is January other-wise 0. Due to lagging, the number of examples used to train the model is n = 258 . The fit of this equation returns MSE=299.35. The training data for RDT uses the t or number of the month, ar-rivals of the previous 12 months (since there are 13 parameters in the parametric model, Y t  X  5 , b i ( i =0 ... 10) and t ), and the current month X  X  arrival number as dependent variable. With this format, the raw data is converted into a total of 252 instances. With 30 RDT and minimal number of examples to split a node to be 3, the train-ing MSE is 188.4 and leave-one-out MSE is 192.5. If the minimal number of examples is chosen to be 2, the training MSE is 53.4 and leave-one-out MSE is 64.5. When the minimal number is set to 1, the leave-one-out MSE is 30.1 and training MSE is trivially 0. It is important to understand that even with minimal number set to 1, each random tree is still not exactly the same. They still choose different features to split and different random thresholds. For each choice of RDT X  X  construction, the improvement over parametric model is obvious. Alternatively, we also experimented to use 6 previous months of arrival data instead of 12, the MSE increases about 20 to 30% from using 12 months data, but is still lower than the parametric model. We have experimented and compared RDT, CART, GUIDE and RF with a number of benchmark datasets, and some of then were used previously to evaluate the performance of regression tree al-gorithms [Segal, 2004].
The important characteristics of twelve datasets are summarized in Table 5. Most of these datasets are available from UCI. Basket-ball is provided as part of the GUIDE software package, amount is the part of KDD X 09 Donation Dataset to predict donation amount if someone does donate, ccf is a credit card transaction dataset to esti-mate the transaction amount, and diabetes is used in LARS. These datasets contain both continuous and categorical independent vari-ables. The number of independent variables ranges from 4 in 12. servoto40in2.aileron.
The stopping criterion for RDT is not to split a node if the num-ber of examples in the node is less or equal than the bigger of 2 Figure 7: Poisson regression and RDT on tourists arrival 4. auto MPG 200 198 5 2 5. basketball 131 132 18 7 and the 0.1% of the number of training examples. For random for-est, the minimal number of examples per node is set to 2, and the random subset of features to cons ider splitting crite rion is 80%, as suggested by Breiman. We construct 30 trees each for RDT and RF. Similarly, we also choose the minimal number of examples per node for CART as 2 to be comparable. For use of the GUIDE software, we use all the default selections and always construct a multiple linear regression model at the leaf node with all indepen-dent variables. As reviewed in Section 1.1, GUIDE also provide four other regression methods at the leaf node and other pruning techniques. We ran a rather exhaustive search to use many com-binations on the abalone and the servo datasets, 135 in total for abalone and 75 for servo by either using all features or leaving one feature out from the independent variable, and still find that linear model using all features return one of the best results. We didn X  X  run an exhaustive search to find a possibly better GUIDE model for other datasets since our main interest is on simple approach with decent results. Results are summarized in Table 6 (a) and (b) as well as Figure 8. For easier identification, each dataset is assigned an ID. We high-4. auto MPG 3.3275 3.4192 3.58194 3.3430 5. basketball 0.6271 0.6104 0.9223 0.6243 10. housing 5.2512 5.4127 7.0038 5.3812 light the algorithm with the least mean square error with bold fonts in Table 6 (a) and summarize pair-wise win(+)/loss(-) counts in Ta-ble 6(b). To be specific,  X  X in X  means the error rate is lower. In Fig-ure 8, we calculate the ratio of the error rate of RDT over the other three methods on each of the 12 datasets, such as RDT/GUIDE. When RDT X  X  error rate is lower, the ratio will be lower than the  X  y =1  X  baseline.

RDT has achieved the least error in 6 out of 12 datasets, followed by 4 for GUIDE, 1 for each by RF and CART. Between RDT and GUIDE, RDT has lower error rates in 8 out of 12 datasets. In the 4 cases that RDT loses. GUIDE X  X  error rate, on the other hand, is the lowest among all four algorithms. Expect for three datasets, 2. aileron (GUIDE lower), 6. cart (GUIDE lower), and 11. pol (RDT lower), their respective error rates are very close in value, which can be seen in Figure 8, i.e, their ratio are close to the  X  y The results of RDT and RF are rather close for every dataset, al-though RDT X  X  results are still lower than that of RF in 11 out of 12 dataset. The ratios of RDT over RF are rather close to the  X  y line as shown in Figure 8. This is actually not surprising since in essence both RF and RDT construct ensembles of decision trees and average the predicted results from each tree. Their difference, as summarized in Section 2, is how each tree is constructed. The improvement of RDT over the single decision tree CART is sig-nificant. This can be clearly observed by the distribution of  X  X rror ratio points X  between RDT and CART as shown in Figure 8. As a summary, 11 out 12 of these error ratio points are significantly lower than the  X  y =1  X  baseline, and 10 out of 12 points are the lowest among all 36 points.
In Table 7, we recorded the number of nodes in the decision trees constructed by RDT, RF and CART for each of the 12 datasets. The size of RDT and RF is the average number of nodes of all 30 trees. The results are quite interesting. The size of RDT and RF are rather  X  X egular X  and similar across different datasets. This is obvious by Figure 8: Error rate ratio between RDT and RF, CART and GUIDE on Benchmark Datasets examining the numbers in Table 7 (b) that is obtained by dividing the number of leaf nodes by the number of training size.
There are two datasets, 2. aileron and 7. elevator, that CART X  X  error rates approximately double the respective results of all other three methods. For the aileron dataset, CART constructed a tree with one single node that always predicts the global average, and for the elevator dataset, built a tree with 7 nodes. In the same time, RDT and RF return trees with about 3500 nodes for aileron, and either 859 by RDT or 122 by RF for elevator. For aileron, CART obviously didn X  X  find any  X  X ingle feature split X  that can reduce the variance after the splits, therefo re decided to produce just one sin-gle node. The dataset very likely belongs to the kind of  X  X OR X  type of problems that each feature by itself has no distinctive values, but multiple feature examined collectively can separate the examples. Since RDT doesn X  X  use a ny splitting criterion, but randomly choose feature and threshold value to split. Some of these random splits may be able to uncover these combinatorial feature tests. For cat-egorical combined feature tests , as long as, all features are in the same decision path starting from the root of a random tree, they will test exactly the same condition. For continuous combinatorial feature tests, as long as there are no strict requirement for  X  X xact X  decision thresholds, such as f 1  X  exactly a and f 2  X  exactly b , randomly chosen decision thresholds rather than a and b may also have some distinguishing value when used collectively. Each tree in RF is constructed from bootstrap samples. Bootstrap sample can change the distributions of feature tests and some features without distinguishing values in the training set may happen to have value when evaluated from the bootstraps.
We carefully designed a few highly nonlinear and stochastic prob-lems to test the performance and behavior of RDT and other algo-rithms. We include the detailed results of one dataset. For the same feature set, the true function stochastically chooses one of four multi-variate functions, i ln ( x i ) . Each feature x i is bounded by [0 ,m i ] ,andthemax-imal sum of every feature is therefore M =  X  = true value will be computed by one of the multi-dimensional func-tions respectively. We have tested a dimension of 5 and chosen 1.0, 2.0, 3.0, 4.0, and 5.0 to be the upper bound m i for each feature. Both training and testing data contain 10000 examples uniformly distributed in the feature space.
 The results are summarized in the plot as well as error table in Figure 9. The least error is obtained by both RDT and GUIDE with negligible difference, followed by RF. Each method appears to have grouped the data into four distinctive  X  X lusters. X  The predictions by CART and RF obviously have much larger variance than that of RDT and GUIDE. From the prediction plots, their  X  X pening angles X  are wider than that of RDT and GUIDE. RF clearly have  X  X oved X  the predictions of many examples by CART  X  X loser X  to the perfect line y = x , resulting in its lower error rate, i.e., 76.76 vs. 91.03. RDT and GUIDE appear to have very similar  X  X ystematic errors X . However, the opening angle of GUIDE appears to be narrower than that of RDT, but the sickness of each  X  X luster X  is wider than that of RDT, implying possibly less bias but higher variance.
We have applied RDT and the other three algorithms to train a response time model for a storage server. The dependent vari-able latency is related to the number of threads, rwratio, rsratio, request size, and throughput. The feature set is relatively static, and is well-known in the storage community. Conceptually, it is believed that for non-saturated and saturated cases (related to the number of threads and other features), the true regression mod-els are quite different. However, the practical difficulty is that there is no fixed Threadno for jumping from non-saturated to sat-urated. It depends on workload characteristics including rsratio, rwratio, and request size, but not throughput. For example, when rsratio=0, rwratio=1 and request size=4, the saturation point is at Threadno=19. However, for a second case when rsratio=0, rwratio=0 now, and request size=4, the saturation point is at Threadno=11. The challenge to use any formula type of regression methods, ei-ther parametric or nonparametric, is the overwhelming difficulty to come up with the right form of a singe function or multiple ker-nels that can cover both saturated and unsaturated cases. However, there are not two types, but many types of saturated and unsaturated cases. For these reasons and many others, regression tree type of approaches are quite attractive.
 The dataset is collected from a storage controller with 16 drives. There are 5832 examples in total. Threadno is an integer value from 1 to 64, both rwratio and rsratio are continuous from 0 to 1, request size is from 4 to 32, and the dependent variable latency is from 0.000263 to 0.212902. We randomly split the data into two halves as training and testing sets, compared all four algorithms, and the results are summarized in Figure 10.

The improvement by RDT over both CART and RF is clearly shown by its nearly perfect match to the  X  y = x  X  line. As a result, its error is only about 20% of CART. The obvious help from RF is to replace the limited number of leaf nodes (thus limited number of unique predictions) by a larger number of nodes from 30 trees. However, there still appears to be significant amount of correlations among these trees. Visually, we have observed that RF  X  X hortens X  each of the horizontal predictions of CART by adding many more bars in between them. However, the envelope of these bars still resemble that of CART. GUIDE X  X  prediction is very similar to RDT, and is also a close match to the  X  y = x  X  line. The MSE of each approach is summarized in the table of Figure 10, with RDT being the least among all four methods.

When deploying a regression model tree model, some applica-tions prefer math formulas rather than if-then-else rules. Indeed, each regression tree can be converted to an equation with the help of sigmoid function,  X  ( x )= 1 1+ exp (  X  t  X  x ) .Thevalueof t controls the sharpness of the switch. With t = 100 , the switch is nearly binary. A simple rule  X  X f a&gt; 0 . 3 then predict 4 else 5 X  can be converted into 4  X   X  ( x  X  0 . 3) + 5  X   X  (0 . 3  X  x ) . Nested if-then-else rules can be converted into nested sigmoid functions, and multiple trees can just be added on.
We used the synthetic dataset described in Section 4.3 to study MSE 1.80472e-2 7.98088e-2 9.24913e-2 2.03346e-2 the efficiency of training. GUIDE is not included in the study since we only have interactive software executable. We record the run-ning time to produce the model excluding the time to read the train-ing data into memory and the time to write the tree onto the file sys-tem. For RF, it includes the time to generate each bootstrap sam-ples. We ensure that twice the size of the training data can always be held in main memory (twice is required to include the bootstrap for RF), otherwise the splitting criterion computation by CART and RF will swap the data in and out of main memory and incur sig-nificant amount of extra running time. The number of independent variable is chosen as 5, 15, 30, and the number of examples is 1000, 2000, 4000, and 8000. Both RF and RDT construct 30 trees. The tests were run 10 times, and the average results are plotted in Figure 11. The size of data set is calculated by the product of the number of independent variables by the number of examples. RDT is obvi-ously more efficient than RF, which is clearly due to the extra cost of testing the sp litting criterion an d generating bootstraps employed by RF. RDT is slightly more costly than CART when the data size is small, i.e,  X  50000 , however it becomes more efficient when the data size is large. This is due to the extra cost of CART to compute the splitting criterion for every available feature. In summary, RDT is a rather efficient approach particularly for large dataset.
We suggest the number of decision trees to be 30 for regres-sion problems. Empirically, for the datasets we have experimented, 30 trees return satisfactory results both in accuracy and efficiency. Analytically, the variance reduction is the most significant with the first few number of trees, formally  X   X  k where k is the number of trees. The minimal number of e xamples for sp litting is s uggested to be 2 or 3 or the bigger of 2 and 0.1% of the number of training examples. The number 2 or 3 is a common default number by de-cisions trees, such as CART and C4.5, for both classification and regression problems. We provide this additional limit of 0.1% of the number of training examples since the average of 30 uncorre-lated random decision trees of about 1000+ nodes each will already provide enough  X  X ine-tuning X  values. Figure 11: Training efficiency of RDT and RF versus CART
We have considered to make random decision tree more com-plicated to test if better performance could be maintained. There are a number of possi bilities. One simple w ay is to choose a non-random decision threshold, the second is to fit a linear model at the leaf node similar to GUIDE, and the last one is to use bootstraps instead of original training set. Since each method is orthogonal, we decide to investigate each possibility separately. For simplicity, we test all variations on the abalone dataset described in Table 5.
When the minimal number of examples is rather small, like 2, one would think that a decision threshold that minimizes variance after the split will not be helpful. The intuition is that when the min-imum is small, each tree starts to fit closely on the data set. For this reason, we varied the minimal number of examples to split from 2 to 50. For each choice, we constructed two sets of RDTs, one with random threshold and the other with a threshold to reduce variance. Their respective MSEs are plotted in Figure 12. First, regardless of RDT or RDT with none random threshold, it appears that the MSE actually does not increase significantly when the minimal num-ber of examples per leaf increases. The results of pure RDT and RDT with variance reduction threshold are  X  X wisted X  together, im-plying that none-random decision threshold does not seem to help much. Since the number of examples in the leaf node is rather small, fitting a linear model with many parameters will not make much sense. Instead, we choose to use one feature that has not be randomly selected if any or the parent node X  X  testing feature. From Figure 12, when the minimal number is small, there isn X  X  much dif-ference from standard RDT. However, when the number is more than 30, their difference starts to be evident. Sometimes, RDT with linear fit improves the error, but other times, it actually increases the error by some margin. Our conjecture is that it might be due to the choice of one independent feature in the linear model that may produce rather large predictions when | b | in a + b  X  x is big. In the third variation, we experimented to train each RDT from a sepa-rate bootstrap sample. From the plot in Figure 12, using bootstraps appears detrimental to random decision trees. For 47 out 50 cases, the error rate actually increases. And the increase is particularly significant, by about 0.1 to 0.2, when the minimal number is more than 25. Bootstrap is designed to help greedy-based learners, such as traditional decision trees, to overcome variance. However, ran-dom decision tree is not a greedy-based algorithm. It uses decision tree merely as a representation but does not employ any heuristic to prefer some feature over others.
Key areas of previous regression research has been summarized in Section 1.1. There are a number of significant works on regres-sion ensembles. For example, multiple SVM-based regression has been applied successfully to recognize face [Yan et al., 2001]. A genetic algorithm-based approach to combine multiple neural net-work regression has been examined in [Zhou et al., 2001]. Previ-ously, a rule-based approach to combine multiple regression model has been explored in [Indurkhya and Weiss, 2001]. The term  X  X andom regression X  has been used to train multiple random para-metric models. The idea of random decision tree was originally proposed in [Fan et al., 2003]. Some notable previous works of randomization include data randomization as in Breiman X  X  bagging and random forest [Breiman, 2001], feature subset randomization in [Amit and Geman, 1997], and randomly select top-k attributes proposed by Dietrich. A comprehensive study of random decision trees for various types of classification problems and a rather com-plete survey of different ways to randomize can be found in a recent work [Liu, 2005]. Several reasons to explain random decision tree X  X  performance for classification problems can be found in [Fan et al., 2005]. A recent comparative study of many PET trees include RDT can be found in [Zhang et al., 2005].
As a solution to the rather difficult process to choose the right re-gression method from an extensive list of parametric and nonpara-metric techniques, many of which may either need to pre-specify the component functions or have many parameters and options, we extend the use of random decisions tree (RDT) as a general frame-work for regression problems. As compared to related regression tree algorithms, there are two parameters in RDT versus 2 param-eters in CART, 3 in RF, and multiple parameters (15  X  number of unique non-empty feature subsets) in GUIDE. However, RDT with default parameter value is significantly more accurate than CART, RF and GUIDE with their comparable and default parameter val-ues, in 15, 14 and 12 out of 16 benchmark datasets respectively. Running RDT is as simple and efficient as CART, but significantly more efficient than RF. We also applied RDT to two datasets where carefully chosen parametric models work very well, but RDT also has obtained even more accurate results. We explained the reason why RDT works by illustrating its proper bias to approximate the true mechanism of the problem, and its significant reduction in vari-ance. In particular, RDT can closely discover the hidden variable of the true regression function that GUIDE appears to misinterpret. In addition, RDT X  X  predictions are very continuous and refined.
Regression is an integral part of data mining. Simplicity to use, high accuracy, efficiency and generality are among some of the most important requirements for data mining software. We think that random decision tree provides such a solution for regression problems. Random decision tree has been shown previously to re-turn highly accurate model for binary and multi-class classification and posterior probability problems, the work in this paper extends its application to yet another important area of data mining.
Amit, Y. and Geman, D. (1997). Shape quantization and recognition with randomized trees. Neural Computation , 9(7):1545 X 1588.

Breiman, L. (2001). Random forests. Machine Learning , 45(1):5 X 32.
 Breiman, L., Friedman, J., Olshen, R., and Stone, C. (1984). Classification and Regression Trees .Wadsworth.

Fan, J. and Huang, L.-S. (2001). Goodness-of-fit tests for parametric regression models. Journal of the American Statistical Association , 96(454):640 X 664.
 Fan, W., Greengrass, E., McCloskey, J., Yu, P. S., and
Drummey, K. (2005). Effective estimation of posterior probabilitie s: Explaining the accura cy of randomized decision tree approaches. In Proceedings of the 5th IEEE International Conference on Data Mining (ICDM 2005) , pages 154 X 161.

Fan, W., Wang, H., Yu, P. S., and Ma, S. (2003). Is random model better? on its accuracy and efficiency. In Proceedings of
Third IEEE International Conference on Data Mining (ICDM X 2003) .
 Hardle, W. (1990). Applied Nonparametric Regression .
 Cambridge University Press.

Hastie, T. and Tibshirani, R. (1986). Generalized additive models. Statistical Science , 1:297 X 318.

Indurkhya, N. and Weiss, S. M. (2001). Solving regression problems with rule-based ensemble classifiers. In ACM International Conference Knowledge Discovery and Data Mining (KDD01) , pages 287 X 292.
 Kedem, B. and Fokianos, K. (2002). Regression Models for Time Series Analysis .

Liu, T. F. (July 2005). The u tility of ra ndomness in decision tree ensemble. Master X  X  thesis, Faculty of Information Technology, Monash University.

Loh, W.-Y. (2002). Regression trees with unbiased variable selection and interaction detection. Statistica Sinica , 12:361 X 386.

McCullagh, P. and Nelder, J. A. (1989). Generlized linear models, 2nd edition . Chapman and Hall, London.

Nelder, J. A. and Wedderburn, R. W. M. (1972). Generlized linear models. Journal of Royal Statistical Survey. Series A , 135:370 X 384.

Segal, M. R. (2004). Machine learning benchmarks and random forest regression. available from eScholarship repository, http://repositories.cdlib.org/cbmb/bench rf regn/.

Yan, J., Li, S., Zhu, S., and Zhang, H. (2001). Ensemble svm regression based multi-view face detection system. Technical Report MSR-TR-2001-09, Microsoft Research.

Zhang, K., Xu, Z., Peng, J., and Buckles, B. P. (2005). Learning through changes: An empirical study of dynamic behaviors of probability estimation trees. In Proceedings of the 5th IEEE
International Conference on Data Mining (ICDM 2005) , pages 817 X 820.
 Zhou, Z. H., Wu, J. X., Tang, W., and Chen., Z. Q. (2001).
Combining regresson estimators: Ga-based selective neural network ensemble. International Journal of Computational
Intelligence and Applica tions, 2001, 1( 4):341-356 , 1(4):341 X 356.
