 Probabilistic topic models have become popular tools for the unsupervised analysis of large document collections [ 1 ]. These models posit a set of latent topics , multinomial distributions over words, and assume that each document can be described as a mixture of these topics. With algorithms for fast approxiate posterior inference, we can use topic models to discover both the topics and an assignment of topics to documents from a collection of documents. (See Figure 1.) These modeling assumptions are useful in the sense that, empirically, they lead to good models of documents. They also anecdotally lead to semantically meaningful decompositions of them: topics tend to place high probability on words that represent concepts, and documents are represented as expressions of those concepts. Perusing the inferred topics is effective for model verification and for ensuring that the model is capturing the practitioner X  X  intuitions about the documents. Moreover, producing a human-interpretable decomposition of the texts can be a goal in itself, as when browsing or summarizing a large collection of documents.
 In this spirit, much of the literature comparing different topic models presents examples of topics and examples of document-topic assignments to help understand a model X  X  mechanics. Topics also can help users discover new content via corpus exploration [ 2 ]. The presentation of these topics serves, either explicitly or implicitly, as a qualitative evaluation of the latent space, but there is no explicit quantitative evaluation of them. Instead, researchers employ a variety of metrics of model fit, such as perplexity or held-out likelihood. Such measures are useful for evaluating the predictive model, but do not address the more explatory goals of topic modeling. In this paper, we present a method for measuring the interpretatability of a topic model. We devise two human evaluation tasks to explicitly evaluate both the quality of the topics inferred by the model and how well the model assigns topics to documents. The first, word intrusion , measures how semantically  X  X ohesive X  the topics inferred by a model are and tests whether topics correspond to natural groupings for humans. The second, topic intrusion , measures how well a topic model X  X  decomposition of a document as a mixture of topics agrees with human associations of topics with a document. We report the results of a large-scale human study of these tasks, varying both modeling assumptions and number of topics. We show that these tasks capture aspects of topic models not measured by existing metrics and X  X urprisingly X  X odels which achieve better predictive perplexity often have less interpretable latent spaces. Topic models posit that each document is expressed as a mixture of topics. These topic proportions are drawn once per document, and the topics are shared across the corpus. In this paper we will consider topic models that make different assumptions about the topic proportions. Probabilistic Latent Semantic Indexing (pLSI) [ 3 ] makes no assumptions about the document topic distribution, treating it as a distinct parameter for each document. Latent Dirichlet allocation (LDA) [ 4 ] and the correlated topic model (CTM) [ 5 ] treat each document X  X  topic assignment as a multinomial random variable drawn from a symmetric Dirichlet and logistic normal prior, respectively.
 While the models make different assumptions, inference algorithms for all of these topic models build the same type of latent space: a collection of topics for the corpus and a collection of topic proportions for each of its documents. While this common latent space has explored for over two decades, its interpretability remains unmeasured.
 Pay no attention to the latent space behind the model Although we focus on probabilistic topic models, the field began in earnest with latent semantic analysis (LSA) [ 6 ]. LSA, the basis of pLSI X  X  probabilistic formulation, uses linear algebra to decom-pose a corpus into its constituent themes. Because LSA originated in the psychology community, early evaluations focused on replicating human performance or judgments using LSA: matching performance on standardized tests, comparing sense distinctions, and matching intuitions about synonymy (these results are reviewed in [ 7 ]). In information retrieval, where LSA is known as latent semantic indexing (LSI) [ 8 ], it is able to match queries to documents, match experts to areas of expertise, and even generalize across languages given a parallel corpus [9]. The reticence to look under the hood of these models has persisted even as models have moved from psychology into computer science with the development of pLSI and LDA. Models either use measures based on held-out likelihood [ 4 , 5 ] or an external task that is independent of the topic space such as sentiment detection [ 10 ] or information retrieval [ 11 ]. This is true even for models engineered to have semantically coherent topics [12].
 For models that use held-out likelihood, Wallach et al. [ 13 ] provide a summary of evaluation techniques. These metrics borrow tools from the language modeling community to measure how well the information learned from a corpus applies to unseen documents. These metrics generalize easily and allow for likelihood-based comparisons of different models or selection of model parameters such as the number of topics. However, this adaptability comes at a cost: these methods only measure the probability of observations; the internal representation of the models is ignored. Griffiths et al. [ 14 ] is an important exception to the trend of using external tasks or held-out likelihood. They showed that the number of topics a word appears in correlates with how many distinct senses it has and reproduced many of the metrics used in the psychological community based on human performance. However, this is still not a deep analysis of the structure of the latent space, as it does not examine the structure of the topics themselves.
 We emphasize that not measuring the internal representation of topic models is at odds with their presentation and development. Most topic modeling papers display qualitative assessments of the inferred topics or simply assert that topics are semantically meaningful, and practitioners use topics for model checking during the development process. Hall et al. [ 15 ], for example, used latent topics deemed historically relevant to explore themes in the scientific literature. Even in production environments, topics are presented as themes: Rexa (http://rexa.info), a scholarly publication search engine, displays the topics associated with documents. This implicit notion that topics are have semantic meaning for users has even motivated work that attempts to automatically label topics [ 16 ]. Our goal is to measure the success of interpreting topic models across number of topics and modeling assumptions. Although there appears to be a longstanding assumption that the latent space discovered by topic models is meaningful and useful, evaluating such assumptions is difficult because discovering topics is an unsupervised process. There is no gold-standard list of topics to compare against for every corpus. Thus, evaluating the latent space of topic models requires us to gather exogenous data. In this section we propose two tasks that create a formal setting where humans can evaluate the two components of the latent space of a topic model. The first component is the makeup of the topics. We develop a task to evaluate whether a topic has human-identifiable semantic coherence. This task is called word intrusion , as subjects must identify a spurious word inserted into a topic. The second task tests whether the association between a document and a topic makes sense. We call this task topic intrusion , as the subject must identify a topic that was not associated with the document by the model. 3.1 Word intrusion To measure the coherence of these topics, we develop the word intrusion task; this task involves evaluating the latent space presented in Figure 1(a). In the word intrusion task, the subject is presented with six randomly ordered words. The task of the user is to find the word which is out of place or does not belong with the others, i.e., the intruder . Figure 2 shows how this task is presented to users. When the set of words minus the intruder makes sense together, then the subject should easily identify the intruder. For example, most people readily identify apple as the intruding word in the set { dog, cat, horse, apple, pig, cow } because the remaining words, { dog, cat, horse, pig, cow } make sense together  X  they are all animals. For the set { car, teacher, platypus, agile, blue, Zaire } , which lacks such coherence, identifying the intruder is difficult. People will typically choose an intruder at random, implying a topic with poor coherence. In order to construct a set to present to the subject, we first select at random a topic from the model. We then select the five most probable words from that topic. In addition to these words, an intruder word is selected at random from a pool of words with low probability in the current topic (to reduce the possibility that the intruder comes from the same semantic group) but high probability in some other topic (to ensure that the intruder is not rejected outright due solely to rarity). All six words are then shuffled and presented to the subject. 3.2 Topic intrusion The topic intrusion task tests whether a topic model X  X  decomposition of documents into a mixture of topics agrees with human judgments of the document X  X  content. This allows for evaluation of the latent space depicted by Figure 1(b). In this task, subjects are shown the title and a snippet from a document. Along with the document they are presented with four topics (each topic is represented by the eight highest-probability words within that topic). Three of those topics are the highest probability topics assigned to that document. The remaining intruder topic is chosen randomly from the other low-probability topics in the model.
 The subject is instructed to choose the topic which does not belong with the document. As before, if the topic assignment to documents were relevant and intuitive, we would expect that subjects would select the topic we randomly added as the topic that did not belong. The formulation of this task provides a natural way to analyze the quality of document-topic assignments found by the topic models. Each of the three models we fit explicitly assigns topic weights to each document; this task determines whether humans make the same association.
 Due to time constraints, subjects do not see the entire document; they only see the title and first few sentences. While this is less information than is available to the algorithm, humans are good at extrapolating from limited data, and our corpora (encyclopedia and newspaper) are structured to provide an overview of the article in the first few sentences. The setup of this task is also meaningful in situations where one might be tempted to use topics for corpus exploration. If a users use a topic browser to find relevant documents, for example, they will likely be provided with similar views of the documents (e.g. title and abstract, as in Rexa).
 For both the word intrusion and topic intrusion tasks, subjects were instructed to focus on the meanings of words, not their syntactic usage or orthography. We also presented subjects with the option of viewing the  X  X orrect X  answer after they submitted their own response, to make the tasks more engaging. Here the  X  X orrect X  answer was determined by the model which generated the data, presented as if it were the response of another user. At the same time, subjects were encouraged to base their responses on their own opinions, not to try to match other subjects X  (the models X ) selections. In small experiments, we have found that this extra information did not bias subjects X  responses. To prepare data for human subjects to review, we fit three different topic models on two corpora. In this section, we describe how we prepared the corpora, fit the models, and created the tasks described in Section 3. We then present the results of these human trials and compare them to metrics traditionally used to evaluate topic models. 4.1 Models and corpora In this work we study three topic models: probabilistic latent semantic indexing (pLSI) [ 3 ], latent Dirichlet allocation (LDA) [ 4 ], and the correlated topic model (CTM) [ 5 ], which are all mixed membership models [ 17 ]. The number of latent topics, K , is a free parameter in each of the models; here we explore this with K = 50 , 100 and 150 . The remaining parameters  X   X  k , the topic multinomial distribution for topic k ; and  X  d , the topic mixture proportions for document d  X  are inferred from data. The three models differ in how these latent parameters are inferred. pLSI In pLSI, the topic mixture proportions  X  d are a parameter for each document. Thus, pLSI is not a fully generative model, and the number of parameters grows linearly with the number of documents. We fit pLSI using the EM algorithm [ 18 ] but regularize pLSI X  X  estimates of  X  d using pseudo-count smoothing,  X  = 1 .
 LDA LDA is a fully generative model of documents where the mixture proportions  X  d are treated as a random variable drawn from a Dirichlet prior distribution. Because the direct computation of the posterior is intractable, we employ variational inference [ 4 ] and set the symmetric Dirichlet prior parameter,  X  , to 1.
 CTM In LDA, the components of  X  d are nearly independent (i.e.,  X  d is statistically neutral). CTM allows for a richer covariance structure between topic proportions by using a logistic normal prior over the topic mixture proportions  X  d . For each topic, k , a real  X  is drawn from a normal distribution and exponentiated. This set of K non-negative numbers are then normalized to yield  X  d . Here, we train the CTM using variational inference [5].
 We train each model on two corpora. For each corpus, we apply a part of speech tagger [ 19 ] and remove all tokens tagged as proper nouns (this was for the benefit of the human subjects; success in early experiments required too much encyclopedic knowledge). Stop words [ 20 ] and terms occurring in fewer than five documents are also removed. The two corpora we use are 1.) a collection of 8447 articles from the New York Times from the years 1987 to 2007 with a vocabulary size of 8269 unique types and around one million tokens and 2.) a sample of 10000 articles from Wikipedia (http://www.wikipedia.org) with a vocabulary size of 15273 unique types and three million tokens. 4.2 Evaluation using conventional objective measures There are several metrics commonly used to evaluate topic models in the literature [ 13 ]. Many of these metrics are predictive metrics; that is, they capture the model X  X  ability to predict a test set of unseen documents after having learned its parameters from a training set . In this work, we set aside 20% of the documents in each corpus as a test set and train on the remaining 80% of documents. We then compute predictive rank and predictive log likelihood.
 To ensure consistency of evaluation across different models, we follow Teh et al. X  X  [ 21 ] approximation of the posterior topic proportions for document d . For pLSI  X   X  d is the MAP estimate; for LDA and CTM  X   X  d is the mean of the variational posterior. With this information, we can ask what words the model believes will be in the document and compare it with the document X  X  actual composition. Given document w d , we first estimate  X   X  d and then for every word in the vocabulary, we compute appeared in document w d (we follow the convention that lower rank is better).
 The average word likelihood and average rank across all documents in our test set are shown in Table 1. These results are consistent with the values reported in the literature [ 4 , 5 ]; in most cases CTM performs best, followed by LDA. 4.3 Analyzing human evaluations The tasks described in Section 3 were offered on Amazon Mechanical Turk (http://www.mturk.com), which allows workers (our pool of prospective subjects) to perform small jobs for a fee through a Web interface. No specialized training or knowledge is typically expected of the workers. Amazon Mechanical Turk has been successfully used in the past to develop gold-standard data for natural language processing [ 22 ] and to label images [ 23 ]. For both the word intrusion and topic intrusion tasks, we presented each worker with jobs containing ten of the tasks described in Section 3. Each job was performed by 8 separate workers, and workers were paid between $0.07  X  $0.15 per job. Word intrusion As described in Section 3.1, the word intrusion task measures how well the inferred topics match human concepts (using model precision , i.e., how well the intruders detected by the subjects correspond to those injected into ones found by the topic model).
 Let  X  m k be the index of the intruding word among the words generated from the k th topic inferred by model m . Further let i m k,s be the intruder selected by subject s on the set of words generated from the k th topic inferred by model m and let S denote the number of subjects. We define model precision by the fraction of subjects agreeing with the model, Figure 3 shows boxplots of the precision for the three models on the two corpora. In most cases LDA performs best. Although CTM gives better predictive results on held-out likelihood, it does not perform as well on human evaluations. This may be because CTM finds correlations between topics and correlations within topics are confounding factors; the intruder for one topic might be selected from another highly correlated topic. The performance of pLSI degrades with larger numbers of topics, suggesting that overfitting [4] might affect interpretability as well as predictive power. Figure 4 (left) shows examples of topics with high and low model precisions from the NY Times data fit with LDA using 50 topics. In the example with high precision, the topic words all coherently express a painting theme. For the low precision example,  X  X axis X  did not fit in with the other political words in the topic, as 87 . 5% of subjects chose  X  X axis X  as the intruder.
 The relationship between model precision, MP m k , and the model X  X  estimate of the likelihood of the intruding word in Figure 5 (top row) is surprising. The highest probability did not have the best interpretability; in fact, the trend was the opposite. This suggests that as topics become more fine-grained in models with larger number of topics, they are less useful for humans. The downward sloping trend lines in Figure 5 implying that the models are often trading improved likelihood for lower interpretability.
 The model precision showed a negative correlation (Spearman X  X   X  =  X  0 . 235 averaged across all models, corpora, and topics) with the number of senses in WordNet of the words displayed to the subjects [ 24 ] and a slight positive correlation (  X  = 0 . 109 ) with the average pairwise Jiang-Conrath similarity of words 1 [25].
 Topic intrusion In Section 3.2, we introduced the topic intrusion task to measure how well a topic model assigns topics to documents. We define the topic log odds as a quantitative measure of the agreement between the model and human judgments on this task. Let  X   X  m d denote model m  X  X  point estimate of the topic proportions vector associated with document d (as described in Section 4.2). Further, let j m d,s  X  X  1 ...K } be the intruding topic selected by subject s for document d on model m and let j m d,  X  denote the  X  X rue X  intruder, i.e., the one generated by the model. We define the topic log odds as the log ratio of the probability mass assigned to the true intruder to the probability mass assigned to the intruder selected by the subject, The higher the value of TLO m d , the greater the correspondence between the judgments of the model and the subjects. The upper bound on TLO m d is 0. This is achieved when the subjects choose intruders with a mixture proportion no higher than the true intruder X  X .
 Figure 6 shows boxplots of the topic log odds for the three models. As with model precision, LDA and pLSI generally outperform CTM. Again, this trend runs counter to CTM X  X  superior performance on predictive likelihood. A histogram of the TLO of individual Wikipedia documents is given in Figure 4 (right) for the fifty-topic LDA model. Documents about very specific, unambiguous concepts, such as  X  X indy Hop, X  have high TLO because it is easy for both humans and the model to assign the document to a particular topic. When documents express multiple disparate topics, human judgments diverge from those of the model. At the low end of the scale is the article  X  X ook X  which touches on diverse areas such as history, science, and commerce. It is difficult for LDA to pin down specific themes in this article which match human perceptions.
 Figure 5 (bottom row) shows that, as with model precision, increasing predictive likelihood does not imply improved topic log odds scores. While the topic log odds are nearly constant across all numbers of topics for LDA and pLSI, for CTM topic log odds and predictive likelihood are negatively correlated, yielding the surprising conclusion that higher predictive likelihoods do not lead to improved model interpretability. We presented the first validation of the assumed coherence and relevance of topic models using human experiments. For three topic models, we demonstrated that traditional metrics do not capture whether topics are coherent or not. Traditional metrics are, indeed, negatively correlated with the measures of topic quality developed in this paper. Our measures enable new forms of model selection and suggest that practitioners developing topic models should thus focus on evaluations that depend on real-world task performance rather than optimizing likelihood-based measures.
 In a more qualitative vein, this work validates the use of topics for corpus exploration and information retrieval. Humans appreciate the semantic coherence of topics and can associate the same documents with a topic that a topic model does. An intriguing possibility is the development of models that explicitly seek to optimize the measures we develop here either by incorporating human judgments into the model-learning framework or creating a computational proxy that simulates human judgments. Acknowledgements David M. Blei is supported by ONR 175-6343, NSF CAREER 0745520 and grants from Google and Microsoft. We would also like to thank Dan Osherson for his helpful comments. References
