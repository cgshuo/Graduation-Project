
Detecting outlying subspaces is a relatively new research problem in outlier-ness analysis for high-dimensional dat a. An outlying subspace for a given data point p is the sub-space in which p is an outlier. Outlying subspace detection can facilitate a better characterization process for the de -tected outliers. It can also enable outlier mining for high-dimensional data to be performed more accurately and effi-ciently. In this paper, we proposed a new method using ge-netic algorithm paradigm for searching outlying subspaces efficiently. We developed a technique for efficiently comput -ing the lower and upper bounds of the distance between a given point and its k th nearest neighbor in each possible subspace. These bounds are used to speed up the fitness evaluation of the designed genetic algorithm for outlying subspace detection. We also proposed a random sampling technique to further reduce the computation of the genetic algorithm. The optimal number of sampling data is speci-fied to ensure the accuracy of the result. We show that the proposed method is efficient and effective in handling out-lying subspace detection problem by a set of experiments conducted on both synthetic and real-life datasets.
Outlier detection is an important research problem in data mining that aims to find a specific number of ob-jects that are considerably dissimilar, exceptional and in -consistent with respect to the majority records in the in-put databases. Numerous research work in outlier detection has been proposed such as the distribution-based methods [3][6], the distance-based methods [9][10][12], the densi ty-based methods [2][8][13] and the clustering-based methods [1][5][7][11][15][19], etc.

In this paper, we focus on the problem of outlying sub-space detection for high dimensional data, which is a com-plementary problem of outlier detection. The major task of outlying subspace detection is to find the subspaces (subset s of features) in which each data point exhibits significant de -viation from the rest of population. An outlying subspace for a data point is a subspace (a subset of features) in which this data can be considered as an outlier. The problem of outlying subspace can be formulated as follows: given a data point, find the subspaces in which this point is consid-erably dissimilar, exceptional or inconsistent with respe ct to the remaining population in the database [14].

Outlier mining can be benefited from outlying subspace detection in many aspects. First, outlying subspace detec-tion can contribute to a better characterization of the outl iers detected. The characterization of outliers mainly involve s presenting the subspaces in which these outliers exist. In high-dimensional space, it is important to not only mine outliers but also find the context in which these outliers ex-ist. In conventional outlier detection methods, each detec ted outlier can only be characterized by the subspace chosen by users that is used for outlier detection. Outlying subspace detection makes it possible to give a more informative char-acterization of the outliers detected by finding their outly ing subspaces. Second, outlying subspace detection can enable outlier mining to be performed more accurately. It can allow outliers to be detected from more than one subspaces. This is important to many practical applications. For instance, in credit card fraud detection, online credit card transactio ns are needed to be scanned in order to determine whether each transaction is legal or fraudulent. The conventional out-lier detection methods usually rely on a pre-specified fea-ture subspace to detect outliers, which may cause them to miss many potential outliers hidden in other feature sub-spaces and fail to raise necessary alarms. Therefore, outly -ing subspace analysis plays a crucial role for outlier min-ing in this sitation to identify the correct feature subspac es in which outliers can be accurately mined. Finally, outly-ing subspace detection can help outlier detection methods to mine outliers in high-dimensional space more efficiently . It is an important intermediate step in example-based out-lier mining, which detects outliers based on a set of outlier examples supplied by domain experts [16][17]. The basic idea of this method is to find the outlying subspaces of these outlier examples, from which more outliers that have sim-ilar outlier-ness characteristics to the given examples ca n be found more efficiently by only investigating the detected outlying subspaces.

Unfortunately, due to the exponential growth of the number of subspaces with respect to the dimension of the dataset, the problem of outlying subspace detection is NP-hard by nature. The straightforward exhaustive search is apparently infeasible to this problem, especially for high -dimensional datasets. In response to the inherent hardness of this problem, the state-of-the-art methods were propose d to employ heuristics in speeding up the search process in order to render this problem tractable. Zhang et al. pro-posed a dynamic outlying subspace search algorithm that utilizes a sample-based learning process to efficiently ide n-tify the outlying subspaces for the given points [14][18]. Two heuristic pruning strategies employing the upward and downward closure property are devised to reduce search space. Its major drawbacks, however, lie in the unsatisfac-tory accuracy of the metric used for measuring outlying de-gree of points in subspaces, the binary fashion of its result and the difficulty in specifying the distance threshold. Zhu et al. draw on a genetic algorithm to solve the example-based outlier detection problem [16][17]. The major limita -tion of this method is that it is computationally expensive t o compute the outlying degree of points in subspaces. This is because that it uses a cell-based partitioning technique th at scales poorly in high-dimensional space.

In this paper, we develop a method based on genetic al-gorithm to solve the outlying subspace detection problem that well copes with the drawbacks of the existing methods. The main contributions of this paper are summarized as fol-lows: 1. A new metric, called Subspace Outlying Factor (SOF), 2. A genetic algorithm-based method is proposed for out-3. The concepts of the lower and upper bounds of D k , 4. The random sampling technique is utilized in our 5. Last but not the least, we show that the proposed
To define outlying subspace, we need to first devise the metric for measuring outlier-ness of the given data point in different subspaces. In this work, we use D k , the distance between a point and its k th nearest neighbor, in our outlier-ness metric, called Subspace Outlying Factor (SOF) . Math-ematically, the SOF of a subspace s w.r.t a given point p is defined as the ratio of D k ( p ) in s against the averaged D in s for points in the dataset D , i.e.
 Intuitively, the higher the ratio is, the higher the D k of p is when compared to other points, therefore the higher outlier -ness of p is and vice versa. Our definition of SOF leads to the following definition of SOF Outlying Subspaces : Definition 1. SOF Outlying Subspaces: Given an in-put dataset D , parameters n and k , a subspace s is a SOF Outlying Subspace for a given data point p if there are no more than n  X  1 other subspaces s  X  such that SOF ( s  X  , p ) &gt; SOF ( s, p ) .

The above definition is equivalent to say that the top n subspaces having the largest SOF values are considered to be outlying subspaces.
As the lower and upper bounds of D k of data points are established by means of their respective k NN Lookup Ta-ble, we therefore first introduce k NN Lookup Table prior to our discussion on the bounds of D k .
 Definition 2: Full k NN Lookup Table: A Full k NN Lookup Table of a data p , denoted as T p , is a  X   X  k table
Figure 1. Partial k NN Lookup Table of a data point containing information about its k nearest neighbors in each single dimension of full data space with  X  dimensions. The entry x ij of the table represents the j th nearest neighbor of p in the i th dimension, where 1  X  i  X   X  and 1  X  j  X  k . It is important to note that the lower and upper bounds of D k ( p ) will vary in different subspaces and only a portion of T p is actually used to construct the bounds of D k ( p ) in a particular subspace s . That is, only the dimensions relevant to s are needed to be considered. As such, we propose the notion of Partial k NN Lookup Table that is defined as a view of the corresponding full k NN Lookup Table.
 Definition 3: Partial k NN Lookup Table: A Partial k NN Lookup Table of a data p with respect to a subspace s , de-noted as T p s , is a | s | X  k view (logical table) of the full k NN Lookup Table of p , with each entry x ij being the j nearest neighbor of p in the i th dimension, where d i  X  s , 1  X  i  X | s | and 1  X  j  X  k . | s | is the number of dimen-sions of s . Apparently, the Partial k NN Lookup Table is a selection of its corresponding full k NN Lookup Table, i.e. 3.1 Lower Bound of D k
Given a subspace s and the number of nearest neighbor k , we will first calculate the following two constructs,  X  and  X  . These two constructs will be utilized in constructing the lower bound of D k of p in s .  X  and  X  are defined as follows: where j k  X  1 | s | k denotes the maximum integer that does not
When  X  and  X  are available, we sort T p s based on the values of D  X  d neighbor in d i ( d i  X  s ) and select the  X  dimensions that have the lowest D  X  d d : Then we define the Lower Bound Set ( LBS ) of p in s as where each element e i ( 1  X  i  X | s | ) is a positive integer and specified as follows: The lower bound of D k ( p ) for a point p in s , denoted as LB s ( D k ( p )) , is given as nearest neighbor in dimension d i  X  s .

Figure 1 presents the Partial k NN Lookup Table of a point. All the elements in LBS s ( p ) has been highlighted using darker background in the table. As we discussed ear-lier, there are | s | elements in LBS s ( p ) ; the first | s | X   X  ele-ments come from Column  X  and another  X  elements come from Column  X  + 1 . These | s | elements in LBS s ( D k ( p )) form a lower bound frontier in the Partial k NN Lookup Ta-ble.
 Lemma 1 : bound of D k ( p ) for a point p in subspace s .
 Proof: Let N N Index ( p,d number of point q with respect to p in dimension d i of sub-space s . For instance, if q is the 10 th nearest neighbor of p in the 5 th dimension of s , then N N Index ( p,d The total number of unique points q such that i  X | s | (i.e. falling to the left side of the lower bound fron-tier of the table) is no more than k  X  1 . Without losing gen-erality, let us suppose that there are t such unique points, ( 1  X  t  X  k  X  1 ).

For any point q satisfying N N Index ( p,d all the dimensions d i of s , 1  X  i  X | s | (i.e. locating to the right of the lower bound frontier of the table), we have dist d i ( p, q )  X  D e i d dist s ( p, q )  X 
Suppose, among the t unique points locating to the left of the lower bound frontier of the table, there are t  X  points q satisfying N N Index ( p,s ) ( q )  X  t ( 0  X  t  X   X  t ). Then, the ( t  X  + 1) th nearest neighbor of p in s should locate to the right of the lower bound frontier and D t  X  +1 r t + 1  X  k and D t  X  +1 s ( p )  X  D k s ( p ) . Given D t  X  r D s ( p )  X  3.2 Upper Bound of D k
To compute the upper bound of D k ( p ) of p in s , we need to first obtain the Upper Bound Set (UBS) for p in s . It is defined as follows: where KN N Set d i ( p ) denotes the set of k NNs of p in di-mension d i and it represents each row of T p s . Obviously, we have k  X | U BS s ( p ) | X  k | s | . | U BS s ( p ) | = k when the k NNs of p in each dimension d i  X  s are identical, whereas | U BS s ( p ) | = k | s | when there are no duplicates in T p computing the lower bound of D k ( p ) , U BS s ( p ) is used to store the result of union operation on all k NN sets of p in each single dimension.
 Let point q be the k th nearest neighbor of p in U BS s ( p ) . The upper bound of D k of p in s is defined as the distance between p and q in s as Lemma 2 . Let point q be the k th nearest neighbor of p in U BS s ( p ) , then dist s ( p, q ) is the upper bound of D k of p in s .
 Proof: For two sets set 1 and set 2 such that set 1  X  set and | set 2 | X | set 1 | X  k , if q 1 and q 2 are the k th est neighbor of p in set 1 and set 2 , respectively, then we have dist s ( p, q 1 )  X  dist s ( p, q 2 ) . This is because that D k is monotonically decreasing as the set of points we examine gets larger.
 Now let set 1 and set 2 be instantiated as set 1 = U BS s ( p ) and set 2 = D , where D denotes the whole dataset, we thus have U BS s ( p )  X  X  and |D| X  | U BS s ( p ) | X  k . Based on the above discussion, we will have dist s ( p, q 1 )  X  dist s ( p, q 2 ) , where q 1 and q k th nearest neighbor of p in U BS s ( p ) and D , respectively. Therefore, dist s ( p, q 1 ) is the upper bound of D k of p in s , as desired.
Let LB s ( D k , D ) and U B s ( D k , D ) be the average lower and upper bounds of D k in subspace s for points in dataset D . We define the minimum and maximum values for SOF of p in s as follows:
The approximated SOF of s with respect to p is computed by using the average of SOF min ( s, p ) and SOF max ( s, p ) as follows:
As pointed out in [13], k NN search in subspace s for all the N points in the database requires a complexity of O ( k | s | N 2 ) when s is of a high dimension.

If our approximation scheme of SOF is used, comput-ing the lower bound of D k ( p ) in s only requires sort-ing the  X  th column of T p s and summing up D e i d each d i  X  s , with a complexity of O ( | s | log | s | + | s | ) . While for computing the upper bound of D k ( p ) in s , we need to find the k th NN of p in U BS s ( p ) with a maxi-mum possible size of k | s | . Hence, the complexity will be puting the bounds of D k for all the points in the database is O (( | s | log | s | + | s | + k 2 | s | 2 ) N ) = O ( k 2
By using our approximation technique, we are able to reduce the complexity in computing SOF of a subspace to a linear order with respect to N , leading to a computation saving by up to a factor of N k | s | compared to the case when no approximation is performed. Since N &gt;&gt; | s | and k is usually small in most cases, our approximation technique is thus able to achieve a significant performance improvement.
The k NN Look-up Table for a data point should be first constructed before its lower and upper bounds of D k can thereby be established. The key task involved in construct-ing the k NN Look-up Table of a data point is to find its k NNs in each single dimension of the full data space. To facilitate construction of the tables, we transform the ori g-inal dataset into a few sorting lists, where the number of the sorting lists is equal to the number of dimensions of the dataset. Each sorting list is constructed based on the sorti ng order of data in each dimension. The lengths of all sort-ing lists are identical and equal to the number of data points in the original dataset. Each element in the sorting list has two fields: the ID and the value of a particular point in the related dimension. There can be a few ways to implement k NN search in the sorting lists. In this work, we will study three simple yet efficient methods, namely the list-based, block-based and tree-based methods.
 List-based Method . k NN search can be performed directly on the sorting lists. We need to first locate p in the sorting list and then perform k NN search for p . The preceding and subsequent k points of p are the candidates of its k NNs, from which k NNs of p can be found.

Locating a point p in any one sorting list needs a com-plexity of O ( log 2 N ) and finding k NNs in the sorting list requires another O ( k ) computation. In sum, the compu-tational complexity of employing the sorting list will be O ( log 2 N + k ) . The space complexity of list-based method is O (2 N ) .
 Lemma 3: If a sorting list is partitioned into blocks with an equal size b , then at most 3 blocks in the sorting list are needed to be searched for finding k NNs for any point p if b  X  k . These 3 blocks are the block B to which p belong and the the two adjacent blocks of B .
 Proof: Without losing generality, let us suppose that the sorting list is related to dimension d i ( 1  X  i  X   X  ) and the block index number of B to which p belongs is i . For any point q in Block ( i  X  2) , there are at least k points in Block i  X  1 whose distance to p in dimension d i is less than or equal to dist d i ( p, q ) . Likewise, for any point q in Block ( i + 2) , there are at least k points in Block ( i + 1) whose distance to p in dimension d i is less than or equal to dist d i ( p, q ) . Therefore, all k NNs of p definitely fall into Block ( i  X  1) , i or ( i + 1) .
 Block-based Method . k NN search can also be performed on a block-by-block basis. The basic idea is to partition a sorting list into a number of blocks. These blocks are usually of an equal size and contain more than k points. Each time, only a single block is evaluated. Let B min and B max be the minimum and maximum point values in the current block being loaded, respectively, and p.value be the value of p in the sorting list. If B min  X  p.value  X  B max then this block is the one to which p belongs. We can further locate p in this block and perform k NN search for p . If B min  X  p.value  X  B max is not met for the current block, then another block will be loaded for evaluation.

The computational complexity for finding the block to which p belongs is O (1) for the best case where p is located in the first block we evaluate and O ( N b ) for the worst case where all the blocks in the sorting list are exhausted in the search. The average-case complexity is thus O ( N + b 2 b the time complexity for locating p in the block is O ( log Finding k NNs in the sorting list requires another O ( k ) com-putation. The total time complexity is O ( N + b 2 b + log Since at most 3 blocks in the sorting list need to be searched, the space complexity is therefore O (6 b ) .
 Tree-based Method. The third alternative is to utilize a tree structure to further index the data blocks in the sortin g list. For the sake of simplicity, we construct Binary Trees for performance enhancement in k NN search. Binary tree is simple in structure yet very efficient in k NN search.
The binary tree we use for each sorting list is a balanced rooted tree BT = &lt; V, E &gt; , where V is the node set and E is the edge set. The nodes in V are classified as the block nodes and the indexing nodes . The block nodes are the leaves of the binary tree that represent data blocks in a sorting list that each contains b points, where b  X  k . Each data block in the leaf level will be represented by its mini-mum point value (the first value in the block if the sorting list is ordered in an ascending order) in the binary tree. The indexing nodes are other nodes in the tree primarily used for indexing the data blocks at the bottom level. The immedi-ate indexing node of a block node takes the smallest value and the starting address of the data block. Other indexing nodes (excluding the root) will take the minimum value of its children, together with the addresses of its left and rig ht children. The root will only stores the addresses of its two children. k NN search for a point p in a binary tree also takes two major steps. First, the binary tree is traversed top-down from the root until the block to which p belongs is found. The moment an intermediate indexing node a is reached, the following rule is used to decide the sub-tree for further traversal: If p.value  X  a.rightchild , then right child of a is chosen for traversal. Otherwise, the left child is select ed for traversal . When a bottom indexing node (i.e. the im-mediate parent of a block node) is reached, the block node to which it is pointing is referred and the whole data block is fetched. After the block to which p belongs to has been found, the location of p within the block will be determined and k NN search can be performed.

It will require O ( log 2 N b ) to traverse the binary tree downward from the root to find the block to which p be-longs. The complexity of locating p in the block is O ( log and searching for k NNs of p in 3 blocks requires a com-plexity of O ( k ) . In sum, the time complexity is O ( log log 2 b + k ) = O ( log 2 N + k ) .

The total number of indexing nodes in the tree is approx-imately 2 N b . We load all the indexing nodes of the binary tree as they will be frequently used for k NN search. Since it is not required to load any data blocks until the block to which p belong has been found and at most 3 blocks are needed to be loaded for k NN search for p , thus the space complexity of tree-based method is O ( 2 N b + 6 b ) .
In this section, we will elaborate on the design of the genetic algorithm for outlying subspace detection. Representation. Our GA technique uses the standard bi-nary individual encoding; all individuals are represented by strings with fixed and equal length  X  , where  X  is the num-ber of dimensions of the dataset. Using binary alphabet  X  = { 0 , 1 } for gene alleles, each bit in the individual will take on the value of  X 0 X  and  X 1 X , indicating whether or not its corresponding condition is selected, respectively.
Figure 2. Genetic algorithm for outlying sub-space detection Fitness Function. The fitness function used in the genetic algorithm is defined as the approximated SOF of subspace s with respect to the given point p , as presented in Eq. (10), i.e. A higher value of f fit ( s ) indicates a fitter solution and vice versa. The definition of f fit ( s ) encourages the genetic algo-rithm to produce an increasing number of subspaces having high SOF values as evolution proceeds.
 Selection Operator. In our work, fitness-proportionate se-lection , also known as roulette-wheel selection, is used to select fitter solutions in each step of the evolution. Fitnes s-proportionate selection is a stochastic selection method where the selection probability of a subspace is proportion al to the value of its fitness function f fit ( s ) . Search Operators. The crossover and mutation used in this work is single-point crossover and bit-wise mutation . In our work, all the new individuals generated by crossover and mutation are of the same length, i.e.  X  , as their parent(s), where  X  is number of dimensions of the input database. There are two associated probabilities, p c and p m , used to determine the frequencies for applying crossover and muta-tion, respectively. Normally, we have p c &gt;&gt; p m , meaning that crossover is performed in a much higher frequency than mutation.
 Algorithm. The framework of genetic algorithm for detecting outlying subspace is presented in Figure 2. CompleteSet is the set used to maintain all the subspaces, together with their respective SOF, that have been evalu-ated in the genetic algorithm and CandidateSet is the set used to only store the candidates of SOF Outlying Sub-spaces. The stopping criterion in the WHILE loop is usu-ally that the number of generations performed has reached a pre-specified constant. CandidateSet stores the top n c subspaces in CompleteSet (line 10). In order to achiev-ing good accuracy of the detected outlying subspaces, n c should be substantially larger than n . In Line 11, we perform subspace refinement (will be discussed in the se-quel) and the top n subspaces from all the candidates in CandidateSet are returned as SOF Outlying Subspaces. The Subspace Refinement. Since we approximate SOF in the genetic algorithm, the accuracy of computation is thus somehow limited. To address this problem, we can perform a refinement step on the candidate outlying subspaces in Line 11 of the genetic algorithm (Figure 2). Instead of us-ing the lower and upper bounds of D k for a fast fitness ap-proximation, the refinement step will compute the accurate SOF for all subspace candidates in CandidateSet and the top n outlying subspaces among them will be returned. A pruning optimization strategy can be devised based on the maximum value of SOF (i.e. SOF max ) of different sub-spaces to speed up the computation. The basic idea of this pruning optimization strategy is that, after n subspace can-didates have been evaluated, we start to maintain the mini-mum value of SOF for the top n subspaces we have found thus far, denoted as M inSOF n . Those subspaces satisfying SOF max ( s, p ) &lt; M inSOF n cannot become the top n sub-spaces and can therefore be safely pruned. This is because that the value of M inSOF n is monotonically increasing as more subspaces are examined in the refinement step.
The most computationally expensive step in our genetic algorithm lies in the fitness evaluation of individuals. Thi s is because that the fitness evaluation for each subspace, eithe r in approximated or accurate manner, involves scanning all the points in the dataset. This will be slow as the number of points in the dataset is usually large. To speed up fitness evaluation, we draw on random sampling technique so as to evaluate fitness of individuals only based on the random samples, rather than on the entire dataset.

By using sampling data, the average lower and upper bounds of D k in subspace s , used in SOF approximation, can be computed as follows: where N S denotes the number of points in the sample S and sp i denotes the i th sampling point in S , 1  X  i  X  N S .
Sampling can help improve the efficiency of our method significantly, but the quality of the result may be affected. In what follows, we will discuss convergence of the aver-aged lower and upper bounds of D k when the number of sampling data is increased.

Let X be a variable that can represent the averaged lower or upper bound of D k for a set of data points in a subspace. Let us suppose that there are already i  X  1 sampling points and the i th sampling point is generated. From the conver-gence perspective, we want to ensure that  X   X  2 , where is a positive integer, for  X  i  X  , we have where X i denotes the average value of X for all the first i sampling points.  X  is called convergence factor and is usually a small positive number (say 0.01). The ratio of value of X changes due to the inclusion of the new (i.e. the i ) sample point. Eq. (13) intuitively means that, when each newly generated sampling point does not considerably change the average value of X after the sample reaches a certain size , then we can claim that a convergence of X value of data points have been achieved. X i is defined based on X i  X  1 recursively as follows: where X i denotes the X value of the i th sample point.
Plug Eq. (14) into Eq. (13), we have
After simplification, we can get Lemma 4: The minimum number for the sampling data integer that is no less than ( note the minimum and maximum values of X for all the points in the dataset, respectively.
 Proof: Based on Eq. (15), we need to have | then Eq. (15) can always be satisfied. Therefore, the mini-mum number of sampling points required for X , denoted as N sample ( X ) , is computed as As desired.

In order to produce sufficient sampling data to achieve convergence for both the lower and upper bounds of D k , the optimal (minimum) number of sampling data in subspace s is specified based on Eq (17) as follows: where LB max ( D k ) and LB min ( D k ) are the maximum and minimum values of the lower bound of D k for all the points in the dataset, U B max ( D k ) and U B min ( D k ) are the maxi-mum and minimum values of the upper bound of D k for all the points in the dataset.

Although the optimal number of sampling points has been explicitly specified in Eq. (18), we would face the following dilemma in practice when specifying its value: On one hand, the objective of performing data sampling is to achieve performance boost by only working on a small portion of the original dataset. On the other hand, however, the optimal number of sampling points cannot be specified without evaluating the whole dataset in order to find the global minima and maxima.

Due to the above dilemma, we propose a novel approach to progressively approximate the optimal sampling points in parallel with subspace fitness computation. The basic idea of this progressive approximation approach is to start with a set of sampling point with a minimum size (can be as small as 2 sampling points) and incrementally grow this set when necessary during the course of subspace evaluation in the genetic algorithm. Specifically, the approximation i s performed progressively in the following two iterative ste ps: 1. The local minimum and maximum values of the lower 2. If the number of current sampling points N sample is The above two steps are repeated until N  X  sample &gt; N sample is no longer met.

Having the sampling data for the first subspace, the sam-pling data to be used for subsequent subspaces will be gen-erated in an incremental way. The sampling data for one subspace may or may not be large enough for achieving a convergence for the bounds of D k in the sampling data for another new subspace. To decide this, we need to compute N sample in the new subspace first and then test whether or is met, indicating the current sampling data is sufficient to reach a convergence of the bounds for this new subspace, then we will just utilize the current set of sampling data for this new subspace without introducing any new ones. It is also possible that the current sampling data are not enough, thus more new sampling points will be generated for the new subspace until the convergence can be observed.
We use both synthetic and real-life datasets for per-formance evaluation in our experiments. In the synthetic datasets, we are able to specify the number of instances (tu-ples) ( N ) and dimensions (  X  ) of the datasets generated. We also use four real-life multi-and high-dimensional datase ts from the UCI machine learning repository in our experi-ments. These four datasets called Letter Image (D1, 16-dimensional), Image Segmentation (D2, 19-dimensional), Ionosphere (D3, 34-dimensional) and Musk (D4, 168-dimensional), respectively. No missing values will occur in all the synthetic and real-life datasets. As the experime n-tal setup, we set the number of SOF Outlying Subspaces returned in the end n = 20 , the number of generations for the GA N g = 50 , the population size in each generation P = 50 , the frequency of applying crossover p c = 0 . 8 and the frequency of applying mutation p m = 0 . 2 .
Experiments conducted on synthetic datasets are to test the effects of number of points N and number of dimen-sions  X  of the dataset on the performance of our method. Effect of N on constructing Full kNN Lookup Table. N determines the length of the sorting lists obtained from the original dataset, which will further affect the efficiency o f k NN search of each point in constructing its k NN Lookup Table. In this experiment, the block size b is set to be 1000 and 5000 for the block-based method and 1000 for the tree-based method (Note that the time complexity of tree-based method is independent of the block size b ). The time is averaged over 20 given points for all the methods. Fig-ure 3 presents the results. It shows that the list-based and tree-based methods are very close to each other in terms of running time and are more efficient than the block-based method. This is because that the time complexities of list-based and tree-based methods are both logarithmic w.r.t N while that of the block-based method is approximately lin-ear w.r.t N . Also, when block size b is increased (say from 1000 to 5000 in this experiment), the complexity of the block-based method is decreased. In the extreme, when b approaches N , the time complexity of the block-based method will become O ( log 2 N + k ) , which will be equiv-alent to the complexities of the list-based and tree-based methods.
 Effect of N on SOF computation. N affects the efficiency of fitness evaluation for each subspace in the GA. When no approximation of SOF is used, the time complexity of fit-ness computation using the nested-loop method for k NN search is quadratic with respect to N . Nevertheless, the complexity can be reduced to a linear order of N if our ap-proximation scheme of SOF is employed. Moreover, if we choose to work on the sampling data for performance boost, then the execution time is independent of N in any way. This is because that the number of sampling points we use in fitness evaluation for subspaces is only depended on the characteristics of data, as revealed in Eq. (18). This empir i-cal analysis is confirmed in Figure 4. In this experiment, the execution time is the average time spent in evaluating each subspace. The results of this experiment illustrate that SO F approximation and sampling are very promising in boost-ing performance of our method. Under different N values, our algorithm can run 2-20 times faster than the nested-loop method when using SOF approximation and can run 10-130 times faster when using both SOF approximation and ran-dom sampling.
 Effect of  X  on the search workload of the GA.  X  deter-mines the size of the search workload for subspaces, which is in an exponential order of  X  . This does not necessarily mean that the running time of our algorithm will be expo-nential with respect to  X  whatsoever. The actual running time is depended on how the search workload in the GA is specified. More precisely, if we use a fixed number of generations and population size in each generation of the GA, i.e. a fixed workload , then the total search workload in the GA will be the same under different  X  . Yet, using a fixed search workload in the GA for datasets with different dimensions may not an effective strategy. A varied work-load scheme, in contrast, performs a search workload that is specified by a workload function . The workload function specified by the users is a monotonically increasing func-tion of  X  , reflecting the effect of  X  on the search workload of algorithm. The time complexity may now become expo-nential with respect to  X  as long as the workload function is an exponential function w.r.t  X  . In our experiment, the search workload under the fixed workload scheme is set to be 2500 (50 generations with 50 individuals in each gen-eration) and is stipulated by workload function w =  X  2 in the varied workload scheme. The running time of these two search schemes for detecting SOF Outlying Subspaces of 20 given data points are presented in Figure 5. The running time of our algorithm under fixed workload scheme scales linearly w.r.t  X  while that under varied workload scheme is in a quadratic order of  X  .
Using the real-life multi-and high-dimensional datasets in UCI machine learning repository, we investigate the per-formance of our method in fitness convergence and sub-space refinement.
 Fitness convergence study. GA tends to produce an in-creasing number of fitter individuals as evolution proceeds , referring to as the phenomenon of convergence. In this ex-periment, we investigate the fitness convergence of our tech -nique. For each generation, the number of individuals with relatively high fitness (  X  2 . 0 ) are counted. As we can see from Figure 6 that the number of individuals with high fit-ness for the four datasets is increased as the GA evolves, which indicates a good convergence of our method. A good convergence is beneficial as this will enable our method to find good outlying subspaces w.r.t the given point without exploring a huge number of subspaces.
 Fitness boost by performing refinement in the GA . We study the contribution of the subspace refinement step used in GA to enhancing fitness of the outlying subspaces de-tected, compared to the case when no refinement is in-volved. When no refinement is performed in the GA, the SOF Outlying Subspaces are the top n subspaces in the CandidateSet having the highest approximated SOF val-ues. However, there top n outlying subspaces chosen based on approximated SOF values may not be the true outlying subspaces having the high SOF values. The fitness improve-ment by using the refinement step is due to the extra com-putations performed on the subspaces in CandidateSet in order to get their accurate SOF values and the top n sub-spaces with the highest accurate SOF values are returned as SOF Outlying Subspaces. Figure 7 presents the results. The results demonstrate that the fitness gain by performing the refinement step ranges from 13% and 21% for the four datasets when compared with the case when no refinement is performed.
 Subspace pruning in refinement step of the GA. In this experiment, we would like to study the advantage of em-ploying the subspace pruning strategy, devised based on the bounds of D k , in computation saving in the refine-ment step of the GA. The bounds of D k of subspaces help speed up our method by pruning away those subspaces in CandidateSet that are definitely not relevant to the final SOF Outlying Subspace in the subspace refinement step. In this experiment, we set the number of subspaces in CandidateSet as 1000 and compare it with the number of subspaces whose SOFs have actually been computed in the refinement step. From Figure 8, we can see that our prun-ing strategy is effective in greatly reducing the number of subspaces to be evaluated in the refinement step and such saving ranges from 19% to 41% for the four datasets.
In this paper, we address the problem of outlying sub-space detection. Due to the inherent hardness of this prob-lem, we utilize genetic algorithm as an efficient search method in this work. We proposed a new definition of out-lying subspaces called SOF Outlying Subspace . The lower and upper bounds of D k for any a data point are revealed and three efficient methods are presented for computing the bounds by using the k NN Lookup Tables of data points. We also employ random sampling to significantly improve the performance of our method. The optimal number of sam-pling data ensuring a good approximation of SOF is given, and a novel genetic algorithm is developed for combining subspace fitness evaluation and data sampling. We present the experimental results of our method on both synthetic and real-life datasets. The results demonstrate the efficie ncy and effectiveness of our method in handling outlying sub-space detection.

