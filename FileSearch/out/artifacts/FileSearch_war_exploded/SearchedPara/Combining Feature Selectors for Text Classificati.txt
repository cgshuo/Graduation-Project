 We introduce several methods of combining feature selectors for text classification. Results from a large investigation of these combinations are summarized. Easily constructed combinations of feature selectors are shown to improve peak R -precision and F 1 at statistically significant levels. Categories and Subject Descriptors: H.3 [Information Storage and Retrieval]: Miscellaneous General Terms: Experimentation, Measurement Keywords: text classification, feature selection
We consider combinations of several widely used feature selection methods: document frequency thresholding, infor-mation gain, and the  X  2 methods  X  2 max and  X  2 avg [8]. Each feature selector determines a rank ordering on the docu-ment X  X  features (here words), sorted by score. These lists are taken as input to the combination methods. We con-sider combining two or more input methods using either the scores themselves or the rank ordering of the features.
In highest rank ( HR ) combination, we give as each fea-ture X  X  combined score the highest rank achieved in any of the input score lists. We expect this approach to be suitable if different input methods place different sets of informative features near the tops of their lists. This method discounts the negative information provided by an input; i.e., if one input has high confidence that a feature is uninformative, it will be overruled by any other input which ranks the fea-ture higher. Highest rank combination has previously been used to combine hypothesized labels (rather than features) in classification problems [3]. Similarly, we consider combi-nations using the lowest ( LR ) and average ( AR )rank.
We also consider combining by the feature selectors X  nor-malized scores. Given two or more input vectors of feature scores, normalize them and, for each term, take the largest normalized feature score as the feature X  X  combined score. We may normalize the input feature scores in several different ways. If we normalize input score vectors by dividing each element by the vector X  X  largest element, then the OR effec-tively asks: for which input feature selection method did this term achieve a higher fraction of its largest observed score? We refer to this approach as DMOR (divide by maximum then OR). If on the other hand, we normalize in-put score vectors by dividing each element by the vector X  X  (
L 2 ) norm, then the OR asks: for which input method did this term achieve a higher fraction of the total achieved score across all the terms? We refer to this approach as DLOR (divide by length then OR). Previous feature selection com-bination studies [7] used a normalize then OR combination approach, although to our knowledge they were limited to pairwise combinations using DMOR.

In real world classification problems, low frequency terms are often dominated by noise. Many feature selection meth-ods (particularly those of the  X  2 family) are known to be mislead by infrequent terms [1]. For these reasons, and be-cause the Zipfian distribution of term frequencies leads to significant time savings through the elimination of low docu-ment frequency (DF) terms, text classification studies often disregard them. We call this practice DF cutting ,andsay that, if all terms with DF less than or equal to C are ignored, that we are cutting at level C . We consider combinations of feature selection methods computed at the same cutting level only.
We classify the documents using a local implementation k -Nearest Neighbors with symmetric Okapi term weighting [5]. For all trials, we fixed k at k = 100.

We conducted a preliminary study to explore the large space of possible combination methods. Using 23,149 doc-uments from RCV1-v2 [4], we produce five partitions such that each training set contained 20% of the documents ran-domly assigned. On each of these splits, we ran every ex-perimental setting (i.e., combination type, input methods, cutting level, and number of features). This entailed roughly 197 million classifications of documents.

Figure 1 shows R -precision averaged over the prelimi-nary trials for each method investigated. The top few non-combination and combination methods are labeled. The best performing combination and non-combination methods are based on  X  2 , with the best combinations using only two inputs. The non-combined methods appear to benefit from more aggressive cutting than the best combination methods. We see performance improves for the all-features case with low cutting, while it suffers if the cutting is too high.
We conducted a set of validation experiments, now in-cluding only those combination methods suggested by the previous study, but with many more trials to ensure statis-tical significance. Our validation data is a set of 200,000 new RCV1-v2 documents. We partitioned this new data into 20 disjoint sets of 10,000 documents each, before further divid-Figure 1: Peak R -precision averaged over the preliminary experiments for each method investi-gated. The top few non-combination and combina-tion methods are labeled.
 Table 1: How often combinations beat non-combinations for highest R -precision and F 1 in 20 disjoint test/train sets, c onsidering all feature set sizes. Parentheses contain sign test p-values. ing each of these in half to produce 20 pairs of testing and training sets of 5,000 documents. We again consider the 101 topic categories for our classification task.

We use the non-parametric Fisher Sign test for statisti-cal significance in our improvements, comparing paired R -precisions for each of the 20 trials. For 20 trials, 15 wins is significant with p-value of 0 . 021. Figure 2 depicts how often each method in a pair achieved the highest R -precision, for each of the set sizes. We observe that the combination meth-ods improve over non-combination methods at a statistically significant level over the majority of feature set sizes.
While combination methods achieved higher R -precision over the majority of individual set sizes, recall that we se-lected methods from our preliminary study to attain a high-est peak R -precision. Table 1 shows how often each method in each pair obtained the highest peak over all set sizes. Because many complete tasks require thresholding to pro-duce label sets, we also report here microaveraged F 1 .We see that combination methods beat out the non-combination methods for peak R -precision, at statistically significant lev-els, for every pair observed. The improvement is similarly clear in the F 1 results, although we note that reversals oc-curred on a small number of the validation partitions. Figure 2: Bars depict how often each method achieved the higher R -precision in 20 trials, compar-ing combination and non-combination approaches.
We have seen that easily constructed combinations of fea-ture selectors can achieve higher peak R -precision and mi-croaveraged F 1 than their non-combined feature selection counterparts.

Future work might investigate the robustness of these combinations with respect to corpus characteristics (e.g., the skew in topic distribution) or the combination of other input feature selection methods (e.g., bi-normal separation [2]).
We refer the interested reader to [6] for a more detailed report of our investigation and results.
This work was supported by NSF IIS 0122466 (MALACH).
