 Similarity search, or finding approximate nearest neighbors, is an important technique for many applications. Many recent research demonstrate that hashing methods can achieve promising results for large scale similarity search due to its computational and memory efficiency. However, most existing hashing methods treat all hashing bits equally and the distance between data examples is calculated as the Hamming distance between their hashing codes, while different hashing bits may carry different amount of information. This paper proposes a novel method, named Weighted Hashing (WeiHash), to assign different weights to different hashing bits. The hashing codes and their corresponding weights are jointly learned in a unified framework by simultaneously preserving the similarity between data examples and balancing the variance of each hashing bit. An iterative coordinate descent optimization algorithm is designed to derive desired hashing codes and weights. Extensive experiments on two large scale datasets demonstrate the superior performance of the proposed research over several state-of-the-art hashing methods. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Hashing, Similarity Search
Similarity search, also known as approximate nearest neighbor search, has many applications such as content-based image retrieval, similar document detection and collaborative filtering. Many real world applications of similarity research need to process a huge amount of data within a high-dimensional space to answer a query. Simple similarity search methods in the original high-dimensional space do not scale up due to the excessive cost in storage and processing when the data size grows. Recently, hashing methods [5, 10] have been successfully used for approximating nearest neighbor search due to its fast query speed and low storage cost. The basic idea of hashing is to design compact binary codes in a low-dimensional space for data points and preserve their similarities. More specifically, each data point will be hashed/mapped into a compact binary code, and similar points in the original feature space should be hashed into close points in the binary hashing code space. Hashing methods have become a promising and popular choice for efficient similarity search due to its two main advantages, reduced storage cost and fast query time.
Locality-Sensitive Hashing (LSH) [1] is one of the most commonly used data-independent hashing methods. It utilizes random linear projections, which are independent of training data, to map data points from a high-dimensional feature space to a low-dimensional binary space. Another class of hashing methods are called data-dependent methods, whose projection functions are learned from training data. These data-dependent methods include spectral hashing (SH) [10], principal component analysis based hashing (PCAH) [4], self-taught hashing (STH) [12] and iterative quantization (ITQ) [2]. SH learns the hashing codes based on spectral graph partitioning and forcing the balanced and uncorrelated constraints into the learned codes. PCAH utilizes principal component analysis (PCA) to learn the projection functions. STH combines an unsupervised learning step with a supervised learning step to learn effective hashing codes. ITQ learns an orthogonal rotation matrix to refine the initial projection matrix learned by PCA so that the quantization error of mapping the data to binary codes is minimized. Compared with the data-independent methods, these data-dependent methods generally provide more effective hashing codes.

Hashing methods generate promising results by success-fully addressing the storage and search efficiency challenges. However, most existing hashing methods treat all hashing bits equally and the similarity between data examples is represented by the Hamming distance between their hashing codes. But different hashing bits carry different amount of information (e.g., [3]), where the hashing bit with larger variance contains more information. Therefore, it is unreasonable to use a same weight for different hashing bits. This paper proposes a novel hashing method, named Weighted Hashing (WeiHash), to assign different weights on different hashing bits to capture their importance. The hashing codes and their corresponding weights are jointly learned in a unified framework by simultaneously preserving the similarities between data examples and balancing the variance of each hashing bits. An iterative coordinate descent algorithm is designed as the optimization procedure. This section first states the problem setting of WeiHash. Assume there are total n training data examples, denoted as: X X X = { x 1 ,x 2 ,...,x n }  X  R R R n  X  m , where m is the dimensionality of the feature. The main purpose of weighted hashing is to map these training examples to the optimal binary hashing codes Y Y Y = { y 1 ,y 2 ,...,y n }  X  { X  1 , 1 } through a hashing function f : R R R m  X  { X  1 , 1 } k , such that the similarities among data examples in original feature space are preserved in the hashing codes with appropriate weights assigned to hashing bits. Here k is the number of hashing bits and y j = f ( x j ).
One of the key problems in hashing methods is similarity preserving, which indicates that similar data examples should be mapped to similar hashing codes. Most existing hashing methods [3, 10, 12] define the similarity between two hashing codes accordingly to their Hamming distance. The Hamming distance between two binary codes y i and y j is given by the number of different bits between them, which can be calculated as 1 4 k y i  X  y j k 2 . However, the Hamming distance may not accurately represent the similarity between two hashing codes since different hashing bits carry different amount of information. Therefore, we propose to use the weighted Hamming distance to capture the code similarity as 4 k w  X   X   X  ( y i  X  y j ) k the importance of different bits and  X   X   X  is the element-wise vector multiplication.

To measure how well the similarity between data examples is represented by the binary hashing codes, one natural way is to minimize the weighted Hamming distance as follow: Here, S S S is the similarity matrix which is calculated based on the original features. To meet the similarity preservation criterion, we seek to minimize this quantity, because it incurs a heavy penalty if two similar examples are mapped far away. There are many different ways to define the similarity matrix nice property in many machine learning applications [9, 12]. In particular, the corresponding similarities are computed by Gaussian functions, i.e. , S S S ij = e determined automatically by local scaling [11].

By introducing a diagonal n  X  n matrix D D D , whose entries are given by D D D ii = P n j =1 S S S ij and a diagonal k  X  k matrix W W W with diagonal elements to be the weight vector w , Eqn.1 can be rewritten as: where L L L is the graph Laplacian and tr () is the matrix trace. By minimizing this term, the similarity between different data examples can be preserved in the learned hashing codes.
Different weights are assigned to different hashing bits in this work to better measure the similarity between hashing codes. This is also observed in previous research work that different hashing bits carry different amount of information and the hashing bit with larger variance should contain more information. Therefore, more weight should be assigned to the hashing bit with larger variance and we propose to follow the proportion principle that the weight of a hashing bit should be proportional to its variance denoted as: where w p and var ( y p ) 1 are the weight and variance of the p -th hashing bit.
In order to maximize the performance of the hashing codes, many existing hashing methods [9, 10, 12] enforce hard orthogonality constraint among hashing bits, e.g. ,
Y Y Y T Y Y Y = I I I . The hard orthogonality constraint forces the hashing bits to be uncorrelated with each other. However, these constraints may sometimes be problematic, since most of the variance is contained in a few top projections for many real-world datasets. But the orthogonality constraints often force hashing methods to choose some directions with low variance progressively, which may substantially reduce the effectiveness of hashing codes. Therefore, instead of imposing hard orthogonality constraint on hashing bits, we impose a soft orthogonality term as follows: where kk F is the matrix Frobenius norm. The above soft orthogonality constraint has certain tolerance to non-orthogonality, which allows the learning approach to choose appropriate projection directions.
The proposed unified framework combines the above three components as follows: The bit balance constraint Y 1 Y 1 Y 1 = 0 requires each bit to have equal chance to be 1 or -1 and P p w p = 1 is the weight normalization constraint.
Directly minimizing the objective function in Eqn.5 is intractable because of the discrete constraint. Therefore, we propose to relax this constraint and drop the bit balance constraint Y 1 Y 1 Y 1 = 0 first (we will discuss the constraint later). However, even after the relaxation, the objective function is still difficult to optimize since Y Y Y and W W W are coupled together and it is non-convex with respect to Y Y Y and W W W jointly. var ( y p ) = 1 n P n j =1 ( y p j  X  mean ( y p )) 2
We propose to use a coordinate descent algorithm [7] for solving this relaxed optimization problem by iteratively optimizing the objective with respect to Y Y Y and W W W . In particular, after initializing W W W , the relaxed problem can be solved by doing the following two steps iteratively similar as [8], until convergence.

Step 1: Fix W W W , optimize w.r.t. Y Y Y : The above problem is still non-convex. However, the objective function is differentiable with respect to Y Y Y and the gradient of Eqn.6 can be calculated as follows: With this obtained gradient, L-BFGS Quasi-Newton method is applied to solve this optimization problem. Step 2: Fix Y Y Y , solve for W W W :
We can obtain the close form solution of W W W as: where var ( y p ) can be directly computed from the hashing codes Y Y Y . By solving Eqns.6 and 8 iteratively, the optimal values of Y Y Y and W W W can be obtained.
After obtaining the optimal solution for the relaxed problem, we need to binarize them to obtain binary hashing codes that satisfy the relaxed constraints. The binary hashing codes for the training set can be obtained by thresholding Y Y Y . It was pointed out in [4] and [9] that desired hashing codes should also maximize the entropy to ensure efficiency. Following the maximum entropy principle, a binary bit that gives balanced partitioning of the whole dataset should provides maximum information. Therefore, we set the threshold for binarizing the p -th bit to be the median of y p . In particular, if p -th bit of y j is larger than median value, y p j is set to +1, otherwise y p j is set to -1. In this way, the binary code achieves the best balance and the bit balance constraint Y 1 Y 1 Y 1 = 0 in Eqn.5 can also be satisfied.
A linear hashing function is utilized to map data examples to the binary hashing codes as: where H H H is a k  X  m parameter matrix representing the hashing function. Then the optimal hashing function can be obtained by minimizing k Y Y Y  X  HX HX HX k 2 .
NUS -WIDE dataset is created by NUS lab as a benchmark for evaluating image similarity search techniques. It contains total 269 , 648 images. 500-dimensional visual features are extracted using a bag-of-visual-word model with local SIFT descriptor. We randomly partition this dataset into two parts, 268 K as training data examples and around 1 K for query test. MIRFLICKR -1 M dataset is collected from Flicker images for image retrieval and similar visual concept detection tasks. This dataset contains 1 million image examples. 512-dimensional GIST descriptors are extracted from these images and are used as image features for learning the binary hashing codes. We randomly choose 990 K image examples as the training set and 10 K for testing. The parameters  X  and  X  are tuned by cross validation on the training set. The number of nearest neighbors is fixed to be 7 when constructing the graph Laplacian for all experiments. For LSH, we randomly select projections from a Gaussian distribution with zero-mean and identity covariance to construct the hash tables.
The search results are evaluated based on the labeled semantic tags. If an example shares same semantic tag with a query, it is a relevant example, otherwise it is irrelevant. We use several metrics to measure the performance of different methods. For evaluation with Hamming Ranking , we calculate the precision at top k that is the percentage of relevant neighbors among the top k returned examples, where we set k to be 200 in the experiments. We also utilize the precision-recall value [9] for evaluation, which is a widely used metric in information retrieval applications. For evaluation with Hash Lookup , all the examples within a fixed weighted Hamming distance, r , of the query are evaluated. In particular, following [6] and [10], a weighted Hamming distance r = 2 is used to retrieve the neighbors in the case of Hash Lookup . The precision of the returned examples falling within weighted Hamming distance 2 is reported. Note that if a query returns no points inside the Hamming ball with weighted Hamming distance 2, it is treated as zero precision.
The proposed WeiHash approach is compared with five different methods, i.e. , Spectral Hashing (SH) [10], PCA Hashing (PCAH) [4], Latent Semantic Hashing (LSH) [1], Self Taught Hashing (STH) [12] and Isotropic Hashing (IsoHash) [3]. IsoHash is compared here due to its superior performance over many other hashing methods such as ITQ and KLSH. We evaluate the performance of different methods by varying the number of hashing bits in the range of { 8 , 16 , 32 , 64 , 128 } .

Two sets of experiments are conducted on each dataset to evaluate the performance of WeiHash. In the first set of experiments, we report the precision values for the top 200 retrieved examples in Fig.1(a)-(b). The precision values for retrieved examples with weighted Hamming distance 2 are reported in Fig.1(c)-(d). From these comparison results, we can see that WeiHash gives the best performance among all hashing methods on both datasets. In Fig.1(c)-(d), the precision values of Hash Lookup decrease significantly with the increasing number of hashing bits. This is because when using longer hashing bits, the Hamming space becomes increasingly sparse and very few data points fall within the Hamming ball with radius 2, resulting in many 0 precision queries. However, the precision values of WeiHash are still consistently higher than other methods.

In the second set of experiments, the precision-recall curves with 16 and 32 hashing bits on both datasets are reported in Fig.2. It is clear that among all of these comparison methods, WeiHash shows the best performance. From the reported figures, we can see that LSH does not Figure 1: Precision results on two datasets with different hashing bits. (a)-(b): Precision of the top 200 retrieved examples using Hamming Ranking . (c)-(d): Precision within Hamming radius 2 using Hash Lookup . perform well in most cases. This is because the LSH method is data-independent and may lead to inefficient codes in practice. For SH and STH, although these methods try to preserve the similarity between data examples in their learned hashing codes, they treat each bit equally and preserve the example similarity represented by standard Hamming distance while our WeiHash approach assigns different weights to different hashing bits based on their variance to capture the importance of each bit and the weighted Hamming distance is utilized for similarity preservation. IsoHash achieves better performance than SH and STH since it somehow tries to balance the variance along different projection dimensions. However, hashing bits are still treated equally whereas in WeiHash, the optimal hashing codes and weights are jointly learned to achieve better hashing performance.
This paper proposes a novel Weighted Hashing method to assign different weights to different hashing bits. The hashing codes and their corresponding weights are jointly learned in a unified framework by simultaneously preserving the similarities between data examples and balancing the variance of each hashing bit. Two large scale image datasets have been used to demonstrate the advantage of the proposed research over several state-of-the-art hashing methods. This work is partially supported by NSF research grants IIS-0746830, CNS-1012208 and IIS-1017837. This work is also partially supported by the Center for Science of Information (CSoI), an NSF Science and Technology Center, under grant agreement CCF-0939370.
 Figure 2: Precision-Recall behavior on two datasets. (a)-(b): Precision-Recall curve with 16 hashing bits. (c)-(d): Precision-Recall curve with 32 hashing bits. [1] M. Datar, N. Immorlica, P. Indyk, and V. S. Mirrokni. [2] Y. Gong and S. Lazebnik. Iterative quantization: A [3] W. Kong and W.-J. Li. Isotropic hashing. In NIPS , [4] R.-S. Lin, D. A. Ross, and J. Yagnik. Spec hashing: [5] R. Salakhutdinov and G. E. Hinton. Semantic hashing. [6] J. Wang, S. Kumar, and S.-F. Chang. Semi-supervised [7] Q. Wang, L. Si, and D. Zhang. A discriminative [8] Q. Wang, L. Tao, and H. Di. A globally optimal [9] Q. Wang, D. Zhang, and L. Si. Semantic hashing using [10] Y. Weiss, A. Torralba, and R. Fergus. Spectral [11] L. Zelnik-Manor and P. Perona. Self-tuning spectral [12] D. Zhang, J. Wang, D. Cai, and J. Lu. Self-taught
