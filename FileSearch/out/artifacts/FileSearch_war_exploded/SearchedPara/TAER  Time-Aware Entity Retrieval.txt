 Retrieving entities instead of just documents has become an important task for search engines. In this paper we study entity retrieval for news applications, and in particular the importance of the news trail history (i.e., past related arti-cles) in determining the relevant entities in current articles. This is an important problem in applications that display retrieved entities to the user, together with the news article.
We analyze and discuss some statistics about entities in news trails, unveiling some unknown findings such as the persistence of relevance over time. We focus on the task of query dependent entity retrieval over time. For this task we evaluate several features, and show that their combinations significantly improves performance.

Entity search has become a new important feature of cur-rent Web search engines. It helps people find directly the information they are after and to reformulate the query pre-cisely in a natural way. For such reasons, Entity Retrieval (ER) is becoming a major area of interest in Information Retrieval (IR) research and is quickly being adopted in com-mercial applications. Published research on ER has concen-trated on the tasks of people search and finding related en-tities [1], where evaluation corpora have been developed (at TREC [3] and INEX [4] respectively). One of the promising areas of application of ER models in the commercial world is in news search 1 . A possible application consists in enriching
Work performed while intern at Yahoo! Research. This work is partially supported by the EU Large Scale Integrated Project LivingKnowledge (contract no. 231126). http://news.bbc.co.uk, http://news.google.com, http://news.yahoo.com the user interface by placing retrieved entities next to the news article the user is currently looking at.

In this paper we address the problem of ranking entities in news applications. We define the task of Time-Aware Entity Retrieval (TAER) which takes into account the evolution of entity relevance over time in a news topic thread. To eval-uate the effectiveness of systems performing such task we analyze an extension of the TREC 2004 Novelty corpus [8], annotating relevance at the level of entities [5]. We then evaluate features and ranking models for the TAER task. Dealing with ER in news is particularly interesting as news articles are often focused on entities such as people, compa-nies, countries, etc. It is also a challenging task as, differ-ently from standard ER tasks, there is the time dimension involved. Given a news topic, the decision about which en-tities should be retrieved or not changes with time. Not all frequently appearing entities should be considered relevant to the topic (e.g., news agencies) and new important entities may appear later in the story (e.g., witness of a murder).
We propose a system which takes into account both in-formation from the current news article as well as from the past relevant articles in order to detect the most important entities in the current news. Our main findings, obtained by analyzing dataset and features, are that sentence nov-elty is worse than pure sentence relevance as an indicator of entity relevance; entities that become relevant have a high probability of remaining relevant the next article and the entire news thread; the relevant history of an article (e.g. the previous relevant articles) can be exploited as a source of information for TAER.

The paper is organized as follows. Section 2 presents pre-vious related work on entity search. Section 3 defines the task we address comparing it to standard ER and intro-duces the dataset we created for evaluating time-aware en-tity search. Section 4 presents an experimental evaluation of extracted features for time-aware entity search. The paper ends with a conclusion section. Searching for entities is a common user activity on the Web. There is an increasing effort in the research commu-nity in developing entity search techniques and in building evaluation benchmarks. One example is the expert search task evaluated in the context of the TREC Enterprise Track [3], where the goal is to find entities (people) that have rel-evant expertise about a topic of interest. Language models-based approaches [1] are among the most promising tech-niques for ranking experts. The INEX Entity Ranking Track is another evaluation initiative where the task is to return a list of relevant Wikipedia entities for a given topic using an XML collection [4]. In this context, Vercoustre et al. [9] use Wikipedia categories and link structure together with entity examples to improve ER effectiveness. In the TREC 2009 Entity Track [2] the task of finding related entities given one entity as query (e.g.,  X  X irlines that currently use Boeing 747 planes X ) was investigated. Compared to previous work on ER we analyze the usefulness of the time dimension for this task. In [5] we introduced the task of time-aware entity retrieval and some features for it. In this paper we describe additionally properties of the dataset, exploit sentence rele-vance, and study machine learning combinations of features.
Standard Entity Retrieval is defined as follows: For example, the ER task was performed in [10] using Wikipedia as a document collection. Consider the following user sce-nario: a user types a query (or topic) into a news search engine and obtains a list of relevant results, ordered by time. Furthermore, the user subscribes to this query so in the fu-ture she will continue to receive the latest news on this query (or topic). We are interested in ER tasks related to this user scenario. Standard ER could be used to show to the user the most interesting entities for the query . The temporal dimension is not needed here. However, if the user is ob-serving a current document, we may want to show the most relevant entities of the document for her query (or topic). This prompts the following definition:
This is a newly defined task that can be useful, for exam-ple, in news verticals for presenting the user more than just a ranked list of documents. In the news context we define the task for most considered entity types: persons, locations, or-ganizations, and products. More formally, we define a  X  X ews thread X  relevant to a query as the list of relevant documents D =[ d 1 ...d n ]. Then, given a document d i we define its history as the list of relevant documents H =[ d 1 ...d i  X  1 chronologically ordered pre-dating the document d i .Given an entity e ,wenoteas d e, 1 the first document in which the entity occurred in the news thread. Note that such a doc-ument is not necessarily the first document in D as entities may appear only in subsequent documents. Additionally, we will note as d e,  X  1 as the last document in H containing e .
The TREC Novelty Track in 2004 was based on a col-lection of news articles and a set of topics for evaluating retrieval of novel information over ranked lists of documents for each topic. The systems had to retrieve information (i.e., sentences in this case) relevant to the topic and not yet present in the retrieved results [8].
 We selected the 25  X  X vent X  topics from the latest TREC Novelty collection (2004). We annotated the documents as-sociated with those topics using state of the art NLP tools Table 1: Probabilities of relevance for different entity [10] in order to extract entities of type person, location, or-ganization, and product. Then, six human judges assessed the relevance of the entities in each document with respect to the topic grading each entity on the 3-points scale: Relevant, Related, Not Relevant 2 . Double assessments on six topics shown an assessors X  agreement of 0.5232 (Cohen X  X  Kappa). More information about the data is available in [5].
The TREC 2004 Novelty collection consists of an average of 31.2 articles per topic distributed over time. After the annotation, each document contains on average 26.5 anno-tated entities among which 7.6 were judged relevant. On average each topic contains 63.4 entities which have been marked relevant at least once over the topic timeline.
We now investigate the relation between entities, sentence and relevance. Let n s , r s indicate that a sentence s is novel or relevant respectively. Let t e indicate the type of entity e , and let us denote by r e the fact that e is relevant, and r otherwise. On average, a sentence contains 1 . 46 entities, a relevant sentence contains 1 . 88 entities, and a novel sentence contains 1 . 92 entities which indicates the presence of more information. The unconditional probability of a relevant entity in a sentence P ( r e )is0 . 411 (we first sample a sentence and then an entity in that sentence). The probability of finding a relevant entity in a relevant sentence P ( r e | 0.547 with a 95% bootstrap confidence interval of [0 . 534 0 . 559], well above P ( r e ). The probability of a relevant entity in a novel sentence P ( r e | n s )is0 . 510 [0 . 491  X  0 . 531] which is below the probability in a relevant sentence.

This gives the following high level picture. Relevant sen-tences contain slightly more entities than non-relevant ones. Novel sentences contain slightly more entities than relevant (but not-novel) ones; however, entities in novel sentences are more likely to be irrelevant than in not-novel sentences.
In Table 1 we look at relevance probabilities per entity type (e.g., the probability of person entity being relevant would be noted P ( r e | t e = person )). We see that sentence novelty is less important than sentence relevance regard less of the entity type . Organization entities are more likely in a relevant sentences than the rest.

As compared to a classic document collection, in a news corpus the time dimension is an additional available feature. How useful is the information from past news articles? The probability of an entity being relevant in a document given
The evaluation collection we have created is available for download at: http://www.l3s.de/~demartini/deert/ Figure 1: Probabilities of entity relevance given its rele-that it was relevant the first time it appeared ( d e, 1 )is0 . 893 [0 . 881  X  0 . 905] which shows how in most cases an entity which is relevant at the beginning of its appearance stays relevant for the rest of the news thread. It is also impor-tant to observe just the previous document where the entity appeared. The probability of an entity being relevant in a document given that it was relevant the previous time it ap-peared is 0 . 701 [0 . 677  X  0 . 726]. Conversely, the probability of a relevant entity changing relevance status form one story to the next is 0 . 3. Another characterization of this is the probability of an entity being relevant in a document given that it was relevant in the i-th document of its history. This is shown in Figure 1 for relevant, related and not-relevant entities. We can see that relevant entities are the most sta-ble over time while related entities tend to change relevance status over time (either to relevant or to not-relevant).
For performing the TAER task we exploit features defined in [5]. In detail, we consider the following features: the frequency of an entity e in a document d ,noted F ( e, d ). We will use this feature as our baseline. Then, we consider the average or the sum of BM25 score of the sentences where e appears in d (noted AvgBM 25 s ( e, d )and SumBM 25 s ( e, d ) respectively) 3 . We also consider the number of times an entity e appears as subject of a sentence in the document d ,noted F subj ( e, d ); the length of the first sentence where e appears in document d ,noted FirstSenLen ( e, d ); and the position of the first sentence where e appears in d (e.g, the 4th sentence in the document), noted FirstSenPos ( e, d ).
As the dataset analysis shown that past related articles may contain important information about entity relevance, we also consider a number of features that take into con-sideration the document history H : the frequency (i.e., the number of times it appears) of the entity e in the history H ,noted F ( e, H ); the document frequency of e in H ,noted DF ( e, H ); the frequency of entity e in the first document where the entity appeared, noted F ( e, d e, 1 ); the frequency of entity e in the previous document where the entity ap-peared, noted F ( e, d e,  X  1 ); and the number of other entities with which the entity co-occurred in a sentence in the set of past documents H ,noted CoOcc ( e, H ).
We compare the effectiveness of different features and fea-ture combinations using several performance metrics. In or-
We computed the BM25 scores of sentences with respect to a disjunctive query consisting of all the terms in the topic title using b =0 . 75, k 1 =1 . 2. der to evaluate the complete entity ranking produced by the proposed features, we compute Mean Average Precision (MAP). For completeness, as we aim at showing the user few entities, we check for early precision as well. We report values for Precision@3 (P@3), Precision@5 (P@5), and we test for statistical significance using the t-test. To compute the measures we consider related entities as non-relevant. Many of the features we use are based on entity frequency, hence entity scores in the ranking will have many ties. For this reason, the evaluation measures we have computed are aware of ties, that is, they consider the average value of the measure for all possible combinations of tied scores [7].
We can have an initial analysis of such features by check-ing how entity relevance probability changes with the fea-tures value. Figure 2 shows the probability of an entity be-ing relevant given different values of the features described above. We see that all are correlated with relevance over their entire domain.
 Figure 2: Probability of an entity being relevant given
Table 2 shows effectiveness values obtained when ranking entities in a document according to defined features, where no local feature performs better than the simple frequency of entities in the document. For comparison, a feature that assigns the same score to each entity would obtain a MAP value of 0 . 42 with a ties-aware measure. The second best lo-cal features is SumBM 25 s (0.52 MAP) which takes into consideration relevance of sentences where the entity ap-pears. On the other hand, the features looking at the first sentence where the entity appears in the news article (First-SenLen, FirstSenPos) do not perform well (0.45 and 0.43 MAP respectively). In order to exploit the position of the first sentence where an entity appears we need to deal with the problem of headers in news articles (e.g., news agency codes): as articles have different header lengths, it is not easy to detect the beginning of the article body.

In general, history features perform better than local fea-tures and the highest performance is obtained by ranking entities according to its frequency in the past documents ( F ( e, H )). All history features but F ( e, d e, 1 ) significantly improved over the baseline in terms of MAP. In terms of early precision (P@5) only F ( e, H ) and the similar feature DF ( e, H ) improve over the baseline. Moreover, features us-ing the entire history H are performing better than features looking at single documents in the past.

It is also interesting to note that, when identifying rele-vant entities for a document, the frequency of the entity in the previous document in the story F ( e, d e,  X  1 ) is a better evidence than the frequency in the current document. This may be an indication of how people read news: some entities become relevant to readers after repeated occurrences. If an entity appears in the current and previous documents it is more likely to be relevant.

We additionally weighted the scores obtained from dif-ferent documents in H with both the document length and BM25 score of the document with respect to the query. This approach did not improve the effectiveness of the original features without per-document weighting. Given these re-Table 2: Effectiveness of individual features and of fea-sults we conclude that the evidence from the past is very im-portant for ranking entities appearing in a document. Thus, we expect effectiveness of methods that exploit the past to improve as the size of H grows. That is, the more history is available the better we can rank entities in the current news.
The y-axis of Figure 3 plots the average MAP for all the documents with history size | H | using the feature F ( e, H ). For | H | &lt; 20 the effectiveness of F ( e, H ) increases together Figure 3: Mean Average Precision values for documents with | H | up to values of 0.7. Results for higher values of show no clear trend due to the fact that there are just a few datapoints.

So far we have presented different features for ranking entities that appear in a document. Combining them in an appropriate manner yields a better ranking of entities; however, because the distribution of relevance probability is different among features, we need a way for combining them. In order to combine two or more features together we used Machine Learning (ML) techniques. We performed 2-fold cross validation training a multinomial logistic regression model with a ridge estimator [6] with default parameters for ranking entities in each document.

Table 2 presents a combination of every local and his-tory feature. The combination of all local features performs better then the baseline and then the single local features. When all the features are combined (local+history) we ob-tain the best effectiveness. Such improvements are anyway negligible if compared with the best 2 features combination, that is, F ( e, d )and F ( e, H ) obtaining a MAP of 0.68 [5]. Therefore, we can see how these two simple features per-form very well and that it is difficult to improve over such approach.
In this paper we have addressed the problem of entity search and ranking over time. For this purpose, we defined an original entity search task and further analyzed a time-stamped test collection for evaluating it. One of the conclu-sions is that determining the relevance of a sentence is very important to determine the relevance of an entity; more so than determining sentence novelty. In fact novel sentences introduce more entities than non-novel sentences, but many of these are not relevant.

We have evaluated features both from the current doc-ument and from previous ones in the document X  X  history in order to find relevant entities in a given document. We have experimentally shown that past frequency of entities is the most important of the features explored so far, more important than entity frequency in the current document. We have tested several combinations of proposed features obtaining an overall statistically significant improvement of 15% in terms of MAP over the baseline that considers the frequency of entities in the document.
