 Cross-modal retrieval has been an emerging topic over the last years, as modern applications have to efficiently search for multimedia documents with different modalities. In this study, we propose a cross-modal hashing method by follow-ing a cluster-based joint matrix factorization strategy. Our method first builds clusters for each modality separately and then generates a cross-modal cluster representation for each document. We formulate a joint matrix factorization pro-cess with the constraint that pushes the documents X  repre-sentations of the different modalities and the cross-modal cluster representations into a common consensus matrix. In doing so, we capture the inter-modality, intra-modality and cluster-based similarities in a unified latent space. Finally, we present an efficient way to generate the hash codes us-ing the maximum entropy principle and compute the binary codes for external queries. In our experiments with two pub-licly available data sets, we show that the proposed method outperforms state-of-the-art hashing methods for different cross-modal retrieval tasks.
  X 
Information systems  X  Multimedia information sys-tems; Hashing; cross-modal retrieval; matrix factorization
In multimedia applications, hashing techniques have been widely used for large-scale similarity search, such as locality sensitive hashing [4], iterative quantization [5] and spectral hashing [8]. The key idea is to design hash functions and learn similarity preserving binary codes for data represen-tation with low storage cost and fast query speed. In the aforementioned hashing methods, given a query from one modality, for example an image-query, results are efficiently retrieved from an image database. In this study, we focus on cross-modal hashing, for example, given a query from an image modality, how to return the most relevant results from a textual modality. The rich representation of a doc-ument with different modalities has many applications; for instance, in a real-world multimedia application, given an image-query, a search engine can return relevant text docu-ments to describe its details.

To handle the large amount of available multimedia con-tent with different modalities in modern applications, several cross-modal hashing methods have been proposed [1, 6, 11, 13]. The main challenge of cross-modal hashing is how to learn the binary codes by capturing both the inter-modality and intra-modality similarities [2, 12]. For instance, [1] con-structs a unified space and learns groups of hash functions with eigendecomposition and AdaBoost to ensure that if two documents with different modalities are relevant, then their corresponding hash codes are similar. [6] extends spectral hashing to cross-modal retrieval by formulating the learn-ing of binary codes as a tractable eigenvalue problem. [11] presents an iterative scheme to learn a shared hamming space using a graph regularization formulation and a set of binary classifiers. [13] constructs heterogeneous hamming spaces and then connect them, while preserving the local structure using an anchor-based representation. Finally, [12] considers inter-modality and intra-modality consistency to generate a common hamming space, and integrates a linear regression model to learn hash functions so that the binary codes for new documents can be efficiently generated.
In [2], authors make the first attempt to compute hash codes using joint/collective matrix factorization with a la-tent factor model. Each matrix corresponds to the docu-ments X  representations for each modality, and then by jointly factorizing the matrices unified hash codes are generated. By revealing the associations between the different modal-ities and computing the similarities in each modality, the joint matrix factorization hashing method of [2] achieves high cross-modal retrieval accuracy. Nonetheless, the base-line joint matrix factorization that is used in [2] does not preserve the similarities that documents have in their neigh-borhoods/clusters in the same modality, nor considers the associations that the neighborhoods have in the different modalities.

To capture the inter-modality and intra-modality similar-ities at a neighborhood-based level, in this study we intro-duce a cluster-based joint matrix factorization method for cross-modal hashing. Our method consists of three steps, firstly we propose an efficient way to generate cross-modal clust er representations for the documents. In the second step, we incorporate the generated cross-modal cluster rep-resentations into a joint factorization technique to compute the inter-modality and intra-modality similarities of the doc-uments in the different modalities. This is achieved by for-mulating a joint matrix factorization process with the con-straint that pushes the documents X  representations of the different modalities and the cross-modal cluster representa-tions into a common consensus matrix. Finally, we calcu-late the unified hash codes based on the maximum entropy principle [8], as well as the projection matrices to generate hash codes for external documents that do not belong to the database. Our experiments on two benchmark data sets demonstrate the superiority of the proposed hashing method over competitive hashing methods for different cross-modal retrieval tasks.
In the remainder of the paper, we use the following no-tation, numbers are denoted by lower case letters e.g., a ; matrices by plain upper case letters e.g., A ; sets by calli-graphic upper case letters e.g., A ; and vectors by lower case bold letters e.g., a .

Let n be the number of documents and m the number of modalities. Given j = 1 : : : m and i = 1 : : : n , each i -th document is represented in the j -th modality by a d j dimensional feature vector: with d j  X  = d j  X  for two different modalities j and j  X  j -th modality, the n feature vectors are stored in a matrix X ( j )  X  X  X  d j  X  n , where the i -th column is the feature vector x i . In our method, we assume that the inter-modality and intra-modality similarities are calculated in a unified hash code with length k for each document i : The problems that our cross-modal hashing method faces are formally defined as follows:
Definition 1. (Internal Documents' Hash Codes) \To compute a consensus binary matrix B  X  X  0 ; 1 } k  X  n , where the i -th column corresponds to the uni ed hash code b i of an internal document i ."
Definition 2. (External Documents' Hash Codes) \To calculate m projection matrices P ( j )  X  X  X  k  X  d j , with j = 1 ; : : : ; m , so as for any external document i that does not belong to B , given its representation x ( j ) i in modality j to generate the respective hash code b i ." In the following Section, we detail each step of our method.  X  j =1 ; : : : ; m we cluster the n feature vectors x ( j ) power iteration [7]. Thus, we have m different clusterings: where q j is the number of clusters in the j -th modality, with q j  X  = q j  X  for two modalities j , j w -th cluster of the j -th modality, with w  X  1 ; : : : q Z  X  X  X  q  X  n be the cross-modal cluster matrix, with q = q 1 + q belongs to cluster c , and 0 otherwise. The i -th column of Z is a representation (feature vector) of document i based on all the generated clusters in the m modalities. Hence, if two documents, e.g. i and p , belong to the same clusters in most of the m modalities, then the respective i -th and p -th columns (representations) of the cross-modal cluster matrix Z will be similar.
To capture the intra-modality similarities of the n docu-ments,  X  j =1 ; : : : ; m we consider the following matrix fac-torization of X ( j ): ber of latent factors in matrix factorization, equal to the length of the hash codes b i . In addition, we consider the matrix factorization of the cross-modal cluster matrix Z as capture the associations of the n documents at the cluster-based level. Given the m matrices X ( j ) and the cross-modal cluster matrix Z , in total we have m + 1 individual matrix factorizations: As presented in Section 3.1, the cluster assignments are non-negative ( Z  X  0), with the feature vectors also being non-negative in each matrix X ( j )  X  0; consequently, each matrix factorization in Eq. (3) is subject to:
All matrices in Eq. (3) have to be jointly factorized to si-multaneously compute the inter-modality and intra-modality similarities of the documents in the m modalities, and to capture the associations with the cross-modal cluster ma-trix. We formulate a joint matrix factorization process with the constraint that pushes the matrices X ( j ) of the m modal-ities and the cross-modal cluster matrix Z into a common consensus matrix B  X   X  X  X  k  X  n , with B  X  corresponding to a shared k -dimensional latent space of the n internal docu-ments. To consider the case of having the representation of an external document in modality j , we denote the projec-tion of each modality j into the shared k -dimensional space m projection matrices P ( j ) and the consensus matrix B  X  we formulate the problem of joint matrix factorization as a minimization problem [3]. Based on Eqs. (3) and (4), we have to minimize the following loss function L for the joint matrix factorization:  X  where || X || F denotes the Frobenius norm. In the second line proximation error of each factorized matrix X ( j ) , while the term || P ( j ) X ( j )  X  B  X  || 2 F is the disagreement measurement between the product P ( j ) X ( j ) and the consensus matrix B . Accordingly, in the third line of Eq. (5), the first term ||
Z  X  U z V z || 2 F denotes the approximation error of the factor-ized cross-modal cluster matrix Z ; and || V z  X  B  X  || 2 spective disagreement measurement . The product P ( j ) X matrix V z and the consensus matrix B  X  are comparable, as they have the same dimensionality ( k  X  n ).  X  j = 1 ; : : : ; m each parameter j tunes the weight of modality j over the joint factorization, and the respective disagreement term; accordingly, parameter z for the cross-modal cluster ma-trix. For reasons of simplicity we set 1 = : : : = m = z = . In our approach, we solve the minimization problem of L , using the  X  X ultiplicative rules X  [3], an iterative update pro-cedure, where in each iteration we fix B  X  and minimize L over the rest of matrices, and then we fix the rest of matrices and minimize L over B  X  ; The outcome of the joint factoriza-tion when minimizing the loss function L of Eq. (5) is com-puting matrices U (1) ; : : : ; U ( m ) ; P (1) ; : : : ; P
We generate the unified hash codes in B  X  X  0 ; 1 } k  X  n by bi-narizing the real values of the consensus matrix B  X   X  X  X  k thus generating a binary vector b i = [ y 1 ; y 2 ; : : : ; y internal document i . According to [8], efficient hash codes should also maximize the entropy; based on the maximum entropy principle, a binary bit that gives balanced partition-ing of the whole data set should provide maximum informa-tion. Hence, given the code length k , then  X  a = 1 ; : : : ; k we set a threshold thres a for binarizing the a -th bit, with thres a being the median 1 of the a -th row in B  X  . If a bit y a &gt; thres a , then we set y a = 1 and 0 otherwise. In doing so, the binary code achieves the best balance. To generate the hash code of an external document i that does not be-long to matrix B , we use the projection matrices, computed by the joint matrix factorization when minimizing the loss function in Eq (5). Given the feature vector x ( j ) i  X  X  X  1  X  d of the external document i in the j -th modality, we use the following real-value vector:
We use the median as a threshold, because mean values are sensitive to extremely high or low values in B  X  . which is then binarized as in the internal case, creating the hash code b i for the external document i . We use two real-world data sets Wiki 2 and NUS-WIDE 3 . The Wiki data set consists of 2,866 Wikipedia documents, with each document containing a text and one correspond-ing relevant image. In addition, images are represented by 128-dimensional SIFT feature vectors [10] and text by 10-dimensional topics vectors. All documents (image-text pairs) are labeled by 10 semantic categories. We split the data set as follows, 20% as query set; 5% as cross-validation set to tune the parameters of each method; while the re-maining 75% of the data set is considered as the database, from which the hash codes are learnt and the results are retrieved. NUS-WIDE consists of 269,648 image-tag pairs from Flickr, where we keep the image-tag pairs that belong to one of the 10 largest concepts [11, 14]. Images are repre-sented by 500-dimensional SIFT vectors and text as 1000-dimensional vectors, by performing PCA on the original tag occurrences [14]. The query set consists of 1% of the dataset; 1% is used as cross-validation set; while the remaining data set is the database to retrieve the results. The hash codes are learned from 5K image-text pairs from the database [2].
As both data sets are bi-modal with images and texts, we evaluate our hashing method on the following cross-modal retrieval tasks: (i) image-query  X  text results and (ii) text-query  X  image results. Following the evaluation protocol of relevant studies [2, 11, 14], we measure the performance of cross-modal hashing in terms of mAP . Given a query and the top result set R , Average Precision ( AP ) is defined as: where l is the number of true neighbors in the retrieved set R , that is, the results which belong to the same category with the query; P ( r ) denotes the precision of the top re-trieved results in set R and ( r )=1 if the r -th result is a true neighbor and 0 otherwise. In the experiments we set |R| =50, and mAP is computed by averaging the AP values over all the queries. We repeated our experiments five times and we report mean mAP values and standard deviations over the runs. In the proposed C luster-based J oint M atrix F actorization H ashing method ( C-JMFH ), we varied the parameter in [10  X  4 10  X  1 ], concluding in 10  X  2 and 10  X  3 for Wiki and NUS-WIDE , respectively. To evaluate the impact of the cross-modal cluster matrix when computing the consensus matrix B  X  in Eq. (5), and consequently the binary codes in B , we use as baseline a variant of our method, namely JMFH , which does not consider the cross-modal cluster matrix in the joint factorization process. We use IMH 4 [12] as baseline, and we compare the proposed C-JMFH method http ://www.svcl.ucsd.edu/projects/crossmodal/ http://lms.comp.nus.edu.sg/research/NUS-WIDE.htm http://staff.itee.uq.edu.au/shenht/publications.htm with CMF H 5 [2], a hashing method that also follows a joint matrix factorization strategy when generating the binary codes. The parameter values in the baselines were deter-mined by cross-validation, using the publicly available im-plementations and we report the best results.

Figures 1 and 2 show the results for the cross-modal re-trieval tasks in Wiki and NUS-WIDE , respectively. The baseline IMI method has poor performance, compared with the rest of cross-modal hashing methods, by not following a joint matrix factorization strategy and thus not computing the inter-modality and intra-modality similarities as well as CMFH, JMFH and C-JMFH do. As also observed in rel-evant studies [2, 6], the retrieval accuracy does not neces-sarily improve when increasing the number of bits in many baseline methods such as IMI, as it does not follow a joint matrix factorization strategy. Meanwhile, we observe that CMFH and JMFH achieve comparable performance, as both hashing methods jointly factorize the representations in the different modalities, with the retrieval accuracy increasing when a larger number of bits is selected. This happens be-cause in these methods the hamming space corresponds to the latent space of the joint matrix factorization process, consequently encoding more information for a large number of bits. The proposed C-JMFH method boosts the mAP retrieval accuracy by incorporating the cross-modal cluster representations of the documents when learning the hash codes. Compared to the second best method, the proposed C-JMFH method achieves a relative improvement of 3.1-12.9% in all the cross-modal retrieval tasks, with the excep-tional case of 1.9% relative improvement in the case of a low selection of number of bits, that is, the case of 16 bits in NUS-WIDE for the text  X  image cross-modal retrieval task. To verify the superiority of the proposed C-JMFH method, we used the paired t -test and we found that the dif-ferences between the reported results for C-JMFH against the competitive hashing methods were statistically signifi-cant for p&lt; 0.05. Figure 1: Performance evaluation on Wiki for the cross-modal retrieval tasks.
In this study, we presented a cross-modal hashing method using a cluster-based representation into a joint factorization process. In our experiments, we showed that the proposed method outperforms other state-of-the-art hashing methods in terms of cross-modal retrieval accuracy. Although we focused on the case of cross-modal retrieval, that is, given a query from one modality to retrieve results from another htt p://ise.thss.tsinghua.edu.cn/MIG/publications.jsp
Figure 2: Performance evaluation on NUS-WIDE . modality, an interesting future direction is to extend our method to multimodal retrieval, where given a query from one or more modalities to retrieve documents from all the different modalities [9]. [1] M. M. Bronstein, A. M. Bronstein, F. Michel, and [2] G. Ding, Y. Guo, and J. Zhou. Collective matrix [3] J. Gao, J. Han, J. Liu, and C. Wang. Multi-view [4] A. Gionis, P. Indyk, and R. Motwani. Similarity [5] Y. Gong and S. Lazebnik. Iterative quantization: A [6] S. Kumar and R. Udupa. Learning hash functions for [7] F. Lin and W. W. Cohen. Power iteration clustering. [8] R. Lin, D. A. Ross, and J. Yagnik. SPEC hashing: [9] X. Liu, Y. Mu, B. Lang, and S. Chang. Mixed [10] D. G. Lowe. Distinctive image features from [11] S. Moran and V. Lavrenko. Regularised cross-modal [12] J. Song, Y. Yang, Y. Yang, Z. Huang, and H. T. Shen. [13] Y. Wang, X. Lin, L. Wu, W. Zhang, and Q. Zhang. [14] Y. Zhen and D. Yeung. Co-regularized hashing for
