 It is widely agreed that no units of the text can be understood in isolation, but in relation to their context . Research es in discourse parsing aim to acquire such relations in text, which is fund a-mental to many natura l language processing a p-plications such as question answering, automatic summarization and so on.

One important issue behind discourse parsing is the representation of discourse structure . Rh e-torical Structure Theory (RST) (Mann and Thompson, 1988) , one of the most influential discourse theories, posits a hierarchical gener a-tive tree representation , as illustrated in Figure 1 . The leaves of a tree correspond to contiguous text spans called Elementary Discourse Units (EDUs) 1 . The adjacent EDUs are combined i nto the larger text spans by rhetorical relation s (e.g., C ontrast and E laboration ) and the larger text spans continue to be combined until the whole text constitutes a parse tree . T he text spans linked by rhetorical relation s are annotated as either nucleu s or satellite depending on how sal i-ent they are for interpretation . It is attractive and challenging to parse the whole text into one tree . analogous to a constituency based syntactic tree except that the const ituents in the discourse trees are text spans , previous researches have ex plored different constituency based syntactic parsing techniques (eg. CKY and chart parsing ) and va r-ious features ( eg. length, position et al. ) for di s-course parsing ( Soricut and Mar cu , 2003; Joty et al. , 2012 ; Reitter, 2003; LeThanh et al., 2004; Baldridge and Lascarides , 2005; Sub ba and Di Eugenio , 2009; Sagae , 2009; Hernault et al. , 2010 b ; Feng and Hirst , 2012 ) . However, the e x-isting approaches suffer from at least one of the follo wing three problems . First, i t is difficult to design a set of production rules as in syntactic parsing , since there are no determinate gener a-tive rules for the interior text spans . Second, the different levels of discourse units (e.g. EDU s or larger text span s ) occu r r ing in the generative process are better represent ed with different fe a-tures, and th us a uniform framework for di s-course analysis is hard to develop . Third, to reduce the time complexity of t h e state -of -the -art constituency based parsing techn iques , the a p-proximate parsing ap proaches are prone to trap in local maximum .

In this paper, we propose to adopt the depen d-ency structure in discourse representation to overcome the limitations mentioned above. Here is t he basic idea : the discourse structu re c onsists of EDUs which are linked by the binary, asy m-metrical relations called dependency relations . A dependency relation holds between a subordinate EDU called the dependent , and another EDU on which it depends called the head , a s illustrated in Figur e 2 . E ach EDU ha s one head . So , the d e-pendency structure can be seen as a set of head -dependent link s , which are labeled by functional relations. Now , we can analyze the relations b e-tween EDUs directly , without worrying about any interior text span s . Since d ependency trees contain much fewer nodes and on average they are simpler than constituency based trees , the c urrent dependency parsers can have a relative ly low comput ation al complexity. Moreover, c o n-cerning linearization, it is well known that d e-pendenc y structure s can deal with non -p rojective relations , while constituency -based models need the addition of complex mechanisms like tran s-formations, movements and so on. In our work, we adopt the graph based dependency parsing techniques learned from large s ets of annotated dependency trees . The Eisner (1996) algorithm and maximum spanning tree (MST) algorithm are used respectively to parse the optim al proje c-tive and non -projective dependency tree s with the large -margin learning technique (Crammer and Singer , 2003 ) . To the best of our knowledge, we are the first to apply the dependency structure and introduce the dependency parsing techniques i n to discourse analysis.
 The rest of this paper is organized as follows. Section 2 formally defines discourse dependen cy structure and introduces how to build a discourse dependency treebank from the existing RST co r-pus . Section 3 presents the discourse parsing a p-proach based on the Eisner and MST algorithm s . Section 4 elaborates on the large -margin learning technique as well as the features we use . Section 5 discusse s the experimental results . Section 6 in troduces the related work and Section 7 co n-cludes the paper . ,e 2 and e 3 denote three EDUS, and the directed 2.1 Discourse Dependency Structure Si milar to the syntactic dependency structure defined by McDon a ld (200 5a, 2005b ), we insert an artificial EDU e 0 in the beginning for each document and label the dependenc y relation lin k-ing from e 0 a s ROOT . This treatment will si m-plify both formal definitions and computational implementations. Normally, we assume that each EDU should have one and only one head except for e 0 . A la beled directed arc is used to represent the d ependency relation from one head to its d e-pendent. Then, discourse dependency structure can be forma lized as the labeled directed graph, where nodes correspond to EDUs and labeled arcs correspond to labeled dependency relations.
W e assume that the text 2 T is composed of n + 1 EDUs including the artificial e 0 . That is T = e 0 e 1 e 2 ... e n . Let R ={ r 1 , r 2 , ... , r finite set of functional relations that hold b e-tween two EDUs. Then a discourse dependency graph can be denoted by G =&lt; V , A &gt; where V d e-notes a set of nodes and A denotes a set of l a-beled directed arcs , such that for the text T = e 0 e ... e n and the label set R the following holds: (1) V = { e 0 , e 1 , e 2 , ... e n } (3) I f &lt; e i , r , e j &gt;  X  A then &lt; e k , r  X  , e (4) I f &lt; e i , r , e j &gt;  X  A then &lt; e i , r  X  , e
The third condition assures that each EDU ha s one and only one head and the fourth tells that only one kind of dependency relation holds b e-tween two EDUs. According to the definition, we illustrate all the 9 possible unlabeled depen d-ency trees for a text containing three EDUs in Figure 2. The dependency trees 1  X  to 7  X  are pr o-jective while 8  X  and 9  X  are non -projective with crossing arcs . 2.2 Ou r Discourse Dependency Tree b ank To automatically conduct discourse dependency parsing, construct ing a discourse dependency treebank is fundamental . I t is costly to manually construct such a treebank from scratch. For t u-nately, RST D iscourse Treebank ( RST -D T ) ( Carlson et al., 2001 ) is an available resource to help with .

A RST tree constitute s a hierarchic al structure for one document through rhetorical relations. A total of 110 fine -grained relations (e.g. Elabor a-tion -part -whole and List ) were used for tagg ing RST -DT . They can be categorized into 18 classes (e.g. Elaboration and Joint ) . All these relations can be hypotactic ( X  X ononuclear X ) or paratactic ( X  X ulti -nuclear X ). A h ypotactic relation hold s between a nucleus span and an adjacent satellite span , whil e a paratactic relation connect s two or more equally important adjacent nucleus spans . For convenience of computation , we convert the n -ary ( n &gt;2) RST trees 3 to binary tree s through adding a new node for the latter n -1 nodes and as sume each relation is conn ected to only one nucl eus 4 . This departure from the original theory is not such a major step as it may appear, since any nucleus is known to contribute to the esse n-tial meaning. Now , each RST tree can be seen as a headed constituency based binary tree wher e the nucle i are heads and the children of each node are linearly ordered . Given three EDUs 5 , Figure 1 shows the possible 8 headed constitue n-cy based trees where t he superscrip t * denote s the heads (nuclei) . We use dependency tree s to simu late the headed c onstituency based trees.
Contrasting Figure 1 with Figure 2, we use dependency tree 1  X  to simulate binary trees 1 and 8 , and dependency tress 2  X  -7  X  to simulate binary trees 2 -7 corresponding ly . The rhetorical rel a-tions in RST trees are kept as the functi onal rel a-tions which link the two EDU s in dependency trees. With this kind of conversion , we can get our discourse dependency treebank . It is worth noting that the non -projective tree s like 8  X  and 9  X  do not exist in our dependency t reebank , though they are eligible according to the de finition of discourse dependency graph . 3.1 System Overview As stated above, T = e 0 e 1 ... e n represents an input text (document) where e i denotes the i th EDU of T . We use V to denote all the EDU nodes and V  X  R  X  V discourse dependency arcs . The goal of discourse dependency parsing is to parse an optimal spa n-ning tree from V  X  R  X  V -0 . Here we follow the arc factored method and define the score of a d e-pendency tree as the sum of the score s of all the arcs in the tree. Thus, the optimal dependency tree for T is a spanning tree with the highest score and obtained through the function DT ( T , w ) :
DT T argmax w here G T means a possible spanning tree with score T G and  X  ( ) denote s the score of its feature representation f ( e i , r , e j ) and a weight vector w .

N ext , two basic problems need to be solved : how to find the de pendency tree with the highest score for T given all the arc scores ( i.e. a parsing problem) , and how to learn and compute the score s of arcs according to a set of arc features ( i.e. a learning problem) .

The following of this section address es the first problem . Given the text T , we first reduc e the multi -digraph composed of all possible arcs to the digraph . The digraph keep s only one arc &lt; e i , r , e j &gt; between two nodes which satisfies  X  ( ) . Th us , we can proceed with a reduction from labeled parsing to unlabeled parsing. Next, t wo algorithms, i.e. the Eisner algorithm and MST algorithm, are pr e-sented to parse the projective and non -proj ective unlabeled dependency trees respectively. 3.2 Eisner Algo ri thm It is well known that projective dependency par s-ing can be handled with the Eisner algorithm (1996) which is based on the bottom -up dynamic programming techniques with the time complex i-ty of O( n 3 ) . The basic idea of the Eisner alg o-rithm is to parse the left and right dependents of a n EDU independently and combine them at a later stage. This reduces the overhead of inde x-ing head s . Only two bina ry variables , i.e. c and d , are required to specify whether the head s occur leftmost or rightmost and whether an item is complete.
 algorithm . A dynamic programming table E[ i , j , d , c ] is used to represent the highest scored sub tree spanning e i to e j . d indicates whether e i the head ( d =1) or e j is head ( d =0) . c indicates whether the sub tree will not tak e any more d e-pendents ( c =1) or it needs to be completed ( c =0 ). The algorithm begins by initializing all length -one sub tree s to a score of 0.0. In the in ner loop , the first two step s (Line s 7 and 8) are to construct the new dependency arcs by taking the max imum over all the internal indices ( i  X  q  X  j ) in the span, and calculating the value of m erging the two su b-tree s and adding one new arc . T he last two steps (Line s 10 and 11) attempt to achieve an optimal left/right subtree in the span by add ing the corr e-sponding left/right sub tree to the arcs that have been added previously . This algorithm con siders all the possible sub trees . We can then get the optimal dependency tree with the score E [0 , n , 1 ,1 ] . 3.3 Maximum S panning T ree A lgorithm As the bottom -up Eis ner Algorithm must mai n-tain the nested structural constraint , it cannot parse the non -projective dependency trees like 8  X  and 9  X  in Figure 2. However, the non -projective dependency does exist in real discourse. For e x-ample, the earlier text main ly talks about the to p-ic A with mentioning the topic B , while the latter text gives a supplementary explanat ion for the topic B . This example can constitute a non -projective tree and its pictorial diagram is exhi b-ited in Figure 4 . Following the work of McDo n-ald (200 5 b) , we formalize discourse dependency parsing as search ing for a maximum spanning tree (MST) in a directed graph. Figure 4 : Pictorial D iagram of N on -projective T rees
Chu and Liu (1965) and Edmonds (1967) i n-dependently proposed the virtually identical a l-gorithm named the Chu -Liu/Edmonds algorithm, for finding MST s on direct ed graph s (McDonald et al. 2005b) . Figure 5 shows the details of the Chu -Liu/Edmonds algorithm for discourse par s-ing. Each node in the graph greedily selects the incoming arc with the highest score. If one tree results, the algorithm ends. Otherwise, there must exist a cycle. The algorithm contracts the identified cycle into a single node and recalc u-lates the scores of the arcs which go in and out of the cycle. Next, the algorithm recursively call itself on the contracted graph. Finally, those arcs which go in or out of one cycle will recover themselves to connect with the original nodes in V . Like McDonald et al. (2005b), we adopt an efficient implementation of the Chu -Liu/Edmonds algorithm that is proposed by Ta r-jan (1997) with O( n 2 ) time complexity.
Figure 5: Chu -Liu/Edmonds MST Algorithm In Section 3, we assume that the arc scores are available . In fact, the score of each arc is calc u-lated as a linear combination of feature weight s . Thus , we need to determine the features for arc represent ation first . W ith r eferring to McDonald et al. (2005 a ; 2005b ), we use the Margin Infused Relaxed Algorithm (MIRA) to learn the feature weight s based on a training set of documents annotated with dependency structures  X   X   X   X  where y i denotes the correct dependency tree for the text T i . 4.1 Features Following (Feng and Hirst , 2012 ; Lin et al., 2009 ; H ernault et al., 201 0b ), we explore the following 6 feature types combined with relations to repr e-sent each labeled arc &lt; e i , r , e j &gt; . (1) WORD : The first one word, the last one word, and the first bigrams in each EDU, the pair of the two first words and the pair of the two last words in the two EDUs are extracted as features. (2) POS: The first one and two POS tags in each EDU, and the pair of the two first POS tags in the two EDUs are extracted as features. (3) Position : These features concern w hether the two EDUs are includ ed in the same sentence , and t he position s where the two EDUs are located in one sentence, one paragraph, or one document. (4) Length : The length of each EDU. (5) Syntactic: POS tags of the dominating nodes as defined in Soricut and Marcu (2003) are e x-tracted as features . W e use the syntactic tree s from t he Penn Treebank t o find the dominating nodes, . (6) Semantic similarity : W e compute the s e-mantic relatedness between the two EDUs based on Word Net. T he word p air s are extracted from ( e , e j ) and their similarity is calculated . Then, we can get a weighted complete bipartite graph where words are deemed as nodes and s imilarity as weights. From this bipartite graph, we get the maximum weighted matching and use the ave r-aged weight of the match es as the similarity b e-tween e i and e j . In particular, we use path_similarity, wup_similarity, res_similarity, jcn_similarity and lin_similarity provided by the nltk.wordnet.similarity (Bird et. al., 2009) pac k-age for calculat ing word similarity.
 r e lation label s from RST -DT . O ne is composed of 19 coarse -grained relations and the other 11 1 f ine -grained relations 6 . 4.2 MIRA based Learning Margin Infused Relaxed Algorithm ( MIRA ) is an online algorithm for multiclass classification and is extended by Taskar et al. (2003) to cope with structur ed classification. 
Figure 6 gives the pseudo -code of the MIRA algorithm ( McDonld et al. , 2005 b ). This alg o-rithm is designed to update the parameters w u s-ing a single training instance  X   X  , iteration. On each update, MIRA attempts to keep the norm of the change to the weight ve ctor as small as possible, which is subject to co n-structing the correct dependency tree under co n-sideration with a margin at least as large as the loss of the incorrect dependency trees. We define the loss of a discourse dependency tree ' noted by ( , ') that have incorrect heads. Since there are exp o-nentially many possible incorrect dependen cy tree s and thus exponentially m any margin co n-straints, here we relax the optimization and stay with a single best dependency tree vector w j . In this algorithm, t he successive u p-dat ed values of w are accumulated and averaged to avoid overfitting. 5.1 Prepa ration We tes t our methods experimentally using the discourse dependency treebank which is built as in Section 2. The training part of t he corpus is composed of 342 document s and contains 18 , 765 EDUs , while the test part consists of 38 doc u-ments and 2 , 346 EDUs . The number of EDUs in each document ranges between 2 and 304 . Two sets of relations are adopted. O ne is composed of 19 relations a nd Table 1 shows the number of each relation in the training and test corpus. The other is composed of 111 relations . Due to space limitation, Table 2 only list s the 10 highest -distributed relations with regard to their freque n-cy in the training corpus.

T he following experiments are conducted : (1) to measure the parsing performance with diffe r-ent rela tion sets and different feature types ; (2) to compare our parsing methods with the state -of -the -art discourse parsing methods. Table 2: 10 Highest Distributed Fine -grained Relations 5.2 Feature Influence on Two Relation Sets So far, research es on discourse parsing avoid adopting too fine -grained relations and the rel a-tion set s containing around 20 labels are widely used . In our experiments, we observe that adop t-ing a fine -grained relation set can even be help ful to build ing the discourse tree s . Here, we conduct experiments on two relation sets that contain 19 and 111 label s respectively. At the same time, different feature types are tested their effect s on discourse parsing.
 Table 3: Performance U sing C oarse -grained R e-lations.
 Table 4: Performance Using F ine -grained R el a-tions.
 Based on the MIRA leaning algorithm, the Eisner algorithm and MST algori thm are used to parse the test document s resp ectively . Referring to the evaluation of syntactic dependency parsing , we use u nlabeled a ccuracy to c alculate the ratio of EDUs that correctly identif y their heads , l a-beled a ccuracy the ratio of EDU s that have both correct heads and correct relations . Tabl e 3 and T able 4 show the performance on two relation sets. The numbers (1 -6) repres ent the cor r e-sponding feature types de scribed in Section 4.1 .
From Table 3 and Table 4 , we can see that the addition of more feature type s , except the 6 th fe a-ture type ( s em antic similarity), can promote the performance of relation labeling , whether using the coarse -grained 19 relations and the fine -grained 111 relations . As expected, t he first and second types of feature s (WORD and POS) are the ones which play an important r ole in building and labeling the discourse dependency tree s . These two types of features attain similar pe r-formance on two relation sets. The Eisner alg o-rithm can achieve u nlabeled accuracy a round 0.36 and labeled accuracy around 0.26 , while MST algorithm achieves unlabeled accuracy around 0.20 and labeled accuracy around 0.1 4 .
T he third feature type (Position) is also very helpful to discourse parsing . With the ad dition of this feature type , both unlabeled accuracy and labeled accuracy exhibit a marked inc rease . E s-pecially , when applying MST algorithm on di s-course parsing , unlabeled accuracy rises from around 0.20 to around 0.73 . This result is co n-sistent with Hernault  X  s work (2010b) whose e x-periments have exhibited the usefulness of those position -related features . The o ther two types of features which are related to l ength and s yntactic parsing , only promote t he perfor mance slightly .
As we employed the MIRA learning algorithm, it is possible to identify which specific features are useful, by looking at the weight s l earned to each feature using the training data . Table 5 s e-lects 10 features with the highest weights in a b-solute va l ue for the parser which uses the coarse -grained relations, while Table 6 selects the top 10 fe a tures for the parser using the fine -grained r el a tions. Each row denotes one feature: the left part before the symbol  X  &amp;  X  is from one of the 6 fe a ture types and the right part denotes a specific relation. From Table 5 and Table 6, we can see that some features are reasonable. For example, The six th fe ature in Table 5 represents that the dependency relation is preferred to be labeled Explanation with t he fact that  X  because  X  is the first word of the dependent EDU . From these two tables, we also o bserve that most of the heavily weighted features are usual ly related to th ose highly distributed relations. When using the coarse -grained relations, the popular relations (eg. Elaboration , Attribution and Joint ) are a l-ways preferred to be labeled. When using the fine -grained relations, the large relations inclu d-i ng List and Elaboration -object -attribute -e are given the precedence of labeling. This phenom e-non is mainly caused by the sparseness of the training corpus and the imbalance of relations. To solve this problem, the augment of training corpus is necessary.
 Table 5 : Top 10 Feature Weights for Coarse -grained Relation Labeling (Eisner Algorithm) Table 6: Top 10 Feature Weights for Coarse -grained Relation Labeling (Eisner Algorithm)
U nlike previous discourse parsing approaches, our methods combine tree building and relation labeling into a uniform framework naturally. This means that relations play a role in building the dependency tree s tructure . From Table 3 and Table 4, we can see t hat fine -grained relations are more helpful to building unlabeled discourse tree s more than the coarse -grained relations . The best result of unlabeled accuracy using 111 rel a-tions is 0.7506, better than the best performance (0.7447) using 19 relations. We can also see that the labeled accuracy us i ng the fine -grained rel a-tions can achieve 0.4309, only 0.06 lower than the best labeled accuracy (0.4915) using the coarse -grained relations.

In addition, c omparing the MST algorithm with the Eisner algorithm, Table 3 and Table 4 show that th e ir performance s are not significan t-ly differen t from each other . But we think that MST algorithm has more potential in discourse dependency parsing , because our converted di s-course dependency treebank contain s only pr o-jective t rees and somewhat suppress es the MST algorithm to exhibit its advantage of parsing non -projective trees. In fact, we observe that some non -project i ve dependencies produced by the MST algorithm are even reasonable than what they a re in the dependency treebank . Thus , it is important to build a manually labeled discourse dependency t reebank , which will be our future work . 5.3 Comparison with Other Systems The state -of -the -art discourse parsing methods normally produce the constituency b ase d di s-course trees. To comprehensively evaluate the performance of a labeled constituency tree , the blank tree structure (  X  S  X  ), the tree structure with nuclearity indication (  X  N  X  ) , and the tree structure with rhetorical relation indication but no nuclea r-ity indication (  X  R  X  ) are evaluated respectively using the F measure (Marcu 2000) . we adopt MIRA and Eisner algorithm to conduct discourse parsing with all the 6 types of features and then convert the produced projective d e-pendency trees to constituency based trees through their correspondence as stated in Section 2 . Our parsers using two relation sets are named Our -coarse and Our -fine respectively. T he i n-putted EDUs of our parsers are from the standard segmenta tion of RST -DT. Other text -level di s-course pars ing methods include : (1 ) Percep -coarse : w e replace MIRA with the averaged pe r-ceptron learning algorithm and the other settings are the same with Our -c oarse ; (2 ) H ILDA -manual and HILDA -seg are from Hernault (20 10 b )  X  s work , and their inputted EDU s are from RST -DT and their own EDU segmenter respectively ; ( 3 ) Le Thanh indicates the results giv en by Le Thanh el al. (2004) , which built a multi -level rule based parser and used 14 rel a-tions evaluated on 21 docu ments fro m RST -DT; (4 ) Marcu denote s the results given by Ma r-cu (2000)  X  s decision -tree based pa rser which used 15 relations evaluated on unspecified documents . for all the parsers mentioned above. Human d e-notes the manual ag reement between two human annotators. From this table, we can see that both our parsers perform better than all the other parsers as a whole, t hough our parsers are not developed directly for constituency based trees . Our parsers do not exhibit obvious adv antage than HILDA -manual on labeling the blank tree structure, because our parsers and HILDA -manual all perform over 94 % of Human and this performance level somewhat reaches a bottl e-neck to promote more. However, our parsers outperform the other parsers on both nuclearity and relation labeling . Our -coarse achieves 94 . 2 % and 91. 8% of the human F -score s, on labeling nuclearity and relation respectively, while Our -fine achieves 95.2% and 87.6%. We can also see that the averaged perceptron learning algorithm, t hough simple, can achieve a comparable pe r-formance, better than HILDA -manual . The parsers HILDA -seg , LeThanh and Marcu use their own automatic EDU segmenters and exhibit a relatively low performance. This means that EDU segmentation is important to a pract ical discourse pars er and worth further investigation . tion labeling, we follow Hernault el al. (2010 a ) and use Macro -averaged F -score (MAFS) to evaluate each relation . Due to space limitation, we do not list the F scores for each relation. Macro -averaged F -score is not influenced by the number of instances that are contained in each relation. Weight -averaged F -score (WAFS) weights the performance of each relation by the number of its existing instances. Table 8 co m-pares our parser Our -coarse with other parsers HILDA -manual , Feng ( Fen g and Hirst, 2012 ) and Baseline . Feng ( Feng and Hirst, 2012 ) can be seen as a strengthened version of HILDA which adopts more features and conducts feature selection. Baseline always picks the most fr e-quent relation (i.e. Elaboration) . From the results, we find that Our -coarse consistently provides superior performance for most relations over other parsers, and therefore results in higher MAFS and WAFS . So far, the existing discourse parsing techniques are mainly based on two well -known treeba nk s . O ne is the Penn Discourse TreeBank (PDTB) (Prasad et al. , 2007) and the other is RST -DT . sen tation by taking an implicit/explicit conne c-tive as a predication of two adjacent sentences (arguments). Then the dis course relation between each pair of sentences is annotated independently to characterize its predication. A majority of r e-searches regard discourse parsing as a classific a-tion task and mainly focus on exploiting various linguistic features and classifiers when using PDTB (Wellner et al. , 2006; Pit ler et al. , 2009; Wang et al. , 2010). However, the predicate -arguments annotation scheme itself has such a limitation that one can only obtain the local di s-course relations without knowing the rich context.
In con trast, RST and its t reebank enable pe o-ple to derive a complete represen tation of the whole discourse. R esearches have begun to i n-vestigate how to construct a RST tree for the given text. Since the RST tree is similar to the constituency based syntactic tre e except that the constituent nodes are different, the syntactic parsing techniques have been borrowed for di s-course parsing ( Soricut and Marcu , 2003; Baldridge and Lascarides , 2005; Sagae , 2009; Hernault e t al. , 2010 b; Feng and Hirst, 2012 ) . Soricut and M arcu (2003) use a standard bottom -up chart parsing algorithm to determine the di s-course structure of sentences. Baldridge and La s-carides (2005) model the process of discourse parsing with the proba bi listic head driven parsing techniques. Sa ga e (2009) appl y a transition based constituent parsing approach to con struct a RST tree for a document. Hernault et al. (2010 b ) d e-velop a greedy bottom -up tree building strategy for discourse parsing . The two adjacent text spans with the closest relations are combined in each iteration . As the extension of Hernault  X  s work, Feng and Hirst (2012) further explore va r-ious features aiming to achieve better perfo r-mance. However, a s analyzed in Section 1, there exist three limitations with the constituency based discourse repres entation and parsing. W e innovatively adopt the dependency structure , which can be benefited from the existing RST -DT, to represent the discourse . To the best of our knowledge, this work is the first to apply d e-pendency structure and dependency parsing tec hniques in discourse analysis . In this paper, we present the benefits and feas i-bility of applying dependency structure in text -level discourse parsing . Through t he correspon d-ence between constituency -based trees and d e-pendency trees , we build a discourse dependency t reebank by convert ing t he existing RST -DT . Based on dependency structure , we are able to directly analyze the relations between the EDUs without worrying a bout the additional interior text spans , and apply the existing state -of -the -a rt depend ency parsing techniques which have a relatively low time complexity. In our work, we use the graph based dependency parsing tec h-niques learned from the annotated dependency trees. T he Eisner algorithm and the MST alg o-rithm are applied to parse the optimal projective and non -projective dependency trees respectively based on the arc -factored model. To calculate the score for each arc, six types of features are e x-plored to represent the arc s and the feature weights are learned based on the MIRA learni ng technique. Experimental results exhibit the effe c-tiveness of the proposed approaches . In the f u-ture , we will focus on non -projecti ve discourse dependency parsing and explore more effective features.
 This work was partially supported by N ational High Technology Research and Development Program of China (No. 2012AA011101), N a-tional Key Basic Research Program of China (No. 2014CB340504 ) , National Natural Science Foundation of China (No. 61273278 ), and N a-tional Key Technology R&amp;D Program (No: 2011BAH10B04 -03) . We also thank the three anonymous reviewers for their helpful comments .
