
In most practical problems involving classification, the aim of building models is to later use them to select a subset of cases to which some action is to be applied. A typical example is training a classifier, after a pilot campaign, to predict which customers are most likely to buy after a marketing action. The offer is then targeted to the customers which, according to the model X  X  predictions, are the most likely buyers. Unfortunately, this is not what the marketer wants. They want to target people who will buy because they received the offer.

These two aims are clearly not equivalent, certain cus-tomers may buy the product even if they have not been targeted by a campaign. Targeting them at best incurs additional cost. At worst, excessive marketing may annoy them and prevent any future purchases. It is in fact well known in the advertising community that campaigns do put off some percentage of customers, there are however no easy means of identifying them. See [1], [2] for more information.
Similar problems arise very frequently in medicine. In a typical clinical trial, a random subgroup of patients is assigned treatment A and the other, treatment B or placebo. A statistical test is then performed to assess the overall difference between the two groups. If, however, treatment A only works for a subgroup of people (e.g. people with some genetic traits) and not for others, such a fact might go undetected. In some cases the analysis is carried out separately in several subgroups, but there is no systematic methodology for automatic detection of such subgroups or modeling differences in response directly.

Despite its ubiquity and importance, the problem has received scarce attention in literature [1] X  X 3], where it is known as uplift modeling, differential response analysis, incremental value modeling or true lift modeling. Typically a random sample of the population is selected and subjected to the action being analyzed (a medical treatment or marketing campaign). This sample is called the treatment dataset. Another, disjoint, random sample is also selected, to which the action is not applied. This is the control dataset, serving as the background against which the results of the action will be evaluated. The task now is to build a model predicting not the probability of objects belonging to a given class, but the difference between such probabilities on the two sets of data: treatment and control.

If the treatment group selection is completely random, this type of modeling has another advantage: it allows for modeling the effect caused by the action. Objects are often subject to other actions (such as competition X  X  marketing campaigns) the influence of which cannot be taken into account directly. By selecting random treatment and control groups, we automatically factor out all such effects, as they apply to those groups equally. A more thorough motivation for uplift modeling can be found in [2].

While decision trees are no longer an active research area, they are still widely used in the industry (included in practically all commercial analytical products), and, as a historically first Machine Learning approach are a natural first candidate to adapt to the uplift methodology. Adapting other models will be a topic of future research.

We now describe the contribution of our paper. While ap-proaches to uplift decision tree learning are already present in literature [1], [2], [4], they are quite basic and use simple splitting criteria which maximize class differences directly. Also, no specific pruning methodology is described. The uplift decision trees we propose are more in the style of modern algorithms [5] X  X 7], which use splitting criteria based on information theory. Unlike [1], which only allows two class problems and binary splits, our algorithm can handle arbitrary number of classes and multiway splits.

Moreover, all steps of the proposed methods are carefully designed such that they are direct generalizations of standard decision trees used in classification, by which we specifically mean the CART [7] and C4.5 [6] approaches. That is, when the control group is empty, they behave identically to decision trees known in the literature. The advantages of this approach are twofold: first, when no control data is present (this can frequently happen at lower levels of the tree), it is natural to just try to predict the class, even though we are no longer able to perform uplift modeling; second, the fact that, as a special case, the methods reduce to well known, well justified and well researched approaches, corroborates the intuitions behind them and the design principles used. A. Notation
Let us now introduce the notation used throughout the paper. The situation considered is different from standard Machine Learning setting in that we now have two datasets (samples): treatment and control. This presence of double datasets necessitates a special notation.

Recall that nonleaf nodes of decision trees are labeled with tests [6]. A test may have a finite number of outcomes. We create a single test for each categorical attribute, the outcomes of this test are all attribute X  X  values, as is done for example in C4.5. For each numerical attribute X we create several tests of the form X&lt;v , where v is a real number. A test is created for each v being a midpoint between two consecutive different values of the attribute X present in data (treatment and control datasets are concatenated for this purpose). We omit further details as they can be found in any book on decision trees [6], [7].

Tests will be denoted with uppercase letter A . The dis-tinguished class attribute will be denoted with the letter Y . The class attribute is assumed to have a finite domain and all tests are assumed to have finite numbers of outcomes, so all probability distributions involved are discrete. Values from the domains of attributes and test outcomes will be denoted by corresponding lowercase letters, e.g. a will denote an outcome of a test A , and y one of the classes. Similarly, a is the sum over all outcomes of a test A , and y is the sum over all classes.

The probabilities estimated based on the treatment dataset will be denoted by P T and on the control dataset by P
C . P T ( Y ) will denote the probability distribution of the attribute Y estimated on the treatment sample, and P
T ( y ) the corresponding estimate of the probability of the event Y = y ; notation for tests and the control sample is analogous. Conditional probabilities are denoted in the usual manner, for example P C ( Y | a ) is the class probability distribution conditional on the test outcome A = a estimated from the control sample.

We will always use Laplace correction while estimating the probabilities P T and P C .

Additionally, let N T and N C denote the number of records in the treatment and control samples respectively, and N T ( a ) and N C ( a ) , the number of records in which the outcome of a test A is a . Finally let N = N T + N C and N ( a )= N T ( a )+ N C ( a ) .

Despite its practical importance the problem of uplift modeling has received surprisingly little attention in liter-ature. Most available publications are business whitepapers offering only vague descriptions of algorithms used [8], [9]. The details are typically omitted, probably not to disclose the inner workings of commercial products. Below we discuss the handful of available research papers.

There are two overall approaches to uplift modeling. The obvious approach is to build two separate classifiers, one on the treatment and the other on the control dataset. For each classified object we then subtract the class probabil-ities predicted by the control group classifier from those predicted by the treatment group model. This approach suffers from a major drawback: the pattern of differences between probabilities can be quite different then the pattern of the probabilities themselves, so predicting treatment and control probabilities separately can result in poor model performance [1], [4], [10]. In case of decision trees, it does not necessarily favor splits which lead to different responses in treatment and control groups, just splits which lead to predictable outcomes in each of the groups separately.
This brings us to the second class of methods, which attempt to directly model the difference between treatment and control probabilities.

The first paper explicitly discussing uplift modeling was [2]. It presents a thorough motivation including several use cases. A modified decision tree learning algorithm is also proposed, albeit with very few details given. It is only stated that the tree building algorithm favors splits for which on one side of the split the outcome rates in the treated group are higher than in the control group by more than in the whole population, and on the other this difference is respectively smaller. The actual splitting criterion used is probably similar to  X  X  P discussed below.

Hansotia and Rukstales [1] offer a detailed description of their uplift approach. They describe two ideas, one based on logistic regression, the other on decision trees. The decision tree part of [1] again describes two approaches. The first is based on building two separate trees for treatment and control groups with cross-validation used to improve the ac-curacy of probability estimates. The second approach, most relevant to this work, builds a single tree which explicitly models the difference between responses in treatment and control groups.

The algorithm uses a splitting criterion called  X  X  P , which selects tests maximizing the difference between the differences between treatment and control probabilities in the left and right subtrees. Suppose we have a test A with outcomes a 0 and a 1 . The splitting criterion used in [1] is defined as
 X  X  P ( A )= P T ( y 0 | a 0 )  X  P C ( y 0 | a 0 ) where y 0 is a selected class. The criterion is based on maximizing the desired difference directly, while our ap-proach follows the more modern criteria based on informa-tion theory. Our experiments demonstrate that this results in significant performance improvements. Moreover,  X  X  P works only for binary trees and two-class problems while our approach works for multiway splits and with an arbitrary number of classes (in Section IV we generalize the  X  X  P measure to multiway splits).

In [4], the authors propose a decision tree building method for uplift modeling. The tree is modified such that every path ends with a split on whether a given person has been treated (mailed an offer) or not. Otherwise the algorithm is a standard decision tree construction procedure from [11], so all remaining splits are selected such that the class is well predicted, while our approach selects splits which lead to large differences between treatment and control distributions. In [12] logistic regression has been applied, along with a simple approach based on building two separate Naive Bayes classifiers.

The problem has been more popular in medical literature where the use of treatment and control groups is common. Several approaches have been proposed for modeling the difference between treatment and control responses based on regression analysis. One example are nested mean mod-els [13] X  X 15] similar to regression models proposed in [12]. An overview with a list of related literature can be found in [16]. The purpose of those methods is different from the problem discussed here, as the main goal of those approaches is to demonstrate that the treatment works after taking into account confounding factors, while our goal is to find subgroups in which the treatment works best. Also, only linear models are used, and typically the problem of regression not classification is addressed.

In [17] the authors set themselves an ambitions goal of modeling long term influence of various advertising channels on the customer. Our approach can be seen as a small part of such a process which only deals with a single campaign. Otherwise the approach is completely different from ours.
Action rules discovery [18], [19] is concerned with finding actions which should be taken to achieve a specific goal. This is different from our approach as we are trying to identify groups on which a predetermined action will have the desired effect.

Ways of measuring performance of uplift models are discussed in [1], [3], [4], these include analogues of ROC and lift curves. See Section IV for more details.
A key part of a decision tree learning algorithm is the criterion used to select tests in nonleaf nodes of the tree. In this section we present two splitting criteria designed especially for the uplift modeling problem.

While previous approaches [1], [2] used directly the difference between response probabilities, i.e. the predicted quantity, we follow an approach more typical to decision trees, that is modeling the amount of information that a test gives about this difference.

We will now describe several postulates which a splitting criterion should satisfy, later we will prove that our criteria do indeed satisfy those postulates. 1) The value of the splitting criterion should be minimum 2) If A is statistically independent of Y in both treatment 3) If the control group is empty, the criterion should
Postulate 1 is motivated by the fact that we want to achieve as high a difference between class distributions in the treatment and control groups as possible. Postulate 2 says, that tests statistically independent of the class should not be used for splitting, just as in standard decision trees. Note however, that the analogy in this case is not perfect. It is in fact possible for the treatment and control class distributions after the split to be more similar than before, so the splitting criterion can take negative values. This means that an independent split is not necessarily the worst. The-orem 3.2 and discussion below further clarify the situation. A. Splitting criteria based on distribution divergences
As we want to maximize the differences between class distributions in treatment and control sets, it is natural that the splitting criteria we propose are based on distribution di-vergences [20] X  X 22]. A distribution divergence is a measure of how much two probability distributions differ. We will only require that the divergence of two discrete distributions be nonnegative, and equal to zero if and only if the two distributions are identical. We will use two distribution divergence measures, the Kullback-Leibler divergence [20], [22] and the squared Eu-clidean distance [21]. Those divergences, from a distribution Q =( q 1 ,...,q n ) to a distribution P =( p 1 ,...,p n ) ,are defined respectively as The Kullback-Leibler divergence is a well known and widely used information theoretic measure. The squared Euclidean distance is less frequently applied to compare distributions, but has been used in literature [21], [23], and applied for example to Schema Matching [24].

We will argue that the squared Euclidean distance has some important advantages which make it an attractive alternative to the Kullback-Leibler measure. First, it is symmetric, which will have consequences for tree learning when only control data is present. We note however, that the asymmetry of Kullback-Leibler divergence is not necessarily a problem in our application, as the control dataset is naturally a background from which the treatment set is supposed to differ.

A second, more subtle advantage of squared Euclidean distance is its higher stability. The KL divergence tends to infinity if one of the q i probabilities tends to zero, while the corresponding p i remains nonzero. This makes estimates of its value extremely uncertain in such cases [24]. Moreover, it is enough for just one of control group probabilities in one of the tree branches to have a small value for the KL-divergence to be extremely large, which may result in selection of a wrong attribute.

The proposed splitting criterion for a test A is defined for any divergence measure D as
D gain ( A )= D P T ( Y ): P C ( Y ) | A where D P T ( Y ): P C ( Y ) | A is the conditional divergence defined below. Substituting for D the KL-divergence and squared Euclidean distance we obtain our two proposed splitting criteria KL gain and E gain .

The intuition behind the definition is as follows: we want to build the tree such that the distributions in the treatment and control groups differ as much as possible. The first part of the expression picks a test which leads to most divergent class distributions in each branch. We subtract the divergence between class distributions on the whole dataset in order to obtain the increase or gain of the divergence resulting from splitting with test A . This is completely analogous to how entropy gain [6] and Gini gain [7] are defined for standard decision trees. In fact we will show that the analogy goes deeper, and KL gain reduces to entropy gain when the control set is missing, and E gain reduces to Gini gain when either control or treatment set is missing. Recall that for both measures we use Laplace correction while estimating P
C and P T , so that absent datasets lead to uniform class probability distributions.

The key problem is the definition of conditional diver-gence. Conditional KL-divergences have been used in liter-ature [22] but the definition is not directly applicable to our case. The difficulty comes from the fact that the probability distributions of the test A may differ in the treatment and control groups. We have thus chosen the following definition (recall that N = N T + N C and N ( a )= N T ( a )+ N C ( a ) ):
D ( P T ( Y ): P C ( Y ) | A ) where the relative influence of each test value is proportional to the total number of training examples falling into its branch in both treatment and control groups. Notice that when treatment and control distributions of A are identical, the definition reduces to conditional divergence as defined in [22].

The theorem below shows that the proposed splitting criteria do indeed satisfy our postulates.

Theorem 3.1: The KL gain and E gain test selection crite-ria satisfy postulates 1 X 3. Moreover, if the control group is empty, KL gain reduces to entropy gain [5], and when either the treatment or control set is empty, E gain reduces to Gini gain [7].

The proof can be found in the Appendix. More properties of divergences can be found in [20], [22]. The  X  X  P splitting criterion used in [1] only satisfies the first two postulates.

Notice that the value of KL gain and E gain can be negative. Splitting a dataset can indeed lead to more similar treatment and control distributions in all leaves. This is a variant of the well known Simpson X  X  paradox [25]. However, it is usually desirable that the assignment of cases to treat-ment and control groups be independent from all attributes in the data. For example in clinical trials, great care is taken to ensure that this assumption does indeed hold. We then have the following theorem, which ensures that in such a case, both gains stay nonnegative, just as is the case with entropy and Gini gains for classification trees.

Theorem 3.2: If outcomes of a test A are indepen-dent of the assignment to treatment and control groups, i.e. P C ( A )= P T ( A ) then both KL gain and E gain are nonnegative.

The proof can be found in the Appendix. Recall that in this case KL gain becomes the conditional divergence known in the literature [22]. B. Normalization: correcting for tests with large number of splits and imbalanced treatment and control splits
In order to prevent a bias towards tests with high number of outcomes standard decision tree learning algorithms nor-malize the information gain dividing it by the information value (usually measured by entropy) of the test itself [6]. In our case the normalization factor is more complicated, as the information value can be different in the control and treatment groups.

Moreover, we would like to punish tests which split the control and treatment groups in different proportions since such splits indicate that the test is not independent from the assignment of cases between the treatment and control groups. Apart from violating the assumptions of randomized trials, such splits lead to problems with probability estima-tion. As an extreme example consider a test which puts all treatment group records in one subtree and all control records in another; the tree construction will proceed based on only one dataset, as in classification tree learning (except for KL gain and empty treatment dataset), but the ability to detect uplift will be completely lost.

The proposed normalization value for a test A is given by (recall again that N = N T + N C is the total number of records in both treatment and control datasets) I ( A )= H for the KL gain criterion, and J ( A )= Gini for the E gain criterion. For the sake of symmetry, we use entropy related measures for KL gain and Gini index related measures for E gain , although one can also imagine using different types of gain and normalization factors. The first term is responsible for penalizing uneven splits. The unevenness of splitting proportions is measured using the divergence between the distributions of the test outcomes in treatment and control datasets. Tests which are strongly dependent on group assignment will thus be strongly penal-ized (note that the value of I ( A ) can be arbitrarily close to infinity). However, penalizing uneven splits only makes sense if there is enough data in both treatment and control groups. The KL ( P T ( A ): P C ( A )) term is thus multiplied by H N T N , N C N which is close to zero when there is a large imbalance between the number of data in treatment and control groups (analogous Gini based measures are used for E gain ). The result is that when only treatment or only control data are available the first term in the expression is zero, as penalizing uneven splits no longer makes sense.
The following two terms penalize tests with large number of outcomes [6]. We use the sum of entropies (Gini indices) of the test X  X  outcomes in treatment and control groups weighted by the number of records in those groups.
One problem we encountered was, that small values of the normalizing factor can give high preference to some tests despite their low information gain. Solutions described in literature [26] involve selecting a test only if its information gain is greater or equal to the average gain of all remaining attributes, and other heuristics. We found however that just adding 1 2 to the value of I or J gives much better results. Since the value is always at least 1 2 , it cannot inflate too much the information value of a test.
 Notice that when N C =0 the criterion reduces to H ( P T ( A )) + 1 2 which is identical to normalization used in standard decision tree learning (except for the extra 1
After taking the normalizing factors into account, the final splitting criteria become
The key step of tree pruning will be discussed after the next section which describes assigning scores and actions to leaves of the tree.
 C. Application of the tree
Once the tree has been built its leaves will contain sub-groups of objects for which the treatment class distribution differs from control class distribution. The question now is how to apply the tree to score new data and make decisions on whether the action (treatment) should be applied to objects falling into a given leaf. In general the action should be applied only if it is expected to be profitable. We thus annotate each leaf with an expected profit, which will also be used for scoring new data.

We assign profits to leaves using an approach similar to [1], [4] generalized to more than two classes. Each class y is assigned a profit v y , that is, the expected gain if a given object (whether treated or not) falls into this class. There is also a fixed cost c of performing a given action (treatment). Let P T ( Y | l ) and P C ( Y | l ) denote treatment and control class distributions in a leaf l . If each object in a leaf is treated, the expected profit (per object) is equal to  X  c + y P T ( y | l ) v y . If no object in the leaf is treated, the expected profit is y P C ( y | l ) v y . So the expected gain from treating each object falling into that leaf is Objects falling into l should be treated only if this value is greater than zero. The value itself is used for scoring new data. D. Pruning
Decision tree pruning is a step which has decisive in-fluence on the generalization performance of the model. There are several pruning methods, based on statistical tests, Minimum Description Length principle, and so on. Full discussion is beyond the scope of this paper, see [6], [26] X  [28] for details.

We choose the simplest, but nevertheless effective pruning method based on using a separate validation set [27], [28]. For the classification problem, after the full tree has been built on the training set, the method works by traversing the tree bottom up and testing for each node, whether replacing the subtree rooted at that node with a single leaf would improve accuracy on the validation set. If this is the case, the subtree is replaced, and the process continues.
In the case of uplift modeling obtaining an analogue of accuracy is not easy. One option is assigning costs/profits to each class (see the previous section) and pruning subtrees based on the total increase in profits obtained by replacing a subtree with a leaf. Unfortunately this method is ineffective. The total expected gain in profit obtained in the leaves is identical to that obtained in the root of a subtree. To see this sum (2) over all leaves, weighting by the probability of ending up in each leaf.

We have thus devised another measure of improvement, the maximum class probability difference which can be viewed as a generalization of classification accuracy to the uplift case. The idea is to look at the differences between treatment and control probabilities in the root of the subtree and in its leaves, and prune if, overall, the differences in leaves are not greater than the difference in the root. In each node we only look at the class for which the difference was largest on the training set, and in addition remember the sign of that difference such that only differences which have the same sign on the training and validation sets contribute to the increase of our analogue of accuracy. This procedure is consistent with the goal of maximizing the difference between treatment and control probabilities.

More precisely, while building the tree on the training set, for each node t , we remember the class y  X  ( t ) for which the difference P T ( y  X  | t )  X  P C ( y  X  | t ) is maximal, and also remember the sign of this difference s ( t )= we are examining a subtree with root r and leaves l 1 ,...,l We calculate the following quantities with the stored values of y  X  and s , and all probabilities computed on the validation set: d ( r )= d ( r )= s ( r ) P T ( y  X  ( r ) | r )  X  P C ( y  X  ( r ) | r ) , where N ( l i ) is the number of validation examples (both treatment and control) falling into leaf l i . The first quantity is the maximum class probability difference of the unpruned subtree, and the second is the maximum class probability difference we would obtain on the validation set if the subtree was pruned and replaced with a single leaf. The subtree is pruned if d 1 ( r )  X  d 2 ( r ) .

The class y  X  is an analogue of the predicted class in standard classification trees. In case either the treatment or the control dataset is absent, the missing probabilities are set to zero (we do not use Laplace correction in this step). It is then easy to see that, as long as the same sets are missing in the training and validation data, d 1 and d 2 reduce to standard classification accuracies of the unpruned and pruned subtree (note, that when the treatment set is missing, the value of s will be negative guaranteeing that both d 1 and d 2 are nonnegative).

In this section we present the results of experimental evaluation of the proposed models. We compare four mod-els: uplift decision trees based on E gain and KL gain ,the method of [1] based on the  X  X  P criterion and an approach which builds separate decision trees for the treatment and control groups. Throughout this section we will refer to those models respectively as  X  X uclid X ,  X  X L X ,  X  X eltaDeltaP X  and  X  X oubleTree X .

In order to be able to compare against the DeltaDeltaP method [1] we had to modify the  X  X  P criterion to work for tests with more than two outcomes. The modification is
 X  X  P ( A )=max where a and a vary over all outcomes of the test A , and y 0 is a selected class (say the first). In other words, we take the maximum difference between any two branches, which reduces to the standard  X  X  P criterion for binary tests.
For the DoubleTree classifier we used our own implemen-tation of decision trees, identical in all possible respects to the uplift based models. This decision was made in order to avoid biasing the comparison by different procedures used during tree construction, such as different details of the pruning strategy or the use of Laplace corrections. A. Methods of evaluating uplift classifiers
Discussions on assessing the quality of uplift models can be found in [1], [3]. In most classifier testing schemes, some amount of data is set aside while training, and is later used to assess performance. Using this approach with an uplift classifier is more difficult. We now have two test sets, one containing treated, the other control objects. The test set for the treatment group is scored using the model, and the scores can be used to calculate profits and draw lift curves. However in order to assess the gain in profit we need to take into account the behavior on the control group. This is not easy, as records in the treatment group do not have natural counterparts in the control group.

To select appropriate background data, the control dataset is scored using the same model. The gain in profits resulting from performing the action on p percent of the highest scored objects is estimated by subtracting the profit on the p percent highest scored objects from the control set from the profit on the highest scored p percent of objects from the treatment dataset. This solution is not ideal, as there is no guarantee that the highest scoring examples in the treatment and control groups are similar, but it works well in practice. All approaches in literature use this method [1], [3].
Note, that when the sizes of treatment and control datasets differ, profits calculated on the control group should be weighted to compensate for the difference.

From an equivalent point of view this approach consists of drawing two separate lift curves for treatment and control groups using the same model, and then subtracting the curves. The result of such a subtraction will be called an uplift curve . In this work we will use those curves to assess model performance. To obtain easy to compare numerical values we computed areas under uplift curves (AUUC) and the heights of the curve at the 40th percentile.

Notice that, contrary to lift curves, uplift curves can achieve negative values (the results of an action can be worse than doing nothing), and the area under an uplift curve can also be negative. Figure 1 shows the uplift curves for the four analyzed classifiers on the splice dataset.
 B. Dataset preparation
The biggest problem we faced was lack of suitable data to test uplift models. While the problem itself has wide applicability, for example in clinical trials or marketing, there seems to be very little publicly available data involving treatment and control groups. This has been noted in other papers, such as [17], where simulated data were used in experiments.

We resorted to another approach: using publicly available datasets from the UCI repository and splitting them into treatment and control groups. Table I shows the datasets used in our study, as well as the condition used for splitting each dataset. For example the hepatitis dataset was split into a treatment dataset containing records for which the condition steroid =  X  X ES X  holds, and a control dataset containing the remaining records. The group assignment condition was chosen using the following rules: 1) If there is an attribute related to an action being 2) Otherwise, pick the first attribute which gives a reason-We note that the selection of the splitting conditions was done before any experiments were carried out in order to avoid biasing the results.

A further preprocessing step was necessary in order to remove attributes which are too correlated with the splitting condition. The presence of such attributes would bias the results, since the KL and Euclid methods use the normal-ization factors I and J to penalize the use of such attributes, while other methods do not. A simple heuristic was used: 1) A numerical attribute was removed if its averages in 2) A categorical attribute was removed if the probabilities Again, we note that the decision to remove such attributes has been made, and the thresholds selected, before any experiments have been performed. The number of removed attributes (vs. the total number of attributes) is shown in Table I.

Class profits were set to 1 for the most frequent class, and 0 for the remaining classes. The cost of applying an action was set to 0 . This way, the profits reflect the difference between the probabilities of the most frequent class. C. Experimental results
To test the significance of differences between classifiers, we use the statistical testing methodology described in [29]. First, all classifiers are compared using Friedman X  X  test, a nonparametric analogue of ANOVA. If the test shows significant differences, a post-hoc Nemenyi test is used to assess which of the models are significantly different.
All algorithm parameters have been tuned on artificial data, not on the datasets shown in Table I.

Table II shows the results of applying the classifiers to the datasets in Table I. Each cell contains the AUUC (Area Under the Uplift Curve) obtained by 2  X  5 crossvalidation. The best classifier for each dataset is marked in bold. It can be seen that the model based on the squared Euclidean distance had a clear advantage. We now proceed to quantify these results using statistical tests.

We first applied the Friedman X  X  test to check whether there are significant differences between the classifiers. The test result was that the models are significantly different with the p-value of 0 . 0029 . We thus proceeded with the post-hoc Nemenyi test in order to assess the differences between specific classifiers. Figure 2 displays the results graphically. The scale marks the average rank of each model over all datasets; lower rank means a better model. For example, the model based on the squared Euclidean distance criterion had an average rank of 1 . 625 , while the double tree based approach, an average rank of 3 . 06 . The horizontal line in the upper part of the chart shows the critical difference at the significance level of 0 . 01 , that is the minimum difference between the average ranks of classifiers, which is deemed significant.

It can be seen that Euclid is a clear winner. It is sig-nificantly better than both the DoubleTree and DeltaDeltaP approaches. The two methods we propose in this paper, KL and Euclid are not significantly different, but the Euclidean distance based version did perform better. Also, the KL algorithm is not significantly better than other approaches.
We conclude that methods designed specifically for up-lift modeling (Euclid) are indeed better than building two separate classifiers. Moreover this approach significantly outperforms the DeltaDeltaP criterion [1], [2]. In fact there was no significant difference between DeltaDeltaP and Dou-bleTree. We suspect that the KL method also outperforms the DeltaDeltaP and DoubleTree approaches, but more ex-periments are needed to demonstrate this rigorously.
We also compared the results for the height of the uplift curve at the 40th percentile. Friedman X  X  test showed significant differences (with the p-value of 5 . 4  X  10  X  5 we proceeded with the Nemenyi test to further investigate the differences. We only show the results graphically in Fig-ure 3. The results are confirmed also in this case, although sometimes only at the significance level of 0 . 05 .
The paper presents a method for decision tree construction for uplift modeling. Two splitting criteria and a tree pruning method have been designed specifically for this purpose, and demonstrated experimentally to significantly outperform previous approaches to uplift modeling. The methods are more in style of modern decision tree learning and in fact reduce to standard decision trees if the control dataset is missing. Future work will concentrate on adapting other classification methods to the problem of uplift modeling.
This work was supported by Research Grant no. 4149/B/T02/2010/38 of the Polish Ministry of Science and Higher Education (Ministerstwo Nauki i Szkolnictwa Wy  X  zszego) from research funds for the period 2010 X 2012. N ( a )= N T ( a )+ N C ( a ) . It is a well known property of both Kullback-Leibler and E divergences that they are zero if and only if their arguments are identical distributions and are greater than zero otherwise. Combined with the fact that the unconditional terms in the definitions of KL gain and E gain do not depend on the test this proves postulate 1.
To prove postulate 2 notice that when the test A is inde-pendent from Y then P T ( Y | a )= P T ( Y ) and P C ( Y | P C ( Y ) for all a . Thus, for any divergence D , giving
D
To prove 3 let n be the number of classes, and U the uniform distribution over all classes. It is easy to check that Now, if P C ( Y )= U and, for all a , P C ( Y | a )= U (recall the use of Laplace correction while estimating the probabilities), and since N C =0 ,wehave N = N T , and N ( a )= N T ( a ) . It follows that Similarly E gain ( A )= E P T ( Y ): U Thesymmetryof E implies that when the treatment dataset is empty E gain ( A ) is equal to the Gini gain of A on the control sample.
 tion it follows that P T ( a )= P C ( a )= P ( a ) , which will be used several times in the proof. Notice that KL ( P T ( Y ): P
C ( Y )) can be written as equal to z log z ; f is strictly convex. For every class y we have where the inequality follows from Jensen X  X  inequality and the convexity of f . The desired result follows:
KL ( P T ( Y ): P C ( Y )) = A similar proof can be found in [20]. For the squared Euclidean distance, notice that for every class y where the inequality follows from Jensen X  X  inequality and the convexity of z 2 .Wenowhave
E ( P T ( Y ): P C ( Y )) =
