 A central goal of computational neuroscience is to understand how the brain transforms sensory input into spike trains, and considerable effort has focused on the development of statistical models that can describe this transformation. One of the most successful of these is the linear-nonlinear-Poisson (LNP) cascade model, which describes a cell X  X  response in terms of a linear filter (or recep-tive field), an output nonlinearity, and an instantaneous spiking point process [1 X 5]. Recent efforts have generalized this model to incorporate spike-history and multi-neuronal dependencies, which greatly enhances the model X  X  flexibility, allowing it to capture non-Poisson spiking statistics and joint responses of an entire population of neurons [6 X 10].
 Point process models accurately describe the spiking responses of neurons in the early visual path-way to light, and of cortical neurons to injected currents. However, they perform poorly both in higher visual areas and in auditory cortex, and often do not generalize well to stimuli whose statis-tics differ from those used for fitting. Such failings are in some ways not surprising: the cascade model X  X  stimulus sensitivity is described with a single linear filter, whereas responses in the brain reflect multiple stages of nonlinear processing, adaptation on multiple timescales, and recurrent feedback from higher-level areas. However, given its mathematical tractability and its accuracy in capturing the input-output properties of single neurons, the model provides a useful building block for constructing richer and more complex models of neural population responses.
 Here we extend the point-process modeling framework to incorporate a set of unobserved or  X  X id-den X  neurons, whose spike trains are unknown and treated as hidden or latent variables. The unob-served neurons respond to the stimulus and to synaptic inputs from other neurons, and their spiking activity can in turn affect the responses of the observed neurons. Consequently, their functional properties and connectivity can be inferred from data [11 X 18]. However, the idea is not to simply build a more powerful statistical model, but to develop a model that can help us learn something about the underlying structure of networks deep in the brain.
 Although this expanded model offers considerably greater flexibility in describing an observed set of neural responses, it is more difficult to fit to data. Computing the likelihood of an observed set of spike trains requires integrating out the probability distribution over hidden activity, and we need sophisticated algorithms to find the maximum likelihood estimate of model parameters. Here we introduce a pair of estimation procedures based on variational EM (expectation maximization) and the wake-sleep algorithm. Both algorithms make use of a novel proposal density to capture the dependence of hidden spikes on the observed spike trains, which allows for fast sampling of hidden neurons X  activity. In the remainder of this paper we derive the basic formalism and demonstrate its utility on a toy problem consisting of two neurons, one of which is observed and one which is designated  X  X idden X . We show that a single-cell model used to characterize the observed neuron performs poorly, while a coupled two-cell model estimated using the wake-sleep algorithm performs much more accurately. We begin with a description of the encoding model, which generalizes the LNP model to incorporate non-Poisson spiking and coupling between neurons. We refer to this as a generalized linear point-process (glpp) model 1 [8, 9]. For simplicity, we formulate the model for a pair of neurons, although it can be tractably applied to data from a moderate-sized populations (  X  10-100 neurons). In this section we do not distinguish between observed and unobserved spikes, but will do so in the next. Let x t denote the stimulus at time t , and y t and z t denote the number of spikes elicited by two neurons at t , where t  X  [0 ,T ] is an index over time. Note that x t is a vector containing all elements  X  is sufficiently small that we observe only zero or one spike in every bin: y t ,z t  X  X  0 , 1 } . The conditional intensity (or instantaneous spike rate) of each cell depends on both the stimulus and ( y cells are then given by where k y and k z are linear filters representing each cell X  X  receptive field, h yy and h zz are filters operating on each cell X  X  own spike-train history (capturing effects like refractoriness and bursting), and h zy and h yz are a filters coupling the spike train history of each neuron to the other (allowing the model to capture statistical correlations and functional interactions between neurons). The  X   X   X  notation represents the standard dot product (performing a summation over either index or time): where the index i run over the components of the stimuli (which typically are time points extending into the past). The second expression generalizes to h  X  z [ t  X   X ,t ) .
 The nonlinear function, f , maps the input to the instantaneous spike rate of each cell. We assume here that f is exponential, although any monotonic convex function that grows no faster than expo-nentially is suitable [9]. Equation 1 is equivalent to f applied to a linear convolution of the stimulus and spike trains with their respective filters; a schematic is shown in figure 1.
 The probability of observing y t spikes in a bin of size  X  is given by a Poisson distribution with rate parameter  X  yt  X  , ally independent terms, where Y and Z represent the full spike trains, X denotes the full set of stimuli, and  X   X  { k  X  yt and  X  zt depend only on the process history up to time t , making y t and z t conditionally inde-pendent given the stimulus and spike histories up to t (see Fig. 1c). If the response at time t were to depend on both the past and future response, we would have a causal loop , preventing factorization and making both sampling and likelihood evaluation very difficult.
 The model parameters can be tractably fit to spike-train data using maximum likelihood. Although the parameter space may be high-dimensional (incorporating spike-history dependence over many time bins and stimulus dependence over a large region of time and space), the negative log-likelihood is convex with respect to the model parameters, making fast convex optimization methods feasible for finding the global maximum [9]. We can write the log-likelihood simply as where c is a constant that does not depend on  X  . Maximizing log P ( Y, Z | X,  X  ) is straightforward if both Y and Z are observed, but here we are interested in the case where Y is observed and Z is  X  X idden X . Consequently, we have to average over Z . The log-likelihood of the observed data is given by where we have dropped X to simplify notation (all probabilities can henceforth be taken to also depend on X ). This sum over Z is intractable in many settings, motivating the use of approximate methods for maximizing likelihood. Variational expectation-maximization (EM) [20, 21] and the wake-sleep algorithm [22] are iterative algorithms for solving this problem by introducing a tractable approximation to the conditional probability over hidden variables, where  X  denotes the parameter vector determining Q .
 The idea behind variational EM can be described as follows. Concavity of the log implies a lower bound on the log-likelihood: where Q is any probability distribution over Z and D KL is the Kullback-Leibler (KL) divergence between Q and P (using P as shorthand for P ( Z | Y,  X  ) ), which is always  X  0 . In standard EM, Q takes the same functional form as P , so that by setting  X  =  X  (the E-step), D KL is 0 and the bound is M-step), which is equivalent to maximizing the expected complete-data log-likelihood (expectation taken w.r.t. Q ), given by Each step increases a lower bound on the log-likelihood, which can always be made tight, so the algorithm converges to a fixed point that is a maximum of L (  X  ) . The variational formulation differs in allowing Q to take a different functional form than P (i.e., one for which eq. 8 is easier to max-imize). The variational E-step involves minimizing D KL ( Q, P ) with respect to  X  , which remains positive if Q does not approximate P exactly; the variational M-step is unchanged from the standard algorithm.
 doing so in place of the variational E-step above results in the wake-sleep algorithm [22]. In this algorithm, we fit  X  by minimizing D KL ( P, Q ) averaged over Y , which is equivalent to maximizing the expectation which bears an obvious symmetry to eq. 8. Thus, both steps of the wake-sleep algorithm involve maximizing an expected log-probability. In the  X  X ake X  step (identical to the M-step), we fit the true model parameters  X  by maximizing (an approximation to) the log-probability of the observed data Y . In the  X  X leep X  step, we fit  X  by trying to find a distribution Q that best approximates the conditional dependence of Z on Y , averaged over the joint distribution P ( Y, Z |  X  ) . We can therefore think of the wake phase as learning a model of the data (parametrized by  X  ), and the sleep phase as learning a consistent internal description of that model (parametrized by  X  ).
 Both variational-EM and the wake-sleep algorithm work well when Q closely approximates P ,but may fail to converge to a maximum of the likelihood if there is a significant mismatch. Therefore, the efficiency of these methods depends on choosing a good approximating distribution Q ( Z | Y,  X  )  X  one that closely matches P ( Z | Y,  X  ) . In the next section we show that considerations of the spike generation process can provide us with a good choice for Q . choice for Q ( Z | Y,  X  ) , let us consider a simple example: suppose a single hidden neuron (whose full response is Z ) makes a strong excitatory connection to an observed neuron (whose response is Y ), so that if z t =1 (i.e., the hidden neuron spikes at time t ), it is highly likely that y t +1 =1 (i.e., the observed neuron spikes at time t +1 ). Consequently, under the true P ( Z | Y,  X  ) , which is the probability over Z in all time bins given Y in all time bins, if y t +1 =1 there is a high probability that z t =1 . In other words, z t exhibits an acausal dependence on y t +1 . But this acausal dependence is not captured in Equation 3, which expresses the probability over z t as depending only on past events at time t , ignoring the future event y t +1 =1 .
 Based on this observation  X  essentially, that the effect of future observed spikes on the probability of unobserved spikes depends on the connection strength between the two neurons  X  we approximate P (
Z | Y,  X  ) using a separate point-process model Q ( Z | Y,  X  ) , which contains set of acausal linear filters from Y to Z . Thus we have a sum over past and future time: from t  X   X  to t +  X   X   X  . For this model, the parameters are  X  =(  X  k z ,  X  h zz ,  X  h zy ) . Figure 2 illustrates the model architecture.
 We now have a straightforward way to implement the wake-sleep algorithm, using samples from Q to perform the wake phase (estimating  X  ), and samples from P ( Y, Z |  X  ) to perform the sleep phase (estimating  X  ). The algorithm works as follows: One advantage of the wake-sleep algorithm is that each complete iteration can be performed using only a single set of samples drawn from Q and P . A theoretical drawback to wake-sleep, however, is that the sleep step is not guaranteed to increase a lower-bound on the log-likelihood, as in variational-EM (wake-sleep minimizes the  X  X rong X  KL divergence). We can implement variational-EM using the same approximating point-process model Q , but we now require multiple steps of sampling for a complete E-step. To perform a variational E-step, we draw samples (as above) from Q and use to  X  . We can then perform noisy gradient descent to find a minimum, drawing a new set of samples for each evaluation of D KL ( Q, P ) . The M-step is equivalent to the wake phase of wake-sleep, achievable with a single set of samples.
 One additional use for the approximating point-process model Q is as a  X  X roposal X  distribution for Metropolis-Hastings sampling of the true P ( Z | Y,  X  ) . Such samples can be used to evaluate the true log-likelihood, for comparison with the variational lower bound, and for noisy gradient ascent of the likelihood to examine how closely these approximate methods converge to the true ML estimate. For fully observed data, such samples also provide a useful means for measuring how much the entropy of one neuron X  X  response is reduced by knowing the responses of its neighbors. To verify the method, we applied it to a pair of neurons (as depicted in fig. 1), simulated using a stimulus consisting of a long presentation of white noise. We denoted one of the neurons  X  X bserved X  and the other  X  X idden X . The parameters used for the simulation are depicted in fig. 3. The cells have similarly-shaped biphasic stimulus filters with opposite sign, like those commonly observed in ON and OFF retinal ganglion cells. We assume that the ON-like cell is observed, while the OFF-like cell is hidden. Both cells have spike-history filters that induce a refractory period following a spike, with a small peak during the relative refractory period that elicits burst-like responses. The hidden cell has a strong positive coupling filter h zy onto the observed cell, which allows spiking activity in the hidden cell to excite the observed cell (despite the fact that the two cells receive opposite-sign stimulus input). For simplicity, we assume no coupling from the observed to the hidden cell 2 . Both types of filters were defined in a linear basis consisting of four raised cosines, meaning that each filter is specified by four parameters, and the full model contains 20 parameters (i.e., 2 stimulus filters and 3 spike-train filters).
 Fig. 3b shows rasters of the two cells X  responses to a repeated presentations of a 1s Gaussian white-noise stimulus with a framerate of 100Hz. Note that the temporal structure of the observed cell X  X  response is strongly correlated with that of the hidden cell due to the strong coupling from hidden to observed (and the fact that the hidden cell receives slightly stronger stimulus drive). Our first task is to examine whether a standard, single-cell glpp model can capture the mapping from stimuli to spike responses. Fig. 3c shows the parameters obtained from such a fit to the observed data, using 10s of the response to a non-repeating white noise stimulus (1000 samples, 251 spikes). Note that the estimated stimulus filter (red) has much lower amplitude than the stimulus filter of the true model (gray). Fig. 3d shows the parameters obtained for an observed and a hidden neuron, estimated using wake-sleep as described in section 4. Fig. 3e-f shows a comparison of the performance of the two models, indicating that the coupled model estimated with wake-sleep does a much better job of capturing the temporal structure of the observed neuron X  X  response (accounting for 60% vs. 15% of the PSTH variance). The single-cell model, by contrast, exhibits much worse performance, which is unsurprising given that the standard glpp encoding model can capture only quasi-linear stimulus dependencies. Although most statistical models of spike trains posit a direct pathway from sensory stimuli to neu-ronal responses, neurons are in fact embedded in highly recurrent networks that exhibit dynamics on a broad range of time-scales. To take into account the fact that neural responses are driven by both stimuli and network activity, and to understand the role of network interactions, we proposed a model incorporating both hidden and observed spikes. We regard the observed spike responses as those recorded during a typical experiment, while the responses of unobserved neurons are mod-eled as latent variables (unrecorded, but exerting influence on the observed responses). The resulting model is tractable, as the latent variables can be integrated out using approximate sampling methods, and optimization using variational EM or wake-sleep provides an approximate maximum likelihood estimate of the model parameters. As shown by a simple example, certain settings of model param-eters necessitate the incorporation unobserved spikes, as the standard single-stage encoding model does not accurately describe the data.
 In future work, we plan to examine the quantitative performance of the variational-EM and wake-sleep algorithms, to explore their tractability in scaling to larger populations, and to apply them to real neural data. The model offers a promising tool for analyzing network structure and network-based computations carried out in higher sensory areas, particularly in the context where data are only available from a restricted set of neurons recorded within a larger population.
