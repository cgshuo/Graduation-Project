 Identifying approximately identical strings is key for many data cleaning and data integration processes, including sim-ilarity join and record matching. The accuracy of such tasks crucially depends on appropriate choices of string similarity measures and thresholds for the particular dataset. Manual selection of similarity measures and thresholds is infeasible. Other approaches rely on the existence of adequate historic ground-truth or massive manual effort.
 To address this problem, we propose an Active Learning al-gorithm which selects a best performing similarity measure in a given set while optimizing a decision threshold. Active Learning minimizes the number of user queries needed to arrive at an appropriate classifier. Queries require only the label match/no match , which end users can easily provide in their domain. Evaluation on well-known string match-ing benchmark data sets shows that our approach achieves highly accurate results with a small amount of manual la-beling required.
 H.2.m [ Database Management ]: Miscellaneous X  Data cleaning Active Learning; string similarity; similarity measures; sim-ilarity join; string matching; record matching; deduplication
Data integration and -cleaning are fields with longstand-ing importance, that still have space for improvement [19]. Many of the tasks in those fields rely on string similar-ity measures, sometimes also a threshold that allows string matching. These include record matching [11], similarity join [21] and schema matching [17].
 One immediate application of approximate string matching is similarity join. It is the simple form of record match-ing, where the matching depends only on one key attribute. Differently from a simple join operation, equivalent entities from both databases may only have similar values in this key attribute. Specification of a similarity measure and thresh-old define a solution to the similarity join problem. It can then be reduced to an algorithmic problem, that can be ef-ficiently solved, depending on the similarity measure [21]. Record matching relies on similarity measures to compute similarity values on attribute level [11]. In the Fellegi-Sunter model, binary comparison vectors are based on similarity measures and thresholds for the involved attributes, that need to be specified. In many other popular approaches similarity measures need to be specified, while thresholds for these functions themselves are not relevant. E.g., in SVM-based approaches, similarity values of several attributes are combined to a similarity vector. Then, a decision hyperplane is learned from training data. In decision tree based ap-proaches, several thresholds are learned to constitute predi-cates in logical rules. These thresholds may be inconsistent for the same attribute and similarity measure across differ-ent predicates used in a tree.
 Bleiholder et al. point out that the effectiveness of the record matching step in data integration is mostly affected by the quality of the similarity measure and the choice of a simi-larity threshold [5]. Figure 1 illustrates that the choice of a similarity measure can make a huge difference. Here, the measure TagLink equipped with a suitable threshold can match pairs of strings perfectly, while MongeElkan will pro-duce many errors with any threshold.
 In [11], a large scale comparison of frameworks for record matching is done. The authors find that supervised match-ing approaches require less configuration effort and knowl-edge than others, but aspects like the choice of similarity measures still have to be determined manually.
 In this paper we study how to find a suitable similarity measure from a given pool and optimize a corresponding threshold semi-automatically. We propose an Active Learn-ing approach to this problem, in order to minimize human input. The user is iteratively queried for the ground-truth, i.e., whether a pair of strings is equivalent or not. A stop-ping criterion suggests that a sufficiently accurate choice can be done and will terminate this loop. It will return the cho-sen similarity measure along with an appropriately tuned threshold as the final output. q 0.667 Figure 4: Maximum F 1 -score values for a measure s i may be achieved by multiple candidate thresholds tor defined by Equation (1) for s i and a fixed t . We compute the empirical F-measure of p , denoted as F 1 ( Q m , p ), based on the known ground-truth for Q m = { q 1 , . . . , q m } . Among all candidate thresholds for s i , we consider only those with the highest empirical F-measure. From these best threshold candidates we select only one as a threshold t m,i as described in Section 3.1.2.
 The candidate thresholds for s i are found as follows. We real number line in at most m + 1 intervals. The candidate thresholds are then the arithmetic means of each (closed) interval (and additionally the minimal value s i and the max-imal value plus a small constant s i +  X  ). Note that any point within one interval yields the same empirical F-measure for the corresponding candidate predictor.
 Figure 3 illustrates this. There are 11 labeled pairs q 1 that are aligned in a linear ordering, according to their sim-ilarity value as measured by s i (increasing from left to right, not shown). The candidates for a new threshold t 11 ,i for similarity measure s i are the  X  X iddles X  of each of the 12 indicate these candidate thresholds; the corresponding em-pirical F-measures are shown in the text labels.
 olds with empirical F-measure of 0 . 8. This is the best value, and so we use as a threshold t m,i for s i the middle of this interval. All string pairs q with s i ( q )  X  t m,i are now clas-sified by the resulting prediction function p 11 ,i as matches (shaded or red area). Obviously, this prediction function errs on some of the queried pairs, namely q 3 (false negative) and q 7 (false positive).
There are two types of ambiguity when selecting p  X  m : (i) for each measure s i several candidate thresholds might lead to the same empirical F-measure, and (ii) after all n might achieve the highest value F  X  pred ( m ) of the empirical F-measure.
 Ambiguities of type (i) are illustrated in Figures 3 (thresh-olds of the same interval are equivalent) and 4 (there may even be different equivalent intervals). We resolve those by selecting as the threshold t m,i for s i the middle of the X  X ight-most X  interval (i.e., containing the highest similarity values) among all these empirically optimal choices (e.g., between q and q 2 in Figure 4). This choice has little impact on per-formance of the algorithm.
 Ambiguity in case (ii) occurs since many predictors can have the same F-measure in a given iteration. As shown in the evaluation (Section 4), this becomes significantly less pro-nounced in later iterations. We devise here a simple sec-ondary heuristic ranking to pick the best from these highest scoring predictors.
 For each candidate predictor p m,i we inspect the interval containing the selected threshold for p m,i : the lower the number of pairs with unrevealed labels in that interval, the higher the secondary ranking for the predictor. Our heuris-tic intuition is that the spread of these noisy pairs are ex-pected to be more concentrated in good performing hypothe-ses, which leave few intermediate pairs unqueried.
In the following, we introduce an aggregated prediction function p m that is computed in each iteration m . This function p m is required by the query strategy described in Section 3.3. We describe how this is done in Sections 3.2.1 and 3.2.2. In Section 3.3, we explain how we define the most uncertain pair based on p .
 In this paper, we use the term hypothesis as defined in [18]. A hypothesis is a classifier (or a configuration that is suffi-cient to identify one), that explains the data by generaliz-ing the ground-truth. In our case, a hypothesis h = ( s, t ) is determined by a specific similarity measure s from S = { s 1 , . . . , s n } and a corresponding threshold t  X  R . Con-sequently, the hypothesis space is H = S  X  R . Restating Equation (1), each hypothesis h = ( s, t ) is assigned a pre-diction function defined by p h ( q ) = 1 :  X  s ( q )  X  t . We will write h m,i as shorthand for ( s i , t m,i ) and p m,i for p also define as F pred ( m, i ) := F 1 { Q m , p m,i } the empirical F score for hypothesis h m,i w.r.t. observed labels only (recall that Q m is the set of already labeled string pairs).
We account for redundancy among similarity measures by clustering measures that are mutually similar. It is impor-tant to notice about the hypotheses that they are not in-dependent of one another. Some similarity measures may not even be defined similarly, but produce a very highly correlated ordering of pairs. In order to combine their pre-dictions in a meaningful way, it is important to account for this redundancy. Otherwise some similarity measures might Figure 5: Structure of the aggregated prediction f unction p m (a fictional example with n = 8 ) Symbol Definition
C 1 , . . . , C k  X  S k c lusters of similarity measures p m ,i : Q  X  { 0 , 1 } prediction function o f p C j m : Q  X  [ 0 , 1] (intermediate) prediction function p m : Q  X  [ 0 , 1] aggregated prediction function w m ,i , w C j m weights u sed in the aggregation p t arget m := 1  X  r m target prediction v alue b e overrepresented and dominate the voting.
 We represent similarity measures by the vector of similar-ity values, indexed by all considered string pairs. The re-dundancy among two similarity measures is then computed by their mutual Pearson correlation coefficient. Calculat-ing the matrix of correlations between all pairs of similarity measures yields a weighted undirected graph. The nodes correspond to similarity measures and the weighted edges to the pairwise Pearson correlation. Clusters C 1 , . . . , C this graph correspond to sets of mutually similar similarity measures. We apply a graph clustering algorithm [16] that maximizes the so-called modularity of a graph clustering to achieve this. We used the implementation Linloglayout [1]. The clustering is computed once in the beginning and re-mains the same over the course of all iterations. In our experiments with a set of 24 similarity measures, k ranged from 2 to 5 and the cluster sizes from 1 to 12.
In the following, we explain how we combine in each itera-tion m the individual prediction functions p m,i to an overall prediction function p m .
 We do this in two steps, which we illustrate in Figure 5. The first step combines the prediction functions of hypothe-ses whose similarity measures are in the same cluster C to an intermediate prediction function p C m . The second step combines all predictions functions p C m to the overall predic-tion function p m .
 We estimate the predictive power of a prediction function p m,i with the F-measure on the seen labels. We assign weights w m,i = F pred ( m, i ) to favour promising predictions over poor predictions. The prediction functions p C m are de-fined as follows: These prediction functions are then combined to the overall prediction function p m : The weights w C m for the functions p C m reflect how promising the hypotheses whose similarity measures belong to cluster C are on average.
 The expression for p m ( q ) can be expanded and then simpli-fied to the following: p m ( q ) =
In terms of [18], the strategy can be described as an in-stance of uncertainty sampling. That is, the paradigm that is based on a prediction function for unseen labels and as-signs a measure of certainty to each prediction. The basic insight is that there is little value in querying the instances whose labels can be predicted with high certainty. Con-versely, the most gain for the learning progress can be ex-pected from querying the most uncertain instances. The prediction function p m : Q  X  [0 , 1] calculates a real value for each string pair. We now define our notion of un-certainty based on this function.
 A clear prediction of p m ( q ) = 1 . 0 implies that each hy-pothesis predicts the query to be a match. Conversely, p ( q ) = 0 . 0 means that each hypothesis predicts a non-match.
 When the overall prediction p m ( q ) is equal to 0 . 5, the label of q is highly uncertain. This prediction can only occur if sufficiently highly-weighted or at least many hypotheses pre-dict either label. This means that no matter which label will turn out to be true, a significant part of the hypotheses will see its prediction disproved. I.e., significant in the sense of a combination of the individual confidences. The first idea is hence to define the next query as Yet, to achieve fast convergence, we have to correct this notion of uncertainty to account for the overall prediction p ( q ) which can be biased in early iterations. We will there-fore replace the value 0 . 5 by a target prediction , denoted by p m  X  [0 , 1].
 Since the thresholds have to be set to some initial setting, they are bound to be inappropriate in the beginning. As-sume that the thresholds in iteration m are such that the functions p m,i are collectively biased towards predicting mat-ches. That is, each function p m,i has a high false positive rate w.r.t. the whole ground-truth. Then the set of pairs that get a committee prediction close to 0 . 5 will consist of more non-matches than matches. So the label of such a pair is actually not most uncertain, when the hypotheses are col-lectively biased. The target prediction p target m for iteration m must be higher than 0 . 5 if the hypotheses are generally biased towards predicting matches. An analogous argument applies in the reverse situation. We measure the current bias of the committee by calculat-i ng the ratio of match labels among the revealed labels. We define the target prediction p target m as the complement, or ratio of non-match labels among the seen labels: It will yield a value higher than 0 . 5, if the committee was biased towards matches in the past, as desired. This way the target prediction self-regulates until the thresholds adjust to the respective regions where the labels are truly uncertain. The next query is hence defined as follows: Note that the query q m minimizing the expression above may not be uniquely defined. Therefore, we devised a sec-ondary ranking of the pairs to further distinguish the pairs. This ensures that informative queries are done even when the prediction p m is not (yet) well prepared to decide this. We measure the variance of ranks w.r.t. the different linear orderings that the similarity measures give. It can be calcu-lated by unsupervised data. This is defined as follows. For each pair, calculate the rank that it is assigned by each similarity measure s . That is, the pair q with the highest similarity value s ( q ) will get rank 0 w.r.t. s , the pair with the second highest will get rank 1, and so on. Then calculate for each pair the variance with respect to its similarity rank across all similarity measures. The higher this variance is, the higher the pair be will appear in the secondary ranking for querying.
In an approach using Active Learning it is useful to have a stopping criterion, i.e., a function indicating when the query-ing process can be terminated. It is important not to stop too early, when the solution is still improving. On the other hand, the algorithm should not issue additional queries after the solution is stable.
 The stopping criterion can be designed independently of the query strategy and the ranking of candidate predictors. We tried a lot of other intuitive heuristics, but none seemed to be more successful than the one we will introduce now. The main problem in devising a stopping criterion is the scarcity of information that is inherent to Active Learning. A trivial stopping criterion is a threshold on the number of queried labels. However, we noticed that the algorithm  X  X rogresses X  at different speeds for different data sets. We measured the progress of the algorithm in terms of the true F-measure F true ( m ) := F ( Q, p  X  m ) of the currently best rank-ing prediction function p  X  m (for the ranking criterion, refer to Section 3.1.2). The true F-measure uses all hidden labels and can hence not be used by the algorithm.
 However, (at least for the considered data sets) another mea-sure seems to capture this progress well and does not require the hidden ground-truth. This measure is the total number of changes in thresholds that have occurred since the begin-ning of the algorithm: where  X  is equal to 1 if its arguments are equal and 0 oth-erwise. A stopping criterion can be devised as a condition that T C m has exceeded a certain threshold. In Section 4.3 we compare different thresholds for T C m in context of our data sets.
In this section we evaluate our approach under several aspects. These include user labeling effort vs. accuracy (Section 4.3), convergence behavior (Section 4.4), impact of the stopping criterion (Section 4.5), and other aspects, such as impact of the predictor ranking (Section 4.6). rized in Table 3. Four data sets are created by us (marked with + in Table 3). To obtain the ground-truth we pursued a similar approach as [8], with a final manual review. We have also found and corrected errors in the ground-truth in 3 existing data sets ( business , animal , and bird2 ). Details of these errors as well as all data sets can be downloaded from http://pvs.ifi.uni-heidelberg.de/team/lb/ .
 Similarity measures. In our experiments we use n = 24 similarity measures s 1 , . . . , s 24 taken from the library Sec-ondString [2]. Some of these measures depend only on the two input strings, while others take into account frequencies of substrings across the whole data set. Our approach treats the similarity measures as black boxes.
 eral parameters described in Section 3 do not have a notable impact on the behaviour of the algorithm. These include initial values of thresholds, and selecting the actual thresh-old value among several candidates with identical F 1 -scores. Therefore, we report only the results obtained via imple-mentation choices described above. We use in the following some symbols defined in Table 1. Recall that for a fixed iteration number m , the (intermedi-ate) result of the learning process is the best predictor p (if m is the final iteration, p  X  m is the final result of the pro-cess). The following two metrics evaluate the convergence and accuracy of p  X  m . The true F 1 , denoted as F true ( m ), is the F 1 -score of the best predictor p  X  m taking into account the labels of all string pairs in Q (i.e., the complete ground-truth): This metric estimates the generalization capability of p  X  since it takes into account all labels of the data set, not only the queried ones.
 The maximal F 1 (symbol F max ) is the F 1 -score of the best possible prediction function across the hypothesis space H = S  X  R w.r.t. the whole ground-truth, i.e., The match ratio in iteration m (denoted r m ) is the ratio of match labels versus non-match labels among the observed pairs. Finally, T C m is the total number of threshold changes , which is the cumulative number of times when a threshold has changed for any similarity measure in any iteration.
USPresidents + personal names 43 43 1,849 43 173 43 24.86% faoMembers + country names 194 194 37,636 194 2,633 192 7.29% nobelLaureates + personal names 839 839 703,921 839 27,011 831 3.08% Figure 6: The empirical F 1 -scores decrease with higher number of iterations.
Table 4 reports results for all data sets after learning prediction models with two different stopping criteria A: T C m  X  75 and B: T C m  X  130. For criterion A, the average number of queries (or iterations) is 7 and never exceeds 10, which shows that the labeling effort for the user is very low. At the same time, the ratios F true /F max of solution quality (i.e. F true ) to maximum achievable quality ( F max , specific to a data set) are high. This indicates that the final matching prediction functions have been learned well.
 In case of stopping criterion B, the ratios F true /F max general higher, but not to a large degree. Also here the aver-Figure 7: The ratio of true F 1 -scores of the best pre-dictor ( F true ( m ) ) to F max vs. total number of thresh-old changes T C m (each box/whisker plot shows dis-tribution over all data sets). age number of iterations until stop is relatively low (around 15) which implies an acceptable labeling effort. Summariz-ing, we conclude that Active Learning performs very well, and allows minimizing user effort while achieving accurate prediction models.
Figure 8 gives more insight into convergence behavior of some selected representative data sets. While animal and business (top row) illustrate benign changes of F true ( m ) (the faoMembers 0.961 9 L2 L -0.300 0.958 99.70% 21 L2 ME 0.949 0.897 93.37% fodorZagrat 0.978 8 TL 0.751 0.959 98.09% 17 JWT 0.644 0.968 99.05% nobelLaur. 0.989 7 JWT 0.399 0.790 79.93% 15 ST 0.443 0.882 89.21% coraATDV 0.800 6 ME 0.551 0.648 81.07% 15 L2 L -0.675 0.736 92.07% T able 4: Results after model learning with stopping criteria A: T C F true /F max gives the ratio of solution quality (i.e. F true ) to maximum achievable quality ( F max ). Similar-M= Mixture , JC= Jaccard . Note that thresholds may be negative. F -score of the best predictor, specific to iteration number m ), UVA and nobelLaureates (bottom row) show less desir-able behavior of F true ( m ). We also note that in the first iter-ations highest-ranked similarity measure changes frequently (indicated by vertical lines), while after 7 iterations the mea-sure stabilizes. This is less pronounced for the  X  X ad X  cases UVA and nobelLaureates .
 Figure 6 shows that the empirical F 1 -scores F pred ( m, i ) de-crease with growing iteration number m for all measures. This can be attributed to the fact that the uncovered labels come only from the  X  X oisy X  area that contains all hard-to-separate pairs. This also explains why F  X  pred ( m ) decreases below the actual performance F true ( m ). So, the empirical F -scores do not approximate the true performance, but are sufficient to compare the quality of current predictors.
Note that the output prediction function p  X  m is chosen from the p h (with h  X  H ). The aggregated prediction func-tion p m will not be output. It is only used to define the next query to the user. In fact, p m performs significantly worse than the individual predictor p  X  m chosen in each time step by the algorithm. Our experiments showed that it is very unsta-ble for most data sets (it does not converge) and shows worse F -scores most of the time; often significantly. A reason is that even a bad similarity measure s i can achieve precision of r m and recall of 1 . 0 by a small threshold (for r m = 0 . 3 this equals w m,i = F pred ( m, i ) = 0 . 462). So p m may always be somewhat influenced by inappropriate measures.
The choice of the stopping criterion determines the trade-off between user labeling effort and accuracy. Figure 8 shows that for animal and business the matching quality F  X  pred increases rapidly in the first few iterations and remains vir-tually unchanged after that. For UVA and nobelLaureates more than 10 iterations are needed in order to achieve a stable quality level (even not so in the case of UVA). This refines the findings from Section 4.3: for some data sets, less then 10 iterations are sufficient (i.e., stopping criterion A), while others require more labeling effort. Thus, the choice of a universal stopping criterion is hardly possible. Depend-ing on the application, either low user effort (criterion A) or potentially higher accuracy (criterion B) must be preferred. Figure 9 shows on the right y -axis the values of T C m used in our stopping criterion (i.e. cumulative number of threshold changes over all measures in S ). More conclusive is Figure 7: we see that the variance of the true F 1 -scores over all data sets decrease for T C m values above 75, suggesting this as a potential threshold.
Figure 9 shows (on the left axis) the F 1 -score of the best ranked predictor (line with crosses) among all predictors with maximal empirical F 1 -scores in iteration m (shaded area). It illustrates the use of a secondary heuristic ranking explained in Section 3.1.2, especially when a lot of compet-ing predictors (with the same best empirical F 1 -scores) exist in the early iterations.
 Figure 8 also shows that the match ratio r m is very balanced, while tending towards lower values. Considering the skewed label distribution (see Table 3), this constitutes a very bal-anced sample. This indicates that the query strategy works well in terms of selecting string pairs with informative labels. to take part in a benchmark competition for text retrieval. I t is trained online on a stream of queries. They introduce score-distributional threshold optimization , which uses a sta-tistical model to estimate a threshold. The considered simi-larity measure is TFIDF with some preprocessing (like stem-ming and stop word removal). The approach works with any quality metric that is a function of the contingency ta-ble, like the F-measure. The paper outlines the straightfor-ward empirical method to optimize thresholds w.r.t. a given quality metric, that exhaustively considers all thresholds on the present training data. The authors dismiss this sim-ple approach, because of its drawbacks in their online prob-lem setting. Most computations involved in their method can be updated incrementally. Also, it is able to produce a broad prediction for the threshold with only sparse super-vised data. It is unclear, however, how score-distributional threshold optimization performs on only a few samples, since it is only evaluated in a very specific online setting. The ap-proach is only evaluated on TFIDF and does not point out any method to compare several similarity measures. The authors of [10] use the straightforward empirical method w.r.t. optimizing accuracy. Additionally, they formulate a statistical model with a bivariate Gaussian distribution. They show that this model captures the notion of an accu-racy-optimal threshold well. Unfortunately, the evaluation of the optimization is not conclusive. Only one data set with artificial edit variations is used, and one similarity mea-sure (Levenstein edit distance). The result suggests, that eventually the threshold arrives in the optimal interval. In the one experiment this happens close to 40 used samples. One sample corresponds to a ranking list which requires 8 labels ( relevant or irrelevant to the query). The authors suggest to use clustering as an unsupervised method to ap-proximate ground-truth to mitigate the labeling effort. The evaluation does not measure how good the intermediately approximated thresholds are, in terms of any quality met-ric. The sampling method for documents and queries is not described. Finally, a quality metric for similarity measures is introduced (arithmetic mean of accuracy and the size of the output interval of optimized thresholds). This metric is evaluated on eight similarity measures. The results show that this measure preserves the ranking by accuracy. Hence the empirical method for threshold optimization can also be used to compare similarity measures in terms of accuracy.
We have developed a novel method for finding a simi-larity measure and an appropriate threshold that work well specifically for given data, in order to solve the string match-ing problem. It requires no existing ground-truth and can be used by end-users. The experimental evaluation shows that good results can be achieved with only very few itera-tions. We propose two stopping criteria based on a notion of progress. Their thresholds have been determined empir-ically. The result of our proposed algorithm can directly be used in important applications like similarity join, record matching and schema matching. [1] https://code.google.com/p/linloglayout/. [2] http://secondstring.sourceforge.net/. [3] A. Arampatzis and A. van Hameran. The [4] J. Attenberg and F. Provost. Inactive learning?: [5] J. Bleiholder and F. Naumann. Data fusion. ACM [6] P. Christen. A comparison of personal name matching: [7] P. Christen. Data Matching . 2012. [8] W. W. Cohen. Data integration using similarity joins [9] W. W. Cohen, P. D. Ravikumar, and S. E. Fienberg. [10] R. Da Silva, R. Stasiu, V. M. Orengo, and C. A. [11] H. Koepcke and E. Rahm. Frameworks for entity [12] A. McCallum, K. Nigam, and L. H. Ungar. Efficient [13] D. Menestrina, S. E. Whang, and H. Garcia-Molina. [14] A. E. Monge, C. Elkan, and others. The Field [15] F. Naumann and M. Herschel. An introduction to [16] A. Noack. Modularity clustering is force-directed [17] E. Rahm and P. A. Bernstein. A survey of approaches [18] B. Settles. Active Learning . 2012. [19] M. Stonebraker, I. F. Ilyas, S. Zdonik, G. Beskales, [20] S. Tejada, C. A. Knoblock, and S. Minton. Learning [21] C. Xiao, W. Wang, X. Lin, J. X. Yu, and G. Wang.
