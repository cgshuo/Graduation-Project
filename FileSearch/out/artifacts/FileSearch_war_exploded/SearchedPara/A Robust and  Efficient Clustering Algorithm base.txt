
Many data clustering algorithms have been proposed in breviated as CSM), which runs in linear time to the size of input data set. Combining the features of partitional and hierarchical clustering methods, algorithm CSM partitions the input data set into several small subclusters in the first phase, and then continuously merges the subclusters based on cohesion in a hierarchical manner in the second phase. It is shown by our performance studies that the use of cohesion as the measurement enables algorithm CSM to be not only very robust to the existence of outliers but also able to lead to better clustering results than those by most prior meth-ods. In a specific comparison with algorithm CURE which is known to be one excellent clustering method, algorithm 
CSM is able to achieve the clustering results of the same quality as CURE while only incurring a much shorter, in orders, execution time. The rest of the paper is organized as follows. Section 2 presents the preliminaries. Algorithm CSM is presented in Section 3. Performance studies are conducted in Section 4. 
This paper concludes with Section 5. 2. PRELIMINARIES Given a desired number of clusters k, data clustering is the process of partitioning the input data points into k clusters so that the points in each cluster are similar to one another and are different from the points in other clusters. In this subsection, we review several prior algorithms re-lated to this study. As its name implies, a hierarchical clustering algorithm establishes a hierarchical structure as the clustering result. 
With the hierarchical structure, we can obtain different clus-tering results for different similarity requirements. Most existing hierarchical clustering algorithms are vari-ations of the single-link and complete-link algorithms. Both algorithms require the time complexity of O (n 2 log n) and the space complexity of O (n2). Owing to their good quality of clustering results, hierarchical algorithms are widely used, especially in the document clustering and classification. The outline of a general hierarchical clustering algorithm is given below. 
Hierarchical Clustering Algorithm 1. Initially, each data point forms a cluster by itself. 2. The algorithm repetitively merges the two closest clus-ters. 3. Output the hierarchical structure constructed. A single-link clustering algorithm is different from a complete-link clustering algorithm in the inter-cluster distance mea-surement, i.e., Step 2. The single-link algorithm uses the distance between the two closest points of the two clusters as the inter-cluster distance, i.e., while the complete-link algorithm uses the distance of two farthest points as the inter-cluster distance, i.e., 
However, the single-link algorithm suffers from so-called chain-ing effect and the complete-link clustering algorithm has 583 P~ partitions. In each partition, the hybrid algorithm per-forms a hierarchical clustering algorithm to get Ci clusters. This process continues until exactly k clusters are identi-fied. This algorithm is shown to be very efficient in both aspects of computation and memory space. Finally, the al-gorithm performs a top-down process to map all points of each subcluster in level i to the cluster which their centroid belongs to in level i + 1. However, using only one point to represent the w:hole cluster may easily lose some information about the distributions of clusters, which are important to the similarity of two clusters. 
Another hybrid clustering algorithm, BIRCH, is designed to deal with a large data set. Algorithm BIRCH uses cluster features (CF) to represent a cluster. Given the CF of a clus-ter, one can ob~tain the centroid, radius and diameter of that cluster easily (in constant time). Further, the CF vector of a new cluster formed by merging two subclusters can be di-rectly derived from the CF vectors of the two subclusters by algebra operations. Algorithm BIRCH is one efficient clus-tering algorithm. However, adopting CF vector to represent a cluster will unavoidably suffer from the same problem as the k-means algorithm. As a result, BIRCH is appropriate for data sets consisting of only isotropic clusters. 
In this section, we describe the details of cohesion-based self-merging algorithm (abbreviated as CSM). In Section 3.1, we propose a new measurement for the similarity of two subclusters, cohesion, which is intrinsically different from other prior measurements. Cohesion is more appropriate for inter-cluster similarity measurement because it does not judge the similarity of two subclusters by only some data points. Rather, the cohesion measurement takes the dis-tributions of t:he two clusters into account. In Section 3.2, based on cohesion, we devise a new clustering algorithm, CSM, which fully utilizes the features of cohesion. 
As shown in Figure 2, although the distance between the centroids of the two subclusters in Figure 2(a) is the same as that in Figure 2(b), the two subclusters shown in Figure 2(b) are more inclined to be merged into a new cluster. In addition to the distance between centroids, another method to measure the inter-cluster distance of two subclusters, Ci each pi E Ci and pj E Cj. However, this complex computa-tion has its own drawback and cannot even distinguish the two cases sho~rn in Figure 2. Note that the average complete distances of these two clusters are very close to each other, i.e., 20.40 in Figure 2(a) and 21.57 in Figure 2(b). (In con-trast, as can be verified later, the corresponding cohesions are 0.013 in Figure 2(a) and 0.189 Figure 2(b), indicating the better capability of distinguishing one from another by using the cohesion measurement.) Several alternatives, such as the distance between the closest (or farthest) points of the two clusters, could be employed to redeem this deficiency. However, those measurements are very vulnerable to random noises (outliers). Consequently, we propose a new similarity measurement, namely cohesion, based on the joinability of a data point to another cluster. First, same as in [16], we Figure 3: An illustration of computation of cohesion. of first cluster Cll, cl, is (5,6) and the centroid of clus-ter Cl2, c2, is (14, 10). The radii of these two clusters, rl definition of joinability, we can derive that join (A, Clz) = larly, join (C, Cll) = Exp (-Id(C'c2)-d(c'cl,'2 )l) = 0.095 and the joinabilities of D, and E according to Cll are 0.038, and 0.064 respectively. Thus, the cohesion of these two sub-clusters is chs (Cll, CI2)= (0.001 + 0.002 + 0.095 + 0.038 + 0.064)/5 = 0.0398. 
It is worth mentioning that this similarity measurement of cohesion is robust to the existence of outliers due to the following two reasons: (1) Using the cohesion measurement, two subclusters are considered to evaluate this inter-cluster similarity; (2) According to the cohesion measurement, the contribution of each point is limited to be in the range of [0, 1]. This important feature limits the possible impact of outliers. 
Now, we describe the proposed algorithm based on cohe-sion self-merging as follows. Algorithm CSM //Input: The input data set, the size of the data set, n, the number of subclusters, m and the desired number of clus-ters, k. //Output: The hierarchical structure of the k clusters. 1. Initially select rn centroids arbitrarily. 2. Assign each point to the closest subcluster based on the distance to the centroids. 3. Obtain the centroid of each subcluster. 4. Repeat Step 2 and Step 3 until no points change between clusters. 5. Obtain the cohesions, chs (Ci, Cj) for all pairs of subclus-tars, Ci and Cj. 6. Build a heap, QCHS, with the cohesions of all combina-tions. 7. Extract the maximal cohesion, say chs(Ci,Cj), from 8. If Ci and Cj do not belong to the same subcluster, then merge the two subclusters which they belong to into a new subcluster. 9. Repeat Step 7 and Step 8 until the number of clusters is equal to k. 
Example 3.2: Consider the data set shown in Figure 4. Recall that better visual effect can be achieved if these 
Figure 6: Clustering results of algorithm CSM. Data Set 1 100000 32 5 84.3 sec Data Set 2 2696 32 2 0.68 sec Table 2. Execution details on each data set by CSM. We perform our experiments on the data sets shown in 
Figure 5. Data Set 1 is the same as the one used in CURE [5]. As stated in [5], both algorithm BIRCH and single-link clustering algorithm cannot partition this data set correctly. 
The second data set is a famous single-link example. How-ever, we add some random noises for experimental purposes. 
The number of points and parameter setting for each data set is shown in Table 2. Here, we choose the parameter m with which algorithm CSM can easily obtain the similar clustering results each time. Note that some of the data sets contain too many points to be clearly shown in the pa-per. Hence, we only show a sampled subset of the original data sets. For the same reason, we show only the sampled subsets of the clustering results while we indeed perform all algorithms on the whole data sets. 
The clustering results of algorithm CSM are shown in Fig-ure 6. As shown, algorithm CSM is able to successfully par-tition these data sets. We also applied other implemented algorithms on these data sets. Note that because of their time complexity, hierarchical algorithms, including CURE, single-link, and complete-link, are applied on random sam-pled subsets of some large data sets. Among these algo-rithms, only algorithm CURE can obtain the correct clus-tering results on all data sets. Finally, we conduct a series of experiments on the sensitivity analysis on the value of m. 
In these experiments, it is shown that when the value of m is between 5 and 16, the clustering results are similar to the one shown in Figure 7(a), when the value of m is between 16 and 64, the results are similar to Figure 6(a), and when [1] K. S. Beyer, J. Goldstein, R. Ramakrishnan, and [2] P. S. Bradley, K. P. Bennett, and A. Demiriz. [3] M.-S. Chen, J. Han, and P. S. Yu. Data Mining: An [4] R. C. Dubes. How many clusters are best? -an [5] S. Guha, R. Rastogi, and K. Shim. CURE: An [6] S. Guha, R. Rastogi, and K. Shim. ROCK: A Robust [7] J. Hertz, A. Krogh, and R. G. Palmer. Introduction to [8] A. K Jain, M. N. Murty, and P. J. Flynn. Data [9] C.-R. Lin and M.-S. Chen. On the optimal clustering [10] N. M. Murty and G. Krishna. A hybrid clustering [11] R. T. Ng and J. Han. Efficient and Effective [12] Y.-J. Oyang, C.-Y. Chen, and T.-W. Yang. A study [13] C. R. Palmer and C. Faloutsos. Density biased [14] A. K. H. Tung, J. Han, L. V. S. Lakshmanan, and [15] C.-H. Yun, K.-T. Chuang, and M.-S. Chen. An [16] T. Zhang, R. Ramakrishnan, and M. Livny. BIRCH: 
