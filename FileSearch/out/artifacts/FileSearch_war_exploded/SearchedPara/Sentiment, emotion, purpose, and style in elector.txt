 1. Introduction
Elections, the cornerstone of democratic process, occur across the globe and often involve tens of millions of potential voters. Social media platforms such as Twitter give new opportunities to the electorate, the politicians, news corporations, and other participants to make their voice directly accessible to a large audience. However, the number of posts pertaining to a single event or topic such as a national election can grow to the hundreds of millions. The large number of tweets negates toral tweets, and specifically, analyzing sentiment and emotions in electoral tweets, can be beneficial for a number of down-stream applications:
Understanding the role of target entities : A number of entities tweet during elections, for example, the politicians, the vot-ers, the disenfranchised, news corporations, non-governmental organizations, special interest groups, etc. Analyzing the extent to which tweets from various entities help shape public sentiment will improve our understanding of how social media is used during elections. It is also of interest to identify which portions of the voting electorate tweet about politics during elections. For example, some studies have shown that the more partisan electorate tend to tweet more, as do members from minority groups ( Lassen &amp; Brown, 2011 ).  X 
Determining how public sentiment is shaped : Some tweets (or some sets of tweets) have more impact in shaping public opinion than others. Determining characteristics of influential tweets is particularly useful.

Nowcasting and forecasting : Tweet streams have been shown to help identify current public opinion towards the candi-dates in an election (nowcasting) ( Conover, Goncalves, Ratkiewicz, Flammini, &amp; Menczer, 2011; Golbeck &amp; Hansen, 2011 ). Some research has also shown the predictive power of analyzing electoral tweets to determine the number of votes a candidate will get (forecasting) ( Bermingham &amp; Smeaton, 2011; Lampos, Preotiuc-Pietro, &amp; Cohn, 2013; Tumasjan,
Sprenger, Sandner, &amp; Welpe, 2010a ), however, other research expresses skepticism at the extent to which this is possible ( Avello, 2012 ).

Identifying key electoral issues : Electoral tweets can be analyzed to determine the extent to which voters are concerned about particular issues. For example, does the electorate value economic development much more than environment pro-tection, and to what extent? Other related problems include identifying contentious issues ( Maynard &amp; Funk, 2011 ) and detecting voter polarization ( Conover et al., 2011 ).

Impact of fake tweets : Often during elections there is an increase of artificially generated tweets from twitterbots, botnets, and sock-puppets. Understanding the impact of these tweets on public sentiment and automatic methods to filter out such tweets are both important research problems.

Contributions of this work: Traditional information retrieval systems usually identify facts such as what a person is doing, at what time, in what location, etc. In this paper we analyze electoral tweets for more subtly expressed information such as sentiment (positive or negative), the emotion (joy, sadness, anger, etc.), the purpose or intent behind the tweet (to our knowledge, this is the first tweets dataset annotated for all of these phenomena. We also developed two automatic sta-tistical systems that use the annotated data for training and predict emotion and purpose labels in new unseen tweets. These experiments establish baseline results for automatic systems on this new data.

Data Annotation: We designed two detailed online questionnaires and annotated the tweets by crowdsourcing to Ama-zon X  X  Mechanical Turk. 1 We obtained over 100,000 responses from about 3000 annotators. We present an extensive analysis of the annotations which lend support to interesting conclusions such as electoral tweets almost always express the emotion of the tweeter as opposed to somebody else X  X , the predominant emotion in these tweets is disgust followed by trust, electoral tweets convey negative emotions twice as often as positive emotions, and that different intents of tweeting may be associated with the same emotion. All the data created as part of this project: about 100,000 responses to questions about emotions, purpose, and style in electoral tweets are made available: http://www.purl.org/net/PoliticalTweets2012 .

Automatic Classifiers: We developed a classifier for emotion detection that obtains an accuracy of 56.84%. We show how the stimulus identification task can be framed as a classification task that circumvents more complicated problems of detect-ing entity mentions and coreferences. On this stimulus classification task, our supervised classifier obtains an F-score of 58.30.

We show that emotion detection alone can fail to distinguish between several different types of purpose. For example, the same emotion of disgust can be associated with many different kinds of purpose such as  X  X o criticize X ,  X  X o vent X , and  X  X o rid-icule X . Thus, detecting purpose provides information that is not obtained simply by detecting sentiment or emotion. We developed a preliminary system that automatically classifies electoral tweets as per their purpose, using various features that have traditionally been used in tweet classification, such as word ngrams and elongated words, as well as features per-taining to eight basic emotions. We show that resources developed for emotion detection are also helpful for detecting pur-pose. We then add to this system features pertaining to hundreds of fine emotion categories. We show that these features lead to significant improvements in accuracy above and beyond those obtained by the competitive preliminary system. The system obtains an accuracy of 44.58% on a 11-class task and an accuracy of 73.91% on a 3-class task. The various emotion lexicons are made freely available. 2
The rest of the paper is organized as follows. In Section 2 , we present related work. Section 3 presents the data annotation step and also a detailed analysis of the annotations obtained. In Section 4 , we describe an automatic classifier for detecting emotions (Section 4.1 ), an experiment showing that emotion detection although related to purpose detection is in fact a dif-tions for future work. 2. Related work
Related work is organized into two sub-sections: (1) on annotating text for sentiment, emotion, style, and categories such as purpose, and (2) on automatic classifiers for detecting these categories. 2.1. Annotating text for sentiment, emotion, and style
In the past, separate efforts have been made to annotate text for sentiment, emotion, individual aspects of style such as sarcasm and irony, and categorization by goals.

Sentiment and Emotion: Wiebe and Cardie (2005) annotated sentences in newspaper articles for sentiment (positive, neg-ative, or neutral). Pang and Lee (2008) (Section 7) present a detailed survey of sentiment annotation effort on product reviews and blog posts. Strapparava and Mihalcea (2007) annotated newspaper headlines for sentiment and emotion. They used six emotions that ( Ekman, 1992 ) argued to be the most basic X  X oy, sadness, anger, fear, disgust, and surprise.
Mohammad (2012b) used emotion-word hashtags such as #joy, #sadness, and #anger to compile tweets associated with eight basic emotions proposed by Plutchik (1980)  X  X kman X  X  six, trust, and anticipation. There has also been work on man-ually creating word X  X motion association lexicons, for example, the NRC Emotion Lexicon ( Mohammad &amp; Turney, 2010 ),
WordNet Affect ( Strapparava &amp; Valitutti, 2004 ), and the Affective Norms for English Words. Style: Filatova (2012) annotated Amazon product reviews for sarcasm and irony by crowdsourcing. Davidov, Tsur, and Rappoport (2010) collected tweets and Amazon product reviews and annotated them for sarcasm. Gonz X lez-Ib X  X ez,
Muresan, and Wacholder (2011), Liebrecht, Kunneman, and van den Bosch (2013) used tweets with #sarcasm and #sarcastic as labeled data for sarcasm. Carvalho, Sarmento, Silva, and de Oliveira (2009) labeled newspaper sentences with irony. Reyes,
Rosso, and Veale (2013) used #irony to compile a set of ironic tweets. We manually label tweets into one of eight categories of style including sarcasm, exaggeration, irony, and understatement. To our knowledge, this is the first such dataset.
Purpose-like Categories: There is no related work on detecting purpose from tweets. Below are some tweet annotation efforts for purpose-like categories. Naaman, Boase, and Lai (2010) organized 3379 tweets into the categories of information sharing, self promotion, opinions, statements, me now, questions, presence maintenance, and anecdote. Sriram, Fuhry,
Demir, Ferhatosmanoglu, and Demirbas (2010) annotated 5407 tweets into news, events, opinions, deals and private messages.

Here for the first time, we annotate the same dataset (electoral tweets) for a wide variety of sentiment, emotion, purpose, and style labels. 2.2. Classifiers for sentiment, emotion, and purpose-like categories Over the last decade, there has been an explosion of work exploring sentiment analysis. Surveys by Pang and Lee (2008),
Liu and Zhang (2012), Mart X nez-C X mara, Mart X n-Valdivia, Ure X al X pez, and Montejor X ez (2012) give summaries. (The Marti-nez survey focuses specifically on tweets.) Two state-of-the-art approaches particularly noteworthy and relevant to social media posts are: the NRC-Canada system ( Kiritchenko, Zhu, &amp; Mohammad, 2014; Mohammad, Kiritchenko, &amp; Zhu, 2013a;
Zhu, Kiritchenko, &amp; Mohammad, 2014 ), which uses many lexical-features including those from sentiment lexicons, and often benefit from the same kinds of features and system configurations as used in sentiment classifiers, we build our emo-tion classifier by drawing heavily from the NRC-Canada sentiment analysis system.

Sentiment : The NRC-Canada sentiment system was designed to detect the sentiment of short informal textual messages such as tweets. The system is based on a supervised statistical text classification approach leveraging a variety of surface-form, semantic, and sentiment features generated from word and character ngrams, manually created and automatically generated sentiment lexicons, parts of speech, word clusters, and Twitter-specific encodings such as hashtags and creatively spelled words and abbreviations ( yummeee, lol, etc. ). The sentiment features are primarily derived from novel high-coverage tweet-specific sentiment lexicons. These lexicons are automatically generated from tweets with sentiment-word hashtags and from tweets with emoticons. The system achieved the best results in SemEval-2013 shared task on Sentiment Analysis in Twitter (Task 2) ( Wilson et al., 2013 ) and again in 2014 when the shared task was repeated (Task 9) ( Rosenthal, Nakov, Ritter, &amp; Stoyanov, 2014 ).

More complex approaches, such as recursive deep models ( Socher et al., 2012, 2013 ) work in a bottom-top fashion over a parse-tree structure of a sentence to infer the sentiment label of the sentence as a composition of the sentiment expressed by its constituting parts: words and phrases. These models do not require any hand-crafted features or semantic knowledge, such as a list of negation words. However, they are computationally intensive and need substantial additional annotations (word and phrase-level sentiment labeling) to produce competitive results.
 Emotion : Automatic emotion detection has been proposed for many different kinds of text ( Aman &amp; Szpakowicz, 2007; &amp; Matsumoto, 2008; Zhe &amp; Boucouvalas, 2002 ). More recently there has been work on tweets as well ( Bollen, Pepe, &amp; Mao, 2009; Choudhury, Counts, &amp; Gamon, 2012; Kim, Gilbert, Edwards, &amp; Graeff, 2009; Mohammad, 2012b; Tumasjan, confusion in tweets. Tumasjan et al. (2010b) study Twitter as a forum for political deliberation. Mohammad (2012b) developed a classifier to identify emotions using tweets with emotion word hashtags as labeled data. However, none of this work explores the many semantic roles of emotion such as the who is feeling, what emotion, and towards whom.
Who, what sentiment/emotion, and towards whom/what : Detecting who is feeling, what emotion, and towards whom is essentially a semantic role-labeling problem ( Gildea &amp; Jurafsky, 2002 ). The semantic frame for  X  X motions X  in FrameNet text. Nonetheless, it is usually easy to deduce. Other roles such as Reason, Degree, and Event are also of significance, and remain suitable avenues for future work.

To the best of our knowledge, there exists no work on semantic role labeling of emotions in tweets. However, there is work on the related task of extracting opinions and the topics of opinions. Much of this work is focused on opinions about
Sentiment Analysis (Task 4), automatic systems had to detect sentiment towards aspects terms in the Restaurant and Laptop domains.
 Consider: The lasagna was great, but the service left a lot to be desired.

We can gather from this sentence that the customer has a positive sentiment towards the lasagna, but a negative senti-ment towards the service. With over 40 teams participating, the NRC-Canada system ( Kiritchenko, Zhu, Cherry, &amp;
Mohammad, 2014 ) obtained the best results overall. We adapt this system to detect the stimulus of emotions in the electoral tweets data.
 Purpose: To the best of our knowledge, there is no work yet on classifying electoral or political tweets by purpose. Collier,
Son, and Nguyen (2011) classified flu-related tweets into avoidance behavior, increased sanitation, seeking pharmaceutical intervention, wearing a mask, and self reported diagnosis. Caragea et al. (2011) classified earthquake-related tweets into medical emergency, people trapped, food shortage, water shortage, water sanitation, shelter needed, collapsed structure, food distribution, hospital/clinic services, and person news. There exists work on determining political alignment of tweeters ( Conover, Goncalves, et al., 2011; Golbeck and Hansen, 2011 ), identifying contentious issues and political opinions ( Maynard timent in political tweets ( Bermingham &amp; Smeaton, 2011; Chung &amp; Mustafaraj, 2011; O X  X onnor, Balasubramanyan, Routledge, &amp; Smith, 2010 ).

Other electoral tweets classifiers for downstream applications : Automatic analysis, especially of sentiment, emotion, and purpose, has applications such as determining political alignment of tweeters ( Conover, Goncalves, et al., 2011; Golbeck and Hansen, 2011 ), identifying contentious issues and political opinions ( Maynard &amp; Funk, 2011 ), detecting the amount of polarization in the electorate ( Conover, Ratkiewicz, et al., 2011 ), and even predicting the voting intentions or outcome of elections ( Bermingham &amp; Smeaton, 2011; Lampos et al., 2013; Tumasjan et al., 2010a ). One of the more recent works ( Lampos et al., 2013 ) analyzes tweets from UK and Austria and successfully predicts voting intention in more than 300 polls across the two countries. They filter out irrelevant tweets using a bilinear model on both the word space and the user space.
In this paper, our goal is to annotate a single electoral tweets dataset with a number of sentiment-, emotion-, and pur-pose-related labels, and establish baselines on what certain automatic classifiers can achieve when made to automatically identify these labels in previously unseen test data. We hope that the dataset and the classifiers will be useful in many of the downstream applications such as those listed above and earlier in the Introduction.
 3. Data collection and annotation for sentiment, emotions, purpose, and style
In the subsections below we describe how we collected tweets pertaining to the 2012 US presidential elections and annotated them for sentiment, emotions, purpose, and style. 3.1. Identifying electoral tweets
We created a corpus of tweets by polling the Twitter Search API, during August and September 2012, for tweets that con-tained commonly known hashtags pertaining to the 2012 US presidential elections. Table 2 shows the query terms we used.
Apart from 21 hashtags, we also collected tweets with the words Obama, Barack, or Romney. We used these additional terms because they are names of the two presidential candidates, and the probability that these words were used to refer to somebody else in tweets posted in August and September of 2012 was low.

The Twitter Search API was polled every four hours to obtain new tweets that matched the query. Close to one million tweets were collected, which we make freely available to the research community. The query terms which produced the highest number of tweets were those involving the names of the presidential candidates, as well as #election2012, #campaign, #gop, and #president.

We used the metadata tag  X  X  X so_language_code X  X  to identify English tweets. Since this tag is not always accurate, we also discarded tweets that did not have at least two valid English words. We used the Roget Thesaurus as the English word inven-tory. This step also helps discard very short tweets and tweets with a large proportion of misspelled words. We discarded retweets, which can easily be identified through the presence of RT, rt, or Rt in the tweet (usually in the beginning of the post). Finally, there remained close to 170,000 original English tweets. 3.2. Annotating emotions through crowdsourcing
We used Amazon X  X  Mechanical Turk and CrowdFlower to crowdsource the annotation of the electoral tweets. tionnaires posted on Mechanical Turk are called HITs (human intelligence tasks) . CrowdFlower acts as intermediary between job requesters and crowdsourcing platforms such as Mechanical Turk and SamaSource. It provides additional quality control measures such as a mechanism to avoid annotations from annotators who consistently fail to correctly respond to certain gold questions X  X uestions for which the job requester has provided answers. Crowdflower recommends that the job requester provide gold answers to questions in 5 X 10% of the HITs.
 We randomly selected about 2000 tweets, each by a different Twitter user. We set up two questionnaires on Mechanical
Turk through CrowdFlower. The questions to be included in the two questionnaires as well as the options to be provided within each question were established after soliciting responses from our colleagues at the National Research Council
Canada. We restricted annotations to Turkers from the United States. We also requested that only native speakers of English attempt the HITs.

The first questionnaire was used to determine the presence of emotions in a tweet, the style of the tweet, and the purpose of the tweet. It also had a question to verify whether the tweet was truly relevant to US politics. Below is an example: Questionnaire 1: Emotions in the US election tweets Tweet: Mitt Romney is arrogant as hell.
 Q1. Which of the following best describes the Emotions in this tweet?
Q2. Which of the following best describes the Style of this tweet?
Q3. Which of the following best describes the Purpose of this tweet? Q4. Is this tweet about US politics and elections? We posted 2042 HITs corresponding to 2042 tweets. We requested responses from at least three annotators for each HIT.
The response to a HIT by an annotator is called an assignment . In Mechanical Turk, an annotator may provide assignments for as many HITs as they wish. Thus, even though only three annotations are requested per HIT, about 400 annotators contrib-uted assignments for the 2,042 tweets.

Observe that we implicitly grouped the options for Q3 into three coarse categories by putting extra vertical space between the groups. These coarse categories correspond to oppose (to point out hypocrisy, to point out mistake, to disagree, to ridicule, to criticize, to vent), favor (to agree, to praise, to support), and other . Even though there is some redundancy among the fine categories, they are more precise and may help annotation. Eventually, however, it may be beneficial to com-bine two or more categories for the purposes of automatic classification. The amount of combining will depend on the task at hand, and can be done to the extent that anywhere from eleven to two categories remain.

The tweets that were marked as having one emotion were chosen for annotation by Questionnaire 2. Here we asked various questions pertaining to emotional state such as who is feeling the emotion, what emotion, towards whom, and more. We decided to classify the emotions in tweets into one of eight basic emotion categories proposed by Plutchik.
However, these categories are fairly coarse. For example, disgust, dislike, hate, disappointment, and indifference all fall under the category of disgust. In order to make annotation easier, we presented the Turkers with a larger list of 19 pos-sible emotions. The annotations for 19 emotions were eventually mapped into the 8 basic emotions as follows: trust, acceptance, admiration, and like were mapped to trust; fear mapped to fear; surprise, uncertainty, amazement to surprise; sadness mapped to sadness; disgust, dislike, hate, disappointment, and indifference mapped to disgust; anger to anger; anticipation and vigilance to anticipation; joy and calmness to joy. The data analysis and automatic emotion classification we present in the following sections all work at the level of the eight basic emotions. Below is an example of Question-naire 2: Questionnaire 2: Who is feeling what, and towards whom? Tweet: Mitt Romney is arrogant as hell.
 Q1. Who is feeling or who felt an emotion?
Q2. What emotion? Choose one of the options from below that best represents the emotion. Q3. If there is a better word for describing the emotion (than the ones listed above), then type it here:
Q4. If when answering Q2 you have chosen an emotion from the  X  X  X ther emotions X  X  category or if you answered Q4, then
Q5. How strongly is the emotion being expressed in this tweet? Q6. Towards whom or what? In other words, who or what is the stimulus of the emotion? Q7. Which words in the tweet help in identifying the emotion? Q8. What reason can be deduced from the tweet for the emotion? What is the cause of the emotion? Q9. This tweet is about which of the following issues: Q10. If the tweet is about an issue not listed above, then type it here: We requested responses from at least five annotators for each of these HITs.

After performing a small pilot annotation effort, we realized that the stimulus in most of the electoral tweets was one among a handful of entities. Thus we reformulated question 6 as shown below: Q6b. Which of these best describes the target of the emotion? Barack Obama and/or Joe Biden Mitt Romney and/or Paul Ryan Some other individual Democratic party, democrats, or DNC Republican party, republicans, or RNC Some other institution Election campaign, election process, or elections The target is not specified in the tweet None of the above.

Even though it is possible that more than one option may apply for a tweet, we allowed the Turkers to select only one option for each question. We did this to encourage annotators to select the option that best answers the questions. We wanted to avoid situations where an annotator selects multiple options just because they are vaguely relevant to the ques-tion. Before going live, the survey was approved by the ethics committee at the National Research Council Canada. Both questionnaires, in exactly the form the Turkers saw it, are made publicly available. 3.3. Annotation analyses
Apart from the quality control measures employed by CrowdFlower (the use of gold questions), we deployed additional measures to discard poor annotations. For each annotator and for each question, we calculated the probability with which the annotator agreed with the response chosen by the majority of the annotators. We identified poor annotators as those that had an agreement probability that was more than two standard deviations away from the mean. All annotations by these annotators were discarded. 6
We determine whether a tweet is to be assigned a particular category based on strong majority. That is, a tweet belongs to category X only if more than half of the annotators agree with each other. A minimum of at least two annotations per tweet are required for Questionnaire 1, whereas a minimum of at least three annotations per tweet are required for Questionnaire 2.

Percentage of tweets in each of the five categories of Q1 is shown in Table 3 . Observe that the majority category for Q1 is  X  X uggests an emotion X  X 87.98% of the tweets were identified as having an emotional attitude.

Table 4 gives the distributions of the various options for question 2 (style). Simple statements are predominant in elec-toral tweets, but a fair percentage of these tweets correspond to exaggeration and sarcasm. None of the tweets was marked as being an understatement.

Analysis of responses to Q3 revealed that the category  X  X o motivate or to incite action X  was often confused with the cat-egory  X  X o support X . Thus we merged the two categories into one. Also, the category  X  X o be entertaining X  was highly confused with many other options, thus we ignored this category completely. Percentage of tweets in eleven categories of Q3 is shown in Table 5 . Observe that the majority category for purpose is  X  X o support X  X 26.49%. Fig. 1 gives the distributions of the three coarse categories of purpose. Observe, that the political tweets express opposition (58.07%) much more often than favor (31.76%).
 Responses to question 4, revealed that a large majority (95.56%) of the tweets are relevant to US politics and elections. Thus the hashtags shown earlier in Table 2 were effective in identifying political tweets.

As mentioned earlier, only those tweets that were marked as having an emotion (with high agreement) were annotated further through Questionnaire 2. 7 Responses to Q1 of Questionnaire 2, showed that in the vast majority of the instances (99.825%), the tweets contain emotions of the tweeter. The data did include some tweets that referred to emotions of others (Romney, GOP, and president), but these instances were rare.
Figs. 2 and 3 give the distributions of the various options for Questions 2 and 6b of Questionnaire 2. Disgust (47.75%) is by far the most dominant emotion in the tweets of 2012 US presidential elections. The next most prominent emotion is that of trust (24.03%). About 58% of the tweets convey negative emotions towards someone or something. Fig. 3 shows that the stimulus of emotions was often one of the two presidential candidates (close to 55% of the time) X  X bama: 29.90%, Romney: 24.87%.

Figs. 4 and 5 give the distributions of the options for Questions 4 and 5, respectively. Observe that most of the emotion expressions are of medium intensity (65.51% of the tweets). Question 5 was optional, and annotators were asked to respond only if the emotion chosen in question 2 is not one of the given positive or negative emotions. Fig. 5 shows that negative emotions were more dominant than the positive ones in the electoral tweets.

Table 6 gives the distributions of the options for Question 9. Observe that the distribution is highly skewed towards the responses, and so we do not show their distributions. 3.3.1. Inter-annotator agreement
We calculated agreement statistics on the full set of annotations, and not just on the annotations with a strong majority as described in the previous section. Table 7 shows inter-annotator agreement (IAA) for the questions X  X he average percentage of times two annotators agree with each other. Another way to gauge agreement is by calculating the average probability with which an annotator picks the majority class. The last column in Table 7 shows the average probability of picking the majority class (APMS) by the annotators (higher numbers indicate higher agreement). Observe that there is high agreement on deter-mining whether a tweet has an emotion or not, and on determining whether the tweet is related to the 2012 US presidential elections or not. The questions in Questionnaire 2 pertaining to the emotional state, stimulus, etc. were less straightforward and tend to require more context than just the target tweet for a clear determination, but yet the annotations had moderate agreement.
 4. Automatically detecting emotions and purpose
We now present automatic classifiers that use some of the annotations described above as training data and predict emo-tions and purpose in unseen test tweets. In the subsections below we present: (1) a basic automatic system to determine who is feeling what emotion, and towards whom (Section 4.1 ) (2) the correlations and distinctions between emotions and purpose (Section 4.2), (3) a basic automatic system to automatically classify tweets into eleven categories of purpose (Section 4.3 ). The objective of these experiments is to establish baseline results on this new electoral tweets dataset using features from state-of-the-art sentiment analysis systems, and also to establish the relationship between purpose and emo-tions in electoral tweets. We leave the automatic determination of style as well as other aspects of affect such as determining the reason behind the emotion for future work. The data and annotations are made freely available. 4.1. Automatically detecting semantic roles of emotions in tweets
Since in most instances (99.83%) the experiencer of emotions in a tweet is the tweeter, in this section we focus on auto-matically detecting the other two semantic roles: the emotional state and the stimulus.

We treat the detection of emotional state and stimulus as two subtasks for which we train state-of-the-art support vector machine (SVM) classifiers. SVM is a learning algorithm proved to be effective on many classification tasks and robust on large feature spaces. In our experiments, we exploited several different classifiers and found that SVM outperforms others such as maximum-entropy models (i.e., logistic regression). We also tested the most popular kernels such as the polynomial and RBF kernels with different parameters in 10-fold cross validation. We found that a simple linear kernel yielded the best perfor-mance. We used the LibSVM package ( Chang &amp; Lin, 2011a ).

Our system builds on the classifiers and features used in two previous systems: (1) the system described in Mohammad (2012b) which was shown to perform significantly better at emotion detection than some other previous systems on the news paper headlines corpus and (2) the system described in ( Mohammad, Kiritchenko, &amp; Zhu, 2013b ) which ranked first (among 44 participating teams) in a 2013 SemEval competition on detecting sentiment in tweets (Task 2) ( Wilson et al., 2013 ). In each experiment below, we report results of ten-fold stratified cross-validation. 4.1.1. Detecting emotional state Below we present the features used for detecting emotional state in electoral tweets and the results obtained with them.
Features: We included the following features for emotional state in tweets, where emotional state can be one of eight possible basic emotions: joy, sadness, anger, fear, surprise, anticipation, trust, and disgust.

Word ngrams : unigrams (single words) and bigrams (two-word sequences). All words were stemmed with Porter X  X  stemmer ( Porter, 1980 ).
 Punctuations : number of contiguous sequences of exclamation marks, question marks, or a combination of them.
Elongated words : the number of words with the ending character repeated more than 3 times, e.g.,  X  X  X oooo X  X  and  X  X  X annnnnn X  X . Elongated words have been used similarly by Brody and Diakopoulos (2011) .

Emoticons : presence/absence of positive and negative emoticons. The emoticon and its polarity were determined through a simple regular expression adopted from Christopher Potts X  tokenizing script. past for sentiment analysis applications by Go, Bhayani, and Huang (2009) and Mohammad et al. (2013a) .
Emotion Lexicons : We used the NRC word X  X motion association lexicon ( Mohammad &amp; Turney, 2010 ) to check if a tweet contains emotional words. The lexicon contains human annotations of emotion associations for about 14,200 word types.
The annotation includes whether a word is positive or negative (sentiments), and whether it is associated with the eight basic emotions (joy, sadness, anger, fear, surprise, anticipation, trust, and disgust). If a tweet has three words that have associations with emotion joy, then the LexEmo emo joy feature takes a value of 3. The NRC lexicon was used for emotion classification of newspaper headlines by Mohammad (2012a) , and such emotion lexicon features have been generated from WordNet Affect in earlier works ( Alm &amp; Ovesdotter, 2008; Aman &amp; Szpakowicz, 2007 ).

Negation features : We examined tweets to determine whether they contained negators such as no, not, and should n X  X  .An additional feature determined whether the negator was located close to an emotion word (as determined by the emotion lexicon) in the tweet and in the dependency parse of the tweet. The list of negation words was adopted from Christopher Potts X  sentiment tutorial. 9 Negation features of this kind were used earlier by Polanyi and Zaenen (2004), Kennedy and Inkpen (2005), Choi and Cardie (2008), Taboada, Brooke, Tofiloski, Voll, and Stede (2011) .

Position features : We included a set of position features to capture whether the feature terms described above appeared at the beginning or the end of the tweet. For example, if one of the first five terms in a tweet is a joy word, then the feature LexEmo joy begin was triggered.

Combined features Though non-linear models like SVM (with non-linear kernels) can capture interactions between fea-tures, we explicitly combined some of our features. For example, we concatenated all emotion categories found in a given tweet. If the tweet contained both surprise and disgust words, a binary feature LexEmo _ surprise _ disgust was triggered. Results: Table 8 shows the results. We use accuracy as the evaluation metric. Note that accuracy equals micro-averaged P,
R, and F in this case as the categories are mutually exclusive. We included two baselines: the random baseline corresponds to a system that randomly guesses the emotion of a tweet, whereas the majority baseline assigns all tweets to the majority category (disgust). Since the data is significantly skewed towards disgust, the majority baseline is relatively high.
The automatic system obtained an accuracy of 56.84%, which is significantly higher than the majority baseline. It should be noted that the highest scores in the SemEval 2013 task for detecting sentiment of tweets (Task 2) was around 69% ( Mohammad et al., 2013b; Wilson et al., 2013 ). That task even though related involved only three classes (positive, negative, and neutral). Thus it is not surprising that for an 8-way classification task, the performance is somewhat lower. Further anal-ysis showed that our system obtains highest accuracies on the categories of disgust (71.86%) and trust (54.51%). These were here) revealed that the positive emotions (joy and trust) tended to be confused for each other and many of tweets with non-disgust negative emotions were often marked as disgust.

As mentioned earlier, human annotators do not always agree with each other. To estimate the human performance, for each tweet we randomly sample a human annotation from its multiple annotations. We compare it with the majority cat-egory chosen from the remaining human annotations for that tweet. Such sampling is conducted over all tweets and then evaluated. The resulting accuracy is 69.80%.

Table 9 shows the results of ablation experiments X  X he accuracies obtained with one of the feature groups removed. The higher the drop in performance, the more useful is that feature. Observe that the ngrams are the most useful features, followed by the emotion lexicons. Most of the gain is due to word ngrams, but character ngrams provide small additional gains as well. 10 The NRC emotion lexicon improved results as well. Paying attention to negation was also beneficial, however, emotional encodings such as elongated words, emoticons, and punctuations did not help much. It is possible that much of the discriminating information they might have is already provided by unigram and character ngram features. 4.1.2. Detecting emotion stimulus
As discussed earlier, instead of detecting and labeling the original text spans, we ground the emotion stimulus directly to the predefined entities. This allows us to circumvent mention detection and co-reference resolution on linguistically less well-formed text. We treat the problem as a classification task, in which we classify a tweet into one of the eight categories defined in Fig. 3 . The categories of  X  X arget not specified X  and  X  X one of the above X  were combined to form the negative class. Features: We used the features listed below for detecting emotion stimulus: Word ngrams : Same as described earlier for emotional state.

Lexical features : We collected lexicons that contain a variety of words and phrases describing the categories in Fig. 3 . For example, the Republican party may be called as  X  X  X op X  X  or  X  X  X rand Old Party X  X ; all such words or phrases are put into the lexicon called  X  X  X epublican X  X . We counted how many words in a given tweet are from each of these lexicons.
Hashtag features : Hashtags related to the U.S. election were collected. We organized them into different categories and use them to further smooth the sparseness. For example, #4moreyear and #obama are put into the same hashtag lexicon and any occurrence of such hashtags in a tweet triggers the feature hashtag _ obama _ generalized , indicating that this is a gen-eral version of hashtag related to president Barack Obama.
 Position features : Same as described earlier for emotional state.

Combined features : As discussed earlier, we explicitly combined some of the above features. For example, we first concat-enate all lexicon and hashtag categories found in a given tweet X  X f the tweet contains both the general hashtag of  X  X  X bama X  X  and  X  X  X omney X  X , a binary feature  X  X  X ashtag_general_ obama_romney X  X  takes the value of 1.

Results: Table 10 shows the results on this task. Overall, the system obtains an F-measure of 58.30. The table also shows baselines calculated just as described earlier for the emotional state category. We added results for an additional baseline, rule-based system , here that chose the stimulus to be: Obama if the tweet had the terms obama or #obama ; Romney if the tweet had the terms romney or #romney ; Republicans if the tweet had the terms republican, republicans, or #republicans ;
Democrats if the tweet had the terms democrats, democrat, or #democrats; and Campaign if the tweet had the terms #elec-tion or #campaign . If two or more of the above rules were triggered in the same tweet, then a label was chosen at random.
This rule-based system obtained an F-score of only 48.62, showing that there were many tweets where key words alone were not sufficient to disambiguate the true stimulus. Observe that the SVM-based automatic system performs markedly better than the majority baseline and also the rule-based system baseline. 4.2. Distinctions between emotion and purpose
The task of detecting purpose is related to sentiment and emotion classification. Intuitively, the three broad categories of purpose,  X  X ppose X ,  X  X avor X , and  X  X ther X , roughly correspond to negative, positive, and objective sentiment. Also, some fine-grained categories seem to partially correlate with emotions. For example, when angry, a person vents. When overcome with admiration, a person praises the object of admiration.

Since the tweets are annotated for both emotion and purpose, we can investigate the relationship between the two. Fig. 6 shows the percentage of tweets pertaining to different categories of emotion and purpose. The tweets with the purpose  X  X avor X  ( X  X upport X  and  X  X raise X ) mainly convey the emotions of admiration, anticipation, and joy. On the other hand, the tweets disgust. The purpose  X  X o praise, admire, or appreciate X  is highly correlated with the emotion admiration. criticize X , and even many instances of  X  X o vent X  are associated with the emotion dislike. Thus, a system that only determines emotion and not purpose will fail to distinguish between these different categories of purpose. It is possible for people to have the same emotion of dislike and react differently: either by just disagreeing, pointing out the mistake, criticizing, or resorting to ridicule. 4.3. Automatically identifying purpose
To automatically classify tweets into eleven categories of purpose ( Table 5 ), we trained a Support Vector Machine (SVM) classifier. The eleven categories were assumed to be mutually exclusive, i.e., each tweet was classified into exactly one cat-egory. In the second set of experiments, the eleven fine-grained categories were combined into 3 coarse-grained  X   X  X ppose X , and the results were averaged. Paired t-test was used to confirm the significance of the results. We used the LibSVM package ( Chang &amp; Lin, 2011b ) with linear kernel and default parameter settings. Parameter C was chosen by cross-validation on the training portion of the data (i.e., the nine training folds).

The gold labels were determined by strong majority voting. Tweets with less than 2 annotations or with no majority labels were discarded. Thus, the dataset consisted of 1072 tweets for the 11-category task, and 1672 tweets for the 3-cate-gory task. The tweets were normalized by replacing all URLs with http://someurl and all userids with @someuser. The tweets were tokenized and tagged with parts of speech using the Carnegie Mellon University Twitter NLP tool ( Gimpel et al., 2011 ). 4.3.1. A basic system for purpose classification
Features: Each tweet was represented as a feature vector with the following groups of features. ngrams: presence of ngrams (contiguous sequences of 1, 2, 3, and 4 tokens), skipped ngrams (ngrams with one token replaced by *), character ngrams (contiguous sequences of 3, 4, and 5 characters);
POS: number of occurrences of each part-of-speech; word clusters: presence of words from each of the 1000 word clusters provided by the Twitter NLP tool ( Gimpel et al., 2011 ). These clusters were produced with the Brown clustering algorithm on 56 million English-language tweets. They serve as alternative representation of tweet content, reducing the sparcity of the token space; all-caps: the number of words with all characters in upper case;
NRC Emotion Lexicon:  X  number of words associated with each emotion  X  number of nouns, verbs, etc., associated with each emotion  X  number of all-caps words associated with each emotion  X  number of hashtags associated with each emotion negation: the number of negated contexts. Following ( Pang, Lee, &amp; Vaithyanathan, 2002 ), we defined a negated context as a segment of a tweet that starts with a negation word (e.g.,  X  X o X ,  X  X houldn X  X  X ) and ends with one of the punctuation marks: emotion in a negated context become negated (e.g.,  X  X ot perfect X  becomes  X  X ot perfect_NEG X ,  X  X MOTION_trust X  becomes  X  X MOTION_trust_NEG X ). The list of negation words was adopted from Christopher Potts X  sentiment tutorial. punctuation: the number of contiguous sequences of exclamation marks, question marks, and both exclamation and question marks; emoticons: presence/absence of positive and negative emoticons. The polarity of an emoticon was determined with a simple regular expression adopted from Christopher Potts X  tokenizing script. hashtags and elongated word: the number of hashtags and the number of words with one character repeated more than 2 times, e.g.  X  X oooo X .

Results: Table 11 presents the results of the automatic classification for the 11-category and 3-category problems. For comparison, we also provide the accuracy of a simple baseline classifier that always predicts the majority class. The percent-age of error reduction over the baseline is 23.22% for the 11-category classification and 37.78% for the 3-category classification.

Table 12 shows the classification results broken-down by category. As expected, the categories with larger amounts of labeled examples ( X  X o praise X ,  X  X o support X ,  X  X o provide information X ) have higher results. However, for one of the higher fre-quency categories,  X  X o ridicule X , the F-score is relatively low. This category incorporates irony, sarcasm, and humor, the con-cepts that are hard to recognize, especially in a very restricted context of 140 characters. The four low-frequency categories sifier to build adequate models. The categories within  X  X ppose X  are more difficult to distinguish among than the categories within  X  X avor X . However, for the most part this can be explained by the larger number of categories (6 in  X  X ppose X  vs. 3 in  X  X avor X ) and, consequently, smaller sizes of the individual categories.

In the next set of experiments, we investigated the usefulness of each feature group for the task. We repeated the above classification process, each time removing one of the feature groups from the tweet representation. Table 13 shows the results of these ablation experiments for the 11-category and 3-category problems. In both cases, the most influential fea-tures were found to be ngrams and emotion lexicon features.
 4.3.2. Adding features pertaining to hundreds of fine emotions
Since the emotion lexicon had a significant impact on the results, we further created a wide-coverage tweet-specific lex-ical resource following on work by Mohammad (2012b) . Mohammad (2012b) showed that emotion-word hashtagged tweets are a good source of labeled data for automatic emotion processing. Those experiments were conducted using tweets per-taining to the six Ekman emotions because labeled evaluation data exists for only those emotions. However, a significant advantage of using hashtagged tweets is that we can collect large amounts of labeled data for any emotion that is used as a hashtag by tweeters. Thus we polled the Twitter API and collected a large corpus of tweets pertaining to a few hundred emotions.

We used a list of 585 emotion words compiled by Zeno G. Swijtink as the hashtagged query words. not to dwell on the question of whether each of the words in this set is truly an emotion or not. Our goal was to create and distribute a large set of emotion-labeled data, and users are free to choose a subset of the data that is relevant to their application.

Given a dataset of sentences and associated emotion labels (emotion-word hashtags), we computed the Strength of Asso-ciation (SoA) between a word w and an emotion e to be: where PMI is the pointwise mutual information.
 freq  X  e  X  is the total number of tokens in sentences with label e .
 N is the number of word tokens in the corpus. Similarly: tokens in sentences that do not have the label e . Thus, Eq. 1 is simplified to:
Since PMI is known to be a poor estimator of association for low-frequency events, we ignore words that occur less than five times in the corpus.

If a word tends to occur more often in a tweet with a particular emotion label, than in a tweet that does not have that label (emotion word hashtag), then that word X  X motion pair will have an SoA score that is greater than zero. the pairs (word, emotion) that had positive SoA were pulled together into a new word X  X motion association resource, that we call Hashtag Emotion Lexicon . The lexicon contains around 10,000 words with associations to 585 emotion-word hashtags.
We used the Hashtag Lexicon for classification by creating a separate feature for each emotion-related hashtag, resulting in 585 emotion features. The values of these features were calculated as the sum of the PMI scores between the words in a tweet and the corresponding emotion-related hashtag. Table 14 shows the results of the automatic classification using the new lexical resource. The Hashtag Lexicon significantly improved the performance of the classifier on the 11-category task.
Even better results were obtained when both lexicons were employed. 5. Conclusions
Given that social media is playing a growing role in elections world wide, automatically analyzing posts on platforms such as Twitter has a number of applications such as determining support for political parties or candidates, identifying stance of various groups on key electoral issues, determining amount of voter polarization, detecting impact of mass tweets from set labeled for various affectual phenomena. Here, for the first time, we collected and annotated a common dataset (2012 US presidential elections tweets) for a number of labels pertaining to sentiment, emotions, style, and purpose. We designed questionnaires specifically for annotation on a crowdsource platform. We analyzed the data to show that electoral tweets are rich in emotions and mostly convey the feelings of the tweeters themselves. The predominant emotion in these tweets is disgust followed by trust. Electoral tweets convey negative emotions twice as often as positive emotions.
We also developed supervised automatic classifiers for detecting emotional state, emotion stimulus, and purpose (or intent) of the tweets. These classifiers used many of the annotations described above for training. The results establish base-lines for automatic systems on this new data. We show that even though the purpose classifier benefits from emotion fea-tures, emotion detection alone can fail to distinguish between several different types of purpose. For example, the same emotion of disgust can be associated with many different kinds of purpose such as  X  X o criticize X ,  X  X o vent X , and  X  X o ridicule X . Thus, detecting purpose provides information that is not provided simply by detecting sentiment or emotion.
All of the electoral tweets and associated annotations are made freely available. possible to obtain even better results by modeling user behavior based on multiple past tweets. Another avenue for future tions of tweets by purpose differ across developed and developing world. We are interested in using purpose-annotated tweets as input in a system that automatically summarizes political tweets. We are also interested in automatically identifying other semantic roles of emotions such as degree, reason, and empathy target (described in Table 1 ).
 Appendix A
The histograms of the annotations from Questionnaires 1 and 2 are shown in Tables 15 and 16 . Note that since the tweets with gold questions were used for quality control by CrowdFlower, some tweets were annotated more than three times in
Questionnaire 1 and more than 5 times in Questionnaire 2. Only tweets with at least three annotations for Questionnaire 2 and with sufficient agreement among the annotators for emotional state, emotional stimulus, and purpose were used as training and test instances in the automatic classification experiments.
 References
