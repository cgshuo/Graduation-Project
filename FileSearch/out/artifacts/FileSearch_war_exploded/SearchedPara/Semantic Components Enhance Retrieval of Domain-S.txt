 We seek to leverage knowledge a bout information organization in a domain to effectively and efficiently meet targeted information needs of expert users. Th e semantic components model represents document content in a manner that is complementary to full text and keyword indexing. Semantic component instances are segments of text about a partic ular aspect of the main topic of the document and may not correspond to structural elements in the document. This paper desc ribes the semantic components model and presents experimental evidence from a large interactive searching study show ing that semantic components, used to supplement full text and keyword indexing and to extend the query language, enhanced the retrieval of domain-specific documents in response to realistic queries posed by real users. H.3.1 [Information Storage and Re trieval]:Content Analysis and Indexing H.3.3 -Indexing methods [Information Storage and Retrieval]: Information Search and Retrieval -Search process H.3.7 [Information Storage and Retrieval]: Digital Libraries. Experimentation. Semantic components. Some information needs of domai n experts are highly specific and precision-oriented in that the need can be satisfied by just one, or a few, documents to answer a ques tion or to support a decision. Consider, for example, Kirsten, a hypothetical family practitioner who is seeing a long-standing patient with severe asthma who is newly pregnant. Kirsten needs to decide whether to continue her patient X  X  current medications or to change to a different regime to protect the fetus. Kirsten does not need documents that describe asthma, or documents that recomme nd diagnostic tests for asthma. She knows her patient has asthma. She needs a document that describes the safety, or risks, of specific asthma medications in pregnant women, and she needs to find it quickly because she is only allotted ten minutes for the entire patient visit. Information retrieval (IR) systems for domain-specific digital libraries typically return documents based on scores related to matching the words in user queries to document repres entations consisting of words extracted from document text (full text indexing) or keywords, usually assigned from a controlled vocabulary (keyword indexing). Our work seeks to supplement exis ting representations of document content by exploiting domain-speci fic characteristics of document types and content. Domain e xperts like Kirsten often have extensive knowledge about how in formation in the domain is typically organized and expressed within documents [1, 2]. They often understand how entities in the domain relate to each other and know what kind of information will satisfy a particular information need. Domain experts may also have considerable knowledge about the types of documents that exist in the domain and what types of documents can be found in a particul ar digital library. The goal of our work is to leverage expert s X  knowledge about information to more effectively and efficiently meet their information needs. We have previously introduced the semantic components model for representing document content in a way that complements existing indexing techniques [3, 4] and demonstrated that the model can be used to represent information needs [4]. The semantic components model le verages information about document types and the kinds of information they contain and allows users to express queries against domain-specific components of documents as we ll as against w hole documents. In this paper we report on the retrieval performance of an implementation of the semantic components model on top of an existing information retrieval sy stem. Our general goal was to answer the question: Can semantic components aid precision-oriented searches by im proving document ranking ? To do this, we compared two systems, one with semantic components and one without, in an empirical, realis tic, interactive searching study. We studied domain experts in a controlled setting while they searched for documents relevant to four search scenarios designed to mimic real information needs. The study involved users and documents from sundhed.dk [5], the national Danish health portal. Intended for both healthcare professionals and citizens in De nmark, it has been operational since 2001 and contains nearly 25,000 documents. Sundhed.dk uses a combination of full text indexing and manual keyword indexing with both uncontrolled terms and terms from three health-related controlled vocabular ies. Our  X  X ontrol X  system (System 1) used the existing full text and keyword indexing. The  X  X xperimental X  system (System 2) used the existing indexing plus semantic components. In this experiment we st udied semantic components in the medical domain, but the semantic components model itself is not limited to any particular domain. The contributions of this paper are: 1. A description of a prototype im plementation of the semantic 2. The results of a searching study, in which 30 physician users The remainder of the paper is organized as follows. We provide an overview of the semantic com ponents model in Section 2. In Section 3 we review areas of rela ted work. In Sections 4 and 5 we describe our experi mental methods and results, respectively. We discuss the results in Secti on 6 and conclude in Section 7. Documents in domain-specific collections can be classified into document classes by grouping documents that will tend to contain the same kinds of information. Documents may be classified by the type of topic that is the main focus or by the main purpose of the document. The appropriate classification sc heme depends on the nature of the document collec tion and the domain. In health-related collections we have analyzed, topic type is a natural axis for classification. For example, we have found documents about diseases (one document class) and documents about medications (another document class). Docume nts in a document class tend to contain information about a finite set of aspects of the topic type that are important in the domain. We call these aspects semantic components . For example, documents about diseases often contain information about etiology (i.e., causation) and treatment (two semantic components) whereas documents about medications often cont ain information about dosage and side effects (also semantic components). A semantic component instance is one or more segments of text that contain information about a particular semantic component. The segments of text that comprise a semantic component instance can vary in length and may or may not be contiguous. Semantic component instances ma y overlap with instances of other semantic components. Thus any given text in a document can belong to zero, one, or multiple semantic component instances. The semantic component is the type (that is, a label that indicates the type) for the se mantic component instances that correspond to a particular aspect. For example, the text segments that describe the etiology and treatment of a particular disease are instances of the etiology and treatment semantic components, respectively, in the document class about diseases . Semantic components can be useful for information retrieval in three ways: (1) a list of the semantic components present in each document in a hit list provides a short synopsis of the document and can help a searcher decide whether a particular document is likely to be useful; (2) a searcher can specify that his search is for documents about a topic that also contain a particular semantic component; and (3) the searcher can search for terms within semantic components. In the in troductory example, the searcher might request documents about as thma that contain the term  X  X regnancy X  within a treatment semantic component instance. Document classes are related to the familiar notion of document genre. A number of authors have suggested using document genre to improve information retr ieval (such as [6-8]). Freund and coauthors analyzed the information tasks of software engineers [9] and demonstrated a correlation between information tasks and document genre [8]. Turner et al. [10] created a model in the public health domain in which genre was one component. They used content analysis and expert users to identify elements in public health gray literature that could be extracted to create a searchable database of document surrogates. Some of th e key elements in the proposed surrogate, such as description of the problem, description of the intervention, and target popula tion are similar to semantic components but were not linked to particular document types. Studying and exploiting the organization of information in specific document types is not new. Dillon [1] and Bishop [2] provide evidence that readers can manipulate subdocument components in the context of a familiar document model, but neither specifically addresses IR uses. Discourse models have been created for various types of documents in preparation for supporting retrieval, summarization, and abstracting tasks, for example abstracts [11] and article s [12] about empirical research, articles about crop agriculture [13] and judicial opinions [14]. Purcell et al. developed models of three types of medical research articles [15] although the contexts they identified are more closely tied to the document organization than our semantic approach. To the extent that document st ructure corresponds to semantic content, structure can be useful for efficiently identifying semantic components. When sema ntic structure is explicit, as with XML markup, structure can be exploited directly. Once semantic components are identified for a document collection, new documents could be authored in a structured fashion, simplifying semantic component use. Semantic components are similar, but not necessarily identical, to facets [16] of a document X  X  main topic. Some components correspond quite naturally to facets, such as etiology and treatment as facets of diseases. However, semantic components can also contain information that might not be considered a topic facet, such as scheduling instructions for a given hospital within a practical information component for documents about surgical operations. Semantic components can also combine two or more concepts that are different facets of a topic, such as combining epidemiology and natural history into general information . Since semantic components are intended to facilitate retrieval, not to describe the domain, knowing the contents of a particular document collection or the co mmon information needs among users of the collection may lead to selecting semantic components with varying degrees of specificity to represent document content. Hearst and colleagues use facets to enhance search in Flamenco, a framework for using hierarchical faceted categorization to explore search results [17]. Flamenco uses metadata values for each facet to give searchers a browsing-like view of search results (such as viewing recipes by  X  X ish Type X ). Unlike Flamenco, the  X  X alue X  for a semantic component is the text that pertains to a component. This allows the semantic components for each document class to be chosen at varying levels of specificity. More general components encompass more te xt, provide more exhaustive indexing, and cover a wider range of queries while more specific components support greater precision. This flexibility makes semantic components applicable to a wide range of documents. Semantic components also re semble keywords that are sometimes used for document inde xing (and that often represent facets of topics). Medical litera ture is often indexed with the Medical Subjects Heading (MeSH) vocabulary [18]. Some semantic components correspond to descriptors (the main terms) or qualifiers (subheadings) in MeSH. Etiology , for example, is both a descriptor and a qualifier. Qualifiers can be used in conjunction with a descriptor to indicate a particular aspect (or facet) of a topic. For example, the descriptor/qualifier pair appendicitis/ etiology might be used to index a document about appendicitis (a disease) that di scussed etiology (a semantic component for documents about diseases). One important difference is that for keyword indexing, the descriptor or descriptor/qualifier pair is bound to the document as a whole whereas an instance of the etiology semantic component in a document about appendicitis is bound to particular segment(s) of text in the document. Another difference is that keywords can describe any of the content in a document whereas we use semantic components to improve search precision by adding supplementary indexing that pertai ns to specific aspects of the main topic of the document. A variety of tasks related to IR involve text at subdocument granularity. We briefly discuss those most pertinent to this work: content analysis, text segmenta tion, and passage retrieval. Content analysis [19], frequently used in social science research, is the systematic evaluation of the content of various forms of communication. It typically invol ves coding (labeling) units of information within a message (suc h as text, audio or video), a process very similar to semantic component indexing. The two main differences are purpose (content analysis is a research technique whereas semantic co mponent indexing is an IR technique) and the model (semantic component indexing occurs in the context of a document cla ss/semantic component model whereas the coding scheme will vary across different research projects and may or may not i nvolve document classification). Text segmentation divides text in to sections based on changes in topic or subtopic. It has been studied in the context of several problems but the one most closely related to our work is dividing documents into sections corres ponding to subtopics to aid in information retrieval [20] and disp lay of retrieval results [21]. Using text segmentation for IR is one example of a broader group of passage retrieval techniques, in which documents are split into a set of passages and similarity to the query is computed for each passage instead of for whole docum ents. Liu and Croft classify approaches to splitting documents into passages as structural, semantic, window-based, and arbitrary [22]. Semantic component instances might be considered a form of semantic passages, although not all document text is necessarily included in any of the semantic component instances . A more significant difference between our approach and passage retrieval is that we propose to use information about seman tic component instances to supplement, not replace, whole-document retrieval techniques. Our work takes inspiration fro m the superimposed schematics work by Bowers, Delcambre, and Maier [23] that identified semantically significant content elements in text documents in the forestry domain, and show ed how a human user could identify the corresponding text with in a document. Instead of identifying a complete schema for the information elements in documents, we identify less granul ar semantic components that can be used for retrieval. In this study we addressed the research question: Can a search system with semantic component s produce better search results than a basic system without semantic components ? In this paper we answer the question from a system perspective, comparing document rankings from two search systems for users X  queries using a reference standard based on the relevance judgments of a family practitioner with expertise in research and clinical care. We created an experimental search system based on the existing sundhed.dk portal that consisted of documents, a search engine, and two different search interfaces. Figure 1 shows a schematic of the experimental search system. With the permission of sundhe d.dk, we copied all 24,712 documents owned by sundhed.dk as of July 2006 (including keyword and metadata fields). These pages contain information about health and healthcare and also about the Danish healthcare system. For example, the portal includes referral guidelines that specify when a family practitioner should refer a patient to a specialist, what tests must be done first, and what records should be sent to the specialist. The portal also includes information about what services are available and whether they are subsidized by the government. Some information is written for healthcare providers and some for patients and their families, but all the documents are available to anyone on the public web portal. The operational sundhed.dk web por tal uses Ultraseek [24], a commercial search engine devel oped by Verity Inc., and later acquired by Autonomy Corporation [25]. We were granted a temporary license for the Ultraseek 5.6 software by Ensight (now Metier), the Danish distributor for Verity/Autonomy products. Sundhed.dk gave us copies of its configuration files so that we could mimic the operational system. Ultraseek provides three main functionalities in the sundhed.dk portal: (1) it indexes all the documents, (2) it generates a search interface, implemented as a web page, and (3) it performs requested searches and generates a web page with a ranked list of links to documents that comprise the search result. Both the indexing and the search interface are customizable by setting parameters through an administrative interface and by editing the code that generates the user interface. The internal algorithms for searching the indexes and for ra nking results are proprietary and cannot be viewed or modified. Documentation on the Ultraseek website describes the scoring algor ithm in very general terms as taking into account term frequenc y, term location within the document, rarity of individual terms, occurrence of multiple query terms, and document quality  X  X ased on numerous factors X  [26]. The operational sundhed.dk site o ffers two search interfaces, a simple search (a single search box only) and an advanced search that provides several filters and the ability to designate terms as desired or required. We created two interfaces to our search system that we labeled as System 1 and System 2. The System 1 interface provides a simple search box plus two filters from the sundhed.dk advanced interface that are controlled by pulldown menus, one to filter documents by the region of Denmark to which the documents apply (labeled Regionalt indhold in the interface) and one to filter documents by an existing document classification (labeled Informationstype in the interface) used by sundhed.dk. We included these two filters after discussions with physician users and indexers, pl us a review of the sundhed.dk search log, indicated these filters to be useful and frequently used. The default behavior for both filters is to include all documents (apply no filter). Queries typed into the search box use the Ultraseek query syntax, which includes wildcard expansion when an asterisk is included in a search term. The left panel of Figure 2 shows a cropped screenshot of the System 1 interface. System 2 has the same features as System 1 plus the ability to further specify the search using se mantic components. To search using System 2, the searcher types one or more search terms into the search box labeled Search and optionally chooses an item from the pulldown menus for the two filters, as for System 1. In addition, the searcher can (optionally) enter one or more search terms into one or more of the text boxes for the semantic components. The right panel of Figure 2 shows a cropped screenshot of the System 2 interface. The text boxes are grouped by document class (the name of the class is in bold font) and labeled with the semantic component. The circle highlights a search term (the Danish equiva lent of pregnan*) in a text box associated with the semantic component for treatment . System 1 was produced by standard Ultraseek code, configured to mimic the operational system. System 2 was based on the code for System 1 but with customization of the web interface code. We used random sampling and purposeful browsing to analyze the sundhed.dk document collection and then selected six document classes and associated semantic components to index document content. Because it was not feasible to manually index the semantic components in 24,712 documents, we chose a subset of documents for indexing by ex ecuting a variety of searches applicable to each of the four scenarios for the searching study. Our goal was to identify documents most likely to be retrieved at a high rank by the users, plus all documents relevant to the search scenarios. Our document selection resulted in nearly all relevant documents of interest being inde xed with semantic components, as is discussed in more detail in Section 5.3. Seven experienced indexers, who had received training with respect to semantic components and to our semantic component indexing software, indexed 371 docum ents. Indexing consists of first selecting a document cla ss from a menu then designating semantic component instances by highlighting any amount of contiguous text, right-clicking to get a menu of semantic components, then clicking on the appropriate label. Additional segments of text can be added to existing semantic component instances by repeating the sel ection and right-clicking sequence. We stored the semantic component da ta in metadata fields that we added to each indexed document. Data included the indexer-assigned document class, a list of the semantic components present in the document, the size of each semantic component instance (the number of characters in the instance), and the text in each semantic component instance. After configuring Ultraseek to index our newly-defined metada ta fields, when present, we indexed all 24,712 documen ts with Ultraseek. After completing the searching e xperiment, we retrospectively analyzed the distribution of documents indexed with semantic components. We did not want to bias the results by indexing only the relevant documents that contained words related to the scenarios so we deliberately indexed documents that were likely to be returned by searches for the four scenarios. In other words, in addition to relevant documents we indexed the nonrelevant documents most likely to compet e with relevant documents for ranking. To assess our results we calculated the percentage of retrieved documents that had been indexed and the percentage of highly relevant documents that ha d been indexed. If a difference between systems were due only to System 2 preferentially returning indexed documents, the percentage of highly relevant documents in the result would be directly related to the percentage of indexed documents in the result. We configured both System 1 and System 2 to return 100 hits, ordered by similarity score. Sy stem 1 returned documents using the Ultraseek similarity algorithm based on full text indexing of the title, body, keywords, and designated metadata fields. If a value was selected for either of the two filters, Informationstype or Regionalt indhold , documents matching the query term were returned only if the document also contained the appropriate value in the metadata field for the selected filter(s). System 1 did not search metadata fields that represent semantic components. System 2 sent the query in the ma in (simple) search box plus the values for the two filters, if any, to the Ultraseek search engine exactly the same as for System 1. Unlike System 1, System 2 intercepted the result list and sim ilarity scores, and sent a second query with the terms that were entered into the semantic component fields as a fielded s earch of the indicated semantic component metadata fields. The similarity scores for the second search were determined solely by the similarity of the semantic component part of the query to the corresponding semantic component instances in the re trieved documents. Documents without an instance of the reque sted semantic component were not returned from the second search and were assigned a similarity score of zero. An asterisk in a query term acted as a wildcard and matched any text in a word. If only an asterisk and no other characters were entered in a search box for a semantic component, the asterisk acted as both a wildcard and a filter. In other words, the asterisk matc hed any text but only documents that contained an instance of th e requested semantic component were returned from the second search. Documents were returned only if they appeared in the result of the first  X  X opical X  query. Document ranking was determined by a final similarity score that was the average of the similarity scores from the two searches. In summary, System 1 returned documents (that matched the filters, if any) ordered by their similarity to a simple query as calculated by Ultraseek based on full text indexing and keyword indexing. System 2 returned documents (that matched the filters, if any) ordered by the average of the similarity to a topical query and the similarity of any queries applied to particular semantic components. System 2 returned exactly the same documents that would have been returned by Sy stem 1 (but re-ranked) unless a query to System 2 included an as terisk-only semantic component query, in which case it returned only documents from the topical query that also contained an instance of the semantic component. The results displays for both Sy stem 1 and System 2 mimicked the operational system. Both systems displayed the title, a snippet of text showing the query term in context, the document ID, the Region (if any) for which the document was written, the document type ( Informationskategori ) used by the operational system, and a summary written by the document author. In addition, System 2 also displayed: (1) the document class selected by the indexer from our list of six document classes ( Documenttyper ) and (2) a list of semantic components appearing in the document plus an integer to indicate the size, in number of characters, of the semantic component instance. A convenience sample (as disti nguished from a random sample) of 30 Danish family practice physicians from the Aarhus Region who were familiar with sundhed.dk participated in the searching study. The physicians were paid an amount equivalent to what they could have earned in their practice during the two hours of the study plus travel expenses. Table 1 summarizes the self-reported medical and searching experience of the 30 participants. We studied each subject separately in a two hour block that consisted of a training session followed by an experimental session. We performed the studi es during five consecutive days to maintain consistency. Each study session followed the same sequence. The training session c onsisted of an introduction to semantic components and to the interfaces for Systems 1 and 2 plus a series of guided searches using the two systems. Each training session lasted about 45 minutes. The experimental session consisted of four search sessions, one for each scenario. Each subject used System 1 for two scenarios and System 2 for two scenarios. We randomized th e order of scenarios and system use. Fifteen physicians used System 1 for the first two scenarios and System 2 for the second two. The other fifteen physicians used the two systems in reverse or der. We also varied the order of the scenarios in a random fashi on. We randomly selected 15 of the 24 possible sequences of four scenarios; these were randomly assigned to two physicians, one who started with System 1 and one who started with System 2. The searcher used one of the two interfaces to enter queries and view the results. The searcher coul d click on any of the hits in the result list to view the full docum ent. For any documents the searcher considered relevant, we asked him or her to record an explicit relevance judgment. A researcher observed each search session and asked the searchers to think aloud. At the end of each scenario the searcher filled out a brief questionnaire and participated in an interview. Each participant also completed a final questionnaire after having comp leted all four scenarios. We do not present the interview and que stionnaire data in this paper. Each scenario represented a typical information need that might be encountered in the context of a patient visit in order to make a decision about patient care. We developed the scenarios following the methodology of Borlund [27]. Our scenarios are very much in line with prior work on clinical questions [28], adapted to the specifics of the Danish healthcare system and the information available in sundhed. dk. The scenarios represent needs for information availa ble in the sundhed.dk document collection but of variable difficulty to find. We asked searchers to search as they would in real life, letting the constraints of the clinical setting determine how l ong they would search and when they would either be satisfied or abandon the search. Table 2 provides a condensed summary of each scenario. Evaluations of IR systems are often carried out in the context of existing test collections that include a document collection, a set of queries, and relevance judgmen ts. Evaluating a model with a new form of indexing and a corresponding new query model precludes use of existing test collections. We employed a real document collection and real users in interactive search sessions to perform as realistic an evalua tion as possible. For documents viewed, we asked the searcher to record a graded relevance judgment of 0 to 3. We used the four point scale of Sormunen that classifies documents as irrelevant , marginally relevant , fairly relevant , and highly relevant [29]. Our reference standard consisted of graded relevance j udgments made independently by a domain expert using the same s cale. The standard included documents we identified as releva nt during scenario creation plus all documents identified as releva nt (rating 1 X 3) by at least one searcher. Thus the reference sta ndard incorporated searcher input but is here applied independently of individual searchers X  Scenario judgments. Whereas th e searchers typically quit searching after identifying a single highly relevant document for each session, this method allowed us to a ssess the complete ranked lists returned by each system. Table 3 shows the number of highly relevant documents per scenario in the reference standard. There are two distinct perspectives for evaluating the search system performance in this study: (1) the user perspective, using only a searcher X  X  own relevance assessments for his or her queries, and (2) the system persp ective, considering all relevant documents returned by a system regardless of user assessment. Improved system performance doe s not guarantee improved user performance [30-32]. We believe both evaluation perspectives are valuable and that good system performance is necessary, if not sufficient, for successful searching. In this paper we report a system-based evaluation of seman tic components. We are also planning to evaluate results from a user perspective, examining searcher behavior in detail and comparing systems based on relevant documents explicitly identified by each searcher. To compare the two systems we defined a best (system-oriented) query for each search session (where a session is all the queries posed by one searcher for one scenar io). We did this in two ways, one by calculating average precision (AP) and one by calculating discounted cumulative gain (DCG) [33] for each query. If multiple queries in the same sessi on had identical AP or DCG, we designated the first such query as the best query. We used the graded relevance judgments in the reference standard to calculate AP and DCG. Because AP is calculated using binary relevance judgments, we considered onl y highly relevant documents (relevance rating of 3) as relevant for calculating AP. This ensured that only documents that satisfied the targeted information need in the scenarios were treated as relevant, a much stricter standard than is used in many retrieval studies, such as those using TREC data sets. Graded relevance judgments are inherent in the DCG metric, hence all relevance data are incorporated in DCG. Because we were simulating a search setting where information needs are very specific, we assigned values of 0, 1, 10, and 100 to doc uments with relevance ratings of 0, 1, 2, and 3 respectively. We us ed a factor of 10 to separate the values because marginally relevant and partially relevant documents are generally not very useful in the setting we simulated. Similarly, because the scenarios simulate a setting where time available for searching is limited, we used a discounting parameter of base 2 to simulate a  X  X usy X  user [33]. We chose AP and DCG after cons idering a variety of metrics popular in the IR literature. Use of P@5, P@10, or R-Precision lacked enough power to discrimina te among the queries in many of our sessions because of the sparseness of highly relevant documents. These metrics genera ted too many ties, and often equaled zero when AP or DCG was positive. If ties are ignored, none of those three metrics woul d have changed which were the best queries. We also considered bpref, but found that it too was unsuitable. Bpref is generally robust to incomplete relevance judgments but it relies on comparing the ranks of pairs of relevant and judged nonrelevant documents. It requires that nonrelevant documents have as much chance of being judged as relevant Table 3. Number of highly relevant documents per scenario documents. It also lacks discriminating power if the number of comparisons is too small [34]. The pool of documents judged by our domain expert were documents deemed relevant by human searchers, not a ranking algorithm. Therefore the pool was biased and very small; we had very few judged nonrelevant documents. AP is appealing because it reflects the quality of document ranking and has been termed stable and discriminating [35]. It only accepts binary relevance judgments, but in the setting we simulated searchers are usually in terested only in highly relevant documents. Treating all other documents as nonrelevant is a reasonable choice. However, this resulted in a very small number of relevant documents, a situation in which most metrics are less stable. DCG may be more stable with few relevant documents than other metrics because, while it assigns more value to highly relevant documents, it incorporat es ranking information about all relevant documents [36]. We compared the two experimental search systems based on mean average precision (MAP) a nd mean normalized discounted cumulative gain (nDCG) of the best queries in each session. Search performance is influenced by both the search system and the scenario, and we hypothesized that system might interact with scenario, such that System 2 might have better performance on some scenarios and worse performance on others, so we compared the systems using a mixed effect two-way factorial analysis of variance (ANOVA) model [37]. The search system is a fixed effect since we are interested only in comparing System 1 and System 2. Search scenario is a random factor; we studied only a small subset of all possible search scenarios but we are interested in being able to generalize to other scenarios. System 2 achieved a higher mean performance for each scenario and for all scenarios combined using either MAP or nDCG. Over all scenarios, the improvement was 35.5% as measured by MAP and 25.6% by nDCG. ANOVA found no interaction between system and scenario for either MAP or nDCG. The difference between System 1 and System 2 was statistically significant for both MAP (p &lt; 0.02) and nDCG (p &lt; 0.01). As expected, given the varying difficulty of the scenarios, the difference among scenarios was highly significant usi ng either metric. Tables 4 and 5 show the mean performance and st andard error (SE) by system and scenario of the best (system-oriented) query for each session using MAP (Table 4) and nDCG (Table 5). Figure 4 shows the shape of the average nDCG curve for each system over all scenarios combined. This plot indicates that System 2 returns relevant documents at rank 1 more often than System 1, and this early retrieval is responsible for its better performance. We did not require searchers to use semantic components when using System 2. We compared performance of System 1 to System 2 without regard to whether a searcher used semantic components in a query submitted to System 2. We did this for three reasons: (1) the System 2 results display included information about semantic com ponents whether or not the query used semantic components, (2) we wanted to assess the overall effect of making semantic compone nts available to searchers and choices about whether to use a feature is part of such an assessment, and (3) we wanted to maintain the randomization applied at the beginning of the study. Using this approach ensured that the results are less likely to over-predict the effects of usage in an operational setting. Twenty-nine of the 30 searchers us ed semantic components in at least some of their queries. Fifty-six (93%) of the 60 search sessions with System 2 contai ned at least one semantic component query. The best query included at least one semantic component in 46 (82%) of those 56 sessions, whether determined by AP or by DCG. For all but two System 2 sessions, the best query was the same when determined by either AP or DCG. In both cases, documents with relevan ce scores of 1 or 2 increased DCG but not AP. Queries to System 2 resulted in 506 instances of retrieving a document with a rele vance score greater than zero. In 92 (18%) instances, the rank of a relevant document was changed by a semantic component part of the query, 46 for the better and 46 for the worse. Changes ranged from a rank improvement of 94 (from 96 th to 2 nd ) to a 17 place worsening (from 24 to 41 and from 25 to 42). The mean change was an improvement in rank by 8.1 places. We examined the possible effect on our results of disagreements with respect to relevance judgmen ts. Sixteen of the 41 documents in the relevance standard were sc ored as highly relevant. For 14 of those 16, there was substan tial agreement: all the users who clicked on those documents rated them as either highly relevant or fairly relevant. Only two docum ents were controversial. The single relevant document for S cenario B was judged highly or fairly relevant by eight searcher s but judged irrelevant by six and marginally relevant by one. The disagreement concerned (1) whether the document only applied to the first trimester of pregnancy, and (2) whether any search, instead of a phone call, was an appropriate action. On e of the nine highly relevant documents for Scenario C was rate d as either highly or fairly relevant by 12 searchers but two searchers rated it irrelevant. Because the only highly relevant document for Scenario B was controversial, we also calculated nDCG when Scenario B was excluded and repeated the analysis of variance. The improved performance of System 2 was highly significant (p &lt; 0.002). We strategically chose documents to be indexed before the searching study because our resources for manual semantic component indexing were limite d and then retrospectively analyzed the effect of our choices. Our analysis investigated whether the higher rate at whic h System 2 returned relevant documents could be explained so lely by System 2 returning documents with semantic compone nt indexing at a higher rate than System 1. Table 4. MAP of the best query per session (Mean  X  SE) 1 0.28  X  0.03 0.53  X  0.11 0.21  X  0.05 0.19  X  0.03 0.31  X  0.03 2 0.56  X  0.06 0.58  X  0.11 0.26  X  0.03 0.27  X  0.06 0.42  X  0.04 Table 5. nDCG of the best query per session (Mean  X  SE) 1 0.41  X  0.03 0.57  X  0.08 0.38  X  0.07 0.34  X  0.03 0.43  X  0.03 2 0.60  X  0.06 0.60  X  0.08 0.50  X  0.04 0.46  X  0.07 0.54  X  0.03 The searchers identified 37 documents as being at least marginally relevant (score  X  1) to one of the scenarios. The reference standard included t hose 37 plus an additional 4 documents we identified before the study that were not viewed by any of the searchers, resulting in a total of 41 documents. We indexed 30 of the 37 user-relevant documents and all 4 additional ones. Of the seven documents th at were not indexed, only three received at least one relevance ra ting of 3 and none were scored as highly relevant in the reference standard. Because we deliberately tried to index all relevant documents, we were concerned that the presence of semantic component indexing alone might bias the performan ce results. Of the 14993 hits returned by all 343 queries in the study, 5459 hits (36%) had been indexed with semantic component s. (The same document could be returned by multiple queries). Of the 5459 indexed hits, 508 (9%) were highly relevant and 4398 hits (81%) were irrelevant. The remaining hits were marginally or partially relevant. Table 6 shows the rate at which each system returned indexed documents (Fi) and the rate each returned highly relevant documents (Fr) in ranks 1 to 30 (i n increments of 10), and at all ranks. We defined these rates as: where Ti is the number of documents indexed with semantic components that were retrieved by a system over all queries, Tr is the total number of documents re trieved by a system over all queries (indexed or not), and Ri is the number of highly relevant documents retrieved by a system ove r all queries (indexed or not). For asterisk-only semantic co mponent queries, System 2 only returned documents with that se mantic component and therefore Fi = 1.0 for those queries. This effect explains the overall higher Fi for System 2. If we consider only System 2 queries without an asterisk filter (S2 no *), overall Fi is nearly identical to System 1. This is not surprising since Syst em 2 returns the same documents as would be returned by System 1 for the same topical query. Figure 4 shows the corresponding rates for all ten groups of ranks (up to document rank 100) expressed as ratios. The blue columns (on the left) show Fi for System 2 divided by Fi for System 1. The red columns (right) show Fr of System 2 divided by Fr of System 1. Although System 2 re turned more indexed documents than System 1, the rate at which System 2 returned highly relevant documents exceeds what could be expected based solely on the higher rate of returning indexed documents. Plotting the same ratios of System 2 to Syst em 1, but excluding queries with an asterisk in a semantic compone nt box, results in a graph with a similar profile (not shown) but the S2/S1 ratios (all ranks) are 1.06 for Fi and 1.35 for Fr . We focus on the data for the first 30 ranks because those ranks are of most interest to searchers and because so few highly relevant documents were returned by either system at the higher ranks. System 1 and System 2 returned 105 and 163 highly relevant documents at ranks 1-10, respectively, but only 3 and 10 highly relevant documents at ranks 51-60. Small changes in the number of relevant documents caused the seemingly erratic behavior of the ratio for Fr at higher ranks. The addition of semantic com ponents consistently improved search performance as measured by MAP and nDCG for the best query in each session, suggesting that semantic components can be a valuable supplement to existing indexing techniques. The semantic components model allows additional specification of the query, so the results reflect both the use of semantic component information in a query to retu rn relevant documents and the ability of searchers to use the model to express information needs. We recognize, of course, that this study had limitations. The experiment was limited to a si ngle user group searching over a single collection of documents in a single domain. The number of search scenarios was quite small, especially compared to the number of topics typical of TREC, but unlike laboratory-style evaluations we had 30 domain experts as end-users who formulated queries and interacted with the system, resulting in 120 search sessions. The small num ber of relevant documents per scenario reflects both the document collection and the nature of the scenarios. Greater information redundancy might dilute (or enhance) the results. This is the first empirical study to evaluate semantic components and establishing generalizability will require more research, but we be lieve our current results warrant further investigation into the poten tial usefulness of this model. In this paper we present only the results of the system-oriented evaluation. We collected extensiv e data about the search sessions Table 6. Fi and Fr for System 1 (S1) and System 2 (S2) Document 1  X  10 0.58 0.74 0.61 0.089 0.130 0.104 11  X  20 0.48 0.58 0.48 0.045 0.065 0.067 21  X  30 0.41 0.47 0.40 0.010 0.025 0.024 Figure 4. Fi and Fr expressed as a ratio System 2/System 1 and about user reactions to the new method of searching that will provide insight about how search ers used semantic components, whether they believe the model a llows more precise expression of information needs, and the role semantic components played in finding and recognizing documents users thought were relevant. Because we framed the task as finding information necessary to make a clinical decision, searchers entered queries and examined results until either finding the desired information or declaring the search a failure. Search sessions ranged from 1 to 11 queries with a mean of 2.85 and a median of 2 queries per session. Because semantic components were new to the searchers, we believe they engaged in some experimentation with the search interfaces, especially with System 2. In addition, we saw some obvious reasons for query failure, including mistakes in using the query syntax and searches that were either too broad (thousands of hits) or too narrow (zero or few hits). Little has been written about system evaluation in the context of interactive search sessions with multiple queries. For this evaluation we identified the query with the best performance in each search session in order to compare the potential performance of the two systems. However, because we are interested in supporting searchers with limited time, the number of queries to co mplete a search is important. We are collaborating to develop a metric that considers query sequence as well as individual query performance. We successfully predicted and i ndexed all the highly relevant documents in the reference standard, although there may be highly relevant documents in the collection that we did not discover. We also predicted and indexed a reasonable selection of nonrelevant documents likely to be returned by the queries in the study. Over half of the highest-ranked documents returned by either system had been indexe d with semantic components. Given that System 1 ignores se mantic component indexing and that there were few relevant documents, this means that a high proportion of the nonrelevant docum ents likely to compete with relevant documents for retrieva l were indexed, minimizing any bias towards retrieval of relevant documents simply because they had been indexed. Comparing the ratio of the two systems for Fi and Fr shows that System 2 returned highly relevant documents at a rate higher than would be exp ected based on the indexing rate alone. This indicates that the improved performance of System 2 cannot be attributed to our selective indexing. We have described an implemen tation of semantic components on top of an existing domain-specific digital library and have presented experimental evidence demonstrating that semantic components can enhance document re trieval. Our results are from a realistic interactive searching study, in which 30 domain experts searched on four scenarios. Comparing the two search systems shows an improvement in doc ument ranking when semantic components are used to express queries and to rank documents. We plan future work in three main areas: the usefulness of semantic components, the scalability of semantic components, and variations on the current semantic component model. We would like to test the sema ntic components model in other domains and validate its usefulne ss by implementing the model in an operational system. We would also like to test whether less-expert users could benefit from semantic components. Semantic components will be useful only if indexing is scalable, consistent, and accurate. In this study we used manual semantic component indexing. We are evaluating the cost and quality of manual indexing in another study. In the future we plan to investigate the automatic identifi cation of semantic components. We have presented one model that uses semantic components to aid document retrieval. Several va riations also may be useful. Initial feedback during model development indicated that too many document classes and se mantic components would be confusing. A flat list of sema ntic components, without document classes, would be simpler to im plement and use, but whether it would be as effective remains to be seen. For some tasks, document genre seems very importa nt whereas for others it may be less useful. It is worth inve stigating whether document classes can be subsumed by semantic components alone, so that a semantic component query natu rally results in retrieving documents of the appropriate class. Another approach is a mixed model. For some information needs, specifying document class might be sufficient and semantic component distinctions within the class might be unnecessary, whereas for other needs both semantic components and docum ent classes seem useful. This work was supported in part by the National Science Foundation, grant numbers 0514238, 0511050 and 0534762 and by the National Library of Medicine Training Grant 5-T15-LM07088. The physician participants were paid by a grant from the Kvalitetsudviklingsudvalget for Almen Praksis I Aarhus Amt. Any opinions, findings, conclusions, or recommendations expressed here are those of th e authors and do not necessarily reflect the views of the National Science Foundation. We thank Vibeke Luk, our primary contact and facilitator at sundhed.dk, for her assistance in organizing and conducting this research. We also thank Dr. Jens Rubak for recruiting study participants and Frans la Cour of Ensight (now Metier) for granting us a temporary license to use Ultraseek. [1] Dillon, A. Reader's models of text structures: the case of [2] Bishop, A. Document structur e and digital libraries: How [3] Price, S. L., Delcambre, L. M., Nielsen, M. L., Tolle, T., [4] Price, S. L., Delcambre, L. M. and Nielsen, M. L. Using [5] URL: http://www.sundhed.dk, Accessed: August 1, 2007. [6] Crowston, K. and Kwasnik, B. H. Can document-genre [7] Rauber, A. and M X ller-K X gler, A. Integrating automatic [8] Freund, L., Toms, E. G. and Clarke, C. L. Modeling task-[9] Freund, L., Toms, E. G. and Waterhouse, J. Modeling the [10] Turner, A. M., Liddy, E. D., Bradley, J. and Wheatley, J. A. [11] Liddy, E. D. The discourse-level structure of empirical [12] Teufel, S. and Moens, M. Su mmarizing scientific articles: [13] Paice, C. D. and Jones, P. A. The identification of important [14] Conrad, J. G. and Dabney, D. P. A cognitive approach to [15] Purcell, G. P., Rennels, G. D. and Shortliffe, E. H. [16] Foskett, A. C. The subject approach to information . Library [17] Hearst, M. A. Clustering versus faceted categories for [18] Medical Subject Headings Home Page. URL: [19] Krippendorff, K. Content analysis: An introduction to its [20] Hearst, M. and Plaunt, C. Subt opic structuring for full length [21] Hearst, M. A. TextTiling: Segmenting text into multi-[22] Liu, X. and Croft, W. B. Passage Retrieval Based On [23] Bowers, S., Delcambre, L. a nd Maier, D. Superimposed [24] URL: http://www.ultraseek.com , Accessed: August 1, 2007. [25] URL: http://www.autonomy.com, Accessed: April 5, 2007. [26] Ultraseek FAQ: How does Ultraseek create scores for items [27] Borlund, P. The IIR evaluation model: A framework for [28] Ely, J., Osheroff, J., Gorman, P., Ebell, M., Chambliss, M., [29] Sormunen, E. Liberal relevance criteria of TREC -Counting [30] Hersh, W., Turpin, A., Price, S., Kraemer, D., Olson, D., [31] Turpin, A. and Hersh, W. Why batch and user evaluations do [32] Turpin, A. and Scholer, F. User performance versus [33] J X rvelin, K. and Kek X l X inen, J. Cumulated gain-based [34] Buckley, C. and Voorhees, E. M. Retrieval evaluation with [35] Buckley, C. and Voorhees, E. M. Evaluating evaluation [36] Voorhees, E. M. Evaluation by highly relevant documents. 
