 Almost all approaches to automatic speech recogni-tion (ASR) using Hidden Markov Models (HMMs) to model the time dependency of speech are also based on phones, or context-dependent sub-phonetic units derived from them, as the atomic unit of speech modeling. In phonetics, a phone is a shorthand no-tation for a certain configuration of underlying artic-ulatory features (AFs) (Chomsky and Halle, 1968): /p/ is for example defined as the unvoiced, bi-labial plosive, from which /b/ can be distinguished by its V
OICED attribute. In this sense, instead of describ-ing speech as a single, sequential stream of sym-bols representing sounds, we can also look at speech as the result of a process involving several paral-lel streams of information, each of which describes some linguistic or articulatory property as being ei-ther absent or present.

A multi-stream architecture is a relatively simple approach to combining several information sources in ASR, because it leaves the basic structure of the Hidden Markov Model and its computational complexity intact. Examples combining different observations are audio-visual speech recognition (Potamianos and Graf, 1998) and sub-band based speech processing (Janin et al., 1999). The same idea can also be used to combine different classi-fiers on the same observation. In a multi-stream HMM setup, log-linear interpolation (Beyerlein, 2000) can be derived as a framework to integrat-ing several independent acoustic models given as Gaussian Mixture Models (GMMs) into the speech recognition process: given a  X  X eight X  vector  X  = {  X  0 , X  1 ,  X  X  X  , X  M } , a word sequence W , and an acoustic observation o , the posterior probability p ( W | o ) one wants to optimize is written as: C is a normalization constant, which can be ne-glected in practice, as long as normalization  X  i  X  i = const is observed. It is now possible to set p ( W | o )  X  p ( o | W ) (Beyerlein, 2000) and write a speech recognizer X  X  acoustic model p ( o | W ) in this form, which in logarithmic representation reduces to a simple weighted sum of so-called  X  X cores X  for each individual stream. The  X  i represent the  X  X m-portance X  of the contribution of each individual in-formation source.

Extending Kirchhoff X  X  (Kirchhoff, 1999) ap-proach, the log-likelihood score combination method to AF-based ASR can be used to combine information from M different articulatory features while at the same time retaining the  X  X tandard X  acoustic models as stream 0. As an example using M = 2 , the acoustic score for /z/ would be computed as a weighted sum of the scores for a (context-dependent sub-)phonetic model z , the score for F RICATIVE and the score for V OICED , while the score for /s/ would be computed as a weighted sum of the scores for a (context-dependent sub-) phonetic model s , the score for F RICA -TIVE and the score for N ON V OICED . The free parameters  X  i can be global (G), or they can be made state-dependent (SD) during the optimization process, thus changing the importance of a feature given a specific phonetic context, as long as overall normalization is observed. (Metze, 2005) discusses this stream setup in more detail. To investigate the performance of the proposed AF-based model, we built acoustic models for 68 ar-ticulatory features on 32h of English Spontaneous Scheduling Task ESST data from the Verbmobil project (Wahlster, 2000), and integrated them with matching phone-based acoustic models.

For training robust baseline phone models, 32h from the ESST corpus were merged with 66h Broad-cast News  X 96 data, for which manually annotated speaker labels are available. The system is trained using 6 iterations of ML training and uses 4000 con-text dependent (CD) acoustic models (HMM states), 32 Gaussians per model with diagonal covariance matrices and a global semi-tied covariance matrix (STC) in a 40-dimensional MFCC-based feature space after LDA. The characteristics of the training and test sets used in the following experiments are summarized in Table 1.

The ESST test vocabulary contains 9400 words including pronunciation variants (7100 base forms) while the language model perplexity is 43.5 with an out of vocabulary (OOV) rate of 1%. The language model is a tri-gram model trained on ESST data Table 1: Data sets used in this work: The ESST test set 1825 is the union of the development set ds2 and the evaluation set xv2 . containing manually annotated semantic classes for most proper names (persons, locations, numbers). Generally, systems run in less than 4 times real-time on Pentium 4-class machines. The baseline Word Error Rate is reported as adaptation  X  X one X  in Ta-ble 2; the system parameters were optimized on the ds2 data set. As the stream weight estimation pro-cess can introduce a scaling factor for the acoustic model, we verified that the baseline system can not be improved by widening the beam or by readjust-ing the weight of the language model vs. the acous-tic model. The baseline system can also not be im-proved significantly by varying the number of pa-rameters, either by increasing the number of Gaus-sians per codebook or by increasing the number of codebooks. 2.1 MMI Training of Stream Weights To arrive at an optimal set of stream weights, we used the iterative update rules presented in (Metze, 2005) to generate stream weights  X  i using the Max-imum Mutual Information (MMI) criterion (Bahl et al., 1986).

Results after one iteration of stream weight esti-mation on the 1825 and ds2 data sets using step size = 4  X  10  X  8 , initial stream weight  X  0 i 6 =0 = 3  X  10  X  3 , and lattice density d = 10 are shown in Table 2 in rows  X  X F (G) on 1825  X  and  X  X F (G) on ds2  X : As there are only 68 stream weights to es-timate, adaptation works only slightly better when adapting and testing on the same corpus ( X  X heat-ing experiment X : 22.6% vs. 22.8% word error rate (WER) on ds2 ). There is no loss in WER (24.9%) on xv2 when adapting the weights on ds2 instead of 1825 , which has no overlap with xv2 , so gen-eralization on unseen test data is good for global stream weights, i.e. weights which do not depend on state or context. 2.2 Speaker-specific Stream Weights The ESST test 1825 set is suitable to test speaker-specific properties of articulatory features, because it contains 16 speakers in 58 different recordings. As 1825 provides between 2 and 8 dialogs per speaker, it is possible to adapt the system to individual speak-ers in a  X  X ound-robin X  or  X  X eave-one-out X  experi-ment, i.e. to decode every test dialog with weights adapted on all remaining dialogs from that speaker in the 1825 test set. Using speaker-specific, but global (G), weights computed with the above set-tings, the resulting WER is 21.5% (row  X  X F (G) on speaker X  in Table 2).

Training parameters were chosen to display im-provements after the first iteration of training with-out convergence in further iterations. Consequently, training a second iteration of global (i.e. context independent) weights does not improve the perfor-mance of the speaker adapted system. In our ex-periments we reached best results when comput-ing state-dependent (SD) feature weights on top of global weights using the experimentally determined smaller learning rate of SD = 0 . 2  X  . In this case, speaker and state dependent AF stream weights fur-ther reduce the word error rate to 19.8% (see bottom row of Table 2). 2.3 ML Model Adaptation When training speaker-dependent articulatory fea-ture weights in Section 2.2, we were effectively per-forming supervised speaker adaptation (on separate adaptation data) with articulatory feature weights. To compare the performance of AFs to other ap-proaches to speaker adaptation, we adapted the baseline acoustic models to the test data using supervised maximum likelihood linear regression (MLLR) (Leggetter and Woodland, 1994) and con-strained MLLR, which is also known as  X  X eature-space adaptation X  (FSA) (Gales, 1997).

The ESST data has very little channel variation so that the performance of models that were trained on both ESST and BN data can be improved slightly on ESST test dialogs by using FSA, while MLLR already leads to over-specialization (Table 2, rows  X  X SA/ MLLR on ds2 ). The results in Table 2 None 25.0% 24.1% 26.1% FSA on ds2 22.5% 25.4% FSA on speaker 22.8% 21.6% 24.3% MLLR on ds2 16.3% 26.4% MLLR on speaker 20.9% 19.8% 22.4% MMI-MAP on ds2 14.4% 26.2% MMI-MAP on speaker 20.5% 19.5% 21.7% AF (G) on 1825 23.7% 22.8% 24.9% AF (G) on ds2 22.6% 24.9% AF (SD) on ds2 22.5% 26.5% AF (G) on speaker 21.5% 20.1% 23.6% AF (SD) on speaker 19.8% 18.6% 21.7% Table 2: Word error rates on the ESST test sets us-ing different kinds of adaptation. See Table 1 for a description of data sets. show that AF adaptation performs as well as FSA in the case of supervised adaptation on the ds2 data and better by about 1.3% absolute in the speaker adaptation case, despite using significantly less pa-rameters (69 for the AF case vs. 40*40=1.6k for the FSA case). While supervised FSA is equiva-lent to AF adaptation when adapting and decoding on the ds2 data in a  X  X heating experiment X  for di-agnostic purposes (22.5% vs 22.6%, rows  X  X SA/ AF (G) on ds2  X  of Table 2), supervised FSA only reaches a WER of 22.8% on 1825 when decod-ing every ESST dialog with acoustic models adapted to the other dialogs available for this speaker (row  X  X SA on speaker X ). AF-based adaptation reaches 21.5% for the global (G) case and 19.8% for the state dependent (SD) case (last two rows). The AF (SD) case has 68*4000=276k free parameters, but decision-tree based tying using a minimum count re-duces these to 4.3k per speaker. Per-speaker MLLR uses 4.7k parameters in the transformation matrices on average per speaker, but performs worse than AF-based adaptation by about 1% absolute. 2.4 MMI Model Adaptation In a non-stream setup, discriminative speaker adap-tation approaches have been published using condi-tional maximum likelihood linear regression (CM-LLR) (Gunawardana and Byrne, 2001) and MMI-MAP (Povey et al., 2003). In supervised adapta-tion experiments on the Switchboard corpus, which are similar to the experiments presented in the pre-vious section, CMLLR reduced word error rate over the baseline, but failed to outperform conven-tional MLLR adaptation (Gunawardana and Byrne, 2001), which was already tested in Section 2.3. We therefore compared AF-based speaker adaptation to MMI-MAP as described in (Povey et al., 2003).
The results are given in Table 2: using a com-parable number of parameters for adaptation as in the previous section, AF-based adaptation performs slightly better than MMI-MAP (19.8% WER vs. 20.5%; rows  X  X MI-MAP/ AF (SD) on speaker X ). When testing on the adaptation data ds2 as a di-agnostic experiment, MMI-MAP as well as MLLR outperform AF based adaptation, but the gains do not carry over to the validation set xv2 , which we attribute to over-specialization of the acoustic mod-els (rows  X  X LLR/ MMI-MAP/ AF (SD) on ds2 ). This paper presented a comparison between two approaches to discriminative speaker adaptation: speaker adaptation using articulatory features (AFs) in the multi-stream setup presented in (Metze, 2005) slightly outperformed model-based discriminative approaches to speaker adaptation (Gunawardana and Byrne, 2001; Povey et al., 2003), however at the cost of having to evaluate additional codebooks in the articulatory feature streams during decoding. In our experiments, we used 68 AFs, which requires the evaluation of 68 models for  X  X eature present X  and 68 models for  X  X eature absent X  for each frame during decoding, plus the computation necessary for stream combination. In this setup however, the adap-tation parameters, which are given by the stream combination weights, have an intuitive meaning, as they model the importance of phonological features such as V OICED or R OUNDED for word discrimina-tion for this particular speaker and phonetic context. Context-dependent stream weights can also model feature asynchrony to some extent, so that this ap-proach not only improves automatic speech recogni-tion, but might also be an interesting starting point for future work in speaker clustering, speaker iden-tification, or other applications in speech analysis.
