 1. Introduction
A time series is a set of statistics, usually collected at regular intervals. Over the past several decades much research have been done to develop and improve the time series forecasting models ( Zhang, 2003 ). Arti fi cial neural networks (ANNs) have been widely used to forecast time series and solve different engineering problems ( Wu and Chau, 2013; Khashei and Bijari, 2012; Sharma and Srinivasan, 2013; Maf fi ni Santos et al., 2014; Ismail et al., 2013 ). The features of ANNs make them very useful tool, de by Zhang (2001) as follows: First, ANNs are nonparametric methods and do not need any assumptions about the underlying model. Second, ANNs are nonlinear models. This makes them fl exible and powerful tool in modeling complex real world systems which are often nonlinear. Third, ANNs are universal function approximators that they can approximate any complex function with desired accuracy given a large enough input data.
Recently, radial basis function neural networks (RBFNNs) became attractive in practical applications such as forecasting ( Yu et al., 2008 ) fault detection ( Chai and Qiao, 2014 ) and others ( Langoni et al., 2006; Park et al., 2011 ) due to their simpler design process and improved generalization ability compared to neural networks ( Yu et al., 2011; Xie et al., 2011 ). However, there are some drawbacks in architecture selection including: input variables and neurons of hidden layer and determining network parameters (centers, width and weights) of such networks ( Pen et al., 2006 ). Most of the works in the literature consider two phase learning with fi xed structure ( Schwenker et al., 2001 ). However, according to the previous works ( Pen et al., 2006; Schwenker et al., 2001 ) considering both architecture and network parameters selection simultaneously has shown better results.
 The problem of optimizing the network size and parameters of RBF neural networks is a mixed integer hard optimization problem that there is no analytical method available to solve it and in the literature it has been suggested to be considered ( Aladag, 2011 ). There has been quite a number of papers that only focus on determining the parameters of RBF networks for time series forecasting while optimizing the number of hidden nodes and the input variables is not considered. Evolutionary algorithms have also been applied to optimize only the training phase of RBF networks ( Lacerda et al., 2005; Leung et al., 2003; Lee and Ko, 2009; Sheta and De Jong, 2001; Harpham et al., 2004; Sheikhan et al., 2013 ). There are a few papers considering optimization of both structure and network parameters of RBFNNs simultaneously.
Yu et al. (2009) propose a hybrid algorithm which combines particle swarm optimization and back propagation algorithm for training RBFNs. Du and Zhang (2008) develop a novel encoding scheme to train RBF networks by GA. This encoding scheme includes both the architecture and the parameters of the RBF networks. Leung et al. (2003) developed an improved GA for tuning the structure and parameters of neural network. They have showed that their proposed GA works better than standard GA. size and parameters of RBF neural networks is a mixed integer hard optimization problem, we develop a novel hybrid evolutionary algorithm to optimize architecture and network parameters simul-taneously. Genetic algorithm (GA) binary coding scheme recently developed by Du and Zhang (2008) is applied for representing architecture of RBF neural networks. For each chromosome (archi-tecture) modi fi ed version of adaptive particle swarm optimization (APSO) algorithm developed by Zhan et al. (2009) is used as the training method of RBF networks. APSO is also incorporated by a mutation operator to overcome the premature convergence of PSO algorithm.
 estimation of needed resources is dif fi cult and crucial. In recent years prediction methods have been used to forecast emergency supplies demand after natural disasters. To our knowledge a few of the conducted research uses statistical methods or arti intelligence for forecasting demand of emergency supplies after disaster such as Xu et al. (2010) . Song et al. (1996) also apply fuzzy evaluating techniques for forecasting earthquake damages. Due to complicated structure of the problem the most used technique is expert judgment method ( Xu et al., 2010 ). We apply the proposed
GA  X  APSO based RBFNNs method for forecasting demand of emergency supplies after the earthquake in 2012 in Iran because the corresponding time series contains nonlinearity and irregular-ity due to characteristics of natural disasters.
 detailed discussion of RBFNNs. Overview of PSO algorithm and proposed modi fi ed APSO are presented in Section 3 .Anovelhybrid evolutionary based RBF networks method is presented in Section 4 .
Experimental results in real case study and comparison with previous methods are given in Section 5 . Section 6 contains conclusion. 2. RBF networks made up of simple processing units, which has a natural propen-sity for storing experiential knowledge and making it available for use. ANNs are synthetic networks that imitate biological neural networks  X  ( Haykin, 2007 ).
 (MLP) with classic back-propaga tion (BP) learning algorithm to determine a prediction model. The BP algorithm adjusts ANN's weight parameters in the training phase such that the model outputs are aligned closely with the desired output (target). To predict accurately on unseen data in the problem, part of the available data is stored as the testing set to evaluate ANN's generalization ability. The BP algorithm tends to trap in the local optimum solutions instead of reaching the global optimum ( Hegazy and Ayed, 1998 ). ( Yu et al., 2011 ) because of their simple design and more computationally ef fi cient process with improved generalization ability, compared to neural networks ( Xie et al., 2011 ). RBF networks were introduced into the neural network literature by
Broomhead and Lowe (1988) , in which their main applications are function approximation and time series forecasting, as well as classi fi cation or clustering tasks. 2.1. Structure of RBF networks tron networks and consist of three layers. The input layer is only used to connect the network to its environment. The hidden layer contains a number of nodes, which apply a nonlinear transforma-tion to the input variables, using a radial basis function. The output layer is linear and serves as a summation unit. The typical structure of an RBF neural network with only one output node is depicted in Fig. 1 .
 than the number of network inputs, so input space is transformed into higher dimensional space, where pa tterns become linearly separable ( Cover, 1965 ).
 expressed as follows ( Yu et al., 2011 ). For a forecasting problem, the inputs of the network are past lagged observations. Consider a set of N data points in the input space R d and their target values in R . D  X f X  x i ; y i  X  A R d R ; i  X  1 ; 2 ; ... ; N j f  X  x i dimensional output values. The RBF approach to approximate a interpolation function f involves the use of M functions  X  is a RBF and de fi ned as follows:
The c j s are the locations of the centers of the radial basis functions, || . || denotes the norm, and u is the network input vector. traditional neural networks, it is important in RBF networks to proper initial network parameters such as locations of centers of hidden units because their performances are critically dependent on the choice of the centers ( Chen et al., 1991; Huang et al., 2005 ).
Typical approaches to this problem belong to one of the categories introduced by Kalouptsidis and Theodoridis (1993) . The approx-imation of the function f may be expressed as a linear combination of the RBFs ^ f  X  u  X  X   X  M useful properties for RBF networks ( Harpham and Dawson, 2006 ).
Here we use Gaussian basis function which is the most popular and widely used radial basis function used by many other authors for forecasting of time series ( Schwenker et al., 2001; Chng et al., 1996 ).  X jj u c j jj X   X  exp  X jj u c j jj X  where r j is the width factor of the basis j . 2.2. Training algorithms
One of the most important issues in the RBFNNs applications is the network learning, i.e., to optimize the parameters, including the center vectors, the widths of the basis functions, and the linear output weights connecting the RBF hidden nodes to the output nodes. Another important issue is to determine the network structure or the number of RBF hidden neurons as compact as possible to secure its generalization abilities and reduce computa-tional problems.

The comprehensive review of different learning methods with their weaknesses and strength for RBF networks is provided by Schwenker et al. (2001) and Pen et al. (2006) . Based on these papers we can conclude that (1) While many of existing papers use two phase learning meth-(2) Majority of training methods are only applicable to RBF net-(3) In the literature, there are no general rules to determine the 3. Particle swarm optimization: overview 3.1. PSO in general
PSO is a population based evolutionary computation technique introduced by Kennedy and Eberhart (1995) , inspired by group behavior of birds fl ocking and fi sh schooling. In PSO, the potential solutions called particles and swarm is equivalent to population in genetic algorithm. A PSO model is initialized with a swarm of random particles and searches for optima by updating generations.
Assume a D -dimensional searching space. In this search space each particle has two main features: position and velocity. Considering aswarmof N particles seeking for optimum point, position and velocity of each particle represented by X i  X  ( x i 1 , x ( v following equations: v  X  k  X  1  X  X   X  v id  X  k  X  X  c 1 rand 1  X  p id  X  k  X  x id  X  k  X  x  X  k  X  1  X  X  x id  X  k  X  X  v id  X  k  X  1  X  X  6  X  coef fi cients and rand 1 and rand 2 are two independently uniformly distributed random variables within range [0,1]. P nd is determined according to the global or local version of PSO. In the local-version of PSO p l d (best position in the neighborhood) and in the global-version p gd (best position among all particles until current step) are used instead of p nd . Complete process of PSO algorithm is as follows:
Step 1: Set the parameters and generate a population of particles with random features containing D dimensions. Set the current position of particle i as p id and position of best initial particle as p gd ( g best ).

Step 2: Modify the velocity and position of particles using Eqs. (5) and (6) .
 Step 3: Calculate fi tness function of each particle.

Step 4: Compare the former ( p id ) and current fi tness functions of particles. If the current value surpasses, put the position and objective function of p id equal to current position and fi function.

Step 5: Specify the best particle of the current population. If its fi tness function is better than p gd , modify p gd and put its position and objective function equal to the best particle one.
Step 6: Check a stopping criterion, if its satis fi ed p gd output; otherwise, go to step 2.

Since the origination, PSO has received much attention in different engineering optimization problems and undergone many changes. Here we present a brief review of hybrid and adaptive versions of PSO. 3.2. Adaptive versions of PSO
Some research has been done in order to adapt PSO parameters in response to particles status, time and other information about search space. Shi and Eberhart (1998) bring forward inertia weight and recommend a linearly decreasing relationship between  X  and generations where G is the maximum number of generations, and g is the current number of generations. In this equation  X  max and set to 0.9 and 0.4 respectively. Eberhart and Shi (2001) propose a random version setting  X  for dynamic system optimization. Clerc and Kennedy (2002) altered Eq. (6) and introduced constriction factor. Other two important parameters need to be set are acceleration coef fi cients ( c 1 and c 2 ). Kennedy and Eberhart (1995) suggested to be set c 1 and c 2 at fi xed value of 0.2. Ratnaweera et al. (2004) present linear time-varying acceleration coef fi cients for proposed PSO that result in equilibrating between local and global searches. This approach improves the algorithm performance and makes it more viable than algorithms with parameters. Chatterjee and Siarry (2006) consider the acceleration coef fi cients constant and propose a time varying nonlinear func-tion for inertia factor adaptation. Ko et al. (2007) propose a nonlinear time-varying evolution (NTVE-PSO) to adapt para-meters. In fact inertia weight and acceleration coef fi cients values nonlinearly decrease or increase according to the current and maximum number of iterations. Zhan et al. (2009) de fi ne evolu-tionary factor by calculating mean distance of each particle to all other ones. Their proposed algorithm adapts inertia weight and acceleration factors considering evolutionary factor. We will dis-cuss this algorithm in details in the next section. Clerc (2010) has pointed out that parameter adaptation can enhance the algorithm performance and lead to improved results. 3.2.1. Proposed modi fi ed APSO
APSO fi rstly was developed by Zhan et al. (2009) , in which evolutionary state estimation (ESE) is used to control inertia weight (  X  ) and acceleration coef fi cients ( c 1 , c 2 ). APSO includes two fundamental phases called ESE and elitist learning strategy (ELS), respectively. Firstly, in the ESE step, according to the population distribution information and particles position, average distance of each particle from all the others is calculated. Euclidian metric is used to calculate the average distance then evolutionary factor f is obtained by Eq. (8) . f  X  d g d min d where globally best particle distance is indicated by d g d min denote maximum and minimum distances, respectively. They have used fuzzy classi fi cation method to classify f and also offered crisp classi fi cation. Then defuzzi fi cation techniques have been applied by Zhan et al. (2009) . According to the computational experiments, this process is computationally time consuming in comparison with the crisp classi fi cation while there is no signi -cant difference between solution qualities. Here crisp classi is applied based on the obtained values for distances during initial search. Considering obtained values for f , searching status is classi fi ed into one of the four stages; exploration, exploitation, convergence and jumping out ( Zhan et al., 2009 ). Then according to the obtained information, acceleration coef fi cients are adapted to control the searching procedure. We omit the details due to the brevity. For more details about acceleration coef fi cients adjusting the reader is referred to Zhan et al. (2009) .
 status of the exploration, exploitation or jumping out, it is directly calculated by Eq. (9) ; if not ESL step will be done and then inertia weight will be adapted by the same equation.  X   X  f  X  : R  X  -R  X   X   X  f  X  X  1 the convergence step, Gaussian perturbation is used in ESL phase to help global best particle jump out from local optimum. Here a new mutation operator is applied to guide global best particle to a potentially better region by extending the variables range. 3.3. Mutation operator algorithms suffer from them: (1) basic PSO algorithms lost diversity fast that lead to premature convergence ( Wang et al., 2013 ); (2) PSO algorithm slows down rapidly around the global best ( Choi et al., 2009 ). Mutation operator is proposed to deal with these drawbacks. We apply mutation operator similar to mutation operator developed by Coello Coello et al. (2004) . This mutation operator improves the exploratory capabilities by extending the covered range of decision variables at the beginning of the search.
The numbers of particles affected by mutation operator are decreased using nonlinear function based on the number of iterations. Fig. 2 depicts the pseudocode of the proposed mutation operator; where max ( i ) and min ( i ) are maximum and minimum of dimension i obtained until the current iteration, respectively. operator is as follows: 4. GA  X  APSO based RBFNNs method for time series forecasting and parameters of RBF neural networks simultaneously is a mixed integer hard optimization problem ( Schwenker et al., 2001 ) and has been offered to consider ( Schwenker et al., 2001; Aladag, 2011 ). For tackling with this hard problem we develop GA  X  APSO that results in parsimonious RBF neural networks. The proposed algo-rithm contains two main stages. Firstly, architecture (input variable and hidden neurons) of RBF neural networks is represented by GA binary coded chromosome. Fig. 3 depicts the applied coding scheme.

Proposed encoding scheme consists of three parts, representing all of the elements of the RBF neural networks including both architecture elements (input variables and neurons of hidden layer) and parameters (centers, width and weights). First part represents the selected inputs. The position of one gene in the chromosome shows the selected sequence of one input. Number 1 for each gene in fi rst part speci fi es that the corresponding input is chosen, and vice versa. The second shows the number of neurons. The real number inputs/hidden neurons are obtained by transforming the binary number to integer form. For example, assume that the number of the hidden nodes is 37, in binary coding scheme it will be represented using six binary bits and the binary number is 101001. The last part deals with the optimization of the network parameters. These parameters are real numbers and represented explicitly.

GA algorithm framework applied here bene fi ts the proposed algorithm in Melanie (1998) and Burjorjee (2007) . The fi second parts of the coding scheme are investigated by GA only and the last part that is related to learning part of RBFNNs will be optimized through modi fi ed APSO.

Since initial solution affects evolutionary algorithms perfor-mance considerably, we run Matlab toolbox for RBFNNs that use orthogonal least square algorithm and apply obtained solution as initial solution for GA  X  APSO.

Step 1: Initialization . Generate initial population of N individual chromosomes as coding scheme described in Section 4 and set the parameters.

Step 2: Fitness function evaluation .Calculate fi tness function for each potential solution using modi fi ed APSO. Perform proposed APSO for training the fi xed architecture as discussed in Section 4 .
Step 3: New structure selection . Choose individuals according to the fi tness function to generate new solutions using crossover and mutation operations ( Melanie, 1998; Burjorjee, 2007 ) only on the binary part of the selected individuals.

Step 4: If a stopping criterion is met, give the output RBFNN; otherwise, repeat steps 2  X  4.

Step 5: Exploit the obtained parsimonious RBFNNs in step 4 for forecasting time series. 5. Experimental results
We illustrate the proposed method superior performance through two well-known benchmark time series modeling pro-blems: the Wolf's sunspot data and the Mackey  X  Glass time series. These datasets are used in order to an easy comparison with other existing methods. The sunspot data is the number of sunspots and groups of sunspots present on the surface of the sun and obtained from Hyndman . Mackey  X  Glass series, based on the Mackey differential equation, is widely regarded as a benchmark for comparing the generalization ability of different methods. The data of the Mackey  X  Glass time series are obtained from the IEEE Neural Networks Council Standards Committee website (Working Group on Data Modeling Benchmarks, http://neural.cs.nthu.edu. tw/jang/benchmark/ ).

In addition, the developed GA  X  APSO network will be applied to a real dataset: forecasting of relief items demand time series after disaster. Uncertainty of disasters makes it very hard to forecast emergency resource demands after disaster. Also fl uctuation of commodities demand after natural disasters is usually severe and irregular. It increases the complexity of emergency resource management ( Xu et al., 2010 ).
 For two benchmark data sets, two simulations are carried out. The fi rst one only uses the same inputs as other models in order to make a fair comparison; the second one uses a large number of input candidates such that the proposed approach can automati-cally select the proper input variables combination.

The parameters used to run the GA  X  APSO are set as follows: maximum velocity ( V max ) at 18% of the dynamic search range of each variable, as suggested by Shi (2001) . Acceleration coef adapting strategy and bounds are determined as proposed by Zhan et al. (2009) , population size is 35, crossover and mutation probabilities are 0.92 and 0.05 respectively. Maximum generation is 65 for genetic algorithm and 200 for APSO. The mutation rate is set to 1/ L where L is the length of chromosome. Also for each data set, we run 30 replication of the proposed algorithm which the corresponding results are summarized in Table 1 . 5.1. Application to the Wolf ' s sunspot data
The well-known Wolf's annual sunspot numbers are challen-ging in many respects and constitute one of the benchmark data sets in nonlinear time series. This time series is regarded as nonlinear and non-Gaussian and is often used to evaluate the effectiveness of nonlinear models. The plot of this time series suggests that there is a cyclical pattern with a mean cycle of about 9.5  X  11 years. The sunspot data have been extensively studied with a vast variety of linear and nonlinear time series models. The mean square error (MSE) for the training data and the testing data is Year Error Year Error regarded as the objective function of each chromosome to opti-mize the network. 5.1.1. Case 1
The data we consider in this paper contains the annual number of sunspots from 1700 to 1987, giving a total of 288 observations. The data from year 1700 to 1920 (total 221 data) are used as the training set while the data from year 1921 to 1987 (total 67 data) are used as the test set. This division for training and testing data is kept the same with Zhang (2003) to make a fair comparison. Among 30 runs, the results are summarized in Table 1 . Also Table 2 lists the results by Zhang (2003) for comparison. We can see that the forecasting errors signi fi cantly reduces in our method and with the number of hidden nodes of 3, it can be seen that our algorithm really produces a parsimonious network. Fig. 4 (upper) shows the original data and the predicted values for both the training data and the test data. Fig. 4 (lower) shows the prediction error. It shows that the residual series has no pattern. 5.1.2. Case 2
Because the cycle period of the sunspot data is about 11 years, we consider the 11 input candidates y ( t 1), y ( t 2), ... affect the present value, y ( t ). Between these variables, the input vector selected by GA  X  APSO algorithm is y ( t 1), y ( t 2), y ( t 5), y ( t 8), y ( t 11). The best result among 30 runs is listed in Table 3 .
Compared with the results obtained in Case 1, we can see that appropriate selection of inputs can make the prediction error much smaller. Fig. 5 (upper) shows the predicted and target values for both the training and testing data. The difference is also seen on a fi ner scale in Fig. 5 (lower), which shows that the prediction error is smaller than that of Fig. 4 . 5.2. Application to the Mackey  X  Glass time series
The time series prediction based on the chaotic Mackey  X  Glass differential equation is a standard benchmark problem in the areas of neural networks, fuzzy systems and hybrid systems for compar-ing the learning and generalization abilities of different algorithms. The Mackey  X  Glass differential equation was fi rst introduced as a model of white blood cell production equation and generated from the following equation: dx  X  t  X  dt  X  where a  X  0.2, b  X  0.1, c  X  10 are most often used in the previous research ( Harpham and Dawson, 2006; Du and Zhang, 2008; Chen t Error et al., 2005 ). Different values of  X  produce various degrees of chaos.
Similar to other works, the root mean square error (RMSE) for the training data or the testing data is used as the objective function. 5.2.1. Case 1 time delay parameter of Eq. (10) to be  X   X  17, and predict the value y ( t  X  6) from the possible input candidates y ( t 18), x ( t 17), , y ( t ). Two thousand data points are generated with an initial condition of x (0)  X  1.2 based on the fourth-order Runge  X  method with time step  X  0.1. From the generated time series, 1000 input  X  output pairs with the following formats: [ y ( t 18), x ( t-17), ... , y ( t ); y ( t  X  6)] are extracted, in which the pairs are used to train the model while the remaining 500 data pairs are used to test the model.
 algorithm together with some other recent results for comparison.
We note that our proposed algorithm obtains much smaller prediction error than other methods. Fig. 6 (upper) shows the original time series and the evolved RBF network output, its difference is seen on a fi ner scale in Fig. 6 (lower). 5.2.2. Case 2 among 30 runs, the best result is listed in Table 5 . Among these y ( t 14), y ( t 17), y ( t 19).
 appropriate selection of inputs can make the prediction error much smaller. Fig. 7 (upper) shows the predicted and the desired values for both the training and testing data obtained in this case.
The difference is also seen on a fi ner scale in Fig. 7 (lower), which shows that the prediction error is smaller than that of Fig. 6 . 5.3. Application to emergency supply demand forecasting destructive disasters in Iran. The data set of our empirical example is collected from the Iranian Red Crescent. We select three most used critical relief items including: bread, water and canned food. Our selected data set contains one month delivered items from 13
August 2012. Due to characteristics of earthquake and sudden rise in demands, the time series of relief items demand after t Error earthquake has a great deal of irregularity and nonlinearity. Figs. 8  X  10 depict these time series.
 ( Ratnaweera et al., 2004 ), they are compared applying the time series of emergency supply demand. Absolute percentage error (APE) and mean absolute percentage error (MAPE) are used as fi tness function.
 APE  X  y  X  k  X  y  X  k  X  y  X  k  X  100  X  11  X 
MAPE  X  1 m  X  m where y  X  k  X  is the k th output of RBFNNs and y  X  k  X  is the correspond-ing output; m is the number of test data. In order to test the ef fi ciency of our algorithm, the data are divided into two subsets: training and testing sets. The training set is used to compute the network parameters. We use fi rst 24 days as training set and simulate the results for the last week to assess the generalization capability. 30 replications are done for both the proposed and basic PSO algorithms which Figs. 11  X  13 depict the average results obtained from these replications. Also numerical results are represented in Tables 6  X  8 .

PSO in forecasting demands of relief items. The results indicate the effectiveness of modi fi ed features of PSO. 6. Conclusion works (RBFNNs) that solve the problem of time series forecasting is a dif fi cult task because many parameters (number of hidden neurons, input variables, centers, width and output layer's weights) have to be set at the same time. Many authors have shown that evolutionary algorithms can help in fi nding the optimal values for these parameters. Many of existing works use two phase learning methods but performance of RBF networks can be improved through learning methods that simultaneously adjust the whole set of the parameters.
 to determine both architecture (input variables and neurons of hidden layer) and network parameters (centers, width and weights) of RBFNNs simultaneously. Our proposed algorithm generated new architecture applying genetic algorithm (GA) and applied modi fi ed adaptive particle swarm optimization (APSO) incorporated by mutation operator to determine the training parameters ef fi ciently. To evaluate the performance of proposed algorithm, it was compared with several well-known methods. Simulation results indicated that our model has better forecasting accuracy with computational ef fi ciency. Finally a real world case study was presented. The proposed method was applied to forecast the demand of emergency supplies after earthquake. Also the obtained results were compared with the basic PSO which veri fi ed the superiority of our proposed method.
 References
