 Users X  daily activities, such as dining and shopping, inherently reflect their habits, intents and preferences, thus provide invalu-able information for services such as personalized information rec-ommendation and targeted advertising. Users X  activity informa-tion, although ubiquitous on social media, has largely been un-exploited. This paper addresses the task of user activity classi-fication in microblogs, where users can publish short messages and maintain social networks online. We identify the importance of modeling a user X  X  individuality, and that of exploiting opinions of the user X  X  friends for accurate activity classification. In this light, we propose a novel collaborative boosting framework com-prising a text-to-activity classifier for each user, and a mechanism for collaboration between classifiers of users having social con-nections. The collaboration between two classifiers includes ex-changing their own training instances and their dynamically chang-ing labeling decisions. We propose an iterative learning procedure that is formulated as gradient descent in learning function space, while opinion exchange between classifiers is implemented with a weighted voting in each learning iteration. We show through exper-iments that on real-world data from Sina Weibo, our method out-performs existing o ff -the-shelf algorithms that do not take users X  individuality or social connections into account.
 Categories and Subject Descriptors I.2.6[ArtificialIntelligence]: Learning; I.5.1[PatternRecognition]: Models General Terms Algorithms; Experimentation Keywords Boosting; Collaborative Classification; Activity Classification; So-cial Regularization.
Social media platforms have become the most popular means for users to share online happenings about and around them, creating a rich repository of users X  activity information. As users X  activities inherently reflect their habits, intents and preferences, the ability to understand users X  activity information is obviously invaluable for man y services such as personalized information recommendation and targeted advertising. The task of activity recognition in social media, which is the task addressed in this paper, is about identifying what a user is doing at a particular time, given his or her social media postings such as microblog posts.

In the simplest sense, an  X  X ctivity X  can be any action that hu-mans can perform. Examples include activities of daily living, such as doing housework, as well as social activities and entertain-ment, such as watching a movie. Traditionally, activity recogni-tion is performed based on sensor readings obtained from, for in-stance, sensor-enabled environments or smartphones. It is a well-researched area in artificial intelligence, ubiquitous computing and computer vision, with wide applications in health care and smart home systems (e.g., [28, 13]). In social computing and data mining, however, research on activity recognition is just beginning, with much of users X  activity information unexploited.

Note that activity recognition should not be confused with event detection , which has a similar goal of detecting real-life happen-ings in social streams. While an  X  X vent X  can generally be defined as  X  X omething happening at a specific time and place X  [25], most existing work focuses on public events that often generate large-scale or even population-level responses in social media. Examples include the use of Twitter 1 to detect and monitor natural disasters, political events and unusual activities such as protests [24, 17, 32].
The public nature of these events allows detection algorithms to utilize a large amount of aggregated data to, for instance, de-tect bursts or irregularities in topical contents within a short time span [25, 23]. Activity recognition, in contrast, is user-centric. Consider two users X  microblog posts, saying  X  X he referee is day-dreaming X  about a football match. While one user may be a specta-tor watching the match on TV at home, the other can be a commen-tator making live updates about the match. Yet, they could have posted similar contents at about the same time. The user-centric nature of activity recognition thus calls for personalized learning algorithms that can model the diversity and individuality of di ent users. Such a nature also results in data scarcity, because data of a single user are often too limited to train an accurate activity recognition model.

Currently, there exist two approaches to activity recognition in social media. The first is an extraction approach that extracts ac-tivities indicated by certain action verbs. For example, the work in [31] explored the use of Twitter to predict activities that users may perform in the near future, based on tweets mentioning a fu-ture time frame (published before 6 p.m. and contained the word  X  X onight X ) and a predefined activity word ( X  X o watch X ). This ex-traction approach, however, may fall short due to the brief, infor-mal communication style of microblogs. For instance, a post men-http: // www.twitter.com tioning  X  X amburger X  may indicate that one is eating, even without action verbs like  X  X ating" or  X  X aving". The second approach casts activity recognition as a classification task. The work in [18] used Conditional Random Fields (CRF) classifiers to predict users X  fu-ture check-in locations, which are considered implied activities in the work. The classification approach, although avoiding the need to locate specific action verbs, is not without problems. For in-stance, the aforementioned example of two users posting about a football match shows that the mapping of textual contents to activ-ities is not injective. We therefore need to resort to classifiers that can fully exploit the individuality of users, while addressing data scarcity resulting from the need for personalized models.
This paper proposes a collaborative learning framework to ad-dress the activity recognition task in social media, in particular in microblogs where users can share short messages and main-tain social networks online. Instead of having one  X  X lobal X  classi-fier which cannot capture the individuality of users, our framework maintains many  X  X ocal X  classifiers, one for each user, while allow-ing collaborations between classifiers of socially connected users to alleviate data scarcity for improved classification performance. In the learning process, di ff erent learners compete (within each en-semble of classifiers for each user) and collaborate (across ensem-ble classifiers for di ff erent users) based on a designed mechanism for optimal performance. Knowledge is generated and transferred in a well controlled way, so that the classifier for one particular user can be well informed by others on the social network. Our learning framework is semi-supervised, as it can benefit from par-tially labeled data when unlabeled instances of one particular user dynamically receive labeling suggestions from neighboring users. Experiments on a data set we collected from Sina Weibo 2 , a major microblog in China, validate the advantages of our model over sev-eral supervised baselines and state-of-the-art methods that perform activity classification with global classifiers.

We summarize the distinctive features of our proposed frame-work as follows.
The rest of this paper is organized as follows. Section 2 intro-duces related studies and distinguishes our work from them. Sec-tion 3 formally defines our proposed framework, while Section 4 details the learning process of it. Section 5 describes experimental results before we conclude in Section 6.
The proposed framework is related to two threads of work, namely collective classification, and multi-task / transfer learning. http: // www .weibo.com /
Collective classification is concerned with classification prob-lems on networked data [26], with the basic assumption that labels of connected nodes are more likely to be correlated. Di ff proaches have been proposed to incorporate this auxiliary correla-tion information. For example, some researchers proposed to align the labels of the classifiers [19, 20], while others allowed classi-fiers to borrow features from neighboring classifiers [7]. We also notice that there is one approach named as  X  X etwork boosting X  [29]. However, it assumes that the classifier is learned by a combination of classifiers on a random graph and each node on the graph should have the same set of training instances.

Collective classification has been applied to social media related tasks, such as sentiment analysis [27, 14]. Similar to our frame-work, existing work also uses a network-based regularization, and can be performed in a semi-supervised fashion. The major dif-ference is that in collective classification, network regularization is performed on the group of instances to be classified, whereas ours is on a group of connected but distinctive classifiers. In other words, collective classification maintains a global classifier with inter-dependent instances, which is in clear contrast with our frame-work of collaborative learning with many local classifiers.
Multi-task learning (MTL) [6] is a learning setting where multi-ple related learning tasks are carried out jointly. It has been shown that when task relationships are properly exploited, the overall clas-sification performance can be improved [1, 3, 4]. Our model can be naturally viewed as a special case of multi-task learning, since in our model the classifiers are on distinctive but related tasks, and the training of them is performed simultaneously.

Most MTL algorithms work by exploring unknown task rela-tions [1, 3, 4]. Usually the relatedness between models of the tasks are captured by assuming that model parameters can be well de-scribed in some more  X  X ompact X  representations, e.g, a few tight clusters or a subspace with low dimensionality. These assumptions can either be expressed through a regularization term on a joint loss function [10], or through some hardwired architecture [6]. Another thread of MTL explicitly learns the task relationships [16, 33, 34]. This thread of work usually assumes a reasonable amount of train-ing data for the individual tasks; an amount that is su ffi taining an acceptable model even when each task is trained alone. We do not make this assumption in our task, considering that a user often only has tens or hundreds of text messages on average. Such an amount of data is far from enough for obtaining a reasonable model. In other words, our task su ff ers from a more serious lack of training data.

There are two major characteristics di ff erentiating our model from most other MTL models. Firstly, the measure of modeling related-ness / similarity in our model is rather distribution-dependent, which is fairly unique for our microblog-based classification tasks. In other words, the classification tasks are related not only in terms of model prediction behaviors, but also the distribution of the data they will likely see. Secondly, task relatedness in our model is ex-ploited with a mechanism of dynamic opinion exchange (including training instances and labeling decisions) between related learning agents. This is in clear contrast with the static regularization over model parameters in most MTL models.
In this section, we first formulate our task of text-based activ-ity recognition in social media, and then present our collaborative learning framework. Figur e 1: Illustration of a user X  X  ego graph with notations and settings. The bars represent training instances.
We represent a social network as a graph G = {V , E} , with node set V = { u 1 ,..., u |V| } representing di ff erent users, and { e } , i , j  X  X  1 ,..., |V|} specifying the social relations between them Each user u i has S i short microblog posts, denoted by the set { x resenting the bag-of-word feature vector of the post indexed by ( i , s ). Some of the posts are labeled with predefined activities, e.g. E ntertainment , W orking , or N one . For most of the users, only part of their instances (posts) are labeled. Without loss of generality, we index the labeled instances by { 1 ,  X  X  X  , S  X  i } , and the unlabeled ones by { S  X 
We adopt the one-vs.-rest classification strategy to classify in-stances into activities. For each activity, the label of each x noted as y i , s , therefore takes binary values in { + 1 examine one activity label at a time under the one-vs.-rest strategy, we drop the activity index from the label y i , s for a concise repre-sentation. After applying our model to all activity labels, we can later combine the binary classification decisions to arrive at a final labeling decision. We omit this part and focus on our learning task in the subsequent discussions.

Our learning task is to find a mapping function from the feature vector x i , s to the label y i , s . In our collaborative learning frame-work, we assign a  X  X ocal X  classifier to each user to accommodate the user X  X  individuality. Formally, our learning task is to find: where F i (  X  ) is the classifier of user u i . This formulation grants us more modeling flexibility to describe the variety of users, but it naturally demands more training data for each user and apparently aggravates the problem of data scarcity. To alleviate this problem, we introduce a collaboration mechanism between the classifiers of di ff erent users, based on the following intuitions: Fig. 1 shows an example of a user u 1  X  X  ego graph, which contains his friends u 2 , u 3 and u 4 . Each user has his own data and classifier. In what follows, we define the formal notations of classification and collaboration.
Our model can take the the directionality (i.e., e i j , e strength of edges (i.e., e i j  X  R + ) into consideration, although in experiments we assume the edges are binary and undirected.
We define the following cost function over the classifiers F |V| } in a social network: where J 1 (  X  ) is the margin-based cost defined on the local data of each classifier, and J 2 (  X  ) is the collaboration term promoting opin-ion exchange between classifiers.
 The cost function for the margin of local data (including labeled and unlabeled) is: where The second term J 2 (  X  ) in Eq. (2) serves as a social regularization term to agitate the opinion exchange between classifiers, for better use of both labeled and unlabeled data distributed across the social network. Our design exploits two intuitions: Thus, the social regularization term is defined as follows: with the dis-similarity between two neighboring classifiers, F F , measured as the di ff erence between their classification deci-sions on the local data possessed by each of them:
Assuming that a user can only see messages posted by his / friends in a social network, the above equation captures our intu-ition that a user u i  X  X  classifier should work similarly with his friend u j  X  X  classifier on both users X  local data 4 . It is easy to un-derstand that although the similarity is explicitly defined on two socially connected classifiers, one classifier can still be a another that is several hops way through intermediate classifiers. However, one appealing property our model possesses is that, a classifier for one user is fairly robust to irrelevant social circles of his friends. We explain this with the following example. Suppose a college student Bob knows his friend Jerry in school, and Jerry has another social circle of colleagues in Microsoft, including a senior researcher Chris . Although the classifier for Bob may be a by that of Chris through Jerry , this e ff ect is minimal in our model if Bob  X  X  activity mentioning (e.g.,  X  X ent to a pub today, got totally drunk  X   X ) is significantly di ff erent from Chris  X  X  (e.g.,  X  X  am at Lake Tahoe for NIPS. Interesting workshops this year! X ).
This section is devoted to the learning process in our collabo-rative classification framework. Simply put, our learning model extends the conventional boosting model [21] to maintain multiple classifiers, one for each user, and introduces a new collaboration mechanism for socially connected classifiers.

The learning process can be viewed as a generalization of single classifier boosting, with much enriched mechanisms: The competing mechanism inherited from traditional boosting al-gorithm ensures a large margin on the local data owned by each user. As empirically verified by much work, this large margin of-ten induces good generalization property of the classifiers. The collaboration mechanism, on the other hand, greatly alleviates the scarcity of training data for each individual classifier, while still maintains the locality of data transferring. It therefore avoids the spurious regularization problem faced by many collective classifi-cation models [14].
Our learning process is based on a collaborative boosting pro-cedure, formulated as gradient descent in learning function space.
Here we assume the graph is undirected, and we assume an edge between two users only if they follow each other. For directed graphs, we only check the data from one direction.
 Since the process involves an iterative algorithm, we introduce an index t in our notations hereafter to indicate the learning iteration, and T to denote the total number of learning iterations. For in-is that at the final learning process.

As a boosting algorithm, F i , T is learned through greedily grow-ing the ensemble of base learners f i , t [11]: where f i , t can be simple classifiers like decision trees, and T is the total learning steps when F i , t converges on training data. To make the representation clear, we call the ensemble F i , t (  X  and use the term learner for f i , t  X  X .

Similar to the functional space gradient descent for single-task boosting [21], we define the optimization procedure of the cost function for each function F i , t (  X  ) as: where the inner product  X  F , G  X  = the space of x  X  X , and  X  is a small value for first order Taylor expansion.
 According to Eq. (9), the desired direction for each classifier F function from a set of candidate functions from the function space, f + 1  X  F , to approximate the direction of each step. F i , combination of functions, or more rigorously, F i , t  X  lin( ease of training, we use a f i , t + 1 which itself is the sum of two base learners, each taking care of a term of J (  X  ) in Eq. (2) where we have beled and unlabeled), while the second term f (2) i , t + ion exchange between the classifiers of di ff erent users.
In the remainder of this section, we explain in detail how the two terms jointly determine the gradient descent, and how we control their interaction with a deterministic annealing process for achiev-ing a better local optima. values 5 : defined on data X i . A new classifier is then defined for user u maximizing the inner product of  X  X  X  F i J 1 ( F i , t ) and f
F or unlabeled data we take the sub-gradient [5] over  X   X  Figure 2: Illustrations of the training process for user u flow of classifiers X  opinions, and bars represent training instances. to finding a function that minimizes a weighted error: given the fact that y f ( x ) = 1  X  2 I [ y , f ( x )] , and I by: where Z 1 = tion constant.
 local data of u 1 and the classifier F 1 , t from previous step.
Taking the negative derivative of J 2 ( F i , t ) for F i where N i stands for the neighbors of user u i with cardinality For each u i , we generate a base learner f (2) i , t + 1 own training samples as well as those from its friends j  X  X . During the training process, the opinions of di ff erent users X  classifiers are exchanged through the following two types of pseudo labels . Pseudo labels of this type are defined on user u i  X  X  local data: with corresponding weight where Z 2 is a normalization constant:
As suggested in Eq. (18), if u i  X  X  classifier F i , t disagrees with the vote from its neighbors we flip the label of x i , s to train the new classifier. Intuitively, when more friends are involved in the voting, the weight associated with the instance gets larger. As shown in Fig. 2(b), friends of u their classifiers to u 1 and vote on the data of u 1 for learning the The second type of pseudo labels is defined as: and the corresponding weight is: This means when checking with its friends X  data x j , r , if the classifier F (  X  ) is di ff erent from its friend X  X  classifier F j , t margin on the data, we flip the label of x j , r to train the new classi-fier. As shown in Fig. 2(c), friends of u 1 pass both their data and classifiers to u 1 and the di ff erence between their classifiers and u classifier is measured on their data for learning the learner f
Putting the learning with the two types of pseudo-labels together, the learning of f (2) i , t + 1 can be summarized as: where the last equation corresponds to a classification problem with di ff erent weights on its own data and its friends X  data. The classification error is computed as:
In this case, both the friend X  X  data and opinion on classification (margin on their data) are passed to u i .

Note that the unlabeled data, although making the local fidelity term J 1 non-convex, serve as the medium in the opinion exchange term J 2 . By using social regularization, i.e., by friends X  voting and by checking friends X  data, we use f (2) i , t + 1 to correct the margins of f + 1 in each step. Thus, the regularization is regarded as a help from friends to jump out from the local optima of the unlabeled data X  X  margin. In the next section, we describe in more detail our approach to getting out of poor local optima with a deterministic annealing technique.
In this section, we present a learning process that can control the gradient descent method. In general, we need to find a greedy solution to the optimal J ( F 1 ,..., F |V| ). However, if we emphasize too much on the regularization term, all the classifiers tend to be strongly a ff ected by its neighboring classifiers. In our problem, we want to maintain the flexibility of individual classifiers, which means we care more about J 1 ( F 1 ,..., F |V| ). Therefore, we design a mechanism that can decrease the objective function of both J ( F ) and J 1 ( F 1 ,..., F |V| ). This is done by first relaxing the controlling parameter  X  in J ( F 1 ,..., F |V| ) to be dynamically changing by itera-tions, and then we learn a global parameter for all the users to make sure the cost function for the network is decreasing in each step. In each iteration, we choose a  X  i , t + 1 to make sure all J function for F i , t in the descent direction in function space is: where  X  i , t + 1 is the parameter that can guarantee J ( F  X  (23) respectively. The following lemma describes how to choose a suitable  X  i , t + 1 .

L emma 1. Denote as: a i =  X  X  X  J 2 ( F i , t ) , f (2) i ,  X  X  X  We have [30] : Given the learning objections of f (1) i , t + 1 and f (2) (23) respectively, we have a i &lt; 0 and c i &lt; 0. The objective in Eq. (26) can be rewritten as: L (  X  i , t + 1 ) = a i  X  2 0 , c i &lt; 0. The Lemma can be proved using some quadratic function analysis, which we omit here due to space limit.
 Since we have |V| users, we need to find a  X  t + 1 to form an objective function that satisfies all users X  functions. The following remark describes how we select the  X  t + 1 at iteration t + 1.

R emark 1. Deterministic Annealing: We select  X  t + 1 = min to guarantee all the learners lead to the decreasing of the function-als J ( F 1 ,..., F |V| ) and J 1 ( F 1 ,..., F |V| ) .
 After determining the objective function for each iteration, and finding the possible direction of gradient in the function space, we want to go as far as we can using the Newton X  X  method. We opti-mize the step size  X  t + 1 for the overall objective function of at itera-tion t + 1 as follows:
By taking the derivatives of  X  t + 1 , we define (the detailed deriva-tion is omitted due to the limited space): g (  X  t + 1 ) =  X  lows: until convergence.

T heorem 1. Convergence: The deterministic annealing proce-dure will finally converge to a local optimum of maximum margin of each classifier.
 We skip the full proof here due to space constraint. The intuition is that since we choose a direction to ensure that the objective func-be less than zero. Moreover, when J 1 ( F 1 ,..., F |V| ) converges to local optimum (follow the way of proof in [21]), c i  X  X  tends to be zero. Then the selected  X  i , t + 1 tends to be zero given the constraint in Lemma 1. In this case, the social regularization does not a the further update of classifiers. The final base learner of this pro-cedure is the same as semi-supervised boosting on each classifier, but the margin on unlabeled data has been refined through the pro-cedure.
Putting the contents in Sections 4.1 to 4.4 together, we get our collaborative boosting algorithm, known as SocialBoost . Algo-rithm 1 provides the flowchart of our algorithm.
 Algorithm 1 SocialBoost Algorithm.

In this section, we present our experiments on real world data collected from Sina Weibo. All algorithms are evaluated based on classification accuracy.
We compared our algorithms with a total of six baseline meth-ods, summarized as follows.
 Linear Classification Without Regularization: The first baseline is a generic linear classifier: Graph-regularized Classification: Inspired by the collective clas-sification algorithm in [14], we considered two types of graph-based regularization on top of LS :
In both LS-Data and LS-Net , classification is performed with a linear classifier with square loss and graph Laplacian regulariza-tion. Note that both methods perform instance-level regularization. Boosting Algorithms: We considered the following three boosting-based methods with varying types of learning settings:
W e experimented with both supervised and semi-supervised set-tings, and found the square loss yields superior or comparable per-formance compared to hinge loss and logistic regression loss. Our experiments are based on a data set we crawled from Sina Weibo. The full data set contains short messages and social graphs of about 10 million users. The original data set is unlabeled, and we exploited the  X  X heck-in X  information provided by users to derive the gold standard for evaluation. A check-in indicates the point of interest where the user was located. About 0.85 million users have at least one check-in. We describe below how we constructed the final data set for experiments from this set of users.
 Constructing user sub-graphs: For each user, we traversed his social graph to find his followees who (i) follow him mutually, and (ii) also have at least one check-in. Then we build a node-induced sub-graph from this set of users. We call such sub-graph a user sub-graph , and we include only mutual followers to ensure that each sub-graph represents a reasonable community with a closely connected group of users. There are 60k user sub-graphs containing more than 10 nodes, and 316 containing at least 50 nodes. We focus on these 316 sub-graphs hereafter.

We further filtered out users in the 316 sub-graphs with fewer than 10 messages to obtain our final data set. After filtering, each sub-graph contains an average of 24 . 49  X  11 . 58 users and 57 39 . 38 edges. Each user has on average 23 . 88  X  16 . 93 messages with check-in information.
 Assigning activity labels: We used a rule-based program to infer the activity associated with each message based on its check-in lo-cation, posting time and named entities mentioned in the text. We identified a total of 33 activities, which we group into 3 main cate-gories (surrogate labels): 1. E ntertainment : include watching movies, going to Karaoke, 2. L iving : include having meals, staying at home, going to spa 3. O utdoor : include sports, travel, waiting for bus, etc. We engaged an external annotator to manually examine about 3,200 messages. The inferred labels are found to be correct for over 90% of them.
 Text preprocessing: We used the Stanford Chinese segmenter tokenize the messages, and filtered out stop-words based on a list of 1,208 common Chinese words. We used Mallet for the rest of preprocessing and for converting the messages into feature vectors. We demonstrate the superiority of our algorithm in two settings. First, we test all the algorithms on each of the 316 sub-graphs (sin-gle sub-graph setting). Second, we combine some of the sub-graphs into a larger graph to form a multiple sub-graph setting, and the idea of which is to create a network with multiple circles of users. In each experimental trial, we split the data set into a labeled train-ing set (30%), an unlabeled set (50%) and a validation set (20%). Experiments are performed in a transductive setting [15] with 10 random trials. Reported results are averages of the 10 trials on the unlabeled sets. The validation sets are used for early stopping of the boosting-based algorithms.

Among the tested algorithms, LS , AdaBoost and MultiTaskBoost are supervised methods. LS-Data , LS-Net , SSMarginBoost and SocialBoost are transductive semi-supervised approaches, where unlabeled data are involved in the training process.
 Table 1 presents the results of the various algorithms on three ran-domly selected sub-graphs. We first observe that SocialBoost out-performs the other algorithms in most cases. LS-Data and LS-Net only achieve slight improvements over the LS algorithm. Both SSMarginBoost and MultiTaskBoost beat their generic counterpart AdaBoost on the three graphs, showing the advantages of the semi-supervised setting and exploiting task relationships.

In Fig. 3, we summarize the average performance of all algo-rithms over the 316 sub-graphs. We can see that SocialBoost per-forms the best on average for all three activity categories. We now compare the performance of the di ff erent algorithms in the multiple sub-graph setting, formed by combining the three sub-graphs described in the last experiment. Table 2 reports the results on the combined graph, showing that SocialBoost still outperforms all other algorithms.

One interesting observation we make is that the instance-level regularization in LS-Data and LS-Net reduces classification accu-racy. Our conjecture is the following. The social relations between distantly connected users (e.g., friend X  X  friend) are in general fairly weak, especially when the users are from two di ff erent circles of their common friends. These distant connections, however, still play a rather important role in the regularization given the large number of them. To be more specific, for LS-Data , the text simi-larity graph could get rather noisy with more messages in a large so-T able 2: Comparison of di ff erent algorithms on a combined graph. (# node: 59, # edge: 167, # message: 1186) cial graph, which may hurt the performance of the semi-supervised learning algorithm. For LS-Net , the message-message similarity is derived from the social relations between users. The assump-tion behind this is that text messages of a user and those of his friends tend to mention the same activity. The regularization could easily become harmful when too much regularization is added to one global classifier, resulting in a form of under-fitting. We want to note here that activity classification might be more sensitive to users than tasks like sentiment analysis [14]. One can assume that one user X  X  sentiment can be consistent on social media whereas ac-tivities can be more diverse.

On the other hand, SocialBoost is more robust to distant con-nections of a user. As we stated in Section 3.2, in SocialBoost , a classifier is only influenced by its neighbors on instances they own (which are a subset of the messages they can both see with the Weibo policy). As a result, a classifier of user u i is less a by that of a two-hop friend u k , if u k is from another social circle and has little in common with u i . In an extreme case, if u always talk about di ff erent things and have no common word fea-tures in their messages, then their classifiers can only be a by instances owned by their common friend, say u j . Moreover, if u j  X  X  messages deviate greatly from u i  X  X , then the decision from u  X  X  classifier usually has a small margin and hence less weight in voting.

Knowing that the linear classifiers do not perform well on a net-work with multiple user sub-graphs, we further create a larger net-work with 38 sub-graphs to study the performance of the boosting-based algorithms. Table 3 shows the results. When there are suf-ficient labeled data, the large amount of unlabeled data may not help as much as in the case when labeled data are limited. As a result, SSMarginBoost does not show improvements over Ad-aBoost . Moreover, the global classifier learned by MultiTaskBoost from all data eventually dominates the classification results. For SocialBoost , as it always considers local data of users, it does not depend heavily on the size of training set. Furthermore, it is always able to benefit from social network information.
In this section, we investigate the e ff ect of social relations on the performance of SocialBoost . The experiments are designed as follows. We alter the connections between users in the user-sub-Table 3: Comparison of di ff erent algorithms on a combined graph of 38 sub-graphs. (# node: 812, # edge: 1,431, # message: 18,169) graphs to introduce noise / randomness to them, apply SocialBoost to the altered sub-graphs, and compare its performance with that achieved on the original sub-graphs. If social connections are in-deed useful for activity classification as we expected, then there shall be a decay in classification performance as randomness is in-troduced to the original sub-graphs.

We consider the following three ways of altering the original sub-graphs, with increasing noise levels / spurious connections: Table 4 reports the performance of SocialBoost under these alter-ation settings, on the combined graph of three sub-graphs used in last experiment. Not surprisingly, SocialBoost works best on the original connections. Its performance deteriorates as the number of spurious connections increases, and is the worst when connectiv-ity is completely reversed. These confirm the usefulness of social connections in boosting classification performance.
 Table 4: Performance of SocialBoost on the original social con-nections and under the three alteration settings. (# node: 59, # edge: 167, # message: 1186)
We also tested the runtime of SocialBoost with varying numbers of users in the network, and compared it with MultiTaskBoost . The base learner is a decision tree with a maximum depth of 3. Starting with a network with 10% of randomly selected user sub-graphs, we
W e experimented with di ff erent percentages of random edges and observed similar qualitative results. progressi vely add another 10% of randomly selected sub-graphs to the network until all sub-graphs are added. When the number of user sub-graphs in the network increases, the number of users as well as the number of text messages increase monotonically. The final network combining all 316 sub-graphs contains 5,787 users, 14,124 edges and 136,594 text messages. In each experimental trial, we run both algorithms for three learning iterations and record their runtime results. Fig. 4 shows that SocialBoost requires more runtime than MultiTaskBoost , and its runtime grows superlinearly with respect to the number of users. This is because when a user is added to the network, our algorithm will actually perform learning not only on the user alone, but also on his / her friends for opinion exchange.
 Figur e 4: Runtime results of MultiTaskBoost and Social-Boost .
This paper presents a collaborative boosting algorithm for the ac-tivity classification problem in microblogs. Our work is motivated by two properties of the problem, namely users X  individuality and data scarcity resulted from the need for personalized classifiers. We propose a collaborative learning framework that maintains a classi-fier for each user, while allowing opinion exchange between clas-sifiers that are connected through social relationships based on our proposed collaboration mechanism. We show through experiments on real-world data from Sina Weibo that our algorithm outperforms several state-of-the-art algorithms and baselines.
 We conclude by outlining our future work. Firstly, the proposed SocialBoost algorithm in essence maintains classifiers distributed over users in a social network. We would like to try implement-ing our algorithm in a distributed environment and experimenting with larger data sets. Secondly, we plan to explore more applica-tions of our algorithm. While the work in this paper is motivated by the nature of the activity classification task, we expect our algo-rithm to be generally applicable to other tasks with a similar nature. One such task may be sentiment analysis, for which it may be de-sirable to maintain personalized sentiment classifiers for individual users while exploiting their friends X  opinions for better classifica-tion results. Another example would be pattern-based information extraction, where we are interested in the e ff ectiveness of using so-cial regularization to bootstrap extraction patterns induced by base learners from social voting. We appreciate the kind suggestions and helpful discussion from Dr. Hang Li when writing this paper. We would like to thank the reviewers for their helpful comments to improve this paper. We also thank the support of Hong Kong CERG grants 621211 and 620812.
