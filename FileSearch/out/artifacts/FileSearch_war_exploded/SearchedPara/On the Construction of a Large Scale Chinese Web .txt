 Test collections are considered as the standard dataset for evaluating the effectiveness accelerating the research of IR. The good quality of an IR system performed in small scale test data. An IR system can be taken as practicable one only when it is capable to give a satisfying result under the evaluation of a large scale test collection, which is the key to validate and improve IR techniques and systems. 
Because the Web enjoys a rapid growth on information volume and a great diversity of subjects, a popular way to construct a large scale test collection is to make the Web the superset of the collection  X  that is the reason why it is known as the web test collection. The English web test collections provided by TREC[5] orient the English IR, and the Japanese web test collection provided by NII[6] can serve for the Japanese IR. But a coequal scale Chinese web test collection could not be available by researchers before our work. To promote technologies of the Chinese IR, we built the CWT100g, and in this paper we introduce the methodology on building such a large method is not limited to the Chinese language; and we hope it contribute to build test collections of other languages. 
In this paper, after introducing terms, we first present the goals and design Finally, we cover related works in section 6 and conclude in section 7. query set and the relevance judgment set [7]. 
A document set is a collection of documents whose contents are used for text analyses by IR systems, i.e. they are the direct objects processed by IR systems. This set is the epitome of its superset. queries is usually between a few score to several hundred queries. 
A relevance judgment set is a collection of standard answers for queries. It is used to compare IR systems X  returning results under given queries. The more an IR system is close to the relevance judgment set, the higher quality it gets. To keep the authority convictive in practice. 
Herein, we sometimes use the terms  X  documents  X ,  X  queries  X  and  X  relevance judgments  X  to denote the above three sets respectively. 
Host name is an identification of a certain web site. Usually, a site can be identified the most popular host name as the identification of each web site. For example, both  X  X ww.pku.edu.cn X  and  X  X opher.pku.edu.cn X  point to the web server of Peking that of the latter,  X  X ww.pku.edu.cn X  is taken as the popular host name of this physical web site and the  X  X opher.pku.edu.cn X  is filtered in the process of the site selection. 3.1 Three Considerations to the Documents be good eno ugh to model the Web. To qualify the epitome of the Web, the documents should be broad in subject domains and large enough in size. Kennedy and Huang et documents  X  1) should the documents be sampled statically or dynamically? 2) to what extent do the documents represent its superset? 3) and what is a suitable size of documents satisfying the needs of both general and special IR goals? 
For the first consideration, we prefer a static rather than a dynamic sampling method in getting a large scale documents from the Web. A static method means web pages are crawled during a specific period meanwhile a dynamic method means pages are crawled at a random time and added to the documents continuously. It seems that evaluations of IR systems. Because when the scale of documents is large, manpower consumed of finding relevance judgments are difficult in acquiring even if the documents do not change, let alone the documents vary as the time goes. Our selection is a trade-off consideration for the representative of documents to its superset as well as the convenience of evaluations of IR systems. The data size of documents is a couple of orders of magnitude smaller than the occasion requires. Generally, constructing a large collection adopts a static manner. and subjects of the documents should be diverse and keep the balance. Although it is objectively, accommodating the large, diverse and balance sampling subjects is documents is in contradiction with manpower exhausted in confirming the relevance judgments. The more documents are sampled, the higher the representative would get. Although this circumstance brings a higher representative and balance, too many documents will return a mass of results for a query and exhaust much human labor in finding out relevance judgments when doing evaluations. Fortunately, by using the concentrated, and human labor can be cut down. mainstream. A dataset with this order of magnitude not only can be delivered conveniently, but also is capable to meet the demand of the link analysis of a web IR system. The link analysis proved to improve the precision of a web IR system. 3.2 The Practice of the CWT100g The CWT100g is the achievement of practicing to model the Chinese Web. It includes the documents, the queries, and the relevance judgments. 
The documents come from 17,683 sites out of the 1,000,614 web sites that are by Tianwang Search Engine [11] before Feb. 1, 2004. After selecting and processing to both sites and web pages, we sampled 5,712,710 pages, about 90 gigabyte disk spaces. Each page in the collection has a "text/html" or  X  X ext/plain" MIME type received from the HTTP server response message. If the crawling capacity of Tianwang, 0.2 billion unique URL pages accumulated on Feb.1, 2004, is equally as large as the corpora of the Chinese Web, we presume that the nearly 6 million pages is a moderate size to represent the Chinese web pages. The queries were numbered TD1-TD50 for a Topic Distillation (TD) task and NP1-NP285 for a navigational task. The topic distillation task involves finding relevant homepages, and the navigational task is to find out a Home/Named Page (NPHP) [12, 13]. They are sampled from user query logs of Tianwang between Apr. 2002 and June 2004, and then compiled by assessors. 
The relevance judgments, 1,178 for the TD queries and 285 for the NPHP queries, are constructed by taking advantage of both Tianwang techniques on the web search and the improved pooling method. 
After the brief illustration, we will focus on the construction of the documents, and then show how to deal with queries and judgments. The documents are the basis of a test collection and require a reasonable constructing strategy, while the queries would evaluation phase. The first important thing in constructing the CWT100g is to ensure the document highly simulating web test-bed. It comes down to how to sample web sites and crawl pages within them. 
The characteristics of the Chinese Web help determine the sampling strategy. In choosing the candidate sites to form the CWT100g, and then discuss the choice strategies to sample sites and pages. 4.1 The Characteristics of the Chinese Web Several famous works have been known in detecting the characteristics of the Web in [3, 4, 14, 15]. 
As to the study of the Chinese Web, Yan and Li [16] found that in the early 2002 there were about 50 million pages and 50 thousand web sites with unique IP addresses, the average size of a page was 13KB, and the Chinese Web was highly connected; Meng et al [17] validated that in 2002 Tianwang could cover 37% of the total Chinese web pages, and 50% of the high ranked pages. To keep an overall coverage and a fresh update, Tianwang adopts an incremental crawling strategy and unique URLs among 1,000,614 different sites. 
Based on the above works, we got a report on the relationship between the site number and the page number within various sites. All the data were fetched from the Tianwang data on Feb. 1, 2004. Figure 1 shows the distribution of pages within sites in a linear scale plot. Y-axis denotes the site number and x-axis denotes the number of pages within a site. Figure 1 shows that sites with a large number of pages are small amount and those with a few pages are large amount. For example, we found only 1.9% sites held upward of 500 pages, whereas most sites held pages less than 500. The distribution is so extreme that if the full range was shown on the axes, the curve would be a perfect L shape. To verify whether the distribution accords with a power law as being claimed by Adamic and Huberman [3, 4], we plotted Fi gure 2 according to the data provided by Tianwang. 
If we directly show a log-log scale plot for the same data of Figure 2, the tail end of [18] does. A linear relationship now extends over 6 decades (1-10 6 ) pages vs. the earlier 4 decades: (1-10 4 ) sites. The slope is 0.68, which makes it a Zipf-like [19] distribution. selection and the web crawler, which are illustrated as follows. 4.2 The Process of the Site Selection 
The URL all set , the superset (the Chinese Web) of the CWT100g X  X  documents, is the collection of 0.2 billion unique URLs crawled by Tianwang. The host names were sites mentioned above, facilitates to eliminate the disturbance of sites with extremely fewer or larger pages. Besides, the module of the filtering sites and the module of the unstable and spamming sites and limit the remnant candidate sites within the Chinese scope in order to build the model of the Chinese Web. 4.2.1 The Module of Restricting Scopes Since the superset provided by Tianwang is within the Chinese scope. The module of restricting scopes is simplified to remove the host names with weird suffixes. According to the supplier of domain name registrations, http://www.net.cn, and the CWT100g are restricted in host names ending with "cn", "com", "net", "org", "info", "biz", "tv", "cc", "hk", "tw". 4.2.2 The Module of Filtering Sites brought by the URL all set ; 2) filtering out host names with non-80 ports, 3) removing names, 5) distinguishing the importance of names, 6) and finding out the sites built by parsing strings of names, step 4) and 5) are finished online by site detecting programs, and the step 6) is done semi-automatically. Supposing D 0 denotes the initial URL all set. After each step, it will produce a new following steps is based on the previous ones. find out different host names standing for the same site, and keep the popular name as step are calculated as: 4.2.3 Determining Sampling Sites To support algorithms based on linkage analyses, it needs to preserve sites with enough links. Each site has more than 10 out links. 
For determining sample sites for the CWT100g, the first step is to estimate the number of sites needed by it. Supposing the size of a single web page (a document) is 15KB[16], the total 100 gigabyte will contain 7 million pages. According to the above (7m/0.2b=X/500000 =&gt; X =(7/200)*500000) sites. Considering some sites maybe fail CWT100g. 
Up to now, results are nearly out of noise data. Again we study the distribution of figure 2. The result repeatedly shows that the distribution of pages within sites of the Chinese Web accords with a Zipf-like law. Meanwhile, it illustrates that the existence of noise sites does not affect the distribution. 4.3 The Strategy of Document Selection After determining sampling sites D 7 , the next step is to crawl pages in terms of them.  X  X lain/html X  and  X  X lain/text X  types are widely used and easily indexed by IR systems, we only preserve pages whose MIME types are one of them. If the pages are written Files with DOC and PDF formats are excluded from the documents. Due to difficult in judging the validity of a dynamic page, URLs with question marks are gotten rid of the documents. 
There are two main issues in the crawling process, the storage format for raw web pages and the strategy of crawling pages. The former requires: 1) less disk spaces, 2) parallel so that all pages crawled within a short interval in time, such as 10 to 15 days, to stand for a web snapshot of a certain period; 2) crawling all static pages on sites as completely as possible. To limit redundant pages from a large site, the crawling process will be broken off when the size of the log file is larger than 2 gigabyte. 
The strategy of crawling pages: we use the modified version of TSE [21] as the crawling system whose strategy is similar to the breadth first crawling method. Because sites in D 7 are all independent each other, it is easy to parallel the process of crawling pages. The detail is described as follows. group is tokenized by an unique filename to stand for a subtask, such as, xaa, coarse granularity. We use a task as a ba sic unit to allocate the workload. Because the distribution of pages within sites in the Chinese scope obeys a Zipf-like law, the sites sampled later will have larger amount of pages than those before and need more crawling resources to crawl pages. Through assigning different number of tasks among crawling machines, we make the workload balanceable. 2) In the machine executing one or more tasks, we start a TSE module for each Each TSE module has 10 threads working together, and pages crawled are stored into those large sites, the TSE stops when the size of its log file is larger than 2 gigabyte. overriding files with same names, each file name is renamed to append a suffix of the subtask name. 4.4 The Properties of Documents and Experiments among 17,683 sites in the CWT100g X  X  documents. 
In the documents, there are 4,814,747 pages with a non-zero out-degree, and the total number of links is 127,519,080. Each page averagely has 26 links. The distribution of out-degree among pages is shown in Figure 4. It is a power-law distribution. There are 4,252,696 (74%) pages encoded with GB2312, 295,274 pages with GBK, 12,230 pages with BIG5, and 254,547 pages with other types, such as ISO-8859-1, ISO-2022-CN, and US-ASCII. 
The CWT100g is now freely available to social circles [1], and has been designated as the web test collection of SEWM-2004 Chinese Web Track [1] and HTRDPE-2004 [2] for evaluating all participant IR systems. After completing the first part of the CWT100g, the documents, we will illustrate how to obtain the other two parts, the queries and the relevance judgments. 5.1 The Construction of the Queries CWT100g X  X  queries are sampled from user query logs of Tianwang between Apr. state of the Web. To avoid queries includes the barren ones whose relevance assessors. A query is also known as a topic in the TREC. Referring to the information on the TREC-2003 Web Track [12, 13], we constructed 50 queries for a topic distillation task and 285 queries for a navigational task. 5.2 The Construction of the Relevance Judgments Relevance judgments require completeness and a consistency. Completeness means that all relevance documents are judged relevantly. Consistency means that all persons will subjectively consider the relevance of some documents with the same judgment. 
The CWT100g X  X  relevance judgments include two parts. One is for the topic distillation task; the other is for the navigational task. The latter is easily constructed to create. Here we use the improved pooling method to get the final set. 
The traditional judgment pools are created as follows. 1) Participants submit their retrieval runs. 2) Assessors choose a number of runs and merge them as a pool for the specific queries. 3) For each selected run, the top X documents are added to the pool because the relevant information is incomplete in the pool and the evaluation will be pooling plus method that yields effective judgments with the help of search engines, participants are simulated. the results of 8 groups (http://www.cwirf.org/2004WebTrack/result.htm) and those of five search engines including Google, Yisou, Baidu, Sogou, Zhongsou. Across the 75 TD queries, 1,178 pages were judged releva nt to 52 queries (average of 22.65 pages per query). 23 (75-52) queries were discarded due to the queries with too many or too few relevance judgments. query set and the relevance judgment set [7]. Web test collections developed include: VLC2, WT2g, WT10g, .GOV, .GOV2[5], collections is getting bigger and more complete. 
In the areas of constructing test collections, Kennedy and Huang et al [8, 9] proposed three aspects on the considera tion of the representative of documents. web test collections. The WT10g [22] has 1.7 million web pages, taking VLC2 as its superset, and the relevance judgments coming from TREC451-500  X  from search limited to the English pages and  X .gov X  domain. It is neither a model of the Web nor a suitable test collection for other language IR systems. The pooling method [10] makes the range of constructing relevance judgments concentrate, and cut down human labor, but it will fail whenever there are few participants. CWT100g takes the Chinese Web as its superset, and aims to sample 100 gigabyte web pages to model the Chinese Web. If we follow the way of the TREC to build the from a special domain name, the data content would be neither popular nor wide enough to represent the Web. Thus the CWT100g adopts the strategy of sampling sites and crawling all their pages. For those extreme large sites, the crawling process stops when the size of log file is larger than 2 gigabyte. The CWT100g has been functioned as a test-bed and benchmark of the Chinese IR and proved valid and useful. Its documents, queries and relevance judgments, and the pooling plus method are applied to evaluate eight different systems. 
The documents of the CWT100g, sampled from the overall China-wide web pages, contribute a model of the Chinese Web. The total 90 gigabyte, 5.71 million documents, provides a reusable test collec tion for the Chinese IR. The approaches on when new trends come. All the toolkits we used are freely available. 
Further, we discovered the distribution of pages within sites of the Chinese Web obeys a Zipf-like law instead of a power law ever been considered and put forward a method of filtering host alias to save about 25% cost while crawling pages. the Chinese scope. This work described therein is supported in part by by NSFC grant 60435020, NSFC Grant 60603056 and 863 Grant 2006AA01Z196. 
