 The big data era is witnessing a prevalent shift of data from homogeneous to heterogeneous, from isolated to linked. Ex-emplar outcomes of this shift are a wide range of graph data such as information, social, and knowledge graphs. The unique characteristics of graph data are challenging tra-ditional search techniques like SQL and keyword search. Graph query is emerging as a promising complementary search form. In this paper, we study how to improve graph query by relevance feedback. Speci cally, we focus on knowl-edge graph query, and formulate the graph relevance feedback (GRF) problem. We propose a general GRF framework that is able to (1) tune the original ranking function based on user feedback and (2) further enrich the query itself by min-ing new features from user feedback. As a consequence, a query-speci c ranking function is generated, which is better aligned with the user search intent. Given a newly learned ranking function based on user feedback, we further investi-gate whether we shall re-rank the existing answers, or choose to search from scratch. We propose a strategy to train a bi-nary classi er to predict which action will be more bene cial for a given query. The GRF framework is applied to search-ing DBpedia with graph queries derived from YAGO and Wikipedia. Experiment results show that GRF can improve the mean average precision by 80% to 100%.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Knowledge Graph; Graph Query; Relevance Feedback
Querying graph data such as information, social, and knowl-edge graphs is always a challenging task. On the one hand, c  X  c  X  due to their complex schemas and varying information de-scriptions, it is extremely hard for users to formulate struc-tured queries like SPARQL without spending hours digest-ing the schema [24]. On the other hand, unstructured search techniques like keyword search are not expressive enough to explore the structure of data to the maximum extent.
Graph querying [37, 17, 24], where users formulate infor-mation needs as graph patterns based on their own knowl-edge and vocabulary and look for matches in a graph, was recently under active investigation as a promising search technique for graph data. We focus on graph query over knowledge graphs in this work. Knowledge graphs contain a wealth of valuable information, with nodes representing en-tities and edges representing various relations between en-tities. Recent years have witnessed the blossom of large-scale knowledge graphs, such as DBpedia [18], Freebase [8], Google's Knowledge Graph [1], and YAGO [31].

There are other query forms for knowledge graphs that can be converted to graph query, such as logic query [7], natural language query [6], and exemplary query [15, 22]. Graph query can also serve as an intermediate query form to support question answering [42]. Figure 1 shows a graph query \Find a Toronto professor at age of 70 who joined Google recently," and its possible match. We put a question mark \?" on the node we are searching for an answer.
Since the query is formulated based on the user's own vo-cabulary, there come vagueness and ambiguity. If 70 yrs means one's age, it implies the person was born around 1945 . Toronto is possibly a match for University of Toronto . Fur-thermore, Google and Geoffrey Hinton are not directly con-nected in the knowledge graph, but by a third node, DNNre-search , built on the fact that Google acquired DNNresearch founded by Geoffrey Hinton . For this query, Prof. Hinton shall be returned as a candidate answer.
The problem of answering a knowledge graph query is to nd the matches of the query in a knowledge graph and sort them with respect to a ranking function. The goal is to re-turn the best set of answers a user is looking for, a central problem of accessing knowledge graphs. The usage of graph query is not limited to knowledge graphs. It is also evident in e.g., social networks [33], cyber security [11] and data centers [41]. The task of answering graph queries is very challenging for two reasons. First, the vocabulary of differ-ent users can vary dramatically. According to a prominent study on the human vocabulary problem [13], about 80-90% of the times two persons will give different representations when they are asked to name the same concept. Second, the underlying data graphs are highly heterogeneous. For exam-ple, Google Knowledge Graph [1] contains over 35 thousand different types of edges, while YAGO [31] has over 350 thou-sand different types of nodes. Such high heterogeneity re-sults in miscellaneous query interpretations.

A graph query technique usually crafts a generic ranking function to answer all queries [37]. However, given the high variety in query formulation, such functions are usually sub-optimal for individual queries, resulting in poor answer qual-ity. A natural solution to alleviate this situation is to develop query-speci c ranking functions, i.e., speci cally tailored for each query. To achieve that, query ambiguity and vagueness need to be correctly resolved, which in turn requires new in-formation in addition to the query itself. Relevance feedback (RF), i.e., users indicating the (ir)relevance of a handful of answers, is one promising source. It is user-friendly, easy to get. Relevance feedback has been studied extensively and proven to be effective in document retrieval [29, 9] and image retrieval [40]. However, despite its naturalness, it has not yet been studied for graph query. Due to the unique characteris-tics of graph query, traditional relevance feedback methods are not directly applicable. In this paper, we de ne and study the graph relevance feedback (GRF) problem, which aims to achieve query-speci c search via relevance feedback, and thus improve the search quality.

Our GRF framework works towards query-speci c search from two directions: (1) Tune the original ranking func-tion based on the preference evidence mined from user feed-back, and (2) enrich the original query with new information discovered from user feedback, that is, the information the user might have in mind but did not explicitly specify when formulating the query. For the former, we propose Query-Speci c Tuning , which uses the original ranking function as prior and adaptively tunes it according to user feedback. For the latter, we propose algorithms to infer more infor-mation from user feedback: (1) Type Inference which infers the expected answer type of query nodes, and (2) Context Inference which infers the context of query nodes. Putting all these together, we produce a query-speci c ranking func-tion, and show that searching with the new ranking function can signi cantly improve answer quality.

While the primary goal of GRF is to improve answer qual-ity, there is an accompanying efficiency issue. Given a new ranking function, the existing relevance feedback methods in IR usually re-rank all of the documents from scratch. How-ever, it could be costly for knowledge graph queries as the number of potential matches could be huge. GRF has a sec-ond option: Re-rank the top-k answers in the original answer list. Certainly, this option might affect answer quality as the best answers might not show up in the original answer list due to the change of the ranking function. Therefore, GRF is faced with a decision: In what situation, it has to search the answers from scratch in order to assure answer quality? In this work, we propose strategies to train a binary clas-si er, which can help control the trade-off between answer quality and query response time.
 Contributions To the best of our knowledge, this work is the rst formal attempt to investigate relevance feedback in graph querying. We summarize the contributions as follows: 1. The relevance feedback problem was formulated for 2. We developed techniques to infer three types of im-3. We identi ed the runtime-quality trade-off in answer-
We evaluated the GRF framework on answering graph queries in real-life knowledge graphs. Experiment results show that it can improve the precision of a state-of-the-art graph querying technique by 80% to 100%, and meanwhile make a good trade-off between quality and runtime.
A knowledge graph, G = ( V; E ), is a labeled graph with nodes representing entities such as Barack Obama and edges representing various relations between entities, e.g., Barack Obama isPresidentOf United States . Each entity is associ-ated with a set of types/classes, and the classes form a class hierarchy via the subClass relation. A graph query is a la-beled graph Q = ( V Q ; E Q ). The labels on nodes and edges are provided from a user's own vocabulary, so that she does not need to have intimate knowledge of the schema in order to formulate a query.
Search techniques for knowledge graphs vary across a spec-trum of expressivity and user-friendliness. At one end are the structured query languages such as SPARQL [3], which are very expressive but require users to have a fairly good un-derstanding of the underlying data schema. Due to the ever-growing heterogeneity of knowledge graphs, users often nd themselves facing the information overload problem [24], i.e., the data schema is too complex to grasp. Lying at the other end are unstructured search techniques like keyword search, which are easy to use but can not express structural con-straints in queries. Graph querying emerges in the middle of the spectrum: On the one hand, users can express their belief or constraints via the structure of the query graph. On the other hand, users formulate queries using their own knowledge and vocabulary, and thus can stay agnostic about the complex data schema. In this work, we will target graph pattern matching (graph query for short).
The accompanying query vagueness and ambiguity make effective answer ranking a rst-class citizen in graph query-ing: the most relevant answers should be shown to the user rst. Existing graph querying techniques have crafted var-ious kinds of ranking functions. For example, [24] de nes a ranking function considering both the syntactic similarity and the semantic coherence between queries and matches, while SLQ [37] employs a conditional random eld (CRF) model to learn and estimate the probability that a match is relevant to a query. In general, one can decompose a rank-ing function into three steps: extracting a set of features to characterize each match, appropriately weighting each fea-ture, and aggregating the weighted features to generate a nal relevance score for each match. Without loss of gener-ality, we denote a ranking function as F (  X  ( Q ) j Q; ), which evaluates the relevance of a match  X  ( Q ) to a query Q based on a few features and their corresponding weights .
We introduce the ranking function of SLQ to give a con-crete example. To measure the relevance of a match  X  ( Q ), SLQ relies on a set of transformations f f i g that matches a query node (edge) to a set of candidate entities (relations) in G . For example, in Figure 1, the abbreviation transfor-mation could match Prof. to Professor , while the topol-ogy transformation could identify the path between Geof-frey Hinton and Google as an edge match. A set of weights = f i ; i g is assigned to the transformations in a way that more selective transformations are assigned with higher weights. Given a query node v 2 V Q and a candidate entity  X  ( v ) 2 V , the node match score is de ned as a weighted sum of the transformations: The edge match score is de ned similarly: where e 2 E Q and  X  ( e ) is an edge match in G which could be either an edge or a path.

SLQ employs a CRF-based probabilistic ranking function which aggregates the node and edge match scores to estimate the conditional probability of a match  X  ( Q ) being relevant to Q under a given : P (  X  ( Q ) j Q; ) = 1 where Z is a normalization factor. F (  X  ( Q ) j Q; ) can be de-ned by log P (  X  ( Q ) j Q; ). SLQ is a speci c example of a feature-based graph matching, where each f i is a feature measure and i ; i are the weight (importance) of this fea-ture in the nal ranking function.
Despite the efforts researchers have made to craft a univer-sally good ranking function, such a generic ranking function is often sub-optimal. For example, if a user is interested in the annual yield of apples (fruit), she will most likely get overwhelmed by results about Apple, the computer company and the yield of iPhones or iPads, unless she is able to artic-ulate her information need very clearly to the query engine, which usually implies a tedious and frustrating trial-and-error procedure. A natural idea to improve search effective-ness is to somehow acquire more query-speci c information for query disambiguation, and learn a query-speci c rank-ing function. Relevance feedback is a user-friendly manner to provide additional information to guide a query engine to return better results.
 De nition 1 (Graph Relevance Feedback) Given a query Q and a knowledge graph G , a ranking function F , a set of relevant (positive) matches M + , and a set of non-relevant (negative) matches M , graph relevance feedback works to nd a query-speci c ranking function ~ F for Q based on the user feedback, such that other relevant matches will be ranked higher by ~ F than by F .

Note that we do not constrain the way for acquiring user feedback. It could be explicit feedback (user manually judg-ing matches), implicit feedback (relevance information in-ferred from user behavior such as clicks), or even pseudo feedback (blindly assuming all top ranked matches from an initial search are relevant). we provide an overview of a general framework to ap-proach the GRF problem in this section. It has three key components: Query-speci c Tuning, Type Inference, and Context Inference.
 Query-speci c Tuning The parameters in a ranking function F (  X  ( Q ) j Q; ) usually represent feature importance in a general sense. That is, how important each feature is when no additional query-speci c information is available. However, for different users and queries, feature preferences differ. Therefore, we propose Query-speci c Tuning to learn query-speci c parameters from user feedback, and there-fore tune the generic ranking function to be better aligned with the query intent. We design a feature re-weighting mechanism and employ regularization to prevent over tting to the user feedback (Section 4).

In addition to tuning the original ranking function, we also enrich the original query with additional discriminative information inferred from user feedback. When a user was formulating a query, there might be more information she had in mind but did not state explicitly. Back to Figure 1, by \Toronto", the user could mean Toronto city or University of Toronto . While she had more information in her mind, such as what type of things she was referring to (a city or a university), there is little chance for a graph query engine to infer such implicit information by solely looking at the key-word \Toronto". Adding this information back to the query may greatly improve answer quality. User feedback provides the possibility to reveal such information. Speci cally, we propose algorithms to infer two types of implicit information from user feedback: entity type and entity context. Type Inference It is observed that the relevant entities of a query node usually belong to the same, or at least similar, classes. Therefore, the type information of positive feedback could shed light on the implicit types of query nodes. For example, if a user marks Michael Phelps as relevant, it is more likely she is looking for some persons , not places or lms . If we have a ne-grained ontology, we can further in fer that the user might be looking for athletes or even Olympic athletes . To quantify the implicit type information of a query node, we gather its corresponding entities in the positive matches (positive entities), and compute a relevance score for each candidate entity by measuring how similar it is to the positive entities based on their types (Section 5). Context Inference When a user was formulating a query, she had a speci c context in mind about the interested in-formation. User feedback could also help infer such context. In document search, contexts are the words in the same sentence or paragraph with the matched keywords, while in knowledge graphs, the entities having a direct relation with the matched entities become their context. For ex-ample, Toronto city is the birthplace of many persons, the home to many companies, etc., while University of Toronto is the employer of many professors, in affiliation with many research institutes, and so on. In other words, an entity's context comprises of the entities adjacent to it in the knowl-edge graph. We employ the type distribution in an entity's context as a quantitative proxy. Similar to Type Inference, a relevance score is de ned for each candidate entity by mea-suring the context similarity between the candidate entity and the positive entities (Section 6).
Our GRF framework rst tunes the original ranking func-tion F (  X  ( Q ) j Q; ) using the query-speci c information pro-vided by the user feedback. As we have discussed, the orig-inal parameter re ects feature importance in a general sense. However, different users may prefer to formulate the same information need in different ways; and from time to time, a user's own preferences may also change. Therefore, each query brings its own view of feature importance , and user feedback provides us with the evidence to learn it.
Intuitively, we shall nd that maximizes the scores of the positive matches while minimizing the scores of the neg-ative matches: g ( ) =
However, a problem of Eq (4) is that it does not respect the original model , which contains valuable information that should not be ignored. In other words, re ects which features are more important when we have no additional query-speci c information. A better strategy is then to start from the original (query-independent) , and adjust it based on the (query-speci c) evidence provided by the feedback. To achieve that, we add a regularization term to Eq (4). g ( ) = (1 ) g ( ) + R ( ; ) ; 0 &lt; 1 : (5)
The regularization term R is a function of and . An example is  X   X  2 2 if both and are vectors. It is de-signed to penalize that deviates too far from . The bal-ance parameter controls the trade-off between the query-independent information and the query-speci c information: A large implies high sensitivity to the change of , which means that we highly respect the original parameters and do not allow to deviate far from it. On the other hand, when is small, we put more weight on the user feedback, and thus give more freedom for to be adapted according to the feedback information. One can either manually set or learn it from training instances. The formal de nition of query-speci c tuning is given as follows: De nition 2 (Query-speci c Tuning) Given a query Q and the corresponding user feedback, a ranking function F (  X  ( Q ) j Q; ), and a , the query-speci c tuning algorithm maximizes g ( ) to nd the optimal for the query, and outputs a query-speci c ranking function F (  X  ( Q ) j Q; ).
The convergence and complexity of the tuning algorithm depends on the mathematical form of the ranking function F and the regularization term R . One good practice is to care-fully select them such that it becomes a convex optimization problem, for which efficient solutions exist.
 Query-speci c Tuning for SLQ To give a concrete ex-ample, we now describe how to apply query-speci c tuning to SLQ. The ranking function of SLQ is given in Eq (3). We choose the following empirical form for ease of optimization:
F (  X  ( Q ) j Q; ) = and L 2 regularization:
To get the nal objective function, we substitute Eq (6) into Eq (4), and then substitute the result as well as Eq (7) into Eq (5). One can easily prove that the outcome is a convex function. The domain of is the Euclidean space which is apparently a convex set. Therefore, we end up with a convex optimization problem, which can be efficiently solved by plenty of optimization tools. We use the open-source library JOptimizer [2] to do the optimization.
Query-speci c search can also be achieved by enriching the query itself with additional information. Types are one kind of such information. Types are very discriminative. For the query \Toronto", just knowing that the user is referring to a university could already lter out a lot of non-relevant answers, such as Toronto city , Toronto Raptors and China-town (Toronto) . However, if users do not explicitly put type information in queries, graph query engines are agnostic to it. Fortunately, user feedback, especially positive feedback, can help infer such implicit information. In this section, we propose a type inference algorithm to infer the type of entities from positive feedback.

In comparison with traditional relevance feedback in infor-mation retrieval, a signi cant advantage of knowledge graph for this task is that the type information is sometimes avail-able. The ontology of a knowledge graph is where the type information resides. An ontology contains a set of classes (types) such as person and place , and these classes form a class hierarchy via the subClass relation. Each entity could be an instance of multiple classes, and therefore is associ-ated with one or more types. For example, Barack Obama is a politician , lawyer , writer , etc., in Freebase.
For each query node, we have a set of positive entities from the positive matches. Given a candidate entity of the qu ery node, in order to examine whether its type(s) are what the query node expects, we can instead examine whether its type(s) are \similar" to the types of the positive entities. Computing semantic similarity against an ontology is a well-studied problem, and various semantic similarity measures have been proposed, each with its own pros and cons (see [14] for a comparison). We do not elaborate on the comparison of different semantic similarity measures, but choose the one based on information content [26] because it is intuitive and efficient to compute.
 Information content measures how informative a class is. More formally, suppose there are in total N entities in the knowledge graph, among which n entities belong to a class c , the information content of c is then de ned as log p ( c ), where p ( c ) = n N . In other words, a class bears more in-formation content if it has fewer instances. The semantic similarity of two classes, c 1 and c 2 , is the information con-tent of their most informative superclass in the ontology: where S ( c 1 ; c 2 ) is the set of classes that subsume both c and c 2 . Furthermore, the semantic similarity of two entities ent 1 and ent 2 is de ned as follows: where c 1 and c 2 range over the respective types of ent 1 ent 2 . In other words, it is the information content of the most informative class the two entities are both found in. An example is shown in Figure 2. There are four classes, PLACE, CITY, COUNTRY, and CAPITAL forming a three-layer class hierarchy. The p attribute indicates the frequency of a class. The inf o attribute indicates the information content of each class. Suppose p has the value shown in Figure 2. There are four entities, Paris , Berlin , Munich and China . Consider the semantic similarity of Paris to other entities. Intuitively, Berlin is the most similar entity to Paris , since they are both CAPITALs. Munich is less similar as a CITY. Lastly, China is the least similar entity to Paris because they are only common as PLACEs. The semantic similarity measure de ned above can re ect this in-tuition. For example, the semantic similarity between Paris and Berlin sim info ( P aris; Berlin ) = 7 : 64, is greater than sim info ( P aris; M unich ) = 4 : 32.

After computing the pair-wise semantic similarity between a candidate entity and each positive entity, we de ne the type relevance score to measure the relevance of this candi-date entity to the query node. We simply take the average (the range of sim info is scaled to [0 ; 1]): De nition 3 (Type Relevance Score) Given a query node v , a candidate entity ent , and a set of positive entities f ent 1 ; :::; ent n g , the type relevance score of ent is de ned as:
People always bear a speci c context in mind when they refer to something. Take \Toronto" for example. If the user means Toronto city , she might think it is the home to many companies' headquarters, the birthplace of many people, a beautiful place that has been written in many books, etc. On the other hand, if what she means is University of Toronto , then the context in her mind would be professors, students, departments, and so on. Since users rarely put such infor-mation into queries, we have no access to it. User feedback can help infer the context of a query and thus further dis-ambiguate it.

We propose a context inference algorithm to infer the con-text of each query node from positive feedback. Quite dif-ferent from the traditional relevance feedback in document search, where context often refers to the words in the same sentence or document with a matched keyword, the struc-ture of knowledge graphs provide a more clear-cut de nition of context: the context of an entity is the entities directly adjacent to it in the knowledge graph. It is possible to ex-tend this de nition by incorporating multiple-hop neighbors. To cope with the context in a quantitative manner, we use the type distribution in a context as its proxy. The context of Toronto city is now de ned as the type distribution of entities that are linked to the node Toronto city : De nition 4 (Contextual Type Distribution) Suppose there are L classes, f c 1 ; :::; c L g , in the ontology, and n number of entities in an entity ent 's neighbors belonging to class c i , i = 1 ; :::; L , respectively, then the contextual type distribution of ent is a discrete distribution d ent ( i ) = ( wh ere n =
It is possible to explore the class hierarchy and the infor-mation content of classes to get a more comprehensive de-scription of a context, but we nd from an empirical study that the performance of this strategy is worse than the cur-rent simple strategy. A possible reason is that the contexts of the non-relevant entities become more similar to those of the relevant entities when incoporating the general su-perclasses in the class hierarchy, which makes the context information less discriminative.

The similarity of two contexts is simply de ned as the intersection of the corresponding type distributions: De nition 5 (Contextual Similarity) Given two entities ent 1 and ent 2 and their contextual type distributions d and d ent 2 , their contextual similarity is de ned as:
Sim ilar to type inference, we de ne a relevance score for each candidate entity based on its contextual similarity to the positive entities. More formally: De nition 6 (Contextual Relevance Score) Given a query node v , a candidate entity ent , and a set of positive entities f ent 1 ; :::; ent n g , the contextual relevance score of ent is de ned as:
The two types of implicit query information from user feedback: entity type and entity context can be computed efficiently online. They can be treated as additional features and plugged into the existing ranking function, e.g., Eq (1). We denote the weights of s t and s n as w t and w n , which can be either manually set or learned using a few training instances.
After putting all the possible modi cations together, we end up with a new ranking function which is better aligned with a user's search intent. The next step is to apply the new ranking function to the answers the user has not seen so far. For traditional relevance feedback in information retrieval, a second search is usually conducted from scratch or on the basis of the initial search [36]. However, it could be costly for graph querying as the potential match space is huge. Therefore, we explore an alterative option, that is, simply to re-rank the answer list from the initial search using the new ranking function. The problem is stated formally as follows: We have retrieved an initial list of top-k answers using the original ranking function F and generated a new ranking function ~ F . Now there is a binary decision problem: (Plan A) Simply re-rank the k answers in the initial list (re-ranking) or (Plan B) Search from scratch with the new ranking function (re-searching). The former strategy is very efficient, but may lose some good answers. On the other hand, Plan B incurs a non-negligible time overhead, but has the potential to discover good answers missing from the initial top-k list. Obviously, this decision depends on many factors, and itself is a trade-off between answer quality and query response time.

In this section, we formulate this decision as a binary classi cation problem: Given a query, predict which query execution plan we shall choose. We propose an automatic method to build a ground-truth training set and learn a bi-nary classi er on this training set. The key step is to build the training set, which takes two steps: (1) Feature Ex-traction, to convert a query into a feature vector, and (2) Label Assignment, to decide which class label (re-ranking or re-searching) we should assign to each query. After con-structing a training set like this, the training of a binary classi er becomes straightforward.

Given a ground-truth query set, i.e., all relevant matches for each query are known, the construction process of a train-ing set is as follows: For each query in the query set, (1) Send the query to the base graph query engine and fetch the ini-tial top-100 answers using the original ranking function F ; (2) Mark the top-10 answers as relevant or non-relevant ac-cording to the ground truth; (3) Run our GRF framework to learn a new ranking function ~ F ; (4) Re-rank the rest answers in the initial list using ~ F and let L r be the new list (feedback removed); (5) Conduct a fresh search using ~ F and let L s this top-100 list (feedback removed); (6) Extract a feature vector for the query using the feature extraction strategy in Section 7.1; (7) Assign a class label (re-ranking or re-searching) to the query using the label assignment strategy in Section 7.2.
The goal of feature extraction is to characterize each query using a set of features which can help us decide which exe-cution plan we should take for this query. Two underlying factors affect the decision making of this trade-off, query am-biguity and query complexity . If a query is ambiguous, the quality of the initial answer list is more likely to be poor, and the potential gain in effectiveness of re-searching is thus large. If a query is complex, it may take a long time to search, and the potential gain in efficiency of re-ranking is large. To capture these two factors, we identify three classes of features: query features, match features, and feedback features. Due to space limitations, we only give a high-level description of each feature class with intuitive examples. Query Features Features in this class include query graph size, selectivity of terms, and the average number of terms on each query node. The intuition is to differentiate queries that are more speci c and less ambiguous.
 Match Features The initial search process and results can also give hints for the decision. Features in this class include the number of candidate nodes and the average match score of the initial matches. For example, if the nodes of a query have a lot of candidate nodes in the knowledge graph, then re-searching may incur a high time overhead. If the average match score of a query is low, it might imply that the query is ambiguous and the initial search result is poor, so we may need to search again with the new ranking function in the hope of discovering more relevant matches.
 Feedback Features The feedback matches could also pro-vide useful information. For example, the number of pos-itive matches, the ranking of the positive matches in the initial answer list, and the average match score of the posi-tive matches and the negative matches.

Putting all the features together, we are able to convert each graph query into a 18-dimensional feature vector. The next step is to assign a class label to each training query.
The task of label assignment is to assign a class label to each query in the training set. The question is, if we know the search result and the time cost of both the query execu-tion plans, which plan, re-ranking or re-searching, is more bene cial? Intuitively, if the quality gain of re-searching is large and it takes a reasonable time, re-searching is more favored. Here we propose a quantitative measure to take both quality and runtime into consideration, where a thresh-old can be employed to control the trade-off.

Suppose we have an effectiveness measure h to evaluate how good a result list is, e.g., Average Precision (AP), then the gain of doing re-searching for the query is de ned as: wh ere t s and t r are the time cost of re-searching and re-ranking, respectively. We then assign the class label of Q , l ( Q ), using the following strategy: where 2 R is a pre-de ned threshold controlling the trade-off.

After constructing a training set as above, we are ready to train a binary classi er to make decision for future queries. In Section 8 we use random forest as the classi er and show that our strategy can achieve a good trade-off between an-swer quality and query time. In practice, one can try differ-ent classi ers to select the best one.
In this section, we empirically evaluate our methods. SLQ is used as the base graph query engine. We rst describe the experiment design (Section 8.1) and then answer the follow-ing questions: (1) Can our GRF framework improve the search effectiveness of SLQ? If so, how much does each com-ponent contribute? (Section 8.2) (2) What is the impact of the hyper-parameters in GRF? (Section 8.3) (3) Can our learning-based algorithm lead to good trade-off between an-swer quality and query run time? (Section 8.4) Knowledge Graph DBpedia [18] is a popular knowledge graph extracted from Wikipedia. After indexing DBpedia 3.9 1 using SLQ, we end up with a knowledge graph contain-ing 4 : 6 M nodes and 100 M edges. The ontology of DBpedia contains 529 classes which form a 8-layer class hierarchy. Graph Queries Plenty of benchmarks (e.g., TREC [4]) are available for researchers to evaluate their relevance feedback methods in document retrieval. Unfortunately, there is no widely-accepted benchmark for graph querying. Previous studies have resorted to ad-hoc evaluation. Here, we propose two methods to generate graph queries with ground-truth.
Our rst query generating method capitalizes the list pages in Wikipedia. A list page contains structured information about a topic. For example, the page \List of States and ht tp://wiki.dbpedia.org/Downloads39?v=2zd Territories of the United States" contains a table about US states and their capital city, popularity, etc. We can there-fore construct graph queries from such structured informa-tion. Figure 3(a) shows an example. Its answers can be ex-tracted from the list page. We manually crafted 50 queries and denote this query set as WIKI. Most of them have two nodes and one edge.

Due to the miscellaneous ways of structured information representation in Wikipedia, it is hard to automatically con-struct graph queries. Therefore, we propose a second query generating approach which is more automated and thus more scalable. This approach leverages the YAGO ontology [31], a rather ne-grained ontology with 350 K classes forming a 20-layer class hierarchy. It contains a lot of highly spe-ci c classes which can be converted to graph queries. For example, there is a YAGO class about \Naval Battles of World War II Involving the United States", from which we can formulate the graph query in Figure 3(c). Another ad-vantage is that DBpedia entities are annotated with YAGO classes, which makes it feasible to build the ground truth Figure 3(d) shows a sample answer. We select 100 YAGO classes and convert them into ground-truth queries. This query set is denoted as YAGO. YAGO queries are more di-verse in terms of structure: The number of query nodes range from 1 to 4, while the number of query edges range from 0 to 3. All the queries have multiple answers so that users have a chance to provide some positive feedback. Experiment Pipeline Besides the knowledge graph and the graph query sets, we also need a way to obtain user feed-back. Following the convention in IR [19], we simulate ex-plicit feedback using ground truth, that is, we use the ground truth of a query to determine the relevance of several top-ranked matches from an initial search and use them as user feedback. For a given graph query, our experiment runs as follows: (1) Use SLQ to retrieve the initial top-100 matches. (2) Use the ground truth of the query to identify the rel-evant and non-relevant matches in the top-N fb . (3) Take the result as user feedback to run GRF and obtain a new ranking function. (4) Finally, run SLQ with the new ranking function to retrieve a new list of top-100 matches. The feed-back size N fb is a hyper-parameter (typically set to 10, if not otherwise stated). Although we use explicit feedback as the default setting, we also evaluate GRF using pseudo feedback , i.e., the top N fb initial matches are blindly treated as rele-vant, to test GRF's performance when feedback information is noisy or even erroneous. We choose Mean Average Pre-cision at different cutoffs (MAP@ K , K = 1 ; 5 ; 10 ; 20, etc.) as the main evaluation metric. For explicit feedback, feed-back matches are removed before calculating MAP for the sake of fair comparison. We use paired Student's t test with p = 0 : 05 for signi cance test.
The question we want to answer in this experiment is, can our GRF framework improve the search quality of SLQ? If so, how much contribution each component makes? We rst experiment with explicit feedback on both query sets: Do model selection (choose values for hyper-parameters) on one query set and then test on the other. The experiment results are shown in Figure 4. The horizontal axis ( K ) indicates different cut-offs and the vertical axis shows MAP@ K . We
Y AGO classes are excluded from the following experiments. Figure 4: Performance of different GRF variants on: (a) WIKI, (b) YAGO.
 Table 1: GRF vs. SLQ with varying query size.
 ev aluate the contribution of the three components: query-speci c tuning, type inference and context inference. GRF is able to bring a signi cant improvement in mean average precision. In comparison with the baseline SLQ, all GRF variants get signi cantly better performance. Moreover, the results demonstrate that all of the three components are use-ful and complement each other: query-speci c tuning alone already results in a signi cant improvement, and adding the other two components further improves. The full GRF pipeline achieves the best performance on both query sets, improving over the baseline by 102% on WIKI and 86% on YAGO (MAP@20).

We also compare the performance of SLQ and GRF with varying query graph size (number of query nodes). We test on YAGO queries since they are more diverse in structure. Experiment results are shown in Table 1. MAP@20 is re-ported and statistically signi cant results are bold-faced. In comparison with SLQ, the performance of GRF is consis-tent across query sizes (1-3). An exception is when query size is 4. The quality of the initial search by SLQ is very poor and there are few relevant matches ranked top-10 in the initial answer lists. Due to the lack of positive feedback, the improvement of GRF is tiny. It is worth a further study for the situation where only negative feedback is available. This also includes the situation where a query only has one answer and the answer is not in the initial top list. Selec-tive application of GRF based on query difficulty can also be investigated [5].

As a complementary experiment, we also evaluate GRF with pseudo feedback. The results are shown in Table 2. GRF still signi cantly outperforms SLQ when the feedback information is noisy or even erroneous.
We evaluate four hyper-parameters: balance parameter , feedback size N fb , and weights of the two relevance scores from type and context w t and w c . We only report the ex-periment results on WIKI. The observations on YAGO are similar and thus omitted for brevity.

Figure 5(a) shows the results of different s in query-speci c tuning. = 1 means we do not do re-weighting, which is the baseline SLQ. As we discussed, controls the balance between user feedback and the original ranking func-SL Q WI KI 0 .23 0 .21 0.2 4 0. 25 0. 27 0 .28 GRF WI KI 0 .73 0.5 8 0. 52 0. 50 0. 49 0 .49 SLQ Y AGO 0 .40 0 .35 0.3 3 0. 32 0. 36 0 .39
GRF Y AGO 0 .82 0.6 6 0. 60 0. 57 0. 58 0 .61 tio n. Experiment results show that a moderate value of is more appropriate. When is too small (e.g., 0 : 1), we over-t to the user feedback and cannot generalize well to unseen results. Therefore, regularization is helpful in this case to prevent over tting. In the following experiments, will be set to 0 : 3.
 The feedback size N fb also affects system performance. Intuitively, we should get better performance if we have more feedback information. In this experiment, feedback matches are not excluded before evaluation, because otherwise the experiment results of different N fb will become not directly comparable. Figure 5(b) shows that, as N fb increases, MAP increases in the mean time, which is as expected. But the gain of large N fb (e.g., 20) is relatively small. Since a larger feedback size incurs a heavier burden for acquiring user feed-back, N fb = 10 seems a good setting.

Finally, we evaluate the impact of w t and w c . Basically, w t and w c specify how much weight we put on each rele-vance score. Figure 5(c) and Figure 5(d) show that a mod-erate value is more appropriate. When the weight of either component is too large (e.g., 10), we are over tting to that component and system performance is therefore affected.
In this experiment, we examine the trade-off between an-swer quality and runtime (Section 7). Average Precision@20 is used as the effectiveness metric h . We experiment on YAGO and train a random forest using training sets gener-ated with different . We employ the leave-one-out strategy for evaluation: For each query, we train a random forest on all the other queries and test on that query. The reported results are averaged over all the queries. Experiment results are shown in Figure 6. In this dataset, the re-ranking strat-egy achieves good performance: The answer quality does not decrease very much while the runtime is almost negligible. As shown by the results, the threshold can help make a good trade-off between answer quality and runtime. Knowledge Graph Search . Various techniques have been proposed to search knowledge graphs. One popular search paradigm is structured search [3, 16], where queries are for-mulated using exact schema items (entity names, classes, re-lations) of the knowledge graph. But the usability of struc-tured search is reduced due to the high and ever-growing heterogeneity of knowledge graphs. For this reason, keyword search [12, 23, 39] has been explored in order to improve ac-cessibility. [35] gives a recent survey of the study in this line. However, users also lose the chance to specify query constraints to better express their information need.
Graph querying techniques [37, 17, 24] emerge as an alter-native search technique. They allow users to (1) formulate queries using their own vocabulary, and (2) express query N
Fi gure 6: Answer quality vs. Runtime on YAGO constraints via graph structure. Various ranking functions have been proposed. For example, [24] de nes a ranking function which combines both semantic coherence and syn-tactic similarity, while NeMa [17] takes both syntactic simi-larity and the structural proximity of the matched nodes into account. However, all these ranking functions are generic. We propose to convert these generic ranking functions into query-speci c ranking functions using user feedback.
The idea of searching for more entities based on a set of positive entities bears some similarity to entity search [21], where the goal is to nd more entities given a set of exem-plar entities. Another work similar in spirit to ours is [30], which employs pseudo-relevance feedback to improve key-word search over RDF graphs. To the best of our knowl-edge, our work is the rst attempt to investigate relevance feedback in graph querying, which is quite different from keyword search.
 Relevance Feedback in IR . Relevance feedback has been studied extensively in information retrieval. A relevance feedback method is usually designed for a speci c retrieval model. For example, [38] incorporates relevance information into the language model, [28] works for the vector space re-trieval model, and [27] employs relevance information to re-estimate the parameters in a probabilistic retrieval model. Therefore, they are not directly applicable to the graph query paradigm.

The idea of incorporating user feedback with the origi-nal model is not new. Traditional relevance feedback meth-ods also combine the feedback information with the origi-nal query/model to produce the nal ranking function (e.g., [28, 27, 38]). We employ regularization techniques to pre-vent over tting, which is conceptually similar to [32], where a regularized EM algorithm is proposed for the language model. But our ways of regularization are quite different because of the different retrieval models.

New relevance feedback methodologies recently under ex-ploration in information retrieval can potentially be applied to graph relevance feedback as well. [25] and [34] exploit the relations between retrieved results and de ne a query-speci c ranking function on all the results, instead of on each individual result. [19] works to predict the optimal balance parameter between the original model and the feedback in-formation for each query, instead of a xed balance param-eter for all the queries. Finally, [20] explores the idea of a non-uniform context by assigning higher weights to words closer to a matched keyword. The idea can also be applied to our GRF framework. A possible way is to treat the en-tities in the context of an entity differently based on their similarity or relation strength to the entity.

The efficiency issue of relevance feedback has also attracted some attention in IR [36, 10]. Their focus is to reduce the runtime of the second search. We approach this problem from a different perspective. Instead of always conducting a second search, we also consider the option of simply re-ranking the initial list. Our experiment results show that it can be a reasonable strategy: Depending on queries, the answer quality might not decrease too much.
In this work, we identi ed the limits of the generic rank-ing mechanism employed by existing graph querying tech-niques, and proposed to combine relevance feedback with graph querying in order to achieve query-speci c ranking, i.e., GRF. We developed a novel GRF framework which works to tune the original ranking function as well as in-ferring additional information to enrich the query itself. We further identi ed an accompanying efficiency issue of GRF and proposed a classi cation mechanism to control the trade-off between answer quality and runtime. As veri ed by the experiments, our GRF framework can signi cantly improve the precision of a state-of-the-art graph querying technique, and make a good trade-off between answer quality and run-time in the mean time.

As the rst attempt to study relevance feedback in graph querying, our work opens up an array of interesting future directions, e.g., discriminative feature mining from positive and negative feedback, and personalizing graph querying.
This research was sponsored in part by the Army Re-search Laboratory under cooperative agreements W911NF-09-2-0053 and NSF IIS 0954125. The views and conclusions contained herein are those of the authors and should not be interpreted as representing the official policies, either ex-pressed or implied, of the Army Research Laboratory or the U. S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notice herein. [1] Google Knowledge Graph. http://www.google.com/ [2] JOptimizer. http://www.joptimizer.com . [3] SPARQL 1.1. http://www.w3.org/TR/ [4] TREC: relevance feedback track. http://trec.nist. [5] G. Amati, C. Carpineto, and G. Romano. Query [6] J. Berant, A. Chou, R. Frostig, and P. Liang. [7] J. Berant and P. Liang. Semantic parsing via [8] K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and [9] C. Carpineto and G. Romano. A survey of automatic [10] MA Cartright, J. Allan, V. Lavrenko, and [11] W. Eberle and L. Holder. Applying graph-based [12] S. Elbassuoni and R. Blanco. Keyword search over [13] G. W. Furnas, T. K. Landauer, L. M. Gomez, and [14] M. Gan, X. Dou, and R. Jiang. From ontology to [15] N. Jayaram, M. Gupta, A. Khan, C. Li, X. Yan, and [16] G. Kasneci, F. M. Suchanek, G. Ifrim, M. Ramanath, [17] A. Khan, Y. Wu, C. C. Aggarwal, and X. Yan. NeMa: [18] J. Lehmann, R. Isele, M. Jakob, A. Jentzsch, [19] Y. Lv and C. Zhai. Adaptive relevance feedback in [20] Y. Lv and C. Zhai. Positional relevance model for [21] S. Metzger, R. Schenkel, and M. Sydow. Aspect-based [22] D. Mottin, M. Lissandrini, Y. Velegrakis, and [23] J. Pound, A. K. Hudek, I. F. Ilyas, and G. Weddell. [24] J. Pound, I. F. Ilyas, and G. Weddell. Expressive and [25] T. Qin, T. Liu, X. Zhang, D. Wang, and H. Li. Global [26] P. Resnik. Semantic similarity in a taxonomy: An [27] S. E. Robertson and K. S. Jones. Relevance weighting [28] J. J. Rocchio. Relevance feedback in information [29] I. Ruthven and M. Lalmas. A survey on the use of [30] S. Shekarpour, K. Hoffner, J. Lehmann, and S. Auer. [31] F. M. Suchanek, G. Kasneci, and G. Weikum. Yago: A [32] T. Tao and C. Zhai. Regularized estimation of mixture [33] L. Terveen and D. W. McDonald. Social matching: A [34] C. Wang, E. Yilmaz, and M. Szummer. Relevance [35] H. Wang and C. C. Aggarwal. A survey of algorithms [36] H. Wu and H. Fang. An incremental approach to [37] S. Yang, Y. Wu, H. Sun, and X. Yan. Schemaless and [38] C. Zhai and J. Lafferty. Model-based feedback in the [39] Q. Zhou, C. Wang, M. Xiong, H. Wang, and Y. Yu. [40] X. S. Zhou and T. S. Huang. Relevance feedback in [41] B. Zong, R. Raghavendra, M. Srivatsa, X. Yan, A. K. [42] L. Zou, R. Huang, H. Wang, J. X. Yu, W. He, and
