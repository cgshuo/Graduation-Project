 Data mining algorithms look for patterns in data. While most existing data mining approac hes look for patterns in a single data table, multi-relational data mining (MRDM) approac hes look for patterns that involve multiple tables (relations) from a relational database. In recen t years, the most common types of patterns and approac hes considered in data mining have been extended to the multi-relational case and MRDM now encompasses multi-relational (MR) as-sociation rule disco very, MR decision trees and MR distance-based metho ds, among others. MRDM approac hes have been successfully applied to a num ber of problems in a va-riet y of areas, most notably in the area of bioinformatics. This article pro vides a brief introduction to MRDM, while the remainder of this special issue treats in detail adv anced researc h topics at the fron tiers of MRDM.
 relational data mining, multi-relational data mining, inductiv e logic programming, relational asso ciation rules, relational decision trees, relational distance-based metho ds existing data mining approac hes are prop ositional and look for patterns in a single data table. Relational data mining (RDM) approac hes [16], man y of whic h are based on induc-tive logic programming (ILP , [35]), look for patterns that in-volve multiple tables (relations) from a relational database. To emphasize this fact, RDM is often referred to as multi-relational data mining (MRDM, [21]). In this article, we will use the terms RDM and MRDM interc hangeably . In this introductory section, we tak e a look at data, patterns, and algorithms in RDM, and men tion some application ar-eas. (relations) and not just one table. The example database in Table 1 has two relations: Customer and MarriedTo . Note that relations can be de ned extensionally (by tables, as in our example) or intensionally through database views (as explicit logical rules). The latter typically represen t re-lationships that can be inferred from other relationships. For example, having extensional represen tations of the re-lations mother and father, we can intensionally de ne the relations grandparen t, grandmother, sibling, and ancestor, among others.
 eral kno wledge about the domain of discourse. For example, if we have extensional relations listing the atoms that mak e a comp ound molecule and the bonds between them, functional groups of atoms can be de ned intensionally . Suc h general kno wledge is called domain kno wledge or bac kground kno wl-edge.
 Table 1: A relational database with two tables and two classi cation rules: a prop ositional and a relational. Customer table
ID Gender Age Income TotalSp ent BigS c1 Male 30 214000 18800 Yes c2 Female 19 139000 15100 Yes c3 Male 55 50000 12400 No c4 Female 48 26000 8600 No c5 Male 63 191000 28100 Yes c6 Male 63 114000 20400 Yes c7 Male 58 38000 11800 No c8 Male 22 39000 5700 No ... ... ... ... ... ...
 MarriedT o table
Spouse1 Spouse2 c1 c2 c2 c1 c3 c4 c4 c3 c5 c12 c6 c14 ... ...
 Prop ositional rule IF Income &gt; 108000 THEN BigSp ender = Yes Relational rule big spender (C1,Age1,Income1,T otalSp ent1) married to (C1,C2) ^ customer (C2,Age2,Income2,T otalSp ent2,BS2) ^
Income2 108000. lational database. They are typically stated in a more ex-pressiv e language than patterns de ned on a single data table. The ma jor types of relational patterns extend the types of prop ositional patterns considered in single table data mining. We can thus have relational classi cation rules, relational regression trees, and relational asso ciation rules, among others.
 ble 1, whic h involves the relations Customer and MarriedTo . It predicts a person to be a big spender if the person is mar-ried to someb ody with high income (compare this to the rule that states a person is a big spender if he has high income, listed above the relational rule). Note that the two persons C1 and C2 are connected through the relation MarriedTo . rst-order logic (also called predicate or relational logic). Essen tials of predicate logic include predicates ( MarriedTo ) and variables ( C1, C2 ), whic h are not presen t in prop osi-tional logic. Relational patterns are thus more expressiv e than prop ositional ones.
 order logic, whic h is strongly related to deductiv e databases, is used as the formalism for expressing relational patterns. E.g., the relational rule in Table 1 is a logic program clause. Note that a relation in a relational database corresp onds to a predicate in rst-order logic (and logic programming). data to nd relational patterns that involve multiple rela-tions. Most other data mining approac hes assume that the data resides in a single table and require prepro cessing to integrate data from multiple tables (e.g., through joins or aggregation) into a single table before they can be applied. Integrating data from multiple tables through joins or aggre-gation, however, can cause loss of meaning or information. N ame; Age; SpendsALot ) and pur chase ( CustI D; P roductI D; Date; V alue; P aymentM ode ), where eac h cus-tomer can mak e multiple purc hases, and we are interested in characterizing customers that spend a lot. Integrating the two relations via a natural join will give rise to a relation pur chase 1 where eac h row corresp onds to a purc hase and not to a customer. One possible aggregation would give rise to the relation customer 1( CustI D; Age; N of P urchases; T otal V alue; SpendsALot ). In this case, however, some in-formation has been clearly lost during aggregation. customer and pur chase are considered together. customer ( CID; N ame; Age; yes ) This pattern says: \a customer spends a lot if she is older than 30, has purc hased a pro duct of value more than 100 and paid for it by credit card." It would not be possible to induce suc h a pattern from either of the relations pur chase 1 and customer 1 considered on their own.
 ple tables directly , RDM systems are usually able to tak e into accoun t generally valid bac kground (domain) kno wl-edge given as a logic program. The abilit y to tak e into ac-coun t bac kground kno wledge and the expressiv e power of the language of disco vered patterns are distinctiv e for RDM. given single table are referred to as attribute-v alue or prop o-sitional learning approac hes, as the patterns they nd can be expressed in prop ositional logic. RDM approac hes are also referred to as rst-order learning approac hes, or relational learning approac hes, as the patterns they nd are expressed in the relational formalism of rst-order logic. A more de-tailed discussion of the single table assumption, the prob-lems resulting from it and how a relational represen tation alleviates these problems is given by Wrob el [50] (Chapter 4 of [16]). terns to nd patterns valid in a given database. The searc h algorithms used here are very similar to those used in single table data mining: one can searc h exhaustiv ely or heuristi-cally (greedy searc h, best-rst searc h, etc.). Just as for the single table case, the space of patterns considered is typically lattice-structured and exploiting this structure is essen tial for achieving eciency . The lattice structure is traversed by using re nemen t operators [46], whic h are more complicated in the relational case. In the prop ositional case, a re nemen t operator may add a condition to a rule anteceden t or an item to an item set. In the relational case, a new relation can be introduced as well.
 of mac hine learning, man y RDM algorithms come form the eld of inductiv e logic programming (ILP , [35; 30]). Situ-ated at the intersection of mac hine learning and logic pro-gramming, ILP has been concerned with nding patterns expressed as logic programs. Initially , ILP focussed on au-tomated program syn thesis from examples, form ulated as a binary classi cation task. In recen t years, however, the scop e of ILP has broadened to cover the whole spectrum of data mining tasks (classi cation, regression, clustering, asso-ciation analysis). The most common types of patterns have been extended to their relational versions (relational classi-cation rules, relational regression trees, relational asso cia-tion rules) and so have the ma jor data mining algorithms (decision tree induction, distance-based clustering and pre-diction, etc.).
 a generic approac h of upgrading single table data mining algorithms (prop ositional learners) to relational ones ( rst-order learners). Note that it is not trivial to extend a single table data mining algorithm to a relational one. Extending the key notions to, e.g., de ning distance measures for multi-relational data requires considerable insigh t and creativit y. Eciency concerns are also very imp ortan t, as it is often the case that even testing a given relational pattern for validit y is computationally exp ensiv e, let alone searc hing a space of suc h patterns for valid ones. An alternativ e approac h to RDM (called prop ositionalization) is to create a single table from a multi-relational database in a systematic fashion [28] (Chapter 11 of [16]): this approac h shares some eciency concerns and in addition can have limited expressiv eness. ber of possible patterns even in the single table case: this num ber is in practice limited by setting some parameters (e.g., the largest size of frequen t itemsets for asso ciation rule disco very). For relational pattern languages, the num-ber of possible patterns is even larger and it becomes nec-essary to limit the space of possible patterns by pro viding more explicit constrain ts. These typically specify what re-lations should be involved in the patterns, how the relations can be interconnected, and what other syn tactic constrain ts the patterns have to obey. The explicit speci cation of the pattern language (or constrain ts imp osed upon it) is kno wn under the name of declarativ e bias [38]. with structured data and domain kno wledge, whic h would be dicult to address with single table approac hes. RDM has been used in di eren t areas, ranging from analysis of business data, through environmen tal and trac engineering to web mining, but has been esp ecially successful in bioin-formatics (including drug design and functional genomics). Bioinformatics applications of RDM are discussed in the ar-ticle by Page and Cra ven in this issue. For a comprehensiv e surv ey of RDM applications we refer the reader to Dzeroski [20] (Chapter 14 of [16]). duction to inductiv e logic programming, whic h (from the viewp oint of MRDM) is mainly concerned with the induc-tion of relational classi cation rules for two-class problems. It then pro ceeds to introduce the basic MRDM techniques of disco very of relational asso ciation rules, induction of rela-tional decision trees and relational distance-based metho ds (that include both classi cation and clustering). The arti-cle concludes with an overview of the MRDM literature and Internet resources. logic programming (ILP) is concerned with the dev elopmen t of techniques and tools for relational data mining. Pat-terns disco vered by ILP systems are typically expressed as logic programs, an imp ortan t subset of rst-order (predi-cate) logic, also called relational logic. In this section, we rst brie y discuss the language of logic programs, then pro-ceed with a discussion of the ma jor task of ILP and some approac hes to solving it. clauses as rst-order rules, where the conclusion part is termed the head and the condition part the body of the clause. The head and body of a clause consist of atoms, an atom being a predicate applied to some argumen ts, whic h are called terms. In Datalog, terms are variables and con-stan ts, while in general they may consist of function sym bols applied to other terms. Ground clauses have no variables. par ent ( X; Y ). It reads: \if X is a paren t of Y then X is the father of Y or X is the mother of Y" ( _ stands for logical or). par ent ( X; Y ) is the body of the clause and f ather ( X; Y ) _
DB terminology LP terminology relation name p predicate sym bol p attribute of relation p argumen t of predicate p tuple h a 1 ; : : : ; a n i ground fact p ( a 1 ; : : : ; a relation p -predicate p -a set of tuples de ned extensionally relation q predicate q de ned as a view de ned intensionally mother ( X; Y ) is the head. par ent , f ather and mother are predicates, X and Y are variables, and par ent ( X; Y ), f ather ( X; Y ), mother ( X; Y ) are atoms. We adopt the Pro-log [4] syn tax and start variable names with capital let-ters. Variables in clauses are implicitly univ ersally quan-ti ed. The above clause thus stands for the logical form ula 8 X 8 Y : f ather ( X; Y ) _ mother ( X; Y ) _ : par ent ( X; Y ). Clauses are also view ed as sets of literals, where a literal is an atom or its negation. The above clause is then the set f f ather ( X; Y ) ; mother ( X; Y ) ; : par ent ( X; Y ) g . actly one atom in the head. As compared to de nite clauses, program clauses can also con tain negated atoms in the body. While the clause in the paragraph above is a full clause, the clause ancestor ( X; Y ) par ent ( Z; Y ) ^ ancestor ( X; Z ) is a de nite clause ( ^ stands for logical and). It is also a recur-sive clause, since it de nes the relation ancestor in terms of itself and the relation par ent . The clause mother ( X; Y ) par ent ( X; Y ) ^ not mal e ( X ) is a program clause. grams are sets of program clauses. A set of program clauses with the same predicate in the head is called a predicate de nition. Most ILP approac hes learn predicate de nitions. tion in a relational database. A n -ary relation p is formally de ned as a set of tuples [48], i.e., a subset of the Cartesian pro duct of n domains D 1 D 2 : : : D n , where a domain (or a type) is a set of values. It is assumed that a relation is nite unless stated otherwise. A relational database (RDB) is a set of relations.
 gumen ts of a predicate corresp ond to the attributes of a relation. The ma jor di erence is that the attributes of a relation are typed (i.e., a domain is asso ciated with eac h attribute). For example, in the relation lives in ( X; Y ), we may want to specify that X is of type per son and Y is of type city . Database clauses are typed program clauses. In deductiv e databases, relations can be de ned extension-ally as sets of tuples (as in RDBs) or intensionally as sets of database clauses. Database clauses use variables and func-tion sym bols in predicate argumen ts and the language of DDBs is substan tially more expressiv e than the language of RDBs [31; 48]. A deductiv e Datalog database consists of de nite database clauses with no function sym bols. terms. For a full treatmen t of logic programming, RDBs, and deductiv e databases, we refer the reader to [31] and [48].
 mostly concerned with deductiv e inference. Inductiv e logic programming, on the other hand, is concerned with induc-tive inference. It generalizes from individual instances/obser-vations in the presence of bac kground kno wledge, nding regularities/h ypotheses about yet unseen instances. of learning logical de nitions of relations [42], where tuples that belong or do not belong to the target relation are given as examples. From training examples ILP then induces a logic program (predicate de nition) corresp onding to a view that de nes the target relation in terms of other relations that are given as bac kground kno wledge. This classical ILP task is addressed, for instance, by the seminal MIS system [46] (righ tfully considered as one of the most in uen tial an-cestors of ILP) and one of the best kno wn ILP systems FOIL [42].
 target relation p (positiv e examples) and tuples that do not belong to p (negativ e examples). Giv en are also bac kground relations (or bac kground predicates) q i that constitute the bac kground kno wledge and can be used in the learned de -nition of p . Finally , a hypothesis language, specifying syn-tactic restrictions on the de nition of p is also given (either explicitly or implicitly). The task is to nd a de nition of the target relation p that is consisten t and complete, i.e., explains all the positiv e and none of the negativ e tuples. con tains positiv e and N negativ e examples, and bac kground kno wledge B . The task is to nd a hypothesis H suc h that 8 e 2 P : B ^ H j = e ( H is complete) and 8 e 2 N : B ^ H 6j = e ( H is consisten t), where j = stands for logical implication or entailmen t. This setting, introduced by Muggleton [34], is thus also called learning from entailmen t. In an alternativ e setting prop osed by De Raedt and Dzeroski [15], the require-men t that B ^ H j = e is replaced by the requiremen t that H be true in the minimal Herbrand mo del of B ^ e : this setting is called learning from interpretations.
 and H can be a clausal theory . In practice, eac h e is most often a ground example (tuple), B is a relational database (whic h may or may not con tain views) and H is a de nite logic program. The seman tic entailmen t ( j =) is in practice replaced with syn tactic entailmen t ( ` ) or pro vabilit y, where the resolution inference rule (as implemen ted in Prolog) is most often used to pro ve examples from a hypothesis and the bac kground kno wledge. In learning from entailmen t, a positiv e fact is explained if it can be found among the answ er substitutions for h pro duced by a query ? b on database B , where h b is a clause in H . In learning from interpretations, a clause h b from H is true in the minimal Herbrand mo del of B if the query b ^: h fails on B . daug hter ( X; Y ), whic h states that person X is a daugh ter of person Y , in terms of the bac kground kno wledge relations f emal e and par ent . These relations are given in Table 3. There are two positiv e and two negativ e examples of the target relation daug hter . In the hypothesis language of def-inite program clauses it is possible to form ulate the follo wing de nition of the target relation, whic h is consisten t and complete with resp ect to the bac k-ground kno wledge and the training examples.
 hypothesis language and the complexit y of the target con-cept, the target predicate de nition may consist of a set of clauses, suc h as if the relations mother and f ather were given in the bac k-ground kno wledge instead of the par ent relation. guage of program clauses. As the complexit y of learning gro ws with the expressiv eness of the hypothesis language, restrictions have to be imp osed on hypothesized clauses. Typical restrictions are the exclusion of recursion and re-strictions on variables that app ear in the body of the clause but not in its head (so-called new variables).
 is a binary classi cation task, where one of two classes is assigned to the examples (tuples): (positiv e) or (nega-tive). Classi cation is one of the most commonly addressed tasks within the data mining comm unit y and includes ap-proac hes for rule induction. Rules can be generated from decision trees [43] or induced directly [33; 7].
 adopt the covering approac h of rule induction systems. In a main loop, a covering algorithm constructs a set of clauses. Starting from an empt y set of clauses, it constructs a clause explaining some of the positiv e examples, adds this clause to the hypothesis, and remo ves the positiv e examples ex-plained. These steps are rep eated until all positiv e examples have been explained (the hypothesis is complete).
 clauses are constructed by (heuristically) searc hing the space of possible clauses, structured by a specialization or general-ization operator. Typically , searc h starts with a very general rule (clause with no conditions in the body), then pro ceeds to add literals (conditions) to this clause until it only covers (explains) positiv e examples (the clause is consisten t). most often the case, the criteria of consistency and com-pleteness are relaxed. Statistical criteria are typically used instead. These are based on the num ber of positiv e and negativ e examples explained by the de nition and the indi-vidual constituen t clauses. ing the covering algorithm for clause/rule set induction, let us now look at some of the mec hanisms underlying single clause/rule induction. In order to searc h the space of rela-tional rules (program clauses) systematically , it is useful to imp ose some structure upon it, e.g., an ordering. One suc h ordering is based on -subsumption, de ned below.
 of terms t i to variables V i . Applying a substitution to a term, atom, or clause F yields the instan tiated term, atom, or clause F where all occurrences of the variables V i are sim ultaneously replaced by the term t i . Let c and c 0 be two program clauses. Clause c -subsumes c 0 if there exists a substitution , suc h that c c 0 [41].
 daug hter ( X; Y ) par ent ( Y; X ) : Applying the substitution = f X=mar y; Y =ann g to clause c yields notation daug hter ( X; Y ) par ent ( Y; X ) thus stands for f daug hter ( X; Y ) ; par ent ( Y; X ) g where all variables are as-sumed to be univ ersally quan ti ed, overline denotes logical negation, and the commas denote disjunction. According to the de nition, clause c -subsumes c 0 if there is a substitu-tion that can be applied to c suc h that every literal in the resulting clause occurs in c 0 . Clause c -subsumes the clause under the empt y substitution = ; , since f daug hter ( X; Y ) ; par ent ( Y; X ) g is a prop er subset of f daug hter ( X; Y ) ; f emal e ( X ) ; par ent ( Y; X ) g : Furthermore, under the substi-tution = f X=mar y; Y =ann g , clause c -subsumes the clause c 0 = daug hter ( mar y; ann ) -subsumption introduces a syn tactic notion of generalit y. Clause c is at least as general as clause c 0 ( c c 0 ) if c -subsumes c 0 . Clause c is more general than c 0 ( c &lt; c 0 ) if c c 0 holds and c 0 c does not. In this case, we say that c 0 is a specialization of c and c is a generalization of c 0 . If the clause c 0 is a specialization of c then c 0 is also called a re nemen t of c . The only clause re nemen ts usually considered by ILP systems are the minimal (most general) specializations of the clause.
 If c -subsumes c 0 then c logically entails c 0 , c j = c 0 .
The rev erse is not alw ays true. As an example, [23] gives the follo wing two clauses c = list ( cons ( V; W )) list ( W ) and c 0 = list ( cons ( X; cons ( Y; Z ))) list ( Z ). Giv en the empt y list, c constructs lists of any given length, while c 0 constructs lists of even length only , and thus c j = c 0 .
However, no substitution exists suc h that c = c 0 , since imp ossible. Therefore, c does not -subsume c 0 .

The relation introduces a lattice on the set of reduced clauses [41]. This means that any two reduced clauses have a least upp er bound ( lub ) and a greatest lower bound ( glb ). Both the lub and the glb are unique up to equiv a-lence (renaming of variables) under -subsumption. Re-duced clauses are the minimal represen tativ es of the equiv-alence classes of clauses de ned by -subsumption. For example, the clauses daug hter ( X; Y ) par ent ( Y; X ) ; par ent ( W; V ) and daug hter ( X; Y ) par ent ( Y; X ) -sub-sume one another and are thus equiv alen t. The latter is reduced, while the former is not.
 lowing de nition: The least general generalization ( lgg ) of two clauses c and c 0 , denoted by lgg ( c; c 0 ), is the least upp er bound of c and c 0 in the -subsumption lattice [41]. The rules for computing the lgg of two clauses are outlined later in this chapter.
 tion are purely syn tactic notions since they do not tak e into accoun t any bac kground kno wledge. Their computation is therefore simple and easy to be implemen ted in an ILP sys-tem. The same holds for the notion of generalit y based on -subsumption. On the other hand, taking bac kground kno wledge into accoun t would lead to the notion of seman tic generalit y [39; 6], de ned as follo ws: Clause c is at least as general as clause c 0 with resp ect to bac kground theory B if B [f c gj = c 0 .
 putationally more feasible. Namely , seman tic generalit y is in general undecidable and does not introduce a lattice on a set of clauses. Because of these problems, syn tactic generalit y is more frequen tly used in ILP systems. ming for the follo wing reasons:
As sho wn above, it pro vides a generalit y ordering for hy-potheses, thus structuring the hypothesis space. It can be used to prune large parts of the searc h space. tan t ILP techniques: gram clauses in a top-do wn manner, from general to speci c hypotheses, using a -subsumption-based specialization op-erator. A specialization operator is usually called a re ne-men t operator [46]. Giv en a hypothesis language L , a re-nemen t operator maps a clause c to a set of clauses ( c ) whic h are specializations (re nemen ts) of c : ( c ) = f c 0 j c 0 2 L ; c &lt; c 0 g : of minimal (most general) specializations of a clause under -subsumption. It emplo ys two basic syn tactic operations: Figure 1: Part of the re nemen t graph for the family rela-tions problem. apply a substitution to the clause, and add a literal to the body of the clause.
 structured by the -subsumption generalit y ordering. In this lattice, a re nemen t graph can be de ned as a directed, acyclic graph in whic h nodes are program clauses and arcs corresp ond to the basic re nemen t operations: substituting a variable with a term, and adding a literal to the body of a clause.
 family relations problem de ned in Table 3, where the task is to learn a de nition of the daug hter relation in terms of the relations f emal e and par ent .
 where an empt y body is written instead of the body true . The re nemen t operator generates the re nemen ts of c , whic h are of the form where L is one of follo wing literals: literals having as argumen ts the variables from the head of the clause: X = Y (this corresp onds to applying a sub-stitution X=Y ), f emal e ( X ), f emal e ( Y ), par ent ( X; X ), par ent ( X; Y ), par ent ( Y; X ), and par ent ( Y; Y ), and literals that introduce a new distinct variable Z ( Z 6 = X and Z 6 = Y ) in the clause body: par ent ( X; Z ), par ent ( Z; X ), par ent ( Y; Z ), and par ent ( Z; Y ).
 This assumes that the language is restricted to de nite clauses, hence literals of the form not L are not considered, and non-recursiv e clauses, hence literals with the predicate sym bol daug hter are not considered.
 tice, with the clause d ( X; Y ) that covers all example (positiv e and negativ e). Its re nemen ts are then consid-ered, then their re nemen ts in turn, an this is rep eated until a clause is found whic h covers only positiv e exam-ples. In the example above, the clause daug hter ( X; Y ) f emal e ( X ) ; par ent ( Y; X ) is suc h a clause. Note that this clause can be reac hed in sev eral ways from the top of the lattice, e.g., by rst adding f emal e ( X ), then par ent ( Y; X ) or vice versa.
 level-wise, using heuristics based on the num ber of positiv e and negativ e examples covered by a clause. As the branc h-ing factor is very large, greedy searc h metho ds are typically applied whic h only consider a limited num ber of alterna-tives at eac h level. Hill-clim bing considers only one best alternativ e at eac h level, while beam searc h considers n best alternativ es, where n is the beam width. Occasionally , com-plete searc h is used, e.g., A best-rst searc h or breadth-rst searc h. This searc h can be bound from below by using so-called bottom clauses, whic h can be constructed by least general generalization [37] or inverse resolution/en tailmen t [36]. ILP system LINUS [29], is based on the idea that the use of bac kground kno wledge can introduce new attributes for learning. The learning problem is transformed from rela-tional to attribute-v alue form and solv ed by an attribute-value learner. An adv antage of this approac h is that data mining algorithms that work on a single table (and this is the ma jorit y of existing data mining algorithms) become applicable after the transformation.
 class of ILP problems. Thus, the hypothesis language of LI-NUS is restricted to function-free program clauses whic h are typed (eac h variable is asso ciated with a predetermined set of values), constrained (all variables in the body of a clause also app ear in the head) and nonrecursiv e (the predicate sym bol the head does not app ear in any of the literals in the body).
 transforming them into prop ositional form consists of the follo wing three steps:
The learning problem is transformed from relational to attribute-v alue form.

The transformed learning problem is solv ed by an attribute-value learner.

The induced hypothesis is transformed bac k into rela-tional form.
 dev elop ed for prop ositional problems, including noise-hand-ling techniques in attribute-v alue algorithms, suc h as CN2 [8], to be used for learning relations. It is illustrated on the simple ILP problem of learning family relations. The task is to de ne the target relation daug hter ( X; Y ), whic h states that person X is a daugh ter of person Y , in terms of the bac kground kno wledge relations f emal e , mal e and par ent . = f ann; eve; ian; mar y; tom g . There are two positiv e and two negativ e examples of the target relation. The training examples and the relations from the bac kground kno wledge are given in Table 3. However, since the LINUS approac h can use non-ground bac kground kno wledge, let us assume that the bac kground kno wledge from Table 4 is given. the ILP problem into attribute-v alue form, is performed as follo ws. The possible applications of the bac kground predi-cates on the argumen ts of the target relation are determined, taking into accoun t argumen t types. Eac h suc h application introduces a new attribute. In our example, all variables are of the same type per son . The corresp onding attribute-v alue learning problem is given in Table 5, where f stands for f emal e , m for mal e and p for par ent . The attribute-v alue tuples are generalizations (relativ e to the given bac kground kno wledge) of the individual facts about the target relation. get relation, and propositional featur es denote the newly constructed attributes of the prop ositional learning task. When learning function-free clauses, only the new attributes (prop ositional features) are considered for learning. induces the follo wing if-then rule from the tuples in Table 5: Class = if [ f emal e ( X ) = true ] ^ [ par ent ( Y; X ) = true ] into clauses. In our example, we get the follo wing clause: daug hter ( X; Y ) f emal e ( X ) ; par ent ( Y; X ) : terminate clauses [17; 30], whic h allo w the introduction of determinate new variables (whic h have a unique value for eac h training example). There also exist a num ber of other approac hes to prop ositionalization, some of them very re-cen t: an overview is given by Kramer et al. [28]. possible to transform an ILP problem into a prop ositional (attribute-v alue) form ecien tly. De Raedt [13] treats the relation between attribute-v alue learning and ILP in detail, sho wing that prop ositionalization of some more complex ILP problems is possible, but results in attribute-v alue problems that are exp onen tially large. This has also been the main reason for the dev elopmen t of a variet y of new RDM/ILP techniques by upgrading prop ositional approac hes. prop ositional learning algorithms. In particular, they share the learning as searc h paradigm, i.e., they searc h for pat-terns valid in the given data. The key di erences lie in the represen tation of data and patterns, re nemen t opera-tors/generalit y relationships, and testing coverage (i.e., whe-ther a rule explains an example).
 for upgrading prop ositional algorithms to deal with rela-tional data and patterns. The key idea is to keep as much of the prop ositional algorithm as possible and upgrade only the key notions. For rule induction, the key notions are the re nemen t operator and coverage relationship. For distance-based approac hes, the notion of distance is the key one. By carefully upgrading the key notions of a prop ositional algo-rithm, a RDM/ILP algorithm can be dev elop ed that has the original prop ositional algorithm as a special case. dev elop ILP systems for rule induction, well before it was form ulated explicitly . The well kno wn FOIL [42] system can be seen as an upgrade of the prop ositional rule induction program CN2 [8]. Another well kno wn ILP system, PR O-GOL [36] can be view ed as upgrading the AQ approac h [33] to rule induction.
 to dev elop a num ber of RDM approac hes that address data mining tasks other than binary classi cation. These include the disco very of frequen t Datalog patterns and relational as-sociation rules [11] (Chapter 8 of [16]) [10], the induction of relational decision trees (structural classi cation and regres-sion trees [27] and rst-order logical decision trees [3]), and relational distance-based approac hes to classi cation and clustering ([25], Chapter 9 of [16]; [22]). The algorithms dev elop ed have as special cases well kno wn prop ositional algorithms, suc h as the APRIORI algorithm for nding fre-quen t patterns; the CAR T and C4.5 algorithms for learning decision trees; k -nearest neigh bor classi cation, hierarc hi-cal and k -medoids clustering. In the follo wing three sections, we brie y review how eac h of the prop ositional approac hes has been lifted to a relational framew ork, highligh ting the key di erences between the relational algorithms and their prop ositional coun terparts. is one of the most commonly studied tasks in data mining. Here we rst describ e frequen t relational patterns (frequen t Datalog patterns) and relational asso ciation rules (query ex-tensions). We then look into how a well-kno wn algorithm for nding frequen t itemsets has been upgraded do disco ver frequen t relational patterns. sider patterns in the form of Datalog queries, whic h reduce to SQL queries. A Datalog query has the form ? A 1 ; A 2 ; : : : A n , where the A i 's are logical atoms. This query on a Prolog database con taining predicates per son , par ent , and hasP et is equiv alen t to the SQL query on a database con taining relations Person with argumen t Id , Parent with argumen ts Pid and Kid , and HasPet with argumen ts Pid and Aid . This query nds triples (x, y, z), where child y of person x has pet z.
 itemsets (whic h are sets of items occurring together). Con-sider the itemset f per son; par ent; chil d; pet g . The mark et-bask et interpretation of this pattern is that a person, a par-ent, a child, and a pet occur together. This is also partly the meaning of the above query . However, the variables X , Y , and Z add extra information: the person and the paren t are the same, the paren t and the child belong to the same family , and the pet belongs to the child. This illustrates the fact that queries are a more expressiv e varian t of itemsets. of frequency . Giv en that we consider queries as patterns and that queries can have variables, it is not immediately obvious what the frequency of a given query is. This is re-solv ed by specifying an additional parameter of the pattern disco very task, called the key. The key is an atom whic h has to be presen t in all queries considered during the dis-covery pro cess. It determines what is actually coun ted. In the above query , if per son ( X ) is the key, we coun t persons, if par ent ( X; Y ) is the key, we coun t (paren t,child) pairs, and if hasP et ( Y; Z ) is the key, we coun t (owner,p et) pairs. This is describ ed more precisely below.
 f X 1 ; : : : X m g to a Datalog database r corresp onds to asking whether a grounding substitution exists (whic h replaces eac h of the variables in Q with a constan t), suc h that the con-junction A 1 ; A 2 ; : : : A n holds in r . The answ er to the query pro duces answ ering substitutions = f X 1 =a 1 ; : : : X suc h that Q succeeds. The set of all answ ering substitutions obtained by submitting a query Q to a Datalog database r is denoted answ erset ( Q; r ).
 answ er substitutions for the variables in the key atom for whic h the query Q succeeds in the given database, i.e., The relativ e frequency (supp ort) can be calculated as suming the key is per son ( X ), the absolute frequency for our query involving paren ts, children and pets can be calculated by the follo wing SQL statemen t: itiv e mark et-bask et interpretation "customers that buy A typically also buy C ". Giv en A and C have supp orts f A and f C , resp ectiv ely, the con dence of the asso ciation rule is de ned to be c A ! C = f C =f A . The task of asso ciation rule disco very is to nd all asso ciation rules A ! C , where f
C and c A ! C exceed presp eci ed thresholds (minsup and minconf ).
 itemsets. Supp ose we have two frequen t itemsets A and C , suc h that A C , where C = A [ B . If the supp ort of A is f and the supp ort of C is f C , we can deriv e an asso ciation rule A ! B , whic h has con dence f C =f A . Treating the arro w as implication, note that we can deriv e A ! C from A ! B ( A ! A and A ! B implies A ! A [ B , i.e., A ! C ). manner from frequen t Datalog queries. From two frequen t queries Q 1 =? l 1 ; : : : l m and Q 2 =? l 1 ; : : : l m where Q 2 -subsumes Q 1 , we can deriv e a relational asso ci-ation rule Q 1 ! Q 2 . Since Q 2 extends Q 1 , suc h a relational asso ciation rules is named a query extension.
 plication of the form ? l 1 ; : : : l m ! ? l 1 ; : : : l (since variables in queries are existen tially quan ti ed). A shorthand notation for the above query extension is ? l ; : : : l m ; l m +1 ; : : : l n . We call the query ? l the body and the sub-query l m +1 ; : : : l n the head of the query extension. Note, however, that the head of the query extension does not corresp ond to its conclusion (whic h is ? l 1 ; : : : l m ; l m +1 ; : : : l n ).
 and Q 2 =? per son ( X ) ; par ent ( X; Y ) ; hasP et ( Y; Z ) are fre-quen t, with absolute frequencies of 40 and 30, resp ectiv ely. The query extension E =? per son ( X ) ; par ent ( X; Y ) hasP et ( Y; Z ) can be considered a relational asso ciation rule with a supp ort of 30 and con dence of 30 = 40 = 75%. Note the di erence in meaning between the query extension E and two obvious, but incorrect, attempts at de ning relational asso ciation rules. The clause per son ( X ) ; par ent ( X; Y ) ! hasP et ( Y; Z ) (whic h stands for the logical form ula 8 XY Z : per son ( X ) ^ par ent ( X; Y ) ! hasP et ( Y; Z )) would be inter-preted as follo ws: "if a person has a child, then this child has a pet". The implication ? per son ( X ) ; par ent ( X; Y ) ! ? hasP et ( Y; Z ), whic h stands for ( 9 XY : per son ( X ) ^ par ent ( X; Y )) ! ( 9 Y Z : hasP et ( Y; Z )) is trivially true if at least one person in the database has a pet. The correct interpretation of the query extension E is: "if a person has a child, then this person also has a child that has a pet." the RDM system WARMR [10]. WARMR tak es as input a database r , a frequency threshold minf req , and declarativ e language bias L . The latter speci es a key atom and input-output mo des for predicates/relations, discussed below. for disco vering frequen t patterns, whic h performs levelwise searc h [2] through the lattice of itemsets. APRIORI starts with the empt y set of items and at eac h level l considers sets of items of cardinalit y l . The key to the eciency of APRIORI lies in the fact that a large frequen t itemset can only be generated by adding an item to a frequen t itemset. Candidates at level l + 1 are thus generated by adding items to frequen t itemsets obtained at level l . Further eciency is achiev ed using the fact that all subsets of a frequen t itemset have to be frequen t: only candidates that pass this tests get their frequency to be determined by scanning the database. of Datalog queries for queries that are frequen t in the given database r . In analogy to itemsets, a more complex (sp e-ci c) frequen t query Q 2 can only be generated from a sim-pler (more general) frequen t query Q 1 (where Q 1 is more general than Q 2 if Q 1 -subsumes Q 2 ; see Section 2.3 for a de nition of -subsumption). WARMR thus starts with the query ? key at level 1 and generates candidates for frequen t queries at level l + 1 by re ning (adding literals to) frequen t queries obtained at level l .
 Table 6: An example speci cation of declarativ e language bias settings for WARMR. warmo de key(p erson(-)). warmo de(paren t(+, -)). warmo de(hasP et(+, cat)). warmo de(hasP et(+, dog)). warmo de(hasP et(+, lizard)).
 predicates per son , par ent , and hasP et , and the declara-tive bias in Table 6. The latter con tains the key atom par ent ( X ) and input-output mo des for the relations par ent and hasP et . Input-output mo des specify whether a variable argumen t of an atom in a query has to app ear earlier in the query (+), must not (-) or may, but need not to ( ). Input-output mo des thus place constrain ts on how queries can be re ned, i.e., what atoms may be added to a given query . nemen t graph of queries at level 1 with the query ? per son ( X ). At level 2, the literals par ent ( X; Y ), hasP et ( X; cat ), hasP et ( X; dog ) and hasP et ( X; lizard ) can be added to this query , yielding the candidate queries ? per son ( X ) ; hasP et ( X; dog ), and ? per son ( X ) ; hasP et ( X; lizard ). Taking the rst of the level 2 queries, we the follo wing literals are added to obtain level 3 queries: par ent ( Y; Z ) (note that par ent ( Y; X ) cannot be added, be-cause X already app ears in the query being re ned), hasP et ( Y; cat ), hasP et ( Y; dog ) and hasP et ( Y; lizard ). quen t in APRIORI, not all sub-queries of a frequen t query need be frequen t queries in WARMR. Consider the query ? per son ( X ) ; par ent ( X; Y ) ; hasP et ( Y; cat ) and assume it is frequen t. The sub-query ? per son ( X ) ; hasP et ( Y; cat ) is not allo wed, as it violates the declarativ e bias constrain t that the rst argumen t of hasP et has to app ear earlier in the query . This causes some complications in pruning the generated candidates for frequen t queries: WARMR keeps a list of infrequen t queries and chec ks whether the gener-ated candidates are subsumed by a query in this list. The WARMR algorithm is given in Table 7.
 Table 7: The WARMR algorithm for disco vering frequen t Datalog queries.
 Algorithm WARMR( r , L , key , minfr eq ; Q ) Input: Database r ; Declarativ e language bias L and key ; Output: All queries Q 2L with frequency minfr eq 1. Initialize level d := 1 2. Initialize the set of candidate queries Q 1 := f ?-key g 3. Initialize the set of (in)frequen t queries F := ; ; I := ; 4. While Q d not empt y 5. Find frequency of all queries Q 2Q d 6. Mo ve those with frequency below minfr eq to I 7. Up date F := F [Q d 8. Compute new candidates: 9. Incremen t d 10. Return F Function WARMRgen( L ; I ; F ; Q d ); 1. Initialize Q d +1 := ; 2. For eac h Q j 2Q d , and for eac h re nemen t Q 0 j 2L of Q 3. Return Q d +1 ting follo wing the upgrading recip e (see Section 2.6). The ma jor di erences are in nding the frequency of queries (where we have to coun t answ er substitutions for the key atom) and the candidate query generation (by using a re ne-men t operator and declarativ e bias). WARMR has APRI-ORI as a special case: if we only have predicates of zero arit y (with no argumen ts), whic h corresp ond to items, WARMR can be used to disco ver frequen t itemsets.
 ber of approac hes that extend the disco very of frequen t item-sets with, e.g., hierarc hies on items [47], as well as ap-proac hes to disco vering sequen tial patterns [1], including general episo des [32]. The individual approac hes men tioned mak e use of the speci c prop erties of the patterns consid-ered (very limited use of variables) and are more ecien t than WARMR for the particular tasks they address. The high expressiv e power of the language of patterns considered has its computational costs, but it also has the imp ortan t adv antage that a variet y of di eren t pattern types can be explored without any changes in the implemen tation. sitionalization, i.e., to transform MRDM problems to prop o-sitional (single table) form. WARMR is rst used to disco ver frequen t queries. In the prop ositional form, examples cor-resp ond to answ er substitutions for the key atom and the binary attributes are the frequen t queries disco vered. An attribute is true for an example if the corresp onding query succeeds for the corresp onding answ er substitution. This approac h has been applied with considerable success to the tasks of predictiv e toxicology [9] and genome-wide predic-tion of protein functional class [24]. to data mining. Upgrading this approac h to a relational setting has thus been of great imp ortance. In this section, we rst look into what relational decision trees are, i.e., how they are de ned, then discuss how suc h trees can be induced from multi-relational data. tional prediction is de ned by a two-place target predicate tar get ( Exampl eID; ClassV ar ), whic h has as argumen ts an example ID and the class variable, and a set of bac kground kno wledge predicates/relations. Dep ending on whether the class variable is discrete or con tinuous, we talk about rela-tional classi cation or regression. Relational decision trees are one approac h to solving this task.
 It predicts the main tenance action A that should be tak en on mac hine M ( maintenance ( M; A )), based on parts the ma-chine con tains ( haspar t ( M; X )), their condition ( worn ( X )) and ease of replacemen t ( irreplaceabl e ( X )). The target predicate here is maintenance ( M; A ), the class variable is A , and bac kground kno wledge predicates are haspar t ( M; X ), worn ( X ) and irreplaceabl e ( X ). Figure 3: A relational decision tree, predicting the class variable A in the target predicate maintenance ( M; A ). as prop ositional decision trees. Internal nodes con tain tests, while leaves con tain predictions for the class value. If the class variable is discrete/con tinuous, we talk about rela-tional classi cation/regression trees. For regression, linear equations may be allo wed in the leaves instead of constan t class-v alue predictions: in this case we talk about relational mo del trees.
 while the tree in Figure 2 is a relational regression tree. Table 8: A decision list represen tation of the relational de-cision tree in Figure 3. maintenance ( M; A ) haspar t ( M; X ) ; worn ( X ) ; maintenance ( M; A ) haspar t ( M; X ) ; worn ( X ) ; !, maintenance ( M; A ) A = no maintenance The latter predicts the degradation time (the logarithm of the mean half-life time in water [19]) of a chemical com-pound from its chemical structure, where the latter is rep-resen ted by the atoms in the comp ound and the bonds be-tween them. The target predicate is deg rades ( C; Log HLT ), the class variable Log HLT , and the bac kground kno wledge predicates are atom ( C; AtomI D; Element ) and bond ( C; A 1 ; A 2 ; BondT ype ). The test at the root of the tree atom ( C; A 1 ; cl ) asks if the comp ound C has a chlorine atom A 1 and the test along the left branc h chec ks whether the chlorine atom A 1 is connected to a nitrogen atom A 2. ference between prop ositional and relational decision trees is in the tests that can app ear in internal nodes. In the re-lational case, tests are queries, i.e., conjunctions of literals with existen tially quan ti ed variables, e.g., atom ( C; A 1 ; cl ) and haspar t ( M; X ) ; worn ( X ). Relational trees are binary: eac h internal node has a left (yes) and a righ t (no) branc h. If the query succeeds, i.e., if there exists an answ er substi-tution that mak es it true, the yes branc h is tak en. among nodes, i.e., a variable introduced in a node can be referred to in the left (yes) subtree of that node. For exam-ple, the X in irreplaceabl e ( X ) refers to the mac hine part X introduced in the root node test haspar t ( M; X ) ; worn ( X ). Similarly , the A 1 in bond ( C; A 1 ; A 2 ; BT ) refers to the chlo-rine atom introduced in the root node atom ( C; A 1 ; cl ). One cannot refer to variables introduced in a node in the righ t (no) subtree of that node. For example, referring to the chlorine atom A 1 in the righ t subtree of the tree in Figure 2 mak es no sense, as going along the righ t (no) branc h means that the comp ound con tains no chlorine atoms.
 conjunction of the literals in the node itself and the literals on the path from the root of the tree to the node in question. For example, the test in the node irreplaceabl e ( X ) in Fig-ure 3 is actually haspar t ( M; X ) ; worn ( X ) ; irreplaceabl e ( X ). In other words, we need to send the mac hine bac k to the man ufacturer for main tenance only if it has a part whic h is both worn and irreplaceable. Similarly , the test in the node bond ( C; A 1 ; A 2 ; BT ) ; atom ( C; A 2 ; n ) in Figure 2 is in fact consequence, one cannot transform relational decision trees to logic programs in the fashion "one clause per leaf" (unlik e prop ositional decision trees, where a transformation "one rule per leaf" is possible).
 rst-order decision lists, whic h are ordered sets of clauses (clauses in logic programs are unordered). When applying a decision list to an example, we alw ays tak e the rst clause that applies and return the answ er pro duced. When apply-ing a logic program, all applicable clauses are used and a set of answ ers can be pro duced. First-order decision lists can be represen ted by Prolog programs with cuts (!) [4]: cuts ensure that only the rst applicable clause is used. atom ( C; A 1 ; cl ) deg rades ( C; Log HLT )).
 Table 9: A decision list represen tation of the relational re-gression tree for predicting the bio degradabilit y of a com-pound, given in Figure 2. deg rades ( C; Log HLT ) atom ( C; A 1 ; cl ) ; deg rades ( C; Log HLT ) atom ( C; A 1 ; cl ) ; deg rades ( C; Log HLT ) atom ( C; A 3 ; o ) ; deg rades ( C; Log HLT ) Log HLT = 6 : 73 : Table 10: A logic program represen tation of the relational decision tree in Figure 3. a ( M ) haspar t ( M; X ) ; worn ( X ) ; irreplaceabl e ( X ) b ( M ) haspar t ( M; X ) ; worn ( X ) maintenance ( M; A ) not a ( M ) ; A = no aintenance maintenance ( M; A ) b ( M ) ; A = repair in house maintenance ( M; A ) a ( M ) ; not b ( M ) ; A = send back gression tree in a depth-rst fashion, going down left branc hes rst. At eac h leaf, a clause is output that con tains the prediction of the leaf and all the conditions along the left (yes) branc hes leading to that leaf. A decision list obtained from the tree in Figure 3 is given in Table 8. For the rst clause ( send back ), the conditions in both internal nodes are output, as the left branc hes out of both nodes have been follo wed to reac h the corresp onding leaf. For the second clause, only the condition in the root is output: to reac h Table 11: The TDIDT part of the SCAR T algorithm for inducing relational decision trees. if Termina tionCondition (Examples) then else the repair in house leaf, the left (yes) branc h out of the root has been follo wed, but the righ t (no) branc h out of the irreplaceabl e ( X ) node has been follo wed. A decision list pro duced from the relational regression tree in Figure 2 is given in Table 9.
 tree is more complicated. It requires the introduction of new predicates. We will not describ e the transformation pro cess in detail, but rather give an example. A logic program, corresp onding to the tree in Figure 3 is given in Table 10. sion trees are upgrades of the two most famous algorithms for inducting prop ositional decision trees. SCAR T [26; 27] is an upgrade of CAR T [5], while TILDE [3; 14] is an up-grade of C4.5 [43]. According to the upgrading recip e, both SCAR T and TILDE have their prop ositional coun terparts as special cases. The actual algorithms thus closely follo w CAR T and C4.5. Here we illustrate the di erences between SCAR T and CAR T by looking at the TDIDT (top-do wn induction of decision trees) algorithm of SCAR T (Table 11). if a termination condition is satis ed, e.g., if all examples belong to the same class c . If yes, a leaf is constructed with an appropriate prediction, e.g., assigning the value c to the class variable. Otherwise a test is selected among the possible tests for the node at hand, examples are split into subsets according to the outcome of the test, and tree construction pro ceeds recursiv ely on eac h of the subsets. A tree is thus constructed with the selected test at the root and the subtrees resulting from the recursiv e calls attac hed to the resp ectiv e branc hes.
 case is in the possible tests that can be used in a node. While in CAR T these remain (more or less) the same regardless of where the node is in the tree (e.g., A = v or A &lt; v for eac h attribute and attribute value), in SCAR T the set of possible tests crucially dep end on the position of the node in the tree. In particular, it dep ends on the tests along the path from the root to the curren t node, more precisely the variables app earing in those tests and the declarativ e bias. To em-phasize this, we can think of a Genera teTests pro cedure being separately emplo yed before evaluating the tests. The inputs to this pro cedure are the tests on positiv e branc hes from the root to the curren t node and the declarativ e bias. These are also inputs to the top level TDIDT pro cedure. the form schema(CofL,T andM) , where CofL is a conjunction of literals and TandM is a list of type and mo de declarations for the variables in those literals. Tw o suc h schema state-men ts, used in the induction of the regression tree in Figure 2 are as follo ws: schema((b ond(V, W, X, Y), atom(V, X, Z)), [V:chemic al:'+', W:atomid:'+', X:atomid:'-', Y:b ondtyp e:'-', Z:element: '=']) and schema(b ond(V, W, X, Y), [V: chemic al:'+', W:atomid:'+', X:atomid:'-', Y:b ondtyp e: '=']). In the lists, eac h variable in the conjunction is follo wed by its type and mo de declaration: '+' denotes that the variable must be bound (i.e., app ear in TestsOnY esBr anchesSofar ), -that it must not be bound, and = that it must be replaced by a constan t value.
 in Figure 2, TestsOnY esBr anchesSofar = atom ( C; A 1 ; cl ). Taking the declarativ e bias with the two schema statemen ts above, the only choice for replacing the variables V and W in the schemata are the variables c and A 1, resp ec-tively. The possible tests at this stage are thus of the form bond ( C; A 1 ; A 2 ; BT ) ; atom ( C; A 2 ; E ), where E is replaced with an elemen t (suc h as cl -chlorine, s -sulph ur, or n -nitrogen), or of the form bond ( C; A 1 ; A 2 ; BT ), where BT is replaced with a bond type (suc h as sing le , doubl e , or aromatic ). Among the possible tests, the test bond ( C; A 1 ; A 2 ; BT ) ; atom ( C; A 2 ; n ) is chosen. among the fastest MRDM approac hes. They have been suc-cessfully applied to a num ber of practical problems. These include learning to predict the bio degradabilit y of chemi-cal comp ounds [19] and learning to predict the structure of diterp ene comp ounds from their NMR spectra [18]. cluding prediction and clustering, it is necessary to upgrade the key notion of a distance measure from the prop ositional to the relational case. Suc h a measure could then be used within standard statistical approac hes, suc h as nearest-neigh-bor prediction or hierarc hical agglomerativ e clustering. In their system RIBL, Emde and Wettsc herec k [22] prop ose a relational distance measure. Belo w we rst brie y discuss this measure, then outline how it has been used for relational classi cation and clustering [25]. amples that have the form of vectors of attribute values. They essen tially sum up the di erences between the ex-amples' values along eac h of the dimensions of the vectors. Giv en two examples x = ( x 1 ; : : : ; x n ) and y = ( y their distance migh t be calculated as where the di erence between attribute values is de ned as di erence( x i ; y i ) = instance or case) can be describ ed by a set of facts about multiple relations. A fact of the target predicate of the form tar get ( Exampl eID; A 1 ; :::; A n ) speci es an instance through its ID and prop erties, and additional information can be speci ed through bac kground kno wledge predicates. In Ta-ble 12, the target predicate memb er(PersonID,A,G,I,MT) speci es information on mem bers of a particular club, whic h includes age, gender, income and mem bership type. The bac kground predicates car ( Owner ID; CT; T S; M ) and house ( Owner ID; Distr ictI D; Y; S ) pro vide information on prop erty owned by club mem bers: for cars this includes car type, top speed and man ufacturer, for houses the district, construction year and size. Additional information is avail-able on districts through the predicate distr ict ( Distr ictI D; P; S; C ), i.e., the popularit y, size, and coun try of the district.
 Table 12: Tw o examples on whic h to study a relational dis-tance measure. mem ber( person1 ; 45 ; male ; 20 ; gold ) mem ber( person2 ; 30 ; female ; 10 ; platinum ) car( person1 ; wagon ; 200 ; volkswagen ) car( person1 ; sedan ; 220 ; mer cedesb enz ) car( person2 ; roadster ; 240 ; audi ) car( person2 ; coup e ; 260 ; bmw ) house( person1 ; mur gle ; 1987 ; 560 ) house( person1 ; monte carlo ; 1990 ; 210 ) house( person2 ; mur gle ; 1999 ; 430 ) district( monte carlo ; famous ; large ; monac o ) district( mur gle ; famous ; smal l ; slovenia ) is as follo ws. To calculate the distance between two ob-jects/examples, their prop erties are tak en into accoun t rst (at depth 0). Next (at depth 1), objects immediately related to the two original objects are tak en into accoun t, or more precisely , the distances between the corresp onding related objects. At depth 2, objects related to those at depth 1 are tak en into accoun t, and so on, until a user-sp eci ed depth limit is reac hed. mem ber( person1 ; 45 ; male ; 20 ; gold ) ground kno wledge in Table 12 and a depth limit of 2. e 1 = member ( per son 1 ; 45 ; mal e; 20 ; gold ) and e = member ( per son 2 ; 30 ; f emal e; 10 ; platinum ), the prop-erties of the persons (age, gender, income, mem bership type) are rst compared and di erences between them calculated and summed (as in the prop ositional case). At depth 1, cars and houses owned by the two persons are compared, i.e., distances between them are calculated. At depth 2, the dis-tricts where the houses reside are tak en into accoun t when calculating the distances between houses. Before beginning to calculate distances, RIBL collects all facts related to a person into a so-called case. The case for per son 1 generated with a depth limit of 2 is given in Figure 4.
 bers according to the distance measure. d ( e 1 ; e 2 ) = 1 = 5 ( d ( per son 1 ; per son 2) + d (45 ; 30) + d ( mal e; f emal e )+ d (20 ; 10) + d ( gold; platinum )). With a depth limit of 0, the iden ti ers per son 1 and per son 2 are treated as discrete values, d ( per son 1 ; per son 2) = 1 and we have d ( e (1 + (45 30) = 100 + 1 + (20 10) = 50 + 1) = 5 = 0 : 67; the denominators 100 and 50 denote the highest possible di er-ences in age and income.
 the facts directly related to the two persons and partition them according to the predicates: we thus have F 1 ; car = f car( person1 ; wagon ; 200 ; volkswagen ), car( person1 ; sedan ; 220 ; mer cedesb enz ) g ; F 2 ; car = f car( person2 ; roadster ; 240 ; audi ), car( person2 ; coup e ; 260 ; bmw ) g ; F 1 ; house = f house ( person1 ; mur gle ; 1987 ; 560 ), house( person1 ; monte carlo ; 1990 ; 210 ) g ; and F 2 ; house = f house( person2 ; mur gle ; 1999 ; 430 ) g : Then d ( per son 1 ; per son 2) = ( d ( F 1 ; car ; F d ( F 1 ; house; F 2 ; house )) = 2. Distances between sets of facts are calculated as follo ws. We tak e the smaller set of facts (or the rst, if they are of the same size): for d ( F 1 ; house; F 2 ; house ), we tak e F 2 ; house . For eac h fact in this set, we calculate its distance to the nearest elemen t of the other set, e.g., F 1 ; house , summing up these distances (the house of person2 is closer to the house of person1 in mur gle then to the one in monte carlo ). We add a penalt y for the possible mismatc h in cardinalit y and normalize with the cardinalit y of the larger set: d ( F 1 ; house; F 2 ; house ) = [1 + min ( d ( house ( per son 2 ; mur gle; 1999 ; 430) ; 0 : 5 [1 + min ((0 + (1999 1987) = 100 + j 430 560 j = 1000) = 3 ; = 0 : 5 + 0 : 5 min (0 : 25 = 3 ; 1 : 31 = 3) = 13 = 24. note that both cars of person1 are closer to the audi of per-son2 than to the bmw . We thus have d ( F 1 ; car ; F 2 ; car ) = = 0 : 5 [(1 + j 200 240 j = 100 + 1) = 3 ; (1 + j 220 240 j = 100 + 1) = 3] = 11 = 15. Thus, at level 1, d ( per son 1 ; per son 2) = 0 : 5 (13 = 24 + 11 = 15) = 0 : 6375 and d ( e 1 ; e 2 ) = (0 : 6375 + (45 30) = 100 + 1 + (20 10) = 50 + 1) = 5 = 0 : 5975. is tak en into accoun t when calculating d ( F 1 ; house; F We have d ( mur gle; montecar lo ) = (0 + 1 + 1) = 3 = 2 = 3. However, since the house of person2 is closer to the house of person1 in mur gle then to the one in monte carlo , the value of d ( F 1 ; house; F 2 ; house ) does not change as it equals 0 : 5 [1 + min ((0 + (1999 1987) = 100 + j 430 560 j = 1000) = 3 ; = 0 : 5 + 0 : 5 min (0 : 25 = 3 ; (2 = 3 + 0 : 31) = 3) = 13 = 24. d ( e is thus the same at level 1 and level 2 and is equal to 0.5975. not a metric [44]. However, some relational distance mea-sures that are metrics have been prop osed recen tly [45]. De-signing distance measures for relational data is still a largely open and lively researc h area. Since distances and kernels are strongly related, this area is also related to designing kernels for structured data (Gaertner; this issue). ily adapt classical statistical approac hes to prediction and clustering, suc h as the nearest-neigh bor metho d and hierar-chical agglomerativ e clustering, to work on relational data. This is precisely what has been done with the RIBL distance measure.
 diction, more precisely classi cation. It uses the k -nearest neigh bor ( k NN) metho d in conjunction with the RIBL dis-tance measure to solv e the problem addressed. RIBL was successfully applied to the practical problem of diterp ene structure elucidation [18], where it outp erformed prop osi-tional approac hes as well as a num ber of other relational approac hes.
 sidering lists and terms as elemen tary types, much like dis-crete and numeric values. Edit distances are used for these, while the RIBL distance measure is follo wed otherwise. RIBL2 has been used to predict mRNA signal structure and to automatically disco ver previously unc haracterized mRNA signal structure classes [25].
 the RIBL distance measure [25]. RDBC uses hierarc hical agglomerativ e clustering, while FOR C adapts the k -means approac h. The latter relies on nding cluster cen ters, whic h is easy for numeric vectors but far from trivial in the rela-tional case. FOR C thus uses the k -medoids metho d, whic h de nes a cluster cen ter as the existing case/example that has the smallest sum of squared distances to all other cases in the cluster and only uses distance information. and Lavra c [16] pro vides a cross-section of the state-of-the-art in this area at the turn of the millennium. This intro-ductory article is largely based on material from that book. mer Scho ol on Inductive Logic Programming and Know ledge Disc overy in Datab ases (ILP&amp;KDD-97), held 15{17 Septem-ber 1997 in Prague, Czec h Republic, organized in conjunc-tion with the Seventh International Workshop on Inductive Logic Programming (ILP-97). The teac hing materials from this event are available on-line at http://www-ai.ijs.si/ SasoDzeroski/ILP2/ilpkdd/ . A summer school on rela-tional data mining, covering both basic and adv anced top-ics in this area, was organized in Helsinki in August 2002, preceding ECML/PKDD 2002 (The 13th Europ ean Confer-ence on Mac hine Learning and The 6th Europ ean Confer-ence on Principles and Practice of Kno wledge Disco very in Databases). The slides are available online at http://www-ai.ijs.si/SasoDze rosk i/RD MScho ol/ . A re-port on this event by Dzeroski and Zenk o is included in this special issue.
 nual workshop on Multi-Relational Data Mining. The rst Europ ean event on this topic was held in 2001 ( http://mrdm.dantec.nl/ ). Tw o international workshops were organized in conjunction with KDD-2002 and KDD-2003 ( http://www-ai.ijs.si/SasoDzer oski /MRDM 2002 / and MRDM2003/ ). Of interest are the workshops on Learning Statistic al Models from Relational Data held at AAAI-2000 ( http://robotics.stanford.edu /srl/ work shop .html ) and IJCAI-2003 ( http://kdl.cs.umass.edu/events/s rl20 03/ ) and the workshop on Mining Graphs, Trees and Sequenc es , held at ECML/PKDD-2003 ( http://www.ar.sanken. osaka-u.ac.jp/MGTS-2003CFP.h tml ).
 the topic of multi-relational data mining. Tw o journal spe-cial issues address the related topic of using ILP for KDD: Applie d Arti cial Intel ligenc e (vol. 12(5), 1998), and Data Mining and Know ledge Disc overy (vol. 3(1), 1999). ture. For an overview of the ILP literature, see Chapter 3 of the RDM book [16]. ILP-related bibliographic information can be found at ILPnet2's on-line library ( http://www.cs. bris.ac.uk/~ILPnet2/Library/ ).
 the ann ual ILP workshop. The rst International Workshop on Inductive Logic Programming (ILP-91) was organized in 1991. Since 1996, the pro ceedings of the ILP workshops are published by Springer within the Lecture Notes in Arti cial Intelligence/Lecture Notes in Computer Science series. mac hine learning and arti cial intelligence conferences. The same goes for a num ber of journals, including Journal of Logic Programming , Machine Learning , and New Gener a-tion Computing . Eac h of these has published sev eral special issues on ILP . Special issues on ILP con taining extended versions of selected pap ers from ILP workshops app ear reg-ularly in the Machine Learning journal.
 a book Inductive Logic Programming , edited by Muggleton [35], while selected pap ers from ILP-95 app eared as a book Advanc es in Inductive Logic Programming , edited by De Raedt [12]. Authored books on ILP include Inductive Logic Programming: Techniques and Applic ations by Lavra c and Dzeroski [30] and Foundations of Inductive Logic Program-ming by Nienh uys-Cheng and de Wolf [40]. The rst pro-vides a practically orien ted introduction to ILP , but is dated now, given the fast dev elopmen t of ILP in the recen t years. The other deals with ILP from a theoretical persp ectiv e. site @ IJS ( http://www-ai.ijs.si/ ilpnet2/ ) is of special interest. It con tains an overview of ILP related resources in sev eral categories. These include a list of and pointers to ILP-related educational materials, ILP applications and datasets, as well as ILP systems. It also con tains a list of ILP-related events and an electronic newsletter. For a detailed overview of ILP-related Web resources we refer the reader to Chapter 16 of the RDM book [16]. states (Domingos; this issue), (multi-)relational data min-ing (MRDM) is a eld whose time has come. The presen t article pro vides an entry point into this lively and exciting researc h area. Since man y of the MRDM techniques around have their origin in logic-based approac hes to learning and inductiv e logic programming (ILP), an introduction to ILP was given rst. Three ma jor approac hes to MRDM were covered next: relational asso ciation rules, relational deci-sion trees and relational distance-based approac hes. Finally , a brief overview of the literature and Internet resources in this area were pro vided for those looking for more detailed information. While a variet y of successful applications of MRDM exist, these are not covered in this article: we refer the reader to [20], as well as Page and Cra ven; Domingos; and Geto or (this issue).
 recen t adv ances in MRDM. To some exten t, hot topics in MRDM mirror hot topics in data mining and mac hine learn-ing. These include ensem ble metho ds (not covered here, see, e.g., Chapter 12 of [16]), kernel metho ds (Gaertner; this is-sue), probabilistic metho ds (De Raedt and Kersting; this is-sue), and scalabilit y issues (Blo ckeel and Sebag; this issue). The same goes for application areas, with computational bi-ology and bioinformatics being the most popular (Page and Cra ven; this issue). Web mining and link mining (link analy-sis/link disco very) follo w suit (Domingos; this issue; Geto or; this issue).
 ble is righ tfully attracting an ever increasing amoun t of re-searc h e ort. It is imp ortan t to realize that the form ulation of multi-relational data mining is very general and has as special cases mining of data in the form of sequences, trees, and graphs. A surv ey article (W ashio and Moto da; this is-sue) and a position statemen t (Holder and Cook; this issue) on mining data in the form of graphs are included in this special issue. Exploring represen tations that are richer than prop ositional and poorer than full rst-order logic may be well worth while, because of the possibilit y to design more ecien t algorithms.
 portan t and exciting researc h eld of multi-relational data mining. It also outlines recen t adv ances and interesting open questions. (The latter include, for example, the seamless integration of MRDM approac hes within actual database managemen t systems (DBMSs) and using the query opti-mization techniques of the DBMSs to impro ve the eciency of MRDM approac hes.) In this way, we hop e to attract the atten tion of data mining researc hers and stim ulate new re-searc h and solutions to open problems in this area, whic h can have practical applications and far reac hing implications for the entire eld of data mining. [1] R. Agra wal and R. Srik ant. Mining sequen tial patterns. [2] R. Agra wal, H. Mannila, R. Srik ant, H. Toivonen, [3] H. Blo ckeel and L. De Raedt. Top-do wn induction of [4] I. Bratk o. Prolog Programming for Arti cial Intel li-[5] L. Breiman, J. H. Friedman, R. A. Olshen, and [6] W. Bun tine. Generalized subsumption and its appli-[7] P. Clark and R. Bosw ell. Rule induction with CN2: [8] P. Clark and T. Niblett. The CN2 induction algorithm. [9] L. Dehasp e, H. Toivonen, and R. D. King. Finding fre-[10] L. Dehasp e and H. Toivonen. Disco very of frequen t dat-[11] L. Dehasp e and H. Toivonen. Disco very of Relational [12] L. De Raedt, editor. Advanc es in Inductive Logic Pro-[13] L. De Raedt. Attribute-v alue learning versus induc-[14] L. De Raedt, H. Blo ckeel, L. Dehasp e, and W. Van [15] L. De Raedt and S. Dzeroski. First order jk -clausal [16] S. Dzeroski and N. Lavra c, editors. Relational Data [17] S. Dzeroski, S. Muggleton, and S. Russell. PAC-[18] S. Dzeroski, S. Schulze-Kremer, K. Heidtk e, K. Siems, [19] S. Dzeroski, H. Blo ckeel, B. Kompare, S. Kramer, [20] S. Dzeroski. Relational Data Mining Applications: An [21] S. Dzeroski, L. De Raedt, and S. Wrob el, editors. Pro-[22] W. Emde and D. Wettsc herec k. Relational instance-[23] P. Flac h. Logical approac hes to mac hine learning -an [24] R.D. King, A. Karw ath, A. Clare, and L. Dehasp e. [25] M. Kirsten, S. Wrob el, and T. Horv ath. Distance Based [26] S. Kramer. Structural regression trees. In Proceedings [27] S. Kramer and G. Widmer. Inducing Classi cation and [28] S. Kramer, N. Lavra c, and P. Flac h. Prop ositionaliza-[29] N. Lavra c, S. Dzeroski, and M. Grob elnik. Learning [30] N. Lavra c and S. Dzeroski. Inductive Logic Pro-[31] J. Llo yd. Foundations of Logic Programming , 2nd edi-[32] H. Mannila and H. Toivonen. Disco vering generalized [33] R. Mic halski, I. Mozeti c, J. Hong, and N. Lavra c. The [34] S. Muggleton. Inductiv e logic programming. New Gen-[35] S. Muggleton, editor. Inductive Logic Programming . [36] S. Muggleton. Inverse entailmen t and Progol. New Gen-[37] S. Muggleton and C. Feng. Ecien t induction of logic [38] C. Nedellec, C. Rouv eirol, H. Ade, F. Bergadano, and [39] T. Niblett. A study of generalisation in logic programs. [40] S.-H. Nienh uys-Cheng and R. de Wolf. Foundations of [41] G. Plotkin. A note on inductiv e generalization. In [42] J. R. Quinlan. Learning logical de nitions from rela-[43] J. R. Quinlan. C4.5: Programs for Machine Learning . [44] J. Ramon. Clustering and instanc e based [45] J. Ramon and M. Bruyno oghe. A polynomial time com-[46] E. Shapiro. Algorithmic Program Debugging . MIT [47] R. Srik ant and R. Agra wal. Mining generalized asso-[48] J. Ullman. Principles of Datab ase and Know ledge Base [49] V. Van Laer and L. De Raedt. How to Upgrade Prop o-[50] S. Wrob el. Inductiv e Logic Programming for Kno wl-
