 Mongolian language has a wide influence in the world. It is used in China, Mongo-lia, Russia and other countries where the pronunciations are almost the same but the writing forms are different from each other. The Mongolian language used in China is called "T raditional M ong olian" . The corresponding Mongolian language used in Mongolia is called "Cyrillic Mongolian" , which letters are borrowed from to the t raditional Mongolian language .
 letter has several different presentation forms. The concrete presentation forms are determined by their positions (initial, medial or final) occurred in words. It leads to a phenome non that many Mongolian words have t he some presentation forms with different codes. In fact, most people only input the words according to their presen-tation forms without considering their codes. Therefore, the words bearing the same presentation forms w ith different codes may be incorrect in the Mongolian text. It results in the statistical information from the corresponding Mongolian corpus is inaccurate and weakens the performance of the language model in Mongolian in-formation processing, such as speec h recognition [1], information retrieval [2], ma-chine translation [3] and so on.
 the literature . Chuanjie Su [4] and Jun Zhao [5] adopt ed language model to correct the coding errors. Sloglo [6] proposed a correction method based on the finite au-tomata. Bule Jiang [7] used a rule -based approach to deal with th e correction prob-lem. However, t h ese approaches can only correct part of the coding errors , and the words that do not follows the spelling rules or out of vocabulary (OOV) cannot be corrected.
 ters to express the words being the same presentation forms with the different codes. I n detail of o ur approach , the words can be converted into same forms by the Intermediate characters . Then, the language models ( without processing , in In-termediate characters) were constructed. And these language models are compared by perplexity and accuracy of Mongolian speech recog nition. Experimental results show that the propos ed approach not only greatly reduced the perplexity of the N -gram language model , but also greatly reduced the word error rate ( WER ) of Mon-golian speech recognition .
 istics of Mongolian encoding. Section 3 describes the Mongolian Intermediate characters. Section 4 gives the Mongolian language model based on the Intermedi-ate characters. Secti on 5 briefly introduces the process of speech recognition. Sec-tion 6 shows the experiment al results . Section 7 draws the conclusion. Mongolian characters contain two character types: nominal characters and presen-tatio n characters. According to Universal Coded Character Set ( UCS ) ISO/IEC 10646 and PRC N ational S tandard GB 13000 -2010, Mongolian character set only includes the nominal characters , and the units larger than one letter or less than one letter are not encoded . Generally, Mongolian letter set refers to the nominal char-miss ion, processing, storage, displaying. A few coding standards that created by some commercial companies use the presentation characters to encode Mongolian words [8].
 forms. Each nominal characters has several presentation forms according to its po-sitions in words [8]. Table 1 shows Mongolian no minal characters and its corre-sponding presentation forms. From Table 1, we can see that some characters have different nominal forms but same presentation forms. with the incorrect Mongolian code in Mongolian corpus. The reasons are twofold: first, the pronunciations of some letters are often confused in Mongolian dialects, such as the vowels "u" and "v", the vowels "o" and "u", the consonants "t" and "d", and thus Mongolian people living this regions often make many typo errors in text ; second, some typists only care about whether the presentation forms of the words are correct or not, rather than the codes of these words, and freely replace the co r-rect letter with another one with same presentation form s . The typo errors in Mon-golian corpus makes it difficult for us to count, retrieval of the text, as well as train-ing Mongolian language model. We use an example to illustrate this.
 is "undusuten". According to the analysis on a Mongolian corpus including 76 mil-lion Mongolian words, this word appears 102532 times, and only 24708 times of same presentation forms. Actually, there are 291 words that have the same presen-tation forms as the word "  X  X  X  X  X  X  X  X  X  X  X  X  X  " (meaning: minority). Figure 1 shows the Mon-golian word "  X  X  X  X  X  X  X  X  X  X  X  X  X  " (meaning: minority) and its typos whose freque ncy is greater than 100 in the corpus. This paper put s forward a novel method to represent the Mongolian word s accord-ing to the characteristics of Mongolian presentation form s. Th is method uses Latin character s to represe nt the nominal letters, and the nominal characters with same presentation forms a re represented by the same Latin character s . These Latin char-acters are called Intermediate characters. In some cases, a nominal character is con-verted into multiple Intermedi ate characters depending on its positions in words. That is, the words in same presentation forms are mapping to the same Intermediate character s string. I t is worth pointing out that a string of Latin cha racters can only correspond to a string of Intermediate character s and a string of Intermediate char-acters can correspond to one or multiple strings of Latin characters with some presentation forms .
 pus without processing and the corresponding Mongolian corpus represented in In-termediate characters.
 Intermediate characters form. This takes the advantages of regular expression that the rules can be easily expressed by regular expressions. At the same time, we in-tegrate some rules to correct the spelling errors. In this paper, we summarize 116 transformation and cor rection rules of Intermediate characters. We do the related statistics that these rules can cover most of the Mongolian word. Table 2 shows part of these rules. " _'"'&amp; *^ " represents the Mongolian control character; ":ANY:" represents of any Mongolian letter; ":VOW: " represents all vowels; ":CSNT:" means all consonants; "G", "U", "V", "A", "I" and other characters represent Inter-mediate forms which are defined. In Tabl e 2 , No 1 -7 is the Intermediate characters conversion rule and No 8 -10 is the correct rule. For example, the 291 Mongolian words having the same presentation forms as Mongolian words "  X  X  X  X  X  X  X  X  X  X  X  X  X  " ( meaning: minority) will be converted into the same Intermedi ate character s string "UnTO-sOTAn". Language model is a mathematical model to describe the inherent laws of natural language. It is the core of computational linguistics. In theory, the structure of lan-guage model is to induce, discover, and obtain the inherent laws of natural language in s tatistical and structural aspects. L anguage model are crucial components in many Natural Language Processing (NLP) applications, such as speech recogni-tion, handwriting recognition, machine tra nslation, information retrieval and so on. model. The probability of a Mongolian word sequence  X  =  X  1  X  2 ... ...  X   X  can be written in the form of conditional probability: words  X  1  X  2 ... ...  X   X   X  1 . We can now use this model to estimate the probability of seeing sentences in the corpus by providing a sim ple independence assumption based on the Markov assumption [10]. Corresponding to the language model, the current word is only related to the previous n -1 words. From the equation (1), we can see that the target of language model is how to estimate the con ditional proba-bility of the next word in the list using  X  (  X   X  |  X  ability estimation method we used is the maximum likelihood estimation (MLE).  X  (  X  ) means the total count of the N -gram in the corpus. However, a drawback of the MLE is that the N -tuple corpus which does not appear in the training set will be give n zero -P robability. This is not allowed in the NLP . Smoothing algorithm can be used to solve this kind of zero -Probabilities problem. In this paper, we us the Kneser -Ney smoothing algorithm [1 1 ].
 word sequence  X  =  X  1  X  2 ... ...  X   X  are convert ed into its corresponding Intermedi-mate this as: Speech re cognition allows the machine to turn the speech signal into text or com-mands through the process of identification and understanding [12]. The process of speech recognition mainly includes pre -processing, feature extraction, model train-cessing consists of pre -filtering, sampling, quantization, adding window, endpoint detection, and pre -emphasis towards the speech signal. Feature extraction is to ef-fectively extract the features form the speech signal. Speech decoding is to look for the maximum probability of the output word sequences toward the speech signal , which greatly relies on the acoustic model, language model and pronunciation dic-tionary and is carried out by Viterbi algorithm [15]. For the speech feature  X  =  X  speech recognition based on the maximum a post -probability (MAP) is shown as follows: F orm the equation (4), we just need to calculate the maximum product of  X  (  X  |  X  ) condition of word sequence  X  , which is determined by the acoustic model.  X  (  X  ) is the probability of a word sequence  X  , which is determined by the language model.
 network (RNN) [13,14] to build language model, combining with deep neural net-work (DNN) and hidden Markov model (HMM) for acoustic model. Thi s paper uses the language model of 3 -gram and the acoustic model of DNN or LSTM in Mongolian speech recognition. The language model ( without processing , in Inter-mediate characters) was respectively created, compared in perplexity experiments and speech rec ognition experiments. In this paper, the performance of the language model was verified o n the Mongolian corpus through a series of experiments, including the perplexity experiment s and speech recognition experiment s . The evaluation metrics are perplexity (PPL) of language model and WER of speech recognition. 6.1 P erplexity 6.1.1 Data The data sets is constructed by the page content coming from mgyxw.net , mon-gol.people.cn, holvoo.net and more than 20 other websites. Part of those website uses the Mongolian menksoft encoding and the other us es the Mongolian standard encoding. In order to unify coding, the text of Mongolian menksoft encoding was converted in to Mongolian standard encoding. The correct rate reached over 99% by using the Mongolian c onversion toolkit ( http://mtg.mglip.com ) which was de-veloped by Inner Mongolia University. The Mongolian corpus used for construct-validatio n method was used to evaluate the experimental performance . Table 3 lists the details of the t raining set and t esting set ( without processing , in Intermediate characters) in average . We use Train_Mon and Test_Mon to represent the Training set without proce ssing and Testing set without processing respectively, and use Train_IC and Test_IC to represent the Training set in Intermediate character s form and Testing set in Intermediate character s form respectively. Tokens refer to the number of Mongolian words in the dataset ; vocabulary means the number of dis-tinct words ; OOV represents the word which is not included in the training set but in the testing set. It is worth noting that, the case suffixes in the corpus are treated as individual tokens in Mongolian language model training. This is a widely used technique in Mongolian language processing. ing set and 40.74% in testing set . The OOV of testing set decreased 42.97%. The reductio n of vocabulary was due to Mongolian spaces which were incorrectly us ed . should have been continuously w rote with the Mong o lian space (correspond to the keyboard: " -" ) in the text. However, some text use the common spaces instead of Mongolian space in front of many case suffixes and possessive suffixes. The vo-cabulary of the corpus in Intermediate character s form is greatly reduced compared to that of the corpus without processing. The reduction ratio of vocabulary and OOV can be seen that many Mongolian words have same presentation forms but different codes. 6.1.2 Evaluation of Language Model Perplexity is a common metric to measure the performance of a language model. The perplexity PPT (T) of the model  X  (  X   X  |  X  w here  X   X  (  X  ) represents the cross entropy for the testing data T in the model  X  (  X   X  |  X  mance that the lower perplexity is, the better performance of the language model is. toolkit [16]. We trained 1 -gram, 2 -gram and 3 -gram language model toward the corpus without processing and the corresponding Mongolian Intermediate charac-and 3 -gram language model was relatively reduced by 52.9%, 41.98% and 41.46% respectively when the training corpus represented in Intermediate character s form. It is clear that the performance of the Mongolian language model has been signifi-cantly improved by our proposed approach . 6.2 Speech Recognition 6.2.1 Data set This experiment takes the Kaldi [17] speech recognition system as the platform using state -of -the -art acoustic models trained on the Mongolian corpus. The dataset contains approximately 78 hours of speech , in which 70 hours of speech ( 62794 sentences ) is used to train the acoustic model training and which 8 hours of speech ( 6987 sentences ) acts as testing set. The pronunciation dictionary consists of 38235 words. experiment 6.1 . In addition, this experiment is performed on the basis of perplexity experiment. 6.2.2 Evaluatio n of Speech Recognition When testing, the acoustic model of speech recognition system is kept unchanged and the language model is changed to calculate, compare the WER in the experi-ment. The evaluation metric is WER defined as follows : where I, D, S are the numbers of numeric insertions, deletions and substitutions, respectively. N is the total number of numeric entities in the corpus.
 gram and 3 -gram language model toward the corpus without processing and the corresponding Mongolian Intermediate characters, respectively. Experimental re-sults are shown in Table 5 . We can see that the WER in testing set with the language model trained o n the dataset Train_IC is significant lower than that with the lan-guage model trained on the dataset Train_Mon . Meanwhile, the WER of DNN+3 -gram model [18] and LSTM+3 -gram model has been respectively reduced by 3 6.7 % , 34.03% using Intermediate characters , great ly improving the performance of Mongolian speech recognition. It also proves that converting the training corpus into Intermediate character s form can make the language model performs better. This paper presents a method that combining different presentation form using In-termediate characters to build Mongolian language model. The experimental results show that this method decrease s the vocabulary by 41% and reduce the perplexity of 3 -gram lang uage model by 41.46% . Meanwhile , the WER for the 3 -gram lan-guage model decrease around 30% when comparing with the language model trained without processing in t he Mongolian speech recognition. This approach not only effectively improves the performance of Mongolian language model, but also greatly enhances the accuracy of Mongolian speech recognition. It is of great sig-nificance to related technological development of Mongolian natural language pro-cessing. order to improve the retrieval and statistics performance of the Mongolian words. Acknowledgements. This research was partially supported by the China National Nature Science Foundation (No.61263037 and No.61563040), Inner Mongolia nature science foundation (No.2014BS0604) and the program of high -level talents of Inner Mongolia University.
 1.
 5.
 6.
 18.

