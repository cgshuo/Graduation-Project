 Search queries have evolved beyond keyword queries. Many complex queries such as verbose queries, natural language question queries and document-based queries are widely used in a variety of applications. Processing these complex queries usually requires a series of query operations, which results in multiple sequences of reformulated queries. However, previ-ous query representations, either the  X  X ag of words X  method or the recently proposed  X  X uery distribution X  method, can-not effectively model these query sequences, since they ig-nore the relationships between two queries. In this paper, a reformulation tree framework is proposed to organize mul-tiple sequences of reformulated queries as a tree structure, where each path of the tree corresponds to a sequence of reformulated queries. Specifically, a two-level reformulation tree is implemented for verbose queries. This tree effectively combines two query operations, i.e., subset selection and query substitution, within the same framework. Further-more, a weight estimation approach is proposed to assign weights to each node of the reformulation tree by considering the relationships with other nodes and directly optimizing retrieval performance. Experiments on TREC collections show that this reformulation tree based representation sig-nificantly outperforms the state-of-the-art techniques. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Experimentation, Performance Reformulation Tree, Verbose Query, Information Retrieval
Although short keyword queries are still very common in web search, the increasing diversity of search applications and information needs has led to increasing complexity in queries. For example, verbose (or long) queries have become more and more popular in web search. In community-based Q&amp;A, users pose natural language questions as queries. In patent retrieval, the whole document (patent application) is considered as the query. These complex queries help users express their information need naturally and save the ef-fort of picking keywords. However, processing these queries poses a significant challenge for search systems.
Dealing with complex queries usually requires a series of query operations. For example, a typical process of deal-ing with a verbose query can be described as follows. The system first selects a subset of query words from the orig-inal query to remove noisy information. Then, the gener-ated subset query is further modified to handle vocabulary mismatch. Finally, weights are assigned to queries gener-ated at each step. Depending on the application, the above process could become more complicated. For example, in cross-lingual retrieval, the original verbose query needs to be translated into a foreign language query before applying any further operation. The above process will generate mul-tiple sequences of reformulated queries, where each sequence records a way of modifying the original query using several query operations. These reformulation sequences capture the relationships between the reformulated queries. Fig. 1 displays some examples of the reformulation sequences, where the subset query is selected from the original query at the first step of the sequence and the second step further substitutes the subset query.

However, previous query representations cannot model these reformulation sequences well. The  X  X ag of words X  rep-resentation is widely used in information retrieval. Using this representation, the original query is transformed into a set of weighted words. Some extensions to this repre-sentation introduce phrases [19] and latent words [14][20]. Figure 1: The reformulation sequences generated for a verbose query Q  X  X ny efforts proposed or un-dertaken by world governments to seek reduction of iraqs foreign debt X  This representation does not explicitly model a reformu-lated query, which serves as the basis of the reformulation sequences. An example of the  X  X ag of words X  representation is shown in Fig. 2 (a). Recently, the  X  X uery distribution X  representation [27] was proposed to transform the original query into a set of reformulated queries. For example, Xue et al [30] represents a verbose query as a set of subset queries. This representation indeed considers a reformulated query as the basic unit, but it fails to capture the relationships between the reformulated queries. Therefore, the sequences of reformulated queries still cannot be modeled using this representation. An example of the  X  X uery distribution X  rep-resentation is shown in Fig. 2 (b).

In this paper, a novel query representation is proposed to transform a complex query into a reformulation tree, where the nodes at each level of this tree correspond to the refor-mulated queries generated using a specific query operation. Using this representation, a reformulation sequence is natu-rally modeled as a path from the root node to the leaf node. The construction of the reformulation tree simulates the pro-cess of applying a series of query operations to the complex query. Furthermore, weight is assigned to each node of the reformulation tree, which indicates the importance of the corresponding reformulated query. The estimation of the weight for a node considers not only the characteristics of this node itself, but also its relationships with other nodes. Different with previous reformulation models that treat re-trieval models as independent steps, we estimate the weights on the reformulation tree by directly optimizing the perfor-mance of retrieval models, which considers the reformulation model and the retrieval model in a joint view.

Verbose queries, as a typical example of complex queries, have attracted much attention recently. Previous research on verbose queries either weights the query words in the original query [16, 15, 3] or selects the best subset of query words from the original query [12]. Relatively little research considers combining multiple query operations together for improving verbose queries. Therefore, as an implementation of the reformulation tree framework, a two-level tree struc-ture is constructed for verbose queries, where the first level corresponds to the subset query selection operation and the second level corresponds to the query substitution operation. A weight estimation method is also described, which in-corporates the relationships between different reformulated queries and directly optimizes the retrieval performance.
Fig. 2 (c) shows an example reformulation tree. The first level of this tree consists of two subset queries extracted from the original query, i.e.,  X  X eductions iraqs foreign debt X  and  X  X raqs foreign debt X . At the second level, each subset query is further modified to generate query substitutions. For exam-ple,  X  X raqs foreign debt X  X as been modified to  X  X raqs external debt X . Furthermore, weight is assigned to each node of this tree, which measures the importance of each reformulated query. Compared with other representations, the reformu-lation sequences as shown in Fig. 1 are captured using the reformulation tree.

The contributions of this paper can be summarized as four folds. First, a tree based query representation is proposed to deal with complex queries, which models a series of query operations and captures the relationships between the re-formulated queries. Second, a specific implementation, i.e., the two-level reformulation tree, is introduced for verbose queries, which combines two important operations, subset {0.09 reduction, 0.09 iraqs, 0.09 foreign, 0.09 debt, ...} {0.55 seek reduction iraqs, 0.23 seek reduction iraqs debt, 0.05 undertaken iraqs debt, 0.03 efforts seek reduction iraqs ... } Figure 2: Different query representations for a ver-bose query  X  X dentify any efforts proposed or under-taken by world governments to seek reduction of iraqs foreign debt X  query selection and query substitution. Third, a weight es-timation method is designed by incorporating the relation-ships between different reformulated queries and directly optimizing retrieval performance. Fourth, detailed experi-ments are conducted to show that the tree-based represen-tation outperforms other query representations for verbose queries.
In this section, we first describe previous work on complex queries, especially on verbose queries and then we review previous query representation approaches.
As described in the introduction, complex queries have been widely used in different applications. Some examples include the verbose query, the natural language question query and the document-based query.

Kumaran and Allan [11] studied shortening a verbose query through human interaction. Bendersky and Croft [2] discov-ered key concepts from a verbose query. These key concepts were combined with the original query to improve the re-trieval performance. Kumaran and Carvalho [12] learned to automatically select subset queries using several query qual-ity predictors. Balasubramanian et al [1] extent [12] for web long queries.

Lease et al [16] developed a regression model to assign weights to each query word in the verbose query by using the secondary features. Lease [15] further combined their regression model with the Sequential Dependence Model, which achieved significant performance improvement. Ben-dersky et al [3] proposed a unified framework to measure the weights of words, phrases and proximity features underlying averbosequery.

A natural language question query is widely used in a community-based Question and Answer service such as Ya-hoo! Answers and Quora. Previous work [8, 9, 24] stud-ied effectively finding previously answered questions that are relevant to a new question asked by a user. Different retrieval models have been proposed to calculate the simi-larity between questions. For example, a translation based retrieval model [8] were developed to deal with the vocabu-lary mismatch between the semantically related questions.
A document-based query allows users to directly submit a document as a query. A typical example is in patent retrieval [13], where the whole patent application is submitted to the search system in order to find the relevant patents. Many features are extracted from a patent application and these features are the basis of retrieving relevant patents.
In this paper, a tree-based representation is proposed to improve the complex query. A specific implementation for verbose queries is described. This implementation combines subset selection and query modification within the same framework, which has not been explored in previous work.
In this section, we review two types of query representa-tions,  X  X ag of words X  and  X  X uery distribution X .

The  X  X ag of words X  representation transforms the origi-nal query into a set of terms, either weighted or not. These terms include the words and phrases from the original query and the latent words and phrases extracted from the cor-pus. For example, the relevance model approach [14] adds latent words to the original query, the sequential dependency model [19] detects the phrase structure, and the latent con-cept expansion model [20] uses proximity features and latent words. This type of representation does not consider how to use words and phrases to form actual reformulated queries. In other words, a reformulated query is not explicitly mod-eled in this representation.

The  X  X uery distribution X  representation transforms the original query into a set of reformulated queries, where each query is assigned a probability. This probability helps mea-sure the importance of the query. For example, Xue et al [30] represented a verbose query as a distribution of sub-set queries and a modified Conditional Random Field is proposed to estimate the probability for each subset query. This type of representation indeed considers the reformu-lated query as the basic unit, but it assumes independence between the reformulated queries. When multiple query op-erations are applied, this independence assumption usually does not hold.

In this paper, the proposed  X  X eformulation tree X  repre-sentation extends the  X  X uery distribution X  representation by modeling the relationships between reformulated queries us-ing the tree structure.

Some previous work also considers the relationships be-tween queries. Boldi et al [4] proposed to build a query-flow graph that modeled web users X  search behaviors. Specifi-cally, the directed edges between two queries indicated that they were likely to belong to the same search mission. Mei et al [17] presented a general framework to model search se-quences, where a search sequence is represented as a nested sequence of search objects. The above work focuses on short keyword queries and uses query logs to capture the rela-tionships between the queries submitted within the same search session. In contrast, in this paper, we study complex queries and model the relationships between the reformu-lated queries. Furthermore, the construction of the refor-mulation tree proposed in this paper is closely related to the final retrieval performance, while previous work studies the query graph or sequence independently from the retrieval model.

Guo et al [6] proposed a CRF-based model for query re-finement, which combines several tasks like spelling correc-tion, stemming and phrase detection. This model focuses on morphological changes of keyword queries such as spelling correction and stemming, but does not consider complex queries.
In this section, we summarize several basic concepts used in this paper.

A complex query ( q ) is more complicated than a short key-word query. Examples of complex queries include verbose queries, natural language question queries and document-based queries. In this paper, we will focus on verbose queries.
A query operation ( r ) indicates a query processing tech-nique. In this paper, we focus on two query operations, i.e. subset query selection and query substitution. Subset query selection [11, 12, 1, 30] selects a subset of query words from the original query. Query substitution [10, 26, 29] replaces the original query word with a new word. Other examples of query operations include query translation, query segmen-tation and so on.

A reformulated query ( q r ) is the output of applying a query operation. A reformulation sequence is a sequence of refor-mulated queries by applying a series of query operations.
A reformulation tree ( T ) is a tree structure that organizes the reformulated queries generated by different query op-erations. Each path of T corresponds to a reformulation sequence.
In this section, we first describe the framework for gener-ating the reformulation tree T . Then, we compare this tree-based representation with previous query representations. Finally, the principle of the weight estimation is described.
Suppose that n query operations { r 1 ,r 2 , ..., r n } are re-quired to process a complex query q . Then, q is transformed into a n -level tree T .Eachnodeof T represents a reformu-lated query q r . From now on, if not explicitly indicated, we use q r to represent both a node of T and the correspond-ing reformulated query. The root node of T represents the original query q , which can be considered as a special refor-mulated query. The i th level of T are generated by applying the i th operation r i to the nodes at the ( i  X  1)th level. An arc is added between the nodes at the ( i  X  1)th level and the nodes at the i th level if the latter is the output of applying r to the former. Therefore, each path of T corresponds to a reformulation sequence. Furthermore, weight w ( q r )isas-signed to each node of T , which measures the importance of the corresponding reformulated query q r .

When T is used for retrieval, the retrieval score of a doc-ument D is calculated using Eq. 1. where w ( q r ) is the weight assigned to the node correspond-ing to the reformulated query q r and sc ( q r ,D )isthere-trieval score of using q r to retrieve D . In general, sc ( T,D ) is calculated by combining the retrieval score of using each reformulated query q r in T ,where w ( q r )isusedasthecom-bination weight. The calculation of sc ( q r ,D ) depends on implementation.
In this subsection, we compare different query representa-tions using the example in the introduction. Fig. 2 displays the  X  X ag of words X  representation, the  X  X uery distribution X  representation and the  X  X eformulation tree X  representation.
In the  X  X ag of words X  representation, the basic unit is words or phrases. This representation may tell you that the important words in the original query are  X  X eduction X ,  X  X raqs X  X nd X  X ebt X , but how these words can be used together to form a new query is not clear.

The  X  X uery distribution X  representation extends the  X  X ag of words X  representation by explicitly modeling a reformu-lated query. For example, this representation lists the top ranked subset queries such as  X  X eek reduction iraqs X  and  X  X eek reduction iraqs debt X . However, the relationships be-tween the reformulated queries are not captured using this representation.

When a series of query operations are applied, we need a representation that models the reformulation sequences generated using these operations. The  X  X eformulation tree X  representation is designed to solve this problem. For exam-ple, in Fig. 2, the subset queries and the query substitutions are organized into a tree structu re. This structure indicates that we need to first select subset queries and then conduct query substitution. It captures the relationships between  X  X raqs foreign debt X  and  X  X raqs external debt X , since the lat-ter is the output of applying the query subsitution operation to the former.
The weight assigned to each node in the tree indicates the importance of the corresponding reformulated query. This weight should characterize both the features of this node itself and its relationships with other nodes. Therefore, the weight of q r , i.e., w ( q r ), is calculated in Eq. 2. where par ( q r ) denotes the parent node of q r . f k is the query feature extracted from q r and  X  k is the parameter. Eq. 2 shows that the weight of q r is not only decided by its own query features { f k } but also by the weight of its parent node par ( q r ). Intuitively, if q r is important, its children should also receive high weights. In this way, the relationships be-tween reformulated queries are incorporated into the weight estimation.

Note that Eq. 2 provides the principle of weight estima-tion. How to calculate w ( q r ) will depend on the implemen-tation.
In this section, we describe a two-level reformulation tree for verbose queries. We first describe the query operations used to construct the reformulation tree, i.e. subset query se-lection and query substitution. Then, we introduce a stage-based weight estimation method to assign weight to each node. Finally, the retrieval model is described.
The construction of the reformulation tree for verbose queries consists of two steps: first, subset queries are se-lected from the original query; second, the subset queries generated in the previous step are further modified to gen-erate query substitutions.

We follow Kumaran and Carvalho [12] X  X  method to gener-ate subset queries. All query words from the original verbose query are considered. If the length of the verbose query is bigger than ten, we first rank all query words by their idf values and then pick the top ten words for the subset query generation. Then, all subset queries with length between three and six words are generated.

The passage analysis technique [29] is used to generate query substitutions. In order to replace one word from the original query, all the passages containing the rest of the query words are first extracted. Then, three meth-ods are used to generate candidates for query substitution from these passages. Morph considers the morphologically similar words as candidates. Pattern considers the words matching the patterns extracted from the original query as candidates. Wiki considers the Wikipedia redirect pairs as the candidates. More details can be found in [29]. Finally, the top ranked candidates are used as query substitutions.
Given the above two query operations, the reformulation tree for the verbose query can be generated in this way. First, all subset queries with length between three to six are extracted from the original query. Each subset query is assigned a weight. How to estimate the weight will be described in the next subsection. According to this weight, we will pick the top ranked subset queries to construct the first level of the reformulation tree. SubNum is a parameter that controls the number of nodes at the first level. Second, among these SubNum subset queries, we further modify the top ModNum queries to generate query substitutions, which constructs the second level of the reformulation tree. Mod-Num is another parameter that controls the number of nodes that will be substituted.

For example, the reformulation tree used in the introduc-tion (Fig. 2) can be constructed in two steps. This process is illustrated in Fig. 3. First, we pick the top two sub-set queries  X  X eductions iraqs foreign debt X  and  X  X raqs foreign debt X  to construct the first level of the reformulation tree. Second, we modify these two subset queries respectively. For the first subset query,  X  X educe iraqs foreign debt X  is gener-ated by replacing  X  X eduction X  with  X  X educe X . For the second subset query, two query substitutions, i.e.  X  X raqs foreign debt X  and  X  X raqs external debt X  are generated.
Eq. 2 indicates that the weight of a node in the refor-mulation tree depends on both its intrinsic features and the weight of its parent node. However, how to estimate the weight is still unclear. In this part, we describe a stage-based weight estimation method. The learning-to-rank based pa-rameter estimation strategy [28] is used as the basis, which transforms a query feature into a corresponding retrieval feature.

In the initial stage, the root node (the original query q )is assigned the weight 1, i.e. w ( q )=1.

In Stage I , after the subset queries q sub are generated, we calculate the weight of q sub using Eq. 3. Figure 3: The process of constructing a reformula-tion tree
Eq. 3 instantiates Eq. 2 by focusing on the subset queries. f k is the query feature extracted from q sub and  X  sub k is the corresponding parameter. Since the root node is the parent of every subset query, its weight w ( q ) = 1, is used in Eq. 3.
In order to estimate {  X  sub k } by directly optimizing the re-trieval performance, we transform each query feature f sub into the corresponding retrieval feature F sub k ,where F calculated in Eq. 4.
 where sc ( q sub ,D ) is the retrieval score of using q sub trieve D . The calculation of sc ( q sub ,D ) depends on the retrieval model. The retrieval feature F sub k combines the retrieval score of each subset query q sub using their corre-sponding query feature f sub k ( q sub ) as the combination weight. In general, F sub k indicates how well documents are ranked if f k is used as the weight to combine subset queries.
Now, we obtain a set of retrieval features { F sub k } .The problem of estimating {  X  sub k } to combine the query features { f corresponding retrieval features { F sub k } to achieve the best retrieval performance. The latter problem is typically solved using learning to rank techniques. In this paper, the ListNet method [5] is adopted to learn {  X  sub k } on the training set.
After obtaining {  X  sub k } , we can assign the weight for each subset query according to Eq. 3.
 In Stage II , we assign weights to the substituted queries. The weight of a substituted query q mod is calculated using Eq. 5. where q sub istheparentnodeof q mod .ComparedwithEq. 3, the weights of the subset queries w ( q sub ) generated in Stage I are incorporated in Eq. 5.

Similarly, in order to estimate {  X  mod k } , we transform f into the corresponding retrieval feature F mod k using Eq. 6. where q sub istheparentnodeof q mod .Ingeneral, F mod k tells how well the documents are ranked if f mod k is used as the weight to combine the substituted queries. Thus, the parameters {  X  mod k } are learned by combining these retrieval features { F mod k } using ListNet.
In this part, we describe the query features used to char-acterize the subset queries and the substituted queries.
The features used to characterize the subset queries are mainly query quality predictors. This type of features have been widely used in previous research [12][30]. Examples of query quality predictors include Mutual Information [11], Query Scope [7] and Query Clarity [25]. In addition, pas-sage information is considered. The number of passages that contain a subset query provides strong evidence for the qual-ity of a subset query. Whether a subset query contains key concepts is also considered as a feature. These key concepts were discovered by Bendersky et al [2].

The features used to characterize the substituted queries include the type of methods of generating substituted queries. As described in Section 5.1,  X  X orph X  indicates using the morphologically similar words as candidates,  X  X attern X  indi-cates using the pattern based method and  X  X iki X  indicates using the Wikipedia redirect page. The passage information is also considered as one feature. Furthermore, the number of possible segmentations of a substituted query is used as another feature. This feature can be directly obtained using the passage analysis technique [29].

The details of features are summarized in Table 1.
The retrieval score of using a reformulation tree T can be calculated using Eq. 1. For example, given the refor-mulation tree shown in Fig. 2, the retrieval score can be calculated as follows: sc ( T,D )=0 . 36  X  sc (Original Query ,D )
In this paper, the sequential dependency model (SDM) [19] is used to calculate sc ( q r ,D ), which has been widely used in previous work [2, 30] as a state-of-the-art technique. Using SDM, the score of a document can be calculated as follows: sc ( q r ,D )=  X  T where T ( q r ) denotes a set of query words of q r , O ( q notes a set of ordered bigrams extracted from q r and U ( q denotes a set of unordered bigrams extracted from q r .  X   X 
O and  X  U are parameters controlling the weights of dif-ferent parts and are usually set as 0.85, 0.1 and 0.05 [19]. P ( t | D ), P ( o | D )and P ( u | D ) are calculated using the lan-guage modeling approach [22, 31].

The SDM model can be easily implemented using the Indri query language [18]. Fig. 4 shows an example of Indri query for SDM model.
Four TREC collections, Gov2, Robust04, ClueWeb (Cat-egory B) and Wt10g are used for experiments. Robust04 is a newswire collection, while the rest are web collections. The statistics of each collection are reported in Table 2. For each collection, two indexes are built, one not stemmed and the other stemmed using the P orter Stemmer[23]. Stemming transforms a word into its root form, which is conducted ei-ther during indexing or during query processing. The latter case treats stemming as a part of query reformulation, which has been shown effective for web search [21]. Both cases are considered in this paper using two types of indexes. No stopword removal is done during indexing. For each topic, the description part is used as the query. A short list of 35 stopwords and some frequent stop patterns (e.g.,  X  X ind information X ) are removed from the description query.
The query set of each collection is split into a training set and a test set. On the training set, the parameters  X  are learned. On the test set, the learned parameters  X  k are used to assign weight to the reformulation tree generated from each test query. Specifically, ten-fold cross validation is used, where the query set is split into ten folds. Each time nine folds are used for training and one fold is used for test. This process repeats ten times.

Several baselines are compared. QL denotes the query likelihood language model [22, 31]. SDM denotes the se-quential dependence model [19]. KC denotes the key con-cept method proposed by Bendersky et al [2]. Note that we do not report KC on ClueWeb, since the key concept query is not provided on ClueWeb in [2]. QL+SubQL and DM+SubQL [30] are the subset query distribution methods, Table 2: TREC collections used in experiments which combine the original query with a distribution of sub-set queries. QL+SubQL uses QL for both the original query and the subset queries, while DM+SubQL uses SDM for the original query and uses QL for the subset queries. In this paper, QL+SubQL and DM+SubQL are trained using the global features mentioned in [30]. SDM, KC, QL+SubQL and DM+SubQL are the state-of-the-art techniques for ver-bose queries. SDM and KC can be classified as the  X  X ag of words X  representation, while QL+SubQL and DM+SubQL can be considered as the X  X uery distribution X  X epresentation. Therefore, the comparisons with these baselines help show the advantages of the  X  X eformulation tree X  representation. The proposed reformulation tree approach is denoted as RTree, which uses SDM as the underlying retrieval model. Two parameters are used during the tree construction, Sub-Num and ModNum ,where SubNum denotes how many sub-set queries are kept in the reformulation tree and ModNum denotes among those kept subset queries how many are fur-ther modified to generate query substitutions. In this paper, SubNum takes all subset queries generated and ModNum is set as 10. The effect of these parameters will be explored in the following part of this paper.

The standard performance measures, mean average preci-sion (MAP), precision at 10 (P10) and the normalized dis-counted cumulative gain at 10 (NDCG10), are used to mea-sure retrieval performance. In order to improve readability, we report 100 times the actual values of these measures. The two-tailed t-test is conducted for significance.

The Lemur 4.10 toolkit is used to build the index and run the query. The ireval package provided in the toolkit is used for evaluation and significance test.
In Table 3, we show some examples of the generated re-formulation trees. As mentioned previously, some stopwords and stop patterns are removed from the original query. Those words are kept to improve readability. Note that they are not used for retrieval and subset query generation.
Table 3 shows that the subset queries and the substituted queries are effectively combined within the same framework. For example, given the original query X  X hat allegations have been made about enrons culpability in the california energy crisis X , the reformulation tree first generates high quality subset queries  X  X nrons culpability california energy crisis X  and  X  X alifornia energy crisis X  and then further modifies  X  X al-ifornia energy crisis X  as two substituted queries  X  X alifornia gas prices X  and  X  X alifornia electricity crisis X . Table 3: Examples of the reformulation tree. The top ranked nodes are displayed. In the original query Q , the stopwords and stop structures are ital-icized.

The first experiment is conducted to compare the retrieval performance of the proposed RTree method with the base-lines. The baseline methods include QL, SDM, KC, QL+Sub QL and DM+SubQL. The results are shown in Table 4. The best performance is bolded.

Table 4 shows that RTree outperforms all the baseline methods. Specifically, RTree performs better than the  X  X ag of word X  representations, SDM and KC. Using the non-stemmed index, RTree significantly improves SDM and KC on all four collections with respect to all three performance measures. For example, on ClueWeb, RTree improves SDM by 12.2% and 20.0% with respect to MAP and NDCG10, respectively. On Wt10g, RTree improves KC by 11.4% and 11.0% with respect to MAP and NDCG10, respectively. On the Porter-stemmed index, similar results are also observed. These results show that the  X  X eformulation tree X  represen-tation is more effective than the  X  X ag of words X  representa-tion on modeling verbose queries, since the former explicitly models the reformulated query, while the latter only consid-ers words and phrases.

Furthermore, RTree also outperforms the  X  X uery distri-bution X  representations, QL+SubQL and DM+SubQL. Us-ing the non-stemmed index, RTree outperforms QL+SubQL and DM+SubQL on all four collections with respect to all three measures. Most of the improvements are significant. For example, on ClueWeb, RTree improves QL+SubQL by 17.5% and 27.5% with respect to MAP and NDCG10, re-spectively. Also, RTree improves DM+SubQL by 12.1% and 21.5% with respect to MAP and NDCG10, respectively. The results using the Porter-stemmed index are similar. These observations indicate that the X  X eformulation tree X  represen-tation is better than the X  X uery distribution X  X epresentation, Figure 5: Analysis of relative increases/decreases of MAP over QL. since the former effectively combines different reformulation operations within the same framework.

It is not surprising that RTree brings more improvements over the baselines using the non-stemmed index than using the Porter-stemmed index, since some effect of query substi-tutions, especially those generated using the morphologically similar words, is already provided by the Porter stemmer.
Table 4 shows that RTree significantly outperforms the baseline methods. In this part, we make detailed compar-isons between RTree and the baseline approaches.

First, we compare RTree with SDM and KC. Specifically, we analyze the number of queries each approach increases or decreases over QL. Fig 5 shows the histograms of SDM, KC and RTree based on the relative increases/decreases of MAP over QL. The non-stemmed index is used in Fig. 5. Similar results are observed using the Porter-stemmed index. Table 5: Comparisons with QL+SubQL and DM+SubQL.  X + X ,  X = X  and  X - X  denote that RTree performs better, equal or worse than QL+SubQL and DM+SubQL with respect to MAP.

RTree vs. QL+SubQL vs. DM+SubQL Gov2 71.81% 0.67% 27.52% 63.09% 0.67% 36.24% Robust04 68.27% 0.00% 31.73% 63.05% 0.00% 36.95% Wt10g 62.89% 2.06% 35.05% 62.89% 1.03% 36.08% ClueWeb 65.31% 2.04% 32.65% 70.41% 2.04% 27.55% Table 6: The effect of subset query selection and query substitution with respect to MAP MAP Gov2 Robust04 Wt10g ClueWeb SDM 23.98 23.30 16.76 11.53 KC 24.88 23.87 17.45 n/a QL+SubQL 23.36 22.85 16.81 11.01 DM+SubQL 24.82 23.65 18.25 11.54 RTree-Subset 25.80 24.76 18.11 11.73 RTree 26.70 25.07 19.44 12.94
Fig. 5 shows that RTree improves more queries than SDM and KC. For example, on Gov2, RTree improves 110 queries out of the total 150 queries, while SDM and KC improve 89 and 91, respectively. On Robust04, RTree improves 174 queries out of the total 250 queries, while SDM and KC improve 129 and 153 queries, respectively. At the same time, RTree also hurts less queries than SDM and KC. These observations indicate that RTree is more robust than both SDM and KC.
 Furthermore, we compare RTree with QL+SubQL and DM+SubQL. QL+SubQL and DM+SubQL only consider subset query selection, while RTree combines both subset query selection and query substitution. The comparisons be-tween them indicate whether RTree effectively combines two query operations to improve verbose queries. Specifically, we analyze the percent of queries where RTree performs better than QL+SubQL and DM+SubQL, respectively. The re-sults using the non-stemmed index are reported in Table 5. Table 5 shows that RTree consistently outperforms QL+ SubQL and DM+SubQL for 60%-70% queries on all four collections. These results indicate that RTree provides an effective way to combine different query operations, which significantly improves the retrieval performance of verbose queries.
RTree combines subset query selection and query substi-tution together using a two-level reformulation tree. Previ-ous experiments have demonstrated the general effect of this approach. In this part, we split the effect of subset query selection and query substitution. Specifically, we propose a one-level reformulation tree, which only consists of sub-set queries. This one-level reformulation tree is denoted as RTree-Subset. The comparisons between RTree-Subset and other approaches using the non-stemmed index are shown in Table 6.

In Table 6, RTree-Subset outperforms the baseline meth-ods, which indicates the effect of subset queries in the re-formulation tree. When query substitutions are introduced, RTree further improves RTree-Subset. Thus, both subset se-lection and query substitution account for the performance Figure 6: The effect of the parameter SubNum .x-axis denotes SubNum and y-axis denotes MAP. of RTree. RTree-Subset also performs better than other subset query selection methods such as QL+SubQL and DM+SubQL.
As described in Section 5.1, there are two parameters used during the process of constructing the reformulation tree, SubNum and ModNum . SubNum denotes the number of subset queries used in the reformulation tree and ModNum denotes the number of subset queries that are modified to generate query substitutions. In this subsection, we explore the effect of these two parameters. The Porter-stemmed index is used. Fig. 6 shows the effect of the parameter SubNum ,where SubNum takes the values 5, 10, 20, 30 and  X  X ll X , where ModNum is fixed as 10. Here,  X  X ll X  indicates using all subset queries generated.

Fig. 6 shows that the best number of subset queries used in the reformulation tree is inconsistent. On Gov2, the performance becomes stable after using the top 20 subset queries. On Robust04 and Wt10g, the performance keeps increasing when more subset queries are considered. On ClueWeb, the performance drops when more than the top 20 queries are used. One possible explanation for these ob-servations is provided. Robust04 and Wt10g are relatively Table 7: The effect of the parameter ModNum with respect to MAP small collections, thus using more subset queries is likely to retrieve more relevant documents. However, when the size of the collection becomes very large such as Gov2, using more subset queries does not help much retrieval all relevant doc-uments. If the collection is not only big but also contains much noise such as ClueWeb, using more subset queries even hurts the performance.

Table 7 displays the retrieval performance when ModNum takes three different values, i.e. 3, 5 and 10, where SubNum is set as 10.

Table 7 shows that there is not much performance change when ModNum is bigger than 3, which indicates that mod-ifying the top three subset queries is enough to achieve most of the performance of RTree.
We now discuss the efficiency of using the reformulation tree model for retrieval. The online cost of this model comes from three aspects, i.e. reformulated query generation, query feature extraction, and retrieval.

The efficiency of the reformulated query generation de-pends on the reformulation operations involved. For ex-ample, generating the subset queries is very efficient. In contrast, generating query substitutions using the passage analysis is more time consuming, since it needs to analyze a lot of passages.

The efficiency of the query feature extraction also depends on the query features used. Some query features are expen-sive such as query clarity, while some features are relatively cheap such as the frequency in query logs.

Both of these steps can be optimized if large scale query logs are available. We can limit the reformulated queries to those appearing in query logs. In this way, instead of generating queries, we simply search the query logs, which can be efficiently implemented using the index. Also, all query features can be precomputed, which speeds up the query feature extraction.

In terms of the efficiency of retrieval, Eq. 1 shows that the retrieval score of each reformulated query ( sc ( q r is required. At first glance, this appears to be inefficient, since we need to run multiple queries. However, this can be easily optimized. Eq. 7 shows that sc ( q r ,D ) consists of the scores of the words and bigrams in q r .Sinceallthe reformulated queries in the reformulation tree are generated from the same original query, they share many words and bigrams. Thus, the scores of these words and bigrams can be reused by different reformulated queries. For example, the words and bigrams in the subset queries all come from the original query. Thus, we only need to calculate the scores for every word and bigram in the original query and then reuse these scores for each subset query. Further, although query substitutions may introduce new words and bigrams, the number of these new words and bigrams is limited. For example, Table 7 shows that query substitutions generated from the top three subset queries are sufficient to achieve good retrieval performance. Complex queries pose a new challenge to search systems. In order to combine different query operations and model the relationships between the reformulated queries, a new query representation is proposed in this paper, where the original query is transformed into a reformulation tree. A specific implementation is described for verbose queries, which com-bines subset query selection and query substitution within a principled framework. In the future, this query representa-tion will be applied to other search tasks involving complex queries such as the cross-lingual retrieval and diversifying the search results.
 This work was supported in part by the Center for Intel-ligent Information Retrieval and in part by ARRA NSF IIS-9014442. Any opinion s, findings and conc lusions or rec-ommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor. [1] N. Balasubramanian, G. Kumaran, and V. Carvalho. [2] M.BenderskyandW.B.Croft.Discoveringkey [3] M. Bendersky, D. Metzler, and W. B. Croft. Learning [4] P. Boldi, F. Bonchi, C. Castillo, D. Donato, A. Gionis, [5] Z.Cao,T.Qin,T.-Y.Liu,M.-F.Tsai,andH.Li.
 [6] J. Guo, G. Xu, H. Li, and X. Cheng. A unified and [7] B. He and I. Ounis. Inferring query performance using [8] J. Jeon, W. B. Croft, and J. H. Lee. Finding similar [9] V. Jijkoun and M. de Rijke. Retrieving answers from [10] R. Jones, B. Rey, O. Madani, and W. Greiner. [11] G. Kumaran and J. Allan. A case for shorter queries, [12] G. Kumaran and V. R. Carvalho. Reducing long [13] L. Larkey. A patent search and classification system. [14] V. Lavrenko and W. B. Croft. Relevance based [15] M. Lease. An improved Markov random field model [16] M. Lease, J. Allan, and W. B. Croft. Regression rank: [17] Q. Mei, K. Klinkner, R. Kumar, and A. Tomkins. An [18] D. Metzler and W. B. Croft. Combining the language [19] D. Metzler and W. B. Croft. A Markov random field [20] D. Metzler and W. B. Croft. Latent concept expansion [21] F.Peng,N.Ahmed,X.Li,andY.Lu.Context [22] J. M. Ponte and W. B. Croft. A language modeling [23] M. F. Porter. An algorithm for suffix stripping. [24] S. Riezler, A. Vasserman, I. Tsochantaridis, V. Mittal, [25] Y. Z. S. Cronen-Townsend and W. B. Croft.
 [26] X. Wang and C. Zhai. Mining term association [27] X. Xue and W. B. Croft. Representing queries as [28] X. Xue and W. B. Croft. Modeling subset [29] X. Xue, W. B. Croft, and D. A. Smith. Modeling [30] X. Xue, S. Huston, and W. B. Croft. Improving [31] C. Zhai and J. Lafferty. A study of smoothing
