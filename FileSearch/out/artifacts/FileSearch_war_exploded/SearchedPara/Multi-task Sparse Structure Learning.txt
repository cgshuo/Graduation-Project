 Multi-task learning (MTL) aims to improve generalization performance by learning multiple related tasks simultane-ously. While sometimes the underlying task relationship structure is known, often the structure needs to be esti-mated from data at hand. In this paper, we present a novel family of models for MTL, applicable to regression and clas-sification problems, capable of learning the structure of task relationships. In particular, we consider a joint estimation problem of the task relationship structure and the individual task parameters, which is solved using alternating minimiza-tion. The task relationship structure learning component builds on recent advances in structure learning of Gaussian graphical models based on sparse estimators of the precision (inverse covariance) matrix. We illustrate the effectiveness of the proposed model on a variety of synthetic and bench-mark datasets for regression and classification. We also con-sider the problem of combining climate model outputs for better projections of future climate, with focus on temper-ature in South America, and show that the proposed model outperforms several existing methods for the problem. I.5.1 [ Pattern Recognition ]: Models X  Structural ; J.2 [ Computer Applications ]: Physical Sciences and Engineering X  Earth and atmospheric sciences Algorithms
The fundamental assertion of multi-task learning (MTL) is that learning over multiple related tasks can outperform learning each task in isolation. The past few years have seen an increase in activity in this area where new methods and applications have been proposed. From the methods perspective, there have been contributions to novel formula-tions for describing task structure and incorporating them in the learning framework [15, 22, 23, 24, 31]. Meanwhile MTL has been applied to problems ranging from object detection in computer vision to web image and video search [37] to multiple microarray data set integration in computational biology [24, 40], to name a few.

Much of the existing work in MTL assumes the tasks rela-tionship structure to be known (see Section 2). However, in many problems, there is only a high level understanding of the task relationships, and hence the structure of the tasks relationship needs to be learned from the data.

Considerable recent advances have been made in the area of structure learning in probabilistic graphical models, where one estimates the (conditional) dependence structure be-tween random variables in a high-dimensional distribution [25, 28, 5, 21, 36]. In particular, assuming sparsity in the condi-tional dependence structure, i.e., each variable is dependent only on a few others, there are estimators based on con-vex (sparse) optimization which are guaranteed to recover the correct dependency structure with high probability, even when the number of samples is small compared to the num-ber of variables.

In this paper, we present a family of models for MTL, col-lectively referred to as Multi-task Sparse Structure Learning (MSSL), capable of learning the structure of task relation-ships as well as parameters for individual tasks. For MSSL, we consider a joint estimation problem of the task relation-ship structure and the individual task parameters, which is solved using alternating minimization. The task relationship structure learning component builds on recent advances in structure learning of Gaussian graphical models based on sparse estimators of the precision (inverse covariance) ma-trix [28, 5, 21, 36].

MSSL has important practical implications: given a set of tasks, one can just feed the data from all the tasks without any knowledge or guidance on tasks relationship structure, and MSSL will figure out which tasks are related and also es-timate task specific parameters. The structure is learned by considering a multivariate Gaussian prior with sparse preci-sion (inverse covariance) matrix, so that the structure learn-ing problem in the multi-task learning setting reduces to the structure learning problem for a Gaussian graphical model. We consider two different ways of imposing the sparse preci-sion structure X  X irst, across the task specific parameters for regression or classification, and second, across the task spe-cific residual errors for regression. The corresponding esti-mation/optimization problems are solved using suitable first order methods, including proximal updates and alternating direction method of multipliers.

MSSL can be applied to regression and classification prob-lems, and we perform experiments on both classes of prob-lems. Through experiments on a wide variety of datasets, we illustrate that MSSL is competitive with and usually out-performs several baselines from the existing MTL literature. Furthermore, the task relationships learned by MSSL are ac-curate and consistent with domain knowledge on the prob-lem. We also consider the problem of combining climate model outputs for better projections of future climate, with focus on temperature in South America. With tempera-ture forecast in each location as a task, MSSL estimates a meaningful dependency structure, e.g., connecting places around the Amazon, but not connecting places across the Andes, etc., without any knowledge of the spatial locations of the tasks, and outperforms several existing methods for the problem.

The rest of the paper is structured as follows. Section 2 briefly discusses related work in multi-task learning. Sec-tion 3 gives a gentle introduction to the proposed multi-task sparse structure learning (MSSL) approach. Section 4 discusses a specific form of MSSL where the task depen-dency structure is learned based on the task parameters, which we refer to as p -MSSL. Section 5 discusses another specific form of MSSL where the task dependency structure is learned based on the task residual errors, which we re-fer to as r -MSSL. Section 6 discusses experimental results on regression and classification using synthetic, benchmark, and climate datasets. We conclude in Section 7.
One of the key questions in MTL is to identify which tasks are related to each other, so that it can incorporate this knowledge when learning all tasks simultaneously. Based on the relationship assumed between the tasks we can broadly classify MTL methods into three classes: (1) all tasks are related, (2) tasks are organized in clusters, and (3) tasks are related in a tree/graph structure.

The first class of MTL methods assume that all tasks are related and information about a task is shared with all other tasks. [15] assumes all task parameters are close to each other. [31] constrains all tasks to share a common set of features. [22, 23] make sparsity and low-rank assumptions about the structure of the parameter matrix.

In clustered multi-task learning, the hypothesis is that models of tasks from the same group are closer to each other compared to tasks from a different group. In [4] task clus-tering is enforced by considering a mixture of Gaussians as a prior over task coefficients. [14] proposes a task clustering regularization to encode cluster information in the MTL for-mulation. [41] employs a non-parametric prior distribution over task coefficients to encourage task clustering.
For graph structured MTLs, two tasks are related if they are connected in a graph, i.e. the connected tasks are sim-ilar. The similarity of two related tasks can be represented by the weight of the connecting edge [24, 45].

Our focus in this work is on problems which model task re-latedness as an undirected graph. We learn such a structure assuming it is not available a priori. The model proposed in [44] is closest to our work. In [44] a matrix-variate normal distribution is used as a prior for the matrix representing the model parameters for all tasks, and then such prior distribu-tion hyperparameter captures the covariance matrix among all task coefficients. While their model learns the covariance matrix, we directly learn the (sparse) precision matrix (in-verse of covariance matrix) among tasks parameters, which has a meaningful interpretation in terms of conditional inde-pendence and partial correlation. To the best of our knowl-edge, this is the first MTL approach which measures the relationship among tasks by using partial correlation and also learns it from the data. In this section we describe the basic idea of our framework. We start by giving a small description of the notations used throughout the paper, followed by a short introduction to the structure estimation problem.
Let K be the number of tasks, d be the number of covari-ates which are shared across all tasks, and n k be the number of samples for each task. Also X k  X  R n k  X  d be the covariates matrix (input data) for the k -th task, y k  X  R n k  X  1 be the response for the k -th task, and W  X  R d  X  K be the parameter matrix, where columns are parameters w k for each task and  X w j denotes the j -th row of W .

For regression, we consider a linear model y k = X k w k + e k for each task where e k is the residual error for the k -th task. When we consider task relatedness based on the residual errors e k , we assume that all tasks have the same number of training samples n and we define the matrix E = ( e 1 , e 2 ,..., e K )  X  R n  X  K . We denote the j -th row of E as  X e . Also  X   X  R K  X  K is a matrix that captures the task relationship structure.

MTL can benefit from knowing the underlying structure relating the tasks while learning parameters for each task. In situations where some of the tasks may be highly depen-dent on each other, the strategy of isolating each task does not exploit the potential information one may acquire from other related tasks. We first describe how we capture the task relatedness and then we introduce our multi-task sparse structure learning (MSSL) framework, which is data driven and learns the structure of task relatedness while learning the parameters for each task.
Let r = ( r 1 ,...,r d ) be a d -variate random vector with joint distribution P . Any such distribution can be char-acterized by an undirected graph G = ( V,E ), where the vertex set V represents the d covariates of r and edge set E represents the conditional dependence relations between the covariates of r . If r i is conditionally independent of r the other variables, then the edge ( i,j ) is not in E . Assum-ing r  X  N ( 0 ,  X  ), the missing edges correspond to zeros in the inverse covariance matrix or precision matrix given by  X   X  1 =  X  , i.e., (  X   X  1 ) ij = 0  X  ( i,j ) /  X  E .
Classical estimation approaches [12] work well when d is small. Given n i.i.d. samples r 1 ,..., r n from the distribu-tion, the empirical covariance matrix is  X   X  = 1 n P n i =1  X r )
T ( r i  X   X r ), where  X r = 1 is rank-deficient and its inverse cannot be used to estimate the precision matrix  X  . However, for a sparse graph where most of the entries in the precision matrix are zero, several methods exist that can estimate  X  [28, 16].
For ease of exposition, consider a simple linear model, y k = X k w k + e k , for each task. MSSL will learn both the task parameters w k for all tasks and the structure which will be estimated based on some information from each task. Further, the structure is used as inductive bias in the model parameter learning process so that it may improve tasks generalization capability. So, the learning of one task is biased by other related tasks [10].

We investigate and formalize two ways of learning the re-lationship structure, represented by  X  : (a) modeling  X  from the task specific parameters w k ,  X  k = 1 ,..,K , and (b) mod-eling  X  from the residual errors e k ,  X  k = 1 ,..,K . Based on how we model  X  , we propose p -MSSL (from tasks parame-ters) and r -MSSL (from residual error).

At a high level, the estimation problem in MSSL is of the form: min where L (  X  ) denotes a suitable task specific loss function, B (  X  ) is the inductive bias term, and R 1 (  X  ) , R 2 (  X  ) denote suitable sparsity inducing regularization terms. The interaction be-tween parameters w k and the relationship matrix  X  is cap-tured by the B (  X  ) term. When  X  k,k 0 = 0, the parameters w k and w k 0 have no influence on each other.
If the tasks are unrelated, one can learn the columns w k of the parameter matrix W independently for each of the K tasks. However, if there exist relationships between the K tasks, learning the columns of W independently fails to take advantage of these dependencies. In such a scenario, we propose to utilize the precision matrix  X   X  R K  X  K among the tasks in order to capture pairwise partial correlations.
In the p -MSSL model we assume that each row  X  w j of the matrix W follows a multivariate Gaussian distribution with zero mean and covariance matrix  X  , i.e.  X  w j  X  X  ( 0 ,  X  )  X  j = 1 ,...,d , where  X   X  1 =  X  . The problem then is to estimate both the parameters w 1 ,..., w K and the precision matrix  X  . By imposing such a prior over the rows of W , we are capable of explicitly estimating the dependency structure between the tasks via the precision matrix  X  .

Having a multivariate Gaussian prior over the rows of W , its posterior can be written as where the first term in the right hand side denotes the con-ditional distribution of the response given the input and pa-rameters, and the second term denotes the prior over rows of W . In this paper, we consider the penalized maximiza-tion of (2), assuming that the parameter matrix W and the precision matrix  X  are sparse. We provide two specific in-stantiations of this model. First, we consider a Gaussian conditional distribution, wherein we obtain the well known least squares regression problem (Section 4.1). Second, for discrete labeled data, choosing a Bernoulli conditional dis-tribution leads to a logistic regression problem (Section 4.2). Assume that where we assume  X  = 1 and consider penalized maximiza-tion of (2). We can write this optimization problem as mini-mization of the negative logarithm of (2), which corresponds to a linear regression problem [19]
Further, assuming  X  and W are sparse, we can add ` 1 -norm regularizers over both parameters and obtain a regu-larized regression problem [19] where  X , X  &gt; 0 are penalty parameters.

In this formulation, the term involving the trace outer product Tr( W X W T ) affects the rows of W , such that if  X  ij 6 = 0, then the columns w i and w j are constrained to be similar. We also observe that for a fixed  X  , since we have the inductive bias term, the regularization over W is more like the elastic-net penalty, since it has both ` 1 and ` ularization on W . Such elastic-net type penalties have the added advantage of picking up correlated relevant covariates (unlike Lasso), but have the downside that they are hard to interpret statistically. The objective function is neverthe-less well defined from an optimization perspective. Also we note that modeling the relationship as a multivariate Gaus-sian distribution is not necessarily a restrictive assumption. Recently there has been work in developing the Gaussian copula family of models [26] which addresses a large class of problems commonly encountered. Our framework can be extended to such models.

Next, we illustrate methods to solve the optimization prob-lem (5) efficiently. We propose an iterative optimization al-gorithm which alternates between updating W (with fixed  X  ) and  X  (with fixed W ). This results in alternating be-tween solving two non-smooth convex optimization problems in each iteration.
 Alternating Optimization: The alternating minimiza-tion algorithm proceeds as follows. 1. Initialize  X  0 = I K and W 0 = 0 d  X  K 2. For t = 1 , 2 ,... do
Note that this alternating minimization procedure is guar-anteed to converge to a minima, since the original prob-lem (5) is convex in each argument  X  and W [18].
 Update for W: The update step (6) is a general case of the formulation proposed by [33] in the context of climate model combination, where in our proposal  X  is any positive definite precision matrix, rather than a fixed Laplacian matrix.
Using vec() notation we can re-write the optimization problem in (6) as min where  X  is the Kronecker product, I d is a d  X  d identity ma-trix, and P is a permutation matrix that converts the column stacked arrangement of W to a row stacked arrangement.  X  is a block diagonal matrix where the main diagonal blocks are the task data matrices X k ,  X  k = { 1 ,..,K } and the off-diagonal blocks are zero matrices. Therefore the problem is a ` 1 penalized quadratic optimization program, which we solve using established proximal gradient descent methods such as FISTA [6].

We note here that the optimization of (6) can be scaled up considerably by using an ADMM [8] based algorithm, which decouples the non-smooth ` 1 term from the smooth convex terms in the objective of (6). Alternating method of multipliers (ADMM) [8] is a strategy that is intended to blend the benefits of dual decomposition and augmented La-grangian methods for constrained optimization. It takes the form of a decomposition-coordination procedure, in which the solutions to small local problems are coordinated to find a solution to a large global problem [8].
 Update for  X : The update step for  X  , given in (7), is known as sparse inverse covariance selection problem [5] which can be efficiently solved using ADMM [8]. We refer interested readers to [8] Section 6.5 for details on derivation of the updates. Given the matrix W ( t +1) , (7) can be solved to obtain  X  ( t +1) by iterating the following ADMM steps. 2. For l = 1 , 2 ,... do 3. Output  X  ( t +1) =  X  l , where l is the number of steps
Each ADMM step can be solved efficiently. For the  X  -update, we can observe from the first order optimality con-dition of 9 and the implicit constraint  X  0 that where S = W ( t +1) T W ( t +1) . Next we take the eigenvalue decomposition of  X  ( Z ( l )  X  U ( l ) )  X  S = Q X Q T , where  X  = diag(  X  1 ,  X  X  X  , X  K ) and Q T Q = QQ T = I . We now mul-tiply 12 by Q T on the left and by Q on the right to get  X  e
 X   X  e  X   X  1 =  X , where e  X  = Q T  X Q . Now  X  = Q e  X Q T satisfies 12.
 The Z -update 10 can be computed in closed form: where S  X / X  (  X  ) is an element-wise soft-thresholding opera-tor [8].
As described previously our model can also be applied to classification. Let us assume that where h (  X  ) is the sigmoid function, and Be( p ) is a Bernoulli distribution. Therefore, following the same construction as in Section 4.1, learning the parameters W , and  X  can be achieved by optimization of the following objective. min
The loss function in Eq. (16) is the logistic loss, where we have considered a 2-class classification setting. Note that the objective function is similar to the one obtained for multi-task learning with linear regression in Section 4.1. There-fore, we use the same alternating minimization algorithm described in Section 4.1 to optimize (16).

In general, we can consider any Generalized Linear Model (GLM) [30], with different link functions h (  X  ), and there-fore different probability density functions such as Poisson, Gamma etc. for the conditional distribution (15). For any such model, our framework requires optimization of an ob-jective of the form where L (  X  ) is a convex loss function obtained from a GLM.
In r -MSSL we assume that the rows of the residual error matrix E ,  X e j  X  X  (0 ,  X  ) ,  X  j = 1 ,...,n , where  X   X  1  X  is the precision matrix among the tasks. In contrast to p -MSSL the relationship among tasks is modeled in terms of partial correlations among the errors e 1 ,..., e K instead of considering explicit dependencies between the parameters w ,..., w K for the different tasks.

Finding the dependency structure among the tasks now amounts to estimating the precision matrix  X  . Such models are commonly used in spatial statistics [27] in order to cap-ture spatial autocorrelation between geographical locations. For example, in domains such as climate or remote sensing, there often exist noise autocorrelations over the spatial do-main under consideration. Incorporating this dependency through the precision matrix of the residual errors is then more interpretable than explicitly modeling the dependency among the parameters W .

We assume that the parameter matrix W is fixed, but unknown. Since the rows of E follow a Gaussian distribu-tion, maximizing the likelihood of the data, penalized with a sparse regularizer over  X  , reduces to the following opti-mization problem min
We use the alternating minimization scheme illustrated in previous sections to optimize the above objective. Note that since the objective is convex in each of its arguments W and  X  , and thus is guaranteed to reach a local minima through alternating minimization [18]. Further, the model can be extended to losses other than the squared loss which we obtain by assuming that the columns of E are i.i.d Gaussian.
In the experiments section r -MSSL was used for regres-sion problems, while for classification we used p -MSSL. In fact, p -MSSL can be applied to both regression and classi-fication problems. r -MSSL, instead, can only be applied to regression problems, as the residual error of a classification problem is clearly not Gaussian.
In this section we provide experimental results to show the effectiveness of the proposed framework for both regression and classification problems.
For the regression problems we start with experiments on synthetic data and then move to the problem of predicting air surface temperature in South America.

To select the penalty parameter  X  we used a stability se-lection procedure described in [29]. It is a subsampling ap-proach which provides a way to find stable structures and hence a principle to choose a proper amount of regulariza-tion for structure estimation.
We created a synthetic dataset with 13 tasks. Each task has 100 data instances with 30 covariates so that we have a data matrix in R 100  X  30 . The training set for each task consists of 60 randomly chosen rows from the data matrix so that we have X  X  R 60  X  30 . The parameter vector for each task w k are chosen so that tasks 1-4 are related to each Figure 1: RMSE error per task comparison between p -MSSL and Ordinary Least Square over 30 inde-pendent runs. p -MSSL gives better performance on related tasks (1-4 and 5-10) but no significant im-provement on unrelated tasks (11-13). other but unrelated to the remaining tasks. Similarly tasks 5-10 form a cluster. Tasks 11, 12, and 13 are not related to any of the other tasks. In this way we have two clusters of tasks and also three outlier tasks. The prediction variables are generated as y k = Xw k +  X  where  X   X  N (0 ,I 100 ). We train the p -MSSL model on this dataset and test it on the remaining 40 data instances.

Figure 1 is a box-plot of RMSE error for p -MSSL and for the case where Ordinary Least Squares (OLS) was applied individually for each task. As expected, sharing information between tasks leads to better prediction accuracy. p -MSSL does well on related tasks 1-10 but there is no significant improvement on tasks 11-13. Assuming a dense precision matrix or a precision matrix where tasks 11-13 are related to the other tasks leads to higher RMSE values.

Figures 2(a) and 2(b) depict the precision matrix esti-mated by p -MSSL algorithm and its graph representation. The missing edges in the graph correspond to zeros in the precision matrix, and it means that those pair of tasks are conditionally independent of each other given the other tasks. As can be seen, our model is able to recover the true de-Figure 2: Structure estimated by the p -MSSL algo-rithm on the synthetic dataset: (a) precision matrix; (b) graph representation of the task dependencies encoded by the precision matrix. pendency structure among tasks. The block structure is reflected in the estimated precision matrix.

Sensitivity analysis of p -MSSL sparsity parameters  X  (con-trols sparsity on  X  ) and  X  (controls sparsity on W ) on the synthetic data is presented in Figure 3. As we increase  X  we encourage sparsity on W and having more zeros on W it becomes harder for p -MSSL to capture the true relationship among the column vectors (tasks parameters), since it learns  X  based on W . As a consequence of non-accurate structure estimation, p -MSSL performance decreases as we can see in the figure. We also can see that for small  X  values p -MSSL performance decreases, since we have a dense precision ma-trix which is different from the ground truth (sparse matrix). In other words, learning an accurate precision matrix leads to an improvement of p -MSSL performance. Figure 3: Average RMSE error on the test set for all tasks varying parameters  X  (controls sparsity on  X ) and  X  (controls sparsity on W).
A Global Climate Model (GCM) is a complex mathemat-ical representation of the major climate system components (atmosphere, land surface, ocean, and sea ice), and their interactions. They are run as computer simulations, to pre-dict climate variables such as temperature, pressure, pre-cipitation etc. over multiple centuries. Several GCMs have been proposed by climate science institutes from different countries around the world. The forecasts of future climate variables as predicted by these models have high variability which in turn introduces uncertainty in analysis based on these predictions [1]. The main reason for such uncertainty in the response of the GCMs is due to model variability and can be greatly reduced by suitably combining outputs from multiple GCMs [1].

In this analysis we consider the problem of GCM out-puts combination for land surface temperature prediction in South America. Being the world X  X  fourth-largest continent, covering approximately 12% of the Earth X  X  land area, the climate of South America varies considerably. The Amazon river basin in the north has the typical hot wet climate suit-able for the growth of rain forests. The Andes Mountains, on the other hand, remain cold throughout the year. The desert regions of Chile is the driest part of South America.
We use 10 GCMs from the CMIP5 dataset [35]. The de-tails about the origin and location of the datasets are listed in Table 1.

The global observation data for surface temperature is obtained from the Climate Research Unit (CRU) 1 . We align the data from the GCMs and CRU observations to have the same spatial and temporal resolution, using publicly avail-able climate data operators (CDO) 2 . For all the experiments, we used a 2 . 5 o  X  2 . 5 o grid over latitudes and longitudes in South America, and monthly mean temperature data for 100 years, 1901-2000, with records starting from January 16, 1901. In total, we consider 1200 time points (monthly data) and 250 spatial locations over land 3 . For the MTL framework, each geographical location forms a task (regres-sion problem).
 Baselines and Evaluation: We consider the following 4 baselines for comparison and evaluation of MSSL perfor-mance. We will refer to these baselines and MSSL as the  X  X odels X  in the sequel and the constituent GCMs as  X  X ub-models X . The four baselines are: (1) Average Model is the current technique used by Intergovernmental Panel on Climate Change (IPCC) 4 , which gives equal weight to all GCMs at every location; (2) Best GCM which uses the predicted outputs of the best GCM in the training phase (lowest RMSE), this baseline is not a combination of mod-els, but a single GCM instead; (3) Linear Regression is an Ordinary Least Squares (OLS) regression for each geo-graphic location; and (4) Multi Model Regression with Spatial Smoothing (S 2 M 2 R) is the model recently pro-posed by [33], which is a special case of MSSL with pre-defined dependence matrix  X  equal to the Laplacian matrix. This model is referred to as S 2 M 2 R. It incorporates spatial smoothing using the graph Laplacian over a grid graph.
For our experiments, we considered a moving window of 50 years of data for training and the next 10 years for test-ing. This was done over 100 years of data, resuting in 5 train/test sets. Therefore, the results are reported as the average RMSE over these test sets.

Table 2 reports the average and standard deviation RMSE for all 250 geographical locations. While average model has the highest RMSE, r -MSSL has the smallest RMSE in http://www.cru.uea.ac.uk https://code.zmaw.de/projects/cdo
Dataset is available at first author X  X  home page. http://www.ipcc.ch comparison to the baselines. The performance of OLS and S M 2 R are very similar.
 Table 2: Mean and standard deviation of RMSE over all locations for r -MSSL and the baseline algo-rithms. r -MSSL performs best in predicting tem-perature over South America.

Figure 4 shows the precision matrix estimated by r -MSSL algorithm and the Laplacian matrix assumed by S 2 M Not only is the precision matrix for r -MSSL, able to cap-ture the relationship between a geographical locations X  im-mediate neighbors (as in a grid graph) but it also recov-ers relationships between locations that are not immediate neighbors.
 Figure 4: The Laplacian matrix (on grid graph) as-sumed by S 2 M 2 R (left) and the Precision matrix learned by r -MSSL (right). r -MSSL can capture spatial relations beyond immediate neighbors.

The RMSE per geographical location for average model and r -MSSL is shown in Figure 5. As previously mentioned, South America has diverse climate and not all of the GCMs are designed to take into account and capture this. Hence, averaging the model outputs as done by IPCC, reduces pre-diction accuracy. On the other hand r -MSSL performs bet-ter because it learns the right weight combination on the model outputs and incorporates spatial smoothing by learn-ing the task relatedness.

Figure 6 presents the relatedness structure estimated by r -MSSL among the geographical locations. The regions con-nected by blue lines are dependent on each other. We im-mediately observe that locations in the northwest part of South America are densely connected. This area has a typ-ical tropical climate and comprises the Amazon rainforest which is known for having hot and humid climate through-out the year with low temperature variation [32].

The cold climates which occur in the southernmost parts of Argentina and Chile are clearly highlighted. Such areas have low temperatures throughout the year, but there are large daily variations [32].

An important observation can be made about South Amer-ica west cost, ranging from central Chile to Venezuela pass-ing through Peru which has one of the driest deserts in the world. These areas are located to the left side of Andes Figure 5: [Best viewed in color] RMSE per location for each one of the four approaches. Since S 2 M 2 R produced almost the same RMSE than OLS, it is not shown in the figure.
 Mountains and are known for arid climate. The average model is not performing well on this region compared to r -MSSL. We can see the long lines connecting these coastal regions, which probably explains the improvement in terms of RMSE reduction achieved by r -MSSL. The algorithm uses information from related locations to enhance its perfor-mance on these areas.

The lack of connecting lines in central Argentina can be explained by the temperate climate which presents greater range of temperatures than the tropical climates and may have extreme climatic variations. That is a transition area between the very cold southernmost region and the hot and humid central area of South America. It also comprises Patagonia, a semiarid scrub plateau that covers nearly all of the southern portion of mainland, whose climate is strongly influenced by the South Pacific air current which increases the regions temperature variability [32]. Due to such high variability it becomes harder to provide accurate tempera-ture predictions.

Figure 7 presents the dependency structure using a chord diagram. Each point on the periphery of the circle is a lo-cation in South America and represents the task of learning to predict temperature at that location. The locations are arranged serially on the periphery according to the respec-tive countries. We immediately observe that the locations in Brazil are heavily connected to parts of Peru, Colombia and parts of Bolivia. These connections are interesting as these parts of South America comprise the Amazon rainfor-est. We also observe that locations within Chile and Ar-gentina are less densely connected to other parts of South America. A possible explanation could be that while Chile which includes the Atacama Desert is a dry region located to the west of the Andes, Argentina, especially the southern part experiences heavy snowfall which is different from the hot and humid rain forests or the dry and arid deserts on the west coast. Both these regions experience climatic condi-tions which are disparate from the northern rain forests and from each other. The task dependencies estimated from the data reflect this disparity.
 Figure 6: Relationships between geographical loca-tions estimated by r -MSSL algorithm. The blue lines indicate that connected locations are depen-dent on each other. Also see Figure 7 for a chord diagram of the connectivity graph.
For the classification problems we present experimental results on four well known classification benchmark datasets. Datasets: We test the performance of the proposed MSSL algorithm on the following four data sets: (a) Landmine Detection: Data from 19 different land-mine fields were collected, which have distinct types of char-acteristics. Each object in a given data set is represented by a 9-dimensional feature vector and the corresponding binary label (1 for landmine and 0 for clutter) [41]. The feature vectors are extracted from radar images, concate-nating four moment-based features, three correlation-based features, one energy ratio feature and one spatial variance feature. The goal is to classify between mine or clutter. (b) Spam Detection: Email spam dataset from ECML 2006 discovery challenge 5 . This dataset consists of two prob-lems: In Problem A, we have emails from 3 different users (2500 emails per user); whereas in Problem B, we have emails from 15 distinct users (400 emails per user). We performed feature selection to get the 500 most informative variables using Laplacian Score feature selection algorithm [20]. The goal is to classify between spam vs. ham. For both problems, we create different tasks for different users. http://www.ecmlpkdd2006.org/challenge.html Figure 7: [Best viewed in color] Chord graph rep-resenting the structure estimated by r -MSSL algo-rithm. (c) Mnist: MNIST dataset 6 consists of 28  X  28-size images of hand-written digits from 0 through 9. We create binary classification problems for each pair of digits, totaling 45 tasks. The number of samples for each classification problem is about 15000. (d) Letter: The handwritten letter dataset 7 consists of eight tasks each of which is a binary classification problem for two letters: a/g, a/o, c/e, f/t, g/y, h/n, m/n and i/j. The input for each data point consists of 128 features represent-ing the pixel values of the handwritten letter. The number of data points for each task varies from 3057 to 7931. Baseline algorithms: Four baselines algorithms were con-sidered in the experiments and the regularization parameters for all algorithms were selected using cross-validation from { 0.01, 0.1, 1, 10, 100 } . The algorithms are: (1) Logistic Regression (LR) , learns separate logistic regression mod-els for each task; (2) Joint Feature Selection (JFS) [3] employs a ` 2 , 1 -norm regularization term to capture the task relatedness from multiple related tasks constraining all mod-els to share a common set of features; (3) CMTL [45] incor-porates a regularization term to induce clustering between tasks and then share information only to tasks belonging to the same cluster; and (4) Low rank MTL algorithm [2] assumes that related tasks share a low dimensional subspace and applies a trace regularization norm to capture that. Results: Table 3 shows the results obtained by each algo-rithm for all datasets.

Figure 8 shows the behavior of each algorithms when the number of labeled samples for each task varies. MTL algo-rithms have better performance compared to LR when there are few labeled samples available. p -MSSL also gives better results for all ranges of sample size when compared to the other algorithms. http://yann.lecun.com/exdb/mnist/ http://ai.stanford.edu/  X  btaskar/ocr/ Algorithms methods at  X  = 0 . 05 . Figure 8: Average classification error obtained from 10 independent runs versus number of training data points for all tested methods on Spam-15-users dataset.

In the Landmine detection dataset, samples from tasks 1-10 were collected at foliated regions and 11-19 are collected at regions that are bare earth or desert (these demarcations are good, but not absolutely precise, since some barren areas have foliage, and some largely foliated areas have bare soil as well). Therefore we expect two dominant clusters of tasks, corresponding to the two different types of ground surface conditions. In Figure 9 we show the graph structure rep-resenting the precision matrix estimated by p -MSSL. One can see that tasks from foliate regions (1-10) are densely connected to each other while tasks with data input from desert areas (11-19) also form a cluster.
In this paper we propose a framework for multi-task struc-ture learning. Our proposed framework simultaneously learns the tasks and their relatedness, with the task dependencies defined as edges in an undirected graphical model. The problem formulation leads to a bi-convex problem which can be efficiently solved using alternating minimization. We show that the proposed framework is general enough to be specialized to Gaussian models and GLM X  X . Extensive ex-periments on benchmark and climate datasets illustrate that structure learning not only improves multi-task prediction performance, but also captures very informative correlated behaviors within tasks.
 Acknowledgments: The research was supported by NSF grants IIS-1029711, IIS-0916750, IIS-0953274, CNS-1314560, Figure 9: Graph representing the dependency struc-ture among tasks captured by precision matrix es-timated by p -MSSL. Tasks from 1 to 10 and from 11 to 19 are more densely connected to each other, indicating two clusters of tasks.
 IIS-1422557, CCF-1451986, IIS-1447566, and by NASA grant NNX12AQ39A. AB acknowledges support from IBM and Yahoo. FJVZ would like to thank CNPq for the financial support. ARG was supported by Science without Borders grant from CNPq, Brazil. Access to computing facilities were provided by University of Minnesota Supercomputing Institute (MSI). [1] IPCC fifth assessment report. Intergovernmental [2] J. Abernethy, F. Bach, T. Evgeniou, and J.-P. Vert. [3] A. Argyriou, T. Evgeniou, and M. Pontil. Multi-task [4] B. Bakker and T. Heskes. Task clustering and gating [5] O. Banerjee, L. El Ghaoui, and A. d X  X spremont. [6] A. Beck and M. Teboulle. A fast iterative [7] M. Bentsen et al. The Norwegian Earth System [8] S. Boyd, N. Parikh, E. Chu, B. Peleato, and [9] V. Brovkin, L. Boysen, T. Raddatz, V. Gayler, [10] R. Caruana. Multitask learning: A knowledge-based [11] W. Collins et al. Development and evaluation of an [12] A. P. Dempster. Covariance selection. Biometrics , [13] J. Dufresne et al. Climate change projections using [14] T. Evgeniou, C. A. Micchelli, and M. Pontil. Learning [15] T. Evgeniou and M. Pontil. Regularized multi X  X ask [16] J. Friedman, T. Hastie, and R. Tibshiran. Sparse [17] H. B. Gordon et al. The CSIRO Mk3 climate system [18] A. Gunawardana and W. Byrne. Convergence [19] T. Hastie, R. Tibshirani, and J. Friedman. The [20] X. He, D. Cai, and P. Niyogi. Laplacian score for [21] C. Hsieh, I. Dhillon, P. Ravikumar, and A. Banerjee. [22] A. Jalali, P. Ravikumar, S. Sanghavi, and C. Ruan. A [23] S. Ji and J. Ye. An Accelerated Gradient Method for [24] S. Kim and E. P. Xing. Tree-Guided Group Lasso for [25] S. L. Lauritzen. Graphical Models . Oxford University [26] H. Liu, F. Han, M. Yuan, J. Lafferty, and [27] K. V. Mardia and R. Marshall. Maximum likelihood [28] N. Meinshausen and P. Buhlmann. High-dimensional [29] N. Meinshausen and P. B  X  uhlmann. Stability selection. [30] J. A. Nelder and R. Baker. Generalized linear models . [31] G. Obozinski, B. Taskar, and M. I. Jordan. Joint [32] V. A. Ramos. South America. In Encyclopaedia [33] K. Subbian and A. Banerjee. Climate Multi-model [34] Z. M. Subin, L. N. Murphy, F. Li, C. Bonfils, and [35] K. E. Taylor, R. J. Stouffer, and G. A. Meehl. An [36] H. Wang, A. Banerjee, C. jui Hsieh, P. Ravikumar, [37] X. Wang, C. Zhang, and Z. Zhang. Boosted multi-task [38] W. Washington et al. The use of the Climate-science [39] M. Watanabe et al. Improved climate simulation by [40] C. Widmer and G. R  X  atsch. Multitask learning in [41] Y. Xue, X. Liao, L. Carin, and B. Krishnapuram. [42] S. Yukimoto, Y. Adachi, and M. Hosaka. A New [43] L. Zhang, T. Wu, X. Xin, M. Dong, and Z. Wang. [44] Y. Zhang and D.-Y. Yeung. A convex formulation for [45] J. Zhou, J. Chen, and J. Ye. Clustered Multi-Task
