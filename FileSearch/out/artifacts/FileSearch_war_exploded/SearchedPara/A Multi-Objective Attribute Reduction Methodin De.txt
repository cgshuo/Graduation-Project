 Rough set theory is an important mathematical tool to handle imprecision, vagueness and uncertainty in data analysis [1]. The classical Pawlak rough set quired to be fully correct or certain. To handle this problem, Yao [2] proposed a decision-theoretic rough set model (DTRS) with a tolerance of errors by in-troducing Bayesian decision principle into rough set theory. As one of the most important concepts in DTRS or other rough set models, attribute reduction [2, 3] plays a key role in many areas including machine learning and data mining [4, 5]. By using attribute reduction, we can delete redundant attributes and induce a more simplified knowledge representation result.
 a minimum attribute subset that satisfies some specific criteria [6]. The mini-mum set of attributes is usually called an attribute reduct. In recent years, many different definitions of attribute reduct in DTRS have been proposed based on different criteria, such as region based attribute reducts [7 X 9], decision cost based attribute reducts [10 X 12] and entropy based attribute reducts [13 X 15]. Based on these different kinds of attribute reducts, the indiscernibility matrix algorithm-design related attribute reduction methods. However, most attribute reduction methods in DTRS are single objective reduction, that is to say, only one kind of attribute reduct definition based on one specific criterion is considered in the perfectly on the specific criterion but poorly on other criteria. For example, a positive region based attribute reduct could keep the same positive region, but it may generate a large decision cost. In the real word, it is not easy to clearly fig-users choosing the most appropriate attribute reduct to design attribute reduc-tion method. The single objective attribute reduction methods prefer a certain criterion and may obtain a biased result.
 tion method based on NSGA-II (Non-Dominated Sorting in Genetic Algorithm) [18] in DTRS. Firstly, we represent the attribute reduction as a multi-objective optimization problem by considering several different criteria, including positive characterize the utility of an attribute reduct from different views. As rough set theory usually measures the classification ability by computing the size of posi-tive region, the positive region criterion ensures classification ability unchanged based the obtained reduct, while the decision cost reflects the additional con-straints on the problem, and the mutual information shows the characteristic of data itself (here is the uncertainty). Secondly, we apply NSGA-II on the multi-objective problem to obtain a set of candidate solutions, and adopt a wrapper crowding distance and the elitist mechanism of NSGA-II, we can find a set of solutions with optimal performance on above three criteria. After obtaining a set of solutions, we use several classifiers to evaluate corresponding accuracy based on each candidate solution and output the best one as the final attribute reduct. concepts about our work. Section 3 introduces the attribute reduct based on a multi-objective optimization problem, and gives the corresponding attribute re-duction approach. Section 4 shows the experimental results. Section 5 concludes the paper. In this section, we will summarize some basic concepts about decision-theoretic rough set model.
 2.1 Decision-theoretic rough set model In DTRS, given a decision system DS = ( U,At = C  X  D, { V a | a  X  At } , { I a | a  X  At } ), where U is a finite nonempty set of objects, At is a finite nonempty set of attributes, C is a set of condition attributes describing the objects, and D is a set of decision attributes that indicates the classes of objects. V a is a nonempty set of values of a  X  At , and I a : U  X  V a is an information function that maps U | X  a  X  A ( I a ( x ) = I a ( y )) } .
 a state X or not in X . The probabilities for these two complement states can be in classifying an object x into the sets the positive region POS ( X ), the boundary region BND ( X ) and the negative region NEG ( X ), respectively. When an object actually belongs to X , let  X  PP , X  BP , X  NP denote the costs of taking actions of a ,a B ,a N , respectively. Similarly, let  X  PN , X  BN , X  NN denote the costs of taking individual action: rules are suggested as follows:  X 
PN , that is, the cost of classifying an object x belonging to X into the positive region POS ( X ) is less than or equal to the cost of classifying x into the boundary region BND ( X ), and the both of these costs are strictly less than the cost of classifying x into the negative region NEG ( X ). The reverse order of cost is used for classifying an object not in X . Then, we derive the following condition on cost functions [19]: Finally, the final minimum-risk decision rules (P)-(B) can be written as Where the parameters  X  ,  X  are defined as: 2.2 Three kinds of criteria in decision-theoretic rough set model an attribute reduct from different views, including positive region, decision cost and mutual information.
 classification ability, therefore, the positive region is the most commonly used criterion for defining an attribute reduct.
 U induced by D , and  X  A denote the partition induced by the set of attributes A  X  At . Based on the threshold (  X , X  ), one can divide the universe U into three regions of the decision patition  X  D : region is. By using the positive region criterion, we can define an attribute reduct which keeps the positive region unchanged or expanded.
 Decision cost. DTRS is based on Bayesian decision procedure and the principle of making decisions is minimizing the decision cost. In DTRS, decision cost is a very important notion and it can be intuitively considered as the criterion for defining an attribute reduct.
 The decision cost can be described as [9]: dc = X namely  X  PP =  X  NN = 0, then the decision cost can be rewritten as: Based on the decision cost criterion, a minimum cost attribute reduct in DTRS decision cost is minimum.
 Mutual Information. Since mutual information in Shannon X  X  information the-ory can be used to evaluate the relevance between attributes and class labels, definitions about mutual information present as follows [20]: if A  X  At , the entropy of A is is the equivalence class under a set of attribute A .
 if A  X  At , B  X  At , U/IND ( A ) = { A 1 ,A 2 ,  X  X  X  ,A m } and U/IND ( B ) = { B 1 ,B 2 ,  X  X  X  ,B n } , the conditional entropy of A with reference to B is At } ) , R  X  C , the mutul information between conditional attribute set R and the decision class D can be defined as Usually, the mutual information between C and D has the maximum value, then an attribute reduct can be defined to find a minimal attribute set with the maximum mutual information value.
 In this section, we will define a multi-objective attribute reduct in DTRS and propose a reduction method based on NSGA-II. 3.1 Multi-objective attribute reduct brings the difficulty of choosing which kind of attribute reduct for different ap-plications. Therefore, to address this problem, we propose a multi-objective at-tribute reduct in this paper.
 functions with some related constraints, and it can be described as follows [21]: constraints and h j ( Y ) = 0 define equality constraints.
 region size, minimizing decision cost and maximizing mutual information, which characterize the utility of an attribute reduct from different views. The definition of multi-objective attribute reduct is presented as follows.
 At } ) ; R  X  C is a multi-objective attribute reduct if only if f region, a small decision cost and a large mutual information, respectively. problem may be conflicting with each other. That is to say, it is impossible to achieve the optimal performance on all sub objective functions at the same time. Instead, many multi-objective optimization problems output a set of Pareto optimal solutions for users. Therefore, we usually have more than one attribute reduct satisfying the definition of multi-objective attribute reduct. Algorithm 1 Multi-objective attribute reduction algorithm Input: A decision system DS with corresponding loss functions; The genera-Output: The reduct R ; 1: R  X  X  X  2: Generate P 0 randomly; //the initial parent population 3: F = Non-dominated-sort( P 0 ) ; // F = ( F 1 ,F 2 ,... ), all nondominated 4: for t = 0 to G do 5: S t =  X  , i = 1; 6: Q t = Genetic-operators( P t ); 7: R t  X  P t  X  Q t ; 8: F = Non-dominated-sort( R t ); // F = ( F 1 ,F 2 ,... ), all nondominated 9: repeat 10: S t = S t  X  F i and i = i + 1; 11: until | S t | X  N 12: Last front to be included: F l = F i ; 13: if | S t | = N then 15: else 17: K = N  X  X  P t +1 | ; 18: Crowding-distance-assignment( F i ); 19: Sort( F i ); // sort solutions in F i with crowding distance in descending 20: SortPart( F i ); //sort solutions with the length of reduct when crowing 21: P t +1 = P t +1  X  F i [1 : K ]; //choose the first K elements of F i 22: end if 23: end for 24: Compute the accuracy of solutions in P t based on several classifiers; 25: Select the chromosome with best performance from P t to be R ; 26: return R ; 3.2 Multi-objective attribute reduction algorithm ing multi-objective optimization problem. In this regard, we introduce a multi-objective attribute reduction method based on NSGA-II algorithm, which is a kind of efficient genetic algorithm for multi-objective optimization problem. S-accuracy as the final attribute reduct, while the accuracy is evaluated by several different classifiers ensemble.
 the classical NSGA-II algorithm. Differing from the general genetic algorithm, NSGA-II proposed the crowding distance and the elitist mechanism. Let us con-sider t th generation of NSGA-II algorithm. Suppose the parent population at this generation is P t and its size is N , while the offspring population created from P t is Q t having N members. After obtaining the offspring population, to preserve elite members of the combined parent and offspring population R t = P t  X  Q t (of and so on), and the solutions with less levels will be chosen. If the solutions have same levels, the solutions with less crowding distance will be chosen. from the classical NSGA-II, we check the offspring individuals whether repeat-ed, and if they are repeated, then generate a new one. Meanwhile, to make the method more suitable for obtaining an attribute reduct, when the crowing dis-tance of solutions are also same, we compare the length of attribute reduct and choose the solution with the shortest length. Finally, after obtaining a set of can-didate solutions, we apply several classifiers to evaluate corresponding accuracy based on each candidate solution and output the best one as the final attribute reduct. More details can be found in Algorithm 1. 4.1 Dataset There are 8 UCI datasets [22] applied in our experiments. The basic information attributes, | U | is the number of objects, and | D | is the number of classes. We also replace the missing values and discretize all continuous attributes by using WEKA filters [23]. 4.2 Experimental setting In our experiments, for each dataset, 10 times 10-fold cross-validation is applied classifiers in Weka [23] are considered to evaluate the classification accuracy, in-cluding NaiveBayes, J48(C4.5), RandomForest and SMO(SVM). For each data set, different loss functions are generated randomly in the interval of (0,1), which  X 
NN = 0 and (  X  PN  X   X  BN )(  X  NP  X   X  BP ) &gt; (  X  BP  X   X  PP )(  X  PN  X   X  NN ). 4.3 Experimental results The experimental results are shown in Tables 2-6. In these tables, PRER, M-CR, MIR represent the positive region expanding reduct [7], the minimum deci-sion cost reduct [10], the maximum mutual information reduct [20], respectively. MOAR is our multi-objective attribute reduct by considering positive region, decision cost and mutual information simultaneously. The best performance of each row in the tables is boldfaced, in addition, the average ranks of these four methods on all datasets are also recorded.
 choose solutions, when their crowding distances are same.
 on different reducts. For NaiveBayes and SMO, MOAR can produce the best accuracy on all datasets. For J48 and RandomForest, MOAR could also get the best result on most datasets.
 average rank on the misclassification cost criterion. An interesting result is that MOAR is better than MCR, while MCR is a single-objective attribute reduct by minimizing decision cost only. The reason is that the Bayesian decision cost minimized in MCR is a kind of expected cost, while the misclassification cost is a kind of actual cost. Although MOAR considers the expected decision cost as well, it also considers other two criteria as the optimization objectives, the result shows the robustness of MOAR.
 tribute reduction is a better choice for decision-theoretic rough set model, as it can achieve a better accuracy with a lower misclassification cost. In this paper, a multi-objective attribute reduct definition is proposed based on tribute reduct based on one criterion perform perfectly on the specific criterion but poorly on other criterion. In addition, a multi-objective attribute reduction attribute reduct with optimal classification accuracy from a set of Pareto solu-tions. Experiments results show the effectiveness and robustness of our proposed method.
 This paper is supported by the National Natural Science Foundations of China (Grant Nos. 61403200, 71671086), and the Natural Science Foundation of Jiangsu Province (Grant No. BK20140800).

