 2: Department of Psychology, and Neuroscience Institute, Princeton University, Princeton NJ, USA When applying off-the-shelf machine learning algorithms to data with spatial dimensions (images, geo-spatial data, fMRI, etc) a central question arises: how to incorporate prior information on the individual image voxels as features, the voxel spatial information is ignored. Indeed, if we randomly shuffled the voxels, the algorithm would not notice any difference. Yet in many cases the spatial arrangement of the voxels together with prior information about expected spatial characteristics of the data may be very helpful. We are particularly interested in the situation when the trained classifier is used to identify relevant spatial regions. To make this more concrete, consider the problem of training a classifier to distinguish two different brain states based on fMRI responses. Successful classification suggests that the voxels used are important in discriminating between the two classes. Hence we could use a successful classifier to learn a set of discriminative voxels. We expect that these voxels will be spatially compact and clustered. How can this prior knowledge be incorporated into the training of the classifier? In summary, our primary objective is improving the ability of the trained classifier to usefully identify the spatial pattern of discriminative information. However, incorporating spatial information into boosting may also improve classification accuracy. do this by adding a spatial regularization kernel to the standard loss minimization formulation of boosting. We then design an associated boosting algorithm by using coordinate descent on the regularized loss. We show that the algorithm minimizes the regularized loss function and has a natural interpretation of boosting with additional adaptive priors/weights on both spatial locations and training examples. We also show that it exhibits a natural grouping effect on nearby spatial locations with similar discriminative power.
 We believe our contributions are fundamental and relevant to a variety of applications where base classifiers are attributed with a known auxiliary variable and prior information is known about this auxiliary variable. However, since our study is motivated by the particular problem of voxel selection in fMRI analysis, we briefly review the state of the art in this domain so as to put our contribution into a concrete context. Briefly, the fMRI voxel selection problem is to use the fMRI signal to identify a subset of voxels that are key in discriminating between two stimuli. One expects such voxels to be spatially compact and clustered. Traditionally this is done by thresholding a statistical univariate test score on each voxel [1]. Spatial smoothing prior to this analysis is commonly employed to integrate activity from neighboring voxels. An extreme case is hypothesis testings on clusters of voxels rather than on voxels themselves [2]. The problem with these methods is that they greatly sacrifice the spatial resolution of the results and averaging could hide fine patterns in data. An alternative is to spatially selects discriminating wavelet components, not voxels. A more promising spatially aware approach selects voxels with tree-based spatial regularization of a univariate statistic [5, 6]. This can achieve both spatial precision and smoothness but uses a complex regularization method. Our proposed method also selects single voxels with the help of spatial regularization but operates in a multivariate classifier framework using a simpler form of regularization.
 Recent research has suggested that multivariate analysis has potential advantages over univariate tests [7, 8], e.g. it brings in machine learning algorithms (such as boosting, SVM, etc.) and there-fore might capture more intricate activation patterns involving multiple voxels. To ensure spatial clustering of selected voxels, one can run a searchlight (a spherical mask) [9] to pre-select clustered informative features. In each searchlight location, a multivariate analysis is performed to see whether the masked area contains informative data. One can then train a classifier on the pre-selected voxels. A variant of this two-stage framework is to train classifiers on a few predefined masks, and then aggregate these classifiers by boosting [10, 11]. This is faster but assumes detailed prior knowledge to select the predefined masks. Unlike two-stage approaches, [12] directly uses AdaBoost to train classifiers with  X  X ich features X  (features involving the values of several adjacent voxels) to capture spatial structure in the data. Although exhibiting superior performance, this method selects  X  X ich features X  rather than individual discriminating voxels. Moreover, there is no control on the spatial smoothness of the results. Our method is similar to [12] in that we combine the feature selection and classification into one boosting process. But our algorithm operates on single voxels and uses simple spatial regularization to incorporate spatial information.
 The remainder of the paper is organized as follows. After introducing notation in  X  2, we formu-late our spatial regularization approach in  X  3 and derive an associated spatially regularized boosting algorithm in  X  4. We prove an interesting property of the algorithm in  X  5 that guarantees the simulta-neous selection of equivalent locations that are spatially close. In  X  6, we test the algorithm on face gender detection, OCR image classification, and fMRI experiments. In a supervised learning setting, we are given m training instances X = { x i  X  R n ,i = 1 ,...,m } and corresponding binary labels Y = { y i =  X  1 ,i = 1 ,...,m } . Using the training instances X , we select a pool of base classifiers H = { h j : R n  X  X  X  1 , +1 } ,j = 1 ,...,p } . Our objective is to train that h j  X  H  X   X  h j  X  H , thus all values in  X  can be assumed to be nonnegative. Boosting is a technique for constructing from X , Y and H the weight  X  of a composite classifier to best predict the labels. This can be done by seeking  X  to minimize a loss function of the form: The result of a conventional boosting algorithm is determined by the m  X  p matrix M = [ y i h j ( x i )] [15]. Under a component permutation  X  x i = Px i , the base classifiers become  X  h j = h j  X  P  X  1 ; so  X  i.e., the arrangement of the components can be arbitrary as long as it is consistent.
 The weights  X  of a composite classifier not only indicate how to construct the classifier, but also the relative reliance of the classifier on each of the n instance components. To see this, assume each h j depends on only a single component of x  X  R n , i.e., for some standard basis vector e k , and the association between base classifiers and components explicit, let s be the function s ( j ) = k if h indicates the relative importance the classifier assigns to each instance component. Although we used decision stumps above for simplicity, more complex base classifiers such as decision trees could be used with proper modification of mapping from  X  to  X  . We call  X  the component importance map . Suppose the instance components reflect spatial structure in the data, e.g. the components are samples along an interval or pixels in an image. Then the component importance map is indicating the spatial distribution of weights that the classifier employs. Presumably a good classifier distributes map is indicating how discriminative information is spatially distributed. It is in this aspect of the classifier that we are particularly interested. Now as shown above, conventional boosting ignores spatial information. Our objective, pursued in the next sections, is to incorporated prior information on spatial structure, e.g. a prior on the component importance map, into the boosting problem. To incorporate spatial information we add spatial regularization of the form  X  T K  X  to the loss (1) where the kernel K  X  R n  X  n ++ is positive definite. For concreteness, we employ the exponential loss l ( y The term  X  T K  X  imposes a spatial smoothness constraint on  X  . To see this, consider the eigen-decomposition K = U  X  U T , where the columns { u j } of U are the orthonormal eigenvectors,  X  j is the eigenvalue of u j and  X  = diag(  X  1 , X  2 ,..., X  n ) . Then the regularizing term can be rewrit-ten as  X  k  X  1 2 U T  X  k 2 2 where U T  X  is the  X  X pectrum X  of  X  under the orthogonal transformation U T . Rather than standard Tikhonov regularization with k  X  k 2 2 = k U T  X  k 2 2 , we penalize the variation in direction u j proportional to the eigenvalue  X  j . By doing so we are encouraging  X  to be close to the eigenvectors u j with small eigenvalues. This encodes our prior spatial knowledge.
 As an example, consider the kernel K =  X I  X  G , where G is a Gaussian kernel matrix: with v j the spatial location of component j , k v i  X  v j k 2 the Euclidean distance (other distances can also be used) between components i and j , and r the radius parameter of the Gaussian kernel. coordinates. So G is a size d 2  X  d 2 matrix. We plot the 6 eigenimages of K with smallest eigenvalues in Figure 1. The regularization imposes a spatial smoothness constraint by encouraging  X  to give more weight to the eigenimages with smaller eigenvalues, e.g. the patterns shown in Figure 1. We now derive a spatially regularized boosting algorithm (abbreviated as SRB) using coordinate descent on (3). In particular, in each iteration we choose a coordinate of  X  with the largest negative gradient and increase the weight of that coordinate by step size  X  . This results in an algorithm similar to AdaBoost, but with additional consideration of spatial location.
 To begin, we take the partial derivative of (3) w.r.t.  X  j 0 : on training instance x i , then the partial derivative in (4) can be written as: amples. Normally, we choose h j 0 to maximize this term. This corresponds to choosing the best base classifier under the current weight distribution. However, here we have an additional term: the performance of base classifier h j 0 is enhanced by a weight  X  s ( j 0 ) on its corresponding component s ( j 0 ) . We call  X  the spatial compensation weight . To proceed, we choose a base classifier h j 0 to maximize the sum of these two terms and then increase the weight of that base classifier by a step size  X  . This gives Algorithm 1 shown in Figure 2. The key differences from AdaBoost are: (a) the new algorithm maintains a new set of  X  X patial compensation weights X   X  ; (b) the weights on training examples w i are not normalized at the end of each iteration.
 Algorithm 1 The SRB algorithm 1: w i  X  1 , 1  X  i  X  m 2:  X   X  0 3: for t = 1 to T do 4:  X   X  Q  X  5:  X   X  X  X  2  X K  X  6: find the  X  X est X  base classifier in the fol-7: choose a step size  X  ,  X  j 0  X   X  j 0 +  X  8: adjust weights: 9: end for 10: Output result: h  X  ( x ) = P p j =1  X  j h j ( x ) To elucidate the effect of the compensation weights, consider the kernel K =  X I  X  G , with G defined a component receives a high compensation weight  X  k = 2  X  (  X   X  k  X   X  X  k ) if some neighboring spatial locations have already been selected (i.e., made  X  X ctive X ) by the composite classifier. On the other hand, the weight of a component is reduced (proportional to the magnitude of parameter  X  ) if it associated with  X  X nactive X  locations that are close to  X  X ctive X  locations.
 We can enhance the algorithm by including a backward step each iteration:  X  j 00  X   X  j 00  X   X  0 , where This helps remove prematurely selected base classifiers [16, 17]. This is Algorithm 2 in Figure 2. Spatial regularization brings no significant computational overhead: Compared to AdaBoost, SRB has additional steps 4,5, which can be computed in time O ( n ) every iteration. Adaptive weight  X  incurs no additional complexity for step 6 in our current implementation.
 We now briefly discuss the choice of step size  X  in Algorithm 1 (  X  1 and  X  2 in Algorithm 2 can be chosen similarly).  X  could be a fixed (small) step size at each iteration. This is not greedy but may necessitate a large number of iterations. Alternatively, one can be greedy and select  X  to minimize the value of the loss function (3) after the change  X  j 0  X   X  j 0 +  X  : s ( j 0 ) . Setting the derivative of (6) to 0 yields: However, for the following slightly more conservative step size we can prove algorithm convergence: Theorem 1. The step size (8) ensures convergence of Algorithm 1.
 condition for  X   X  to reduce the objective (6). Since the objective (3) is nonnegative and each iteration of the algorithm reduces (3), the algorithm converges. The second inequality in (9) uses monoticity while the first inequality in (9) uses the following lemma proved in the supplementary material: Lemma: If 0 &lt;  X   X  min { 3 ( W +  X  W  X  ) W Recall our objective of using the component importance map of the trained classifier to ascertain the spatial distribution of informative components in the data. Ideally, we would like  X  to faithfully represent this information. In general, however, a boosting algorithm will select a sufficient but incomplete collection of base classifiers (and hence components) to accomplish the classification. For example, after selecting one base classifier h j , AdaBoost will adjust the weights of training examples to make the weighted training error of h j exactly 1 2 (totally uninformative), thus preventing the selection of any classifiers similar to h j in the next iteration. In fact, for AdaBoost we can prove that in the optimal solution  X   X  , we can transfer coefficient weights between any two equivalent base classifiers without impacting optimality. So minimizing the loss function (1) does not require any particular distribution among the  X  coefficients of identical components. This is the content of the following proposition. h 2 ( x i ) for all x i  X   X  also minimizes loss function (1) where  X   X  =  X   X   X   X e j basis vector in R p .
 Proof. h j 1 ( x i ) = h j 2 ( x i ) implies that h  X   X  ( x i ) = h  X   X  ( x i ) for all x i  X  X  . What is desirable is a  X  X rouping effect X , in which components with similar behavior under H receive similar  X  weights. We will prove that asymptotically, SRB exhibits a  X  X rouping effect X . In particular, for kernel K =  X I  X  G , G defined in (4), we will look at the minimizer  X   X  = Q  X   X  of the loss on two similar components.
 To proceed, let  X   X  minimize (3) with:  X   X  = Q  X   X  ,  X   X  =  X  2  X K  X   X  , and the corresponding training instance weight w  X  . Let H k denote the subset of base classifiers acting on component k , i.e., H k = { h j  X  X  : s ( j ) = k } . The following lemma is proved in the supplementary material: Assuming K =  X I  X  G , G defined in (4), we have the following result: Theorem 2. Let  X   X   X  = G  X   X  be the smoothed version of vector  X   X  . Then for any k 1 and k 2 : where d ( k 1 ,k 2 ) = | max h j  X  X  k Proof. We prove the following three cases separately: angle inequality on the LHS to obtain the result. triangle inequality on the right hand side of the previous expression yields the result. (3)  X   X  k 1 =  X   X  k 2 = 0 . In this case, the inequality is obvious.
 The theorem upper bounds the difference in the importance coefficient of two components by the This term is small when the two locations are spatially close, or when they are in two neighborhoods that contain a similar amount of important voxels. The second term reflects the dissimilarity between two voxels. This term measures the difference in the weighted performances of a location X  X  best base instances. More generally, we can sort all the training examples by the activation level on a single component. If sorting on locations k 1 and k 2 yields the same results, then d ( k 1 ,k 2 ) = 0 . The first experiment is gender classification using features located on 58 annotated landmark points in the IMM face data set [19] (Figure 3(a)). For each point we extract the first 3 principal components of a 15  X  15 window as features. We randomly choose 7 males and 7 females to do leave-one-out 7-fold cross-validation for 100 trials. AdaBoost yields an average classification accuracy of  X  = 78 . 8% with a standard deviation of  X  = 19 . 9% . SRB (  X  = 0 . 1 , r = 10 pixel-length) achieves  X  = 80 . 5% and  X  = 18 . 7% . The component importance map  X  of SRB reveals both eyes as discriminating areas and demonstrates the grouping effect. (All experiments in this section use  X  = max j ( P i G ij ) . By (10), a larger  X  will make this grouping effect more dominant). The  X  for AdaBoost is less smooth and less interpretable with the most important component on the left chin (Figure 3(b,c)). The second experiment is a binary image classification task. Each image contains the handwritten digits 1,1,0,3 and a random digit, all in fixed locations. Digits 0 and 1 are swapped between the classes (Figure 4(a-d)). The handwritten digit images are from the OCR digits data set [20]. To obtain the training/testing instances we add noise to the images (Figure 4(e)). We test the ability of several algorithms to: (a) find the discriminating pixels, and (b) if a classification algorithm, ac-curately classify the classes. The quality of pixel selection is measured by a precision-recall curve, with ground truth pixels (Figure 4(f)) selected by a t-test on the two classes of noiseless images. This curve is plotted for the following methods: (1) SRB (  X  = 0 . 5 , r = 1  X  (3) thresholding the univariate t-test score; (4) thresholding the first one or two principle compo-nent(s); (5) thresholding the pixel coefficients in an LDA model with diagonal covariance (Gaussian naive bayes classifier); (6) level-set method [6] on a Z-statistics map. We plot the precision-recall curve by varying the number of iterations (for (1),(2)) or the value of the threshold (for (3)-(6)). We also tried all methods with Gaussian spatial pre-smoothing as a preprocessing step. The classifica-tion accuracies are measured for methods (1), (2) and (5) on separate test data.
 The results, averaged over 100 noise realizations, are plotted in Figure 5. SRB showed no loss of classification accuracy nor convergence speed (usually within 100 iterations), and achieved the best pixel selection among all methods. It is better than Gaussian naive Bayes and PCA methods, even when the noise matches the i.i.d. Gaussian assumption of these methods (Figure 5(a,d)). In all cases, local spatial averaging deteriorates the classification performance of boosting.
 In the third experiment, subjects watch a movie during the fMRI scan. The classification task is to discriminate two types of scenes (faces and objects) based on the fMRI responses. Each fMRI responses is a single TR scan of the brain volume. We divide the data (14 subjects, 26 face and 18 object fMRI responses) into 10 cross validation groups and average the classification accuracies. SRB (  X  = 0 . 1 , r = 5 voxel-length) trained for 100 iterations yields accuracy  X  = 73 . 3% with  X  = 9 . 3% across 14 subjects. AdaBoost yields  X  = 75 . 5% with  X  = 4 . 9% . To make sure this  X  = 4 . 6% , which is effectively chance. We note that spatially regularized boosting yields a more clustered and interpretable selection of voxels. The result for one subject (Figure 6) shows that voxels and nicely highlights the relevant FFA area [21] and posterior central sulcus [22, 23]. The proposed SRB algorithm is applicable to a variety of situations in which one needs to boost the performance of base classifiers with spatial structure. The mechanism of the algorithm has a natural interpretation: in each iteration, the algorithm selects a base classifier with the best perfor-mance evaluated under two sets of weights: weights on training examples (as in AdaBoost) and weights on locations. The additional set of location weights encourages or discourages the selection of certain base classifiers based on the spatial location of base classifiers that have already been se-lected. Computationally, SRB is as effective as AdaBoost. We demonstrated the effectiveness of the algorithm both by providing a theoretical analysis of the  X  X rouping effect X  and by experiments on three data sets. The grouping effect is clearly demonstrated in the face gender detection experiment. In the OCR classification experiment, the algorithm shows superior performance in pixel selection accuracy without loss of classification accuracy. The algorithm matches the performance of the state-of-the-art set estimation methods [6] that use a more complex spatial regularization and cycle spinning technique. In the fMRI experiment, the algorithm yields a clustered selection of voxels in positions relevant to the task. An alternative approach, being explored, is to combine searchlight [9] with a strong learning algorithm (e.g. SVM) to integrate spatial locality and accurate classification. The authors thank Princeton University X  X  J. Insley Blair Pyne Fund for seed research funding.
