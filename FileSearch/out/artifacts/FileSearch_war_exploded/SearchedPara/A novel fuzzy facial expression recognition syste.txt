 1. Introduction Emotion Recognition is an important step in Human X  Computer Interaction (HCI). For establishing emotional interac-tions between humans and computers, a system to recognize human emotion is of a high priority. There are many applications that use  X  X  X motion recognition X  X  such as customer services, intel-ligent automobile systems, game and entertainment industry, automated systems that provide aid to psychologists, behavioral and neuroscience researchers and the like. These applications are built based on automated human emotion recognition. It is noted that in this paper we use  X  X  X motion Recognition System X  X  and  X  X  X acial Expression Recognition System X  X  synonymously.
Several various works using different approaches for emotion recognition have been proposed in the literature recently ( Chakraborty et al., 2009 ; Maglogiannis et al., 2009 ; Pantic and Rothkrantz, 2000 ; Cowie et al., 2005 ; Esau et al., 2005 ; Guo and Gao, 2007 ; Ioannou et al., 2005 ; Jamshidnejad and Jamshidined, 2009 ; Ko and Sim, 2009 ; Pantic and Rothkrantz, 2004 ; Bashyal and Venayagamoorthy, 2008 ; Kharat and Dudul, 2008 ; Zeng et al., 2006 ; Muthukaruppan et al., 2007 ; Khan and Bhuiyan, 2009 ).
Table 1 summarizes the aforementioned methods and their main characteristics.

For example, a fuzzy relational approach to human emotion recognition from facial e xpressions was proposed by Chakraborty et al. (2009) . One major drawback of mentioned article is that only three distinct fuzzy sets: HIGH, LO W, and MODERATE for fuzzification were used. Moreover, only three facial features (eye opening, mouth opening, and the length of eyebrow constriction) were considered, and the FCM clustering algorithm that was employed for detecting the lip region is highly time-consuming. In this paper, we solve these drawbacks using more than three f uzzy sets with different types of membership functions, we also use m ore facial features for emotion recognition, and also new facial features extraction algorithms with high precision and accuracy are proposed.

Maglogiannis et al. (2009) presented an integrated system for emotion detection in which they used only eye and mouth expressions for detecting five emotions (Happy, Sad, Neutral,
Surprise, and Angry). The main part of their algorithm is the utilization of an edge detection technique to determine the lines of the eyes and mouth, curves and gradients. It is mentioned that they did not use fuzzy inference system for emotion recognition.
Note that in Table 1 , most of the previous works reported their precision and accuracy on different and/or non-standard image databases. This may cause the results to be incomparable and also somehow unreliable. Besides, a robust approach to recognize emotion from partially occluded facial images has not been proposed yet.
In this paper, we propose a new method for Emotion Recogni-tion from Facial Expression using Fuzzy Inference System (FIS) that is even able to recognize emotions from Partially Occluded Facial Images. One important point considered in our proposed method is that the experimental results are more reliable. It is because we measure the performance and accuracy of our method by a comprehensive and rich emotional Image database named RaFD ( Langner et al., 2010 ) that covers different ages, ethnicities, genders, and emotions with many subjects. So, the reported precision rates for our system are more reliable because these rates show that the proposed system is robust to the changes on age, ethnicity, and gender. In the following, we briefly describe our proposed algorithm and later we explain the algorithm in detail.
Every Emotion Recognition System has three main stages: 1. Face Detection. 2. Feature Extraction. 3. Classification.
 Until now, many algorithms have been proposed for Face Detection. In this article, we suppose that the Face Detection stage has been performed on the frontal face image and the part of image that contains the face is forwarded to the next stage.
The next stage after detecting faces (in most of emotion recognition systems) is facial feature extraction. Facial feature extraction is one of the most complicated and time-consuming stages in emotion recognition systems. The final precision of an emotion recognition system largely depends on precision of this stage. Consequently, selecting precise and efficient algorithms for this stage is of high importance. Here, we suggest new algorithms for facial feature extraction of color images, which finally produce satisfactory performance and precision. Performance and accu-racy of the new feature extraction algorithms are evaluated by 200 images from three different emotional image databases in order to be assured of the reliability of the results.
Generally, we divide facial features into two major categories: 1. The Primary Features: The features that are used for recogniz-ing every six basic emotions. These features are: Eye Opening, Mouth Opening, Eyebrow constriction and Mouth Corners
Displacement. 2. The Secondary Features: The features that are used as the auxiliary attributes for precise recognition of emotions. These features are: Mouth Length, Nose-Side Wrinkles, Existence of the Tooth, Eyebrows Slope, and Lips Thickness.

With this categorization, we are able to recognize different emotions more precisely. Note that, we choose eye opening, mouth opening, and the like as primary facial features because we experimentally concluded that these features are the basic and essential features in emotion recognition for all six basic emotions. But secondary features help differentiate between emotions that are considered the same in terms of the primary features. It is mentioned that primary features such as eye opening, mouth opening, and the like have been chosen as essential features for emotion recognition in most of the previous works.

After extracting facial features, the Emotions Recognition is needed to be addressed. We select six basic emotions proposed by Ekman (1993) for the recognition system. These emotions are: Anger, Disgust, Fear, Happiness, Sadness, and Surprise. Then, we recognize the mentioned emotions using a Fuzzy Inference System (FIS). In this paper, we define 573 fuzzy rules fo r emotion recognition using the fuzzy inference system. By defining this number of fuzzy rules (573 rules), we can recognize differe nt emotions more accurately.
One of the main factors that have an important influence on the final precision of fuzzy inference systems is the membership function parameters. Therefore, here we use Genetic Algorithms for para-meter-tuning of the membership f unctions. By using a Genetic
Algorithm for parameter-tuning of membership functions (MFs), we will be able to select optimal or near-optimal parameters for MFs to achieve a higher performance for our emotion recognition system.
The mentioned stages for our emotion recognition system have been shown in Fig. 1 .

The main contributions of our method are expressed in the following list: 1. We provide a simple solution to emotion recognition from partially occluded facial images. 2. We divide facial features into two major categories: Primary
Features and Secondary Features, to improve the precision of the emotion recognition system. 3. We propose nine new algorithms for facial features extraction.
These algorithms are simple and at the same time accurate leading to high performance in emotion recognition. 4. We tune the fuzzy membership functions X  parameters using a customized Genetic Algorithm for reaching more precise and accurate results for emotion recognition.

The rest of the paper is organized as follows: Section 2 describes new algorithms for facial feature extraction. In Section 3 , we fuzzify the measurements of facial features. In Section 4 ,we propose emotion classification using Fuzzy Inference System (FIS).
Section 5 illustrates emotion recognition from partially occluded facial images. Finding optimal parameters for fuzzy membership function using Genetic Algorithm is presented in Section 6 . The
RaFD Faces Database is introduced in Section 7 . Validation of the proposed algorithms for facial features extraction and also our method for emotion recognition are expressed in Section 8 . And finally, conclusions are drawn in Section 9 . 2. Facial feature extraction
As mentioned earlier, one of the most complicated and time-consuming stage in emotion recognition systems is the facial feature extraction. The final precision of emotion recognition strongly depends on precision of this stage. Consequently, selecting precise and efficient algorithms for this stage are very important.
First of all, we must identify wh at attributes are valuable for emotion recognition. In the literature, various attributes have been selected as interesting features. For example, Eye Opening, Mouth Opening, and Eyebrow Constriction in Chakraborty et al. (2009 ), and Eye Opening, Mouth Opening, and Mouth Corners Angle in Maglogiannis et al. (2009 ), have been chosen as interesting features. In this paper we divide facial features into two sections: The
Primary Features and the Secondary Features. In the following, we explain our proposed algorithms for extracting the aforemen-tioned features. 2.1. Primary features extraction 2.1.1. Eye opening calculation So far, different approaches have been employed for Eye
Opening measurement ( Khanum et al., 2009 ; Sayeed et al., 2007 ; Cowie et al., 2005 ; Chakraborty et al., 2009 ; Maglogiannis et al., 2009 ). In this paper, we propose a new hybrid algorithm using Edge Detection and Skin Color Detection in order to increase the accuracy of the Eye Opening measurement. The first stage of Eye Opening Calculation is Eye Detection. Our Eye Detection Algorithm has been illustrated in Fig. 2 .

In the proposed approach for eye detection, first of all, we transform the image from the RGB into the YCbCr color space (Y is the luminance component and Cb and Cr are the chromi-nance components).
 high-Cb and low-Cr values can be found around the eyes ( Maglogiannis et al., 2009 ). This map is expressed in Eq. (1) EyeMapChr  X  1 3 Cb 2  X  X  255 Cr  X  2  X  Cb Cr  X  1  X  component. Eyes usually contain both dark and bright pixels in the luminance component. Thus, grayscale operators can be designed to emphasize brighter and darker pixels in the lumi-nance component around eye regions. Such operators are dilation and erosion. We use grayscale dilation and erosion with a spherical structuring element to construct the eye map from the luminance ( Maglogiannis et al., 2009 ). This map is defined by Eq. (2). Note that spherical structuring element is a flat disk-shaped structuring element, with an integer radius that specifies the size of this disk
EyeMapLum  X  Dilate  X  Y  X  x , y  X  X  Erosion  X  Y  X  x , y  X  X  X  1 where Y ( x , y ), denotes the image of the face. The eye map from the chrominance is then combined with the eye map from the luminance by an AND (multiplication) operation. The resulting eye map is then dilated and normalized to brighten both eyes and suppress other facial areas. Then, with an appropriate choice of a threshold, we can track the location of the eye region ( Maglogiannis et al., 2009 ).

By using these maps, we will be able to find eye X  X  approximate regions. In the following, the proposed algorithm for this purpose is expressed.

In the proposed algorithm, after finding eye regions by following eye maps, we perform a template matching algorithm on the eye region for finding a window that contains the eye. We use the correlation coefficient for template matching. Indeed, we choose an eye window as eye template and then we move this sliding eye window step by step across the eye regions. At each step, the correlation coefficient is calculated within the sliding eye window. Finally, the window that has the highest correlation coefficient is selected as the window containing the eye. Primary Features Secondary Features
Afterwards, because the area of the iris in the eye region has high values in the following eye maps, we compute eye maps within the detected eye window. Therefore, we transfer areas having high values in the window to its center. As a result, we have a window that contains the eye such that the iris of the eye is located at the center of the window.

It is assumed that the images are frontal face images. Conse-quently, the eye detection algorithm performs on the upper half of the facial image to reduce the computational overload. Indeed, every image that is delivered to our system, is resized to 144 pixels, so the upper half of face image is from the first row to row 72, and lower half of face is from row 73 to 144 of original face image.
 The eye template has been shown in Fig. 3 .

After finding the eye X  X  window, we will find the eye opening value for the left and right eyes. For attaining this, at first, we will find upper and lower eyelids in the eye window, which is explained in the following. Our Eye Opening Calculation Algo-rithm has been illustrated in Fig. 4 .

In our hybrid algorithm for Eye Opening Calculation, at first, we use Canny X  X  edge detector ( Canny and John, 1986 ) for finding the edge of the eyelids. The reason to use the Canny X  X  edge detector is that it shows good localization performance and it is able to minimize multiple responses to a single edge.
Next, the vertical edges will be eliminated by a morphological opening operator with an appropriate structuring element and henceforward with deleting small remaining edges, only the long horizontal edges, will be kept ( Ioannou et al., 2007 ).
Since the edge detector X  X  results for finding eyelids do not possess the required accuracy, we combine the result of this algorithm with the result of eyelids detection using a skin detection algorithm.
Using a combination of different skin detection algorithms that were introduced in ( Gasparini and Schettini, 2006 ; Rahman et al., 2006 ; Peer et al., 2003 ; Chai and Ngan, 1999 ; Garcia and Tziritas, 1999 ; Tsekeridou and Pitas, 1998 ; Wang and Yuan, 2001 ; Gomez and Morales, 2002 ), we can almost precisely segment the eye window into two regions: skin and non-skin regions. For more details, see Ilbeygi and Shah-Hosseini (2010 ).

Now, we can determine the boundary of these regions by subtracting the segmented binary image from its eroded version.
Note that we used binary image erosion with a spherical struc-turing element.

After merging the result of the edge and skin detection algorithms for eyelids detection, we can accurately calculate the amount of eye opening.

For calculating eye openness, we should initially find the eye center. The eye center is the darkest point of the image and it is near to the center of the eye window, so we can compute eye openness by summing the distance from this point to the upper and lower eyelids. Fig. 5 depicts the mentioned algorithms for eye opening calculation.

Since the eye can have a wide range of openness, from the closed eye to the very open one, many different forms of the eye boundary may appear in the eye window. For example, if the eye opening is very high, then the upper or lower eyelid may not be detected and thereupon we cannot calculate the distance between the eye center and the upper or lower eyelid. In this situation, for eye opening measurement, first we find a pixel of the left or right eye X  X  boundary that is placed on the horizontal line that contains the eye center. Then, if the upper eyelid is not detected and the eye boundary, which contains the mentioned pixel, continues up to the upper side of the eye window; we consider the upper side of the eye window as the upper eyelid. In contrast, if the lower eyelid is not detected and the eye boundary continues up to the lower side of the eye window, we consider the lower side of the eye window as the lower eyelid.
 cannot detect the left or right eye X  X  boundaries; then, we cannot calculate the eye openness. 2.1.2. Eyebrow Constriction calculation section. For this purpose, we design a new algorithm that is shown in Fig. 6 .
 in first stage, we select a window above the eye window that its height and width are four-thirds of the height and width of the eye window. Thenceforth, we will be able to determine eyebrow candidates in eyebrow window using Eq. (3) that has appeared in Ioannou et al. (2007 )
Eyebrow _ Candidates  X  Dilate  X  Y  X  x , y  X  X  Erosion  X  Y  X  x , y  X  X  X  3  X  where Y ( x , y ), is the grayscale image of the eyebrow window. window will be converted to the binary image. Then, we carry out a dilation operator with a linear structuring element for connect-ing the small adjacent regions in the binary image. Thereafter, for eliminating false eyebrow candidates, we delete components that have small length in comparison to other components, and also we eliminate components that are located on the upper corners of the eyebrow window. Finally, we select the upper eyebrow candidate from remaining candidates as the Eyebrow Region. then the left eyebrow constriction will be equal to the width of the first right-side point of this line and the right eyebrow constriction will be equal to the width of the first left-side point of the center line (the meaning of right/left side is the right/left side of the face). The stages of Eyebrow Constriction Calculation are shown in Fig. 7 . 2.1.3. Mouth opening calculation
For calculating the amount of Mouth Opening, the primary stage is to detect the mouth region. The proposed algorithm for mouth region detection or indeed lips detection is exhibited in Fig. 8 .

Therefore, for detecting mouth (lips), the mouth window image is first transformed from RGB into the YCbCr color space.
We know that the color of the mouth region contains stronger red component and weaker blue component than other facial regions.
In other words, the chrominance component Cr is greater than Cb in the mouth region. So, the mouth map is constructed by Maglogiannis et al. (2009)
MouthMap  X  Cr 2  X  Cr 2 nCr Cb  X  2  X  4  X  where n  X  0 : 95  X  1 = k  X  The parameter k is the number of pixels within the face mask. For more details, see Maglogiannis et al. (2009 ).
 According to the mouth region detection ( Fig. 8 ), after applying
Eq. (4) on the face image, the Sobel edge detector on the mapped image is performed. Then, by connecting the adjacent lips X  edges using horizontal dilation and eliminating the false edges, we will be able to delineate the window that contains the lips X  edges.
Finally, the window bounding the lips X  edges in the mapped image is selected as the Mouth Window.

Now, we can locate the region of the lips in mouth window by choosing an appropriate threshold, which is obtained by
Threshold  X  Mean  X  Y  X  x , y  X  X  X  k where Y ( x , y ) is the Mouth Window. Moreover, k is a parameter that is used for giving the biggest value to the pixels that are located in lips X  region and its value is determined experimentally. Here, k  X  1.6.
 Fig. 9 shows the stages for lips detection.

Find the edge of the lips using Sobel Edge Detector and then dilate lips X  Perform thresholding algorithm on the window that contains lips edges.
Now, for calculating mouth opening value, we create a plot of the summing intensity, using horizontal integral projection on the lips region, as the first stage. If three maximums or more have been appeared in this plot, then the distance between the first and third maximum determines the value of mouth opening. When only two maximums are observed in the plot of the summing intensity, the gap between the two maximums is the measure of mouth opening. And if only one maximum is detected in this plot, the value of mouth opening is set to zero ( Chakraborty et al., 2009 ). Fig. 10 demonstrates an example of measuring the mouth opening.

In some cases, the aforementioned technique cannot calculate mouth opening properly. As a result, we propose a new algorithm for mouth opening calculation. In our algorithm, we consider some vertical lines from beginning to the end of the mouth region. Then, we scan these lines from top to the bottom and denote every pixel that a transition from light to dark regions exists around it as the inner region of the top lip. Also, we trace these lines from bottom to the top and denote every pixel with transition from dark to light regions as the inner region of the bottom lip. Finally, we calculate the distance between the inner region of the top and bottom lip in each line. The maximum of these distances is reported as the mouth opening value. Fig. 11 has depicted our algorithm X  X  steps for mouth opening calculation.
On about 44% tested images, the first technique calculates mouth opening more accurately than the second technique. For achieving high accuracy for mouth opening calculation, we used both mentioned algorithms for mouth opening calculation. Consequently, we choose the maximum reported value for mouth opening by both algorithms as the final mouth opening value. 2.1.4. Mouth corners displacement measurement
In this article, we propose a new algorithm for mouth corners detection. This algorithm includes four distinct stages that are shown in Fig. 12 .

According to Mouth Corners Displacement Measurement Algo-rithm, the first stage is to dilate lips X  region in binary mouth image for covering mouth corners. Thereupon, we dilate the lips X  region using a spherical structuring element to cover the mouth corners X  region. The mouth corners X  points are dark points in the dilated lips X  region. Thus, we design a map that gives high values to the dark points. Eq. (7) shows such a map MouthCornerMap  X  X  255 lum  X  x , y  X  X  6  X  7  X  where lum ( x , y ) denotes the luminance of the pixel that is located in ( x , y ) coordinate. Now, for detecting left mouth corner pre-cisely, we scan the columns of the mapped image from left to right and also we calculate the maximum value of luminance for each column. If the value of this maximum is greater than the one-third of the global maximum in the dilated lips X  region, the point where this maximum has occurred will be selected as the left mouth corner. By following the same procedure, we find the right mouth corner through scanning the columns from right to left.
 mouth center line, we must at first detect the mouth center line. For detecting mouth center line, two following situations exist: 2.2. Secondary features extraction are used as auxiliary attributes for precise recognition of emotions that have the same values for the primary features.
These features are: the Mouth Length, the Nose-Side Wrinkles, the Existence of the Tooth, the Eyebrows Slope, and the Lips
Thickness. In the following, we will describe why we choose these features and how we calculate those values. 2.2.1. Mouth length calculation
In some cases, the primary features for the Happy and Surprise emotions have the same value and consequently we cannot recognize these emotions precisely. Fig. 14 shows two faces that have the same value for the primary feature depicting Happy and Surprise emotions.

By investigating many images from different emotion databases, we select the mouth length as the distinctive feature between the happy and surprise emotions for images that is similar to Fig. 14 .
Because the mouth length decreas eswhenasurpriseemotionoccurs whereas it increases when a happy emotion is displayed. This feature is also used for better distinguishing between the happy and disgust emotions.

For calculating the mouth length, we compute the horizontal distance between mouth corners that has been found in the previous section. 2.2.2. Nose-side wrinkles detection
Fig. 15 depicts two faces that display fear and disgust emotions while having the same value for primary features.

We inspected many facial images that showed disgust emotion and we found that Nose-Side Wrinkles appear when the disgust emotion occurs. Therefore, we decided to design an algorithm for detecting the nose-side wrinkles. Fig. 16 expresses the proposed wrinkle detection algorithm.

For detecting nose-side wrinkles, we must find the window that contains the nose and its sides. For achieving this, we select a window that its length is equal to 0.6 of the face width and its width is equal to the distance between eyes X  midpoint and the above of the upper lip so that the eyes X  midpoint lies in the center of the upper side of this window (the eyes X  midpoint is the point that lies in the center of line that connects the eyes X  irises). We select this length for nose window because we experimentally found that this length is the suitable length for this window that can cover nose and nose-side wrinkle regions.

After finding nose windows, the candidates for the nose-side wrinkles are detected using the Sobel edge detector. Then, we divide the nose windows to three parts and eliminate all detected edges in middle part (because these edges do not belong to nose-side wrinkles). Finally, by deleting low-slope and small edges, the nose-side wrinkles will be detected in first and third parts of the nose window. Indeed, we determined proper thresh-olds for scope of slope and length of the nose-side wrinkles through investigation many facial images. For example, by these thresholds we eliminate vertical and horizontal edges because correct nose-side wrinkles are not vertical or horizontal. Fig. 17 depicts detected nose window and nose-side wrinkles. 2.2.3. Detecting the existence of the teeth
In some cases, the primary features for the Fear and Surprise emotions have the same value. Fig. 18 proves such a claim.
Based on observing many images containing fear emotion, we deduced that the existence of the teeth is a feature that distin-guishes the fear emotion from surprise. Fig. 18 is an instance of this situation.

For detection existence of the teeth, we select a window that its length is equal to half of the mouth length and its width is equal to the mouth opening value. Then, we find the middle of the mouth. Next, we count the number of dark pixels in this window.
Dark pixels are pixels that their luminance is less than half of the maximum of the luminance values in this window. Then, if the number of the dark pixels in the mentioned window is less than an appropriate threshold, then existence of the teeth is proved.
For example, when the number of dark pixels is less than one-third of all pixels in the mentioned window; then, existence of the tooth is concluded. Note that detecting the existence of teeth will be used only when the mouth opening value is not zero (when the mouth is open). 2.2.4. Eyebrow slope calculation Fig. 19 illustrates the same values of primary features for the Sad and Fear Emotions.

By investigating many pictures that showed sad and fear emotions separately, we select Eyebrow Slope as a feature that discerns the sad emotion from the fear one. In fact, the inner corner of the eyebrows rises up when the sad emotion occurs (in most cases). As a result, the value of the eyebrow constriction in the sad emotion and fear emotion will have the same value, but the outer corner of the eyebrow in the sad emotion is lower than it is in the fear emotion. Therefore, eyebrow slope in the sad emotion is greater than it is in the fear emotion ( Fig. 19 is an instance of this situation). For calculating the value of the eyebrow slope, we propose a formula that appears in Eq. (8)
Eyebrow _ Slope  X  5 U Eyebrow _ Height  X  Eyebrow _ Constriction the width value ( y -coordinate) of the inner corner of the eyebrow from the width value of the outer corner of the eyebrow. The eyebrow constriction value in the sad emotion has high value than its value in the fear emotion in some cases, thereupon, by summing the square of this value with Eyebrow_Height , the eyebrow slope value for the sad emotion will get high value than its value for the fear emotion. Finally, the eyebrow slope value will be computed for input image using Eq. (8) and this value will be used as an auxiliary feature for better recognition of the sad emotion. 2.2.5. Lips thickness calculation emotions have the same value and consequently we cannot recognize these emotions precisely. Fig. 20 shows two faces that have the same values for primary features while depicting Anger and Sad emotions.
 realized that lips X  thickness in the anger emotion has lower value than that of sad emotion when mouth opening value is equal to zero. Therefore, we use this feature to obtain better recognition for anger and sad emotions. In fact, we use lips X  thickness value and some other features such as mouth corners displacement, eye opening, mouth length, and eyebrow constriction values to propose an equation. In the formula presented in Eq. (9), we compute a value called  X  X  X ad Belief X  X  so that when sad belief has high values, then our belief for sad emotion occurrence will be higher than the situation in which sad belief has low values Sad _ Belief  X  5 U MCD 2  X  5 U EO  X  3 U ML  X  LT 2 EC 2  X  9  X  In Eq. (9), MCD is Mouth Corners Displacement value, EO is Eye Opening value, ML is Mouth Length value, EC is Eyebrow Constriction value, and finally LT is Lips Thickness value.
Finally, the Sad Belief value will be computed for the input image using Eq. (9) and this value will be used as an auxiliary feature for better recognition of sad emotion.

In the following of this section, we emphasize some of the ideas and contributions proposed in this section: 1. In the eye opening calculation algorithm, we used a hybrid skin detection algorithm for detecting eyelids so that we could detect eyelids position more precisely for determination of eye opening value. For more detail, the reader is referred to Ilbeygi and Shah-Hosseini (2010) . 2. For mouth opening calculation, we proposed a hybrid approach with the algorithm mentioned by Chakraborty et al. (2009) , which led to a more accurate mouth opening calculation. 3. For mouth corners displacement measurement, we proposed a novel algorithm that never had been seen in previous works. In our proposed algorithm, we used new a simple illumination mapping (Eq. (7)) for detecting mouth corners position. Then, we detected mouth center line in two different situations. As a result, mouth corners displacement was estimated. 4. For determination and calculation of secondary features new algorithms were proposed. Indeed, to the best of our knowl-edge, there exists no algorithm in the literature for detecting nose-side wrinkle detection, detecting the existence of the teeth, lips thickness calculation, and the like. As a result, we had to design new algorithms for detecting and/or estimating secondary features. 3. Fuzzification of facial features
After finding the facial features such as eye and mouth open-ing, we must fuzzify the values of these features for the fuzzy emotion inference. For achieving this goal, the measurements that we calculated for Mouth Opening, Eye Opening, and Eyebrow Constriction are encoded into five distinct fuzzy sets: VERY LOW,
LOW, MODERATE, HIGH, and VERY HIGH; and measurements we obtained on Mouth Corners Displacement is encoded into three distinct fuzzy sets: LOW, MODERATE, and HIGH. And finally, every secondary feature is encoded into two fuzzy sets: LOW and HIGH.
 We used the Gaussian membership function for the LOW, MODERATE, and HIGH fuzzy sets in fuzzification values of Mouth Opening, Eye Opening, and Eyebrow Constriction; also for MODERATE fuzzy set in fuzzification value of Mouth Corners
Displacement. For the VERY LOW fuzzy set in fuzzification values of Mouth Opening, Eye Opening, and Eyebrow Constriction; also for the LOW fuzzy set in fuzzification value of Mouth Corners
Displacement, we chose the Z-Shape membership function. In addition, we selected the S-Shape membership function for VERY High fuzzy set in fuzzification values of Mouth Opening, Eye
Opening, and Eyebrow Constriction; also for High fuzzy set in fuzzification value of Mouth Corners Displacement. For the value of the secondary features, we select Z-Shape and S-Shape mem-bership functions for LOW and HIGH Fuzzy sets, respectively.
Note that if mouth corners are located above the mouth center line, the sign of Mouth Corners Displacement will be positive; else its sign will be negative. The functions of the Gaussian, Z-Shape, and S-Shape membership functions have been depicted in Eqs. (10) X (12), respectively Z _ Shape  X  x , a , b  X  X  S _ Shape  X  x , a , b  X  X 
In Eqs. (10) X (12), the x is any continuous feature; also m , s , a and b are membership function parameters. For best performance, we need to determine the optimal values for these parameters. Details of these are discussed in Section 6 .

Indeed, we inspected many values of different facial features (primary and secondary), and understood that, when these values fall or ascend from a special value, then the belief for being very low or very high (respectively) for these values is 100%. So, we must choose a membership function that assigns 100% of belief for such values. Trapezoidal or S/Z-shaped memberships function is a suitable choice for satisfying this purpose. Also, we choose Gaussian membership function for intermediate functions (such as Low, Moderate, and so on), because distribution of their values is very similar to normal distribution, and Gaussian function is the best choice for modeling their distribution.
Besides, the better choice for membership functions is MFs that have minimum parameters, because we intend to tune these parameters using Genetic algorithm. So if the number of the parameters of selected MFs is minimum, then the search space for finding better parameters is reduced and we can tune MFs parameters precisely. Also, by reducing search space for MFs X  parameters, the required time for tuning parameters using GA is reduced significantly.

So the best choices for membership functions are S/Z-shaped for Very Low and Very High MFs and Gaussian for other MFs, because S/Z-shaped and Gaussian MFs have only two parameters (Trapezoidal MF has four parameters and triangular MF has three parameters).
 Finally, we summarize our aim for selecting S/Z-shaped and
Gaussian MFs as follows: 1. Fit the best beliefs to any value of extracted features. 2. Tune membership function parameter more precisely. 3. Reduce time of parameters tuning.

Every image that is delivered to our system is resized to 144 n 96 pixels, and then we determine minimum and maximum values for each feature in terms of pixel. So we selected proper ranges for each interval of every feature by investigating many facial images in different image databases. For example, we selected 20 pixels as the range of changes for mouth corners displacement, which is shown in Fig. 21 .

For fuzzification, three distinct fuzzy sets are used for the values of mouth corners displacement: LOW, MODERATE, and HIGH. The
S-Shape membership function is selected for both LOW and HIGH verbal variables whereas the Gaussian membership function is chosen for MODERATE verbal variable. A Genetic Algorithm (GA) is used to obtain the parameters of those membership functions, which are: a  X  2.7464 and b  X  4.5570 for LOW MF parameters,  X  5.4072 and s  X  2.2202 for MODERATE MF parameters and a  X  6.6039 and b  X  8.3584 for HIGH MF parameters. In Fig. 21 ,the three membership functions for fuzzification of mouth corners displacement using the aforementioned parameters are shown.
In Fig. 21 , the horizontal axis is the amount of mouth corners displacement in terms of pixel and the values on the vertical axis represents how much belief value is given to mouth corners displacement based on the rele vant membership function (0  X  0% and 1  X  100%). For example, if the amount of mouth corners displace-ment is 3 pixels, then the amount of belief according to the LOW,
MODERATE and HIGH MFs are equal to 95%, 55%, and 0%, respec-tively. Finally, we use these belief values in antecedent part of fuzzy rules for emotion recognition using our fuzzy inference system. It is mentioned that the fuzzification process using Gaussian membership functions has also been used by Contreras et al. (2010) . 4. Emotion classification using fuzzy inference system In this paper, we selected six basic emotions that proposed by
Ekman and Friesen (1978) , as basic emotion classes. These emotions are: Angry, Disgust, Fear, H appiness, Sadness, and Surprise. Pixels using fuzzified values of the facial features as the antecedent part. Two examples of these rules are shown in the following: antecedent parts. However, the antecedent parts could have up to seven parts of conditions using a combination of primary and secondary feature values. As mentioned earlier, 573 fuzzy rules are used for emotion recognition in the fuzzy inference system to increase the accuracy of the proposed recognition system. checking upon various image databases and finally defined our fuzzy rules, similar to the human inference (common knowledge) for emotion recognition. So, we defined fuzzy rules for emotion recognition completely based on human inference manner and tested integrity and correctness of these rules by experimenting with many facial images that included different emotions and checked whether these rules could recognize true emotions (that presented in the tested images) or no.
 we tested all these rules using many facial images that selected randomly from different image databases (in learning phases that performed manually and offline). Afterwards, we corrected some rules and added new rules to initial rules and finally we selected 537 fuzzy rules for emotion recognition.
 emotion recognition. In fact, we use the minimum operator instead of  X  X  X nd X  X  in antecedent part of the fuzzy rules for determining belief of the i th rule. Moreover, if n rules lead to the k th emotion, maximum of belief values for these rules is chosen as the final belief for k th emotion.

Opening is very high, X  X   X  X  X yebrow Constriction is very low, X  X   X  X  X outh Opening is very high, X  X  and  X  X  X outh Corners Displacement is low, X  X  are 75%, 70%, 65%, and 90%, respectively. Then, according to Rule 1 (mentioned above) and Mamdani-type implication operators for fuzzy emotion recognition, the minimum of the belief values (which is 65%) is selected as the belief for surprise emotion. Similarly, belief values for other five emotions using respective fuzzy rules are calculated. Consequently, we obtain a six-element vector such that each element represents the belief of one particular emotion.
 In the above example, we assumed that the belief of  X  X  X ye
Opening is very high X  X  is 75%, the meaning of this phrase is that  X  X  X ye opening X  X  (obtained by the eye opening calculation algorithm) belongs to the membership function for Very High verbal variable with 75% belief. In other words, the value of Very
High membership function for  X  X  X ye opening value X  X  is equal to 0.75. 5. Emotion recognition from partially occluded facial images
One of the main ideas in this article is how to deal with emotion recognition from partially occluded facial images. For this purpose, human inference mechanism is selected as the source of inspiration. For example, if the mouth is occluded with a shawl, eye opening is very high, and eyebrow constriction is very low; then, the person is supposed to have the Surprise or Fear emotion.
We can simulate this human inference using fuzzy inference system. Indeed, if we cannot detect a feature for any reason, then we will set all the membership values of the membership functions for this feature to the value of one. Because membership values come from the interval [0 1], and the value of one is the neutral value of the minimum operator in the antecedent part of the fuzzy rules; we actually eliminate the undetected features from inference system by setting their values to one. In other words, we recognize emotion by using only the detected features; consequently, we may be able to recognize several emotions that have the same attributes for the single image. One of the main reasons that we selected fuzzy inference system for emotion recognition is the intrinsic ability of such systems for inference even with incomplete input.

For example, if eyebrows in Fig. 22 were occluded for any reasons, we could guess that the emotion of this image is surprise.
We mask eyebrow region by a black rectangle and test our system by this image, then system recognizes surprise emotion with 80% belief for this image. Now, suppose that the mouth of the face shown in Fig. 22 was occluded with a shawl, then, we could guess that the emotion of this image is surprise or fear. To test our system in this situation, we mask mouth region of the this image by a black rectangle and then we present this image to our system and surprise and fear emotion were recognized with 80% belief for both of them. It means that our system correctly recognizes the emotion of this image, which is surprise or fear.

In conclusion, the proposed fuzzy system can deal with one of the main challenges of emotion recognition that is emotion recognition from partially occluded facial images. Here, we do not need to make any changes to the system for emotion recognition from partially occluded facial images, and only by setting all the membership values of the membership functions for non-detected features to the value of one, the system will be able to recognize emotion from such images.

Accuracy of our system for emotion recognition from partially occluded facial images depends on the accuracy and integrity of our fuzzy rule-based system for emotion recognition. If the fuzzy rules for emotion recognition are assumed to be complete and correct, then by setting some values of the antecedent part of the rules to one, the emotion(s) that are recognized by the system should be assumed to be correct and reliable. It is noted that when some of the features are not detected due to occlusion, the presence of more than one emotion may be concluded.

Finally, we test the proposed systems in different types of occlusion such as the situations where eye is occluded, mouth or eyebrows are occluded). For example, in the right image of Fig. 22 , if mouth is occluded with a shawl, then we may infer surprise or fear emotions simultaneously. As a result, if the system recog-nizes surprise and fear emotions with equal belief values, then it is concluded that the system works correctly in this situation. To do this, we occlude some of facial features such as eye, mouth, and eyebrows in the test images. Afterwards, the performance of the system is compared by human emotion recognition through subjective judgment. In this experiment, 435 test images are employed. Table 2 demonstrates the recognition rate for different situations of occluded images. 6. Membership function parameters identification
One of the effective factors that have an important influence on the final precision of fuzzy inference systems is the member-ship function parameters identification. In this paper, we find optimal (near-optimal) values for the membership function para-meters by employing Genetic Algorithms. Note that we perform the Genetic Algorithm only for finding membership function parameters for the primary features, and membership function parameters for secondary features are set manually. This is because secondary features only have two membership functions and we can easily select their parameters based on values-set of those. It is mentioned that parameter optimization by a genetic algorithm is not a new idea. But, as far as we know, it has not been used for parameters of an emotion recognition system.
We identify domains of membership functions (support set) by investigating the different facial images databases, as the first stage. Indeed, we investigate many facial images that represent different emotions and then we obtain minimum and maximum of feature values such as Eye Opening and the like. Subsequently, parameters of the Gaussian, the Z-Shape and the S-Shape mem-bership functions ( m , s , a , and b ) will form a chromosome. We select four features that three of them have five membership functions while one of them has three membership functions. And because each membership function has two parameters, then chromosome length will be equal to 36.

For calculating the fitness of each chromosome, first we manually label emotions of 102 facial images with belief values for the six emotions in the form of a vector. Then, we subtract calculated emotion vector from target emotion vector for each image. The sum of these values is defined as the total error of the chromosome. Afterwards, the inverse of the Euclidean norm of the total error (total error is a vector that has six components) is chosen as the chromosome X  X  fitness value.

Note that the emotion vector is a vector with six components that each component contains a belief value for one of the basic emotions. For example, if the elements (components) of this vector, which represent belief values for disgust, anger, surprise, fear, sad, and happy emotions, possess values 0, 0, 0, 0, 0, and 0.79, respectively; then, it is concluded by the system that the emotion of input image is happy. Consequently, every input image is assigned a vector that represents its true (target) emotion. At the same time, the proposed emotion recognition system calculates such a six-element vector for the input image.
As a result, the error of proposed system for the current input image is calculated by subtracting the values of the two mentioned vectors. The formula for calculating the fitness of each chromosome is expressed in Eq. (13), which is defined as the inverse of the calculated error
Chromosome _ Fitness  X  1 :
Here Vt i is the target emotion vector for i th facial image and Vc is the calculated emotion vector for i th facial image. Note that the subtraction Vt i  X  Vc i is done element by element and 9 Vt absolute vector of the six elements of the subtraction. The symbol : x : denotes the norm of x , which is equal to the Euclidean norm of the vector x (here, x is a six-element vector).

Moreover, the parameters of the employed Genetic Algorithm are depicted in Table 3 . This GA is used for finding optimal or near-optimal parameters of membership functions. 7. The RaFD faces database
The Radboud Faces Database (RaFD) ( Langner et al., 2010 )isa set of pictures of 67 models (including Caucasian males and females, Caucasian children, both boys and girls, and Moroccan Dutch males) displaying eight emotional expressions.

The RaFD is a high-quality face database, which contains pictures of eight emotional expressions. Accordingly to the Facial Action Coding System, each model was trained to show the following expressions: anger, disgust, fear, happiness, sadness, surprise, contempt, and neutral. Each emotion was shown with three different gaze directions and all pictures were taken from five camera angles simultaneously. In the following, a short overview of the RaFD image set is presented.

Coding System) coder to show each of the following emotional expressions: 1. happy 2. angry 3. sad 4. contemptuous 5. disgusted 6. neutral 7. fearful 8. surprised gaze directions (without changing head orientation): different camera angles simultaneously. Some examples of the
RaFD image set are shown in Fig. 23 . 8. Results and discussion implemented in two stages; one stage is for evaluating the proposed feature extraction algorithms whereas another stage is for evaluating final precision of the proposed fuzzy emotion recognition system.
 have performed experiments on 200 frontal facial images that were selected randomly from FACES ( Ebner et al., 2010 ), MMI ( Pantic et al., 2005 ), and RaFD ( Langner et al., 2010 ) facial expression databases. Specifically, we randomly selected 50 images from FACSE database, 50 images from MMI database, and 100 images from RaFD database. These 200 images have different emotions and expressions; thereupon, those cover all ranges of
Eye Opening, Eyebrow Constriction, Mouth Opening, and other facial features. Consequently, we comprehensively evaluate the proposed feature extraction algorithms with these diverse test images. Table 4 reports average Precision of the feature extraction algorithms over the test images explained above.

It should be noted that in Table 4 , the precision of a feature extraction algorithm is calculated according to the Precision X  X  formula, which is mentioned in the following:
Precision  X  d b  X  d  X  14  X  where b is the number of incorrect detections that an instance is positive, and d is the number of correct detections that an instance is positive. Aforementioned definitions for b and d are used for calculating the precision for  X  X  X ye Detection, X  X  Eyebrow Detection, X  X   X  X  X outh Corners Detection, X  X   X  X  X outh Center-Line Detection, X  X   X  X  X ose Window Detection, X  X  and  X  X  X ose-Side Wrinkles Detection. X  X 
However, for computing the precision for  X  X  X outh Opening Calcu-lation X  X  and  X  X  X ye Opening Calculation, X  X  b is the number of incorrect calculations and d is the number of correct calculations.
As mentioned earlier, we selected RaFD ( Langner et al., 2010 ) facial images database for evaluating the performance and accuracy of the fuzzy emotion recognition system. This database contains 8040 facial images with differen t emotions in total. Out of 8040 images, 414 frontal images are selected, which include six different emotions and two gaze directions. Consequently, these 414 frontal images are used for evaluation of the fuzzy emotion recognition system. The results of emotion recognition using the fuzzy system have been presented in Table 5 in the form of confusion matrix.
The method for calculating the accuracy of our approach for emotion recognition is explained in following: we have 69 distinct images for every of the six basic emotions, which totally form a set of 414 images. We deliver each image of the 69 images of one special emotion to the fuzzy system. The fuzzy system then generates a six-element vector such that each element of the vector is a belief value for one of those six emotions. Later, if belief value of an emotion is greater than belief values of other five emotions we report this emotion as the winner and mark this image with the winning emotion. In addition, if belief value of a winning emotion is greater than belief values of other emotions but this belief is less than 0.1% or 10%, then, we mark the given image as the image that the fuzzy system cannot recognize its emotion. That is why in Table 5 , the summation of the percen-tages expressed in a row may not be equal to 100%.

For example, for Fear emotion, we presented 69 images to the fuzzy system and following the aforementioned approach, emo-tions of 65 images out of 69 ones were correctly recognized. Thus, recognition rate for Fear emotion becomes (65/69) 100% or 94.20%. Moreover, because emotion of one image was wrongly recognized as sadness instead of fear, then (1/69) 100% or 1.44% is the false recognition rate for sadness emotion. Also, emotions of the other two images were recognized as surprise, which leads to the value (2/69) 100% or 2.89% for false recognition rate of surprise emotion. Finally, since emotion of one image was recognized as fear but belief of this recognition is less than 10%, we eliminate this image from the set of correctly recognized images. As a result, the summation of the percentages of recog-nized emotions for this experiment becomes less than 100% as reported in third row of Table 5 .

At this stage, we intend to compare the accuracy of the proposed fuzzy system with K-nearest neighbor classification (KNN; Mitchell, 1997 ). In the KNN classifier, centers of classes must be determined firstly. Here, we have six classes such that each class is related to one emotion out of total six basic emotions. Center of each class is determined by the Fuzzy C-Mean clustering (FCM; Bezdek, 1973 ). Eventually, we choose k  X  6 and perform KNN classification on the 414 frontal facial images selected for experiment reported in Table 5 . The results of emotion recognition using the KNN are presented in Table 6 .
By comparing between Tables 5 and 6 , the superiority of the proposed fuzzy emotion recognition over the KNN classifier for emotion recognition is revealed.

Table 7 presents a brief comparison between our method and five other emotion recognition systems in the literature.
According to Table 7 , we can see that the accuracy of our method never falls below 91.30% for any expression and average of the recognition rate for our emotion recognition system is higher than other emotion recognition systems.

Note that we cited Table 7 from Khanum et al. (2009 ) and we emphasize that the image databases used for evaluating mentioned algorithms in Table 7 are widely different from our image database. It is worth mentioning that the performance and accuracy of existing methods for emotion recognition are not exactly comparable. This is because each method reports its accuracy for emotion recognition with a specific image database.
And so far, a benchmark database for evaluating the accuracy of emotion recognition systems is not available.

It should be noted that the proposed fuzzy emotion recognition system has been implemented in the MATLAB environment. The average time that is required for recognizing the emotion of a single image on a 2.8 GHz dual-core processor with 3 Gigabyte RAM is approximately equal to 0.4 s. Actua lly, our proposed fuzzy system has two major stages: fuzzy membersh ip function parameters determi-nation with GA that is executed once and offline, and facial expression recognition stage that contains feature extraction and classification phases. The aforementioned time (0.4 s) includes all elapsed times for the second stage of our facial expression recognition system.
The interface of our application for emotion recognition is depicted in Fig. 24 .

Note that, the meaning of 79% that appears in Fig. 24 is not equal to emotion recognition rate for  X  X appy X  emotion, but its value shows the calculated belief value for occurrence of happy emotion in the given image. 9. Conclusions and future works
In this paper, we presented a novel fuzzy-based approach for emotion recognition, which is also able to work with partially occluded facial images. In comparison to other emotion recognition systems, it demonstrates better r ecognition rates. Moreover, to deal with partially occluded images, the maximum belief value, which is the value  X  X  X ne X  X , is assigned to the occluded features in the relevant antecedent parts of fuzzy rules. With this simple and efficient trick, we gain a high accuracy in emotion recognition from partially occluded images such that the results are comparable to subjective judgment.
Furthermore, the performance and accuracy of these new feature extraction algorithms were evaluated by 200 images from three different emotional image databases in order to become sure of the reliability of the proposed feature extraction algorithms. In addition, we divided facial features into two major categories:
Primary Features and Secondary Features. Afterwards, we showed that with this categorization, we will be able to recognize different emotions more precisely. For example, if we do not use secondary features for emotion recognition, then the recognition rate for some of emotions (such as fear or sadness) falls below 60%. emotion recognition system, we determined the values of para-meters of the fuzzy membership functions using a customized
Genetic Algorithm. Nevertheless, the proposed feature extraction algorithms may not be able to show high performance in dealing with facial images that have rigid head motion, wide variation on illumination conditions, and the like.
 performance and precision by employing robust algorithms for facial features extraction related to variations such as rigid head motions and illumination conditions. Additionally, we should employ other optimization algorithms such as Particle Swarm Optimization (PSO), Intelligent Water Drops (IWD) algorithm ( Shah-Hosseini, 2007 ; Shah-Hosseini, 2008 ; Shah-Hosseini, 2009 ), or Galaxy-based Search
Algorithm (GbSA) ( Shah-Hosseini, submitted for publication )for optimizing parameters of membership functions. Moreover, we may be able to model an emotion recognition system using other fuzzy modeling methods such as Active Learning Method (ALM), Takagi-Sugeno, and Sugeno-Yasukawa.
 References
