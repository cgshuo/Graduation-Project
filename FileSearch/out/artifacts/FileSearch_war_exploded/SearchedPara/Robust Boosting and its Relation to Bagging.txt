 Several authors have suggested viewing boosting as a gradi-ent descent search for a good fit in function space. At each iteration observations are re-weighted using the gradient of the underlying loss function. We present an approach of weight decay for observation weights which is equivalent to  X  X obustifying X  the underlying loss function. At the extreme end of decay this approach converges to Bagging, which can be viewed as boosting with a linear underlying loss function. We illustrate the practical usefulness of weight decay for im-proving prediction performance and present an equivalence between one form of weight decay and  X  X uberizing X   X  a statistical method for making loss functions more robust. G.3 [ Mathematics of Computing ]: Probability and Statis-tics; I.5.2 [ Pattern Recognition ]: Design Methodology Algorithms Boosting, Bagging, Robust Fitting
Boosting [9, 8] and Bagging [3] are two approaches to combining  X  X eak X  models in order to build prediction mod-els that are significantly better. Much has been written about the empirical success of these approaches in creating prediction models in actual modeling tasks [4, 1] . The the-oretical discussions of these algorithms [4, 11, 12, 5, 16] have viewed them from various perspectives. The general theo-retical and practical consensus, however, is that the weak learners for boosting should be really weak, while the  X  X eak learners X  for bagging should actually be strong. In tree ter-minology, one should use small trees when boosting and big Copyright 2005 ACM 1-59593-135-X/05/0008 ... $ 5.00. Where  X  X ood X  is defined as making the empirical loss small:
The actual incremental algorithm is an exact or approxi-mate coordinate descent algorithm. At iteration t we have the  X  X urrent X  fit F t , and we look for the weak learner h t which maximizes the first order decrease in the loss, i.e., h t maximizes or equivalently and more clearly it maximizes which in the case of two-class classification is easily shown to be equivalent to minimizing Where w i = learner which minimizes weighted error rate, with the weights being the gradient of the loss. If we use the exponential loss: then it can be shown (e.g. [13]) that (1) is the exact clas-sification task which AdaBoost [9], the original and most famous boosting algorithm, solves for finding the next weak learner. In their original AdaBoost implementation [8], Fre-und and Schapire suggested solving (1) on a  X  X ew X  training data set of size n at each iteration, by sampling from the training dataset  X  X ith return X  with probabilities propor-tional to w i . This facilitates the use of methods for solving non-weighted classification problems for approximately solv-ing (1). We will term this approach the  X  X ampling X  boosting algorithm. [10] has argued that sampling actually improves the performance of boosting algorithms, by adding much needed randomness (his approach is to solve the weighted version of (1) on a sub-sample, but the basic motivation applies to  X  X ampling boosting X  as well). Here is a formal description of a sampling boosting algorithm, given the in-puts described above:
Algorithm 1. Generic gradient-based sampling boosting algorithm 1. Set  X  0 = 0 (dimension of  X  is |H| ). 2. For t = 1 : T , Comments:
Proposition 2. Any loss which is a differentiable, con-vex and decreasing function of the margin has the property: And a linear loss is the only one which attains equality  X  m 1 , m 2 .

Proof. Immediate from convexity and monotonicity. [16] have used generalization error bounds to argue that a good loss for boosting would be even more robust than the linear loss, and consequently non-convex. In particular, they argue that both high-margin and low-margin observa-tions should have low weight, leading to a sigmoid-shaped loss function. Non-convex loss functions present a significant computational challenge, which [16] have solved for small dictionary examples. Although the idea of such  X  X utlier tol-erant X  loss functions is appealing, we limit our discussion to convex loss functions, which facilitate the use of standard fitting methodology, in particular boosting.

Our view of bagging as boosting with linear loss allows us to interpret the similarity  X  and difference  X  between the algorithms by looking at the loss functions they are  X  X oost-ing X . The linear loss of bagging implies it is not emphasizing the badly predicted observations, but rather treats all data  X  X qually X . Thus it is more robust against outliers and more stable, but less  X  X daptable X  to the data than boosting with an exponential or logistic loss.
The view of bagging as a boosting algorithm, opens the door to creating boosting-bagging hybrids, by  X  X obustify-ing X  the loss functions used for boosting. These hybrids may combine the advantages of boosting and bagging to give us new and useful algorithms.

There are two ways to go about creating these intermedi-ate algorithms: The two approaches are obviously equivalent through a dif-ferentiation or integration operation. We will adopt the  X  X eight decay X  approach, but will discuss the loss function implications of the different decay schemes.
We would like to change the loss C (  X  ,  X  ) to be more robust, by first decaying the (gradient) weights w i , then considering the implicit effect of the decay on the loss. In general, we assume that we have a decay function v ( p, w ) which depends on a decay parameter p  X  [0 , 1] and the observation weight w  X  0. We require: This transformation is attractive because it does not entail the arbitrary  X  X hreshold X  determination, but rather decays all weights, with the bigger ones decayed more. However, it is less interpretable in terms of its effect on the underlying loss. For the exponential loss C ( m ) = exp(  X  m ), the power transformation does not change the loss, since [exp(  X  m )] p = exp(  X  mp ). So the effect of this transformation is simply to slow the learning rate (exactly equivalent to decreasing  X  in Algorithm 1).
Our suggested  X  X uberizing X  transformation in (6) to the original loss function (in our case, the exponential loss (2) or the logistic loss (3)) gives a modified loss which is linear for small margins, then as the margin increases starts behaving like the original loss. We can therefore interpret boosting with the huberized loss function as  X  X agging for a while X , until the margins become big enough and reach the non-huberized region. If we boost long enough with this loss, we will have most of the data margins in the non-huberized region, except for extreme outliers, and the resulting weight-ing scheme will revert to the  X  X tandard X  boosting weights, except for extreme outliers.

We can thus consider boosting with the huberized loss to be more robust in the following sense:
This robustness argument is related to that of [11] and others for boosting with the logistic loss (3) rather than the exponential (2), since the logistic loss is approximately linear for negative margins (as its second derivative vanishes when the margin goes to  X  X  X  ). However, the logistic loss is very similar to the exponential for non-negative margins, as shown in [18]. Huberizing, on the other hand, allows for a flexible  X  and specific  X  definition of a linear region which can be adapted to the modeling task at hand.

It is interesting to note that two desirable properties of boosting loss functions are maintained for their huberized versions: This property states that if the hypothesis space of weak learners allows  X  X eak learning X   X  that is, weighted error rate of the weak learners of at most 1 / 2  X   X  for each iteration  X  then the training margins will increase, and generaliza-tion error will go to 0 for large enough training samples (see [20] for more details). Duffy and Helmbold ([7], Theorems 2-4) give sufficient conditions for this property to hold, which apply to the exponential and logistic loss functions. These conditions include strict convexity, and so do not directly apply to the huberized versions. However closer inspection of their conditions exposes that we only need to re-state their Theorem 2, which states that all the (non-normalized) mar-gins are guaranteed to move beyond a fixed point U within O ( n 2 / X  2 ) iterations, and never cross it back. We re-prove this result for Huberized loss functions, and take U = m  X  bigger than  X  2 n , but the improvement in loss is guaranteed to be at least that attained by taking this fixed step size.
Now, we observe that our initial loss is nC ( p ) (0) and thus within this number of iterations: we are guaranteed to have a non-positive loss if we still have loss bigger than C ( p ) ( m  X  ( p )). This obviously gives a contra-diction and we conclude that the total loss after T iterations gives the desired result:
Since the overall loss always decreases in every iteration in line-search boosting, it can never exceed C ( p ) ( m  X  ( p )) in sub-sequent iterations. Since the loss function is non-negative decreasing, no margin can ever be smaller than m  X  ( p ) in subsequent iterations.
 [19] give sufficient conditions for regularized loss functions to be l p -margin maximizing. This sufficient condition holds for the logistic and exponential loss functions, and also for their huberized versions, since it only depends on the loss function X  X  behavior as the non-normalized margin converges to  X  . [18] explain how this property extends approximately to boosting. In essence, it implies that the robust boosting algorithm is seeking to maximize the l 1 margin of the data examples, and under quite general conditions will succeed in doing so.
We now discuss briefly some experiments to examine the usefulness of weight decay and the situations in which it may be beneficial. We use three datasets: the  X  X pam X  and  X  X aveform X  datasets, available from the UCI repository [2]; and the  X  X igits X  handwritten digits recognition dataset, dis-cussed in [15]. These were chosen since they are reasonably large and represent very different problem domains. Since we have limited the discussion here to 2-class models, we se-lected only two classes from the multi-class datasets: wave-forms 1 and 2 from  X  X aveform X  and the digits  X 2 X  and  X 3 X  from  X  X igits X  (these were selected to make the problem as challenging as possible). The resulting 2-class data-sets are reasonably large  X  ranging in size between about 2000 and over 5000. In all three cases we used only 25% of the data for training and 75% for evaluation, as our main goal is not to excel on the learning task, but rather to make it difficult and expose the differences between the models which the different algorithms build.

Our experiments consisted of running Algorithm 1 using various loss functions, all obtained by decaying the observa-tion weights given by the exponential loss function (2). We used  X  X indsorising X  decay as in (5) and thus the decayed versions correspond to  X  X uberized X  versions of (2). In all our experiments, bagging performed significantly worse than all versions of boosting, which is consistent with observations made by various researchers, that well-implemented boost-ing algorithms almost invariably dominate bagging (see for example Breiman X  X  own experiments in [4]). It should be clear, however, that this fact does not contradict the view confidence bounds. there are a lot of interesting theoretical and practical ques-tions that should come into consideration when we design such an algorithm, such as:
In this context, it is interesting to note some previous work on bounding the boosting weights to achieve more robust performance by [6]. The main difference from our approach is that they operate on the re-normalized AdaBoost weights. Their approach thus lacks the gradient descent interpreta-tion in a new loss, since AdaBoost X  X  re-normalization of the weights is equivalent to re-scaling the underlying loss. Thus, [6] X  X  approach amounts to Huberizing the exponential loss at a different point in each iteration. Their algorithm also lacks a guaranteed boosting property like the one we proved in Section 4. However it would be interesting to compare the empirical merit of their approach to ours and perhaps draw conclusions with regard to using  X  X daptive X  robust loss functions.
Jerry Friedman, Trevor Hastie and Ji Zhu contributed to this paper through useful discussions and advice. [1] E. Bauer and R. Kohavi. An empirical comparison of [2] C. Blake and C. Merz. Repository of machine learning [3] L. Breiman. Bagging predictors. Machine Learning , [4] L. Breiman. Arcing classifiers. Annals of Statistics , [5] P. Buhlmann and B. Yu. Analyzing bagging. Annals [6] C. Domingo and O. Watanabe. Madaboost: A
