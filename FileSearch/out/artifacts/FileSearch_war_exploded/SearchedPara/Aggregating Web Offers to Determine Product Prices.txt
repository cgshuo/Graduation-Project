 Historical prices are important information that can help consumers decide whether the time is right to buy a prod-uct. They provide both a context to the users, and facilitate the use of prediction algorithms for forecasting future prices. To produce a representative price history, one needs to con-sider all offers for the product. However, matching offers to a product is a challenging problem, and mismatches could lead to glaring errors in price history. We propose a prin-cipled approach to filter out erroneous matches based on a probabilistic model of prices. We give an efficient algorithm for performing inference that takes advantage of the struc-ture of the problem. We evaluate our results empirically using merchant offers collected from a search engine, and measure the proximity of the price history generated by our approach to the true price history. Our method outperforms alternatives based on robust statistics both in tracking the true price levels and the true price trends.
 H.2.8 [ Database Management ]: Database Applications X  Data Mining ; J.4 [ Social and Behavioral Sciences ]: Eco-nomics Algorithms, Experimentation Data Aggregation, Time Series Analysis
The Internet has become one of the most important sources of information for consumers researching for their next pur-chase. While there are many e-commerce websites dedicated to helping consumers decide what product to buy and where to buy it, few sites exist to help with the question of when to buy the product. In [1, 2], the authors proposed a sys-tem that addresses this need by providing users with prod-uct price history and making recommendations of buy or wait depending on price forecasts and consumer preferences. In their experiments, product prices are obtained through a data vendor. In this paper, we investigate the question of how to aggregate web offers to determine representative product prices, thus circumventing the explicit dependence on data vendors.

If the set of all offers correctly matched to the product were given as input, determining the product prices would have been straight-forward: one can simply take the average of the prices (or the minimum, depending on the applica-tion). Unfortunately, matching offers to products is known to be a difficult problem, and despite much work in the area (reviewed further in Section 2), it is inevitable that a match-ing algorithm will make mistakes. We are thus interested in the following question:
Note that to produce a good and representative price his-tory, it is not necessary (although it would be sufficient) for an algorithm to identify all of the incorrect matches. For example, if we are interested in the average product price, an incorrectly matched offer with a price close to the av-erage will be mostly harmless. Errors are introduced when we fail to identify incorrect matches with prices significantly different than the average (or in the case of minimum, sig-nificantly lower than the true minimum).

As the main challenge in aggregating web offers is due to incorrect offers with prices that are outliers, could the problem be solved using robust statistics [12]? Focusing on determining the average price, for example, could we replace the simple average of product prices by the trimmed mean, where the top and bottom x % of the data points are dis-carded before the average is computed, or by the median, often considered to be robust to outliers? It turns out that these solutions are inadequate for two reasons.

First, errors made by matching algorithms are typically not random. A common form of error is confusing one prod-uct with another. For example, confusing two models of TVs made by the same manufacturer, or confusing the ac-cessories of a product with the product itself. This form of errors leads to entire sets of offers being (mis)matched to the same product. Another common form of error is due to products that are configurable. For example, a digital SLR camera may be packaged together with different lenses. The different configurations may be sold at very different prices. Under both form of errors, trimmed mean will remove some offers from both the most and the least expensive sets, but the resulting price will likely remain a mixture of multiple set of offers, and the final price is not representative of a single product. The median will select an offer from one particular set of offers, but as the composition of offers changes over time, it may select an offer from a different set at in the future, leading to inconsistencies and creating discontinuity in the price history when none is present.

Second, a direct application of robust statistics fails to take advantage of the sequential nature of the data. Using all the matched offers across time can help in two ways. First, determining whether an offer is correctly matched or not at an isolated time point is difficult; a valid price can easily be confused with an outlier. By taking into account the entire price history of an offer, we improve our ability to decide whether the offer itself is an outlier. Also, in trying to disambiguate between multiple clusters of prices (with similar number of offers), we can use the historical prices in deciding which cluster is most likely to be correct.
To address these shortcomings, we propose to model offer prices by a generative process motivated by linear Gaus-sian processes [21]. We model the true product prices as unobserved latent variables that follow some linear dynam-ics. We associate each offer with a Bernoulli variable that determines whether the offer is correct for this product. If it is correct, the offer prices will be drawn according to a Gaussian distribution with a mean equal to the underlying product prices (and variance to be learned, as different of-fers exhibit different price volatilities); otherwise the offer prices will be sampled from a background distribution. This model allows us to take advantage of the sequential nature of the data, and by explicitly modeling whether an offer is correctly matched, the solution is sensitive to the type of errors typically made by matching algorithms described above. To ensure efficiency of our learning algorithm, we design an inference algorithm that takes advantage of an in-dependence assumption and allows us to achieve a quadratic speed-up over standard Kalman filter. The technique may be of independent interest.

The rest of the paper is organized as follows. We review related work in offer matching, Kalman filters, and deciding when to buy in Section 2. We then present a probabilistic model of offer prices in Section 3. We consider how to learn the model parameters in Section 4. To speed-up the learning algorithm, we introduce a technique that makes use of the independence assumptions in the model in Section 5. We evaluate the performance of algorithm using merchant offers collected from a search engine over six months in Section 6. We conclude with our main findings and directions for future research in Section 7.
Matching offers to products is a central problem in ecom-merce [16]. Offers for products are often partly structured (including URLs, prices, and sometimes Universal Product Codes (UPCs)) and partly unstructured (offer titles and textual descriptions). There has been much work in the database and data mining communities on matching struc-tured records to other structured records, including record linkage [8, 19, 20, 25], entity resolution [3, 23], and dupli-cate detection [7, 22]. There has also been some work on matching unstructured text to structured records that uses techniques based on natural language processing [18], seg-mentation [16, 17], and clustering [4]. Our work builds on these past works and assumes that an initial matching of offers to products has been performed, and takes the out-put and creates representative product prices for the prod-ucts. As part of the process, we need to identify incorrect matches where the prices are significantly different than oth-ers. While prices can be used as part of the matching pro-cess, to our knowledge, there has been no work that uses the sequentiality of the prices in determining the correctness of a match. Our work complements the existing work and can be used to handle matching scenarios where some attribute values of a record are slowly changing over time.

The main modeling technique in this paper is by modeling the offer prices as observations generated from a linear dy-namical system (also known as linear Gaussian processes). A good summary of this research area from the machine learn-ing perspective is given in [21]. The study of linear dynam-ical dates back to early work in signal processing, with the Kalman filter being one of the seminal work in the area [14]. Learning the parameters to a linear dynamical system, also known as system identification, has also been well studied and a review is given in [9]. Many extensions to Kalman fil-ters have been proposed, for example, the extended Kalman Filter [26] and particle filters [11, 13, 15]. While these work relax various assumptions on the linearity of the process and provide efficient algorithms for learning, their models do not consider the possibility of erroneous observations.
In our work, we augment the classical linear dynamical model by allowing for observations to be possibly incor-rect. This is related to the robust Kalman filter [27] and Kalman filter with intermittent observations [24]. For the latter work, using a cutoff criteria, one can treat a single outlier as a missing observation. However, in both lines of works, the basic modeling units are individual data points, hence the learned model will not take into account the en-tire offer (consisting of all offer prices) in deciding whether an observation is an outlier. Closer in spirit to our work is the switching state-space model proposed in [10]. One can view the background prices in our model as a separate state-space model, and that the switching variable of [10] controls whether a set of observations is generated by the background prices or the product prices. However, the learned model will instead group the correct and incorrect observations by time; we are interested in grouping them by offer. As a final note, the technique proposed in Section 5 to speed up the computation of the Kalman Filter under independent obser-vation assumptions is novel to our knowledge, and could be of independent interest.

There has been recent work that investigates the question of helping users to decide when to buy a product [1, 2]. The present study extends this line of work and considers how to obtain representative product prices automatically by aggregating web offers. The history can then be presented to users in response to product queries, be fed as input to forecasting algorithms.
We now present a generative model of offer prices moti-vated by linear Gaussian processes. Informally, the model servations, which include all of the offer prices o 1 , o . The unshaded nodes are latent variables in the postulates that there is a set of true but unobserved prices for the product, one for each time step, and that the prices evolve according to some yet-to-be-learned linear dynamics. A set of offers are purportedly matched to the product. An offer is either correctly matched, in which case its prices are drawn according to a distribution parameterized by the product prices, or incorrectly matched, in which case its prices are drawn according to some background price distri-bution. Our goal is to learn the the underlying prices and the correctness of the matching treating the offer prices as observations.

Formally, we consider a discrete-time generative model as illustrated in Figure 1. Let the true but unobserved prod-uct prices be denoted p = { p 1 ,p 2 ,...,p n } . The prices are generated by a linear Gaussian process, determined by four parameters:  X  0 ,  X  2 0 ,  X  , and  X  2  X  , where and N (  X , X  2 ) denotes a Gaussian distribution with mean  X  and variance  X  2 .

Let the number of offers matched to the product be k , which may vary from product to product. For the i -th of-First, an unobserved Bernoulli variable q i , which we call the offer status , is drawn according to parameter  X  i . If q 1, the offer is correctly matched, and the observed offer price o at time t is drawn according to a Gaussian distribution with mean p t and variance  X  2 i , which we call the observa-tion variance of offer i . On the other hand, if q 0, the offer is incorrectly matched, and the observed offer price o i t at time t is drawn according to a Gaussian distribu-tion with mean  X  b and variance  X  2 b that corresponds to some background distribution of prices. We treat this background distribution as exogenously given, and do not learn them in the model as this distribution is estimated using all offers matched to the products of a given category.

Summing up, for each offer i ,
Under this model, the probability of a set of observed prices O = { o 1 , o 2 ,..., o k } can be expressed as a function of the parameters  X  = (  X  0 , X  2 0 , X , X  2  X  , {  X  i } background mean and variance  X  b and  X  2 b by summing out the latent variables, namely, where
P ( O , p , q ;  X ) = P ( p 1 ;  X  0 , X  2 0 )
The parameters of the model can be learned, once for each product, by treating the offer prices as observations; this will be discussed further in Section 4. The learned parameters together with the observations can then be used to jointly infer the most likely product prices. In addition, the param-eters  X  i  X  X , corresponding to the likelihood of an offer being correctly matched, can be used to determine the set of cor-rect matches by comparing its value to a cutoff threshold. The offers deemed correctly matched by the model can then be used to compute the average or the minimum product prices as desired.

Note that in a real running system, offer prices may be missing at times for various reasons, hence we need to make provisions in the generative model to allow for missing ob-servations. We assume that with some fixed probability r an observation may be missing, independent of whether the offer is correctly matched or not. This assumption allows us to simplify the derivation of the inference procedure, as the probability of a missing observation can be factored out from the likelihood function in Equation (1).
At runtime, given the offer prices, we can learn the values of the parameters of the model by maximizing the likelihood function. As the model consists of both latent and observed variables, we can generalize the Expectation-Maximization (EM) framework [6] to learn the parameters. Following stan-dard EM derivation (see, e.g., Chapter 9 of [5]),
Due to the interactions between the latent variables q and p , an exact inference is intractable in the E-step. Instead, we consider an approximate inference procedure by variational methods, discussed further in Section 4.1. The optimization of parameters in the M-step are fairly straightforward, and for completeness we present them in Section 4.2. The overall approach is motivated by the learning algorithm for switch-ing state-space models [10], with adaptations to account for the differences in the model.
Given the observations O , the distribution of the latent product price variables p and latent offer status variables q are not independent; this can be verified from the graphical structure of the model (Figure 1) that these variables are not d -separated given the observations. In order to perform efficient inference, we apply variational approximation in the E-step.

Specifically, we focus on the following structural approx-imation to Q ( j ) ( p , q ) that decouples the variables p and q , viz.
 We then find the factors Q ( p ), Q ( q ) that minimizes the KL-divergence to the correct non-independent distribution P ( p , q | O ;  X  old ). Using the general result from Chapter 10 of [5], this can be found by iteratively solving the following system until convergence: where the constants ensure that the factors Q ( p ) and Q ( q ) integrates to one.

From Equations (2) and (1), we can verify that log Q ( q ) can be expressed as an exact factorization over the individual offer statuses, i.e., where
Solving for Q ( q i ) for each offer i , we find that which has the natural interpretation that the log odds be-tween Q ( q i = 1) and Q ( q i = 0) is precisely the ratio of the log likelihoods of offer i being generated by the product prices or through the background distribution.

From Equations (3) and (1), we can verify that log Q ( p ) can be expressed as an exact factorization according to the linear dynamics that underlie the product prices, i.e., log Q ( p ) = log Q ( p 1 ) + where log Q ( p 1 ) = E q [log P ( p i | O , q ;  X )] + constant and for 1 &lt; t  X  n ,
As that the factorization preserves the structure of the dy-namics, we can apply the forward-backward algorithm (also known as Kalman filter and smoother) for inferring the la-tent variables given the parameters. To take into account the offer statuses q (over which we are taking expectations), it is necessary and sufficient to divide the observation vari-ance  X  2 i of offer i by Q ( q i = 1) in the algorithm (the same has been observed in [10]). This has the effect of decreasing the influence of offer i to the product prices if the likelihood of offer i being correctly matched is low.

Note that just as in the standard derivation of inference for linear dynamical systems, instead of determining the dis-tribution Q ( p ) exactly, we only determine the required ex-pectations under the distribution. We follow the notations and derivations from [5] with suitable modifications. For-mally, let 1 k denote a column vector of ones of length k , and 1 k  X  k denote a square matrix of all ones of size k  X  k ( not to be confused with the identity matrix of size k  X  k ). Let  X  be a k  X  k matrix that corresponds to the modified observation variances, with the i -th diagonal entry equal to Q ( q i =1) , and zero elsewhere. Let o t denotes all observed of-fer prices at time t . The forward stage, corresponding to the Kalman filter, can be computed iteratively from t = 1 to n , where for t = 1, and for 1 &lt; t  X  n ,
The backward stage, corresponding to the Kalman smoother, can be computed iteratively from t = n  X  1 down to 1, where
For both the variational E-step and the subsequent M-step, the required expectations are given by
To sum up, in the variational E-step, we alternate between computing Q ( p ) and Q ( q ) until convergence based on the system of equations described in Equations (2) and (3). The factor Q ( p ) is not determined explicitly, but rather we use the forward-backward algorithm to determine the required expectations. The factor Q ( q ) is determined explicitly using the log odds described in Equation (4).
 Note that the most expensive computation step in the E-step is inverting the k  X  k matrices (  X  2 0 1 k  X  k ( p t  X  1 1 k  X  k + X ) in computing the Kalman gain matrices K the forward stage. Note that these are not diagonal matrices and hence a direct computation of the inverses will take time O ( k 3 ). However, one can take advantage of the structure of these matrices to avoid the need to take inverses explicitly. This is discussed further in Section 5.
In the M-step, we learn the parameters  X  = (  X  0 , X  2 0 , X   X  served data. The optimization can be solved analytically by taking the derivative of each of the parameters and setting it to zero. For completeness, the updates to the parameters are and for 1  X  i  X  k , Putting it altogether, the learning algorithm is given in Algorithm 1.

Algorithm 1: EM Algorithm for learning parameters to generative model of offer prices input : Offer prices matched to a product O output : Parameters to the model  X 
Initialize the values of  X  (random or by heuristics); repeat until convergence ; return  X ;
As mentioned in Section 4.1, the dominant computation in the learning algorithm is inverting the matrices needed in the forward stage of the inference algorithm. We now consider how this step can be sped up by avoiding the explicit need to compute the inverse.

As this technique may be of independent interest, we de-rive it in a case more general than the one considered in Sec-tion 4.1. Let s 0 be some scalar quantity, c = [ c 1 ,c 2 be a column vector of length k , and D be a diagonal matrix of size k  X  k with the diagonal elements being s 1 ,s 2 ,...,s Consider the matrix M formed by For convenience of notation, let c 0 = 1. We can show by induction the following lemmas related to the matrix M . Lemma 1. The determinant of M equals Lemma 2. The inverse of M equals Lemma 3. The product of vector a T = [ s 0 c 1 ,s 0 c 2 ,...,s and M equals where ( a T M  X  1 ) i is the i -th entry of the vector ( a
By setting c = 1 k , D =  X , and s 0 =  X  2 0 or p t  X  1 as ap-propriate, together with Lemma 3, we can avoid taking the matrix inverse explicitly, and instead compute the Kalman gain filter directly in O ( k ) time. Note that the evaluation of the determinant of M using Lemma 1 will only take O ( k ) time, as one can first pre-compute the product Q 0  X  `  X  k in O ( k ) time and obtain the required product in the sum-mand by division, thus avoiding taking O ( k ) time to eval-uate each of the k summands. Likewise this applies to the evaluation in Lemma 3.
We now present an empirical evaluation of our technique using real product and offer data obtained from a commerce search engine.
We obtain a set of products related to televisions from a commerce search engine for which we want to create price histories. The products include both televisions and acces-sories such as remote controls and mounts. For some of these products, we manage to obtain its Universal Product Code (UPC), a unique identifier.

For the products with UPCs, we obtain a set of offers that are matched to each of them according to the search engine. We have also obtained the offer prices over a five-month period from mid July 2011 to mid December 2011. We filter out any product that include offers with prices that exhibit a regime change (prices going up or down by more than 50% from day to day), as these are often due to two offers being confused as one, and are not proper candidate for evaluation.

For some of these offers, we manage to obtain their UPCs, and we treat an offer with a UPC that is equal to that of the product as ground truth. There are certainly limitation to this ground truth set X  X t is possible that an offer with a missing UPC could be a correct match; it is also possible that the UPC is incorrect. However, obtaining human judgments for whether an offer is correctly matched to the product is a difficult and costly process, and an attempt to obtain judgment via Mechanical Turk has resulted in very noisy labels, hence we settle on employing UPCs as ground truth. To eliminate some of the mistakes due to incorrect UPCs, we filter out cases where the ground truth includes offers for which their prices differ by over 100%, as these are signs that some of the UPCs are incorrect. We also require that a product has to have there are at least three offers in the ground truth set.

To identify aberrations in the ground truth set, we com-pare the daily average prices according to the ground truth, and the daily average prices according to all offers with prices between the lowest and the highest prices among the offers in the ground truth. A priori , there is no reason based on a price argument that an offer within this range is not a correct match. Therefore, if many of the offers are missing, it is an indication that the set of offers in the ground truth for this product is too small. As the usefulness of our met-rics depend on a sampling argument (see next subsection), the results measured on products for which this happens are less reliable. We remove from consideration products where the daily averages according to the two computation differs by more than 5% at the peak. After these processing steps, our evaluation is conducted over a total of 700 products.
For each product, we compute the average product price history and the minimum product price history according to the ground truth set. We measure the difference between these price histories (both average and minimum) and the ones generated algorithmically via two metrics.

Our first metric is the mean scaled absolute difference ( MASE ), designed to measure how closely the generated price history tracks the price level of the true price history. Given the price history p  X  computed from ground truth and the algorithmic price history p , the MASE is defined as: The lower the MASE, the better the performance of the algorithm. We denote the MASE for the average price his-tory as Avg-MASE and that for the minimum price history as Min-MASE. We report the numbers averaged across all products.

Our second metric is the mean absolute change difference ( MACD ), designed to measure how closely the generated price history tracks the price trends in the true price history. Given the price history p  X  computed from ground truth and the algorithmic price history p , the MACD is defined as:
MACD ( p  X  , p ) = 1 This metric captures the average difference between the change in prices according to each history in relative terms. The lower the MACD, the better the performance of the algo-rithm. We denote the MACD for the average price his-tory as Avg-MACD and that for the minimum price history as Min-MACD. We report the numbers averaged across all products.

As a final note, we discuss how the choice of the ground truth set affects the usefulness of these metrics. If the ground truth set contains all and only correctly matched offers, the metrics will measure the desired difference between the true price histories and the algorithmically genearted ones. As-suming that the correct matches in the ground truth set is only sampled (uniformly) from among all of the correct matches, and that the prices of the correctly matched of-fers are distributed according to a Gaussian distribution, the average price history based on the ground truth is an unbi-ased estimator of the true average price history, and hence we expect Avg-MASE and Avg-MACD to be good metrics, provided the samples are sufficiently dense. On the other hand, the minimum price history based on the ground truth will systematically overestimate the true minimum price his-tory. Hence, the Min-MASE and Min-MACD should be in-terpreted with caution. However, given the importance of minimum price histories for consumers in deciding when to buy a product, we decide to keep these metrics in the eval-uation.
There are two approaches to computing the average price history using the probabilistic model proposed in this paper. First, we can use the inferred latent prices p computed in the final iteration of the variational E-step in Algorithm 1. In the experiment, we label this method as Latent . Alterna-tively, we can use the parameters {  X  i } k i =1 that correspond to the likelihoods of the offers being correct matches. Together with a cutoff threshold  X  , we can select all offers with  X  and take the average over these offers. In the experiment, we label this method as Match .

To compute the minimum price history, the inferred latent prices are not directly applicable. Therefore, we follow the paradigm of Match as described in the preceding paragraph, and instead of taking the average we take the minimum after offers are selected.

In all of our experiments, we have selected  X  to be 0 . 6. After inspecting the values of the parameters {  X  i } k i =1 number of products, we notice that the values are typically very close to 0 or 1. Hence, the choice of  X  has minimal effect on the performance of Match .
We compare our approach to three alternatives in our experiments for the evaluation on average price histories. These algorithms are (labels in parentheses):
We choose the trimmed mean and the median as they are common and popular alternatives to the simple average, and they are considered more robust to outliers. We also choose the Kalman filter and smoother applied as a post-processing step after the average is taken, as it can help to smooth away anomalous averages that are caused by the occasional outliers.

For the evaluation on minimum price histories, the median no longer makes sense. Instead, we select the price of the offer closest to the 10-th percentile as the product price. We call this method as the Trimmed Min ( Trim ) as it is analogous to removing the top and bottom 10% of the offer prices before taking the minimum. We continue to evaluate the Kalman filter and smoother as a post-processing step applied to the minimum taken over all offer prices.
Finally, as baseline, we consider using the entire set of matched offers to generate the price history. In the experi-ment, we label that as All .
The performances of our algorithms, the baseline, and the alternatives are presented in Table 1. Note that while these percentages may appear small, they do matter to prospective consumers. For example, for a $2,000 TV (a typical price for a brand-named LCD TV), a MASE of 5% corresponds to a difference of $100 on average between the true price and the reported price each day. Such errors can significantly decrease credibility of the system. Likewise, a MACD of 1% corresponds to a difference of $20 on average between the true price change and the reported price change. The errors quickly add up and may lead to poor recommendations for consumers who are deciding between buying now or wait.
For each metric, we present its value, followed by % dif-ference from the baseline in the next row. Recall that under all four metrics, the lower the value, the better the perfor-mance; improvements over baseline will give a negative % difference. Our main findings are: Table 1: Overall Results of All Algorithms Under Four Metrics. The best performing approaches are highlighted in bold.
To better understand when we could expect Match to do well, and under what conditions Match will outperform All and vise versa, we analyze different groupings of the products to identify factors that are highly correlated with performance. We identify two factors X  X raction of offers matched in the ground truth, and the difference in prices among matched offers X  X s the two most influential factors. Their effects on Avg-MASE and Avg-MACD are presented in Figure 2. Their effects on Min-MASE and Min-MACD exhibit the same trends and are omitted due to space limi-tations. The key observations are:
Examining the cases for which our method performs poorly, we note that a common cause is due to the existence of multiple sets of offers, each with distinct price ranges, be-ing mapped to the same product, and in the ground truth set, offers from two or more sets are deemed correct. This problem often also leads to large differences in the prices of the matched offers, a problem noted before. This leads to poor performance for Match , as the probabilistic model often learns parameters that corresponds to one set of offers, and Match produces an average price history that corresponds only to the set selected.

We find it intuitively unlikely that a set of correctly matched offers should have exhibited such big variations in prices. While it is possible that this is simply a reflection of dif-ferent pricing strategies by the merchants, we think a more likely explanation is that the offers are fundamentally dif-ferent in some ways, for example, TVs sold with accessories versus just the TVs, or the price is inclusive of shipping or extended warranty. We believe a representative price history should avoid mixing these different offers, as the underlying product sold is different. Instead, a better solution is to group the like offers together and present separate price his-tories for each of the groups.
 Despite both being based on the same probabilistic model, Latent performs significantly worse than Match . Examining some of the products for which the difference is large, we notice that poor performance often happens when either (1) the composition of the set of offers changes, causing a large discrete jump occurs in the true price history, or (2) there is an offer considered likely to be correct by the model ex-hibits a temporary change in prices. As the latent price vari-ables attempt to balance the price dynamics of the model (as governed by  X  and  X  2  X  ) and the set of new observations, Latent enters into a transition period and adjusts to these changes gradually . The errors during this transition pe-riod are high. On the other hand, Match reacts immediately under both situations, and therefore does not suffer from a transition period. As both types of situations are common in our dataset, Latent performs worse than Match .
We study the problem of how to aggregate web offers to create representative price histories, taking into account that offers that are matched to a product are not always correct. To solve this problem, we propose a probabilistic model for how offer prices are generated, extending the classical linear dynamical system to allow for possibly incorrect observa-tions. We give an EM algorithm for finding the parameters that maximize likelihood, and work around the computa-tionally intractable exact inference needed in the E-step by variational approximation. To further speed up the learning algorithm, we take advantage of the structure of our prob-lem that assumes independence among offers, and establish certain matrix equalities that allow us to circumvent the need to take matrix inverses in computing the Kalman gain filter, leading to quadratic speedup of the algorithm. This technique may be of independent interest.

We conduct an extensive evaluation of our approach using data obtained from a commerce search engine. We consider two metrics in evaluation X  X ASE for evaluating how well a method tracks the true price level, and MACD for evaluating how well a method tracks the true price trend. Our method significantly outperforms alternatives including the median, the trimmed mean, and post-processing using Kalman fil-ter and smoother. The improvements over the baseline are statistically significant. How the output of the probabilistic model is used has large influence over the performance. We find that it is significantly better to use the learned likeli-hood of whether offers are correctly matched than to use the latent price variables. This work addresses a limitation of the experiments in [1] which depends on data vendor for product prices, and constitutes a step towards building a system that helps consumers decide when to buy a product.
We conclude with several interesting future research di-rections. First, can the probabilistic model proposed in the paper be integrated into some matching algorithm to im-prove the matching process itself? While some proposed matching algorithms in the literature can use price as one of the matching features, we are not aware of any solution that takes advantage of the sequentiality of the offers. Combin-ing a dynamic model of prices like the one presented here with matching will likely be both fruitful and challenging. Second, we have considered only a state-space model with only a single state in our probabilistic model. Better perfor-mances may be possible by adopting a higher-dimensional state-space model. One may also want to revisit the gener-ative process for the correctly matched offers, as it is com-mon for merchants to keep offer prices constant for periods of time before adjustments. Finally, looking at the entire set of offers open up new possibility in predicting future product prices. An important direction is to revisit the al-gorithms proposed in [1], and see if by using all offers one can improve forecasting accuracies, and help make better buy-or-wait recommendations to consumers. [1] R. Agrawal, S. Ieong, and R. Velu. Ameliorating [2] R. Agrawal, S. Ieong, and R. Velu. Timing when to [3] O. Benjelloun, H. Garcia-Molina, D. Menestrina, [4] M. Bilenko, R. Mooney, W. Cohen, P. Ravikumar, and [5] C. M. Bishop. Pattern Recognition and Machine [6] A. P. Dempster, N. M. Laird, and D. B. Rubin.
 [7] A. K. Elmagarmid, P. G. Ipeirotis, and V. S. Verykios. [8] I. P. Fellegi and A. B. Sunter. A theory for record [9] Z. Gharamani and G. E. Hinton. Parameter [10] Z. Gharamani and G. E. Hinton. Variational learning [11] N. J. Gordon, D. J. Salmond, and A. F. M. Smith. [12] P. J. Huber. Robust Statistics . Wiley, 1981. [13] M. Isard and A. Blake. CONDENSATION  X  [14] R. E. Kalman. A new approach to linear filtering and [15] K. Kanazawa, D. Koller, and S. Russel. Stochastic [16] A. Kannan, I. E. Givoni, R. Agrawal, and A. Fuxman. [17] M. Michelson and C. Knoblock. Creating relational [18] R. Mitkov. Anaphora Resolution . Longman, 2002. [19] H. B. Newcombe, M. J. Kennedy, S. J. Axford, and [20] P. Ravikumar and W. W. Cohen. A hierarchical [21] S. Roweis and Z. Ghahramani. A unifying review of [22] S. Sarawagi and A. Bhamidipaty. Interactive [23] S. Singh, K. Schultz, and A. McCallum. Bi-directional [24] B. Sinopoli, L. Schenato, M. Franceschetti, K. Poolla, [25] W. E. Winkler. Overview of record linkage and [26] P. Zarchan and H. Musoff. Fundamentals of Kalman [27] J. Zhong and S. Sclaroff. Segmenting foreground
