 Modern web search engines reply heavily on data-driven approaches that go beyond traditional information retrieval (IR) models by incorporating additional features into machine-learned rankers. Typical ranking features include static link analysis features like PageRank, click-through data and document classifiers [2, 6, 10]. The quality of a learned ranker depends to a large degree upon the amount of training data such as human relevance judgments, user feedback and the size of the index or web-graph. 
The web is a global resource, serving users in hundreds of regions who speak hun-dreds of different languages. Optimizing a web search ranker for each of these lan-annotated data. Even after collecting annotations, ranking features derived from click-through data may not be available for markets with small numbers of users, while link analysis features such as PageRank may not be as helpful for nascent markets with fewer documents and links. Rather than collecting expensive annotated data for each new low-resource market, several strategies have been applied to exploit existing data or models. One approach is to exploit a mark et with more training data, such as Eng-lish/US, via model adaptation (e.g. [1, 7]). Another approach, which we explore in this paper, is to use cross-lingual feedback from a high-resource market. 
In this study we focus on linguistically non-local (LNL) queries, defined by [8] as concepts that are searched for by users in different markets. For instance, the concept [world cup] [copa mundial] and [coupe du monde] are LNL since they are all about the world cup. In contrast, [brooklyn beaches] is a local query. In practice, a query in language L1 is considered LNL if it has a high-confidence translation into a language L2 and the query translation is found in L2 query logs. By this metric, [8] found that at least 11.5% of Chinese queries from their dataset were LNL. Figure 1 shows query volume over time for several LNL queries. Queries in Korean, Russian, Arabic and large scale (2004 -2010) and a much shorter time frame (summer 2010). Although this years, it does suggest that these queries share similar user intent, even though they are in different languages. The query [jk rowling] also shows similar query volume over time in four European markets (France, UK, US, Spain), again indicating related user intent, possibly related to the publication of Harry Potter books or news headlines. 
Cross-lingual relevance feedback works by retrieving results for a LNL query in Results from L2 are assumed to be better than L1 results, and can be used to improve L1 results, with the help of a translation dictionary. Note that this is not an instance of cross-lingual information retrieval, since the goal is still to return results in L1 only. 
Our model, called the unified model, generalizes the existing cross-lingual relev-ance feedback models by incorporating both query expansion and document re-ranking to further amplify the signal from the high-resource ranker. We use a learning to rank approach, which requires labeled training data. Unfortunately, there are no publicly available corpora with relevance judgments for non-English web search. The datasets used in our experiments are sampled from real-world datasets indexed by a commercial search engine. We present experiments on datasets from two markets, Korean/South Korea and Russian/Russia, using English/US as the assisting market. Our evaluation shows that the proposed unified model outperforms two previous cross-lingual relevance feedback models across two different domains. Our unified model extends two previous models, MultiPRF, by Chinnakotla et al. [5], and the model proposed by Gao et al. [8], which we will refer to as DocSim. In this section, we review these models before presenting the unified model. 
The baseline IR system used in our experiments is based on the language modeling (LM) framework. In this approach, documents are ranked by the similarity of their LMs  X   X  to the query LM  X   X  , using Kullback-Leibler divergence. Document lan-guage models are smoothed using the collection LM via Dirichlet smoothing [13]. limited. Pseudo relevance feedback (PRF) attempts to overcome this problem by as-suming that the top n documents are relevant and extracting additional k query terms from them. The terms are weighted according to how often they appear in the feed-back documents and how relevant the feedback documents are to the original query. The feedback relevance model  X   X  is then combined with the original query model using a mixture model [11], which we will refer to as monolingual PRF (MonoPRF). 
Chinnakotla et al. [5] extended the monolingual PRF model to include cross-lingual documents. Given an LNL query and search results in L1 and L2, PRF is per-formed in both languages. PRF terms from L2 are translated back into L1 using a final model is called multilingual PRF (MultiPRF) because it does PRF in the query language (L1,  X   X  ) as well as in the assisting language (L2,  X   X  X  X  X  X  X  ), combining them with a mixture model of Equation (1) The intuition behind the MultiPRF model is that the L2 corpus is larger than the orig-inal L1 corpus, so there are likely to be more relevant documents in L2. Doing query translation (to retrieve the assisting language feedback documents) and then back translation (to translate back the PRF model) also yields a query expansion effect, i.e., synonyms and related terms are added to the query. 
Gao et al. [8] introduced the concept of LNL queries, and presented a document re-trieval model for cross-lingual relevance feedback, which we will refer to as DocSim. created over the documents, connecting L1 documents with L2 documents. The weight of each edge is the cross-lingual document similarity, which is calculated via cross-lingual cosine similarity. Finally, a relational ranking support vector machine is applied so that the ranks of L1 documents move closer to the ranks of similar L2 doc-uments. For example, the official world cup webpage in Arabic is very similar to the DocSim model will re-rank the Arabic page to also have a high rank. Both the MultiPRF model and the DocSim model are motivated by the same observa-than the L1 ranker. But they are developed based on two different, yet complementa-ry, assumptions. MultiPRF exploits the fact that L1 and L2 queries have shared query intent , and works via cross-lingual query expansion. In contrast, the DocSim model assumes that the documents that are relevant to an LNL query contain related docu-ment content , albeit in different languages. 
Our unified model is intended to build on both complementary assumptions, and is a significant extension of the previous research in two aspects. First, we extend both the MultiPRF model and the DocSim model to handle web document structure. Second, the unified model takes the learning to rank framework to which a wide va-both MultiPRF models and DocSim models, rather than being just limited to query expansion (MultiPRF) or document similarity (DocSim), are incorporated. In our implementation, we used a neural net ranker, called LambdaRank [4], which has been shown empirically to optimize NDCG (Normalized Discounted Cumulative Gain [9]). In the next section we describe the web document structural aspect of the features ranker in detail. 3.1 Web Document Structure Web documents consists of multiple streams, or fields, which can be divided into text and queries used to access the page. [6 ] analyzed cross-stream perplexity and found that different language styles are used for composing the document body, title, the query language model than the body language model is. Therefore, each stream should be modeled separately and combined, rather than modeling the document as a single bag of words extracted from different streams. Similarly, BM25F combines weighted term frequencies from different fields, recognizing that some fields are more salient than others [12]. 
For cross-lingual relevance, document structure is important because popularity fields are the most useful for estimating relevance, but are also more likely to be miss-ing for low-resource languages. Cross-lingual relevance feedback can project popular-ity fields from the richer market back onto the low-resource market. One potential pitfall could be translation, since popularity fields (such as anchor texts and user que-with machine translation systems than body text (or even title text), which usually consists of full sentences. 
Another major advantage of incorporating web document structure into the model is speed. If relevance can be approximated by shorter document fields (such as anchor Each document similarity calculation involves word-by-word translation and then cosine similarity, and for example the model of Gao et al. [8] does  X  X  X  similarity calculations. 3.2 Ranking Features The features used in the ranker can be grouped into three categories, monolingual features, MultiPRF features, and DocSim features. 
Monolingual features include baseline ranking features that are used in almost all web search ranking models, such as PageRank (which is query-independent) and BM25F (which is query-dependent). A baseline retrieval model and monolingual PRF model were built for each document stream. In our experimental dataset, documents  X  X llfields X . Ranking scores for each stream were defined as monolingual features. 
MultiPRF features are derived as follows. A MultiPRF model was built for each document stream, and ranker scores for each stream were used as a feature. Overall, there are 5 MultiPRF features. DocSim features are derived as follows. A single L1 document is compared to each L2 document, and then the cross-lingual similarity score, defined as a DocSim fea-ture, is normalized and combined using a weighted average. Intuitively, this score represents the rank of similar L2 documents. This DocSim feature is computed for each of the 4 document streams. Two standard similarity functions were applied to each document-feedback document pair: Jaccard similarity and cosine similarity. As were all in Latin. Certain words may also appear in Latin, even when the document is in another language (e.g.,  X  X indows X ). Overall, there were two monolingual and two DocSim features. 4.1 Data The unified model targets monolingual search in languages with few training re-sources. We used a re-ranking experimental paradigm where we try to improve the web search results by re-ranking documents retrieved from the entire web using a commercial search engine. We used data from two language/region settings that are linguistically and culturally different from the English/US setting, to see how well feedback from a better ranker from an unrelated domain can improve results. The do-mains we selected were Korean/South Korea and Russian/Russia. They are quite dif-ferent from each other in order to see how well the model generalizes across domains. 
Since we are only interested in linguistically non-local (LNL) queries, as defined above, we further filtered the data by selecting queries with high confidence transla-tions and queries whose translations were present in English/US query logs. All que-were considered high-confidence if back -translation produced a fuzzy match to the original query. The high-confidence English query translations that occurred in a large set of English/US queries were selected as LNL queries. snippet. For each query, all URLs that were annotated for relevance were crawled, and their anchor texts were also retrieved. Many documents could not be crawled, due to dead pages or errors. Only queries with 10 or more judged documents and non-empty feedback results were kept. Table 1 shows the statistics of the final evaluation dataset used in our experiments. 4.2 IR Setup Each crawled document was parsed and split into different streams (fields): url, title, body, anchor text and everything, which included all the other streams. Each feedback snippet. Since the snippet contains a small amount of text highly relevant to the query, documents, while greatly speeding up the document comparison calculation. 
A unigram index was built for each document stream. Each stream was tokenized according to the document language. We used a Viterbi decoder based on a unigram model to break a URL string into tokens. As in the World Wide Web, documents from different languages exist in the same global corpus, although real search engines have more sophisticated techniques for region and language matching. 5.1 PRF Baselines The baseline IR model is defined in Section 2 and has only one tunable parameter, the Dirichlet parameter. The monolingual PRF model (MonoPRF) does PRF based on documents returned by the baseline IR model, and has three additional parameters: the number of feedback documents, the number of feedback terms and the mixture model parameter  X  , as in Equation (1) where  X 0 X  . The MultiPRF model is a mixture model over the MonoPRF model and the cross-lingual PRF model, as defined in Equ-ation (1). It has four additional parameters: the number of cross-lingual feedback doc-uments and terms, the number of translations per feedback term, and the mixture model parameter  X  . 
In our experiments, the model parameters were tuned using leave-one-out cross-validation and grid search. Each model X  X  parameters were tuned separately, so for instance, the Dirichlet parameter could end up being different for baseline, MonoPRF and MultiPRF. 
Results comparing the baseline IR model with monolingual and multilingual PRF on the two LNL web datasets are shown in Figure 2. MultiPRF outperforms the base-line and the monolingual PRF model in most cases (except for NDCG at 1 for Ko-rean). The improvements for the Russian domain are all statistically significant. 5.2 DocSim Baseline The DocSim model uses LambdaRank to learn a ranking model, based on the 4 doc-ument similarity features computed only on the  X  X llfields X  stream of each document, as described in Section 4.2. The LambdaRank model was a single layer neural net-work with 200 iterations. All reported results are from 5-fold cross-validation. 
In contrast with the MultiPRF baseline, which uses query expansion for feedback, the DocSim model learns to rank L1 documents similar to the rank of similar L2 doc-uments, based on the assumption that the L2 ranker is better. If too few feedback doc-uments are used, there may be no similar documents to learn from. However, if too many are used, there may be too much noise in the features for the model to learn a coherent ranker. Results of applying the DocSim model with different numbers of feedback documents are shown in Figure 2. With a small number of feedback docu-ments, the results are often worse than the baseline. However, as the number of feed-back documents increases, the NDCGs improve. For the Korean domain, 25 feedback documents performed best, while for the Russian domain, 50 feedback documents were best. 5.3 Unified Model For the final comparison, machine-learned rankers were built using baseline features (PageRank and monolingual BM25F) plus features from each model. The MultiPRF ranker had as features ranker scores from the MultiPRF rankers, while the DocSim Table 2 shows that cross-lingual relevance feedback almost always outperforms the monolingual baseline. For the Russian domain, the unified model outperforms both the MultiPRF model and the DocSim model, except at NDCG at 10, where MultiPRF does slightly better. For the Korean domain, the unified model outperforms both DocSim and MultiPRF at NDCG at 3 and 10, but does worse at NDCG at 1. Overall, the unified model is the best performer in our experiments across both datasets. The motivating hypotheses behind the cross-lingual relevance feedback model was that linguistically non-local (LNL) queries in different languages have similar query intent and relevant documents have related content, so a poor ranker should be able to get direct feedback from a better ranker in another language. The original experiments with MultiPRF used CLEF collections, wh ere the queries were compared over the different sites to ensure that a high percentage of them will find some relevant docu-ments in all [language/domain] collections [3]. In contrast, extracted LNL queries were simply those that had been searched for in both languages, so there may not be as much relevant content in L2. Surprisingly, queries that skewed heavily towards L1 were not always harmed by MultiPRF. For instance, the queries [  X  X  X  ] (naver) and [  X   X  X  X  X  X  X  X  X  ] (in touch) are already knows that [naver] is a navigational query to the Korean website, so the feed-relevant, and the irrelevant terms have much lower weight. 
As expected, MultiPRF did harm queries when the query translation was bad. Al-though query translation used a state-of-the-art MT system and the translations were filtered for  X  X igh confidence X  translations via back translation, some queries were still quial  X  X ortune telling X  or  X  X alm reading X . 
However, even queries that are truly LNL and correctly translated can be harmed by MultiPRF. For instance, in the first example in Table 3, searching for [wildlife] in the English/US domain brings up many US-specific wildlife associations, which harm the Russian results. In the second example, the English results help re-focus the query towards working and living in France (and getting visas) instead of visiting and tour-ing France. We presented a cross-lingual feedback model that aims to improve a ranker from a market with few training resources using feedback from a better ranker with richer training resources. Focusing on linguistically non-local queries allows the model to Our model extends and generalizes prior work by incorporating both query-and doc-ument-level features. Query expansion using multilingual pseudo-relevance feedback document similarity features leverage related content in both languages, using transla-document structure to further amplify the noisy signal from the better ranker. The cross-lingual unified relevance feedback model outperformed the monolingual base-line across two different domains. 
While the results of this pilot study are pr omising, the biggest hurdle we faced was data size, and in future work we would like to apply our model to a much larger data-set. Unfortunately, we are unaware of any publicly available web search relevance judgments for languages other than English. 
Another promising direction is to exploit more cross-lingual web features. For ex-ample, there are many cross-lingual anchor texts (e.g., English links pointing to Chi-types of features would give stronger evid ence of shared conten t across cross-lingual documents, or shared cross-lingual query intent. The English/US domain is also richer in popularity fields (such as anchor text and clickstream features) than some other domains. Exploiting this structural asymmetry should improve the feedback model even more. 
