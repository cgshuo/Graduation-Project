 Inderjit S. Dhillon INDERJIT @ CS . UTEXAS . EDU Dept. of Computer Science, University of Texas, Austin, TX 78712, USA Topic models can be understood as a class of statistical models for document collections that model documents as admixtures over topics . Specifically, each topic is modeled as a distribution over words, and each document is a sep-arate mixture of such topics (or specifically, the word dis-tributions comprising the topics). Such an admixture can be contrasted with a vanilla mixture of topics, where each document would be drawn from a single topic.
 A popular set of topic models is PLSA (Hofmann, 1999), which uses the multinomial distribution as the word dis-tribution for any topic, and its Bayesian counterpart, LDA (Blei et al., 2003), which adds Dirichlet priors. While these topic models have proved enormously useful in modeling varied document collections and have attracted a long line of work with numerous extensions (see (Blei, 2012) for a review of LDA applications and trends), it has some crucial lacunae that arise from its basic use of the multinomial dis-tribution to model word distributions for topics. There are several reasons which make the multinomial distribution an inadequate distribution for documents and topics. The primary issue is that it does not model dependencies be-tween words: if the word  X  X ernels X  appears in a document (specifically, a machine learning paper), the appearance of the word  X  X raphs X  might be less likely. Alternatively, if the word  X  X lassification X  appears,  X  X upervised X  is more likely to appear than in general documents. Indeed, typical coher-ence metrics that quantitatively measure the goodness of fit of various topic models primarily test for such dependence among estimated top words for the topics (see Sec. 6.2). A second caveat is that the multinomial distribution does not model absences of words. Lastly, the multinomial word distribution does not leverage varying document lengths. For instance, with large counts of other words, some spe-cific word might become less likely.
 To address the issue of modeling word absences, Reisinger et al. (2010) proposed the use of von Mises-Fisher distri-bution for topic distributions. But while this addresses one issue with multinomials, it does not model word dependen-Figure 1: A Poisson MRF can provide interesting insights into a text corpus including multiple word senses (hubs of graph) and semantic concepts (coherent subgraphs). cies, nor does it leverage document lengths in any substan-tative way.
 In this paper, we propose using Poisson MRFs (Yang et al., 2012) for topic distributions and using the resulting admix-ture of Poisson MRFs (APM) for modeling document col-lections. These Poisson MRFs allow modeling multivariate count data and use dependencies among the count variables to represent the joint distribution compactly. Moreover, since these are graphical model distributions, the depen-dencies are Markov with respect to an undirected graph, which thus provides a visually appealing representation of any topic X  X ee Fig. 1 X  X n contrast to just a list of words as in PLSA/LDA.
 We position the Poisson MRF in context of topic models by showing that the conditional distributions of the classi-cal LDA model can be written as a Poisson MRF where the underlying graph has no edges and hence no depen-dencies between words; this connection X  X hich was only recently discovered in the context of matrix factorization (Gopalan et al., 2013) X  X ot only puts into relief the as-sumptions made by LDA but also opens the door to other approximate inference schemes for LDA (which however we do not explore here). In other contributions of this pa-per, we define a new class of models called admixtures and show that this class generalizes previous topic models, which thus opens the door to other topic models based on non-Poisson distributions. Finally, we provide qualitative as well as quantitative evidence for the benefits of APM by training the APM model on both a subset of the Grolier encyclopedia and the CMU 20 Newsgroup dataset. First, we review the Poisson MRF model (PMRF) as pro-posed by Yang et al. (2012). Second, we contextualize the independent PMRF model by showing an equivalence with the conditional distributions of LDA. Finally, we propose a novel prior distribution for PMRFs that can be viewed as a generalization of the Gamma distribution. 2.1. PMRF Definition By assuming that the conditionals of the joint distribution are univariate Poisson, Yang et al. (2012) recently proposed a PMRF model that provides a joint distribution over mul-tivariate count data. They also provided a tractable way to estimate the parameters of such a PMRF using ` 1 reg-ularization and proved that the estimator is guaranteed to recover the underlying dependency structure with some as-sumptions including sparsity of the parameters. The model PMRF(  X  ,  X  ) is defined as follows: Pr where  X   X  R p and  X   X  { R p  X  p  X  : diag ( X ) = 0 } . By construction, the conditional distribution of a variable x given all other variables x \ s is a univariate Poisson with canonical parameter  X  s =  X  s +  X  T s x and mean (standard) parameter  X  s = exp(  X  s ) . An illustration of the density of a 2D Poisson with negative, zero and positive dependency can be seen in Fig. 2. Other observations about a PMRF:  X  The dependency parameter  X  is analogous to the  X  If  X  = 0 , then the PMRF reduces to an independent  X  Negative dependencies can help model sparse data Figure 2: The densities of three 2D Poisson MRFs that show possible dependency structures between two words. Negative dependencies (left) suggest that two words rarely co-occur whereas positive dependencies (right) suggest that two words often co-occur.
 The negativity constraint on  X  is required under the formu-lation above to ensure that the distribution is normalizable (Yang et al., 2012). However, Yang et al. (2013) propose a slight modification to the sufficient statistics of the PMRF that removes this constraint and allows positive parameter values X  X ee reference for details. For simplicity of nota-tion and wording, throughout the rest of the paper, the ba-sic PMRF notation will be used for derivations though the APM model uses this slightly modified PMRF. 2.2. LDA Conditionals Equivalent to Independent In this section, we place Poisson MRFs in the context of topic models by showing the equivalence between the conditionals of LDA and an independent Poisson MRF. LDA assumes the following generative process for a new document given that the topic weights for the document w and the topic distribution parameters  X  1 ...k are known: 1) Draw  X  x  X  Poisson(  X   X  ) 2) For each of the  X  x words: (a) Draw topic index z  X  Categorical( w ) (b) Draw word v  X  Categorical(  X  z ). Notice that because  X  x is inde-pendent of the other variables in LDA, it is often sim-ply ignored when estimating the model parameters. In our model, however,  X  x cannot be ignored because words can be dependent. By marginalizing out the topic vari-able z , step 2 can be collapsed into a draw from a Multi-nomial with a single parameter  X   X  , which is simply a weighted average over the topic distribution parameters  X  This yields the following modified step: 2 X ) Draw docu-ment x  X  Mult(  X   X  = P k j =1 w j  X  j | N =  X  x ). Therefore, the probability of a document x given w and  X  1 ...k is: Pr Amazingly, this Poisson-Multinomial joint distribution is equivalent to p independent Poissons (Bishop et al., 2007): where  X   X  = P p s =1  X  s and  X  x = P p s =1 x s . Therefore, a PMRF directly generalizes the conditional distribution of PLSA/LDA by relaxing the independence assumption. To more fully generalize LDA, priors must be added to a PMRF as proposed next. 2.3. Adding Priors to a PMRF Similar to LDA X  X  prior, a conjugate prior on the parameters of a PMRF can be defined as being proportional to: exp {  X  T  X  +  X  T  X   X   X   X  A (  X  ,  X )  X   X   X  k  X  k 2 2  X   X  k vec ( X ) k where  X  s,  X  s &gt; 0 ,  X   X  0 ,  X   X  &gt; 0 and  X  &gt; max One observation is that when  X  = 0 , exp(  X  s ) is essentially Gam ( shape =  X  s ; scale = 1) . Therefore, for independent Poissons, this is similar to using Gamma priors. The poste-rior distribution merely modifies the hyperparameters to be  X   X  =  X  + x and  X   X  =  X  + 1 . Therefore, because this prior adds pseudo-counts  X  to the observations for parameter es-timation, this prior for PMRFs is analogous to a Dirichlet prior for multinomials as in LDA. Figure 3: (Left) In mixtures , documents are drawn from exactly one component distribution. (Right) In admixtures , documents are drawn from a distribution whose parameters are a convex combination of component parameters. In a simple mixture model , an observation is assumed to come from exactly one of k possible components. An il-lustration of this type of model is shown in Fig. 3 (left) in which documents are drawn from exactly one of two com-ponent distributions X  X he  X  X opics X  in the case of document modeling. On the other hand, for admixtures each docu-ment is drawn from a distribution whose parameters can be any convex combination of the component parameters, allowing each document to be explained by multiple com-ponents as illustrated in Fig. 3 (right). 3 Given this intuition about admixtures, the probability of a single observation x from an admixture of some base dis-tribution (e.g. multinomial, von Mises-Fisher, PMRF) X  assuming that the admixture weights w and component canonical parameters  X  =  X  1 ...k are given X  X s defined as: Pr where  X  allows for the mixing to occur in a suitable trans-formation of the parameter space. In the context of expo-nential families, the mixing could occur either in the canon-ical parameter space in which case  X  would be the iden-tity function, or it could occur in the mean parameter space (such as the mean  X  and covariance  X  for a multivariate Gaussian). For this paper, unless otherwise specified, we will assume that  X  is equal to the identity function. If priors are given for the admixture weights w and the topic parameters  X  1 ...k with parameters  X  and  X  hyper-parameters respectively, the joint distribution of a single observation and the parameters is: This gives the joint distribution over a set of n independent observations as: Intuitively, this admixture model formulation means that each observation can be explained by a mixture of a rel-atively small number of component distributions parame-terized by  X  j . In the special case where w is an indicator vector, this distribution becomes a standard mixture model where each observation is explained by only one compo-nent. In the special case where k = 1 , the admixture sim-ply reduces to every observation being drawn from a sin-gle base distribution. Therefore, this admixture formula-tion generalizes both single and mixture distributions. This admixture model also generalizes previous topic mod-els and provides a general framework for defining new ad-mixture models based on any parametric distribution. In the next sections, several examples of previous admixture models are given followed by the formulation of this pa-per X  X  main model X  X n admixture of Poisson MRFs which, to the authors X  best knowledge, is the first admixture model to allow dependencies between words.
 Example 1 -LDA As shown in Sec. 2.2, LDA assumes that each document is draw from an admixture of multino-mials. The admixture weights and the parameters for each topic multinomial are drawn from a Dirichlet prior. It is important to notice that LDA mixes in the standard multi-nomial mean parameter space (i.e.  X  LDA is the canonical to mean parameter transformation).
 Example 2 -Population Admixtures In the genetic community, the term admixture has been used to de-scribe a population produced by interbreeding several previously-isolated populations into a new admixed pop-ulation. Pritchard et al. (2000) use a model equivalent to LDA to explore this concept. Under this population model, the original ancestors of a population correspond to topics and individuals correspond to documents .
 Example 3 -SAM The Spherical Admixture Model (SAM) as proposed by Reisinger et al. (2010) is an ad-mixture model where the base distribution is a Von Mises-Fisher distribution X  X he independent Gaussian analog de-fined on the unit hypersphere. The model, which is moti-vated by the observation that cosine distance is an impor-tant document similarity, assumes Dirichlet and Von Mises-Fisher priors on the admixture weights and component pa-rameters respectively. With the background on PMRFs and the development of admixtures, the main model of this paper X  X n admixture of Poisson MRFs (APM) X  X an be developed. Relaxing the independence assumption of previous admixture mod-els such as LDA, APM assumes that the base distribution is a PMRF. This yields the following joint distribution: where Pr Dir ( w |  X  ) is a Dirichlet prior on the admixture weights (similar to LDA) and Pr(  X  j ,  X  j ) is the PMRF prior defined in Sec. 2.3. Because of the equivalence described in Sec. 2.2, APM subsumes the expressive power of LDA. The primary difference between an independent APM and LDA is that LDA mixes in the standard Multinomial pa-rameter space whereas APM mixes in the canonical pa-rameter space. An interesting open area for future research could be admixing the component PMRFs in a different parameter space such as the mean parameter space. 4 Fun-damentally, however, this model is much more expressive than all previous admixture models because it allows for dependencies between words. 4.1. Topic Representation In the APM model, topics are represented as PMRFs, and therefore, each topic provides a full graph over words showing word dependencies rather than just a list of words as in independent models such as LDA (see Fig. 4 in Sec. 6 for example topic graphs). This representation opens up a whole new area for interpreting, exploring and visualizing topics using a graph. In addition, all the metrics and algo-rithms on graphs such as tree width or shortest path could be used to explore each topic. 4.2. Document Representation Documents could be represented in at least two different ways under the APM model. First, they could be rep-resented by their admixture weights, and therefore, APM could be used as a type of dimensionality reduction tech-nique. Second, each document can be represented as a full graph over words just like a topic because each document is associated with an admixed PMRF. This graph represen-tation provides a powerful new way to visualize and sum-marize a document that was not possible with independent models like LDA. The parameters of an admixture of Poisson MRFs can be estimated by minimizing the negative log posterior. Be-cause the true log-likelihood of a Poisson MRF is compu-tationally intractable for complex multivariate distributions (Wainwright &amp; Jordan, 2008), the pseudo log-likelihood X  which approximates the joint distribution as a product of node conditionals X  X ill be used instead. With the Dirich-let prior on w and the prior described in Sec. 2.3 on the component parameters, the approximate posterior is:  X  the canonical parameter of a univariate Poisson. 5.1. Enforcing Sparsity of  X  j by ` 1 Regularization For interpretability, generalizability and computational tractability, the parameters of high-dimensional MRFs are often assumed to be sparse (i.e. a small number of non-zeros compared to zeros). This sparsity assumption is usu-ally incorporated into the problem by adding an ` 1 regu-larization term to the objective function. This ` 1 regular-ized estimator has been shown to have theoretical guaran-tees on structural recovery for Bernoulli MRFs/Ising Mod-els (Ravikumar et al., 2010), Gaussian MRFs (Ravikumar et al., 2011) and, more recently, Poisson MRFs (Yang et al., 2012; 2013). For similar reasons, APM assumes that the parameter matrices (  X  j ) for each topic PMRF are sparse and, like the aforementioned methods, estimates this sparse solution by using an ` 1 regularization term. Intuitively, this sparsity assumption makes sense because most words are only directly related to a small subset of other relevant words. 5.2. Unconstrained Optimization Along with the regularization of the  X  j parameter matrices, APM requires that the columns of the admixture weights matrix W be probability vectors (i.e. properly defined mix-ture weights that lie on the k -dimensional simplex). This leads to the following unconstrained optimization problem: where  X  is the ` 1 regularization parameter, W is the set of all possible matrices such that the columns are probability vectors and  X  W ( W ) = { 0 , if W  X  W ;  X  , otherwise } . 5.3. Proximal Optimization Algorithms Because the objective in (5) is composed of a differentiable term (i.e.  X  P ) and two non-differentiable terms (i.e.  X  and  X  P k j =1 k vec ( X  j ) k 1 ), a simple gradient descent algo-rithm cannot be used to solve the problem. Therefore, in this paper, we use a proximal optimization algorithm (Parikh &amp; Boyd, 2013). Essentially, a proximal algorithm is an iterative algorithm that computes each new parameter estimate using only the previous estimate and the differen-tiable term. After finding a new estimate, a proximal algo-rithm applies the prox operator(s) to this estimate to incor-porate the non-differentiable terms of the objective function and iterates until convergence.
 The prox operator for the ` 1 regularization term in (5) is the simple soft thresholding operator: S  X  ( z ) = sign( z ) max( | z |  X   X , 0) . The prox operator for  X  is simply the best Euclidean projection onto the simplex which can be computed using the algorithm from (Chen &amp; Ye, 2011). With these operators, any proximal algo-rithm can optimize (5) but, in this paper, a FISTA-like algorithm was used (Beck &amp; Teboulle, 2009). Because APM is a model with many parameters and FISTA-like algorithms are only first-order optimization methods, the models trained for this paper required many iterations to converge ( &gt; 5000). Using faster and more complex prox-imal optimization algorithms such as a proximal Newton method would be an excellent area for future work. Because previous admixture models have been indepen-dent, it is difficult to directly compare APM to previous models. Therefore, first, an experiment was conducted by running APM (with k = 5 and p = 500 ) on approximately 31,000 articles of the Grolier encyclopedia. 5 Visualizations of the topics were constructed using the graph visualization program Gephi 6 in order to show some qualitative results on the model output and suggest that APM can provide a more interesting, intuitive and visually appealing represen-tation of topics than merely a list of words as in standard topic models. Two topics of this run can be seen in Fig. 4 and other topic graph examples are given in the appendix. Also, a simple experiment was conducted to give some evi-dence, though inconclusive, that the APM model subsumes the power of the LDA model because of the model equiva-lence described in Sec. 2.2. 6.1. Qualitative Experiment The graphs as seen in Fig. 4 have many interesting struc-tural features that can be interpreted. 7 In the first example (Fig. 4a), the word  X  X usical X  is a hub word that connects the two concepts of  X  X heatre X  and  X  X usic X . A similar idea happens in the second example (Fig. 4b) where  X  X empera-ture X  connects the concepts of  X  X eat X ,  X  X as X  and  X  X eg X . Another interesting feature is chains of words whose end-points are not directly related but only related through other words. For example, the chain  X  X usic X   X   X  X usical X   X   X  X heater X   X   X  X lays X  suggests that  X  X usic X  and  X  X lays X  are related, albeit indirectly. The chain  X  X un X   X   X  X as X   X   X  X emperature X   X   X  X eat X   X   X  X uclear X  in Fig. 4b shows the connection that the sun is related to nuclear reactions through the words  X  X eat X ,  X  X as X  and  X  X emperature X . For other chains, the endpoints are not related even though each edge seems reasonable. For example, the chain  X  X ovel X   X   X  X ng X   X   X  X lays X   X   X  X heater X   X   X  X usical X   X   X  X usic X   X   X  X hurch X  has logical connections for each edge but  X  X ovel X  is not usually associated with  X  X hurch X .
 Though these features give evidence for the usefulness and power of APM, they do not capture any of the negative de-pendencies between words X  X ords that do not tend to co-occur. For example, the words  X  X ovel X  and  X  X ath X  would not tend to co-occur. This might be helpful in excluding documents from certain categories in document categoriza-tion. For example, if the words  X  X istory X ,  X  X ar X  and  X  X oli-tics X  appear in a document, the document is unlikely to be science literature. Though it may be more difficult to visu-alize these negative dependencies, negative dependencies can provide interesting structural information of the under-lying dataset. 6.2. Coherence Experiment Though the APM model gives significantly more informa-tion than simply a list of topic words as in LDA, another experiment was conducted to give evidence that APM can be used to produce a list of topic words similar to other topic models. The APM model was applied to the CMU 20 Newsgroup dataset evaluated with the two metrics ex-plained next. Because of the complexity of APM, only the top 200 words were used. Please see Sec. 8 for some pos-sible future research that could make APM more scalable. This experiment is meant only to be a preliminary experi-ment. Extensive experiments on larger datasets and explo-ration of the parameters of the model are significant areas of future work but are outside the scope of this paper since this paper focuses on model definition, contextualization and parameter estimation.
 Two topic coherence metrics that have been shown to cor-relate with human annotators were used in this experiment. First, the UMass coherence metric introduced by Mimno et al. (2011) and further explored by Stevens &amp; Kegelmeyer (2012), evaluates the intrinsic coherence of the generated topics by computing co-occurrence statistics from the train-ing data. Letting each topic t be an ordered list of top m words t = ( v 1 ,  X  X  X  ,v m ) , the UMass coherence metric is defined as follows: where D ( v a ,v b ) and D ( v b ) are the co-occurrence and marginal co-occurrence statistics in the training corpus and is introduced to avoid taking the log of zero. Loosely, this measures how well the model fits the training data. The second coherence metric, Pointwise Mutual Informa-tion (PMI), was introduced by Newman et al. (2010) and has also been shown to correlate with human judgments of topic coherence. The PMI metric is defined as follows: coh PMI ( t ) = where Pr( v a ,v b ) and Pr( v a ) are computed from the local co-occurrence statistics in a sliding window of an external corpus. To compute the probabilities for the PMI metric, a recent dump of Wikipedia was used with a sliding window of 20 words.
 Stevens &amp; Kegelmeyer (2012) explored the importance of in both coherence metrics and, in light of this, was set to 10  X  12 as it was in (Stevens &amp; Kegelmeyer, 2012). For simplicity, a set of topic words was chosen based on the  X  parameter of the PMRF. In general, because a PMRF con-tains information about word dependencies, the best words could be chosen using some sort of graph density algorithm such as the one described in (Yuan &amp; Zhang, 2013). LDA was trained using the MATLAB Topic Modeling Toolbox, 8 which uses the Gibbs sampling method de-scribed in (Steyvers &amp; Griffiths, 2007) and was run for 5000 iterations. For both LDA and APM, the hyperpa-rameters  X  and  X  were set to  X  = 200 /p and  X  = 50 /k respectively as suggested by the documentation of the tool-box. For APM, the parameter  X  was set near 10  X  7 , which was chosen so that there would be some edges in the initial iterations X  X owever, as discussed in the following section, the final converged APM solution did not have any edges. Because LDA and APM might perform differently with dif-ferent number of topics k , three values for k = { 5 , 10 , 15 } were evaluated for both LDA and APM. We might expect that LDA will need more topics to model the data because LDA assumes independence and hence has less parameters. 6.3. Results from Preliminary Experiments Results for APM and LDA on the 20 Newsgroup dataset can be seen in Fig. 5. The topic words chosen for both models when k = 10 can be found in the appendix. APM seems to outperform LDA in this simple 200-word exper-iment for the UMass metric whereas APM is only compa-rable to LDA for the PMI metric. Because APM directly models the co-occurrence of words, it seems reasonable that APM would perform better in the UMass coherence metric, which focuses on model fit and internal coherence. However, on the PMI coherence metric, APM only seems to do at least as well as LDA suggesting APM should prob-ably be studied through an extensive experimental compar-ison as suggested in the future works section. In addition, as expected because of model complexity, LDA seems to perform better with larger k . APM X  X  performance seems to degrade as the k increases which might be due to the estimation procedure not fully converging given the high model complexity.
 Interestingly, though some of the initial iterations of APM included many word dependencies, the final iterations gave admixtures of independent Poisson MRFs. This is likely due to the fact that only a small text collection was used, and therefore, the power of dependencies is not needed to appropriately model the data. This result could also be caused by the choice of the regularization parameter  X  . Though  X  was chosen by simply trying several values, an important area for future research is how to choose  X  ap-propriately for the application. However, this result shows that APM can effectively model words even in the inde-pendent case and can perform competitively with LDA as expected by the equivalence discussion in Sec. 2.2. Figure 5: APM seems to outperform LDA in a simple 200-word experiment when the number of topics is small but is only comparable to LDA for a larger number of topics. (Median score is shown to reduce the effect of outliers.) Many probabilistic models for documents have been con-structed using the multinomials. Nigam et al. (2000) intro-duced a mixture of multinomials to model document col-lections, and later, Hofmann (1999) proposed an admix-ture of multinomials called Probabilistic Latent Semantic Analysis (PLSA). This model was followed by the very successful Latent Dirichlet Allocation (LDA) topic model proposed by Blei et al. (2003) that added priors to the dis-tributions as well as provided a more coherent framework for extending the model. There have been numerous exten-sions of LDA that incorporate other knowledge such as au-thor information (Steyvers et al., 2004), time (Blei &amp; Laf-ferty, 2006) and topic dependency (Blei &amp; Lafferty, 2005). However, none of these models considers dependencies be-tween words since the base distribution is multinomial. Replicated Softmax (Hinton &amp; Salakhutdinov, 2009) uses a restricted Boltzmann machine (RBM) with parameter bi-ases to create a generative model for word count vectors. The hidden layer is binary-valued and allows for topic parameters to be mixed in the canonical parameter space (similar to APM). Wordfish (Slapin &amp; Proksch, 2008) is a Poisson IRT (Item Response Theory) model that attempts to characterize the latent position of a political party based on political manifestos (e.g. determining left or right wing political views). Though Wordfish also adds fixed-effect parameters, this model is similar to an independent APM model with k = 2 (i.e. only one latent dimension). Both Replicated Softmax and Wordfish significantly differ from APM because they do not consider word dependencies. Sparse Word Graphs (Nallapati et al., 2007) attempts to create graph visualizations of the topics by combining LDA and Bernoulli MRFs (Ising model) in a two-stage ap-proach. First, LDA is used to estimate the topic assign-ments for every word in the corpus. Then, these topic as-signments are used to train k independent Bernoulli MRFs for each topic. To transform the LDA output into the input for the Bernoulli MRF estimation algorithm, binary word-document matrices are constructed for each topic based on the LDA topic assignments. Though this leads to a graph over words for each topic, one major difference with APM is that this two-stage method is not a unified probabilistic model but rather two separate probability models. Another significant difference is that Sparse Word Graphs estimates simpler Bernoulli MRFs instead of PMRFs as in APM. In (Hu et al., 2011), users can interactively add soft con-straints to LDA so that the probability of the words in the constraint set will tend to be similar (e.g. either all low or all high probability). The soft dependency is added through a latent constraint variable and only provides in-direct dependence of words rather than direct dependence between words as in APM. Another difference is that these constraints can only be supplied as user-specified disjoint groups of words rather than automatically-discovered arbi-trary structure as in APM.
 Collins et al. (2001) develop a generalization of PCA by us-ing the likelihood of exponential families as the loss func-tion instead of squared loss X  X hich would correspond to Gaussian errors. While exponential PCA is related to ad-mixtures, it does not place constraints on the admixture weights but rather allows them to be arbitrary real num-bers. This is analagous to the difference between SVD and constrained non-negative matrix factorization (NMF). Scalability Because APM allows for dependencies be-tween all words, the model is quadratic in the number of words p . Therefore, scalability could be an significant ob-stacle to overcome in future research. However, since spar-sity is assumed on the dependencies, the effective num-ber of parameters can be reduced significantly. For Gaus-sian MRFs, this fact was recently exploited by Hsieh et al. (2013) to find the dependency parameters X  X he precision matrix in this case X  X ven for very high dimensional data. We believe that some of the intuitions in (Hsieh et al., 2013) could be employed to effectively scale APM.
 Empirical Study Because this paper focuses on model definition, extensive empirical experiments were not con-ducted. In future work, several parameter settings such as the choice of hyperparameters (  X  ,  X  ) or the regularization parameter  X  could be evaluated or automatically fitted in a Bayesian manner. Also, extensive user studies on the ef-fectiveness of visualizing the topics could be conducted to consider the usefulness of this model in real-world settings. This work lays the foundation for a new class of topic models based on an admixture of Poisson MRFs that can model dependencies between words unlike all previous topic models that assume word independence. Independent Poisson MRFs are shown to generalize the conditional dis-tributions of LDA, which thus suggests that APM subsumes the expressive power of LDA and adds significantly greater modeling power than LDA. In addition to APM, a general-ized class of admixture models is defined which opens the way for admixtures of any parametric distribution. For pa-rameter estimation of this new model, a tractable method using the approximate posterior is explained. Finally, sev-eral experiments give evidence that APM can provide visu-ally appealing and interpretable results as well as subsume the power of the LDA model. The development of APM opens up a whole new area of research with many inter-esting open questions in both theory (e.g. scalability, other admixtures, hyperparameter choice) and applications (e.g. visualization, user interaction, document exploration). D. Inouye was supported by the NSF Graduate Research Fellowship via DGE-1110007. P. Ravikumar acknowl-edges support from NSF via IIS-1149803 and DMS-1264033, and ARO via W911NF-12-1-0390. I. Dhillon ac-knowledges support from NSF via CCF-1117055.
 Beck, A. and Teboulle, M. A fast iterative shrinkage-thresholding algorithm for linear inverse problems.
SIAM Journal on Imaging Sciences , 2(1):183 X 202, Jan-uary 2009. ISSN 1936-4954.
 Bishop, Y., Fienberg, S., and Holland, P. Sampling mod-els for discrete data. In Discrete Multivariate Analysis:
Theory and Practice , chapter 13, pp. 435 X 456. Springer, 2007.
 Blei, D. Probabilistic topic models. Communications of the ACM , 55(4):77 X 84, November 2012. ISSN 1053-5888. Blei, D., Ng, A., and Jordan, M. Latent dirichlet allocation. JMLR , 3:993 X 1022, 2003.
 Blei, D. M. and Lafferty, J. D. Correlated topic models. In NIPS , pp. 147 X 154, 2005.
 Blei, D. M. and Lafferty, J. D. Dynamic topic models. In ICML , pp. 113 X 120, 2006. ISBN 1595933832.
 Chen, Y. and Ye, X. Projection onto a simplex. arXiv preprint arXiv:1101.6081 , pp. 1 X 7, 2011.
 Collins, M., Dasgupta, S., and Schapire, R. E. A general-ization of principal component analysis to the exponen-tial family. In NIPS , pp. 617 X 624, 2001.
 Gopalan, P., Hofman, J., and Blei, D. Scalable recom-mendation with poisson factorization. arXiv preprint arXiv:1311.1704 , 2013.
 Hinton, G. and Salakhutdinov, R. Replicated softmax: An undirected topic model. NIPS , pp. 1607 X 1614, 2009. Hofmann, T. Probabilistic latent semantic analysis. In Un-certainty in Artificial Intelligence (UAI) , pp. 289 X 296. Morgan Kaufmann Publishers Inc., 1999.
 Hsieh, C., Sustik, M. A., Dhillon, I. S., Ravikumar, P. K., and Poldrack, R. A. BIG &amp; QUIC: Sparse inverse co-variance estimation for a million variables. In NIPS , pp. 3165 X 3173, 2013.
 Hu, Y., Boyd-Graber, J., and Satinoff, B. Interactive topic modeling. In ACL , pp. 248 X 257, 2011.
 Mimno, D., Wallach, H. M., Talley, E., Leenders, M., and
McCallum, A. Optimizing semantic coherence in topic models. In EMNLP , pp. 262 X 272, 2011.
 Nallapati, R., Ahmed, A., Cohen, W., and Xing, E. Sparse word graphs: A scalable algorithm for capturing word correlations in topic models. ICDM , pp. 343 X 348, 2007. Newman, D., Noh, Y., Talley, E., Karimi, S., and Bald-win, T. Evaluating topic models for digital libraries.
In ACM/IEEE Joint Conference on Digital Libraries (JCDL) , pp. 215 X 224, 2010. ISBN 9781450300858.
 Nigam, K., Mccallum, A. K., Thrun, S., and Mitchell,
T. Text classification from labeled and unlabeled doc-uments using EM. Machine Learning , 39(2-3):103 X 134, 2000.
 Parikh, N. and Boyd, S. Proximal algorithms. Foundations and Trends in Optimization , 1(3):123 X 231, 2013.
 Pritchard, J. K., Stephens, M., and Donnelly, P. Inference of population structure using multilocus genotype data. Genetics , 155(2):945 X 59, June 2000. ISSN 0016-6731. Ravikumar, P., Wainwright, M. J., and Lafferty, J. D. High-dimensional Ising model selection using l1-regularized logistic regression. The Annals of Statistics , 38(3):1287 X  1319, June 2010. ISSN 0090-5364.
 Ravikumar, P., Wainwright, M. J., Raskutti, G., and Yu,
B. High-dimensional covariance estimation by minimiz-ing l1-penalized log-determinant divergence. Electronic Journal of Statistics , 5:935 X 980, 2011. ISSN 1935-7524. Reisinger, J., Waters, A., Silverthorn, B., and Mooney, R. J. Spherical topic models. In ICML , pp. 903 X 910, 2010. Slapin, J. and Proksch, S. A scaling model for estimating time-series party positions from texts. American Journal of Political Science , 52(3):705 X 722, 2008.
 Stevens, K. and Kegelmeyer, P. Exploring topic coherence over many models and many topics. In EMNLP-CoNLL , pp. 952 X 961, 2012.
 Steyvers, M. and Griffiths, T. Probabilistic topic models.
In Latent Semantic Analysis: A Road to Meaning , pp. 424 X 440. 2007.
 Steyvers, M., Smyth, P., Rosen-Zvi, M., and Griffiths, T.
Probabilistic author-topic models for information dis-covery. In KDD , pp. 306 X 315, 2004. ISBN 1581138881. Wainwright, M. J. and Jordan, M. I. Graphical models, exponential families, and variational inference. Foun-dations and Trends in Machine Learning , 1(12):1 X 305, 2008. ISSN 1935-8237.
 Yang, E., Ravkiumar, P., Allen, G. I., and Liu, Z. Graph-ical models via generalized linear models. In NIPS , pp. 1367 X 1375, 2012.
 Yang, E., Ravikumar, P., Allen, G., and Liu., Z. On poisson graphical models. In NIPS , pp. 1718 X 1726, 2013.
 Yuan, X. and Zhang, T. Truncated power method for sparse
