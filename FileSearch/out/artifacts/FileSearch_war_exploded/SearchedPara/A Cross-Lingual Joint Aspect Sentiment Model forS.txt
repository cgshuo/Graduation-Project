 Sentiment analysis in various languages has been a research hotspot with many applications. However, sentiment resources (e.g., la-beled corpora, sentiment lexicons) of different languages are unbal-anced in terms of quality and quantity, which arouses interests in cross-lingual sentiment analysis aiming at using the resources in a source language to improve sentiment analysis in a target language. Nevertheless, many existing cross-lingual related works rely on a certain machine translation system to directly adapt the labeled da-ta from the source language to the target language, which usually suffers from inaccurate results generated by the machine transla-tion system. On the other hand, most sentiment analysis studies focus on document-level sentiment classification that cannot solve the aspect dependency problem of sentiment words. For instance, in the reviews on a cell phone, long is positive for the lifespan of its battery, but negative for the response time of its operating system. To solve these problems, this paper develops a novel Cross-Lingual Joint Aspect/Sentiment (CLJAS) model to carry out aspect-specific sentiment analysis in a target language using the knowledge learned from a source language. Specifically, the CLJAS model jointly de-tects aspects and sentiments of two languages simultaneously by incorporating sentiments into a cross-lingual topic model frame-work. Extensive experiments on different domains and different languages demonstrate that the proposed model can significantly improve the accuracy of sentiment classification in the target lan-guage.
 I.2.7 [ Artificial Intelligence ]: Natural Language Processing Algorithms, Experimentation, Performance Sentiment Analysis, Cross-Lingual Sentiment Analysis, Topic Mod-el, Aspect Discovery
With the flourish of Web2.0 based applications, the Web has been full of various user-generated contents in different languages, which contain rich sentiment information (e.g., comments, reviews, blogs, etc.). It is no doubt that such information is of great value. For instance, based on the reviews or comments left by previous customers, product or service providers may improve the quality of their services or products, while incoming customers may choose well-content services or products. For this reason, sentiment anal-ysis aiming at identifying the sentiment polarity (i.e., positive or negative) of given texts has rapidly emerged into a research hotspot with broad application prospects, including product recommenda-tion and service follow-up etc., and has thus attracted much atten-tion from both academia and industries in recent years [13, 19, 22]. A couple of sentiment analysis related issues have been extensively studied for online customer reviews, ranging from coarse-grained document-level sentiment classification [16, 28] to fine-grained ex-traction of aspects and opinion expressions [6, 20].

Currently, for some languages (e.g., English), there have been lots of sentiment resources available for conducting sentiment anal-ysis, including labeled corpora and sentiment lexicons [19]. How-ever, for other languages, sentiment resources are often insufficient or of low quality. Therefore, it is desirable to develop cross-lingual mechanisms so as to utilize the knowledge learning from those resource-rich languages to improve sentiment analysis in other lan-guages. In doing so, a direct approach is to translate the resources in the resource-rich language to the target language using a machine translation tool [1, 23]. However, it is known that machine tr ansla-tion is a very challenging task and the existing machine translation system is thus not powerful enough to generate very accurate re-sults. Particularly, a machine translation system usually generates only one result, which may not be suitable for the situation therein.
In reviews, customers often pay more attention to specific as-pects rather than the whole entity. Here, an aspect means a part or a feature of the entity, e.g., the rooms or the location of a hotel. A customer tends to be more interested in some particular aspects when making a purchase or booking decision. Therefore, it is nec-essary and appealing to simultaneously consider aspects and sen-timents in sentiment analysis. However, although there have been some studies on cross-lingual sentiment analysis [1, 23], they are designed on document level and consequently cannot handle the aspect dependency problem of sentiment words, which means that a sentiment word may have different sentiment polarities regard-ing different aspects of an entity. For instance, in the reviews on a cell phone, long is positive for its battery X  X  lifespan, but nega-tive for the response time of its operating system. On the other hand, probabilistic topic models have been widely used on aspect-level sentiment analysis in recent years, because they provide an unsupervised and thus flexible way in discovering aspects from re-views [11, 12]. However, most of them perform well in a specific language and cannot be readily transferred to other languages. The main reason is that the resources are unbalanced among different languages. Directly applying these methods on poor-quality corpo-ra often leads to poor results. Therefore, it is a potential solution to use a cross-lingual topic model to help sentiment analysis in a target language by exploiting the resources in a source language. However, the existing cross-lingual topic models are not suitable for sentiment analysis, as they do not take sentiment into account.
To address the above problems, in this paper we propose a nov-el Cross-Lingual Joint Aspect/Sentiment (CLJAS) model that can jointly extract aligned aspects and aspect-dependent sentiments in two different languages via a unified framework. To the best of our knowledge, this is the first study in the open literature on de-veloping an unsupervised cross-lingual topic model for sentiment analysis. The proposed CLJAS model can help sentiment analysis in a resource-poor language by exploiting its correspondences with a resource-rich language, which, however, does not require paral-lel corpora, a machine translation system, and any labeled senti-ment reviews. Extensive experiments on real datasets in differen-t domains and different languages demonstrate that the proposed model can be successfully used in practical applications and sig-nificantly improve the accuracy of sentiment classification in target languages.

The rest of this paper is organized as follows. Section 2 reviews the related work of sentiment analysis on sentence and aspect levels and cross-lingual topic models. Section 3 elaborates the proposed CLJAS model. In Section 4, through comparison with existing rep-resentative models, we validate the effectiveness and investigate the performance of the CLJAS model. Finally, Section 5 concludes the paper.
In this section, we first give an overview of sentiment analysis on sentence level and aspect level, respectively, and then present related studies on cross-lingual topic models.
Sentence-level sentiment analysis has two tasks, i.e., subjectivity classification and sentiment classification. Subjectivity classifica-tion aims at separating objective sentences from subjective ones. It has been earnestly explored in the field of natural language process-ing [24, 25]. Sentence-level sentiment classification considers each sentence as a separate unit and assumes that a sentence contain-s only one opinion. Existing studies on sentence-level sentiment classification can roughly be divided into two categories.
The first category is based on various statistical techniques. For instance, Pang and Lee [18] first showed that sentence-level extrac-tion can improve the performance of sentiment analysis on docu-ment level. They developed a cascaded approach that first filters out objective sentences using a global min-cut inference. After-ward, the extracted subjective sentences are adopted as the input of a document-level sentiment classifier. Hu and Liu [10] proposed a bootstrapping technique to determine the semantic orientation of each opinion sentence using WordNet.

The second category makes use of the advanced natural language processing tools, such as, syntactic parsers. For example, Meena and Prabhakar [15] showed how atomic sentiments of individual phrases can be combined together in the presence of conjuncts to decide the overall sentiment of a sentence. Yao and Li [27] pro-posed a kernel-based approach to classify sentiments of sentences by incorporating multiple features from lexical and syntactic levels. Tan et al. [21] invented a linguistic approach that combines type-d dependencies and subjective phrase analysis to detect sentence-level sentiment polarity.

In general, all of the above studies have demonstrated that re-garding each sentence as a separate unit is reasonable in senti-ment analysis and sentence-level sentiment analysis is helpful for document-level sentiment classification in some cases.
As compared to sentiment analysis on sentence level, sentiment analysis on aspect level is more delicate. There have been a few ap-proaches proposed to detect aspects and aspect-specific sentiment, but not in a unified way. The approach presented in [5] first detect-s aspects using Local Latent Dirichlet Allocation (LDA) [2] and then identifies aspect-sensitive polarities of adjectives using polar-ity propagation based on an aspect-specific polarity graph. How-ever, it simply selects adjectives as opinion words, which cannot cover all opinion words. In [14], Lu et al. proposed an opti-mization framework to combine different signals for determining aspect-aware sentiment polarities. However, the aspects were pre-defined with manually selected keywords, and the opinion words were extracted beforehand. Unlike these studies, our model pre-sented in this paper can detect aspects and aspect-specific sentiment in different languages within a unified framework.

There have also existed unified models developed by incorpo-rating sentiment factors into classic topic models to jointly detec-t aspects and sentiments. The Joint Sentiment/Topic (JST) mod-el [12] is the first LDA based model considering topics and senti-ments simultaneously. The Aspect and Sentiment Unification Mod-el (ASUM) [11] follows a similar generative process to JST. But, in ASUM a sentiment-topic pair is selected for a sentence rather than a word as in JST. In essence, both ASUM and JST aim at de-tecting sentiment-coupled aspects/topics rather than explicitly de-tecting sentiments specific to aspects/topics. MaxEnt-LDA [30] jointly discovers both aspects and aspect-specific opinion words by integrating a supervised maximum entropy component to sep-arate opinion words from objective ones. However, it does not fur-ther identify aspect-aware sentiment polarities of opinion words, which are very important for sentiment analysis. The Joint Aspec-t/Sentiment model proposed in [26] detects aspect-specific opin-ion words and identifies the sentiment polarities of opinion words. H owever, it is designed for monolingual scenarios instead of mul-tilingual ones.
For a cross-lingual topic model, the key is to find a bridge for connecting different languages. Existing models usually combine different languages via parallel or comparable corpora. For exam-ple, CI-LDA [7] preserves connections between languages by shar-ing distributions over latent topics of bilingual documents. Switch LDA [17] extends CI-LDA such that it can control the proportions of languages in each multilingual topic by introducing a per-word switch variable. Symmetric Correspondence LDA proposed in [8] is a topic model that incorporates a hidden variable to control a pivot language. Since parallel or comparable corpora are relatively scarce resources, this kind of models are confined in their applica-tions.

Cross-lingual topic models on unaligned texts may be applied to a broader class of corpora. For instance, Zhang, Mei, and Zhai [29] incorporated soft bilingual dictionary-based constraints into Prob-abilistic Latent Semantic Analysis (PLSA) so that it can extrac-t shared latent topics in text data of different languages. Boyd-Graber and Blei [3] developed the MUltilingual TOpic (MUTO) model to exploit matching across languages on term level to detec-t multilingual latent topics from unaligned texts. However, these models do not consider sentiment factors and thus cannot help cross-lingual sentiment analysis.

Moreover, Boyd-Graber and Resnik [4] proposed an LDA-based holistic model for multilingual sentiment analysis on word level. But, this model is supervised, while the model proposed in this pa-per is unsupervised. Besides, albeit our model is also LDA-based, it is on sentence level, which makes it more appropriate for aspect and sentiment detection from reviews. In this section, we elaborate the proposed Cross-Lingual Joint Aspect/Sentiment (CLJAS) model, which can jointly detect aspect-s and aspect-specific sentiments of two languages simultaneously. The key idea behind CLJAS is two-fold. First, for all reviews from the same domain, we assume that they share the same topic distri-bution despite they are in different languages. In this way, the word-s in the reviews from the same domain but in different languages tend to be assigned with the same topic. Second, a bilingual dic-tionary is adopted to connect the topics in two languages. Through dictionary-based translation, we can exploit correspondences be-tween two different languages to learn semantically aligned topic distributions. Note that in this study we cope with the language transferring problem using a bilingual dictionary for two reasons. First, a bilingual dictionary can usually provide multiple transla-tion words to a source word, which enables the proposed model to select the most suitable one for the situation at hand; Second, a bilingual dictionary based model does not require large-scale paral-lel corpora and is thus more flexible. Figure 1 depicts the graphical representation of the CLJAS model and the notation used in it is ex-plained in Table 1. In what follows we describe the model in more detail. Since CLJAS is basically a left-right symmetrical model, as shown in Figure 1, where the left and right sides correspond to the source and target languages, respectively, in order to make it easy to understand we first describe it from one side and then explain its cross-lingual mechanism.

Assume that we have a corpus of D customer reviews from a specific domain and in different language. Each review d is a list of sentences, each sentence s in review d is a list of words, and the n th word w d,s,n in sentence s of review d is an entry from a vocabulary with V distinct words. Each sentence s in any review d i s associated with an aspect variable z d,s . Similar to ASUM [11] and JAS [26], the CLJAS model also assumes that all words in a sentence are generated from the same topic. Therefore, the aspect variable, z d,s , is shared by all words in sentence s .
Essentially, CLJAS is an LDA-based model. In the tradition-a l LDA model, topics are associated with documents and words are associated with topics. CLJAS extends LDA by first adding two variables, i.e., sentiment label and subjectivity label. Specifi-cally, in the CLJAS model, topics are associated with documents, sentiment labels are associated with subjectivity labels, and words are associated with topics, sentiment labels and subjectivity label-Algorithm 1 T he generation process of the CLJAS model on the target language side. s. For each aspect z , CLJAS learns three multinomial distributions over words, which model its objective semantics,  X  z , and its pos-itive and negative sentiments,  X  z, pos and  X  z, neg , respectively. In the CLJAS model, one draws an aspect z d,s for each sentence s of any review d from a document-specific distribution over aspects  X  . Then, for each word w d,s,n in sentence s , one draws a subjec-tivity label, v d,s,n , and a sentiment label, l d,s,n , where v cates that w d,s,n is a sentiment-bearing word or an objective one, whilst l d,s,n further suggests that w d,s,n , if it is sentiment-bearing, conveys a positive or a negative sentiment. Finally, one draws the word, w d,s,n , from a word distribution corresponding to objective semantics  X  z , or the positive  X  z, pos or negative  X  z, neg the topic z .

Note that the CLJAS model differs from the state-of-the-art JST and ASUM models in that it first explicitly separates sentiment-bearing words from objective ones and then assigns their sentiment polarities. In addition, since CLJAS is a cross-lingual model, an ex-tra language label x d,s,n is introduced to word w d,s,n . Its role will be described later on. Therefore, for each word w d,s,n , there are three indicator variables associated with it, namely, language label x d,s,n , subjectivity label v d,s,n and sentiment label l d,s,n rithm 1 presents the generation process of the CLJAS model on t he target language side. The generation process on the source lan-guage side is not presented, but it can be deduced likewise.
In order to leverage the resources in the source language to im-prove sentiment analysis in the target language, CLJAS jointly de-tects aspects and sentiments of the two languages simultaneous-ly. Upon the above description on one side, next we explain its cross-lingual mechanism. In CLJAS, each aspect z d,s correspond-s to two clusters of word distributions, namely, (  X  src ,z  X  for the target language. The binary switch variable, x d,s,n to indicate that the intermediate variable y d,s,n is generated from the word distributions of the source language or the target language. In more detail, let X  X  take a word, w d,s,n , of review d in the target language as an example. If x d,s,n = tgt , suggesting that y is directly generated from the word distributions of the target lan-guage, then we choose a specific word distribution from the target to y d,s,n ; If x d,s,n = src , suggesting y d,s,n is generated from the word distributions for the source language, then we choose a specific word distribution from the source language cluster (  X   X 
In the model, asymmetric hyperparameters  X  src / tgt src / tgt prior knowledge on choosing between the source and target lan-guages. They enforce knowledge transfer from the auxiliary re-views in the source language to sentiment analysis in the target language. In particular, for reviews in the source language, we set  X  tgt &gt;  X  src src such that they can contribute more to topic modeling on reviews in the target language; for reviews in the target language, we set  X  tgt src &gt;  X  tgt tgt such that they can adopt more knowledge from the source language.

Finally, the CLJAS model employs Gibbs sampling [9] for in-ference. Specifically, for the reviews in the target language, we first draw z d,s by the conditional probability presented in Equa-tion (1). Then, we can jointly draw values for v d,s,n , l and y d,s,n as the conditional probabilities presented in Equation-s (2)-(5), respectively. For the sake of space limitation, we will not go through the corresponding details.
In this section, we validate the proposed CLJAS model via ex-tensive experiments. Specifically, we first examine its validity on discovering aspects, validate the effectiveness of its cross-lingual mechanism, and study the effect of the bilingual dictionary on its behavior. Next, we investigate its effectiveness and performance by applying it to practical sentiment classification tasks and quantita-tively comparing with four baselines.
In order to validate the proposed CLJAS model, we conducted experiments on real datasets that contain hotel reviews and product reviews in different languages (including English, Chinese, French, German, Spanish, Dutch, and Italian), collected from a few well-known websites. In the experiments, English is taken as the unique source language and the others are target ones. The product re-views cover four domains, i.e., electronics, kitchen, network, and health. In more detail, for English, the hotel dataset contains 12000 reviews collected from Booking.com 1 , and the product dataset in h ttp://www.booking.com each domain has 2000 reviews collected from Amazon 23 . For Chi-nese, the hotel dataset contains 12000 reviews, downloaded from an open source website 4 , and the product dataset contains 2000 re-views for each domain, collected from Jingdong 5 , one of the most popular e-shopping websites in China. For other languages (in-cluding French, German, Spanish, Dutch, and Italian), the datasets are collected from Booking.com, each of which contains 4000 ho-tel reviews. All of the datasets were preprocessed with sentence segmentation. For Chinese, we further used ICTCLAS 6 to conduct word segmentation over sentences.

Besides datasets, other three resources are needed in the CLJAS model, including a bilingual dictionary between the source and tar-get language, a few seed words and an opinion lexicon of each language. The bilingual dictionary is used to establish correspon-dences between two different languages. As in a bilingual dictio-nary a source word usually corresponds to multiple target word-s (translation words), some of which may not be suitable for the review scenarios, we employed a simple filtering strategy to over-come the interference of those irrelevant translation words. Specif-ically, given a source word, we first computed the Term Frequency (TF) for each of its corresponding translation words in the corpora in the target language; Next, we chose at most K terms with the highest TF values as candidate translation words; Finally, a trans-lation probability was assigned to each candidate translation word w i according to T F ( w i ) / P K i =1 T F ( w i ) . In our experiments, K was set to be 5.

The seed words of each language used in the CLJAS model are subjective words with clear sentiment polarity. They are adopted to identify sentiment polarities of other subjective words. For En-glish, we select 10 positive seed words and 10 negative seed words with highest TF in the corpus. For other languages, the seed words are obtained by translating the English ones. Table 2 presents the full lists of seed words of all target languages, where for each seed word an English translation word is provided in subsequent brack-ets, which was obtained by Google Translate .

In the proposed model, an opinion lexicon is used to separate subjective words from objective ones 7 . The English and Chinese opinion lexicons used in the experiments are acquired from the commonly-used knowledge base, HowNet 8 . The English one con-tains 3687 terms, while the Chinese one contains 5997 terms. For other languages, the opinion lexicons are obtained by translating the English one using Google Translate . If an opinion lexicon of a certain language is not available, all adjectives in the corpus in this language can be considered as subjective words.
In the experiments, we ran 100 iterations of Gibbs sampling, which were experimentally proven adequate for obtaining stable results. We set  X  z = 50 /T for each aspect z and  X  w = 0 . 1 for each word w in the vocabulary, as in [9]. The default values for  X  w and  X  neg w were set to 0.1. For each positive seed word w ,  X  was set to 0; for each negative seed word w ,  X  pos w was set to 0. For each positive non-seed word w ,  X  neg w was set to 0.01; For each neg-ative non-seed word w ,  X  pos w was set to 0.01. The value of  X  was set to 0.001 for each sentiment label. Without loss of generality, h ttp://www.seas.upenn.edu/  X  mdredze/datasets/sentiment/ http://llt.cbs.polyu.edu.hk/  X  lss/ACL2010_Data_SSLi.zip http://nlp.csai.tsinghua.edu.cn/ http://www.360buy.com http://ictclas.cn/index.html
Note that the CLJAS model does not use the polarity information of words in the lexicon. http://www.keenage.com/html/e_index.html  X  Q ) )  X   X   X  ) )  X   X   X  )  X   X  P , l )  X 
Chinese
French
German
Spanish
Italian
Dutch Topic 2 Topic 6 Topic 7 the above parameters were set the same as in [26]. For the targe t language, the values of  X  tgt src and  X  tgt tgt are asymmetrical. We fixed  X  tgt = 0 . 001 , but changed  X  tgt src from 0.001 to 1 to investigate the effect of language transfer on model performance.
For the purpose of comparison, we adopted four representative models as the baselines. We chose USL (Universal Sentiment Lex-icon) and SVM as baselines, because USL is a typical unsupervised approach to sentiment classification, where a universal opinion lex-icon collected from HowNet is adopted to determine the polarity of a review, and SVM is a typical supervised model for sentimen-t classification. We also employed the state-of-the-art ASUM and JAS models as baselines, as they both consider aspect in sentiment analysis. Note that both ASUM and JAS are monolingual models, whilst CLJAS is a cross-lingual model. The purpose of compar-ing CLJAS with these two monolingual aspect-sentiment models is mainly to examine the effectiveness of the cross-lingual mechanism implemented in CLJAS.
This experiment is to investigate the performance of the devel-oped CLJAS model on discovering aspects on given datasets. For the purpose of illustration, Table 3 presents three major aspects dis-covered by CLJAS on hotel reviews, where, for each aspect, top 30 words are listed and sorted by its word probability distribution in a descending order.

It can be seen in Table 3 that the CLJAS model can effectively detect major aspects from the reviews in both the source and tar-get languages. The extracted words under each aspect are quite coherent and meaningful. Moreover, the CLJAS model can obtain bilingually aligned aspects due to the mechanism of sharing the same topic distribution. This mechanism enables us to improve the aspect modeling on the data in the target language by leveraging the rich resources in the source language.
This experiment aims to evaluate the proposed cross-lingual mech-anism. For this purpose, we calculated the perplexity [2] of the test set under different  X  tgt src , which controls the degree of knowledge transfer between the source and target languages, i.e., the extent that the knowledge learned from the dataset in the source language is used to learn knowledge for the dataset in the target language. It is known that the lower the perplexity, the better the performance of the topic model [2].

In the experiment, for each domain, 2000 reviews were used for model training and the rest reviews for inference. Table 4 presents the perplexity values of the test sets in Chinese under different do-mains. As seen in Table 4, with the increase of  X  tgt src under different domains basically decreases in a monotonic man-ner. Therefore, the performance of the model was improved by in-troducing the cross-lingual mechanism. The improvement mainly benefits from the knowledge transferred from the data in the source language. According to the experimental results, we observe that within a certain range, the larger the  X  tgt src , the more improvement that can be achieved. However,  X  tgt src cannot be too large; Other-wise, it leads to the model to learn knowledge almost completely from the source language and neglect the characteristics of the tar-get language.

We further evaluated the proposed cross-lingual framework on other languages. Taking English as the source language, Table 5 presents the perplexity values of test sets in different languages un-der the hotel domain. It can be observed from Table 5 that, with the increase of  X  tgt src , the perplexity monotonically decreases un-der different languages. Therefore, we can claim that the proposed cross-lingual mechanism is effective for different domains and for different languages. Table 4: The perplexity of test sets in Chinese under differen t domains with exponentially increasing  X  tgt src . Table 5: The perplexity of test sets in different languages un der the hotel domain with exponentially increasing  X  tgt src .
A s aforesaid, in the CLJAS model a bilingual dictionary is need-ed to bridge the two languages. In order to investigate the depen-dency of the performance of the CLJAS model on the quality and scale of the bilingual dictionary, we compared the perplexity of the same test datasets of different domains in Chinese with two differ-ent bilingual dictionaries, which contain 224,385 and 41,814 pairs of translation words, respectively. Figure 2 presents the experimen-tal results. We can observe that there is no significant difference between the results corresponding to the two dictionaries of differ-ent size. This observation suggests that the proposed CLJAS model is basically stable regarding the quality and scale of the bilingual dictionary. Figure 2: The perplexity of Chinese test sets in different do-m ains with different bilingual dictionaries.
In this experiment, we investigate the validity and performance of the CLJAS model by evaluating the quality of the aspect-dependent sentiment lexicons extracted using CLJAS. Specifically, we first adopted CLJAS to extract aspect-dependent sentiment lexicons and further applied them to practical sentiment classification tasks. Nex-t, we evaluated the quality of the aspect-dependent sentiment lex-icons according to how much they improve the performance of practical sentiment classification, as compared to the aforesaid four baselines. Table 6 (a) presents the results of both CLJAS and the baselines in sentiment classification tasks in different domains. We also compared CLJAS with the baselines on hotel review datasets in different languages. The multilingual experimental results are pre-sented in Table 6 (b). In these two tables, the accuracy of sentiment classification on both positive and negative reviews as well as their combination is listed for all methods. The bottom row presents the accuracy of sentiment classification averaged on different domain-s/languages.

As CLJAS is an unsupervised model, we first compared it with the three unsupervised baselines, i.e., USL, ASUM, and JAS. It can be seen in Table 6 (a) that for datasets in different domains CLJAS consistently outperforms USL, as it increases the accuracy of sen-timent classification by 0.087. This is because the sentiment lex-icon extracted by CLJAS for determining the sentiment polarities of reviews is aspect-dependent, while the sentiment lexicon used in USL is aspect-independent. For datasets in other languages, the performance superiority of CLJAS is more obvious. Specifically, as observed in Table 6 (b), CLJAS performs better than USL by a per-formance improvement of 0.173 on average. In USL, since the mul-tilingual opinion lexicons are obtained by translating the English one, they suffer from the loss of accuracy caused by the translation. Therefore, the performance of the unsupervised method based on translation for multilingual sentiment analysis is unsatisfactory. As for ASUM and JAS, we can find in Table 6 (a) that CLJAS im-proves the accuracy of sentiment classification by 0.061 and 0.063, respectively, while in Table 6 (b) the corresponding accuracy im-provement is 0.126 and 0.141. That is to say, CLJAS performs bet-ter than ASUM and JAS as well. This is because CLJAS combine the corpora of two languages together and can thus capture more statistics characteristics than ASUM and JAS that depends only on monolingual corpora.

We also compared CLJAS with the typical supervised learning approach, SVM. Specifically, for each domain, 50% data was used for training and the rest 50% data was used for testing. Particularly, we first employed LibSVM 9 to train a sentiment classifier and then predicted sentiment polarities on test data. It should be mentioned that we tried different kernel functions to refine the performance of SVM and finally adopted a linear one as the kernel function, with which LibSVM gets the best results. From Table 6 (a), we can ob-serve that CLJAS outperforms SVM on average, as it increases the accuracy of sentiment classification by 0.03. This counterintuitive phenomenon may be because the training data of product reviews in different domains used in SVM is of poor quality, whilst the unlabeled data used in CLJAS is of relatively high quality. From Table 6 (b), we can observe that the performance of CLJAS is infe-rior to that of SVM. This is intuitive, as CLJAS is an unsupervised method while SVM is supervised.

In general, CLJAS performs better than USL, ASUM, and JAS in different domains and different languages. Therefore, it can be concluded that no matter in what domains and in what languages, the CLJAS model is effective and exhibits better performance than the state-of-the-art unsupervised models, which highlights its value in practical applications. h ttp://www.csie.ntu.edu.tw/  X  cjlin/libsvm/
I n this paper, we have proposed for the first time an unsuper-vised Cross-Lingual Aspect/Sentiment (CLJAS) model for senti-ment analysis. The CLJAS model requires neither parallel corpora nor labeled sentiment reviews. Through the cross-lingual mech-anism, the model can improve sentiment analysis in a target lan-guage by leveraging data and knowledge available in a source lan-guage. Particularly, CLJAS can obtain bilingually aligned aspect-s from the reviews in the same domain due to the mechanism of sharing the same topic distribution. CLJAS can also extract aspect-dependent sentiment lexicons that are essential for sentiment anal-ysis. We have further compared the CLJAS model with both super-vised and unsupervised representative baselines by applying them to practical sentiment classification tasks. Experimental results have demonstrated that as compared to the unsupervised baselines, CL-JAS can significantly improve the accuracy of sentiment classifica-tion, while comparing to the supervised baseline CLJAS also ex-hibits comparable performance, which have convincingly validated its effectiveness and highlighted its value.
 This work is supported by National Program (973 Program) on Key Basic Research Project of China under grant No. 2013CB329602 and 2012CB316303, National Natural Science Foundation of Chi-na (No. 61173008, 61100175, 61232010, 61303244). It is also sup-ported by National High-Tech Research and Development Program of China under grant numbered 2013AA013204, National HeGao-Ji Key Project under grant numbered 2013ZX01039-002-001-001, and "Strategic Priority Research Program" of the Chinese Academy of Sciences under grant numbered XDA06030200. [1] C. Banea, R. Mihalcea, J. Wiebe, and S. Hassan.
 [2] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet [3] J. Boyd-Graber and D. M. Blei. Multilingual topic models [4] J. Boyd-Graber and P. Resnik. Holistic sentiment analysis [5] S. Brody and N. Elhadad. An unsupervised aspect-sentiment [6] Z. C  X  acilia, s. N. Mathia, S. Heiner, and S. Michael. [7] E. Erosheva, S. Fienberg, and J. Lafferty. Mixed-membership [8] K. Fukumasu, K. Eguchi, and E. Xing. Symmetric [9] T. L. Griffiths and M. Steyvers. Finding scientific topics. In [10] M. Hu and B. Liu. Mining and summarizing customer [11] Y. Jo and A. H. Oh. Aspect and sentiment unification model [12] C. Lin and Y. He. Joint sentiment/topic model for sentime nt [13] B. Liu. Sentiment analysis and subjectivity. Handbook of [14] Y. Lu, M. Castellanos, U. Dayal, and et al. Automatic [15] A. Meena and T. Prabhakar. Sentence level sentiment [16] R. Moraes, J. F. Valiati, and W. P. G. Neto. Document-level [17] D. Newman, C. Chemudugunta, P. Smyth, and M. Steyvers. [18] B. Pang and L. Lee. A sentimental education: Sentiment [19] B. Pang, L. Lee, and S. Vaithyanathan. Thumbs up?: [20] K. Suin, Z. Jianwen, C. Zheng, H. O. Alice, and L. Shixia. A [21] L. K.-W. Tan, J.-C. Na, Y.-L. Theng, and K. Chang.
 [22] P. D. Turney. Thumbs up or thumbs down?: semantic [23] X. Wan. Co-training for cross-lingual sentiment [24] J. Wiebe and E. Riloff. Creating subjective and objective [25] J. Wiebe, T. Wilson, R. Bruce, M. Bell, and M. Martin. [26] X. Xu, S. Tan, Y. Liu, X. Cheng, and Z. Lin. Towards jointly [27] T. Yao and L. Li. A kernel-based sentiment classification [28] A. Yessenalina, Y. Yue, and C. Cardie. Multi-level structured [29] D. Zhang, Q. Mei, and C. Zhai. Cross-lingual latent topic [30] W. X. Zhao, J. Jiang, H. Yan, and X. Li. Jointly modeling
