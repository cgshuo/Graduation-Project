 The automated analysis of social networks has become an important problem due to the proliferation of social net-works, such as LiveJournal, Flickr and Facebook. The scale of these social networks is massive and continues to grow rapidly. An important problem in social network analysis is proximity estimation that infers the closeness of differ-ent users. Link prediction, in turn, is an important ap-plication of proximity estimation. However, many meth-ods for computing proximity measures have high compu-tational complexity and are thus prohibitive for large-scale link prediction problems. One way to address this problem is to estimate proximity measures via low-rank approxima-tion. However, a single low-rank approximation may not be sufficient to represent the behavior of the entire net-work. In this paper, we propose Multi-Scale Link Prediction (MSLP), a framework for link prediction, which can handle massive networks. The basic idea of MSLP is to construct low-rank approximations of the network at multiple scales in an efficient manner. To achieve this, we propose a fast tree-structured approximation algorithm. Based on this ap-proach, MSLP combines predictions at multiple scales to make robust and accurate predictions. Experimental results on real-life datasets with more than a million nodes show the superior performance and scalability of our method. H.2.8 [ Database Management ]: Database Application X  Data mining ; J.4 [ Computer Applications ]: Social and Behavioral Sciences X  Sociology Algorithms, Experimentation Low Rank Approximation, Hierarchical Clustering, Link Pre-diction, Social Network Analysis
Social network analysis has become essential due to the proliferation of social networks, such as LiveJournal, MyS-pace, Flickr and Facebook. The scale of these social net-works is massive and continues to grow rapidly. For exam-ple, Facebook now has more than 900 million active users with over 700,000 new users joining everyday. This has sig-nificantly changed the way people interact and share infor-mation with others and has led to unprecedented research opportunities.

An important problem for social network analysis is prox-imity estimation that infers the  X  X loseness X  of different users. Proximity measures quantify the interaction between users based on the structural properties of a graph, such as the number of common friends. An important application of proximity estimation in social networks is link prediction, which is a key problem in social network analysis [15, 23]. Using proximity estimation for link prediction is based on the assumption that a pair of users with a high proximity score indicates they are close in terms of social relatedness and hence this pair of users will have a good chance to be-come friends in the future.

Simple proximity measures, such as neighborhood-based measures, e.g., Adamic-Adar score [1] and common neigh-bors [18], can be computed efficiently. However, they de-scribe a very localized view of interaction. There are more comprehensive proximity measures that capture a broader perspective of social relationships by considering all paths between users. These path based methods, such as Katz [12] or rooted PageRank [15], are often more effective. Nonethe-less, they are also well known for their high computational complexity and memory usage, which limits their applica-bility to massive graphs with more than a million users.
To solve this problem, a great deal of work has been done on scalable proximity estimation [4, 22]. One basic idea is to perform dimensionality reduction on the original graph and then compute the proximity based on its low-rank ap-proximation. Recently, clustered low rank approximation (CLRA) has been proposed which develops a fast and mem-ory efficient method [20, 23].

However, a single low-rank approximation may not be suf-ficient to represent the whole network. Furthermore, the approach in [20] only uses a single clustering structure mak-ing it sensitive to a particular clustering and biased against links that happen to be between clusters. More notably, a recent study has shown that large social networks tend to lack large well-defined clusters which suggests that a single clustering structure can be problematic [14].
To address the above problems, we propose a multi-scale approximation of the graph to obtain multiple granular views of the network in order to perform link prediction in a scal-able and accurate manner. This is achieved by taking a hi-erarchical clustering approach and generating low-rank ap-proximations at each level in the hierarchy. Although we use a single hierarchical representation of the graph, we do not require it to be the optimal structure. The main purpose of using hierarchical clustering is not to detect the underlying community structure of the graph, but to use it as a tool for efficient multi-scale approximation. In the experimental section, we show that under clustering structures of vary-ing quality, our proposed algorithm can still achieve better results compared to other link prediction algorithms (for ex-ample, in the Epinions network [19] which was also used in [14]).

Specifically, in this paper we propose a robust, flexible, and scalable framework for link prediction on social networks that we call, multi-scale link prediction (MSLP). MSLP ex-ploits different scales of low-rank approximation of social networks by combining information from multiple levels in the hierarchy in an efficient manner. Higher levels in the hi-erarchy present a more global view, while lower levels focus on more localized information. MSLP works by first per-forming hierarchical clustering on the graph by utilizing a fast graph clustering algorithm, and then performing multi-scale approximation based on the produced hierarchy. Since different levels have different approximation, each level will give different approximated proximity scores. MSLP com-bines approximated proximity scores from each level and makes the final prediction based on the combined scores. As a result, MSLP captures both local and global information of the network.

We list the benefits of our framework as follows:
The rest of the paper is organized as follows. In Section 2, we survey some related work on link prediction. Next, some background material is introduced in Section 3. In Section 4, we propose our proposed MSLP algorithm. Ex-perimental results on real-world large-scale social networks are presented in Section 5. Finally, we present our conclu-sions in Section 6.
Link prediction refers to the problem of inferring new in-teractions among members in a network. The first system-atic treatment of the problem appeared in [15], where a vari-ety of proximity measures, such as Common Neighbors [18] and the Katz measure [12] were used as effective methods for link prediction. In addition to unsupervised approaches, there is also rising interest in supervised approaches for link prediction [3, 10, 16]. In supervised link prediction, node and/or edge features are extracted from the network and link prediction is treated as a classification problem. How-ever, engineering good features and handling the class im-balance problem are still challenging tasks. Recently, link prediction has been shown to benefit from exploring addi-tional information external to the network, such as node or edge attributes [11, 21]. However, these approaches require additional information, which may be difficult to obtain due to privacy and security issues.

Many popular proximity measures that are used for link prediction have high computational complexity and do not scale well to large-scale networks. A great deal of recent work has been devoted to speed up the computation. For example, [24] truncates the series expansion of Katz and only considers paths of limited length. In [8, 15, 22], dimensional-ity reduction methods, such as the eigen-decomposition, are used to construct low-rank approximations of a graph, which are then used to compute approximated proximity measures. The more recent work in [4] applies the Lanczos/Stieltjes procedure to iteratively compute upper and lower bounds of a single Katz value and shows that these eventually converge to the real Katz value.

Efficient proximity estimation is essential for scalable link prediction. However, one should be able to make accurate and robust predictions with the estimated measures. For example, [22] explores the low-rank approximation of social networks to speed up large-scale link prediction. Another way to improve the link prediction performance is to ex-plore the community structure of a network. For example, LinkBoost [6] explores the community structure by a novel degree dependent cost function and shows that minimiza-tion of the associated risk can lead to more links predicted within communities than between communities. However, considering a single community structure may not lead to robust predictions, because even detecting the  X  X est X  com-munity structure itself is still an open question.

Very little work has been done using hierarchical struc-tures for link prediction. One exception is the method pro-posed by [5], which works by sampling a number of compet-itive hierarchical random graphs from a large pool of such graphs. Each sampled graph is associated with a proba-bility indicating the strength of community structure over the original network. The probability of a link appearing between any two nodes is averaged over the corresponding connecting probability on the sampled graphs. However, to predict potential links, this algorithm needs to enumerate and average over almost all possible hierarchical partitions of a given network and thus is very costly to compute even for small networks. Compared with [5], our algorithm is much more efficient in terms of speed and thus can be scaled up to large-scale link prediction problems with millions of users.
Assume we are given a graph G = ( V , E ), where V = { 1 ,  X  X  X  ,n } is the set of vertices representing the users in a social network and E = { e ij | i,j  X  V} is the set of weighted edges quantifying the connection between user i and user j . Let A = [ a ij ] be the corresponding n  X  n adjacency matrix of G such that a ij = e ij , if there is an edge between i and j and 0 otherwise. For simplicity, we assume G is an undirected graph, i.e., A is symmetric.

As shown in [15], proximity measures can be computed from A . Many of these measures can be represented as a matrix function f ( A ), where the ( i,j )-th element represents the value of a proximity measure between user i and user j [9]. One popular measure is the number of common neigh-bors, which can be captured by f cn ( A ) = A 2 , describing a very localized view of interactions between vertices by con-sidering only paths of length 2. A more extensive measure is the popular Katz measure [12]. Such path-based proximity measures often achieve better accuracy at the cost of higher computational complexity. The Katz measure is defined as follows where I is the identity matrix and  X   X  1 / k A k 2 is a damping parameter. As we can see, the Katz measure takes O ( n 3 time, which is computationally infeasible for large-scale net-works with millions of nodes.

Here, dimensionality reduction methods, such as the sin-gular value decomposition (SVD), play an important role. These methods are particularly useful, since it suffices to have a reasonably good estimation of a given proximity mea-sure for most applications. Furthermore, low-rank approxi-mation of the adjacency matrix serves as a useful conceptual and computational tool for the graph.

Assume that we are given a rank-r approximation of the n  X  n matrix A as follows where U is an n  X  r orthonormal matrix (i.e. U T U = I r is an identity matrix), and S is an r  X  r matrix. Using this low-rank approximation  X  A , the CN measure can be approx-imated as f cn ( A )  X  US 2 U T . Similarly, the Katz measure is approximated by In general, f ( A )  X  Uf ( S ) U T , which requires less computa-tional resources as the matrix function is only evaluated on the much smaller S matrix.

However, computing the low-rank approximation of a mas-sive graph via SVD or other popular dimensionality reduc-tion methods can still be a computational bottleneck. Re-cently, the technique of clustered low rank approximation (CLRA) was proposed in [20] as a scalable and accurate low-rank approximation method. The basic idea of CLRA is to preserve important structural information by cluster-ing the graph G into c disjoint clusters. Then it computes a low-rank approximation of each cluster, which is finally extended to approximate the entire graph.

Assume that the graph has been clustered into c clusters and the vertices are ordered as follows where the diagonal blocks A ii , i = 1 ,...,c , correspond to the local adjacency matrix of each cluster i . For every clus-ter, the best rank-r approximation is computed as A U
 X  i U T i , where  X  i is a diagonal matrix with the r largest eigenvalues of A ii , and U i is an orthonormal matrix with the corresponding eigenvectors. Finally, CLRA aligns the low-rank approximation of each cluster together to obtain the clustered low-rank approximation of the entire adjacency matrix A . Mathematically,
A  X  where S ij = U T i A ij U i , for i,j = 1 ,...,c , which is the op-timal S in the least squares sense. Note that the block-diagonal matrix U = diag( U 1 ,...,U c ) is also orthonormal and S ii =  X  i are diagonal. It is shown in [20] that CLRA achieves accurate approximations while being efficient in both computational speed and memory usage. However, the drawback of CLRA is that it only uses one clustering structure, whereas it has been shown that many large social networks lack such structure [14]. In this paper, we over-come this limitation by taking a multi-scale approach.
Based on the estimated proximity measures, we can per-form link prediction on social networks. Link prediction deals with networks that evolve over time Specifically, given a snapshot of a network at time t 1 , the task is to predict links that would form at a future time step t 2 . A high prox-imity score between two users captures the high correlation between them and thus a high chance to form a new link in the future.
In this section we present our multi-scale link prediction (MSLP) framework for social networks. Our method mainly consists of three phases: hierarchical clustering, subspace approximation and multi-scale prediction. Specifically, we first construct a hierarchy tree with a fast top-down hierar-chical clustering approach. Then, a multi-scale low-rank ap-proximation to the original graph is computed when travers-ing the hierarchy in a bottom-up fashion. An important technical contribution of our paper is a fast tree-structured approximation algorithm that enables us to compute the subspace of a parent cluster quickly by using subspaces of its child clusters. This allows us to compute each level X  X  low-rank approximation efficiently. Finally, we combine prox-imity measures, which are computed using the multi-scale low-rank approximation of the graph, and make our final predictions.
The first step of our method is to hierarchically cluster or partition a given graph. The purpose of this is to effi-ciently generate a multi-scale approximation of the graph using the constructed hierarchical structure. This, in turn, makes predictions more accurate and robust as we combine predictions at each level of the hierarchy in the final step.
Generally, there are two main approaches for hierarchical clustering: agglomerative (or bottom-up) approach and divi-sive (or top-down) approach. The agglomerative approach initially treats each vertex as one cluster and continually merges pairs of clusters as it moves up the hierarchy. The Table 1: Percentage of within-cluster edges using Graclus. Numbers in brackets represent random clustering. It can be seen that Graclus is quite ef-fective in finding good clustering structure. (these networks contain about 2 million nodes  X  details are given in Table 3.) divisive approach takes the opposite direction, that is, all vertices are placed in a single cluster and recursively parti-tioned into smaller clusters. Due to the large scale of the problem and the availability of efficient clustering software, such as Graclus [7], we employ the divisive approach in our work.

Given a graph G = ( V , E ), our goal is to construct a level ` hierarchy, so as to generate a multi-scale view of the graph. We form c nodes (clusters) at the first level of the hierar-chy by clustering V into c disjoint sets V (1) 1 , V (1) 2 where the superscript denotes the level of the hierarchy. Then, we proceed to the second level of the hierarchy tree by further clustering each node in the first level V (1) i 1 ,...,c , and generate c child nodes from each of them as V sequence, we will have c 2 nodes on the second level of the hierarchy. This process repeats until the desired number of levels ` is reached. Many classic clustering methods can be used as a base clustering method. In this paper, we use the Graclus algorithm [7] to cluster each node because of its ability to scale up to very large graphs. However, our algorithm can utilize any other graph clustering method.
The hierarchy of A at level p , after sorting the vertices, can be written as where  X  c = c p is the number of nodes in level p and each diagonal block A ( p ) ii , i = 1 ,...,  X  c , is an m i  X  m can be viewed as a local adjacency matrix of cluster i at contains the set of edges between clusters i and j .
As it is desirable to capture most of the links within clus-ters, we compare with random clustering in terms of the percentage of within-cluster links on three large-scale social networks in Table 1. For each level of the hierarchy tree, the within-cluster links are those that connect two vertices in the same cluster. As shown in Table 1, the percentage of within-cluster edges of random clustering is much smaller than the hierarchical clustering scheme used in this paper, and the gap becomes much larger when going down the hi-erarchy. Even at the deepest level, the clustering scheme we use can still capture more than half of the edges compared with less than 10% in the LiveJournal and MySpace graphs when using random clustering.
 As a final remark, we note that the hierarchical clustering scheme is also very fast. Clustering the three networks of Table 1 into 5 levels with 2 clusters at each level can be completed in just 5 minutes on a 8-core 3.40GHz machine. In the next section, we show how to use the hierarchy structure to efficiently construct a multi-scale approximation of large-scale graphs.
After constructing the hierarchy for a given graph, we can compute low-rank approximations of A at each level of the hierarchy to obtain a multi-scale approximation. Specifi-cally, we employ CLRA to obtain the approximation. Fig-ure 1 gives an example of a simple three level hierarchy to better illustrate our method. By applying CLRA on each of the 3 levels in the example, we have 3 clustered low-rank approximations of A as follows where the columns of U ( p ) i are the set of orthonormal basis vectors forming the subspace for cluster i at level p and S as a special case of CLRA, where the entire graph is treated as a single cluster, which yields a global view of the entire matrix A . Lower levels in the hierarchy will preserve more local information within each cluster. Thus, each level of approximation concentrates on different levels of granularity, resulting in a multi-scale approximation of A .

An important issue here is how to compute each level X  X  approximation of A efficiently. A straightforward solution would be use standard dimensionality reduction methods, Figure 2: Approximating parent cluster X  X  subspace. Figure 3: Principal angles between parent cluster X  X  subspace and two other subspaces: child cluster X  X  subspace diag ( U ( C ) 1 ,U ( C ) 2 ) and parent cluster X  X  sub-space U ( P ) tree computed using Algorithm 1. such as SVD. This can be computed efficiently for clusters at the deepest level of the hierarchy tree, since the size of each cluster is relatively small. However, the computational cost becomes prohibitive as the size of the cluster increases, which is the case for upper levels in the hierarchy tree. We propose a more scalable and effective method to address this issue. For clarity and brevity, we focus on a local view of the hierarchy as shown in Figure 2, where A ( P ) is a parent cluster and A ( C ) 11 and A ( C ) 22 are its two child clusters.
A key observation we make is that the subspaces be-tween any two adjacent levels in the hierarchy tree should be close to each other. That is, U ( P ) should be close to are in A ( P ) should be captured by its child clusters A and A ( C ) 22 . Consequently, if we are given diag( U ( C ) one should be able to compute U ( P ) faster than computing it from scratch. Thus, we propose an algorithm that uses the child cluster X  X  subspace to compute the parent cluster X  X  subspace.

Our proposed method, tree-structured approximation of subspace , is listed in Algorithm 1. The main idea is to con-struct a matrix Y = A ( P )  X  that covers as much of the range space of A ( P ) as possible. This can be done efficiently using  X  are sparse. Then, an orthonormal matrix Q is computed from Y as a basis for the range of Y (e.g. using the QR-decomposition). Finally, Q is further used to compute U ( P ) via the eigen-decomposition of the matrix B = Q T AQ . This last step is also fast since B is a small cr  X  cr matrix, where r is the rank of the approximation of each child node.
The subspace approximation scheme in Algorithm 1 is more efficient than the truncated eigen-decomposition (EIG), since the latter needs to be computed from scratch and is time consuming when dealing with large-scale matrices. We note that Y = ( AA T ) A  X  can be used for higher accuracy, though we did not find any significant improvement in the results.

Figure 3 shows principal angles between the parent clus-ter X  X  subspace U ( P ) eig computed via eigen-decomposition and the child cluster X  X  subspace diag( U ( C ) 1 ,U ( C ) 2 ) for the Flickr dataset. The cosine of principal angles are close to 1, sup-porting our observation that the subspaces of two adjacent
Algorithm 1: Tree-structured approximation of domi-nant subspace of parent cluster from child clusters Input : n  X  n adjacency matrix of parent cluster
Output : dominant subspace for parent cluster A ( P ) , Compute n  X  cr matrix Y = A  X .
 Compute Q as an orthonormal basis for the range of Y . Compute B = Q T AQ . // A  X  Q ( Q T AQ ) Q T Compute rank-r eigen-decomposition of B  X  V  X  V T . Compute U ( P ) = QV .
 Algorithm 2: Multi-Scale Link Prediction (MSLP)
Input : adjacency matrix A , number of levels ` , number
Output : top-k predictions. /* Hierarchical clustering */ for i = 0 to ` do end /* Subspace approximation */ // approximation for deepest level.
 // approximation for intermediate levels. for i = `  X  1 to 0 do end /* Multi-scale prediction */ for i = ` to 0 do end
P = w 0 K 0 + w 1 K 1 + ... + w ` K ` . return top-k predictions according to P . levels are close to each other. We also show principal angles between U ( P ) eig and the parent cluster X  X  subspace U ( P ) puted using Algorithm 1. We see that these subspaces are even closer to each other, showing that our algorithm can accurately approximate the parent cluster X  X  subspace.
As mentioned in Section 2, many proximity measures for link prediction f ( A ) are expensive to compute on large-scale networks because of their high complexity. One solution is to approximate A by a low-rank approximation  X  A and then compute approximated proximity measures with f (  X  A ) to make predictions. This stems from the idea that most of the action in A can be captured by a few latent factors, which can be extracted with low-rank approximations of A .
It has been shown that CLRA provides an accurate and scalable low-rank approximation, and can be used for effi-Table 2: Computational time (in minutes) for sub-space approximation by MSLP (Algorithm 1) and EIG on three large-scale social networks.
 cient proximity estimation [23]. However, CLRA just uses one clustering structure making it sensitive to a particular clustering and biased against links that appear between clus-ters. Our proposed method alleviates such problem with a multi-scale approach.

The main idea is that, under a hierarchical clustering, all links will eventually belong to at least one cluster. That is, even if we miss a between-cluster link at a certain level, it still has a good chance of getting corrected by upper levels as it will eventually become a within-cluster link. Moreover, links that lie within clusters at multiple levels, such as from the deepest level, get emphasized multiple times. Those links will have the propensity of being included in the final prediction, which aligns with the intuition that links are more likely to form within tight clusters.

Once the multi-scale low-rank approximation of A is ob-tained, we now perform multi-scale link prediction. From each low-rank approximation of the hierarchy,  X  A ( i ) 0 , 1 ,...,` , the approximated proximity measure can be com-measures for each link, which are combined to make final predictions. Formally, our multi-scale predictions are given by where w i  X  X  are the weights for different levels and g (  X  ) is the predictor, e.g. top-k scoring links. For simplicity, we use the same weight for all levels in this work, i.e. w i = 1 / ( ` + 1). The entire flow of our proposed method, Multi-Scale Link Prediction (MSLP), is listed in Algorithm 2. Next, we ana-lyze the computation time and memory usage of MSLP. Computation time: As mentioned earlier, the hierarchi-cal clustering is fast and linear in the number of edges in the network and can be finished in a few hundred seconds on networks with 2 million nodes. Computing the approxi-mated proximity scores as a final step for a given user is sim-ply a matrix multiplication of low-rank matrices and time complexity is O ( `nr 2 ). In general, we set the number of clusters c and the rank in each cluster r to be fairly small. Among the three phases of MSLP, the subspace approxima-tion phase is the dominant part of the computation time. In Table 2, we compare the CPU time for subspace approx-imation by Algorithm 1 and EIG on three large-scale social networks with about 2 million users. We can see in Table 2 that for each intermediate level from 4 to 0, the subspace approximation in MSLP is up to 10 times faster than that of EIG, demonstrating the effectiveness of Algorithm 1. Fur-thermore, since we operate on each cluster independently, MSLP can be easily parallelized to gain greater speedups. Memory usage: For a rank-r approximation, EIG needs to store r eigenvectors and eigenvalues which takes O ( nr + r ) memory. Compared with EIG, CLRA is memory efficient as it only takes O ( nr + c 2 ` r 2 ) memory for a larger rank-c approximation [20]. MSLP basically has the same memory usage as CLRA. While MSLP achieves a multi-scale approx-imation, it is not necessary to store the subspaces for all levels simultaneously. We can reuse the memory allocated for the child cluster X  X  subspace to store the parent cluster X  X  subspace using Algorithm 1.
In this section we present experimental results that eval-uate both accuracy and scalability of our method, Multi-Scale Link Prediction (MSLP), for link prediction. First we present a detailed analysis of our method using the Karate club network as a case study. This will give a better under-standing of our algorithm and illustrate where it succeeds. Next we provide results under different parameter settings on a large social network. Lastly, we compare MSLP to other popular methods on massive real-world social networks with millions of users and demonstrate its superior performance.
We first start our performance analysis on a well-known small social network, Zachary X  X  Karate club network [25]. The Karate club network represents a friendship network among 34 members of the club with 78 links. The clustering structure of the Karate club network is a standard example for testing clustering algorithms. We adopt the clustering results from [2], where the clustering is found via modular-ity optimization. Figure 4 shows the hierarchy of the Karate club network. The first level has 2 clusters (circle and trian-gle) with 68 within-cluster links and the second level has 4 clusters (red, yellow, green and blue) with 50 within-cluster links.

As the Karate club network is a small network, we apply the leave-one-out method to compare different methods. We first remove a single link from the network, treat the held out edge as 0 in A , and perform link prediction on the resulting network. For each leave-one-out experiment, we compute the rank of the removed link based on its proximity measure. If the rank of the removed link appears in the top-k list, we count it as a hit . The number of top-k hits is the number of hits out of all leave-one-out experiments.

We compare MSLP to four other methods: RandCluster, common neighbors (CN), Katz and CLRA. In RandCluster, we randomly partition the graph into 4 clusters and compute the Katz measure using CLRA with these clusters. Figure 5 shows the number of top-k hits for each method. Clearly, our method significantly outperforms other methods by achiev-ing a much higher number of hits. This implies that MSLP makes more accurate predictions by considering the hierar-chical structure of the network. RandCluster performs the worst, while CLRA has comparable performance with Katz indicating that the network X  X  property can be captured by a few latent factors.

For a better illustration of the advantage of our method, we annotate Figure 4 with the results of top-3 hits. The solid blue links correspond to hits made by MSLP and the dashed red links are hits made by both CLRA and MSLP, i.e. the set of links successfully predicted by CLRA is a subset of that of MSLP. We can see that all hits made by Figure 4: Hierarchy of the Karate club network with 2 levels, two clusters on the first level and four clus-ters on the second level.
 CLRA are within-cluster links (green cluster), showing that CLRA favors within-cluster links. In contrast, MSLP can predict not only more within-cluster links, but also links between clusters (red and yellow). The ability to correctly predict both within and between-cluster links is one of the main advantages of our multi-scale approach.
In this section we present the results of link prediction on large real-world datasets. We start by examining how the parameters of MSLP affect performance. Particularly, we investigate how different hierarchical clustering structures impact the performance of MSLP. For this, we use a large real-world network: Epinions , which is an online social net-work from Epinions.com with 32,223 users and 684,026 links [19].

Next we use three real-world massive online social net-works with millions of nodes: Flickr [17], LiveJournal and MySpace [22], and compare MSLP to other methods. These datasets have timestamps associated with them and we sum-marize each snapshot in Table 3. The adjacency matrix at the first timestamp, A t 1 , is used to compute proximity mea-sures, and the adjacency matrix at the next timestamp, A t is used for testing and evaluation. Since these networks are very large, we randomly select 5,000 users and evaluate on these users. Performance measures are averaged over 30 it-erations of such sampling.

As pointed out in [13], most of all newly formed links in social networks close a path of length two and form a triangle, i.e., appear in a user X  X  2-hop neighborhood. All three datasets show that this is the case for at least 90% of test links in the second timestamp. For similar reasons as in [3], we focus on predicting links to users that are within its 2-hop neighborhood.
 Table 3: Summary of networks with timestamps.
 Network Date # of nodes # of links Flickr 5/6/2007 1,994,422 42,890,114 5/17/2007 1,994,422 43,681,874 LiveJournal 3/4/2009 1,757,326 84,366,676 4/3/2009 1,757,326 85,666,494
MySpace 1/11/2009 2,086,141 90,918,158 2/14/2009 2,086,141 91,587,516 Figure 5: Number of top-k hits for different methods on the Karate club network.
We evaluate the accuracy of different methods by comput-ing the true positive rat e (TPR) and the false positive rate (FPR), defined by for all links in a sampled test set. Our evaluation is based on receiver operating characteristic (ROC) curve and its area under the ROC curve (AUC) that present achievable TPR with respect to FPR. Predicting links with proximity mea-sures involves some thresholding on the measures to pro-duce top-k predictions. The ROC curves captures the full spectrum of prediction performance by varying the decision threshold.

However, in a practical sense, a user is recommended only a small number of top-k predictions and the hope is that most of them are correct. Thus, we focus on the region of low FPR by plotting FPR along the x -axis in log-scale, since it reflects the quality of these top-k links. In the same spirit, we also use the Precision at Top -k , i.e., the number of correct predictions out of top-k recommendations, as our evaluation metric.
 Other methods for comparison: We have carefully cho-sen a variety of proximity measures to compare with: Pref-erential Attachment (PA), Adamic-Adar score (AA), Ran-dom Walk with Restarts (RWR), common neighbors (CN) and Katz [15]. The actual values of Katz quickly becomes difficult to compute as scale increases due to its high compu-tational cost. Therefore, we employ the Lanczos method [4] for its speed and good approximation of the real Katz val-ues. We also consider a supervised machine learning method (LR) [3, 10]. For the latter, we extracted five network-based features: paths of lengths 3, 4 and 5, CN, and AA. Using these features, a logistic regression model is trained over a sampled set of positive and negative links from 10,000 users as in [3].
Next we evaluate on different parameter settings by vary-ing the three main parameters of MSLP: number of levels in the hierarchy ` , number of clusters at each node in the Table 4: Varying rank of approximation on Epin-ions dataset. MSLP consistently outperforms EIG and CLRA for different ranks in terms of AUC and precision at top-20. 10 0.8247 4.52 0.8075 5.07 0.8533 5.42 20 0.8303 4.91 0.7928 4.93 0.8550 5.62 50 0.8168 5.09 0.7527 4.31 0.8287 5.54 100 0.7903 4.98 0.7037 3.72 0.7985 5.21 200 0.7605 4.62 0.6539 3.11 0.7663 4.77 Figure 6: Robustness results for MSLP and CLRA on Epinions dataset. Numbers in brackets are the % of vertices shuffled. hierarchy c , and rank r . We fix ` = 3, c = 2, and r = 20 while changing one parameter at a time and measure AUC and precision at top-20. The Epinions network does not have time information, thus we randomly sample a number of links and treat them as test links in A t 2 . The sampling is performed such that about 90% of test links appear in a user X  X  2-hop neighborhood.

We compare the performance of our method to two other low-rank approximation methods: eigen-decomposition (EIG), clustered low rank approximation at the deepest level in the hierarchy tree (CLRA).
 Rank: Table 4 shows performance of the three low-rank approximation methods with different ranks. MSLP consis-tently performs better in terms of both AUC and Precision at Top-20 than the other two methods. The accuracy of CLRA deteriorates as the rank increases. This implies that the low-rank approximation of each cluster starts to accumu-late noise at larger ranks. It is shown that low-rank approx-imation methods tend to perform best at an intermediate rank [15], which is also the case here with r = 20. Hierarchical clustering structure: Next we experiment with various hierarchical clustering structures. Table 5 shows how the performance changes as the hierarchical clustering structure changes. For a complete comparison, results of other methods are also given in Table 5(c). The second col-umn in Tables 5(a) and 5(b) represents the percentage of within-cluster edges. It is clear that as the number of clus-ters at the bottom level increases the percentage decreases. While the accuracy of CLRA degrades as the percentage decreases, MSLP is still able to perform better than other Table 5: Varying hierarchical clustering structure by changing (a) the number of clusters per node at each level and (b) the number of levels on Epinions dataset. Results show that MSLP is not only more robust than CLRA to different clustering struc-tures, but also outperforms other methods in most cases. Percentage is the percentage of within-cluster edges (Numbers in brackets represent percentage of within-cluster edges of random clustering). (a) Changing the number of clusters per node at each level. methods in all cases with the only exception of Table 5(a) at c = 5. The results clearly show that MSLP is robust to different hierarchical structures.

Furthermore, to see that MSLP is robust to cluster struc-tures, we randomly perturb clusters at the deepest level of the hierarchy by moving vertices from their original cluster to another random cluster. Figure 6 shows the result of mov-ing 0%, 10% and 20% of vertices. Even with 10% of vertices shuffled, MSLP still outperforms CLRA with no shuffling. It is clear that, while CLRA X  X  performance decreases rapidly, MSLP still performs well in low FPR regions.
In this section, we present results on real-world networks with millions of nodes presented in Table 3. We construct a hierarchical structure with ` = 5 and c = 2 for all three networks, and use r = 100 for EIG, CLRA and MSLP. We set  X  = 0 . 0005 for the Katz measure, which yields the best results.

Tables 6 and 7 give AUC and precision at top-100 results for the various methods, respectively. MSLP-Katz gives a significant improvement over the Katz measure and outper-forms all other methods. Specifically, it gains a relative im-provement of up to 4% in AUC and 15% in precision over the next best performing method. We emphasize the superior Table 6: AUC results for Flickr , LiveJournal and MySpace datasets.
 Table 7: Precision at top-100 results for Flickr , Live-Journal and MySpace datasets.
 performance in terms of precision at top-100 of MSLP-Katz as shown in Table 7. This is a very appealing aspect of MSLP as it reflects the quality of top recommendations. In contrast, MSLP-CN remains comparable to CN, but per-forms better than EIG and CLRA. Surprisingly, the super-vised method LR does not perform well, which is consistent with results found in [3]. Note that we only use network-based features and no additional features for training. How-ever, engineering for more features is a difficult task and constructing good features itself can be computationally ex-pensive.

Figure 7 gives ROC curves focused on the low FPR region for the three large-scale networks. We note that only one representative method from methods that have similar per-formance is plotted for the sake of clarity. We observe that MSLP-Katz performs the best in all three datasets with sig-nificant improvements over Katz. For a given TPR, MSLP reduces FPR by 10% on average and at most 20% compared to others in all datasets. For completeness, we present the full range of the ROC curve for LiveJournal in Figure 8. Note that much of the performance boost comes from the left side of the curve, which corresponds to the area of in-terest. That is, MSLP achieves good prediction quality for the highest predicted scores.
 While dimensionality reduction methods, such as EIG and CLRA, tend to perform well in all three datasets, they are limited to a single low-rank representation of the network. Furthermore, CLRA has the largest drop in relative per-formance in terms of precision compared to MSLP in the MySpace dataset, where only 56% of the edges are within clusters, whereas MSLP achieves the best result. Overall, the superior performance of MSLP illustrates the effective-ness of our multi-scale approach.
 We note that the majority of time is taken by computing CLRA at the deepest level and thereafter low-rank approx-imations of upper levels can be obtained efficiently due to Algorithm 1. However, CLRA can be easily parallelized as computing the subspace of each cluster is independent to other clusters. Thus, MSLP can achieve much more speedup by using a parallel implementation and serve as a highly scalable method for link prediction.
In this paper, we have presented a general framework for multi-scale link prediction by combining predictions from multiple scales using hierarchical clustering. A novel tree-structured approximation method is proposed to achieve fast and scalable multi-scale approximations. Extensive exper-imental results on large real-world datasets have been pre-sented to demonstrate the effectiveness of our method. This significantly widens the accessibility of state-of-the-art prox-imity measures for large-scale applications.

For future work, we plan to investigate methods to learn the weights for various levels of the hierarchy, since some levels may have better predictions and deserve larger weights in the final prediction. In this work, we use a balanced hierarchical structure mainly for its simplicity in combining predictions. However, a more realistic setting would be to use an unbalanced hierarchical clustering structure. The issue here is how to combine predictions from different levels as some links may not receive predictions at certain levels. We also plan to develop a parallelized version of MSLP as each level of the hierarchy can be easily parallelized.
This research was supported by NSF grants CCF-1117055 and CCF-0916309. [1] L. A. Adamic and E. Adar. Friends and neighbors on MSLP performs the best on all three datasets. [2] G. Agarwal and D. Kempe. Modularity-maximizing [3] L. Backstrom and J. Leskovec. Supervised random [4] F. Bonchi, P. Esfandiar, D. F. Gleich, C. Greif, and [5] A. Clauset, C. Moore, and M. E. J. Newman.
 [6] P. M. Comar, P.-N. Tan, and A. K. Jain. LinkBoost: [7] I. S. Dhillon, Y. Guan, and B. Kulis. Weighted graph [8] D. M. Dunlavy, T. G. Kolda, and E. Acar. Temporal [9] E. Estrada and D. J. Higham. Network properties [10] M. A. Hasan, V. Chaoji, S. Salem, and M. Zaki. Link [11] W. H. Hsu, J. Lancaster, M. S. R. Paradesi, and [12] L. Katz. A new status index derived from sociometric [13] J. Leskovec, L. Backstrom, R. Kumar, and [14] J. Leskovec, K. J. Lang, A. Dasgupta, and M. W. [15] D. Liben-Nowell and J. Kleinberg. The link prediction [16] R. N. Lichtenwalter, J. T. Lussier, and N. V. Chawla. [17] A. Mislove, H. S. Koppula, K. P. Gummadi, [18] M. E. J. Newman. Clustering and preferential [19] M. Richardson, R. Agrawal, and P. Domingos. Trust [20] B. Savas and I. S. Dhillon. Clustered low rank [21] S. Scellato, A. Noulas, and C. Mascolo. Exploiting [22] H. H. Song, T. W. Cho, V. Dave, Y. Zhang, and [23] H. H. Song, B. Savas, T. W. Cho, V. Dave, Z. Lu, I. S. [24] C. Wang, V. Satuluri, and S. Parthasarathy. Local [25] W. W. Zachary. An information flow model for
