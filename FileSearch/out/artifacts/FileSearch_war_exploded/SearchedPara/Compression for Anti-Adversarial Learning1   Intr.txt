 There is an increasing use of machine l earning algorithm s for more efficient and reliable performance in areas of com puter security. Like any other defense mechanisms, machine learning based security systems have to face increasingly aggressive attacks from the adversary. The attacks are often carefully crafted to target specific vulnerabilities in a learning system, for example, the data set used to train the system, or the internal logic of the algorithm, or often both. In this paper we focus on both causative attacks and exploratory attacks. Causative attacks influence a learning system by altering its training set while their exploratory counterparts do not alter training data but rather exploit mis-classifications for new data. When it com es to consider security violations, we focus on integrity violations where the ad versary X  X  goal is injecting hostile input into the system to cause more false negatives. In contrast, availability violations refer to cases where the adversary aims at increasing false positives by preventing  X  X ood X  input into the system 1 .
Many security problems, such as intrusi on detection, malw are detection, and spam filtering, involve lear ning on strings. Recent st udies demonstrate that su-perior classification perfo rmance can be achieved with modern statistical data compression models instrumented for such learning tasks [2,3,4]. Unlike tra-ditional machine learning methods, compression-based learning models can be used directly on raw strings without error-prone preprocessing, such as tokeniza-tion, stemming, and feature selection. The methods treat every string input as a sequence of characters instead of a vect or of terms (words). This effectively eliminates the need for defining word boundaries. In addition, compression based methods naturally take into consideration both alphabetic and non-alphabetic characters, which prevents information loss as a result of preprocessing.
The robustness of compression-based learning algorithms was briefly discussed recently [5]. To the best of our knowledge, there is not a full scale investiga-tion on the susceptibility of this type of learning algorithm to adversarial at-tacks, and thus no counter-attack techniques have been developed to address potential attacks against compressors trained in learning systems. In this paper, we demonstrate that compression-based learning algorithms are surprisingly re-silient to carefully plotted attacks that can easily devastate standard learning algorithms. We further demonstrate that as we launch more aggressive attacks as in the worst case where the adversary has a full knowledge of training data, compression-based algorithms failed as expected. We tackle the worst case with a proposal of a new technique that analyzes subsequences strategically extracted from given data. We achieved near-zero performance loss in the worst case in the domain of spam filtering.

The remainder of this paper is organi zed as follows. In Section 2, we briefly review the current state-of-the-art da ta compression model that has been fre-quently used in machine learning and data mining techniques. Section 3 demon-strates that learning systems with modern compressors are resilient to attacks that have a significant impact on standard learning algorithms. We show that modern compressors are susceptible to attacks when the adversary alters data with negative instances in training data. We propose a counter-attack learning method that enhances the compression-based algorithm in Section 4. Section 5 presents the experimenta l results. Section 6 concludes the work and discusses future directions. Statistical data modeling plays an important role in arithmetic encoding [6] which turns a string of symbols into a rational number between [0 , 1). The number of bits needed to encode a symbol depends on the probability of its appearance in the current context. Finite-context modeling estimates the prob-ability of an incoming symbol based on a finite number of symbols previously encountered. The current state-of-the-art adaptive model is prediction by partial matching (PPM) [7,8,9]. PPM is one of the best dynamic finite-context models that provide good estimate of the true entropy of data by using symbol-level dy-namic Markov modeling. PPM predicts the symbol probability conditioned on its k immediately prior symbols, forming a k th order Markov model. For exam-The total number of contexts of an order-k model is O ( |  X  | k +1 ), where  X  is the alphabet of input symbols. As the order of the model increases the number of contexts increases exponentially. H igh-order models are more likely to cap-ture longer-range correlations among ad jacent symbols, if they exist; however, an unnecessarily high order can result in context dilution leading to inaccurate probability estimate. PPM solves the dilemma by using dynamic context match between the current sequence and the on es that occurred previously. It uses high-order predictions if they exist, otherwise  X  X rops gracefully back to lower order predictions X  [10]. More specifically, the algorithm first looks for a match of an order-k context. If such a match does not exist, it looks for a match of an order k  X  1 context, until it reaches order-0. Whenever a match is not found in the current context, the model falls back to a lower-order context and the total probability is adjusted by what is called an escape probability . The escape probability models the probability that a symbol will be found in a lower-order context. When an input symbol x i is found in context c k i where k  X  k ,the conditional probability of x i given its k th order context c k i is: where p ( Esc | c j i ) is the escape probability conditioned on context c j i .Ifthesym-bol is not predicted by the order-0 model, a probability defined by a uniform distribution is predicted.
 PPMC [11] and PPMD [12] are two well known variants of the PPM algorithm. Their difference lies in the estimate of the escape probabilities. In both PPMC and PPMD, an escape event is counted every time a symbol occurs for the first time in the current context. In PPMC, the escape count and the new symbol count are each incremented by 1 while in PPMD both counts are incremented by 1 / 2. Therefore, in PPMC, the total symbol count increases by 2 every time a new symbol is encountered, while in PPMD the total count only increases by 1. When implemented on a binary computer, PPMD sets the escape probability to | d | 2 | t | ,where | d | is the number of distinct symbols in the current context and | t | is the total number of symbols in the current context.

Now, given an input X = x 1 x 2 ...x d of length d ,where x 1 ,x 2 ,...,x i is a sequence of symbols, its probability given a compression model M can be esti-mated as where x j i = x i x i +1 x i +2 ...x j for i&lt;j .
 Consider binary classification problems: X X  X  where Y X  X  + ,  X  X  .Givena set of training data, compression-based classification works as follows. First, the algorithm builds two compression models, one from each class. Each compression model maintains a context tree, together with context statistics, for training data in one of the two classes. Then, to classify an unlabeled instance, it requires the instance to run through both compression models. The model that provides a better compression of the instance makes the prediction. A common measure for classification based on compression is minimum cross-entropy [13,3]: where | X | is the length of the instance, x i  X  k ,...,x i is a subsequence in the in-stance, k is the length of the context, and M c is the compression model associated with class c .

When classifying an unlabeled instan ce, a common practice is to compress it with both compression models and check to see which one compresses it more efficiently. However, PPM is an incremen tal algorithm, which means once an unlabeled instance is compressed, the m odel that compresses it will be updated as well. This requires the changes made to both models be reverted every time after an instance is classified. Although, the PPM algorithm has a linear time complexity, the constants in its complexity are by no means trivial. It is desirable to eliminate the redundancy of updating and then reverting changes made to the models. We propose an approximation algorithm (See Algorithm 1) that we found works quite well in practice. Given context C = x i  X  k ...x i  X  1 in an unlabeled instance, if any suffix c of C has not occurred in the context trees of the compression models, we set p ( Esc | c )= 1 | A | , thus the probability of x i is discounted by 1 | A | ,where | A ||  X  | , the size of the alphabet. More aggressive discount factors set the prediction further away from the decision boundary. Empirical results will be discussed in Section 5.

Compression-based algorithms have demonstrated superior classification per-formance in learning tasks where the input consists of strings [3,4]. However, it is not clear whether this type of learning algorithm is susceptible to adversarial attacks. We investigate several ways to attack the compression-based classifier on real data in the domain of e-mail spam filtering. We choose this domain in our study for the following reasons: 1.) previous work has demonstrated great success of compression-bas ed algorithms in this domai n [3]; 2.) it is conceptu-ally simple to design various adversarial attacks and establish a ground truth; 3.) there have been studies of several variations of adversarial attacks against standard learning algorithms in this domain [14,5].

Good word attacks are designed specifically to target the integrity of statisti-cal spam filters. Good words are those tha t appear frequently in normal e-mail but rarely in spam e-mail. Existing studies show that good word attacks are very effective against the standard learning algorithms that are considered the state of the art in text classification [14]. The attacks against multinomial na  X   X ve Bayes and support vector machines with 500 good words caused more than 45% decrease in recall while th e precision was fixed at 90% in a previous study [5]. We repeated the test on the 2006 TREC Public Spam Corpus [15] using our implementation of PPMD compressors. It turns out that the PPMD classifier is surprisingly robust against good word attacks. With 500 good words added to 50% of the spam in e-mail, we observed no significant change in both precision and recall (See Figure 1) 2 . This remains true even when 100% of spam is ap-pended with 500 highly ranked good words. Its surprising resilience to good word attacks led us to more aggressive attacks against the PPMD classifier. We ran-domly chose a legitimate e-mail message from the training corpus and appended it to a spam e-mail during testing. 50% of the spam was altered this way. This time we were able to bring the average recall value down to 57.9%. However, the precision value remains above 96%. Figure 1 shows the accuracy, precision and recall values when there are no attacks, 500-goodword attacks, and in the worst case, attacks with legitimate training e-mail. Details on experimental setup will be given in Section 5. In this section, we present a robust anti-adversarial learning algorithm that is resilient to attacks with no assumptions on the adversary X  X  knowledge of the system. We do, however, assume that the adversary cannot alter the negative instances, such as legitimate e-mail messages, normal user sessions, and benign software, in training data. This is a reasonable assumption in practice since in many applications such as intrusion detection, malware detection, and spam filtering, the adversary X  X  attempts are mostly focused on altering positive in-stances to make them less distinguishable among ordinary data in that domain.
Now that we know the adversary can alter the  X  X ad X  data to make it appear to be good, our goal is to find a way to separate the target from its innocent looking. Suppose we have two compression models M + and M  X  .Intuitively, a positive instance would compress better with the positive model M + than it would with M  X  , the negative model. When a positive instance is altered with fea-tures that would ordinarily appear in negative data, we expect the subsequences in the altered data that are truly positive to retain relatively higher compression rates when compressed against the positive model. We apply a sliding window approach to scan through each instance an d extract subsequences in the sliding window that require a smaller number of bits when compressed against M + than M  X  . Ideally, more subsequences would be identified in a positive instance than in a negative instance. In practice, ther e are surely exceptional cases where the number of subsequences in a negative ins tance would exceed its normal average. For this reason, we decide to compute the difference between the total number of bits required to compress th e extracted subsequences S using M  X  and M + ,re-spectively. If an instance is truly positive, we expect Bits M  X  ( S ) Bits M + ( S ), where Bits M  X  ( S ) is the number of bits needed to compress S using the neg-ative compression model, and Bits M + ( S ) is the bits needed using the positive model. Now for a positive instance, not only we expect a longer list of subse-quences extracted, but also a greater disc repancy between the bits after they are compressed using the two different models.

For the adversary, any attempt to attack this counter-attack strategy will always boil down to finding a set of  X  X isleading X  features and seamlessly blend them into the target (positive instance). To break down the first step of our counter-attack strategy, that is, extracting subsequences that compress better against the positive compression model, the adversary would need to select a pass, together with the  X  X ad X  ones, our first round screening. To attack the second step, the adversary must find good words that compress better against offset the impact of the  X  X ad X  words in the extracted subsequences. These two goals inevitably contradict each other, thus making strategically attacking the system much more difficult.
 We now formally present our algorithm. Given a set of training data T ,where T = T +  X  X   X  , we first build two compression models M + and M  X  from T + and T  X  , respectively. For each training instance t in T ,wescan t using a sliding window W of size n , and extract the subsequence s i in the current sliding window W if Bits subsequence extraction. Next, for each instance t in the training set, we compute d been extracted in the first step. We then compute the classification threshold by maximizing the information gain: For a more accurate estimate, r should be computed using k -fold cross validation. To classify an unlabeled instance u , we first extract the set of subsequences S from u in the same manner, then compute d u = Bits M  X  ( S )  X  Bits M + ( S ). If d description of the algorithm is given in Algorithm 2. We evaluate our counter-attack algorithm on e-mail data from the 2006 TREC Public Spam Corpus [15]. The data consists of 36,674 spam and legitimate email messages, sorted chronologically by r eceiving date and evenly divided into 11 subsets D 1 ,  X  X  X  ,D 11 . Experiments were run in an on-line fashion by training on the i th subset and testing on the subset i + 1. The percentage of spam messages in each subset varies from approximately 60% to a little bit over 70%. The good word list consists of the top 1,000 unique words from the entire corpus ranked according to the frequency ratio. In order to allow a true apples-to-apples comparison among compression-based algorithms and standard learning algorithms, we preprocessed the entire corpus by removing HTML and non-textual parts. We also applied stemmi ng and stop-list to all terms. The to , from , cc , subject ,and received headers were retained, while the rest of the headers were removed. Messages that h ad an empty body after preprocessing were discarded. In all of our experiments, we used 6 th -order context and a sliding window of 5 where applicable.

We first test our counter-attack algorithm under the circumstances where there are no attacks and there are only good word attacks. In the case of good words attacks, the adversary has a general knowledge about word distributions in the entire corpus, but lacks a full knowledge about the training data. As discussed in Section 3, the PPMD-based classifier d emonstrated superior performance in these two cases. We need to ensure that our anti-adversarial classifier would perform the same. Figure 2 shows the comparison between the PPMD-based classifier and our anti-adversarial classifier when there are no attacks and 500-goodword attacks on 50% of spam in each test set.

As can be observed, the performance are comparable between both algo-rithms, with and without good word attacks. For each algorithm, both precision and recall values remain nearly unchanged before and after attacks. Note that for PPMD-based classifiers, we tried both incremental compression and bit ap-proximation. The results shown were obtained using incremental compression. With bit approximation, we achieved significant run-time speedup, however, with 5% performance drop, from 94.2% to 89.2%, in terms of recall values. Due to its run-time efficiency, we used bit approximation in all experiments involving our anti-adversarial classifiers. We used 10-fold cross validation to determine the threshold value r by maximizing the information gain. In practice, overfit-ting may occur and several k (number of folds) values should be tried in cross validation. We also experimented when the number of good words varied from 50 to 500, and the percentage of spam altered from 50% to 100% in each test set, the results remain similar.

We conducted more aggressive attacks w ith exact copies of legitimate mes-sages randomly chosen from the training set. We tried two attacks of increasing strength by attaching one legitimate message and five legitimate messages, re-spectively, to spam in the test set. In total 50% of the spam in the test set was altered in each attack. Figure 3 illustrates the results. As can be observed, our anti-adversarial classifier is very robust to any of the attacks, while the PPMD-based classifier, for which the results are shown in Figure 4, is obviously vulnerable to the more aggressive attacks. Furthermore, similar results were ob-tained when: 1.) the percentage of altered spam increased to 100%; 2.) legitimate messages used to alter spam were randoml y selected from the test set, and 3.) in-jected legtimate messages were randoml y selected from data sets that are neither training nor test sets.

To make the matter more complicated, we also tested our algorithm when legitimate messages were randomly scattered into spam. This was done in two different fashions. In the first case, we first randomly picked a position in spam; then we took a random length (no greater than 10% of the total length) of a legitimate message and inserted it to the selected position in spam. The two steps were repeated until the entire legitima te message has been inserted. Empirical results show that random insertion does not appear to affect the classification performance. In the second case, we ins erted terms from a legitimate message in a random order after every termsinspam.Theprocessisasfollows:1.) tokenize the legitimate message and randomly shuffle the tokens; 2.) insert a random number of tokens (less than 10% of the total number of tokens) after terms in spam. Repeat 1) and 2) until all tokens are inserted to spam. We observed little performance change when  X  3. When  X  2, nearly all local context is completely lost in the altered s pam, the average reca ll values dropped to below 70%. Note that in the latter case (  X  2), the attack is most likely useless to the adversary in practice since the scrambled instance would also fail to deliver the malicious attacks the adversary has set out to accomplish.
Previous studies [14,17,5] show that retraining on altered data as a result of adversarial attacks may improve the performance of classifiers against the attacks. This observation is further verified in our experiments, in which we ran-domly select 50% of spam in the training set and appended good words and legit-imate e-mail to it, separately. Figure 5 shows the results of the PPMD classifier and the anti-adversarial classifier with 500-goodword attacks and 1-legitimate-mail attacks in both training and test data. As can be observed, retraining improved, to an extent, the classification results in all cases. We demonstrate that compression-based classifiers are much more resilient to good word attacks compared to standard learning algorithms. On the other hand, this type of classifier is vulnerable to attacks when the adversary has a full knowledge of the training set. We propose a counter-attack technique that extracts and analyzes subsequences that are more informative in a given instance. We demonstrate that the proposed technique is robust against any attacks, even in the worst case where the adversary can alter positive instances with exact copies of negative instances take n directly from the training set.

A fundamental theory needs to be developed to explain the strength of the compression-based algorithm and the anti-adversarial learning algorithm. It re-mains less clear, in theory, why the compression-based algorithms are remarkably resilient to strategically designed attacks that would normally defeat classifiers trained using standard learning algorithms. It is certainly to our great interest to find out how well the proposed counter-attack strategy performs in other domains, and under what circumstances this seemingly bullet-proof algorithm would break down.
 The authors would like to thank Zach Jorgensen for his valuable input. This work was partially supported by Air Force Office of Scientific Research MURI Grant FA9550-08-1-0265, National Institutes of Health Grant 1R01LM009989, National Science Foundation (NSF) Grant Career-0845803, and NSF Grant CNS-0964350, CNS-1016343.

