 Aggregate outputs learning differs from the classical super-vised learning setting in that, training samples are packed into bags with only the aggregate outputs (labels for classi-fication or real values for regression) known. This setting of the problem is associated with several kinds of application background. We focus on the aggregate outputs classifi-cation problem in this paper, and set up a manifold regu-larization framework to deal with it. The framework can be of both instance level and bag level for different testing goals. We propose four concrete algorithms based on our framework, each of which can cope with both binary and multi-class scenarios. The experimental results on several datasets suggest that our algorithms outperform the state-of-art technique.
 H.3.m [ Information Storage and Retrieval ]: Miscella-neous X  Learning ; I.2.6 [ Artificial Intelligence ]: Learn-ing X  ConceptLearning ; I.5.2 [ Pattern Recognition ]: De-sign Methodology  X  Classifierdesignandevaluation Algorithms, Experimentation, Theory Aggregate Outputs Classification, Manifold Regularization
The concept of aggregate outputs learning (AOL) was first proposed by [4]. It is a problem elicited from the applica-tion of analysis of single particle mass spectrometry (SPMS)  X  This work is supported by NSFC (Grant No. 60835002 and No. 60675009).
 data. The goal is training to predict quantity of blackcarbon (BC) for a certain given single particle. However, the avail-able instrument is not precise enough and can only measure the aggregated BC (sum of BC) for bags of training parti-cles. The problem becomes how to make use of the training samples and the aggregated BC to train an accurate predic-tor for single particle. The SPMS problem is of regression nature, and the authors of [4] also extended it to the classi-fication setting which is what we concern in this paper. It is different from classical supervised classification in that, the training samples are packed into several bags, and only aggregated labels, which indicate the numbers of samples from different classes in each bag, are provided. We show an example of training dataset for this setting in Table 1. Table 1: An example of training dataset for aggre-gate outputs classification. There are 8 instances, each with two features of height and weight of a person. The instances are packed into 2 bags. The aggregated labels show how many males and females in each bag.

There is scarce literature of AOL. The original work of [4] is the only one that strictly follows the setting as far as we know. In [4], the authors adapt three classical su-pervised learning algorithms to solve the aggregate outputs learning problem, namely k-nearest-neighbor (kNN), arti-ficial neural network (ANN) and support vector machine (SVM). Both classification and regression versions of the algorithm are provided. We are only interested in solving the aggregate outputs classification (AOC) problem in this paper, and we denote the three classification algorithms as AOC-kNN, AOC-ANN and AOC-SVM.

In this paper, we set up a manifold regularization frame-work [2] to cope with the AOC problem, both instance-and bag-level versions of algorithm are proposed. We aim to use a function in a Reproducing Kernel Hilbert Space (RKHS) to predict the label of any sample or the aggregated label of any bag. Our algorithms can serve the binary classification as well as the multi-class scenario, which is not covered in [4] in detail. The experimental results are promising.
For simplicity, we first introduce in the binary classifica-tion scenario, where each sample belongs to one of the two classes. Suppose we are given a training dataset with  X  sam- X  th sample. We assume that each of the samples x  X  is from the same input space  X  and  X   X   X   X  . We also assume that each x  X  is associated with a hidden  X   X  that takes the value of  X  1 for two different classes. The label space is denoted by  X  . However, these labels cannot be observed directly. The  X  samples are aggregated into  X  disjunct bags, and an  X   X   X  aggregating matrix  X  is provided. The element of  X  is given by We denote the  X  bags as B 1 , B 2 ,..., B  X  ,witheach B  X   X  .  X  is the space of the bags. We are also provided with an  X  -dimensional column aggregated labels vector  X  ,with its each element  X   X  equaling the sum of the hidden labels associated with the samples in the  X  th bag. For example, given the  X  th bag containing 5 positive samples and 3 neg-ative samples, the value of  X   X  is 2. Our goals for the AOC problem can be both transductive and inductive. For the transductive goal, we hope to give the label of each training x that is as close to  X   X  as possible. Very often, our inductive goal is simplified to get a good predictive accuracy on a test dataset with known labels that is from the same distribution as that of the training set.

The above goals are of instance level, which means that our trained classifier is supposed to act on single instance. We also propose to train bag-level classifiers in this paper. Our bag-level goal can only be i nductive, since all the aggre-gated labels of the training bags are given. We thus expect our bag-level classifier to give as accurate aggregated labels on test bags as possible.

Moreover, the bag-level classifier can also serve the goal of instance classification. All we need is to deem the instances for testing as bags with only one instance. As shown in our experiments, they also yield competitive results of error rate.

When considering the case of multi-class classification, we only need to add a modification to the formulation men-tioned above. We suppose  X  classes are taken into account in the problem, where  X  is an integer and  X   X  3. Rather than ascalarofvalue  X  1, any of the hidden labels is now repre-sented by a  X  -dimensional 0-1 vector y with its  X  th element indicating whether the associated sample belongs to the  X  th class or not. The vector  X  is replaced by an  X   X   X  matrix  X  with  X   X  X  X  equaling the number of the samples in the  X  th bag that belongs to the  X  th class.
We start with binary classification case first. After choos-ing a Mercer kernel  X  :  X   X   X   X   X  , an RKHS ensues. We denote it as  X   X  and it is a space of functions  X   X   X  .There is also an associated norm  X  X  X  X   X  for  X   X  .(Onecanreferto [5] for more details of Mercer kernel, RKHS and the asso-ciated norm). We thus want to learn a soft label function  X   X  X  X   X  so that where  X  (  X , X , X  ) is the loss function concerning the aggregate outputs.  X   X   X  2  X  isthesquareofnormin  X   X  . It penalizes the complexity of  X  in  X   X  .  X   X   X  2  X  is a term that penalizes the unsmoothness of  X  on the given samples.  X  1 and  X  2 are two positive parameters that control the tradeoff between the terms. We explain each of the three terms as follows.
For the convenience in the later optimization process, we use two kinds of loss function  X  (  X , X , X  )inthispaper where can be comprehended as a vector version of the double sided hinge loss.  X  2 (  X , X , X  ) is a quadratic loss.

By manipulating the expanded version of representor the-orem in [2], the learned function  X  shall have the form of
As a result, where  X  is the kernel matrix defined on the training dataset.  X  =[  X  1 , X  2 ,..., X   X  ]  X  is the vector of representing weights. Using the vector form one could write where  X   X  is the vector of the values by imposing  X  on the training samples.  X   X   X  2  X  term is used to penalize the unsmoothness of func-tion  X  .Weuse where  X  is the graph Laplacian regularizer ([2]).
Thus, when using  X  1 as loss function, (2) becomes the following optimization problem min when  X  2 is used, it becomes min We denote the two algorithms corresponding to (11) and (12) as AOC-manifold-il-L1 and AOC-manifold-il-L2.
For the optimization problem (11), it can be reformulated as subject to
This is a standard quadratic optimization setting and can be solved efficiently.

The solution of the optimization problem (12) can be writ-teninananalyticalform Now, let us consider the multi-class classification scenario. We need to modify our formulation (2) to
Similar with (3) and (4), we adapt the two loss functions as
By using the representor theorem, the optimization prob-lems using the two different loss functions become min
These two optimization problems can be solved similarly to the binary case, and we omit the details for briefness.
For bag-level classifier in binary classification case, we want to learn a function  X  Where  X   X  :  X   X   X   X   X  is a Mercer kernel.  X   X  associated RKHS with a norm  X  X  X  X   X  instance-level scenario, we can use two different loss func-tions
The two optimization problems guided by representor the-oremcanbewrittenas min min
Algorithms associated with (25) and (26) will be men-tioned as AOC-manifold-bl-L1and AOC-manifold-bl-L2. Here  X   X  is the kernel matrix on the training bags. We use the set kernel from [3] and write where  X  is the instance-level kernel matrix we used before.  X   X  can be calculated similarly.

The solutions of (25) and (26) can be given by an opti-mization problem and an analytical form, similarly to case of instance-level classifier.

The bag-level classifiers for the multi-class scenario can be constructed analogously. We omit the derivation here due to the lack of space.
In this section, we conduct our experiments on ionosphere from UCI datasets ([1]). It is originally collected for su-pervised learning. There are no natural bags constructed specifically for the AOC problem. Thus we need to create the aggregation ourselves. There are two parameters  X  and  X   X  we vary for the aggregation in our experiments.  X  is the  X  X andomness X  X ithin bags ([4]) and  X   X  is the number of sam-ples per bag.

Ouralgorithms include AOC-manifold-il-L1, AOC-manifold-il-L2, AOC-manifold-bl-L1 and AOC-manifold-bl-L2. We have realized AOC-kNN and AOC-ANN in [4] for compar-ison. We did not compare with AOC-SVM mainly because it is too time-consuming. For the ionosphere dataset with 351 samples for training, the problem cannot be solved in one day.

We evaluate the performances of the six algorithms on both instance level and bag level. For the instance-level evaluation, we conduct both transductive and inductive ex-periments. We list these settings and descriptions in Table 2, where  X  X orm of error X  criterion for bag-level setting is  X   X   X   X   X  2 for binary case and  X   X   X   X   X   X  for multi-class case.
Each setting of our experiments is repeated 100 times to give statistical results. The difference between each running time is due to the random swap indicated by  X  and the ran-domly divided dataset for training and testing.

Ionosphere is a dataset targeting free electrons in the iono-sphere. There are 351 samples in the dataset, and each one has 34 features. The samples are labeled with  X  X ood X  or  X  X ad X , indicating whether the data reveals the structure of the ionosphere or not. The results are reported in Fig. 1 X 3 and Table 3. AOC-manifold-bl-L1 and AOC-manifold-bl-L2 have very similar performances on this dataset, and their curves overlap for the most part.

From the results of the experiments, we reach the follow-ing conclusions: 1. Our four algorithms based on manifold regularization respectively. respectively. Table 3: Average run time for the si xalgorithms on ionosphere , with the instance-level transduction setting. 2. For the run time of the algorithms, AOC-kNN is the 3. There is no definite conclusion about which of the 4. Algorithms using  X  1 and  X  2 loss function yield similar
In this paper, we set up a manifold regularization frame-work for the AOC problem, and propose four new algorithms for both instance-and bag-level settings. Our algorithms can be used for both binary and multi-class cases, while the latter one is not specified in the former works. Also, we conduct experiments on several datasets, with the results suggesting the advantage of our methods. [1] A. Asuncion and D. Newman. UCI machine learning [2] M. Belkin, P. Niyogi, and V. Sindhwani. On manifold [3] T. Gartner, P. Flach, A. Kowalczyk, and A. Smola. [4] D. Musicant, J. Christensen, and J. Olson. Supervised [5] B. Scholkopf and A. Smola. Learningwithkernels .MIT
