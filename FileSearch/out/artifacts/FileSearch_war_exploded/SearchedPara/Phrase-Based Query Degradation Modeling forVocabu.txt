 Our goal is to find short speech utterances which contain a query word. We accomplish this goal by ranking the set of utterances by our confidence that they contain the query word, a task known as Ranked Utterance Retrieval (RUR). In particular, we are interested in the case when the user X  X  query word can not be anticipated by a Large Vocabulary Continuous Speech Recognizer X  X  (LVCSR) decod-ing dictionary, so that the word is said to be Out-Of-Vocabulary (OOV).

Rare words tend to be the most informative, but are also most likely to be OOV. When words are OOV, we must use vocabulary-independent tech-niques to locate them. One popular approach is to search for the words in output from a phoneme rec-ognizer (Ng and Zue, 2000), although this suffers from the low accuracy typical of phoneme recogni-tion. We consider two methods for handling this in-accuracy. First, we compare an RUR indexing sys-tem using phonemes with two systems using longer recognition units: words or phoneme multigrams. Second, we consider several methods for handling the recognition inaccuracy in the utterance rank-ing function itself. Our baseline generative model handles errorful recognition by estimating term fre-quencies from smoothed language models trained on phoneme lattices. Our new approach, which we call query degradation, hypothesizes many alternate  X  X ronunciations X  for the query word and incorpo-rates them into the ranking function. These degra-dations are translations of the lexical phoneme se-quence into the errorful recognition language, which we hypothesize using a factored phrase-based statis-tical machine translation system.

Our speech collection is a set of oral history interviews from the MALACH collection (Byrne et al., 2004), which has previously been used for ad hoc speech retrieval evaluations using one-best word level transcripts (Pecina et al., 2007; Olsson, 2008a) and for vocabulary-independent RUR (Ols-son, 2008b). The interviews were conducted with survivors and witnesses of the Holocaust, who dis-cuss their experiences before, during, and after the Second World War. Their speech is predominately spontaneous and conversational. It is often also emotional and heavily accented. Because the speech contains many words unlikely to occur within a gen-eral purpose speech recognition lexicon, it repre-sents an excellent collection for RUR evaluation.
We were graciously permitted to use BBN Tech-nology X  X  speech recognition system Byblos (Prasad et al., 2005; Matsoukas et al., 2005) for our speech recognition experiments. We train on approximately 200 hours of transcribed audio excerpted from about 800 unique speakers in the MALACH collection. To provide a realistic set of OOV query words, we use an LVCSR dictionary previously constructed for a different topic domain (broadcast news and conver-sational telephone speech) and discard all utterances in our acoustic training data which are not covered by this dictionary. New acoustic and language mod-els are trained for each of the phoneme, multigram and word recognition systems.

The output of LVCSR is a lattice of recogni-tion hypotheses for each test speech utterance. A lattice is a directed acyclic graph that is used to compactly represent the search space for a speech recognition system. Each node represents a point in time and arcs between nodes indicates a word oc-curs between the connected nodes X  times. Arcs are weighted by the probability of the word occurring, so that the so-called  X  X ne-best X  path through the lat-tice (what a system might return as a transcription) is the path through the lattice having highest proba-bility under the acoustic and language models. Each RUR model we consider is constructed using the ex-pected counts of a query word X  X  phoneme sequences in these recognition lattices. We consider three ap-proaches to producing these phoneme lattices, using standard word-based LVCSR, phoneme recognition, and LVCSR using phoneme multigrams. Our word system X  X  dictionary contains about 50 , 000 entries, while the phoneme system contains 39 phonemes from the ARPABET set.

Originally proposed by Deligne and Bimbot (1997) to model variable length regularities in streams of symbols (e.g., words, graphemes, or phonemes), phoneme multigrams are short se-quences of one or more phonemes. We produce a set of  X  X honeme transcripts X  by replacing transcript words with their lexical pronunciation. The set of multigrams is learned by then choosing a maximum-likelihood segmentation of these training phoneme transcripts, where the segmentation is viewed as hid-den data in an Expectation-Maximization algorithm. The set of all continuous phonemes occurring be-tween segment boundaries is then chosen as our multigram dictionary. This multigram recognition dictionary contains 16 , 409 entries.

After we have obtained each recognition lat-tice, our indexing approach follows that of Olsson (2008b). Namely, for the word and multigram sys-tems, we first expand lattice arcs containing multi-ple phones to produce a lattice having only single phonemes on its arcs. Then, we compute the ex-pected count of all phoneme n -grams n  X  5 in the lattice. These n -grams and their counts are inserted in our inverted index for retrieval.

This paper is organized as follows. In Section 2 we introduce our baseline RUR methods. In Sec-tion 3 we introduce our query degradation approach. We introduce our experimental validation in Sec-tion 4 and our results in Section 5. We find that using phrase-based query degradations can signifi-cantly improve upon a strong RUR baseline. Finally, in Section 6 we conclude and outline several direc-tions for future work. Each method we present in this paper ranks the ut-terances by the term X  X  estimated frequency within the corresponding phoneme lattice. This general approach has previously been considered (Yu and Seide, 2005; Saraclar and Sproat, 2004), on the ba-sis that it provides a minimum Bayes-risk ranking criterion (Yu et al., Sept 2005; Robertson, 1977) for the utterances. What differs for each method is the particular estimator of term frequency which is used. We first outline our baseline approach, a generative model for term frequency estimation.

Recall that our vocabulary-independent indices contain the expected counts of phoneme sequences from our recognition lattices. Yu and Seide (2005) used these expected phoneme sequence counts to es-timate term frequency in the following way. For a query term Q and lattice L , term frequency b tf estimated as b tf is an estimate for the number of words in the utter-ance. The conditional P ( Q |L ) is modeled as an or-der M phoneme level language model, so that b tf ity of a query phoneme q j being generated, given that the phoneme sequence q j  X  M +1 , . . . , q j  X  1 = q Here, E P in lattice L of the phoneme sequence q j  X  1 compute these counts using a variant of the forward-backward algorithm, which is implemented by the SRI language modeling toolkit (Stolcke, 2002).
In practice, because of data sparsity, the language model in Equation 1 must be modified to include smoothing for unseen phoneme sequences. We use a backoff M -gram model with Witten-Bell discount-ing (Witten and Bell, 1991). We set the phoneme language model X  X  order to M = 5 , which gave good results in previous work (Yu and Seide, 2005). One problem with the generative approach is that recognition error is not modeled (apart from the un-certainty captured in the phoneme lattice). The es-sential problem is that while the method hopes to model P ( Q |L ) , it is in fact only able to model the probability of one degradation H in the lattice, that is
P ( H |L ) . We define a query degradation as any phoneme sequence (including the lexical sequence) which may, with some estimated probability, occur in an errorful phonemic representation of the audio (either a one-best or lattice hypothesis). Because of speaker variation and because recognition is error-ful, we ought to also consider non-lexical degrada-tions of the query phoneme sequence. That is, we should incorporate P ( H | Q ) in our ranking function.
It has previously been demonstrated that allow-ing for phoneme confusability can significantly in-crease spoken term detection performance on one-best phoneme transcripts (Chaudhari and Picheny, 2007; Schone et al., 2005) and in phonemic lat-tices (Foote et al., 1997). These methods work by allowing weighted substitution costs in minimum-edit-distance matching. Previously, these substitu-tion costs have been maximum-likelihood estimates of P ( H | Q ) for each phoneme, where P ( H | Q ) is easily computed from a phoneme confusion matrix after aligning the reference and one-best hypothesis transcript under a minimum edit distance criterion. Similar methods have also been used in other lan-guage processing applications. For example, in (Ko-lak, 2005), one-for-one character substitutions, in-sertions and deletions were considered in a genera-tive model of errors in OCR.

In this work, because we are focused on construct-ing inverted indices of audio files (for speed and to conserve space), we must generalize our method of incorporating query degradations in the ranking function. Given a degradation model P ( H | Q ) , we take as our ranking function the expectation of the generative baseline estimate N L  X   X  P ( H |L ) with re-spect to P ( H | Q ) , b tf G ( Q, L ) = where H is the set of degradations. Note that, while we consider the expected value of our baseline term frequency estimator with respect to P ( H | Q ) , this general approach could be used with any other term frequency estimator.

Our formulation is similar to approaches taken in OCR document retrieval, using degradations of character sequences (Darwish and Magdy, 2007; Darwish, 2003). For vocabulary-independent spo-ken term detection, perhaps the most closely re-lated formulation is provided by (Mamou and Ram-abhadran, 2008). In that work, they ranked ut-terances by the weighted average of their match-ing score, where the weights were confidences from a grapheme to phoneme system X  X  first several hy-potheses for a word X  X  pronunciation. The match-ing scores were edit distances, where substitution costs were weighted using phoneme confusability. Accordingly, their formulation was not aimed at ac-counting for errors in recognition per se, but rather for errors in hypothesizing pronunciations. We ex-pect this accounts for their lack of significant im-provement using the method.

Since we don X  X  want to sum over all possible recognition hypotheses H , we might instead sum over the smallest set H such that  X  . That is, we could take the most probable degra-dations until their cumulative probability exceeds some threshold  X  . In practice, however, because degradation probabilities can be poorly scaled, we instead take a fixed number of degradations and normalize their scores. When a query is issued, we apply a degradation model to learn the top few phoneme sequences H that are most likely to have been recognized, under the model. In the machine translation literature, this process is commonly re-ferred to as decoding .

We now turn to the modeling of query degrada-tions H given a phoneme sequence Q , P ( H | Q ) . First, we consider a simple baseline approach in Sec-tion 3.1. Then, in Section 3.2, we propose a more powerful technique, using state-of-the-art machine translation methods to hypothesize our degradations. 3.1 Baseline Query Degradations Schone et al. (2005) used phoneme confusion ma-trices created by aligning hypothesized and refer-ence phoneme transcripts to weight edit costs for a minimum-edit distance based search in a one-best phoneme transcript. Foote et al. (1997) had previ-ously used phoneme lattices, although with ad hoc edit costs and without efficient indexing. In this work, we do not want to linearly scan each phoneme lattice for our query X  X  phoneme sequence, preferring instead to look up sequences in the inverted indices containing phoneme sequences.

Our baseline degradation approach is related to the edit-cost approach taken by (Schone et al., 2005), although we generalize it so that it may be applied within Equation 2 and we consider speech recognition hypotheses beyond the one-best hypoth-esis. First, we randomly generate N traversals of each phonemic recognition lattice. These traver-sals are random paths through the lattice (i.e., we start at the beginning of the lattice and move to the next node, where our choice is weighted by the out-going arcs X  probabilities). Then, we align each of these traversals with its reference transcript using a minimum-edit distance criterion. Phone confusion matrices are then tabulated from the aggregated in-sertion, substitution, and deletion counts across all traversals of all lattices. From these confusion ma-trices, we compute unsmoothed estimates of P ( h | r ) , the probability of a phoneme h being hypothesized given a reference phoneme r .

Making an independence assumption, our base-line degradation model for a query with m phonemes is then P ( H | Q ) = efficiently compute the most probable degradations for a query Q using a lattice of possible degrada-tions and the forward backward algorithm. We call this baseline degradation approach CMQD (Confu-sion Matrix based Query Degradation). 3.2 Phrase-Based Query Degradation One problem with CMQD is that we only allow in-sertions, deletions, and one-for-one substitutions. It may be, however, that certain pairs of phonemes are commonly hypothesized for a particular refer-ence phoneme (in the language of statistical machine translation, we might say that we should allow some non-zero fertility ). Second, there is nothing to dis-courage query degradations which are unlikely un-der an (errorful) language model X  X hat is, degrada-tions that are not observed in the speech hypothe-ses. Finally, CMQD doesn X  X  account for similarities between phoneme classes. While some of these de-ficiencies could be addressed with an extension to CMQD (e.g., by expanding the degradation lattices to include language model scores), we can do bet-ter using a more powerful modeling framework. In particular, we adopt the approach of phrase-based statistical machine translation (Koehn et al., 2003; Koehn and Hoang, 2007). This approach allows for multiple-phoneme to multiple-phoneme substi-tutions, as well as the soft incorporation of addi-tional linguistic knowledge (e.g., phoneme classes). This is related to previous work allowing higher or-der phoneme confusions in bigram or trigram con-texts (Chaudhari and Picheny, 2007), although they used a fuzzy edit distance measure and did not in-corporate other evidence in their model (e.g., the phoneme language model score). The reader is re-ferred to (Koehn and Hoang, 2007; Koehn et al., 2007) for detailed information about phrase-based statistical machine translation. We give a brief out-line here, sufficient only to provide background for our query degradation application.

Statistical machine translation systems work by converting a source-language sentence into the most probable target-language sentence, under a model whose parameters are estimated using example sen-tence pairs. Phrase-based machine translation is one variant of this statistical approach, wherein multiple-word phrases rather than isolated words are the basic translation unit. These phrases are gener-ally not linguistically motivated, but rather learned from co-occurrences in the paired example transla-tion sentences. We apply the same machinery to hy-pothesize our pronunciation degradations, where we now translate from the  X  X ource-language X  reference phoneme sequence Q to the hypothesized  X  X arget-language X  phoneme sequence H .

Phrase-based translation is based on the noisy channel model, where Bayes rule is used to refor-mulate the translation probability for translating a reference query Q into a hypothesized phoneme se-quence H as Here, for example, P ( H ) is the language model probability of a degradation H and P ( Q | H ) is the conditional probability of the reference sequence Q given H . More generally however, we can incorpo-rate other feature functions of H and Q , h i ( H, Q ) , and with varying weights. This is implemented us-ing a log-linear model for P ( H | Q ) , where the model covariates are the functions h i ( H, Q ) , so that The parameters  X  i are estimated by MLE and the normalizing Z need not be computed (because we will take the argmax). Example feature functions in-clude the language model probability of the hypoth-esis and a hypothesis length penalty.

In addition to feature functions being defined on the surface level of the phonemes, they may also be defined on non-surface annotation levels, called fac-tors . In a word translation setting, the intuition is that statistics from morphological variants of a lex-ical form ought to contribute to statistics for other variants. For example, if we have never seen the word houses in language model training, but have examples of house , we still can expect houses are to be more probable than houses fly . In other words, factors allow us to collect improved statistics on sparse data. While sparsity might appear to be less of a problem for phoneme degradation modeling (because the token inventory is comparatively very small), we nevertheless may benefit from this ap-proach, particularly because we expect to rely on higher order language models and because we have rather little training data: only 22 , 810 transcribed utterances (about 600k reference phonemes).
In our case, we use two additional annotation lay-ers, based on a simple grouping of phonemes into broad classes. We consider the phoneme itself, the broad distinction of vowel and consonant, and a finer grained set of classes (e.g., front vowels, central vowels, voiceless and voiced fricatives). Figure 1 shows the three annotation layers we consider for an example reference phoneme sequence. After map-ping the reference and hypothesized phonemes to each of these additional factor levels, we train lan-guage models on each of the three factor levels of the hypothesized phonemes. The language models for each of these factor levels are then incorporated as features in the translation model.

We use the open source toolkit Moses (Koehn et al., 2007) as our phrase-based machine transla-tion system. We used the SRI language model-ing toolkit to estimate interpolated 5-gram language models (for each factor level), and smoothed our estimates with Witten-Bell discounting (Witten and Bell, 1991). We used the default parameter settings for Moses  X  X  training, with the exception of modi-fying GIZA++  X  X  default maximum fertility from 10 to 4 (since we don X  X  expect one reference phoneme to align to 10 degraded phonemes). We used default decoding settings, apart from setting the distortion penalty to prevent any reorderings (since alignments are logically constrained to never cross). For the rest of this chapter, we refer to our phrase-based query degradation model as PBQD. We denote the phrase-based model using factors as PBQD-Fac.

Figure 2 shows an example alignment learned for a reference and one-best phonemic transcript. The reference utterance  X  X now white and the seven dwarves X  is recognized (approximately) as  X  X o white a the second walks X . Note that the phrase-based system is learning not only acoustically plau-sible confusions, but critically, also confusions aris-ing from the phonemic recognition system X  X  pe-culiar construction. For example, while V and K may not be acoustically similar, they are still confusable X  X ithin the context of S EH  X  X ecause multigram language model data has many exam-ples of the word second . Moreover, while the word dwarves ( D-W-OW-R-F-S ) is not present in the dictionary, the words dwarf ( D-W-AO-R-F ) and dwarfed ( D-W-AO-R-F-T ) are present ( N.B. , the change of vowel from AO to OW between the OOV and in vocabulary pronunciations). While CMQD would have to allow a deletion and two substitutions (without any context) to obtain the correct degrada-tion, the phrase-based system can align the complete phrase pair from training and exploit context. Here, for example, it is highly probable that the errorfully hypothesized phonemes W AO will be followed by K , because of the prevalence of walk in language model data. An appropriate and commonly used measure for RUR is Mean Average Precision (MAP). Given a ranked list of utterances being searched through, we define the precision at position i in the list as the pro-portion of the top i utterances which actually contain the corresponding query word. Average Precision ( AP ) is the average of the precision values computed for each position containing a relevant utterance. To assess the effectiveness of a system across multi-ple queries, Mean Average Precision is defined as the arithmetic mean of per-query average precision, we report statistically significant improvements in MAP, we are comparing AP for paired queries us-ing a Wilcoxon signed rank test at  X  = 0 . 05 .
Note, RUR is different than spoken term detec-tion in two ways, and thus warrants an evaluation measure (e.g., MAP) different than standard spoken term detection measures (such as NIST X  X  actual term weighted value (Fiscus et al., 2006)). First, STD measures require locating a term with granularity finer than that of an utterance. Second, STD mea-sures are computed using a fixed detection thresh-old. This latter requirement will be unnecessary in many applications (e.g., where a user might prefer to decide themselves when to stop reading down the ranked list of retrieved utterances) and unlikely to be helpful for downstream evidence combination (where we may prefer to keep all putative hits and weight them by some measure of confidence).
For our evaluation, we consider retrieving short utterances from seventeen fully transcribed MALACH interviews. Our query set contains all single words occurring in these interviews that are OOV with respect to the word dictionary. This gives us a total of 261 query terms for evalua-tion. Note, query words are also not present in the multigram training transcripts, in any language model training data, or in any transcripts used for degradation modeling. Some example query words include BUCHENWALD , KINDERTRANSPORT , and SONDERKOMMANDO .

To train our degradation models, we used a held out set of 22 , 810 manually transcribed utterances. We run each recognition system (phoneme, multi-gram, and word) on these utterances and, for each, train separate degradation models using the aligned reference and hypothesis transcripts. For CMQD, we computed 100 random traversals on each lattice, giving us a total of 2 , 281 , 000 hypothesis and refer-ence pairs to align for our confusion matrices. We first consider an intrinsic measure of the three speech recognition systems we consider, namely Phoneme Error Rate (PER). Phoneme Error Rate is calculated by first producing an alignment of the hypothesis and reference phoneme transcripts. The counts of each error type are used to compute P ER = 100  X  S + D + I N , where S, D, I are the num-ber of substitutions, insertions, and deletions respec-tively, while N is the phoneme length of the refer-ence. Results are shown in Table 1. First, we see that the PER for the multigram system is roughly half that of the phoneme-only system. Second, we find that the word system achieves a considerably lower PER than the multigram system. We note, however, that since these are not true phonemes (but rather phonemes copied over from pronunciation dictionar-ies and word transcripts), we must cautiously inter-pret these results. In particular, it seems reasonable that this framework will overestimate the strength of the word based system. For comparison, on the same train/test partition, our word-level system had a word error rate of 31.63. Note, however, that au-tomatic word transcripts can not contain our OOV query words, so word error rate is reported only to give a sense of the difficulty of the recognition task.
Table 1 shows our baseline RUR evaluation re-sults. First, we find that the generative model yields statistically significantly higher MAP using words or multigrams than phonemes. This is almost cer-tainly due to the considerably improved phoneme recognition afforded by longer recognition units. Second, many more unique phoneme sequences typ-ically occur in phoneme lattices than in their word or multigram counterparts. We expect this will in-crease the false alarm rate for the phoneme system, thus decreasing MAP.

Surprisingly, while the word-based recognition system achieved considerably lower phoneme er-ror rates than the multigram system (see Table 1), the word-based generative model was in fact in-distinguishable from the same model using multi-grams. We speculate that this is because the method, as it is essentially a language modeling approach, is sensitive to data sparsity and requires appropri-ate smoothing. Because multigram lattices incor-porate smaller recognition units, which are not con-strained to be English words, they naturally produce smoother phoneme language models than a word-based system. On the other hand, the multigram system is also not statistically significantly better than the word-based generative model, suggesting this may be a promising area for future work.
Table 1 shows results using our degradation mod-els. Query degradation appears to help all sys-tems with respect to the generative baseline. This agrees with our intuition that, for RUR, low MAP on OOV terms is predominately driven by low recall. 1 Note that, at one degradation, CMQD has the same MAP as the generative model, since the most prob-able degradation under CMQD is almost always the reference phoneme sequence. Because the CMQD model can easily hypothesize implausible degrada-tions, we see the MAP increases modestly with a few degradations, but then MAP decreases. In con-trast, the MAP of the phrase-based system (PBQD-Fac) increases through to 500 query degradations us-ing multigrams. The phonemic system appears to achieve its peak MAP with fewer degradations, but also has a considerably lower best value.

The non-factored phrase-based system PBQD achieves a peak MAP considerably larger than the peak CMQD approach. And, likewise, using addi-tional factor levels (PBQD-Fac) also considerably improves performance. Note especially that, using multiple factor levels, we not only achieve a higher MAP, but also a higher MAP when only a few degra-dations are possible.

To account for errors in phonemic recognition, we have taken two steps. First, we used longer recog-nition units which we found significantly improved MAP while using our baseline RUR technique. As a second method for handling recognition errors, we also considered variants of our ranking func-tion. In particular, we incorporated query degrada-tions hypothesized using factored phrase-based ma-chine translation. Comparing the MAP for PBQD-Fac with MAP using the generative baseline for the most improved indexing system (the word system), we find that this degradation approach again statisti-cally significantly improved MAP. That is, these two strategies for handling recognition errors in RUR ap-pear to work well in combination.
 Although we focused on vocabulary-independent RUR, downstream tasks such as ad hoc speech retrieval will also want to incorporate evidence from in-vocabulary query words. This makes our query degradation approach which indexed phonemes from word-based LVCSR particularly at-tractive. Not only did it achieve the best MAP in our evaluation, but this approach also allows us to construct recognition lattices for both in and out-of-vocabulary query words without running a second, costly, recognition step. Our goal in this work was to rank utterances by our confidence that they contained a previously unseen query word. We proposed a new approach to this task using hypothesized degradations of the query word X  X  phoneme sequence, which we produced us-ing a factored phrase-based machine translation model. This approach was principally motivated by the mismatch between the query X  X  phonemes and the recognition phoneme sequences due to errorful speech indexing. Our approach was constructed and evaluated using phoneme-, multigram-, and word-based indexing, and significant improvements in MAP using each indexing system were achieved. Critically, these significant improvements were in addition to the significant gains we achieved by con-structing our index with longer recognition units.
While PBQD-Fac outperformed CMQD averag-ing over all queries in our evaluation, as expected, there may be particular query words for which this is not the case. Table 2 shows example degrada-tions using both the CMQD and PBQD-Fac degra-dation models for multigrams. The query word is Mengele . We see that CMQD degradations are near (in an edit distance sense) to the reference pronun-ciation ( M-EH-NX-EY-L-EH ), while the phrase-based degradations tend to sound like commonly oc-curring words ( mental, meant a lot, men they. . . , mentally ). In this case, the lexical phoneme se-quence does not occur in the PBQD-Fac degrada-tions until degradation nineteen. Because delet-ing EH has the same cost irrespective of context for CMQD, both CMQD degradations 2 and 3 are given the same pronunciation weight. Here, CMQD performs considerably better, achieving an average precision of 0.1707, while PBQD-Fac obtains only 0.0300. This suggests that occasionally the phrase-based language model may exert too much influence on the degradations, which is likely to increase the incidence of false alarms. One solution, for future work, might be to incorporate a false alarm model (e.g., down-weighting putative occurrences which look suspiciously like non-query words). Second, we might consider training the degradation model in a discriminative framework (e.g., training to op-timize a measure that will penalize degradations which cause false alarms, even if they are good can-didates from the perspective of MLE). We hope that the ideas presented in this paper will provide a solid foundation for this future work.
