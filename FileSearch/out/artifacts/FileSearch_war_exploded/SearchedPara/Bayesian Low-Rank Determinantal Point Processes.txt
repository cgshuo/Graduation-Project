 Determinantal point processes (DPPs) are an emerging model for encoding probabilities over subsets, such as shopping baskets, selected from a ground set, such as an item cat-alog. They have recently proved to be appealing models for a number of machine learning tasks, including prod-uct recommendation. DPPs are parametrized by a pos-itive semi-definite kernel matrix. Prior work has shown that using a low-rank factorization of this kernel provides scalability improvements that open the door to training on large-scale datasets and computing online recommendations, both of which are infeasible with standard DPP models that use a full-rank kernel. A low-rank DPP model can be trained using an optimization-based method, such as stochastic gradient ascent, to find a point estimate of the kernel parameters, which can be performed efficiently on large-scale datasets. However, this approach requires care-ful tuning of regularization parameters to prevent overfitting and provide good predictive performance, which can be com-putationally expensive. In this paper we present a Bayesian method for learning a low-rank factorization of this kernel, which provides automatic control of regularization. We show that our Bayesian low-rank DPP model can be trained effi-ciently using stochastic gradient Hamiltonian Monte Carlo (SGHMC). Our Bayesian model generally provides better predictive performance on several real-world product recom-mendation datasets than optimization-based low-rank DPP models trained using stochastic gradient ascent, and better performance than several state-of-the art recommendation methods in many cases.
Online shopping revenue has grown significantly in recent years. Central to the online retail experience is the recom-mendation task of  X  X asket completion X , where we seek to compute predictions for the next item that should be added  X  C urrently at Google DeepMind.
 to a shopping basket, given a set of items already present in the basket. Determinantal point processes (DPPs) of-fer an attractive model for basket completion, since they jointly model set diversity and item quality or popularity . DPPs also offer a compact parameterization and efficient algorithms for performing inference.

A distribution over sets that models diversity is of particu-lar interest when recommendations are complementary. For example, consider a shopping basket that contains a smart-phone and a SIM card. A collaborative filtering method based on user and item similarities, such as a matrix factor-ization model [27], would tend to provide recommendations that are similar to the items already present in the basket but not necessarily complementary. In this example, matrix factorization might recommend other similar smartphones to complete this basket, which may not be appropriate since the basket already contains a smartphone. In contrast, a complementary recommendation for this basket might be a smartphone case, rather than another smartphone. In this setting, DPPs would be used to learn the inherent item di-versity present within the observed sets (baskets) that users purchase, and hence can provide such complementary rec-ommendations.

DPPs have been used for a variety of machine learning tasks [16, 18, 19]. DPPs can be parameterized by a M  X  M positive semi-definite L matrix, where M is the size of the item catalog. There has been some work focused on learning DPPs from observed data consisting of example subsets [1, 10, 12, 17, 24], which is a challenging learning task that is conjectured to be NP-hard [18]. Some of this recent work has involved learning a nonparametric full-rank L matrix [12, 24] that does not constrain L to take a particular paramet-ric form, while other recent work has involved learning a low-rank factorization of this nonparametric L matrix [10]. A low-rank factorization of L enables substantial improve-ments in runtime performance compared to a full-rank DPP model during training and when computing predictions, on the order of 10-20x or more, with predictive performance that is equivalent to or better than a full-rank model.
The low-rank DPP model presented in [10] uses stochastic gradient ascent to maximize an objective function defined in terms of a low-rank factorization of L . While this ap-proach for model learning is efficient on large-scale data, it has some drawbacks. Careful tuning of regularization hy-perparameters is required to prevent overfitting and provide good predictive performance. This tuning can be performed using a line search over the range of possible regularization settings. This procedure is expensive since it entails train-ing a separate model for each regularization setting and then s electing the model that performs best. The optimization-based approach also provides a point estimate that commits to a single most probable setting for each learned parameter, which can be problematic in that it does not consider uncer-tainty. In contrast to this approach, we present a Bayesian low-rank DPP model that provides better predictive per-formance and robust regularization, without the need for expensive hyperparameter tuning.

Our work makes the following contributions: 1. We present a Bayesian low-rank DPP model, which uses an efficient stochastic gradient Hamiltonian Monte Carlo (SGHMC) algorithm for learning from observed data. 2. The Bayesian low-rank DPP model does not require ex-pensive hyperparameter tuning and provides robust regular-ization, in contrast to prior work on an optimization-based learning algorithm for low-rank DPPs. 3. A detailed experimental evaluation on several real-world datasets shows that our Bayesian model provides better pre-dictive performance than existing low-rank DPP models. Our model also provides significantly better predictive per-formance than several other recommendation methods in many cases.
DPPs originated in statistical mechanics [23], where they were used to model distributions of fermions. Fermions are particles that obey the Pauli exclusion principle, which in-dicates that no two fermions can occupy the same quantum state. As a result, systems of fermions exhibit a repulsion or  X  X nti-bunching X  effect, which is described by a DPP. This repulsive behavior is a key characteristic of DPPs, which makes them a capable model for diversity. We now proceed with some details of DPPs, including how they are defined and a method for efficient learning.
A point process is a distribution over configurations of points selected from a ground set Y , which are finite subsets of Y . In this paper we deal only with discrete DPPs, which describe a distribution over subsets of a discrete ground set of items Y = 1 , 2 , . . . , M , which we also call the item catalog. A discrete DPP on Y is a probability measure P on 2 Y (the power set or set of all subsets of Y ), such that for any A  X  Y , the probability P ( A ) is specified by P ( A )  X  det( L A context of basket completion, Y is the item catalog (inven-tory of items on sale), and Y is the subset of items in a user X  X  basket; there are 2 |Y| possible baskets. The notation L Y notes the principal submatrix of the DPP kernel L indexed by the items in Y , which is the restriction of L to the rows and columns indexed by the elements of Y : L A  X  [ L ij ] Intuitively, the diagonal entry L ii of the kernel matrix L captures the importance or quality of item i , while the off-diagonal entry L ij = L ji measures the similarity between items i and j .

The normalization constant for P follows from the ob-servation that P A  X   X  X  det( L A  X  ) = det( L + I ). The value det( L A ) associates a  X  X olume X  to basket A from a geometric viewpoint, and its probability is normalized by the volumes of all possible baskets A  X   X  Y . Therefore, we have Figure 1: A graphical model for the low-rank DPP m odel.
 We use a low-rank factorization of the M  X  M L matrix, for the M  X  K matrix V , where M is the number of items in the item catalog and K is the number of latent trait dimensions. This low-rank factorization of L leads to sig-nificant efficiency improvements compared to a model that uses a full-rank L matrix when it comes to model learning and computing predictions [10]. This also places an implicit constraint on the space of subsets of Y , since the model is re-stricted to place zero probability mass on subsets with more than K items (all eigenvalues of L beyond K are zero). We see this from the observation that a sample from a DPP will not be larger than the rank of L [11].
Our learning task is to fit a DPP kernel L based on a collection of N observed subsets A = { A 1 , . . . , A N } , where each subset A n is composed of items from the item catalog Y . These observed subsets in A constitute our training data, and our task is to infer L from A . The log-likelihood for seeing A is where [ n ] indexes the observations or objects in A . Recall from (2) that L = VV T .

Figure 1 shows the graphical model for our Bayesian low-rank DPP model. We place a multivariate Gaussian prior on each item in our model. Our prior distribution on V is given by where v i is the row vector from V for item i , and all items i share the same precision  X  . We furthermore place a con-jugate gamma prior on  X  : p (  X  | a 0 , b 0 ) = Gamma(  X  ; a The joint distribution over A , V and  X  , as depicted in Fig-ure 1, is To draw samples from the posterior p ( V ,  X  |A , a 0 , b The first conditional distribution is and is sampled in Line 4 in Algorithm 1.
The second conditional distribution p ( V | A ,  X  ) does not have the same simple form, and is: where const indicates an additive constant independent of V . In the following section, we consider sampling from p ( V |A ,  X  ).
We estimate the conditional distribution p ( V |A ,  X  ) using stochastic gradient Hamiltonian Monte Carlo (SGHMC) [7]. Hamiltonian Monte Carlo (HMC) [9, 26] is a Markov chain Monte Carlo (MCMC) method that uses the gradient of the log-density of the target distribution to efficiently explore the state space of the target. HMC defines a Hamiltonian function, an idea borrowed from physics, in terms of the tar-get distribution that we wish to sample from. The Hamilto-nian function has a potential energy term, corresponding to the target distribution, and a kinetic energy term, defined in terms of auxiliary momentum variables. By updating the momentum variables using the gradient of the log-density of the target distribution, we simulate a Hamiltonian dynam-ical system that enables proposals of distant states, thus allowing HMC to move rapidly through the state space of the target. We cannot simulate directly from the continu-ous Hamiltonian dynamics, so HMC uses a discretization of this continuous system composed of a number of  X  X eapfrog steps X .

HMC requires computation of the gradient of the log-density of the target distribution over all training instances with each iteration of the algorithm, which is expensive or infeasible for large datasets or a complex target. SGHMC addresses this issue by using stochastic gradients that are computed on minibatches, where each minibatch is com-posed of training instances sampled uniformly at random from the full training set. SGHMC adds a friction term to the momentum update, which counteracts the effects of noise from the stochastic gradients. The estimates computed by SGHMC samples are not unbiased any more (notice that there is no accept-reject step in Algorithm 1, and the distri-bution that is sampled from is different from the true poste-rior), but due to the effectively faster mixing, we are able to efficiently train our Bayesian low-rank DPP model on large-scale datasets.

Since we learn p ( V |A ,  X  ) by SGHMC, we need to effi-ciently compute the gradient of the log-density for this distri-bution. We begin by computing the gradient of log-likelihood,  X  X  / X  V , which will be a M  X  K matrix. For i  X  1 , . . . , M and k  X  1 , . . . , K , we need a matrix of scalar derivatives, log-likelihood, we have  X  X   X  X  Algorithm 1 S ampling algorithm for learning p ( V |A ,  X  ) 1: i nitialize V randomly, W = 0 2: samples := {} 3: repeat 4: sample  X  | V , a 0 , b 0 according to (7) 5: // approximately sample V |A ,  X  : 6: for leapfrogSteps j = 1 , . . . , L do 7: W :=  X   X   X  U ( V )  X   X  W + N (0 , 2(  X   X   X   X  )  X  ) 8: V := V + W 9: end for 10: samples := { samples , V } 11: until sufficient samples have been taken
Examining the first term of the derivative, we see that where a i denotes row i of the matrix A = L  X  1 [ n ] and v denotes column k of V [ n ] . Note that L [ n ] = V [ n ] puting A is a relatively inexpensive operation, since the number of items in each training instance A n is generally small for many recommendation applications.

For the second term of the derivative, we see that where b i denotes row i of the matrix B = I m  X  V ( I k + V
T V )  X  1 V T . Computing B is a relatively inexpensive op-eration, since we are inverting a K  X  K matrix with cost O ( K 3 ), and K (the number of latent trait dimensions) is usually set to a small value.

We now proceed with computing the gradient of the log-density for p ( V |A ,  X  ) shown in Equation 9. Looking at one component of this gradient, we have:
Our SGHMC algorithm is shown in Algorithm 1. In this algorithm,  X   X  U ( V ) is a noisy estimate of the log-density gra-dient of p ( V |A ,  X  ) computed for a minibatch,  X  &gt; 0 is the learning rate,  X   X  [0 , 1] is the momentum coefficient, and W is the auxiliary momentum variable.  X   X  is an estimate of the noise from the gradient, which we ignore by setting  X   X  = 0 and relying on small  X  , as explained in [7]. We find that setting  X  = 0 . 01 and  X  = 1 . 0  X  10  X  5 or  X  = 1 . 0  X  10 with a minibatch size of 1000 instances, works well for the datasets we tested.
We compute singleton next-item predictions, given a set of observed items. An example of this class of problem is  X  X asket completion X , where we seek to compute predictions for the next item that should be added to shopping basket, given a set of items already present in the basket. We use a k -DPP to compute next-item predictions. A k -DPP is a distribution over all subsets A  X  Y with cardinality k , where Y is the ground set, or the set of all items in the item catalog. Next item predictions are done via a condi-tional density. We compute the probability of the observed basket A , consisting of k items. For each possible item to be recommended, given the basket, the basket is enlarged w ith the new item to k + 1 items. For the new item, we de-termine the probability of the new set of k + 1 items, given that k items are already in the basket, using a Monte Carlo estimate from the samples. Ignoring burn-in samples, and letting s index the S remaining V ( s ) samples in samples , p ( A +1 | A, A , a 0 , b 0 ) = Z P ( A +1 | A, V ) p ( V |A , a where A +1 indicates set A enlarged to contain a new item b from the catalog Y . The samples in V implicitly marginal-ize out  X  from the posterior density. We run the sampler to generate 2,000 samples, and discard the first 1,800 samples as burn-in. From [10], we see that the conditional probabil-ity for an item b in the singleton set B , given the appearance of items in A , is where L A bb denotes diagonal element bb from the k -DPP ker-nel matrix conditioned on A , L A ;  X  A 1 ,  X  A 2 , . . . ,  X  eigenvalues of L A ; and e 1 (  X  A 1 ,  X  A 2 , . . . ,  X  A mentary symmetric polynomial on these eigenvalues. See [10] for full details on how to efficiently compute conditional den-sities for a k -DPP given an observed basket.
In this section we evaluate the performance of Bayesian low-rank DPP model on on the task of basket completion for several real-world datasets. We compare to several com-peting recommendation methods, including an optimization-based low-rank DPP [10] and two matrix factorization mod-els [13, 27], and find that our approach provides better pre-dictive performance in many cases.
 We formulate the basket-completion task as follows. Let A test be a subset of n  X  2 co-purchased items (i.e, a basket) from the test-set. In order to evaluate the basket completion task, we pick an item i  X  A test at random and remove it from A test . We denote the remaining set as A test  X  1 . Formally, A test  X  1 = A test { i } . Given a ground set of possible items Y = 1 , 2 , ..., M , we define the candidates set C as the set of all items except those already in A test  X  1 ; i.e., C = Y A test  X  1 Our goal is to identify the missing item i from all other items in C .
Our experiments are based on several datasets: 1. Amazon Baby Registries -Amazon 1 is one of the world X  X  leading online retail stores. The Amazon Baby Reg-istries dataset [12] is a public dataset consisting of 111,006 registries or  X  X askets X  of baby products. The choice of this dataset was motivated by the fact that it has been used by several prominent DPP studies [10, 12, 24]. The registries are collected from 15 different categories (such as  X  X eeding X ,  X  X iapers X ,  X  X oys X , etc.), and the items in each category are disjoint. We maintain consistency with prior work by evalu-ating each of its categories separately using a random split of w ww.amazon.com 70% of the data for training and 30% for testing. The low-rank DPP models trained on this dataset were built with K = 30 trait dimensions.

In addition to the above evaluation, we also constructed a dataset composing of the concatenation of the three most popular categories: apparel, diaper, and feeding. This three-category dataset allows us to simulate data that could be ob-served for department stores that offer a wide range of items in different product categories. Its construction is deliber-ate, and concatenates three disjoint subgraphs of basket-item purchase patterns. This dataset serves to highlight differences between DPPs and models based on matrix fac-torization (MF). Collaborative filtering-based MF models  X  which model each basket and item with a latent vector  X  will perform poorly for this dataset, as the latent trait vectors of baskets and items in one subgraph could be arbitrarily rotated, without affecting the likelihood or predictive error in any of the other subgraphs. MF models are invariant to global rotations of the embedded trait vectors. However, for the concatenated dataset, these models are also invariant to arbitrary rotations of vectors in each disjoint subgraph, as there are no shared observations between the three cate-gories. A global ranking based on inner products could then be arbitrarily affected by the basket and item embeddings arising from each subgraph. 2. MS Store -This dataset is based on data from Mi-crosoft X  X  web-based store 2 . The dataset is composed of 243,147 baskets consisting of commonly purchased items from a catalog of 2097 different hardware and software prod-ucts. We randomly sampled of 80% of the data for training and kept the remaining 20% for testing. Recall from Sec-tion 2.1 that a low-rank DPP places zero probability mass on subsets with more than K items, where K is the num-ber of trait dimensions in V or the rank of L . With this constraint in mind, we use K = 15 trait dimensions for the low-rank DPP models trained on this data, since the largest observed basket in this dataset is composed of 15 items. 3. Belgian Retail Supermarket -This public dataset includes 88,163 baskets consisting of 16,470 unique super-market items. It was collected in a Belgian retail supermar-ket over three non-consecutive time periods, as described in [3, 4]. Again, we randomly sampled 80% of the data for training and kept the remaining 20% for testing. We use K = 76 trait dimensions for the low-rank DPP model trained on this data, since the largest observed basket in this dataset is composed of 76 items.

Since we are interested in the basket completion task, which requires baskets containing at least two items, we re-move all baskets containing only one item from each dataset before splitting the data into training and test sets.
We evaluate against several baselines: 1. Full-rank DPP -This DPP model is parameterized by a full-rank L matrix, and uses a fixed-point optimization algorithm called Picard iteration [24] for learning L . As described in [10], a full-rank DPP does not scale well to datasets containing large item catalogs during training or when computing predictions. m icrosoftstore.com 2. L ow-rank DPP trained using stochastic gradi-ent ascent (SGA) -This DPP model is parameterized by a low-rank L matrix that is factorized using a V matrix composed of latent item trait vectors, and has a likelihood function identical to our Bayesian low-rank DPP. In contrast to our Bayesian approach, this optimization-based model is trained using stochastic gradient ascent, and uses regular-ization based on item popularity. We selected the regular-ization hyperparameter for this model using a line search performed with the training set. 3. Poisson Factorization (PF) -Poisson factorization (PF) is a prominent variant of matrix factorization designed specifically for implicit ratings [13]. The likelihood of the PF model is based on the Poisson distribution, which is use-ful with implicit datasets (e.g. datasets based on click or purchase events). The evaluations in this paper are based on the publicly available implementation 3 from [5]. In PF, Gamma priors are placed on the trait vectors. Following [6, 13], we set the Gamma shape and rate hyperparameters to 0.3. 4. Reco Matrix Factorization (RecoMF) -RecoMF is a matrix factorization model powering the Xbox Live rec-ommendation system [27]. The likelihood term of RecoMF uses a sigmoid function to model the odds of a user lik-ing or disliking an item in the dataset. Unlike PF, Re-coMF requires the generation of synthetic negative training instances, and uses a scheme for sampling negatives based on popularity. RecoMF places Gaussian priors on the trait vectors, and gamma hyperpriors on each. We use the hyper-parameter settings described in [27], which have been found to provide good performance for implicit recommendation data. 5. Associative Classifier (AC) -Since association rules are often used for market basket analysis [2, 15], we consider an associative classifier as a competing method. We use the publicly available implementation [8] of the Classi-fication Based on Associations (CBA) algorithm [22]. We set minimum support and minimum confidence thresholds of 1.0% and 20.0%, respectively.

We use a flexible prior in our Bayesian low-rank DPP model by setting a 0 = model is not sensitive to these settings.

The matrix-factorization models are parameterized in terms of users and items. Since we have no explicit users in our data, we construct  X  X irtual X  users from the contents of each basket for the purposes of our evaluation, where a new user u m is constructed for each basket b m . Therefore, the set of items that u m has purchased is simply the contents of b . Additionally, we use K = 40 trait dimensions for the matrix-factorization models.
In the following evaluation we consider three measures: 1. Mean Percentile Rank (MPR) -Computing the Percentile Rank of an item requires the ability to rank the item j against all other items in C . Therefore, the MPR evaluation results don X  X  include the AC model, which ranks
N ote that [5] is actually an implementation of PF with a social component, which was disabled in the course of our evaluations since the data does not include a social graph. only those items for which an association rule was found. For other models we ranked the items according to their probabilities of completing the missing set Y n  X  1 . Namely, given an item i from the candidates set C , we denote by p the probability P ( Y n  X  X  i }| Y n  X  1 ). The Percentile Rank (PR) of the missing item j is defined by where I (  X  ) is an indicator function and |C| is the number of items in the candidates set. The Mean Percentile Rank (MPR) is the average PR of all the instances in the test-set: w here T is the set of test instances. MPR is a recall-oriented metric commonly used in studies that involve implicit rec-ommendation data [14, 21]. MPR = 100 always places the held-out item for the test instance at the head of the ranked list of predictions, while MPR = 50 is equivalent to random selection. 2. Precision@ k -We define precision@ k as w here rank t is the predicted rank of the held-out item for test instance t . In other words, precision@ k is the fraction of instances in the test set for which the predicted rank of the held-out item falls within the top k predictions. 3. Popularity-weighted precision@ k -Datasets used to evaluate recommendation systems typically contain a pop-ularity bias [28], where users are more likely to provide feed-back on popular items. Due to this popularity bias, conven-tional metrics such as MPR and precision@ k are typically biased toward popular items. Using ideas from [28], we de-fine popularity-weighted precision@ k : where w t is the weight assigned to the held-out item for test of occurrences of the held-out item for test instance t in the training data, and  X   X  [0 , 1]. The weights are normalized, so that P j  X  X  w j = 1. This popularity-weighted precision@ k measure assumes that item popularity follows a power-law. By assigning more weight to less popular items, for  X  &gt; 0, this measure serves to bias precision@ k towards less popular items. For  X  = 0, we obtain the conventional precision@ k measure. We set  X  = 0 . 5 in our evaluation.

Figures 2, 3, and 4 show the performance of each method and dataset for our evaluation measures. Our Bayesian low-rank DPP model is denoted as  X  X GHMC low-rank DPP X  in these figures, in reference to its learning algorithm. Note that we could not feasibly train the full-rank DPP or AC models on the Belgian dataset, since these models do not scale to datasets with large item catalogs. The Bayesian SGHMC low-rank DPP generally outperforms the optimiza-tion-based SGA low-rank DPP by a moderate amount on most metrics and datasets, which illustrates the advantage o f our Bayesian approach. We attribute this improvement to the more robust regularization provided by a Bayesian model. By averaging over all settings of the parameters that are compatible with both the observed data and the prior, our Bayesian low-rank DPP model deals with uncertainty more effectively than the SGA low-rank DPP model, which commits to a single most probable setting for each param-eter. An additional advantage of our Bayesian approach is that it provides a predictive distribution instead of just a single point estimate, which enables the confidence in the prediction to be estimated according to its variance. We can therefore make use of this predictive confidence when making recommendations.

We see that the RecoMF model outperforms all other models on all metrics for the Amazon Diaper dataset. For all other datasets, the Bayesian low-rank DPP model outper-forms non-DPP models on MPR by a sizeable margin, and provides consistently provide high MPR across all datasets. For the precision@ k metrics, the Bayesian low-rank DPP of-ten leads or provides good performance that is close to the leader.

Limitations. The popularity-weighted precision@ k re-sults in Figure 4 highlight a limitation of the DPP models. For this metric RecoMF generally provides the best perfor-mance, with the DPP models in second place. This behavior may result from the scheme for sampling negatives by popu-better predictive performance for less popular items. larity in RecoMF, which tends to improve recommendations for less popular items [27]. We conjecture that a different choice of prior for our Bayesian low-rank DPP model may improve our performance on this metric. It is also impor-tant to note the limitations of this metric, including the assumption that item popularity follows a power-law, and the power-law exponent  X  setting of 0.5 used when comput-ing the metric for each dataset. Due to these limitations, the popularity-weighted precision@ k results we present here may not fully reflect the empirical popularity bias actually present in the data.
Several algorithms for learning the DPP kernel matrix from observed data have been proposed. Ref. [12] presented one of the first methods for learning a non-parametric form of the full-rank kernel matrix, which involves an expectation-maximization (EM) algorithm. This work also considers using projected gradient ascent on the DPP log-likelihood function, but finds that this is not a viable approach since it usually results in degenerate estimates due to the projec-tion step. In [24], a fixed-point optimization algorithm for full-rank DPP learning is described, called Picard iteration. Picard iteration has the advantage of being simple to imple-ment and performing much faster than EM during training. Ref. [10] shows that a low-rank DPP model can be trained far more quickly than Picard iteration and therefore EM, while enabling much faster computation of predictions than is possible with any full-rank DPP model.

Ref. [1] presented Bayesian methods for learning a DPP kernel, with particular parametric forms for the similarity and quality components of the kernel. Markov chain Monte Carlo (MCMC) methods are used for sampling from the pos-terior distribution over kernel parameters. Furthermore, [1] uses a full-rank DPP kernel and thus shares the scalability issues common to any full-rank DPP model. In contrast to this work, and similar to [10, 12, 24], our approach uses a non-parametric form of the kernel and therefore does not assume any particular parametrization.

A method for partially learning the DPP kernel is studied in [17]. The similarity component of the DPP kernel is fixed, and a parametric from of the function for the quality compo-nent of the kernel is learned. This is a convex optimization problem, unlike the task of learning the full kernel, which is a more challenging non-convex optimization problem.
A number of approaches to the basket completion problem that we focus on in this paper have been proposed. Ref. [25] describes a user-neighborhood-based collaborative filtering method, which uses rating data in the form of binary pur-c hases to compute the similarity between users, and then generates a purchase prediction for a user and item by com-puting a weighted average of the binary ratings for that item. A technique that uses logistic regression to predict if a user will purchase an item based on binary purchase scores ob-tained from market basket data is described in [20]. Ad-ditionally, other collaborative filtering approaches could be applied to the basket completion problem, such as the one-class matrix factorization model in [27] and Poisson factor-ization [13], as we illustrate in this paper.
We have presented a Bayesian method for learning a low-rank factorization of the DPP kernel from observed data. Previous low-rank DPP approaches have focused on learn-ing a point estimate of kernel using an optimization method, which requires expensive tuning of regularization hyperpa-rameters to prevent overfitting and provide good predictive performance. We have shown that our Bayesian low-rank DPP model generally provides better predictive performance than an optimization-based low-rank DPP model without the need for hyperparameter tuning. Our experimental eval-uation using several real-world datasets in the domain of rec-ommendations for shopping baskets also shows that in many cases our model provides better predictive performance than competing methods, including two state-of-the-art models based on matrix factorization.
We thank Gal Lavee and Shay Ben Elazar for many helpful discussions. We thank Nir Nice for supporting this work. [1] R. H. Affandi, E. Fox, R. Adams, and B. Taskar. [2] R. Agrawal, T. Imieli  X nski, and A. Swami. Mining [3] T. Brijs. Retail market basket data set. In Workshop [4] T. Brijs, G. Swinnen, K. Vanhoof, and G. Wets. Using [5] A. J. Chaney. Social Poisson factorization (SPF). [6] A. J. Chaney, D. M. Blei, and T. Eliassi-Rad. A [7] T. Chen, E. Fox, and C. Guestrin. Stochastic gradient [8] F. Coenen. TLUCS KDD implementation of CBA [9] S. Duane, A. D. Kennedy, B. J. Pendleton, and [10] M. Gartrell, U. Paquet, and N. Koenigstein. Low-rank [11] J. Gillenwater. Approximate inference for [12] J. A. Gillenwater, A. Kulesza, E. Fox, and B. Taskar. [13] P. Gopalan, J. M. Hofman, and D. M. Blei. Scalable [14] Y. Hu, Y. Koren, and C. Volinsky. Collaborative [15] S. Kotsiantis and D. Kanellopoulos. Association rules [16] A. Kulesza and B. Taskar. Structured determinantal [17] A. Kulesza and B. Taskar. Learning determinantal [18] A. Kulesza and B. Taskar. Determinantal point [19] J. T. Kwok and R. P. Adams. Priors for diversity in [20] J.-S. Lee, C.-H. Jun, J. Lee, and S. Kim.
 [21] Y. Li, J. Hu, C. Zhai, and Y. Chen. Improving [22] B. Liu, W. Hsu, and Y. Ma. Integrating classification [23] O. Macchi. The coincidence approach to stochastic [24] Z. Mariet and S. Sra. Fixed-point algorithms for [25] A. Mild and T. Reutterer. An improved collaborative [26] R. M. Neal. Mcmc using hamiltonian dynamics. [27] U. Paquet and N. Koenigstein. One-class collaborative [28] H. Steck. Item popularity and recommendation
