 Not only is Wikipedia a comprehensive source of quality informa-tion, it has several kinds of internal structure (e.g., relational sum-maries known as infoboxes ), which enable self-supervised infor-mation extraction. While previous efforts at extraction from Wiki-pedia achieve high precision and recall on well-populated classes of articles, they fail in a larger number of cases, largely because incomplete articles and infrequent use of infoboxes lead to insuf-ficient training data. This paper presents three novel techniques for increasing recall from Wikipedia X  X  long tail of sparse classes: (1) shrinkage over an automatically-learned subsumption taxon-omy, (2) a retraining technique for improving the training data, and (3) supplementing results by extracting from the broader Web. Our experiments compare design variations and show that, used in con-cert, these techniques increase recall by a factor of 1.76 to 8.71 while maintaining or increasing precision.
 H.4 [ Information Systems Applications ]: Miscellaneous Management, Design Information Extraction, Wikipedia, Semantic Web
We are motivated by a vision of self-supervised information ex-traction  X  systems which can autonomously gather and organize semantic data from a large number of Web pages. Such a system could be useful for next-generation information retrieval, question answering and much more. Autonomy is crucial, since the scale of available knowledge is vast. We share this vision with a number of other projects, such as Snowball [1], KnowItAll [10] and Tex-trunner [3], but in contrast to systems which seek to extract from arbitrary Web text, we argue that Wikipedia is an important focus for extraction. If we can render much of Wikipedia into semantic form, then it will be much easier to expand from that base. Figure 1: The number of article instances per infobox class has a long-tailed distribution.

Focusing on Wikipedia largely solves the problem of inaccurate and unreliable source data [11], but introduces new challenges. For example, many previous systems (e.g., Mulder [12], AskMSR [5], and KnowItAll [10]) exploit the presence of redundant information on the Web, enabling powerful statistical techniques; however, the Wikipedia corpus has greatly reduced duplication. On the other hand, Wikipedia has several attributes that significantly facilitate extraction: 1) Infoboxes, tabular summaries of an object X  X  key at-tributes, may be used as a source of training data, allowing for self-supervised learning. 2) Wikipedia gives important concepts their own unique identifier  X  the URI of a definitional page. The first reference to such a concept often includes a link which can be used for disambiguation. As a result, homonyms are much less of a problem than in unstructured text. 3) Wikipedia lists and categories provide valuable features for classifying pages.

In previous work, we developed Kylin  X  a self-supervised sys-tem for information extraction from Wikipedia [26]. Kylin looks for sets of pages with similar infoboxes, determines common at-tributes for each class, creates training examples, learns extractors, and runs them on each page  X  creating new infoboxes and com-pleting others.
Kylin works extremely well for popular infobox classes where users have previously created sufficient infoboxes to train an effec-tive extractor model. For example, in the  X  X .S. County X  class Kylin has 97 . 3% precision with 95 . 9% recall. Unfortunately, however, many classes (e.g.,  X  X rish Newspapers X ) contain only a small num-ber of infobox-containing articles. As shown in Figure 1, 1442 of 1756 ( 82% ) classes have fewer than 100 instances, and 709 ( have 10 or fewer instances. For classes sitting on this long tail, Kylin can X  X  get enough training data  X  hence its extraction perfor-mance is often unsatisfactory for these classes.

Furthermore, even when Kylin does learn an effective extrac-tor there are numerous cases where Wikipedia has an article on a topic, but the article simply doesn X  X  have much information to be extracted. Indeed, another long-tailed distribution governs the length of articles in Wikipedia; among the 1.8 million pages, are short articles and almost 800,000 (44.2%) are marked as stub pages, indicating that much-needed information is missing.
In order to create a comprehensive semantic knowledge base summarizing the topics in Wikipedia, we must confront both of these long-tailed challenges. We must train extractors to operate on sparsely populated infobox classes and we must resort to other information sources if a Wikipedia article is superficial.
In this paper we describe three novel approaches for improving the recall of extraction of Wikipedia infobox attribute values.
Our techniques work best in concert. Together, they improve recall by a factor of 1.76 to 8.71 while maintaining or increasing precision. The area under the precision-recall curve increases by a factor of between 1.96 to 23.32, depending on class. In addition to showing the great cumulative effect of these techniques, we analyze several variations of each method, exposing important engineering tradeoffs.
We start by defining the problem under consideration: infobox completion. Recall that an infobox is a relational summary of an article: a set of attribute / value pairs describing the article X  X  sub-ject (see [26] for an example). Not every article has an infobox and some infoboxes are only partially instantiated with values. We seek to create or complete infoboxes whenever possible. Given a Wikipedia page, we seek to identify the infobox class, thus retriev-ing its associated schema, and extract as many attribute values as possible from the article (or possibly from the greater Web). In this paper, we concentrate on the extraction process  X  specifically on increasing recall for sparse classes.

Before describing our three new methods for increasing Kylin X  X  recall, we review the system X  X  basic architecture [26]. As shown in Figure 2, Kylin has three primary components: the preprocessor, a module which generates classifiers, and one which generates Con-ditional Random Fields (CRF) [13] extractors. The figure shows the data flow, but the components are invoked in a pipeline in the order described above. We describe them in turn.
The preprocessor selects and refines infobox schemata, choosing relevant attributes; it then generates machine-learning datasets for
Unless noted otherwise, all statistics are taken from the 07/16/2007 snapshot of Wikipedia X  X  English language version. Figure 2: Kylin performs self-supervised information extrac-tion, using Wikipedia inforboxes for training data. training sentence classifiers and extractors. Refinement is neces-sary for several reasons. For example, schema drift occurs when au-thors create an infobox by copying from a similar article and chang-ing attribute values. If a new attribute is needed, they just make up a name, leading to schema and attribute duplication. For example, six different attribute names are used to describe the location of an  X  X ctor X  X  X  death:  X  X eath location X ,  X  X eathlocation X ,  X  X eath_place X ,  X  X eathplace X ,  X  X lace_of_death X  and  X  X ocation of death X .

The initial Kylin implementation used a naive approach to refine-ment: scanning the corpus and selecting all articles with the same infobox template name. Only the attributes used in at least the articles were selected. As we discuss in the next section, one benefit of building a taxonomy over the set of infobox classes is the ability to recognize closely rel ated and duplicate classes.
The preprocessor constructs two types of training datasets  X  those for sentence classifiers, and CRF attribute extractors. For each article with an infobox mentioning one or more target at-tributes, Kylin tries to find a unique sentence in the article that mentions that attribute X  X  value. The resulting labelled sentences form positive training examples for each attribute; other sentences form negative training examples. If the attribute value is mentioned in several sentences, then one is selected heuristically.
Kylin learns two types of classifiers. For each class of article be-ing processed, a heuristic document classifier is used to recognize members of the infobox class. For each target attribute within a class a sentence classifier is trained in order to predict whether a given sentence is likely to contain the attribute X  X  value. Robust techniques exist for document classification (e.g., Naive Bayes, Maximum Entropy or SVM approaches), but Kylin X  X  simple heuristic technique, which exploits Wikipedia X  X  list and category features, worked well.

Sentence classification, i.e. predicting which attribute values (if any) are contained in a given sentence, can be seen as a multi-class, multi-label text classification problem. Kylin uses a Maximum En-tropy model [18] with a variety of features: bag of words, aug-mented with part of speech (POS) tags. To decrease the impact of the noisy and incomplete training dataset, Kylin applies bagging (instead of boosting [19]).
Extracting attribute values from a sentence is best viewed as a sequential data-labelling problem. Kylin uses the CRF model with a wide variety of features (e.g., POS tags, position in the sentence, capitalization, presence of digits or special characters, relation to anchor text, etc.). Instead of training a single master extractor to clip all attributes, Kylin trains a different CRF extractor for each attribute, ensuring simplicity and fast retraining. As mentioned previously, when trained on infobox classes with copious instances (e.g., 500 or more), Kylin learns excellent extractors. The preci-sion ranged from a percentage in the mid-70s to high-90s and recall from low-50s to mid-90s, depending on attribute type and infobox class. Though Kylin is successful on those popular classes, its per-formance decreases on the long-tail of sparse classes where there is insufficient training data. The next two sections describe new tech-niques for solving this problem. In Section 5 we explain how we extend Kylin to handle the long tail of short articles.
Although Kylin performs well when it can find enough training data, it flounders on sparsely populated infobox classes  X  the ma-jority of cases. Our first attempt to improve Kylin X  X  performance uses shrinkage, a general statistical technique for improving esti-mators in the case of limited training data [24]. McCallum et al. applied this technique for text classification in a hierarchy classes by smoothing parameter estimate of a data-sparse child with its par-ent to get more robust estimates [16].

Similarly, we use shrinkage when training an extractor of an instance-sparse infobox class by aggregating data from its parent and children classes. For example, knowing that Performer IS-APerson ,and Performer.loc=Person.birth_plc , we can use values from Person.birth_plc to help train an extractor for Perf orm er. l oc . The trick is automatically generating a good subsumption hierarchy which relates attributes between parent and child classes. Thus, we first describe our method for creating an ontology relating Wiki-pedia infoboxes, then describe our approach to shrinkage, and end the section with an empirical exploration of our technique.
The Kylin Ontology Generator (KOG) is an autonomous system that builds a rich ontology by combining Wikipedia infoboxes with WordNet using statistical-relational machine learning [27]. At the highest level KOG computes six different kinds of features, some metric and some Boolean: similarity measures , edit history pat-terns , class-name string inclusion , category tags , Hearst patterns search-engine statistics, and Wo rd N e t mappings. These features are combined using statistical-relational machine learning, specifically joint inference over Markov logic networks [21], extending [23].
Figure 3 shows KOG X  X  architecture. First, its schema cleaner scans the infobox system to merge duplicate classes and attributes, and infer the type signature of each attribute. Then, the subsump-tion detector identifies the subsumption relations between infobox classes, and maps the classes to WordNet nodes. Finally, the schema mapper builds attribute mappings between related classes, espe-cially between parent-child pairs in the subsumption hierarchy. KOG X  X  taxonomy provides an ideal base for the shrinkage technique, as de-scribed below.
Given a sparse target infobox class C , Kylin X  X  shrinkage mod-ule searchs upwards and downwards through the KOG ontology to aggregate training data from related classes. The two crucial ques-tions are: 1) How far should one traverse the tree? 2) What should be the relative weight of examples in the related class compared to those in C ? For the first question, we search to a uniform distance, l , outward from C . In answer to the second question, we evaluate several alternative weighting schemes in Section 3.3. The overall shrinkage procedure is as follows: 1. Given a class C , query KOG to collect the related class set: 2. For each attribute C.a (e.g., Perf orm er. l oc )of C : 3. Train the CRF extractors for C on the new training set.
This section addresses two questions: 1) Does shrinkage over the KOG ontology help Kylin to learn extractors for sparse classes? What if the target class is not sparse? 2) What is the best strategy for computing the training weights, w ij ? To answer these questions we used the 07/16/2007 snapshot of en.wikipedia.org as a source dataset. We tested on four classes 2 , namely  X  X rish news-paper X  (which had 20 infobox-contained instance articles),  X  X er-former X  (44),  X  X aseball stadium X  (163), and  X  X riter X  (2213). These classes represent various degrees of  X  X parsity X  in order to provide better understanding of how shrinkage helps in different cases. For the  X  X rish newspaper X  and  X  X erformer X  classes, we manually la-beled all the instances to compute precision and recall values. Par-ticularly, we count the ground-truth as the attribute values con-tained in the articles  X  meaning a 100 percent recall is getting ev-ery attribute value which is present in the article. For the  X  X aseball stadium X  and  X  X riter X  classes, we manually labeled 40 randomly-selected instances from each. All the following experiments use 4-fold cross validation.

After schema cleaning, KOG identified 1269 infobox classes and mapped them to the WordNet l attice (82115 synsets). We found that although the whole ontology is quite dense, the current number of Wikipedia infoboxes is relatively small and most pathes through the taxonomy cover three or fewer infobox classes, which dimin-ishes the effect of path-length threshold l . Table 1 shows the de-tailed parent/children classes for each testing case. In the follow-ing, we mainly focus on testing weighting strategies.
In average there are around 7 attributes per class, so we actually tested for around 4  X  7=28 extractors. the two sparsest classes, precision is also markedly improved. Irish newspaper(20) Newspaper(1559)  X  Baseball stadium(163) Stadium(1642)  X 
We considered three strategies to determine the weights w aggregated data from parent / children classes: Uniform: w ij =1 , which weights all training samples equally. Size Adjusted: w ij = min { 1 , k | C | +1 } ,where k (10 in our exper-iments) is the design parameter, and | C | is the number of instance articles contained in C . The intuition is that the bigger C is, the less shrinkage should rely on other classes.
 Precision Directed: w ij = p ij ,where p ij is the extraction pre-cision when applying the extractor for C i .a j on the appropriate sentences from C -class articles and comparing them with existing infobox values.

Even with limited parent / children classes for smoothing, all forms of shrinkage improve extraction performance. Figure 4 shows the precision / recall curves for our different weighting strategies; parenthetical numbers (e.g.,  X  X erformer (44) X  denote the number of positive examples. We draw several conclusions:
First, with shrinkage, Kylin learns better extractors, especially in terms of recall. For those very sparse classes such as  X  X erformer X  and  X  X rish newspapers X , the recall improvement is dramatic: 55% and 457% respectively; and the area under the precision and recall curve (AUC) improves 57% and 1386% respectively.

Second, we expected precision-directed shrinkage to outperform the other methods of weighting, since it automatically adapt to dif-ferent degrees of similarity between the targent and related classes. However, the three weighting strategies turn out to perform com-paratively on the infobox classes used for testing. The most likely reason is that to achieve total autonomy Kylin estimates the preci-sion, p ij , of an extractor by comparing the values which it extracts to those entered manually in existing infoboxes. It turns out that in many cases Wikipedia editors use different expressions to describe attribute values in the infoboxes than they do in the article text. Naturally, this makes the accurate estimation of p ij extremely dif-ficult. This, in turn, biases the quality of weighting. In the future, we hope to investigate more sophisticated weighting methods.
Finally, Shrinkage also helps the quality of extraction in popular classes (e.g., for  X  X riter X ), though the improvement is quite mod-est. This is encouraging, since  X  X riter X  (Figure 1d) already had over two thousand training examples.
Our experiments show that shrinkage enables Kylin to find extra data within Wikipedia to help train extractors for sparse classes. A complementary idea is the notion of harvesting additional training data even from the outside Web? Leveraging information outside Wikipedia, could dramaticaly improve Kylin X  X  recall. To see why, we note that the wording of texts from the greater Web are more diverse than the relatively strict expressions used in many places in Wikipeidia. 3 Training on a wider variety of sentences would im-prove the robustness of Kylin X  X  extractors, which would potentially improve the recall.

The trick here is determining how to automatically identify rel-evant sentences given the sea of Web data. For this purpose, Kylin
It is possible that Wikipedia X  X  inbred style stems from a pattern where one article is copied and modified to form another. A general desire for stylistic consistency is another explanation. utilizes TextRunner, an open infor mation extraction system [3], which extracts relations { r | r = obj 1 , predicate, obj of about 100 m illion Web pages. Im portantly for our purposes, Tex-trunner X  crawl includes the top ten pages returned by Google when queried on the title of every Wikipedia article. In the next sub-section, we explain the details of our retraining process; then we follow with an experimental evaluation.
Recall that each Wikipedia inf obox implicitly defines a set of semantic triples { t | t = subject, attribute, value } where the sub-ject corresponds to the entity wh ich is the article X  X  title. These triples have the same underlying schema as the semantic relations extracted by TextRunner and this allows us to generate new training data.

The retrainer iterates through each infobox class C and again through each attribute, C.a , of that class collecting a set of triples from existing Wikipedia infoboxes: T = { t | t.attribute = The retrainer next iterates through T , issuing TextRunner queries to get a set of potential matches R ( C.a )= { r | X  t  X  T t.subject, r.obj 2 = t.value } , together with the corresponding sentences which were used by TextRunner for extraction. The re-trainer uses this mapped set R ( C.a ) to augment and clean the train-ing data for C  X  X  extractors in two ways: by providing additional positive examples, and by eliminating false negative examples which were mistakenly generated by Kylin from the Wikipedia data. ADDING POSITIVE EXAMPLES: Unfortunately, TextRunner X  X  raw mappings, R ( C.a ) , are too noisy to be used as positive train-ing examples. There are two causes for the noise. The most obvi-ous cause is the imperfect precision of TextRunner X  X  extractor. But false positive examples can also be generated when there are mul-tiple interpretations for a query. Consider the TextRunner query r.obj 1 = A, r.predicate =? ,r.obj 2 = B ,where A is a person and B is his birthplace. Since many people die in the same place that they were born, TextRunner might return the sentence  X  X ob died in Seattle. X   X  a poor trai ning example f or birthplace.
Since false positives can greatly impair training, the Kylin re-trainer morphologically clusters the predicates which are returned by TextRunner (e.g.,  X  X s married to X  and  X  X as married to X  are grouped). We discard any predicate that is returned in response to a query about more than one infobox attribute. Only the k most common remaining predicates are then used for positive training examples; in our experiments we set k =1 to ensure high precision.
 FILTERING NEGATIVE EXAMPLES: As explained in [26], Kylin considers a sentence to be a negative example unless it is known to be positive or the sentence classifier labels it as poten-tially positive. This approach eliminates many false negatives, but some remain. A natural idea is to remove a sentence from the set of negative examples if it contains the word denoting the relation itself. Unfortunately, this technique is ineffective if based soley on Wikipedia content. To see why, consider the Person.spouse at-tribute which denotes the  X  X arriage X  relation  X  X ecause the word  X  X pouse X  seldom appears in natural sentences, few false negatives are excluded. But by using TextRunner, we can better identify the phrases (predicates) which are harbingers of the relation in ques-tion. The most common are used to eliminate negative examples.
We note that another way of generating the set, T , would be to collect baseline Kylin extractions for C.a instead of using existing infoboxes. This would lead to a cotraining approach rather than simple retraining. One could iterate the process of getting more training date from TextRunner with improvements to the Kylin ex-tractor [4].

By adding new positive examples and excluding sentences which might be false negatives, retraining generates a greatly improved training set, as we show in the next subsection.
We ask two main questions: 1) Does retraining improve Kylin X  X  extractors? 2) Do the benfits from retraining combine synergisti-cally with those from shrinkage? Before addressing those ques-tions we experimented with different retraining alternatives (e.g., just adding positive examples and just filtering negatives). While both approaches improved extractor performance, the combination worked best, so the combined method was used in the subsequent study.

We evaluate retraining in two different cases. In the first case, we use nothing but the target class X  infobox data to prime TextRunner for training data. In the second case, we first used uniform-weight shrinkage to create a training set which was then used to query TextRunner. Figure 5 shows the results of these methods on four testing classes.

We note that in most cases retraining improves the performance, in both precision and recall. When compared with shrinkage, re-training provides less benefit for sparse classes but helps more on the popular class  X  X riter. X  This makes sense because without many tuples to use for querying TextRunne r, retraining has little effect. For example, for  X  X erformer (44) X  retraining added 10 positive ex-amples and filtered 20 negative examples; for  X  X riter (2213) X  re-training added 2204 positive and filtered 3568 negative examples. We suspect that full cotraining would be more effective on sparse classes when shrinkage was unavailable. Finally, we observe syn-ergy between shrinkage and retraining, leading to the biggest im-provement. Particularly, on the two sparsest classes  X  X rish newspa-per X  and  X  X erformer X , the combination improved recall by 585% and 72% respectively, with remarkable improvement in precision as well; and the AUC improved 1680% and 74% respectively.
While shrinkage and retraining improve the quality of Kylin X  X  extractors, the lack of redundancy of Wikipedia X  X  content makes it increasingly difficult to extract additional information. Facts that are stated using uncommon or ambiguous sentence structures hide from the extractors. In order to retrieve facts which can X  X  be ex-tracted from Wikipedia, we would like to exploit another corpus, in particular the general Web. On the surface, the idea is simple: train extractors on Wikipedia articles and then apply them to rele-vant Web pages. An obvious benefit of this approach is the ability to find new facts which are not contained in Wikipedia at all.
The challenge for this approach  X  as one might expect  X  is maintaining high precision. Since the extractors have been trained on a very selective corpus, they are unlikely to discriminate irrele-vant information. For example, a Kylin extractor for Person.birthdate has been trained on a set of pages all of which have as their primary subject that person X  X  life. Such extractors become inaccurate when applied to a page which compares the lives of several people  X  even if the person in question is one of those mentioned.
To ensure extraction quality, it is thus crucial to carefully se-lect and weight content that is to be processed by Kylin X  X  extrac-tors. In our work, we view this as an information retrieval problem, which Kylin X  X  web extraction module solves in the following steps: It generates a set of queries and utilizes a general Web search en-gine, namely Google, to identify a set of pages which are likely to contain the desired information. The top-k pages are then down-loaded, and the text on each page is split into sentences, which are processed by Kylin. Each extraction is then weighted using a com-bination of factors. CHOOSING SEARCH ENGINE QUERIES: The first important step is to ensure that the search engine returns a set of highly rele-vant pages. A simple approach is to use the article title as a query. For example, let us assume that we are interested in finding the birth date of Andrew Murray, a writer. The corresponding Wiki-pedia page is titled  X  X ndrew Murray (minister) X . The information in parentheses is used in Wikipedia to resolve ambiguities, but we remove it to increase recall. To improve result relevance, we place quotes around the remaing string, here  X  X ndrew murray X  .

Although such a query might retrieve many pages about Mur-ray, it is possible that none among the top contains the person X  X  birth date which we might be interested in. We therefore run sev-eral more restrictive queries which not only limit results to pages containing the article title, but that also include other keywords to better target the search.

One such query is the quoted artic le title followed by the attribute name, as in  X  X ndrew murray X  birth date . While this in-creases the chance that a returned page contains the desired infor-mation, it also greatly reduces recall, because the terms  X  X irth date X  might not actually appear on a relevant page. For example, consider the sentence  X  X ndrew Murray was born in 1828. X .

Such predicates which are indicative of attributes, like  X  X as born in X  for the birth date , we have computed already, as described in section 4. We generate an appropriate query for each predicate, which combines the quoted title as well as the predicate, as in  X  X ndrew murray X  was born in . The combined results of all queries (title only, title and attribute name, as well as title and any attribute predicate) are retrieved for further processing. WEIGHTING EXTRACTIONS: Pages which do not contain the preprocessed article title, here  X  X ndrew Murray X , are discarded. Then, using an HTML parser, formatting commands and scripts are removed, and sentences are identified in the remaining text.
Since most sentences are still irrelevant, running Kylin X  X  extrac-tors on these directly would result in many false positives. Recall that unlike Wikipedia X  X  articles, web pages often compare multiple related concepts, and so we would like to capture the likeliness that a sentence or extraction is relevant to the concept in question. A variety of features may be indicative of content relevance, but we focused on two in particular:
Each retrieved sentence is then sent to Kylin for extraction, and for each extraction a combined score is computed. This score takes into account both factors  X  s and  X  r as well as the confidence  X  reported by Kylin X  X  extractors; it is obtained in the following way: First, each of the three parameters  X  s , X  r , X  c is normalized by apply-ing a linear mapping into the intervals [  X  s , 1] , [  X  r , respectively, where 1 corresponds to the optimal value and  X  and  X  c are user-defined parameters. With  X   X  s ,  X   X  r ing the normalized weights, the combined score is then obtained as COMBINING WIKIPEDIA AND WEB EXTRACTIONS: Our final question is: how can we combine extraction results from Wi-kipedia and the Web? Despite our efforts in identifying relevant Web pages and weighting sentences, it is likely that extractions from Wikipedia will be more precise. After all, in Wikipedia we can be sure that a given page is highly relevant, is of high quality, and has a more consistent structure, for which Kylin X  X  extractors have been particularly trained. Yet, Kylin may err on Wikipedia too, especially when the extractors confidence score is low. Figure 7: When applying Kylin to Web pages, improvements due to shrinkage and retraining become even more apparent.
A straight-forward combination of the extractors always returns the extraction with highest score, as measured in terms of confi-dence for extractions from Wikipedia and the weighted combina-tion score web for extractions from the Web. In order to balance the weights of extractors, we adjust the score of extractions from the web to 1  X  (1  X  score web )  X  ,where  X  is a new parameter.
In this section we would like to answer two questions: 1) Which factors are important in scoring extractions from the Web? and 2) When combining extractions from Wikipedia and the Web, can recall be significantly improved at an acceptable precision?
In previous sections, we computed recall as the proportion of facts contained in the infoboxes that our system was able to auto-matically extract from the text. In this section, however, we are also interested in how many new facts Kylin can extract from the Web, and so we change our definition of recall: we assume that there exists some correct value for each attribute contained in the infobox template of an article and set recall to be the proportion of correct attribute values relative to all attributes. Note that this is a very conservative estimate, since there may not always exist an ap-propriate value. For example, there exists no death date for a writer who has not died yet.

For all experiments, we queried Google for the top-100 pages containing the article title, and the top-10 pages containing the ar-ticle title plus attribute name (or associated predicate). Each new extraction  X  for which no ground truth existed in Wikipedia  X  was manually verified for correctness by visiting the source page.
In our first series of experiments, we used Shrink-Retrain  X  the best extractors trained on Wikipedia  X  and applied different scoring functions to select the best extraction for an attribute. Fig-ure 6 shows our results: The CRF extractor X  X  reported confidence performed poorly in isolation. Giving priority to extractions from pages at a higher positio n in Google X  X  returned result lists and re-solving ties by confidence, yielded a substantial improvement. Sim-ilarly, we tried giving priority to extractions which were fewer sen-tences apart from the occurrence of the Wikipedia article title on a page, again resolving ties by extractor confidence. The large im-provements in precision and recall (as highlighted in the figure 6) show that much of the returned text is irrelevant, but can be re-weighted using simple heuristics. Finally, we were interested if a weighted combination of these factors would lead to synergies. We set  X  s = . 1 ,  X  r = . 7 ,  X  c = . 9 , so that each factor was roughly weighted by our observed improvement (results were not sensitive to minor variations). On all datasets, performance was comparable or better than the best factor taken in isolation. extractions from Wikipedia and the Web, shrink-retrain-Web, performs even better.
In our second series of experiments, we combined extractions from Wikipedia and the Web. In both cases, we applied the Shrink-Retrain extractor, but scored extractions from the Web using the weighted factor combination with  X  = . 4 . The results, shown in Figure 8, show large improvements in recall at higher precision for the  X  X aseball stadium X  (34%) and  X  X riter X  (63%) datasets, and at moderately improved precision for the  X  X rish newspaper X  and  X  X erformer X  datasets. The AUC was substantially expanded in all cases, ranging from 14% to 75%. Compared to the original base-line system, the area has expanded between 96% and 2232%. Ta-ble 2 shows the detailed accumulative improvements of AUC for various scenarios. Another interesting observation is that Shrink-age tends to address more the first long-tailed challenge  X  sparse classes(e.g.,  X  X rish newspaper(20) X ), and resorting to the Web tends to address more the second long-tailed challenge  X  short articles(e.g., many  X  X riter X  articles are short ones about noteless writers).
In the future, we would like to automatically optimize the param-eters  X  s ,  X  r ,  X  c ,  X  based on comparing the extractions with values in existing infoboxes.
In the preceding sections we have discussed how our work re-lates to past work on shrinkage and cotraining. In this section, we discuss the broader context of previous work on unsupervised in-formation extraction, approaches for exploiting ontologies in infor-mation extraction, and other Wikipedia-based systems.
 UNSUPERVISED INFORMATION EXTRACTION: Since the Web is large and highly heterogeneous, unsupervised and self-super-vised learning is necessary for scaling. Several systems of this form have been proposed. S NOWBALL [1] iteratively generates extrac-tion patterns based on occurrences of known tuples in documents to extract new tuples from plain texts. MULDER [12] and AskMSR [5, 9] use the Web to answer questions, exploiting the fact that most important facts are stated multiple times in different ways, which licenses the use of simple syntactic processing. Instead of utilizing redundancy, Kylin exploits Wikipedia X  X  unique structure and the presence of user-tagged data to train machine learners. Patwardhan and Riloff proposed a decoupled information extraction system by first creating a self-trained relevant sentence classifier to identify relevant regions, and using a semantic affinity measure to automat-ically learn domain-relevant extraction patterns [20]. Kylin uses the similar idea of decoupling when applying extractors to the general Web. Differently, Kylin uses IR-based techniques to select relevant sentences and trains a CRF model for extractions.
 ONTOLOGY-DRIVEN INFORMATION EXTRACTION: There have been a lot of work on leveraging ontology for information ex-traction. The SemTag and Seeker [8] systems perform automated semantic tagging of large corpora. They use the TAP knowledge base [22] as the standard ontology, and match it with instances on the Web. PANKOW [6] queries Google with ontology-based Hearst patterns to annotate named entities in documents. Matuszek et al. uses Cyc to specify Web searches to identify and verify com-mon senses candidates [15]. The similar idea is utilized in On-toSyphon [17] where ontology combined with search engines are used to identify semantic instances and relations. In contrast, Kylin automatically constructs the Wikipedia infobox ontology and uses it to help training CRF extractors by shrinkage.
 OTHER WIKIPEDIA-BASED SYSTEMS: Dakka and Cucerzan trained a classifier to label Wikipedia pages with standard named entity tags [7]. Auer and Lehmann developed the DBpedia [2] sys-tem which extracts information from existing infoboxes within arti-cles and encapsulate them in a semantic form for query. In contrast, Kylin populates infoboxes with new attribute values. Suchanek et al. implement the Y AGO system [25] which extends WordNet us-ing facts extracted from Wikipedia X  X  category tags. But in contrast to Kylin, which can learn to extract values for any attribute, Y only extracts values for a limited number of predefined relations.
Kylin has demonstrated the ability to perform self-supervised in-formation extraction from Wikipedia [26]. While Kylin achieved high precision and reasonable recall when infobox classes had a large number of instances, most classes sit on the long tail of few instances. For example, 82% classes can provide fewer than 100 training examples, and for these classes Kylin X  X  performance is un-acceptable. Furthermore, even when Kylin does learn an effective extractor there are many cases where Wikipedia X  X  article on a topic is too short to hold much-needed information.

This paper describes three powerful methods for increasing re-call w.r.t. the above to long-tailed challenges: shrinkage, retrain-ing, and supplementing Wikipedia extractions with those from the Web. Our experiments show that each of these methods is effective individually. Particularly, shrinkage addresses more the first long-tailed challenge of sparse classes, and the latter two address more the second long-tailed challenge of short articles. We evaluate de-sign tradeoffs within each method. Most importantly, we show that in concert, these methods constitute a huge improvement to Kylin X  X  performance (Figure 8):
Despite this success, much remains to be done. We hope to de-vise a better weighting scheme for shrinkage by comparing the KL divergence between the target and mapped classes. We wish to extend our retraining technique to full cotraining. There are sev-eral ways to better integrate extraction of Web content with that of Wikipedia, ranging from improved Google querying policies to DIRT-style analysis of extraction patterns [14]. We thank Eytan Adar, Michelle Banko, Ivan Beschastnikh, Doug Downey, Oren Etzioni, Travis Kriplean, Cynthia Matuszek, David McDonald, Alan Ritter, Stefan Schoenmackers, Jue Wang, the UW KnowItAll and Wikipedia groups, and the anonymous reviewers for valuable conversations and suggestions. This work was supported by NSF grant IIS-0307906, ONR grant N00014-06-1-0147, SRI CALO grant 03-000225 and the WRF / TJ Cable Professorship.
