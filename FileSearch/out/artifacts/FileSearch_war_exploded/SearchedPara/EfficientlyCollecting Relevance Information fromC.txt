 Various click models have been recently proposed as a prin-cipled approach to infer the relevance of documents from the clickthrough data. The inferred document relevance is potentially useful in evaluating the Web retrieval system-s. In practice, it generally requires to acquire the accurate evaluation results within minimal users X  query submission-s. This problem is important for speeding up search engine development and evaluation cycle and acquiring reliable e-valuation results on tail queries. In this paper, we propose a reordering framework for efficient evaluation problem in the context of clickthrough based Web retrieval evaluation. The main idea is to move up the documents that contribute more for the evaluation task. In this framework, we propose four intuitions and formulate them as an optimization problem. Both user study and TREC data based experiments validate that the reordering framework results in much fewer query submissions to get accurate evaluation results with only a little harm to the users X  utility.
 H.3.3 [ Information Systems ]: Information Search and Re-trieval Algorithms, Experimentation Clickthrough Data, Evaluation, Click Model
Evaluation of Web retrieval systems is critical for improv-ing search techniques. So far, the dominant method for eval-uation has been the Cranfield evaluation method. However, it may be not so fit for evaluating the Web retrieval sys-tems due to two reasons: 1) judgments become extremely expensive for large and dynamic Web content and variant information needs and 2) it is very difficult for the assessors to understand the needs behind the queries.

It is a promising direction to automatically evaluate We-b retrieval systems by using clickthrough data from users. Click models [9, 5, 8] have been validated to be effective to estimate the documents X  relevance from the clickthrough data without human judges X  effort. One potential applica-tion of click models is to evaluate the query-specific retrieval performance with the estimated documents X  relevance infor-mation. The Web retrieval systems X  overall performance can be aggregated by the query-specific performance.
 A quick solution [7] that utilizes click models to evaluate IR system on a query works as follows: First, it presents the unchanged ranking for all submissions of a query, and then it can collect the clicks from the users. With these clickthrough data, a click model can be utilized to infer the documents X  relevance. The inferred relevance can be consid-ered as an extended graded relevance score, and a graded relevance based IR metrics (such as DCG[12] and RBP[16]) can be used to measure the retrieval system X  X  performance.
However, there are two drawbacks of this quick solution: 1) It is difficult to get reliable evaluation result on tail queries. Besides the popular queries, the performance on tail queries are extremely important for the search engines due to long tail phenomenon [2]. Since a tail query is submitted infre-quently, the retrieved documents, especially those ranked lowly, are seldom examined, leading to unreliable relevance estimation about them. Thus the evaluation results based on the unreliable inferred relevance may be questionable. 2) Even for the frequently submitted queries, it may take a long period to collect enough relevance information for the reli-able evaluation result, and this slows down the development and evaluation cycle. Therefore, it needs a more efficient way to collect relevance information for evaluation.
In this paper, we propose to study the efficient evaluation problem in the context of evaluating Web retrieval systems with clickthrough data. It is to evaluate systems accurate-ly with minimal number of query submissions. For efficient evaluation, it aims at collecting more valuable relevance in-formation to reduce the number of submissions required for accurate evaluation. Generally, retrieved documents con-tribute differently to Web retrieval evaluation in two aspects. First, the contribution of a document depends on its rank in the result list. For example, a highly ranked document plays a more important role in determining the performance of a Web retrieval system than those documents with lower ranks; Second, with increasing collected clickthrough data, the benefit of collecting more information about a document would change. For example, if the relevance of a documen-t is reliably acquired, it is not so interesting to continue collecting more relevance information about this document. Therefore, we can selectively collect the relevance informa-tion from the documents with most contribution for evalua-tion.

In this paper, we propose a reordering documents frame-work to efficiently collect relevance information for Web re-trieval evaluation. A key component in this framework is the reordering function , which determines how to present the retrieved documents. Generally, the design of reorder-ing functions follows the intuitions of moving up the most contributing documents. However, the decisions based on various intuitions may be inconsistent. A big challenge in designing a reordering function is how these intuitions can be integrated together. To meet this challenge, we formu-late these intuitions into an optimization problem. This op-timization problem is solved approximately with a greedy algorithm.

The selection of reordering functions depends on the spe-cific evaluation task. There are two important tasks in Web retrieval evaluation: First, given a Web retrieval system, an interesting problem is to measure its performance by some metrics (one-system evaluation task), and the score can be compared with other or future systems (on the same query set); Second, it is also interesting to compare the relative performance of two Web retrieval systems directly (two-system comparison task). In this paper, we propose reorder-ing functions for these two tasks respectively.

To validate our framework, we conduct a user study and the TREC data based experiments. The user study veri-fies that our proposed reordering framework can collect rel-evance information more efficiently for evaluation with little harm to the users X  utility. To further analyze the perfor-mance of the reordering functions on large dataset and more informative scenarios, we conduct a study with TREC data (the submitted runs and the corresponding relevance judg-ments). The basic experiment shows that the reordering functions generally evaluate much more efficiently than the baselines with the similar user utility. Additional experi-ments show that the reordering functions are also robust for the performance of the evaluated system, the measure selection and the user click model selection.

The contributions of this papers are: 1. We propose a document reordering framework to solve 2. We propose four intuitions that can help collecting rel-3. We integrate these intuitions in an optimization formu-4. We run both real user study and TREC data based
As a central research topic in Web retrieval, evaluation has been studied extensively. Web retrieval evaluation gen-erally follows the Cranfield paradigm, using a gold standard test collection to evaluate retrieval systems with common re-trieval performance measures such as MAP and nDCG [12]. In the context of Cranfield paradigm evaluation, some algo-rithms have been proposed for reducing the judges X  effort, while keeping the evaluation results reliable. Yilmaz et al. [20] proposed to sample some documents for judgment and Carterette et al. [3] proposed to select documents that can distinguish systems best for judgment. We use the similar idea that focus on collecting relevance information of the most contributed documents. However, in the context of clickthrough based evaluation, we cannot ask the assessors to directly judge a document, but can just present a ranked document list to collect relevance information.

Many studies have attempted to leverage implicit feed-back information such as clickthroughs to evaluate Web re-trieval systems. There are two main categories of methods based on  X  X bsolute metric X  and  X  X elative comparison test X , respectively. In the first category, it predicts the absolute performance of a single Web retrieval system. It is either by inferring absolute relevance of retrieved documents from clickthrough data and then calculating the score for some traditional measures [4, 7], or by some new measures defined on the clickthrough pattern directly [19]. The methods of the second category [13, 18] attempt to compare two sys-tems by leveraging clickthroughs. In this framework, [21] proposed a method to learn powerful test statistics for more efficiently comparing two retrieval systems. Our proposed method is different from [21] that we focus on query-specific evaluation and we can collect relevance information for ef-ficiently estimating absolute metric score and relative com-parison adaptively.
A fundamental problem of using clickthrough data is that the clicking can not be mapped to relevance directly; for example, it was found that a user X  X  clickthroughs have bias [14]. Some works [14, 1, 6] extract preference relationship between document pairs from query submissions and their corresponding clickthroughs. Radlinski and Joachims [17] proposed a method to actively collect preference relationship between documents by reordering the presented list. In this paper, we also propose a reordering method, but for retrieval evaluation purpose.

Recently, some click models have been proposed to pro-vide a principled approach to infer the document relevance from clickthrough data [9, 5, 8]. These methods model the documents X  relevance, user behaviors and their relations in a probability directed graph model. They generally follow some hypotheses such as examination hypothesis (a docu-ment is clicked only if its snippet has been examined) and cascade hypothesis (the users examine the snippets in the up-down manner). Based on the clicking information of some sessions about one query, it can infer the distribu-tion of documents X  relevance to the corresponding query. Dupret et al. [7] also used click model to infer document relevance for retrieval system evaluation. However, to the best of our knowledge, there has been no work aiming at efficiently collecting document relevance for Web retrieval evaluation purpose based on these click models.
The users X  click behaviors provide relevance information for the examined documents, and such information can po-tentially help us to evaluate the retrieval systems. General-ly, the clickthrough is very valuable but sparse, so it needs a method to make good use of these clickthrough data to get a reliable evaluation result efficiently, especially for the tail queries. There are generally two styles of Web retrieval evaluation: one-system evaluation and two-system compari-son. In both tasks, we expect a method to efficiently collect relevance information to get accuracy results.

For one-system evaluation, it measures the performance of each systems by a score, and then these systems can be compared by their scores. This methodology is commonly used, especially in the test collection based evaluation (like TREC). In this methodology, a key problem is how to pre-dict the absolute performance score of a retrieval system. Given a ranked list and the manual relevance judgments, it usually employs a metric such as MAP and DCG to calculate the score. In the context of evaluation based on clickthrough data, we need to efficiently collect relevance information for reliably predicting a retrieval system X  X  performance. We de-fine efficient one-system evaluation problem as follows. Problem 1. (Efficient One-system Evaluation Problem): Given a ranked list A and an IR metric S , the efficient one-system evaluation problem is to collect relevance information for predicting A  X  X  S score accurately within minimal number of query submissions.

For two-system comparison, it compares two Web retrieval systems directly. For example, a new invented ranking al-gorithm is supposed to be compared with the existing algo-rithms to check whether it can outperform and replace them. They can be compared by the absolute performance scores, but for direct comparison purpose, it usually can be done more efficiently. We define two-system comparison problem as follows.
 Problem 2. (Efficient Two-system Comparison Problem): Given two ranked list A , B and an IR metric S , the efficient two-system comparison problem is to collect relevance infor-mation for telling the S based comparison result between A and B accurately within minimal number of query submis-sions.
For efficient evaluation, it is supposed to collect more valu-able relevance information at each query submission to re-duce the number of submissions needed for accurate evalu-ation. Generally, retrieved documents contribute differently for evaluating Web retrieval systems, so we can select to col-lect the relevance information about those most contributed (valuable) documents. Intuitively, we can move them up so that they are more likely to be examined. We study the problem of how to reorder the retrieved documents for more efficiently collecting relevance information for Web retrieval evaluation.
The reordering function attempts to interact with the re-trieval systems and the users for more efficiently collecting relevance information for evaluation. It works as follows: 1. For a query q ,a search engine returns the ranked doc-2. For a query submission by a user ,a reordering func-3.Theuserexaminesthe query impression and clicks 4. It updates the each document X  X  inferred relevance to 5. repeat 2-4 for each submission of the query q
This procedure can be extended to reorder the documents retrieved by two search engines A and B , and thus these two systems can be compared by the collected relevance infor-mation. The terminology is summarized in Table 1 briefly.
The main idea of reordering framework is to move up the documents that are more valuable for evaluation, so that they have larger chance to be examined. In this section, we would detail four intuitions that affect the benefit of a document being examined: relevance uncertainty reduction, document X  X  absolute rank, document X  X  rank difference in t-wo lists, and relevance level for more examination. (In this paper, the X  X enefit X  X f a document being examined or of pre-senting a list are both discussed in terms of the contribution to evaluation) 1. Relevance Uncertainty Reduction. The benefit of a document being examined is affected by its relevance un-certainty reduction. The uncertainty of document relevance is generally reduced when it is examined, but the reduc-tions for documents are different. For example, if we have been very confident about the relevance of a document, it may provide little relevance information by further examin-ing this document. Therefore, it is a good choice to move up the document with large relevance uncertainty reduction. 2. Document Rank. A document X  X  rank affects the benefit of the document being examined. The highly ranked documents usually contribute more to the overall perfor-mance score of a retrieval system. Most IR metrics model this rank weight in their formulas. For example, in (the most commonly used version of) DCG , the document at rank k can be weighted as 1 log 2 ( k +1) . Thus it generally requires to infer relevance of highly ranked document more accurately. With this intuition, it is supposed to keep the original rank-ing, i.e., keep the document ranked highly in the original list still ranked highly. 3. Document Rank Difference. In the context of comparing two systems, retrieved documents have different effect on distinguishing two systems. For example, if a doc-ument is ranked at rank 1 by system A and is ranked 100 by another system B , its relevance information is very helpful to distinguish these two systems. On the contrary, collecting relevance information of a document that is ranked at the same positions by two systems may be not so interesting. Therefore, in addressing the comparison problem, it is sup-posed to move up the documents that are ranked differently. Carterette et al. [3] used the similar idea for retrieval system comparison, but in the context of constructing minimal test collections for Cranfield paradigm evaluation. 4. More Examination. It can collect more relevance in-formation if encouraging the users to examine deeper. Docu-ment relevance information can be collected only if the doc-ument is examined, so we can collect more information if the users examine more documents. The users X  examination per-sistence relies on the relevance of the examined documents. The users would generally stop examining when their infor-mation need has been satisfied by examining highly relevant documents. To encourage the users to examine deeper, one possible way is to delay the satisfaction by moving the rele-vant documents down. One concern of using this strategy is that it obviously harms the users X  utility. We will carefully test its effect on users X  utility and evaluation efficiency, and determine whether it should be used.

These four intuitions are obviously useful for more ef-ficiently collecting relevance information, but it is a chal-lenge to integrate them together into one reordering func-tion. Specifically, they are not consistent to each other well. For example, following intuition 4 requires to rank relevant document lower, but intuition 2 tell us to keep the original ranking, which generally ranks a relevant document highly. We will define the benefit of a document being examined with a document weight function , and formulate the docu-ment reordering into an optimization problem.
The efficient evaluation problem is to find a method that can get accurate absolute or relative metric score of retrieval systems. In the reordering framework, we can estimate doc-uments X  relevance distribution. The overall metric score is integrated by relevance of documents in the ranking, so we can also estimate its distribution. Intuitively, the metric s-core is more reliable once the estimated distribution has a s-mall variance. For each query submission, we would present the retrieved documents in an order so that the expected variance of performance metric score can be maximally re-duced, i.e., where l is a presented document list, S is the absolute metric score for one ranked list or metric score difference between two lists (e.g., DCG or  X  DCG ,etc.),and h is the click-through history about the specific query q . V ( S | h )isthe variance of the metric score S based on the existing click-through history h ,and V ( S | l, h ) is the expected variance of the metric score S based on the history h and the relevance information newly collected from the clicks on the presented list l .

We have discussed four intuitions that are potentially help-ful for efficient evaluation problem . In this section, we will integrate them in the formulation of maximizing performance score variance reduction.
The uncertainty of the metric score ( V ( S )) is determined by the uncertainty of the documents X  relevance estimation ( V ( R i )), so a document is valuable if its uncertainty can be largely reduced by a user X  X  examination. We can formulate the inferred relevance variance reduction from examining a document d i as follows:  X  V ( R i )= P ( C i =1 |  X  R i ) X  1 V ( R i )+ P ( C i =0 where R i is the inferred relevance and  X  R i is the actual rele-vance of d i ; P ( C i =1 |  X  R i )and P ( C i =0 |  X  R i and skipped probability given the actual relevance level  X  respectively. In most click models, it assumes that the click-ing probability equals to the actual relevance level, i.e., P ( C 1 |  X 
R i = r )= r [9, 10]. Similarly, we use this linear transla-tion assumption between click rate and relevance, and the real relevance can also be inferred from the click rate if other translation function is defined.  X  1 V ( R i )and X  0 V ( R variance reduction for clicking and skipping cases respec-tively. If we observe that d i is clicked in the next query submission, the click model can update the relevance distri-bution for d i and the new variance (denoted as V ( R i | 1 ,E i = 1)) is available, so  X  1 V ( R i ) can be defined by the variance difference. The inferred relevance distribution can be usually formulated as a polynomial [9], so the variance can be calculated by numerical integration with multiple bins.  X  0 V ( R i ) can be defined in the similar way for the skipping case. Formally, they are defined as:
Unfortunately, the actual relevance level  X  R i is unknown, so we approximate it with a document weight function (de-noted as w 1 ) by substituting the actual relevance with the best of our current knowledge, i.e., expected value of in-ferred relevance distribution (denoted as  X  R i ). This doc-ument weight can reflect the benefit from examining this document in the aspect of document relevance uncertainty reduction. w 1 ( d i )= P ( C i =1 |  X  R i ) X  1 V ( R i )+ P ( C i =0
For one-system evaluation problem, the benefit of a docu-ment being examined also depends on the document X  X  rank. A higher ranked documents affects the metric score more. The rank effect differs for measures. In this paper, we take DCG as example ( S = DCG ). DCG measures the per-formance of a ranked list with graded relevance judgments [12]. Though nDCG is more commonly used in test col-lection based evaluation, we use its unnormalized version for two reasons: 1) the total number of relevant documents in real Web is usually dynamic and unknown, and 2) the precision-based metrics can reflect users X  utility well in Web search [11, 16]. Since the inferred relevance is a real value, we extend DCG to handle relevance of real value from 0 to 1. Thus the DCG of a ranked list A can be formulated as Click models can infer a distribution of document relevance, so the expected value and variance of the DCG distribu-tion can be expressed as (assuming the relevance between retrieved documents are independent): In one-system evaluation problem, we can reduce the un-certainty of the absolute metrics score. The contribution of each document does not only depend on its variance reduc-tion but also its original rank. The weight of document at rank k is 1 / log 2 2 ( k + 1). Combining both intuition 1 and 2, the benefit of examining document d i (denoted as w 1 , 2
For two-system comparison problem, the benefit a docu-ment being examined also relies on the relative rank differ-ence between these two ranked lists. The documents ranked more differently can distinguish systems better. Again, we take difference between DCG s as example ( S = X  DCG ). The expected value and variance of DCG score difference from two ranked lists A and B can be expressed as: The positive E ( X  DCG A,B ) value indicates that the expect-ed DCG score of list A is higher than that of list B , while negative value indicates that the expected DCG score of list B is higher. We expect to reduce the uncertainty of  X  DCG value, so one document X  X  contribution to the overall perfor-mance difference can be formulated as: It is denoted as w 1 , 3 for using intuition 1 and 3.
The users are likely to examine the highly ranked docu-ments, so it can collect more valuable relevance information by presenting the more valuable documents higher. We can sort the documents according to their weight of w 1 , 2 (Equa-tion 4) for one-system evaluation and w 1 , 3 (Equation 5) for two-system comparison. These two reordering functions are denoted as rf (1 , 2) and rf (1 , 3) respectively ( rf is short for reordering function ).
The benefit of presenting a list can be calculated by aggre-gating the benefit of the documents in the list being exam-ined, so we expect the users to examine more documents. In the DCG and  X  DCG formulation, the reduction of metric uncertainty is equal to the sum of rank-weighted document relevance uncertainty reduction, so we can define list benefit by summing up the benefit of documents a user has exam-ined.

Assuming the ranked list is ( d 1 ,...,d n ), the list benefit function w ( d 1 ,...,d n ) can be defined as: where w  X  is a document benefit function (can be w 1 , 2 or w 1 , 3 , for one-system evaluation and two-system comparison problem respectively). Generally, the users always examine the first ranked document, but the probability of examining deeper documents depends on the relevance of documents ranked above. The above document relevance factor can be plugged in the list benefit function . Where P ( E 2 =1 |  X  R 1 ) is the probability of continuing ex-amining document d 2 given actual relevance  X  R 1 . Again, we don X  X  know the actual relevance  X  R 1 , so we replace it with expected value of d 1  X  X  relevance distribution. w ( d 1 ,...,d n )= w  X  ( d 1 )+ P ( E 2 =1 |  X  R 1 ) w ( d
Obviously, an optimal presented document list is to maxi-mize the list benefit function . In this way, we have aggregate all intuitions in this optimization framework. Unfortunately, this problem is intractable. We approximate it by a greedy algorithm: at each step, we assign each document a weight that can approximately reflect the benefit of selecting this document and append the document with the largest weight to the end of the presented list. As suggested by Equation 6, the benefit of selecting a document d i is composed by two parts: the benefit from the document itself ( w  X  ( d i ), can be w 1 , 2 or w 1 , 3 ) and the benefit from documents ranked below. The second part can be obtained by multiplying probabil-ity of continuing examining at current document and the expected benefit of examining the list below. However, the benefit from the below list is unknown since the below list has not determined when selecting the current document. We approximate the benefit of examining below by the max-imal document benefit of the unselected documents. Thus, the weight of a document d i in an unselected document set D can be formulated as:
Here the probability of examination next document is al-so be defined in the click model (e.g., the click chain mod-el defines P ( E i +1 | R i )=  X  2 R i +  X  3 (1  X  R i ). In this way, the document weight w ( d i ) can be denoted as w 1 , 2 , 4 w  X  = w Equation 7). With the weight function, the greedy algo-rithm is presented in Algorithm 1. We select the maximal weighted document from the unselected document set, ap-pend it to the presented list, until all documents have been selected or the length of the presented list is enough. This version of reordering functions with two different document and rf (1 , 3 , 4) respectively, for both using intuition 4. Since some click models have online version [9, 10], and the re-ordering function is fast ( O ( n )for rf (1 , 2) and rf (1 , 3), and Input : Document set D = { d 1 ,...,d n } , list length n
Output : Reordered Ranked List l begin end
We conducted a user study experiment and a simulation based experiment to verify the usefulness of document re-ordering framework. In this section, we introduce the user study experiment.
The user study is designed to resemble the common search engine usage. We recruit 50 college students. The users were asked to answer 10 questions with the help of our sys-tem. These questions contain both open-information ques-tions (with many answers) and close-information questions (with only one answer) and vary in difficulty and topic. For each question, we designed a query, submitted it to two com-mercial Web search engines and collected the top 10 results from both engines. The questions and queries were designed according to the guideline that: (1) at least one answer can be found in the results of each search engine; and (2) the per-formance of these two search engines can be distinguished (relative DCG score difference is larger than 0 . 1).
In the user study, a user was presented with a question following ten ranked results in a page. The ten presented results are either from one single commercial search engine or interleaved from the two commercial search engines. The snippet of a document is generated by the search engine which returned the corresponding document. In the inter-leaving case, if these two search engines return an identical document (with the same URL), the snippet is randomly selected from those two corresponding snippets. The user was asked to answer this question with the help of the listed results and she can click the link for browsing the original document. She can continue to the next question once she can answer current question or she found that the presented results can not answer the current question. Since our study aims at the efficient query-specific Web retrieval evaluation, the experimental user interface is a little different from those real search engines that the users can not reformulate their queries for an information need.

We implemented five reordering functions in our user s-tudy system: three of them are for one-system evaluation task, and two of them are for two-system comparison task. For one-system evaluation task, the baseline function is ebase (short for evaluation baseline) presents the results A un-changed. We tested two reordering functions rf (1 , 2) and rf (1 , 2 , 4) in section 4.3 to reorder the documents from A . For two-system comparison task, the baseline function ( cbase , short for comparison baseline) and the tested reordering function is rf (1 , 3 , 4). cbase presents all-A for half query submissions and all-B for other half, e.g., presents A at 2 i query submission and another at (2 i +1) th query submis-sion (which list was presented at 2 i th submission is random-ly determined for each query). The baselines and tested reordering functions are summarized in Table 2 (it also con-tains two other reordering functions balance and rf (1 , 3) for two-system comparison task, used in TREC based ex-periments). Each user submitted a query once, so we can collect 50 submissions for each query. There are 5 tested re-ordering functions, so each reordering function receives (50 query submissions/5 functions) = 10 submissions of each query. Within the 10 questions each user answers, every re-ordering function would provide the results of 2 questions to avoid the user bias.

The proposed reordering functions need a click model to estimate the relevance distribution of the retrieved docu-ments. In this experiments, we adopted click chain mod-el [9], which is a state-of-art Bayesian click model for rele-vance inference. In the model, the parameters were learned from 1 M sampled click logs of a commercial search engine:  X  1 =0 . 97 , X  2 =0 . 34 , X  3 =0 . 23. The performance of a reordering function can be mea-Type Function Interpretation
One-ebase presenting A system rf (1 , 2) using intuition 1, 2 Evaluation rf (1 , 2 , 4) using intuition 1, 2, 4
Two-cbase presenting A, B alternatively system balance balanced strategy [13]
Comparison rf (1 , 3) using intuition 1, 3 sured in two dimensions: the evaluation efficiency and the users X  utility. For evaluation, as defined in efficient evalua-tion problem , a X  X ood X  X eordering function is supposed to get the accurate evaluation result with minimal number of query submissions. On the other hand, as discussed in Section 4, the reordering function (especially with intuition 4  X  more examination intuition ) moves up the documents ranked low-er by a retrieval system, so it may harm the users X  utility. A search engine usually uses only a small volume of traffic for evaluation. Nevertheless, a good reordering function should not hurt the users X  utility too much. Therefore, we also need to measure the users X  utility of using a reordering function.
In our experiment, we asked three assessors to judge of the results X  relevance. They judged the results by assigning each of them a relevance level between 0 and 5. We find that the they agree to each other very well (the disagreement is 0 . 11, according to the disagreement for scalar judgment defined in [15]), and we use the average score as the gold relevance lev-el. The search engine X  X  real performance on a query can be measured by DCG based on the actual relevance judgments (denoted as  X  DCG ).

Evaluation Efficiency. A more efficient evaluation method can get more accurate result with same number of query sub-missions. So we use evaluation accuracy at N submissions of aquery q to measure the efficiency of an evaluation method. The accuracy can be measured by comparing real metric s-core (denoted as  X  DCG ) calculated by manual judgment and the inferred metric score (denoted as DCG ) by estimated document relevance.

Since  X  DCG and DCG are of the different value scales, we linearly transform estimated DCG using DCG = aDCG + b . For the two-system comparison task, such transformation would not change the comparison result, because it affects two systems scores in the same way. In the one-system e-valuation task, though it tries to get an absolute score for a retrieval system, the score is also used for comparing with the score of other or future systems. In conclusion, the trans-formation can rescale the score values, but it will not change the evaluation result.

For one-system evaluation task, we expect to predict the metric score of a retrieval system. If the performance score from inferred relevance is close to the real performance score, it indicates that the evaluation is accurate. The closeness can be measured by relative difference (denoted as rdiff ) between  X  DCG and DCG (see Equation 8). The linear trans-formation parameters can affect the rdiff value. For each reordering function, we set the parameters a and b to mini-mize the average relative difference on all queries.
For two-system comparison task, we check the correctness of comparison between two retrieval systems. It raises an error when the sign symbol of the predicted  X  DCG A,B does not conform to that of real  X   X  DCG A,B , i.e.,  X  DCG A,B  X   X 
DCG A,B &lt; 0. They can be integrated to get the error ratio by comparing the predicted comparison results and real comparison results, i.e., where 1 is an indicative function, returning 1 if the input is true and 0 otherwise.

Users X  Utility. It is to measure how the users X  utility affected by using a reordering function, so we compare the users X  utility from the reordered list and that from original list. In our experiment, we measure the users X  utility by the relative DCG score. For one-system evaluation (for A ), the users X  utility is defined as: where  X  DCG L is the average of real DCG value for reordered lists for the query submissions, and  X  DCG A is the real DCG score for original retrieved result A . For two-system com-parison (between A and B ), the users X  utility is defined as the ratio between average DCG score of reordered lists and average DCG score of two compared lists: The results from the user study is presented in Table 3. In the table, row 2-4 ( ebase , rf (1 , 2) and rf (1 , 2 , 4)) show the results for one-system evaluation and row 5-6 ( cbase and rf (1 , 3 , 4)) show the results for two-system comparison. The second column is the accuracy results: relative difference for one-system evaluation and error ratio for two-system com-parison. The third column is the users X  utility results (rel-ative utility to that from original list(s)). The numbers in the brackets are the relative percentage difference compared that of baselines.

In the one-system evaluation results, we can find that the both rf (1 , 2) and rf (1 , 2 , 4) perform much more efficiently than ebase . In this experiment, we find that rf (1 , 2) per-forms even better than rf (1 , 2 , 4), though the improvement is not so obvious on such small dataset. For users X  utili-ty, both two reordering functions decrease users X  utility, and rf (1 , 2) harms much less than rf (1 , 2 , 4).

In the two-system comparison results, we find that rf (1 , 3 , 4) can distinguish two search engines accurately on all ques-tions but cbase can distinguish accurately on only 7 of the 10 questions, and the reordering function has little harm to the users X  utility.
The preliminary user study shows that reordering func-tions can more efficiently collect relevance information for Web retrieval evaluation. We further deploy a large scale experiment on the TREC dataset, which contains various types of queries, system retrieval results (runs) and the rel-evance judgments. We can simulate a user X  X  behavior on the retrieved results of TREC submitted runs, and analyze the performance of reordering functions in multiple scenar-ios according to retrieval result quality, system similarity, etc.
In the experiment, we need two types of data: 1) the queries, corresponding retrieval results and the relevance judgments, and 2) users X  examination and click behavior.
In our experiment, we use the queries, results and judg-ments of TREC terabyte06 dataset. It contains 50 queries (801-850), 71 runs X  results, and three-way scale judgments. In addition, we use the same click chain model [9] as that used in the user study to simulate the users X  behavior. Users X  behavior is also affected by the documents X  relevance. In our basic experiments, we present the results with the empirical setting of R =0 . 9 for highly relevant documents, R =0 . 6 for relevant documents, and R =0 . 2 for non-relevant docu-ments.

The experiment works as follows: 1) For a simulated query submission, a reordering function determines the presented ranking of retrieved documents (from one or two runs); 2) The simulated users would examine and click on the pre-sented results; 3) The estimated relevance information is updated based the collected clickthrough data. 4) The per-formance of the reordering function is measured by the u-tility from the presented ranking and the accuracy of the systems X  estimated performance. We will reuse the metrics defined in section 5.2.

Besides the reordering functions tested in the user study, we also test balance and rf (1 , 3) in the TREC based exper-iments (see Table 2). balance reordering function was pro-posed to compare two retrieval systems by Joachims [13], and Radlinski et al.[18] showed that it works better than the methods solely relying on the features of users X  implicit feedback. rf (1 , 3) uses intuition 1 and 3 for reordering the retrieved documents (see Section 4.3).
We present the evaluation efficiency and users X  utility re-sults for one-system evaluation in Figure 1 and Figure 2 respectively.

In Figure 1, we find that both rf (1 , 2) and rf (1 , 2 , 4) are much more efficient than ebase . rf (1 , 2 , 4) performs only a little better than rf (1 , 2). ebase performs similarly as the re-ordering function at the first few query submissions, because all these functions emphasize the top ranked documents for the first few submissions. However, as the query submis-sion number increases, the benefit of presenting the original ranked list becomes little, because we have collected enough relevance information about the top documents.

In Figure 2, we find that both rf (1 , 2) and rf (1 , 2 , 4) pro-vide similar users X  utility as ebase for the first few query submissions, and then the utility stays in a little worse lev-el. Users X  utility decreases only about 2% by rf (1 , 2), and 3.5% by rf (1 , 2 , 4). Since rf (1 , 2) provides similar evalu-ation efficiency as rf (1 , 2 , 4) but hurts users X  utility much less, rf (1 , 2) is supposed to be a better choice in practice. In the experiments below, we use rf (1 , 2) only. Figure 1: One-system Evaluation: Relative Differ-ence Figure 2: One-system Evaluation: Users X  Utility
The accuracy and users X  utility results for two-system com-parison task is presented in Figure 3 and Figure 4 respec-tively.

In Figure 3, we find that rf (1 , 3 , 4) works best for most cases, rf (1 , 3) is only a little worse than rf (1 , 3 , 4), and both reordering functions work better than the two base-lines. balance performs better than cbase for first few sub-missions, because it more emphasizes the highly ranked doc-uments. This result is consistent with Radlinski et al.[18]. [18] reported that balance strategy can reflect the retrieval systems X  performance better than absolute user feedback across the queries. Their experiments were based on the arXiv.org query log, in which each query string should be submitted very few on average. Our results also show that balance performs not so well when the query submissions are more because it only collects the top ranked documents X  information.

Figure 4 also shows that balance can provide a better user-s X  utility than the average of two lists, because each list may miss some relevant documents at the top, but the other con-tains. rf (1 , 3) decreases utility by about 2%, and rf (1 , 3 , 4) decrease by about 6%. The experimental results suggest that balance is a good strategy for better users X  utility, and rf (1 , 3) can evaluate efficiently with little harm.
In conclusion, our findings from the basic experiment are as follows: 1) the reordering functions perform much effi-ciently than the baselines for both one-system evaluation and two-system comparison task; 2) When using intuition Figure 3: Two-system Comparison: Error Ratio
Figure 4: Two-system Comparison: Users X  Utility 4, it hurts the users X  utility more but the efficiency improve-ment is not so much, so it suggests to use rf (1 , 2) for one-system evaluation and rf (1 , 3) for two-system comparison.
The above experimental results show that the proposed reordering functions can perform well on average. The use-fulness of the reordering functions may be affected by some other factors such as performance of the evaluated retrieval systems, the measure used for evaluation and click model that employed in the reordering functions. In this section, we explore how these factors affect the efficiency and users X  utility of the reordering functions.
It would be more informative and useful to compare the performance of reordering functions in various scenarios, such as bad vs. good retrieval performance, and compar-ing two systems that differ much vs. comparing two similar systems. Thus each such possible scenario should be tested separately to understand the relative strength and weakness of the reordering functions. The following are the different scenarios that we would test in our experiments: bad vs. good retrieval performance: This can be simulated by controlling the (average) metric ( DCG is used) score of the ranked list(s).

Comparison of systems with similar or different retrieval performance: This can be simulated by reg-ulating the relative difference in retrieval performance. In the experiments, we use the relative difference of DCG score (denoted as RDCG , defined in Equation 12 to measure the performance difference, and this conforms to our intuition very well.

Variations of parameters and the corresponding scenarios are summarized in Table 4. The experimental results are presented in Table 5 and Table 6. The numbers in brackets of error ratio or relative difference column are relative per-centage changes of these values compared to the baseline. In the utility column, the values are relative percentage change compared to the utility of original retrieval result(s).
Our findings are as follows: 1) the proposed reordering functions outperform the baselines in the presented scenar-ios; 2) Both one-system evaluation and two-system compar-ison are more difficult for those hard queries. For those queries, the relevant documents are not be highly ranked by the system, so it makes such relevant documents unlike-ly to be examined. The users X  relative utility of reordering function on hard topics are higher. For those hard topics, the original list may rank the relevant documents lower, so the reordering function probably move up those documents and improve the users X  utility; 3) The results indicate that two-system comparison problem is quite easy for those very different system pairs, which conforms to the intuition.
The inferred relevance information can be interpreted as graded relevance judgment by extending its value range from 0to1. Wehavestudied DCG as an IR metric for evaluating retrieval systems. We would further study the effect of other measures.
 Sys. Perf Q. Err. Ratio Utility
Perf. Sim. Freq. cbase rf ( 1 , 3 ) rf ( 1 , 3 )
Good Low 100 0 . 10 0 . 05(  X  50 . 0%)  X  2 . 0%
Bad Low 100 0 . 25 0 . 07(  X  72 . 0%)  X  8 . 5% Measures Q. Rel. Diff. Utility
RBP(0.6) 100 0 . 07 0 . 06(  X  14 . 3%)  X  2 . 0% RBP(0.95) 100 0 . 23 0 . 09(  X  60 . 9%)  X  0 . 9% Measures Q. Err. Ratio Utility
RBP(0.6) 100 0 . 18 0 . 14(  X  22 . 2%)  X  2 . 0% RBP(0.95) 100 0 . 20 0 . 06(  X  70 . 0%)  X  2 . 2%
Besides DCG , there are also some other IR metrics based on graded relevance judgments. For example, RBP is a recently proposed graded relevance metric [16] expressed as Equation 13. RBP has no recall component and can adapt to different retrieval applications by adjusting the parameter p .

We would test whether the reordering functions can per-form well for predicting score of RBP . To adapt the reorder-ing functions to RBP , we also need to replace the benefit function w 1 , 2 and w 1 , 3 with the new formulation w 1 , 2 w 1 , 3 , defined in Equation 14 and 15. The derivation is sim-ilar to that of DCG .
We conduct two experiments, respectively setting p as 0 . 95 and 0 . 6, which have different effects on the ranks. The re-sults are presented in Table 7 and 8 respectively. The results of RBP (0 . 95) and those of DCG are very similar due to the similar rank weight of DCG and RBP (0 . 95). However, the performance pattern of using RBP (0 . 6) is quite different from that of using DCG and RBP (0 . 95). With RBP (0 . 6), it benefits less in using the proposed reordering functions compared to the baselines. The metric RBP (0 . 6) weights very heavily on the top documents, so both the reordering functions and the baseline methods would keep the docu-ments that highly ranked in the original list highly for the query submissions.

From the results, we find that the reordering functions work consistently well with different metrics (with parame-ter setting) used.
In the reordering framework, a very important component is the click model. First, the document benefit is estimated based on the document relevance inferred by the click model; second, the inferred document relevance is utilized to predict the system performance score directly.

However, different users and their information needs may lead variant clicking behaviors. In practice, a click model used in a search engine may not match the real users X  search behavior so well. In this section, we would check whether the proposed reordering functions are robust with a mis-Click Model Q. Rel. Diff. Utility (0.97,0.5,0.15) 100 0 . 19 0 . 10(  X  47 . 4%)  X  1 . 5% Table 10: Two-system Comparison: User Models Click Model Q. Err. Ratio Utility (0.97,0.5,0.15) 100 0 . 19 0 . 06(  X  68 . 4%)  X  1 . 8% matched click model, i.e., the real users X  behavior is more like model M user , but we use a mismatched model M reorder in the reordering framework.

In the experiments, we still use the click chain model with parameters  X  1 =0 . 97 , X  2 =0 . 34 , X  3 =0 . 23 to simulate the users X  click ( M user ) exactly same as that used in above experiments, but use another click model ( M reorder )inthe reordering framework to infer the document relevance for reordering documents and evaluating system performance. We use DCM and CCM(0.97,0.5,0.15) as two tested click models ( M reorder ). DCM X  X  parameters are also learned from thesame1 M sample of click logs. CCM X  X  parameters are set as  X  1 =0 . 97 , X  2 =0 . 5 , X  3 =0 . 15. The results from these two models are presented in Table 9 and 10. Comparing the results with the basic results, we find the metric scores are very similar, indicating that the reordering functions are robust with various click models.
In this paper, we propose a document reordering frame-work to solve the efficient evaluation problem in the con-text of Web retrieval evaluation based on clickthrough data. Four intuitions are proposed to speed up the relevance in-formation collecting for evaluation purpose, and they are integrated together in an optimizing function. Both our us-er study and TREC based experiments validate that the reordering framework can efficiently collect relevance infor-mation for one-system evaluation and two-system compari-son, with little harm to the users X  utility. For the reordering functions, the experimental results suggest to use rf (1 , 2) for one-system evaluation task and rf (1 , 2 , 4) for two-system comparison task.
 This work is supported by NSFC Grant (60933004, 70903008, 61073082) and HGJ Grant 2011ZX01042-001-001. [1] Agichtein, E., Brill, E., Dumais, S., and Ragno, [2] Baeza-Yates, R., Gionis, A., Junqueira, F. P., [3] Carterette, B., Allan, J., and Sitaraman, R.
 [4] Carterette, B., and Jones, R. Evaluating search [5] Chapelle, O., and Zhang, Y. A dynamic bayesian [6] Dou, Z., Song, R., Yuan, X., and Wen, J.-R. Are [7] Dupret, G., Murdock, V., and Piwowarski, B.
 [8] Dupret, G. E., and Piwowarski, B. Auser [9] Guo, F., Liu, C., Kannan, A., Minka, T., Taylor, [10] Guo, F., Liu, C., and Wang, Y. M. Efficient [11] Huffman, S. B., and Hochster, M. How well does [12] J  X  arvelin, K., and Kek  X  al  X  ainen, J. Cumulated [13] Joachims, T. Unbiased evaluation of retrieval quality [14] Joachims, T., Granka, L., Pan, B., Hembrooke, [15] Mizzar o, S. Measuring the agreement among [16] Moffat, A., and Zobel, J. Rank-biased precision [17] Radlinski, F., and Joachims, T. Active exploration [18] Radlinski, F., Kurup, M., and Joachims, T. How [19] Wang, K., Walker, T., and Zheng, Z. Pskip: [20] Yilmaz, E., Kanoulas, E., and Aslam, J. A. A [21] Yue, Y., Gao, Y., Chapelle, O., Zhang, Y., and
