 We present a general method of parallel query processing that allows scalable performance on distributed inverted files. The method allows the realization of a hybrid that com-bines the advantages of the document and term partitioned inverted files.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Search process Algorithms, Performance Inverted Files, Parallel and Distributed Computing
The distributed inverted file is a well-known index data structure for supporting fast searches on very large text col-lections[1,2,4,3,5,7,9]. Aninvertedfileiscomposed of a vocabulary table and a set of posting lists. The voca-bulary table contains the set of relevant terms found in the collection. Each of these terms is associated with a posting list which contains the document identifiers where the term appears in the collection along with additional data used for ranking purposes. To solve a query, it is necessary to get the set of documents associated with the query terms and then perform a ranking of these documents in order to select the top K documents as the query answer. In this paper we assume posting list items composed of pairs of document identifier and frequency in which the associated term appears in the given document.

The approach used by well-known Web search engines to the parallelization of inverted fi les is pragmatic, namely they use the document partitioned approach. Documents are evenly distributed on P processors and an independent in-verted file is constructed for each of the P sets of documents. The disadvantage is that each user query has to be sent to the P processors which leads this strategy to a poor O ( P scalability. Apart from the communication cost, sending a copy of every query to each processor increases overheads associated with large number of threads and disk operations that have to be scheduled. It can also present imbalance at posting lists level (this increases disk access and interproces-sor communication costs). The advantage is that document partitioned indexes are easy to maintain since insertion of new documents can be done locally.

Another competing approach is the term partitioned in-dex in which a single inverted file is constructed from the whole text collection to then distribute evenly the terms with their respective posting lists onto the processors. It is not necessary to broadcast queries to all processors. Ne-vertheless, the load balance is sensitive to queries referring to particular terms with high frequency and posting lists of differing sizes. In addition index construction and main-tenance is much more costly in communication. However, this strategy is able to achieve O (1) scalability by using the parallel query processing method proposed in this paper.
The principle behind our proposal can be explained by analogy with the classic round-robin strategy for dealing with a set of jobs competing to receive service from a proces-sor. Under this strategy every job is given the same quantum of CPU so that jobs requiring large amounts of processing cannot monopolize the use of the CPU. This scheme can be seen as bulk-synchronous in the sense that jobs are allowed to perform a set of operations during their quantum. In this setting we define quanta in computation, disk accesses and communication given by respective  X  X toms X  of size K where K is the number of documents to be presented to the user.
Using this round-robin form of parallel query processing we propose a hybrid distributed inverted file that after con-struction starts as a document partitioned index and evolves to become a term partitioned index in accordance with the user query traffic. In this composite distributed inverted file some terms are kept as in the document partitioned index and others (in most cases the most frequent ones) are kept as in the term partitioned index. The reasons for the bet-ter efficiency, scalability and stability of this approach comes from the following points: ( i ) index construction in the com-posite index is performed as in the document partitioned index and thereby it retains its advantages for this process, ( ii ) most referenced terms in user queries are treated as in the term partitioned index allowing the O (1) scalability for those terms, ( iii ) terms with very large posting lists keep their less relevant list items as in the document index which reduces communication and improves load balance.
The parallel processing of queries is basically composed of a phase in which it is necessary to fetch (usually from disk) parts of all of the posting lists associated with each term present in the query, and perform a ranking of documents in order to produce the results. After this, additional pro-cessing is required to produce the answer to the user. At the parallel server side, queries arrive from a receptionist machinethatwecallthe broker .

The broker machine is in charge of routing the queries to the cluster X  X  processors and receiving the respective answers. It decides to which processor routing a given query by us-ing a load balancing heuristic. Overall the broker tends to evenly distribute the queries on all processors.

The processor in which a given query arrives is called the ranker for that query since it is in this processor where the associated document ranking is performed. Every query is processed using two major steps: the first one consists on fetching a K -sized piece of every posting list involved in the query and sending them to the ranker processor. In the second step, the ranker performs the actual ranking of documents and, if necessary, it asks for additional K -sized pieces of the posting lists in order to produce the K best ranked documents that are passed to the broker as the query results. We call this iterations . Thus the ranking process can take one or more iterations to finish. In every iteration a new piece of K pairs (doc id, frequency) from posting lists are sent to the ranker for every term involved in the query.
Under this scheme, at a given interval of time, the ranking of two or more queries can take place in parallel at differ-ent processors along with the fetching of K -sized pieces of posting lists associated with other queries. We assume a sit-uation in which the query arrival rate in the broker is large enough to let the broker distribute Q = qP queries onto the P processors.

For experimentation we use the vectorial method for per-forming the ranking of documents along with the filtering technique proposed in [6]. Consequently, the posting lists are kept sorted by frequency in descending order. Once the ranker for a query receives all the required pieces of posting lists, they are merged into a single list and passed through-out the filters. If it happens that the document with the least frequency in one of the arrived pieces of posting lists passes the filter, then it is necessary to perform a new iter-ation for this term and all others in the same situation.
The search engine is implemented on top of the BSP model of parallel computing [8] as follows. In BSP the computa-tion is organized as a sequence of supersteps . During a su-perstep, the processors may per form computations on local data and/or send messages to other processors. The mes-sages are available for processing at their destinations by the next superstep, and each superstep is ended with the barrier synchronization of the processors. The underlying commu-nication library ensures that a ll messages are available at their destinations before starting the next superstep.
Thus at the beginning of each superstep the processors get into their input message queues both new queries placed there by the broker and messages with pieces of posting lists related to the processing of queries which arrived at previous supersteps. The processing of a given query can take two or more supersteps to be completed. All messages are sent at the end of every superstep and thereby they are sent to their destinations packed into single messages to reduce communication overheads.
 Query processing is divided in  X  X toms X  of size K ,where K is the number of documents presented to the user as part of the query answer. These atoms are scheduled in a round-robin manner across supersteps and processors. The asyn-chronous tasks are given K sized quanta of processor time, communication network and disk accesses. These quanta are granted during superteps, namely they are processed in a bulk-synchronous manner. As all atoms are equally sized then the net effect is that no particular task can restrain others from using the resources.

In the document partitioned index the broker performs a broadcast of every query to all processors. First and exactly as in the term partitioned index (described below), the bro-ker sends one copy of the query to their respective ranker processors. Secondly, the ranker sends a copy of every query to all other processors. Next, all processors send K/P pairs (doc id, frequency) of their posting lists to the ranker which performs the documents ranking. This implies performing disk operations to fetch K/P -sized pieces of posting lists. In the case of one or more query terms passing both filters, the ranker sends messages to all processors asking for additional K/P pairs (doc id, frequency) of the respective posting lists.
In the term partitioned approach, we distribute the terms and their posting lists in an uniformly at random manner onto the processors. For a given query, the broker sends the query to its ranker processor which upon reception sends messages to the processors containing query terms asking for the first K pairs (doc id, frequency) of every term present in the query. The same is repeated if one or more terms pass the filters in the ranking operation.

A crucial fact in the above descriptions is that ranking is detached from posting list fetches. On current cluster technology the two dominant factors in the running time are the costs of document ranking and fetching lists from disk. Communication is very efficient and thereby less important in the overall cost. This means that communication can be unbalanced without degrading performance significantly. In our scheme ranking is well-balanced by definition and list fetching is kept balanced by using list caching of most frequent terms in queries. In this way the performance of the term partitioned index is no longer sensitive to very frequent terms which allows it to achieve O (1) scalability.
It is not difficult to see that the asymptotic average cost per query of the document partitioned index can be repre-sented by whereas the cost of the term partitioned index can be rep-resented by where D is cost of disk accesses, G and L are the costs of inter-processors communication and synchronization respec-tively, and r the average number of iterations per query. In particular the value of r is a function of the length of the posting lists and indicates that the extra cost of the docu-ment partitioned index can be hidden only by queries requi-ring many iterations or terms with large posting lists. This Figure 1: Query response times for cases with and without round-robin (RR) at superstep level. makes a case for looking at an intermediate situation since index construction and maintenance is simpler and more ef-ficient in the document partitioned index than in the term partitioned index.

During query processing, under an observed query traffic of
Q queries per unit time, the round-robin principle is ap-plied as follows. Once Q new queries are evenly injected onto the P processors, their processing is started in iterations as described above. At the end of the next or subsequent su-perstep some queries, say n queries, all requiring a single iteration, will finish their processing and thereby at the fol-lowing superstep n new queries can start their processing. Queries requiring more iterations will continue consuming resources during a few more supersteps. Alternatively if one is only interested in query throughput and (traffic permit-ting) one can inject Q new queries in each superstep. This has the effect of improving load balance since more activ-ity is performed per processor per superstep. However, the response time of small queries is affected since they must compete with a comparatively larger number of queries.
Figure 1 shows this situation for cases with q =8,32and 128 for P = 32 using the term and document partitioned indexes (details of the experiments are presented in the Ap-pendix).
The collection of documents is assumed to be distributed on the processors using the rule id doc % P where % is the remainder (mod) operator. The starting point is a set of
P inverted files constructed in each processor consider-ing the document there stored, namely a document parti-tioned index. In addition our scheme assumes that the terms are distributed circularly onto the processors using the rule id term % P . The proposed composite index can be seen as a caching scheme where the slower devise is the document partitioned index and the faster one is the term partitioned index.

The broker selects circularly the ranker processor for each query among the P processors. Upon reception of a given query, the ranker sends messages to the processors contain-ing query terms as in the term partitioned index. Namely, for each query term t the ranker sends a message to proces-sor p = t % P asking for the best K pairs of the posting list associated with t . The first time these pairs are requested from the processor p ,itonlycontains K/P of these pairs since at this point it is actually a document partitioned in-dex. Thus the processor p performs a broadcast to all pro-cessors of term t to ask for the best K/P pairs as in the document partitioned index. The communication cost upto this point is no more than twice the cost of the document partitioned index. Once the processor p has received the P pairs of size K/P (itself included) it sends to the ranker the best K pairs and ranking proceeds as described above. However a copy of these K pairs is retained in the processor p for future queries. If the ranker determines a new iteration for term t this process is repeated and so on.

It is a matter of proper tuning with main memory limita-tions the setting of the number of chunks of K pairs to be kept for each term and the specific terms that are treated in this way. Most queries will require one or two iterations and it is well-known that a small percentage of terms appears very frequently in queries. That is, less frequent queries re-quiring more than three or four iterations can be treated as in the document partitioned index and the same can be applied to the less frequent query terms. Notice that after a certain number of queries have been processed, the cost of communication involved in the construction of a term parti-tioned index which is equivalent to the current one held by the composite index tends to be similar. Before operation the composite index can be warmed up with a query log.
In practice it is also possible to keep the K -sized chunks in disk and remove from disk the previous K/P -sized chunks of the starting document partitioned index in each processor. Overall as queries goes by there is a gradual re-organization of posting lists stored in the total disk space as a whole across all the processors. Notice that disk fragmentation is not an issue here because there is no impediment for maintaining a K -sized chunk distributed in P smaller K/P chunks in the same processor.

In figures 2, 3 and 4 we show performance results for the composite index (details of the experiments are presented in the Appendix). Curves labeled D stands for the document partitioned index, T stands for the term partitioned index and C for the composite inverted file. The figure 2 shows the gain in communication in a situation in which the whole term partitioned index is constructed. The curve labeled  X  X ocument to term X  shows the amount of communication that it is necessary to perform for moving the posting lists to produce a term partitioned index starting from a docu-Figure 3: Communication during query processing.
Figure 4: Running times during query processing. ment partitioned index. The curve for the composite index shows that actual communication demanded by this strat-egy during the execution of our query log. The amount of communication is significantly smaller. The figure 3 shows the total amount of communication incurred by the three in-dexes during the processing of queries. On the other hand, the figure 4 shows the running time obtained by the three strategies. For larger number of processors the differences are fairly more relevant.
In this paper we have described and evaluated an efficient method for performing parallel query processing upon dis-tributed inverted files. We have shown how to apply the method to formulate a new index which we call the compos-ite inverted file. This index overcome the main drawback of the term partitioned index which is index construction and maintenance, and yet it is able to achieve a very similar performance. We believe that this composite index is a bet-ter alternative to document partitioned indexes for cases in which the ranking method does not require performing post-ing list intersection operations. We are currently testing our strategy on a 1TB text collection.
 We used a 2GB text sample of the Chilean Web taken from the www.todocl.cl search engine. The text is in Spanish. Using this collection we generated a 1.5GB index structure with 1,408,447 terms. Queries were selected at random from a set of 127,000 queries taken from the todocl log. We also performed experiments with text in English of similar size taken from a collection of crawled documents along with a query log from an experimental search engine at Yahoo! Labs. We took 300,000 random queries from the query log making sure that all terms are in the document collection. In practice we did not observe any significant differences in the results from both text collections. We also performed experiments with a larger 12GB sample drawing the same qualitative conclusions to the ones presented in this paper based on the results obtained with the 2GB Spanish sample.
Theexperimentswereperformedonaclusterwithdual processors (2.8 GHz) that use NFS mounted directories. This system has 2 racks of 6 shelves each with 10 blades to achieve 120 processors. In every run we process 10,000 queries in each processor. That is the total number of queries processed in each experiment reported in the paper is 10,000 P . For our collection the values of the filters C ins and C were both set to 0.1 and we set K to 1020. On average, the processing of every query finished with 0 . 6 K results after 1.5 iterations. [1] C.Badue,R.Baeza-Yates,B.Ribeiro,andN.Ziviani. [2] F. Cacheda, V. Plachouras, and I. Ounis. Performance [3] A. MacFarlane, J. McCann, and S. Robertson. Parallel [4] M. Marin, C. Bonacic, V. Gil-Costa, and C. Gomez. A [5] W. Moffat, J. Webber, Zobel, and R. Baeza-Yates. A [6] M. Persin, J. Zobel, and R. Sacks-Davis. Filtered [7] B. Ribeiro-Neto and R. Barbosa. Query performance [8] L. Valiant. A bridging model for parallel computation. [9] W. Xi, O. Sornil, M. Luo, and E. A. Fox. Hybrid
