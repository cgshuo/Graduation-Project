 W ord alignment X  X etection of corresponding w ords between tw o sentences that are translations of each other  X  X s usually an intermediate step of statisti-cal machine translation (MT) (Bro wn et al., 1993; Och and Ne y , 2003; K oehn et al., 2003), b ut also has been sho wn useful for other applications such as construction of bilingual le xicons, w ord-sense disambiguation, projection of resources, and cross-language information retrie v al.

Maximum entrop y (ME) models ha v e been used in bilingual sense disambiguation, w ord reordering, and sentence se gmentation (Ber ger et al., 1996), parsing, POS tagging and PP attachment (Ratna-parkhi, 1998), machine translation (Och and Ne y , 2002), and FrameNet classification (Fleischman et al., 2003). The y ha v e also been used to solv e the w ord alignment problem (Garcia-V area et al., 2002; Ittycheriah and Rouk os, 2005; Liu et al., 2005), b ut a sentence-le v el approach to combining kno wledge sources is used rather than a w ord-le v el approach.
This paper describes an approach to combin-ing e vidence from alignments generated by e xist-ing systems to obtain an alignment that is closer to the true alignment than the indi vidual align-ments. The alignment-combination approach (called A CME ) operates at t he le v el of alignment links, rather than at the sentence le v el (as in pre vious ME approaches). A CME uses ME to decide whether to include/e xclude a particular alignment link based on feature functions that are e xtracted from the in-put alignments and li n gui stic features of the w ords. Since alignment combination relies on e vidence from e xisting alignments, we focus on alignment links that e xist in at least one input alignment. An important challenge in t h i s approach is the selection of appropriate links when tw o aligners mak e dif fer -ent alignment choices.

W e sho w that A CME yields a significant relati v e error reduction o v er the input alignment systems and heuristic-based combinations on three dif ferent lan-guage pairs. Using a higher number of input align-ments and partitioning the training data into disjoint subsets yield further error -rate reductions. The ne xt section briefly o v ervie ws ME models. Section 3 presents a ne w ME approach to combin-ing e xisting w ord alignment systems. Section 4 de-scribes the e v aluation data, input alignments, and e v aluation metrics. Section 5 presents e xperiments on three language pairs, upper bounds for alignment error rate in alignment combination, and MT e v alu-ation on English-Chinese and English-Arabic. Sec-tion 6 describes pre vious w ork on alignment combi-nation and ME models on w ord alignment. In a statistical classification problem, the goal is to estimate the probability of a class y in a gi v en con-ing data contain e vidence for all pairs of ( y , x ) , it is tri vial to compute the probability distrib ution p . Un-fortunately , due to training-data sparsity , p is gener -ally modeled using only the a v ailable e vidence.
Gi v en a collection of f acts, ME chooses a model consistent with all the f acts, b ut otherwise as uni-form as possible (Ber ger et al., 1996). F ormally , the e vidence is represented as feature functions, i.e., bi-nary v alued functions that map a class y and a con-te xt x to either 0 or 1, i.e., h m : Y  X  X  X  { 0 , 1 } , where Y is the set of all classes and X is the set of all f acts. The biggest adv antage of maximum entrop y models is that the y are able to focus on the selection of feature functions rather than on ho w such func-tions are used. An y conte xt can be used to define feature functions without concern for the indepen-dence of the feature functions from each other or the rele v ance of the feature functions to the final deci-sion (Ratnaparkhi, 1998).

Each feature function h m is associated with a model parameter  X  m . Gi v en a set of M feature func-tions h 1 , . . . , h M , the probability of class y gi v en a conte xt x is equal to: where Z x is a normalization constant. The contri-b ution of each feature function t o the final decision, i.e.,  X  m , can be automatically computed using Gen-eralized Iterati v e Scaling (GIS) algorithm (Darroch and Ratclif f, 1972). The final classification for a gi v en instance is the class y that maximizes p ( y | x ) . Let e = e 1 , . . . , e I and f = f 1 , . . . , f J be tw o sentences in tw o dif ferent languages. An align-ment link ( i, j ) corresponds to a translational equi v-alence between w ords e i and f j . Let A k be an alignment between sentences e and f , where each element a  X  A k is an alignment link ( i , j ) . Let A = { A 1 , . . . , A n } be a set of alignments between e and f . W e refer to the true alignment as T , where each a  X  T is of the form ( i, j ) . The goal of A CME is to combine the information in A such that the combined alignment A C is cl o s er to T . A straightforw ard solution is to tak e the intersection or union of the indi vidual alignments. In this paper , an additional model is learned to combine outputs of A 1 , . . . , A n .

In our combination frame w ork, first, n dif fer -ent w ord-alignment systems, A 1 , . . . , A n , generate w ord alignments between a gi v en English sentence and a foreign-language (FL) sentence. Then a F ea-tur e Extr actor tak es the output of these alignment systems and the parallel corpus (which might be en-riched with linguistic features) and e xtracts a set of feature functions based on linguistic properties of the w ords and the input alignments. Each feature function h m is associated with a model parameter  X  m . Ne xt, an Alignment Combiner decides whether to include or e xclude an alignment link based on the e xtracted feature functions and the model parame-ters associated with them.

F or each possible alignment link a set of features is e xtracted from the input alignments and linguistic properties of w ords. The features that are used for representing an alignment link ( i , j ) are as follo ws: 1. P art-of-speech tags ( posE, posF , prevposE, 2. Outputs of input aligners ( out ): Whether 3. Neighbors ( neigh ): A neighborhood of an 4. F ertilities ( f ertE, f ertF ): The number of 5. Monotonicity ( mon ): The abs o l ute dif ference
Our combination approach emplo ys feature func-tions deri v ed from a subset of the features abo v e. Assuming Y = { y es , no } represents the set of classes, where each class denotes the e xistence or absence of a link in the combined alignment, and X is the set of features abo v e, we generate v arious feature functions h ( y , x ) , where y  X  Y and x are instantiations of one or more features in X . T able 1 lists the feature sets with an e xample feature func-tion for each. 1 F or e xample, the feature function in the fifth ro w has a v alue of 1 if there are 2 neighbor -ing links to ( i, j ) that e xist in the input alignment A and the alignment link ( i , j ) e xists in A C .
In combining e vidence from dif ferent alignments, it is assumed that, when an alignment link is left out by all aligners, that particular link should not be incl u ded in the final output. Since the majority of all possible w ord pairs are unaligned in real data, the inclusion of all possible w ord pairs in the train-ing data leads to sk e wed results, where the learning algorithm i s biased to w ard labeling the links as in-v alid. T o of fset this problem, our training data in-cludes only alignment links that appear in at least one input alignment.

Once the feature functions are e xtracted, we learn the model parameters using the Y ASMET ME pack-age (Och, 2002), which is an ef ficient implementa-tion of the GIS algorithm. The alignment combination techniques are e v aluated in this paper using data from three language pairs, as sho wn in T able 2.
 T able 2: Data Used for Combination Experiments.
Input alignments are generated using tw o e xist-ing w ord alignment systems: GIZA++ (Och, 2000) and SAHMM (Lopez and Resnik, 2005). Both sys-tems are run in tw o dif ferent directions with def ault configurations. W e indicate the tw o directions using the notation Al ig n e r ( en  X  f l ) and Al ig n e r ( f l  X  en ) , where en is English, f l is either Chinese ( c h ), Arabic ( ar ), or Romanian ( r o ).

T o train both systems, additional data w as used for the three language pairs: 107K English-Chinese sentence pairs (4.1M/3.3M English/Chinese w ords); 44K English-Arabic sentence pairs (1.4M/1M En-glish/Arabic w ords); 48K English-Romanian sen-tence pairs (1M/1M English/Romanian w ords). 5
POS tags were generated using the MXPOST tag-ger (Ratnaparkhi, 1998). P OS tagger for English w as trained on Sections 0-18 of the Penn T reebank W all Street Journal corpus. On the FL side, we used POS tagger for only Chinese and it w as trained on Sections 16-299 of Chinese T reebank.

F or comparison purposes, three additional heuristically-induced alignments are generated for each system: (1) Intersection of both direc-tions ( Al ig n e r (int)); (2) Union of both directions (
Al ig n e r (union)); and (3) The pre viously best-kno wn heuristic combination approach called gr ow-dia g-final (K oehn et al., 2003) ( Al ig n e r (gdf)).
In our e v aluation, we tak e A to be the set of align-ment links for a set of sentences, S to be the set of sure alignment links, and P be the set of proba-ble alignment links (in the gold standard). Precision (
P r ), recall ( R c ) and alignment error rate ( AE R ) are defined as follo ws: 6
Our gold standard for each language pair is a manually aligned corpus. Englis h-Chinese annota-tions distinguis h between sure and probable align-ment links (i.e., S  X  P ), b ut there is no such distinc-tion for the other tw o language pairs (i.e., P = S ).
Because of the a v ailability of limited manually annotated data, e v aluations are performed using 5-fold cross v alidation. Once the alignments are gen-erated for each fold (using one as the test set and the other 4 folds as training set), the results are concate-nated to compute precision, recall and error rate on the entire set of sentence pairs for each data set. 7 This section presents se v eral e xperiments and re-sults comparing AER of A CME to those of standard alignment approaches on English-Chinese data. W e also present e xperiments on additional languages, analyses based on precision and recall, an upper -bound oracle analysis, and MT e v aluations. 5.1 English-Chinese Experiments The e xperiments belo w test the ef fects of input alignments, feature set, data partitioning, number of inputs, and size of training data on the performance of A CME. 2 Input alignments: T able 3 sho ws the AER for GIZA++ and SAHMM (in each direction), three heuristic-based combinations and A CME using 2 uni-directional alignments as input and all features described in Section 3. 8 (W e use  X  A CME[2] X  in this section to refer to A CME applied to tw o input alignments and A CME[4] in later sections to refer to A CME applied to four input alignments.)
Using 2 GIZA++ uni-directional alignments as in-put, A CME yields a 22.0% AER X  X  relati v e error re-duction of 25.9% o v er GIZA++(gdf). Similarly , us-ing 2 SAHMM uni-directional alignments as input, A CME produces a 20.6% AER X  X  relati v e error re-duction of 28.0% and 25.4% o v er SAHMM(gdf) and SAHMM(int), respecti v ely . T able 3: Comparison of GIZA++ and SAHMM to A CME[2] (on English-Chinese).
 F eatur e Set: T o e xamine the ef fects of each fea-ture on the performance of A CME, we compute the AER under a v ariety of conditions, remo ving each feature one at a time. A CME is e v aluated using 2 uni-directional GIZA++ alignments as input on English-Chinese data. Using all features, the AER is 22.0%. Our e xperiments sho w that there is no sig-nificant increase in AER for the remo v al of features corresponding to monotonicity (22.1%), neighbors (22.8%), POS on English side (22.9%), POS on foreign-language s ide (22.9%). On the other hand, deleting POS tags on both sides yields an AER of 25.2% and deleting the fert ility features increases the AER to 25.9%. This indicates that both POS tags (or fertilities) contrib ute hea vily to w ard the de-cision as to whether a particular alignment should be included/e xcluded.
 P artitioning Data: Pre vious w ork sho wed that partitioning the data into disjoint subsets and learn-ing a dif ferent model for each partition impro v es the performance of the alignment systems (A yan et al., 2005). T o test whether this same principle ap-plies to alignment combination with maximum en-trop y modeling, the training data w as partitioned us-ing POS tags for English and the FL, and dif ferent weights were learned for each partition.
 T able 4: Application of A CME[2] on P artitioned Data (on English-Chinese).

T able 4 presents the AER for A CME[2], using ei-ther tw o GIZA++ alignments or tw o SAHMM align-ments, on English-Chinese data. W ithout an y parti-tioning, A CME achie v es an AER of 22.0 (GIZA++) and 20.6 (SAHMM). Using English POS tags for data partitioning results in a significant reduction in AER: 19.8% (GIZA++) and 18.0% (SAHMM).
 Interestingly , using foreign-language (FL) tags on their o wn or together with English POS tags does not pro vide an y impro v ement. Ov erall when A CME[2] is applied to partitioned data (using posE for parti-tioning) a relati v e error reduction of 33 X 37% o v er GIZA++(gdf) and SAHMM(gdf) is achie v ed.
 Number of Input Alignments: T able 5 presents the English-Chinese AER for A CME[1] (using ei-ther GIZA++ or SAHMM in only one direction), A CME[2] (using either GIZA++ or SAHMM in tw o directions) and A CME[4] (using GIZA++ and SAHMM, each in tw o directions).

Re g ardless of the number of inputs, partitioning the data (using English POS tags) yields lo wer AER than no partitioning. Using one GIZA++ alignment as input, A CME[1] with partitioning impro v es the AER to 26.9% and 25.5% for each direction, respec-ti v ely . Similarly , using one SAHMM alignment as input, A CME[1] with partitioning reduces the AER to 22.9% and 24.7%. A CME[2] with partitioning reduces the AER to 19.8% and 18.0% for GIZA++ and SAHMM, respecti v ely . Finally , using all four input alignments, A CME[4] with partitioning yields a 15.6% AER X  X  relati v e error reduction of 21.2% and 13.3% o v er each A CME[2] case.
 T able 5: Application of A CME to 1, 2 and 4 Input Alignments (on English-Chinese).
 Size of T raining Data to Obtain Input Align-ments: In general, statistical alignment systems impro v e as the size of the training data increases. W e present the AER for GIZA++ and A CME[2] us-ing GIZA++ alignments as input, where GIZA++ is trained on dif ferent sizes of data. W e started with 20K sentence pairs of FBIS data and increased it to all a v ailable FBIS data (241K sentence pairs).
Figure 1 compares the alignment performance of: (1) uni-directional GIZA++ (ea ch direction); (2) GIZA++(gdf); and (3) A CME[2] with all fea-Figure 1: Ef fects of T raining Data Size Used for Ini-tial Alignments on the performance of GIZA++ and A CME[2] (on English-Chinese). tures and English POS partitioning. W ith only 20K sentence pairs, A CME[2] achie v es an AER of 23.7% in contrast to 34.3% AER for GIZA++(gdf). W ith 241K sentence pairs, A CME[2] yields 18.3% AER in contrast to 27.7% AER for GIZA++(gdf). W e should emphasize that A CME[2] on only 20K sentence pairs yields a lo wer AER than those of all GIZA++ alignments obtained on 241K sen-tence pairs. Ov erall A CME[2] achie v es a relati v e error reduction of 31 X 38% o v er the input align-ments, and a relati v e error reduction of 31 X 34% o v er GIZA++(gdf) for dif ferent sizes of training data. 5.2 Expanding to Additional Languages W e also in v estig ated the applicability of A CME to additional language pairs. T able 6 presents the AER for GIZA++ and SAHMM (in each direction), three combination heuristics (gdf, int and union), and A CME[2] and A CME[4] on English-Arabic and English-Romanian data. W e should emphasize that no POS tagger on the FL side w as used for these e xperiments.

On English-Arabic data, A CME[2] (with POS partitioning and including all features) yields 21.4% (20.7%) AER X  X  relati v e error reduction of 24.6% (13.0%) o v er the best combination heuristic with GIZA++ (SAHMM) alignments. A CME[4] re-duces the AER to 18.1% X  X  relati v e error reduc-tion of 36.3% and 23.9% o v er GIZA++(int) and SAHMM(int), respecti v ely .

On English-Romanian data, A CME[2] (with POS partitioning and including all features) yields 24.7% (26.2%) AER X  X  relati v e error reduction of 14.3% (10.6%) o v er the best combination heuristic with GIZA++ (SAHMM) alignments. A CME[4] re-ments (on English-Arabic and English-Romanian). duces the AER to 22.3% X  X  relati v e error reduc-tion of 22.6% and 23.9% o v er GIZA++(int) and SAHMM(int), respecti v ely . 5.3 Pr ecision, Recall and Upper -Bound W e no w turn to a precision vs. recall analysis of dif-ferent alignments to elucidate the nature of the dif-ferences between tw o alignments.

Figure 2 presents precision and recall v alues for three combined alignments using GIZA++ (int, union, gdf) as well as results for A CME[2] and A CME[4] on three dif ferent language pairs. F or all three pairs, the ranking of the combined align-ments is the same with respect to precision and recall. GIZA++(int) yields the highest precision (nearly 95%) b ut the lo west recall (53 X 57%). Both union and gdf methods achie v e lo w precision (56 X  68%) b ut high recall (75 X 83%), and gdf is better than union. By contrast, A CME[2] yields signifi-cantly higher precision (nearly 87%) b ut lo wer recall (67 X 75%) with respect to union and gdf. A CME[4] has higher precision and recall than A CME[2] X  X n absolute increase of 2 X 3% and 4%, respecti v ely .
Ne xt we compute a n oracle upper -bound in AER where mismatched input alignments are assumed to be resolv ed perfectly within the alignment combina-tion frame w ork (i.e., an or acle chooses the correct output in cases where the input aligners mak e dif fer -ent choices). 9
T able 7 presents the upper bounds using a generic alignment combiner (denoted Or acle ) with 2 and 4 input alignments on three language pairs, assuming a perfect resolution of mismatched input alignments. F or English-Chinese, the upper bound is 9.4% (us-T able 7: Oracle Upper Bounds on AER for Align-ment Combination ing Or acle [2]) and 4.7% (using Or acle [4]). The English-Arabic data e xhibits a slightly higher upper bound of 5.5% for Or acle [4]. The upper bounds for AER on English-Romanian data are e v en higher (up to 17.7%), which indicates that the input alignments are significantly w orse than o t hers. This may be one of the main contrib uting f actors to the lo wer im-pro v ement of A CME on English-Romanian in com-parison to the other tw o language pairs. 5.4 MT Ev aluation T o determine the contrib ution of impro v ed align-ment in an e xternal application, we e xamined the impro v ement in an of f-the-shelf phrase-based MT system Pharaoh (K oehn, 2004) on both Chinese and Arabic data. In these e xperiments, all components of the MT system were k ept the same e xcept for the component that generates a phrase table from a gi v en alignment.
 The input alignments were generated using GIZA++ and SAHMM on 107K (44K) sentence pairs for Chinese (Arabic). A CME (wit h English POS partitioning) combines alignments using model parameters learned from the corresponding manu-ally aligned data. MT output is e v aluated using the standard MT e v aluation metric BLEU (P apineni et al., 2002). 10 T able 8 present s the BLEU scores on MTEv al X 03 data for 5 dif ferent Pharaoh runs, one for each alignment. The parameters of the MT system were optimized on MTEv al X 02 data using minimum error rate training (Och, 2003).

F or the language model, the SRI Language Mod-eling T oolkit w as used to train a trigram model with modified Kneser -Ne y smoothing on 155M w ords of English ne wswire te xt, mostly from the Xinhua por -tion of the Gig a w ord corpus. During decoding, the number of English phrases per FL phrase w as lim-ited to 100 and the distortion of ph ra ses w as lim-ited by 4. Based on the observ ations in (K oehn et al., 2003), we also limited the phrase length to 3 for computational reasons.
 T able 8: Ev aluation of Pharaoh with Dif ferent Initial Alignments using BLEU (in percentages)
F or both languages, A CME[2] and A CME[4] outperform the other three alignment combination techniques. A CME[4], for instance, yields the BLEU scores of 25.59% for Chinese and 45.54% for Arabic X  X n absolute 1.6-1.7% BLEU point increase o v er the best of the other three alignment combina-tions. The dif ferences between the BLEU scores for A CME and the other three BLEU scores are statisti-cally significant, using a significance test with boot-strap resampling (Zhang et al., 2004). ME models ha v e been pre viously applied to se v eral NLP problems, including w ord alignments. F or in-stance, the IBM models (Bro wn et al., 1993) can be impro v ed by adding more conte xt dependencies into the translation model using a ME frame w ork rather than using only p ( f j | e i ) (Garcia-V area et al., 2002). In a later study , Och and Ne y (2003) present a log-linear combination of the HMM and IBM Model 4 that produces better al ignments than either of those. The major adv antage of these tw o methods is that the y do not require manually annotated data.
The alignment process can be modeled as a prod-uct of a transition model and an observ ation model, where ME models the observ ations (Ittycheriah and Rouk os, 2005). Significant impro v ements are re-ported using this approach b ut the need for lar ge manually aligned data is a bottleneck. An alterna-ti v e ME approach models alignment directly as a log-linear combinati on of feature functions (Liu et al., 2005). Moore (2005) and T askar et al. (2005) represent alignments with se v eral feature functions that are then combined in a weighted sum to model w ord alignments. Once a confidence score is as-signed to all links, a non-tri vial search is in v ok ed to find the best alignment using the scores associated with the links. The major dif ference between these approaches and that of A CME is that we use the ME model to predict the correct class for each align-ment link independently using outputs of e xisting alignment systems, instead of generating them from scratch at the le v el of the whole sentence, thus elim-inating the need for an e xhausti v e search o v er all possible alignments, i.e., pre vious approaches w ork globally while A CME is a localized model. A dis-cussion of these tw o contrasting approaches can be found in (T illmann and Zhang, 2005).

A recent attempt to combine outputs of dif fer -ent alignments vie ws the combination problem as a classifier ensemble in the neural netw ork frame w ork (A yan et al., 2005). Ho we v er , this method is subject to the unpredictability of random netw ork initializa-tion, whereas A CME is guaranteed to find the model that maximizes the lik elihood of training data. W e presented a ne w approach, A CME, to combin-ing the outputs of dif ferent w ord alignment systems by reducing the combination problem to the le v el of alignment links and using a maximum entrop y model to learn whether a particular alignment link is included in the final alignment.

Our results indicate that A CME yields significant relati v e error reduction o v er the input alignments and their heuristic-based combinations on three dif-ferent language pairs. Moreo v er , A CME pro vides similar relati v e impro v ements for dif ferent sizes of training data for the input alignment systems. W e ha v e also sho wn that using a higher number of input alignments, and partitioning the training data into disjoint subsets and learning a dif ferent model for each partition yield further impro v ements. W e ha v e tested impact of the reduced AER on MT and ha v e sho wn that alignments generated by A CME yield statistically significant impro v ements in BLEU scores in tw o dif ferent languages, e v en if we don X  t emplo y a POS tagger on the FL side. Ho we v er , additional studies are needed to in v esti-g ate wh y huge impro v ements in AER result in rela-ti v ely smaller impro v ements in BLEU scores.
Because A CME is a supervised learning ap-proach, it requires annotated data; ho we v er , our e x-periments ha v e sho wn that significant impro v ements can be obtained using a small set of annotated data.
