 We address the problem of query segmentation: given a keyword query submitted to a search engine, the task is to group the key-words into phrases, if possible. Previous approaches to the problem achieve good segmentation performance on a gold standard but are fairly intricate. Our method is easy to implement and comes with a comparable accuracy.
 Categories and Subject Descriptors : H.3.3 [Information Search and Retrieval]: Query formulation General Terms : Algorithms, Experimentation Keywords : Query Segmentation
Most Web search queries are sequences of keywords that may comprise complete phrases or compound concepts, e.g., new yorkyankees . Such phrases and concepts can be exploited by search engines as indivisible units in order to improve retrieval pre-cision, or to allow for query reformulation on the level of phrases instead of keywords. Ideally, Web searchers would assist the en-gines by enclosing their phrases in quotes, but experience shows that many searchers aren X  X  even aware of this option. Hence, search engines apply pre-retrieval algorithms that automatically segment queries in order to second-guess the user X  X  intended phrases and to improve the overall user experience. Our contribution in this respect is a new and simple approach to the task of query segmen-tation that achieves a segmentation accuracy comparable to state-of-the-art algorithms.
 Related Work. Recent research suggests a variety of approaches to query segmentation. For instance, Bendersky et al. [1] use a two-stage procedure. Guo et al. [4] and Yu and Shi [9] use methods based on conditional random fields. However, Yu and Shi explicitly focus on query segmentation in the context of text stored in rela-tional databases and use database-specific features that are not ap-plicable in the Web setting. One of the earliest approaches to Web query segmentation is by Risvik et al. [6]. They segment queries by computing so-called connexity scores that measure mutual in-formation within a segment and the segment frequency in a query log. Jones et al. [5] also use a mutual-information-based scoring that finds segments in which adjacent terms have high mutual in-formation. However, neither Risvik et al. nor Jones et al. experi-mentally evaluate the segmentation accuracy of their approaches. In more recent papers, methods based on mutual information are used as baselines and they are shown not to perform as good as more involved methods, such as the supervised learning method by Bergsma and Wang [2]. Bergsma and Wang use statistical features obtained by Web queries and from query logs, as well as depen-dency features that are focused on noun phrases. They also es-tablished a gold standard corpus of 500 queries, each segmented by three human annotators. Subsequent work [8, 10] has adopted the corpus, as do we in our evaluations. Bergsma and Wang X  X  su-pervised learning method is trained on queries segmented by the same annotator who also segmented the gold standard queries. This leaves some doubts with regard to their otherwise remarkably good accuracy. Instead of the supervised approach that requires training data, Tan and Peng [8] and Zhang et al. [10] suggest unsupervised methods based on expectation maximization and eigenspace sim-ilarity. Tan and Peng X  X  method, like ours, uses n -gram frequency counts from a large Web corpus, but unlike our method, it tries to establish segment scores via expectation maximization. In a sec-ond step they also check whether segments are prominently used in Wikipedia. Instead, Zhang et al. suggest to compute segment scores from the eigenvalues of a correlation matrix corresponding to the query. Hence, all three approaches, Bergsma and Wang X  X , Tan and Peng X  X , and Zhang et al. X  X , rely on intricate models and techniques whose optimization involves several hyperparameters. By contrast, our approach to query segmentation implements a straightforward usage of n -gram frequency counts, which performs just as well.
The basic and major assumption of our approach is that phrases contained in queries actually exist on the Web. The idea then is to use the Web itself as a corpus of potential query phrases. The largest obtainable collection of Web phrases is the Google corpus [3]; it contains n -grams of length 1 to 5 from the 2006 Google index along with occurrence frequencies. Based on these Web occurrence frequencies our approach scores a query X  X  possible segmentations and outputs the  X  X est X  choice.

We regard a query q as a sequence ( w 1 ,w 2 ,...,w n ) of words. A valid segmentation S for q is a sequence of disjunct seg-ments s , each a contiguous subsequence of q , whose concatenation equals q .Thereare 2 n  X  1 valid segmentations for q ,and potential segments that contain at least two keywords from algorithm derives a score for a valid segmentation as follows. First, the n -gram frequency count ( s ) of each potential segment trieved. For n -grams up to n =5 the frequencies can be obtained directly from the corpus; for longer n -grams up to mations are made analogously to the set-based method described in [8]. Having the frequencies at hand, all valid segmentations are enumerated systematically, and for each segmentation S ascoreis Table 1: Segmentation performance on the gold standard. computed according to the following function: The factor | s | | s | gives significant weight to long segments com-pared to shorter ones in order to compensate the power law distri-bution of occurrence frequencies on the Web. For example, york X  has a much larger count than  X  X ewyorkyankees X  ,so that the exponential scoring function helps us to avoid segmen-tations like  X  X ewyork X  X  X ankees X  . For a query q we choose from all valid segmentations the segmentation S that maximizes score ( S ) . This na X ve approach is competitive with the more in-volved methods, as our evaluation shows.
We have indexed the Google n -gram corpus in an inverted file in a way similar to [7]. As a query corpus we use the gold standard for query segmentation established by Bergsma and Wang [2], which was also used in subsequent evaluations [8, 10]. Table 1 contains the results reported on that corpus for the mutual information (MI) baseline of [8], the results of the best performing methods from [2, 8, 10], and the results of our na X ve approach. The table should be read as follows. Three annotators X  X , B, and C X  X ndependently segmented the 500 queries of the gold standard, which were orig-inally drawn from the AOL 2006 query log. The annotators seg-mented 220 of the 500 queries in the same way, denoted in the row named  X  X gree. X  As for the segmentation accuracy measures, we report performances on the following three levels: the query accu-racy is the ratio of segmented queries that match the gold standard, the break accuracy is the ratio of decisions between two consecu-tive words (different/same segment) that match the gold standard, and, at the segment level we measure how well the segments found match the gold standard by means of segment precision, segment recall, and segment F -Measure.
 The results in Table 1 show that the approach of Bergsma and Wang [2] is slightly better than the other approaches on annota-tor A as well as on the queries all annotators agree upon. However, it should be noted that their approach is based on a supervised learn-ing algorithm that was explicitly trained on queries segmented by annotator A (the agreement queries also match A X  X  segmentation), and that it requires a lot of additional Web search queries in order to segment one query. Bergsma and Wang did not report exact perfor-mance values for the annotators B and C and they did not measure segment level performance. Zhang et al. [10] did not report perfor-mances on the query or the break level. As for the two approaches of Tan and Peng [8] and Zhang et al. [10], note that our method is marginally better on annotator A, approximately 0 . 1 worse on an-notator B, and in a 0 . 05 range on annotator C as well as the agree-ment queries. Our performance on annotator B can be improved in a postprocessing step which looks for single keyword segments in the segmentation with the highest score, and which adds them to the respective left or right segment if the occurrence frequency count ( s ) of the resulting segment s is above a threshold of This tweak then also achieves a 0 . 05 range on annotator B (query: 0.442; break: 0.774; seg F: 0.579). However, we do not wish to add this tweak to our method for reasons of simplicity, and more importantly, because it appears to be overfitted to annotator B.
As for runtime performance, we are able to segment more than 3000 queries per second on a single machine with sufficient main memory to store all the n -gram counts (about 12 GB).
In this paper, we presented an approach to query segmentation that is competitive with the best algorithms developed so far, while being less intricate at the same time. As for future work, the eval-uation of the state-of-the-art-approaches with respect to gains in retrieval performance is an interesting open problem. To measure segmentation accuracy on a larger scale we are also working on a gold standard that includes significantly more queries. [1] M. Bendersky, W. B. Croft, and D. Smith. Two-stage query [2] S. Bergsma and Q. I. Wang. Learning noun phrase query [3] T. Brants and A. Franz. Web 1T 5-gram Version 1. Linguistic [4] J. Guo, G. Xu, H. Li, and X. Cheng. A unified and [5] R. Jones, B. Rey, O. Madani, and W. Greiner. Generating [6] K. M. Risvik, T. Mikolajewski, and P. Boros. Query [7] B. Stein, M. Potthast, and M. Trenkmann. Retrieving [8] B. Tan and F. Peng. Unsupervised query segmentation using [9] X. Yu and H. Shi. Query segmentation using conditional [10] C. Zhang, N. Sun, X. Hu, T. Huang, and T.-S. Chua. Query
