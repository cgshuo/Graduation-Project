 Recently, data mining on uncertain data str eam has attracted a lot of attentions because of the widely existed imprecise data generated from a variety of streaming applications, such as the data from sensor network and RFI D application. In such applications, the volume of data is very huge, and tuple X  X  arriving rate is quite fast so that it is infeasible to reserve all tuples in memory to be visited for multiple times. A key task for streaming applications is to devise one-pass algorithms to calculate the high-quality approximate result efficiently with each tuple visited a t most once. Undoubtedly, mining uncertain data streams is much more difficult than min ing deterministic data streams because of the usage of uncertain data model. As the most widely used uncertain data model, the possible world model generates a huge number of the possible world instances from an uncertain data set with the sum of probabilities equal to 1. However, the number of the possible world instances blowups exponentially when new tuples arrive, making it impossible to combine medial results generated from all of possible world instances for the final query results.

As one of the most important tasks in data mining field, clustering algorithm aims at finding some clusters on one data set, with the inner similarity of clusters maximized and the outer similarity among clusters minimized. Currently, most traditional cluster-ing methods on deterministic data sets treat the distances between tuples as the unique factor to construct clusters. For example, the k -means algorithm[1] aims at finding k clusters with smallest value of the sum of squared errors (SSE). Let C j be the j -th cluster, and c j the central point of C j , the SSE of C j is calculated as follows. However, the distance between tuples may not be the only critical metric when clus-tering uncertain data streams. Consider an example that all tuples arriving from an un-certain data stream at some time point are deployed in Figure 1. The value affiliated to each tuple represents the corre sponding occurrence probability. Clearly, all tuples can be easily categorized into four clusters according to distance metric, denoted as A , B , C and D . The distributions of B and D are same, but the probabilities of them differ a lot. In D , the probability of tuples are small (equal to 2 0% ), but in B , the probability of tuples are large ( equal to 8 0% ). According to the semantic of the probability value, we can learn that more tuples will occur in B than in D in a randomly selected possible world instance, which means that the quality of cluster B is better than D (We will depict it in detail in next section).

Currently, the uncertainty of a tuple can be described in several ways, such as con-tinuous (/discrete) probability density functi on[13] and distribution parameter[2]. Ag-garwal and Yu proposed one method to handle the distribution parameter case[2]. Their method can be extended to cope with probability density function case. Above two cases belong to attribute-level uncertainty , which implies that each tuple must occur but the values of some attributes are uncertain. Another important case is the point prob-ability model , in which each tuple t is affiliated with a probability p ( t ) , meaning the occurring probability of t .The point probab ility case is a modest and realistic model in the uncertain stream field, and can be treated as an existential uncertainty [12,13]. The work by Aggarwal and Yu can not be extended to cope with our model directly [2].
We made following contributions in this paper. First, we define a new metric to mea-sure the quality of a cluster on uncertain data streams, where the distance between tuples and the proba bilities of tuples are c onsidered together. Second, we propose one novel method to cluster uncertain data streams with detailed analysis and extend a hy-brid decay mechanism. Finally, experiment al results on synthetic and real data sets can evaluate the effectiveness and efficiency of our proposed method.

This paper is organized as follows. After defining the semantics of clusters X  quality in Section 2, the detailed algorithms are shown in Section 3 and 4. In Section 5, we describe the confident decay mechanism as an e xtension. Section 6 re ports experimental results. Section 7 reviews the related work. Finally, we give a brief conclusion and point out the future work in Section 8. Traditionally, we construct clusters over a deterministic database by using the distance metric, such as Manhattan distance, Euclid ean distance, etc. Tuples close to each other are treated to create one cluster because thes e tuples share some similar characteristics in general. However, the distance metric is not enough to judge the quality of a cluster on uncertain database. Actually, the uncertainty information must be considered together.
As the most widely used model in managing uncertain data, the possible world model consists of numerous possible world instances where parts of tuples will occur and the rest will not occur. The probability of each possible world instance is calculated as the product of the proba bility of tuples in the instance and the product of the probability of tuples not existed in the instance. It is important that the sum of the probability of all possible world instances is equal to 1.
 For example, the possible world instances constructed based on B and D in Figure 1 are shown in Table 1 and 2 resp ectively. Because each tuple is indepen-dent to others, there are 2 4 possible world instances for each data set. For the pos-sible world instance only containing t 1 , t 2 and t 3 , the probability is calculated as: 0 . 8  X  0 . 8  X  0 . 8  X  (1  X  0 . 8 )=0 . 10 24 (Table 1). For the possible world instance only con-(Table 2). Let E denote the expected number of tuples in a randomly selected possible world instance based on an uncertain data set. Then, where W is the whole possible world space, p ( w ) is the probability of a possible world instance w ,and | w | is the number of tuples in w . In this example, E ( B )=3 . 2 , E ( D )= 0 . 8 . In fact, the value of E for an uncertain data base C can be calculated in an easier way, as shown below.

According to above analysis, we can learn that the quality of an uncertain cluster C is deeply influen ced by two factors, E ( C ) and r ( C ) ,where r ( C ) is the radius of C . Either larger E ( C ) or smaller r ( C ) means better quality. We define the quality of an uncertain cluster in Definition 1.
 The goal of this paper is to find some groups of tuples with high quality in streaming model. In this paper, we propose a novel method, named PMicro, to cluster uncertain data stream with the use of a micro-clustering framework. Firstly proposed in [5] for large data sets, the micro-clustering model was also adapted in [4] for the case of determin-istic data streams and in [2] for the case of uncertain data streams. In [2], Aggarwal and Yu proposed Error based Cluster Feature (ECF) to handle uncertain data streams. However, their work cannot be applied int o the point probability model directly, which is the core task in our paper.

Obviously, the main challenge is how to integrate the uncertainty into the micro-clustering statistics and algorithms. Assume a data stream consists of a set of multi-dimensional records x 1 ,  X  X  X  ,x k ,  X  X  X  arriving at time stamps t 1 ,  X  X  X  ,t k ,  X  X  X  . Each record x contains d dimensions denoted by x i =( x 1 i ,  X  X  X  ,x d i ) . Different from the determinis-tic data streams, each record x k is also affiliated with a probability value p ( x i ) .Inorder to cope with the point probability model, we define a variety of micro-clusters, named Probability Cluster Feature (PCF) (Definition 2).
 Definition 2 (PCF). A Probability Cluster Feature (P CF) for a set of d-dimensional wherein CF 2 x ( C ) and CF 1 x ( C ) each correspond to a vector of d entries. The defini-tion of each entry is as follows.  X  CF 2 x ( C ) maintains the sum of squares of the data values for each dimension, i.e,  X  CF 1 x ( C ) maintains the sum of data values for each dimension, i.e, the value of  X  E ( C ) maintains the expected number of tuples in a randomly selected possible  X  n ( C ) maintains the number of points in C ,i.e, n ( C )= n .  X  t ( C ) is the time point of the most recent point, i.e., t ( C )= t i n . Property 1 (PCF additive). Let C 1 , C 2 denote two sets of points. PCF ( C 1 C 2 ) can be calculated based on PCF ( C 1 ) and PCF ( C 2 ) .
 The correctness of the additive property is straightforward. The values of entries CF 2 x , CF 1 x , E and n in PCF ( C 1 C 2 ) are the sum of the corresponding entries in PCF ( C 1 ) and PCF ( C 2 ) .Thevalueof t ( C 1  X  C 2 ) is equal to ma x( t ( C 1 ) ,t ( C 2 )) . Besides the simplicity, the additive property is also very useful to cluster a huge dataset. Algorithm 1. PMicro ( c micro ,  X  )
Based on PCF, we propose Algorithm PMicro (Algorithm 1) to cluster uncertain data streams. It uses two input parameters, c micro and  X  . The parameter c micro limits the maximum number of PCF in the buffer S . The elastic coefficient  X  is used in Algorithm SelectPCF (Algorithm 2) to select one cluster for merging. We will explain the usage of parameter  X  in detail in the next section.

Initially, a buffer S is created to reserve some PCFs during the running time. As mentioned earlier, the maximum size of S is c micro (Line 1). For each newly arrival point x t at time t , we invoke subroutine SelectPCF (Algorithm 2) to find one cluster for maintenance. The goal of subroutine SelectPCF is to return NULL if x t is an outlier, or to return a PCF if any cluster in S is near to x t (see details in next section). If x t happens to be near to a PCF (denoted as pcf ), we update pcf by using property 1. Otherwise, we also generate a new PCF for x t in S . Finally, if the size of S has exceeded the predefined parameter c micro , the least recently updated PCF must be removed from S . As mentioned in Algorithm 1, the goal of subroutine SelectPCF is to select one cluster in a set S  X  X ear X  to x t . One main issue left is how to judge the priority of the cluster? In traditional methods, researchers often treat the cluster nearest the point as the optimal cluster by using the distance metric. However, the distance metric is not enough in the uncertain database management field. The Quality of a cluster defined in Definition 1 integrates both the distance metric and the uncertainty. According to this semantic, the quality of the candidate cluster in S should benefit a lot after absorbing the new point. In other words, the goal of Algorithm SelectPCF is to select one cluster with maximum be calculated by r = CF 2 x ( C )  X  ( CF 1 x ( C )) 2 .

A direct solution is to calculate  X Q ( C ) for all C  X  S . However, this method will result in lots of computation cost. Instead, we propose a heuristic method to handle this problem. This heuristic method consists of two phases. First, it will select only a small part of clusters, close to the point. Second, it will select the candidate cluster from such a small dataset. Clearly, although this method cannot get the optimal cluster in theory, it is very efficient in practice.
 Algorithm 2. SelectPCF( S , x ,  X  ) Algorithm 2 describes the detailed steps of how to select a candidate cluster out of S . At first, the algorithm will try to check whether the new tuple is an outlier or not by checking the distance between the point and the center of a cluster. Parameter  X  is predefined to restrict the range. A NULL will be returned if an outlier is detected. Otherwise, we will try to construct S , containing only a small part of tuples in S .In this step, only part of near clusters from t he new point are selected. The parameter  X  will be suggested by the experiment. Finally, we should calculate the change of the cluster quality. It returns the cluster with maximum quality change in S (at Lines 4-7). Example 1. We will take an example as shown in Figure 2. Suppose a new stream point x ( p = 50%) arrives, firstly the distance between x and all clusters will be calculated , then the nearest distance (between x and cluster C ) will be selected as the minimal distance d min ( m in C  X  S ( d ( C, x ))) , then according to the parameter elas-tic coefficient  X  (suppose  X  =1.2), except B, C, D , all clusters whose distance is more than 1 . 2  X  d min will be discarded. Next, our approach will run the second round se-lection to calculate the quality change of cluster Q ( C ) , the results are respectively Q ( B )=  X  0 . 27 ,Q ( C )=  X  0 . 0 7 ,Q ( D )=+0 . 13 , the calculation detail is elaborated in Figure 2 ( B and B X  respectively rep resent the cluster before and after x insert into it, and the radius is supposed to be calculated already). As a result, the cluster D will be selected as the final cluster to absorb the new arrival point.
 In static database, each record has same weight on the clustering res ult, while in the data stream model, user would like to pay more a ttention to the recent data comparing to the old data, thus stream algorithms adopt a tim e decay mechanism to reduce the affection of old data. Such as UMicro [2] improved the CluStream [4], the latter adopted the land mark window, which means points in each tim e stamp are equal weight. UMicro used an exponential decay function in order to define t he weights of the diff erent data points for constructing the micro-clusters. The weight W t ( x ) of a data point x ,attheendof time interval between [ t begin ,t end ] is defined as The limitation of this method is that in the d ecay process, it does not consider the data quality (uncertainty) factor, for instance the 90% confident point and 10% confident point arrive at the same time, then they w ill suffer same decay ratio and be eliminated at the same time. However, in practice, we suppose, the high confident point should have more weight not only on clustering but also decaying, since the hi gh confident points are more valuable than the lower ones. We should use this high confident information during a longer period of time. Based on thi s intuition, we design a confident weighted decay mechanism, the decay func tion is defined as follows: We note that all the data structure and algorithms discussed in this paper can be eas-ily extended to the case of the confident weighted discussed above. As in UMicro [2], the lazy approach is employed to reduce t he update time for the continuously decay-ing statistics in the micro-clusters. In the lazy approach, the time decay factor for a micro-cluster is updated only when it is m odified by the addition of a new data point to the micro-cluster. This approach can ma intain the PCF at a modestly accurate, while without significantly affect on the complexity of the algorithm.

Now we will interpret the decay function from another perspective, for one thing, when static database progress to data stream, user would like to pay more attention to the recent data, so the damped window or slid ing window model replace the landmark model. In fact, the inherent different among above three models is the time decay func-tion, for example, in the landmark window, older data and recent data have the same weight on the mining result, while, in the sliding window the older points (out of win-dow) are overall removed from the result, and the damped window model (Equation 1) balance between above two supreme situations. For another, when the deterministic data stream develops to uncertain data stream, the time is not the only decay factor under consideration, due to the data quality also should be emphasized. Thus, we design such probability and time hybrid d ecay mechanism comparing to the only time-weighted decay function which is widely us ed in deterministic stream. In this section, we demonstrate the effectiveness of PMicro on improving the quality of uncertain data stream clustering. Specifically, we show: i) PMicro can get high quality clustering result to the pure distance-based method; ii) Our method can achieve supe-rior performances; iii) We show how sensitive the clustering quality is in relevance to the parameters. We implemen ted PMicro as well as comparative method with Matlab v7R14. All experiments were conducted on a PentiumIV 3.0 GHz PC with 1GB mem-ory, running Microsoft Windows XP.
 Experimental Competitor. In the experiments, we compare our technique against UMicro [2] method. UMicro is an uncertainty stream clustering method focused on the effect of uncertainty on the computations of distance. We extend it to include probabil-ity calculation component. Note that although UMicro [2] is also designed to tackle the problem of clustering on uncertain stream, but the uncertain model is different, direct comparison is unreasonable. Thus, before comparison we change our point probability dataset to the standard error model data. Th e transform method is described as: larger deviation or error in UMicro , means the worse of the data quality, as the same, in PMi-cro , the more smaller existence probability of the data, the worse of the data quality. So we use x t  X  p t to simulate the standard error used in UMicro .

Note that the parameters of implementations for the two stream clustering methods are set to match each other for the purpose of the comparison, and the micro-cluster number c micro is set to match the class number of each data set.
 Validation Measure. We use Average of Quality (AQ) to measure the effectiveness of the cluster result. The AQ is defined as the mean of clusters X  quality. Given a set of clusters C i (1 &lt;i&lt;k ) ,thenwehave AQ = 1 k k i =1 Q ( C i ) . AQ is a probability num-ber that allows comparison of the variation of clusters that have significantly different points. In general, the larger of AQ value is, the greater quality of the clustering result. Experimental Data Sets. For our competition, we use a number of classic benchmark data sets which are widely employed in [4], [7] and [2].
  X  Synthetic data set is generated using continuous ly drifting cluster s. The probability  X  Network Intrusion Detection data set consists of raw TCP connection records from  X  Forest Covertype data set is obtained from the UCI machine learning repository 6.1 Clustering Quality Evaluation First, the clustering quality of PMicro is compared with that of UMicro . In Figure 3, the effectiveness of the approach is illustrated with progression of the stream. On the X-axis, we have illustrated the progression of the data stream in terms of the number of points. It is clear that in each case, the PMicro method provides superior quality to UMicro under the AQ criterion. This advantage in cluster quality remains over the pro-gression of the entire data stream. In the case of the Forest Cover data set, the advantage is a little less, since most of the probability centralized, thus clusters are relatively less improved in the cluster quality. In the cases of synthetic data sets, the probability quality improvement of the our PMicro method is quite high, and could often be greater than 8%. It is because PMicro uses the probability information in order to decide assignment of points to clusters. 6.2 Performance Results Second, we test the efficiency of our stream clustering method. In Figure 4, we have compared the efficiency of th e clustering method on different data sets. On the X-axis, we also illustrate the progression of the data stream in terms of the number of points, whereas on the Y-axis, we illustrate the time cost with the data stream progression.
The result shows the execution time for the three data sets. We can see that both the execution time of PMicro and UMicro grow linearly as the s tream proceeds, and PMicro is more efficient than UMicro. It is because the processing bottleneck mainly lies in high dimensions calculation and UMicro usually has double size dimensions (due to error dimensions) comparing to PMicro. We note that this is quite modest considering the fact that even the processing step of the PMciro method is twice as much as UMciro because of the addition of probability information calculation. Thus, PMicro is not only effective, but is also a very efficient clustering method for data streams. 6.3 Sensitivity Analysis Finally, we show how sensitive the clustering quality is in relevance to the elastic factor  X  , the decay rate  X  , and the radius threshold  X  .

The elastic factor plays an important role in choosing a proper set of clusters that is used for the second phase selection. The experiment about the sensitivity of elastic factor is shown in Figures 5a. It demonstrates that as long as we choose this parameter not too variant from the standard deviation, PMicro could generate very fairly cluster-ing result. Another important parameter which may affect the clustering quality is the different decay mechanism elaborated in Sec tion 5. Figures 5b shows that our probabil-ity and time hybrid decay mech anism has significantly improved the quality of clusters result which is in accordance to the intuition. The sensitivity of the radius threshold  X  (in Algorithm 2) is described in Figures 5c. Further analysis on this parameter provides a deep understanding of skewness of different data set. Clustering a data stream is one of the most challenging tasks in data stream field. The rate of stream is quite fast and the volume of a stream is huge, so that we can only devise a space-efficient one-pass algorithm to answer the query in an online style. As the first piece of work on clustering data streams, STREAM algorithm [3] extended the well known k-means method for data stream computation. Subsequently, a CluS-tream named framework was shown for more flexible clustering analysis by using the micro-cluster methodology. Zhou et al. also proposed CluWin [6], a solution for sliding-window model, to cluster data streams by combining Exponential Histogram with Clus-ter Feature.

Most recently, clustering a probabilistic data stream became more and more impor-tant because of the uncertain nature in many applications. However, most of present work focused on processing a static uncertain database, which required to visit tuples multiple times, thus were incapable of processing an uncertain stream. The goals of FDBSCAN [9] and FOPTICS [10] algorithms were to find density based cluster from uncertain data. The UK-means [11] algorithm also extended the K-means method.
The first work on clustering a probabilistic data stream was named as UMicro [2], proposed by Aggarwal and Yu. UMicro used a very general model of the uncertainty in which it assumed that only standard error of individual entries was available. Authors showed that the use of even such modest uncertainty information during the mining pro-cess was sufficient to greatly improved the quality of the results than a purely determin-istic method. However, this work can not be simply extended to handle the probabilistic point model, which is the goal of this paper. Clustering uncertain data streams is one of the most critical tasks in data mining field. In this paper, we propose a novel algorithm to handle this problem, named PMicro . Traditional methods often use the distance as the only metric to construct clusters on deterministic data streams. Contrarily, PMicro method also uses the uncertainty infor-mation besides the distance metric. According to the experimental results, both the time consumption and the per-tuple processing cost are very low. One piece of our ongoing work is to devise novel solutions on other uncertain data models.

