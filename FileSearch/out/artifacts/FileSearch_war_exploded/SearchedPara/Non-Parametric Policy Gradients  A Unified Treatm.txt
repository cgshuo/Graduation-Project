 Kristian Kersting kristian.kersting@iais.fraunhofer.de Kurt Driessens kurt.driessens@cs.kuleuven.be Acting optimally under uncertainty is a central prob-lem of artificial intelligence. If an agents learns to act solely on the basis of the rewards associated with ac-tions taken, this is called reinforcement learning (Sut-ton &amp; Barto, 1998). More precisely, the agent X  X  learn-ing task is to find a policy for action selection that maximizes its reward over the long run.
 The dominant reinforcement learning (RL) approach for the last decade has been the value-function ap-proach. An agent uses the reward it occasionally re-ceives to estimate a value-function indicating the ex-pected value of being in a state or of taking an action in a state. The policy is represented only implicitly, for instance as the policy that selects in each state the action with highest estimated value. As Sutton et al. (2000) point out, the value function approach, how-ever, has several limitations. First, it seeks to find de-terministic policies, whereas in real world applications the optimal policy is often stochastic, selecting differ-ent actions with specific probabilities. Second, a small change in the value-function parameter can push the value of one action over that of another, causing a dis-continuous change in the policy, the states visited, and overall performance. Such discontinuous changes have been identified as a key obstacle to establishing con-vergence assurances for algorithms following the value-function approach (Bertsekas &amp; Tsitsiklis, 1996). Fi-nally, value-functions can often be much more complex to represent than the corresponding policy as they en-code information about both the size and distance to the appropriate rewards. Therefore, it is not surprising that so called policy gradient methods have been de-veloped that attempt to avoid learning a value function explicitly (Williams, 1992; Baxter et al., 2001; Konda &amp; Tsitsiklis, 2003). Given a space of parameterized policies, they compute the gradient of the expected reward with respect to the policy X  X  parameters, move the parameters into the direction of the gradient, and repeat this until they reach a local optimum. This direct approximation of the policy overcomes the lim-itations of the value function approach stated above. For instance, Sutton et al. (2000) show convergence even when using function approximation.
 Current policy gradient methods have focused on propositional and continuous domains assuming the environment of the learning agent to be representable as a vector-space. Nowadays, the role of structure and relations in the data, however, becomes more and more important (Getoor &amp; Taskar, 2007): information about one object can help the agent to reach conclu-sions about other objects. Such domains are hard to represent meaningfully using a fixed set of features. Therefore, relational RL approaches have been devel-oped (D X zeroski et al., 2001), which seek to avoid ex-plicit state and action enumeration as  X  in principle  X  traditionally done in RL through a symbolic repre-sentation of states and actions. Existing relational RL approaches, however, have focused on value functions and, hence, suffer from the same problems as their propositional counterparts as listed above.
 In this paper, we present the first model-free pol-icy gradient approach that deals with relational and propositional domains. Specifically, we present a non-parametric approach to policy gradients, called NPPG . Triggered by the observation that finding many rough rules of thumb of how to change the way to act can be a lot easier than finding a single, highly accurate policy, we apply Friedmann X  X  (2001) gradient boosting. That is, we represent policies as weighted sums of regression models grown in a stage-wise op-timization. Such a functional gradient approach has recently been used to efficiently train conditional ran-dom fields for labeling (relational) sequences using boosting (Dietterich et al., 2004; Gutmann &amp; Ker-sting, 2006) and for policy search in continuous do-mains (Bagnell &amp; Schneider, 2003). In contrast to the supervised learning setting of the sequence labeling task, feedback on the performance is received only at the end of an action sequence in the policy search set-ting. The benefits of a boosting approach to functional policy gradients are twofold. First, interactions among states and actions are introduced only as needed, so that the potentially infinite search space is not explic-itly considered. Second, existing off-the-shelf regres-sion learners can be used to deal with propositional and relational domains in a unified way. To the best of the authors X  knowledge, this is the first time that such a unified treatment is established. As our experimen-tal results show, NPPG can even significantly improve upon established results in relational domains. We proceed as follows. We will start off by reviewing policy gradients and their mathematical background. Afterwards, we will develop NPPG in Section 3. In Section 4, we will present our experimental results. Before concluding, we will touch upon related work. Policy gradient algorithms find a locally optimal policy starting from an arbitrary initial policy using a gra-dient ascent search through an explicit policy space. Consider the standard RL framework (Sutton &amp; Barto, 1998), where an agent interacts with a Markov Deci-sion Process (MDP). The MDP is defined by a number of states s  X  X  , a number of actions a  X  X  , state-transition probabilities  X  ( s, a, s 0 ) : S X A X S X  [0 , 1] that represent the probability that taking action a in state s will result in a transition to state s 0 and a re-ward function r ( s, a ) : S X A7 X  IR. When the reward function is nondeterministic, we will use the expected rewards R ( s, a ) = E s,a [ r ( s, a )], where E s,a denotes the expectation over all states s and actions a . The state, action, and reward at time t are denoted as s t  X  X  , a  X  X  and r t ( or R t )  X  IR.
 The agent selects which action to execute in a state following a policy function  X  ( s, a ) : S X A X  [0 , 1]. Current policy gradient approaches assume that the policy function  X  is parameterized by a (weight) vector  X   X  IR n and that this policy function is differentiable with respect to its parameters, i.e., that  X  X  ( s,a, X  )  X  X  ists. A common choice is a Gibbs distribution based on a linear combination of features: where the potential function  X ( s, a ) =  X  T  X  sa with  X  sa the feature vector describing state s and action a . This representation guarantees that the policy specifies a probability distribution independent of the exact form of the function  X . Choosing a parameterization, cre-ates an explicit policy space IR n with n equal to size of the parameter vector  X  . This space can be traversed by an appropriate search algorithm.
 Policy gradients are computed w.r.t. a func-tion  X  that expresses the value of a policy in an environment,  X  (  X  ) = P s  X  X  d  X  ( s ) P a  X  X   X  ( s, a )  X  Q  X  ( s, a ) , where Q  X  is the usual (possibly dis-counted) state-action value function, e.g., Q  X  ( s, a ) = E sibly discounted) stationary distribution of states un-der policy  X  . We assume a fully ergodic environment so that d  X  ( s ) exists and is independent of any starting state s 0 for all policies.
 A property of  X  for both average reward reinforcement learning and episodic tasks with a fixed starting state s 0 is that (Sutton et al., 2000, Theorem 1) Important to note is that this gradient does not in-clude the term  X  X   X  ( s )  X  X  . Eq. (2) allows the computation of an approximate policy gradient through exploration. By sampling states s through exploration of the envi-ronment following policy  X  , the distribution d  X  ( s ) is automatically represented in the generated sample of encountered states. The sum P a  X  X  ( s,a )  X  X  Q  X  ( s, a ) then becomes an unbiased estimate of  X  X   X  X  and can be used in a gradient ascent algorithm.
 Of course, the value Q  X  is unknown and must be esti-mated for each visited state-action pair either by us-ing a Monte Carlo approach or by building an explicit representation of Q , although in this latter case care must be taken when choosing the parameterization of the Q-function (Sutton et al., 2000). A drawback of a fixed, finite parameterization of a policy such as in Eq. (1) is that it assumes each feature makes an independent contribution to the policy. Of course it is possible to define more features to capture combinations of the basic features, but this leads to a combinatorial explosion in the number of features, and hence, in the dimensionality of the optimization problem. Moreover, in continuous and in relational environments it is not clear at all which features to choose as there are infinitely many possibilities. To overcome these problems, we introduce a different policy gradient approach based on Friedmann X  X  (2001) gradient boosting. In our case, the potential function  X  in the Gibbs distribution 1 of Eq. (1) is represented as a weighted sum of regression models grown in an stage-wise optimization. Each regression model can be viewed as defining several new feature combinations. The resulting policy is still a linear combination of fea-tures, but the features can be quite complex. Formally, gradient boosting is based on the idea of functional gradient ascent, which we will now describe. 3.1. Functional Gradient Ascent Traditional gradient ascent estimates the parameters  X  of a policy iteratively as follows. Starting with some initial parameters  X  0 , the parameters  X  m in the next iteration are set to the current parameters plus the gradient of  X  w.r.t. to  X  , i.e.,  X  m =  X  0 +  X  1 + . . . +  X  where  X  m =  X  m  X   X  X / X  X  m  X  1 is the gradient multiplied by a constant  X  m , which is obtained by doing a line search along the gradient. Functional gradient ascent is a more general approach, see e.g. (Friedman, 2001; Dietterich et al., 2004). Instead of assuming a lin-ear parameterization for  X , it just assumes that  X  will be represented by a linear combination of func-tions. Specifically, one starts with some initial func-tion  X  0 , e.g. based on the zero potential, and itera-tively adds corrections  X  m =  X  0 +  X  1 + . . . +  X  m . In contrast to the standard gradient approach,  X  m here denotes the so-called functional gradient, i.e.,  X  m =  X  m  X  E s,a [  X  X / X   X  m  X  1 ]. Interestingly, this func-tional gradient coincides with what traditional policy gradient approach estimate, namely (2), as the follow-ing theorem says.
 Theorem 3.1 (Functional Policy Gradient) For any MDP, in either the average-reward or start-state formulation, This is a straightforward adaptation of Theorem 1 in (Sutton et al., 2000) and is also quite intuitive: the functional policy gradient indicates how we would like the policy to change in all states and actions in order to increase the performance measure  X  .
 Unfortunately, we do not know the distribution d  X  ( s ) of how often we visit each state s under policy  X  . This is, however, easy to approximate from the empirical distribution of states visited when following  X  : where the state s is sampled according to  X  , denoted as s  X   X  . We now have a set of traning examples from the distribution d  X  ( s ), so we can compute the value f m ( s, a ) of the functional gradient at each of the training data points for all 2 actions a applicable in s . We can then use these point-wise functional gradients to define a set of training examples { ( s, a ) , f m ( s, a ) } and then train a function f m : S X A7 X  IR so that it minimizes the squared error over the training exam-ples. Although the fitted function f m is not exactly the same as the desired functional gradient in Eq. (3), it will point in the same general direction assuming there are enough training examples. So, taking a step  X  m =  X  m  X  f m will approximate the true functional policy gradient ascent. As an example, we will now derive the point-wise func-tional gradient of a policy parameterized as a Gibbs distribution (1). For the sake of readability, we denote  X ( s, a ) as  X  a and  X ( s, b ) as  X  b .
 Proposition 3.1 The point-wise functional gradient of  X  with respect to a policy parameterized as a Gibbs distribution (1) equals to Q ( s, a )  X   X  X  ( s,a )  X   X  with  X  X  ( s, a )  X   X ( s 0 , a 0 ) To see this, consider the first case; the other cases can be derived in a similar way. Due to the Gibbs distribution, we can write (  X  X  ( s, a )) / (  X   X ( s, a )) =  X   X  a Assuming (  X   X  b ) / (  X   X  a ) = 0, i.e.,  X  a and  X  b are in-rewrite the state-action gradient as which simplifies  X  due to the definition of  X  ( s, a )  X  to The key point is the assumption (  X   X  b ) / (  X   X  a ) = 0. This actually means that we model  X  with k func-tions f k , one for each action a , i.e.,  X ( s, a ) = f a turn, we estimate k regression models f a m . In the ex-periments, however, learning a single regression model f m did not decrease performance so that we stick here to the conceptually easier variant of learning a single regression model f m .
 We call policy gradient methods that follow the out-lined functional gradient approach non-parametric pol-icy gradient s, or NPPG for short. They are non-parametric because the number of parameters can grow with the number of episodes. 3.2. Gradient Tree Boosting NPPG as summarized in Alg. 1 describes actually a family of approaches. In the following, we will develop a particular instance, called TreeNPPG , which uses regression tree learners to estimate f m in line 6. In TreeNPPG , the policy is represented by sums of regression trees . Each regression tree can be viewed Algorithm 1 : Non-Parametric Policy Gradient
Let  X  0 be the zero potential (the empty tree) for m = 1 to N do return final potential  X  =  X  0 +  X  1 + . . . +  X  m as defining several new feature combinations, one cor-responding to each path in the tree from the root to a leaf. The resulting policies still have the form of a linear combination of features, but the features can be quite complex. The trees are grown using a regres-sion tree learner such as CART (Breiman et al., 1984), which in principle runs as follows. It starts with the empty tree and repeatedly searches for the best test for a node according to some splitting criterion such as weighted variance. Next, the examples R in the node are split into R s (success) and R f (failure) according to the test. For each split, the procedure is recursively applied, obtaining subtrees for the respective splits. As splitting criterion, we use the weighted variance on R s and R f . We stop splitting if the variance in one node is small enough or a depth limit was reached. In leaves, the average regression value is predicted. We propose to use regression tree learners because a rich variety of variants exists that can deal with finite, continuous and even relational data. Depending on the type of the problem domain at hand, one can in-stantiate the TreeNPPG algorithm by choosing the appropriate regression tree learner. We will give sev-eral examples in the following experimental section. Our intention is to investigate how well NPPG works. To this aim, we implemented it and investigated the following questions: (Q1) Does TreeNPPG work and, if so, are there cases where it yields better results than current state-of-the-art methods? (Q2) Is TreeNPPG applicable across finite, continuous, and relational domains? In the following, we will describe the experiments car-ried out to investigate the questions and their results. (Q1) Blocks World: A Relational Domain As a complex domain for empirical evaluation of TreeNPPG , we consider the well-known blocks world (Slaney &amp; Thi  X ebaux, 2001). To be able to compare the performance of TreeNPPG to other relational RL (RRL) systems, we adopt the same experimental environment as used for RRL by e.g. Driessens and D X zeroski (2005). We learn in a world with 10 blocks and try to accomplish the on ( A, B ) goal. This goal is parameterized and appropriate values for A and B are chosen at same the time as a starting state is gen-erated. Thus, the reinforcement learn learns a single policy that stacks any two blocks. Although this is a relatively simple goal in a planning context, both RRL and our TreeNPPG algorithm use a model free ap-proach and only learn from interactions with the envi-ronment. For the given setup this means that there are approximately 55 . 7 million reachable states 3 of which 1 . 44 million are goal states. The minimal number of steps to the goal is 3 . 9 on average. The percentage of states for other minimal solution sizes are given in Fig. 2. The agent only receives a reward of 1 if it reaches the goal in the minimal number of steps. The probability of receiving a reward using a random strat-egy is approximately 1 . 3%. To counter this difficulty of reaching any reward, we adopted the active guid-ance approach as proposed by Driessens and Dzeroski (2004), presenting the RL agent with 10% of expert traces during exploration in all experiments. We apply TreeNPPG to this relational domain by simply employing the relational regression tree learner Tilde (Blockeel &amp; De Raedt, 1998). Rather than using attribute-value or threshold tests in node of the tree, Tilde employs logical queries. Furthermore, a placeholder for domain elements (such as blocks) can occur in different nodes meaning that all occurrences denote the same domain element. Indeed, this slightly complicates the induction process, for example when generating the possible tests to be placed in a node. To this aim, it employs a classical refinement operator under  X  -subsumption. The operator basically adds a literal, unifies variables, and grounds variables. When a node is to be splitted, the set of all refinements are computed and evaluated according to the chosen heuristic. Except for the representational differences, Tilde uses the same approach to tree-building as the generic tree learner. Because single episodes are too short and thus generate too few examples for Tilde to learn meaningful trees, we postpone calling Tilde until an episode brings the cumulated number of ex-amples over 100. As a language bias for Tilde we employ the same language bias as used in published RRL experiments, including predicates such as clear , on , above and compheight 4 . Each learned model  X  m , updates the potential function  X  using a step-size  X  m = 1. We count on the self-correcting property of tree boosting to correct over-or under-stepping the target on the next iteration.
 We ran experiments using three versions of the RRL system, i.e, TG (Driessens et al., 2001), RIB (Driessens &amp; Ramon, 2003), and Trendi (Driessens &amp; D X zeroski, 2005), which represent the current state-of-the-art of RRL systems, and our TreeNPPG algorithm. Af-ter every 50 learning episodes, we fixed the strategy learned by RRL and tested the performance on 1000 randomly generated starting states. For TreeNPPG fixing the strategy is not required as it uses the learned strategy for exploration directly. Fig. 1 shows the re-sulting learning curves averaged over 10 test-runs as well as the standard deviations of these curves. As shown, TreeNPPG outperforms all tested versions of the RRL system. After approximately 2000 learning episodes, TreeNPPG solves 99% of all presented test-cases in the minimal number of steps. With less than 4 steps per learning episode on average, this means that TreeNPPG generalizes the knowledge it col-lected by visiting less than 8000 states to solve 99% of 54 . 3 million states. Around this time, TreeNPPG has generated a list of 500 trees on average. One ob-servation that can not be made directly on the shown graph is the stability of the TreeNPPG algorithm. Where the performance of the RRL system using TG for regression can vary substantially between experi-ments, TreeNPPG follows an extremely similar path in each iteration of the experiment. The only variation between experiments is the exact time-frame of the phase transition in the results, as shown by the peak in TreeNPPG  X  X  standard deviation curve. Fig. 2 shows the results in more detail, plotting the percentage of solved test-cases with respect to the minimal solution length. As one can see, TreeNPPG gradually learns to solve problems with growing solution sizes. The graph also shows the percentage of test-cases for each solution size.
 To summarize, the results clearly show that question Q1 can be answered affirmatively. (Q2) Corridor World: A Continuous Domain To qualitatively test whether TreeNPPG is also ap-plicable in continuous domains, we considered a sim-ple continuous corridor domain. The task is to navi-gate a robot from any position pos 0  X  [0 , 10] in a one-dimensional corridor [0 , 10] to one of the exists at both ends (0 and 10). At each time t = 0 , 1 , 2 , . . . , the robot can go either one step to the left ( a =  X  1) or one step to the right ( a = 1); the outcome, however, is uncertain with pos t +1 = pos t + a + N (1 , 1). The robot is given a reward after each step equal to  X  1 if pos  X  (0 , 10) and equal to 20 otherwise, i.e., it reaches one of the exists. Fig. 3 shows how the stochastic policy evolves with the number of iterations, i.e., calls to the tree learner. These results are averaged over 30 reruns. In each run, we selected a random starting position pos 0 uniformly in (0 , 10), gathered learning examples from 30 episodes in each iteration, and used a step size  X  m = 0 . 7. As one can see, the robot gradually learns to go left in the left section of the corridor and right in the right section. Quite intuitively, the uncertainty is highest in the middle part of the corridor.
 We also ran experiments in a grid-world version of this problem. We do not report on the successful results here because finite domains are a special case of the re-lational and continuous cases. To summarize, question Q2 can also be answered affirmatively. Within reinforcement learning (RL), there are two main classes of solution methods: value-function meth-ods seek to estimate the value of states and actions whereas policy-based methods search for a good pol-icy within some class of policies.
 Within policy-based methods, policy gradients have received increased attention, see e.g. (Williams, 1992; Baxter et al., 2001; Guestrin et al., 2002; Konda &amp; Tsitsiklis, 2003; Munos, 2006) as a non-exhausting list. Most closely related to NPPG is the work of Bagnell and Schneider (2003), see also (Bagnell, 2004). They proposed a functional gradient policy algorithms in reproducing kernel hilbert spaces for continuous do-mains. So far, however, this line of work has not considered (relationally) structured domains and the connection to gradient boosting was not employed. Within value function approaches, the situation is slightly different. NPPG can be viewed as automat-ically generating a  X  X ariable X  propositionalization or discretization of the domain at hand. In this sense, it is akin to tree-based state discretization RL ap-proaches such as (Chapman &amp; Kaelbling, 1991; Mc-Callum, 1996; Uther &amp; Veloso, 1998) and related ap-proaches. Within this line of research, there have been some boosting methods proposed. Ernst et al. (2005) showed how to estimate Q functions with ensemble methods based on regression trees. Riedmiller (2005) keeps all regression examples and re-weights them ac-cording to some heuristic. Both neither consider policy gradients nor relational domains.
 Recently, there have been some exciting new develop-ments in combining the rich relational representations of classical knowledge representation with RL. While traditional RL requires (in principle) explicit state and action enumeration, these symbolic approaches seek to avoid explicit state and action enumeration through a symbolic representation of states and ac-tions. Most work in this context, however, has fo-cused on value function approaches. Basically, a num-ber of relational regression algorithms have been de-veloped for use in this RL system that employ rela-tional regression trees (Driessens et al., 2001), rela-tional instance based regression (Driessens &amp; Ramon, 2003), graph kernels and Gaussian processes (G  X artner et al., 2003) and relational model-trees (Driessens &amp; D X zeroski, 2005). Finally, there is an increasing num-ber of dynamic programming approaches for solv-ing relational MDPs (Kersting et al., 2004; Sanner &amp; Boutilier, 2005; Wang et al., 2007). In contrast to NPPG , they assume a model of the domain. It would be interesting, however, to combine NPPG with these approaches along the line of (Wang &amp; Dietterich, 2003). A parametric step into this direction has been already taken by Aberdeen (2006). We have introduced the framework of non-parametric policy gradient ( NPPG ) methods. It seeks to leverage the policy selection problem by approaching it from a gradient boosting perspective. NPPG is fast and straightforward to implement, combines the expressive power of relational RL with the benefits of policy gra-dient methods, and can deal with finite, continuous, and relational domains in a unified way. Moreover, the experimental results show a significant improvement over established results; for the first time, a (model-free) relational RL approach learns to solve on ( A, B ) in a world with 10 blocks.
 NPPG suggests several interesting directions for fu-ture research such as using more advanced regression models, developing actor-critic versions of NPPG es-timating a value function in parallel to reduce the vari-ance of the gradient estimates, and exploiting NPPG  X  X  ability to learn in hybrid domains with both discrete and continuous variables within real-world domains such as as robotics and network routing. Most interest-ing, however, is to address the more general problem of learning how to interact with (relationally) struc-tured environments in the presence observation noise. NPPG is naturally applicable in this case and, hence, paves the way towards (model-free) solutions of what can be called relational POMDPs. This is a topic of high current interest since it combines the expres-sive representations of classical AI with the decision-theoretic emphasis of modern AI.
 Acknowledgments The authors would like to thank the anonymous reviewers for their valuable com-ments. The material is based upon work performed while KK was with CSAIL@MIT and is supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division, under Contract No. NBCHD030010, and by a Fraunhofer ATTRACT fellowship. KD is a post-doctoral research fellow of the Research Fund -Flanders (FWO).
 Aberdeen, D. (2006). Policy-gradient methods for planning. Advances in Neural Information Process-ing Systems 18 (pp. 9 X 17).
 Bagnell, J. (2004). Learning decisions: Robustness, uncertainty, and approximation . Doctoral disserta-tion, Robotics Institute, Carnegie Mellon Univer-sity, Pittsburg, Pa, USA.
 Bagnell, J., &amp; Schneider, J. (2003). Policy search in reproducing kernel hilbert space (Technical Report CMU-RI-TR-03-45). Robotics Institute, Carnegie Mellon University, Pittsburg, Pa, USA.
 Baxter, J., Bartlett, P., &amp; Weaver, L. (2001). Ex-periments with infinite-horizon, policy-gradient es-timation. Journal of Arificial Intellifence Research (JAIR) , 15 , 351 X 381.
 Bertsekas, D. P., &amp; Tsitsiklis, J. (1996). Neuro-dynamic programming . Belmont, MA: Athena Sci-entific.
 Blockeel, H., &amp; De Raedt, L. (1998). Top-down induc-tion of first order logical decision trees. Artificial Intelligence , 101 , 285 X 297.
 Breiman, L., Friedman, J., Olshen, R., &amp; Stone, C. (1984). Classification and regression trees . Belmont: Wadsworth.
 Chapman, D., &amp; Kaelbling, L. P. (1991). Input gener-alization in delayed reinforcement learning: An al-gorithm and performance comparisions. Proceedings of the 12th International Joint Conference on Arti-ficial Intelligence (pp. 726 X 731) Sydney, Australia. Dietterich, T., Ashenfelter, A., &amp; Bulatov, Y. (2004).
Training conditional random fields via gradient tree boosting. Proceedings of the 21st International Con-ference on Machine Learning (pp. 217 X 224). Banff, Canada.
 Driessens, K., &amp; D X zeroski, S. (2005). Combining model-based and instance-based learning for first order regression. Proceedings of the 22nd Interna-tional Conference on Machine Learning (pp. 193 X  200) Bonn, Germany.
 Driessens, K., &amp; Dzeroski, S. (2004). Integrating guid-ance into relational reinforcement learning. Machine Learning , 57 , 271 X 304.
 Driessens, K., &amp; Ramon, J. (2003). Relational instance based regression for relational reinforcement learn-ing. Proceedings of the 20th International Confer-ence on Machine Learning (pp. 123 X 130) Washing-ton, DC, USA.
 Driessens, K., Ramon, J., &amp; Blockeel, H. (2001).
Speeding up relational reinforcement learning through the use of an incremental first order de-cision tree learner. Proceedings of the 12th Euro-pean Conference on Machine Learning (pp. 97 X 108), Freiburg, Germany.
 D X zeroski, S., De Raedt, L., &amp; Driessens, K. (2001). Re-lational reinforcement learning. Machine Learning , 43 , 7 X 52.
 Ernst, D., Geurts, P., &amp; Wehenkel, L. (2005). Tree-based batch mode reinforcement learning. Journal of Machine Learning Research (JMLR) , 6 , 503 X 556. Friedman, J. (2001). Greedy function approximation:
A gradient boosting machine. Annals of Statistics , 29 , 1189 X 1232.
 G  X artner, T., Driessens, K., &amp; Ramon, J. (2003). Graph kernels and gaussian processes for relational rein-forcement learning. International Conference on In-ductive Logic Programming (pp. 146 X 163) Szeged, Hungary.
 Getoor, L., &amp; Taskar, B. (2007). An introduction to statistical relational learning . MIT Press. Guestrin, C., Lagoudakis, M., &amp; Parr, R. (2002). Co-ordinated reinforcement learning. Proceedings of the 19th International Conference on Machine Learning (pp. 227 X 234) Sydney, Australia.
 Gutmann, B., &amp; Kersting, K. (2006). TildeCRF: Con-ditional random fields for logical sequences. Proceed-ings of the 17th European Conference on Machine Learning (pp. 174 X 185). Berlin, Germany.
 Kersting, K., van Otterlo, M., &amp; De Raedt, L. (2004). Bellman goes relational. Proceedings of the Twenty-
First International Conference on Machine Learn-ing (ICML-2004) (pp. 465 X 472). Banff, Canada. Konda, V. R., &amp; Tsitsiklis, J. N. (2003). On actor-critic algorithms. SIAM J. Control Optim. , 42 , 1143 X 1166.
 McCallum, A. (1996). Reinforcement learning with se-lective perception and hidden state . Doctoral dis-sertation, University of Rochester, New York, NY, USA.
 Munos, R. (2006). Policy gradient in continuous time.
Journal of Machine Learning Research (JMLR) , 7 , 771 X 791.
 Riedmiller, M. (2005). Neural fitted Q iteration -First experiences with a data efficient neural reinforce-ment learning method. Proceedings of the 16th Eu-ropean Conference on Machine Learning (pp. 317 X  328) Porto, Portugal.
 Sanner, S., &amp; Boutilier, C. (2005). Approximate linear programming for first-order MDPs. Proceedings of the 21st conference on Uncertainty in AI (UAI) (pp. 509-517) Edinburgh, Scotland.
 Slaney, J., &amp; Thi  X ebaux, S. (2001). Blocks world revis-ited. Artificial Intelligence , 125 , 119 X 153. Sutton, R., &amp; Barto, A. (1998). Reinforcement learn-ing: An introduction . Cambridge, MA: The MIT Press.
 Sutton, R. S., McAllester, D., Singh, S., &amp; Mansour,
Y. (2000). Policy gradient methods for reinforce-ment learning with function approximation. Ad-vances in Neural Information Processing Systems 12 (pp. 1057 X 1063). MIT Press.
 Uther, W., &amp; Veloso, M. (1998). Tree based discretiza-tion for continuous state space reinforcement learn-ing. Proceedings of the 15th National Conference on Artificial Intelligence (AAAI-96) (pp. 769 X 774) Portland, OR, USA.
 Wang, C., Joshi, S., &amp; Khardon, R. (2007). First order decision diagrams for relational mdps. Proceedings of the 20th International Joint Conference on Artifi-cial Intelligence (pp. 1095 X 1100). Hyderabad, India: AAAI press.
 Wang, X., &amp; Dietterich, T. (2003). Model-based policy gradient reinforcement learning. Proceedings of the 20th International Conference on Machine Learning (pp. 776-783) Washington, DC, USA.
 Williams, R. (1992). Simple statistical gradient follow-ing algorithms for connectionist reinforcement learn-
