 In the ever-expanding era of social media, many scientific disciplines, such as health and health-care, biology, and learning sciences, have adopted computational approaches to exploit patterns and behaviors in large datasets (Wang et al., 2015; Chen and Lonardi, 2009; Baker and Yacef, 2009). In contrast, the primary methods for behavioral sciences still rely on lab experiments with limited amount of subjects, which are time consuming and financially expensive. In addition to this, it is also difficult to obtain a set of samples with geograph-ical variations in traditional lab-based behavioral experiments.

While the social media data are abundantly available, computational approaches to behavioral sciences using Twitter are not well-studied. Even when statistical techniques are applied to these tasks, their concentration has been on simple sta-tistical significance tests and descriptive statis-tics (De Charms, 2013; Zhang et al., 2013). There-fore, we believe that statistical natural language processing techniques are needed for insightful analysis and interpretation in behavioral studies.
In this paper, we use Twitter as a corpus for computational behavioral science. More specifi-cally, we focus on a case study of analyzing an-noying behaviors. To do this, we exploit a corpus of 9 million tweets (Cheng et al., 2010), and ex-tract the tweets that describe these behaviors us-ing the #petpeeve hashtags. #petpeeve is a pop-ular Twitter hashtag, which describes behaviors that might be annoying to others. An example of #petpeeve tweets is shown in Figure 1. To fa-cilitate the analysis, we manually annotate 3,375 tweets with 60 fine-grained categories, which will be described in Section 3. We use a sparse mixed-effects topic model to analyze the salient words in each category, as well as the geographic varia-tions. We show that lexical, syntactic, and seman-tic features enhance the automatic categorization of annoying behaviors; and that the performance is further improved with a novel lexical and frame-semantic embedding based data augmentation ap-proach. Our main contributions are three-fold:  X  We provide a Twitter corpus with fine- X  We qualitatively analyze the Twitter language  X  We propose various linguistic features and a
We outline related work in the next section. The dataset is described in Section 3. We introduce the approach for analyzing #petpeeve Tweets in Sec-tion 4. Experimental results are shown in Sec-tion 5. We discuss possible applications in Sec-tion 6, and conclude in Section 7. Psychologists, behavioral scientists, and computer scientists have studied a wide-range of methods for behavior extraction (Mast et al., 2015). For ex-ample, in lab experiments, arm and body postures (Marcos-Ramiro et al., 2013) are often used to ex-tract self-touch and gestures, while eye gaze (Fu-nes Mora and Odobez, 2012), head pose (Ba and Odobez, 2011), face location and motion (Nguyen et al., 2012), and full-body pose (Shotton et al., 2013) can also be used as cues to extract gaz-ing, nodding, and arm-related behaviors. There are also significant amount of studies of extract-ing facial and speech features to understand smil-ing (Bartlett et al., 2008), eye contact (Marin-Jimenez et al., 2014), and verbal behaviors (Basu, 2002).

With the surge of interest in computational social science (Lazer et al., 2009), Twitter has become a popular resource to study data-driven methods in social science (Miller, 2011). For ex-ample, O X  X onnor et al. (2010a) align the Twit-ter messages with public opinion time series to study computational political science. Ritter et al. (2010) study Twitter dialogues using a clus-tering approach. Bollen et al. (2011) use a sen-timent analysis approach to predict the Ameri-can stock market via Twitter. Li et al. (2014b) have investigated the alignment of Twitter mood with weather for sentiment analysis. In recent years, language technology researchers have fo-cused on developing genre-specific Twitter part-of-speech tagging (Gimpel et al., 2011), named entity recognition (Ritter et al., 2011), summariza-tion (O X  X onnor et al., 2010b), sentiment analy-sis (Agarwal et al., 2011), event extraction (Ritter et al., 2012; Li et al., 2014a), paraphrasing (Xu et al., 2014), machine translation (Ling et al., 2013), and dependency parsing (Kong et al., 2014) meth-ods. To the best of our knowledge, even though there have been studies on using Twitter hashtags to study language-related behaviors (Gonz  X  alez-Ib  X  anez et al., 2011; Bamman and Smith, 2015), Twitter NLP approaches to non-linguistic behav-iors are not well studied in general. We use the Twitter corpus with 9 million sam-pled messages collected in prior work (Cheng et al., 2010), which includes a total of 121K users. The dataset includes latitude and longitude infor-mation.

We extract 3,375 tweets 1 with #petpeeve hash-tags. We follow past work to annotate the tweets (Ritter et al., 2012; Li et al., 2014a): we apply the LDA clustering + human-identification approach to label the categories of the described annoying behaviors in these tweets. The human annotation process includes two stages: first, the annotators identify the 50 categories from the clus-tering process, and use these topics as a candi-date label set to annotate the data; in the second stage, the categories are refined (to 60 classes) from the first pass, and the data is re-annotated with the refined human-specified category labels. Due to the complexity of this fine-grained anno-tation task, the inter-annotator agreement rate be-tween two annotators is moderate (0.445).
 of the dataset are shown in Table 1. In our random samples, the states that post the most #petpeeve tweets are NY, MD, CA, NJ, FL, GA, VA, TX, NC, PA, and DC. In our predictive experiments, we randomly select 60% of tweets for training, and 40% for testing. In this section, we describe our methods for the qualitative and quantitative analyses. In particular, we briefly review a supervised approach of using sparse mixed-effects topic model to visualize the topical words to analyze this behavior data. For the quantitative task of automatic categorization of tweets, we propose a novel approach to create ad-ditional training data, using continuous lexical and semantic representations. 4.1 Supervised Topic Modeling To analyze the salient words for each category of annoying behaviors, we utilize SAGE (Eisenstein et al., 2011), a state-of-the-art mixed-effect topic model, which has been used in several NLP ap-plications (Sim et al., 2012; Wang et al., 2012). SAGE is ideal for our text analytic purposes, be-cause it is supervised, and it builds relatively clean topic models by considering the additive effects and the background distribution of words. There-fore, we can use SAGE to visualize the salient words for each category of annoying behaviors using the 3,375 #petpeeve tweets. Each tweet is treated as a document, and we use Markov Chain Monte Carlo for inference. To facilitate the geographical analysis, we use Google X  X  reverse geocoding service to extract the state information from coordinates, and apply SAGE for visualiza-tion. 4.2 Embedding-Based Data Augmentation In addition to the visualization task, we also ask the question: can we use linguistic cues to predict tweets that describe different annoying behaviors? We formulate the problem as a multiclass classi-fication task, and consider the following feature sets:  X  Lexical Features : we extract unigrams as  X  Part-of-Speech Features : to model shallow  X  Dependency Triples : to better understand  X  Frame-Semantics Features : SE-Embeddings for Data Augmentation Since the Twitter messages are often short and noisy, and the training data is relatively scarce for each class, we consider the feasibility of leveraging external resources, in particular, continuous word embed-dings (Mikolov et al., 2013a) to enhance the mul-ticlass text categorization model.

Two major challenges for leveraging word em-beddings for tweet classification are: 1) because word embeddings are continuous, it is difficult to fuse them with other discrete syntactic and se-mantic features; 2) it is not straightforward how one should transform the word-level representa-tion to the tweet-level representation. In our pre-liminary experiments, we have evaluated the con-tinuous word representation method (Turian et al., 2010), as well as incorporating neighboring words in the embeddings as additional features, but both methods fail to outperform the lexical baseline that uses only bag-of-word unigrams.

To solve this problem, we propose the use of neighboring words in continuous representations to create new instances to augment the training dataset. More specifically, in the embedding vo-cabulary W , we search for the k-nearest-neighbor (knn) word w for a query term using cosine sim-ilarity between query ~ Q and target word vectors ~ W : For each word in a tweet, we query the exter-nal embeddings, and replace them with their knn words to create a new training instance. For ex-ample, consider the tweet  X  X eing late is terrible X  with the punctuality label, after searching for knn words for each token, we create a new training in-stance:  X  X e behind are bad X  with the same label. Frame-Semantic Embeddings Although lexi-cal (Mikolov et al., 2013a) and dependency based embeddings (Levy and Goldberg, 2014) have been studied, semantic-based embedding is still less un-derstood. We consider the continuous embedding of semantic frames (Baker et al., 1998). To do this, we semantically parsed 3.8 million tweets using SEMAFOR (Das et al., 2010), and built a continu-ous bag-of-frame model to represent each seman-data augmentation approach to create additional instances with these semantic frame embeddings. Features Precision Recall F1
Lexical .341 .342 .341 +POS .345 .346 .346 +Dependency* .349 .350 .350 +Semantic Frames* .365 .367 .366 5.1 Qualitative Analysis We show the results of the visualization of salient words for each category of tweets in Table 2. SAGE clearly does a good job identifying annoy-ing specific behaviors in each category. For ex-ample, in the traffic category, we see that the key-words  X  X op X  and  X  X ulled X  that associate with traf-fic stop are identified. Also,  X  X low X  and  X  X peed X  are also recognized as annoying behaviors dur-ing traffic. In the selfishness category, the word  X  X NLY X  and  X  X elfish X  are corrected identified. In the silence category, we see that the word  X  X  X  is promising, because it indicates the behavior when someone reads a blackberry message without re-ply. We see that many slang expressions are asso-ciated with various labels.

In Table 3, we show the geographical varia-tion of tweets. The word  X  X mv X  (DC-Maryland-Virginia) is correctly associated with MD and DC, and when we search the database, these #petpeeve tweets mainly refer to the 2010 snowstorm in the Winter affecting these areas. The  X  X addy X  is prominent in the state of Florida, while the word  X  X ims X  is also identified, showing the unique car culture of this southern state. 5.2 Quantitative Evaluation Experimental Setup We use the logistic regres-sion model from LibShortText (Yu et al., 2013) as the classifier in our 60-way multi-class classifi-cation experiments. Grid search is used to select the best hyper-parameter using the training data only. A final classifier is then trained using the best hyper-parameters and test set results are re-ported. We set k = 5 for knn in our data augmen-tation experiments: the training data is expanded to 5 times of the original size. We use a paired two-tailed student X  X  t test to assess the statistical significance.

Word2Vec is used to train various lexical and semantic embedding models. We consider three lexical embeddings and one frame-semantic em-beddings for data augmentation: 1) Google-News Lexical Embeddings trained with 100 bil-lion words (Mikolov et al., 2013b); 2) Twitter Lex-ical Embeddings trained with 51 million of words; 3) Urban Dictionary lexical embeddings trained with 53 million of words from slang definitions and examples; 4) Twitter Semantic Frame Embed-dings trained with 27 million frames.
 Varying Feature Sets We compare various fea-tures in Table 4. We see that adding shallow part-of-speech features does not have a strong effect on the performance, but adding the dependency triples significantly outperforms the lexical base-line. We see that the semantic frames are partic-ular useful, showing a 7% relative improvement over the baseline.
 The Effectiveness of Data Augmentation Table 5 shows the results of data augmentation. We see that using the Google News lexical embeddings to augment the training data brings a 6.1% relative F1 improvement over the lexical baseline. When considering the additional frame-semantic embed-dings from Twitter, our system obtains the best F1 of 0.380, bringing a 3.8% improvement over the no data augmentation baseline with all linguistic features. We provide a case study of automatically cat-egorizing annoying behaviors using #petpeeve Tweets. We hope that this study can further solicit relevant research on fine-grained analysis of an-noying behaviors in different dimensions, and use computational approaches to improve social good. For example, by using coordinates and other APIs, one might analyze the annoying behaviors in the public working environments (e.g., office, meeting rooms, etc.). By understanding what annoys their employees, companies can renovate their working setups, refine their policies, and improve the satis-faction and productivity of their employees.
In addition to #petpeeve Tweets, there are many other interesting hashtags that align well with tra-ditional topics in behavior sciences. For exam-ple, hashtags like #occupywallstreet can be used to study crowd behaviors in terms of a political un-rest. The #ALS hashtag can be used to study public behaviors in reaction to philanthropic campaigns. Overall, Tweets from carefully selected hashtags can be inexpensive to obtain, and facilitate signif-icant amount of behavioral studies. In this paper, we have presented a case study of the annoying behaviors using Twitter as a corpus. Our fine-grained visualization approach shows insights of different categories of these behaviors, with the geographical effects. We also show that linguis-tic cues are useful to categorize these behaviors automatically, and that using lexical and semantic embeddings as a data augmentation method sig-nificantly improves the performance.
