 Clustering is an important data mining problem. Most of the earlier work on clustering focussed on numeric attributes which have a natural ordering on their attribute values. Recently, clustering data with categorical attributes, whose attribute values do not have a natural ordering, has received some attention. However, previous algorithms do not give a formal description of the clusters they discover and some of them assume that the user post-processes the output of the algorithm to identify the final clusters. 
In this paper, we introduce a novel formalization of a cluster for categorical attributes by generalizing a definition of a cluster for numerical attributes. We then describe a very fast summarization-based algorithm called CACTUS that discovers exactly such clusters in the data. CACTUS has two important characteristics. First, the algorithm requires only two scans of the dataset, and hence is very CACTUS outperforms previous work by a factor of 3 to 10. Second, CACTUS can find clusters in subsets of all attributes and can thus perform a subspace clustering of the data. This feature is important study the performance of CACTUS on real and synthetic datasets. Clustering is an important data mining problem. The goal of clustering, in general, is to discover dense and sparse regions in a dataset. Most previous work in clustering focussed on numerical data whose inherent geometric properties can be exploited to naturally define distance functions between points. However, many datasets also consist of categorical KDD-9c) San Diego CA USA 
Cwyright ACM 1999 l-581 13-143-7/99/08...$5.00 attributes X  on which distance functions are not naturally defined. Recently, the problem of clustering categorical data started receiving interest [GKR98, GRS99]. 
As an example, consider the MUSHROOM dataset in the popular UC1 Machine Learning repository [CBM98]. Each tuple in the dataset describes a sample of gilled mushrooms using twenty two categorical attributes. For instance, the cup color attribute can take values from the domain {brown, bufJ; cinnamon, gray, green, pink, purple, red, white, yellow}. It is hard to reason that one color is  X  X ike X  or  X  X nlike X  another color in a way similar to real numbers. 
An important characteristic of categorical domains is that they typically have a small number of attribute values. For example, the largest domain for a categorical attribute of any dataset in the UC1 Machine Learning repository consists of 100 attribute values (for an attribute of the Pendigits dataset). Categorical attributes with large domain sizes typically do not contain information that may be useful for grouping tuples into classes. For instance, the Cus tomerId attribute in the TPC-D database benchmark [Cou95] may consist of millions of values; given that a record (or a set of records) takes a certain CustomerId value (or a set of values), we cannot infer any information that is useful for classifying the records. Therefore, it is different from the age or geographical location attributes which can be used to group customers based on their age or location or both. Typically, relations contain 10 to 50 attributes; hence, even though the size of each categorical domain is small, the cross product of all their domains and hence the relation itself can be very large. 
In this paper, we introduce a fast summarization-based algorithm called CACTUS2 for clustering categorical data. CACTUS exploits the small domain sizes of categorical at-tributes. The central idea in CACTUS is that summary in-formation constructed from the dataset is sufficient for dis-covering well-defined clusters. The properties that the sum-mary infoi-mation typically fits into main memory, and that it can be constructed efficiently-typically in a single scan of the dataset-result in significant performance improvements:  X  X ttributes whose domain is totally ordered are called numeric, attributes whose domain is not ordered are called caregoricuf. a factor of 3 to 10 times over one of the previous algorithms. Our main contributions in this paper are: 1. In this section, we discuss previous work on clustering cate-gorical data. The EM (Expectation-Maximization) algorithm is a popular iterative clustering technique [DLR77, CS96]. Starting with an initial clustering model (a mixture model) for the data, it iteratively refines the model to fit the data better. After an indeterminate number of iterations, it terminates at a locally optimal solution. The EM algorithm assumes that the entire data fits into main memory and hence is not scalable. We now discuss two previous scalable algorithms STIRR and ROCK for clustering categorical data. 
Gibson et al. introduce STIRR, an iterative algorithm based on non-linear dynamical systems [GKR98]. They represent each attribute value as a weighted vertex in a graph. Multiple copies bI, . b  X , m&gt; called basins, of this set of weighted vertices are maintained; the weights on any given vertex may differ across basins. bl is called the principal basin; bS,... , b, are called non-principal basins. Starting with a set of weights on all vertices (in all basins), the system is  X  X terated X  until a fixed point is reached. Gibson et al. argue that when the fixed point is reached, the weights in one or values on each attribute: the first with large positive weights and the second with small negative weights, and that these groups correspond intuitively to projections of clusters on the attribute. However, the automatic identification of such sets of closely related attribute values from their weights requires a non-trivial post-processing step; such a post-processing step was not addressed in their work. Moreover, the post-processing step will also determine what  X  X lusters X  are output. Also, as we show in Section 3.2, certain classes of clusters are not discovered by STIRR. 
Guha et al. introduce ROCK, an adaptation of an agglom-erative hierarchical clustering algorithm, which heuristically optimizes a criterion function defined in terms of the number of  X  X inks X  between tuples [GRS99]. Informally, the number of links between two tuples is the number of common neighbors3 they have in the dataset. Starting with each tuple in its own cluster, they repeatedly merge the two closest clusters till the required number (say, K) of clusters remain. Since the com-plexity of the algorithm is cubic in the number of tuples in the dataset, they cluster a sample randomly drawn from the dataset, and then partition the entire dataset based on the clus-ters from the sample. Beyond that the set of all  X  X lusters X  together may optimize a criterion function, the set of tuples in each individual cluster is not characterized. In this section, we formally define the concept of a cluster over categorical attributes, and other concepts used in the remainder of the paper. We then compare the class of clusters allowed by our definition with those discovered by STIRR. 3.1 Cluster Definition Intuitively, a cluster on a set of numeric attributes identifies a  X  X ense region X  in the attribute space. That is, the region consists of a significantly larger number of tuples than expected. We generalize this intuitive notion for the class of hyper-rectangular clusters to the categorical domain.4 
As an illustrative example, the region [l, 21 x [2,4] may correspond to a cluster in the 3-d space spanned by three numeric attributes. In general, the class of rectangular regions can be expressed as the cross product of intervals. Since domains of categorical attributes are not ordered, the concept of an interval does not exist. However, a straightforward generalization of the concept of an interval to the categorical domain is a set of attribute values. Consequently, the generalization of rectangular regions in the numeric domain to categorical domain is the cross product of sets of attribute values. We call such regions interval regions. 
Intuitively, a cluster consists of a significantly larger number of tuples than the number expected if all attributes were independent. In addition, a cluster also extends to as large a region as possible. We now formalize this notion for categorical domains by first defining the notion of a tuple belonging to a region, and then the support of a region, which is the number of tuples in the dataset that belong to the region. Definition 3.1 Let Al, . . . , A, be a set of categorical at-tributes with domains Q, . . . , YD,, respectively. Let the dataset D be a set of tuples where each tuple t: The support UD (ai, aj) of the attribute value pair (ai, aj) with respect to D is defined as follows: Atuplet = (t.Al,... ,t.A,) E D is said to belong to the Do(S) of S is the number of tuples in D that belong to S. 
If all attributes Al, . . . , A, are independent and the at-tribute values in each attribute are equally likely (henceforth referred to as the attribute-independence assumption) then the expected support E[oo (S)] of a region S = Si x . . . x S,, is ] 01. r X ;S: i z 1:: z I% X  X . As before, the expected stood from the context, we write o(S) instead of go(S), and a(ai, aj) instead of gD(Ui, aj). Finally, we note that the at-tribute independence assumption can be modified to take any prior information into account; e.g., the marginal probabilities of attribute values. 
Intuitively, a(~;, uj) captures the co-occurence, and hence the similarity, of attribute values oi and uj. Values ui and uj are said to be strongly connected if their co-occurrence (~(ui, uj)) is significantly higher (by some factor (Y) than the value expected under the attribute-independence.5 We now define D* to formalize this intuition, and then give a formal definition of a cluster. Definition 3.2 Let a; E Di, uj E Vj, and Q &gt; 1. The attribute values ai and aj are strongly connected with respect tODifaD(Ui,Uj) &gt; @~&amp;].Thefunctiono~(ui,uj)is defined as follows: Let Si c 2)i and Sj C  X  X j, i # j, be two sets of attribute values. An element ai E Si is strongly connected with Sj if, for all x E Sj, a; and x are strongly connected. Si and Sj are said to be strongly connected if each oi E Si is strongly connected with Sj and each uj E Sj is strongly connected with Si. and a &gt; 1. Then C = (Cl,. . . , Cn) is a cluster over {AI,... , A,} if the following three conditions are satisfied. (1) For all i,j E {l,... , n}, i # j, C X  X  and Cj are strongly no C,! &gt; Ci such that for all j # i, C,! and Cj are strongly connected. (3) The support 00(C) of C is at least LY times the expected support of C under the attribute-independence assumption. 
We call Ci the cluster-projection of C on Ai. C is called a sub-cluster if it satisfies conditions (1) and (3). A cluster C subspace cluster on S; if JSJ = k then C is called a k-cluster. 
We now extend our notion of similarity to attribute value pairs on the same attribute. Let al, us E Di and x E Dj. If (ui, x) and ( ua, x) are strongly connected then (ui, us) are  X  X imilar X  to each other with respect to Aj. The level of similarity is the number of such distinct attribute values x E Vj. We now formalize this intuition. Definition 3.4 Let al, a2 E Di. The similarity yj(al, a~) between ai and ua with respect to Aj (j # i) is defined as follows. yj(ui,ua) ef ]{x E Vj : o*(ui,x) &gt; Oanda*(us,x) &gt; O}] 
Below, we define the summary information which we need later to describe the CACTUS algorithm. The summary in-formation is of two types: (1) inter-attribute summaries and (2) intra-attribute summaries. The inter-attribute summaries consist of all strongly connected attribute value pairs where each pair has attribute values from different attributes; the intra-attribute summaries consist of similarities between at-tribute values of the same attribute. Definition 3.5 Let Al, . . . , A, be a set of categorical at-tributes with domains VI,. . . ,V, respectively, and let D be a dataset. The inter-attribute summary C~J is defined as: CIJ dgf {Cij : i,j E (1,. . . , n}, i # j} where Cij d!f {(Ui~aj,O~(ai,aj)) : Ui E Vi,aj E Vj, anda$(oi,aj) &gt; 0} The intra-attribute summary CII is defined as: CrrdEf{Cii:i,j~ {l,...,n}andi#j}where {(oil, @a, yj(oii 7 a~!)) : oil, Uia E Vi, and 3.2 Discussion We now compare the class of clusters allowed by our definition with the clusters discovered by STIRR. For the comparison, we generate test data using the data generator developed by Gibson et al. for evaluating STIRR [GKR98]. We consider three datasets shown in Figures 1, 2, and 3. Each dataset consists of 100000 tuples. DSl and DS2 have two attributes, DS3 has three attributes where each attribute consists of 100 attribute values. These tuples are distributed over all attribute values on each attribute according to the attribute-independence assumption. We control the location and the size of clusters in each dataset by distributing an additional number of tuples (5% of the total number in the dataset) in regions designated to be clusters thus increasing their supports above the expected value under the attribute-independence assumption. In Figures 1, 2, and 3, the cluster-projection of each cluster is shown within an ellipse. The boundaries of the cluster-projections (ellipses) of a cluster are connected by lines of the same type (e.g., solid, dashed etc.). 
We ran STIRR on the datasets shown in Figures 1,2, and 3, and manually selected the basin that assigns positive and negative weights respectively to attribute values in different cluster-projections. To identify the cluster projections, we observed the weights allocated by STIRR and isolated two groups such that the weights in each group have large magnitude and are close to each other. The cluster-projections identified by STIRR are shown in Figures 4, 5, and 6. first non-principal basin (bs) for every attribute (as shown in Figure 4). When run on the dataset DS2, the first non-principal basin (bs) on Al identifies the two groups: (0,. . . ,9} and (10,. . . , 17) (as shown in Figure 5). The second non-principal basin (bs) on Al identifies the following two groups: (0,. . . ,6} and (7,. . . ,17}. Thus, no basin identifies the overlap between the cluster-projections. It may be possible to identify such overlaps through a non-trivial post processing step. However, it is not clear how many basins are required and how to recognize that cluster-projections overlap from the weights on attribute values. We believe that any such post-processing step itself will be similar to the 
CACTUS algorithm. The result of running STIRR on the dataset DS3 is shown in Figure 6. STIRR merged the two cluster-projections on the second attribute, possibly because one of the cluster-projections participates in more than one cluster. should be valid classes of clusters, and our cluster definition includes these classes. CACTUS correctly discovers all the implanted clusters from the datasets DSl, DS2, and 
DS3. Thus, our definition of a cluster and hence CACTUS, which discovers all clusters allowed by our definition, seems to identify a broader class of clusters than that discovered by STIRR. Since it is not possible to characterize clusters discovered by STIRR, we could not construct any example datasets from which CACTUS does not retrieve the expected clusters and STIRR does. However, it is possible that such types of clusters exist. 
From these experiments, we observe that STIRR fails to discover the following classes of clusters: (1) clusters of  X  X andidate X  clusters which can then be validated to deter-mine the actual set of clusters. CACTUS consists of three phases: summarization, clustering, and validation. In the summarization phase, we compute the summary information from the dataset. In the clustering phase, we use the sum-mary information to discover a set of candidate clusters. In the validation phase, we determine the actual set of clus-ters from the set of candidate clusters. We introduce a hy-pothetical example which we use throughout the paper to illustrate the successive phases in the algorithm. Consider a dataset with three attributes A, B, and C with domains tively. Let the strongly connected attribute value pairs be as shown in Figures 7 and 8. 4.1 Summarization Phase In this section, we describe the summarization phase of 
CACTUS. We show how to efficiently compute the inter-attribute and the ii-ma-attribute summaries, and then describe the resource requirements for maintaining these summaries. 
Categorical attributes usually have small domains. Typical categorical attribute domains considered for clustering consist of less than a hundred or, rarely, a thousand attribute values. 
An important implication of the compactness of categorical domains is that the inter-attribute summary Cij for any pair of attributes Ai and Aj fits into main memory because the number of all possible attribute value pairs from A; and A, equals ]DiJ . ]Ioj]. For the rest of this section, we assume that the inter-attribute summary of any pair of attributes fits easily into main memory. (We will give an example later to support this assumption, and to show that typically inter-attribute summaries for many pairs of attributes together fit into main memory.) However, for the sake of completeness, we extend our techniques in Section 5 to handle cases where this trait is violated. The same argument holds for the intra-attribute summaries as well. 4.1.1 Inter-attribute Summaries 
We now discuss the computation of the inter-attribute sum-maries. Consider the computation of Cij, i # j. We initialize a counter to zero for each pair of attribute values (ai, aj) E 2)i x Vj, and start scanning the dataset D. For each tuple t E D, we increment the counter for the pair (t.A;,t.Aj). 
After the scan of D is completed, we compute g* by set-ting to zero all counters whose value is less than the threshold  X  X j =a.*. Thu s, counts of only the strongly con-nected pairs are reiained. The number of strongly connected pairs is usually much smaller than ]23i] . ]Dj 1. Therefore, the set of strongly connected pairs can be maintained in special-ized data structures designed for sparse matrices [DER86].6 
We now present a hypothetical example to illustrate the resource requirements of the simple strategy described above. 
Consider a dataset with 50 attributes each consisting of 100 attribute values. Suppose we have 100 MB of main memory (easily available on current desktop systems). Assuming that each counter requires 4 bytes we can maintain counters for 2500 (= ,~~~~&amp;r~,) attribute pairs simultaneously. With 50 attributes, we have to evaluate 1225 attribute pairs. Therefore, we can compute all inter-attribute summaries together in just one scan of the dataset. The computational and space requirements here are similar to that of obtaining counts of pairs of items while computing frequent itemsets [AMS+96]. Quite often, a single scan is sufficient for computing Cr 
In some cases, we may need to scan D multiple times-each scan computing Y&amp;j for a different set of (i, j) pairs. 
The computation of the inter-attribute summaries is CPU-intensive, especially when the number of attributes n is high, because for each tuple in the dataset, we have to increment 9 counters. Even if we require multiple scans of the dataset, the I/O time for scanning the dataset goes up but the total CPU time-for incrementing the counters-remains the same. Since the CPU time dominates the overall summary-construction time, the relative increase due to multiple scans is not significant. For instance, consider a dataset of 1 million tuples defined on 50 attributes, each consisting of 100 attribute values. Experimentally, we found that the total time for computing the inter-attribute summaries of the dataset with 1 million tuples is 1040 seconds, whereas a scan of the dataset takes just 28 seconds. Suppose we partition all the 1225 pairs of attributes into three groups consisting of 408, 408, and 409 pairs respectively. The computation of the inter-attribute summaries of attribute pairs in each group requires a scan of the dataset. The total computation time will be around 1096 seconds, which is only slightly higher than computing the summary in one scan. 4.1.2 Intra-attribute Summaries 
In this section, we describe the computation of the intra-attribute summaries. We again exploit the characteristic that categorical domains are very small and thus assume that the intra-attribute summary of any attribute Ai fits in main memory. Our procedure for computing Cii reflects the evaluation of the following SQL query: Select Tl.A, T2.A, count(*) From Cij as Tl(A,B), Cij as T2(A,B) Where Tl .A # T2.A and Tl .B = T2.B Group by Tl .A, T2.A Having count &gt; 0; 
The above query joins Cij with itself to compute the set of attribute value pairs of Ai strongly connected to each other with respect to Aj. X  Since Cij fits in main memory the self-join and hence the computation of C$ is very fast. We will observe in the next section that, at any stage of our algorithm, we only require C$ for a particular pair of attributes Ai and 77 
Aj. Therefore, we compute Cii, (j # i), for each (i,j) pair whenever it is required. notation CXY to denote the inter-attribute summary between attributes X and Y.) The inter-attribute summaries CAB, 
CBC, and CAC correspond to the edges between attribute values in the figure. The intra-attribute summaries CzA, %J3* % are shown in Figure 8. 4.2 Clustering Phase 
In this section, we describe the two-step clustering phase of CACTUS that uses the attribute summaries to compute candidate clusters in the data. In the first step, we analyze each attribute to compute all cluster-projections on it. In the second step, we synthesize, in a level-wise manner, candidate clusters on sets of attributes from the cluster-projections on individual attributes. That is, we determine candidate clusters on a pair of attributes, then extend the pair to a set of three attributes, and so on. We now describe each step in detail. 4.2.1 Computing Cluster-Projections on Attributes their domains. The central idea for computing all cluster-projections on an attribute is that a cluster (Cl,. . . , Cn) over the set of attributes {AI, . . . , A,} induces a sub-cluster over any attribute pair (A;, Aj), i # j. In addition, the cluster-projection Ci on Ai of the cluster C is the intersection of the cluster-projections on Ai of 2-clusters over attribute pairs (Ai, Aj), j # i. For example, the cluster-projection {bi, ba} on the attribute B in Figure 7 is the intersection of {bi, ba, bs} (the cluster-projection on B of the 2-cluster 
B of the 2-cluster ({al, a2, as, ad}, {bi, bz})). We formalize the idea in the following lemma. 
Lemma 4.1 Let C = (Ci, . . . , Cn) be a cluster on the set of attributes {Al, . . . , An}. Then, over the pair of attributes (Ai, Aj). (2) There exists a set {Ci : j # i and (C!, Cj) is a 2-cluster over (Ai, Aj)} such that Ci = nj+Cf. the first pairwise cluster-projection step, we cluster each at-tribute Ai with respect to every other attribute Aj, j # i to find all cluster-projections on Ai of 2-clusters over (Ai, Aj). 
In the second intersection step, we compute all the cluster-projections on Ai of clusters over {Al, . . . , A,} by inter-secting sets of cluster-projections from a-clusters computed in the first step. However, the problem of computing cluster-projections of P-clusters in the pairwise cluster-projection step is at least as hard as the NP-complete clique problem [GJ79].* 
The following lemma formalizes the computational complex-ity. The proof is given in the full paper [GGR99]. 
Lemma 4.2 Let Ai and A, be two attributes. The problem of computing all cluster-projections on Ai of 2-clusters over (Ai, Aj) is NP-complete. projection problem, we exploit the following property which, we believe, is usually exhibited by clusters in the categorical domain. If a cluster-projection Ci on Ai of one (or more) cluster(s) is larger than a fixed positive integer, called the distinguishing number (denoted K), then it consists of a small identifying set-which we call the distinguishing set-of attribute values such that they will not together be contained in any other cluster-projection on A;. Thus, the distinguishing set distinguishes C; from other cluster-projections on Ai. 
Note that a proper subset of the distinguishing set may still belong to another cluster-projection, and that two distinct clusters may share an identical cluster-projection (as in Figure 1). in almost all cases. Even for the most restrictive version, which occurs when the distinguishing number is 1 and all cluster-projections of the set of clusters are distinct, the as-sumption only requires that each cluster consist of a set of attribute values-one on each attribute-that does not belong to any other cluster. For the example in Figure 7, the sets {ai} or {aa} identify the cluster-projection {ai, aa} on the attribute A. We now formally state the assumption. 
Distinguishing Subset Assumption: Let Ci and Ci each of size greater than IE be two distinct cluster-projections on the attribute Ai. Then there exist two sets Si and S;l such that We call K the distinguishing number. Paitwise Cluster-Projections 
We compute cluster-projections on Ai of 2-clusters over the attribute pair (Ai, Aj) in two steps. In the first step, we find all possible distinguishing sets (of size less than or equal to K) on A;. In the second step, we extend with respect to Aj some 78 of these distinguishing sets to compute cluster-projections on 
Ai. Henceforth, we write  X  X luster-projection on Ai X  instead of a  X  X luster-projection on Ai of a 2-cluster over (Ai, Aj) . X  
Distinguishing Set Computation: In the first step, we rely on the following two properties to find all possible distinguishing sets on Ai. (1) All pairs of attribute values in a distinguishing set are strongly connected; that is the distinguishing set forms a clique. (2) Any subset of a distinguishing set is also a clique (monotonic@ property). These two properties allow a level-wise clique generation similar to the candidate generation in apriori [AMS+96]. That is, we first compute all cliques of size 2, then use them to compute cliques of size 3, and so on until we compute all cliques of size less than or equal to K. 
Let Ck denote the set of all cliques of size equal to k. We give an inductive description of the procedure to generate the set Ck. The base case C X  X  when k = 2 consists of all pairs of strongly connected attribute values in Di. These pairs can easily be found from C$. The set Ck+i is computed from the set join-used in the candidate generation step of the frequent itemset computation in the apriori algorithm [AMS+96]. We also remove all the candidates in C X  X +l that contain a proper k-subset not in Ck (a la subset pruning in apriori). Extension Operation: In the second step, we  X  X xtend X  some of the candidate distinguishing sets computed in the first step to compute cluster-projections on Ai of 2-clusters on (Ai, Aj). The intuition behind the extension operation is illustrated in Figure 9. Suppose we want to extend {ui , us} on A with respect to B. We compute the set {bi, ba} of attribute values on B strongly connected with {ai, us}. We then extend {ai, ua} with the set of all other values {us, ~4) on A that is strongly connected with {bit bs}. 
Informally, the extension of a distinguishing set S c Di adds to S all attribute values in Vi that are strongly connected with the set of all attribute values in Vj that S is strongly connected with. We now introduce the concepts of sibling set, subsetflug, and the participation count to formally describe the extension operation. Definition 4.1 Let Ai and Aj be two attributes with domains Vi and Vj. Let CS( be the set of cluster-projections on A X  X  of 2-clusters over (Ai, Aj). Let VS{ be a set of candidate distinguishing sets, with respect to Aj, on attribute Ai. The sibling set 5 X ; of Si E VS{ with respect to the attribute Aj is defined as follows: I$ ] is called the sibling strength of Si with respect to Aj. The subset Jug of Si E VSf with respect to a collection of that Si c S. Otherwise, the subset flag of Si is not set. The participation count of Si E VS{ with respect to C, is the sum of the sibling strengths with respect to Aj of all supersets OfSiinC,. 
Informally, the subset flag and the participation count serve the following two purposes. First, a cluster-projection may consist of more than one distinguishing set in VSf . Therefore, if we extend each set in VS{ a particular cluster-projection may be generated several times, once for each distinguishing set it contains. To avoid the repeated generation of the same cluster-projection, we associate with each distinguishing set a subset flag. The subset flag indicates whether the distinguishing set is a subset of an existing cluster-projection in CSS. Therefore, if the subset flag is set then the distinguishing set need not be extended. For the example shown in Figure 7, the distinguishing sets {ui} and {us} on A can both be extended to the ,cluster-projection {al, ~2). Second, the distinguishing subset assumption applies only to cluster-projections of size greater than K. Therefore, a clique of size less than or equal to K may be a cluster-projection on its own even though it may be a subset of some other cluster-projection. To recognize such small cluster-projections, we associate a participation count with each distinguishing set. If the participation count of a distinguishing set with respect to CSS is less than its sibling strength then it may be a small cluster-projection. Algorithm 4.1 Extend(VSi , Cij) /* output: csi */ /* Initialization */ csi = cp Reset the subset flags and the participation counts of all distinguishing sets in VSS to zero foreach Si E VSi end /*for*/ Identify and add small cluster-projections (of size 5 K) to CS! The pseudocode for the computation of CS: is shown in Al-gorithm 4.1. Below, we describe each step in detail. Initialization: The first two steps initialize the procedure: we set CS: = 4, and the subset flags and their participation counts of all distinguishing sets in VS: to zero. Extending Si: Let 5 X : be the sibling set of Si with respect to Aj . Let Cf be the sibling set of Sf with respect to Ai. Then, we extend Si to the cluster-projection CF. Add CF to CS{ . Prune subsets of Cf : Suppose CF was extended from Si. Then, by definition, subsets of Cf cannot be the distinguish-ing sets of other cluster projections on Ai. Therefore, we set (to 1) all subset flags of subsets of Cf (including Si) in VSi. The participation count of each of these subsets is also in-creased by IS, X  J-the sibling strength of Si. 
Identifying small cluster-projections: While extending dis-tinguishing sets, we only choose sets whose subset flags are not set. We check if each unextended distinguishing set Si whose subset flag is set can be a small (of size less than K) cluster-projection. If the participation count of S; equals its sibling strength, then Si cannot be a cluster-projection on its own. Otherwise, S; may be a cluster-projection. Therefore, we add Si to CS: . requires only the inter-attribute summary Cij and the intra-attribute summary Cji. Since CQ and C$ fit into main memory, the computation is very fast. Intersection of Cluster-projections 
Informally, the intersection step computes the set of cluster-projections on Ai of clusters over {A,, . . . , A,} by succes-sively joining sets of cluster-projections on Ai of 2-clusters over attribute pairs (A;, Aj), j # i. For describing the proce-dure, we require the following definition. Definition 4.2 Let Si and Ss be two collections of sets of attribute values on Ai. We define the intersection join S1 n S2 between &amp; and Sa as follows: St fl Sa dgf 
Let CS{ be the set of cluster-projections on Ai with respect to Aj, j # i. Let ji = 1 if i &gt; 1, else jr = 2. Starting with S = CS: X , the intersection step executes the following operation for all k # i. 
The resulting set S is the set of cluster-projections on Ai of clusters over {Al, . . . , A,}. Besides being a main-memory operation, the number of cluster-projections on Ai with re-spect to any other attribute Aj is usually small; therefore, the intersection step is quite fast. 
Further optimizations are possible over the basic strategy described above for computing cluster projections. For instance, we can combine the computation of C$ and that of CS: because, for each cluster-projection in CS: , we compute its sibling set which is a cluster-projection in CS:. However, we do not consider such optimizations because the clustering phase takes a small fraction (less than 10%) of the time taken by the summarization phase. (Our experiments in Section 6 confirm this observation.) 4.2.2 Level-wise Synthesis of Clusters In this section, we describe the synthesis of candidate clus-ters from the cluster-projections on individual attributes (com-puted as described in Section 4.2). The central idea is that a cluster on a set of attributes induces a sub-cluster on any sub-set of the attributes (monotonicity property). The monotonic-ity property follows directly from the definition of a cluster. We also exploit the fact that we want to compute clusters over the set of all attributes {Al, . . . , A,}. Informally, we start with cluster-projections on AI and then extend them to clus-ters over (Al, AZ), then to clusters over (Al, AZ, As), and so on. 
Let Ci be the set of cluster-projections on the attribute Ai, i = l,...,n. Let C X  denote the set of candidate clusters defined over the set of attributes Al, . . . , Ak. Therefore, Cl = Ci. We successively generate C X +l from C X  X  until C X  is generated or C X +l is empty for some lc + 1 &lt; 12. The generation of C k+1 from C X  proceeds as follows. Set c X +l = 4. For each element ck = (cl,. . . , ck) E c X , we attempt to augment ck with a cluster projection ck+l on the cluster on (Ai, A k+l)-which can be checked by looking up &amp;(k+l)-we augment ck to ck+ X  = (cl,. . . , ck+l) and add Ck+l to c X +1. of candidate clusters proceeds as follows. We start with the set {ai, aa} on A. We then find the candidate 2-cluster {({%,az), @l,b2))) over the attribute pair (A, B), and then the candidate 3-cluster {({al, aa}, { bi , bz }, {cl, CZ}) } over {A, B, C). 4.3 Validation 
We now describe a procedure to compute the set of actual clusters from the set of candidate clusters. Some of the candidate clusters may not have enough support because some of the 2-clusters that combine to form a candidate cluster may be due to different sets of tuples. To recognize such false candidates, we check if the support of each candidate cluster is greater than the required threshold. Only clusters whose support on D passes the threshold requirement are retained. we start scanning the dataset D. For each tuple t E D, we increment the support of the candidate cluster to which t belongs. (Because the set of clusters correspond to disjoint interval regions, t can belong to at most one cluster.) At the end of the scan, we delete all candidate clusters whose support in the dataset D is less than the required threshold: (Y times the expected support of the cluster under the attribute-independence assumption. our cluster definition, and hence the following theorem holds. 
Theorem 4.1 Given that the distinguishing subset assump-tion holds, CACTUS finds all and only those clusters that sat-isfy Definition 3.3. 
In this section, we extend CACTUS to handle unusually large attribute value domains as well as to identify clusters in subspaces. 80 5.1 Large Attribute Value Domains 
Until now, we assumed that the domains of categorical attributes are such that the inter-attribute summary of any pair of attributes and the intra-attribute summary of any attribute fits in main memory. For the sake of completeness, we modify the summarization phase of CACTUS to handle arbitrarily large domain sizes. 
Recall that the summary information only consists of strongly connected pairs of attribute values. For large domain sizes, the number of strongly connected attribute value pairs (either from the same or from different attributes) relative to the number of all possible attribute value pairs is very small. We exploit this observation to collapse sets of attribute values on each attribute into a single attribute value thus creating a new domain of smaller size. The intuition is that if a pair of attribute values in the original domain are strongly connected, then the corresponding pair of transformed attribute values are also strongly connected, provided the threshold for strong connectivity between attribute values in the transformed domain is the same as that for the original domain. Let Ai be an attribute with an unusually large domain Di. M &lt; IDil be the maximum number of attribute values per attribute so that the inter-attribute summaries and the intra-attribute summaries fit into main memory. Let c = [%I. We construct Do: of size M from Vi by mapping for a given 2 E (0,. . . ,M-l},thesetofattributevalues{a.c+l,...,a. c + c} to the value z + 1. Formally, We set the threshold for the strong connectivity involving attribute values in DoI as if Vi was being used. We then compute the inter-attribute summaries involving Ai using the transformed domain Vi. For each attribute value ai E that participates in a strongly connected pair (ai,aj) (aj E 
Vj, 3 # z), we expand ai to the set of all attribute values thepairs(ai.c+l,aj),.. .,(ai.c+c,aj). Wethenscanthe dataset D to count the supports of all these pairs, and select the strongly connected pairs among them; they constitute the inter-attribute summary Cij. 
The number of new pairs whose supports are to be counted is less than or equal to c . (Cij ( where (Cij 1 represents the number of strongly connected pairs in Vi x Vj. If this set of pairs is still larger than main memory, we can repeat the above transformation trick. However, we believe that such repeated application will be rare. 5.2 Clusters in Subspaces 
CACTUS does not discover clusters in subspaces for the following reason. The order Al, . . . , A, in which cluster-projections on individual attributes are combined may not be the right order to find a subspace cluster C. For instance, if C spans the subspace defined by a set of attributes {As, As, A4} (when n &gt; 4) then the level-wise synthesis described in Section 4.2.2 will not find C. 
The extension to find subspace clusters exploits the mono-tonicity property of subspace clusters. That is, a cluster in a subspace S induces a subcluster on any subset of S. 
The monotonicity property again motivates the apriori-style level-wise synthesis of candidate clusters from the cluster-projections on individual attributes. The algorithm differs in two ways from the algorithm to find clusters over all attributes. 
The first difference is that we do not restrict that a cluster-projection on an attribute should participate in Z-cluster with  X  X very other attribute. The second difference is in the proce-dure for generating the set of candidate clusters. We now dis-cuss both differences. 
We skip the intersection of cluster-projections on each attribute Ai with respect to every other attribute Aj (j # i) for the following two reasons. First, a cluster in subspace 
S may not induce a 2-cluster on a pair of attributes not in S, and hence the intersection of cluster-projections on an attribute in S with respect to every other attribute may return an empty set. Second, the intersection may cause the loss of maximality (condition (2) in Definition 3.3) of a subspace cluster. For instance, a cluster-projection on Ai with respect to Aj corresponds to a a-cluster over (A;, Aj) which, by definition, is a subspace cluster; truncating such a cluster-projection in the intersection step will no longer yield a maximal cluster on (Ai, Aj). 
In the candidate generation algorithm, we let C X  denote the set of candidate clusters defined on any set of k-attributes (not necessarily {Al, . . . , Ak}). Otherwise, the candidate generation proceeds exactly as in Section 4.2.2. The reason is that a subspace cluster on k attributes may not always be in the first k attributes. 
For a cluster c E C X  in a subspace consisting of k attributes, the above candidate generation procedure examines 2 X  -(k + 1) candidates. Depending on the value of k (say, larger than 15), the number of candidate clusters can be prohibitively high. The problem of examining a large number of candidate clusters has been addressed by Agrawal et al. [AGGR98]. 
They use the minimum description length principle to prune the number of candidate clusters. Their techniques apply directly in our scenario as well. Therefore, we do not address this problem; instead, we refer the reader to the original paper [AGGR98]. 
In this section, we show the results of a detailed evaluation of the speed and scalability of CACTUS on synthetic and real datasets. We also compared the performance of CAC-TUS with the performance of STIRR. X  Our results show that 
CACTUS is very fast and scalable; it outperforms STIRR by a factor between 3 and 10. gWe intend to compare CACTUS and ROCK after our ongoing implemen-tation of ROCK is complete. 6.1 Synthetic Datasets We first present our experiments on synthetic datasets. The test datasets were generated using the data generator devel-oped by Gibson et al. [GKR98] to evaluate STIRR. (See Sec-tion 3.2 for a description of the data generator.) We set the number of tuples to 1 million, the number of attributes to 10 and the number of attribute values for each attribute to 100. 
In all datasets, the cluster-projections on each attribute were [0,9] and [lo, 191 ( as shown in Figure 1). We fix the value of (Y at 3, and the value of the distinguishing number K at 2. For STIRR, we fixed the number of iterations to be lo-as suggested by Gibson et al. [GKR98]. 
CACTUS discovered the clusters in the input datasets shown in Figures 1,2, and 3. 
Figure 10 plots the running time while increasing the number of tuples from 1 to 5 million. Figure 11 plots the running time while increasing the number of attributes from 4 to 50. Figure 12 plots the running time while increasing the number of attribute values from 50 to 1000 while fixing the number of attributes at 4. While varying the number of attribute values, we assumed that until 500 attribute values, the inter-attribute summaries would fit into main memory; for a larger number of attribute values we took the multi-layered approach described in Section 5. In all cases, CACTUS is 3 to 10 times faster than STIRR. 6.2 Real Datasets 
In this section, we discuss an application of CACTUS to a combination of two sets of bibliographic entries. The results from the application show that CACTUS finds intuitively meaningful clusters from the dataset thus supporting our definition of a cluster. cles related to database research [Wie] and the second set con-sists of 30919 bibliographic entries for articles related to The-oretical Computer Science and related areas [Sei]. For each article, we use the following four attributes: the first author, the second author, the conference or the journal of publication, and the year. If an article is singly-authored then the author X  X  name is repeated in the second author attribute as well. The sizes of the first author, the second author, the conference, and the year attribute domains for the database-related, the theory-related, and the combined sets are {3418,3529,1631,44}, {8043,8190,690,42}, and { 10212,10527,2315,52} respec-tively. We combined the two sets together to check if CAC-
TUS is able to identify the differences and the overlap be-tween the two communities. Note that for these domains, some of the inter-attribute summaries and the intra-attribute summaries-especially those involving the first author and the second author dimensions-do not fit in main memory. HOW-ever, we choose this particular dataset because it is easier to verify the validity of the resulting clusters (than for some other 82 publicly available datasets, e.g., the MUSHROOM dataset from the UC1 Machine Learning repository). 
Table 5.1 shows some of the 2-clusters on the first au-thor and the second author attribute pair. We only present the database-related cluster-projections to illustrate that CAC-TUS identifies the differences between the two communities. 
We verified the validity of each cluster-projection by querying on the Database Systems and Logic Programming bibliogra-phy at the web site maintained by Michael Ley [Ley]. Sim-ilar cluster-projections identifying groups of theory-related researchers as well as groups that contribute to both fields also exist. Due to space constraints, we show some cluster-projections corresponding to the latter two types in the full paper [GGR99]. 
Table 2 shows some of the cluster-projections on the con-ference attribute computed with respect to the first author attribute. The first row consists exclusively of a group of database-related conferences, the second consists exclusively of theory-related conferences, and the third a mixture of both reflecting a considerable overlap between the two communi-ties. 
In this paper, we formalized the definition of a cluster when the data consists of categorical attributes, and then introduced a fast summarization-based algorithm CACTUS for discover-ing such clusters in categorical data. We then evaluated our algorithm against both synthetic and real datasets. 
In future, we intend to extend CACTUS in the following three directions. First, we intend to relax the cluster definition by allowing sets of attribute values on each attribute which are  X  X lmost X  strongly connected to each other. Second, motivated by the observation that inter-attribute summaries can be in-crementally maintained under addition and deletion of tuples, we intend to derive an incremental clustering algorithm from 
CACTUS. Third, we intend to  X  X ank X  the clusters based on a measure of interestingness, say, some function of the support of a cluster. 
AcknowIedgements: We thank Prabhakar Raghavan for sending us the bibliographic data. [AGGR98] Rakesh Agrawal, Johannes Gehrke, Dimitrios Gunopu-[AMS+96] [BD76] [CBM98] [DER86] [ DLR77] [ GGR99] [GJ79] [GKR98] [GRS99] [Ram971 [Sei] [Wie] 
