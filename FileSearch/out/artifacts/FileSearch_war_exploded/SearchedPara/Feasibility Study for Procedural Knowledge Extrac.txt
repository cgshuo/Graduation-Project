 Technology Intelligence is an activity helping companies or organizations to make better decisions by gathering and providing information about the state-of-the-art technologies [1]. Recently, the systems supporting Technology Intelligence have been actively developed to assist researchers and practitioners to make strategic technology plans [2]. Usually, these systems import text mining methodologies to analyze tacit information inside company or on the Internet. However, they focused on extracting declarative knowledge , which describes objects and events by specifying the properties which characterize them; it does not pay attention to extract the actions needed to obtain a result, but only on its properties [3]. Therefore, we propose a methodology that enables to build procedural knowledge using text mining technique based on deep language processing. In general, procedural knowledge has been contrasted with propositional knowledge or declarative knowledge . Even though two kinds of knowledge have been defined differently in different domain, Sahdra and Thagard [4] summarized them as shown in Table 1. 
If we could build the Knowledge-how information from documents, there could be a lot of application analyzing such highly organized procedural knowledge. As an example, the procedural knowledge in th e biomedical domain enables doctors or conveniently. So, they can improve the quality of medication services as well as technology enhancements. Moreover, it is also beneficial to the policy makers in governments or companies on building new plans preparing for the upcoming highly diversified world. We explain related work in section 2 and describe how to model and extract the procedural knowledge in PubMed 1 abstracts in section 3. Section 4 shows two major experiments; purpose/solution sentence classification and unit procedure identification. The results on ho w to extract procedural knowledge using text mining methodologies are followed at section 5. At last, we summarize and conclude in section 6. A lot of research on extracting information like terminology, entity, and concept using various resources such as dictionary, thesaurus, or ontology has been published continuously until now [5-7]. The research on relation or event extraction between them also has been popular these days . However, those works have been focusing on knowledge-that instead of knowledge-how. Even though Jung, et al. [8] extracted procedural knowledge and built ontology from the web documents like eHow 2 and wikiHow 3 , their target documents are already structured (listed) in a bulleted sequential form. For an example in wikiHow, there is an article labeled  X  X ow to Celebrate National Egg Month X . It contains 6 sequential instructions which are imperative sentences. From the article, he extracted sequential actions and built wiki-authors. In addition, parsing the sentences is straightforward since almost of them are simple sentences rather than compound or complex sentences. 3.1 Modeling We modeled the procedural knowledge which is structured to solve a specific purpose or goal. That is, procedural knowledge in a document consists of a set of unit procedures and each unit has a common purpose to be resolved by the procedures. So, the target document could be represented as a pair of purpose and its solution(s) Fig. 1. 
As depicted in Fig. 1, we defined procedural knowledge as a combination of a purpose and a corresponding solution. And the solution consists of one or more unit procedures having relationships with each other. The unit procedure is a triple combination of Target , Method , and Action ; Target is defined as diseases, symptoms, objects, organs, and so on. Method is treatments, operations, medications, etc. Action is a predicate part connecting or relating Target with Method to explain how to apply with medical doctors who have supported us because of their professional knowledge in the medical domain and being one of the best benefit recipients from this research. 3.2 Extraction Procedures According to the model constructed in subsection 3.1, we designed how the procedural knowledge could be extracted. The major four steps to build procedural knowledge are as semantic features using various natural language processing techniques; such POS tagging, syntactic parsing, predicate-argument structure tagging, and ontology based terminology identification. 2) Classifying the purpose and solution sentences among the entire sentences belonging to a document. The features gathered in the step 1 are supplied categories (purpose/solution/other). 3) Identifying the unit procedures in each purpose or Target/Action/Method. Unit procedure must have at least two entities except that in sentence. 4) Assigning the relationship between two unit processes. The relationship could be sequential, parallel, casual, etc. 3.3 Target Documents In this paper, the target document for proced ural knowledge is confined to the areas of Gastric Cancer and Spinal Disease by the help of medical doctors having been working together. That X  X  because those diseases have more probability of sentences containing appropriate procedural knowledge as well as they are popular and familiar topics people are interested in. Most of review papers or case study papers popular in the biomedical domain are not appropriate for procedural knowledge extraction since they do not include experiments or methodologies which contain procedural information. The target document is semantically divided into several blocks by authors on submitting their papers. The blocks are classified as OBJECTIVE, BACKGROUND, METHODS, RESULTS, and CONCLUSIONS in general. Sometimes, one or more block are omitted or merged. By the way, we are focusing on classifying purpose and solution parts which include two or more out of the three [10] because we only select the sentences possibly containing one or more entities (Target, Action, and Method). In general, the solution part consists of one or more methodological sentences. 3.4 Training Corpus We developed a training corpus for extracting procedural knowledge by the help of two medical doctors. Total 1309 documents are tagged with purpose/solution labels which contain one or more unit processes (Triple: Target, Action, and Method). In addition, the Spinal Disease (949 documents) and Gastric Cancer (360 documents). The experiments are divided into two parts; purpose/solution classification, unit process identification. The former classifies sentences into one of three classes: purpose, solution, purpose/solution sentences. For preparing the two experiments, several text mining techniques are applied to the target documents explained in the next subsection. 4.1 Preprocessing The target documents are preprocessed with Part Of Speech (POS) Tagging, Syntactic Parsing, Predicate-Argument Structure Tagging, and Ontology Mapping. The POS tagging has been applied using Enju parser 4 . Predicate-argument structure [11] is applied, which is a representation of the meaningful relationships of words in a sentence according to the relation between pr edicate and its arguments. At last, the ontology mapping for terminology identification is added. The terms corresponding to ontology item in UMLS 5 , UniProt 6 , or GO(Gene Ontology) 7 are marked. This deep processed information is utilized on training or testing the machine learning based algorithms explained in subsection 4.2. 4.2 Purpose/Solution Sent ence Classification Extracting purpose/solution sentences from an abstract could be regarded as a classification problem selecting one category out of three categories such as purpose, solution, and other. For this task, we utilized two machine learning approaches, Support Vector Machines (SVMs) 8 and Conditional Random Fields (CRFs) 9 . The reason why we applied CRFs, frequently used in sequence labeling problem, in addition to the SVMs is that the order of the semantic blocks in abstract are sequential. 
The features for this experiment consist of four kinds of items. 1) Content features: unigrams and bigrams in target sentence. Stemming and Stopwords elimination are applied. 2) Position features: sentence number of target sentence in the abstract. The purpose sentence tends to be located at the first few sentences and the solution sentences are rather later part of the abstract. 3) Neighbor features: content features of previous and next k sentences of the target sentence. 4) Ontological features: ontology terms in the UMLS, UniProt, and GO. 4.3 TAM Identification This experiment is to extract the three entities as Target, Action, and Method (abbreviated as TAM) using CRFs algorithm with 4 kinds of features as follows. 1) Word features: word, word lemma, POS tag, whether first character is capital or not, whether all characters are capital or not. 2) Context features: words and POS tags of previous and next k words of the target word. 3) Predicate-argument structure: predicate type and its argument words an d POS tags. 4) Ontological features: ontology terms in the UMLS, UniProt, and GO. This task is to find the boundary of the word or phrase that is recognized as Target, Action, or Method. Therefore, we used most widespread representation so-called IOB tags for chunking of each entity. The B and I tags are suffixed with the entity type, e.g. B-Target, I-Target, B-Action, I-Action, B-Method, and I-Method. Of course, it is not necessary to specify a chunk type for tokens that appear outside an entity, so these are just labeled O. An example of this scheme is shown in Fig. 2. 4.4 TAM Association In previous sub-section, we identified all entities in each sentence. However, a sentence may contain one or more TAM triples. In some cases, one or more entities between two TAMs could be shared. In an example sentence,  X  X 1 and T2 was A1 by etc. could be extracted. So, the entities in a sentence should be engaged with each other in a way that each action tries to c onnect to each Target or Method and decides whether it is appropriate to be a Target or Method of the Action, as depicted in Fig. 3. There are one Target, one Action, and two Methods in the example sentence. Based electrograms X ,  X  X nalyzed X ,  X  X requency-domain method X &gt;. This task could be regarded as binary classification at the Action entity since it is a decision problem of whether each link from Action to either Target or Method is appropriate or not. 
Therefore, we applied SVMs binary clas sification algorithm so as to decide whether each link from Action to Target (or Method) is feasible or not, using the following three features. 1) Position features: position (previous: negative value, next: positive value) of Target (or Method) from Action. 2) Context features: words and POS tags between two entities. 3) Predicate-argument structure: predicate type and predicate word between two entities, whether the Target (or Method) is related through predicate-argument structure. The Target or Method entity could be absent while the Action should exist. So, the triple &lt;Target, Action, &gt; or &lt;, Action, Method&gt; is possible, but &lt;Target, , Method&gt; is not. 4.5 Relation Extraction After associating TAMs to identify a unit pr ocess in a sentence, we need to associate each TAM to the other based on their relationships. In Fig. 3, there are two unit processes consisting of a triple entity, TAM, as described in subsection 4.4. The two processes have parallel relationship with each other because the two methods (CFE and DF) are carried out separately and in parallel, according to the example sentence. Fig. 4 shows an additional example of a sequential relation. There are three unit processes and they are engaged in sequen tial manner according to the clue words,  X  first X  ,  X  then X  , and  X  finally X  . 
This task could be thought of as a multi-class classification because we have to find one among several relationships between the pair of unit processes in a sentence or adjacent (previous and next) sentences, using various clue words. For this task, we defined only two relationships such as sequential and parallel relationship as a feasibility task. And we utilized SVMs for this binary classification task using the following three feature groups. 1) Position features: position (previous, next) of the features: words and POS tags between the processes, POS tag of Action word in the &lt;T,A,M&gt;. 3) Predicate-argument structure: predicate type and predicate word related to the two processes. 5.1 Results on Purpose/Solution Sentence Classification and SVMs methods are applied to train purpose/solution sentence classification models. The F-1 score of purpose sentence classification using CRFs achieved 85% while it is relatively low (69%) in solution sentence classification. The reason why the performance is rather bad in solution sentence classification is that there are quite a few sentences that have not at least two entities out of the TAM, even though the sentence sequences of the abstract affect the performance in assigning categories of the sentences. 
However, the result using SVMs is quite promising since the F-1 scores of the two tasks are 87% and 80% respectively in Table 2. Recall that the model for this experiment is not only for a sentence classification but also for checking whether the sentence contains TAM or not. Actually, some sentences in METHODS block could not be assigned to solution category because they only have at most one component of TAM. So it is rather different from the ge neral sentence classification [9], [10] which performs over 0.90 in their F-1 scores. The both machine learning methods show in common that performance on purpose is better than that on solution because of the consistency in writing the purpose sentences. Usually,  X  to ~  X ,  X  the aim of this study ~  X , and  X  the goal is ~  X  are the sentence patterns frequently observed in purpose sentences, while it is hard to find the common pattern in solution sentences. 5.2 Results on TAM Identification (leave-two-out method) and only the CRFs method is applied to train TAM identification model. As we mentioned previously, the performance below does not include partial matching in multi-word entities since most of the medical terms are very sensitive in the semantic perspective according to medical experts. For example, the substring such as  X  cooperative ataxia rating scale  X ,  X  ataxia rating scale  X , or cooperative ataxia rating scale  X , shown in subsection 3.3. The result on Action entity shows high compared to the other two becau se the number of words in Action entity is at most 2-3 and the main word is verb or verb equivalent. On the contrary, Target and Method entities are large in their length and they contain relatively more adverbs/adjectives as well as composite nouns. 5.3 Results on TAM Association and Relation Extraction We also used leave-two-out method with SVMs classification method and got a result like Table 4. According to the result, the performance of Action-Target Association is superior to Action-Method. It X  X  because the syntactic variation in sentence of the Method components is much complicated than that of the Target which is usually located in the beginning part of sentence. Additionally, we also performed relation identification experiment using SVMs classification method with leave-two-out method. We only focused on two relations (Sequential and Parallel) as a feasibility task. The parallel relation identification is better than the sequential one since its clue is more direct that the sequential clue because the parallel clues are  X  X oth X ,  X  X nd X ,  X  X s well as X ,  X  X t the same time X , and so on. We proposed a procedural knowledge modeling and extraction method for Technology Intelligence based on machine learning approaches with deep language processing analysis. The experiments showed that the proposed approach is quite promising because it shows 63%~82% in each step of the procedural knowledge extraction steps, even though we applied strict guidelines in evaluating the performance. In addition, we built a handcrafted valuable training corpus with two medical doctors, which have 1309 PubMed abstracts categorized into 8 diseases from both gastric cancer and spinal disease. For future work, we plan to apply the approach to the full-text of documents such as papers, patents, and/or reports. Acknowledgments. This research was partially supported by the MKE(The Ministry of Knowledge Economy), Korea, under the ITRC(Information Technology Research Center) support program supervised by the NIPA(National IT Industry Promotion Agency) (NIPA-2011-C1090-1111-0008). 
