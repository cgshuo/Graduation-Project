 Google Research University of Cambridge University of Cambridge University of Cambridge Google Research translation and alignment under a synchronous context-fre e grammar. We use PDAs to com-performance for large-scale SMT . 1. Introduction
Synchronous context-free grammars (SCFGs) are now widely u sed in statistical machine translation, with Hiero as the preeminent example (Chiang 2 007). Given an SCFG and an n -gram language model, the challenge is to decode with them, that is, to apply them to source text to generate a target translation.
 terms of the formal languages and relations involved. We wil l use this description to introduce and analyze pushdown automata (PDAs) for machi ne translation. This formal description will allow close comparison of PDAs to ex isting decoders which are based on other forms of automata. Decoding can be described i n terms of the following steps: 1. Translation: T =  X  2 ( { s } X  G ) 2. Language Model Application: L = T  X  M 3. Search:  X  l = argmax l  X  L L The composition { s } X  G in Step 1 that generates T can be performed by a modified
CYK algorithm (Chiang 2007). Our interest is in the differen t types of automata that can be used to represent T as it is produced by this composition. We focus on three types of representations: hypergraphs (Chiang 2007), weighted fi nite state automata (Iglesias et al. 2009a; de Gispert et al. 2010), and PDAs. We will give a f ormal definition of PDAs in Section 2, but we will first illustrate and compare these di fferent representations by a simple example.

Step 1 yields the translations T = {  X  t 1 t 2 t 2 t 3 t 4 examples of the different representations of these transla tions. We summarize the salient features of these representations as they are used in decodi ng.

Hypergraphs. As described by Chiang (2007), a Hiero decoder can generate t ranslations 688
Weighted Finite State Automata (WFSAs). Because T is a regular language and M is
Pushdown Automata. Like WFSAs, PDAs are easily generated from RTNs, as will be of several existing and novel translation algorithms. We no te that PDAs have long been used to describe parsing algorithms (Aho and Ullman 197 2; Lang 1974), and it is well known that pushdown transducers , the extended version of PDA with input and output labels in each transition, do not have the expressive power needed to generate synchronous context-free languages. For this reason, we do not use PDAs to implement
Step 1 in decoding: throughout this article a CYK-like parsi ng algorithm is always used for Step 1. However, we do use PDAs to represent the regular la nguages produced in
Step 1 and in the intersection and shortest distance operati ons needed for Steps 2 and 3. 1.1 HiPDT: Hierarchical Phrase-Based Translation with PDA s
We introduce HiPDT, a hierarchical phrase-based decoder th at uses a PDA representa-tion for the target language. The architecture of the system is shown in Figure 2, where 690 we contrast it with HiFST (de Gispert et al. 2010). Both decod ers parse the sentence with a grammar G using a modified version of the CYK algorithm to generate the t ranslation search space as an RTN. Each decoder then follows a different path: HiFST expands the RTN into an FSA, intersects it with the language model, an d then prunes the result;
HiPDT performs the following steps: expansion is performed. In HiFST, the RTN representation is immediately expanded to an FSA. In HiPDT, the PDA pruned expansion or shortest path co mputation is done after the language model is applied, so that all computation is done with respect to both the translation and language model scores.
 the development history of our FST and SMT systems. RTN algor ithms were available in OpenFST at the time HiFST was developed. HiPDT was develop ed as an extension to HiFST using PDA algorithms, and these have subsequently bee n included in OpenFST. A possible alternative approach could be to produce a PDA dir ectly by traversing the
CYK grid. WFSAs could then be generated by PDA expansion, wit h a computational complexity in speed and memory usage similar to the RTN-base d approach. We present
RTNs as the initial translation representation because the generation of RTNs during parsing is straightforward and has been previously present ed (de Gispert et al. 2010).
We note, however, that RTN composition is algorithmically m ore complex than PDA (and FSA) composition, so that RTNs themselves are not ideal representations of T if a language model is to be applied. Composition of PDAs with FSA s will be discussed in Section 3.3.

HiPDT and HiFST both benefit from the compactness offered by W FSA epsilon removal, determinization, and minimization operations. When appli ed to PDAs, these operations treat parentheses as regular symbols. Compact representat ions of RTNs are shared by both approaches. Figure 4 illustrates the PDA representati on of the translation space under a slightly more complex grammar that includes rules wi th alternative orderings of nonterminals. The rule S  X  X  X 1 s 2 X 2 , t 1 X 1 X 2 i produces the sequence  X  t and S  X  X  X 1 s 2 X 2 , t 2 X 2 X 1 i produces  X  t 2 t 5 t alternative orderings of the phrases  X  t 3 t 4  X  and  X  t 5 source-to-target alignment, or synchronous parsing, unde r the SCFG in a two-step composition rather than one synchronous parsing stage. For example, by using M as the and de Gispert et al. (2010) for the RTN/FSA and in Dyer (2010b ) for hypergraphs. In
Section 4 we analyze how PDAs can be used for alignment. 1.2 Goals We summarize here the aims of this article.

We will show how PDAs can be used as compact representations o f the space T X  X  X  s 1 , t 3 t 4 i X  X  X  s 3 , t 5 t 6 i S  X  X  X 1 s 2 X 2 , t 1 X 1 X 2 i S  X  X  X 1 s 2 X 2 , t 2 X 2 X 1 i 692
We will show both theoretically and experimentally that the PDA representation is
We will propose a two-pass translation decoding strategy fo r HiPDT based on 2. Pushdown Automata
Informally, pushdown transducers are finite-state transdu cers that have been aug-mented with a stack. Typically this is done by adding a stack a lphabet and labeling each transition with a stack operation (a stack symbol to be p ushed onto, popped, or read from the stack) in addition to the usual input and output labels (Aho and Ullman 1972; Berstel 1979) and weight (Kuich and Salomaa 1986; Petr e and Salomaa 2009). Our equivalent representation allows a transition to be labele d by a stack operation or a regular input/output symbol, but not both. Stack operation s are represented by pairs of open and close parentheses (pushing a symbol on and poppin g it from the stack).
The advantage of this representation is that it is identical to the finite automaton repre-sentation except that certain symbols (the parentheses) ha ve special semantics. As such, several finite-state algorithms either immediately genera lize to this PDA representation or do so with minimal changes. In this section we formally defi ne pushdown automata and transducers. 2.1 Definitions
A (restricted) Dyck language consist of  X  X ell-formed X  or  X  X  alanced X  strings over a
Dyck language over three pairs of parentheses (see Berstel 1 979 for a more detailed presentation).
 f from A to A . Intuitively, f maps an open parenthesis to its corresponding close over the alphabet b A = A  X  A is then the language defined by the following context-free grammar: S  X   X  , S  X  SS and S  X  aS  X  a for all a  X  A . We define the mapping c mapping r B : A  X   X  B  X  by r B ( x 1 . . . x n ) = y 1 . . . y of values K , two binary operations  X  and  X  , and two designated values 0 and 1.
The operation  X  is associative, commutative, and has 0 as identity. The operation  X  is associative, has identity 1, distributes with respect to  X  , and has 0 as annihilator: for all a  X  K , a  X  0 = 0  X  a = 0. If  X  is also commutative, we say that the semiring is commutative .
 ring via the negative-log mapping, is often used in practice for numerical stability. The tropical semiring ( R  X  X  X  X  , min, + ,  X  , 0) is derived from the log semiring using the Viterbi approximation . These three semirings are commutative.
 open and close parenthesis alphabets, Q is a finite set of states, I  X  Q the initial state, with weight  X  1). 694 n . We then define p [  X  ] = p [ e 1 ], n [  X  ] = n [ e n w [ e n ]. A path  X  is accepting if p [  X  ] = I and n [  X  ]  X  F . A path  X  is balanced if r
A balanced path  X  accepts the string x  X   X   X  if it is a balanced accepting path such that r ( i [  X  ]) = x .
 where P ( x ) denotes the set of balanced paths accepting x . A weighted language is recognizable by a weighted pushdown automaton iff it is cont ext-free. We define the size of T as | T | = | Q | + | E | .
 such that c  X  ( r b  X  ( i [  X  ]))  X   X   X  : In other words, the number of open parentheses that are not cl osed along  X  is bounded.
If T has a bounded stack, then it represents a regular language. F igure 5 shows non-regular, regular, and bounded-stack PDAs. A weighted finite automaton (FSA) can be viewed as a PDA where the open and close parentheses alphabet s are empty (see Mohri 2009 for a stand-alone definition).
 output alphabet,  X  and  X  are the finite open and close parenthesis alphabets, Q is a finite set of states, I  X  Q the initial state, F  X  Q the set of final states, E  X  Q  X  (  X   X  {  X  } )  X  K  X  Q a finite set of transitions, and  X  : F  X  K the final weight function. Let following presentation focuses on acceptors, rather than t he more general case of trans-ducers. This is adequate for the translation applications w e describe, with the exception of the treatment of alignment in Section 4.3, for which the in tersection algorithm for
PDTs and FSTs is given in Appendix A. 3. PDT Operations
In this section we describe in detail the following PDA algor ithms: Replacement , Com-position , Shortest Path , and (Pruned) Expansion . Although these are needed to implement
HiPDT, these are general purpose algorithms, and suitable f or many other applications outside the focus of this article. The algorithms described in this section have been implemented in the PDT extension (Allauzen and Riley 2011) o f the OpenFst library (Allauzen et al. 2007). In this section, in order to simplify the presentation we will only consider machines over the tropical semiring ( R +  X  X  X  X  , min, + ,  X  , 0). However, for each operation, we will specify in which semirings it can be a pplied. 3.1 Recursive Transition Networks
We briefly give formal definitions for RTNs that will be needed to present the RTN expansion operation. Examples are shown earlier in Figures 1(b) and 3(a). Informally, an RTN is an automaton where some labels, nonterminals, are r ecursively replaced by other automata. We give the formal definition for acceptor s; the extension to RTN transducers is straightforward.
 bet, ( T  X  )  X   X  N is a family of FSTs with input alphabet  X   X  N , and S  X  N is the root nonterminal.
 that  X  =  X  1 e 1 . . . X  n e n  X  n + 1 with i [  X  k ]  X   X   X  x such that x k is accepted by ( R , i [ e k ]) and x = i [  X  to x can be defined in the same recursive manner.
 of Figure 6 and the sequence x = a a b . The path in the automata T  X  =  X  1 e 1  X  2 , with i [  X  1 ] = a , i [ e 1 ] = X 1 , and i [  X  3.2 Replacement
This algorithm converts an RTN into a PDA. As explained in Sec tion 1.1, this PDT operation is applied by the HiPDT decoder in Step 1, and examp les are given in earlier sections (e.g., in figures 1 and 3).
 terminal. The source and destination states of these transi tions are used to define the matched opening and closing parentheses, respectively, in the new PDA. Each RTN nonterminal transition is deleted and replaced by two new tr ansitions that lead to and 696 from the automaton indicated by the nonterminal. These new t ransitions have matched parentheses, taken from the source and destination states o f the RTN transition they replace. Figure 6 gives a simple example.
 T equivalent to R defined by the 8-tuple (  X  ,  X  ,  X  , Q , E , I , F ,  X  ) with Q =  X  = I = I S , F = F S ,  X  =  X  S , and E = with = i [ e ]  X  N otherwise.
 | T | = O ( whose size is always linear in the size of R . In this article, we assume this optimization is always performed. We note here that RTNs can be defined and t he replacement operation can be applied in any semiring. 3.3 Composition
Once we have created the PDA with translation scores, Step 2 i n Section 1.1 applies the language model scores to the translation space. This is done by composition with an FSA containing the relevant language model weights.
 weighted finite-state transducers (Bar-Hillel, Perles, an d Shamir 1964; Nederhof and
Satta 2003). OpenFST supports composition between automat a T is a weighted pushdown transducer and T 2 is a weighted finite-state transducer. If both T 1 and T 2 are acceptors, rather than transducers, the composition of a PDA and an FSA produces a PDA containing their intersection, and so n o separate intersection algorithm is required for these automata. Given this, we des cribe only the simpler, special case of intersection between a PDA and an FSA, as this is sufficient for most of the translation applications described in this article. The alignment experiments of
RTN R R accepts a a b and a b b .
 notation of Section 2.1, in this example  X  = { 3, 5 } and  X  = {
Section 4.3 do require composition of transducers; the algo rithm for composition of transducers is given in Appendix A.
 in Figure 8. These correspond to all paths through T 1 and T a synchronized reading of strings from { a , b }  X  . The algorithm is very similar to the composition algorithm for finite-state transducers, the di fference being the handling of the parentheses. The parenthesis-labeled transitions a re treated similarly to epsilon transitions, but the parenthesis labels are preserved in th e result. This adds many unbalanced paths to T . In this example, T has five paths but only one balanced path, so that T accepts the string a a b b .
 where T = T 1  X  T 2 as follows: 1. The new state space is in the product of the input state spac es: Q  X  Q 2. The new initial and final states are I = ( I 1 , I 2 ), and F = { ( q 3. Weights are assigned to final states ( q 1 , q 2 )  X  Q as  X  ( q 4. For pairs of transitions ( q 1 , a 1 , w 1 , q  X  1 )  X  E 698 When T 2 has input- X  transitions, an epsilon filter (Mohri 2009; Allauzen, Riley , and
Schalkwyk 2011) generalized to handle parentheses can be us ed. Note that Steps 1 and 2 from the initial state and needed in Step 4 are actually gener ated. The complexity of
Composition requires the semiring to be commutative. 3.4 Shortest Distance and Path Algorithms
With a PDA including both translation and language model wei ghts, HiPDT can ex-tract the best translation (Step 3a in Section 1.1). To this e nd, a general PDA shortest distance/path algorithm is needed.
 and the shortest distance in T is the weight of such a path. We show that when T has a bounded stack, shortest distance and shortest path can be computed in
O ( | T | 3 log | T | ) time (assuming T has no negative weights) and O ( | T | gives a pseudo-code description of the shortest-distance a lgorithm, which we now discuss.
 the shortest distance from the start state I to the final state recursively calculates one incoming open parenthesis transition, we denote by C s reached by a balanced path starting from s . If s has several incoming open parenthesis transitions, a naive implementation might lead to the state s in C tially many times. This is avoided by memoizing the shortest distance from s to states in
C . To do this, G ET D ISTANCE ( T , s ) calculates d [ s , s sets of transitions
These are the transitions with label a leaving states in C e = ( s  X  , a , w  X  , q  X  ), e  X   X  B [ s , a ] the following holds 3
If d [ s , s  X  ] is available, the shortest distance from q to q s can be computed trivially by Equation (5). For any state s with incoming open paren-necessary values.
 G
ET D ISTANCE ( T , 5) is called. The distance d [5, 7] is computed, and following tran-sitions are logged: B [5, ( 1 ]  X  X  (7, ) 1 , 0, 8) } and B [5, ( transition (4, ( 2 , 0, 5) is processed, its matching transition (7, ) dant re-calculation of distances along the shortest balanc ed path from state 4 to state 9. description may be easier to follow after reading the worked example in Figure 10. Note that the sets C s are not computed explicitly by the algorithm.
 q has been visited. G ET D ISTANCE ( T , s ) starts a new instance of the shortest-distance a state is dequeued and its outgoing transitions examined (l ines 7 X 11). Transitions labeled by non-parenthesis are treated as in Mohri (2009) (l ines 7 X 8). When a transition 700 G balancing e are then relaxed (lines 14 X 16).
 is O ( | E | ) in the worst case. This last observation also implies that t he accumulated number of transitions examined at line 16 is in O ( Z | Q | | E |
Z denotes the maximal number of times a state is inserted in the queue for a given call of G ET D ISTANCE . Assuming the cost of a queue operation is  X  ( n ) for a queue containing n elements, the worst-case time complexity of the algorithm c an then be first queue discipline leads to a time complexity in O ( | T |
C  X  X  are acyclic, using a topological order queue discipline l eads to a O ( | T | complexity.
 or a hypergraph into a PDA, the polynomial dependency in | T | becomes a linear dependency both for the time and space complexities. Indeed , for each q in T , there transition e , there exists a unique close parenthesis transition e
When each component of the RTN is acyclic, the complexity of t he algorithm is O ( | T | ) in time and space.
 shortest path by keeping track of parent pointers. The notio n of shortest path requires shortest-distance operation as presented here and the shor test-path operation can be applied in any semiring having the path property by using the natural order defined by  X  : a  X  b iff a  X  b = a . However, the shortest distance algorithm given in Figure 9 can be extended to work for k -closed semirings using the same techniques that were used b y Mohri (2002).
 recognizes s . PDA recognition is closely related to CFG parsing; a CFG can be repre-sented as a PDT whose input recognizes the CFG and whose outpu t identifies the parse (Aho and Ullman 1972). Lang (1974) showed that the cubic tabu lar method of Earley can be naturally applied to PDAs; others give the weighted ge neralizations (Stolcke 1995; Nederhof and Satta 2006). Earley X  X  algorithm has its a nalogs in the algorithm in
Figure 9: the scan step corresponds to taking a non-parenthesis transition at line 10, the predict step to taking an open parenthesis at lines 14 X 15, and the complete step to taking the closed parentheses at lines 16 X 18.

Specialization to Translation. Following the formalism of Section 1, we are interested in applying shortest distance and shortest path algorithms to automata created as
L = T p  X  M , where T p , the translation representation, is a PDA derived from an RT N (via replacement) and M , the language model, is a finite automaton.
 reasoning is as follows. Given a state q in T p , there exists a unique s to C s belongs to at most | M | components. 3.5 Expansion As explained in Section 1.1, HiPDT can apply Step 3b to genera te translation lattices.
This step is typically required for any posterior lattice re scoring strategies. We first 702 describe the unpruned expansion. However, in practice a pru ning strategy of some sort is required to avoid state explosion. Therefore, we also des cribe an implementation of the PDA expansion that includes admissible pruning under a l ikelihood beam, thus controlling on-the-fly the size of the output lattice. 3.5.1 Full Expansion. Given a bounded-stack PDA T , the expansion of T is the FSA T equivalent to T . A simple example is given in Figure 11.
 the FSA as the expansion proceeds along paths through the PDA . In the new FSA, parentheses are replaced by epsilons, and as open parenthes es are encountered on
PDA transitions, they are  X  X ushed X  into the FSA state labels ; in this way the stack depth is maintained along different paths through the PDA. C onversely, when a closing parenthesis is encountered on a PDA path, a corresponding op ening parenthesis is in Figure 11, expansion along that path halts.
 typically with more states and transitions than the origina l PDA, and the number of added states is controlled only by the maximum stack depth of the PDA.
 of K . The set of states in its FSA expansion T  X  are then has a bounded stack ensures that Q  X  is finite. Transitions are added to T Figure 12.
 plexity of the algorithm is linear in the size of T  X  . However, the size of T exponential in the size of T , which motivates the development of pruned expansion, as discussed next. 0,  X  3.5.2 Pruned Expansion. Given a bounded-stack PDA T , the pruned expansion of T with threshold  X  is an FST T  X   X  obtained by deleting from T  X  shortest distance in T .

FST pruning algorithm would lead to a complexity in O ( | T
Assuming that the reverse T R of T is also bounded-stack, an algorithm whose com-distance algorithm from the previous section to T R and then using this to prune the expansion as it is generated. To simplify the presentation, we assume that F = { f } and  X  ( f ) = 0.
 at FSAs. For an FSA, the cost of the shortest path through a tra nsition ( q , x , w , q computed by the shortest distance algorithm, as discussed i n Section 3.4. However, distances of the form d [ q  X  , f ] are not readily available. To compute these, a shortest distance algorithm is run over the reversed automaton. Reve rsal preserves states and transitions, but swaps the source and destination state (se e Figure 13 for a PDA ex-ample). The start state in the reversed machine is f , so that distances are computed cost of the shortest path through an FSA transition ( q , x , w , q d [ I , q ] + w + d R [ f , q  X  ].
 dled such that distances through them are calculated over ba lanced paths. For example, if T in Figure 13 was an FSA, the shortest cost of any path through t he transition because d [5, 10], the shortest distance from 5 to 10, is found via a path through the transition (7, ) 1 , 0, 8).
 can be done using quantities computed by the PDA shortest dis tance algorithm. For a 704 / 20 / 10 / 20 / 10 can be found as 4 T , and d R [ n [ e  X  ], f ] is computed by the PDA shortest distance algorithm over T and B [5, ( 2 ] = { 7, ) 2 , 0, 9 } ; the shortest distance algorithm over T 1, 000 (trivially, here); the cost of the shortest path throu gh e is contribute to any path that would survive pruning. Prior to e xpansion of a PDA T to an
FSA T  X  , the shortest distance d in T is calculated. Transitions e = ( q , a , w , q expanded as transitions e = (( q , z ), q , w , ( q  X  , za )) in T by Equation (7).
 complicated than the simple description given here. Pseudo -code describing the Open-FST implementation is given in Appendix B.
 property. 4. HiPDT Analysis and Experiments: Computational Complexi ty
We now address the following questions: hypergraph representation, with an emphasis on Hiero-styl e SCFGs. We assess our analysis for FSA and PDA representations by contrasting HiF ST and HiPDT with large grammars for translation and alignment. For convenience, w e refer to the hypergraph representation as T h , and to the FSA and PDA representations as T 1. SCFG Translation: Assuming that the parsing of the input is performed by a 2. Intersection: The intersection of a CFG T h with a finite automaton M can be 3. Shortest Path: The shortest path algorithm on the hypergraph, RTN, and tation is equivalent in time and superior in space complexit y to the CFG/hypergraph representation, in general, and it can be superior in both sp ace and time to the FSA representation depending on the relative SCFG and language model (LM) sizes. The
FSA representation favors smaller target translation gram mars and larger language models. 706 tions mentioned previously (Figure 3 and accompanying disc ussion). For the FSA representation, these operations can offset the exponenti al dependencies in the worst-case complexity analysis. For example, in a translation of a 15-word sentence taken at random from the development sets described later, expans ion of an RTN yields a
WFSA with 174  X  10 6 states. By contrast, if the RTN is determinized and minimize d prior to expansion, the resulting WFSA has only 34  X  10 3 magnitude are typical. In general, the original RTN, hyperg raph, or CFG representation can be exponentially larger than the RTN/PDT optimized as de scribed.
 rank 2 and a relatively small number of nonterminals, this co mplexity analysis can be extended to other grammars. For SCFGs of arbitrary rank l time for hypergraphs becomes O ( | G || s | l N + 1 | M |
For more complex SCFGs with rules of rank greater than 2, such as SAMT (Zollmann and Venugopal 2006) or GHKM (Galley et al. 2004), this sugges ts that PDA represen-tations may offer computational advantages in the worst cas e relative to hypergraph representations, although this must be balanced against ot her available strategies such as binarization (Zhang et al. 2006; Xiao et al. 2009) or scope pruning (Hopkins and
Langmead 2010). Of course, practical translation systems i ntroduce various pruning procedures to achieve much better decoding efficiency than t he worst cases given here. periments, which will be used throughout the remainder of th is article (except when stated otherwise). In the following sections we assess the c omplexity discussion with a contrast between HiFST (FSA representation) and HiPDT (PDA representation) under large grammars. 4.1 Translation Grammars and Language Models
Translation grammars are extracted from a subset of the GALE 2008 evaluation par-allel text; 6 this is 2.1M sentences and approximately 45M words per langu age. We report translation results on a development set tune-nw (1,755 sentences) and a test set test-nw (1,671 sentences). These contain translations produced by the GALE program and portions of the newswire sections of the NIST evaluation sets MT02 through MT06. In tuning the systems, MERT (Och 2003) iterative parameter e stimation under IBM BLEU 8 is performed on the development set.
 to-target and target-to-source directions. We then follow published procedures (Chiang 2007; Iglesias et al. 2009b) to extract hierarchical phrase s from the union of the directional word alignments. We call a translation grammar (G) the set of rules extracted from this process. For reference, the number of ru les in G that can apply to the tune-nw is 1.1M, of which 593K are standard non-hierarchical phrase s and 511K are strictly hierarchical rules.
 first language model, denoted M 1 , is a 4-gram estimated over 1.3B words taken from the target side of the parallel text and the AFP and Xinhu a portions of the
English Gigaword Fourth Edition (LDC2009T13). We use both K neser-Ney (Kneser and Ney 1995) and Katz (Katz 1987) smoothing in estimating M model reduction is required, we apply Stolcke entropy pruni ng (Stolcke 1998) to M under the relative perplexity threshold  X  . The resulting language model is labeled as M  X  1 .

For aggressive enough pruning, the original 4-gram model ca n be effectively reduced to a trigram, bigram, or unigram model. For both the Katz and t he Kneser-Ney 4-gram language models: at  X  = 7 . 5 E  X  05 the number of 4-grams in the LM is effectively reduced to zero; at  X  = 7 . 5 E  X  4 the number of 3-grams is effectively 0; and at  X  = 7 . 5 E  X  3, only unigrams remain. Development set perplexities incr ease as entropy pruning becomes more aggressive, with the Katz smoothed mod el performing better under pruning (Chelba et al. 2010; Roark, Allauzen, and Rile y 2013).
 ing M 1 with a zero-cutoff stupid-backoff 5-gram model (Brants et a l. 2007) estimated over 6.6B words of English newswire text; M 2 is estimated as needed for the n -grams required for the test sets. 708 4.2 Exact Decoding with Large Grammars and Small Language Mo dels
We now compare HiFST and HiPDT in translation with our large g rammar G . In this case we know that exact search is often not feasible for HiFST .

If this limit is reached in decoding, the process is killed. of times each decoder succeeds in finding a hypothesis under t he memory limit when decoding with various entropy-pruned LMs M  X  1 . With  X  = 7 . 5  X  10 can only decode 218 sentences, and HiPDT succeeds in 703 case s. The difference in success rates between the decoders is more pronounced as the language model is more aggressively pruned: for  X  = 7 . 5  X  10  X  7 HiPDT succeeds for all but three sentences. to FSA; this operation depends only on the translation gramm ar and does not benefit from any reduction in the language model size. Subsequent in tersection of the FSA with the language model can still pose a challenge, although as the language model grammar with the language model prior to expansion and this o peration nearly always finishes successfully. The subsequent shortest path (or pru ned expansion) operation is prone to failure, but the risk of this can be greatly reduced b y using smaller language models.
 4.3 Alignment with Inversion Transduction Grammars
We continue to explore applications characterized by large translation grammars G and small language models M . As an extreme instance of a problem involving a large translation grammar and a simple target language model, we c onsider parallel text alignment under an Inversion Transduction Grammar (ITG) (W u 1997). This task, or something like it, is often done in translation grammar indu ction. The process should of the source sentence. In alignment the target language mod el is extremely simple:
It is simply an acceptor for the target language sentence so t hat | M | is linear in the length of the target sentence. In contrast, the search space needs now to be represented with pushdown transducers (instead of pushdown automata) k eeping track of both translations and derivations, that is, indices of the rules in the grammar (Iglesias et al. 2009a; de Gispert et al. 2010; Dyer 2010b).
 follows. First, we obtain word-to-word translation rules o f the form X  X  X  s , t i based on probabilities from IBM Model 1 translation tables estima ted over the parallel text, where s and t are one source and one target word, respectively (  X  16 M rules). Then, we allow monotonic and inversion transduction of two adjace nt nonterminals in the usual ITG style (i.e., add X  X  X  X 1 X 2 , X 1 X 2 i and X  X  X  X we allow unrestricted source word deletions ( X  X  X  s ,  X  i ), and restricted target word ficiency reasons, disallows the insertion of two consecutiv e target words. We make no claims about the suitability or appropriateness of this s pecific grammar for either alignment or translation; we introduce this grammar only to define a challenging alignment task.
 for alignment. These sentences come from the same Chinese-t o-English parallel data described in Section 4.1. Hard limits on memory usage (10GB) and processing time (10 minutes) were imposed for processing each sentence pair . If HiPDT or HiFST ex-ceeded either limit in aligning any sentence pair, alignmen t was stopped and a  X  X em-ory/time failure X  was noted. Even if the resource limits are not exceeded, alignment may fail due to limitations in the grammar. This happens when either a particular word pair rule that is not in our Model 1 table, or more than one cons ecutive target insertions are needed to reach alignment. In such cases, we record a  X  X ra mmar failure, X  as opposed to a  X  X emory/time failure. X  aligns only 41% of the sentence pairs under these time and mem ory constraints. The reason for this low success rate is that HiFST must generate a nd expand all possible derivations under the ITG for a given sentence pair. Even if i t is strictly enforced that the FSA in every CYK cell contains only partial derivati ons which produce sub-strings of the target sentence, expansion often exceeds the memory/time constraints.
In contrast, HiPDT succeeds in aligning all sentence pairs t hat can be aligned under the grammar (89%), because it never fails due to memory or tim e constraints. In this experiment, if alignment is at all possible, HiPDT will find t he best derivation. Align-ment success rate (or coverage) could trivially be improved by modifying the ITG to allow more consecutive target insertions, or by increasing the number of word-to-word 710 rules, but that would not change the conclusion in the contra st between HiFST and HiPDT.

The language model M is replaced by an acceptor for the target sentence, and if we assume that the target sentence length is proportional to th e source sentence length, it follows that | M | X  X  s | and the worst-case complexity for HiPDT in alignment mode is rithm of Dyer (2010b).

HiPDT is more efficient in ITG alignment and this is consisten t with its linear depen-dence on the grammar size, whereas HiFST suffers from its exp onential dependence.
This use of PDAs in alignment does not rely on properties spec ific either to Hiero or to ITGs. We expect that the approach should be applicable w ith other types of
SCFGs, although we note that alignment under SCFGs with an ar bitrary number of nonterminals can be NP-hard (Satta and Peserico 2005). 5. HiPDT Two-Pass Translation Architecture and Experiment s
The previous complexity analysis suggests that PDAs should excel when used with large translation grammars and relatively small n -gram language models. In hierar-chical phrase-based translation, this is a somewhat unusua l scenario: It is far more typical that translation tasks requiring a large translati on grammar also require large language models. To accommodate these requirements we have developed a two-pass decoding strategy in which a weak version of a large lang uage model is ap-plied prior to the expansion of the PDA, after which the full l anguage model is applied to the resulting WFSA in a rescoring pass. An effecti ve way of generating weak language models is by means of entropy pruning under a th reshold  X  ; these are the language models M  X  1 of Section 4.1. Such a two-pass strategy is widely used in automatic speech recognition (Ljolje, Pereira, and Riley 1 999). The steps in two-pass translation using entropy-pruned language models are give n here, and depicted in Figure 14.

Step 1. We translate with M  X  1 and G using the same parameters obtained by MERT
Step 2. These translation lattices are pruned at beamwidth  X  : [  X 
Step 3. We remove the M  X  1 scores from the pruned translation lattices, reapply the fu ll system with the larger language model M 2 . If  X  =  X  or if  X  = 0, the translation lattices obtained in Step 3 should be identical to lattices produced b y the baseline system (i.e., the rescoring step is no longer needed). The aim is to increas e  X  to shrink the language model used at Step 1, but  X  will then have to increase accordingly to avoid pruning away desirable hypotheses in Step 2. 5.1 Efficient Removal of First-Pass Language Model Scores Us ing Lexicographic Semirings
The two-pass translation procedure requires removal of the weak language model scores used in the initial expansion of the translation sear ch space; this is done so that only the translation scores under G remain after pruning. In the tropical semiring, the weak LM scores can be  X  X ubtracted X  at the path level from t he lattice, but this involves a determinization of an unweighted translation la ttice, which can be very inefficient.

Roark, Sproat, and Shafran 2011) h w 1 , w 2 i over the tropical weights w operations  X  and  X  :
The PDA algorithms described in Section 3 are valid under thi s new semiring because it is commutative and has the path property. In particular, t he PDA representing { s } X  G is constructed so that the translation grammar score appear s in both w duplicated). In the first-pass language model, w 1 has the n -gram language model scores and the w 2 are 0. After composition, the resulting automata have the co mbined trans-lation grammar score and language model score in the first dim ension, and the second dimension contains the translation grammar scores alone. P runing can be performed under the lexicographic semiring with a threshold set so tha t only the combined scores in the first dimension are considered. The resulting automat a can easily be mapped back into the regular tropical semiring such that only the transl ation scores in the second 712 dimension are retained (this is a linear operation done by th e fstmap operation in the
OpenFST library). 5.2 Translation Quality and Modeling Errors in Two-Pass Dec oding
We wish to analyze the degree to which the two-pass decoding s trategy introduces  X  X odeling errors X  into translation. A modeling error occur s in two-pass decoding whenever the decoder produces a translation whose score is l ess than the best attainable under the grammar and language model (i.e., whenever the bes t possible translation is discarded by pruning at Step 2). We refer to these as modeli ng errors, rather than search errors, because they are due to differences in scores assigned by the models
M 1 and M  X  1 .
 tem that performs exact translation, without pruning in sea rch, under the grammar G and language model M 1 . This would allow us to address the following questions: capable of exact decoding under both G and M 1 . To create a suitable baseline we there-and refer to this reduced grammar as G small . This process reduces the number of strictly hierarchical rules that apply to our tune-nw set from 511K to 189K, while the number of standard phrases is unchanged.
 possible candidate hypotheses with the language model and t o extract the shortest path hypothesis. Because an exact decoding baseline is thus avai lable, we can empirically evaluate the proposed two-pass strategy. Any degradation i n translation quality can only be due to the modeling errors introduced by pruning unde r  X  with respect to the entropy-pruned M  X  1 .
 of entropy pruning threshold  X  . Performance is reported after first-pass decoding with
M 1 (Step 1, Section 5), and after rescoring with M 1 (Step 3, Section 5) the first-pass lattices pruned at alternative  X  beams. The first column reports the baseline for either
Kneser-Ney and Katz language models, which are found by tran slation without entropy pruning, that is, performed with M 1 . Both yield 34.5 on test-nw .
 quate because we are always able to recover the baseline perf ormance. As expected, the harsher the entropy-pruning of M 1 (as we lower  X  ) the greater  X  must be to recover from the significant degradation in first-pass decoding. But even at a harsh  X  = 7 . 5  X  10 when first-pass performance drops over 7 BLEU points, a relat ively-low value of  X  = 15 can recover the baseline performance.
 conclusion from the figure is that the choice of LM smoothing d oes impact first-pass translation performance. For entropy pruning at  X  = 7 . 5  X  10 better for smaller beamwidths  X  . These results are consistent with the test set perplexities of the entropy pruned LMs (Table 2), and are als o in line with other studies of Kneser-Ney smoothing and entropy pruning (Chelba et al. 2 010; Roark, Allauzen, and Riley 2013).
 10  X  7 . As expected, modeling errors decrease as the beamwidth  X  increases, although we find that the language model with Katz smoothing has fewer m odeling errors.
However, modeling errors do not necessarily impact corpus l evel BLEU scores. For wide beamwidths (e.g.,  X  = 15 here), there are still some modeling errors, but these are either few enough or subtle enough that two-pass decoding under eit her smoothing method yields the same corpus level BLEU score as the exact decoding baseline.
  X  Kneser-Ney Katz 714 5.3 HIPDT Two-Pass Decoding Speed and Translation Performa nce perform exact decoding (see Table 3). In contrast, HiPDT is a ble to do exact decoding so we study tradeoffs in speed and translation performance. The speed of two-pass decoding can be increased by decreasing  X  and/or increasing  X  , but at the risk of degradation in translation performance. For grammar G and language model M plot in Figure 16 the BLEU score against speed as a function of  X  for a selection of  X  values. BLEU score is measured over the entire test set test-nw but speed is calculated only on sentences of length up to 20 words (  X  500 sentences). In computing speed we measure not only the PDA operations, but the entire HiPDT dec oding process described in Figure 14, including CYK parsing and the application of M these unusually slow decoding speeds are a consequence of th e large grammars, lan-guage models, and broad pruning thresholds chosen for these experiments; in practice, translation with either HiPDT or HiFST is much faster.
  X  and the likelihood beamwidth  X  work together to balance speed against translation quality. For every entropy pruning threshold  X  value considered, there is a value of  X  for which there is no degradation in translation quality. Fo r example, suppose we want to attain a translation quality of 34.5 BLEU: then  X  should be set to 12 or greater. If the goal is to find the fastest system at this level, then we choose  X  = 7 . 5  X  10 is explained by Figure 17, where decoding and rescoring time s are shown for various different entropy pruning thresholds  X  and for likelihood beamwidths  X  = 15, 12, 9, 8, 7. values of  X  and  X  that achieve at least the translation quality target of 34.5 . As  X  increases, decoding time decreases because a smaller langu age model is easier to apply; however, rescoring times increase, because the larger valu es of  X  lead to larger WFSAs after expansion, and these are costly to rescore. The balanc e occurs at  X  = 7 . 5  X  10 and a translation rate of 3.0 words/sec. In this case, entrop y pruning yields a severely shrunken bigram language model, but this may vary depending on the translation grammar and the original, unpruned LM. 5.4 Rescoring with 5-Gram Language Models and LMBR Decoding HiPDT translation system under the large translation gramm ar G . We demonstrate that
HiPDT can be used to generate large, compact representation s of the translation space that are suitable for rescoring with large language models o r by alternative decoding procedures. We investigate translation performance by app lying versions of the lan-guage model M 2 estimated with stupid backoff. We also investigate minimum Bayes risk (MBR) decoding (Kumar and Byrne 2004) as an alternative search strategy. We are particularly interested in lattice MBR (LMBR) (Tromble et a l. 2008), which is well suited for the large WFSAs that the system can generate; we use the im plementation described by Blackwood, de Gispert, &amp; Byrne (2010). There are two param eters to be tuned: a scaling parameter to normalize the evidence scores and a wor d penalty applied to the hypotheses space; these are tuned jointly on the tune-nw set. Results are reported in Figure 18.
 interpolated with M 1 , gives consistent gains over initial results obtained with M
After 5-gram rescoring there is already +0.5 BLEU improveme nt compared with G
With a richer translation grammar we have generated a richer lattice that allows gains to be gotten by our lattice rescoring techniques. 716 pruning beamwidth narrows, and at all values of  X  LMBR gives improvement over the MAP hypotheses. Because LMBR relies on posterior distri butions over n -grams, we conclude that HiPDT is able to generate compact representat ions of large search spaces with posteriors that are robust to pruning conditions.

Again, with appropriate choices of  X  and  X  we can easily reach a compromise between decoding speed and final performance of our HiPDT system. For instance, with  X  = we are losing only 0.5 BLEU after LMBR compared to  X  = 7 . 5  X  10 6. Related Work
There is extensive prior work on computational efficiency an d algorithmic complexity in hierarchical phrase-based translation. The challenge i s to find algorithms that can be made to work with large translation grammars and large langu age models. Chiang (2007) developed the cube-growing algorithm, and mo re recently Huang and
Mi (2010) developed an incremental decoding approach that e xploits the left-to-right nature of n -gram language models.
 not been as extensively studied; this is undoubtedly due to t he difficulties inherent in finding exact translations for use in comparison. Using a rel atively simple phrase-based translation grammar, Iglesias et al. (2009b) compared sear ch via cube-pruning to an exact FST implementation (Kumar, Deng, and Byrne 2006) and f ound that cube-pruning suffered significant search errors. For Hiero translation, an extensive comparison of search errors between the cube pruning and FSA implementati on was presented by studied in phrase-based translation by Zens and Ney (2008). Relaxation techniques have also recently been shown to find exact solutions in parsing (K oo et al. 2010), phrase-based SMT (Chang and Collins 2011), and in tree-to-string tr anslation under trigram language models (Rush and Collins 2011); this prior work inv olved much smaller grammars and languages models than have been considered her e.
 studied previously by Dyer (2010b), who showed that a single synchronous parsing al-gorithm (Wu 1997) can be significantly improved upon in pract ice through hypergraph compositions. We developed similar procedures for our HiFS T decoder (Iglesias et al. 2009a; de Gispert et al. 2010) via a different route, after no ting that with the space of translations represented as WFSAs, alignment can be perfor med using operations over WFSTs (Kumar and Byrne 2005).
 translation systems (Prasad et al. 2007), we believe our use of entropy-pruned language models in two-pass translation to be novel. This is an approa ch that is widely used in automatic speech recognition (Ljolje, Pereira, and Riley 1 999) and we note that it relies on efficient representation of very large search spaces T for subsequent rescoring, as is possible with FSAs and PDAs. 7. Conclusion
In this article, we have described a novel approach to hierar chical machine translation using pushdown automata. We have presented fundamental PDA algorithms including composition, shortest-path, (pruned) expansion, and repl acement and have shown how these can be used in PDA-based machine translation decoding and how this relates to and compares with hypergraph and FSA-based decoding.
 now address the questions laid out in Sections 4 and 5: 718 is whether inadmissible pruning methods can be applied to th e PDA-based systems that are analogous to those used in current hypergraph-based sys tems such as cube-pruning (Chiang 2007). Another is whether a hybrid PDA X  X SA system, w here some parts of the
PDA are pre-expanded and some not, could provide benefits ove r full pre-expansion (FSA) or none (PDA). We leave these questions for future work .
 Appendix A. Composition of a Weighted PDT and a Weighted FST
Given a pair ( T 1 , T 2 ) where T 1 is a weighted pushdown transducer and the T weighted finite-state transducer, and such that T 1 has input and output alphabets  X  and  X  and T 2 has input and output alphabets  X  and  X  , then there exists a weighted pushdown transducer T 1  X  T 2 , which is the composition of T ( x , y )  X   X   X   X   X   X  :
We also assume that T 2 has no input- X  transitions, noting that for T sitions, an epsilon filter (Mohri 2009; Allauzen, Riley, and Schalkwyk 2011) generalized to handle parentheses could be used.
 transition e 1 = ( q 1 , a , b , w 1 , q  X  1 ) in T 1 , transitions out of ( q following rules. If b  X   X  , then e 1 can be matched with a transition ( q resulting in a transition (( q 1 , q 2 ), a , c , w 1 + w with staying in q 2 resulting in a transition (( q 1 , q 2 e is also matched with staying in q 2 , resulting in a transition (( q
T . The initial state is ( I 1 , I 2 ) and a state ( q 1 , q final. Weight values are assigned as  X  (( q 1 , q 2 )) =  X  Appendix B. Pruned Expansion
Let d R and B R be the data structures computed by the shortest-distance al gorithm applied to T R . For a state q in T  X  (or equivalently T  X  shortest path from the initial state to q .

T expressed as: 720 known; we then have all the required information for decidin g whether e should be pruned or retained. In order to ensure that each state is visi ted once, we need to ensure that d [( q , z )] is known when ( q , z ) is visited so we can apply an A among the states sharing the same stack.
 order  X  such that
We also assume that all states sharing the same stack will be d equeued consecutively ( z 6 = z  X   X  for all ( q , q  X  ), ( q , z )  X  ( q  X  , z  X  cache some computations (the D data structure as described subsequently). algorithm is applied to T R and the absolute pruning threshold is computed accordingly accordingly (lines 6 X 8). The default value in these data str uctures is assumed to be  X  . The queue is initialized containing the initial state (line 9).
 incoming open-parenthesis transition, B contains the balance information for that state and D can be updated accordingly (lines 13 X 18).
 can be pruned using the criterion derived from Equation (B.1 1). If it is retained, the transition in T  X  and ending by a close-parenthesis transition is treated as a meta-below the threshold (lines 36 X 39).
 Acknowledgments References 722
