 ATSUSHI MATSUMURA
University of Tsukuba and ATSUHIRO TAKASU and JUN ADACHI National Institute of Informatics 1. INTRODUCTION
Many electronic documents have become accessible to users directly through the Internet, so it has become more important for users to be able to retrieve the information they want simply and efficiently by phrasing their informa-tion needs in natural language. The Boolean model, which is a simple retrieval model based on set theory and Boolean algebra, does not meet these require-ments because it requires users to form queries using complex logical expres-sions, and it presents the search results in a disordered manner. Although some search methods exist that arrange the output using a vector space model [Harman 1992], there are obvious limitations to their retrieval effectiveness.
One reason for this is that, in such retrieval systems, only query words and their statistical characteristics, such as Term Frequency and the Inverted Doc-ument Frequency (TF X  X DF), are used and the relationships between the query words are discarded.

Much work has been carried out to construct information retrieval (IR) sys-tems using the relationships between words. Farradane proposed using Rela-tional Indexing, and defined nine categories of relationships based on an analy-sis of thought processes following investigations into the psychology of thinking [Farradane 1990a, 1990b]. However, the assignment of relationships was per-formed manually and no significant improvement in retrieval effectiveness was reported. Lu used lexical X  X emantic relationships to connect words and build a structured representation of documents and queries [Lu 1990]. He showed the potential of using index words, together with their lexical X  X emantic relation-ships, to represent and retrieve documents. However, relationships between words were selected manually and both the size of the test database and the number of queries were small. In contrast, Myaeng et al. [1994] have been developing a conceptual IR system that represents a large volume of natural language text as a Conceptual Graph. In this project, natural language pro-cessing (NLP) techniques were evaluated. Their IR system is still at an initial stage of development.

In developing these IR systems, the use of NLP is unavoidable, even though it has not yet yielded any significant improvement in retrieval effectiveness. Con-sidering the amount of processing and the storage requirements of the index, morphological analysis is one practical method that can be used in IR.
To avoid the complex and high-cost processing of NLP, some IR methods use either statistical phrases, which are derived using techniques other than NLP, or proximity relationships between words as an approximation to the syntactic or semantic relationships between words. Mitra et al. [1997] defined a  X  X hrase X  to be any pair of nonfunction words that appear in at least 25 documents of the
TREC-1 1 collection. However, they showed that the use of phrases in IR im-proved performance by only 1%. Furthermore, although syntactic phrases per-formed better than statistical phrases, this advantage disappeared when using single terms in retrieval. Smeaton and Kelledy [1998] repeated these experi-ments in a separate study. Although phrases yielded only small improvements in retrieval effectiveness in these experiments, most experiments show incon-sistent results [Croft et al. 1991]. Thus, experimental results using these meth-ods remain mixed and the effects of each method have not been clearly analyzed. More detailed experimental research is required.

Nonetheless, whereas IR for English text is at least being actively investi-gated, studies with Japanese text are inconclusive. Hyoudo et al. [1998] com-pared proximity operations and dependency operations in Japanese text re-trieval. They used five proximity methods, namely distances within one, two, three, four, and five words. However, because their evaluation examined only whether correct dependency relationships were included in the retrieved docu-ments, the effect on the general IR task was not clear. Hyoudo et al. [1999] also examined three other proximity operations for phrases in Japanese sentences within the same document, sentence, and clause. They compared these methods with a method using dependency operations on the Japanese IR test collection IREX. 2 However, the effects of these methods were not clearly analyzed.
From this perspective, we believe it is necessary to conduct a detailed analy-sis of IR methods that use relationships between words, especially for Japanese text. In our previous research, we proposed a Japanese language IR method us-ing dependency relationships between words in document titles and in queries [Matsumura et al. 1998]. We extracted dependency relationships between words in document titles using only morphological analysis and simple template matching and achieved a title-based document retrieval method. The perfor-mance of our method was limited and similar to that of TF X  X DF. We then devel-oped the method further to deal with natural language texts, such as document abstracts, but our method performed only slightly better than other approaches [Matsumura et al. 1999b]. We also developed an IR method that uses informa-tion about the ordered co-occurrence of words in a sentence as an approximation to the IR method that uses dependency relationships [Matsumura et al. 2000].
In this paper, we propose an IR method that uses dependency relation-ships between words, together with another IR method that approximates this method by using ordered co-occurrence information about words in a sentence.
We show that our two methods are superior to the TF X  X DF method, indepen-dently of the target collection and the search topic set. The difference between our two methods is minor.
 In the following sections, we first describe the details of our two IR methods.
We then present our experimental environment, which uses NTCIR-1, our experimental results. Finally, we discuss the effectiveness of our two IR methods and provide some conclusions. 2. OVERVIEW OF THE TWO METHODS
Our two IR methods use relationships between words in a sentence. We have named the method that uses dependency relationships between words ST , and the method that uses ordered co-occurrences of words CO .
To utilize dependency relationships between words in ST, we propose a Struc-tured Index , represented by a set of binary trees that show dependency rela-tionships between words. Figure 1 shows an example of a Structured Index for the sentence  X   X  ( X  X ffect of Natural Language Processing on Information Retrieval X ).
 In the ST method, words are classified as concept words or relation words .
Concept words symbolize concepts. They include nouns, adjectives, adverbs, and constituents of compound nouns. Relation words describe the relationship between concept words or noun phrases composed of concept words and rela-tion words. They include postpositional particles, auxiliary verbs, verbs, and combinations of these. In grammatical classification of words,  X  X ontent word X  includes nouns, verbs, adjectives, and most adverbs, and  X  X unctional word X  in-cludes postpositional particles, auxiliary verbs, conjunctions, and so on. On the other hand, in our method, verbs and some combinations of  X  X ontent word X  and  X  X unctional word X  are defined as relation words. Consequently, the classifica-tions of  X  X oncept word X  and  X  X elation word X  are different from  X  X ontent word X  and  X  X unctional word. X  Each concept word represents a concept and is placed on a leaf node in the Structured Index. In Figure 1,  X   X  (information),  X   X  (retrieval),  X   X  (natural),  X   X  (language),  X   X  (processing), and  X   X  (effect) are concept words. Each relation word associates two concept words. In
Figure 1,  X   X  (of) and  X   X  (on) are relation words. Relation words are also classified into categories according to their semantic similarity. In Figure 1,  X   X  (of) belongs to the restriction category and  X   X  (on) belongs to the place category. A relation word and its category are placed on an internal node in the Structured Index.

Using the Structured Index method, we can retrieve a document by us-ing dependency relationships between words. We can also retrieve compound nouns by phrases that are composed of the words that form the compound nouns.

However, the success of the ST method depends largely on the effectiveness of the dependency analysis. Therefore, we propose the CO method, which uses ordered co-occurrence information about two words within a sentence as an approximation of the dependency relationships. Because this method requires no dependency analysis, its contribution to retrieval effectiveness can be clearly analyzed. Furthermore, comparing our two methods can reveal the effect of dependency relationships (relation words).

The ST method automatically extracts dependency relationships from nat-ural language sentences without using the complex NLP techniques that were used by Myaeng et al. [1994]. We hope to discover an effective method of using dependency relationships in IR. To achieve a highly efficient retrieval method, we also define a dependency relationship as a triplet of concept word X  X elation word X  X oncept word , in contrast to the binary relation of  X  X odifier X  X odified X  defined by Hyoudo et al. [1998, 1999].

In contrast, the CO method uses a special proximity operator that consid-ers the order of two concept words within a sentence. The proximity operators or phrases in other approaches [Croft et al. 1991; Mitra et al. 1997; Hyoudo et al. 1998] did not use a sentence as the range of operation, with the excep-tion of Hyoudo et al. [1999], who used it in a Japanese IR system to improve performance. We have adopted this method to identify co-occurrence relations of concept words that probably have a semantic relationship.

Figure 2 shows the system overview and the query processing flow. This sys-tem is designed as an experimental environment in which we can compare our two methods. The Structured Index is created from document titles and ab-stracts through morphological, dependency, and compound noun analyses. The user X  X  query is structured in the form of a binary tree using the same process.
Retrieval in our two methods consists of matching the binary tree of a query with a set of binary trees derived from the documents. Our two methods cal-culate similarity scores of matched documents using different scoring param-eters, before ranking them according to the similarity score, and outputting them.
 The following section describes the indexing that is common to the ST and
CO methods, together with the retrieval and scoring method, and the retrieval parameters that characterize ST and CO. 3. INDEXING
In this section, we present two steps for creating a Structured Index using the example sentence  X   X  ( X  X ffect of Natural
Language Processing on Information Retrieval X ). 3.1 Morphological Analysis
To determine dependency relationships between words, we first divide a sen-tence into concept words and relation words, as defined above.

We used ChaSen 1.51 4 as the Japanese morphological analyzer. Sentences were first divided into morphemes and these were tagged with the parts of speech using morphological analysis. Morphemes were identified either as con-cept words or relation words using these parts of speech. Adjacent relation words were joined and dealt with as a single relation word. Relation words that differed only in conjugation were identified as the same relation word. Next, relation words were classified into the 18 categories according to the relation words database.

The relation words database was constructed manually from the titles of 3666 scientific and technical documents. There are 6763 relation words of 256 types. We have defined 18 categories that may be used to classify about 98% of the 6763 relation words according to their semantic similarity, where a cat-egory is a type of semantic relationship between two concept words in a sen-tence, and the relation word is a description of the relationship between them.
Some relation words may be categorized into more than one category. These relation words were classified using the most common meaning found in the investigated titles. In addition, we defined the category other for relation words that cannot be classified according to the above 18 categories. Table I shows the 18 categories, together with typical elements. Through the classification of relation words into the defined categories, we attempted to discover the corre-sponding relationships between the expressions and the meanings of the word relationships.

Using morphological analysis, the example sentence is divided into words suchas X  / X  (effect / of / natural / language / processing / on / information / retrieval). X   X  (informa-tion),  X   X  (retrieval),  X   X  (natural),  X   X  (language),  X   X  (processing), and  X   X  (effect) are tagged as nouns and identified as concept words.  X   X ( is tagged as a postpositional particle and  X   X (  X  5 ) is tagged as a verb. Their combination  X   X  (on) is identified as a relation word and classified in the place category by using a database of relation words. In addition,  X   X  (of) is tagged as a postpositional particle and classified in the restriction category. 3.2 Dependency Analysis
To define the dependency relationships between concept words, we used the order of relation words in a sentence to define templates . The example sentence belongs to the template  X  X  B C X  (C of B on A), 6 where A, B, and
C are concept words or their combinations (compound nouns). The dependency relationship in our method is defined as a triplet, concept word  X  relation word  X  concept word . Since this differs from the output of generic syntactic parsers, we adopted these templates for the dependency analysis. We manually assigned the dependency relationships between words to any template that had two or three relation words if that template appeared more than three times in the 3666 titles used to create the database of relation words. Since titles contain many important words and their syntax is relatively simple, we can easily extract the correct triplets. The comparative table of templates and dependency patterns contains 105 templates (62 containing two relation words and 43 containing three relation words).

Japanese natural language sentences generally end with relation words. To apply a template to natural language sentences in the docu-ment abstracts, we converted all sentences into pseudonatural language sentences, which end with concept words, by simply eliminating these relation words from the end of the sentences. For example, the natural language sentence,  X  . X   X   X  (limitations to the retrieval method using only the statistics of words) by eliminating the last relation words  X   X  (There are).

There were two relationship types for titles comprising at least two relation words. For the first type, only one dependency relationship was possible, while more than two dependency relationships were possible for the second type.
For this second type, we determined the dependency pattern according to the existence of a general word at the end of the sentence. We have defined general words using the following two criteria: less important words, such as  X   X  (study) and  X   X  (effect), and words that have a dependency relationship on the entire sentence that pre-cedes them rather than only a particular word in a sentence.

We manually analyzed the 3666 titles used to define templates , and defined 53 words as general words, including those shown above,  X   X  (proposal), and  X   X  (implementation).

If the dependency pattern was not identified by a template, we used an ex-tended template , in which a relation word is replaced with its category name.
For example, the template  X  X  B C X  (C of B on A) is extended to the
We defined 73 extended templates (40 containing two relation words and 33 containing three relation words).

When the dependency pattern is not determined even by extended templates , we divide the sentence into small parts using the following three heuristics, then assign a dependency pattern to each part using templates or extended templates .

The first heuristic is that when the sentence includes a relation word shown in Table II, which strongly connects two concept words before and after, we regard them as a virtual concept word . After applying the template or the ex-tended template to the sentence in which the relation words have been reduced, we restore the virtual concept word to the dependency pattern of two concept words and a relation word .

The second heuristic is that when the sentence includes the relation words shown in Table III, which connect sentences before and after, we divide the sentence at the relation word . We then apply the template or the extended template to each divided sentence and give a dependency relationship between these sentences with the relation word .

The last heuristic is that when the sentence includes a general word , which has a dependency relationship with the whole sentence that precedes it, we divide the sentence at the concept word just before the general word . We then apply the template or the extended template and assign a dependency relation-ship to the sentence and the general word .

This method is important for maintaining the effectiveness of our retrieval system, because the dependency pattern given by this method is correct locally in most cases, even if the total dependency pattern is incorrect.

The example sentence  X   X  (effect / of / natural / language / processing / on / information / retrieval) be-longs to the template  X  X  B C X  (C of B on A) and is given the left dependency pattern in Figure 3, because the last word of the sentence  X   X  (effect) is one of the general words. On the other hand, another example sen-tence  X   X  ( X  X ptimized computation of recursive queries on deductive databases X ), which belongs to the same tem-plate, is given the right dependency pattern in Figure 3, because the last word,  X   X  (optimized computation), is not a general word.

A compound noun is translated into a sentence by being supplemented with suitable relation words between its constituent words [Miyazaki 1984; Ishizaki 1988].

Although compound nouns are divided into their constituent concept words in morphological analysis, this method experiences major problems when determining which relation words can be inserted between concept words. When the concept word has a special suffix, such as  X   X  (-ed) or  X   X  (-atic), we append the appropriate relation words, such as  X   X  (-ed) or  X   X  (of). When the last concept word of the compound noun is a proper noun, we also add  X   X  (as) as the relation word. The relation word  X   X  (of) is generally added to the remaining concept words.

These heuristics and general principles have been developed through sta-tistical investigations concerned with the co-occurrences of concept words and relation words in compound nouns [Ikeda and Adachi 1997].

For the dependency analysis of compound nouns, we propose the use of word adjacent concept words in compound nouns. If enough samples are used, the bigram statistics may be considered to represent the strength of the relationship between the two concept words in compound nouns. In our system, we used data from 814 bigrams, each of which appeared more than 10 times in 60,507 bigrams collected from the 13,615 titles.

There are two compound nouns, X   X  (information / retrieval) and  X   X  (natural / language / processing), in the example sentence.

First, they are translated into  X   X  (retrieval of information) and  X   X  (processing of language of natural), by inserting the rela-tion word  X   X  (of). The former compound noun is allocated a single dependency relationship. However, it is necessary to analyze the dependency pattern using word bigram statistics for the latter compound noun. The word bigram statistic of  X   X  (natural) and  X   X  (language) is 68, and that of  X   X  (language) and  X   X  (processing) is 43. This shows that the dependency relationship between the former two words is stronger than that between the latter two. Therefore, the dependency relationship between  X   X  (natural) and  X   X  (language) is given first. In this way, the example sentence is structured in the form of a binary tree, as shown in Figure 1.

The whole process is performed automatically, and all sentences are allocated their dependency patterns, as shown for the example sentence, before being organized in the form of a Structured Index. 4. RETRIEVING AND SCORING 4.1 Form of Queries
Queries are in the form of pseudonatural language, similar to article titles, as indexing. Retrieval in ST consists of matching the binary tree of a query and a set of binary trees derived from source documents. In contrast, CO uses the ordered co-occurrence information of two concept words in those binary trees. The main problem of retrieval for each method is to properly define the similarity measures between them.
 4.2 Total Scores of Documents
Figure 4 depicts the flow of our scoring method. First, we consider the differ-ences between the scores of document elements. A document is constructed from several elements, such as the title and the abstract, which have their own characteristics of information, such as the number of included sentences, the length of each sentence, or particular linguistic features. We calculate the score of each document element, which is then divided into two parts: the score for the words in that element and the score for the relationships between the words.
Since the latter score is calculated only if two concept words co-occur, there is no contribution of a concept word, even if it is an important keyword that occurs in isolation. In order to reflect the contribution of such an important keyword to the score, we have to use the score both for the relationships and for the words. These two scoring methods are discussed in the following two subsections.
As a result, we can verify the effect of the relationships between words on re-trieval performance by comparing our method with the keyword-based method. The total score of document d is shown in Eq. (1).
 where the variables in the equation are b : the document element (e.g., title, abstract)
SW b : the score of words within document element b
SR b : the score of relationships between words within the document element b xw b : the weights of SW b xr b : the weights of SR b
Therefore, by setting the values of xw b and xr b independently, we can control the weight of the score for each element. In the following two subsections, we define SW b and SR b . 4.3 Scoring of Words
To calculate the score for the words for each document element, our system uses all of the concept words in the query. When a concept word from the query appears in a document element b , our system scores the element by the modified
TF X  X DF weighting defined by Eq. (2). Relation words are not used when scoring words.
 where tfidf b ( C j ) is the modified TF X  X DF scoring function, which is in the form The variables in the equations are tf b ( C j ) : the frequency of the concept word C j in the document element b df ( C j ) : the number of documents that contain the concept word C 4.4 Scoring of Relationships between Concept Words
A sentence in a document includes several dependency relationships, each of which is represented by a triplet consisting of two concept words and a relation word. This triplet is the smallest scoring unit in ST. In contrast, the scoring unit in CO is an ordered pair of concept words in a sentence.

In our preliminary experiment, the retrieval effectiveness was relatively low when our system used only the exact dependency relationships to score docu-ments. To increase its effectiveness, we assigned relationships to pairs of con-cept words that do not have any dependency relationship. Here, the pseudo-relation word associating a pair of concept words that are not in a dependency relationship is defined as the relation word that is on the path connecting those two concept words and is closest to the root.

For example, in the sentence  X   X  (natural language processing for improvement of the clustering method),  X   X  (improvement of the clustering method) and  X   X  (natural language process-ing for the improvement) are right dependency relationships. However,  X   X  (natural language processing for the clustering method) also has an important meaning, even if it is not a syntactically correct dependency relationship.

Considering such pseudodependency relationships, we can define triplets for all pairs of concept words in a sentence and retrieve more relevant documents.
Using this definition, CO is equivalent to ST without both dependency rela-tionships and pseudodependency relationships. 4.4.1 Scoring in ST. In the ST method, two concept words of the triplet word connecting them is extracted and calculated using the semantic similarity measure. For this semantic similarity measure, we define two matching criteria.
First, we consider the level of matching between two triplets. Even if two triplets have the same concept words, their semantics are often different be-cause of differences in their dependency relationships. We, therefore, evaluate the similarity between the two triplets according to the following three levels. Exact Match : The two relation words are the same.

Category Match : The two relation words are different but their categories are the same.
 Wild Match : The two relation words and their categories are different. We change the scoring factor of ST to reflect the above three levels of matching.
Second, we introduce the notion of the importance of the triplet within the document set that is being retrieved. Because the importance of a triplet in-creases according to the importance of the two concept words in it, we define the importance of the triplet as the product of the concept words X  IDF scores.
In addition, we define the importance of a triplet as zero if any general words are included in it, to reduce noise that may be caused by these general words.
Using the above two matching criteria, the score of a triplet in the element ( TR b ) matched by a triplet in the query ( TR q ) is shown by Eq. (4). where the variables in the equation are
LD ( TR b , TR q ) : the weight for the matching level between TR
Equation (4) does not reflect the frequency of the triplet in the retrieved document. This is measured by Eq. (6).
 4.4.2 Scoring in CO. In the CO method, we calculate a score based on the ordered co-occurrence of two words in a sentence. In our experiment, only concept words are considered. Relation words and general words are regarded as having no importance and are excluded. We define the importance of the co-occurrence of two concept words as the product of their IDF scores. When two concept words occur in reverse order, this occurrence is regarded as different and is not included in the score calculation.

Consequently, the score in CO is equivalent to the function ID , which is scoring factor LD ( TR ) in Eq. (4) is constant regardless of the level of matching,
Eq. (4) is identical to the score in the CO method. By using only ordered co-occurrence of two concept words, we can reveal the effect of relation words, namely dependency relationships. 4.4.3 Total Score of Relationships in ST and CO. We next define the simi-larity score between a query and a document element, with regard to relation-ships between words, using the scores of all matched triplets or pairs in the document element.

We obtained a scoring function from our previous research that is effective for good retrieval performance [Matsumura et al. 1999a, 1999b]: where
The function max selects the maximum score out of all scores of triplets or pairs matched in the document element for each triplet or pair in the query.
Because the frequency information of words is reflected in the TF X  X DF of the total score, the document element sometimes gains an unreasonably high score if all matched triplets or pairs are used for scoring the element. Therefore, when a triplet or pair in the query occurs many times in a document element, only its maximum score is used to calculate the score of the element. That is, the frequency of a triplet or pair is always considered to be one even when it occurs many times. 5. EXPERIMENT AND EVALUATION 5.1 Conditions of the Experiment
In this section, we show the results of experimental retrieval, using the NTCIR-1 set containing about 330,000 documents, 83 search topics, and their relevance judgments. Documents in NTCIR-1 are author-supplied abstracts of conference papers that have been presented at academic meetings. Each document con-tains the following bibliographical information: a document ID, title, and list of author(s); the name and date of the conference; the abstract and keyword(s) assigned by the author(s) of the document; and the name of the host academic society. A topic consists of a title, a description, a detailed narrative, a list of concepts, and a list of fields. For each topic, relevant documents and partially relevant documents are defined.

NTCIR-1 is the largest Japanese test collection with respect to the number of document entries and search topics. It is also unique because the documents in NTCIR-1 are author-supplied abstracts of academic conference papers. It is quite different from other Japanese test collections that are used for newspaper articles, such as the BMIRJ2 7 and IREX collections, and from the well-known English test collection TREC.

In our retrieval experiments, we used the description field, which was rep-resented by a sentence for each topic, as the query, and defined both relevant and partially relevant documents as relevant documents. We used abstracts as the target of retrieval X  X hat is, the weight of the elements in Eq. (1) was 0 except for xw abst and xr abst . Here, we write xw and xr , omitting the subscripts, define xr = 1  X  xw , and use 0  X  xw  X  1 as a bound for scoring documents.
Our method is then equivalent to the TF X  X DF method when xw define the weights of Exact Match ( we ) and Category Match ( wc ) to be equal to 1, because in preliminary experiments we found that  X  wc  X  had almost no influ-ence on the retrieval effectiveness. Therefore, we use the weight of Wild Match (0  X  ww  X  1) as another parameter characterizing the system. When ww
ST is equivalent to CO. Consequently, we can compare these three methods by changing only two parameters.

We performed two experiments. In the first, we divided the document set of NTCIR-1 into two subsets, denoted NTCIR-1a and NTCIR-1b, which each contain about 160,000 documents. We first performed retrieval experiments on
NTCIR-1a and tuned xw and ww for ST and xw for CO to gain the maximum 11-point average precision by investigating a two-dimensional parameter space ( ww versus xw ). We next tested ST and CO with the tuned parameter(s) on
NTCIR-1b. To make a fair evaluation, we selected 70 topics, each of which contained more than five relevant documents in each of the two subsets. This experiment is similar to the routing task at the TREC conference.

In the second experiment, we used all the documents in NTCIR-1. We first randomly selected 41 topics (topic set A) out of 83 topics. We performed retrieval experiments on NTCIR-1 by using topic set A and tuned the parameter(s) for ST and CO, respectively. We then tested the two methods with the tuned parame-ter(s) on the remaining 42 topics (topic set B) in NTCIR-1. This experiment is similar to the ad hoc task at the TREC conference. Table IV shows the experi-mental settings of the two experiments.

To precisely evaluate the effect of dependency relationships between words in IR, we checked the accuracy of extracting dependency relationships from the description fields of the 83 search topics. The process of extracting dependency relationships has three steps, as mentioned in the previous section: morpholog-ical analysis, dependency analysis, and compound noun analysis.

We found 14 error words in the morphological analysis and the accuracies of dependency analysis and compound noun analysis were 66.8 and 91.3%, re-spectively. In the description fields of the 83 search topics, there are 214 triplets to be analyzed using dependency analysis and 219 triplets to be analyzed us-ing compound noun analysis. The accuracy of each analysis is defined as the proportion of triplets that were correctly analyzed. 5.2 Results of the First Experiment
Table V and Figure 5 show the 11-point average precision versus the two param-eters ww and xw when searching NTCIR-1a. For clarity, only the data xw are shown in Figure 5. A fixed ratio of TF X  X DF, given in the upper right corner of the figure, is used for each curve. When xw = 1 (dash-dot-dot line in Figure 5),
ST is equivalent to the TF X  X DF method, and the 11-point average precision of these systems is 0.2529. On the other hand, when xw = 0, ST uses only de-pendency relationships and when xw = 0 and ww = 1, ST uses only ordered co-occurrences of concept words. With the optimum values of the two parame-ters for ST, the maximum 11-point average precision was 0.2944 for parameters ( xw , ww ) = (0 . 8, 0 . 6), an increase of about 16.4% over the TF X  X DF method. We also optimized the parameter xw of CO ( ww = 1). The maximum 11-point average precision of CO was 0.2885 at xw = 0 . 8, which is about 14.1% higher than that of the TF X  X DF method. These results are summarized in Table VI. Figure 6 illustrates the recall versus precision figures for TF X  X DF, ST, and CO, optimized using NTCIR-1a.

Table VII shows the results of test retrieval on NTCIR-1b. The 11-point av-erage precision of ST tested on NTCIR-1b using tuned parameters was 0.2487, which is 17.9% higher than that of TF X  X DF (0.2110). Similarly, the 11-point av-erage precision of CO was 0.2465, which is 16.8% higher than that of TF X  X DF.
Figure 7 illustrates the recall versus precision figures for TF X  X DF, ST, and CO tested on NTCIR-1b. This figure shows that ST and CO improve the precision at most recall levels. The difference between ST and CO is small.
 Figure 8 shows the difference in average precision per topic compared with
TF X  X DF. Results vary, depending on the topics, but our two methods achieve an extensive improvement of the average precision.
 Table VIII shows statistical significance tests for our two methods and the TF X  X DF method. The difference between ST and TF X  X DF and between CO and
TF X  X DF are significant. However, the difference between ST and CO is not statistically significant.

The first experiment shows that when the target documents in the database changed, ST and CO were still more effective than the TF X  X DF method, and the difference between ST and CO was small. 5.3 Results of the Second Experiment The results of tuning the systems using topic set A are summarized in Table IX.
The optimized 11-point average precision of ST using topic set A was 0.3011 with parameters ( xw , ww ) = (0 . 8, 0 . 7). This is 22.4% higher than that of TF X  IDF (0.2459). The 11-point average precision of CO optimized using topic set A was 0.2973 with parameter xw = 0 . 8, which is 20.9% higher than that of TF X  X DF.

Using the optimized parameter(s), we next performed test retrievals using topic set B. The results of retrieval experiments are shown in Table X. The 11-point average precision of ST tested using topic set B was 0.2592, which is still 14.8% higher than that of TF X  X DF (0.2258). The 11-point average precision of
CO was 0.2535, which is 12.3% higher than that of TF X  X DF. Figure 9 illustrates the recall versus precision figures for TF X  X DF, ST, and CO tested using topic set B. This shows that ST and CO improve precision at almost all recall levels. ST is shown to be superior to CO at medium recall levels.

Figure 10 shows the difference between the approaches and TF X  X DF, ex-pressed in average precision per topic. The result varies depending on the topic, as shown in the first experiment.
 Table XI shows statistical significance tests for our two methods and the
TF X  X DF method. The differences between ST and TF X  X DF, and between CO and TF X  X DF are statistically significant. The difference between ST and CO is also significant.
 The second experiment shows that ST and CO are more effective than the TF X  X DF method for new search topics. While the difference between ST and
CO is small, it is statistically significant. 5.4 Error Analysis
In this section, we analyze the results of our two experiments. As shown in Figures 8 and 10, the ST and CO methods return poor results with the same queries. Through analysis of these results, we found that our methods sometimes had difficulty extracting appropriate relationships, and that the weighting of extracted relationships is sometimes inappropriate. These two factors adversely influence the retrieval effectiveness. 5.4.1 The Effect of Extracting Inappropriate Relationships. For query No. 0061  X   X  ( X  X  method for extracting automatically hierarchical relations of words from linguistic resources X ), the relationship  X   X  (word) X  X   X  (hierarchical) is important, but the incorrect relationships  X   X  (word) X  X   X  (relation),  X   X  (word) X  X   X  (automatically), or  X   X  (word) X  X   X  (extracting) were extracted and given inappropriately high scores.

Other queries encountered the same problems. For query No. 0037  X   X  ( X  X ffective utilization of a buffer for data transfer X ), the relationships between  X   X  (data) and  X   X  (effective) or  X   X  (transfer) and  X   X  (utilization) were extracted. For query No. 0067  X  idering (the use of) school facilities as a refuge in natural disasters, or articles introducing the actual condition of them X ), the incorrect relationships, such as  X   X  (disaster) and  X   X  (facilities),  X   X  (disaster) and  X   X  (place  X   X  (in) and  X   X  (facilities) were given inappropriately high scores. These inappropriate relationships were produced as pseudodependency relationships. This suggests a reconsideration of the use of pseudodependency relationships.
In contrast, correct relationships in the query can be extracted incor- X   X  ( X  X he effectiveness of topologies of in-formation networks X ), the incorrect relationship  X   X  (network) and  X   X  (effectiveness) caused the retrieval to be less effective. At the same time, the correct relationship  X   X  (topologies) and  X   X  (effectiveness) was extracted incorrectly from a long sentence in the document. The reason these incorrect dependency relationships were extracted is that templates are not applicable to long complex sentences.

The definition of concept words and relation words is also imper-fect. For example, the word  X   X  (embedding) in query No. 0044  X   X  ( X  X echniques for embedding an electronic watermark into high frequency components of the image X ) and  X   X  (extracting) in query No. 0061 became both concept words and relation words according to the context in the document. This problem suggests that we should refine the usage of the triplet.
Relevant words also reduce the effectiveness of retrieval. For example, the following synonyms were critical for the retrieval performance:  X   X  (word) in query No. 0061 and  X   X  (term),  X   X  (data transfer) in query No. 0037 and  X   X  (frame transfer) or  X   X  (packet switching), and  X   X  (Braille translation) in query No. 0077  X   X  ( X  X he processing of mathematical expressions in Braille translation X ) and  X   X  (an abbreviation of  X  X raille translation X ). Systems must maintain a dictionary to deal with these relevant words.

Anaphora resolution would be effective for IR. If we could determine the subject to which the pronoun  X   X  (them) refers in query No. 0067, relevant documents could be retrieved. However, the solution of this problem requires complex NLP techniques, so it is a topic for future work. 5.4.2 Effect of the Weighting Scheme of Word Relationships. Problems were experienced with the weighting scheme of word relationships that were cor-rectly extracted.  X   X  (technique) in query No. 0044 and  X   X  (method) in query No. 0061 are not important words. The noise in the retrieval outputs increased as the relationships with these words were given unreasonably high scores by our scor-ing function. We could improve them if we classified these words into general words or refined the scoring function.

The relationship between  X   X  (information) and  X   X  (network) in query No. 0049 was given an unreasonably high score. Each of the words in the relationship is an important keyword, but their combination is extremely common and only decreases the retrieval precision, showing a problem with the score weighting.

The retrieval fails when the matched sentence does not contain the document subject. For example, query No. 0067 is exactly matched with a sentence in a document, but this document is not relevant because the sentence provides background about the document subjects. This shows that the analysis of the role of the sentence in a document is useful for IR. 6. CONCLUSION
We have proposed two IR methods that both utilize the relationships between words. One method uses dependency relationships between words ( ST ), while the other is an approximation to ST that uses ordered co-occurrence information about words ( CO ). We performed two experimental evaluations on our methods, and compared them with the TF X  X DF method using the Japanese test collection for IR systems NTCIR-1.

In both experiments, ST and CO outperformed TF X  X DF. In the first experi-ment, we divided NTCIR-1 into two sub-test collections, NTCIR-1a and NTCIR-1b, and performed tuning using NTCIR-1a and testing using NTCIR-1b. In the test retrieval, the mean average precisions of CO and ST were approximately 17 and 18% higher than that of TF X  X DF, respectively, showing that both of our methods are effective regardless of differences in the target databases. In the second experiment, we divided topics into set A and set B. We tuned the two methods using set A and tested using set B. In the test retrieval, the mean average precision of CO was about 21% higher than that of TF X  X DF, and the mean average precision of ST was about 22% higher than that of TF X  X DF, show-ing that the effectiveness of our two methods is independent of the search topic set. We showed that the difference between our methods and TF X  X DF methods is significant by using statistical tests.

In contrast, the difference between the effectiveness of ST and that of CO is small in both experiments. Statistical tests showed that the difference between our two methods is significant in the second experiment, but not in the first experiment.

The effect of ST nevertheless deserves more discussion. Through an error analysis of our two experiments, it was revealed that more precise determi-nation of dependency relationships, which could not be achieved by simple co-occurrence information, would improve the precision of retrieval. In addition, we found in our preliminary experiments that the accuracy of extracting de-pendency relationships was critical to the retrieval effectiveness of ST. Further improvements in extracting dependency relationships, which are achieved by refinement of the definition of relation words and templates , may improve the superiority of ST over CO .

Refinement of the compound noun analysis is also an important issue. Since our method uses only title statistics, there is ample scope for further improve-ment of the compound noun analysis. We will adopt the statistics both of title and abstract, or syntactic relations proposed by Yoon et al. [2001].
A topic for future research is to show that our methods are widely applica-ble for different documents and different search topics. When we conducted a preliminary experiment using the newspaper article collection of IREX, some improvement in performance over TF X  X DF was obtained, although it was small [Matsumura et al. 1999b]. This result shows that our method is effective for collections from different fields. We intend to experiment using various collec-tions, such as BMIRJ2 and NTCIR-2, 9 besides IREX, and to develop an exten-sively applicable IR method.

We must also analyze the retrieval output of each topic and use this analy-sis to generalize the system. In our experiments, optimization was performed to reach the highest 11-point average precision results, which was calculated as the mean of the average precisions of all topics. Therefore, the effect of the relationships between words in IR for each topic was not clear. We must op-timize the average precision for each topic and clarify the dependence of the relationships of words to retrieval performance.

Moreover, we must refine the scoring function to reflect the results of this analysis. To assess the improvement of the word score, we conducted a pre-liminary experiment that used a 2-Poisson model instead of TF X  X DF. In this experiment, although the superiority of our methods over the baseline (2-Poisson model) was small, we were able to greatly improve the effectiveness of our model from what we learned in these experiments. To utilize our meth-ods more effectively, improvement of the score for the relationships between words is indispensable. For example, we must verify term weight techniques other than IDF for scoring concept words in a triplet, such as information gain, mutual information, or Chi-square statistic. We also require the normaliza-tion of the score for each document element, refinement of the definition of the score for the relationships, and methods for combining the scores for the words and the relationships between words that are more effective than the lin-ear combination. Next, we will explore methods that automatically weight for each document element, according to their characteristics, such as the length of each element. For this purpose, we will perform an exhaustive examination throughout the parameter space.

Improving efficiency is another topic for future research as well as improv-ing effectiveness. Because we have designed our system as an experimental environment, the computational costs of both indexing and retrieval have not been optimized. To compare the two methods, i.e., CO and ST, as well as pa-rameter tuning, the current version of the index contains redundant informa-tion, such as the dependency relationships and the frequency information of words that are not used in our retrieval process. This redundant informa-tion increases the index sizes and the time to make them, so it is necessary to eliminate words and relationships in advance, as well as to improve the speed of the dependency analysis. To increase the retrieval speed, improving the matching scheme of dependencies (triplets) may be necessary as well as reducing the index size. We will first estimate the computational complexity of the retrieval processes for the ST and the CO, constructing optimized systems for both methods, and reveal the problem of the matching scheme of the ST.
Then, we will refine the matching scheme of the ST if necessary. Resolutions of these problems would make our systems useful in real world IR, such as Web searches.

Now that we have shown the advantage of IR methods that use relationships between words for Japanese text, we will extend our two methods to other languages, such as English, to achieve an effective cross-lingual retrieval method using the relationships between words. The Structured Index approach may be elaborated to form a language-independent index representation. This research is supported by the  X  X esearch for the Future Program, X  JSPS-RFTF96P00602, of the Japan Society for the Promotion of Science.

We used the NACSIS Test Collection 1. This collection was constructed by the NACSIS R &amp; D Department using a part of the  X  X atabase of Academic Conference Papers X  provided by academic societies. 10
