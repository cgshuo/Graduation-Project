 Department of Statistics, University of Pennsylvania Department of Statistics, University of Washington Department of Statistics, Harvard Univerity A determinantal point process (DPP) is a distribution over configurations of points. The defining characteristic of the DPP is that it is repulsive, which makes it useful for model-ing diversity. Recently, DPPs have played an increasingly important role in machine learning and statistics with ap-plications both in the discrete setting X  X here they are used as a diverse subset selection method (Affandi et al., 2012; 2013b; Gillenwater et al., 2012; Kulesza &amp; Taskar, 2010; 2011a; Snoek et al., 2013) X  and in the continuous setting for generating point configurations that tend to be spread out(Affandi et al., 2013a; Zou &amp; Adams, 2012). Formally, given a space  X   X  R d , a specific point config-uration A  X   X  , and a positive semi-definite kernel func-tion L :  X   X   X   X  R , the probability density under a DPP with kernel L is given by where L A is the | A | X | A | matrix with entries L ( x , y ) for each x , y  X  A . This defines a repulsive point process since point configurations that are more spread out according to the metric defined by the kernel L have higher densities. Building on work of Kulesza &amp; Taskar (2010), it is intuitive to decompose the kernel L as where q ( x ) can be interpreted as the quality function at point x and k ( x , y ) as the similarity kernel between points x and y . The ability to bias the quality in certain locations while still maintaining diversity via the similarity kernel offers great modeling flexibility.
 One of the remarkable aspects of DPPs is that they of-fer efficient algorithms for inference, including computing marginal and conditional probabilities (Kulesza &amp; Taskar, 2012), sampling (Affandi et al., 2013a;b; Hough et al., 2006; Kulesza &amp; Taskar, 2010), and restricting to fixed-sized point configurations ( k -DPPs) (Kulesza &amp; Taskar, 2011a). How-ever, an important component of DPP modeling, learning the DPP kernel parameters, is still considered a difficult, open problem. Even in the discrete  X  setting, DPP kernel learning has been conjectured to be NP-hard (Kulesza &amp; Taskar, 2012). Intuitively, the issue arises from the fact that in seeking to maximize the log-likelihood of Eq. (1) , the numerator yields a concave log-determinant term and the normalizer a convex term, leading to a non-convex objective. This non-convexity holds even under various simplifying as-sumptions on the form of L . Furthermore, when  X  is either a large, discrete set or a continuous subspace, computation of the likelihood is inefficient or infeasible, respectively. This precludes the use of gradient-based and black-box op-timization methods.
 Attempts to partially learn the kernel have been studied by, for example, learning the parametric form of the quality 2011b), or learning a weighting on a fixed set of kernel experts (Kulesza &amp; Taskar, 2011a). So far, the only attempt to learn the parameters of the similarity kernel k ( x , y ) has used Nelder-Mead optimization (Lavancier et al., 2012), which lacks theoretical guarantees about convergence to a stationary point. Moreover, the use of Nelder-Mead (and other black-box optimization methods) relies heavily on exact computation of the likelihood.
 In this paper, we consider parametric forms for the qual-ity function q ( x ) and similarity kernel k ( x , y ) and propose Bayesian methods to learn the DPP kernel parameters  X  using Markov chain Monte Carlo (MCMC). In addition to capturing posterior uncertainty rather than a single point estimate, our proposed methods apply without approxima-tion to large-scale discrete and continuous DPPs when the likelihood can only be bounded (with any desired precision). In Sec. 2, we review DPPs and their fixed-sized counterpart ( k -DPPs). We then explore maximum likelihood estimation (MLE) algorithms for learning DPP and k -DPP kernels. Af-ter examining the shortcomings of the MLE approach, we propose a set of techniques for Bayesian posterior inference of the kernel parameters in Sec. 3. In Sec. 4, we derive a set of DPP moments that can be used for model assessment, MCMC convergence diagnostics, and in low-dimensional settings for learning kernel parameters via numerical tech-niques. Finally, in Sec. 5 we use DPP learning to study the progression of diabetic neuropathy based on the spatial dis-tribution of nerve fibers and also to study human perception of diversity of images. 2.1. Discrete DPPs/ k -DPPs For a discrete base set  X  = { x 1 , x 2 ,..., x N } , a DPP de-fined by an N  X  N positive semi-definite kernel matrix L is a probability measure on the 2 N possible subsets A of  X  : Here, L A  X  [ L ij ] x i , x j  X  A is the submatrix of L indexed by the elements in A and I is the N  X  N identity matrix (Borodin &amp; Rains, 2005).
 In many applications, we are instead interested in the proba-bility distribution which gives positive mass only to subsets of a fixed size, k . In these cases, we consider fixed-sized DPPs (or k -DPPs) with probability distribution on sets A of cardinality k given by where  X  1 ,..., X  N are the eigenvalues of L and e (  X  1 ,..., X  N ) is the k th elementary symmetric polyno-mial (Kulesza &amp; Taskar, 2011a). Note that e k (  X  1 ,..., X  can be efficiently computed using recursion (Kulesza &amp; Taskar, 2012). 2.2. Continuous DPPs/ k -DPPs Consider now the case where  X   X  R d is a continuous space. DPPs extend to this case naturally, with L now a kernel operator instead of a matrix. Again appealing to Eq. (1) , the DPP probability density for point configurations A  X   X  is given by where  X  1 , X  2 ,... are eigenvalues of the operator L . The k -DPP also extends to the continuous case with where  X  1:  X  = (  X  1 , X  2 ,... ) .
 In contrast to the discrete case, the eigenvalues  X  i for con-tinuous DPP kernels are generally unknown; exceptions include a few kernels such as the Gaussian. Assume that we are given a training set consisting of sam-ples A 1 ,A 2 ,...,A T , and that we model these data using a DPP/ k -DPP with parametric kernel with parameters  X  . We denote the associated kernel matrix for a set A t by L A t ( X ) and the full kernel matrix/operator by L ( X ) . Likewise, we denote the kernel eigenvalues by  X  ( X ) . In this section, we explore various methods for DPP/ k -DPP learning. 3.1. Learning using Optimization Methods To learn the parameters  X  of a discrete DPP model, recalling Eq. (3) we can maximize the log-likelihood L ( X ) = Lavancier et al. (2012) suggest using the Nelder-Mead sim-plex algorithm (Nelder &amp; Mead, 1965). This method evalu-ates the objective function at the vertices of a simplex, then iteratively shrinks the simplex towards an optimal point. Al-though straightforward, this procedure does not necessarily converge to a stationary point (McKinnon, 1998). Gradient ascent and stochastic gradient ascent are attractive due to their theoretical guarantees, but require knowledge of the gradient of L ( X ) . In the discrete DPP setting, this gradient can be computed straightforwardly, and we provide exam-ples for discrete Gaussian and polynomial kernels in the Supplement.
 We note, however, that both of these methods are susceptible to convergence to local optima due to the non-convex like-lihood landscape. Furthermore, these methods (and many other black-box optimization techniques) require that the likelihood is known exactly. From the determinant in the de-nominator of Eq. (3) , we see that when the number of base items N is large, computing the likelihood or its derivative is inefficient. A similar inefficiency arises when we expect large sets A t , as determined by  X  . Both of these challenges limit the general applicability of these MLE approaches. Instead, in Sec. 3.3, we develop a Bayesian method that only requires an upper and lower bound on the likelihood. We focus on the large N challenge and discuss in the Sup-plement how analogous methods can be used for handling large observation sets, A t .
 The log-likelihood of the k -DPP kernel parameter is L ( X ) = which presents an addition complication due to needing a sum over n k terms in the gradient.
 For continuous DPPs/ k -DPPs, once again, both MLE optimization-based methods require that the likelihood is computable. Recalling Eq. (5) , we note the infinite product in the denominator. As such, for kernel operators with infi-nite rank (such as the Gaussian), we are forced to consider approximate MLE methods based on an explicit truncation of the eigenvalues. Gradient ascent using such truncations further relies on having a known eigendecomposition with a differentiable form for the eigenvalues. Unfortunately, such approximate gradients are not unbiased estimates of the true gradient, so the theory associated with stochastic gradient based approaches does not hold. 3.2. Bayesian Learning for Discrete DPPs Instead of optimizing the likelihood to get an MLE, we propose a Bayesian approach to estimating the posterior distribution over kernel parameters: for the DPP and, for the k -DPP, P ( X  | A 1 ,...,A T )  X  X  ( X ) Here, P ( X ) is the prior on  X  . Since neither Eq. (8) nor Eq. (9) yield a closed-form posterior, we resort to approximate techniques based on Markov chain Monte Carlo (MCMC). We highlight two techniques: random-walk Metropolis-Hastings (MH) and slice sampling. We note, however, that other MCMC methods can be employed with-out loss of generality, and may be more efficient in some scenarios.
 In random-walk MH, we use a proposal distribu-tion f (  X   X  |  X  i ) to generate a candidate value  X   X  given the current parameters  X  i , which are then accepted or rejected with probability min { r, 1 } where The proposal distribution f (  X   X  |  X  i ) is chosen to have mean  X  . The hyperparameters of f (  X   X  |  X  i ) tune the width of the distribution, determining the average step size. See Alg. 1 of the Supplement.
 While random-walk MH can provide a straightforward means of sampling from the posterior, its efficiency requires tuning the proposal distribution. Choosing an aggressive proposal can result in a high rejection rate, while choosing a conservative proposal can result in inefficient exploration of the parameter space. To avoid the need to tune the proposal distribution, we can instead use slice sampling (Neal, 2003). We first describe this method in the univariate case, follow-ing the  X  X inear stepping-out X  approach described in Neal (2003). Given the current parameter  X  i , we first sample y  X  Uniform [0 , P ( X  i | A 1 ,...,A T )] . This defines our slice with all values of  X  with P ( X  | A 1 ,...,A T ) greater than y included in the slice. We then define a random interval around  X  i with width w that is linearly expanded until nei-ther endpoint is in the slice. We propose  X   X  uniformly in the interval. If  X   X  is in the slice, it is accepted. Otherwise, becomes the new boundary of the interval, shrinking it so as to still include the current state of the Markov chain. This procedure is repeated until a proposed  X   X  is accepted. See Alg. 2 of the Supplement.
 There are many ways to extend this algorithm to a multidi-mensional setting. We consider the simplest extension pro-posed by Neal (2003) where we use hyperrectangles instead of intervals. A hyperrectangle region is constructed around  X  i and the edge in each dimension is expanded or shrunk depending on whether its endpoints lie inside or outside the slice. One could alternatively consider coordinate-wise or random-direction approaches to multidimensional slice sampling.
 As an illustrative example, we consider synthetic data gen-erated from a two-dimensional discrete DPP with k ( x i , x j ) = exp  X  where  X  = diag (0 . 5 , 0 . 5) and  X  = diag (0 . 1 , 0 . 2) . We con-sider  X  to be a grid of 100 points evenly spaced in a 10  X  10 unit square and simulate 100 samples from this DPP. We then condition on these simulated data and perform poste-rior inference of the kernel parameters using MCMC. Fig. 1 shows the sample autocorrelation function of the slowest mixing parameter,  X  11 , learned using random-walk MH and slice sampling. Furthermore, we ran a Gelman-Rubin test (Gelman &amp; Rubin, 1992) on five chains starting from overdispersed starting positions and found that the average partial scale reduction function across the four parameters to be 1.016 for MH and 1.023 for slice sampling, indicating fast mixing of the posterior samples. 3.3. Bayesian Learning for Large-Scale Discrete and When the number of items, N , for discrete  X  is large or when  X  is continuous, evaluating the normaliz-ers det( L ( X ) + I ) or Q  X  n =1 (  X  n ( X ) + 1) , respectively, can be inefficient or infeasible. Even in cases where an explicit form of the truncated eigenvalues can be computed, this will only lead to approximate MLE solutions, as discussed in Sec. 3.1.
 On the surface, it seems that most MCMC algorithms will suffer from the same problem since they require knowledge of the likelihood as well. However, we argue that for most of these algorithms, an upper and lower bound of the posterior probability is sufficient as long as we can control the ac-curacy of these bounds. We denote the upper and lower bounds by P + ( X  | A 1 ,...,A T ) and P  X  ( X  | A 1 ,...,A respectively. In the random-walk MH algorithm we can then compute the upper and lower bounds on the acceptance ratio, The threshold u  X  Uniform [0 , 1] can be precomputed, so we can often accept or reject the proposal  X   X  even if these bounds have not completely converged. All that is necessary is for u &lt; min { 1 ,r  X  } (immediately reject) or u &gt; min { 1 ,r + } (immediately accept). In the case that u  X  ( r  X  ,r + ) , we can perform further computations to increase the accuracy of our bounds until a decision can be made. As we only sample u once in the beginning, this iterative procedure yields a Markov chain with the exact tar-get posterior as its stationary distribution; all we have done is  X  X hort-circuit X  the computation once we have bounded the acceptance ratio r away from u . We show this procedure in Alg. 3 of the Supplement.
 The same idea applies to slice sampling. In the first step of generating a slice, instead of sampling y  X  Uniform [0 , P ( X  i | A 1 ,...,A T )] , we use a rejection sam-pling scheme to first propose a candidate slice We then decide whether  X  y &lt; P  X  ( X  i | A 1 ,...,A T ) , in which case we know  X  y &lt; P ( X  i | A 1 ,...,A T ) and we accept  X  y as the slice and set y =  X  y . In the case where  X  y  X  ( P  X  ( X  i | A 1 ,...,A T ) , P + ( X  i | A 1 ,...,A T )) , we keep increasing the tightness of our bounds until a decision can be made. If at any point  X  y exceeds the newly computed P + ( X  i | A 1 ,...,A T ) , we know that  X  y &gt; P ( X  i | A 1 ,...,A T ) so we reject the proposal. In this case, we generate a new  X  y and repeat.
 Upon accepting a slice y , the subsequent steps for proposing a parameter  X   X  proceed in a similarly modified manner. For the interval computation, the endpoints  X  e are each exam-ined to decide whether y &lt; P  X  ( X  e | A 1 ,...,A T ) (endpoint is not in slice) or y &gt; P + ( X  e | A 1 ,...,A T ) (endpoint is in slice). The tightness of the posterior bounds is increased un-til a decision can be made and the interval adjusted, if need be. After convergence,  X   X  is generated uniformly over the interval and is likewise tested for acceptance. We illustrate this procedure in Fig. 1 of the Supplement.
 The lower and upper posterior probability bounds can be incorporated in many MCMC algorithms, and provide an effective means of garnering posterior samples assuming the bounds can be efficiently tightened. For DPPs, the upper and lower bounds depend on the truncation of the kernel eigenvalues and can be arbitrarily tightened by including more terms.
 In the discrete DPP/ k -DPP settings, the eigenvalues can be efficiently computed to a specified point using methods such as power law iterations. The corresponding bounds for a 3600  X  3600 Gaussian kernel example are shown in Fig. 2. In the continuous setting, explicit truncation can be done when the kernel has Gaussian quality and similarity, as we show in Sec. 5.1. For other continuous DPP kernels, low-rank approximations can be used (Affandi et al., 2013a) resulting in approximate posterior samples (even after con-vergence of the Markov chain). We believe these methods could be used to get exact posterior samples by extending the discrete-DPP Nystr  X  om theory of Affandi et al. (2013b), but this is beyond the scope of this paper. In contrast, a gradient ascent algorithm for MLE is not even feasible: we do not know the form of the approximated eigenvalues, so we cannot take their derivative.
 Explicit forms for the DPP/ k -DPP posterior probability bounds as a function of the eigenvalue truncations follow from Prop. 3.1 and 3.2 combined with Eqs. (8) and (9) , respectively. Proofs are in the Supplement.
 Proposition 3.1. Let  X  1:  X  be the eigenvalues of kernel L . Then and
Y Proposition 3.2. Let  X  1:  X  be the eigenvalues of kernel L . Then and Note that the expression tr ( L ) in the bounds can be eas-ily computed as either P N i =1 L ii in the discrete case or R
 X  L ( x , x ) d x in the continuous case. In this section, we derive a set of DPP moments that can be used in a variety of ways. For example, we can compute the theoretical moments associated with each of our posterior samples and use these as summary statistics in assessing convergence of the MCMC sampler, e.g., via Gelman-Rubin diagnostics (Gelman &amp; Rubin, 1992). Likewise, if we ob-serve that these posterior-sample-based moments do not cover the empirical moments of the data, this can usefully hint at a lack of posterior consistency and a potential need to revise the misspecified prior.
 In the discrete case, we first need to compute the marginal probabilities. Borodin (2009) shows that the marginal ker-nel, K , can be computed directly from L : The m th moment can then be calculated via In the continuous case, given the eigendecomposition of the kernel operator, L ( x , y ) = P  X  n =1  X  n  X  n ( x )  X   X   X  ( x )  X  denotes the complex conjugate of the n th eigenfunc-tion), the m th moment is Note that Eq. (20) generally cannot be evaluated in closed form since the eigendecompositions of most kernel opera-tors are not known. However, in certain cases, such as the Gaussian kernel of Sec. 5.1 with eigenfunctions given by Hermite polynomials, the moments can be directly com-puted. In the Supplement, we derive the m th moment for this Gaussian kernel setting.
 Unfortunately, the method of moments can be challenging to use for direct parameter learning since Eqs. (19) and (20) rarely yield analytic forms that are solvable for  X  . In low dimensions,  X  can be estimated numerically, but it is an open question to estimate these moments for large-scale problems.
 5.1. Simulations We provide an explicit example of Bayesian learning for a continuous DPP with the kernel defined by k ( x , y ) = Here,  X  = {  X , X  d , X  d } and the eigenvalues of the operator L ( X ) are given by (Fasshauer &amp; McCourt, 2012),  X  m ( X ) =  X  is a multi-index. Furthermore, the trace of L ( X ) can be easily computed as tr ( L ( X )) = We test our Bayesian learning algorithms on simulated data generated from a 2-dimensional isotropic kernel (  X  d =  X  ,  X  d =  X  for d = 1 , 2 ) using Gibbs sampling (Affandi et al., 2013a). We then learn the parameters under weakly infor-mative inverse gamma priors on  X  ,  X  and  X  . Details are in the Supplement. We consider the following simulation scenarios: (i) 10 DPP samples with average number of points=18 (ii) 1000 DPP samples with average number of points=18 (iii) 10 DPP samples with average number of points=77 Fig. 3 shows trace plots of the posterior samples for all three scenarios. In the first scenario, the parameter estimates vary wildly whereas in the other two scenarios, the posterior estimates are more stable. In all cases, the zeroth and sec-ond moment estimated from the posterior samples are in the neighborhood of the corresponding empirical moments. This leads us to believe that the posterior is broad when we have both a small number of samples and few points in each sample. The posterior becomes more peaked as the total number of points increases. The stationary similarity kernel allows us to garner information either from few sets with many points or many sets of few points.
 Dispersion Measure In many applications, we are inter-ested in quantifying the overdispersion of point process data. In spatial statistics, a standard dispersion measure is the Ripley K -function (Ripley, 1977). We instead aim to use the learned DPP parameters (encoding repulsion) to quan-tify overdispersion. Importantly, our measure should be invariant to scaling. In the Supplement we derive that, as the data are scaled from x to  X  x , the parameters scale from (  X , X  i , X  i ) to (  X , X  X  i , X  X  i ) . This suggests that an appropriate scale-invariant repulsion measure is  X  i =  X  i / X  i . 5.2. Applications Recent breakthroughs in skin tissue imaging have spurred interest in studying the spatial patterns of nerve fibers in diabetic patients. It has been observed that these nerve fibers become more clustered as diabetes progresses. Waller et al. (2011) previously analyzed this phenomena based on 6 thigh nerve fiber samples. These samples were collected from 5 diabetic patients at different stages of diabetic neuropathy and one healthy subject. On average, there are 79 points in each sample (see Fig. 4). Waller et al. (2011) analyzed the Ripley K -function and concluded that the difference between the healthy and severely diabetic samples is highly significant.
 We instead study the differences between these samples by learning the kernel parameters of a DPP and quantifying the level of repulsion of the point process. Due to the small sample size, we consider a 2-class study of Normal/Mildly Diabetic versus Moderately/Severely Diabetic. We perform two analyses. In the first, we directly quantify the level of repulsion based on our scale-invariant statistic,  X  =  X / X  (see Sec. 5.1). In the second, we perform a leave-one-out classification by training the parameters on the two classes with one sample left out. We then evaluate the likelihood of the held-out sample under the two learned classes. We repeat this for all six samples.
 We model our data using a 2-dimensional continuous DPP with Gaussian quality and similarity as in Eqs. (21) and (22) . Since there is no observed preferred direction in the data, we use an isotropic kernel (  X  d =  X  and  X  d =  X  for d = 1 , 2 ). We place weakly informative inverse gamma pri-ors on (  X , X , X  ) , as specified in the Supplement, and learn the parameters using slice sampling with eigenvalue bounds as outlined in Sec. 3.3. The results shown in Fig. 5 indicate that our  X  measure clearly separates the two classes, concur-ring with the results of Waller et al. (2011). Furthermore, we are able to correctly classify all six samples. While the results are preliminary, being based on only six observations, they show promise for this task. We also examine DPP learning for quantifying how visual features relate to human perception of diversity in different image categories. This is useful in applications such as image search, where it is desirable to present users with a set of images that are not only relevant to the query, but diverse as well.
 Building on work by Kulesza &amp; Taskar (2011a), three image categories X  cars , dogs and cities  X  X ere studied. Within each category, 8-12 subcategories (such as Ford for cars , London for cities and poodle for dogs ) were queried from Google Image Search and the top 64 results were retrieved. For a subcategory subcat , these images form our base set  X  subcat . To assess human perception of diversity, annotated sets of size six were generated from these base sets. How-ever, it is challenging to ask a human to coherently select six diverse images from a set of 64 total. Instead, Kulesza &amp; Taskar (2011a) generated a partial result set of five images from a 5-DPP on each  X  subcat with a kernel based on the SIFT256 features (see the Supplement). Human annotators (via Amazon Mechanical Turk) were then presented with two images selected at random from the remaining subcate-gory images and asked to add the image they felt was least similar to the partial result set. These experiments resulted in about 500 samples spread evenly across the different subcategories.
 We aim to study how the human annotated sets differ from the top six Google results, Top-6 . As in Kulesza &amp; Taskar (2011a), we extracted three types of features from the images X  color features, SIFT descriptors (Lowe, 1999; Vedaldi &amp; Fulkerson, 2010) and GIST descriptors (Oliva &amp; Torralba, 2006) described in the Supplement. We denote these features for image i as f color i , f SIFT i , and f spectively. For each subcategory, we model our data as a discrete 6-DPP on  X  subcat with kernel for feat  X  X  color , SIFT , GIST } and i,j indexing the 64 images in  X  subcat . Here, we assume that each category has the same parameters across subcategories, namely,  X  cat feat subcat  X  cat and cat  X  X  cars , dogs , cities } .
 To learn from the Top-6 images, we consider the samples as being generated from a 6-DPP. To emphasize the human component of the 5-DPP + human annotation sets, we examine a conditional 6-DPP (Kulesza &amp; Taskar, 2012) that fixes the five partial results set images and only considers the probability of adding the human-annotated image. The Supplement provides details on this conditional k -DPP. All subcategory samples within a category are assumed to be independent draws from a DPP on  X  subcat with kernel L subcat parameterized by a shared set of  X  cat each of these samples equally informs the posterior of  X  cat We samples the posterior of the 6-DPP or conditional 6-DPP kernel parameters using slice sampling with weakly informative inverse gamma priors on the  X  cat feat . Details are in the Supplement.
 Fig. 6 shows a comparison between  X  cat feat learned from the human annotated samples (conditioning on the 5-DPP par-tial result sets) and the Top-6 samples for different cate-gories. The results indicate that the 5-DPP + human anno-tated samples differs significantly from the Top-6 samples in the features judged by human to be important for diver-sity in each category. For cars and dogs , human annotators deem color to be a more important feature for diversity than the Google search engine based on their Top-6 results. For cities , on the other hand, the SIFT features are deemed more important by human annotators than by Google. Keep in mind, though, that this result only highlights the diversity components of the results while ignoring quality. In real life applications, it is desirable to combine both the quality of each image (as a measure of relevance of the image to the query) and the diversity between the top results. Regardless, we have shown that DPP kernel learning can be informative of judgements of diversity, and this information could be used (for example) to tune search engines to provide results more in accordance with human judgement. Determinantal point processes have become increasingly popular in machine learning and statistics. While many important DPP computations are efficient, learning the pa-rameters of a DPP kernel is difficult. This is due to the fact that not only is the likelihood function non-convex, but in many scenarios the likelihood and its gradient are either unknown or infeasible to compute. We proposed Bayesian approaches using MCMC for inferring these pa-rameters. In addition to providing a characterization of the posterior uncertainty, these algorithms can be used to deal with large-scale discrete and continuous DPPs based solely on likelihood bounds. We demonstrated the utility of learn-ing DPP parameters in studying diabetic neuropathy and evaluating human perception of diversity in images. This research was supported in part by AFOSR Grant FA9550-12-1-0453 and DARPA Young Faculty Award N66001-12-1-4219.

Affandi, R. H., Kulesza, A., and Fox, E. B. Markov deter-minantal point processes. In Proc. UAI , 2012.

Affandi, R. H., Fox, E.B., and Taskar, B. Approximate inference in continuous determinantal processes. In Proc. NIPS , 2013a.
 Affandi, R.H., Kulesza, A., Fox, E.B., and Taskar, B.
Nystr  X  om approximation for large-scale determinantal pro-cesses. In Proc. AISTATS , 2013b.

Borodin, A. Determinantal point processes. arXiv preprint arXiv:0911.1153 , 2009.
 Borodin, A. and Rains, E.M. Eynard-Mehta theorem,
Schur process, and their Pfaffian analogs. Journal of Sta-tistical Physics , 121(3):291 X 317, 2005.
 Fasshauer, G.E. and McCourt, M.J. Stable evaluation of
Gaussian radial basis function interpolants. SIAM Journal on Scientific Computing , 34(2):737 X 762, 2012.

Gelman, A. and Rubin, D. B. Inference from iterative simulation using multiple sequences. Statistical Science , pp. 457 X 472, 1992.

Gillenwater, J., Kulesza, A., and Taskar, B. Discovering diverse and salient threads in document collections. In Proc. EMNLP , 2012.

Hough, J.B., Krishnapur, M., Peres, Y., and Vir  X  ag, B. Deter-minantal processes and independence. Probability Surveys , 3:206 X 229, 2006.

Kulesza, A. and Taskar, B. Structured determinantal point processes. In Proc. NIPS , 2010.

Kulesza, A. and Taskar, B. k-DPPs: Fixed-size determi-nantal point processes. In Proc. ICML , 2011a.

Kulesza, A. and Taskar, B. Learning determinantal point processes. In Proc. UAI , 2011b.

Kulesza, A. and Taskar, B. Determinantal point processes for machine learning. Foundations and Trends in Machine Learning , 5(2 X 3), 2012.

Lavancier, F., M X ller, J., and Rubak, E. Statistical as-pects of determinantal point processes. arXiv preprint arXiv:1205.4818 , 2012.

Lowe, D. G. Object recognition from local scale-invariant features. In Proc. IEEE International Conference on Com-puter Vision , 1999.

McKinnon, K. I.M. Convergence of the Nelder X  X ead simplex method to a nonstationary point. SIAM Journal on Optimization , 9(1):148 X 158, 1998.

Neal, R. M. Slice sampling. Annals of Statistics , pp. 705 X  741, 2003.

Nelder, J. and Mead, R. A simplex method for function minimization. Computer Journal , 7(4):308 X 313, 1965. Oliva, A. and Torralba, A. Building the gist of a scene:
The role of global image features in recognition. Progress in Brain Research , 155:23 X 36, 2006.
 Ripley, B. D. Modelling spatial patterns. Journal of the
Royal Statistical Society. Series B (Methodological) , pp. 172 X 212, 1977.

Snoek, J., Zemel, R., and Adams, R. P. A determinantal point process latent variable model for inhibition in neural spiking data. In Proc. NIPS , 2013.

Vedaldi, A. and Fulkerson, B. Vlfeat: An open and portable library of computer vision algorithms. In Proc. Interna-tional Conference on Multimedia , 2010.
 Panoutsopoulou, I.G., Kennedy, W.R., and Wendelschafer-
Crabb, G. Second-order spatial analysis of epidermal nerve fibers. Statistics in Medicine , 30(23):2827 X 2841, 2011.
Zou, J. and Adams, R.P. Priors for diversity in generative
