 Many natural language processing tasks rely on the ability to identify words and phrases with equiva-lent meaning but different wording. These alterna-tive ways of expressing the same information are called paraphrases. Several research efforts have produced automatically generated databases of En-glish paraphrases, including DIRT (Lin and Pantel, 2001), the Microsoft Research Paraphrase Phrase Tables (Dolan et al., 2004), and the Paraphrase Database (Ganitkevitch et al., 2013; Pavlick et al., 2015a). A primary benefit of these automatically generated resources is their enormous scale, which provides superior coverage compared to manually compiled resources like WordNet (Miller, 1995). But automatically generated paraphrase resources currently have the drawback that they group all senses of polysemous words together, and do not partition paraphrases into groups like WordNet does Figure 1: Our goal is to partition paraphrases of an input word like bug into clusters representing its dis-tinct senses. with its synsets. Thus a search for paraphrases of the noun bug would yield a single list of para-phrases that includes insect, glitch, beetle, error, mi-crobe, wire, cockroach, malfunction, microphone, mosquito, virus, tracker, pest, informer, snitch, para-site, bacterium, fault, mistake, failure and many oth-ers. The goal of this work is to group these para-phrases into clusters that denote the distinct senses of the input word or phrase, as shown in Figure 1.
We develop a method for clustering the para-phrases from the Paraphrase Database (PPDB). PPDB contains over 100 million paraphrases gen-erated using the bilingual pivoting method (Ban-nard and Callison-Burch, 2005), which posits that two English words are potential paraphrases of each other if they share one or more foreign translations. We apply two clustering algorithms, Hierarchical Graph Factorization Clustering (Yu et al., 2005; Sun and Korhonen, 2011) and Self-Tuning Spec-tral Clustering (Ng et al., 2001; Zelnik-Manor and Perona, 2004), and systematically explore different ways of defining the similarity matrix that they use as input. We exploit a variety of features from PPDB to cluster its paraphrases by sense, including its im-Figure 2: SEMCLUST connects all paraphrases that share foreign alignments, and cuts edges below a dynamically-tuned cutoff weight (dotted lines). The resulting connected components are its clusters. plicit graph structure, aligned foreign words, para-phrase scores, predicted entailment relations, and monolingual distributional similarity scores.
Our goal is to determine which algorithm and features are the most effective for clustering para-phrases by sense. We address three research ques-tions:  X  Which similarity metric is best for sense clus- X  Are better clusters produced by comparing  X  Can entailment relations inform sense cluster-Our method produces sense clusters that are qualita-tively and quantitatively good, and that represent a substantial improvement to the PPDB resource. The paraphrases in PPDB are already partitioned by syntactic type, following the work of Callison-Burch (2008). He showed that applying syntac-tic constraints during paraphrase extraction via the pivot method improves paraphrase quality. This means that paraphrases of the noun bug are sepa-rated from paraphrases of the verb bug , which con-sist of verbs like bother, trouble, annoy, disturb , and others. However, organizing paraphrases this way still leaves the issue of mixed senses within a single part of speech. This lack of sense distinction makes it difficult to decide when a paraphrase in PPDB would be an appropriate substitute for a word in a given sentence. Some researchers resort to crowd-sourcing to determine when a PPDB substitution is valid (Pavlick et al., 2015c).

Our sense clustering work is closely related to the task of word sense induction (WSI), which aims to discover all senses of a target word from large cor-pora. One family of common approaches to WSI aims to discover the senses of a word by clustering the monolingual contexts in which it appears (Nav-igli, 2009). Another uncovers a word X  X  senses by clustering its foreign alignments from parallel cor-pora (Diab, 2003). A more recent family of ap-proaches to WSI represents a word as a feature vec-tor of its substitutable words, i.e. paraphrases (Mela-mud et al., 2015; Yatbaz et al., 2012). In this paper we take inspiration from each of these families of approaches, and we explore them when measuring word similarity in sense clustering.
 The work most closely related to ours is that of Apidianaki et al. (2014), who used a simple graph-based approach to cluster pivot paraphrases on the basis of contextual similarity and shared foreign alignments. Their method represents paraphrases as nodes in a graph and connects each pair of words sharing one or more foreign alignments with an edge weighted by contextual similarity. Concretely, for paraphrase set P , it constructs a graph G = ( V, E ) where vertices V = { p i  X  P } are words in the paraphrase set and edges connect words that share foreign word alignments in a bilingual parallel cor-pus. The edges of the graph are weighted based on their contextual similarity (computed over a mono-lingual corpus). In order to partition the graph into clusters, edges in the initial graph G with contex-tual similarity below a threshold T 0 are deleted. The connected components in the resulting graph G 0 are taken as the sense clusters. The threshold is dynami-cally tuned using an iterative procedure (Apidianaki and He, 2010).
As evaluated against reference clusters derived from SEMEVAL 2007 Lexical Substitution gold data (McCarthy and Navigli, 2007), their method, which we call SEMCLUST, outperformed simple most-frequent-sense, one-sense-per-paraphrase, and random baselines. Apidianaki et al. (2014) X  X  work corroborated the existence of sense distinctions in the paraphrase sets, and highlighted the need for fur-ther work to organize them by sense. In this paper, we improve on their method using more advanced clustering algorithms, and by systematically explor-ing a wider range of similarity measures. To partition paraphrases by sense, we use two ad-vanced graph clustering methods rather than using Apidianaki et al. (2014) X  X  edge deletion approach. Both of them allow us to experiment with a variety of similarity metrics. 3.1 Hierarchical Graph Factorization The Hierarchical Graph Factorization Clustering (HGFC) method was developed by Yu et al. (2006) to probabilistically partition data into hierarchical clusters that gradually merge finer-grained clusters into coarser ones. Sun and Korhonen (2011) ap-plied HGFC to the task of clustering verbs into Levin (1993)-style classes. Sun and Korhonen ex-tended the basic HGFC algorithm to automatically discover the latent tree structure in their clustering solution and incorporate prior knowledge about se-mantic relationships between words. They showed that HGFC far outperformed agglomerative cluster-ing methods on their verb data set. We adopt Sun and Korhonen X  X  implementation of HGFC for our experiments.

HGFC takes as input a nonnegative, symmet-ric adjacency matrix W = { w ij } where rows and columns represent paraphrases p i  X  P , and en-tries w ij denote the similarity between paraphrases sim D ( p i , p j ) . The algorithm works by factorizing W into a bipartite graph, where the nodes on one side represent paraphrases, and nodes on the other represent senses. The output of HGFC is a set of clusterings of increasingly coarse granularity, which we can also represent with a tree structure. The algo-rithm automatically determines the number of clus-ters at each level. For our task, this has the benefit that a user can choose the cluster granularity most appropriate for the downstream task (as illustrated in Figure 5). Another benefit of HGFC is that it probabilistically assigns each paraphrase to a clus-ter at each level of the hierarchy. If some p i has high probability in multiple clusters, we can assign p i to all of them (Figure 3c). 3.2 Spectral Clustering The second clustering algorithm that we use is Self-Tuning Spectral Clustering (Zelnik-Manor and Per-ona, 2004). Like HGFC, spectral clustering takes an adjacency matrix W as input, but the similari-ties end there. Whereas HGFC produces a hierar-chical clustering, spectral clustering produces a flat clustering with k clusters, with k specified at run-time. The Zelnik-Manor and Perona (2004) X  X  self-tuning method is based on Ng et al. (2001) X  X  spectral clustering algorithm, which computes a normalized Laplacian matrix L from the input W , and executes K-means on the largest k eigenvectors of L . Intu-itively, the largest k eigenvectors of L should align with the k senses in our paraphrase set. Each of our clustering algorithms take as input an adjacency matrix W where the entries w ij corre-spond to some measure of similarity between words i and j . For the paraphrases in Figure 1, W is a 20x20 matrix that specifies the similarity of every pair of paraphrases like microbe and bacterium or microbe and malfunction . We systematically inves-tigated four types of similarity scores to populate W . 4.1 Paraphrase Scores Bannard and Callison-Burch (2005) defined a para-phrase probability in order to quantify the goodness of a pair of paraphrases, based on the underlying translation probabilities used by the bilingual piv-oting method. More recently, (Pavlick et al., 2015a) used supervised logistic regression to combine a va-riety of scores so that they align with human judge-ments of paraphrase quality. PPDB 2.0 provides this score for each pair of words in the database. The PPDB 2.0 score is a nonnegative real number that HGFC for query word bug (n) can be used directly as a similarity measure: PPDB 2.0 does not provide a score for a word with itself, so we set P P DB 2 . 0 Score ( i, i ) to be the max-imum P P DB 2 . 0 Score ( i, j ) such that i and j have the same stem. 4.2 Second-Order Paraphrase Scores Work by Rapp (2003) and Melamud et al. (2015) showed that comparing words on the basis of their shared paraphrases is effective for WSI. We define two novel similarity metrics that calculate the simi-larity of words i and j by comparing their second-order paraphrases. Instead of comparing microbe and bacterium directly with their PPDB 2.0 score, we look up all of the paraphrases of microbe and all of the paraphrases of bacterium , and compare those two lists.

Specifically, we form notional word-paraphrase feature vectors v p respond to words with which each is connected in PPDB, and the value of the k th element of v p P P DB 2 . 0 Score ( i, k ) . We can then calculate the cosine similarity or Jensen-Shannon divergence be-tween vectors: Figure 4: Comparing second-order paraphrases for malfunction and fault based on word-paraphrase vectors. The value of vector element v ij is P P DB 2 . 0 Score ( i, j ) . where JS ( v p paraphrase probability distribution for word i is given by its normalized word-paraphrase vector v p 4.3 Similarity of Foreign Word Alignments When an English word is aligned to several foreign words, sometimes those different translations indi-cate a different word sense (Yao et al., 2012). Using this intuition, Gale et al. (1992) trained an English WSD system on a bilingual corpus, using the dif-ferent French translations as labels for the English word senses. For instance, given the English word duty , the French translation droit was a proxy for its tax sense and devoir for its obligation sense.
PPDB is derived from bilingual coropra. We re-cover the aligned foreign words and their associated translation probabilities that underly each PPDB en-try. For each English word in our dataset, we get each foreign word that it aligns to in the Spanish and Chinese bilingual parallel corpora used by Ganitke-vitch and Callison-Burch (2014). We use this to de-fine a novel foreign word alignment similarity met-ric, sim T RANS ( i, j ) for two English paraphrases i and j . This is calculated as the cosine similarity of the word-alignment vectors v a i and v a j where each feature in v a is a foreign word to which i or j aligns, ity p ( f | i ) . 4.4 Monolingual Distributional Similarity Lastly, we populate the adjacency with a distri-butional similarity measure based on WORD 2 VEC (Mikolov et al., 2013). Each paraphrase i in our data set is represented as a 300-dimensional WORD 2 VEC embedding v w i trained on part of the Google News dataset. Phrasal paraphrases that did not have an en-try in the WORD 2 VEC dataset are represented as the mean of their individual word vectors. We use the cosine similarity between WORD 2 VEC embeddings as our measure of distributional similarity. The optimal number of clusters for a set of para-phrases will vary depending on how many senses there ought to be for an input word like bug . It is generally recognized that optimal sense granu-larity depends on the application (Palmer et al., 2001). WordNet has notoriously fine-grained senses, whereas most word sense disambiguation systems achieve better performance when using coarse-grained sense inventories (Navigli, 2009). Depending on the task, the sense clustering for query word coach in Figure 5b with k = 5 clusters may be preferable to the alternative with k = 3 clusters. An ideal algorithm for our task would enable clustering at varying levels of granularity to support different downstream NLP applications.

Both of our clustering algorithms can produce sense clusters at varying granularities. For HGFC this requires choosing which level of the resulting tree structure to take as a clustering solution, and for spectral clustering we must specify the number of timal number of clusters, we use the mean Silhou-ette Coefficient (Rousseeuw, 1987) which balances optimal inter-cluster tightness and intra-cluster dis-tance. The Silhouette Coefficient is calculated for each paraphrase p i as where a ( p i ) is p i  X  X  average intra-cluster distance (average distance from p i to each other p j in the same cluster), and b ( p i ) is p i  X  X  lowest average inter-cluster distance (distance from p i to the nearest ex-ternal cluster centroid). For each clustering algo-rithm, we choose as the  X  X olution X  the clustering which produces the highest mean Silhouette Coef-ficient. The Silhouette Coefficient calculation takes as input a matrix of pairwise distances, so we simply use 1  X  W where the adjacency matrix W is calcu-lated using one of the similarity methods we defined. Pavlick et al. (2015b) added a set of automatically predicted semantic entailment relations for each en-try in PPDB 2.0. The entailment types that they in-clude are Equivalent, Forward Entailment, Reverse Entailment, Exclusive , and Independent . While a negative entailment relationship ( Exclusive or Inde-pendent ) does not preclude words from belonging to the same sense of some query word, a positive en-tailment relationship ( Equivalent, Forward/Reverse Entailment ) does give a strong indication that the words belong to the same sense.

We seek a straightforward way to determine whether entailment relations provide information that is useful to the final clustering algorithm. Both of our algorithms take an adjacency matrix W as input, so we add entailment information by simply Figure 5: HGFC and Spectral Clustering results for coach (n) . Our silhouette optimization sets k = 3 . multiplying each pairwise entry by its entailment probability. Specifically, we set w where p ind ( i, j ) gives the PPDB 2.0 probability that there is an Independent entailment relationship be-tween words i and j . Intuitively, this should increase the similarity of words that are very likely to be en-tailing like fault and failure , and decrease the simi-larity of non-entailing words like cockroach and mi-crophone . We follow the experimental setup of Apidianaki et al. (2014). We focus our evaluation on a set of query words drawn from the LexSub test data (McCarthy and Navigli, 2007), plus 16 additional handpicked polysemous words. 7.1 Gold Standard Clusters One challenge in creating our clustering methodol-ogy is that there is no reliable PPDB-sized standard against which to assess our results. WordNet synsets provide a well-vetted basis for comparison, but only allow us to evaluate our method on the 38% of our PPDB dataset that overlaps it. We therefore evaluate performance on two test sets.
 WordNet+ Our first test set is designed to assess how well our solution clusters align with WordNet synsets. We chose 185 polysemous words from the SEMEVAL 2007 dataset and an additional 16 hand-picked polysemous words. For each we formed a paraphrase set that was the intersection of their PPDB 2.0 XXXL paraphrases with their WordNet synsets, and their immediate hyponyms and hyper-nyms. Each reference cluster consisted of a Word-Net synset, plus the hypernyms and hyponyms of words in that synset. On average there are 7.2 refer-ence clusters per paraphrase set.
 CrowdClusters Because the coverage of Word-Net is small compared to PPDB, and because Word-Net synsets are very fine-grained, we wanted to cre-ate a dataset that would test the performance of our clustering algorithm against large, noisy paraphrase sets and coarse clusters. For this purpose we ran-domly selected 80 query words from the SEMEVAL 2007 dataset and created paraphrase sets from their unfiltered PPDB2.0 XXL entries. We then itera-tively organized each paraphrase set into reference senses with the help of crowd workers on Amazon Mechanical Turk. On average there are 4.0 reference clusters per paraphrase set. A full description of our method is included in the supplemental materials. 7.2 Evaluation Metrics We evaluate our method using two standard metrics: the paired F-Score and V-Measure. Both were used in the 2010 SemEval Word Sense Induction Task (Manandhar et al., 2010) and by Apidianaki et al. (2014). We give our results in terms of weighted av-erage performance on these metrics, where the score for each individual paraphrase set is weighted by the number of reference clusters for that query word. Paired F-Score frames the clustering problem as a classification task (Manandhar et al., 2010). It gen-erates the set of all word pairs belonging to the same reference cluster, F ( S ) , and the set of all word pairs belonging to the same automatically-generated clus-ter, F ( K ) . Precision, recall, and F-score can then be calculated in the usual way, i.e. P = F ( K )  X  F ( S ) V-Measure assesses the quality of a clustering so-lution against reference clusters in terms of clus-tering homogeneity and completeness (Rosenberg and Hirschberg, 2007). Homogeneity describes the extent to which each cluster is composed of para-phrases belonging to the same reference cluster, and completeness refers to the extent to which points in a reference cluster are assigned to a single cluster. Both are defined in terms of conditional entropy. V-Measure is the harmonic mean of homogeneity h and completeness c ; V-Measure = 2  X  h  X  c 7.3 Baselines We evaluate the performance of HGFC on each dataset against the following baselines: Most Frequent Sense (MFS) assigns all para-phrases p i  X  P to a single cluster. By definition, the completeness of the MFS clustering is 1. One Cluster per Paraphrase (1 C 1 PAR ) assigns each paraphrase p i  X  P to its own cluster. By defi-nition, the homogeneity of 1 C 1 PAR clustering is 1. Random (RAND) For each query term X  X  para-phrase set, we generate five random clusterings of k = 5 clusters. We then take F-Score and V-Measure as the average of each metric calculated over the five random clusterings.
 SEMCLUST We implement the SEMCLUST algorithm (Apidianaki et al., 2014) as a state-of-the-art baseline. Since PPDB contains only pairs of words that share a foreign word alignment, in our implementation we connect paraphrase words with an edge if the pair appears in PPDB. We adopt the WORD 2 VEC distributional similarity score sim DIST RIB for our edge weights. Figure 6 shows the performance of the two advanced clustering algorithms against the baselines. Our Figure 6: Hierarchical Graph Factorization Cluster-ing and Spectral Clustering both significantly out-perform all baselines except 1 C 1 PAR V-Measure. performed all baselines except 1 C 1 PAR V-Measure, which his biased toward solutions with many small clusters (Manandhar et al., 2010), and performed only marginally better than SEMCLUST in terms of F-Score alone. The dominance of 1 C 1 PAR V-Measure is greater for the WordNet+ dataset which has smaller reference clusters than CrowdClusters. Qualitatively, we find that methods that strike a bal-ance between high F-Score and high V-Measure tend to produce the  X  X est X  clusters by human judge-ment. If we consider the average of F-Score and V-Measure as a comprehensive performance measure, our methods outperform all baselines. Table 1: Average performance and number of clus-ters produced by our different similarity methods.
On our dataset, the state-of-the-art SEMCLUST baseline tended to lump many senses of the query word together, and produced scores lower than in the original work. We attribute this to the fact that the original work extracted paraphrases from Eu-roParl, which is much smaller than PPDB, and thus created adjacency matrices W which were sparser than those produced by our method. Directly ap-plied, SEMCLUST works well on small data sets, but does not scale well to the larger, noisier PPDB data. More advanced graph-based clustering meth-ods produce better sense clusters for PPDB.

The first question we sought to address with this work was which similarity metric is the best for sense clustering. Table 1 reports the average F-Score and V-Measure across 40 test configurations age across test sets and clustering algorithms, the paraphrase similarity score ( P P DB 2 . 0 Score ) per-forms better than monolingual distributional similar-ity ( sim DIST RIB ) in terms of F-Score, but the re-sults are reversed for V-Measure. This is also shown in the best HGFC and Spectral configurations, where the two similarity scores are swapped between them.
Next, we investigated whether comparing second-order paraphrases would produce better clusters than simply using P P DB 2 . 0 Score directly. Table 1 also compares the two methods that we had for comput-ing the similarity of second order paraphrases  X  co-sine similarity ( sim P P DB.cos ) and Jensen-Shannon divergence ( sim P P DB.JS ). On average across test sets and clustering algorithms, using the direct para-phrase score gives stronger V-Measure and F-score than the second-order methods. It also produces coarser clusters than the second-order PPDB simi-larity methods.

Finally, we investigated whether incorporating automatically predicted entailment relations would improve cluster quality, and we found that it did. All other things being equal, adding entailment in-formation increases F-Score by .014 and V-Measure by .020 on average (Figure 7). Adding entailment information had the greatest improvement to HGFC methods with sim DIST RIB similarities, where it improved F-Score by an average of .03 and V-Measure by an average of .05. We have presented a novel method for clustering paraphrases in PPDB by sense. When evaluated against WordNet synsets, the sense clusters pro-duced by the Spectral Clustering algorithm give a 64% relative improvement in F-Score over the clos-est baseline, and those produced by the HGFC al-gorithm give a 50% improvement in F-Score. We systematically analyzed a variety of similarity met-rics as input to HGFC and Spectral Clustering, and showed that incorporating predicted entailment re-lations from PPDB boosts the performance of sense clustering.

Our sense clustering provides a significant im-provement to the PPDB resource that may improve its applicability to downstream NLP tasks. One pos-sible application of sense-clustered PPDB entries is the lexical substitution task, which seeks to iden-tify appropriate word substitutions. Given a target word in context, it would be reasonable to suggest substitutes from the target word X  X  PPDB sense clus-ter most closely related to the target context. There are many possible ways to choose the best clus-ter for a given context, ranging from simply choos-ing the cluster whose members have highest aver-age pointwise mutual information with the context, to a more complex approach based on training clus-ter representations using a pseudo-word approach as in Melamud et al. (2015). We leave this application for future work. With publication of this paper we are releasing paraphrase clusters for all PPDB 2.0 XXL entries, Figure 7: Histogram of metric change by adding en-tailment information across all experiments. clustering code, and an interface for crowdsourcing paraphrase clusters using Amazon Mechanical Turk. Our Supplementary Material provides additional de-tail on our similarity metric calculation, clustering algorithm implementation, and CrowdCluster refer-ence cluster data development. We also provide full evaluation results across the entire range of our ex-periments, a selection of sense clusters output by our methods, and example content of our WordNet+ and CrowdCluster paraphrase sets.
 This research was supported by the Allen Institute for Artificial Intelligence (AI2), the Human Lan-guage Technology Center of Excellence, and by gifts from the Alfred P. Sloan Foundation, Google, and Facebook. This material is based in part on research sponsored by the NSF grant under IIS-1249516 and DARPA under number FA8750-13-2-0017 (the DEFT program). The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes. The views and conclusions contained in this publication are those of the authors and should not be interpreted as representing official policies or endorsements of DARPA and the U.S. Government.
 We would like to thank Marianna Apidianaki and Alex Harelick for sharing code used in this research, and Ellie Pavlick for her substantive input. We are grateful to our anonymous reviewers for their thoughtful and constructive comments.

