 We propose a classification model of tweet streams in Twit-ter, which are representative of document streams whose sta-tistical properties will change over time. Our model solves several problems that hinder the classification of tweets; in particular, the problem that the probabilities of word oc-currence change at different rates for different words. Our model switches between two probability estimates based on full and recent data for each word when detecting changes in word probability. This switching enables our model to achieve both accurate learning of stationary words and quick response to bursty words. We then explain how to imple-ment our model by using a word suffix array, which is a full-text search index. Using the word suffix array allows our model to handle the temporal attributes of word n -grams ef-fectively. Experiments on three tweet data sets demonstrate that our model offers statistically significant higher topic-classification accuracy than conventional temporally-aware classification models.
 H.2.8 [ Database Management ]: Database Applications X  Data mining ; H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Experimentation Twitter, Document classification, Data streams, Concept drift, Control charts, Suffix arrays
Twitter, a micro-blogging service, has emerged as a new information infrastructure. Over 300 million status mes-sages, called tweets, are posted per day by users all over the world. The demand for classifying tweets in order to orga-nize the massive volume of tweets has become more urgent.
Classifying document streams is a challenging problem because their statistical properties will change over time. This characteristic is called concept drift , and it is one of the hottest topics in data mining [2, 13]. There are multi-ple types of changes, and the effectiveness of strategies for building classifiers depends on the types of changes.
For classifying tweet streams, the trade-off between quick response to bursty words and accurate learning of station-ary words is a significant problem. Conventional methods conduct temporal selection or weighting on documents; so, they cannot handle the different time-scale changes expected for each word. We propose a probabilistic classification model, P-Switch , as an extension of the multinomial naive Bayes classifier. In order to resolve the trade-off, our model switches between the two probability estimates based on full and recent data for each word, by monitoring changes in the probability of word occurrence. It implements monitoring by using a control chart [20], which is a statistical process control tool.

We then describe an implementation method that uses a word suffix array [7]. Our classification model can be formalized as searching the positions of word n -grams of a new tweet, in the training documents that are concate-nated chronologically. We, therefore, regard the construc-tion of full-text search indexes as a learning procedure. This approach can effectively handle word n -grams, which are highly important information given the 140 character limit of tweets.

Experiments on topic classification were conducted on three real-world tweet data sets that have different characteristics in terms of topics, languages, and changes. Results show that our model offers statistically significant improvements over the standard multinomial naive Bayes classifier and conventional document weighting and selection methods in terms of accuracy and macro F 1 measures.

Contributions: (i) analysis of the changes in tweet streams, (ii) proposal of a probabilistic classification model that adapts to and detects changes at word-level, (iii) an implementation approach that uses word suffix arrays for effectively learn-ing temporal factors in word n -grams, and (iv) significantly improved topic classification accuracy for tweet streams.
Outline:  X  2 describes the problem definitions on docu-ment stream classification.  X  3 presents the results of our analysis and preliminary experiments on tweet streams.  X  4 explains our proposed model.  X  5 describes the implementa-tion method.  X  6 presents the results of the experiments. describes related work and  X  8 summarizes the contributions of this study.
We address previous work on document stream classifica-tion and explain the scope of this study.
Problem definitions for document stream classification can be roughly classified into three types depending on how the training and test examples are presented: [P1] Test-Then-Train (Incremental) [6, 35, 11, 38] [P2] Test-Then-Train (Chunk) [17, 16, 15, 30, 18, 12] [P3] Train-Then-Test [8]
Temporally-aware classification methods for batch learn-ing have also been proposed [21, 5, 28, 29]: a classifier is given D  X  t and D t for all t =1 , 2 ,....,T . It outputs a class prediction,  X  c t,j , for each d t,j  X  D t , based on (  X 
Concept drift means that the statistical properties of data change over time [36, 34], and it creates the most difficult situation in document stream classification. There are three major types of concept drift: [C1] sudden shift, [C2] grad-ualdrift,and [C3] recurring themes.

One example of concept drift is found in the spam mail filtering problem because mail is characterized by skewed and changing class distributions, users X  dynamic and grow-ing interests, recurring and periodic themes (e.g., Christmas-related spam), and intelligent and adaptive adversaries.
Previous studies on handling concept drift in document streams employ instance selection, instance weighting, en-semble, and feature selection methods.

Instance selection methods select and adapt to previ-ous training examples [16, 15, 11, 38]. Sliding window is a typical approach that is used to select the most recent pre-vious examples. Klinkenberg et al. proposed a method for automatically deciding the size of window and demonstrated high classification performance in the topic classification of news articles [15].

Instance weighting methods assume that newer in-stances are more essential for classifying the current in-stance [15, 18, 5, 11, 28, 29]. Lebanon et al. proposed a naive Bayes classifier with a weighting scheme for handling temporal document streams [18].

These instance selection and weighting methods will adapt well to C2, but often fail to handle C1, because learning with old data worsens the classification of most recent data when sudden changes occur. There is a trade-off between response quickness, needed for C1, and learning accuracy, needed for C2. For example, using a short window improves response quickness but fails to offer good learning accuracy.
Ensemble methods use a set of classifiers adapted to the current concept [8, 35, 30, 12]. New models are added, and old or poor models are removed, in a fixed period of time or when a decrease in classification accuracy is observed. En-semble methods are effective for C2 and C3; e.g., Katkis et al. proposed a general framework for tracking recurring con-texts using ensemble classifiers and improved email filtering accuracy [12].

Feature selection methods dynamically select the most valuable feature (word) set in order to respond to concept drift, which is characterized by changes to the underlying feature space [6, 35, 11]. Feature selection methods are frequently combined with other methods; e.g., Wenerstrom et al. [35] proposed a feature-adaptive ensemble approach, which uses multiple classifiers with different features.
This study handles the incremental test-then-train prob-lem definition P1, the most basic problem definition, and tackles the trade-off between responding to sudden shift (C1) and gradual drift (C2). We propose a classification model, a new weighting method at the word level, for tweet streams. While conventional instance weighting methods use tempo-ral weighting functions on documents , our model is able to respond to changes in each word. Extending our model to suit ensemble and dynamic feature selection methods in or-der to handle recurring themes (C3) is our future work.
This section provides the results of analysis and prelimi-nary experiments on the topic classification of tweets, where topics are defined by hashtags.
We collected three tweet data sets as shown in Table 1: tweets on 12 Nippon (Japanese) professional baseball teams ( NPB ), tweets on 7 Japanese television networks ( TV ), and tweets on 30 major league baseball teams ( MLB ), by search-ing hashtags via the Twitter Streaming API (statuses/filter), from 13 September 2011 to 26 September 2011.

These data sets consist of tweets that include a single hashtag; that is, a tweet belongs to a single class. Classes are hashtags on baseball teams for the NPB and MLB data sets, and hashtags on television networks for the TV data set. Retweets (including the tweets containing the string  X  X T X  or  X  X T X ), replies, mentions are not included in these data sets. In the problem definition P1, document d means a tweet text without any hashtags, and class label c means a hashtag. For the NPB and TV data sets, we selected noun, verb, and adjective words by using MeCab 0.98 with IPA dictionary 1 for Japanese word segmentation and part of speech labeling. For the MLB data set, we removed stop words and treated punctuation and other non-alphabetic characters as separate tokens. http://mecab.sourceforge.net Figure 1: (a) Cosine similarities between class dis-tributions on 13 September 2011 and all other days. (b) Total number of tweets for each day. Figure 2: Class distribution (ten minute periods) for TV data set, Monday, 19 September 2011.
We analyzed tweet streams in terms of the changes in class distribution and word probability, the occurrence of new words, and the effectiveness of considering word n -grams.
Fig. 1 shows (a) the cosine similarities between class dis-tributions on 13 September 2011 and all other days and (b) the total number of tweets for each day. We can see that the class distribution on the NPB and MLB data sets fluctuate more than that on the TV data set. The TV data set also fluctuates dramatically as shown in Fig. 2, which shows the class distribution over ten minute periods.

In summary, the class distributions in tweet streams fluc-tuate constantly and dramatically at micro and macro time Figure 3: (a) Hourly number of tweets containing  X  X ome-run X  and (b) Daily number of tweets contain-ing  X  X okohama X  from NPB data set. scales. Classification models have to respond to these abrupt and gradual changes in class distribution.
Fig. 3 shows (a) the hourly number of tweets containing  X  X ome-run X  and (b) the daily number of tweets containing  X  X okohama X  from the NPB data set.  X  X ome-run X  is a bursty word since its relevant classes change rapidly. For example, aplayeroftheteam #chibalotte hit a home-run at 4PM and the word probability for  X  X ome-run X  increased suddenly (Fig. 3a). On the other hand,  X  X okohama X  is a relatively sta-tionary word. The most relevant class #baystars , referring to the baseball team that has its home ground in Yokohama, does not change (Fig. 3b).

In summary, the statistical properties of tweet topics change at different rates for different words. Conventional instance weighting and selection methods cannot handle these differ-ences.
In order to analyze the occurrence and change of effective words for classifying tweets, we calculated the chi-squared statistics  X  2 c (see details in [39]) of all words for each class c in each day, and got the daily top 100 words in terms of  X  for each class.

Fig. 4 shows (a) the cosine similarities between the daily top 100 words on 13 September 2011 and all other days and (b) the cumulative number of distinct daily top 100 words. The results of Fig. 4 are averaged over classes.

We can see from Fig. 4b that effective words for classifying Figure 4: (a) Cosine similarities between daily top 100 words on 13 September 2011 and all other days. (b) Cumulative number of distinct daily top 100 words. These results are averaged over classes. Figure 5: (a) Number of kinds of word n -grams that occur more than five times in each day. (b) Distribu-tion of top 200 word n -grams in terms of max c {  X  2 c } These results are averaged over days. Chi-squared statistics max c {  X  2 c } of a notable n -gram is greater than that of the first word of the notable n -gram. tweets emerge continuously. In particular, the words on the TV data set changes dramatically. Moreover, the daily top 100 words in the TV data set change periodically because of the influence of the day of the week, as shown in Fig. 4a.
In summary, classification models have to handle emerging words as well as recurring and periodic themes effectively.
The word context of the tweet X  X hich is highly impor-tant information given the 140 character limit X  X s lost in the bag-of-words model, which assumes that word order is unimportant.

Accordingly, we investigated whether word n -grams were useful for classifying tweets or not. We first calculated the chi-squared statistics max c {  X  2 c } of all word unigrams, bi-grams, and trigrams that occurred more than five times in each day. We then found some notable word n -grams, where the max c {  X  2 c } value of a notable word n -gram is greater than that of the first word of the word n -gram (Fig. 5a). For ex-ample, from the MLB data set,  X  X hite sox X  is more suitable Figure 6: Macro F 1 vs. kernel width of instance selection and weighting methods with multinomial naive Bayes classifier (Eqs. (1)  X  (5) ) for identifying the class #whitesox than  X  X hite X . Moreover, Fig. 5b shows that the distribution on top 200 word n -grams in terms of max c {  X  2 c } contains word bigrams and trigrams (37 . 6%, 52 . 0%, 28 . 1% for the NPB, TV, and MLB data sets).
In summary, considering word sequences can be effective for classifying tweets because tweets are short and limited to 140 characters.
We applied a temporally-aware multinomial naive Bayes classifier, which was proposed by Salles et al. [29], and inde-pendently, by Lebanon et al. [18], to problem definition P1.
The temporally-aware multinomial naive Bayes classifier is given by: where [[  X  ]] is 1 if the predicate is true and 0 otherwise, W ( d is the set of selected words in document d t ,and f c, X  ( w )is the frequency of word w in d  X  . K h ( t  X   X  ) is a kernel function such that: where h is the width of the kernel. Using Eq. (4) means adopting an instance selection method and using Eq. (5) means adopting an instance weighting method. The pro-posal of Salles et al. [29] learns a kernel function with a training data set; however, its learning cost for P1 is ex-tremely high. Accordingly, we used the above kernel func-tions in this preliminary experiment. We note that Lebanon et al. reported that Eq. (5) was the best kernel function in their experiments [18].
We evaluated the macro F 1 measure of the instance selec-tion and weighting methods while changing the value of h (Fig. 6). Feature selection was conducted by using the chi-squared statistics  X  2 c based on documents { d  X  | t  X  h&lt; X  &lt; t } :thatis, W ( d t )= { w | max c {  X  2 c ( w ) } &gt; 30 . 0
For the TV data set, decreasing kernel width h improved classification accuracy. This data set exhibited significant and frequent changes in class and word probabilities and new and important words emerged constantly (see Figs. 2 and 4); therefore, learning from newer tweets was important. The instance weighting method achieved better classification accuracy than the instance selection method in which all data within the kernel are equivalent. We note that macro F 1 is not a monotonic function of kernel width because of the existence of recurrent themes, as shown in Fig. 6b.
For the NPB and MLB data sets, decreasing the kernel width worsened classification accuracy. The instance selec-tion and weighting methods cannot learn stationary words fully if the kernel width is small. These data sets contained sudden and significant shifts in some words at micro time scales, as shown in Fig. 3a. Responding to such shifts is im-portant for achieving further improvements in classification accuracy.
From the results of data analysis and preliminary exper-iments, an ideal classification model for handling concept drift in tweet streams should be able to: [ A1 ]respondto changes in class distribution, [ A2 ] respond to changes in word probability, [ A3 ] handle the emergence of new words effectively, [ A4 ] resolve the trade-off between quick response to bursty words and accurate learning of stationary words, [ A5 ] consider word n -grams, and [ A6 ] treat recurring themes effectively. We propose a method that realizes the first five abilities, A1 to A5. A6 lies outside the scope of this study.
We propose a classification model of tweet streams in which statistical properties will change over time. Our model, called P-Switch , is based on the analysis results shown in
At each time step t =1 , 2 ,... , our proposed model out-puts a class prediction,  X  c t , for a new given document, d follows: from the ( i  X  n +1)-th to ( i  X  1)-th word in d t ,and W ( d the set of selected words in d t . Our model then updates itself based on the true class label c t .

Our model, Eq. (6), is a temporally-aware version of the multinomial naive Bayes classifier. The following sections describe three essential components: class distribution, word probability, and word n -gram probability.
To acquire ability A1, our model incrementally updates the class distribution, p ( c | t ), based on c t : where [[  X  ]] is 1 if the predicate is true and 0 otherwise.
Eq. (7) is the class distribution estimated using an expo-nentially weighted moving average (EWMA), where  X  is a smoothing parameter that represents the degree of weight-ing decrease (0 &lt; X &lt; 1). The weighting for each older data point decreases exponentially, it never reaches zero.
To acquire abilities A2, A3, and A4, our model estimates the temporally-aware word probability for each word.
First, our model chronologically concatenates documents given until t for each class c , D c,t .

A standard multinomial naive Bayes classifier estimates the word probability of w i in class c with the maximum likelihood (ML) method, where f c ( w ) is the frequency of w in D c,t .

Additionally, our model estimates the word probability with the EWMA method, where J c ( w i ) is the position of word w i in D c,t , J { j |D c,t [ j ]= w i , 1  X  j  X |D c,t |} ,and  X  is a smoothing parameter of EWMA (0 &lt; X &lt; 1). Eq. (9) is a closed-form expression of EWMA.

Our model monitors the two probability estimates for each word w i in class c , and switches between them as follows: Note that Eqs. (10) and (11) mean monitoring the statisti-cal process of w i occurring in D c,t with a Bernoulli EWMA control chart[31]. EWMA charts are generally used for de-tecting small shifts in the process mean [20]. The value of L in Eq. (10) determines the upper control limit of the chart, p
ML + L X  c,t .Aslongas p EWMA is within the control limit, the word occurrence process is assumed to be currently sta-ble (i.e., our model uses p ML ).

Fig. 7 clarifies that Eq. (9) preserves the probabilistic in-terpretation. We can see that the EWMA value, p EWMA , tracks the changes in true probability. The EWMA method is able to respond to sudden shifts quickly. On the other hand, the ML method is able to estimate the probability for stationary words more accurately. The ML method, however, is apt to underestimate the probabilities of bursty words. This switching of the two probability estimates for each word enables our model to resolve the trade-off between quick response to bursty words and the accurate learning of stationary words.
To acquire ability A5, our model extends the bag-of-words model to the n -gram language model [26]. It uses the ab-solute discounting method [22, 4] as the smoothing method Figure 7: Simulation of estimating probabilities of word occurrence with ML (Eq. (8) )andEWMA (Eq. (9) ,  X  =0 . 002 ) methods. used in estimating the word n -gram probability: word sequence w i  X  1 i  X  n +1 . The lowest order model in this lin-temporal effects.

The smoothing of Eq. (8) is also calculated according to the absolute discounting method: and Eq. (13) is used instead of Eq. (8).
We can implement our model by constructing full text search indexes (e.g., suffix arrays [19]) for each D c,t .This study uses the word suffix array (WSA) [7]. WSA enables w
Let T [1 ,...,l ] be a text of length l over a constant-sized alphabet  X , tokenized into k words by word-delimiters, and let I be the set of alphabet positions at which new words start. The word suffix array A [1 ,...,k ]isapermutationof i.e., the A array represents the lexicographic order of all suf-fixes, as shown in Fig. 8 (see details in [7]). The set of word positions, B , is obtained as a by-product of constructing WSA. Our model uses B to find J c ( w i i  X  n +1 )in D c,t
WSA can be constructed in O ( l )timeand O ( k )space when using the linear-time construction algorithms for suffix arrays ( e.g. , [23]). It can search w i i  X  n +1 (alphabet length of m ) quickly with a binary search in O ( m log k + f c ( w as in suffix arrays.
 iAB Word-aligned Suffix 1 4 2 a#aa#a#ab#baa#aab#a#aa#baa# 2 22 8 a#aa#baa# 3 9 4 a#ab#baa#aab#a#aa#baa# 4 6 3 aa#a#ab#baa#aab#a#aa#baa# 5 24 9 aa#baa# 6 18 7 aab#a#aa#baa# 7 1 1 ab#a#aa#a#ab#baa#aab#a#aa#baa# 8 11 5 ab#baa#aab#a#aa#baa# 92710baa# 10 14 6 baa#aab#a#aa#baa# Figure 8: Word suffix array.  X # X  is a word boundary. Columns A and B are the set of alphabet and word positions at which word-aligned suffixes start.
 Algorithm 1 Learning word n -gram probability with word suffix arrays.
 Require: { d t ,c t } ,documentstream 1: for each ( d t ,c t ) do 4: end for
This section shows the learning and classification proce-dures for word n -gram probability (Eqs. (8) X (13)).
Learning constructs WSAs for each class. Algorithm 1 shows the learning algorithm for a document stream, { d t The sequential update of WSA is not supported, so, it recon-structs the WSA of class c t when given a new tweet ( d t (lines 2 X 3)
For reducing the learning time, it is effective to divide tweets into periods and reconstruct the newest WSA on the most recent tweets, while retaining old WSAs. We note that it is able to obtain J c ( w i i  X  n +1 ) from the divided WSAs by storing the offset position of each WSA and adding it to the positions of w i i  X  n +1 found by each WSA.

Classification needs the f c ( w i j ), J c ( w i j ), and r ues (0  X  i  X  j&lt;n ) to calculate the word n -gram probability for each word n -gram, w i i  X  n +1 ,inclass c . Algorithm 2 shows the pseudo-code that acquires the above values from a WSA.
Our model uses three heuristics for speed enhancement: (1) caching search results (lines 6 X 11, 13), (2) caching the initial search interval for the first alphabets [19] (line 3), and (3) reducing the number of character comparisons by re-membering the number of matching characters [19] (line 9). Caching the search results of ( n  X  1)-grams enables our model to narrow the initial interval when searching n -grams. The worst case of our approach is O ( m log k + f c ( w i i  X  however, Ferragina et al. reported in [7] that the O ( m log k + f ( w i i  X  n +1 )) algorithm, which includes heuristics 2 and 3, is faster than the O ( m +log k + f c ( w i i  X  n +1 )) [19] and O ( m f ( w i i  X  n +1 )) [1] algorithms.
We used the original implementation of the WSA con-struction algorithm 2 by Fischer [7] and the sais library http://ab.inf.uni-tuebingen.de/people/fischer/wordSA.tgz http://sites.google.com/site/yuta256/sais Algorithm 2 Acquiring classification parameters for class c . Require: d = X  w 1 # w 2 #  X  X  X  # w z # X  = w z 1 ,newdocument Require: R ( a ) , search range of WSA for each alphabet a Require: W ( d ) , set of selected words in d 1: for i =1to z do 2: next if w i  X  W ( d ) 3: R  X  R ( w i [1]) 4: for j = i to i + n  X  1 do 5: s  X  w i j // word ( i  X  j + 1)-gram. 6: if is_cached? ( s )= true then 7: f c ( s ) ,J c ( s ) ,r c ( s ) ,R ( s )  X  get_cache ( s ) 8: else 9: f c ( s ) ,J c ( s ) ,r c ( s ) ,R ( s )  X  search_by_WSA ( s, R ) 10: set_cache ( s, f c ( s ) ,J c ( s ) ,r c ( s ) ,R ( s ) ) 11: end if 12: break if f c ( s ) &lt; 0 13: R  X  R ( s ) // narrow next search range. 14: end for 15: end for 16: return { f c ( w i j ) ,J c ( w i j ) ,r c ( w i j ) | Figure 9: Construction time of word suffix array on tweets of class #dragons on NPB data set. To-tal 53 , 000 tweets, 399 , 825 words, 2 , 701 , 985 alphabets. CPU: Xeon X5680 3.33GHz, Memory: 192GB. which implements Nong et al. X  X  induced sorting algorithm [23], as the suffix array construction algorithm used by WSA. We also used Kasai et al. X  X  algorithm [10] for calculating the longest common prefixes. The character encoding of tweets was UTF-8, and the number of alphabets was 256; i.e., a three-byte Japanese character consisted of three alphabets. Fig. 9 shows the results of constructing WSA on the tweets in class #dragons for the NPB data set: the construction time of WSA was linear to the number of alphabets.
In order to evaluate the impact that our proposed model has on the incremental test-then-train problem definition P1 (see  X  2), we first evaluated our model components and then compared our model against three conventional ver-sions of multinomial naive Bayes classifiers (standard, in-stance weighting, and instance selection), by using the three tweet data sets shown in Table 1.

The classifiers used the chi-squared statistics feature se-lection method [39]: that is, W ( d t )= { w | max c {  X  30 . 0 } , and conducted the smoothing of Eq. (13):  X  =0 . 9.
Our model, P-Switch , is also a temporally-aware version of multinomial naive Bayes. The following sections demon-strate the effects of our model X  X  components.
In order to evaluate the effects by introducing the temporally-aware class distribution described in Eq. (7), we evaluated the Proposal1 model: which extends the standard multinomial naive Bayes classi-fier, MNB : while changing smoothing parameter  X  from 0 . 001 to 0 . 5.
We can see from Fig. 10 that the Proposal1 model, where  X  =0 . 01, performed statistically significantly better than MNB (McNemar X  X  test; p&lt;. 001). We note that the multi-nomial naive Bayes classifier that assumes a uniform class distribution performed poorly.
In order to evaluate the effects of switching between the two probability estimates described in Eq. (10), we evaluated the Proposal2 model: which extends the Proposal1 model, while changing smooth-ing parameter  X  from 0 . 0001 to 0 . 01. Other parameter set-tings were:  X  =0 . 01 and L =0 . 5 for all data sets.
Fig. 11 shows that the Proposal2 model, when  X  =0 . 002, performed statistically significantly better than the Proposal1 model for all data sets (McNemar X  X  test; p&lt;. 001). The switching of the two probability estimates: p EWMA ,which is based on recent data, and p ML , which is based on data that cover long-term period, enables the Proposal2 model to re-solve the trade-off between quick response to bursty words and the accurate learning of stationary words. Moreover, treating class distribution and word probability at different time scales (  X  and  X  ), unlike instance selection and weight-ing methods, is effective in classifying tweet streams.
We also note that the best value for  X  is the same for all three data sets, which have different change characteristics as shown in Figs. 1 and 4.
In order to evaluate the effects of introducing the temporally-aware word n -gram probability described in Eq. (12), we evaluated our P-switch model defined in Eq. (6), while chang-ing the value of n from 1 (corresponds to the Proposal2 model) to 5. Other parameters settings were:  X  =0 . 01,  X  =0 . 002, L =0 . 5 for all data sets.

We can see from Fig. 12 that our model performed best when n = 2, and there were statistical differences (McNe-mar X  X  test; p&lt;. 001) between n = 1 and n = 2 on NPB and other results (McNemar X  X  test; p&lt;. 001 ).
 Figure 10: Effects of introducing temporally-aware class distribution, p ( c | t ) .  X  X niform X  means the multi-nomial naive Bayes classifier that assumes a uniform class distribution.
 Figure 11: Effects of introducing temporally-aware word probability, p ( w i | c, t ) .  X  =0 . 01 and L =0 . 5 . MLB data sets. Larger values of n worsened classification accuracy because the zero frequency problem is serious in higher-order n -gram models [37].  X  6.1 demonstrated that all of our individual contributions are effective in handling concept drift in tweet streams.
We next compared the whole P-Switch model against con-ventional methods for handling concept drift: a temporally-aware multinomial naive Bayes classifier with the instance weighting method ( MNB-W ; Eqs. (1) and (5)), and with Figure 12: Effects of introducing temporally-aware  X  =0 . 002 ,and L =0 . 5 . the instance selection method ( MNB-S ; Eqs. (1) and (4)) (see details in  X  3.3 and [29, 18]). Parameter settings were:  X  =0 . 01,  X  =0 . 002, L =0 . 5, and n = 2. The kernel width of MNB-W and MNB-S was h =10 , 000, which was the best value for the TV data set. Note that the best value of h for the NPB and MLB data sets was  X  (corresponds to MNB) as shown in Fig. 6.

Fig. 13 plots macro F 1 of the classifiers vs. the training number of tweets (time steps). Table 2 is the overall accu-racy and macro F 1 of the classifiers after learning all data.
Although the NPB and MLB data sets contain various changes, MNB performed better than MNB-W and MNB-S; that is, learning stationary words with sufficient data is more important than responding to bursty words for these data sets. Our model resolved the trade-off between quick response and accurate learning by switching between the two probability estimates for each word. It offered a sta-tistically significant improvement over MNB, up to 11 . 04% and 12 . 56% in terms of overall accuracy, and up to 9 . 072% and 7 . 480% in terms of macro F 1 , for the NPB and MLB data sets, respectively (McNemar X  X  test; p&lt;. 001). Considering temporal effects enabled MNB-W and MNB-S to perform better than MNB on the TV data set; how-ever, increasing the number of training tweets is not effective for achieving their higher classification accuracy, as shown in Fig. 13b. This is because using the small kernel width yields rapid dropping of old data. MNB, which learns all data, performed worst because the TV data set contained sudden and significant shifts as shown in Fig. 2. Our P-Switch model was able to respond to both the sudden shift and gradual drift; therefore, it achieved statistically signifi-Figure 13: Number of tweets vs. cumulative macro F 1 on (a) NPB, (b) TV, and (c) MLB data sets.  X  =0 . 01 ,  X  =0 . 002 , L =0 . 5 , n =2 ,and h = 10000 . cant improvements over MNB, up to 20 . 19% and 19 . 60% in terms of overall accuracy and macro F 1 , respectively (Mc-Nemar X  X  test; p&lt;. 001).
As related work on classification of tweets, Sriram et al. devised a naive Bayes classifier for identifying tweet types such as news, events, opinions, deals, and private messages based on author information and tweet texts [32]. Sakaki et al. devised a support vector machine for finding tweets related to an event ( e.g. , earthquake) [27]. Kinsella et al. uses metadata from hyperlinked objects to improve topic classification [14]. The above studies, however, employ batch learning and do not consider concept drift. While sentiment classification methods have been proposed most recently [9, 3, 25], they also do not consider temporal effects.
Some studies that use suffix arrays (SAs) for document classification have been proposed. Teo and Vishwanathan proposed fast and space efficient string kernels based on SAs and used the kernel with the support vector machine [33]. Okanohara and Tsujii proposed a logistic regression model with all-effective substrings, which are found with SAs [24]. These studies target the finding of effective feature spaces for batch learning; our model, on the other hand, uses lazy learning for streaming documents X  X eneralization beyond the training data is delayed until a new document is given.
Salles et al. proposed temporally-aware versions of Roc-chio and k -nearest neighbors classifiers that conduct tempo-ral weighting on documents [29], in addition to the multi-nomial naive Bayes classifier described in Eq. (1). Their ex-periments showed that there were no significant differences among the classifiers. We find that responding to changes at the word level is more important than model selection for classifying document streams.
We proposed a classification model of tweet streams, which are representative of document streams whose statistical prop-erties change over time.

We analyzed tweet streams by examining the changes in class distribution and word probability, the occurrence of new words and recurring themes, and the effectiveness of considering word n -grams. In particular, we demonstrated that tweet streams change significantly at the word level X  the word occurrence probability changes at different rates for different words.

Our model solves this problem by detecting changes in word occurrence probability for each word. It switches be-tween two probability estimates, the maximum likelihood and exponentially weighted moving average estimates, by using a control chart and is very effective in achieving both accurate learning of stationary words and quick response to bursty words. Also, it handles the changes in class distri-butions and extends the bag-of-words model to cover the n -gram language model. It preserves the probabilistic inter-pretation of the multinomial naive Bayes classifier.
We then presented an implementation method that uses a word suffix array. This approach enables our model to effec-tively handle word n -grams, which are highly important in-formation given the 140 character limit of tweets. This new idea, searching the positions of words in documents concate-nated chronologically, allows consideration of the temporal effects on changing word streams.

Experiments on topic classification were conducted using three real-world tweet data sets: tweets about Japanese baseball teams, television networks in Japan, and major league baseball teams. Results demonstrated that our model performed statistically significantly better than conventional instance weighting and selection models. In particular, our model was superior to the multinomial naive Bayes classi-fier with an instance weighting method by up to 16 . 33% and 16 . 35% in terms of accuracy and macro F 1 , for the television network data set. [1] M. I. Abouelhoda, S. Kurtz, and E. Ohlebusch. [2] A. Bifet, J. Gama, M. Pechenizkiy, and I. Zliobaite. [3] S. Brody and N. Diakopoulos.
 [4] S. F. Chen and J. Goodman. An empirical study of [5] L.C.daRocha,F.Mour  X  ao, A. M. Pereira, M. A. [6] S. J. Delany, P. Cunningham, and B. Smyth. ECUE: [7] P. Ferragina and J. Fischer. Suffix arrays on words. In [8] G. Forman. Tackling concept drift by temporal [9] L. Jiang, M. Yu, M. Zhou, X. Liu, and T. Zhao. [10] T. Kasai, G. Lee, H. Arimura, S. Arikawa, and [11] I. Katakis, G. Tsoumakas, E. Banos, N. Bassiliades, [12] I. Katakis, G. Tsoumakas, and I. Vlahavas. Tracking [13] L. Khan, M. Pechenizkiy, and I. Zliobaite, editors. 2nd [14] S. Kinsella, A. Passant, and J. G. Breslin. Topic [15] R. Klinkenberg. Learning drifting concepts: example [16] R. Klinkenberg and T. Joachims. Detecting concept [17] R. Klinkenberg and I. Renz. Adaptive information [18] G. Lebanon and Y. Zhao. Local likelihood modeling of [19] U. Manber and E. W. Myers. Suffix arrays: A new [20] D. C. Montgomery. Introduction to statistical quality [21] F. Mour  X  ao, L. C. da Rocha, R. B. Ara  X ujo, T. Couto, [22] H. Ney, U. Essen, and R. Kneser. On structuring [23] G. Nong, S. Zhang, and W. H. Chan. Linear suffix [24] D. Okanohara and J. ichi Tsujii. Text categorization [25] A. Pak and P. Paroubek. Twitter as a corpus for [26] F. Peng, D. Schuurmans, and S. Wang. Augmenting [27] T. Sakaki, M. Okazaki, and Y. Matsuo. Earthquake [28] T. Salles, L. C. da Rocha, F. Mour  X ao, G. L. Pappa, [29] T. Salles, L. C. da Rocha, G. L. Pappa, F. Mour  X  ao, [30] M. Scholz and R. Klinkenberg. Boosting classifiers for [31] S. E. Somerville, D. C. Montgomery, and G. C. [32] B. Sriram, D. Fuhry, E. Demir, H. Ferhatosmanoglu, [33] C. H. Teo and S. V. N. Vishwanathan. Fast and space [34] A. Tsymbal. The problem of concept drift: definitions [35] B. Wenerstrom and C. Giraud-Carrier. Temporal data [36] G. Widmer and M. Kubat. Learning in the presence of [37] I. H. Witten and T. C. Bell. The zero-frequency [38] E. S. Xioufis, M. Spiliopoulou, G. Tsoumakas, and [39] Y. Yang and J. O. Pedersen. A comparative study on
