 1. Introduction
The success of web search engines is related to their ability to satisfy web users. To accomplish this, it is important to understand the underlying intents of these users in order to provide the most effective retrieval strategies. Previous work have shown that specialized ranking mechanisms can improve the overall quality of web search engines by taking into con-
Jagadish, 2006 ). Further, by knowing the user intent, one can place advertisements in the most appropriated time. In this work, we propose new query representation approaches that enable us to find the category (goal) into which a certain query falls.

According to Broder (2002), Rose and Levinson (2004) , each query submitted to a Web search engine can be classified according to the user goals into at least three distinct classes: informational, navigational, and transactional. In navigational queries, the user is interested in reaching a specific web site and uses the search engine to find its web address. For instance, when a user types a query  X  X  X P&amp;M X , his/her main goal is probably to find the homepage of this journal on the web. In infor-mational queries, the user does not have a particular page in mind and intends to learn more about a specific topic. For instance, in the query  X  X  X mazon forest X , the user would probably be interested in reading documents about the Amazon
Forest in order to learn more about that topic. In transactional queries, also known as resource queries, the user is interested in finding sites that provide a resource. They typically need to perform a transaction or interact with sites that provide a ser-vice. Examples of such services are the download of software, music, movies, pictures, the access to entertainment, the send-sites that allow him to send postal cards.

Automatic query classification is usually performed by representing the queries using features extracted from the search engine database and query logs. More specifically, the main sources of information adopted in literature for query classifi-the queries, (iv) the textual content of the documents related to the queries, and (v) the click through information available on the log of previous queries. We focus on studying the sources (i) through (iv), presenting alternative ways of applying them to determine the users X  goals behind their queries. Although the click through information comprises one of the most successful resources adopted in query classification, we did not include it in this work because we found no publicly avail-able collections providing such information. Thus, our results are useful for better understanding how the first four sources can be explored. Also note that features extracted from click through information can be combined with the ones discussed here to obtain even better classification results.

Our contributions to the topic of query classification include the use of new features to be adopted during the classifica-tion process and a detailed study about the impact of each feature on different tasks and collections. One of the new features we explore here is based on the idea that statistics about the occurrence of the query terms across different domains useful for determining the user goal. We use this assumption to include two new features not mentioned in previous work about query classification. Another feature included in this study is the popularity of the query. As we will show, popularity affects the classification task and is an important new feature we added to the classification process. Further, previous work have pointed that most of the distinct queries submitted to search engines are not popular and that the query distribution is
Zipfian ( Saraiva et al., 2001 ). Thus, we believe the design of classification methods for identifying user goals behind a query should take the popularity of queries into consideration.

The remaining of this work is organized as follows. Section 5 presents the related work. Section 2 presents the proposed solution for the addressed problem and the classification method adopted in this work. In Section 3, we present the features used to represent the queries to be classified. Section 4 presents the experiments we performed, including evaluation meth-odology and results. Finally, Section 6 presents our conclusion and future research directions. 2. Query classification
We address the problem of determining the user goal behind a query submitted to a search engine as a query classifica-tion problem. In this problem, we are given a set of n queries Q  X f q of m features, that is, q i  X f f 1 ; f 2 ; ... ; f m g ; and a taxonomy, that is, a fixed set of t categories C  X f c more training queries, we wish to learn a classification function c : Q ! C that maps queries to their categories. As for our user goals described in Broder (2002) . The remaining taxonomies, C each user goal in C all .

The solution we propose to this problem consists in suggesting the set of features f f queries and then apply a learn-based method to perform the classification task. The learning algorithm selected is the Sup-port Vector Machine (SVM) (Joachims, 1998 ), a state-of-the-art method previously applied to query classification with excel-function (RBF) in our experiments.

In next section, we discuss in details the features used to represent the queries. 3. Query representation
Determining which features should be used to represent a query is a key decision in query classification. In this work, the term feature describes a statistic that represents a measurement of some aspect of a given user X  X  query or a term belonging to the query. To determine the discriminative value of the features, we experimented with several combinations checking the usefulness of each option.

The features we adopted in this study are described in the next sections, grouped according to the source from where they were extracted. In the following sections we describe how we compute the values of each feature for each query. We have considered other alternative ways to calculate feature values such as, for instance, the use of different strategies for matching titles and URLs, and for characterizing the skewness of distributions. However, the resulting features yielded no better results than the set presented here and were removed due to space restrictions. 3.1. Anchor text based features
These features were extracted from the concentration of anchor texts that point to documents present in the query an-swer set. The general intuition behind anchor text features is that a small number of authoritative pages generally exist for a navigational query whereas a large number of pages exist for informational queries. Thus, the distribution of occurrences of the query terms in anchor texts of pages is probably more skewed for navigational queries. For instance, the term  X  X  X ahoo X , which tends to be a navigational query, is probably a very common term in anchor text of links pointing to Yahoo X  X  front page and has far fewer occurrences in other pages. On another hand, the term  X  X  X ancer X , which is an informational query, is prob-ably found in anchor texts of links pointing to many authoritative pages about cancer with a more uniform distribution. 3.2. Page content based features These features were extracted from the content of the pages in the result set retrieved by the user query.
 3.3. URL based features
These features were extracted from URL address of the pages in the result set retrieved by the user query. The general intuition behind these features is that terms in navigational queries are more likely to appear in URL addresses. 3.4. Query based feature
These features were extracted from the user query. The general intuition behind these features is that the size and content of the query are good indicators of its nature.
 3.5. Log based feature This feature was extracted from the log of past queries.
 3.6. Performance issues
Since the query classification has to be done at query time, the performance of the system is a very important issue. The design of such a system will depend fundamentally on the rate that new queries are submitted once the categories of old queries can be cached, reducing the classification costs to less than half (Saraiva et al., 2001 ). Further, the most resource demanding phase of the learning process, the training, is processed offline, not affecting the user experience. Thus, the crit-ical process consists in determining the value of the features which will be used to represent the submitted query. From the proposed ones, # terms , terms , and qpop have no significant impact on processing time. The calculation of the word distribu-tion for each domain name in mqd can also be done offline.
 requires the query be always processed as a phrase.

All the sources of data from where the proposed features are extracted have already been mentioned in literature as pos-sible, and useful, sources of information for computing ranking in search engines (Joachims, 2002; Liu, Xiong, &amp; Li, 2007;
Silva et al., 2009 ). Thus, the values of these features can be simultaneously extracted for both ranking and classification pur-poses, reducing the overhead of the classification system in a real search engine. Thus, the final impact on the performance of the systems would not be high in practice. 4. Experiments 4.1. Datasets In our work, we adopted two collections for experiments. The first is the WT10g collection, which was adopted in the Web
TREC 2001 (Bailey, Craswell, &amp; Hawking, 2003 ). The second is the WBR03 collection, a database extracted from the Brazilian web which contains queries submitted to TodoBR 2 , a real case search engine.
The WT10g collection contains no query log information and was adopted here mainly to facilitate the comparison of our results with other query classification approaches, in particular, the one presented in Kang and Kim (2003, 2004) . It contains about 7.5 Gb of plain text in 1,692,098 documents and about 2.5 million links connecting its pages.

The set of queries used in our experiments with WT10g is based on the one used by Kang and Kim (2003, 2004). Thus, as informational queries, we have used topics 451 X 500 from the TREC-2000 topic relevance task and topics 501 X 550 from the
TREC-2001 topic relevance task, for a total of 100 queries. As transactional queries, we have used 100 service queries ex-tracted from a Lycos log file, also employed by the authors in Kang (2005) . As navigational queries, Kang and Kim have used 145 queries of the TREC-2001 homepage finding task along with other 100 randomly extracted from TREC-2001, for a total of 245 queries. Note that the class distribution used by Kang and King was somewhat arbitrary since previous studies about query type distribution indicate that it is not expected that a user submit much more navigational queries than other types (Baeza-Yates, Calderon-Benavides, &amp; Gonzalez-Caro, 2006 ). To avoid the learning process being biased by such a skewed class distribution, we used only 100 queries randomly selected from the 245 ones originally used by Kang and Kim.
The WBR03 collection was adopted to enable us to investigate the effect of popularity on query classification tasks. We will make the evaluated queries and the web pages from the collection available to the research community to make it pos-sible the reproduction of our experiments. It contains 12,020,513 Web pages, about 140 million links connecting its pages, and roughly 60.3 GB of plain text. We extracted 600 queries from a ToboBR log composed of 11,246,351 queries, using a two step process as follows.

First, we randomly selected queries from the log and classified them until we have at least 200 queries on each category. A total of 2564 queries were classified in this first step. All the queries from WBR03 were classified by humans, such that each query was classified by three human evaluators. The users were asked to indicate the most probable user goal for each query, given their knowledge about the query and the category definitions adopted in Section 1. The possibility of a query having multiple goals is quite common, which makes it hard to determine the original user goal. For instance, given the query MP3, the user might be interested (a) in learning about the MP3 topic, which would make this query informational, (b) in finding web sites that contain MP3 files for download, which would make it transactional, or (c) in reaching the web site of MP3 Inc ( X  X  X ttp://www.mp3.com  X ) which would make it navigational. In such cases, we are interested in providing a classification according to the most expected category for the given query if this is possible. Thus, we asked the users who evaluated the queries to assign to each query the most appropriate category, according to their opinion.

In a second step, we got only 200 queries from each category, using again a random process. In this step, each query was considered as being part of the category whenever at least one person assigned it to this category. Thus, the final dataset is expected to keep a distribution of multiple goal queries close to the one found in the log. The final pool has more non-pop-ular queries than popular ones, since the popularity of queries in the log follows a Zipf distribution (Saraiva et al., 2001).
Table 1 presents some statistics related to collections WBR03 and WT10g along with some examples. The column  X  X  X ulti X  shows the percentage of queries that received two distinct categories in WBR03. From this table, we note that the shortest queries are the navigational ones. The largest are the informational in WBR03 and the transactional in WT10g. The larger sizes of informational queries in WBR03 can be attributed, in part, to the more extensive use of prepositions in Portuguese. 4.2. Evaluation methodology
To perform the experiments, we used the 10-fold cross-validation method (Mitchell, 1997 ) and for all comparisons re-ported in this work, we used the Wilcoxon signed-rank test ( Wilcoxon, 1945 ) for determining if the difference in perfor-mance was statistically meaningful. This is a nonparametric paired test which does not assume any particular distribution of the tested values. In all cases, we only have drawn conclusions from results that were significant in, at least, 5% level.

The performance of the presented methods was evaluated using the conventional accuracy measure, which is defined as the proportion of correctly classified examples. For comparison with other methods, we also used precision, recall, and F measures. Precision p is defined as the proportion of correctly classified examples in the set of all examples assigned to class. F1 is a combination of precision and recall defined as 4.3. Results 4.3.1. Feature analysis
We now evaluate the impact on accuracy of the features. To accomplish this, we conducted two experiments. First, we applied each feature to the datasets in order to determine its individual impact. Since some features may be more useful when not taken in isolation, we then studied the impact of each of them by removing it from the set to be used. Thus, to evaluate the impact of feature qpop , for instance, we represent the query using only qpop in the first experiment and using all the features but qpop in the second experiment. In all these experiments, for each query, we refer to its words as a single feature called terms .

For all datasets, we present the classification accuracy obtained for taxonomies C different tasks: distinguish information from not informational queries, navigational from not navigational queries, transac-bers between parenthesis indicate the relative importance of each feature, being the lowest the best. The features are sorted according to their accuracy obtained for taxonomy C all . We also show the accuracy considering all features, for reference. We were not able to include feature qpop in results reported for WT10g because the queries for this collection were not obtained from a query log. Tables 2 and 3 present the results obtained by applying each feature in isolation in WBR03 and WT10g collections, respectively.
 Tables 4 and 5 present the results obtained by removing each of the proposed features of the query representation in
WBR03 and WT10g collections, respectively. Note that, in these cases, the smaller the accuracy obtained after removing a feature, the greater its capability to provide correct decisions independently of other features. In these two tables, results equal or worse than those obtained with all features are shown in bold.

From Tables 2 X 5 , we can see that feature terms is very effective in query classification, specially to distinguish transac-tional queries. This is probably due to the characteristic vocabulary of these queries; on the other hand, the number of words in the query (# terms ) introduces noise, hurting classification accuracy. We also note that the impact of terms is much more significant in WBR03 than in WT10g. This is due to the poor performance of this feature in distinguishing navigational que-ries in WT10g, as we can see in Table 5 , where the removing of terms leads to a better accuracy.

Anchor based features are good for identifying navigational queries in WT10g, conclusion that was also reported in Kang and Kim (2003) . In spite of the noise introduced on recognizing transactional queries, their contribution is large enough to slightly improve performance regarding taxonomy C all in WBR03. Gains related to taxonomy C icant. As previously observed, in general, dda is more useful than af as we can see in both collections.

Amongst the page based features, title was useful for recognizing navigational and transactional queries in WT10g. By its turn, ddt was very useful in WT10g, in the task of distinguishing navigational queries. Note, however, that it has introduced some noise when used for identifying transactional queries in the same collection. For WBR03, in general, both features either had little impact or introduced noise.
 URL based features presented mixed results, being more useful in the task of identifying navigational queries in WBR03.
In particular, url has slightly improved results in both collections in all tasks. Unlike url , mqd was much better in WBR03 than in WT10g, being clearly more effective than url in WBR03. The inverse was observed for WT10g, for which mqd introduced noise.

The query popularity ( qpop ) was effective in all the classification tasks studied here. It was even able of improving accu-racy in the task of identifying navigational queries. Note it is effective just when combined with other features, increasing their discriminatory nature. Used in isolation, it presents no impact on accuracy. To better understand how qpop impacts other features, Table 6 presents the results obtained by combining each one of these features with qpop in WBR03 using tax-onomy C all . From this table, we can observe that anchor and page features are the ones that take more advantage from qpop .
These features explore differences in the distribution of occurrences of query terms in anchor text and page content observed in navigational queries when compared to informational queries. This difference in term distribution is enhanced when the query popularity is taken into consideration as illustrated in Fig. 1 . This figure depicts the effect of combining qpop with ddt and dda . Note that, in spite of ddt being the most improved feature after the combination with qpop , dda was the most im-proved among the top distinguishing features. As we can see, dda values are smaller for transactional features and larger for the informational ones, with the values for navigational queries generally falling at the middle. When popularity is consid-ered, results are improved mainly because navigational queries are usually more popular than the other ones becoming bet-ter distinguishable. This is also observed in the combination of ddt with qpop . In this case, however, it is much harder to distinguish the queries by only considering the distribution of ddt values, which explains the higher gain obtained after the combination.

From these results we note that, in general, af and # terms are not useful for identifying informational queries. In partic-ular, af also presents a bad performance when used with navigational queries. Further, some results are very different among the datasets which can be explained by the source of the queries and the quality of the available text and link information.
Unlike WT10g, the queries used with WBR were extracted from a unique search engine query log and represent realistic user behavior which impacts particularly on transactional queries. Further, link information is more common in WBR than in
WT10g, leading to different performance for features such as terms , title , ddt , and mqd . 4.4. Comparison with previous research results
In this section we compare our method using different query representations to previous work published in literature, in particular, the methods proposed by Kang and Kim (2003, 2004) and Kang (2005) . From now on, we will refer to these meth-ods as KANG1 and KANG2, respectively. Note that the original implementations of these algorithms are not publicly avail-able. Further, we were not able to implement them due to the lack of a sufficiently detailed description. Thus, our comparison is based on the results of these algorithms for the same collection, tasks and query sets.

We adopted two query sets for these comparisons. The first one was used in the experiments reported in Kang and Kim (2003, 2004) to the task of classifying navigational and informational queries. It comprises 150 training queries (100 navi-gational and 50 informational) and 195 test queries (145 navigational and 50 informational). The second query set was used in Kang (2005) for the task of classifying navigational, informational, and transactional queries. It consists of 200 training queries (100 navigational, 50 informational, and 50 transactional) and 245 test queries (145 navigational, 50 informational, and 50 transactional).

Table 7 shows precision, recall and F1 figures for KANG1, KANG2, and our SVM classifier using different query represen-tations. The comparison was carried out for the same tasks presented in Kang and Kim (2003, 2004); Kang (2005), that is, classifying queries as (a) navigational or informational and (b) navigational, informational or transactional. For both tasks, we report results for queries represented by all the features we have studied and some combinations without terms . By doing so, we are able to analyze the impact of terms on this query set, biased towards navigational queries. Note the first task is somewhat similar to the task we have previously studied of distinguishing navigational from not navigational queries. Thus, for the first task we also report results considering query representations without af and mqd , the features that presented the poorest performance for that task in WT10g. Similarly, for the second task, we report results considering query representa-tions without # terms , af , and mqd , the features that presented the poorest performance when distinguishing all the three query types in WT10g.

For both tasks, the best results were obtained by removing feature terms along with these that presented the worst per-outperformed KANG1 mainly due to its best recall since it was able to classify all the queries. For the second task, our query representation based on all the features, except terms, dda, af, mqd was the best, slightly outperforming KANG2. For both tasks, the use of the query vocabulary (feature terms ) did not improve final results. This can be attributed to the large number of navigational queries in the test sets we used. As we have seen in Table 5 , feature terms presents a particularly poor per-formance when distinguishing navigational queries in WT10g. Further, unlike our previous experiments, we have used the same set of training queries and methodology used in Kang (2005) . As a consequence, the vocabulary used in this comparison with previous work was smaller, which impacted negatively on terms . Note that the results of our feature sets could have been even better if additional features, as qpop , had been used. 4.5. Error analysis
In this section, we evaluate the query misclassifications observed using our more general taxonomy, C analyzed 105 queries from WBR03 and 67 queries from WT10g which were not classified under the same classes chosen by the human evaluators. The aim of this analysis is to point out possible reasons for the decisions of the automatic classifier. navigational, informational, and transactional queries, respectively.

As a result of this analysis, we observed in Tables 8 and 9 that the largest errors commonly involve informational queries, independently of the group of features used. These queries are clearly the most ambiguous. In particular, when using all the features, the commonest error is to confuse transactional queries with informational queries. When checking the feature val-ues for informational and transactional queries, we realized that they are quite similar. For instance, the average number of query terms is high in both categories when compared to navigational queries and the anchor text information related to the queries in both cases (informational and transactional) is usually found in several distinct domains. Note that the inclusion of click information probably would not reduce significantly this type of error, since the expected click patterns for informa-tional and transactional queries are quite similar. In both categories, the clicks are distributed across several possible target pages.

Finally, page features present a very different behavior among the collections, with errors evenly distributed in WBR03 and highly concentrated in WT10g, where half of the errors consisted in classifying transactional queries as informational.
When looking at the feature values, we realized that this is a consequence of the differences in the information available about the features in each collection. For instance, the crawling strategy adopted to create the collections is quite different, with WBR03 being created with priority to the number of domains covered, while WT10g was created by a crawler that tried to get a good coverage of each domain. As a consequence, WT10g contains about 144 documents per domain, while WBR03 contains just about 12 documents per domain. This better coverage of each domain in WT10g contributes to allow the better performance of features such as ddt in WT10g, since there is more text available per domain in WT10g.

Another example of difference between the two collections is the little anchor text information available in WT10g when compared to WBR03. WBR03 contains an average number of anchor text words per document higher than 200, while this average in WT10g is close to five. This difference negatively affects the quality of features based on anchor text in
WT10g. It is important to say that, conclusions about features proposed and experimented with other collections, such as the ones we got from Lee et al. (2005) and Lu et al. (2006) , are similar to the conclusions obtained when experimenting these features in WBR03.

After a manual inspection of the misclassified queries, we noted that many of them would be classified under more than one class. Since each query in WBR03 was evaluated by three subjects, we can study the impact of multiclassification cases in this collection. Table 10 shows the number of errors in two situations. At the first, we considered as correct only the majority class assigned by the evaluators (single-label setting). At the second, we considered as correct any of the assigned classes (multi-label setting). As we can see, from the original 105 errors observed for single-label setting (an accuracy of about 82%), 69 would be considered correct in a multiclassification setting (an accuracy of 93%). For the remaining 36 queries, the automatic classifier was unable to assign a class label chosen by any of the evaluators.

Table 10 also presents examples of these queries. A careful analysis of these errors indicates that there is not a specific reason for them. For instance, the query  X  X  X IM telecommunications X  was incorrectly considered as informational, while the query is navigational. This company is split in several smaller communication companies in Brazil, which resulted in a domain distribution similar to that typically found for informational queries. Further, the company is usually referred to as just  X  X  X IM X  and the inclusion of the word  X  X  X elecommunications X  also contributed to yield an unusual statistical pattern for a navigational query. We believe in this case the inclusion of new features, such as the click through information would contribute to reduce the error rate. However, when examining the errors we can conclude that it is almost impossible to al-ways provide a correct class to each query by using an automatic classifier. 5. Related work
In a classical IR system, the users are basically interested in finding information. Similarly, this is one of the most impor-
Detlor, &amp; Turnbull, 1999 ). However, such systems also play the role of tools for assisting users to locate and access a huge amount of resources available in the web. The study of query logs of web search engines has pointed out many of the main interests of users behind their queries. In Broder (2002) , the authors shown that 48% of the queries in their log were infor-mational, 30% transactional and 20% navigational. The remaining 2% was not classified. Another study ( Spink &amp; Jansen, 2004 ) found that approximately 12 X 24% of the submitted queries were related to e-commerce transactions. In a following study (Jansen, Spink, &amp; Pederson, 2005 ), based on query logs extracted from Altavista in 2002, the authors have shown that search engines were largely used as a navigational tool.

Since the quality of a web search engine is directly related to how well the system is able to met the interests of the users, many studies have focused on questions such as how to take advantage of the user goal information in search engines, aim-queries accurately and efficiently (Kang &amp; Kim, 2003; Kang, 2005; Lee et al., 2005; Lu et al., 2006 ).
 and navigational for improving the quality of results in search engines by applying specialized ranking mechanisms. In their studies, they assumed that the queries were already classified. They concluded that the query category information is useful, since the best ranking strategies are different for each query type. Thus, our work and previous query classification work can be used to provide the required query category information.

Another work that shows the possible advantage of using query category information is presented in Li et al. (2006) . The authors proposed a method for automatically identifying pages constructed with transactional goals. They also show that such classification can be used to improve the quality of search results when processing transactional queries. Thus, the clas-sification of pages and queries can be used as complementary strategies to improve quality of search results for transactional queries.

Regarding query classification, many works have adopted the taxonomy introduced by Broder (2002) , where queries are seen as navigational, informational, and transactional. For instance, Kang and Kim (2003) presented an approach for classi-fying queries as navigational or informational. They performed experiments on the web TREC collection ( Hawking, Voorhees, uments are separated into two sets, the topic documents and the homepage documents, by means of an automatic classifier.
These features consist of the distribution of query answers and the distribution of query terms into these two sets. The third feature explores the query usage rate as anchor text. Finally, the last feature consists of detecting certain part-of-speech tag regularities, such us the absence of verbs in navigational queries.

In a follow up work Kang (2005) dealt with transactional queries. The proposed method consisted on combining the fol-lowing sets of features through a machine learning technique: (1) the decision provided by its previous classifier ( Kang &amp;
Kim, 2003 ); (2) the first and last words in the query; (3) the identification of the query as a filename obtained by means of simple regular expressions; (4) link scores indicating the nature of the query as site, subsite, music, picture, text, appli-cation, service, html, and file. Link scores were obtained by using a training data where anchor-texts were tagged according to the actions associated with the hyperlinks (reading, visiting and downloading). These actions were determined by iden-tifying certain cues using regular expressions. From these features, we also use the query usage rate as anchor text (feature dda ). However, we calculate the similarity between queries and anchor texts using the Vector Space Model. We have no fea-tures similar to the ones based on pages previously classified and part-of-speech tagging. The sets of features (2) X (4) are somewhat related to our feature terms . However, especially features (3) and (4) are based on several heuristic cues whereas we simply use the original set of terms without any transformation. We conducted experiments with the same collections to compare these methods to our proposal. Note, however, that the features used by Kang can be used as additional features in our query representation which makes these works complementary.

Lee et al. (2005) propose the use of user-click behavior and anchor-link distribution for classifying queries as informa-tional or navigational. They performed experiments with 30 popular queries submitted to search engines from their univer-sity, excluding queries that could not be clearly classified by humans into one of the two categories. The anchor text information was obtained by crawling 60 million of pages extracted from the open Open Directory Project. The click through data was obtained by a log of accesses of users from their university to the web. Baeza-Yates et al also have studied the use of log information for query classification (Baeza-Yates et al., 2006 ) reporting that click through information is an excellent source of evidence for query classification. In both cases, the adopted collections are not public, which makes it hard to reproduce the experiments. Further, the collections we found available for experiments do not include click through infor-mation. We included in our study a comparison with the anchor-link distribution strategy proposed by Lee et al. and inves-tigate its impact when combined with new features here proposed.
 In Lu et al. (2006) , authors study the use of several machine learning methods for identifying navigational queries on the
Web, not addressing the detection of informational nor transactional queries. They use thousands of features, extracted from the click through data, query logs and the search engine database. Experiments were performed with 2012 queries randomly selected from a query log. Besides taking into account only navigational queries, the authors neither provided an exhaustive list of the features used nor detailed the size of their database. Further, their collection is not publicly available.
As far as we know, this is the first study on the influence of the query popularity on query classification. Finally we here also propose and study new alternative ways of using previously studied sources of evidence. 6. Conclusions
The evolution of web search engines is deeply grounded in their ability to deal appropriately with different user goals. In this work, we proposed a method for automatically learning how to effectively leverage the available sources of evidence to determine the goal of a user among three possibilities: navigate the web, get information, and perform a transaction. We here addressed this problem as a classification one and studied the impact of several features on the accuracy of the classi-fier. We have shown that the query popularity is a useful feature, able to enhance the classifier performance.
We also successfully tested new ways to calculate previously proposed features. Our experiments have pointed out some differences in the choice of the best set of features for WBR03 and WT10g. This is important to show that the best feature set may change according to the target collection. However, it is important to stress that the conclusions obtained about the performance of the previously proposed features when making experiments with WBR03 are similar to the ones obtained by previous work with other web collections (Lee et al., 2005; Lu et al., 2006 ), which indicates WT10g feature behavior di-verges from WBR03 and from other datasets adopted in literature.

Our experiments indicate that the use of query popularity, which was proposed here, can significantly improve the qual-ity of the classification results. Further, we also found that domain based features can be successfully used in classification tasks. The new query based feature terms proposed here has proved to be especially useful for identifying transactional que-ries. Finally, we also have studied the performance of the feature number of terms which, although being previously men-tioned in literature, have not being used before. Our experiments indicate this feature introduces noise in some of the classification tasks generally hurting the overall results. Further, by using our best set of features, we have reached gains in performance over previous approaches.

As future directions, we intend to consider other information sources such as the click through information and adver-tisements. We also intend to analyze the reliability of our method in a multiclassification setting.
 Acknowledgements This work was partially supported by grants from projects InfoWeb (550874/2007-0 CNPq), InWeb (573871/2008-6
CNPq), SIRIAA (55.3126/2005-9 CNPq); by individual CNPq fellowship grants to Edleno S. de Moura (302209/2007-7), Alti-gran S. Silva (308528/2007-7); by a FAPEAM Posgrad scholarship.
 References
