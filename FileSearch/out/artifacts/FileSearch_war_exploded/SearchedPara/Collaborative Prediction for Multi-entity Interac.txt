 With the rapid growth of Internet applications, there are more and more entities in interaction scenarios, and thus collaborative prediction for multi-entity interaction is be-coming a significant problem. The state-of-the-art methods, e.g., tensor factorization and factorization machine, predict multi-entity interaction based on calculating the similari-ty among all entities. However, these methods are usually not able to reveal the joint characteristics of entities in the interaction. Besides, some methods may succeed in one spe-cific application, but they can not be extended effectively for other applications or interaction scenarios with more en-tities. In this work, we propose a Hierarchical Interaction Representation (HIR) model, which models the mutual ac-tion among different entities as a joint representation. We generate the interaction representation of two entities via tensor multiplication, which is preformed iteratively to con-struct a hierarchical structure among all entities. Moreover, we employ several hidden layers to reveal the underlying properties of this interaction and enhance the model per-formance. After generating final representation, the predic-tion can be calculated using a variety of machine learning methods according to different tasks (i.e., linear regression for regression tasks, pair-wise ranking for ranking tasks and logistic regression for classification tasks). Experimental re-sults show that our proposed HIR model yields significant improvements over the competitive compared methods in four different application scenarios (i.e., general recommen-dation, context-aware recommendation, latent collaborative retrieval and click-through rate prediction).
 H.3 [ Information Storage and Retrieval ]: Information Filtering c  X  Collaborative Prediction, Multi-Entity, Factorization Mod-el, Hierarchical Representation
Nowadays, collaborative prediction plays an importan-t role in many real-world applications, e.g., recommender systems, information retrieval and social network analysis. With the rapid growth of web applications, there are more and more entities in one interaction scenario. For example, there are three entities in tag recommendation (i.e., user, item and tag) [25, 17, 20] and latent collaborative retrieval (i.e., user, query and document) [27, 2], and three or more entities in context-aware recommendation (i.e., user, query and several contexts) [4, 19, 21, 10]. Moreover, in some probability prediction applications, e.g., click-through rate prediction [12, 29, 14], there are even more entities in an in-teraction situation (e.g., device, website, application, adver-tisement and position). Accordingly, it is necessary to mod-el joint characteristics of entities and collaboratively predict interaction among multiple entities.

Matrix factorization [13, 8] is widely used for interaction prediction between two entities, e.g., users and items in gen-eral recommendation. And matrix factorization based meth-ods have been extensively studied, some of which, e.g., ten-sor factorization [28] and factorization machine [16], are im-plemented for predicting multi-entity interaction, e.g., user, query and document in latent collaborative retrieval. Both tensor factorization and factorization machine predict the interaction based on the similarity among all entities, which seems intuitive and reasonable. However, this does not con-form to situations of various applications. Using latent col-laborative retrieval as an example, both tensor factorization and factorization machine compute the similarity among us-er, query and document to predict their interaction. How-ever, the similarity between the user and the query does not contribute to document selection of this user, and final selec-tion is not merely based on the similarity between the user and the document or the similarity between the query and the document. We should measure the correlation between the document and the joint representation of user and query. In addition, there is another disadvantage of conventional methods. They only compute the similarity based on cor-responding dimensional values in the latent vectors, which does not allow rich high-order calculation among values of different dimensions. Therefore, to be general, we provide a latent representation for a multi-entity interaction, which describes how these entities would act being together with Figure 1: Illustration of the hierarchical structure of multi-entity interaction, using latent collaborative retrieval as an example. each other and can reveal their high-order relation. As il-lustrated in Figure 1, when a user retrieves with a query, capturing the mutual action of the user and the query, a joint representation is generated. Similarly, when the user views a document, there will be a joint representation of us-er, query and document, which indicates how this user will prefer a document while retrieving with the query. Based on this joint representation, we could make prediction for the multi-entity interaction.

Recent studies on representation learning [1] in different areas give us great inspiration. Recently, joint representa-tion of entities is employed for linked data [9] and structured data [24]. In recent works of natural language processing, using tensor multiplication, neural tensor networks are suc-cessfully used in learning the representation of two entities for knowledge base completion [22] and the representation of a sentence for semantic compositionality [23]. Tensor multi-plication is also used in modeling contextual representation for context-aware recommendation [21, 10].

In this paper, we present a novel method to learn a Hi-erarchical Interaction Representation (HIR) model for pre-dicting interaction among multiple entities. In our method, each entity is represented as a latent vector. And we use three-dimensional tensor multiplication to capture the joint characteristics of two entities. Tensor multiplication allows high-order calculation between these two entities. Moreover, rather than achieving a score of the interaction in conven-tional methods, HIR generates an interaction representa-tion, which allows interacting with the next entity and can be widely implemented for various tasks. In a multi-entity interaction scenario with n entities, this tensor multiplying procedure can be performed iteratively, which forms a hier-archical architecture with n layers. The i th layer presents the interaction representation of the first i entities. The n th layer is the final representation of all the entities, which describes their joint characteristics. With such a hierarchi-cal structure, we can also add new entities directly with-out re-training the former representations. Based on the final representation, we could select learning methods ac-cording to different application tasks for collaborative pre-diction (i.e., linear regression for regression tasks, pair-wise ranking method for ranking tasks and logistic regression for classification tasks).

The main contributions of this work are listed as follows:
The rest of the paper is organized as follows. In section 2, we review some related work on both general methods and task-specific methods. Section 3 details our HIR mod-el. In section 4, we introduce the learning methods of the HIR model. In section 5, we report experimental results in terms of four tasks and compare with several state-of-the-art methods. Section 6 concludes our work and discusses future research.
In this section, we briefly review related work on both general and task-specific methods, i.e., context-aware rec-ommendation, latent collaborative retrieval, tag recommen-dation and click-through rate prediction.
Matrix Factorization (MF) [13, 8] has become one of the state-of-the-art approaches to collaborative prediction. The basic objective of MF is to factorize a matrix into two low rank matrices, each of which represents the latent factors of entities, e.g., users and items in general recommendation. The original matrix can be approximated via the multiply-ing calculation. However, MF is not able to be implemented directly for collaborative prediction problem of multiple en-tities.

MF has been extended nowadays. Tensor Factorization (TF) [28] extends MF from two dimensions to three or even higher dimensions, which can be utilized for predicting multi-entity interaction. And Factorization Machine (FM) [16] gives a further extension of TF by modeling all interactions between pairs of entities. FM is a flexible model, and other state-of-the-art factorization models including TF, SVD++ [5] and timeSVD++ [6] can be implemented using FM by defining the input data or features [16]. Both TF and FM are successfully used in tag recommendation [25, 17, 20] and context-aware recommendation [4, 19]. FM also leads great improvement in click-through rate prediction [14]. However, predicting multi-entity interaction based on simply calcu-lating the similarity among all entities, TF and FM are not able to better reveal the joint characteristics of interacting entities. Moreover, because FM uses the pair-wise interac-tion between entities to model the prediction of multi-entity interaction, this brings FM another disadvantage that the number of pairs grows exponentially with the number of en-tities.
In this subsection, we introduce some research works in d-ifferent tasks that are related to the collaborative prediction for multi-entity interaction.
The context modeling approaches have made significan-t improvement for context-aware recommendation, which contains three or more entities (i.e., user, query and sev-eral contexts). Recent works on context modeling have fo-cused on integrating contextual information with user-item rating matrix and building factorization models. Incorpo-rating tensor factorization, multiverse recommendation [4] represents the user-item rating matrix with contextual in-formation as a user-item-context rating tensor, which is fac-torized with Tucker decomposition [26]. And FM is appli-cable to a variety of contexts by specifying only the input data [19]. Random decision trees are also been applied here, in which contexts are split and general matrix factorization is performed on each leaf [11]. Furthermore, the work of [30] considers user attributes as priors for user latent vec-tors, and a transfer matrix is used to generate latent vectors from original ones. Similarly, Heterogeneous Matrix Factor-ization (HeteroMF) [3] generates context-specific latent vec-tors of entities using a context-dependent transfer matrix and the original latent vectors of entities. CARS2 [21] and Contextual Operating Tensor (COT) [10] model represents entities in a specific context using contextual representation and outperform previous methods in context-aware recom-mendation. Both of them model contexts as latent vectors, and generate contextual representations of users and items using tensor multiplication.

Recently, modeling the interaction among user, query and document, Latent Collaborative Retrieval (LCR) [27] has been proposed and extended to social network [2]. LCR represents a user with a matrix and generates the joint rep-resentation of user and query via multiplication. Then the prediction is given based on the inner product of latent rep-resentation of query and joint representation of user and query. Modeling the interaction among user, item and tag, tag recommendation can also be viewed as a LCR problem, in which a user retrieves tags with an item. Treating user, item and tag as three dimensions of a tensor, TF has been successfully used in this problem [25, 17]. As an extended version of TF, FM is also applied in tag recommendation and has become one of the state-of-the-art methods [20].
In the complex click-through rate (CTR) prediction prob-lem, there may exist several entities such as user, device, website, application, advertisement, position and so on. Due to its ease of implementation and promising performance, Logistic regression (LR) has been widely used for CTR pre-diction, especially in industrial systems [12, 29]. LR can be used for the multi-entity interaction prediction with one-hot representation of each entity. However, LR or other clas-sifiers have difficulties in discovering latent relation among entities, and the final prediction of LR could be viewed as the sum of biases of all the entities. Recently, as a repre-sentative method of factorization models, FM is applied for CTR prediction and brings a great improvement [14].
The methods mentioned above achieve delightful results in respective applications, which also greatly motivates our work. But they are unable to be extended effectively for other applications or interaction with more entities.
In this section, we introduce our proposed hierarchical in-teraction representation. We introduce the definition and notations of our problem at first. Then we present the in-teraction representation of two entities, followed by the hi-erarchical representation of more entities. Finally, we show how to enhance the model via employing hidden layers.
The problem we study in this paper can be described as follows. In an interaction scenario, suppose that we have an interaction prediction task with n types of entities denoted by { E (1) ,...,E ( n ) } . For each type of entity, we have E { e multi-entity interaction, the relation among e (1) k for multi-entity interaction is to give a prediction  X  y based on all the entities e (1) k
Here, using latent collaborative retrieval as an example, there will be three types of entities (i.e., user, query, doc-ument) denoted by { E (1) ,E (2) ,E (3) } . Specifically, users, queries and documents are denoted by E (1) = { e (1) 1 ,e E Each user, query and document are denoted as e (1) i  X  R e query and document is denoted as y k 1 ,k 2 ,k 3 .

In this work, we would jointly model all the entities with an interaction representation r ( n ) k entities with an interaction representation r ( m ) k teraction representation describes the joint characteristics of entities. We could make prediction of their interaction based on this representation using various learning methods (i.e., linear regression, logistic regression and pair-wise ranking).
We first introduce the case with two entities. To get the joint representation of two entities, we need to employ an interaction function that satisfies: where f (2) (  X  ) denotes an interaction function of two entities. Latent vector r (2) k have being together in an interaction scenario. For instance, if there are user and movie as two entities, their joint rep-resentation captures the characteristics of a user watching a movie, and models how much he or she likes the movie.
To capture the high-order relation between two entities, we use tensor multiplication, as shown in the bottom square of Figure 2. Here, we compute the interaction representation as: where T (1) is a d  X  d  X  d tensor, modeling the interaction of these two entities. The tensor multiplication e (2) k can be defined as: Figure 2: Overview of the learning procedure of HIR. Interaction representation of two entities is shown in the bottom square. Each layer of HIR is shown in the middle square. The top part illus-trates hidden layers between interaction representa-tion and prediction. and Equation 2 can be calculated as: where T (1) m denotes the m th slice of the tensor T (1) , and each slice is a d  X  d matrix.

With such computation, the tensor can model the rich high-order relation between entities. Each slice of the tensor can capture a specific type of relation between these entities.
Now, to predict the interaction of the two entities, we can use a simple linear regression, which can be concluded as: where W is a n -dimensional vector denoting the weights of regression.
After introducing the joint representation of two entities, we would model the joint representation of more entities in general cases. For more entities, we need another interaction function that satisfies: where f ( n ) (  X  ) denotes an interaction function of n entities. r ,...,k n describes the joint characteristics of all the entities in an interaction scenario.

To generate the interaction representation of n entities, we perform the tensor multiplication iteratively, which forms a hierarchical structure with n layers. These procedures can be concluded as: where 1 &lt; m  X  n , and T ( m  X  1) is a d  X  d  X  d tensor, generating the interaction representation in the m th layer. Each layer of HIR is illustrated in the middle square of Figure 2. Thus, the joint representation of all entities can be calculated as:
Taking latent collaborative retrieval for an example, there are three entities, i.e., user, query and document. Their in-teraction representation captures how a user prefers a docu-ment while retrieving with a query, which can be calculated as:
With the joint representation, we can compute the collab-orative prediction. A variety of machine learning methods can be used according to the specific task. For example, with a linear regression method, the prediction of multi-entity in-teraction can be generated as: where W is a n -dimensional vector denoting the weights of regression. Moreover, in the task of latent collaborative retrieval, the prediction can be generated as:
Deep models with multiple layers have shown delightful performance in different areas [1]. As shown in the top square of Figure 2, to enhance the ability of representa-tion and to improve the performance of our model, we can add several hidden layers between the joint representation r
Suppose we add n h hidden layers between the final repre-sentation and the prediction. With each hidden layer mod-eled by a matrix, the joint representation after hidden layers can be calculated iteratively as: where 1  X  m h  X  n h , h ( m h ) k tation after m h hidden layers, and H m h is a d  X  d matrix of the m h th hidden layer. Combining interaction layers and hidden layers, the overall formulation generating the joint representation becomes: Furthermore, with a linear regression method, the final pre-diction can be made via: In this section, we introduce the learning process of the HIR model in three different tasks, i.e., personalised rec-ommendation, personalised ranking and click-through rate prediction.
In recommender systems with explicit rating values, the interaction prediction in HIR becomes rating prediction, which can be modeled as a regression problem. The model can be calculated via a linear regression: where W denotes the weights of linear regression. Therefore, HIR can be learned by minimizing the squared error [8], and the objective function can be written as: where  X  = { E,T,H,W } denotes all the parameters to be estimated, and  X  is a parameter to control the power of regularization.

The derivations of J with respect to the parameters can be calculated as:  X  X   X  X   X  X 
In recommender systems and information retrieval, differ-ent from scenarios with explicit feedbacks, scenarios with implicit feedbacks (i.e., a binary signal such as click, buy, view, etc) are more common in real-world applications. This can be treated as a ranking problem. Bayesian Personalized Ranking (BPR) [18] is a state-of-the-art pairwise ranking method for the implicit feedback data. The basic assump-tion of BPR is that a user prefers a selected item than a negative item. So, using HIR in the BPR framework, we need to maximize the following function: where k n denotes a positive sample, k 0 n denotes a negative sample, and g ( x ) is a nonlinear function which we select as g ( x ) = 1 / (1 + exp (  X  x )). Note that, the prediction here is based on linear regression as shown in Equation 15. Incorpo-rating the negative log likelihood, we can solve the following objective function equivalently: =  X  X ln 1 where
The derivations of J with respect to the parameters can be calculated as:  X  X   X  X  where
Predicting the probability of clicking under a specific con-text, click-through rate prediction can be treated as a clas-sification problem. As regular models for CTR prediction [12, 29], we incorporate logistic regression, and the predic-tion becomes: where W denotes the weights of logistic regression. As in regular logistic regression, we use the negative log likelihood for model learning. The objective function can be written as:
The derivations of J with respect to the parameters can be calculated as:
In three tasks mentioned above, the derivation  X  X  has been calculated. Based on this derivation, we can cal-culate the derivations of hidden layers and joint represen-tation iteratively. Suppose we have  X  X  (0 &lt; m h  X  n h ) hidden layer, we can calculate the deriva-tions of parameters on this layer as: On the first hidden layer, we can obtain the derivation of the joint representation:
Now, with the derivation of the joint representation  X  X  we can calculate all the derivations iteratively. Suppose we culate the derivations of parameters on this layer as: On the first layer, we can obtain the derivation of the first entity: After calculating all the derivations, a solution of learning HIR can be obtained by using stochastic gradient descent (SGD), which has been widely employed [8, 7]. Note that, the learning method can be changed according to different applications.
In this section, we empirically investigate the performance of HIR. As shown in Table 1, we conduct our experiments in four tasks: general recommendation with user and movie , context-aware recommendation with user , item and context , latent collaborative retrieval with user , URL and tag , and click-through rate prediction with device , website , application , advertisement and position . In all experiments, we implement HIR with 0, 1 and 2 hid-den layers, denoted as HIR, HIR+ and HIR++ respectively. Then we analyze the impact of the dimensionality of latent representations and examine the interacting order of enti-ties in context-aware recommendation and latent collabora-tive retrieval. Finally, we visualize representations in latent collaborative retrieval.
Our experiments are conducted on four real datasets for different applications:
For all the datasets, we use 70% for training, 20% for testing, and the remaining 10% data as the validation set for tuning parameters, i.e., the dimensions of latent repre-sentations. And the regulation parameter is set as  X  = 0 . 01. Moreover, we have different evaluation metrics for different tasks: http://grouplens.org/datasets/movielens/ http://grouplens.org/datasets/hetrec-2011/
The p -core of the dataset is the largest subset of the dataset with the property that every user, every item and every tag has to occur in at least p posts. https://www.kaggle.com/c/avazu-ctr-prediction/data
We compare HIR with both general and task-specific meth-ods: http://www.libfm.org/ The top part of Table 2 illustrates the results measured by RMSE and MAE on Movielens-10M, which show that HIR achieves the best results in general recommendation com-pared with MF and FM. HIR, HIR+ and HIR++ improve the performance of FM by 0 . 6%, 1 . 1% and 1 . 5% respectively. The results show that the high-order tensor multiplication in HIR can better represent the joint characteristics of user -item pair and thus improve the performance of collaborative prediction.
 Table 2: Performance of general recommendation and context-aware recommendation evaluated by RMSE and MAE.
The bottom part of Table 2 shows the results of context-aware recommendation measured by RMSE and MAE. HIR outperforms the state-of-the-art context-aware methods. And from the results, we can see that, comparing with COT, the performance improvements of HIR, HIR+ and HIR++ are 2 . 7%, 3 . 8% and 4 . 4% respectively. The improvemen-t involved by the first and second hidden layer are 1 . 1% and 0 . 6% respectively, which proves that hidden layers can improve the performance but yield a smaller improvement than that of HIR itself. Moreover, we can observe that the improvement of hidden layers decreases gradually with its number, which means that the effort of hidden layers has its own limitation and we do not need to set too many hidden layers.
Table 3 illustrates the results of latent collaborative re-trieval measured by recall, precision, F1-score and MAP on the Delicious dataset. Note that, all the methods are implemented under the BPR framework. The results ob-viously show that HIR greatly outperform the compared methods. On recall@5, HIR, HIR+ and HIR++ improve the performance of LCR by 8 . 5%, 12 . 1% and 12 . 7% respec-tively. And on recall@10 and recall@20, the corresponding improvements become 8 . 3%, 11 . 4%, 11 . 8% and 8 . 6%, 8 . 9%, 10 . 1%. Measured by precision and F1-score, the same im-provements can be observed. Moreover, on global evaluation metric MAP, our proposed HIR, HIR+ and HIR++ improve the performance by 5 . 0%, 7 . 8% and 8 . 8% respectively. We can draw similar conclusion that the performance improve-ment of hidden layers decreases gradually with the number of hidden layers in most experiments. The table also shows that, compared with the results on @5 and @10, the second hidden layer of HIR++ on @20 brings a greater improve-ment, which means that when we generate more predicted results, more hidden layers can be used to further improve the performance. The results of click-through rate prediction measured by LogLoss are illustrated in Table 4. Note that, all the meth-ods are implemented with the same objective function un-der LogLoss. We can observe that, both latent factor based methods, i.e., FM and HIR, outperform LR, which shows that latent factor based methods can better discover under-lying relation of different entities. Moreover, in both exper-iments, without or with the position entity in the advertise-ment impression, HIR achieves the best performance. Com-pared with FM, HIR, HIR+ and HIR++ improve the per-formance by 1 . 4%, 2 . 2%, 2 . 5% and 1 . 5%, 2 . 3%, 2 . 5% respec-tively in these two experiments, which shows the promising performance of HIR for multi-entity interaction prediction with a number of entities.
Performance of HIR and compared methods on the Food dataset and the Delicious dataset with varying d is shown in Figure 3. For simplicity, we do not illustrate performance of MF and LR in the figure. Instead of that, we illustrate the performance of several state-of-the-art methods and HIR with 0, 1 and 2 hidden layers in our experiments.
 Table 4: Performance of click-through rate predic-tion evaluated by LogLoss.
As shown in Figure 3(a), for all the methods, with increas-ing d , the value of RMSE decreases at first, then increases after d = 5, which means that 5 is the best dimension for all methods. We can also observe that HIR outperforms the compared methods consistently with all dimensions. More-over, as shown in Figure 3(b), d = 8 is the best parameter for all ethods on the Delicious dataset. We can observe that the impact of dimensionality for latent collaborative retrieval is smaller than that for context-aware recommendation, and HIR performs the best compared to other methods.
In previous sections, we have illustrated the order user , query , document of information retrieval and the order user , context , item of context-aware recommendation. However, for some applications, the orders are sometimes difficult to be determined exactly. In such situations, it will cost lots of labor to determine the proper order. Thus, to examine the impact of interacting order of entities in HIR, we conduct experiments with all possible orders for two tasks: context-aware recommendation on the Food dataset and latent col-laborative retrieval on the Delicious dataset. We implement HIR with 0, 1 and 2 hidden layers in this experiment. Table 5: Performance under different interacting or-der on the Food dataset evaluated by RMSE of HIR.
The RMSE results of HIR under different interacting or-ders on the Food dataset is shown in Table 5. The vari-varying d .
 Table 6: Performance under different interacting or-der on the Delicious dataset evaluated by MAP. ances of RMSE results are also shown in the table. Note that, these results are all based on the same initialization. With the same number of hidden layers, performances of HIR are approximately the same with different interacting orders. The small variances also show that the impact of the interacting order for context-aware recommendation is not significant. Similar conclusion that the impact of the interacting order for latent collaborative retrieval is not sig-nificant can also be drawn from Table 6. In a word, in these applications, different interacting orders often achieve sim-ilar performance. The unnecessary of the settled structure can greatly reduce the labor for analyzing the raw data and selecting a suitable order when there is no obvious one.
Here, using the Delicious dataset as an example, we plan to demonstrate the representations in HIR, and describe some interesting observations. In Figure 4, we use principal component analysis (PCA) to project the representations into a two-dimensional space. The distance in this figure reveals the relation of different representations.
 We select three distinguishable users and name them as Jack , Tom and Marry , as shown in Figure 4(a). The repre-sentations of URL and tag are illustrated in Figure 4(b) and Figure 4(c) respectively, in which we can observe that the representations of similar URLs or tags are close. Figure Figure 4: Visualization of interaction representation in the Delicious dataset. 4(d) shows the joint representations of user and URL . Al-though Jack and Tom have different representations, they have similar interaction representations when visiting pro-gramming websites. Similarly, although Tom and Marry have different representations, they have similar interaction representations when visiting movie websites. Figure 4(e) illustrates the interaction representations of user , URL and tag . We can observe that positive samples and negative sam-ples are clearly divided into two groups in the two-dimensional space according to their interaction representations.
In this paper, for modeling collaborative multi-entity in-teraction, a novel method, i.e., HIR, has been proposed. In HIR, we generate the interaction representation of two en-tities via tensor multiplication and this process is repeated iteratively for the interaction of several entities. This pro-cedure forms a hierarchical structure and can generate the final joint representation. Then the prediction can be cal-culated based on this representation using various learning methods. The experimental results of four different applica-tions show that HIR outperforms both general methods and state-of-the-art task-specific methods.

In the future, we can further investigate the following di-rections. First, in HIR, the dimensions of entities can be adjusted for different datasets, which leads lots of hyper-parameters to be determined. So, we need to find a method to determine the dimensions automatically. Second, since there are rich features in real-world applications, we plan to incorporate the entities X  feature information in HIR. This work is jointly supported by National Basic Research Program of China(2012CB316300), and National Natural Science Foundation of China (61403390, U1435221, 61175003, 61420106015). [1] Y. Bengio, A. Courville, and P. Vincent.
 [2] K.-J. Hsiao, A. Kulesza, and A. Hero. Social [3] M. Jamali and L. Lakshmanan. Heteromf: [4] A. Karatzoglou, X. Amatriain, L. Baltrunas, and [5] Y. Koren. Factorization meets the neighborhood: a [6] Y. Koren. Collaborative filtering with temporal [7] Y. Koren and R. Bell. Advances in collaborative [8] Y. Koren, R. Bell, and C. Volinsky. Matrix [9] K. Li, J. Gao, S. Guo, N. Du, X. Li, and A. Zhang. [10] Q. Liu, S. Wu, and L. Wang. Cot: Contextual [11] X. Liu and K. Aberer. Soco: a social network aided [12] H. B. McMahan, G. Holt, D. Sculley, M. Young, [13] A. Mnih and R. Salakhutdinov. Probabilistic matrix [14] R. J. Oentaryo, E.-P. Lim, J.-W. Low, D. Lo, and [15] C. Ono, Y. Takishima, Y. Motomura, and H. Asoh. [16] S. Rendle. Factorization machines with libfm. TIST , [17] S. Rendle, L. Balby Marinho, A. Nanopoulos, and [18] S. Rendle, C. Freudenthaler, Z. Gantner, and [19] S. Rendle, Z. Gantner, C. Freudenthaler, and [20] S. Rendle and L. Schmidt-Thieme. Pairwise [21] Y. Shi, A. Karatzoglou, L. Baltrunas, M. Larson, and [22] R. Socher, D. Chen, C. D. Manning, and A. Ng. [23] R. Socher, A. Perelygin, J. Y. Wu, J. Chuang, C. D. [24] V. Srikumar and C. D. Manning. Learning distributed [25] P. Symeonidis, A. Nanopoulos, and Y. Manolopoulos. [26] L. R. Tucker. Some mathematical notes on three-mode [27] J. Weston, C. Wang, R. Weiss, and A. Berenzweig. [28] L. Xiong, X. Chen, T.-K. Huang, J. G. Schneider, and [29] L. Yan, W.-J. Li, G.-R. Xue, and D. Han. Coupled [30] L. Zhang, D. Agarwal, and B.-C. Chen. Generalizing
