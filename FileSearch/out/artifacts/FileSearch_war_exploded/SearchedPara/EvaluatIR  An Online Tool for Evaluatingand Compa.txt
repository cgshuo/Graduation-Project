 H.3.4 [Information Storage and Retrieval]: Systems and software  X  performance evaluation .
 Retrieval experiment, evaluation, system measurement.
 A fundamental goal of information retrieval research is to develop new retrieval techniques, and to demonstrate that they attain im-proved effectiveness compared to their predecessors. To quanti-tatively compare IR techniques, the community has developed a range of standard test collections, in particular the TREC collec-tions; see Voorhees and Harman [2005].

Researchers use these collections as experimental test-beds, and use the observed improvements as evidence of the significance of their research contribution. Most commonly, a baseline system is chosen and improvements relative to this are measured and then presented as evidence of superiority. However, these baselines are frequently inappropriate, and there is often little consistency be-tween researchers or research groups as to how effectiveness ex -periments are carried out and then reported. Ideally, the current best published results would be used as a baseline, but such prac-tice is rare; and  X  a further confound on good practice  X  researchers usually only publish summary metrics, which cannot be used to establish statistical significance when used in subsequent compar-isons. The original TREC runs are available for detailed analysis, but are rarely referred to when new methods are proposed.
Instead, authors make use of off-the-shelf software, or of variants of their own software, but neither of these approaches is particu-larly compelling. Any claims based on comparison to such base-lines must be treated with scepticism, and researchers can easily (either inadvertently or deliberately) publish non-competitive  X  X m-provements X  simply by comparing to an even poorer baseline. For example, in some papers the developers of query expansion tech-niques compare to unexpanded baselines; whether the methods im-prove on other expansion techniques is not demonstrated. More broadly, it is often the case that a method that improves on a poor baseline is in effect doing no more than compensating for a defect, and the method cannot improve a system that is already effective.
These issues mean that a reader or referee cannot easily estab-lish whether published results demonstrate a genuine advance in effectiveness, and the enormous labor invested in developing test
