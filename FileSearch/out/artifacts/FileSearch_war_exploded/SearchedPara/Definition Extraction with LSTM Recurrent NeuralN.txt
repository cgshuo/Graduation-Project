 Definitions play an important role in creating and enriching ontology concepts from unstructured text [1]. Definitions are also put to use in Ques tion Answering to solve  X  X hat is X  problems [2]. A big challenge is how to collect defin itions from emerging mass text, since manually extracting definitions from unstr uctured text can be costly and slow. Therefore automatic definition extraction has dra wn much attention in Natural Language Processing.
 cation task where a sentence is classified as definitional or n ot. The key point of this task is to generate features which are usually manually sele cted in previous approaches. For example, DefMiner system [3] specifies 12 features (8 wor d level features,3 sen-tence level features and 1 document level feature). In a weak ly supervised method [4], they use 14 features to describe a sentence. Both methods use many manually selected features (e.g. dependency path distance: distance from the current word to the root of the sentence in the dependency tree) to approximately descr ibe a sentence structure from its dependency tree. However there are two shortcoming s in these features. First of all, they can only reflect part of the dependency tree which may lose hidden structure feature of sentences. Moreover they rely on the output of dep endency parsing which in-volves error propagation. Therefore we focus on studying a m ethod which can generate features directly from raw sentence.
 extraction are automatically learned from raw sentences. I nstead of relying on depen-dency parsing result, we regard a sentence as a word sequence and directly learn from the whole sequence. We generate sentence feature using recu rrent neural network with Long Short-Term Memory (LSTM) due to the reason that LSTM has shown great abil-ity in capturing long-term and short-term dependencies in a sequence. In our method, a sentence will first be transformed into a sequence consiste d of word feature vectors. A LSTM encoder will be trained to encode the sequence into a ve ctor representation. Finally a logistic regression classifier will be used to pred ict the sentence label with its feature vector. Instead of manually selecting features fro m dependency parsing, our sen-tence feature vector is directly generated by our LSTM encod er automatically, which can reduce the work of sentence feature engineering.
 and a Chinese dataset 1 originated from Baidu Baike 2 . The result shows that our method can significantly outperforms current state-of-the-art ap proach in F1 measure by 5.8%, and improve the recall to 92%. The main contributions of this work can be summarized as follows:  X  We propose a new method utilizing LSTM to do definition extrac tion.  X  Our method does not rely on handcraft patterns nor features m anually specified  X  Our method relies little on linguistic features and can be us ed for definition extrac-section 2. Then section 3 describes the method we use to build our definitional classifi-cation model. We describe our experiment in section 4, follo wed by results and analysis in section 5. Finally, our paper is concluded in section 6. The task of definition extraction has attracted many attenti on in the past few years. Pre-vious researches can be divided into three kinds: pattern ba sed [8, 9], semi-supervised learning [5, 4] and supervised approaches [10, 3]. 2.1 Pattern based methods The method focusing on the use of lexico-syntactic patterns , is first put forward by Hearst [11]. However relying on simple definitional pattern s (such as is-a, is called) matching can be noisy, since short patterns may not fully rep resent the sentences X  struc-ture features. Some systems [8, 12] use patterns to extract c andidates which will be further checked by grammar analysis. In order to introduce m ore complex patterns, a method [9] uses part-of-speech (POS) tag patterns rather t han simple sequences of words.
 recall. The expression of definitional sentences can vary gr eatly in different datasets, so it is difficult to craft complete patterns which can identify all definitional sentences. Due to lack of generating ability caused by the simple strategy o f pattern matching, a fully automated way using genetic programming is proposed to lear n individual weights of features [13]. The genetic programming based method takes t he combination of features into consideration but it ignores the importance of the orde r of feature occurrence. 2.2 Semi-supervised methods Semi-supervised approaches [14, 4] show that bootstraping is efficient for definition extraction. The basic idea of these methods is to start from g old sentences and extract more from articles in other datasets such as ACL Anthology Re ference Corpus (ACL ARC) [15].
 tional sentences. WCLs method uses  X  X tar patterns X  to make se ntence clustering and then learns the sentences X  structure in each cluster. Unlik e patterns consisted of short phrases, their  X  X tar patterns X  are drawn from sentences whe re all infrequent words have been replaced with a wildcard (*), so they can capture long di stance dependency of sen-tences with these star patterns. Although they form general ized sentences with WCL, their result for WCL ( F 1 of 75 . 23% ) is close to the performance of method only us-ing star patterns ( F 1 of 75 . 05% ). It shows that even only using sequence structure of frequent words, we can also predict whether a sentence is defi nitional or not well . 2.3 Supervised methods This line of research area is to regard definition extraction task as a supervised sequence classification work. A method [10] proves to work well on a cor pus of Dutch Wikipedia articles. They generate three kinds of features to describe a sentence: sentence-level features (e.g. the position of the sentence in a document); w ord-level statistic features (e.g. bigrams, bag-of-words); word-level syntax features (e.g. determiner type). With these features, they use naive Bayes, maximumu entropy (Max Ent) and the support vector machine (SVM) as different learners and find MaxEnt ha s the best performance. Sentences X  linguistical structural features are taken int o consideration in an approach proposed by [16]. A random forest classifier is used to captur e more deep features of a sentence structure.
 word in a sentence as term, definitional part or others. One of their key contributions is the analysis of a real world corpus: W00 (a manually annotated subset of ACL ARC). They use a combination of lexical, orthography, dictionary lookup and corpus statistics (e.g. sentence position, idf) as features. Meanwhile, they also use features manually selected from dependency tree to describe sentence long dis tance structures. on extracting features from the dependency parsing of a sent ence. Each noun in a sen-tence will be represented into a numeric vector according to its syntactic dependencies with other nouns in the same sentence. The vector representa tion will be fed to two classifiers to determine whether the noun is hyponym, hypern ym or neither of two. If a sentence has both hyponym and hypernym, it will be labeled as definitional. Their work highlights the importance of learning from relations of wor ds.
 many feature engineering work. Meanwhile, structure featu res are chosen from part of the dependency tree, which may not be able to fully describ e the whole sentence structure.
 matically generating sentence features for supervised cla ssification. In our method, we design our procedure based on the assumptio n that we can determine whether a sentence is definitional or not by certain structur es. We call these sentence structures as definitional structures. Definitional struct ure can be a short phrase or a long discontinuous word sequence.
 In order to increase the generalization ability of our model , we use feature vector to reflect the usage of definitional structure and classify sent ence according to its feature vector instead of simple pattern matching.
 a typical challenge is how to generate feature from long-ter m definitional structure. Pre-vious supervised based approaches learn definitional struc ture using n-grams or condi-tional random field (CRF). These algorithms have strong abil ity in learning short-term dependency features but are weak in capturing long-term defi nitional structure. There-fore, we generate sentence feature using LSTM which is fit for both short-term and long-term structure learning.
  X  Token transformation: each word in a sentence will be transformed into a token  X  Word feature generation: each word will be represented as a word vector by cap- X  Sentence feature generation: A LSTM encoder will be used to automatically 3.1 Token transformation In definition extraction, for most of the words, we care more a bout its POS tag rather than the word itself. For example, We do not care the differen ce between  X  X at is an animal X  and  X  X og is an animal X . Thus these two sentences can b e transformed to one sentence  X  X N is an NN X  by replace some words with their POS tag s.
 significant form for definition extraction. In this step, we p ick top N frequent words from our training set. The reason we do not choose words X  tf-i df is that some words (e.g. is, a, etc.) are important to form a definitional patter n in spite of their low idf. These chosen words form a set F which can be regarded as a cluster of words. These words are potential constituent part of definitional patter ns. More precisely, given a sentence S of length n , for the i-th word w i of s , we produce token t i by 3 : 3.2 Word feature generation After transforming each word into a corresponding token, ou r next question is how to generate the features of these tokens. In supervised classi fication based methods, they use words bigrams [10] and POS tag bigrams [17] as sentence fe atures and use infor-mation gain method to reduce the count of features. However, they make their features completely independent with each other. For example, if we o nly use a bigram (Num, N) as feature, then other bigrams (e.g. (Adj, N)) will never b e taken into consideration. token so that we can measure their similarity by calculating their distance. The more similar two tokens are, the less influence will be caused when one changes to another. We use the context of a token (i.e. tokens near the token) to de scribe the token as the context can reflect the usage of the token.
 imum the possibility of the occurrence of a word according to its context. The method works on the assumption that similar words have similar cont ext which is also fit for our situation. Thus we use word2vec to encode tokens in our tr aining set. 3.3 Sentence feature generation Sentence features can be divided into two kinds. One is to gen erate features only within a single sentence (e.g. bag-of-words, n-grams [10], the pre sence of determiners in the defines, etc.). The other kind is using features beyond a sent ence such as the position of a sentence in the document [3]. sition feature for example, sentences appearing at the begi nning of documents are likely to be definitional in wikipedia documents, while things may n ot be the same with other corpus.
 its definitional structure. Previous methods generate feat ures from sentence dependency parsing, which can only reflect part of structure feature. In our method, we will treat a sentence as a sequence of tokens and use LSTM to capture the wh ole structure of the sequence.
 Feature generator Our feature generator consists of a single layer of Long shor t-term memory (LSTM) unit [19] which is designed to overcome th e gradients vanishing problem in recurrent neural network (RNN) [20]. LSTM uses sp ecial gates to control the flow of data in repeating module, which can help them be cap able of learning long-term dependencies. LSTM uses three gates to implement the re peating module: a forget gate f to control of the flow of cell state, an input gate i to control the inputing data and an output gate o to control the output of the module. Each LSTM unit maintains a memory cell c .
 word feature generation step. For the t -th token in x , the output h of LSTM is given by where the output gate is given by The cell state is calculated by where  X  c t denotes a new memory content computed by Meanwhile, the forget and input gates are computed by We use the n -th output h n of LSTM as the feature vector of the sentence. Joint learning of feature generator and classifier Given a sentence feature vector x , we use a logistic regression classifier  X  ( w T x ) to predict the sentence label. The final result is influenced by both parameter w of classifier and the sentence feature vector x . Therefore we train our classifier and sentence feature gener ator together. sequence of word feature vectors s  X  . Let T  X  denote the cluster of s  X  . In the sentence feature generation step, we use h ( s  X  ) to represent the n -th step output of LSTM. We train our model by mining the cost function: vector. Here crossentropy is given by together. An open source implementation, Theano [21] is use d in our work 4 . We calcu-late gradients for all the parameters (i.e. w and parameters of LSTM) and update them using RMSprop optimizer.
 greatly influenced supervised classification based methods . As the training procedure of our method uses gradient descent, we overcome this proble m by setting different learning rate to positive and negative samples according to their count ratio. We prepare two datasets for the evaluation of our method. One is an English dataset [5] consisted of 1,908 definitional sentences and 2,711 non-defi nitional sentences manually annotated from Wikipedia. The other one is a Chinese dataset containing 2,161 defini-tional sentences and 2,161 non-definitional sentences manu ally annotated from Baidu Baike 5 . In this part, we will describe detail experiment settings a nd the way we evaluate our result. 4.1 Experiment settings In the word feature generation step, we use Top-1,000 freque nt words set for English dataset and Top-500 frequent words set for Chinese dataset. Every token will be en-coded into a 50-dimension vector. The output dimension of th e LSTM encoder used in our sentence feature generation step is 50 . The parameters of LSTM encoder are initialized with the uniform distribution with the scale in troduced by [22]. the following implements.  X  Star patterns : A pattern based method where a sentence is classified as a defi nition  X  Bigrams : A pattern based method which uses bigram classifier for soft pattern  X  WCL : A semi-supervised method which uses WCL-3 model to learn lat tices sepa- X  DefMiner :a supervised method which uses long distance features [3].  X  SVM : a supervised method which uses SVM classifier and syntactic dependencies.  X  DR system : a supervised method which only uses syntactic features der ived from  X  our method+LSTM : Our method using LSTM as sentence feature generator.  X  our method+RNN : Our method using RNN as sentence feature generator.
 our sentence encoding layer at the same time. Therefore we ne ed to make sure that whether our LSTM encoder plays an important role in our metho d or the performance is simply influenced by optimizing the logistic regression c lassifier. Sentences in our dataset have rather long length thus RNN cannot generate a go od sentence representa-tion due to gradients vanishing problem. Therefore We use RN N as a control experiment to check whether our LSTM implementation can learn a good sen tence representation vector. 4.2 Measures  X  Precision -The number of definitional sentences correctly labeled by o ur model  X  Recall -The number of definitional sentences correctly labeled by o ur model di- X  F 1 -measure -Calculated by 2 P R P + R with precision(P) and recall(R). We calculate the above three measures to judge the performan ce of our method. Exper-iments are performed with 10-fold cross validation. Datase t is separated into 10 parts and we use one part as testing set and nine parts as training se t in each fold. In this section, we will evaluate the overall performance of our method in comparison with other systems. Additionally, we will show how frequent set influences the perfor-mance and explain the reason. 5.1 Overall performance English dataset. Results of other systems are obtained from aforementioned literatures [2, 3, 5, 7, 6].
 which indicates that our sentence feature generation step d oes play an important role and LSTM can preferably encode a sentence according to its st ructure.
 In our LSTM implementation, we make near 6% progress in the F 1 measure, which proves that our method have a good generating ability in extr acting sentence features for definition extraction. Although precision of our method is lower than WCL and DefMiner, we make a significant improvement in recall perfor mance. We contribute the improvement to the following reasons:  X  Compared with pattern based method, our method does not use a ny manual specific  X  In our method, We use dense feature vectors to encode tokens ( i.e. words or POS  X  We do not manually pick sentence structure features but let o ur LSTM encoder au-in recall and 85.7% in F1 score. Compared with English dataset, sentences in Chi nese dataset have more complicated definitional structures. Con sidering the possible error caused in word segmentation and the structure complexity, w e think it is a satisfying result, which shows that our method X  X  effectiveness in deal ing with different languages. 5.2 Result with different frequent words sets of our system, we perform another experiment. In table 2, we p resent the performance with different top-N frequent words sets. Here the frequenc y count refers to the count of frequency of all the words used in different datasets.
 the reason that it replace all the words with their POS tags. T he treatment will cause information loss since original forms of some words (e.g. is , a, etc.) are more important than their POS tags in definition extraction.
 with more words which are often essential parts of star patte rns in previous approaches. The result shows that frequent words are critical in determi ning a sentence is definitional or not.
 as frequent words. Even with Top-8,000 frequent words set wh ere POS tags are hardly used, our method still obtains a satisfying performance. Co mpared with Chinese, POS tag feature does not make significant effect on definitional e xtraction in English. We believe the main reason lies on the language itself. English definitional sentences have rather common structures which are usually consisted of fre quent words. In our sen-tence feature generation step, our LSTM encoder gradually l earns to reduce the impact of infrequent words since they are rarely involved in definit ional structures. Therefore it does not matter whether we use words X  original forms or the ir POS tags. quent words set becomes larger than 500, which indicates tha t POS tagging replacing step becomes important when dealing with Chinese. The reaso n lies on the fact that Chinese sentences do not have common frequent words based pa tterns which can be used to classify definitional sentences. POS tags of infrequ ent words will participate in classification so replacing these words with their POS tag s can help to improve the performance of our method.
 words set is the best set for our method. Although with this wo rds set, our method cannot reach its best in English dataset, its F 1 score (90.9 %) is actually very close to the highest score (91.2%). A method for the task of definition extraction based on LSTM Re current Neural Net-work has been described in this paper. In order to directly le arn from the whole raw sentence, we propose a new way to generate sentence represen tation.
 sentences, we use a LSTM generator to capture definitional st ructures in a sentence. Next, the dataset of our experiment has been presented to the reader, followed by our detail experiment settings and our results.
 that our method can learn certain structures from sentences well and is competent for definition extraction task. Our method can reduce the work of feature engineering for supervised learning based definition extraction by automat ically learning structure fea-tures from sentence sequence. Besides our method is also pro ved to be effective in multiple languages.
 This work is supported by China National High-Tech Project ( 863) under grant No. 2015AA015401, and Tsinghua University Initiative Scienti fic Research Program (No. 20131089190). Beijing Key Lab of Networked Multimedia also supports our research work.

