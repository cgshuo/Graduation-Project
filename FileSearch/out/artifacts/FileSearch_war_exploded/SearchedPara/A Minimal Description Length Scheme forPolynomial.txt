 Regression models are used to predict the value of a dependent numeric variable from the values of independent (predictor) variables. Commonly used regression methods include linear regression and r egression trees [1]. While linear regres-sion tries to find a global model of the data (a linear equation), regression tree induction finds piecewise models that partition the data space into a number of sub-spaces and induce use a constant or a linear model in each of them. While linear models tend to be oversimplistic, regression trees can sometimes overfit the data. In this paper, we address the task of polynomial regression, i.e., the task of inducing polynomial equations that can be used to predict the value of a numeric variable. Polynomials can also overfit the data. Namely, it is well known that a data set of n points can be perfectly interpolated (and often overfitted) with a polynomial of an ( n  X  1)-th degree.

In order to address the problem of overfitting, different approaches to model selection have been proposed in the literature [3] (pages 193-222). Each ap-proach tries to find an optimal trade-off between the complexity of the induced model and its predictive error and thus avoid overfitting. The minimal descrip-tion length (MDL) principle is one such approach. Following the MDL principle, the quality of a model is estimated by combining the model complexity and the predictive error that the model makes on the training data. The complexity of the model and the error are measured in terms of the number of bits necessary for encoding them. Therefore, MDL bas ed measures of model quality heavily depend on the encoding scheme chosen. Different encoding schemes have been proposed for linear equations and regression trees [7], but to our knowledge no encoding scheme has been proposed for multivariate polynomials.

The aim of this paper is to identify an appropriate MDL scheme for polynomial regression. We consider two MDL schemes. The first one is an ad-hoc scheme proposed in [8]. The second is a novel scheme proposed by this paper and is based on the refined MDL principle [2]. The MDL schemes are implemented andusedinthecontextof Ciper [8], a machine learning system for finding polynomial equations from numeric data. Ciper algorithm performs a heuristic beam search through the space of equat ions, proceeding from simple to more complex equations by using a refinement operator that at each step increases the equation complexity by one. In this paper we also introduce a new refinement operator in Ciper which can increase the complexity of the equation by more then one, combined with a simplification step.

We perform an empirical evaluation on several standard regression datasets from the UCI repository [4]. Within Ciper , we compare the old and the new refinement operator, as well as the two MDL schemes to each other and to linear regression, regression trees and model trees.

The paper is organized as follows. In Section 2, we introduce the task of poly-nomial regression and outline Ciper , a method for inducing polynomials based on heuristic search through the space of candidate polynomials. This sections also includes description of the refinement operator used in Ciper and our pro-posal for a new refinement operator. S ection 3 presents the two MDL schemes compared in the paper. Sect ion 4 presents and discusses the results of the em-pirical evaluation of the MDL schemes. Finally, Section 5 concludes the paper, discusses related work, and proposes directions for further research. The task of polynomial regression is defined as follows: given numeric data, induce a polynomial equation that can predict the value of a target variable. A polynomial over variables x 1 ,x 2 , ...x n canbewrittenintheform: where T i = n j =1 x j a i,j , C i ,i =1 ..m and C 0 are constants, and C i =0.Wesay T i is a term or monomial in P . The length of P is Len ( P )=
An example polynomial equation is P =1 . 2 x 2 y +3 . 5 xy 3 +5 xz +2. This equation has size 3, degree 4 and length 9. 2.1 Ciper Ciper [8] (Constrained Induction of Polynomial Equations for Regression) is a machine learning algorithm for finding polynomial equations. It uses beam search to heuristically search through the space of possible equations for ones that fit the data best.

The top-level outline of the Ciper algorithm is shown in Table 1. First, the beam is initialized either with the simplest polynomial equation P = C ,orwitha user specified minimal polynomial. In every search iteration, a set of polynomials is generated from the polynomials in the beam by using a refinement operator. The coefficients before the terms in a polynomial are fitted by using the method of least squares. For each of the generated polynomials, the value of the minimal description length (MDL) heuristics is calculated. At the end of the iteration, the equations with smallest MDL values are retained in the beam. The evaluation stops when the refinement operator can not generate new equations or when the content of the beam is unchanged in the last iteration. Such a situation occurs when every polynomial generated in the l ast iteration has a worse MDL estimate than the polynomials already in the beam.

We have introduced some optimizations for fitting the coefficients of the gen-erated polynomial stru cture.The data is rep resented as a matrix M ,wherethe number of rows is the number of instances, and the number of columns is the number of terms plus one (the first column is filled with ones). The least squares estimate for the coefficients C i of the equation is where y is the vector of values we are trying to predict. In this equation the multiplication is computationally expensive because of the large number of rows. Let T 1 and T 2 be terms in equation A , T 3 and T 4 terms in equation B , such that T and M T B  X  M B are equal. We store all generat ed elements from the matrices M T  X  M . We reuse them for calculating the mat rices of the subsequently generated polynomials. This optimization considerably lowers the computational cost at the expense of some memory. 2.2 The Ciper Refinement Operator A refinement operator is a function that takes as input an equations structure and generates a new equati on structure by modifying the old one. The original Ciper refinement operator increases the length of an equation by one, either by adding a first degree term or by multiplying an existing term with a variable (Figure 1). Starting with the simplest equation, and iteratively applying this refinement operator, all polynomial equations can be generated.

Givenanexpression x + y , we can refine it in two ways. First, we can include a new linear term yielding x + y + z . Second we can replace an existing term in the expression (e.g. x ) by multiplying it with a variable (e.g. z ), yelding a new expression (e.g. xz + y ). 2.3 The New Refinement Operator Adding a term to a linear (in the paramet ers) equation always decreases its error (at least on training data). Howev er, replacing a term w ith a more complex version thereof (multiplied by a variable) doesn X  X  necessarily decrease the error of the equation. If we add z to x + y , yielding x + y + z , we will reduce the error of the equation. However, if we replace x with xz , yielding xz + y , xz need not be strongly correlated with x and the replacement might actually increase the error of the equation.

This has motivated us to modify the refinement operator in Ciper . Besides the two types of refinement considered in the originla version of Ciper , we introduce a third one. We take a term in the equati on, make a copy thereof, multiply the copy with a new variable and add the product back to the equation. For example, x + y can be refined to x + y + xz by the new operator.

The old refinement operator always increases the complexity of an equation by one. The new refinement operator can increase the complexity of an equation considerably. Because of this, we intr oduce an extra simplification step in Ciper . For every equation added to the beam, we try removing each of its terms: if this yields an equation with better heuristic value, we add the newly formed equation to the beam.
 We will refer to Ciper using the new refinement operator as Ciper-R .
We came to the part of identifying the best equations in the beam. For this we need an objective measure that w ill combine complexity and the error. We will give two alternatives for measuring the complexity of the model. The first is an ad-hoc solution, used in the first version of the Ciper algorithm [8]. The second is based on theoretical results in MDL theory. We present the encoding and the associated complexity measure. 3.1 Ad-Hoc MDL Heuristic The original Ciper implementation used an ad-hoc MDL heuristics, defined as fallows where P is the polynomial equation being evaluated, len ( P ) is its length, MSE ( P ) is its mean squared error, and m is the number of training examples.
This evaluation function is based on the Akaike and Bayesian information criteria for regression model selectio n [3]. The second term of the ad-hoc MDL heuristic function measures the degree of fit of a given equation and the first term introduces a penalty for the complexity of the equation. With this penalty, the MDL heuristic function introduces a preference toward simpler equations. 3.2 Improved MDL Heuristic Following the minimal description length (MDL) principle, among a number of candidate models, we select the one that represents a good trade-off be-tween the model X  X  predictive error and its complexity. The MDL principle com-bines two ideas (or assumptions) about the relation between learning and data compression:  X  regularities in the data can be used to compress the data, i.e., the more  X  the more we are able to compress the data, the more we have learned about
Thus, the complexity of a model can be estimated as its ability to compress data: the larger the compression, the smaller the complexity of the obtained model. More specifically, an MDL estimate of model quality is composed of two components: where the first component L ( H ) corresponds to the length of the encoding of the model (hypothesis) H , while the second one L ( D | H ) is the length of the description of the data when encoded using the model H . 3.3 Encoding Polynomial Structure In order to encode the structure of a polynomial, we follow the refined MDL approach [2]. We first partition the space of candidate models into subgroups H c of models with equal complexity c . A particular model H  X  X  c can be then encoded using N = log |H c | ( log stands for the binary logarithm) bits, where |H c | denotes the number of models in the class H c .

In the case of polynomials, we partition the space of candidate polynomial structures in to classes at several levels . At the highest level, we group together the candidate polynomials with the same length l and the same number of terms nomial structures with one linear term, while G (1 , 2) contains polynomial struc-tures with only one term of second degree. Note that m  X  l . At the second level, All polynomials in this subgroup have m terms with degrees a 1  X  a 2  X  ...  X  a m . Note that m i =1 a i = l . Now we have to calculate how many sub-groups G there are in a single G ( m, l ) group and also calculate how many polynomial structures there are in each G ( a 1 ,a 2 ,...,a m ) group.

The number | G ( a 1 ,a 2 ,...,a m ) | can be easily calculated using a procedure roughly depicted in Figure 3. Given the degree of the first term a 1 we have to choose a 1 variables from the set { x 1 ,x 2 ,...x n } , where variables can appear in the selection more than once. Thus, the number of possibilities for the first term equals the number of combinations with repetition where we select a 1 elements from a set of n elements. This number equals n + a 1  X  1 a thesamereasoningforall m terms we obtain the number of possible structures in
G ( a that are equal, we will encounter the same term many times, which means that the above formula over-estimates the number of possible structures. The remedy is to divide the number with the factorial of repetitions observed in the tuple. For example, when dealing with the case G (5 , 5 , 3 , 2 , 2 , 2), we have to divide with 2!3!, since 5 is repeated twice and 2 is re peated three times. Note also that each multiplicative term decreases by 1 for each d egree value repetition (see Figure 3).
Having the number of equation structures in each G group, we now turn to the problem of calculating the number of G groups within each G ( m, l ). The | G ( m, l  X  m ) | . The first additive term corresponds to the cases when the G groups contain linear terms (there is an a i with value 1), while the second corresponds to the cases when all terms in the G groups have a degree at least 2 (all a i &gt; 1). In the first case, when removing the linear term, we obtain polynomials with m  X  1 terms and length l  X  1. In the second case, we can remove one variable from each of the terms, which leads to polynomials with the same number of terms ( m ) and length m  X  l . Figure 4 depicts the relationship between G and G classes of polynomial structures.

Now, having this partitioning and the number of polynomials in each partition, we can decompose the code for each candidate polynomial in four components. First, we have to encode its length l and for this we need log ( l )+2 log ( log ( l )) bits (the second double logarithm term is necessary, since we do not know the magnitude of l in advance). Second, we encode the number of terms m ,forwhich we need log ( l )bits(rememberthat m  X  l ). Third, we can identify a particular Putting these four components together gives us the final formula: for number of bits necessary to encode the polynomial structure. 3.4 Encoding the Linear Regression Model Rissanen provides a formula for calculating the stochastic complexity of a linear regression model generated by using the method of least squares [6]: W =min  X  { ( N  X  k ) log ( X   X  )+ k log (  X  R )+( N  X  k  X  1) log ( 1 where the  X  index goes through all the possible subsets of variables involved in the linear regression, k is the number of elements in  X  , N is the size of the dataset,  X   X  is the maximum likelihood estimation of the model error, and  X  R = 1 n  X  c T ( M T M ) X  c (where X  c =( M T M )  X  1 M T y and M is the matrix of data). The stochastic complexity of the model is then 2 W . Intuitively, this corresponds to the length of the code necessary to en code the errors of the linear regression model ( L ( D | H )) together with the constant parameters of the linear model. The two are closely related and thus the constant parameters are not encoded separately or with the model structure, which is what is usually done in machine learning algorithms when using the MDL principle in an ad-hoc manner. For further details, see [6]. Our work is an extension of Ciper [8]. As described above we have implemented a new refinement operator as Ciper-R . In addition we have implemented the new MDL heuristic in Ciper-R yielding Ciper-MR .
The main goal of the performed experiments is to evaluate the predictive performance of Ciper , Ciper-R and Ciper-MR i.e. evaluate the two differ-ent heuristics and the two refinement op erators described above. We also made a comparison with the standard regression methods, implemented in the data mining suite Weka [10]. The performance of the methods is evaluated on twenty data sets from the UCI Repository [4] and another publicly available collection of regression data sets [9]. These data set s have been widely used in other compar-ative studies. In all the experiments pres ented here, we estimate the predictive performance on unseen examples using 10-fold cross validation. The predictive performance of a model M is measured in terms of relative root mean squared error ( RRMSE ).

In the first phase of the evaluation we do a performance comparison between the original Ciper algorithm (using the old refinement operator) and the Ciper-R algorithm. In this phase we use the ad-hoc heuristic as used in the original implementation of the algorithm [8]. We show that the new refinement opera-tor has better predictive capabilities than the old one. In the next phase we do a performance comparison between the improved MDL heuristics ( Ciper-MR ) and the ad-hoc MDL heuristic ( Ciper-R ), now using the new refinement opera-tor. We show that the improved MDL heuristic performs better than the ad-hoc heuristic. In the last phase, we compare Ciper-MR to standard regression al-gorithms linear regression, reg ression trees, and model trees.
 4.1 Evaluating the New Refinement Operator Table 2 summarises the results of the performance comparison between the old and the new refinement operator. The statistical significance is tested using a paired t-test and a Wilcoxon signed-rank test. If the p -value is smaller than 0 . 05 then we reject the null hypothesis, and con clude that the difference is statistically significant. The + sign in the table is used when the improvements we introduce perform significantly better according to the paired t-test and the  X  sign is used when they perform worse. We can see that Ciper-R performs significantly better than Ciper on nine datasets according to the paired t-test and worse on one dataset. The p -value calculated from the two-tailed Wilkinson signed-rank test is 0 . 011 which means that Ciper-R and Ciper have significantly different performance. The p -value calculated from the left-tailed Wilkinson signed-rank test is 0 . 005 which means that the difference is not negative. We can conclude that Ciper-R performs significantly better than Ciper . 4.2 Evaluating the New MDL Heuristic Table 3 summarises the results of the performance comparison between Ciper-MR and Ciper-R .Wecanseethat Ciper-MR performs significantly better than Ciper-R on eight datasets accordin g to the paired t-test. The p -value calculated from the two-tailed Wilkinson signed-rank test is 0 . 007 which means that Ciper-MR and Ciper-R have significantly different performace. The p -value calculated from the left-tailed Wilkinson signed-rank test is 0 . 003 which means that the difference is not negative. We can conclude that Ciper-MR performs significantly better than Ciper-R . 4.3 Comparison with Standard Regression Algorithms Table 4 gives an overview of the predictive performance of standard regression methods on our datasets. We see that Ciper-MR performs better than linear regression and regression trees on most of the datasets. Also according to the Wilkinson signed-rank test Ciper-MR is significantly better than both of them. Compared to model trees Ciper-MR performs significantly better on three and worse on two datasets. The p -value calculated from the two-tailed Wilkinson signed-rank test between Ciper-MR and model trees is 0 . 952 which means that the performance of the two algorithms is not significantly different. In this work, we focus on improving the Ciper algorithm for polynomial regres-sion. Two key parts in the algorithm are the refinement operator on equation structures and the heuristic evaluation function used to compare equations. The latter takes into account both the error and the complexity of an equation and is based on the MDL principle.

We have proposed a new refinement operator that makes larger steps in the refinement of structures. We have compl emented this by a simplification step in the Ciper algorithm. We have proposed a principled MDL function that replaces the ad-hoc MDL function used in Ciper so far.

We empirically evaluate these propose d changes by applying the different variants of Ciper on a number of standard regression datasets. The results suggest that Ciper with the new refinement operator performs better than with the old one. Also, using the principled MDL heuristic function is advantageous to using the ad-hoc MDL heuristic. The new Ciper outperforms linear regression and regression trees and is comparable to model trees.

A number of directions for further work have been identified. We focus here on the question of producing piece-wise polynomial models, by combining tree-based/rule-based models with polynomial equations. Such models may have bet-ter predictive capabilities than both equations and tree-based/rule-based models. One way of doing this is clustering the values of the attributes, followed by gen-erating binary attributes for each clust er. We intend to investigate this in the near future.

