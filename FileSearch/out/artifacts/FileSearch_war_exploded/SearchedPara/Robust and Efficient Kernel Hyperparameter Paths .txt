 Friedrich-Schiller-Universit  X  at Jena, Germany Parameterized optimization problems of the form are abundant in machine learning. Here t  X  R is a param-eter, f t : R d  X  R is some function depending on t , and F at parameter value t .
 The solution path problem is to compute an optimal or ap-proximate solution x t  X  F t of the parameterized problem along some parameter interval I  X  R . From the solution path a good parameter value t and a corresponding solu-tion x t can be chosen by some optimization criterion that should not be confused with the objective of the parameter-ized optimization problem. In a machine learning context the parameter t is typically optimized using some measure for the generalization error on test data while x t is com-puted from training data.
 An important example of the abstract parameterized opti-mization problem is where l : R d  X  R is a loss function and r : R d  X  R is some regularizer, e.g. Euclidean regularization r ( x ) = that encourages sparse solutions. This case, namely effi-ciently computing robust regularization paths, has received considerable attention and can be considered solved for the relevant problems in machine learning even when optimiz-ing over positive-semidefinite matrices. Another important example that has received less attention is when f t is given as a function f : R d  X  R that is parameterized by a posi-tive kernel function that itself is parameterized by t  X  R on some set  X  . Here we study a fairly general class of parameterized con-vex optimization problems that contains most of the regu-larization path and kernel hyperparameter path problems. We consider problems of the form where f t : R d  X  R is convex and c t : R d  X  R n is convex in every component for all values of t . We assume that f t ( x ) and Lipschitz continuous in t at any feasible point x , but we do not require convexity (or concavity) of these functions in The feasible region at t is given as with componentwise inequalities. Our goal in this paper is to devise a robust and efficient algorithm for comput-ing an  X  -approximate solution path for Problem (1), i.e., in contrast to the exact solution path problem we only aim for an  X  -approximate solution along the parameter interval instead of an exact solution. Turning to approximate so-lutions leads to much more efficient and robust algorithms than the known exact solution paths algorithms.
 Related work and contributions. Regularized optimiza-tion methods are in widespread use throughout machine learning. Thus, computing regularization paths has re-ceived considerable attention over the last years. The work on regularization paths started with the seminal work by (Efron et al., 2004) who observed that the regularization path of the LASSO is piecewise linear. In (Rosset &amp; Zhu, 2007) a fairly general theory of piecewise linear regular-ization paths has been developed and exact path following algorithm have been devised. Important special cases are support vector machines whose regularization paths have been studied in (Zhu et al., 2003; Hastie et al., 2004), sup-port vector regression (Wang et al., 2006b), where also the loss-sensitivity parameter can be tracked, and the general-ized LASSO (Tibshirani &amp; Taylor, 2011). From the begin-ning it was known, see for example (Allgower &amp; Georg, 1993; Hastie et al., 2004; Bach et al., 2004), that exact reg-ularization path following algorithms suffer from numeri-cal instabilities as they repeatedly need to invert a matrix whose condition number can be poor, especially when us-ing kernels. It also turned out (G  X  artner et al., 2012; Mairal &amp; Yu, 2012) that the combinatorial (and thus also com-putational) complexity of exact regularization paths can be exponential in the number of data points. This trig-gered the interest in approximate path algorithms (Rosset, 2004; Friedman et al., 2007). By now numerically robust, approximate regularization path following algorithms are known for many problems including support vector ma-chines (Giesen et al., 2012b;c), the LASSO (Mairal &amp; Yu, 2012), and regularized matrix factorization and completion problems (Giesen et al., 2012a;c). These algorithms com-pute a piecewise constant approximation with O 1 / segments, where  X  &gt; 0 is the guaranteed approximation error. Notably, the complexity is independent of the num-ber of data points and even matching lower bounds are known (Giesen et al., 2012c).
 The situation is still different for kernel hyperparame-ter path tracking. Exact kernel path tracking algorithms are known for kernelized support vector machines (Wang et al., 2007b), the kernelized LASSO (Wang et al., 2007a), and Laplacian-regularized semi-supervised clas-sification (Wang et al., 2006a; 2012). The exact kernel path tracking algorithms are even more prone to numeri-cal problems than regularization path tracking algorithms since they repeatedly need to invert a kernel matrix whose condition number tends to be poor (large), see Figure 1. Here we address this problem by devising a numerically stable approximate solution path algorithm for parameter-ized problems of the Form (1). The algorithm can be used to compute approximate regularization paths as well as ap-proximate kernel hyperparameter paths. We prove that the resulting path complexity is in O (1 / X  ) , where  X  &gt; 0 again the guaranteed approximation error. This complexity might look disappointing considering that  X  -approximation paths with complexity in O 1 / regularization path problems. Still, this is best possible. A matching lower bound of  X (1 / X  ) has been first proved in (Giesen et al., 2010) for the class of Problems (1). This problem class includes problems, for instance kernel hyper-parameter path problems, whose exact solution path is not piecewise linear as it is the case for the regularization path problems that exhibit a better approximation path complex-ity. We observed the  X (1 / X  ) complexity bound also in ex-periments on various data sets for support vector machines and robust kernel regression that have been kernelized with a Gaussian kernel. Since our approximate solution path algorithm is based on duality we review here some basic facts of duality theory for parameterized optimization problems. We also intro-duce our notation and define approximate solution paths for parameterized optimization problems and bound their complexity.
 Lagrangian duality. The Lagrangian of the parameter-ized convex optimization problem (1) is the following func-tion From the Lagrangian we can derive a dual optimization problem as We call the dual objective function . From the Lagrangian we can also derive an alternative expression for the primal objec-tive function, namely Note that since  X  T c t ( x )  X  0 and thus max  X   X  0  X  T c t ( x ) = 0 can always be achieved by setting  X  = 0) for all x  X  F t . Weak and strong duality. At a fixed parameter value t we have the following well known weak duality property for any x  X  R d and any  X   X  R  X  0 . To see this note that for all x  X  R d and all  X   X  R n  X  0 . Thus, for all x  X  R d , and finally which implies In particular, we have  X   X  t (  X   X  t )  X   X  ( x  X  t ) , where are the dual and primal optimal solutions, respectively. We say that strong duality holds if  X   X  t (  X   X  t ) =  X  t ( x  X  Duality gap and approximate solution. At parameter value t we call the duality gap at ( x, X  )  X  F t  X  R n  X  0 . For  X  &gt; 0 call x  X  F t an  X  -approximate solution of the parameterized optimization problem (1) at parameter value t , if Assume that g t ( x, X  )  X   X  , then we have f Approximate solution path. Let [ t min ,t max ]  X  R be a compact parameter interval and  X  &gt; 0 . We call a function an  X  -approximate solution path of the parameterized opti-mization problem (1), if for all t  X  [ t min ,t max ] 1. x t  X  F t and 2. f t ( x t )  X  f t ( x  X  t )  X   X  .
 We say that the path x : [ t min ,t max ]  X  R d has complexity k  X  N , if x can be computed from k primal-dual optimal pairs ( x  X  t We can bound the complexity of approximate solution paths as follows.
 Theorem 1. Given a parameterized convex optimization problem (1), a parameter interval [ t min ,t max ] , and  X  &gt; 0 If the following conditions 1. the feasible region F t has a nonempty interior, and 2. the problem has an optimal solution x  X  t  X  F t , and 3.  X   X  t (  X  ) is Lipschitz continuous in t for any  X   X  0 4. there exists a function are satisfied for all t  X  [ t min ,t max ] , then there exists an  X  -approximate solution path for the interval [ t min ,t whose complexity is in O (1 / X  ) . The constants in the big-O notation depend only on the functions f t and c t and on the interval [ t min ,t max ] .
 Proof. Let Note that r &lt;  X  since x  X  t exists for all t in the compact in-terval [ t min ,t max ] . Thus we can impose the additional con-straint k x k 2  X  r in the parameterized convex optimization problem (1) without changing its solutions. Since the func-tion f t is convex on R d it is also Lipschitz continuous with respect to it argument x for some constant L 0 &gt; 0 on the set { x  X  F t |k x k 2  X  r } .
 By our assumptions both f t ( x ) and  X   X  t (  X  ) are Lipschitz continuous with respect to t for any feasible x and  X   X  0 respectively, i.e., there exists a constant M &gt; 0 such that and for all t, X   X  [ t min ,t max ] .
 Note that  X  x t (  X  ) is a feasible solution for the primal prob-lem at  X  and  X   X  t is a feasible solution for the dual prob-lem at  X  , because the feasible region of the dual problem does not depend on the parameter t . By Slater X  X  Condition, see for example (Boyd &amp; Vandenberghe, 2004), strong du-ality holds since F t has a nonempty interior, and f the components of c t are convex functions, i.e., we have g Combining these properties we obtain a bound for the fol-lowing duality gap g where the first inequality follows from the Lipschitz con-tinuity of f  X  with respect to x , the second inequality fol-lows from the Lipschitz continuity of the function  X  x t respect to  X  , and the third inequality follows from the Lip-schitz continuity of f t ( x ) and  X   X  t (  X  ) with respect to any feasible x and  X   X  0 , respectively. Hence, a primal-dual solution pair (  X  x t (  X  ) , X   X  t ) is a feasible primal-dual approximate solution pair for all  X  with It follows that there exists an  X  -approximate solution path whose complexity can be bounded by The problem dependent constant in Theorem 1, might look huge at a first glance, but note that for an in-terval with t min = 2  X  10 and t max = 2 10 it turns out that the value of this constant is at most 20 on the data sets that we have tried in our experiments for kernelized SVMs, see Section 6.
 Lower bound The parameterized optimization problems in the lower bound construction in (Giesen et al., 2010) sat-isfy the conditions of Theorem 1. This gives us a lower bound in  X (1 / X  ) on the path complexity for the class of Problems (1) and shows that the complexity analysis in Theorem 1 is asymptotically tight. In the following we assume that strong duality holds for all parameter values in the interval [ t min ,t max ]  X  R simple idea for computing an  X  -approximate solution path makes use of duality and works as follows: 1. Compute the primal-dual pair ( x  X  t , X   X  t ) for t = t 2. Determine  X  x t : [ t min ,t max ]  X  R d and t 0  X  3. At t 0 compute a new optimal primal-dual pair Let t min = t 1 ,...,t k be the points in [ t min ,t max ) an optimal primal-dual pair is computed (Step 3 of the al-gorithm). The path is an  X  -approximate solution path of complexity k . Here we specialize Theorem 1 and the approximate path al-gorithm to the standard hinge loss support vector machine (SVM) that has been kernelized with a Gaussian kernel ma-trix with bandwidth parameter t &gt; 0 . That is, we need to make sure that this SVM meets the necessary conditions of The-orem 1. The primal SVM problem is given as where c is a regularization parameter, y  X  R d is a label vector with entries in { X  1 , +1 } , and is the element-wise multiplication.
 The dual SVM problem can be written as where It is straightforward to see that Assumptions 1.-3. of The-orem 1 are satisfied for the SVM problem. It remains to ensure that also Assumption 4. holds true.
 To ensure Assumption 4. we need to find a function as re-quired in this assumption. Here we discuss two functions that satisfy the requirements. In the first function the bias is fixed and in the second function it depends on the band-width parameter t 0 . We call the first case the fixed bias update rule and the second the dynamic bias update rule . 1. Fixed bias updates. For an optimal primal solution 2. Dynamic bias updates. We can also adapt the bias b Asymptotically the complexity of the SVM kernel path is in
O (1 / X  ) in both cases since Theorem 1 applies. In prac-tice, however, it makes a difference which of the two update rules is used, see Section 6. Although the asymptotic be-havior is the same, the constants are much smaller for the dynamic bias update rule than for the fixed bias update rule. Robust regression is an alternative to least squares regres-sion that uses an ` 1 -loss function instead of an ` 2 -loss func-tion to become more robust against outliers. Robust kernel regression is an extension of robust regression that accom-modates the use of kernels for nonlinear regression. Here we even consider sparse robust kernel regression by adding an additional ` 1 -regularizer that favors sparse solutions. The sparse robust kernel regression problem is given as the following minimization problem where y  X  R d is the output vector, and K t  X  R d  X  d the kernel matrix that is determined by the parameterized Gaussian kernel function k t and d data points x 1 ,...,x The regression function is then given as The dual problem of the sparse robust kernel regression problem is the following maximization problem To apply Theorem 1 we interchange the role of the primal and the dual problem, i.e., we consider as the primal problem, whose dual is given as Obviously, all four conditions of Theorem 1 are met, since the primal problem has a nonempty interior, the problem is bounded (and hence a primal optimum exists), the dual function is Lipschitz continuous with respect to t , and for any optimal dual solution u  X  t we can find a feasible solution  X  u (  X  ) by projecting u  X  ter value  X  . Since K t is differentiable in t (for a Gaussian kernel matrix) the projection itself is Lipschitz continuous. Hence, we can apply Theorem 1 that guarantees the exis-tence of an  X  -approximate hyperparameter solution path of complexity O (1 / X  ) . To validate our theoretical finding, in particular the depen-dence of the path complexity on the guaranteed approxima-tion error  X  , we have conducted experiments for the kernel-ized SVM and also for the robust kernel regression. 6.1. Kernelized SVM We have implemented the approximate path tracking al-gorithm for the kernelized SVM. LIBSVM Version 3.17, whose implementation is described in (Fan et al., 2005), has been used to compute primal-dual optimal pairs. LIB-SVM actually solves the dual problem. If  X   X  t is the optimal dual solution at parameter value t , then the optimal primal solution can be reconstructed by setting w  X  t = y  X   X  t and to the median of the expressions It remains to describe the implementation of the second step of the algorithm. Since we can compute the value of the primal objective function for every value of t 0 , see Sec-tion 4, we can also compute the duality gap The largest t 0 &gt; t for which the duality gap g t 0 is still at most  X  can be simply found by binary search.
 As test environment we used MATLAB, and all data sets that have been used in our experiments were retrieved from the LIBSVM Website, see (Lin). The data sets and results are summarized in Table 1. The regularization parameter c was set to 0 . 1 in all the experiments.
 Dependence on  X  Our theoretical finding that O (1 / X  ) optimal primal-dual pairs are sufficient to approximate the whole kernel hyperparameter solution path was confirmed in our experiments. Figure 3 indicates that the number of optimal primal-dual pairs computed by the algorithm de-pends linearly on 1 / X  .
 Choice of bias update rule The experiments also show that the choice of the bias update rule has a significant in-fluence on the approximation path complexity. As expected the dynamic bias update rule leads to a lower path com-plexity than the fixed bias update rule, since it improves the value of the primal objective function over the fixed bias update rule and thus needs fewer updates to maintain the approximation guarantee. Figure 2 directly compares the two update rules and shows that indeed the dynamic bias update rule performs better. The main difference is that the upper bound, i.e., the approximation of the primal optimum, is much better for the dynamic bias update rule. Figure 2 also shows that our path tracking algorithm, in contrast to a simple grid search, adapts well to regions of interest (especially for the dynamic bias update rule), i.e., the solution is only updated frequently in these regions. 6.2. Robust Kernel Regression We have also implemented the approximate path tracking algorithm for robust kernel regression. The optimal primal-dual pairs at a fixed parameter value t have been computed using the SeDuMi solver (Sturm, 1999). The implementa-tion of the second step of the algorithm is analogous to the implementation for the kernelized SVM since also here we can compute the value of the primal objective function for every value of t 0 and thus the duality gap g  X   X  u t (  X  ) , X   X  The largest t 0 &gt; t for which the duality gap g t 0 is still at most  X  can be found by binary search.
 As test environment we used again MATLAB and follow-ing the example of (Wang et al., 2007a) we generated a synthetic data set by randomly sampling 100 points from the following target function in the interval [  X  4 , 4] and by adding Gaussian noise. Ad-ditionally, we also added 10% outliers to the data set. The data set, i.e., the sample points, and the target function are shown in Figure 4 (on the left). In this figure we also show that, as expected, robust regression performs better in the presence of outliers than for instance the LASSO (Tibshi-rani, 1994). The regularization parameter  X  was set to 0 . 1 in the experiments.
 It is well known that the choice of the bandwidth parameter in the Gaussian kernel has a significant influence on the per-formance of kernel regression methods. This can be seen also in Figure 4 (on the right), where we show the mean absolute error (MAE) on a set of test data points tracked along the kernel hyperparameter path (i.e., the bandwidth path). Note that the test error path in Figure 4 (on the right) has many local minima which is typical for this type of problems. We have presented an algorithmic framework for tracking approximate solutions for a large class of parameterized optimization problems. In particular, the framework allows to track kernel hyperparameter paths and even has the op-timal path complexity of O (1 / X  ) in terms of the prescribed approximation error  X  for this type of problems, for which no efficient approximation schemes had been devised be-fore. The framework also allows to compute approximate regularization paths, but it is not optimal for this easier class of problems (whose exact solution path is piecewise linear which is not true for hyperparameter paths). We have instantiated the algorithmic framework for com-puting approximate kernel hyperparameter paths for SVMs and the robust kernel regression problem, both with Gaus-sian kernel. Our experiments for these applications, in con-trast to exact path algorithms, did not suffer from numerical problems, and confirmed our optimal theoretical complex-ity bounds.
 This work has been supported by a grant of the Deutsche Forschungsgemeinschaft (GI-711/3-2).
 Allgower, Eugene and Georg, Kurt. Continuation and path following. Acta Numerica , 2:1 X 64, 1993.
 Bach, Francis R., Thibaux, Romain, and Jordan, Michael I.
Computing regularization paths for learning multiple kernels. In Advances in Neural Information Processing Systems (NIPS) , 2004.
 Boyd, Stephen and Vandenberghe, Lieven. Convex Opti-mization . Cambridge University Press, 2004.
 Efron, Bradley, Hastie, Trevor, Johnstone, Iain, and Tib-shirani, Robert. Least angle regression. The Annals of Statistics , 32(2):407 X 499, 2004.
 Fan, Rong-En, Chen, Pai-Hsuen, and Lin, Chih-Jen. Work-ing Set Selection Using Second Order Information for Training Support Vector Machines. Journal of Machine Learning Research , 6:1889 X 1918, 2005.
 Friedman, Jerome, Hastie, Trevor, H  X  ofling, Holger, and Tibshirani, Robert. Pathwise Coordinate Optimization. The Annals of Applied Statistics , 1(2):302 X 332, 2007.
Exponential Lower Bound on the Complexity of Reg-ularization Paths. Journal of Computational Geometry (JoCG) , 3(1):168 X 195, 2012.
 Giesen, Joachim, Jaggi, Martin, and Laue, S  X  oren. Approx-imating Parameterized Convex Optimization Problems.
In European Symposium on Algorithms (ESA) , pp. 524 X  535, 2010.
 Giesen, Joachim, Jaggi, Martin, and Laue, S  X  oren. Reg-ularization Paths with Guarantees for Convex Semidefi-nite Optimization. In International Conference on Artifi-cial Intelligence and Statistics (AISTATS) , pp. 432 X 439, 2012a.
 Giesen, Joachim, Jaggi, Martin, and Laue, S  X  oren. Approx-imating parameterized convex optimization problems. ACM Transactions on Algorithms , 9(1):10, 2012b.

Swiercy, Sascha. Approximating Concavely Parameter-ized Optimization Problems. In Advances in Neural In-formation Processing Systems (NIPS) , pp. 2114 X 2122, 2012c.
 Hastie, Trevor, Rosset, Saharon, Tibshirani, Robert, and
Zhu, Ji. The Entire Regularization Path for the Sup-port Vector Machine. In Advances in Neural Information Processing Systems (NIPS) , 2004.
 Lin, Chih-Jen. LIBSVM Tools. Data sets avail-able at www.csie.ntu.edu.tw/  X  cjlin/ libsvmtools/datasets/ .
 Mairal, Julien and Yu, Bin. Complexity analysis of the lasso regularization path. In International Conference on Machine Learning (ICML) , 2012.
 Rosset, Saharon. Following curved regularized optimiza-tion solution paths. In Advances in Neural Information Processing Systems (NIPS) , 2004.
 Rosset, Saharon and Zhu, Ji. Piecewise linear regularized solution paths. The Annals of Statistics , 35(3):1012 X  1030, 2007.
 Sturm, Jos F. Using SeDuMi 1.02, a MATLAB toolbox for optimization over symmetric cones. Optimization Meth-ods and Software , 11-12:625 X 653, 1999.
 Tibshirani, Robert. Regression Shrinkage and Selection Via the Lasso. Journal of the Royal Statistical Society, Series B , 58:267 X 288, 1994.
 Tibshirani, Ryan and Taylor, Jonathan. The solution path of the generalized lasso. The Annals of Statistics , 39(3): 1335 X 1371, 2011.
 Wang, Gang, Chen, Tao, Yeung, Dit-Yan, and Lochovsky,
Frederick H. Solution path for semi-supervised classi-fication with manifold regularization. In IEEE Interna-tional Conference on Data Mining (ICDM) , pp. 1124 X  1129, 2006a.
 Wang, Gang, Yeung, Dit-Yan, and Lochovsky, Frederick H.
Two-dimensional solution path for support vector regres-sion. In International Conference on Machine Learning (ICML) , pp. 993 X 1000, 2006b.
 Wang, Gang, Yeung, Dit-Yan, and Lochovsky, Frederick H. The Kernel Path in Kernelized LASSO. In International Conference on Artificial Intelligence and Statistics (AIS-TATS) , pp. 580 X 587, 2007a.
 Wang, Gang, Yeung, Dit-Yan, and Lochovsky, Frederick H. A kernel path algorithm for support vector machines. In
International Conference on Machine Learning (ICML) , pp. 951 X 958, 2007b.
 Wang, Gang, Wang, Fei, Chen, Tao, Yeung, Dit-Yan, and Lochovsky, Frederick H. Solution Path for Manifold
Regularized Semisupervised Classification. IEEE Trans-actions on Systems, Man, and Cybernetics, Part B , 42(2): 308 X 319, 2012.
 Zhu, Ji, Rosset, Saharon, Hastie, Trevor, and Tibshirani,
Robert. 1-norm Support Vector Machines. In Advances
