 to data analysis and machine learning. The unifying premise behind these methods is the assumption that many types of high-dimensional natural data lie on or near a l ow-manifold learning algorithms. Some recent manifold algorithms include Isomap [14] and Locally Linear Embedding (LLE) [13].
 converge to eigenfunction of the Laplace-Beltrami operator on the underlying manifo ld. We note that in mathematics the manifold Laplacian is a classical object of di fferential of heat diffusions, which is the point of view explored by Coifman and colleagues at Yale University in a series of recent papers on data analysis (e.g., [6]). Hessia n Eigenmaps approach which uses eigenfunctions of the Hessian operator for data representatio n was proposed by Donoho and Grimes in [7]. Laplacian is the trace of the Hessian. F inally, as approximation to the squared Laplacian.
 In the manifold learning setting, the underlying manifold is usually unknown. Therefo re functional maps from the manifold need to be estimated using point cloud data. The com-of the Laplacian Eigenmaps algorithm. We note that in order to prove convergence of a spectral method, one needs to demonstrate convergence of the empirical eigenvalues and manifold learning method. 1.1 Prior and Related Work for any spectral method in the manifold setting.
 an arbitrary probability distribution on the manifold. We also note [4], where a similar Empirical convergence of spectral clustering for a fixed kernel parameter t was analyzed manifold. Recently [8] provided deeper probabilistic analysis in that case. geometry (see, e.g., [5]) the exact nature of that parallel is usually not made preci se. The main result of this paper is to show convergence of eigenvectors of graph Lapla cian the data is sampled from a uniform probability distribution on an embedded manifo ld. In what follows we will assume that the manifold M is a compact infinitely differentiable Riemannian submanifold of R N without boundary. Recall now that the Laplace-Beltrami operator  X  on M is a differential operator  X  : C 2  X  L 2 defined as where  X  f is the gradient vector field and div denotes divergence. eigenfunction by e i . See [12] for a thorough introduction to the subject. We define the operator L t : L 2 ( M )  X  L 2 ( M ) as follows ( is the standard measure): If x i are the data points, the corresponding empirical version is given by corresponding graph Laplacian. We will assume that x i are randomly i.i.d. sampled from M according to the uniform distribution.
 Our main theorem shows that that there is a way to choose a sequence t n , such that the Beltrami operator  X  in probability.
 be the corresponding eigenvalue and eigenfunction of  X  respectively. Then there exists a sequence t n  X  0 , such that where the limits are in probability. The proof of the main theorem consists of two main parts. One is spectral conver gence of two types of convergence are then put together to obtain the main Theorem 2.1. Part 1. The more difficult part of the proof is to show convergence of eigenvalues and eigenfunctions of the functional approximation L t to those of  X  as t  X  0. To demonstrate convergence we will take a different functional approximation 1  X  H t heat operator. While 1  X  H t for each fixed i the i th eigenvalue of 1  X  H t in the sense that for any function f we have k R t f k 2 convergence and lead to the following functions of  X  and L t respectively. Then following theorem is obtained: and L t respectively. Let e t n,i and e t i be the corresponding eigenfunctions. Then assuming that  X  t i  X  1 2 t . The convergence is almost sure.
 Observe that this implies convergence for any fixed i as soon as t is sufficiently small. Symbolically these two theorems can be represented by top line of the following diagram: argument shows that a sequence t n can be chosen to guarantee convergence as in the final Theorem 2.1 and provides the bottom arrow. 4.1 Main Objects and the Outline of the Proof Let M be a compact smooth smoothly embedded k -dimensional manifold in R N with the induced Riemannian structure and the corresponding induced measure .
 As above, we define the operator L t : L 2 ( M )  X  L 2 ( M ) as follows: As shown in previous work, this operator serves as a functional approximatio n to the to the eigenvalues and eigenfunctions, which turn out to need some careful estimates. to the heat equation. Recall that the heat equation on the manifold M is given by distribution. We observe that from the definition of the derivative as that the heat operator H t can be written as H t = e  X  t  X  . We immediately see that  X  = eigenfunctions of the Laplace operator. The i th eigenvalue of 1  X  H t where  X  i as usual is the i th eigenvalue of  X .
 poses no difficulty:  X . This pointwise operator convergence is discussed in [10, 3, 1].
 convergence . If A n is a sequence of operators, we say that A n  X  A uniformly in L 2 if spectral properties.
 It turns out that this type of convergence does not hold for functional approximation L t converges to  X  for each fixed function f , even this convergence is not uniform. Indeed, eigenfunction e i of  X , s.t. standard perturbation theory techniques do not apply. To overcome this obstacle we need the two following key ingredients: Observation 1. Eigenfunctions of 1  X  H t t coincide with eigenfunctions of  X . We now define the perturbation operator lows: small In particular will need two key estimates on the size of the perturbation R t in two different norms. of t such that for all sufficiently small values of t The proof of the Propositions requires technical estimates of the heat kernel and can be found the longer version of the paper enclosed. 4.2 Proof of Theorem 4.1.
 Lemma 4.4 Let e be an eigenvector of  X  with the eigenvalue  X  . Then for some universal constant C The details can be found in the long version. Now we can proceed with the Proof: [Theorem 4.1] Recall also that Now let us fix t and consider the function  X  ( x ) = 1  X  e  X  xt that  X  is a concave and increasing function of x .
 Put x 0 = 1 / monotonicity we observe that Therefore for t sufficiently small Thus k f k 2 = 1. Taking  X  &gt; 0, we split f as a sum of f 1 and f 2 as following: will now deal separately with f 1 and with f 2 .
 From the inequality (4) above, we observe that the basis expansion of f 1 , Since  X  acts by rescaling basis elements, we have k f 1 k Therefore by Proposition 4.3 for t sufficiently small and some constant C  X  Hence we see that Consider now the second summand f 2 . Recalling that f 2 only has basis components with eigenvalues greater than  X  and using the inequality (4) we see that On the other hand, by Proposition 4.2 Thus Finally, collecting inequalities 6 and 9 we see: where C is a constant independent of t and  X  .
 Choosing  X  = t  X  2 k +2 +  X  where 0 &lt;  X  &lt; 2 k +2 yields the desired result. Proposition 5.1 For t sufficiently small where SpecEss denotes the essential spectrum of the operator.
 Proof: As noted before L t f is a difference of a multiplication operator and a compact operator where rg g observe first that We thus see that for t sufficiently small and hence g ( t ) &gt; 1 2 t  X  1 .
 We see that Theorem 3.2 follows easily: Proof: [Theorem 3.2] By the Proposition 5.1 we see that the part of the spectrum of L t are eigenvalues and there are corresponding eigenspaces of finite dimension. Consider now i  X  [0 , Theorem 23 and Proposition 25 in [11], which show convergence of spectral pro perties for the empirical operators. We are finally in position to prove the main Theorem 4.1: Proof: [Theorem 4.1] From Theorems 3.2 and 3.1 we obtain the following convergence results: using the first arrow, we see that Making p ( N ) tend to zero, we obtain convergence in probability.
 [1] M. Belkin, Problems of Learning on Manifolds , Univ. of Chicago, Ph.D. Diss., 2003. [2] M. Belkin, P. Niyogi, Laplacian Eigenmaps and Spectral Techniques for Embedding and [3] M. Belkin, P. Niyogi, Towards a Theoretical Foundation for Laplacian-Based Manif old [4] O. Bousquet, O. Chapelle, M. Hein, Measure Based Regularization , NIPS 2003. [5] F. R. K. Chung. (1997). Spectral Graph Theory . Regional Conference Series in Mathe-[6] R.R.Coifman, S. Lafon, A. Lee, M. Maggioni, B. Nadler, F. Warner and S. Z ucker, [7] D. L. Donoho, C. E. Grimes, Hessian Eigenmaps: new locally linear embedding tech-[8] E. Gine, V. Kolchinski, Empirical Graph Laplacian Approximation of Laplace-Beltra mi [9] M. Hein, J.-Y. Audibert, U. von Luxburg, From Graphs to Manifolds  X  Weak and [11] U. von Luxburg, M. Belkin, O. Bousquet, Consistency of Spectral Clustering , Max [12] S. Rosenberg, The Laplacian on a Riemannian Manifold , Cambridge Univ. Press, 1997. [13] Sam T. Roweis, Lawrence K. Saul. (2000). Nonlinear Dimensionality Reduction by [14] J.B.Tenenbaum, V. de Silva, J. C. Langford. (2000). A Global Geometric Framework
