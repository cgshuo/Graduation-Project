 We introduce the problem of query decomposition, where we are given a query and a document retrieval system, and we want to produce a small set of queries whose union of resulting documents corresponds approximately to that of the original query. Ideally, these queries should represent coherent, conceptually well-separated topics.

We provide an abstract formulation of the query decompo-sition problem, and we tackle it from two different perspec-tives. We first show how the problem can be instantiated as a specific variant of a set cover problem, for which we pro-vide an efficient greedy algorithm. Next, we show how the same problem can be seen as a constrained clustering prob-lem, with a very particular kind of constraint, i.e., clustering with predefined clusters . We develop a two-phase algorithm based on hierarchical agglomerative clustering followed by dynamic programming. Our experiments, conducted on a set of actual queries in a Web scale search engine, confirm the effectiveness of the proposed solutions.
 H.2.8 [Database Management]: Database Applications -Data Mining H.4.3 [Information Systems Applications]: Communications Applications Algorithms Query recommendation, set cover, clustering.
It has been consistently observed over the past years that users typically enter short queries in search engines [15]. One of the many reasons for typing short queries is perhaps the fact that users are looking for information for which they Figure 1: Graphical representation of the query de-composition problem. do not have sufficient knowledge [5], and thus they may not be able to specify precisely their information need. In order to help the users locate information more effectively, most large-scale Web search engines offer query recommendations in response to the queries they receive. These recommen-dations are typically queries similar to the original one, and they are obtained by analyzing the query logs, for instance, finding recommendations by clustering of queries [25], or by identifying frequent re-phrasings [2].

In this paper we address the problem of assisting the user in the information-seeking task from a novel point of view. Our main intuition is to explore the fact that query logs provide a wealth of queries, which are related to the original query in many different ways. Motivated by this observation we introduce a novel paradigm, namely topical query decom-position , where the goal is to assist users in finding the infor-mation they are looking for, by providing to them a suitable set of queries as part of the results to their queries. Ideally, these resulting queries should retrieve coherent, conceptu-ally well-separated sets of documents, whose union should cover almost all documents associated to the original query. In other words, we aim at dissecting the different topical groups underlying a query and presenting them to the user.
On a high level, topical query decomposition has simi-lar goals with both query recommendation and clustering of query results. However, it has important differences from both of these tasks. Similar to query recommendation, given a query, our objective is to return other queries. But while the results in query recommendation are a set of queries or-der by relatedness or frequency , in the case of topical query Figure 2: A possible solution to the problem illus-trated in Figure 1. decomposition we aim at returning a set of queries that cover the answer set of the original query. In other words, while query recommendation can be seen as a clustering task at the queries level, query decomposition may involve cluster-ing as well, but at the documents level.

Query decomposition is also different from clustering the results of a query and returning to the user the clusters: we do not simply return sets of documents grouped by similar-ity, but we group the documents in such a way that each group pre-exists in the query log, given that it has been pre-viously retrieved by other users using the query that repre-sents that group.

A simple graphical representation of our problem is shown in Figure 1. We consider a query log L , which is a list of a set of documents that answer query q . We denote with Q ( q ) the maximal set of queries p i , where for each p set D ( p i ) has at least one document in common with the documents returned by q , this is,
In the example shown in Figure 1 we have that the issued compute a cover , i.e., selecting a subcollection C  X  Q ( q such that it covers almost all of D ( q i ). As stated before, the queries in C should represent coherent, conceptually well-separated set of documents: they should have small over-lap, and they should not cover too many documents outside D ( q i ). One possible solution to the problem instance in Fig-ure 1 is shown in Figure 2. What is missing in this graphi-cal representation is the topical coherence of each query, i.e., how compact is the set of documents it retrieves in the space of topics.

The problem we study in this paper, topical query decom-position, has many potential applications: query filtering: it can be applied to an existing query rec-query diversification: it can produce a diversified set of query-set model: it can be used for selecting terms to rep-query results presentation: it can be used to present the
These are just few examples in the context of web search applications, but we believe that topical query decomposi-tion may find application in any information-seeking context where the users must be helped in better specifying what they are looking for.
 The main contribution of this paper is the introduction of a novel problem, i.e, topical query decomposition and its gen-eral formulation. We also describe two alternative solutions to this problem:
Top-down approach , based on set-covering. Starting from the queries in Q ( q ), this approach tries to handle our problem as a special instance of the weighted set covering problem , where the weight of each query in the cover is given by: its internal topical coherence, the fraction of documents in D ( q ), the amount of documents it retrieves that are not in D ( q ), as well as its overlap with other queries in the solution.
Bottom-up approach , based on clustering. Starting from the documents in D ( q ), this approach tries to build clusters of documents which are compact in the topics space. Since the resulting clusters are not necessarily document sets associated to queries existing in L , a second phase in needed, in which the clusters found in the first phase are  X  X atched X  to the sets that correspond to queries in the query log.
Finally we report the empirical analysis performed on a real-world query log from a Web scale search engine, aimed at assessing on the effectiveness of the proposed algorithms for topical query decomposition.
 The next section describes previous work related to ours. Section 3 defines the problem which is tackled from the per-spective of set cover in Section 4 and of clustering in Sec-tion 5. Section 6 describes the empirical assessment of the methods proposed, and the last section presents some con-cluding remarks.
In the domain of our application, namely Web search en-gines, our work builds upon previous research on query rec-ommendation, expansion, and clustering of query results. In the general formulation, we use ideas from set covering and constrained clustering. These are all topics that have attracted considerable attention in the research community. In this section we discuss some of the work developed in each of these topics.

Query recommendation is the task of suggesting to a user a set of queries related to the query he has just issued. Recently, researchers have used data mining techniques ap-plied on the search engine query logs to build solutions for this problem. In [25] queries are clustered, by means of the density-based clustering algorithm DBSCAN [9] on the ba-sis of four different notions of distance: based on keywords or phrases of the query, based on string matching of keywords, based on common clicked URLs, and based on the distance of the clicked documents in some pre-defined hierarchy. Also the work in [4] proposes a query clustering technique based on common clicked URLs: the query log is represented as a bipartite graph with the vertices on one side represent-ing queries and on the other side URLs. An agglomerative clustering is performed on the graph X  X  vertices to identify re-lated queries and URLs. The algorithm is content agnostic , as it makes no use of the actual content of the queries and URLs, but instead it only focuses on co-occurrences in the query log. As stated in [2], the distance measures discussed above have real-world practical limitations when it comes to identifying similar queries, because two related queries may output different URLs in the first places of their answer sets, thus inducing clicks in different URLs (given that the user clicks are affected by the ordering of the URLs [8]). More-over, as empirically shown e.g., in [15], the average number of pages clicked per answer is very low. To overcome these limitations, the work in [2] clusters queries by representing them as term-weighted vectors obtained by aggregating the term-weighted vectors of their clicked URLs.

A different approach to query clustering for recommenda-tion is in [28], where two different methods are combined. The first method is obtained by modeling search engine users X  sequential search behavior, and interpreting this con-secutive search behavior as client-side query refinement, that should form the basis for the search engine X  X  own query refinement process. The second method is a traditional content-based similarity method used to compensate for the high sparsity of real query log data, and more specifically, the shortness of most query sessions. The two methods are combined together to form a similarity measures for queries. Association rule mining has also been used to discover re-lated queries in [12]. The query log is viewed as a set of transactions, where each transaction represents a session in which a single user submits a sequence of related queries in a time interval.

Our proposal is completely orthogonal to the body of re-search discussed above. In fact, given a query we do not recommend similar queries, nor we return the answer set of documents, instead we return a set of queries that cover the answer set of the original query. In our context clustering is used but at the level of documents, not at the level of queries.

Query expansion is another approach adopted by search engines to suggest related queries [26, 13]. The idea here is to reformulate the query such that it gets closer to the term-weight vector space of the documents the user is looking for. Also in this case our approach is different as we look for other queries that are present in the query log, and thus that have been issued by other users, while query expansion methods construct artificial queries.

Clustering of query results is a common technique used to organize the set of query results, usually the top-ranked ones, into clusters. While query suggestion and query ex-pansion have the ambitious goal of providing the users with better formulated queries, the main application of this tech-nique is to facilitate users X  browsing through search results. This task is usually associated with the problem of extract-ing meaningful phrases, i.e., snippets that summarize the contents of each cluster. Various different approaches for clustering search results have been proposed [14, 27, 17, 11]. Our work is substantially different from all these previous approaches since our goal goes beyond a simple reorganiza-tion of the pages on the base of their similarity: we aim at dissecting a given query into topical  X  X ubqueries X  by mining a query log.

Set covering . As we mentioned in the introduction, one of our approaches is based on the set cover problem. The set cover formulations and the adaptations of the greedy algorithm [7] we use are inspired by related variants of the set cover problem in the literature, such as the red-blue set cover problem [6, 18], and set cover with minimizing the overlap of sets [16].

Constrained clustering is a relatively new field of research, that is receiving a great deal of attention (see [3, 23] for an overview of the field). However, the kind of constraints usu-ally taken in consideration are instance-based constraints, such as must-link (two objects must be placed into the same cluster) and cannot-link (two objects must not be placed into the same cluster) [24]. To the best of our knowledge, the problem of clustering by picking clusters from a set of predefined clusters has not been studied before.
In this section we provide an abstract, general, formula-tion of our problem.

Each instance of the problem consists of a set U of base points R = { r 1 , . . . , r m } , that is, U = { b 1 , . . . , b We write p  X  U when we do not want to make the distinction if the point p of U is blue or red.

A collection S of l sets over U is provided, so that S = { S 1 , . . . , S k } , with S i  X  U . For every set S i  X  X  , we denote by S B i the blue points in S i , and by S R i the red points, that is, S B i = S i  X  B and S R i = S i  X  R . As in [6], one part of our goal is to find a subcollection C  X  S that covers many blue points of U without covering too many red points.
In our application, as will be explained in Section 6, there are weights associated with the set of blue points; each blue point b  X  B has a weight w ( b ) that indicates the relative importance of covering point b . Accordingly, we define the weighted cardinality of sets to be the total weight of the blue points they contain: for each set S with blue and red points
Another characteristic of our problem setting is that we consider a distance function d ( u, v ), defined for any two points u, v  X  U . A special case is when U  X  R t , and the distance function d is the Euclidean distance or any other L -induced distance.

We use the distance function d in order to define the no-tion of scatter sc( S ) for the sets S  X  X  . Given a S , we define the scatter of S to be The above definition of scatter corresponds to the notion of 1-mean . Additionally, one can also define scatter using the notions of 1-median , diameter , radius , or others. For our discussion we are also using the concept of coherence , which we do not define formally, but informally we refer to it as being the opposite of scatter. A set of high scatter has small coherence, and vice versa.
Our goal is to find a subcollection C  X  S that covers al-most all the blue points of U and has large coherence. More precisely, we want that C satisfies the following properties: cover-blue : C covers almost all blue points. The fraction not-cover-red : C covers as few red points as possible. small-overlap : The sets in C have small overlap among coherence : The sets in C have small scatter (large coher-
At an intuitive level, the property of coherence im-plies both small-overlap and not-cover-red . To give a geometric argument for the fact that coherence implies small-overlap and not-cover-red , let us visualize the sets in S as balls in an euclidean space, and consider that coherence translates to balls of small size, e.g., radius. Then covering a predefined space X with balls of small radius forces the balls to have small overlap and not to cover much space outside X . However, we explicitly state the properties of small-overlap and not-cover-red . The reason is that they are intuitive properties for the problems we consider, and second, trying explicitly to satisfy these properties can lead to solutions of better quality.
From the problem definition given in the previous section, it is clear that there is a mapping of our problem to the set-cover problem. Two of the most well-studied methods for solving many variants of the set-cover problem are the greedy approach and Linear Programming (LP). In this section, we discuss how to adapt these two general approaches for the particular problems we consider. In our experimental evalu-ation we have used only the greedy algorithm, however, we also discuss the LP-based algorithm, since we find it to be of theoretical interest.
There is a simple and intuitive greedy algorithm for the set-cover problem [7], which can be adapted for many vari-ants of it, and which achieves a O (log n ) approximation ratio that matches the hardness of approximation lower bound [10].
The basic greedy algorithm forms the cover solution by adding one element at a time. At the i -th iteration, if not all elements of the base set have been covered, the algorithm maintains a partial solution consisting of ( i  X  1) sets, and it adds an i -th set by selecting the one that is locally optimal at that point. Local optimality is measured as a function of the costs of the candidate sets and the elements that have not been covered so far.

In order to instantiate such general algorithm to the topi-cal query decomposition problem, we must take into account the fact that our set of points consists of blue and red points, that the blue points are weighted, the scatter scores sc( S ) of the sets, as well as the requirements of cover-blue , not-cover-red , small-overlap , and coherence . Given the above considerations, we reformulate the basic greedy algo-rithm as shown in Algorithm 1.

The cover parameter  X  controls the fraction of blue points that the algorithm aims at covering, and is measured in Algorithm 1 Greedy Input: Base set U = B  X  R , weights w ( b ) of the blue points Ouput: A cover C  X  X  1: V B  X  X  X  2: V R  X  X  X  3: C  X  X  X  4: while | V B  X  B | w &lt;  X  | B | w do 5: Select S  X  ( S\C ) that minimizes s ( S, V B , V R ) 6: C  X  X  X  X  S } 7: V B  X  V B  X  S B 8: V R  X  V R  X  S R 9: end while 10: Return C terms of the weights of the blue points. The score func-tion s ( S, V B , V R ) is used to evaluate each candidate set S with respect to the elements covered so far by the current solution. For the score function s ( S, V B , V R ), we propose a function that combines three terms: s ( S, V B , V R ) =  X  C where  X  C ,  X  R ,  X  O are parameters that weight the rela-tive importance of the three terms. Our score function s ( S, V B , V R ) is motivated by the requirements of our prob-lem and from approximation algorithms for the-set cover algorithm.

It is interesting to consider the three cases, where one of the parameters  X  is equal to 1 and the other two equal to 0. For  X  C = 1 the problem corresponds to weighted set-cover, where the weight of each cost is the scatter sc( S ). In this case, the goal is only to minimize the cost of the sets selected in the cover. For  X  R = 1 the problem becomes finding a cover of the blue points while introducing as few red points as possible. The use of ratio | S R | | S B \ V | motivated by the approximation algorithm of Peleg [18] for the red-blue set cover problem. Finally, when  X  O = 1 the objective is to minimize the overlap among the sets in the the approximation algorithm of Johnson [16], who considers set cover with minimizing the intersection of the sets selected in the solution.

Other combinations of the parameters  X  C ,  X  R , and  X  O can be used to control the contribution of the three terms in the score function and fine-tune the results according to the application domain. In practice, for specific applications, a good combination of the parameters  X  C ,  X  R ,  X  O may be learned from training data, if available.
We start by considering an Integer Programming (IP) for-mulation of the set cover problem: for each set S  X  S we introduce a 0/1 variable x S , and the task is to The above integer program expresses the weighted version of set cover. A solution can be obtained by relaxing the integrality constraints (3) to (3 0 ): { 0  X  x S  X  1 } , solving the resulting linear program, and then rounding the variables x obtained by the fractional solution. The resulting solution is a O (log n ) approximation to the weighted set cover problem, see, e.g., [22].

One way to allow small overlaps among the sets belonging to the solution, is to require that each one of the blue points is covered by only a few sets. Such a constraint can be written as for some constant c  X  2, enforcing that each point will be covered by at most c sets.

It can be shown that by solving the linear program { (1) , (2) , (4) } and performing randomized rounding to obtain an in-tegral solution provides again an O (log n ) approximation al-gorithm, in which the constraint (4) is inflated by a factor of log n , that is, each point in the final solution belongs to at most c log n sets. The proof, omitted due to space limi-tations, is an easy adaptation of the basic proof that shows the O (log n ) approximation for the set cover problem via randomized rounding.

We also consider adding constraints to satisfy the not-cover-red property: for each red point r  X  R , we introduce a 0/1 variable y r . We then require that at most d red points are covered by ensuring that whenever a set S is selected, the variables y for all red point r  X  S R are set to 1, by The program { (1) , (2) , (4) , (5) , (6) } can be either solved di-rectly by an IP-solver, or again, relax the integrality con-straints, solve the corresponding LP, and round the frac-tional solution.
Another approach to the topical query decomposition prob-lem is based on clustering. At a high level of description, our clustering-based method is a two-phase approach. In the first phase, all points in the set B are clustered using a hierarchical agglomerative clustering algorithm. During this clustering phase, the points in B are clustered with respect to the distance function d , while the information about the sets in the collection S , as well as the information about points in R is ignored. At any given level of the hierarchy the induced clustering intuitively satisfies the requirements of our problem statement: the clusters are non-overlapping, they have high coherence, they are covering the points in B , and no points in R . The problem, of course, is that those clusters are not necessarily corresponding to the sets of the collection S . Thus, in the second phase our method attempts to match the clusters of the hierarchy produced by the agglomerative algorithm with the sets of S .

A graphical representation of the two-phase method is shown in Figure 3. Next we describe the algorithm in more detail.

For the hierarchical clustering phase we adopt the method introduced in [29] (and available in the Cluto toolkit 1 ), http://glaros.dtc.umn.edu/gkhome/views/cluto Figure 3: Depiction of the clustering-based method. that has been shown to outperform the traditional agglom-erative algorithms when clustering document datasets. In this method, the agglomeration process is biased by a hi-erarchical divisive clustering solution that is initially com-puted on the dataset. This is done with the aim of reduc-ing the impact of early-stage errors made by the agglom-erative method, thus producing higher quality clustering. The method starts with a divisive clustering until ters are formed, where n is the number of objects to be clustered. Then, it augments the original feature space by adding ject is then assigned a value to the dimension corresponding to its own cluster, and this value is proportional to the sim-ilarity between that object and its cluster-centroid. Now, given this augmented representation, the overall clustering solution is obtained by using the traditional agglomerative paradigm with the upgma (Unweighted Pair Group Method with Arithmetic mean) clustering criterion function [21].
Once this method has been performed over the set of points B , it produces a dendrogram T whose leaves are the points in B and every node T  X  X  corresponds to a cluster. Let T ( B ) be the set of points in B that correspond to the cluster associated with node T  X  T , or in other terms, the leaves of the subtree rooted in T . Moreover, we denote by child of ( T ) the list of children of T in T .

The objective of the second phase of our method is to select the sets C  X  S according to the requirements of our original problem statement  X  large coverage of B , small coverage of R , small overlap of sets in C , and large coher-ence. We do this by exploiting the clustering produced in the first phase in to order to facilitate the selection of the sets C . The main idea is to match sets of S into clusters of T . In the following we describe how the matching is actually performed. For sake of simplicity, we first describe how to perform the matching in order to achieve complete coverage of B by means of dynamic programming . Then we modify the dynamic programming algorithm to handle the case of partial coverage.
 Complete coverage. For each set S  X  X  and each node T  X  X  we define the matching score m ( T, S ) between S and T to be as follows: that is, we match clusters T of T only to sets S that properly contain the clusters, and the cost is the scatter cost of S . Then given a cluster T  X  X  , we denote by m  X  ( T ) the score of the best matching set in S In other words, we define:
Now we solve the assignment problem from nodes of T to sets in S by dynamic programming on the tree T in a bottom-up fashion. Let M ( T ) be the optimal cost of cover-ing the points of T B with sets in S . We have The meaning of the above equation, is that for each cluster T that we consider in a bottom-up fashion in T , we either match T to a new covering set S  X  X he one with the least cost X  X r we use the solutions obtained for the children of T to make up the covering for T . Among the two options, the one with the least cost is selected.

The motivation of the algorithm, in terms of the require-ments of our problem statement, is as follows: cover-blue By assigning infinite costs to sets that do not not-cover-red : This requirement is achieved since sets that small-overlap : Again, sets with large overlap tend to con-coherence : The objective function of the matching tries
Partial coverage. In almost all of the problem instances encountered in our dataset, it is not possible to cover all of the original set of blue points B , with the sets in S . Further-more, even if a complete cover was possible, it might not be the case that the clusters in the hierarchy tree T are covered by the sets in S . Therefore, we need to adjust the matching algorithm in order to make it work with partial coverage.
The main idea is to relax the constraint that each cluster should be properly contained in the sets of S by adding a penalization term for the z points that are left uncovered. In particular, we define for all sets T  X  T and S  X  S . For the cases of proper containment, T B  X  S B , the above matching score gives m ( T, S ) = sc( S ), as in the case of complete coverage. How-ever, if T B 6 X  S B , the above score function penalizes grad-ually for the points of T B not covered by S B . Penalizing according to the square of the number of uncovered points was chosen among other choices by subjectively reviewing the results of the algorithm on our dataset. The parameter  X 
U weights the relative importance between the two terms, the scatter cost of the sets S and the number of uncovered points. Again, as for the parameters  X   X  of the greedy set cover algorithm, the value of  X  U needs to be selected heuris-tically, ideally to be learned via training data for a specific application at hand. In our experiments we study the be-havior of the algorithm for various measures of interest as a function of the control parameter  X  U .

Given the modified definition of m ( T, S ), the dynamic programming algorithm for the case of partial coverage is identical to the case of complete cover.
In this section we describe how to apply the methods in-troduced in this paper to a real query log. We use a sample from an in-house query log L including 2.9 million distinct queries. A majority of users only looks at the first page of results (as observed by [15] among others), while few users request more result pages. For each query q we record the maximum result page to which any user asking for q in the query log navigated, and consider the set of result documents for the query, which we denote by D ( q ), We emphasize that in contrast to most of the research on query log mining, we use all the documents that are shown to the users, and not only the ones they click upon.

Overall, we have 24 million distinct documents seen by the users. This means that there is certain overlap between the result sets of different queries; otherwise, given that users see at least 10 documents per query, we would have at least 29 million distinct documents if there were no overlap. For each query q , we build a set of candidate queries for q . The candidate queries Q k ( q ) are the ones that have sufficient overlap with the original query, namely:
In the following, we set k = 2 meaning that each candidate query p i should have at least 2 documents in common with the original query q .

A first question is whether there are enough candidates in the query log L for a given query q . In practice, the answer depends basically on the size of | D ( q ) | , as shown in Figure 4. The figure shows the average number of candidate queries in the data, for a sample of 200 queries having a certain number of documents seen by users. We observe that there are about | D ( q ) | / 2 candidates for each query returning | D ( q ) | documents. In practice this is sufficiently large to represent different topical aspects on each query. Figure 4: Number of candidates available for queries having different number of documents seen.

We also checked the size of the maximum cover attainable with this set of candidates. According to our observations, this is a fairly stable fraction of about 60%-70% across all queries that have at least 20 documents seen.
Next we compute for each candidate query p i its scatter sc( D ( p i )) as
For defining the distance between two documents d ( u, v ) in the result set of a candidate query there are many choices. Given that there is a potentially large set of candidate queries p for any query q , each one of them having potentially many documents, and given that we are interested only on an ag-gregate of the distances, we decided to use a coarse-grained metric. Our choice was to use a text classifier to project each document into a space of topics (100 distinct topics), and then use as d (  X  ,  X  ) the Euclidean distance between the topic vectors.

For the distance between two documents d ( u, v ) in the re-sult set of the original query q , we use a more fine-grained metric. We remove stopwords, do stemming, and compute tf  X  idf weights for each term in each document [1]. Using this document representation, we use the standard cosine measure for measuring document similarity during the ag-glomerative clustering process.

Finally, the weight w ( d ) of a document d  X  D ( q ) is given by the number of clicks the document has received when presented to the users in response to query q . The distribu-tion of clicks is very skewed [8] and many documents that are seen by the users have no clicks, so we used the following weighting function: where clicks ( q, d ) is the number of clicks received by docu-ment d when shown in the result set of query q .
Next we picked uniformly at random a set of 100 queries out of the top 10,000 queries submitted by users, and ran the algorithms proposed in this paper over those queries. Given that the greedy algorithms stops when it reaches the maximum coverage possible and queries have different cover sizes, we fixed a cover set size k and evaluated the results of the top-k queries picked by each algorithm, using the following measures: The average results for the set cover method described in Section 4 are summarized in Table 1 for several parameter settings.

From the results of set-cover shown in Table 1, we observe that penalizing only the overlap does not yield good results, and either the scatter of the queries or the red points have to be taken into account.

For the clustering-based method described in Section 5, results are summarized in Table 2. Here, the size of the cover varies with the parameter  X  U . For small values of  X  U , there is not sufficient penalization for partial coverage, and thus Table 1: Average results for the greedy algorithm at cover size |C| = 5 .

Parameters Sum of Red Inter-query  X 
C  X  R  X  O costs fraction overlap Coverage 0 0 1 0.11 0.15 1.07 0.47 0 1 0 0.06 0.04 1.53 0.48 0 1 1 0.06 0.06 1.11 0.44 1 0 0 0.03 0.06 1.32 0.43 1 0 1 0.04 0.08 1.10 0.40 1 0 10 0.05 0.09 1.09 0.39 1 1 0 0.05 0.04 1.41 0.47 1 1 1 0.05 0.07 1.13 0.44 1 10 0 0.06 0.04 1.51 0.47 1 10 10 0.05 0.06 1.12 0.44 10 0 1 0.04 0.08 1.17 0.42 10 1 0 0.03 0.05 1.33 0.44 10 1 1 0.04 0.07 1.16 0.43 Table 2: Average results for the clustering-based algorithm.

Parameter Size Sum of Red Inter-query  X 
U |C| costs fraction overlap Coverage 2 0 1.00 0.00 0.01 1.00 0.06 2 6 2.15 0.01 0.02 1.13 0.12 2 7 2.78 0.01 0.03 1.21 0.14 2 8 3.56 0.01 0.03 1.25 0.16 2 9 4.52 0.02 0.04 1.31 0.20 2 10 5.63 0.02 0.05 1.38 0.23 2 11 7.70 0.03 0.07 1.55 0.29 2 12 10.11 0.05 0.09 1.68 0.34 2 13 14.48 0.08 0.14 1.90 0.43 2 14 18.06 0.13 0.18 2.06 0.50 the resulting solutions tend to involve only few queries that do not cover well the set D ( q ). As the value of  X  U increases more sets are selected in the cover solution. We observe that the results of the clustering method are worse than the ones obtained by the set-cover method. If we observe Table 2 for average cover sizes |C| between 4 . 52 and 5 . 63, the coverage reached is about half of the coverage than the set-cover method at 5 obtains in Table 1, at a comparable level of cost for the solution.

Real examples of the queries returned by the algorithms are shown in Table 3. In the table we have included the top 10 queries returned by the algorithms, as well as they original position in the list ordered by overlap with respect to the issued query. The table provides a qualitative idea of the kind of anatomical dissection of a query obtained with our methods.

As a general observation, the results show that the query decomposition algorithms do not simply follow the original ordering by overlap, but pick queries in various positions. Also, extreme settings for the set-cover algorithm that con-sider only one aspect of cost at a time tend to be reflected qualitatively in the result set. For instance, if the scatter of the queries is given all the cost (  X  C = 1 ,  X  R = 0 ,  X  the queries tend to be more narrow and specific. query.

We introduced topical query decomposition, a novel prob-lem that stands in between query recommendation and clus-tering the results of a query, while having similarities and important differences with both. We provided a general for-mulation and two elegant solutions, namely red-blue metric set cover, and clustering with predefined clusters. The set-cover formulation provides solutions of better quality, and we are currently exploring methods of improving the results of the clustering-based algorithm, by employing other clus-tering algorithms and/or using queries from query logs over larger periods of time. We are currently investigating the applicability of our proposal in other contexts such as tag-based querying of pictures and multimedia. We also consider performing large-scale experiments involving user studies in order to tune the parameters of the algorithms.

Code and data for reproducing the results shown in Ta-ble 3 is available at http://www.yr-bcn.es/querydecomp/ . Acknowledgments: The authors thank Vassilis Plachouras for his valuable help. [1] R. Baeza-Yates and B. Ribeiro-Neto. Modern [2] R. A. Baeza-Yates, C. A. Hurtado, and M. Mendoza. [3] S. Basu, I. Davidson, and K. Wagstaff, editors. [4] D. Beeferman and A. Berger. Agglomerative clustering [5] N. J. Belkin. The human element: helping people find [6] R. D. Carr, S. Doddi, G. Konjevod, and M. V. [7] V. Chv  X atal. A greedy heuristic for the set-covering [8] N. Craswell, O. Zoeter, M. Taylor, and B. Ramsey. An [9] M. Ester, H.-P. Kriegel, J. Sander, and X. Xu. A [10] U. Feige. A threshold of ln n for approximating set [11] P. Ferragina and A. Gulli. A personalized search [12] B. M. Fonseca, P. B. Golgher, E. S. de Moura, [13] B. M. Fonseca, P. B. Golgher, B. P X ossas, B. A. [14] M. A. Hearst and J. O. Pedersen. Reexamining the [15] B. J. Jansen and A. Spink. How are we searching the [16] D. Johnson. Approximation algorithms for [17] K. Kummamuru, R. Lotlikar, S. Roy, K. Singal, and [18] D. Peleg. Approximation algorithms for the [19] B. Poblete and R. Baeza-Yates. Query-sets: Using [20] B. P X ossas, N. Ziviani, W. Meira, and B. Ribeiro-Neto. [21] P.-N. Tan, M. Steinbach, and V. Kumar. Introduction [22] V. Vazirani. Approximation Algorithms . Springer, [23] K. Wagstaff, S. Basu, and I. Davidson. When is [24] K. Wagstaff, C. Cardie, S. Rogers, and S. Schr  X  odl. [25] J.-R. Wen, J.-Y. Nie, H.-J. Zhang, and H.-J. Zhang. [26] J. Xu and W. B. Croft. Improving the effectiveness of [27] H. Zeng, Q. He, Z. Chen, and W. Ma. Learning to [28] Z. Zhang and O. Nasraoui. Mining search engine [29] Y. Zhao and G. Karypis. Evaluation of hierarchical
