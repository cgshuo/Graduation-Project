 Department of Science and System Management, Faculty of Information Science and Technology, The National University of Malaysia, Bangi, Selangor, Malaysia School of Biosciences &amp; Biotechnology, Faculty of Science &amp; Technology, The National University of Malaysia, Bangi, Selangor, Malaysia 1. Introduction
Increasingly sophisticated technology has led to more storage and manipulation of complex data. The characteristics of complex datasets include, but are not limited to, high dimensionality, unstructured and semi-structured data, temporal and spatial patterns, and heterogeneity [1]. Some categories and typical examples of these datasets include spatial data (maps, VLSI chip layouts), biology data (gene and protein sequences), web data (text, http logs), and multimedia data (video clips, voice). These kinds of datasets pose a challenge to the data mining community and require an advance method that can handle the anatomy and representation of data more ef fi ciently. In bioinformatics for example, there are still a number of challenges to be addressed despite many advancements. One main challenge is to identify the most relevant subset of data in a particular classi fi cation. The presence of many irrelevant and redundant features allows for over fi tting and le ss cost-effective models [2 ], which nece ssitates the selection of highly discriminatory features prior to mining the dataset.
Previous research realised that data pre-processing is of considerable importance in most classi fi cation tasks [3 X 5]. Feature transformation and feature selection are some forms of data pre-processing tasks that have been commonly used in the literature [4,6 X 8] . These forms are generally aim to change the data into a simpler dimension that enable the machine learning algorithms to learn faster. Feature construction and feature extraction are two models of feature transformation. The former is de fi ned as a process that discovers missing information about the relationship between features and adds the space of features by inferring or creating new features [9]. Feature extraction is de fi ned as a process that extracts a set of new features from the original features based on some transformation functions and usually only the transformed features are used [10]. From the transformation view, feature construction usually expands the feature space that causes the number of features to be larger than the original features whereas feature extraction reduces the feature space that causes the number of features to be smaller than the original features. Feature selection (FS) takes another view whereby no new features will be generated, instead only a subset of original features is selected from the original features by removing the irrelevant and redundant features. Therefore, it reduces the feature space and at the same time does not change the semantics of the original features. These are the merits of FS that make it preferable over the others.
Generally, FS methods are classi fi ed into two models: fi lter and wrapper, depending on the evaluation measures that they use in distinguishing the differ ent class labels. The former utilises the intrinsic a learning machine to assess subsets of features based on their performance. The fi lter category can be further divided into two groups: the feature weighting or univariate approach that evaluates and ranks the features individually and the feature subsets or multivariate approach that evaluates the goodness of each subset using certain evaluation criteria. The wrapper approach to FS has attracted more attention than the fi lter approach because this approach is seen as a stochastic optimisation that attempts to generate better solutions by employing prior knowledge gained from a previous population after the raw features are fi ltered. However, the higher computational complexity of the wrapper method suggests that the feature space should be pre-reduced using the fi lter model prior to applying the wrapper approach [2]. Additionally, extensive searching using the wrapper approach suffers from over fi tting, particularly in datasets with many irrelevant features and fewer instances [11]. Generally, a common drawback of the fi lter approach is that it ignores any interactions with the induction algorithm, and most proposed techniques are univariate (i.e., they ignore feature dependencies), whereas the wrapper approach suffers from the common weaknesses of higher over fi tting and increased demand on computational resources. In most complex applications, the number of features ranges from moderate (in tens of features) to high dimensional data (in hundreds of features) and therefore the searching of feature subsets become an NP-hard combinatorial problem. The framework of the fi lter and wrapper approaches is seen as a complement when searching is performed. As mentioned in [12],  X  ... the feature subset selection problem requires complex function evaluations which are often not available in closed analytical form or exhibits a nonlinear relationship with the space of feature subset X  . Furthermore, the capability of fi lter feature selection algorithms (FSAs) could be exploited by integrating them with a meta-heuristic method, namely particle swarm optimisation (PSO). Such a combination will be examined in this study.
One key contribution of this study is the proposal of a multivariate fi lter approach and a meta-heuristic approach using a PSO algorithm with support vector machine (SVM) classi fi er applied to protein sequences. The aim of this approach is to increase the classi fi cation accuracy while generating the most discriminatory feature subsets by making use of the strengths of both fi lter and wrapper approaches. In implementing this work, rigorous comparisons were carried out in both phases. In the a protein sequence dataset, and fi ve benchmark UCI datasets of similar complexity for comparison. Our complexity de fi nition is based on moderate to high dimensionality of features, mixed features type (numeric, categorical, and both), unstructured, and semi-structured domains with two classes or more. In the wrapper phase we employed PSO algorithms using three state-of-the-art PSO variants. The use of PSO is motivated by two factors. First, compared to a genetic algorithm, the operation of PSO does not involve crossover and mutation; thus, it is computationally inexpensive, in terms of both memory and runtime [13]. Second, unlike other heuristic techniques, PSO has a fl exible and well-balanced mechanism to enhance global and local exploration abilities [14].

The remainder of this paper is organised as follows. Section 2 reviews literature on feature selection algorithms. Sections 3 and 4 present the fundamentals behind the FSAs and PSO algorithms. Section 5 presents the proposed framework of fi lter and wrapper approach. In Sections 6 and 7, the experimental setup and results are discussed, and in Section 8, we offer a conclusion. 2. Feature selection algorithms
FSAs have been applied in various fi elds, including bioinformatics [15,16], signal processing [17,18], text categorisation [19], image retrieval [20] and pattern recognition [21]. There are several studies that report the overview of state-of-the-art FS methodologies [4,5,22]. In [4] for example, a three-dimensional categorising framework for FSAs is presented. The three dimensions framework was presented based on search strategies (further categorised into complete, sequential and random), evaluation criteria (further clustering). The bene fi ts of FSAs are manifold, but the most important is prediction because it improves the model performance and avoids the over fi tting issue [4,5]. Employing FSAs also produces an effective model because it is able to reduce memory and the time of learning while doing the processing task. Consequently, one would understand the data better by identifying the relevant factors. A typical FSAs process involves four steps namely subset generation, subset evaluation, stopping criterion, and result validation [4,22].

Past literature that focused on fi lter methods mentioned that identifying features independently, or using the so-called univariate approach, was insuf fi cient [23] because features that are useless by themselves can be useful together [5]. Among the approach X  X  merits, the concept of relevance and redundancy become the focus of current literature. Relevance, as de fi ned by Gutlein [23],  X  ... is how crucial the value of a feature for predicting the resulting class value . X  while  X  two features are considered redundant to each other if they are completely correlated X  . Some existing FSAs that have been shown utilised these concepts effectively include correlation feature selection (CFS) introduced by Hall [24] and fast correlation feature selection (FCBF) introduced by Liu and Yu [25]. CFS algorithms issue high merits to subsets that include features that are highly associated to the class feature but have low association to its member. This algorithm exploits heuristic searches such as best fi rst, forward and backward elimination search, with a preference for low redundancy subsets. Similar to CFS, FCBF algorithms consider the relevance of feature to the class, but the feature must also exhibit non-redundancy to the other relevant features. Both algorithms have been successfully used for reducing the feature space, but the capability of these algorithms could be further exploited by integrating with more advanced meta-heuristic approaches, such as the particle swarm optimisation algorithm.
 There are some recent works that attempt to improve the sequential forward selection algorithm. Gutlein [26] in his work introduced the linear forward selection (LFS) algorithm search strategy that aims to reduce the number of subset evaluations in each forward selection step. The LFS algorithm is useful for high dimensionality datasets and reduces the risk of over fi tting by focusing only the top k -rank features. They [26] have demonstrated that CFS with LFS is signi fi cant to a full forward selection method when tested on 12 high dimensionality datasets when tied with na  X   X  ve Bayes and C4.5 classi fi ers. Another interesting approach can be seen in Choo et al. [27] who proposed a fi tness-rough algorithm approach to eliminate irrelevant features. Nevertheless, it is not suitable for non-quantitative datasets due to a higher percentage of information loss. Zhao and Liu [28], for example, introduced INTERACT, an attempt to improve the FCBF method that uses backward elimination. This method is a combination of the symmetrical uncertainty and c-contribution that manages to maintain and improve the classi fi er X  X  accuracy. It discards feat ures with no or low c-contribution. In addition, the INTERACT manages the interaction between the features by designing an exclusive hashing data structure to render the issue of feature ranking. This method was compared with several other existing FS and showed a competitive performance.
 Several past studies that evaluate the FS method empirically include [8,29]. In [8], fi ve FS methods: ReliefF, random forest feature selector, sequential forward selection, sequential backward selection, and Gini index were compared with several classi fi ers. Their results showed that ReliefF and random forest enabled the classi fi ers to achieve the highest increase in classi fi cation accuracy on the average while reducing the number of unnecessary features. An extensive research on FS methods was done by Jeffery et al. [29] to identify the differentially expressed genes in microarray data. Ten FS methods were compared on nine different microarray datasets and reported that the empirical bayes t-statistic performed well across the variation of instance sizes. Although the study is limited to two classes of domains, the study emphasizes the FS choice, the size of features, the instances sizes and the noise in could improve the classi fi er X  X  performance.

In the wrapper method, the meta-heuristic algorithm such as genetic algorithms [30], arti fi cial immune systems [31], ant colony optimisation [32], and particle swarm optimisation [33] are some of the popular algorithms. Soto et al. [30] employed a genetic algorithm with various forms of non-linear fi tness function, namely decision trees, k -nearest neighbours ( k -NN) and a polynomic non-linear function. The study was performed on 73 molecular descriptors for predicting hydrophobicity using an aggregation of neural networks in a chemo-informatics domain. The results were quite promising but rather limited in their domain. Furthermore, Secker et al. [31] employed an arti fi cial immune system in solving clustering problems for protein function prediction, but their experiments were less practical as their system required a highly solution space and time. The use of ant colony optimisation was proposed [32] for text categorisation, and the performance was compared to the genetic algorithm and the several statistical fi lters method. Interestingly, their results on two Reuter X  X  datasets were superior compared to their counterparts. Additionally, the algorithm o f particle swarm optimisation was performed [33] for predicting protein function using naive Bayes and Bayes network classi fi ers on a G-protein-coupled receptors and enzymes dataset. These datasets are based on four kinds of proteins signatures or motifs. The predictive accuracy of the proposed method outperformed the baseline algorithm that use full feature sets. In another study, Lin et al. [34] used PSO to search for the optimal values for SVM and the developed approach was called PSO+SVM. The classi fi cation rate of their approach was better than grid search and had a similar result to GA+SVM. Wang et al. [13] proposed a novel method based on rough sets and PSO in which the PSO was employed to fi nd reducts with fewer features. Their study demonstrated that PSO is able to fi nd minimal reducts ef fi ciently compared to GA-based approaches and several established rough set reduction algorithms.

Another popular form of the FS method is hybrid methods that take advantage of both wrapper and fi lter methods. According to Liu and Yu [4], this approach usually exploits the independent measure for selecting the  X  ... best subsets for a given cardinality and uses the mining algorithm to select the fi nal best subset among the best subsets across different cardinalities X  . Previous studies on this approach include those of Uncu and Turksen [35], and Yang et al. [36]. Uncu and Turksen [35] proposed an integration of fi lter and wrapper methods using k -NN for evaluating the features. This study employed the following independent FSAs during the pre-selection stage to select signi fi cant features: k -NN sequential forward, k -NN sequential backward, correlation coef fi cients and functional dependency concept. Subsequently, these four methods employed k -NN with an exhaustive search strategy to select the best input combination. However, their proposed method would be over fi tting if higher numbers of features were involved, as the wrapper approach was employed in the fi rst stage and rather limited because it tested mathematical functions only.

Yang et al. [36] is closely related to our study but limited to microarray datasets. Their experiments were compared on fi lter, wrapper, and hybrid method ( fi lter and wrapper). The hybrid method effectively improved the performance and selected fewer feature subsets. However, we could hardly compare these results to our study because our protein sequences dataset has a different representation than microarray datasets. Compared to protein sequence datasets, microarray datasets usually involve very high dimensionality feat ures (in more than thousands and ten t housand of features) with small numbers of instances, simply known as the  X  X urse of dimen sionality X  in which the dim ensionality usually exceeds the number of instances. For e.g., these kinds of datasets have a minimum of 5000 genes (features) whereas the instance is less than 100. This kind of dataset is not comparable to other complex dataset having moderate to high feature dimension with large instances. Protein sequences, however, have highly dimensional features (in hundreds of features) with moderate (in hundreds and thousands of instances) to large numbers of instances (in more than ten thousands of instances). Therefore, it is similar to UCI datasets after the feature extraction process was performed on its original data representation. Due to these limitations we would like to con fi rm the suitability of the hybrid approach over the protein sequences dataset and some other complex datasets of similar complexity in particular on different kinds of original data representation, feature X  X  dimension, and the instances sizes. Additionally, for the PSO implementation, we proposed to employ the hamming distance method to map the real number of velocity whereas Yang et al. [28] used the sigmoid function. Theoretically, the hamming distance method would be faster because only a si ngle value (the distance) is used to compare w ith each bit of velocity position.
In this study, we designed and tested the integration of multivariate fi lters with a meta-heuristic approach for a classi fi cation problem following a hybrid approach. This paper extends previous work [37] in two ways. First, instead of focusing on one bioinformatics dataset, it compares fi ve benchmark UCI datasets of similar complexity which have been used in [11,38,39]. Second, our study explores three variants of the PSO algorithms, whereas previous work focused on only a single PSO variant. In this study, we selected three multivariate approaches: correlation feature selection with best search strategy (CFS-BS), correlation feature selection with linear forward search strategy (CFS-LFS), and fast correlation based fi lter (FCBF). The performance of these multivariate fi lters methods was compared with datasets without curve (AUC). All these methods applied SVM for evaluating the selected features. Out of the three multivariate fi lter FS methods, the one that yielded the most competitive result was selected as the outcome of the fi rst stage, and its feature was used in the second stage. For the wrapper approach, we have selected three variants of the PSO algorithm, which we named PSO-1, PSO-2 and PSO-3. The next section describes the fundamental theory behind the selected methods for both phases: the fi ltering phase (i.e. multivariate fi lters) and the wrapper phase (i.e. particle swarm optimisation algorithm). 3. Multivariate fi lters 3.1. Correlation feature selection with best search ( CFS-BS )
Correlation feature selection (CFS) [24] is a fi lter method that originates from statistical methods that It selects feature subsets with a higher degree of correlation to the target class and a lower degree of inter-correlation to each other using a heuristic score, merit the higher correlation between the features subset to the target class and the lower the inter-correlation among them. The merit Eq. (1).
 where,
Merit r cf is the average feature-class correlation r ff is the average feature-feature inter-correlation
Prior to applying the merit correlation between discrete features X and Y . Equation (2) gives the formula for SU. The values of feature-class and feature-feature correlations are calculated using SU prior to search the feature subset space. The numeric features are usually discretised using the minimum description length principle (MDLP) method [40]. The study uses CFS with best fi rst search (CFS-BS) to rank the features according to the heuristic score with Eq. (1).
 where H ( X ) is the marginal entropy for discrete features X H ( Y ) is the marginal entropy for discrete features Y
H ( X, Y ) is the joint entropy of X and Y 3.2. Correlation feature selection with linear forward search ( CFS-LFS )
This kind of multivariate fi lter adopts the same concept as CFS but with a different search strategy and we labelled it as Correlation Feature Selection-Linear Forward Search (CFS-LFS). The difference lies in the way searching is done and it offers a slightly different approach than the traditional sequential forward selection (SFS) algorithm whereby a simple hill-climbing search method is executed. In the traditional SFS algorithm, the total of evaluations expands quadratically with the number of features, N . For each step, the total number of evaluations is similar to the number of remaining features not yet taken into account. This kind of approach is computationally extensive in particular for datasets that have a large number of features. For example, for N number of features, the sequence of subset evaluations would be N , N  X  1 ,N  X  2 ,... , and so forth. Therefore, the upper bound on the number of evaluations is,  X 
In the linear forward search strategy (LFS), instead of using all features during evaluations, it starts by ranking all features and chooses the top-k ranked features as inputs for forward selection. Reported in [26], the LFS has two variants: fi xed set and fi xed width. The fi xed set, as the name implies reduces the number of features to a fi xed set of size k . The beginning ranking is carried out by assessing each feature individually based on their scores using a fi lter or wrapper evaluator. Next, only the k best features are used in the subsequent forward selection while the remaining features are removed. This Besides, the number of potential subset extensions decreases along the step and indirectly reduces the computation complexity. In the fi xed width method, the same initial ranking in fi xed set is performed whereby the search starts with the top-k ranked features. The difference exists during the subsequent forward selection, in which the number of subset extensions in each forward selection step is constantly maintained to a fi xed width k . This is performed by adding the next best feature in the ranking to the set of expansions. This process result to an increase in the theoretical upper bound for the number of evaluations in the forward search process to N x 1 / 2 x k ( k +1) . Because our in itial experiments showed a similar result for each of th ese models, w e omitted the fi xed width model and only focused on the fi xed set model. As suggested from the literatures, we employed k = 50 and k = 100 for medium and high dimensionality of features, respectively. 3.3. Fast correlation based fi lter
Fast correlation-based fi lter FCBF [38], is another type of multivariate fi lter to handle features with high dimensionality. FCBF involves two main steps: relevance analysis and redundancy analysis. The relevance analysis measures information entropy to calculate the dependencies of features. Similar to CFS, it uses symmetrical uncertainty (SU) function as in Eq. (2) to calculate the dependence of features, has a relevance score below a prede fi ned threshold, then it is considered irrelevant and discarded. The redundancy analysis measures predominant features and remove redundant features among the relevant features. A feature is said meaningful if it is predominant in predicting the target class. The predominant de fi nition is described in greater detail in [38]. Based on the predominant concept, they de fi ned FS as a process that identi fi es all predominant features to the class and removes the remaining features. To identify the predominant features and discard redundant features among the relevant features, three heuristic functions were proposed as below [38]: Heuristic 1: ( if S + identifying redundant peers for them.
 Heuristic 2: ( if S + them becomes predominant, follow Heuristic 1; otherwise only remove F remove features in S  X 
Heuristic 3: ( starting point ). The feature with the largest SU and can be a starting point to remove other features; where F peers for F F 4. Particle Swarm Optimisation algorithm ( PSO algorithm ) The PSO algorithm falls underthe class ofcomplex syst ems thatattemptto exploitNature X  X intelligence. In recent years, its ability to solve hard problems ef fi ciently and credibly was clear. This algorithm, inspired by fl ocks of birds and shoals of fi sh, was published by Kennedy and Eberhart [41] to solve non-linear optimisation problems. As such, PSO creates a swarm of candidate solutions in which each potential solution is seen as a particle with a particular rate of change or velocity that operates through the search space. The earlier PSO implementation was meant for continuous search space domain. However, many real world problems can easily be adapted into binary-valued domain. In the area of dimensionality reduction, speci fi cally in the FS problem, each solution of the particle is represented as fi xed length binary strings (i.e. x { 0,1 } , i = 1, 2, ... , n and d = 1, 2, ... , N ). The coordinates x v features, f =( f 1 ,f 2 ,f 3 ,f 4 ,f 5 ) and n = 4, a random initialisation of particles in a swarm could be encoded as follows: 2 5 possible feature subsets, thus the selected features subset in particle x 1 has been reduced to 2 3 .
PSO is initialised as a population of p articles in which each particle re tains its own individual memory the swarm so far. The position of the particle is adjusted by a stochastic velocity, which depends on two forms of distances: the particle X  X  distance from its own best, pbest denoted as p and the particle X  X  distance from the swarm overall X  X  best, gbest denoted as p searching for the optimal soluti on, each particle updates its velocity and position a s follows: Eqs (3) and (4).
Where w is the inertia weight;  X  1 and  X  2 are the acceleration positive constants known as cognitive learning rate and social learning rate respectively; U (0 , 1) is a random function within the range [0,1]. The velocity update in Eq. (3) contains three essential parameters for PSO: the momentum component, the cognitive component and the social component. The fi rst component guides how much the particle recalls its previous velocity via its inertial constant, w . The second component guides how much the particle heads towards its personal best via its cognition learning factor,  X  1 . The last component attracts the particle towards swarm X  X  best ever position via its social learning factor,  X  2 .

Each particle has its own fi tness value that needs to be optimised. In FS problem, evaluation of the and the length of selected feature subsets. The two parameters,  X  and  X  determine the signi fi cance of these two principles. In this study,  X  was set to 0.9 and  X  was set to 0.1, indicating that classi fi cation accuracy had a higher degree than t he subset length. In addition,  X  of the selected feature subset, F feature subsets. The formula for the fi tness function is then de fi ned in Eq. (5) as follows [13]
In applying PSO, several parameters,including inertia weight ( w ), cognition learning factor (  X  1 ), social learning factor (  X  2 ) , velocity limit, population size and number of generations should be determined properly as these in fl uence PSO performance. However, past studies [42] observed that particles tending to operate from pers onal and global best position distan ces caused the ve locities t o reach large values. This phenomena led to large position updates and indi rectly forced the partic les to leave the boundaries of their search space. Therefore, Eberhart and Shi [43] suggested velocity clamping within a maximum value in each feature X  X  dimension. Speci fi cally, the maximum velocity, V dynamic range of each dimension. Moreover, in a recent study [44] suggested the maximum velocity should grow with the problem size.

Inertia weight is another important parameter that can control and reduce the importance of V From the literature, we found that there are numer ous strategies for setting the inertia weight ( w )suchas a nonlinear decreasing inertia weight, a random inertia weight, a constant inertia weight, a time-varying inertia and a fuzzy inertia weight [13,42,43,45 X 47]. The nonlinear decreasing weight inertia usually decreases within the range of 0.9 and 0.4 during a run; the formula for this is shown in Eq. (6). This high value of inertia weight in the beginning allows for exploration, and a much lower value of inertia weight allows the swarm to be more exploitative. In addition, this strategy usually employs constant coef fi cients;  X  1 =  X  2 = 2. However, Eberhart and Shi [43] mentioned that this cannot guarantee a good result for tracking a nonlinear dynamic system. Their concern brings us to explore the random inertia strategy in which inertia weight is de fi ned as a random number within the range of 0.5 and 1.0, resulting in a mean value of 0.75. Besides the nonlinear decreasing weight and the random inertia weight, past studies [45] also mentioned that the use of constriction coef fi cients,  X  may improve the overall performance. The parameters as shown in Eq. (7). The aim of the constriction coef fi cients is to avoid the particles deducing very large values and to manage convergence without the need for velocity clamping [48]. Nevertheless, later studies [49] suggested that it is still useful to limit v of x coef fi cient,  X  , was approximately 0.7298 that can be derived using Eq. (8). where,
Generally, two methods in interpreting the velocity of the binary PSO exist: the hamming distance and the sigmoid function. In the hamming distance method, the distance is calculated based on the number of different bits between two particles which basically correspond to the difference between their positions [13]. Let p particle X  X  position: p be selected but is not selected while the value of negative one implies that this bit should not be selected but is selected. The distance is calculated based on the difference between total number of ones and total number of negative ones [13]. The positive and negative differences in this value allow particles to be more explorative in the searching space. Then the particle X  X  bit is transformed into a requisite 0 and 1 by comparing to this distance value. Whereas, the velocity that employs the sigmoid function is transformed into a requisite 0 or 1 by comparing to a uniformly random value in the interval [0.0, 1.0].
In this paper, we are interested to explore the PSO algorithms with the nonlinear decreasing inertia velocity update. In this paper, the PSO following the nonlinear decreasing weight with hamming distance method is designated as PSO-1, while the PSO that follows the random inertia weight and constriction coef fi cient using sigmoid function is designated as PSO-2 and PSO-3 respectively. Figures 1 and 2 describe the pseudo code of these three PSO variants (adapted from [13]). The details of other parameter settings in these variants are presented in the experimental setup section. 5. The proposed fi lter-wrapper approach
The goal of this study is to fi nd the best combination of FS approaches for the classi fi cation problem from complex domains. Speci fi cally, we propose to examine the FS framework created by integrating multivariate fi lters and meta-heuristic wrapper approaches in a complex classi fi cation problem. The to identifying the best input combination with the wrapper approach. This process is basically composed of three main stages: the discretisation stage, the fi lter stage and the wrapper stage. 5.1. Discretisation stage
Discretisation is a process of quantising continuous attributes [50] that prove to be important to guarantee more accurate and faster learning. Discretisation methods have been developed according to several taxonomies which generally fall into supervised and unsupervised discretisation. Till date there are three kind of taxonom ies reported in literatures [51 X 53]. T he most recent taxonomy [51] proposed four levels of data discretisation taxonomy based on hierarchical approach: (1) hierarchical and non-hierarchical, (2) splitting, merging, and combination, (3) supervised, unsupervised, and combinations, (4) binning, statistics, entropy, and etc. In this study, we employed entropy discretisation speci fi cally a minimum description length principle (MDLP), to descretise the dataset with numeric values. MDLP, proposed by Fayyad and Irani [40], was suggested as one of the most successful supervised discretisation method due to smaller error rates and less modelling time [50]. It has also been widely used in complex domains [54,55]. As a con fi rmation, our initial experiment on the selected datasets also shows a better performance with the MDLP method. 5.2. The integration appr oach  X  Filter and wrapper
The proposed approach for this study adopted the fi lter-wrapper approach (Fig. 3). In the fi lter phase, we examined the multivariate fi lters using WEKA [56] and evaluated them using SVM. The justi fi cation of the classi fi er was mentioned in the previous paper [57]. In WEKA, the SVM classi fi er was implemented by the sequential minimal optimisation (SMO). For both phases, SVM used the normalised values whereby all the discretised value are normalised by default [56] in the fi lter phase and svm-scale function was used in the wrapper phase. For each dataset, we randomly split into a training set (90%) and a testing set (10%). In the wrapper phase, the aim was to further re fi ne the features by only selecting the most optimum features for the classi fi cation task. The implementation of PSO was done in Java and NetBeans IDE 6.9 environment. The optimum features produced are classi fi ed using SVM algorithm in libSVM [58] using radial basis function (RBF) kernel X  X  function. The use of RBF kernel involved the searching of two optimum parameters, C (cost) and  X  (gamma), within the range 2  X  5 and 2 was handled by the WEKA SVM (SMO) within the default ranges and this was performed on the dataset with 10-fold cross validation. Whereas, the searching of these parameters in the wrapper phase was performed using the grid search method, a commonly used method to get the optimum parameter of C and  X  , on the training set (90%) with 10-fold cross validation method. This process produces a set of optimum parameters which was used to retrain the training data. Subsequently, the model fi le was used to predict the testing set (10%).
 6. Experimental setup
The experiment was performed on protein sequences from the pectin lyase-like (PLL) superfamily col-lected from UniProt databases. The functional information was extracted from Pfam, a large collection of protein families, available at http://pfam.sanger.ac.uk/search [60]. The classi fi cation of PLL into various subfamilies was done on the basis of amino acid compositions extracted from COPid, a composition-based protein identi fi cation web server available at http://www.imtech.res.in/raghava/copid/ [61]. The initial data set had 1074 se quences belonging to seven subfamilie s of PLL; however, classes contains insigni fi cant members were excluded in this study, resulting in 859 proteins in four subfamilies (classes). These subfamilies were pertactin (128 instances), glyco hydro 28 (326 instances), pectinesterase (204 instances) and pectate lyase C (201 instances).

A total of 433 features from three categories (amino acid composition (AAC), physico-chemical composition (PCC), and dipeptid e composition (DPC) ) were extr acted from each protein sequence. In addition, the AAC category was composed of 20 features, the PCC category was composedof 13 features, and DPC category was composed of 400 features. The use of these feature categories was motivated by past studies [62 X 64], and Table 1 describes the details of these feature sets. The complexity of data can be justi fi ed based on several factors such as the feature X  X  dimension (FD) or number of features (medium to very high dimensions), the number of instances, and the type of data (semi-structured or/and unstructured), and heterogeneity [1]. These kinds of dataset usually exist in domains such as in biological data, world-wide web data, time series data, spatial data, and graphical data. Therefore, a complex dataset can be de fi ned as having any two of these factors. However, the de fi nition on the number of instances is quite subjective. A small number of instances (less than hundred) with high dimensionality (thousands or more) can also be considered as a complex dataset. The microarray dataset usually contains thousand of genes with limited number of instances [11,65,66]. Previously, Kudo and Sklansky [21] de fi ned dimensional category as follow: low (0 &lt; FD 19), moderate (20 FD 49), high (50 FD). We improvised this de fi nition after analysing the pro fi le of datasets used in [11] and proposed our new de fi nition on the feature X  X  dimension as follow: low (0 &lt; FD 49), moderate (50 FD 99), high (100 FD 999), very high (1000 FD).

Apart from the PLL dataset, we obtained fi ve other datasets from the benchmark UCI repository with similar complexity. These datasets are USCensus90 (DS1), Coil2000 (DS2), Promoters (DS3), Arrhythmia (DS4), and Multi-Features (DS5) whereby the selection was based on several criteria, which involved unstructured data (DS3) and multivariate type of data (DS1, DS2, DS4, DS5), and both moderate and high dimensional datasets. For DNA data, its raw representation was preprocessed by transforming each position into four binary attrib utes, one for each nucle otide [11]. The origin al number of features for DS3 was 57 features. For large datasets such as DS1 and DS2, the instances of the data were removed using weka. fi lters.supervised.instance.Strati fi edRemoveFolds features and these datasets were marked the selected features subsets was considered as an indirect measure [39] using SVM from WEKA [56]. The details of these datasets are presented in Table 2. Table 3 describes the three types of PSO parameter settings mentioned in Section 4. Most of the settings followed closely to the past studies [13,43,48] except for the last two parameters which were based on our preliminary experiments on the selected benchmark UCI datasets.
 7. Experimental results and discussion
This section empirically evaluates the performance of fi lter and wrapper phases by comparing several multivariate FS algorithms and three variants of PSO algorithms for information loss. Information loss is de fi ned as the quantity of information lost in the process of data mining that degrades the classi fi er performance due to dimension reduction, size reduction or missing values [27]. This study adopts classi fi cation accuracy as the measure of information loss; the higher the classi fi cation accuracy, the lower the information loss from the selected FSAs. Apart from the classi fi cation accuracy, some other principles, such as the selected number of features, the modelling time and the area under curve (AUC), are also compared. 7.1. Filter phase
Tables 4 and 5 record the results of the fi lter phase in which the last two rows in each table summarise the &lt; 0.05 (indicated by the letters W/T/L) over the full features set. In addition to average measurement, we emphasise wins/ties/lo sses because the average criteria would be s usceptible to ou tliers [67]. In general, it can be seen that all these FSAs produced a good trend in most of the datasets. We de fi ned a good trend as when the reduction of features maintained or improved the classi fi cation accuracy of the SVM. This result can be seen in CFS-LFS, for example, in which four out of the six datasets produced a good trend with two wins and two ties. For both CFS-BS and FCBF, the results were competitive with two wins and signi fi cant (i.e., wins) over the full set of features (DS5 and DS6). This result indicated the need of the wrapper phase to further optimise the selected features.

On average, all these FSAs achieve competitive classi fi cation accuracy over the full features, but they achieve signi fi cant reduction of dimensionality by selecting only a small number of features from the full feature set. This can be seen from the results of selected number of features. The average numbers of selected features for the three FSAs were 39.48 (CFS-BS), 36.40(CFS-LFS), and 32.33(FCBF). In the DS6 (Bioinformatics) for example, the percentage of the selected number of features was only 12.1% (CFS-BS), 11.5% (CFS-LFS), and 10% (FCBF) and yet the classi fi cation accuracy is improved from 93.03% to 97.69% (CFS-BS), 97.78% (CFS-LFS), and 97.5% (FCBF). These results highlighted that not all features were necessary to achieve high classi fi cation accuracy. Except for the DS4, the proposed FSAs were able to minimise information loss (within 3% allowance) in all datasets based on classi fi cation accuracy compared to the full features. Reviewing the complexity measurement, the use of FSAs was able to decrease the degree of complexity of search space from 2 290 to 2 36 (290 is the average feature length for all datasets, and 36 is the average feature length of the three FSAs).
 Further comparison was made on the modelling time and AUC on full features as well as all of the FSAs. Modelling time was de fi ned as the elapsed time during the classi fi cation on the training datasets while the AUC was de fi ned as the probability of the classi fi er in ranking a randomised positive instance above the negative instance with a perfect condition of  X 1 X . From the experiments, all FSAs showed a signi fi cant reduction of modelling time over the full features set. For modelling time, both CFS-LFS and FCBF won in all the datasets, with FCBF being the fastest and CFS-BS being the slowest. In terms of AUC, all three FS algorithms gave a similar number of wins, ties and losses over the full features set. However, except for the DS1, DS5 and DS6 datasets, the other three datasets demanded further action, as their AUC values were less than  X 1 X .

To sum up, the experimental results of the fi lter phase employing the multivariate FSAs veri fi ed the need of the wrapper phase in optimising its selected feature based on the evaluation criteria mentioned above. In selecting the most suitable FSAs for the next phase, we chose CFS-LFS because it maintained and improved the classi fi cation accuracy over the other FSAs on four out six datasets and gained similar number of wins to FCBF with the least number of losses. 7.2. Wrapper ( Optimisation ) phase
Table 6 compares the results of each PSO variant for ten runs based on classi fi cation accuracy, number of selected features and modelling time on the DS6 or the Bioinformatics (PLL) dataset with the results from the fi lter phase using a multivariate (MV) approach. The comparison was analysed in terms of the average value and t-test ( p -value &lt; 0.05). From the results, we could observe that the fewer selected of feature was about 40% (average number of selected features in PSO-1, PSO-2, and PSO-3 over the number of selected features in MV method) from the fi rst phase, the accuracy results among the PSO variants were better than the MV method. This indicated that the selected features from the fi rst phase still contain noise that could be further enhanced with an optimisation method.

A validation using a t-test also showed that this result of selected length was statistically signi fi cant, with all the three variants of PSO having a p -value less than 0.05. However, in terms of modelling time, both MV+PSO-2 and MV+PSO-3 methods took a longer time than the MV phase. The longer time for these variants is caused by the way the particle X  X  position is updated. In MV+PSO-1, the new particle is updated based on hamming distance method while in MV+PSO-2 and MV+PSO-3, the new is updated based on sigmoid function. The details of this implementation can be found in Figs 1 and 2. 7.3. Discussion of overall results
The experimental results for the six datasets are summarised in Table 7. We analysed the results features (FF), the multivariate (MV) fi lter, and the fi lter and wrapper (MV+PSOs). The last two rows method. The overall results showed that the proposed method produced a good trend in most of the datasets. We de fi ned a good trend as when the selected number of features or the reduction of features produced higher classi fi cation accuracy than the use of the MV method.

From Table 7, we can see that the MV+PSO-1 produced a good trend in most datasets with fi ve wins and one loss in the classi fi cation accuracy. The MV+PSO-1 method also preserved its information loss in all datasets in which the classi fi cation accuracy increased, despite the fewer features (except in the DS5). However, within an allowance of less than 1%, the information loss in the DS5 dataset was considered small and insigni fi cant. Additionally, MV+PSO-1 greatly reduced the unwanted features in all dataset with an average of 50% feature reduction from the features in the MV method. As for modeling time, although on average the MV+PSO-1 took more time than the MV method, but it was statistically insigni fi cant in all the datasets except for DS1 dataset. Both MV+PSO-2 and MV+PSO-3 methods gave similar average classi fi cation accuracy and outperformed the MV method with an average of 5.3% difference while MV+PSO-1 outperformed the MV method with an average of 4.4% difference. In addition, both MV+PSO-2 and MV+PSO-3 methods gave all wins over the MV method in classi fi cation accuracy that implies the information loss is preserved. Similar to MV+PSO-1, both MV+PSO-2 and MV+PSO-3 methods greatly attained smaller numbers of selected features, with an average of more than 50% feature reduction from the features in the MV method.

Overall, the above results demonstrated the superiority of the integrated method over the MV method from several aspects. The proposed method has produced a good trend in most datasets in terms of higher classi fi cation accuracy with a fewer number of selected features. In DS6 (Bioinformatics) dataset for example in the fi rst phase, although the selected number of features was about 12% from the original features, the classi fi cation accuracy was improved more than 4%. Further improvement in this dataset could be seen in the second phase, where the selected number of the features was within the range of 3 X 6% from the original features. On average, the classi fi cation accuracy of the proposed method was about 5% higher than the single approach, despite using less number of features. The high number of wins in the proposed method fu rther emphasised the reliability of the proposed method. In most more information is preserved. However, to accommodate the risk of losing information on a particular domain, the expert involvement would bene fi tthe fi nal analysis. In this study, the information loss is indicated by the improvement of accuracy despite the reduce features. The accuracy results among the three PSO variants were comparatively similar with a difference of less than 1%. This was also true for the number of selected features in which only 50% of the features from the single method was used. Among the three PSO-variants, PSO-3 obtained the least number of selected features but requires the longest modelling time which is almost nine times hi gher than the PSO-1. The fa ster modelling time in MV+PSO-1 and the less number of selected features explained that a hamming distance method is a better choice. Additionally, there is no statistical difference in the modelling time between the MV+PSO-1 with the MV method. In terms of complexity, measurements on all the selected datasets illustrated that the complexity of the search space was reduced from 2 290 to 2 36 in the fi rst phase and 2 36 to 2 15 in the second phase. 8. Conclusion
This study introduced a framework of dimensionality reduction by integrating a multivariate fi lter with a meta-heuristic algorithm, speci fi cally the PSO algorithm, to attain the classi fi cation problems on complex datasets. The need of the proposed method is justi fi ed with the increase of accuracy despite the smaller number of features on most selected datasets. A signi fi cant decrease of the selected features subset in both phases highlighted the importance of pre-processing to guarantee the discriminatory features for classi fi cation. The accuracy and the selected number of features among the three variants of PSO were comparatively similar except on modelling time. Our experimental results conclude the need of data pre-processing particularly for complex datasets prior to classi fi cation via the integration of FSAs approach. A future continuation of our work is to improve the ef fi ciency of the velocity X  X  updates that in fl uence the particle X  X  position. In the current implementation, we followed the standard PSO algorithms using the identi fi ed velocity and inertia weight settings. We believe the improvement of the velocity X  X  update would enhance the performance of the proposed method.
 Acknowledgments
Shuzlina Abdul-Rahman was funded by the Ministry of Science and Technology Innovation, Malaysia (UKM-MGI-NDB0005-2007) and Universiti Teknologi MARA, Shah Alam, Selangor, Malaysia.
 References
