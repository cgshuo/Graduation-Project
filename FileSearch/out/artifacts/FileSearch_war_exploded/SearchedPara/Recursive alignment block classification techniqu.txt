 Abstract Statistical machine translation (SMT) is based on alignment models which learn from bilingual corpora the word correspondences between source and target language. These models are assumed to be capable of learning reorderings. However, the difference in word order between two languages is one of the most important sources of errors in SMT. In this paper, we show that SMT can take advantage of inductive learning in order to solve reordering problems. Given a word alignment, we identify those pairs of consecutive source blocks (sequences of words) whose translation is swapped, i.e. those blocks which, if swapped, generate a correct monotonic translation. Afterwards, we classify these pairs into groups, following recursively a co-occurrence block criterion, in order to infer reorderings. Inside the same group, we allow new internal combination in order to generalize the reorder to unseen pairs of blocks. Then, we identify the pairs of blocks in the source corpora (both training and test) which belong to the same group. We swap them and we use the modified source training corpora to realign and to build the final translation system. We have evaluated our reordering approach both in alignment and translation quality. In addition, we have used two state-of-the-art SMT systems: a Phrased-based and an Ngram-based. Experiments are reported on the EuroParl task, showing improvements almost over 1 point in the standard MT evaluation metrics (mWER and BLEU).
 Keywords Statistical machine translation Word reordering Statistical classification Automatic evaluation 1 Introduction The introduction of Statistical machine translation (SMT) has yielded significant improvements over the initial word-based translation (Brown et al. 1993 ). At the approach) represented a clear improvement in translation quality (Zens et al. 2004 ).
In parallel to the Phrase-based approach, the use of a language model of bilingual units gives comparable results to the Phrase-based approach(Marin  X  o et al. 2006 ).
In both systems, the introduction of some reordering capabilities is of crucial importance for some language pairs (Costa-jussa  X  and Fonollosa 2009 ).
In our approach, we introduce order modifications to the source corpora by using alignment information so that word alignments and translation units become more monotonic together with a novel classification algorithm. The proposed classifica-reordering are dealt by pairs of swapping blocks belonging to the same group. The groups have been created following recursively a co-occurrence block criterion.
This paper is organized as follows. In Sect. 2 , we describe the reordering process and the algorithm which infers the reorderings. In Sect. 3 , we briefly describe the two baseline systems: Phrase-based and Ngram-based system, both capable of producing state-of-the-art SMT translations. Then, in Sect. 4 we set the evaluation framework and describe the experiments and results. Finally, in Sect. 5 we present the conclusions of this work. 2 Reordering based on alignment blocks classification (RABC) 2.1 Motivation SMT systems are trained by using a bilingual corpus composed of bilingual sentences. Each bilingual sentence is composed of a source sentence and a target sentence, and we align them at the word level by using GIZA ?? (Och and Ney monotonization (Kanthak et al. 2005 ), i.e. reordering the words in the source sentence following the order of the words in the target sentence. For instance in a Spanish to English translation, the original sentence El discurso pol X   X  tico fue largo alignment: El#The pol X   X  tico#political discurso#speech fue#was largo#long .In (Popovic and Ney 2006 ), the authors design rules based on Part of Speech (POS) tags and reorder pairs of words both in the training and test sentences. Similarly, we propose one type of monotonization: pairs of consecutive blocks (sequence of words) which swap only if swapped generate a correct monotonic translation. The main difference from (Popovic and Ney 2006 ) is that our approach learns the blocks which swap instead of following a pre-defined set of rules. Figure 1 shows an example of this type of pairs. The reordering based on blocks covers most cases as shown in (Tillmann and Zhang 2005 ). 2.2 Reordering process Our purpose is to model the effect of local block reordering to: (1) monotonize the alignment; and (2) be able to generalize in the test stage. In order to fulfil (1) and (2), the reordering process consists of the following steps (Costa-jussa ` et al. 2008 ):  X  Given a word alignment, we extract a List of Alignment Blocks ( LAB ). An  X  Given the LAB , we apply the Alignment Block Classification algorithm (see  X  We use the criteria of the Alignment Block Classification to reorder the source  X  Given the modified source training corpus, we realign it with the original target  X  We build the systems as shown in Sect. 3 , using the monotonized alignment and 2.3 Algorithm for alignment block classification The objective of this algorithm is to infer block reorderings in case the order of the blocks differs from source to target. The algorithm should be able to cope with swapping examples seen during training; it should infer properties that might allow reordering in pairs of blocks not seen together during training; and finally it should be robust with respect to training errors and ambiguities.

The algorithm consists of two steps: in the first, given the LAB , the algorithm filters the ambiguous Alignment Blocks (i.e. either misaligned or incoherently aligned). We will define the filtered LAB as LAB filt , which will be a subset of LAB and consists of m pairs of blocks f X  a 1 ; b 1  X  ;  X  a 2 ; b 2  X  ; ... ;  X  a m ; b m  X g . In the second step, from the LAB filt , we create the sets A  X f a 1 ; a 2 ; ... ; a m g and following recursively a co-occurrence block criterion (see next Subsection) and has group G n we build a Generalization group Gg n , where n  X  1 ; ... N defined as the Cartesian product between the subsets A n [ A and B n [ B , i.e. Gg n  X  A n B n .  X  a filtering threshold, and therefore limiting the number of allowed unseen pairs, and also by processing with morphological information. Note that we assume that only the elements in Gg n that appear in the training and test corpus set are correct generalizations (see Sect. 4.2 ). 2.4 Outline of the algorithm The first phase of the algorithm filters the possible bad alignments or ambiguities, by using the following criteria:  X  Pairs appearing less than N min times are discarded.  X  Pairs of blocks with a swapping probability ( P swap ) less than a threshold are also
The second phase of the algorithm infers the generalization groups Gg n from the which go from 1 to N .  X  Given the LAB filt , the generalization groups Gg n are constructed using the 2.5 Recursive classification algorithm example  X  Given a LAB filt , classify each Alignment Block into a group, using a co- X  Once all the Alignment Blocks in the LAB filt are classified, the final groups are:  X  Inside the same group, we allow new internal combination in order to generalize 2.6 Using extra information Additionally, the Alignment Block Classification can be used for extracting blocks from a lemmatized corpora. The resulting Gg n will be able to deal with grammar agreement between elements of each block, for instance the pair ( conferencia , par-lamentario ), which does not have gender agreement, would be a correct generalization if we take each block as a lemma. Therefore the generalization would not be influenced by the particular distribution of word inflexions in training database.

Furthermore, we can use a tagger to find out the grammatical function of each word. In case the blocks are constituted of only one word, i.e. the Alignment Blocks are pairs of swapping words, a general grammar rule to take into account for the Spanish to English translation is that in Spanish most adjectives are placed after the noun, whereas in English it is the opposite. However, there are exceptions to this rule and we can not rely completely on it, e.g. big man is translated as gran hombre when it refers to a man who has somehow succeded.

The use of this morphological information is optional. The algorithm itself does not require extra information that is not employed in a standard SMT system. But it is interesting to benefit from morphological information, if available, as have been shown in other studies (Nie X en and Ney 2001 ). In this study we will use lemmas and POS tags. 3 Baseline systems Two baseline systems are proposed to test our approach. The main difference between the two systems is the translation model, which constitutes the actual core of the translation systems. In both cases it is based on bilingual units. A bilingual unit consists of two monolingual fragments, where each one is assumed to be the translation of the other. 3.1 Ngram-based translation model The translation model can be thought of a language model of bilingual units (here called tuples). These tuples define a monotonic segmentation of the training sentence pairs ( f J 1 ; e I 1 ), into K units ( t 1 ;:::; t K ).
 The translation model is implemented using an Ngram language model, (for N = 3):
Bilingual units (tuples) are extracted from any word alignment according to the following constraints:  X  a monotonic segmentation of each bilingual sentence pairs is produced,  X  no word inside a tuple is aligned to words outside the tuple, and  X  no smaller tuples can be extracted without violating the previous constraints.
As a consequence of these constraints, only one segmentation is possible for a given sentence pair, which allows to build a bilingual language model. The bilingual language model is used as translation model. See (Marin  X  o et al. 2006 ) for further details.
 3.2 Phrase-based translation model The basic idea of Phrase-based translation is to segment the given source sentence into units (here called phrases), then translate each phrase and finally compose the target sentence from these phrase translations (Zens et al. 2004 ).

Given a sentence pair and a corresponding word alignment, a phrase (or bilingual phrase) is any pair of m source words and n target words that satisfies two basic constraints: 1. Words are consecutive along both sides of the bilingual phrase, 2. No word on either side of the phrase is aligned to a word out of the phrase.
We limit the maximum size of any given phrase to 7. The huge increase in computational and storage cost of including longer phrases does not provide a significant improvement in quality (Koehn et al. 2003 ) as the probability of reappearance of larger phrases decreases.

Given the collected phrase pairs, we estimate the phrase translation probability distribution by relative frequency in both directions.
 where N (f, e) means the number of times the phrase f is translated by e . 3.3 Additional feature functions In each system, the translation model is combined in a log-linear framework with additional feature functions.  X  The target language model consists of an n-gram model, in which the probability  X  The forward and backwards lexicon models provide lexicon translation  X  The word bonus model introduces a sentence length bonus in order to  X  The phrase bonus model introduces a constant bonus per produced phrase and it
All these feature functions are combined in the decoder. The different weights are optimized on the development set applying the Simplex algorithm (Nelder and Mead 1965 ). 3.4 Enhancing an SMT system with the RABC technique We introduce the RABC technique in an SMT system as follows: 1. Given the candidates to be swapped (Alignment Blocks proposed by the RABC 2. Given the reordered source training corpus, we realign it with the original target 3. We build the phrase-and Ngram-based systems, respectively, using the new
Figure 2 describes the application of the RABC technique on an SMT system. 4 Evaluation framework 4.1 Corpus statistics Experiments have been carried out using the EPPS database (Spanish-English). The EPPS data set corresponds to the parliamentary session transcriptions of the European Parliament and is currently available at the Parliament X  X  website. 1 The training corpus is about 1.3 million sentences. See Table 1 with the corpus statistics. More information can be found at the consortium website. 2 In the case of the results presented here, we have used the version of the EPPS data that was made available by RWTH Aachen University through the TC-STAR consortium. 3
The English POS-tagging has been carried out using freely available TNT tagger (Brants 2000 ) and lemmatization using wnmorph included in WordNet package (Miller et al. 1991 ). In the Spanish case, we have used the Freeling (Carreras et al. 2004 ) analysis tool which generates the POS-tagging and the lemma for each input word. 4.2 Experiments and results First, we study most common reordering patterns found in our task. We have a reference corpus which consists of 500 bilingual sentences manually aligned (Lambert 2008 ). Given the word alignment reference, we can extract the reordering patterns. Most common reordering patterns have been described as in de Gispert and between position x i and y i , in the original and the reordered source sentence composed of the source words appearing in the monotonization of the alignment. word. Table 2 presents the most frequent reordering patterns when aligning from Spanish to English with the EuroParl task.
 The most frequent pattern (0,1)(1,0) usually takes the form of the Noun ? Adj = Adj ? Noun ,asin semana siguiente = following week . Less frequently we find the form Adv1 ? Adv2 = Adv2 ? Adv1 ,asin bastante bien = good enough .

The second most frequent pattern (0,2)(1,0)(2,1) almost always takes the form of a noun followed by a prepositional clause in Spanish ( Noun ? Prep ? Noun ), and Adj ? Noun in English, as in conferencia de premsa = press conference ,or comparative adjectives as in discurso ma  X  s largo = longer speech (de Gispert and Marin  X  o 2003 ).

The third pattern (0,1)(1,2)(2,0) reveals the relationship Adv ? Verb = Pro-noun ? Verb ? Adv existing in cases as nunca podemos = we can never , a relationship impossible to detect when aligning the other way round, given the asymmetry of the alignment models (Brown et al. 1993 ).

When using automatic alignment, even some of the second and third most frequent pattern crosses have a high probability of being wrongly detected, as teor X   X  a puedo = I can conceivably , where the word en in Spanish is ommitted in front of the cross. In this case, it seems advisable to use multi-words, which consists in linking information of the alignment in both directions. Several works in this direction can be found in (Lambert 2008 ).

Experiments presented later in this section deal with the most frequent reordering pattern: (0,1)(1,0). In this work, no additional reorderings are considered, given that the other crossings have a much lower frequency as shown in Fig. 2 and state-of-the-art automatic alignment usually fails to detect them. 4.2.1 LAB filtering parameters N min and P swap The amount of admissible blocks in the LAB , is a function of the minimal number of block co-occurrence ( N min ) and the probability of them being swapped ( P swap )(see Sect. 2.4 ). We determine these parameters, from a subset of the corpus as follows. We remove the 500 manually aligned sentences from the training corpus. We train the Alignment Classification Block algorithm. The reference set is the sorce set modified. The modification consists in reordering the pair of words which swap in the order of the target language. Then, given a swapping of two words, it can be a Success ( S ) if the reference alignment is swapped (e.g. discurso interesante is swapped to interesante discurso and the reference is interesting speech ), or a Failure ( F ) if the reference alignment is not swapped (e.g. gran discurso is swapped to discurso gran but the reference is not big speech ). Combining these two sources of information, we use the Simplex algorithm to minimise the following: We have chosen the cost function Q as a coherent criterion to optimize the number of successes ( N S ) and minimize the number of failures ( N F ). The cost function Q has as argument two quantified variables, and its X  output is a difference between two integers. Therefore the gradient based optimization techniques are not feasible. Note that the underlying problem is a multi-objective optimization, which we have transformed to a simple optimization problem by giving equal importance to the two objectives; i.e. Successes and -Failures . For this kind of problems, direct search techniques such as the Simplex algorithm are adequate.

Figure 3 shows the relation between the two objectives, which gives a curve similar to the ROC curve used in detection theory. An increase in the success rate increases the failure rate in ROC, therefore there is a trade-off between two with the curve, which corresponds to a trade-off that gives the same weight to both objectives. The maximum Q corresponds to the curve of lemmas reordering plus tags. Given the optimum values of N min and P swap , we have also studied the number of good generalizations, i.e. the pairs of words which have been swapped correctly and were not seen swapped during training. Almost half of the Successes are generalizations.

We see that the generalization groups Gg n (see Sect. 2.3 ) help more than deteriorates. The generalizations provide reorderings that would not be done neither by the Phrase-based system nor by the Ngram-based system, and it helps both the realignment and the translation. The word alignment gives priority to the monotonic order, therefore alignment links become more robust. In addition, the translation system can extract and use smaller units if the alignment is more monotonic. Some pairs in Gg n have no sense from the syntactical or semantical point of view; but as phase or in the systems. Only a bad generalization appearing exclusively in the test set will not favour the translation quality. 4.2.2 Reordering experiments in the EuroParl Es2En task Given N min and P swap (5 and 0.33, respectively) which gave the smallest Q , we built the Alignment Block Classes. Before applying the algorithm, we added morpho-logical information: lemmas and tags. We added the two informations sequentially: firstly, we used the lemma alignment to build the LAB and secondly, we removed from the list those pairs of blocks which were not constituted by noun plus adjective. Afterwards we built the Alignment Block Classes (also called RABC) and finished the reordering process.

In order to evaluate the alignment, after the second step, we undid the reorder without changing the alignment links obtained. Table 3 shows the improvement in AER.
Table 4 shows the improvement in both measures mWER and BLEU. Three systems are compared: the baseline system without any reordering; the baseline system swapping only the Noun ? Adj (which is an standard Spanish-English reordering linguistic rule); the baseline system using the RABC technique. The last one shows to outperform in WER and BLEU the first two. Although most reorderings are due to the Noun ? Adj reordering, the RABC is more accurate than the simple rule of swapping them because it is able to statistically learn the pair of Noun ? Adj which are reordered, discriminate those that are not reordered and also add new reorderings of different words (i.e. Adv ? Adj ).

If we compare the performance of the RABC algorithm in both SMT systems, the quality of translation is improved in both cases. However, in the Phrase-based system the RAC seems to perform slightly better. Analysing the errors, we see that in some cases the Ngram-based system has already performed a solution for the type of  X  X  X nknown X  X  words. The words which appear  X  X  X mbedded X  X  inside a tuple are solved in (Marin  X  o et al. 2006 ).

The results show that reordering of words can be learnt using a word classification based on the alignment. See some comparison examples between the baseline reordering and the proposed method in Fig. 4 . 5 Conclusions In this paper, we have introduced a local reordering approach based on Alignment Block Classes which benefits alignment and translation quality. When dealing with local reorderings, we can infer better reorderings than the ones provided only by the translation units (both phrases or tuples).

In the EuroParl task, the alignment block classes (blocks of length 1) have been shown useful both in alignment and in translation. Actually, results show a better performance in several parts of the translation process. 1. The proposed reordering improves the alignment itself because it monotonizes 2. The Alignment Block Classification infers better local reorderings than the ones 3. The Alignment Block Classification infers better local reorderings than the ones 4. Both measures, mWER and BLEU, improve almost over 1 point (in percentage).
Further improvements could be expected when dealing with longer blocks and experimenting with less monotonic pairs of languages. We leave as further research the following points: 1. Evaluate the system with tasks with greater reordering difficulties such as the 2. Deal with the inference of rules based on a wider context, in case morphological References
