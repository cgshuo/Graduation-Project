 1. Introduction
This work introduces a stochastically optimized, granular k-nearest neighbor (knn) classi fi er based on INs, where IN stands for Intervals' Number.

An IN is a mathematical object, which may represent either a
INs have been studied in a series of publications. In particular, as explained in Kaburlasos et al. (2013a) ,ithasbeenshownthattheset
F of INs is a metric lattice with cardinality  X  1 ,where  X  1 cardinality of the set R of real numbers; moreover, F 1 is a cone in a linear space. Note that previous work ( Kaburlasos, 2004; Kaburlasos, 2006 ) has employed the term FIN (i.e., fuzzy interval number) instead term CALFIN, proposed previously for an algorithm which induces a
FIN from a population of measurements, was later replaced by the term CALCIN ( Papadakis and Kaburlasos, 2010 ). Recall that an IN computed by algorithm CALCIN retains all-order data statistics as well as the rich potential of INs, especially in industrial applica-tions, has been demonstrated ( Kaburlasos and Kehagias, 2014; Kaburlasos and Pachidis, 2014; Papadakis and Kaburlasos, 2010 ).
INshavebeenusedinanarrayofcomputationalintelligence applications regarding clustering, classi fi cation and regression swarm based meta-heuristic search algorithm based on the New-tonian laws of gravity. The GSA has already been successfully applied to numerous problems ( Bahrololoum et al., 2012; Li and
Zhou, 2011; Rashedi et al., 2011; Rebollo-Ruiz and Gra X a, 2012, 2013; Rebollo et al., 2012; Sarafrazi and Nezamabadi-pour, 2013;
Taghipour et al., 2010; Yin et al., 2011 ). This work proposes a synergy of GSA with the INknn classi fi er toward improving the capacity of the latter classi fi er. Hence, the (granular) gsaINknn classi fi er emerges whose capacity is demonstrated here compara-tively on 12 benchmark datasets.

In a more general context, the proposed gsaINknn classi fi scheme of the emerging lattice computing (LC) paradigm. Note that LC was originally de fi ned as  X  the collection of Computational
Intelligence tools and techniques that either make use of lattice operators inf and sup for the construction of the computational algorithms or exploit lattice theory for language representation and reasoning  X  ( Gra X a, 2009 ). Recent work has extended the meaning of LC as  X  an evolving collection of tools and methodol-ogies that process lattice ordered data per se including logic values, numbers, sets, symbols, graphs, etc.  X  ( Kaburlasos and
Kehagias, 2014; Kaburlasos et al., 2013a, 2013b ). The LC paradigm provides instruments for granular computing, where uncertainty/ ambiguity is accommodated in partially/lattice-ordered informa-tion granules ( Jamshidi and Nezamabadi-pour, 2013; Kaburlasos, 2010; Liu et al., 2013; Sussner and Esmi, 2011 ).

A number of LC models have already been proposed in the context of mathematical morphology. For instance, morphological neural networks (MNN) including both morphological perceptrons and fuzzy morphological asso ciative memories (FMAMs) ( Sussner and Esmi, 2009, 2011; Sussner and Valle, 2006; Valle and Grande and colleagues have employed a FMAM to implement a fuzzy inferencesystembasedonthecompletelatticestructureoftheclass
Furthermore, Gra X a and colleagues have applied LC techniques to image analysis applications of mathematical morphology ( Gra X a a fuzzy lattice , which has been proposed by Nanda toward fuzzifying a partial order relation ( Nanda, 1989 ). Working independently
Kaburlasos and colleagues, inspired from the adaptive resonance theory (ART) for neural computation ( Carpenter et al., 1991, 1992 ), have proposed a number of fuzzy lattice neural networks for clustering and classi fi cation ( Kaburlasos, 2006 ) operating on fuzzy lattice reasoning (FLR) principles. The FLR classi fi er was introduced in
Kaburlasos et al. (2007) for inducing descriptive decision-making knowledge (rules) in a mathematical lattice data domain, including the space R N as a special case; moreover, the FLR classi successfully applied to a variety of problems such as ambient ozone estimation as well as air quality assessment ( Athanasiadis and Kaburlasos, 2006 ). Recent trends in lattice computing appear in Gra X a (2012) , Kaburlasos (2011) , Kaburlasos and Ritter (2007) .
The layout of this paper is as follows. Section 2 outlines the mathematical background. Section 3 presents the INknn classi including an explanatory application example. Section 4 describes the
GSA optimization algorithm. Section 5 details the gsaINknn classi
Section 6 presents comparatively experimental results regarding 12 summarizing our contribution and delineating future work. 2. Mathematical background
This section outlines general lattice notions followed by a hierarchy of lattices ending up to Intervals' Numbers, or INs for short. there follows the complete lattice (J 1  X  J 1 (L)  X  J(L) [ { O }, lattice of Type-1 intervals , or simply intervals , ordered by the set inclusion relation ( D ). The least and greatest element in lattice (J , D ) are denoted by O  X  [ i , o ] and I  X  [ o , i ]  X  L, respectively. Consider the following de fi nition.

De fi nition 1. A Type-1 Intervals ' Number ,or( Type-1 ) IN for short, is a (Type-1) interval function F .: [0,1] -J 1 which satis (1) h 1 r h 2 ) F h 1 + F h 2 and (2) 8 P D [0,1]: \ Let F 1 denote the set of INs. It turns out that  X  F 1 ;  X  complete lattice with order F  X  G 3 ( 8 h A [0,1]: F h D G ( 8 x A L: F ( x ) r G ( x )). We remark that there are two equivalent representations for an IN F , namely the interval-representation F [0,1] -J 1 and the membership-function-representation F (.): R
An IN F is called trivial iff F h is a trivial interval , i.e. F [0,1] and a A L. The following proposition introduces a metric on lattice  X  F 1 ;  X   X  .
 the following integral exists, a metric function d F 1 : F given by d F 1  X  F ; G  X  X 
Given two N-tuple INs F  X  X  F 1 ;:::; F N  X  A F N 1 and G  X  X  G A
F N 1 , a Minkowski metric d p : F N 1 F N 1 -R  X  0 is de d  X  F ; G  X  X  X  X  d F 1  X  F 1 ; G 1  X  X  p  X   X   X  X  d F 1  X  F N ; G
A Type-2 interval is de fi ned as an interval of Type-1 intervals ; moreover, a Type-2 IN is de fi ned as an interval of Type-1 INs ( Kaburlasos et al., 2012, 2013a ). It turns out that the aforemen-tioned Minkowski metric can extend to N-tuple Type-2 INs. de fi ning a piecewise linear, strictly increasing, cumulative real function c: R -R  X  0 such that c ( x i )  X  1/ n | { x j : j i
A {1, ... , n }; where | S | denotes the cardinality of the set S . Let x such that c ( x 0 )  X  0.5. Finally, IN F is de fi ned such that for x
Kaburlasos et al. (2012) (Section 4.1). In practice, the membership hold the corresponding abscissa values and ordinate values, respectively, of the membership function F . In the context of this work, we de fi ne the weight of an IN F as the number of data samples used to induce IN F . 3. The INknn classi fi er
In the following we present an extension, namely INknn, of the conventional knn classi fi er for k  X  1 in the metric lattice  X  F INs (see in The INknn algorithm ).

Fig. 1 (b) displays the Classi fi cation Accuracy regarding the training/testing datasets versus the (sum of) IN weights per experiment. In particular, Fig. 1 (b) shows that accommodating ever more data into the two INs, fi rst, typically keeps the Classi-fi cation Accuracy on the training dataset in the range 50  X  to the intermingling of the two classes and, second, it progres-sively increases Classi fi cation Accuracy on the testing dataset due to an improvement of INknn's capacity for generalization espe-cially as the number of the testing data decreases.

The optimal number of INs per class depends on the data. For instance, were all the data of a class both near each other and apart from the data in any other class then one (N-tuple) IN per class would suf fi ce; otherwise, more than one INs are required per class. The optimal number of INs in a speci fi c application is not known  X  a priori  X  but rather it is to be induced from the data. 4. The gravitational search algorithm This section delineates the Gravitational Search Algorithm, or
GSA for short, that is a population-based stochastic search heur-istic ( Rashedi et al., 2009, 2011 ), where a population member is called agent . An agent represents (part of) the training data with k (N-tuple) INs per class in N data dimensions. The mechanics of GSA is inspired from the Newtonian laws of gravity and motion.
More speci fi cally, a GSA agent is dealt with as a physical object with mass. The position of an agent represents a solution to an optimization problem, whereas an agent's mass represents the corresponding Classi fi cation Accuracy such that larger masses correspond to better solutions. All agents interact with one another with gravity forces resulting in motion. During the life-time of GSA, agents with large masses grow larger and move more slowly toward computing better solutions. The basic equations of the GSA are presented next.

Given a user-de fi ned number N a of agents, we de fi ne the position X i of agent i as
X  X  X  x 1 i ; ... ; x d i ; ... ; x N i  X  ; i  X  1 ; ... ; N a where N is the search space dimension. A normalized mass for agent i is computed as follows:
M  X  t  X  X  fit i  X  t  X  worst  X  t  X   X  N optimization problem is the minimum (resp. maximum) fi tness of all agents.

We compute the next position x d i  X  t  X  1  X  of agent i A dimension d at time t  X  1 as follows. First, we use Eq. (4) to the set  X  Kbest  X  ; second, we compute the corresponding accelera-tion a d i  X  t  X  by Eq. (5) ; third, we compute the velocity v d time t  X  1 as a fraction of the velocity at time t increased by the acceleration at time t according to Eq. (6) ; fi nally, we compute  X  t  X  1  X  using Eq. (7) .

F  X  t  X  X   X  a  X  t  X  X  F v  X  t  X  1  X  X  X  rand i  X  v d i  X  t  X  X  a d i  X  t  X  X  6  X  x  X  t  X  1  X  X  x d i  X  t  X  X  v d i  X  t  X  1  X  X  7  X  where 10. Update the velocity using Eq. (6) ; 11. Update the next position using Eq. (7) ; 12. end for 13. end while 14. Best solution found. 6. Experiments and results
This section demonstrates the application of the gsaINknn ture on 11 benchmark datasets from the University of California Irvine repository of machine learning databases ( Frank and
Asuncion, 2010 ). In addition, we used Ripley's benchmark dataset from Web site http://www.stats.ox.ac.uk/pub/PRNN/ . 6.1. Benchmark datasets
Table 1 displays selected characteristics of the 12 benchmark datasets used in this work. In particular, note that the original Ecoli benchmark dataset includes 336 instances partitioned in 8 classes.
Nevertheless, since 3 of the classes contain only 2, 2 and 5 instances, we decided to omit them. Hence, we employed a modi fi ed Ecoli dataset with 5 classes including 327 instances.
Furthermore, since one of the 10 classes of the original Yeast dataset includes only 5 instances, we decided to omit it. In conclusion, we employed a modi fi ed Yeast benchmark dataset with 9 classes including 1479 instances. 6.2. Data preprocessing and classi fi er initialization
In a data preprocessing step, N -dimensional instances, where N is the corresponding number of attributes/dimensions, were normalized in the unit hypercube [0,1] N by replacing an attribute value x by the normalized value x norm  X  X  x x min  X  =  X  x where x min and x max stand for the corresponding minimum and maximum attribute values, respectively.

We employed a number of alternative classi fi ers from the literature including the GSA ( Bahrololoum et al., 2012 ), fuzzy-ART ( Carpenter et al., 1991 ), (conventional) knn ( Cunningham and
Delany, 2007 ), SOM ( Kohonen and Somervuo, 2002 ), INknn ( Pachidis and Kaburlasos, 2012 ), GRNN ( Specht, 1991 ) and Support Vector Machine (SVM) ( Vapnik, 1999 ) all implemented in the
C  X  X  programming language. For a fair comparison we used identical datasets for training/testing. When a training/testing dataset was not given explicitly for a benchmark dataset, we engaged a random 70% of the instances for training; whereas the remaining instances were used for testing. Care was taken so that
Both Tables 2 and 3 demonstrate the well-known fact that there is no  X  universally optimal classi fi er  X  , in the sense that a classi fi er may perform well on some datasets and poorly on other ones ( De Falco et al., 2007 ). On the one hand, Table 2 demonstrates the very good accuracy of both GRNN and knn classi fi ers: their ranking is either 1 or 2. Nevertheless, both GRNN and knn need to store the entire training dataset. Both classi fi ers SOM and fuzzy-
ART have demonstrated a moderate performance. The INknn classi fi er typically achieved one of the worst rankings on most benchmark datasets. On the other hand, Table 3 demonstrates that the GSA and/or gsaINknn classi fi ers achieved some of the best recognition results on the testing datasets. Table 2 and especially
Table 3 demonstrate the typically poor performance of the SVM classi fi er.

Table 4 summarizes the overall (average) classi fi cation accuracy of every classi fi er over the 12 benchmark classi fi cation datasets considered in this work. More speci fi cally, each cell in Table 4 displays the average of a column of either Tables 2 or 3 regarding the training datasets and the testing datasets, respectively. In addition, each cell in Table 4 displays (within parentheses) the corresponding overall ranking of each classi fi er. Table 4 indicates that the knn classi fi er has demonstrated the best overall accuracy (ranking 1) on the training datasets followed by the GRNN (ranking 2) and the GSA (ranking 3) classi fi ers. The INknn classi has demonstrated the worst overall accuracy (ranking 8) on the classi fi er regarding three different benchmark datasets. More speci fi cally, different initial values of the parameters N of agents), IN weight and G 0 (gravitational constant), shown in columns two, three and four of Table 6 , were employed resulting in the train and test classi fi cation accuracies (%) shown in the next two columns of Table 6 , respectively. The last column of Table 6 displays the corresponding CPU processing times (in ms). Our experience with different parameter initializations for the gsaINknn classi fi er suggested that the best training/testing classi-fi cation accuracies were typically obtained for smaller  X  IN weight values (hence for larger number N a of agents), whereas the initial the INknn classi fi er is fairly fast. Nevertheless, both the GSA and gsaINknn classi fi ers require substantially more time because they carry out stochastic search. More speci fi cally, the number of agents employed by either classi fi er GSA or gsaINknn increases in proportion to the number of instances in a benchmark dataset thus leading to longer training times. Typically, the GSA is faster than the gsaINknn because the GSA does not involve INs. However, the gsaINknn may reduce the number of its INs, hence it may achieve smaller processing times. For example, for the
Credit Approval dataset in Table 7 , note that the GSA and gsaINknn classi fi ers have employed 240 and 10 agents resulting in proces-sing times of 1,066,730 [ms] and 447,616 [ms], respectively.
Another point of interest regards the effectiveness of the GSA method. Note that alternative stochastic search and optimization methods, namely genetic algorithms, have been employed with an
INknn classi fi er, and similar classi fi cation improvements have been reported. More speci fi cally, genetic algorithms have improved the performance of INknn classi fi ers by estimating optimally the parameters of both positive valuation functions and dual isomorphic functions ( Kaburlasos and Papadakis, 2006; Tsoukalas et al., 2013 ) in classi fi cation experiments regarding the
Iris and the Wine benchmark datasets. We point out that any differences between the classi fi cation results reported in
Kaburlasos and Papadakis (2006) , Tsoukalas et al. (2013) and the corresponding results reported here (regarding the Iris and the
Wine benchmark datasets) are not statistically signi fi cant. Never-theless, a detailed comparison of alternative optimization methods for the INknn classi fi er is a topic for future research.
In conclusion, INs have demonstrated here comparatively a signi fi cant potential in classi fi cation applications, which (poten-tial) is attributed to the fact that an IN can represent all-order data statistics ( Kaburlasos et al., 2013a ). In addition, the computational experiments in this work have demonstrated that INs without optimization result in a rather poor classi fi cation accuracy as shown in both Tables 4 and 5 for the INknn classi fi er; whereas, an optimization (pursued here by the GSA) of the INknn classi has signi fi cantly improved the classi fi cation accuracy. 7. Conclusion
This work has presented an optimized, granular knn classi (with k  X  1) in the metric lattice of Intervals' Numbers (INs).
Optimization was justi fi ed on the grounds of improving the classi fi cation results. Since there are no analytic optimization methods currently available in the metric lattice of INs, we resorted to stochastic optimization techniques. In particular, this paper introduced a synergy, namely gsaINknn, of the gravitational search algorithm (gsa) for stochastic optimization with the INknn classi fi er. An application on 12 benchmark classi fi cation datasets from the literature has demonstrated that the gsaINknn classi achieved better results than either the GSA or the INknn classi alone. Moreover, our experimental results have demonstrated that the proposed gsaINknn classi fi er compares favorably with alter-native classi fi ers from the literature.

Potential future work includes the consideration of more than one (N-tuple) IN per class as well as the employment of alternative tunable non-linear functions v ( x ) and  X  ( x ) rather than the func-tions v ( x )  X  x and  X  ( x )  X  1 x employed in this work. Alternative optimization methods to GSA will also be considered compara-tively in a future work. The IN inputs to the gsaINknn classi in this work have been trivial. However, Proposition 1 has paved the way for accommodating non-trivial IN inputs toward dealing with input uncertainty and/or ambiguity. Further future work extensions include the employment of Type-2 INs toward Comput-ing With Words ( Mendel, 2007 ).

