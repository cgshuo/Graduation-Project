 In the context of civil rights law, discrimination refers to unfair or unequal treatment of people based on member-ship to a category or a minority, without regard to individ-ual merit. Rules extracted from databases by data mining techniques, such as classification or association rules, wh en used for decision tasks such as benefit or credit approval, can be discriminatory in the above sense. In this paper, the notion of discriminatory classification rules is introd uced and studied. Providing a guarantee of non-discrimination i s shown to be a non trivial task. A na  X   X ve approach, like tak-ing away all discriminatory attributes, is shown to be not enough when other background knowledge is available. Our approach leads to a precise formulation of the redlining pro b-lem along with a formal result relating discriminatory rule s with apparently safe ones by means of background knowl-edge. An empirical assessment of the results on the German credit dataset is also provided.
 H.2.8 [ Database Applications ]: Data Mining Algorithms, Economics, Legal Aspects Discrimination, Classification Rules
The word discrimination originates from the Latin dis-criminare , which means to  X  X istinguish between X . In social sense, however, discrimination refers specifically to an ac tion based on prejudice resulting in unfair treatment of people on the basis of their membership to a category, without re-gard to individual merit. As an example, U.S. federal laws [17] prohibit discrimination on the basis of race, color, re -ligion, nationality, sex, marital status, age and pregnanc y in a number of settings, including: credit/insurance scori ng (Equal Credit Opportunity Act); sale, rental, and financing of housing (Fair Housing Act); personnel selection and wage (Intentional Employment Act, Equal Pay Act, Pregnancy Discrimination Act).

Concerning the research side, the issue of discrimination in credit, mortgage, insurance, labor market, education an d other human activities has attracted much interest of re-searchers in economics and human sciences since late  X 50s, when a theory on the economics of discrimination was pro-posed [3]. The literature has given evidence of unfair treat -ment in racial profiling and redlining [14], mortgage dis-crimination [9], personnel selection discrimination [6, 7 ], and wages discrimination [8].

In data mining and machine learning, classification models are constructed on the basis of historical data exactly with the purpose of discrimination in the original Latin sense: i .e. distinguishing between elements of different classes, in or der to unveil the reasons of class membership, or to predict it for unclassified samples. In either cases, classification mo d-els can be adopted as a support to decision making, clearly also in socially sensitive tasks such as the access of applic ants to benefits, to public services, to credit. Now the question that naturally arises is the following. While classificatio n models used for decision support can potentially guarantee less arbitrary decisions, can they be discriminating in the so-cial, negative sense? The answer is clearly yes: it is eviden t that relying on mined models for decision making does not put ourselves on the safe side. Rather dangerously, learn-ing from historical data may mean to discover traditional prejudices that are endemic in reality, and to assign to such practices the status of general rules, maybe unconsciously , as these rules can be deeply hidden within a classifier.
Surprisingly, despite the risk of discrimination poses cle ar ethical and legal obstacles to the practical application of data mining in socially sensitive decision making, to the be st of our knowledge, there is no prior work on the issue. In this paper, we tackle the problem of discrimination in data mining in a rule-based setting, by introducing the notion of discriminatory classification rules , as a criterion to identify the potential risks of discrimination.
The first natural approach to formally tackle the prob-lem is to specify a set of selected attribute values (or, at an extreme, an attribute as a whole) as potentially discrim-inatory : examples include female gender, ethnic minority, low-level job, specific age range. However, this simple ap-proach is flawed, in that discrimination may be the result of several joint characteristics that are not discriminato ry in isolation. For instance, black cats crossing your path are typically discriminated as signs of bad luck, but no su-perstition is independently associated to being a cat, bein g black or crossing a path. In other words, the condition that describes a (minority) population that may be the object of discrimination should be stated as a conjunction of at-tributes values: pregnant women, minority ethnicity in dis -advantaged neighborhoods, senior people in weak economic conditions, and so on. Coherently, we qualify as potentiall y discriminatory (PD) some selected itemsets, not necessar-ily single items nor whole attributes. Two consequences of this approach should be considered. First, single PD items or attributes are just a particular case in this more general setting. Second, PD itemsets are closed under intersection : the conjunction of two PD itemsets is a PD itemset as well, coherently with the intuition that the intersection of two disadvantaged minorities is a, possibly empty, smaller (ev en more disadvantaged) minority as well. In our approach, we assume that the analyst interested in studying discrimina-tion compiles a list of PD itemsets with reference to attribu te and attribute values that are present either in the data, or in his/her background knowledge, or in both.

Discrimination has been identified in law and social study literature as either direct or indirect (sometimes called sys-tematic). Direct discrimination consists of rules or proce -dures that explicitly impose  X  X isproportionate burdens X  o n minority or disadvantaged groups. Indirect discriminatio n consists of rules or procedures that, while not explicitly mentioning discriminatory attributes, intentionally or n ot impose the same disproportionate burdens.

Direct discrimination is modelled through potentially dis-criminatory rules , which are classification rules A , B  X  C that contain potentially discriminatory itemsets A in their premises. We show in Sect. 4.1 that there is always a unique split of the premise into a PD part and a non PD part. A PD rule does not necessarily provide evidence of discrimi-natory actions. In order to measure the  X  X isproportionate burdens X  that a rule imposes, the notion of  X  -protection is introduced as a measure of the discrimination power of a PD classification rule. The idea is to define such a measure as the relative gain in confidence of the rule due to the presence of the discriminatory itemsets. The  X  parameter is the key for tuning the desired level of protection against discrimi na-tion. PD classification rules are extracted (see Fig. 1 left) from a dataset containing discriminatory itemsets. This is the case, for instance, when: Figure 1: Modelling the process of direct (left) and indirect (right) discrimination control.
Concerning indirect discrimination, we consider rules D , B  X  C that are potentially non-discriminatory (PND), i.e., that do not contain PD itemsets. They are extracted (see Fig. 1 right) from a dataset which may or may NOT contain PD itemsets. While apparently safe, PND rules may lead to discrimination as well. As an example, assume that the PND rule  X  X arely give credit to persons from neighborhood 10451 from NYC X  is extracted. This may be or may be not a redlining rule. In order to unveil its nature, we have to rely on additional background knowledge . If we know that in NYC people from neighborhood 10451 are in majority black race, then using the rule above is like using the rule  X  X arely give credit to black-race persons from neighborhoo d 10451 of NYC X , which is definitely discriminatory. This use case resembles the situation described in privacy-preserv ing data mining [2, 15], where an anonymized dataset coupled with external knowledge might allow for the inference of the identity of individuals. In our framework, we assume that background knowledge takes the form of association rules relating a non-discriminatory itemset D to a discriminatory itemset A within the context B . Examples of background knowledge include the one originating from publicly avail-able data (e.g., census data), from privately owned data (e.g., market surveys) or from experts or common sense (e.g. , expert rules about customer behavior). Again, internal au-ditors, regulation authorities, consumer advisory counci ls, and data miners are interested for their own purposes in checking indirect discrimination by identifying PND rules that are to a certain extent equivalent to discriminatory PD rules. In order to model such a situation, we consider an inference model , i.e., a strategy that an analyst, provided with background knowledge, can pursue in order to unveil discriminatory PD rules starting from PND ones.

As an example of the overall processes shown in Fig. 1, consider the rules: a. city=NYC b. race=black, city=NYC Rule (a) can be translated into the statement  X  X eople who live in NYC are assigned the bad credit class X  25% of times. Rule (b) concentrates on  X  X lack people from NYC X . In this case, the additional (discriminatory) item in the premise increases the confidence of the rule up to 75%!  X  -protection is intended to detect rules where such an increase is lower than a fixed threshold  X  .

In direct discrimination, rules such as (a) and (b) above are extracted from the dataset and then  X  -protection can be easily checked (see Fig. 1 left). For instance, if the thresh old for acceptable  X  -protection has been fixed to 3, rule (b) is classified as discriminatory. Tackling indirect discrimin ation is more challenging. Continuing the example, consider the classification rule: c. neighborhood=10451, city=NYC extracted from a dataset where potentially discriminatory itemsets, such as race=black , are NOT present (see Fig. 1 right). Taken in isolation, rule (c) cannot be considered discriminatory or not. Assume now to know that people from neighborhood 10451 are in majority black, i.e., the following association rule holds: d. neighborhood=10451, city=NYC Despite rule (c) contains no discriminatory item, it leads to the (discriminatory) decision of denying credit to a minori ty sub-group (black people) which has been  X  X edlined X  by their ZIP code. In other words, the PD rule: e. race=black, neighborhood=10451, city=NYC can be inferred from (c) and (d) , together with a lower bound of 94% for its confidence. Such a lower bound shows a disproportionate burden (94% / 25%, i.e., 3.7 times) over black people living in neighborhood 10451. We will show a formal theorem that allows us to derive the lower bound for  X  -protection of (e) starting from PND rules (a) and (c) and a lower bound on the confidence of the background rule (d) . Clearly, the proposed inference model provides sufficient conditions for checking indirect discrimination. If the inferred lower bound is not as high as to conclude non  X  -protection, we cannot state that an analyst has no other means to derive the same conclusion, e.g., by using another inference model or additional background knowledge. Throughout the paper, we illustrate the notions introduced by analysing the public domain German credit dataset [11], consisting of 1000 transactions representing the good/bad credit class of bank account holders. The dataset include nominal (or discretized) attributes on personal properties : checking account status, duration, savings status, proper ty magnitude, type of housing; on past/current credits and re-quested credit : credit history, credit request purpose, credit request amount, installment commitment, existing credits , other parties, other payment plan; on employment status : job type, employment since, number of dependents, own telephone; and on personal attributes : personal status and gender, age, resident since, foreign worker.
We recall the notions of itemsets, association rules and classification rules from standard definitions [1, 10, 18]. L et R be a relation with attributes a 1 , . . . , a n . A class attribute is a fixed attribute c of the relation. An a -item is an ex-pression a = v , where a is an attribute and v  X  dom ( a ), the domain of a . We assume that dom ( a ) is finite for every attribute a . A c -item is called a class item. An item is any a -item. Let I be the set of all items. A transaction is a subset of I , with exactly one a -item for every attribute a . A database of transactions, denoted by D , is a set of trans-actions. An itemset X is a subset of I . We denote by 2 I the set of all itemsets. As usual in the literature, we write X , Y for X  X  Y . For a transaction T , we say that T verifies X if X  X  T . The support of an itemset X w.r.t. a non-empty transaction database D is the ratio of transactions in D verifying X : supp D ( X ) = |{ T  X  D | X  X  T }| / |D| , where | | is the cardinality operator. An association rule is an expression X  X  Y , where X and Y are itemsets. X is called the premise (or the body ) and Y is called the con-sequence (or the head ) of the association rule. We say that X  X  Y is a classification rule if Y is a class item and X contains no class item. We refer the reader to [10, 18] for a discussion of the integration of classification and associa tion rule mining. The support of X  X  Y w.r.t. D is defined as: supp D ( X  X  Y ) = supp D ( X , Y ). The confidence of X  X  Y , defined when supp D ( X ) &gt; 0, is: Support and confidence range over [0 , 1]. We omit the sub-scripts in supp D () and conf D () when clear from the context. Since the seminal paper by Agrawal and Srikant [1], a num-ber of well explored algorithms [5] have been introduced in order to extract frequent itemsets, i.e. itemsets with a speci-fied minimum support, and valid association rules, i.e. rule s with a specified minimum confidence. We introduce a key concept for our purposes.

Definition 3.1. [Extended lift] Let A , B  X  C be an as-sociation rule such that conf ( B  X  C ) &gt; 0 . We define the extended lift of the rule with respect to B as: We call B the context, and B  X  C the base-rule.

Intuitively, the extended lift expresses the relative vari a-tion of confidence due to the addition of the extra itemset A in the premise of the base rule B  X  C . In general, the ex-tended lift ranges over [0 ,  X  [. However, if association rules with a minimum support ms &gt; 0 are considered, it ranges over [0 , 1 /ms ]. Similarly, if association rules with base-rules with a minimum confidence mc &gt; 0 are considered, it ranges over [0 , 1 /mc ]. The extended lift can be traced back to the well-known measure of lift [16], defined as: when B = { T  X  X | B  X  T } . When B is empty, the extended lift reduces to the standard lift.
Our starting point consists of flagging at syntax level those itemsets which might potentially lead to discrimination in the sense explained in the introduction. A set of itemsets I X  2 I is downward closed if when A 1  X  X  and A 2  X  X  then A 1 , A 2  X  X  .

Definition 4.1. [PD/PND itemset] A set of potentially discriminatory (PD) itemsets I d is any downward closed set. Itemsets in 2 I \I d are called potentially non-discriminatory (PND).

Any itemset X can be uniquely split into a PD part A and a PND part B = X \ A by setting A to the largest subset of X that belongs to I d 1 . A simple way of defining PD itemsets is to take those that are built from a pre-defined set of items, i.e., to reduce to the case where the granularit y of discrimination is at the level of items.

Example 4.2. For the German credit dataset, we fix I d = 2 I d , where I d is the set of the following (discriminatory) items: personal_status=female div/sep/mar (female and not single), age=(52.6-inf) (senior people), job=unemp/-unskilled non res (unskilled or unemployed non-resident), and foreign_worker=yes (foreign workers). Notice that the PD part of an itemset X is now easily identifiable as X  X  I and the PND part as X \ I d .

It is worth noting that discriminatory items do not neces-sarily coincide with sensitive attributes with respect to p ure privacy protection. For instance, gender is generally cons id-ered a non-sensitive attribute, whereas it can be discrimin a-tory in many decision contexts. Moreover, note that we use the adjective potentially both for PD and PND itemsets. As we will discuss later on, also PND may unveil (indirect) dis-crimination. The notion of potential (non-)discriminatio n is now extended to classification rules.

Definition 4.3. [PD/PND classification rule] A classi-fication rule X  X  C is potentially discriminatory (PD) if X = A , B with A non-empty PD itemset and B PND item-set. It is potentially non-discriminatory (PND) if X is a PND itemset.
 It is worth noting that PD rules can be either extracted from a dataset that contain PD itemsets or inferred as shown in Fig. 1 right. PND rules can be extracted from a dataset which may or may not contain PD itemsets.

Example 4.4. Consider Ex. 4.2, and the rules: a. personal_status=female div/sep/mar b. savings_status=no known savings (a) is a PD rule since its premise contains an item belonging to I d . On the contrary, (b) is a PND rule. Notice that (b) is the base rule of (a) if we consider as context the PND part of its premise. 1 Notice that A is univocally defined. If there were two max-imal A 1 6 = A 2 subsets belonging to I d , then A 1 , A 2 belong to I d as well since I d is downward closed. But then A 1 or A 2 would not be maximal.
We start concentrating on PD classification rules as the potential source of discrimination. In order to capture the idea of when a PD rule may lead to discrimination, we in-troduce the key concept of  X  -protective classification rules. Definition 4.5. [  X  -protection] Let c = A , B  X  C be a PD classification rule, where A is a PD and B is a PND itemset, and let:  X  = conf ( A , B  X  C )  X  = conf ( B  X  C ) &gt; 0 . For a given threshold  X   X  0 , we say that c is  X  -protective if elift (  X ,  X  ) &lt;  X  , where: c is called  X  -discriminatory if elift (  X ,  X  )  X   X  .
Intuitively, the definition assumes that the extended lift o f c w.r.t. B is a measure of the degree of discrimination of A in the context B .  X  -protection states that the added (poten-tially discriminatory) information A increases the confidence of concluding an assertion C under the base hypothesis B only by an acceptable factor, bounded by  X  .

Example 4.6. Consider again Ex. 4.2. Fix  X  = 3 and consider the classification rules: a. personal_status=female div/sep/mar b. age=(52.6-inf) Rule (a) can be translated as follows: if we know nothing about the savings of a person asking for credit, then assign bad credit class (or bad credit class has been assigned in pas t) to non-single women 52% more than the average. The sup-port of the rule is 1.3%, its confidence 27%, and its extended lift 1.52. Hence, the rule is  X  -protective. Also, the confi-dence of the base rule: is 0 . 27 / 1 . 52 = 17 . 8% . Rule (b) states that senior non-single women that want to buy a used car are assigned the bad credit class with a probability more than 6 times higher than the average one for those that ask credit for the same purpose. The support of the rule is 0.3%, its confidence 100%, and its extended lift 6.06. Hence the rule is  X  -discriminatory. Finally, the confidence of the base rule is 1 / 6 . 06 = 16 . 5% .
When the class is binary, the concept of  X  -protection must be strengthened, as highlighted by the next example.
Example 4.7. The following PD classification rule is ex-tracted from the German credit dataset with minimum sup-port of 1%: ExtractCR() CheckAlphaPDCR(  X  ) a-good. personal_status=female div/sep/mar Rule a-good has an extended lift of 0 . 88 . Intuitively, this means that good credit class is assigned to non-single women less than the average of people that want to buy an used car and have no checking status. As a consequence, one can deduce that the bad credit class is assigned more than the average of people in the same context, i.e. the rule: a-bad. personal_status=female div/sep/mar It is worth noting that the confidence of rule a-bad in the ex-ample is equal to 1 minus the confidence of a-good , and the same holds for the confidence of base rules. This property holds in general for binary classes. For a binary attribute a with dom ( a ) = { v 1 , v 2 } , we write  X  ( a = v 1 ) for a = v  X  ( a = v 2 ) for a = v 1 .
 Lemma 4.8. Assume that the class attribute is binary. Let A , B  X  C be a classification rule, and let:  X  = conf ( A , B  X  C )  X  = conf ( B  X  C ) &lt; 1 , We have that conf ( B  X   X  C ) &gt; 0 and: As an immediate consequence, the extraction or the infer-ence of an  X  -protective rule A , B  X  C allows the calcula-tion of the extended lift of the dual rule A , B  X   X  C , which could be  X  -discriminatory. We strengthen the notion of  X  -protection to take into account such an implication.
Definition 4.9. [Strong  X  -protection] Let c = A , B  X  C be a PD classification rule, where A is a PD and B is a PND itemset, and let:  X  = conf ( A , B  X  C )  X  = conf ( B  X  C ) &gt; 0 . For a given threshold  X   X  1 , we say that c is strongly  X  -protective if glift (  X ,  X  ) &lt;  X  , where: If glift (  X ,  X  )  X   X  , we say that c is strongly  X  -discriminatory.
The glift () function ranges over [1 ,  X  [. If classification rules with a minimum support ms &gt; 0 are considered, it ranges over [1 , 1 /ms ]. Moreover, for 1 &gt;  X  &gt; 0:
Let us consider the case of direct discrimination, as mod-elled in Fig. 1 left and with  X  -protection as the underlying measure of discrimination. Given a set of PD classification rules A and a threshold  X  , the problem of checking (strong)  X  -protection consists of finding the largest subset of A con-taining only (strong)  X  -protective rules. This problem is solvable by directly checking the inequality of Def. 4.5 (re sp., Def. 4.9), provided that the elements of the inequality are available. We define a checking algorithm that starts from the set of frequent itemsets, namely itemsets with a given minimum support. This is the output of any of the sev-eral frequent itemset extraction algorithms available at t he FIMI repository [5]. The algorithm is reported in Fig. 2. On the left hand side of the figure, the extraction of PD and PND classification rules is reported. It requires a singl e scan of frequent itemsets ordered by the itemset size k . For k -frequent itemsets that include a class item, a single clas-sification rule is produced in output. The confidence of the rule can be computed by looking only at itemsets of length k  X  1. The rules in output are distinguished between PD and PND rules, based on the presence of discriminatory items in minimum confidence for base rules. setting minimum confidence for base rules. their premises. Moreover, the rules are grouped on the basis of the size group of the PND part of the premise. The out-put is a collection of PD rules PD group and a collection of PND rules PND group . On the right hand side of Fig. 2, the extended lift of a classification rule A , B  X  C  X  PD group is computed from its confidence and the confidence of the base rule B  X  C  X  X ND group .
 The left-hand side of Fig. 3 (resp., Fig. 4) shows the distri-bution of  X  -discriminatory PD rules (resp., strong  X  -discri-minatory PD rules) for minimum support of 1%, 0.5% and 0.3%. The figures highlight how lower support values in-crease the number and the proportion of PD rules and the maximum  X  . Notice that, for a same minimum support,  X  reaches higher values in Fig. 4 than in Fig. 3, since strong  X  -discrimination of a rule implicitly takes into account the complementary class rule, which may have a support lower than the minimum (see e.g., (a-bad) in Ex. 4.7). We report two sample PD rules with decreasing support and increasing extended lift. a1. personal_status=female div/sep/mar a2. age=(52.6-inf) Rule a1 states that among the people employed since one to four years, having a real estate property and with skilled job, the status of being woman and not single leads to hav-ing assigned the bad credit class 2.39 times more than the average. The rule has confidence 48%, which means that the base rule has confidence 0 . 48 / 2 . 39 = 20%. Rule a2 reaches a lift of 9 when compared to the base rule: People with large savings are usually given good credit. However, only 2 cases out of 18 (i.e., 11%) are assigned class=bad . Both of them are senior people!
In addition to minimum support, a widely adopted pa-rameter for controlling rule generation is minimum confi-dence. The right-hand side of Fig. 3 shows how the confi-dence threshold of the base rule affects the distribution of  X  -discriminatory PD rules. Lower confidence thresholds lead to fewer number of discriminatory rules and lower maximum extended lift values. This is consistent with the observati on that the extended lift ranges over [0 , 1 /mc ], where mc is the minimum confidence threshold of base rules.

This is not the case for strong  X  -protection, where acting on minimum confidence of the base rule does not turn out to be an effective control mechanism, as shown in Fig. 4 right.
Let us consider the case of indirect discrimination, as mod-elled in Fig. 1 right. The next example highlights a PND rule which leads to discrimination, and the background knowl-edge that allows for unveiling this.

Example 6.1. Consider again the German credit dataset, but assume now that discriminatory items have been removed from it. Also, consider the following itemset: B = credit_history=critical/other existing credit The following PND classification rules can be extracted: dbc. age=(-inf-30.2] , B Rule (dbc) states that young people in the context B of peo-ple with critical credit history, residence since 2.8 years at least, with savings at most for 100 units, and with no check-ings, are assigned the bad credit scoring with a confidence of 16.7%. Rule (bc) is obtained from (dbc) by discarding the item age=(-inf-30.2] in the premise, and it has a confi-dence of 2.7%. As discussed in Sect. 2, without any further information, we cannot say whether rule (dbc) is discrim-inatory or not. Assume now to know (by some background knowledge) that in the context B above, the set of persons satisfying age=(-inf-30.2] is somewhat related to the set of persons satisfying the discriminatory item personal_-status=female div/sep/mar . If the two sets were exactly the same, we could replace age=(-inf-30.2] in rule (dbc) with the discriminatory item. This would lead us to the PD classification rule: abc. personal_status=female div/sep/mar , B with glift (0 . 167 , 0 . 027) = 6 . 19 , which is considerably high.
In case the two sets of persons coincide only to some ex-tent, we can still obtain some lower bound for the glift () of (abc) . In particular, assume that young people in the con-text B , contrarily to the average case, are almost all non-single women: dba. age=(-inf-30.2] , B Is this enough to conclude that non-single women in the con-text are discriminated? We cannot say that: for instance, if non-single women in the context are at 99% older than 30.2 years, the remaining 1% is involved in the decisions fired by rule (dbc) , hence women in the context are not discrimi-nated by these decisions. As a consequence, we need further information about the proportion of non-single women that are younger than 30.2 years. Assume to know that such a proportion is at least 70%, i.e. : abd. personal_status=female div/sep/mar , B By means of the forthcoming Thm. 6.2, we can state that a lower bound for the glift () value of (abc) is 3 . 19 . As a consequence, the rule (abc) is at least 3 . 19 -discriminatory, i.e., non-single women in the context are imposed by (abc) a burden of at least 3 . 19 times than the average of people in the context. Since the German credit dataset contains the discriminatory items, we can calculate the actual glift () value for (abc) , which turns out to be 3 . 37 .

We formalize the intuitions of this example in the next re-sult, which derives a lower bound for  X  -discrimination of PD classification rules given information available in PND rul es (  X  ,  X  ) and information available from background rules (  X   X  ). The non-trivial proof of the theorem (see [12]) relies on the inclusion-exclusion principle for boolean formulas ov er items, and is omitted for lack of space.

Theorem 6.2. Let D , B  X  C be a PND classification rule, and let:  X  = conf ( D , B  X  C )  X  = conf ( B  X  C ) &gt; 0 . Let A be a PD itemset and let  X  1 ,  X  2 such that: Called: glb ( x, y ) = we have: (i) 1  X  f (1  X   X  )  X  conf ( A , B  X  C )  X  f (  X  ) ,. (ii) for  X   X  0 , if elb (  X ,  X  )  X   X  , the PD classification rule (iii) for  X   X  1 , if glb (  X ,  X  )  X   X  , the PD classification rule It is worth noting that  X  1 and  X  2 are lower bounds for the confidence values of A , B  X  D and D , B  X  A respectively. This amounts to stating that the correlation between A and D in context B within the dataset must be known only with some approximation as background knowledge. Moreover, as  X  1 and  X  2 tend to 1, the lower and upper bounds in (i) tend to  X  . Also, f (  X  ) is monotonic w.r.t both  X  1 and  X  an increase of  X  1 leads to a proportional improvement of the precision of lower and upper bounds, while an increase of  X  leads to a more than proportional improvement.

Example 6.3. Reconsider Ex. 6.1. We have  X  = 0 . 167 ,  X  = 0 . 027 ,  X  1 = 0 . 7 , and  X  2 = 0 . 95 . The lower bound for the glift () value of rule abc is computed as follows. Called: = f (0 . 167) / 0 . 027 = 3 . 19 . Recalling the redlining example, an application of Thm. 6.2 allows us to conclude that black people ( race=black ) are dis-criminated in a context ( city=NYC ) because almost all peo-ple living in a certain neighborhood ( neighborhood=10451 ) are black (this is  X  2 ) and almost all black people live in that neighborhood (this is  X  1 ). In general, this is not the case, since black people live in many different neighbor-hoods. Moreover, in the redlining example we had to pro-vide, as background knowledge, only the approximation  X  2 However, notice that the conclusion of the example is slight ly different from the one above, stating that black people who live in a certain neighborhood ( race=black, neighborhood=-10451 ) are discriminated w.r.t. people in the context ( city=-NYC ). Such an inference can be modelled as an instance of Thm. 6.2.

Example 6.4. Rules (a) and (c) from Sect. 2 a. city=NYC c. neighborhood=10451, city=NYC are instances respectively of B  X  C and D , B  X  C in Thm. 6.2, with B = city=NYC , D = neighborhood=10451 and C = class=bad . Hence,  X  = 0 . 95 and  X  = 0 . 25 .
What should be a set of PD itemsets for reasoning about redlining? Certainly, neighborhood=10451 alone cannot be considered discriminatory. However, the pair A = race=bla-ck, neighborhood=10451 might denote a possible discrim-ination against black people in a specific neighborhood. In general, all conjunctions of items of minority races and nei gh-borhoods is a source of potential discrimination. This set o f itemsets is downward closed, albeit not in the form of 2 J for a set of items J . As background knowledge, we can now re-fer to census data, reporting distribution of population ov er the territory. So, we can easily gather statistics such as ru le (d) from Sect. 2, which can be rewritten as: d. neighborhood=10451, city=NYC This is an instance of D , B  X  A in Thm. 6.2. The other expected background rule is A , B  X  D , which readily has confidence 100%, i.e.  X  1 = 1 , since A contains D . So, we have not to take it into account in this redlining example, which therefore represents a simpler inference problem tha n the one considered in Thm. 6.2. By the conclusion of the theorem, we obtain lower bounds for the confidence and the extended lift of A , B  X  C , i.e., rule (e) from Sect. 2: e. race=black, neighborhood=10451, city=NYC and then its extended lift (w.r.t. the context city=NYC ) is at least 0 . 9375 / 0 . 25 = 3 . 75 . Summarizing, the classifica-tion rule (e) is at least 3.75-discriminatory or, in intuitive words, (c) is a redlining rule imposing a  X  X isproportionate burden X  (of 3.75 times than the average of NYC people) over black-race people living in neighborhood 10451.

Given a set of PND classification rules PND and a set of background rules BR , we define the absolute recall at  X  as the number of  X  -discriminatory PD rules that are inferrable by Thm. 6.2. In order to test the proposed inference model, (i) If  X  2 &gt; 1  X   X  or  X  2 &gt;  X  (iii) If  X  2 (1  X   X  X  )  X  1  X   X  (iii) If  X  2 (1  X   X  X  )  X  1  X   X  Figure 5: Algorithm for checking indirect strong  X  -discrimination. Here BR g is { X  X  A  X  X R | | X | = g } . we simulate the availability of a large set of background rul es under the hypothesis that the dataset contains the discrimi -natory items, e.g., as in the German credit case. We define:
BR = { X  X  A | X PND , A PD , supp ( X  X  A )  X  ms } as the set of association rules X  X  A with a given min-imum support. While rules of the form A , B  X  D seem not to be included in the background rule set, we observe that conf ( A , B  X  D ) can be obtained as supp ( D , B  X  A ) /supp ( B  X  A ), where both rules in the ratio are of the required form. Notice that the set BR contains the most precise background rules that an analyst could use, in the sense that the values for  X  1 and  X  2 in Thm. 6.2 do coin-cide with the confidence values they limit. Next, for each candidate rule X  X  C in PND , we have to enumerate all sub-itemsets D , B  X  X (which are 2 | X | ) such that X can be written as D , B . What we will be looking for to speed up the enumeration and checking process is some necessary conditions on the inequalities to be checked that restrict t he search space. Let us start considering necessary condition s for elb (  X ,  X  )  X   X  . If  X  = 0 the expression is always true, so we concentrate on the case  X  &gt; 0. By definition of elb (), which can respectively be rewritten as: Therefore, (i) is a necessary condition for elb (  X ,  X  )  X   X  . From (ii) and  X  1  X  1, we can conclude elb (  X ,  X  )  X   X  only if  X  +  X   X  1  X   X  X  X  2 , i.e.: Therefore, (iii) is a necessary condition for elb (  X ,  X  )  X   X  as well. The selectivity of conditions (i,iii) lies in the fact that checking (iii) involves no lookup at rules A , B  X  D ; and checking (i) involves no lookup at rules B  X  C . Moreover, condition (iii) is monotonic w.r.t  X  2 , hence if we scan as-sociation rules X  X  A ordered by descending confidence, we can stop checking it as soon as it is false. Finally, we observe that similar necessary conditions can be derived fo r glb (  X ,  X  )  X   X  . The generate&amp;test algorithm that incorpo-rates the necessary conditions is shown in Fig. 5. With reference to the presented test framework, Fig. 6 plots the distribution of the absolute recall of the proposed in-ference model by varying  X  and minimum support. Even for high values of  X  , the number of indirectly discriminatory rules is considerably high. We report below the execution times of the CheckAlphaPNDCR() procedure (on a PC with Xeon 2.4Ghz and 2Gb main memory) for rules in PND and BR having minimum support of 1% and without/with the optimizations discussed earlier. The table shows a gain in the execution time up to 69%.
To the best of our knowledge, this paper is the first to address the discrimination problem in data mining models. Nevertheless, discrimination has been recognized as an is-sue in the tutorial [4, Slide 19] where the danger of building classifiers capable of racial discrimination in home loans h as been put forward. Technically, we measured discrimination through a generalization of lift to cope with contexts , spec-ified as non-discriminatory itemsets. In this sense, there i s a relation with the work of [13], where the notion of con-ditional association rules has been studied. A conditional rule A  X  C / B denotes a context B in which itemsets A and C are equivalent, namely where conf ( A , B  X  C ) = 1 and conf (  X  A , B  X   X  C ) = 1.

Summarizing, we have investigated how discrimination may be hidden in data mining models. Our study considered classification rules, which occur in a variety of approaches in-cluding decision trees, rule-based classifiers, and associ ation rule-based classifiers. As the contributions of the paper, w e have modelled both direct and indirect discrimination, in-troduced (strong)  X  -protection as a measure of the discrimi-natory power of a rule, and, as far as indirect discriminatio n is concerned, devised an inference model as a formal result that is able to infer discriminatory rules from apparently safe ones and some background knowledge.
