 The study of semantic similarity between words has been an integral part of natural language processing and information retrieval for many years. Semantic similarity measures are vital for various applications in natural language processing such as word sense disambiguation (Resnik, 1999), language model-ing (Rosenfield, 1996), synonym extraction (Lin, 1998a) and automatic thesaurus extraction (Curran, 2002).
 text corpora have been used in previous work on se-mantic similarity (Lin, 1998a; Resnik, 1995; Jiang and Conrath, 1998; Lin, 1998b). However, seman-tic similarity between words change over time as new senses and associations of words are constantly created. One major issue behind taxonomies and corpora oriented approaches is that they might not necessarily capture similarity between proper names such as named entities (e.g., personal names, loca-tion names, product names) and the new uses of ex-isting words. For example, apple is frequently asso-ciated with computers on the Web but this sense of apple is not listed in the WordNet. Maintaining an up-to-date taxonomy of all the new words and new usages of existing words is costly if not impossible.
The Web can be regarded as a large-scale, dy-namic corpus of text. Regarding the Web as a live corpus has become an active research topic recently. Simple, unsupervised models have shown to per-form better when n -gram counts are obtained from the Web rather than from a large corpus (Keller and Lapata, 2003; Lapata and Keller, 2005). Resnik and Smith (2003) extract bilingual sentences from the Web to create parallel corpora for machine trans-lation. Turney (2001) defines a point wise mutual information (PMI-IR) measure using the number of hits returned by a Web search engine to recognize synonyms. Matsuo et. al, (2006b) follows a similar approach to measure the similarity between words and apply their method in a graph-based word clus-tering algorithm.

Due to the huge number of documents and the high growth rate of the Web, it is difficult to di-rectly analyze each individual document separately. Search engines provide an efficient interface to this vast information. Page counts and snippets are two useful information sources provided by most Web search engines. Page count of a query is the number a brief window of text extracted by a search engine around the query term in a document. Snippets pro-vide useful information about the immediate context of the query term.

This paper proposes a Web-based semantic simi-larity metric which combines page counts and snip-pets using support vector machines. We extract lexico-syntactic patterns from snippets. For exam-ple, X is a Y indicates there is a high semantic sim-ilarity between X and Y . Automatically extracted lexico-syntactic patterns have been successfully em-ployed in various term extraction tasks (Hearst, 1992).

Our contributions are summarized as follows:  X  We propose a lexico-syntactic patterns-based  X  We integrate different Web-based similarity Given a taxonomy of concepts, a straightforward method for calculating similarity between two words (concepts) is to find the length of the shortest path connecting the two words in the taxonomy (Rada et al., 1989). If a word is polysemous (i.e., having more than one sense) then multiple paths may ex-ist between the two words. In such cases only the shortest path between any two senses of the words is considered for the calculation of similarity. A prob-lem frequently acknowledged with this approach is that it relies on the notion that all links in the taxon-omy represent uniform distances.

Resnik (1995) proposes a similarity measure based on information content. He defines the sim-ilarity between two concepts C 1 and C 2 in the tax-onomy as the maximum of the information content of all concepts C that subsume both C 1 and C 2 . Then the similarity between two words are defined as the maximum of the similarity between any con-cepts that the words belong to. He uses WordNet as the taxonomy and information content is calculated using the Brown corpus.

Li et al., (2003) combines structural semantic in-formation from a lexical taxonomy and informa-tion content from a corpus in a non-linear model. They propose a similarity measure that uses shortest path length, depth and local density in a taxonomy. Their experiments using WordNet and the Brown corpus reports a Pearson correlation coefficient of 0 . 8914 on the Miller and Charles X  (1998) bench-mark dataset. They do not evaluate their method on similarities between named entities. Recently, some work has been carried out on measuring semantic similarity using web content. Matsuo et al., (2006a) propose the use of Web hits for the extraction of communities on the Web. They measure the associ-ation between two personal names using the overlap coefficient, calculated based on the number of Web hits for each individual name and their conjunction.
Sahami et al., (2006) measure semantic similarity between two queries using the snippets returned for those queries by a search engine. For each query, they collect snippets from a search engine and rep-resent each snippet as a TF-IDF weighted term vec-tor. Each vector is L 2 normalized and the centroid of the set of vectors is computed. Semantic similar-ity between two queries is then defined as the inner product between the corresponding centroid vectors. They do not compare their similarity measure with taxonomy based similarity measures.

Chen et al., (2006) propose a web-based double-checking model to compute semantic similarity be-tween words. For two words P and Q , they col-lect snippets for each word from a web search en-gine. Then they count the number of occurrences of word P in the snippets for word Q and the number of occurrences of word Q in the snippets for word P . These values are combined non-linearly to com-pute the similarity between P and Q . This method heavily depends on the search engine X  X  ranking al-gorithm. Although two words P and Q may be very similar, there is no reason to believe that one can find Q in the snippets for P , or vice versa. This observa-tion is confirmed by the experimental results in their paper which reports 0 similarity scores for many pairs of words in the Miller and Charles (1998) data set. In this section we will describe the various similarity features we use in our model. We utilize page counts for simple text queries to define various similarity scores. 3.1 Page Counts-based Similarity Scores For the rest of this paper we use the notation H ( P ) to denote the page count for the query P in a search engine. Terra and Clarke (2003) compare various similarity scores for measuring similarity between words in a corpus. We modify the traditional Jac-card, overlap (Simpson), Dice and PMI measures for the purpose of measuring similarity using page counts. WebJaccard coefficient between words (or phrases) P and Q , WebJaccard( P, Q ) , is defined by, Here, P  X  Q denotes the conjunction query P AND Q . Given the scale and noise in the Web, some words might occur arbitrarily, i.e. by random chance, on some pages. Given the scale and noise in web data, it is a possible that two words man order to reduce the adverse effect due to random co-occurrences, we set the WebJaccard coefficient to zero if the page counts for the query P  X  Q is less than a threshold c . 4 Likewise, we define WebOverlap coefficient, WebOverlap( P, Q ) , as,
We define WebDice as a variant of Dice coeffi-cient. WebDice( P, Q ) is defined as,
We define WebPMI as a variant form of PMI using page counts by, Here, N is the number of documents indexed by the search engine. Probabilities in Formula 4 are esti-mated according to the maximum likelihood princi-ple. In order to accurately calculate PMI using For-mula 4, we must know N , the number of documents indexed by the search engine. Although estimating the number of documents indexed by a search en-gine (Bar-Yossef and Gurevich, 2006) is an interest-ing task itself, it is beyond the scope of this work. In this work, we set N = 10 10 according to the number of indexed pages reported by Google. 3.2 Snippets-based Synonymous Word Page counts-based similarity measures do not con-sider the relative distance between P and Q in a page or the length of the page. Although P and Q occur in a page they might not be related at all. Therefore, page counts-based similarity measures are prone to noise and are not reliable when H ( P  X  Q ) is low. On the other hand snippets capture the local context of query words. We propose lexico-syntactic patterns extracted from snippets as a solution to the problems with page counts-based similarity measures.
To illustrate our pattern extraction algorithm con-sider the following snippet from Google for the query jaguar AND cat .  X  X he Jaguar is the largest cat in Western Hemi-sphere and can subdue a larger prey than can the puma X 
Here, the phrase is the largest indicates a hy-pernymic relationship between Jaguar and the cat. Phrases such as also known as, is a, part of, is an ex-ample of all indicate various of semantic relations. Such indicative phrases have been successfully ap-plied in various tasks such as synonym extraction, hyponym extraction (Hearst, 1992) and fact extrac-tion (Pasca et al., 2006).

We describe our pattern extraction algorithm in three steps.
 Step 1 We replace the two query terms in a snippet by two wildcards X and Y . We extract all word n -grams that contain both X and Y . In our experiments we ex-tracted n -grams for n = 2 to 5 . For example, from the previous snippet we extract the pattern, X is the largest X . In order to leverage the pattern extraction process, we randomly select 5000 pairs of synony-mous nouns from WordNet synsets. We ignore the nouns which do not have synonyms in the WordNet. For nouns with more than one sense, we select syn-onyms from its dominant sense. For each pair of synonyms ( P, Q ) , we query Google for  X  P  X  AND  X  Q  X  and download the snippets. Let us call this col-lection of snippets as the positive corpus . We apply the above mentioned n -gram based pattern extrac-tion procedure and count the frequency of each valid pattern in the positive corpus.
 Step 2 Pattern extraction algorithm described in step 1 yields 4 , 562 , 471 unique patterns. 80% of these pat-terns occur less than 10 times in the positive corpus. It is impossible to learn with such a large number of sparse patterns. Moreover, some patterns might oc-cur purely randomly in a snippet and are not good indicators of semantic similarity. To measure the reliability of a pattern as an indicator of semantic similarity we employ the following procedure. We create a set of non-synonymous word-pairs by ran-domly shuffling the words in our data set of synony-mous word-pairs. We check each pair of words in this newly created data set against WordNet and con-firm that they do not belong to any of the synsets in the WordNet. From this procedure we created 5000 non-synonymous pairs of words. For each non-synonymous word-pair, we query Google for the conjunction of its words and download snippets. Let us call this collection of snippets as the nega-tive corpus . For each pattern generated in step 1 , we count its frequency in the negative corpus.
 Step 3 We create a contingency table as shown in Table 1 for each pattern v extracted in step 1 using its fre-quency p v in positive corpus and n v in negative cor-pus. In Table 1, P denotes the total frequency of all patterns in the positive corpus and N denotes that in the negative corpus.

Using the information in Table 1, we calculate  X  2 (Manning and Sch  X  utze, 2002) value for each pat-tern as, We selected the top ranking 200 patterns experimen-tally as described in section 4.2 according to their  X  2 values. Some of the selected patterns are shown in Table 2. 3.3 Training For each pair of synonymous and non-synonymous words in our datasets, we count the frequency of occurrence of the patterns selected in Step 3 . We normalize the frequency count of each pattern by dividing from the total frequency of all patterns. Moreover, we compute the page counts-based fea-tures as given by formulae (1-4). Using the 200 pattern features and the 4 page counts-based fea-tures we create 204 dimensional feature vectors for each training instance in our synonymous and non-synonymous datasets. We train a two class support vector machine (SVM) (Vapnik, 1998), where class +1 represents synonymous word-pairs and class  X  1 represents non-synonymous word-pairs. Finally, SVM outputs are converted to posterior probabilities (Platt, 2000). We consider the posterior probability of a given pair of words belonging to class +1 as the semantic similarity between the two words. To evaluate the performance of the proposed se-mantic similarity measure, we conduct two sets of experiments. Firstly, we compare the similarity scores produced by the proposed measure against the Miller-Charles X  benchmark dataset. We analyze the performance of the proposed measure with the number of snippets and the size of the training data set. Secondly, we apply the proposed measure in a real-world named entity clustering task and measure its performance. 4.1 The Benchmark Dataset We evaluated the proposed method against Miller-Charles (1998) dataset, a dataset of 30 5 word-pairs rated by a group of 38 human subjects. Word-pairs are rated on a scale from 0 (no similarity) to 4 (perfect synonymy). Miller-Charles X  dataset is a subset of Rubenstein-Goodenough X  X  (1965) orig-inal dataset of 65 word-pairs. Although Miller-Charles X  experiment was carried out 25 years later than Rubenstein-Goodenough X  X , two sets of ratings are highly correlated (Pearson correlation coefficient= 0 . 97 ). Therefore, Miller-Charles ratings can be considered as a reliable benchmark for eval-uating semantic similarity measures. 4.2 Pattern Selection We trained a linear kernel SVM with top N pattern features (ranked according to their  X  2 values) and calculated the Pearson correlation coefficient against the Miller-Charles X  benchmark dataset. Experimen-tal results are shown in Figure 1. From Figure 1 we select N = 200 , where correlation maximizes. Features with the highest linear kernel weights are shown in Table 2 alongside with their  X  2 values. The weight of a feature in the linear kernel can be consid-ered as a rough estimate of the influence it has on the Figure 1: Correlation vs No of pattern features Table 2: Features with the highest SVM linear ker-nel weights final SVM output. WebDice has the highest linear kernel weight followed by a series of patterns-based features. WebOverlap (rank= 18 , weight= 2 . 45 ), We-bJaccard (rank= 66 , weight= 0 . 618 ) and WebPMI (rank= 138 , weight= 0 . 0001 ) are not shown in Table 2 due to space limitations. It is noteworthy that the pattern features in Table 2 agree with the intuition. Lexical patterns (e.g., X or Y, X and Y are, X of Y ) as well as syntactic patterns (e.g., bracketing, comma usage) are extracted by our method. 4.3 Semantic Similarity We score the word-pairs in Miller-Charles dataset using the page counts-based similarity measures, previous work on web-based semantic similarity measures (Sahami (2006), Chen (2006)) and the proposed method (SVM). Results are shown in Ta-ble 4.3. All figures except for the Miller-Charles ratings are normalized into [0 , 1] range for the ease of comparison 6 . Proposed method (SVM) re-ports the highest correlation of 0 . 8129 in our ex-periments. Our implementation of Co-occurrence Double Checking (CODC) measure (Chen et al., 2006) reports the second best correlation of 0 . 6936 . However, CODC measure reports zero similarity for many word-pairs. This is because for a word-pair ( P, Q ) , we might not necessarily find Q among the top snippets for P (and vice versa). CODC mea-sure returns zero under these conditions. Sahami et al. (2006) is ranked third with a correlation of 0 . 5797 . Among the four page counts based mea-sures WebPMI reports the highest correlation ( r = 0 . 5489 ). Overall, the results in Table 4.3 suggest that snippet-based measures are more accurate than page counts-based measures in capturing semantic similarity. This is evident for word-pairs where at least one of the words is a polysemous word (e.g., pairs that include cock , brother ). Page counts-based measures do not consider the context in which the words appear in a page, thus cannot disambiguate Table 4: Comparison with taxonomy based methods the multiple senses.

As summarized in Table 4.3, proposed method is comparable with the WordNet based methods. In fact, the proposed method outperforms simple WordNet based approaches such as Edge-Counting and Information Content measures. However, con-sidering the high correlation between human sub-jects ( 0 . 9 ), there is still room for improvement.
Figure 2 illustrates the effect of the number of snippets on the performance of the proposed Figure 3: Correlation vs No of positive and negative training instances method. Correlation coefficient steadily improves with the number of snippets used for extracting pat-terns. When few snippets are processed only a few patterns are found, thus the feature vector becomes sparse, resulting in poor performance. Figure 3 de-picts the correlation with human ratings for various combinations of positive and negative training in-stances. Maximum correlation coefficient of 0 . 834 is achieved with 1900 positive training examples and 2400 negative training examples. Moreover, Fig-ure 3 reveals that correlation does not improve be-yond 2500 positive and negative training examples. Therefore, we can conclude that 2500 examples are sufficient to leverage the proposed semantic similar-ity measure. 4.4 Named Entity Clustering Measuring semantic similarity between named en-tities is vital in many applications such as query expansion (Sahami and Heilman, 2006) and com-munity mining (Matsuo et al., 2006a). Since most named entities are not covered by WordNet, simi-larity measures based on WordNet alone cannot be Table 5: Performance of named entity clustering used in such tasks. Unlike common English words, named entities are constantly being created. Manu-ally maintaining an up-to-date taxonomy of named entities is costly, if not impossible. The proposed semantic similarity measure is appealing as it does not require pre-compiled taxonomies. In order to evaluate the performance of the proposed measure in capturing the semantic similarity between named entities, we set up a named entity clustering task. We selected 50 person names from 5 categories : tennis players, golfers, actors, politicians and scien-tists, ( 10 names from each category) from the dmoz we measure the association between the two names using the proposed method and baselines. We use group-average agglomerative hierarchical clustering to cluster the names in our dataset into five clusters. We employed the B-CUBED metric (Bagga and Baldwin, 1998) to evaluate the clustering results. As summarized in Table 5 the proposed method outper-forms all the baselines with a statistically significant ( p  X  0 . 01 Tukey HSD) F score of 0 . 7897 . We propose an SVM-based approach to combine page counts and lexico-syntactic patterns extracted from snippets to leverage a robust web-based seman-tic similarity measure. The proposed similarity mea-sure outperforms existing web-based similarity mea-sures and competes with models trained on Word-Net. It requires just 2500 synonymous word-pairs, automatically extracted from WordNet synsets, for training. Moreover, the proposed method proves useful in a named entity clustering task. In future, we intend to apply the proposed method to automat-ically extract synonyms from the web.
