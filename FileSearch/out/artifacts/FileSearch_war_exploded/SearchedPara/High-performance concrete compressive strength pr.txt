 1. Introduction
High-performance concrete (HPC) is a new type of concrete used in the construction industry ( Yeh, 1998 ). HPC works better in terms of performance characteristics and uniformity characteris-tics than high-strength concrete ( Mousavi et al., 2012 ; Yeh and Lien, 2009 ). Apart from the 4 conventional cement ingredients, Portland Cement (PC), water, fi ne aggregates, and coarse aggre-gates, HPC further incorporates cementitious materials, fl blast furnace slag, and a chemical admixture ( Yeh, 1998 ). These additional ingredients make HPC mix proportion calculations and HPC behavior modeling signi fi cantly more complicated than corresponding processes for conventional cement.

Machine learning and AI are attracting increasing attention in academic and empirical fi elds for their potential application to ing, AI techniques have been categorized into two approaches, optimization and prediction, with numerous prediction applica-tions including Arti fi cial Neural Network (ANN), Support Vector Machine (SVM), and Linear Regression Analysis, among others. Optimization applications include the Genetic Algorithm (GA) and Particle Swarm Optimization (PSO).

In the fi eld of civil engineering, much research has focused on hybridizing optimization techniques and prediction techniques. Many papers have reported on hybrid techniques that are able to predict HPC to a high degree of accuracy ( Cheng et al., 2012 ; Peng et al., 2009 ; Yeh, 1999 ). The Evolutionary Support Vector Machine Inference Model (ESIM), one hybridization technique, uses a fast messy Genetic Algorithm (fmGA) and SVM to search simulta-neously for the fi ttest SVM parameters within an optimized legal model ( Cheng and Wu, 2009 ). However, the aforementioned techniques, especially ANN, SVM, and ESIM, are considered  X  black-box  X  models due to massive node sizes and internal connections. Because these models do not provide explicit for-mulae, they do not explain the substance of the associated model, which is a serious disadvantage in practical applications.
Yeh and Lien (2009 ) proposed the novel Genetic Operation Tree (GOT) to overcome this disadvantage. The GOT consists of a GA and an Operation Tree (OT). This model is a practical method for eliciting both an explicit formula and an accurate model from experimental data. Although many studies have used GOT to develop formulae to optimally fi t experimental data ( Chen et al., 2012 ; Peng et al., 2009 ; Yeh et al., 2010 ), this model has yet to achieve results comparable to other prediction techniques such as ANN and SVM. This suggests the potential to further improve the GOT.
 This paper introduces a novel approach based on OT called Genetic Weighted Pyramid Operation Tree (GWPOT) to predict HPC compressive strength. The GWPOT integrates the Weighted Operation Structure (WOS) and Pyramid Operation Tree (POT) models to enhance the prediction capability and the fi t with experimental data.
 Remaining sections in this paper are organized as follows: Section 2 provides a brief explanation of OT, GA, and WOS; Section 3 describes the GWPOT model; Section 4 describes the case study and con fi guration of GWPOT parameters, presents GWPOT model results, and compares those results with those of common prediction techniques; and, fi nally, Section 5 presents study conclusions. 2. Literature review 2.1. Operation Tree (OT) sents the architecture of a mathematical formula. Fig. 1 illustrates an OT model with 31 nodes. In Fig. 1 , the OT model consists of a root value and sub-trees of children, represented as a set of connected nodes. Each node on the OT model has either 0 or 2 child branches, with the former designated as  X  leaf nodes associated with either a variable or constant and the latter associated with a mathematical formula (  X  , , , C , log, etc.) ( Hsie et al., 2012 ). Fig. 2 shows an example of the OT model with a 31-bit-node code. Table 1 lists the bit codes for mathematical operations, variables, and constants. The OT in Fig. 2 may be expressed as
Output  X  log E C  X  avoid a disadvantage common to other prediction techniques ( Peng et al., 2009 ; Yeh and Lien, 2009 ). The branch-and-leaf con fi guration of OT facilitates the deduction of function values and formulae. Input values may thus be substituted into the formula to generate a predicted output value for each data point.
OT performance may be evaluated by calculating the root-mean-squared error (RMSE) between predicted and actual output values ( Yeh et al., 2010 ). The best OT formula is achieved when RMSE reaches the lowest possible value. Because searching the best combination formula to fi t with the data is a discrete optimization problem, an optimization technique capable of solving a discrete problem must be integrated into the OT model ( Peng et al., 2009 ). 2.2. Genetic Algorithm (GA) proposed by Holland (1975 ). GA is based on Darwin's theory of evolution and mimics biological competition in which only com-paratively strong chromosomes survive into the next generation.
Each chromosome in a GA population represents a candidate solution for a given problem and is able to generate a result based on the objective function. Ability to handle various types of objective functions is another advantage of GA.
 population. Each GA generation is subjected to genetic operation processes such as evaluation, selection, crossover, and mutation and generates a new result. A new-generation chromosome will replace the current-generation chromosome if it generates a better result. 2.3. Weighted operation structure (WOS) the OT model proposed by Tsai (2011 ). This study added a constant value to every variable to balance every input variable to help OT generate a better formula. Therefore, WOS assigns weights to every node connection in the OT model so that each WOS element
X x produces node outputs conducted by 2 OT nodes and 2 undeter-mined weights.

The WOS is thus able to search for the best formula in a wider search space with more combinations over a longer time period than the original OT model. Fig. 3 shows a 5-layer weighted operation structure. The example of the WOS model in Fig. 4 may be expressed as Output  X  0 3. Genetic Weighted Pyramid Operation Tree (GWPOT) 3.1. GWPOT architecture
This study proposes a new operation tree algorithm to address the shortcomings of OT called the Genetic Weighted Pyramid Operation Tree (GWPOT). GWPOT applied a pyramid-shaped, 4 connected OT called Pyramid Operation Tree (POT). The signi fi cantly wider search area of GWPOT results from its use of multiple trees that allows coverage of a greater number of combination possibilities. The weighted concept of WOS was integrated into GWPOT due to the success of this concept in imp roving GOT performance. Fig. 5 illus-trates a GWPOT model with 3 layers per tree.

Tuning parameters in the GWPOT model include: mutation rate, crossover rate, weight rate, layers per tree, and total number of trees. Mutation rates and crossover rates were retained from OT. The other parameters are new to GWPOT and explained as follows: (1) weight rate sets the probability of each node having a constant value in the WOS structure; (2) layers per tree sets the number of layers used for each tree; and (3) total number of trees sets the number of trees used to produce the formula in one model process. Four trees were used to assemble the pyramid shape in this study.
GWPOT operations start by generating a population of chromo-somes for the fi rst tree. Every chromosome represents the solution vector of formula components. Next, the evaluation step obtains the objective function (RMSE) for each chromosome. Afterward, the GA optimizer searches for the optimal parameter or best formula combination. GA optimization repeats until the stopping criterion is achieved. Fig. 6 shows the fl owchart for GWPOT.
An explanation of the principal steps in GWPOT follows below: (4) Fitness evaluation :A fi tness function formula, RMSE in the (5) GA procedure : GA begins with the selection process. This study (6) Checking the number of tree : If the number of trees does not (7) Optimal solution : The chromosome with the lowest RMSE is 3.2. GWPOT example
To further clarify the GWPOT procedure, this study applied GWPOT to an example problem. A 3-layer GWPOT was created. This example weights are set between 0.01 and 10.00, and 4 tree-structure phases must be passed to establish the GWPOT model.

The fi rst tree-structure phase begins by generating a population weight. The process continues through the OT training model, evaluation, and GA search procedures. Fig. 7 shows the best chromo-some in the fi rst tree structure, calculated as Tree 1  X  0
X
X w generated randomly from the best solutions from the fi rst tree structure and augmented by additional new bits. The bottom-half population is tasked to identify new tree structure combinations between the fi rst tree and second tree. One additional mathema-tical operator is used to connect the fi rst tree and second tree.
GA fi nds this operator concurrently using other nodes in the second tree-structure phase. Assuming the fi rst tree solution in the second phase is the same as the fi rst tree solution in structure phase, the best solution from second tree structure phase may be illustrated as in Fig. 8 .

Output  X  pleting the second tree-structure process. The second tree-structure tree structure. Fig. 9 shows the best result from the third tree structure, which incorporates the fi rst,second,andthirdtrees.The last tree structure contains the result for all 4 trees, as shown in Fig. 10 ,while Fig. 11 represents the pyramid-shape model. Tree 3  X  X  1 : 1 C 1 : 1 D  X  5 : 9 A 0 : 1 A  X  5  X  Tree 4  X  2 : 4 D  X  0 : 3 A  X  6  X  may be expressed as
Output  X   X   X   X  2 2 Tree 1 2 : 2 Tree 2  X  0 : 5 Tree 3  X  0 3.3. Modi fi ed predicted output value tion and high RMSE in the relationship between actual output and researchers have used single linear regression analysis to modify
X
X
W to assign the same value to the prediction output mean value and the actual output mean value. The equation for the single linear regres-sion analysis is y  X   X   X   X  f  X  8  X  where f is the predicted output value of the operation tree; y is the modi fi ed predicted value;  X  and  X  are the regression coef
According to single linear regression analysis  X  y  X  U f  X  9  X  where y is the mean of actual output values in the dataset; f is the mean of predicted output values in the dataset; y i is the actual output value of the i th data in the dataset. 4. Case study 4.1. The dataset
The dataset used was obtained from Yeh (1998 ) and published total of 1030 concrete samples covering 9 variables were collected the dataset, including cement, fl y ash, slag, water, SP, coarse aggregate, fi ne aggregate, and age, were treated as input variables. The remaining variable, concrete compressive strength, was trea-ted as the output variable. Table 2 shows descriptive statistics for these factors. 4.2. Tuning parameters
In this study, each parameter was set as: crossover rate  X  mutation rate  X  0.05, and weighted rate  X  0.5. Moreover, the total tree was set at 4, with 3, 4, 5, and 6 layers. Furthermore, the total population size and the total number of generations for each tree selected for this study were 100 and 2,000, respectively. 4.3. k-Fold cross validation k -Fold cross validation is a statistical technique that divides study data into k subsamples to determine the accuracy of a prediction model. The original data is divided randomly into k equally sized or approximately equally sized segments, with one subsample used as testing data and the remaining k  X  1 subsamples used as training data. The cross validation process recreates the
AE / + model k times, with each k subsample used exactly once as the validation data. Results from all subsamples or from all folds are then averaged to produce a single value estimation.
 of k may be any suitable number. The current study set the value of k as 5 in order to limit total computational time. Five-fold means that each set uses 20% of the data (206 data points) as testing data and 80% (824 data points) as training data. The data contains 1030
HPC records is divided randomly into 5 equally sized (206 data), with one subsample used as testing data and the remaining 4 subsamples used as training data. Five-fold means that each set uses 20% of the data (206 data points) as testing data and 80% (824 data points) as training data. The cross validation process recreates the model 5 times, with each 5 subsample used exactly tion operations. 4.4. Performance measurement performance-measurement equations: Root Mean Square Error (RMSE), Mean Absolute Error (MAE), and Mean Absolute Percen-tage Error (MAPE). All performance measures used in testing data were combined to create a normalized reference index (RI) in order to obtain an overall comparison ( Chou et al., 2011 ). Each fi tness function was normalized to a value of 1 for the best performance and 0 for the worst. The RI was obtained by calculating the average of every normalized performance measure as shown in Eq. (11) . Eq. (12) shows the function used to normalize the data.
 RI  X  RMSE  X  MAE  X  MAPE x 4.4.1. Root mean square error
Root mean square error (RMSE) is the square root of the average squared distance between the model-predicted values and the observed values. RMSE may be used to calculate the variation of errors in a prediction model and is very useful when large errors are undesirable. RMSE is given by the following equation: RMSE  X  number of data samples. 4.4.2. Mean absolute error
Mean absolute error (MAE) is the average absolute value of the residual (error). MAE is a quantity used to measure how close a prediction is to the outcome. The MAE may be expressed as MAE  X  1 n  X  n j  X  1 j y j ^ y j j X  14  X  number of data samples. 4.4.3. Mean absolute percentage error
Mean absolute percentage error (MAPE) calculates percentage error of prediction. Small denominators are problematic for MAPE because they generate high MAPE values that impact overall value. The MAPE may be expressed as MAPE  X  1 n  X  n j  X  1 data samples. 4.5. Results and discussion
This study compared the performance of the proposed model against two other OT-based tools, the Genetic Operation Tree (GOT) and the Weighted Operation Structure (WOS). After testing a variety of layer numbers, the number of layers needed to build the best GWPOT model formula was determined as between 3 and 6.

Five-fold cross-validation techniques validated the perfor-mance of the models. The average 5-fold results for GOT, WOS, and GWPOT are summarized in Tables 3  X  5 , respectively. Table 3 shows that the 5-layer con fi guration generated the best RI result for the GOT model (0.817). Eq. (16) shows the formula associated with this result. On the other hand, the six-layer con fi guration generated the best overall and average for the WOS model (0.872). Eq. (17) shows the formula associated with this result. y  X  2 : 905  X  0 : 077 log  X  AH  X  H 2  X 
The RI results in Table 5 for the 3-, 4-, 5-, and 6-layer con fi gurations are: 0.263, 0.448, 0.695, and 0.690, respectively. This indicates that the 5-layer con fi guration generated the best RI result for the GWPOT model. The 6-layer con fi guration generated the worst result due to increased model complexity and large chromosome bit number. Further, larger layer numbers require more computational time, which increases the dif fi culty of the best combination.
 Fig. 13 illustrates the best OT produced using GWPOT. The OT in Fig. 13 is generated from the best 5-layer GWPOT model and may y  X  35 : 74  X  2 : 842 log 9 : 9 H  X  0 : 262 H 0 : 725 A 0 : 125 F  X  X  = log  X  2 : 798 D ln ln 83 be decoded into a complete formula using Eqs. (18) through (22) .
Figs. 14 and 15 show scatter diagrams for the actual values and predicted values of, respectively, training data and testing data.
 X 
 X  x 3  X  Third tree output  X  0
 X  y  X  90 : 089  X  17 : 628  X  x 1 4 : 7 x 2 2 : 7 x 3 6 : 2 x  X  relationships between all the input variables (variables A through H ) and the prediction output. Meanwhile, the result shows that
GOT and WOS exclude some input variables. In the GOT formula example (Eq. (16) ), super plasticizer, fi ne aggregate, and coarse aggregate are not included in the formula. Peng et al. stated that excluding some variables from a formula does not mean they do not impact the compressive strength ( Peng et al., 2009 ). The
GWPOT depicts the relationship between all input  X  output variables and shows that each variable has a distinct in fl on HPC compressive strength.
 new OT outperforms the other two. The best-solution con fi tion for each OT was determined and used in this comparison.
These con fi gurations were: 5-layer for the GOT model, 6-layer for the WOS model, and 5-layer for the GWPOT model. Table 6 summarizes comparisons among these models.
 sure, with an RI value of 1.00. GOT and WOS obtained RI values of 0.00 and 0.458, respectively. Due to the superior performance of
GWPOT over the two other OT models, the GWPOT model was tested against other prediction techniques. 4.6. Comparison prediction techniques including SVM, ANN, and ESIM. The GWPOT result was obtained as explained in the previous section using a 5-layer model in fold set one, and 5-fold cross validation was performed on all results. Table 7 presents model results for comparison.
 worse than ESIM. ESIM demonstrated its superiority with a prediction RI value of 0.918 compared to RI values for GWPOT, SVM, and ANN of 0.085, 0.072, and 0.684, respectively. Although log 0.43 0.98 / x 4  X  Fourth tree output  X  6 : 11 E log 2 : 957 D 4 : 906 E  X  the RI value for GWPOT fell short of the RI value for ESIM, GWPOT remains capable of competing with ESIM, as demonstrated by the superior RMSE value obtained by GWPOT (6.379) compared to ESIM (6.574). 4.7. Sensitivity analysis
OT uses a single-tree structure to build its model while GWPOT uses 4 trees to form a pyramid shape. Other OT model con fi tions such as 2-tree and 3-tree exist as well. To increase the validity of the GWPOT concept, this study conducted another comparative analysis that used various combinations of layer numbers and tree numbers to identify differences among these parameters. Each tree number and layer number used the 5-fold cross validation technique to avoid potential bias. Table 8 shows average RI results for each fold set.

As shown in Table 8 , the 5-tree structure with 4 layers generated the best result with an RI value of 0.792. The 5-tree, 5-layer model obtained a good result of 0.790, which differed only slightly from the best RI solution. The worst RI result was 0.296, produced by the 1-tree, 3-layer model.

The unexpected results and the fl exibility of the solution indicate that another OT model may generate the ultimate best solution. However, the 4-tree, 5-layer model obtained the best result in this study. 5. Conclusions
This study develops a new GWPOT to predict HPC compressive strength. Accurately predicting HPC strength is critical to building a robust prediction model for HPC. The GWPOT model employs 4 hierarchical OT to form the pyramidal shape. Multiple trees widen the GA search area, creating a formula that is more complex and more fl exible to fi t with the data. Comparisons between GWPOT and other prediction techniques, including GOT, WOS, SVM, ANN, and ESIM, showed the highly competitive performance of GWPOT in predicting HPC compressive strength.

GWPOT performs comparably to the well-known ESIM in terms of prediction accuracy. However, while ESIM uses a black-box approach that does not depict input  X  output relationships in an explicit formula, GWPOT generates and shows corresponding formulae for these relationships. This comparative transparency gives GWPOT an important advantage in practical applications. In future research, another optimization technique may be devel-oped to replace the GA technique used in this study for further comparison. Additionally, the ef fi cacy of GWPOT may be further tested and veri fi ed on other construction management case studies.
 References
