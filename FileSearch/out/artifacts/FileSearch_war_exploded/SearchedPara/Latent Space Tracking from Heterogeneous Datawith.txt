 In the era of Big Data, heterogeneity of various information generated from a same yet complex underlying system/process has become ubiquitous. Exam-ples of such heterogeneous data include video and audio from a sensor network, acoustic and articulatory signals during a speech, etc. Such heterogeneous data provides complimentary or augmented depiction of the system from different perspectives, allowing more comprehensive understanding of the system than that from homogeneous data. Albeit the high dimensionality and heterogene-ity, these data often exhibits low dimensional nature and can be characterized by a (low dimensional) latent space. Correctly identifying the latent space ben-novel applications (e.g., the anomaly detection). However, learning from het-erogeneous data is highly nontrivial. The requirement of operating in real time imposes further challenges and prevents straightforward extensions of existing methods. Principal Component Analysis ( PCA )[ 7 ] is arguably the most well-known method for extracting the low dimensional latent space. A common assump-tion in applying PCA is that most data is near the low dimensional space. The online PCA [ 12 ] techniques are developed to conduct anomaly detection on data sion [ 17 ] under union-of-subspace assumption. However, PCA not model the relations between the heterogeneous data sources. Therefore, the relation between multiple data sources. And online CCA gradient on generalized Stiefel manifold has been applied to anomaly detection nature of the data.
 Recently, learning from heterogeneous data has attracted much attention in machine learning community, particularly in transfer learning, multi-task learn-data to learn a better model in a target domain, where the two domains are often tasks, each of which may work on a different/heterogeneous data domain [ 6 ]. Multi-view learning leverages multiple views of same instances for better mod-els [ 18 ]. Many of these works assume a common low dimensional latent space, and learn a mapping from each data source/view to the latent space in a super-vised fashion. However, adapting these methods to an online and unsupervised setting (e.g., anomaly detection task) is not straightforward.
 In this paper, we tackle the problem of online learning of heterogeneous data via latent space tracking. In specific, we propose a framework to track the low-dimensional latent structures of heterogeneous data and learn their inher-ent relations. Our formulation incorporates the key insights underlying for anomaly detection from heterogeneous data. We develop an efficient online algorithm that effectively conducts L atent S pace T racking from H eterogeneous data, denoted as LSTH . Based on the learned latent space, we further design an anomaly detection method that reports anomalies significantly outlying the latent space. We test LSTH on both synthetic and real datasets. Experimental neous data for anomaly detection.
 The paper is organized as follows. Section 2 formulates the latent space track-an anomaly detection method as an application of the learned latent space. Experimental results and conclusions are in Section 5 and 6 respectively. Throughout this paper, vectors are represented by lower-case letters (e.g., x ), and matrices are represented by upper-case letters (e.g., U ). By default, all the vectors are column vectors, while row vectors are represented by having a transpose superscript (e.g., x ). We use subscript i and j to index an element x ) in a data steam. The estimate of a variable is represented by having a hat over the variable (e.g., U represents the estimate of U ).
 data samples from a same system at timestamp t , where D x (MFCC). Obtained by appending higher order derivatives of acoustic signal, stocks, and y t is massive news about the market [ 14 ].
 we monitor the joint probability density p ( x t ,y t ) at each timestamp t : a d dimensional latent space ( d D x ,D y ) underlying the data, into which x and y t can be transformed via two linear projectors U  X  R sidered as realizations of a common latent variable that determines the states while U may span a low-rank subspace as in PCA , V may model a latent space impacting only a subset of the features in y t . We constrain U to be orthonormal (i.e., U U = I , where I is the identity matrix) to preserve the magnitude of x t . Thus, the reconstruction error of x x t  X  UU x t 2 . In this case, we measure the probability distribution of x the reconstruction error [ 17 ]: where  X  2 x is the variance of reconstruction error in each dimension. Since the able, they are expected to be close. Hence, we measure p ( y of the projections in the latent space: where  X  2 y is the variance of the difference between x t By substituting Equation ( 2 ) and ( 3 )into( 1 ) and taking the logarithm, the log-likelihood can be represented as In addition, we constrain V to exhibit  X  X roup sparse X  structure so that applying V performs feature selection from y t to identify the most informative features. We use the mixed norm V 1 , 2 D y i =1 v i 2 to introduce sparsity into V , where v i is the i -th row of V .
 To enable tracking in a slowly evolving environment, we apply an exponen-tially decaying window to downweigh the historical samples. In addition, we and V t , respectively. Then we formulate the following optimization problem to find the projectors U and V at timestamp t : the decaying window,  X  balances between projection residual and discrepancy the data stream starts from t =1.
 In the above F ( U, V ; t,  X ,  X ,  X  ), the first term measures the discrepancy of two data sources in the latent space. It has the flavor of the correlation of two projections. Same as PCA , the second term imposes low-dimensional structure in x t . It is important to highlight the V v i 2 indicates the significance of the i -th feature in y t basis chosen. 3.1 A Batch Algorithm We first present a batch algorithm, denoted as bLSTH , to solve U and V for simplicity. The bLSTH algorithm will be further modified into an online version in Section 3.2 .
 Algorithm 1. The Batch Algorithm bLSTH Output: U and V i  X  0, U [0]  X  the first d principal components of X repeat until U [ i ] ,V [ i ]convergeor i is large enough U  X  U [ i ], V  X  V [ i ] used to solve the following optimization problem: The optimization problem in Equation ( 6 ) of Algorithm 1 is a well-studied con-vex optimization problem. Now we focus on the optimization problem in Equa-tion ( 7 ). The objective can be reformulated as: where U U = I and W = V Y . This orthonormality constrained problem is non-convex. However, we are able to find a local minimum within a few itera-tions and our experiments show that even local minimum is able to give good results. Following the idea in [ 8 ], we use a majorization minimization scheme. we construct a surrogate function g k ( U ) that satisfies That is, g k ( U ) is an upper bound of f ( U ) and the equality holds when U = of g ( U ) as in Equation ( 8 ) and the notion of global minimizer. In practice, a surrogate function should be constructed such that its global minimizer is eas-ily obtained. The following two lemmas suggest one form of such g global minimizer.
 Lemma 1. For any given orthonormal matrix U [ k ]  X  R D x g ( U ; a ) defined on the set of orthonormal matrices U  X  R independent of U . And the scalar a chosen as where  X   X  is the maximum eigenvalue of XX .
 Proof. The proof leverages Rayleigh quotient inequality and is omitted for con-ciseness.
 Lemma 2. [ 10 ] The global minimizer of is PQ ,where P  X  Q = A is the Singular Value Decomposition ( Using Lemma 2 , the global minimizer of the surrogate function g tion ( 7 ) can be solved via the iterative majorization minimization process as presented in Algorithm 2 , where G XW = XY V and C x XX .A closed-form solution directly by Lemma 2 . 3.2 An Online Algorithm Here we derive the online algorithm LSTH from bLSTH . We use the solution ( U , V )by bLSTH on the samples X =[ x  X  L +1 ,  X  X  X  ,x 0 ], Y =[ y from timestamp t = 1. We also use an alternating method to track ( U the following definition of projections of x t and y t into the latent space: The online algorithm LSTH consists of an initialization via online updates of U and V , as presented in Algorithm 3 .
 Algorithm 2. Updating U for bLSTH Input: orthonormal U , scalar  X  , cross-covariance matrix G repeat until U [ k ] converged or k is large enough Online Tracking of U t . Upon arrival of new data ( x t ,y estimate the projection of y t at t as follows: of U is of the same form as ( 5 ), except that the historical x Therefore it can be minimized via Algorithm 2 with the only modification that G in Equation ( 9 ) is replaced by t  X  1 k =0  X  k x t  X  k Online Tracking of V t . Given U t solved as in Section 3.2 ,weuse U z at current timestamp t as follows w.r.t V , For the above problem, we derive a Stochastic Coordinate Descent (SCD) method with a similar spirit as [ 9 ]. The SCD admits a row-wise updating of V can be found in Equation ( 13 ) in Algorithm 3 . 3.3 Complexity Analysis The complexity of LSTH is O ( c  X  D 2 x d + D 2 y d ), where c in Equation ( 9 )and c is the number of iterations in majorization minimization for U ( c = 1 suffices in practice). Efficient algorithms for computing the sequentially updated matrix [ 2 ] can be applied to reduce the complexity. D be achieved via active set tricks. Our experiments show that fast for real applications, for example, 20 ms for the XRMB To reduce the complexity of LSTH is very important and it is left for future exploration for now. The basic idea of our anomaly detection method is to monitor U x  X  ( I  X  UU ) x t 2 . We define the a priori error: and use  X  t as the detection statistic. An anomaly is claimed only when p ( x We maintain a sliding window over  X  t with the mean  X  t and standard deviation  X  within the window. When the new ( x t +1 ,y t +1 ) arrives, we compare its  X  Algorithm 3. The Online Algorithm LSTH Parameters : d ,  X  ,  X  ,  X 
Input : data stream:  X  X  X  , ( x 0 ,y 0 ) ,  X  X  X  , ( x t ,y t
Obtain U 0 and V 0 by Algorithm 1 for t =1 , 2 ,... do //update U t get U t via Algorithm 2 with ( U t  X  1 ,  X  , G t , C x,t ) as input //update V t for i =1 , 2 ,...,D y do v end for end for  X  +1 exceeds the threshold, an anomaly is claimed.
 specific, if the anomaly behaves as a sudden outlier after which the data stream goes back to normal state, then the anomalous data point should be excluded for model updating. The other case is that the anomaly is in fact the start of a different stage in the data stream, then the anomalous data point should be included in model updating. These two cases will be addressed in synthetic and real data experiments respectively. In this section, we conduct comparative experiments to demonstrate the perfor-mance of LSTH in tracking the latent space for anomaly detection. All types of tracking methods as well as their corresponding anomaly detection statistics are summarized in Table 1 .
 5.1 Experiments on Synthetic Data We generated a synthetic dataset with continuous data x t model x t = A X  t + n t ,t =1 ,..., 10500. where A  X  R 500 n t is white Gaussian noise. The y t  X  X  are generated as of dimension 1000. The first 50 features of y t  X  X  are relevant to the underlying system, generated via B X  t + m t ,t =1 ,..., 10500 , where B  X  R 50  X  10 and m t The rest 950 dimensions are padded as noise. We introduced sparsity into y by randomly setting half of its values to zero. In the end we round the y non-negative integers. In this way, y t is analogous to the real-world documents in bag-of-words representation. In this generated dataset, we introduced three types of anomalies, all of them are sudden outliers. Type-1 anomaly :at t = 500 , 600 ,..., 10400, x t is distorted to  X  x N (0 , 1). At the same timestamps when A is distorted, B in generating y also distorted to  X  B by randomly re-drawing 5 of its rows from corresponds to the scenario when both x t and y t behave anomalously at same time.
 Type-2 anomaly :at t = 500 , 600 ,..., 10400, only x t is distorted to  X  x A  X   X  + n t with  X   X  t  X  X  (3 . 5 , 1), that is, the latent variable  X  this way, a discrepancy is introduced between the latencies of  X  x corresponds to the scenario when x t has anomalies but y t Type-3 anomaly :At t = 500 , 600 ,..., 10400, three relevant features and three among the rest 950 features of y t are exchanged. This corresponds to the scenario when some relevant features in y t are changed while x Experimental Results on Synthetic Data. We compare all methods in Table 1 for anomaly detection task. For all the methods, the first 100 samples are used for initialization. The  X  in computing detection threshold is varied to maximize the Area Under Curve (AUC) of the precision-recall plot on a training sented in Table 2 . For the three types of anomalies, LSTH best detection performance. CCA is competitive for Type-1 and Type-3 anomalies capture the changes in the signal/latent space. The failure of a same reason as that of CCA on Type-2. On average, PCA based methods perform worst among all the methods except for Type-3. However, by joining two data sources properly, PCAxy is able to detect the change of the  X  X oint X  subspace so as to achieve better performance than PCAx and PCAy .
 of
LSTH at t = 10500. For the relevant ( i =1 ,..., 50) features in y demonstrates that LSTH can successfully identify the relevant features via the mixed norm on V . 5.2 Experimental Results on Real Data: XRMB XRMB [ 16 ] contains synchronous 273-dim MFCC and 112-dim articulatory infor-mation of length 51K. Each timestamp has a label indicating which word it corresponds to. Details on the data are available in [ 1 ]. Speech segmentation experiment is to detect the boundary of words from acoustic and articulatory features. During each segment, a tracking algorithm, e.g., the underlying latent subspace. Upon arrival of a new segment, the underlying latent space has a sudden change. This event may induce a drastic change of the ered as an anomaly. In this case, the claimed anomalous data point should be incorporated in learning the new latent space in the new segment. correlated dimensions [ 3 ]. And y t is designated as the MFCC, which is redun-randomly select 1000 frames for parameter tuning for all the methods, and use the tuned parameters for testing on the rest of the frames. Figure 2 shows the detection statistics of all methods on the parameter tuning dataset. Out of 25 words within the 1000 frames, LSTH is able to identify 15 words with clear and strong spikes in the detection statistics. After each alarm of anomaly (start of a new segment), it quickly adapts to the new latent space in the new segment. PCA based methods only show weak spikes. CCA fails in this case, as the conclu-sion in [ 1 ]. Based on their results, kernel CCA should be a better approach on kernel CCA , so we leave this approach for later research.
 parameters tuned on the training set. The parameters and the performance of different methods are presented in Table 3 . LSTH has an AUC value 0.342 (note that a random guess would give an AUC of 412 / 51000 = 0 . 008) and it is the only method that can detect the boundaries of the words from All the other methods fail with AUC values smaller than 0.05. We developed LSTH , a latent space tracking method for heterogeneous stream-ing data. Under the assumption that anomalies significantly deviate from the latent space, we further designed an anomaly detection method based on Experimental results demonstrate that LSTH  X  X  detection statistics outperform the other state-of-the-art in identifying anomalies. Therefore Future work on LSTH includes non-linear mapping into the latent space via ker-with more than two views of a system.

