 Abstract Lexical cohesion refers to the reader-perceived unity of text achieved by the author X  X  usage of words with related meanings (Halliday and Hasan, 1976 ). This article reports on an experiment with 22 readers aimed at finding lexical cohesive patterns in 10 texts. Although there was much diversity in peoples X  an-swers, we identified a common core of the phenomenon, using statistical analysis of agreement patterns and a validation experiment. The core data may now be used as a minimal test set for models of lexical cohesion; we present an example sug-gesting that models based on mutually exclusive lexical chains will not suffice. In addition, we believe that procedures for revealing and analyzing sub-group pat-terns of agreement described here may be applied to data collected in other studies of comparable size.
 Keywords Lexical cohesion  X  Inter-annotator agreement  X  Cohesion 1 Introduction The quest for finding what it is that makes an ordered list of linguistic forms into a text that is fluently readable by people dates back at least to Halliday and Hasan X  X  ( 1976 ) seminal work on textual cohesion, and gains in importance in applied Beata Beigman Klebanov  X  Eli Shamir language processing community due to the advent of tasks like text generation 1 and text summarization. 2
A number of  X  X  X exturizing X  X  elements were identified, studied and applied, including lexical repetition (Hearst, 1997 ; Hoey, 1991 ), patterns of entity realization (Barzilay &amp; Lapata, 2005 ; Grosz, Joshi, &amp; Weinstein 1995 ; Karamanis, Poesio, Mellish, &amp; Oberlander, 2004 ; Siddharthan &amp; Copestake, 2004 ), rhetorical organi-zation (Mann &amp; Thomson, 1988 ; Marcu, 2000 ).

The existence of lexical cohesion  X  X exture created by using words with related meanings X  X as also been postulated (Halliday &amp; Hasan, 1976 ). However, in contrast with such phenomena as lexical repetition, repeated reference using pronouns, or rhetorical structure often signalled by specific cue phrases, there are no clear form-based clues as to where lexical cohesion is found in the text. That is, no particular textual items are inherently lexically cohesive 3 ; they are cohesive with respect to certain other lexical elements. Various classes of relations were suggested as carriers of cohesion (e.g. synonymy, hyponymy), but researchers noticed that much of lexical cohesive load is carried by relations felt by the reader, but difficult to classify (Halliday &amp; Hasan, 1976 ; Hasan, 1984 ; Morris &amp; Hirst, 2004 ).

The difficulty of identification and characterization of lexical cohesive relations places the reader in the center of the researcher X  X  attention. The fundamental question is how well readers agree on which items in a text are lexically cohesive. If the agreement is very poor, then the notion of lexical cohesion lacks internal coherence, and translates into something idiosyncratic for every individual reader. If there is substantial agreement, then we can try to develop identification procedures and characterization of those instances of lexical cohesion that are well agreed upon. These are likely to be a part of what the text is expected to induce upon an  X  X  X verage X  X  reader, akin to the way repeated reference is systematically induced 4 , and thus a part of the textual structure.

Experimental, reader-based investigation of lexical cohesion is an emerging enterprise (Morris &amp; Hirst, 2005 ). The main challenge is providing a translation of the notion of patterns of lexical cohesion into a task for the readers, such that (1) the results reflect researcher X  X  intuition about what he/she is after; (2) the guidelines are precise enough to enable replication but open enough to compel readers to use their intuition, which is the main source of information in an elicitation experiment.
Morris and Hirst asked people to group related words, to mark related pairs within each group, to name the relation, and to describe the meaning of each group. The researchers observed the average agreement of 63% on grouping, and lower agreement on identifying pairs of related words within each group.

We suggest that groups of words might emerge as a result of combining small pieces, rather than being primary units of analysis. Word groups are global struc-tures; people might be tempted to make global decisions about the main issues in the text, and mark groups in light of those, not necessarily in a way sensitive to the cohesive impact of specific words in their particular place in the text.

For example, in Morris and Hirst X  X  ( 2005 ) experimental text about movie char-acters and actors as possibly inappropriate role models for children, 4 people in-cluded shooting in the same group as drinking , which was entitled  X  X  X ad behaviors X  X ; one person, however, put shooting together with police , reflecting  X  X  X aw/order/ authority X  X  orientation. 5 The question is whether the first placement of shooting was motivated by a global analysis of the text, where bad behaviors was perceived as a more salient issue than law and authority, whereas had people been given the freedom of marking pairwise relations without a need to form coherent groups, they would have connected shooting to both, or maybe just to the latter, as this could have made a stronger, readily perceivable connection, depending on the exact placement and rhetorical arrangement of the mentions of the three concepts.

In the following section, we present our version of a question to which lexical cohesion is an answer. Section 3 reports on the experiment we performed on 22 readers using this question. Section 4 contains analysis of inter-subject agreement and of the way it was used to identify a highly valid core of the phenomenon. Section 5 exemplifies and discusses the resulting structures. We conclude with an indication of potential usefulness of the resource created as a result of the experiment. 2 From lexical cohesion to anchoring Lexical cohesive ties between items in a text draw on word meanings. Sometimes the relation between the members of a tie is easy to identify, like near-synonymy ( dis-ease/illness ), complementarity ( boy/girl ), whole-to-part ( box/lid ), but the bulk of lexical cohesive texture is created by relations that are difficult to classify (Morris &amp; Hirst, 2004 ). Halliday and Hasan ( 1976 ) exemplify those with pairs like dig/garden , belson, 1977 ): Certain things are expected in certain situations, the paradigm example being menu, tables, waiters and food in a restaurant.

However, texts sometimes start with descriptions of situations where many pos-sible scripts could apply. A famous literary text starts with Mother died today . What are the generated expectations? A description of an accident, or of a long illness? A story about what happened to the family afterwards? The speaker X  X  feeling of loneliness? Funeral arrangements? The mother X  X  last wish and its fulfillment? Many directions are easily thinkable at this point.

We suggest that rather than generating predictions, scripts could provide a basis for abduction. Once any normal direction is actually taken up by the following text, there is a connection back to whatever makes this a normal direction, according to the reader X  X  commonsense knowledge (possibly coached in terms of scripts). Thus, had the text developed the illness line, one would have known that it can be best explained-by/blamed-upon/abduced-to the previously mentioned lethal outcome. We will say in this case that illness is anchored 7 by died , and mark it illness fi died . The cited line opens Albert Camus X  novel The Stranger ( 1962 ), that proceeds thus: Mother died today. Or, maybe, yesterday; I can X  X  be sure. The telegram from the Home says YOUR MOTHER PASSED AWAY FUNERAL TOMORROW ...

The mention of yesterday refocuses the first sentence such that now it is seen as describing something that happened today, so continuing with recent history makes sense ( yesterday fi today ). Later, telegram is seen in the light of death being an event that requires immediate reaction from relatives, so an urgent announcement is in order ( telegram fi died ). Both these developments could hardly have been pre-dicted from the first sentence X  X t is probably part of The Stranger X  X  strangeness that he chooses to recount them rather than other things after such an opening, but the text does not lose cohesion. It is these kinds of connections X  X hat is anchored by what X  X hat we want to elicit from readers. 3 Experimental design We chose 10 texts for the experiment: 3 news articles, 4 items of journalistic writing, and 3 fiction pieces. All news texts and one fiction story were taken in full; others were cut at a meaningful break to stay within 1000 word limit. 8 The texts were in English X  X riginal language for all but two literary texts.

Our subjects were 22 students at the Hebrew University of Jerusalem, Israel; 19 undergraduates and 3 graduates, all aged 21 X 29 years, studying various sub-jects X  X omputer science, cognitive science, biology, history, linguistics, psychology. Three participants named English their mother tongue; the rest claimed very high proficiency.

All participants first read the guidelines (Beigman Klebanaov &amp; Shamir, 2005 ) that contained an extensive example annotation, as well as short paragraphs explaining various technical matters (how to mark multiple and complex anchors), and highlighting some conceptual issues. In particular, people were asked to make an effort to separate personal knowledge from what they think is common knowl-edge, and general relations from instantial ones that are specifically constructed in the text using co-reference or predication. 9 Participants then performed a trial annotation on a short news story, after which meetings in small groups were held for them to bring up any questions.

The experiment then started. For each of the 10 text, each person was given the text to read, and a separate wordlist on which to write down annotations. The wordlist contained words from the text, in their appearance order, excluding verbatim and inflectional repetitions. 10 Wordlists numbered between 175 and 339 items. For example, the beginning of The Stranger cited above corresponds to the wordlist in Fig. 1 .

People were instructed to read the text, and then go through the wordlist and ask themselves, for every item on the list, which previously mentioned items help the easy accommodation of this concept into the evolving story, if indeed it is easily accommodated, based on the commonsense knowledge as it is perceived by the annotator. They were encouraged to use a dictionary if needed. 11 Figure 2 shows a possible annotation of the cited extract. 4 Analysis of experimental data We now turn to a detailed presentation of data analysis; section 4.4 provides its summary.

Most of the existing research in computational linguistics that uses human annotators is within the framework of classification, where an annotator decides, for every test item, on an appropriate tag out of the pre-specified set of tags (Marcus, Santorini, &amp; Marcinkiewicz, 1993 ; Poesio &amp; Vieria, 1998 ; Webber &amp; Byron, 2004 ).
Although our task is not that of classification, we start from a classification sub-task, and use agreement figures to guide subsequent analysis. We use the by now standard j statistic 12 (Carletta, 1996 ; Craggs &amp; McGeeWood, 2005 ; Di Eugenio &amp; Glass, 2004 ; Siegel &amp; Castellan, 1998 ) to quantify the degree of above-chance agreement between multiple annotators, and a statistic for analysis of sources of unreliability (Krippendorff, 1980 ). The relevant formulas are given in Appendix B. 4.1 Classification sub-task Classifying items into anchored/unanchored can be viewed as a sub-task in our experiment: Before writing any particular item as an anchor, the annotator asked himself whether the concept at hand is easy to accommodate at all. Agreement on this task averages j = 0.45 (texts range from j = 0.36 to j = 0.51). These figures do not reach j = 0.67, the accepted threshold for deciding that annotators were working under similar enough internalized theories 13 of the phenomenon; however, the figures are high enough to suggest considerable overlaps.

Seeking more detailed insight into the degree of similarity of these theories, we follow the procedure described by Krippendorff ( 1980 ) to find outliers. We calculate the category-by-category co-markup matrix for all annotators 14 ; then for all anno-tators except one, and by subtraction find the portion that is due to this one anno-tator. We then regard the data as two-annotator data (one versus everybody else), and calculate agreement coefficients. We rank annotators according to the degree of agreement with the rest, separately for each text, and average over the texts to obtain the conformity rank of an annotator, between 1 and 22. The lower the rank, the less compliant the annotator.

Annotators X  conformity ranks cluster into 3 groups shown in Table 1 . Group A are consistent outliers X  X heir average rank for the 10 texts is below 2. Group B are, on average, in the bottom half of the annotators with respect to agreement with the common, whereas members of group C display relatively high conformity.

It is possible that groups A, B and C have different interpretations of the guidelines, but our idea of the common (and thus the conformity ranks) is dominated by the largest group, C. Within-group agreement rates are shown in the last column of Table 1 . The two annotators in group A seem to have an alternative under-standing of the task, being much better correlated between each other than with the rest of the people; appendix C gives more details about their annotations.

The numbers for the other two groups could support two scenarios schematically depicted in Fig. 3 : (a) each group settled on a different idea of the phenomenon, where group C is in better agreement on its version than group B on its own; (b) people in groups B and C have basically the same interpretation, but members of C are more systematic in carrying their idea through. It is crucial for our analysis to tell those apart X  X n the case of multiple stable interpretations it is difficult to talk about the anchoring phenomenon; in the core-periphery case, there is hope to identify the common core emerging from 20 out of 22 annotations.

If the two groups have different interpretations, adding a person p from group C to group B would usually not improve the agreement in the target group (B), since p is likely to have a different interpretation than B X  X  members. If, however, the two groups have the same interpretation, moving p from C to B would usually improve the agreement in B, since, coming from a more consistent group, p  X  X  agreement with the common interpretation is expected to be better than that of an average member of group B.

We performed this analysis on groups A and C with respect to B. Adding members of A to B improved the agreement in B only for 1 out of the 10 texts. Thus, the relationship between the two groups seems to be that of different interpretations. Adding members of C to B resulted in improvement in agreement in at least 7 out of 10 texts for every added member. Thus, the difference between groups B and C is that of consistency, not of interpretation; we may now search for the well-agreed-upon core of this interpretation. We exclude the two outliers (group A) from subsequent analysis. The remaining group of 20 annotators exhibits an average agreement of j = 0.48 on anchored/unanchored classification, texts ranging from j = 0.40 to j = 0.54. The improvement in agreement after the exclusion of outliers is significant at p &lt; 0.01 (Wilcoxon matched-pairs signed-ranks test, n = 10, H 1 : agreement scores improved after excluding the two outliers). 4.2 Finding the common core of the classification We now seek a reliably classified subset of the data. The main concern is not including cases of agreement that could be due to chance with intolerably high probability. To estimate this probability, we induce 20 random pseudo-annotators from the 20 actual ones: Each pseudo-annotator marks the same proportion of items as anchored as the respective actual annotator, but chooses the items at random. We model this by letting the i -th pseudo-annotator toss a coin with p (heads) = p i , independently for every item, where p i is the proportion of items marked as an-chored by the actual annotator i in the whole of the dataset; random variables x i represent the outcomes of such tosses. A random variable S ranging between 0 and 20 says how many pseudo-annotators marked an item as anchored, having tossed each his own coin, independently of each other. The expectation and variance of S are given by E ( S )= S i =1 20 p i and V ( S )= S i =1 20 p i (1 X  p i ).

We assume that numerous repeated trials on S are normally distributed. We calculate the probability p (Val) that a single observation from a normal distribution with parameters l = E ( S ), r  X  lating for Val = 0.5, we test how likely it is that none of the pseudo-annotators pseudo-annotators anchored a given item. 15
Now, 1 X  p (Val) is the confidence with which we can reject the hypothesis that certain observed agreement levels are due to chance, since S models chance agreement. We seek values that allow high confidence levels, both for anchoredness and unanchoredness decisions. Thus, with 99% overall confidence ( p &lt; 0.01), we may trust unanimous decisions on unanchoredness, and decisions of at least 13 out of 20 people that an item is anchored. 16 Allowing 95% confidence on each of the two decisions separately, cases of at most 2 markups can be reliably considered as unanchored ( p (2.5) = 0.0256), and anchoring by at least 11 people is enough for anchoredness (1 X  p (10.5) = 0.0290). For the subsequent analysis, we choose the 99% confidence version.

Figure 4 plots the actual data for The Stranger , along with the reliability cutoffs. 4.3 Identifying anchors for anchored items The next step is identifying reliable anchors for the anchored items. We calculated average anchor strength for every text: the number of people who wrote the same anchor for a given item, averaged on all reliably anchored items in a text. The average anchor strength is between 5 and 7 for the different texts. Taking only strong anchors (anchors of at least the average strength), we retain about 25% of all an-1261 pairs of reliably anchored items with their strong anchors, between 54 and 205 per text; we refer to this set as core data .

Strength cut-off is a heuristic procedure; some strong anchors were marked by as few as 6 out of 20 people, making it unclear whether they can be trusted as embodiments of the core of the anchoring phenomenon in the analyzed texts. We thus devised an anchor validation experiment, reasoning as follows: In the original experiment, people were asked to generate all anchors for every item they thought was anchored. In fact, people generated only 1.86 anchors per anchored item. This makes us think that people were most concerned with finding an anchor, making sure that something they think is easily accommodatable is given at least one pre-ceding item to blame for that; they were less diligent in marking up all such items. We conjectured that judging presented anchors would be easier than finding ones, so in the validation experiment people were asked to cross over anchors they did not agree with. 17 of the 20 annotators participated in the validation experiment. We chose 6 out of the 10 texts; each person performed validation on 3 out of those, such that every text received 7 X 9 validation versions. For each text, readers were presented with the same list of words as in the first part, only now each word was accompanied by a list of anchors. For each item, every anchor generated by some reader was included; the order of the anchors had no correspondence with the number of people who gen-erated it. A small number of items also received a random anchor X  X  randomly chosen word from the preceding part of the wordlist. Figure 5 shows such a list for the beginning of The Stranger. 17
Ideally, if lack of markup is merely a difference in attention but not in judgment, all non-random anchors should be accepted. To see the distance of the actual results from this scenario, we calculate the total mass of votes as number of anchored-anchor pairs times number of people, check how many are accept votes, and average over texts. People accepted 62% of non-random pairs, 94% of core data pairs (texts scoring between 90% and 96%), and only 15% of pairs with a random anchor. 4.4 Summary of data analysis We used an anchored/unanchored classification sub-task to establish the existence of common interpretation among 20 out of 22 annotators and to identify items at the reliably agreed-upon core of this interpretation. 18 For items reliably classified as anchored, we identified strong anchors and validated those in an additional exper-iment: Even people who did not actually generate them, accepted them as valid anchors (94% average acceptance rate). We may thus regard the core data as rel-atively uncontroversial manifestation of anchoring patterns in the examined texts, reliably found by a group of 20 readers.
 5 From anchoring to lexical cohesion This section discusses the implication of the current study for modeling and anno-tation of patterns of lexical cohesion. First, we show that the pairwise links organize into interesting global patterns (section 5.1). We then discuss the part-of-speech composition of the cohesive pairs (section 5.2), the character of unreliable annota-tions (section 5.3), and the issue of multi-word items (section 5.4). 5.1 Cohesion beyond word pairs To exemplify the observed patterns, we organize the core data in a graph, where a downward arrow from b to a means that a is a strong anchor for b . Figure 6 shows the two largest connected components for the first 12 sentences of Jay Teitel X  X  1987 Toronto Magazine article titled Outland (shown as appendix D), reproduced in Morris and Hirst X  X  ( 1991 ) and analyzed therein into lexical chains that  X  X  X elineate portions of text that have a strong unity of meaning X  X  (page 23). Numbers inside nodes correspond to Morris and Hirst X  X  chain numbers; no number means a word was not assigned to any chain.

Inspecting the upper component, we see that its right-hand side is rooted in driving and the left-hand one in afflicted . Walking up the structure we notice that the con-nection between the two halves hangs on a single link, going from lights to car . Indeed, lights is anchored by car ,by blindness and by night , which reflects the major rhetorical role played by lights in this text X  X hat of connecting driving issues to environmental lack of light (darkness, dusk, night) and to human ailment (blindness, afflicted, deadly), as reflected in the following passage:  X  X  ... I passed them [those years] driving ... in a Volkswagen afflicted with night blindness. The car X  X  lights never worked ... X  X  apartment_1 suburbs_1
In the second component we notice the pivotal position of neighbourhood ,asa social entity (community, collective, people), as a kind of residential unit (city, suburbs, apartment), and as a physical place (environment, surroundings).

These examples undermine an assumption sometimes made in applied research that lexical cohesive patterns can be represented by mutually exclusive chains 2004 ), realized in Morris and Hirst X  X  ( 1991 ) analysis as well. 19 Putting every word in at most one chain misses important lexically expressed connections between meaning components that are registered by human readers. 5.2 Part-of-speech Another issue in lexical cohesion research is the relationship between the part-of-speech (henceforth, POS ) of an item and its participation in lexical cohesion: It is usually assumed that content words are solely responsible for the lexical cohesive texture (Halliday &amp; Hasan, 1976 ; Morris &amp; Hirst X  X  1991 ). Moreover, applied re-search tends to concentrate on the nominal texture (Al-Halimi and Kazman, 1998 ; shows the split of the wordlists and of the core data by POS.

The table indicates that nouns favor cohesive structures: they are twice more frequent within core items than in the wordlists. Adjectives and verbs keep to their proportion; adverbs and proper nouns tend not to participate in lexical cohesion. Function words were occasionally marked by the annotators, but not systematically enough to make it to the reliable data; core pairs with the dollar sign ($) and with 747 (the number of a popular Boeing model) are rare exceptions.

The implication for an algorithm that would try to replicate core annotations is clear: by looking only at common nouns, verbs and adjectives, it is possible to ignore almost 40% of first mentions, and retain a chance for above 90% recall on the core data. For some texts, exclusion of proper names might cause a more significant recall loss, as in a news story where 10% of core anchors are proper names.

The implications for future annotation work are not so clear. We note the vari-ability in proportions of the different POS both in the wordlists and in the core annotations: Whereas proper names are usually rather marginal, in one of the news texts they comprise 16% of wordlist items, and participate as anchors in 10% of core pairs in another news story. Similarly, adverbs usually carry little cohesion, but in some texts they are not so lightweight; in fact, yesterday , today , tomorrow form salient cohesive patterns in The Stranger .

As for function words, it is tempting, and might indeed be worthwhile, to cut the wordlists down in quarter, likely preserving (almost) all of the cohesive texture. A minor caveat has to do with the specific way in which data was presented in our experiment: The annotators had the wordlist aligned with the actual text through the use of the sharp sign, as shown in Fig. 1 . However, the more items are skipped, the more cumbersome and unhelpful such presentation will be, making the alignment with the text more difficult. The second caveat is of a more general nature: In an exhaustive annotation process, we might not want to miss even the little amount of texture that might be residing in the function words. 5.3 Unreliable annotations The analysis in section 4.2 allows identification of the least reliable annotations as well: those are items marked as anchored by the number of people closest to the expectation of the variable S that models chance agreement, which is between 6 and 7 in our data. Such items, exemplified in Fig. 7 , are neither reliably anchored nor reliably unanchored.
Checking these examples with respect to the text, we suggest that they might reflect an interplay between conflicting influences, such as textual distance, refer-ence, syntax, and the non-specificity of a good anchor.

Thus, pairs like vividly fi remember , holes fi punched stand in syntactic dependency relations in the text, which could intensify their perceived cohesion. A similar influence of referential cohesion is likely in ceiling fi car , girls fi names , girls fi phone_numbers , suburbs fi outland : the text discusses the ceiling of a car, phone numbers and names of girls, and refers to suburbs as outland (this is a possible understanding of the title).

In other cases, it seems that the textual rendering of the concepts works against their cohesiveness X  X or example, reminding and remember are more than 150 wordlist items apart, so their cohesiveness, while still perceptible, is backgrounded.
Cases like medieval, swore, certainty are characterized by a large variety of pro-posed anchors, and by the small number of people who wrote each anchor. The annotators are likely to have vaguely felt that the item ought to be anchored in the text, but had a difficulty anchoring it to any specific preceding item.

We think that a possible interpretation of the unreliably marked cases is that lexical cohesion is not a binary phenomenon, and such cases are examples of an intermediate degree of cohesion: They are anchored in the text enough for some people to notice it, but the anchoring is not as salient as for items marked by a decisive majority of the annotators.
 5.4 Multi-word items Multi-word items are an interesting issue with respect to lexical cohesion. On the one hand, the split into atomic units, as done in the current experiment, seems rather artificial in cases like New York ; indeed, in applied research, phrases are sometimes included as units of analysis (Barzilay and Elhadad, 1997 ; Stokes et al., 2004 ).
On the other hand, the decision regarding the boundaries of complex items is not always easy to make. For example, is New York City a unit, or does City stand on its own? Is the Journal part of Wall Street Journal a separately functioning item, and is Wall Street such an item as well? In fact, the core data contains the pairs busi-ness fi wall_street and report fi journal , where the anchors come from Wall Street Journal in the relevant text.

The main reason for not including phrases in the wordlists is that phrases, as opposed to atomic units, are not marked orthographically in the text, but are in the readers X  eyes. This means both that things often taken to be phrases could be per-ceived in separation as well (as in the Wall Street Journal example above), and that things that are compositional and thus are not good a-priori candidates for phrase-hood, are sometimes perceived as a unit. For example, last_minute is a core-data anchor for hurrying in one of our texts.

In addition, we believe that permitting the split of phrases is useful in trying to pinpoint the part that carries the meaning component that is mostly responsible for the anchoring link. Barzilay and Elhadad ( 1997 ) default to the head noun of the compound as the part that is responsible for the cohesive behavior of the complex item; cases like the aforementioned business fi wall_street suggest that this is not always justified.
Furthermore, the identity of a sequence of atomic items as a unit is often tied to its functioning as a single referring expression: It is possible that a text mentioning Wall Street Journal will keep discussing the journal, never mentioning Wall Street as a separate entity. There is thus a potential tension between referential and lexical structure X  X eaders might or might not perceive parts of a single referring expression as separately functioning lexical units.

The handling of phrases is clearly an issue for future experimental research into lexical cohesion, and into its relationship with other structures in the text. 6 Conclusion This article reports on a reader-based experiment that used common knowledge based conceptual anchoring to operationalize the notion of lexical cohesion . There was much diversity in peoples X  responses. However, we developed statistics-based procedures, which, for this experiment, identified a common core of the phenome-non. This core was validated in an additional experiment with the same group.
The core data may now be used as a resource in computational exploration of lexical cohesion. It can serve as an intrinsic test set for models of lexical cohesion: Any good model should at least get the core part right. Currently, models of lexical cohesion are only evaluated in an extrinsic fashion X  X y their usefulness for a given application (Al-Halimi and Kazman, 1998 ; Barzilay &amp; Elhadad, 1997 ; Green, 1998 ; Hirst &amp; Budanitsky, 2005 ; Stairmand, 1997 ; Stokes et al., 2004 ) . As we saw, models based on mutually exclusive lexical chains built along  X  X  X eaning components X  X  are expected to miss salient connections; such shortcomings can be detected using the intrinsic test, before any high-level application is attempted. To encourage such experimentation, the core data is publicly available at the first author X  X  homepage. 20
An additional use of the data, suggested by an anonymous reviewer, is a sequel experiment in which subjects would be asked to classify the anchoring pairs according to the type of relation they embody. This would allow separation between the stage of identification of cohesive pairs and that of classification, which we believe is important, as merging the two might bias the annotators towards marking pairs with easy-to-classify relations. Such data would be useful for finding out which lexical relations participate in the cohesive structures, and to what extent.
Another contribution of this work is a demonstration of the use of agreement statistics not just for annotation reliability judgment, but also for a more detailed exploration of annotation patterns exhibited by a relatively large group of subjects. The development of resources related to semantic/pragmatic tasks is impeded by the difficulty to attain high inter-annotator agreement (Morris &amp; Hirst, 2005 ; Poesio &amp; Vieira, 1998 ). We believe that finer agreement analysis techniques will lead to more effective and insightful use of experimental data that exhibits robust agreement but not enough to warrant a conclusion that the whole of it has been reliably annotated. Appendix Instantial vs. General Knowledge X  X xtract from Guidelines A text is telling us a story about certain entities. For example, it could tell us that children went out in a boat with their father, there was a storm but they survived because the father was an experienced sailor. Whereas relating father back to children, storm to boat, survival to storm and sailor to boat would be based on the general knowledge of what a family is, where you go in a boat and what can happen there, a connection between sailor and father is not something general/typical, it is created only because in this particular text the two descriptions apply to the same person. That is, the relation is instantial, pertaining to a certain instance (a text), not a general one. Since we are after general knowledge, please do not mark instantial connections.
 Measures of Agreements Let N be the number of items to be classified; m  X  X he number of categories to classify into; k  X  X he number of raters; n ij is the number of annotators who assigned the i -th item to j -th category. We use Siegel and Castellan X  X  ( 1998 ) version of j ;it assumes similar distributions of categories across coders in that it uses the average to estimate the expected agreement (see equation 2). This characteristic has been both criticized (Di Eugenio and Glass 2004 ) and commended (Craggs and McGeeWood, 2005 ); in any case, the current experiment employs 22 coders, making averaging a much better justified enterprise than in studies with very few coders typical in dis-course annotation work (Di Eugenio and Glass, 2004 ). The calculation of the a statistic follows Krippendorff( 1980 ). The j statistic The a statistic Outlying Annotations The dissidence of the 2 members of group A could suggest that the guidelines left some room for a different understanding. These two annotators marked more than twice as many items as anchored as the average of the rest of the people (72% vs. 33%), including many instances that followed syntactic dependency relations. For example, a stretch of text saying  X  X ... I had an idea he looked annoyed, and I said, without thinking... X  X  got analyzed by people in group A into the following relations: idea fi had , annoyed fi looked , thinking fi without . They were the only ones to mark an anchor for idea at all; out of 12 other people who anchored annoyed ,11 named refuse as the anchor and just one had looked ; 6 additional people who anchored thinking all named idea as its anchor. It seems that people from group A were overly influenced by syntax, sometimes at the expense of conceptual relations we were after. Extract from Outland, by Jay Teitel, 1987 contented, the last four or five wanting mainly to be elsewhere. The final two I remember vividly: I passed them driving to and from the University of Toronto in a red 1962 Volkswagen 1500 afflicted with night blindness. The car X  X  lights never worked X  X very dusk turned into a kind of medieval race against darkness, a panicky, mournful rush north, away from everything I knew was exciting, toward everything I knew was deadly. I remember looking through the windows at the commuters mired in traffic beside me and actively hating them for their passivity. I actually punched holes in the white vinyl ceiling of the Volks and then, by way of penance, wrote beside them the names and phone numbers of the girls I would call when I had my own apartment in the city. One thing I swore to myself: I would never live in the suburbs again.

My aversion was as much a matter of environment as it was traffic X  X ne particular piece of the suburban setting: the  X  X  X ruel sun X  X . Growing up in the suburbs you can get used to a surprising number of things X  X he relentless  X  X  X esidentialness X  X  of your surroundings, the weird certainty you have that everything will stay vaguely new-looking and immune to historic soul no matter how many years pass. You don X  X  notice the eerie silence that descends each weekday when every sound it drained out of your neighbourhood along with all the people who X  X e gone to work. I got used to pizza, and cars, and the fact that the cultural hub of my community was the collective TV set. But once a week I would step outside as dusk was about to fall and be absolutely bowled over by the setting sun, slanting huge and cold across the untreed front lawns, reminding me not just how barren and sterile, but how undefended life could be. As much as I hated the suburban drive to school, I wanted to get away from the cruel suburban sun. 21 Reference
 Abstract Lexical cohesion refers to the reader-perceived unity of text achieved by the author X  X  usage of words with related meanings (Halliday and Hasan, 1976 ). This article reports on an experiment with 22 readers aimed at finding lexical cohesive patterns in 10 texts. Although there was much diversity in peoples X  answers, we identified a common core of the phenomenon, using statistical analysis of agreement patterns and a validation experiment. The core data may now be used as a minimal test set for models of lexical cohesion; we present an example suggesting that models based on mutually exclusive lexical chains will not suffice. In addition, we believe that procedures for revealing and analyzing sub-group patterns of agreement de-scribed here may be applied to data collected in other studies of comparable size. Keywords Lexical cohesion  X  Inter-annotator agreement  X  Cohesion 1 Introduction The quest for finding what it is that makes an ordered list of linguistic forms into a text that is fluently readable by people dates back at least to Halliday and Hasan X  X  ( 1976 ) seminal work on textual cohesion, and gains in importance in applied Erratum to: Lang Res Eval Beata Beigman Klebanov  X  Eli Shamir language processing community due to the advent of tasks like text generation 1 and text summarization. 2
A number of  X  X  X exturizing X  X  elements were identified, studied and applied, including lexical repetition (Hearst, 1997 ; Hoey, 1991 ), patterns of entity realization (Barzilay &amp; Lapata, 2005 ; Grosz, Joshi, &amp; Weinstein 1995 ; Karamanis, Poesio, Mellish, &amp; Oberlander, 2004 ; Siddharthan &amp; Copestake, 2004 ), rhetorical organi-zation (Mann &amp; Thomson, 1988 ; Marcu, 2000 ).

The existence of lexical cohesion  X  X exture created by using words with related meanings X  X as also been postulated (Halliday &amp; Hasan, 1976 ). However, in contrast with such phenomena as lexical repetition, repeated reference using pronouns, or rhetorical structure often signalled by specific cue phrases, there are no clear form-based clues as to where lexical cohesion is found in the text. That is, no particular textual items are inherently lexically cohesive 3 ; they are cohesive with respect to certain other lexical elements. Various classes of relations were suggested as carriers of cohesion (e.g. synonymy, hyponymy), but researchers noticed that much of lexical cohesive load is carried by relations felt by the reader, but difficult to classify (Halliday &amp; Hasan, 1976 ; Hasan, 1984 ; Morris &amp; Hirst, 2004 ).

The difficulty of identification and characterization of lexical cohesive relations places the reader in the center of the researcher X  X  attention. The fundamental question is how well readers agree on which items in a text are lexically cohesive. If the agreement is very poor, then the notion of lexical cohesion lacks internal coherence, and translates into something idiosyncratic for every individual reader. If there is substantial agreement, then we can try to develop identification procedures and characterization of those instances of lexical cohesion that are well agreed upon. These are likely to be a part of what the text is expected to induce upon an  X  X  X verage X  X  reader, akin to the way repeated reference is systematically induced 4 , and thus a part of the textual structure.

Experimental, reader-based investigation of lexical cohesion is an emerging enterprise (Morris &amp; Hirst, 2005 ). The main challenge is providing a translation of the notion of patterns of lexical cohesion into a task for the readers, such that (1) the results reflect researcher X  X  intuition about what he/she is after; (2) the guidelines are precise enough to enable replication but open enough to compel readers to use their intuition, which is the main source of information in an elicitation experiment.
Morris and Hirst asked people to group related words, to mark related pairs within each group, to name the relation, and to describe the meaning of each group. The researchers observed the average agreement of 63% on grouping, and lower agreement on identifying pairs of related words within each group.

We suggest that groups of words might emerge as a result of combining small pieces, rather than being primary units of analysis. Word groups are global struc-tures; people might be tempted to make global decisions about the main issues in the text, and mark groups in light of those, not necessarily in a way sensitive to the cohesive impact of specific words in their particular place in the text.

For example, in Morris and Hirst X  X  ( 2005 ) experimental text about movie char-acters and actors as possibly inappropriate role models for children, 4 people in-cluded shooting in the same group as drinking , which was entitled  X  X  X ad behaviors X  X ; one person, however, put shooting together with police , reflecting  X  X  X aw/order/ authority X  X  orientation. 5 The question is whether the first placement of shooting was motivated by a global analysis of the text, where bad behaviors was perceived as a more salient issue than law and authority, whereas had people been given the freedom of marking pairwise relations without a need to form coherent groups, they would have connected shooting to both, or maybe just to the latter, as this could have made a stronger, readily perceivable connection, depending on the exact placement and rhetorical arrangement of the mentions of the three concepts.

In the following section, we present our version of a question to which lexical cohesion is an answer. Section 3 reports on the experiment we performed on 22 readers using this question. Section 4 contains analysis of inter-subject agreement and of the way it was used to identify a highly valid core of the phenomenon. Section 5 exemplifies and discusses the resulting structures. We conclude with an indication of potential usefulness of the resource created as a result of the experiment. 2 From lexical cohesion to anchoring Lexical cohesive ties between items in a text draw on word meanings. Sometimes the relation between the members of a tie is easy to identify, like near-synonymy ( dis-ease/illness ), complementarity ( boy/girl ), whole-to-part ( box/lid ), but the bulk of lexical cohesive texture is created by relations that are difficult to classify (Morris &amp; Hirst, 2004 ). Halliday and Hasan ( 1976 ) exemplify those with pairs like dig/garden , belson, 1977 ): Certain things are expected in certain situations, the paradigm example being menu, tables, waiters and food in a restaurant.

However, texts sometimes start with descriptions of situations where many pos-sible scripts could apply. A famous literary text starts with Mother died today . What are the generated expectations? A description of an accident, or of a long illness? A story about what happened to the family afterwards? The speaker X  X  feeling of loneliness? Funeral arrangements? The mother X  X  last wish and its fulfillment? Many directions are easily thinkable at this point.

We suggest that rather than generating predictions, scripts could provide a basis for abduction. Once any normal direction is actually taken up by the following text, there is a connection back to whatever makes this a normal direction, according to the reader X  X  commonsense knowledge (possibly coached in terms of scripts). Thus, had the text developed the illness line, one would have known that it can be best explained-by/blamed-upon/abduced-to the previously mentioned lethal outcome. We will say in this case that illness is anchored 7 by died , and mark it illness fi died . The cited line opens Albert Camus X  novel The Stranger ( 1962 ), that proceeds thus: Mother died today. Or, maybe, yesterday; I can X  X  be sure. The telegram from the Home says YOUR MOTHER PASSED AWAY FUNERAL TOMORROW ...

The mention of yesterday refocuses the first sentence such that now it is seen as describing something that happened today, so continuing with recent history makes sense ( yesterday fi today ). Later, telegram is seen in the light of death being an event that requires immediate reaction from relatives, so an urgent announcement is in order ( telegram fi died ). Both these developments could hardly have been pre-dicted from the first sentence X  X t is probably part of The Stranger X  X  strangeness that he chooses to recount them rather than other things after such an opening, but the text does not lose cohesion. It is these kinds of connections X  X hat is anchored by what X  X hat we want to elicit from readers. 3 Experimental design We chose 10 texts for the experiment: 3 news articles, 4 items of journalistic writing, and 3 fiction pieces. All news texts and one fiction story were taken in full; others were cut at a meaningful break to stay within 1000 word limit. 8 The texts were in English X  X riginal language for all but two literary texts.

Our subjects were 22 students at the Hebrew University of Jerusalem, Israel; 19 undergraduates and 3 graduates, all aged 21 X 29 years, studying various sub-jects X  X omputer science, cognitive science, biology, history, linguistics, psychology. Three participants named English their mother tongue; the rest claimed very high proficiency.

All participants first read the guidelines (Beigman Klebanaov &amp; Shamir, 2005 ) that contained an extensive example annotation, as well as short paragraphs explaining various technical matters (how to mark multiple and complex anchors), and highlighting some conceptual issues. In particular, people were asked to make an effort to separate personal knowledge from what they think is common knowl-edge, and general relations from instantial ones that are specifically constructed in the text using co-reference or predication. 9 Participants then performed a trial annotation on a short news story, after which meetings in small groups were held for them to bring up any questions.

The experiment then started. For each of the 10 text, each person was given the text to read, and a separate wordlist on which to write down annotations. The wordlist contained words from the text, in their appearance order, excluding verbatim and inflectional repetitions. 10 Wordlists numbered between 175 and 339 items. For example, the beginning of The Stranger cited above corresponds to the wordlist in Fig. 1 .

People were instructed to read the text, and then go through the wordlist and ask themselves, for every item on the list, which previously mentioned items help the easy accommodation of this concept into the evolving story, if indeed it is easily accommodated, based on the commonsense knowledge as it is perceived by the annotator. They were encouraged to use a dictionary if needed. 11 Figure 2 shows a possible annotation of the cited extract. 4 Analysis of experimental data We now turn to a detailed presentation of data analysis; section 4.4 provides its summary.

Most of the existing research in computational linguistics that uses human annotators is within the framework of classification, where an annotator decides, for every test item, on an appropriate tag out of the pre-specified set of tags (Marcus, Santorini, &amp; Marcinkiewicz, 1993 ; Poesio &amp; Vieria, 1998 ; Webber &amp; Byron, 2004 ).
Although our task is not that of classification, we start from a classification sub-task, and use agreement figures to guide subsequent analysis. We use the by now standard j statistic 12 (Carletta, 1996 ; Craggs &amp; McGeeWood, 2005 ; Di Eugenio &amp; Glass, 2004 ; Siegel &amp; Castellan, 1998 ) to quantify the degree of above-chance agreement between multiple annotators, and a statistic for analysis of sources of unreliability (Krippendorff, 1980 ). The relevant formulas are given in Appendix B. 4.1 Classification sub-task Classifying items into anchored/unanchored can be viewed as a sub-task in our experiment: Before writing any particular item as an anchor, the annotator asked himself whether the concept at hand is easy to accommodate at all. Agreement on this task averages j = 0.45 (texts range from j = 0.36 to j = 0.51). These figures do not reach j = 0.67, the accepted threshold for deciding that annotators were working under similar enough internalized theories 13 of the phenomenon; however, the figures are high enough to suggest considerable overlaps.

Seeking more detailed insight into the degree of similarity of these theories, we follow the procedure described by Krippendorff ( 1980 ) to find outliers. We calculate the category-by-category co-markup matrix for all annotators 14 ; then for all annotators except one, and by subtraction find the portion that is due to this one annotator. We then regard the data as two-annotator data (one versus everybody else), and calculate agreement coefficients. We rank annotators according to the degree of agreement with the rest, separately for each text, and average over the texts to obtain the conformity rank of an annotator, between 1 and 22. The lower the rank, the less compliant the annotator.

Annotators X  conformity ranks cluster into 3 groups shown in Table 1 . Group A are consistent outliers X  X heir average rank for the 10 texts is below 2. Group B are, on average, in the bottom half of the annotators with respect to agreement with the common, whereas members of group C display relatively high conformity.

It is possible that groups A, B and C have different interpretations of the guidelines, but our idea of the common (and thus the conformity ranks) is dominated by the largest group, C. Within-group agreement rates are shown in the last column of Table 1 . The two annotators in group A seem to have an alternative under-standing of the task, being much better correlated between each other than with the rest of the people; appendix C gives more details about their annotations.

The numbers for the other two groups could support two scenarios schematically depicted in Fig. 3 : (a) each group settled on a different idea of the phenomenon, where group C is in better agreement on its version than group B on its own; (b) people in groups B and C have basically the same interpretation, but members of C are more systematic in carrying their idea through. It is crucial for our analysis to tell those apart X  X n the case of multiple stable interpretations it is difficult to talk about the anchoring phenomenon; in the core-periphery case, there is hope to identify the common core emerging from 20 out of 22 annotations.

If the two groups have different interpretations, adding a person p from group C to group B would usually not improve the agreement in the target group (B), since p is likely to have a different interpretation than B X  X  members. If, however, the two groups have the same interpretation, moving p from C to B would usually improve the agreement in B, since, coming from a more consistent group, p  X  X  agreement with the common interpretation is expected to be better than that of an average member of group B.

We performed this analysis on groups A and C with respect to B. Adding members of A to B improved the agreement in B only for 1 out of the 10 texts. Thus, the relationship between the two groups seems to be that of different interpretations. Adding members of C to B resulted in improvement in agreement in at least 7 out of 10 texts for every added member. Thus, the difference between groups B and C is that of consistency, not of interpretation; we may now search for the well-agreed-upon core of this interpretation. We exclude the two outliers (group A) from sub-sequent analysis. The remaining group of 20 annotators exhibits an average agree-ment of j = 0.48 on anchored/unanchored classification, texts ranging from j = 0.40 to j = 0.54. The improvement in agreement after the exclusion of outliers is sig-nificant at p &lt; 0.01 (Wilcoxon matched-pairs signed-ranks test, n = 10, H 1 : agree-ment scores improved after excluding the two outliers).
 4.2 Finding the common core of the classification We now seek a reliably classified subset of the data. The main concern is not including cases of agreement that could be due to chance with intolerably high probability. To estimate this probability, we induce 20 random pseudo-annotators from the 20 actual ones: Each pseudo-annotator marks the same proportion of items as anchored as the respective actual annotator, but chooses the items at random. We model this by letting the i -th pseudo-annotator toss a coin with p (heads) = p i , independently for every item, where p i is the proportion of items marked as anchored by the actual annotator i in the whole of the dataset; random variables x i represent the outcomes of such tosses. A random variable S ranging between 0 and 20 says how many pseudo-annotators marked an item as anchored, having tossed each his own coin, independently of each other. The expectation and variance of S are given by E ( S )= S i =1 20 p i and V ( S )= S i =1 20 p i (1 X  p i ).

We assume that numerous repeated trials on S are normally distributed. We calculate the probability p (Val) that a single observation from a normal distribution with parameters l = E ( S ), r  X  lating for Val = 0.5, we test how likely it is that none of the pseudo-annotators pseudo-annotators anchored a given item. 15
Now, 1 X  p (Val) is the confidence with which we can reject the hypothesis that certain observed agreement levels are due to chance, since S models chance agreement. We seek values that allow high confidence levels, both for anchoredness and unanchoredness decisions. Thus, with 99% overall confidence ( p &lt; 0.01), we may trust unanimous decisions on unanchoredness, and decisions of at least 13 out of 20 people that an item is anchored. 16 Allowing 95% confidence on each of the two decisions separately, cases of at most 2 markups can be reliably considered as unanchored ( p (2.5) = 0.0256), and anchoring by at least 11 people is enough for anchoredness (1 X  p (10.5) = 0.0290). For the subsequent analysis, we choose the 99% confidence version.

Figure 4 plots the actual data for The Stranger , along with the reliability cutoffs. 4.3 Identifying anchors for anchored items The next step is identifying reliable anchors for the anchored items. We calculated average anchor strength for every text: the number of people who wrote the same anchor for a given item, averaged on all reliably anchored items in a text. The average anchor strength is between 5 and 7 for the different texts. Taking only strong anchors (anchors of at least the average strength), we retain about 25% of all an-1261 pairs of reliably anchored items with their strong anchors, between 54 and 205 per text; we refer to this set as core data .

Strength cut-off is a heuristic procedure; some strong anchors were marked by as few as 6 out of 20 people, making it unclear whether they can be trusted as embodiments of the core of the anchoring phenomenon in the analyzed texts. We thus devised an anchor validation experiment, reasoning as follows: In the original experiment, people were asked to generate all anchors for every item they thought was anchored. In fact, people generated only 1.86 anchors per anchored item. This makes us think that people were most concerned with finding an anchor, making sure that something they think is easily accommodatable is given at least one pre-ceding item to blame for that; they were less diligent in marking up all such items. We conjectured that judging presented anchors would be easier than finding ones, so in the validation experiment people were asked to cross over anchors they did not agree with. 17 of the 20 annotators participated in the validation experiment. We chose 6 out of the 10 texts; each person performed validation on 3 out of those, such that every text received 7 X 9 validation versions. For each text, readers were presented with the same list of words as in the first part, only now each word was accompanied by a list of anchors. For each item, every anchor generated by some reader was included; the order of the anchors had no correspondence with the number of people who gen-erated it. A small number of items also received a random anchor X  X  randomly chosen word from the preceding part of the wordlist. Figure 5 shows such a list for the beginning of The Stranger. 17
Ideally, if lack of markup is merely a difference in attention but not in judgment, all non-random anchors should be accepted. To see the distance of the actual results from this scenario, we calculate the total mass of votes as number of anchored-anchor pairs times number of people, check how many are accept votes, and average over texts. People accepted 62% of non-random pairs, 94% of core data pairs (texts scoring between 90% and 96%), and only 15% of pairs with a random anchor. 4.4 Summary of data analysis We used an anchored/unanchored classification sub-task to establish the existence of common interpretation among 20 out of 22 annotators and to identify items at the reliably agreed-upon core of this interpretation. 18 For items reliably classified as anchored, we identified strong anchors and validated those in an additional exper-iment: Even people who did not actually generate them, accepted them as valid anchors (94% average acceptance rate). We may thus regard the core data as rel-atively uncontroversial manifestation of anchoring patterns in the examined texts, reliably found by a group of 20 readers. 5 From anchoring to lexical cohesion This section discusses the implication of the current study for modeling and anno-tation of patterns of lexical cohesion. First, we show that the pairwise links organize into interesting global patterns (section 5.1). We then discuss the part-of-speech composition of the cohesive pairs (section 5.2), the character of unreliable annota-tions (section 5.3), and the issue of multi-word items (section 5.4).
 5.1 Cohesion beyond word pairs To exemplify the observed patterns, we organize the core data in a graph, where a downward arrow from b to a means that a is a strong anchor for b . Figure 6 shows the two largest connected components for the first 12 sentences of Jay Teitel X  X  1987 Toronto Magazine article titled Outland (shown as appendix D), reproduced in Morris and Hirst X  X  ( 1991 ) and analyzed therein into lexical chains that  X  X  X elineate portions of text that have a strong unity of meaning X  X  (page 23). Numbers inside nodes correspond to Morris and Hirst X  X  chain numbers; no number means a word was not assigned to any chain.

Inspecting the upper component, we see that its right-hand side is rooted in driving and the left-hand one in afflicted . Walking up the structure we notice that the connection between the two halves hangs on a single link, going from lights to car . Indeed, lights is anchored by car ,by blindness and by night , which reflects the major rhetorical role played by lights in this text X  X hat of connecting driving issues to environmental lack of light (darkness, dusk, night) and to human ailment (blindness, years] driving ... in a Volkswagen afflicted with night blindness. The car X  X  lights never worked ... X  X 
In the second component we notice the pivotal position of neighbourhood ,asa social entity (community, collective, people), as a kind of residential unit (city, suburbs, apartment), and as a physical place (environment, surroundings).

These examples undermine an assumption sometimes made in applied research that lexical cohesive patterns can be represented by mutually exclusive chains apartment_1 suburbs_1 2004 ), realized in Morris and Hirst X  X  ( 1991 ) analysis as well. 19 Putting every word in at most one chain misses important lexically expressed connections between meaning components that are registered by human readers. 5.2 Part-of-speech Another issue in lexical cohesion research is the relationship between the part-of-speech (henceforth, POS ) of an item and its participation in lexical cohesion: It is usually assumed that content words are solely responsible for the lexical cohesive texture (Halliday &amp; Hasan, 1976 ; Morris &amp; Hirst X  X  1991 ). Moreover, applied research tends to concentrate on the nominal texture (Al-Halimi and Kazman, 1998 ; shows the split of the wordlists and of the core data by POS.

The table indicates that nouns favor cohesive structures: they are twice more frequent within core items than in the wordlists. Adjectives and verbs keep to their proportion; adverbs and proper nouns tend not to participate in lexical cohesion. Function words were occasionally marked by the annotators, but not systematically enough to make it to the reliable data; core pairs with the dollar sign ($) and with 747 (the number of a popular Boeing model) are rare exceptions.

The implication for an algorithm that would try to replicate core annotations is clear: by looking only at common nouns, verbs and adjectives, it is possible to ignore almost 40% of first mentions, and retain a chance for above 90% recall on the core data. For some texts, exclusion of proper names might cause a more significant recall loss, as in a news story where 10% of core anchors are proper names.

The implications for future annotation work are not so clear. We note the vari-ability in proportions of the different POS both in the wordlists and in the core annotations: Whereas proper names are usually rather marginal, in one of the news texts they comprise 16% of wordlist items, and participate as anchors in 10% of core pairs in another news story. Similarly, adverbs usually carry little cohesion, but in some texts they are not so lightweight; in fact, yesterday , today , tomorrow form salient cohesive patterns in The Stranger .

As for function words, it is tempting, and might indeed be worthwhile, to cut the wordlists down in quarter, likely preserving (almost) all of the cohesive texture. A minor caveat has to do with the specific way in which data was presented in our experiment: The annotators had the wordlist aligned with the actual text through the use of the sharp sign, as shown in Fig. 1 . However, the more items are skipped, the more cumbersome and unhelpful such presentation will be, making the alignment with the text more difficult. The second caveat is of a more general nature: In an exhaustive annotation process, we might not want to miss even the little amount of texture that might be residing in the function words. 5.3 Unreliable annotations The analysis in section 4.2 allows identification of the least reliable annotations as well: those are items marked as anchored by the number of people closest to the expectation of the variable S that models chance agreement, which is between 6 and 7 in our data. Such items, exemplified in Fig. 7 , are neither reliably anchored nor reliably unanchored.

Checking these examples with respect to the text, we suggest that they might reflect an interplay between conflicting influences, such as textual distance, refer-ence, syntax, and the non-specificity of a good anchor.

Thus, pairs like vividly fi remember , holes fi punched stand in syntactic dependency relations in the text, which could intensify their perceived cohesion. A similar influence of referential cohesion is likely in ceiling fi car , girls fi names , girls fi phone_numbers , suburbs fi outland : the text discusses the ceiling of a car, phone numbers and names of girls, and refers to suburbs as outland (this is a possible understanding of the title).

In other cases, it seems that the textual rendering of the concepts works against their cohesiveness X  X or example, reminding and remember are more than 150 wordlist items apart, so their cohesiveness, while still perceptible, is backgrounded.
Cases like medieval, swore, certainty are characterized by a large variety of pro-posed anchors, and by the small number of people who wrote each anchor. The annotators are likely to have vaguely felt that the item ought to be anchored in the text, but had a difficulty anchoring it to any specific preceding item.

We think that a possible interpretation of the unreliably marked cases is that lexical cohesion is not a binary phenomenon, and such cases are examples of an intermediate degree of cohesion: They are anchored in the text enough for some people to notice it, but the anchoring is not as salient as for items marked by a decisive majority of the annotators. 5.4 Multi-word items Multi-word items are an interesting issue with respect to lexical cohesion. On the one hand, the split into atomic units, as done in the current experiment, seems rather artificial in cases like New York ; indeed, in applied research, phrases are sometimes included as units of analysis (Barzilay and Elhadad, 1997 ; Stokes et al., 2004 ).
On the other hand, the decision regarding the boundaries of complex items is not always easy to make. For example, is New York City a unit, or does City stand on its own? Is the Journal part of Wall Street Journal a separately functioning item, and is Wall Street such an item as well? In fact, the core data contains the pairs busi-ness fi wall_street and report fi journal , where the anchors come from Wall Street Journal in the relevant text.

The main reason for not including phrases in the wordlists is that phrases, as opposed to atomic units, are not marked orthographically in the text, but are in the readers X  eyes. This means both that things often taken to be phrases could be per-ceived in separation as well (as in the Wall Street Journal example above), and that things that are compositional and thus are not good a-priori candidates for phrase-hood, are sometimes perceived as a unit. For example, last_minute is a core-data anchor for hurrying in one of our texts.

In addition, we believe that permitting the split of phrases is useful in trying to pinpoint the part that carries the meaning component that is mostly responsible for the anchoring link. Barzilay and Elhadad ( 1997 ) default to the head noun of the compound as the part that is responsible for the cohesive behavior of the complex item; cases like the aforementioned business fi wall_street suggest that this is not always justified.
Furthermore, the identity of a sequence of atomic items as a unit is often tied to its functioning as a single referring expression: It is possible that a text mentioning Wall Street Journal will keep discussing the journal, never mentioning Wall Street as a separate entity. There is thus a potential tension between referential and lexical structure X  X eaders might or might not perceive parts of a single referring expression as separately functioning lexical units.

The handling of phrases is clearly an issue for future experimental research into lexical cohesion, and into its relationship with other structures in the text. 6 Conclusion This article reports on a reader-based experiment that used common knowledge based conceptual anchoring to operationalize the notion of lexical cohesion . There was much diversity in peoples X  responses. However, we developed statistics-based procedures, which, for this experiment, identified a common core of the phenome-non. This core was validated in an additional experiment with the same group.
The core data may now be used as a resource in computational exploration of lexical cohesion. It can serve as an intrinsic test set for models of lexical cohesion: Any good model should at least get the core part right. Currently, models of lexical cohesion are only evaluated in an extrinsic fashion X  X y their usefulness for a given application (Al-Halimi and Kazman, 1998 ; Barzilay &amp; Elhadad, 1997 ; Green, 1998 ; Hirst &amp; Budanitsky, 2005 ; Stairmand, 1997 ; Stokes et al., 2004 ) . As we saw, models based on mutually exclusive lexical chains built along  X  X  X eaning components X  X  are expected to miss salient connections; such shortcomings can be detected using the intrinsic test, before any high-level application is attempted. To encourage such experimentation, the core data is publicly available at the first author X  X  homepage. 20
An additional use of the data, suggested by an anonymous reviewer, is a sequel experiment in which subjects would be asked to classify the anchoring pairs according to the type of relation they embody. This would allow separation between the stage of identification of cohesive pairs and that of classification, which we believe is important, as merging the two might bias the annotators towards marking pairs with easy-to-classify relations. Such data would be useful for finding out which lexical relations participate in the cohesive structures, and to what extent.
Another contribution of this work is a demonstration of the use of agreement statistics not just for annotation reliability judgment, but also for a more detailed exploration of annotation patterns exhibited by a relatively large group of subjects. The development of resources related to semantic/pragmatic tasks is impeded by the difficulty to attain high inter-annotator agreement (Morris &amp; Hirst, 2005 ; Poesio &amp; Vieira, 1998 ). We believe that finer agreement analysis techniques will lead to more effective and insightful use of experimental data that exhibits robust agreement but not enough to warrant a conclusion that the whole of it has been reliably annotated. Appendix A Instantial vs. General Knowledge X  X xtract from Guidelines A text is telling us a story about certain entities. For example, it could tell us that children went out in a boat with their father, there was a storm but they survived because the father was an experienced sailor. Whereas relating father back to children, storm to boat, survival to storm and sailor to boat would be based on the general knowledge of what a family is, where you go in a boat and what can happen there, a connection between sailor and father is not something general/typical, it is created only because in this particular text the two descriptions apply to the same person. That is, the relation is instantial, pertaining to a certain instance (a text), not a general one. Since we are after general knowledge, please do not mark instantial connections.
 B Measures of Agreement Let N be the number of items to be classified; m  X  X he number of categories to classify into; k  X  X he number of raters; n ij is the number of annotators who assigned the i -th item to j -th category. We use Siegel and Castellan X  X  ( 1998 ) version of j ;it assumes similar distributions of categories across coders in that it uses the average to estimate the expected agreement (see equation 2). This characteristic has been both criticized (Di Eugenio and Glass 2004 ) and commended (Craggs and McGeeWood, 2005 ); in any case, the current experiment employs 22 coders, making averaging a much better justified enterprise than in studies with very few coders typical in discourse annotation work (Di Eugenio and Glass, 2004 ). The calculation of the a statistic follows Krippendorff( 1980 ).
 The K statistic The a statistic C Outlying Annotations The dissidence of the 2 members of group A could suggest that the guidelines left some room for a different understanding. These two annotators marked more than twice as many items as anchored as the average of the rest of the people (72% vs. 33%), including many instances that followed syntactic dependency relations. For example, a stretch of text saying  X  X ... I had an idea he looked annoyed, and I said, without thinking... X  X  got analyzed by people in group A into the following relations: idea fi had , annoyed fi looked , thinking fi without . They were the only ones to mark an anchor for idea at all; out of 12 other people who anchored annoyed ,11 named refuse as the anchor and just one had looked ; 6 additional people who anchored thinking all named idea as its anchor. It seems that people from group A were overly influenced by syntax, sometimes at the expense of conceptual relations we were after. D Extract from Outland, by Jay Teitel, 1987 contented, the last four or five wanting mainly to be elsewhere. The final two I remember vividly: I passed them driving to and from the University of Toronto in a red 1962 Volkswagen 1500 afflicted with night blindness. The car X  X  lights never worked X  X very dusk turned into a kind of medieval race against darkness, a panicky, mournful rush north, away from everything I knew was exciting, toward everything I knew was deadly. I remember looking through the windows at the commuters mired in traffic beside me and actively hating them for their passivity. I actually punched holes in the white vinyl ceiling of the Volks and then, by way of penance, wrote beside them the names and phone numbers of the girls I would call when I had my own apartment in the city. One thing I swore to myself: I would never live in the suburbs again.

My aversion was as much a matter of environment as it was traffic X  X ne particular piece of the suburban setting: the  X  X  X ruel sun X  X . Growing up in the suburbs you can get used to a surprising number of things X  X he relentless  X  X  X esidentialness X  X  of your surroundings, the weird certainty you have that everything will stay vaguely new-looking and immune to historic soul no matter how many years pass. You don X  X  notice the eerie silence that descends each weekday when every sound it drained out of your neighbourhood along with all the people who X  X e gone to work. I got used to pizza, and cars, and the fact that the cultural hub of my community was the collective TV set. But once a week I would step outside as dusk was about to fall and be absolutely bowled over by the setting sun, slanting huge and cold across the untreed front lawns, reminding me not just how barren and sterile, but how undefended life could be. As much as I hated the suburban drive to school, I wanted to get away from the cruel suburban sun. 21 Reference
