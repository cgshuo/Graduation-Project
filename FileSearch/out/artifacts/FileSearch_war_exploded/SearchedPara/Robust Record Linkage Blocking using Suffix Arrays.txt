 Record linkage is an important data integration task that has many practical uses for matching, merging and dupli-cate removal in large and diverse databases. However, a quadratic scalability for the brute force approach necessi -tates the design of appropriate indexing or blocking tech-niques. We design and evaluate an efficient and highly scal-able blocking approach based on suffix arrays. Our suffix grouping technique exploits the ordering used by the index to merge similar blocks at marginal extra cost, resulting in a much higher accuracy while retaining the high scalability of the base suffix array method. Efficiently grouping simi-lar suffixes is carried out with the use of a sliding window technique. We carry out an in-depth analysis of our method and show results from experiments using real and synthetic data, which highlights the importance of using efficient in-dexing and blocking in real world applications where data sets contain millions of records.
 H.3 [ Information Storage And Retrieval ]: Content Anal-ysis and Indexing X  Indexing methods Algorithms, Performance Record Linkage, Blocking, Suffix Arrays
Record linkage is an essential data integration technique that is increasing in importance as more and more data is collected and stored. This technique can be applied to any situation where two or more sets of data need to be linked together and there is an absence of a uniquely identifying key across these data sets. Alternatively, the same link-ing approaches can be used to find matches among records in the same data set for the purposes of duplicate removal [18]. In both of these cases, the main problem that must be overcome is the presence of noise and small differences be-tween records in the data that are to be matched together. Record linkage is therefore an approximate matching tech-nique, aiming to provide the best possible match given the available information. These linkage tasks are common and crucial early steps in most large data mining projects. Thei r main use is to provide a wealth of information that is not readily available under the standard practice of keeping mu l-tiple separate databases for archival or analysis purposes .
As the availability and quantity of data grows over time, so do the number of databases that are created and ulti-mately discarded as legacy systems. These are all useful sources of information if effort is undertaken to link them together. More specifically, record linkage can drasticall y increase the information available for purposes such as lar ge medical health systems [9], business analytics, fraud dete c-tion [17], demographic tracking, government administrati on, and even national security.

Record linkage systems typically consist of three main parts. Blocking is first used to select candidate records to match against the selected record. Record comparison is then carried out, usually using specifically tailored simil arity functions. A classification model is then used to determine whether the two records are a match or a non-match. The matching process is by nature a very computationally inten-sive task. For matches or duplicates to be found across two data sets of size n , up to n 2 comparisons may be required, making a brute force nesting approach infeasible in prac-tice with large data sets. It is therefore very important to consider techniques to reduce the number of pairwise com-parisons that must be made, by making some assumptions about which factors of the data will almost always lead to matches or non-matches. This indexing process is referred to as blocking in the context of record linkage. Blocking techniques can be tailored to favour either the accuracy or efficiency of the linkage task, in a tradeoff manner. The main aim is to make record linkage feasible on large data sets by greatly reducing the number of record pair comparisons that need to be carried out, at the cost of a usually small loss in accuracy. The design of a robust blocking technique based on suffix arrays is the focus of this paper.
A large variety of blocking methods have been created, with each one providing at least a niche benefit for specific data types. One of the first methods to be proposed uses a basic exact-match index on a few key fields (chosen record attributes), and for any one record that requires matching, selects candidate records to match against based on exact matches in one of these key fields. This approach is known as  X  X raditional X  blocking [3]. This basic approach neverthel ess can boast a high accuracy, at the expense of a relatively low matching efficiency. However, the high accuracy and simple implementation make this the method of choice in many industrial settings. The authors of this article are involv ed with the production of a large-scale data mining system that utilises record linkage in the industry. Traditional block ing was chosen for this application, and labeled data is availab le due to the model training process that has been carried out. However, we are able to use this real labeled data to test the performance of different blocking techniques in a direct experimental comparison.

An important reason for choosing a non-traditional block-ing technique is due to the ubiquitous presence of errors in the data. The use of standard exact matches will miss a cor-rect record match even with the slightest difference in field values. To compensate for this, many indexed fields need to be used, and the set of candidate record pairs rapidly in-creases in size, adversely affecting the time required to car ry out the matching. The most important goal of record link-age is to find these similar but non-identical matches with-out adversely reducing performance by such a large amount, and therefore specialised techniques designed to index dat a of this kind are necessary. One such highly-efficient existin g blocking method is called Suffix Array blocking [1], which is our basis for an improved blocking method that retains the high efficiency of Suffix Array blocking while also increas-ing accuracy to an acceptable level by avoiding the miss-classification of records into incorrect blocks.

We give a small example to demonstrate our proposed im-provement to the Suffix Array blocking method. Given two records r 1 =  X  X ohn Smith, 10 Plum Road X  and r 2 =  X  X ohn Snith, 10 Plom Rd. X  to match against each other, we select given name and surname as the key blocking fields. Concate-nating the values of these fields together results in the stri ngs b =  X  X ohnSmith X  and b 2 =  X  X ohnSnith X . These strings are called Blocking Key Values (BKVs) in the context of record linkage. When the minimum suffix length parameter is set to 4 (to be discussed in Section 2.2), Suffix Array blocking will generate suffixes  X  X ith X ,  X  X mith X ,  X  X Smith X ,  X  X nSmith X  , etc. for b 1 , and similarly for b 2 . The suffixes are added to the indexing structure and sorted, as shown in Table 1.
While highly efficient, standard Suffix Array blocking is not able to match records that exhibit qualities such as in this example, where none of the suffixes from r 1 and r 2 match. Our proposed improvement takes effect when the suffixes are added to the indexing structure. This structure holds an alphabetically sorted list of suffixes to enable fast querying for matches against any given input suffix, and is used to find candidate records for matching against any new record. The suffixes  X  X Smith X  and  X  X Snith X  as well as  X  X n-Smith X  and  X  X nSnith X , among others, are adjacent to each other in the ordered list for this example. By comparing adjacent suffixes and grouping together those that exhibit a high degree of similarity, we can carry out a form of cluster-ing or grouping of the blocking result. By grouping similar Table 1: Suffixes generated from two blocking key values (BKVs), where record 1 corresponds to the BKV of  X  X ohnSmith X  and record 2 to the BKV of  X  X ohnSnith X . Minimum suffix length is 4. Jaro simi-larity refers to the Jaro [13] measure as a similarity function between the suffix string in one row and the suffix in the following one. suffixes from r 1 and r 2 we can ensure that these records are added to the same block, which is the desired outcome and and improvement over standard Suffix Array blocking. We cover the improvement in more detail in Sections 3 and 4.
Our main contribution is a substantial improvement to the Suffix Array blocking technique [1], together with an in-depth analysis and experimental results showing the ef-fectiveness of the technique on real and synthetic data. We compare the improved technique against the base technique of Suffix Array blocking, as well as the well-known tradi-tional blocking method. We show that Improved Suffix Ar-ray blocking is able to attain a level of accuracy similar to the highly accurate traditional blocking technique, as wel l as being able to acquire this result by making many fewer full comparisons between data records, comparable to the highly efficient Suffix Array blocking method. More impor-tantly, with careful parameter selection in our experiment s, we found that Improved Suffix Array blocking can attain a 20% increase in accuracy over standard Suffix Array block-ing, with less than a 5% loss in efficiency. The accuracy of Improved Suffix Array blocking also remains within 10% of traditional blocking, while enjoying an efficiency increa se of more than 95% over this standard technique. A high efficiency becomes more important as the data set used in-creases in size, a critical point for data in the order of hun-dreds of millions of records. Scenarios of this type are com-mon in the industry, including the domain of the large-scale record linkage project that we are comparing our results against.

Section 2 introduces the relevant work related to record linkage. In Section 3 we describe our proposed improve-ments to standard Suffix Array blocking, and carry out a more in-depth analysis in Section 4. Section 5 details our experimental method and parameters, the results are dis-cussed in Section 6, and we summarise our conclusions in Section 7. The term  X  X ecord linkage X  was first used by Dunn [6] and Marshall [15], and Fellegi and Sunter [8] proposed a theory based on statistical classification.
 Recent advances in record linkage were undertaken by Aizawa [1], and processes for record linkage projects and methods were improved by Christen [4]. Indexing tech-niques, or blocking methods as they are known in the context of record linkage, were quickly recognised as a key compo-nent for efficiency purposes. Blocking algorithms typically contain extra functionality over standard indexing, to sol ve specific record linkage issues. Blocking is especially impo r-tant due to the inherently high n 2 scalability of unoptimised record linkage. Blocking solutions strive to reduce the num -ber of candidate records for comparison as much as possible, while still retaining an accurate result by ensuring that ca n-didate records that would match the query record are not left out of the candidate set due to the blocking rules.
A variety of blocking methods are currently used in record linkage procedures, with the most well-known ones includ-ing traditional blocking, sorted neighbourhood [11], Q-gr am based blocking [3], Canopy Clustering [16], string map base d blocking [14] and Suffix Array blocking [1].

All blocking methods define a set of key fields from the data to be matched, that are used to determine which block (or blocks) each record is to be placed into. Many of these approaches require a single string to be used as the key on which to find the correct block. Therefore, the values of the key fields are typically concatenated together into one long string. This string is called the Blocking Key Value (BKV) [10]. The selection of key fields to include in the BKV as well as the ordering of these fields is important to consider. A suitable BKV should be the attribute or combination of attributes which are as identifying as possible, uniformly distributed, and having a low error probability.

Hernandez and Stolfo [11] proposed the sorted neighbour-hood blocking method. This approach begins by sorting the input data, then moves a sliding window of size w over the data file, comparing records against each other if they exist in the window at the same time. The time complexity of the sorted neighbourhood method is O ( n log n + wn ) where n is the number of records in each of the two data sets being linked.

The Q-gram based blocking method is achieved by trans-forming the blocking key values into lists of q-grams and creating all combinations of sub-lists. Christen [3] propo sed that q-gram based blocking can achieve better blocking qual -ity results than both standard blocking and the sorted neigh -bourhood approach. However, the number of sub-lists gener-ated depends upon the value of the parameter q , the length of the sub-strings used. The time complexity of Q-gram based blocking is O ( n log n + n 2 b ) where n is the number of records in each of the two data sets, and b is the number of generated blocks [3].

McCallum et al. [16] first proposed Canopy Clustering, a solution for clustering large, high dimensional data sets . This method can be applied to blocking by creating blocks containing records which can be found within the same can-opy cluster. Experimental results are shown for linking bib -liographic citations from the reference sections of resear ch papers [16]. The time complexity of Canopy Clustering is O ( of canopies, f is the average number of canopies a record be-longs to, and k is the number of clusters searched for using k-means (based on the original data) [3].

String map based blocking is based on mapping the block-ing key values (assumed to be strings) to objects in a mul-tidimensional Euclidean space. Jin et al. [14] modified the FastMap algorithm to create StringMap, which has a linear complexity in the number of strings to be mapped.
Christen [4] compared and evaluated these blocking tech-niques, and modified two of them to make them more robust with regards to parameter settings, an important considera -tion for any algorithm that is to be considered for real-worl d applications. The experimental results showed that there are large differences in the number of true matched candi-date record pairs generated by the different techniques, whe n tested using the same data sets. It was also discovered that many of these algorithms were unstable with the selection of the parameter values. We limit the introduction of new pa-rameters with our proposed improvement in order to avoid this potential problem.

Record linkage is a large research area containing many important aspects [2][12]. Our paper focuses exclusively o n the sub-area of finding effective blocking techniques.
Traditional blocking is well-known and is often used in practical applications. This approach works by only compar -ing against records that have the same blocking key value, for example, only comparing records that have the same postcode [13]. The blocking keys are usually chosen to be very general in order to produce a high quality result, while also producing a reasonable reduction in the amount of data required to compare against for each record to be matched. Usually more than one key field is chosen to build the block-ing key value. In the industrial record linkage application used for comparison, the selected key fields are given name, surname, and date of birth. When carrying our matches with the record  X  X ohn Smith, 01/01/1960, 10 Plum Road X , traditional blocking will select all candidate records tha t ex-actly match with  X  X ohn X  on given name, plus all candidate records that exactly match with  X  X mith X  on surname, plus all records that exactly match with  X 01/01/1960 X  on date of birth. A common modification to add a degree of  X  X uzziness X  to the matching is done by applying phonetic encoding (such as Soundex) to the key fields.

One major weakness of traditional blocking is that errors in all of the blocking key values will result in records being inserted into the wrong block. Secondly, the size of each block is generally quite large, causing many unnecessary comparisons to be carried out. Finally, another drawback is that the sizes of the blocks generated depend upon the frequency distributions of each individual field used in the the blocking key value [4]. When fields are combined into a BKV such as for regular Suffix Array blocking, a drastic reduction in block size is typically encountered. The time complexity of traditional blocking is O ( dn log n ) where n is the number of records in each of the two data sets that are being matched and d is the number of key fields chosen [7]. Akiko Aizawa and Keizo Oyama [1] proposed the Suffix Array blocking technique as a fast and efficient blocking method for large scale record linkage. We utilise the Suf-fix Array Blocking plus Key Blocking approach from this paper, with an additional adjustment to handle string char-acters as the individual tokens. Analysis of this technique against many recent alternatives [4] found that the efficienc y gain is very high for this method, but the accuracy can suffer with standard data sets and when the blocking key value is chosen by concatenating several key fields, as is the standar d for comparison.

The main idea of Suffix Array blocking is to insert block-ing key values and their variable length suffixes into a suffix array based inverted index [4]. For example, when the min-imum suffix length parameter ( l ms ) is 4, a BKV of  X  X ohn-Smith X  will be decomposed into the suffix array containing the suffixes  X  X ith X ,  X  X mith X ,  X  X Smith X ,  X  X nSmith X ,  X  X hnSmit h X  and  X  X ohnSmith X . These suffixes are then inserted into the indexing structure and sorted in alphabetical order. An ex-ample inverted index containing suffixes generated from the BKVs of  X  X ohnSmith X  and  X  X ohnSnith X  is shown in Table 1. The purpose of the indexing structure is to find a set of references to original records that contain a certain suffix, when queried with that suffix.
 After generating BKVs, generating suffix arrays from these BKVs, and inserting the suffixes from these suffix arrays into the indexing structure, one further optimisation step is ca r-ried out. An additional parameter is introduced for this pur -pose, maximum block size ( l mbs ). The problem that can be introduced with low values of l ms is that some words may all feature a common suffix (e.g.  X  X ng X  in the English language). This occurrence can result in the block for common suffixes to be extremely large, and this has a significant adverse effec t of the efficiency of standard Suffix Array blocking. There-fore, a blanket rule is introduced to remove any particu-lar block entirely if it contains references to more than l records. The technique retains accuracy by allowing the cor -rect blocking of records that share short but rare suffixes, while excluding matching short suffixes that are common. Since each input BKV is decomposed into multiple suffixes, the removal of one of many redundant  X  X ub-blocks X  does not adversely affect the recall of the result.

Former studies [1] have found that Suffix Array blocking is efficient primarily due to the small but highly relevant set of candidate record pairs that are produced. Another reason is the low complexity of the Suffix Array algorithm compared to some traditional blocking method implementations [7]. A further advantage of Suffix Array blocking over traditional blocking is that it is not prone to blocking key value errors that appear near the beginning of the BKV. If errors occur, usually not all of the suffixes of these BKVs will change, only some of the longer ones. One record will be inserted into several blocks, adding a form of redundancy to try to ensure that true matched record pairs are grouped into the same block.

Suffix Array blocking is able to solve the problem of fields with a large frequency in the database, and avoid exces-sive processing times for these records [4]. One example of this occurs when matching against the record with high frequency values of  X  X ohn X  and  X  X mith X . When traditional blocking is used, the candidate set will consist of all recor ds that have a first name of  X  X ohn X , as well as all records that have a surname of  X  X mith X . This can be an extremely large set when large real world databases are used, with the in-trinsic problem due to common occurrence of these records. There do exist a few solutions that help to improve the exces-sive time taken for records of this type, however. The proces s of combining more than one field for use as the blocking key causes the candidate set for  X  X ohn Smith X  to be greatly re-duced from the traditional blocking method approach, as the number of records highly similar to  X  X ohn Smith X  is always much less than the number of records with  X  X ohn X  as first name plus the number of records with  X  X mith X  as last name in most normal data sets. Improved Suffix Array blocking inherits these benefits. In practical terms, this functiona l-ity is important in near-realtime systems where a user may query for records that match a specific input. In situations like these, it can be disadvantageous for a query consisting of common terms to take an excessively longer time than normal, as would be the case with traditional blocking.
Accuracy measurement for blocking tasks is usually car-ried out with the use of the pairs completeness measure (PC) [4]. This measure is the ratio of the number of true matches that the blocking algorithm correctly includes in the candi -date set to be matched, and the total number of true matches that exist in the dataset and that would all be found when no blocking is used. If true matches are denoted by N m , blocking denoted matches by S m , and blocking denoted non-matches by S u , then pairs completeness is given as:
Pairs completeness measures the recall of the blocking technique. In [4], the pairs quality measure (PQ) is pro-posed as a way to measure the  X  X eduction ratio X  or efficiency of the blocking technique. Pairs quality is a measure of pre-cision , measuring the proportion of chosen candidates for matching that actually correspond to true matches. It is given as:
Suffix Array blocking is designed for efficiency, with the ability to select a very concise set of candidates for match-ing. However, this comes at the expense of the accuracy, or pairs completeness, of the result. The main weakness of this technique is due to the creation of the array of suffixes. Under standard Suffix Array blocking, the chosen key fields are concatenated into the BKV string. An array of suffixes is then generated from the BKV by taking suffixes of in-creasing length. Since every suffix created from the BKV includes the last character of this string, a difference at th e last position of a BKV when compared to an original BKV will cause standard Suffix Array blocking to place the differ-ing record into a separate block than the original, causing a valid comparison to be left out of the candidate set for the matching step. An extension of this problem occurs when the minimum suffix length parameter l ms is too large. An example of this can be seen in Table 1 when minimum suffix length is 4. Careful selection of this parameter X  X  value is therefore essential.
The Suffix Array blocking method is suitable for a wide range of applications, but has one large limitation. If two BKV substrings are identical apart from an error positioned less than l ms characters away from the end of the BKV string, standard Suffix Array blocking will fail to group thes e records into the same block. The  X  X ohnSmith X  and  X  X ohn-Snith X  example shown in Table 1 contains this property when the minimum suffix length parameter l ms is 4. However, it is clear from this example that many of these adjacent suffixes share a high degree of similarity.

We propose an approach towards solving this problem, by carrying out a grouping operation on similar suffixes in the ordered suffix index list. Many methods can be used for grouping or clustering these suffixes. However, the time complexity of the indexing method is important to consider in order to avoid an overall scalability decrease for the rec ord linkage problem. In particular, we have to avoid a large number of comparisons between the BKV suffixes. In the worst case, we can expect nk BKV suffixes when matching among n records where the average BKV length is k (larger values of l ms will reduce this). A full comparison among all of these records will therefore result in a time complexity o f problem we now face is very similar to the original goal of reducing the number of comparisons we have to carry out among the n original records, by instead needing to find a way to reduce the number of comparisons we have to make for the task of linking together kn suffixes.

However, we can utilise the nature of suffix generation along with the necessary step of building the indexing struc -ture for linkage to greatly optimise this process. In the ex-ample in Table 1 we want to avoid comparisons among suf-fixes that were generated from the same BKV. However, we would like to carry out comparisons between similar suffixes that were generated from different BKVs. Each suffix in the suffix array generated from a single BKV will be similar to the other suffixes from that BKV, with the differences occurring near the start of the suffix. As it turns out, the suffixes are required to be ordered before they can be used in the indexing structure which is used to select candidates fo r matching, and indeed, the ordering is usually carried out by the data structure employed. This requirement therefore au -tomatically disperses suffixes that were generated from the same BKV throughout the list. This behaviour can be seen in Table 1, where the record number of each row alternates between 1 and 2, the identifiers for the BKVs of  X  X ohnSmith X  and  X  X ohnSnith X  respectively.

It can also be seen from this example that the indexing structure has a tendency to place similar suffixes from dif-ferent BKVs next to each other. This is useful from an efficiency point of view when attempting to group together similar BKV suffixes. A simple method for grouping that does not cause adverse scalability reductions can be imple-mented by only checking directly neighbouring records when carrying out the grouping. When a close match is found, the blocks can be merged together. There exists an option to use a larger sliding window [11] when processing the suffixes, to compare suffixes that may match closely but be separated by one or two alphabetically similar suffixes. For efficiency, we group only neighbouring suffixes in our experiments, ef-fectively using a window size of 1. Larger values may be se-lected to increase accuracy with diminishing returns at the Algorithm 1 Improved Suffix Array Blocking Input: 1. R p and R q , the sets of records to find matches between. 2. The suffix comparison function similarity threshold j t 3. The mimimum suffix length l ms and the maximum block Let II be the inverted index structure used.
 Let C i be the resulting set of candidates to be used when matching with a record r pi // Index construction: For record r qi  X  R q : Construct BKV b qi by concatenating key fields
Generate suffixes a qi from b qi , where a qi = { s q 1 , s q 2 , . . . , s qy } , | a qi | = y = | b and s qj = b qi . substring( | b qi | X  l ms  X  j + 1 , | b For suffix s qij  X  a qi : // Large Block Removal For every unique suffix s f in II :
If the number of record references paired with s f &gt; l // Suffix grouping (Improved Suffix Array only) For each unique suffix s f in II (sorted alphabetically):
Compare s f to the previous suffix s g using the chosen comparison function (e.g. Jaro) If Jaro ( s f , s g ) &gt; j t : (highly similar) // Querying to gather candidate sets for matching: For record r pi  X  R p : Construct BKV b pi by concatenating key fields
Generate suffixes a pi from b pi , where a pi = { s p 1 , s p 2 , . . . , s py } , | a pi | = y = | b and s pj = b pi . substring( | b pi | X  l ms  X  j + 1 , | b For suffix s pj  X  a pi : cost of efficiency. The grouping technique therefore exploit s the alphabetical ordering required by the indexing structu re. This technique cannot be easily applied to the highly simila r prefix array blocking method, unless the prefix strings or the ordering comparison function are inverted. Our approach is detailed in Algorithm 1. The standard Suffix Array algo-rithm is equivalent to this one, minus the grouping step.
We carried out experiments using the Jaro string compar-ison function [13] as well as the Longest Common Subse-quence (LCS) operator as the similarity metric used to de-cide whether to merge two BKVs together. We found that if a specific comparison function is used in the full compar-ison of two records, this function may be a good choice for the grouping operation as well. The Jaro string comparison function was found to be more well-suited to our problem, and we show only the experiments that were run using this similarity measure.
If, however, LCS is desired as a similarity function, it can be incorporated without the requirement of an extra parameter. If s 1 and s 2 are the two input suffixes, l 1 l are the lengths of these suffixes, l lcs is the length of any result of the LCS operation, and l ms is the minimum suffix length parameter, then we can define the grouping result as:
Grouping ( s 1 , s 2 ) = Allowing up to a difference in length of l ms between the longest BKV string and the LCS result has the effect of al-lowing groupings between records that would be erroneously omitted due to errors in the last l ms characters of the BKV under standard Suffix Array blocking, while avoiding spuri-ous grouping results that decrease pairs quality unnecessa r-ily and which would likely be removed in the record linkage full comparison step due to low similarity.
Pairs completeness and pairs quality are not the only mea-surements of interest for comparing different blocking tech -niques. These measurements do not take into account the computational complexity underlying the algorithm used. We analyse the computational complexity changes intro-duced due to the grouping technique in this section.
Standard Suffix Array blocking will generate gn suffixes on average, if k is the average BKV length, g = k  X  l ms + 1 and n is the number of records to match with one another. An indexing structure is used to allow for O ( gn log gn ) con-struction and O (log gn ) query time for a single record. In the worst case of l ms = 1 and every suffix being grouped to-gether, the indexing structure will contain k suffix keys, each referencing n data set items, causing query time for a single record to equal O ( kn log kn ). However, with a normal data set, the indexing structure is usually able to separate reco rds into distinct blocks and allow for an O ( b log nk ) query time, where b is a value that depends on the data set used, with data containing more potential linkages having a higher b .
The addition of the grouping operation has an effect on both the construction of the indexing structure as well as th e query operation. For index construction, the list of suffixes of length kn must be traversed once. Grouping results can be stored by modifying the inverted index on the fly. While the time taken to construct the indexing structure may be slightly longer in practice due to grouping, it does not affec t the intrinsic computational complexity that is required.
The time complexity of the querying stage is usually more important than index construction, and the grouping result has an effect on this stage as well. For any set of suffixes generated from the query BKV, the goal is to extract the set of record identifiers to be used to select the candidate record set for matching. Each suffix query of the indexing structure takes an expected O (log gn ) time. Grouping adds extra record identifier results to this step, but the computa -tional complexity is not modified if the window size is fixed (in our experiments it is fixed to 1). Under our proposed technique, the number of additional grouping results is lim -ited to the chosen window size. Therefore, the time taken may be slowed by this small constant factor at this stage due to grouping, but again, the time complexity with regards to n or k is unchanged.
 Figure 1: An example showing suffix exclusion due to l ms and l mbs . Ordering sensitivity is also shown.
We carry out a more thorough analysis into the time complexity of the proposed Improved Suffix Array block-ing method, as well as adding insight into why it is effective and where it may fall short. We describe the approach in detail in Algorithm 1, which includes definitions for each component used in this section.

Standard Suffix Array will miss a correct blocking if there is a mistake within the last l ms characters of the duplicate BKV. It will also miss a correct blocking if the mistake oc-curs within a suffix that is common enough to be excluded due to the maximum block size condition. This condition acts as a way to dynamically extend the minimum suffix length based on the rarity of the suffixes towards the end of each specific word. If one suffix is excluded due to the maxi-mum block size condition, all smaller suffixes from the same word are excluded as well. These two suffix exclusion rules combine to exclude a continuous set of suffixes from one po-sition up to the end of the BKV string. This behaviour is shown with an example in Figure 1.

We can build a model to estimate the probability of vari-ous types of errors that can occur between a true BKV and a  X  X irty X  duplicate, such as character replacement, insert ion, deletion, or swapping. Given the definitions above, we can simplify our model by assuming that the true BKV b p and the dirty duplicate b q have the same length. We can then as-sume that the probability for a difference between b p and b to occur at any character position to be c . The  X  l mbs exclu-sions X  area in Figure 1 acts as a way to dynamically extend l ms based on the rarity of the suffixes towards the end of the BKV, and changes in size in different BKVs. However, we can simplify our model by assuming that the average length of the longest suffix excluded due to the maximum block size condition is l se over all record BKVs in the data set, where l se  X  l ms and l se can be visualised as the combined length of the  X  l mbs exclusions X  and  X  l ms  X  regions in Figure 1. We then have the probability for standard Suffix Array blocking to miss a potential match between two records as:
Improved Suffix Array blocking is able to solve this prob-lem under two conditions. The main requirement is that at least one suffix from each of the two BKVs being compared must be adjacent to each other in the ordered suffix list used in the inverted index structure. With a sliding window ap-proach instead of a strict adjacency rule, we can extend the rule to allow  X  X loseness X  given by half of the window length instead of strict adjacency. The probability for Improved Suffix Array blocking to miss a grouping is difficult to quan-tify, as it depends on the type of data used. However, the act of generating multiple suffixes from a single BKV results in a significant amount of redundancy that we can exploit. It turns out that in the vast majority of cases, at least one suffix from each BKV end up close together, allowing the grouping of the two BKVs to occur. This redundancy is a key to the robustness of the grouping approach, as the pro-cess is able to handle multiple errors in the BKV strings, and multiple missed suffix groupings, as long as only one suffix from each BKV matches up. Examples of BKVs producing suffixes that are dispersed throughout the alphabetically or -dered inverted index are shown in Tables 1 and 2. From these examples it is clear that highly related suffixes from each BKV end up close to each other in the inverted index list.

We define complete grouping separation as the effect that can occur when grouping fails. A simple example occurs when selecting the two BKVs  X  X andbag X  and  X  X andtag X , the latter of which is a misspelling of the first. If there exists other BKVs that are ordered alphabetically between these two BKVs, such as the string  X  X andlebars X , there exists the potential to separate some of the suffix strings ordered in-dexing structure. However, each BKV will have many suf-fixes, all of which are compared to their alphabetically sort ed neighbors for grouping. For complete grouping separation to occur, separation must occur for every single suffix and the neighbor we would like to group it with. The example given above exhibits these characteristics, with the suffix  X  X ndle -bars X  separating  X  X ndbag X  and  X  X ndtag X , and the suffix  X  X d-bar X  separates  X  X dbag X  and  X  X dbar X , etc, as can be seen in Table 2. Nevertheless, this type of scenario is very unlikel y to occur in practice, as the BKV which carries out the sep-aration requires three characteristics. Firstly, the begi nning of the separating BKV must be identical to the two BKVs that should have been grouped together. This type of be-haviour is usually quite rare, typically occurring when wor ds are constructed from multiple parts ( X  X and X  and  X  X ag X ), and especially uncommon when records consist mostly of names (in the case of identity matching). Secondly, the difference s in the two BKVs that should be grouped must occur within the last l ms characters, otherwise they will be grouped due to sharing a common suffix. Thirdly, the end of the separat-ing BKV must be significantly different from both of the two BKVs that should have been grouped together. It can be seen from Table 2 that even in this specifically constructed example, the similarity is quite high, and grouping could st ill occur if the similarity threshold for grouping is reduced.
Complete grouping separation, while rare, can be reduced even further by extending the window size of the grouping operation. In our experiments, the result is good even when the window size is limited to 1. Extending the window size will increase the number of candidate selected for matching , usually reducing the pairs quality. Additionally, the time complexity of the grouping operation is O ( nk ) when the win-dow size w = 1, but w = 2 will cause the grouping operation to double in cost compared to w = 1, so for practical appli-cations, lower values of this parameter are necessary to all ow for a rapid grouping operation. More complex window-based techniques can also be used, such as the approach described by Yan et al. [19].

The second requirement for grouping to successfully occur is simply that the two BKVs being compared must exhibit enough similarity to pass the similarity check that the grou p-Table 2: An occurrence of complete grouping sepa-ration . Two BKVs that should be blocked together are  X  X andbag X  and  X  X andtag X . However, the suffixes of a third BKV  X  X andlebars X  will separate all suffixes of the original two BKVs, causing complete separa-tion and therefore Improved Suffix Array will not be able to improve its result over standard suffix array, due to the optimisation that only allows grouping of adjacent suffixes. Minimum suffix length ( l ms ) must be 3 or more for this condition to occur for this example. Jaro similarity refers to the similarity between the suffix string on one line and the suffix on the following one. ing operation carries out. The process is robust to multiple errors in the dirty duplicate BKV, allowing grouping if only one suffix matches up to a suffix from the clean BKV. How-ever, this suffix match must pass the string similarity check (e.g. Jaro similarity). In some examples, all suffix compar-isons may contain too many errors to be grouped together. This loss of a true match is typically unavoidable, as every blocking method will find it difficult to block together two records with these characteristics, and even if the records are entered into the same block, the full record comparison carried out by the record linkage process would likely dis-card the two records as a non-match, or at least assign a very low similarity score.

Finally, the use of the grouping technique guarantees that no loss in pairs completeness will occur compared to stan-dard Suffix Array blocking.
 Lemma 1. The pairs completeness (recall) of Improved Suffix Array blocking (ISAB) is always greater than or equal to that of Standard Suffix Array blocking (SSAB).
Proof. Let T i be the true matching records of record r . Let C ssa i and C isa i be the candidate records matched by SSAB and ISAB respectively. Then P C ssa ( r i ) = | C ssa matching suffixes, and ISAB adds approximate suffixes to this result, C ssa  X  C isa . Thus, P C isa ( r i )  X  P C ssa The reverse is not true for pairs quality. In most cases, a slight loss in pairs quality will occur when utilising group ing. However, as is evident in some of our results using synthetic data, some situations do occur where both pairs complete-ness and pairs quality is higher when using Improved Suffix Array over standard Suffix Array blocking (E.g. Figure 5).
Our experiments are designed to compare Improved Suf-fix Array blocking against standard Suffix Array blocking as well as traditional blocking, primarily using the measur e-ments of pairs completeness and pairs quality. We run the experiments on two real data sets as well as a synthetic one. The real data sets are sourced from an insurance company where a large-scale record linkage module exists as part of a larger surveillance system. The  X  X dentity X  data set con-sists of personally identifying information such as names and addresses. The synthetic data set was generated us-ing the Febrl [5] tool with standard settings. Even though the source for the real data contains millions of records, ex -amples need to be hand labeled to produce accurate test data sets. Therefore, we were only able to obtain n = 4135 records for the real identity data set. For our results to be comparable, we used n = 5000 records for the synthetic data set. We use larger data sets in our performance comparison experiments.

Our experiments are conducted as follows: 1. We vary l ms while keeping l mbs = 12. 2. We vary l mbs while keeping l ms = 6. 3. We utilise a large scale database implementation to 4. We vary the BKV composition to demonstrate that our All experiment results shown use Jaro for the grouping sim-ilarity function, and the threshold for determining Jaro si m-ilarity between two strings is set at 0.85 for all experiment s.
The results from varying l ms can be seen in Figures 2 and 3. The results from varying l mbs are displayed in Fig-ures 4 and 5. It is clear that for a good selection of l we can obtain a very high pairs completeness (accuracy), while achieving a pairs quality (efficiency) very similar to the highly efficient standard Suffix Array blocking. It is also clear from the pairs quality results that a large time saving can be achieved by utilising Improved Suffix Array blocking over the traditional blocking method, while exper i-encing only a slight loss in accuracy or pairs completeness. The advantages for using Improved Suffix Array blocking over standard Suffix Array blocking are most notable in the experiments using synthetic data, which turned out to be more  X  X irty X  with a higher frequency of errors compared to the real data. This shows the robustness of Improved Suf-fix Array blocking as it is able to gracefully handle data with more errors and mistakes. This quality may be a key advantage in some data environments where error-sensitive techniques cannot be used.

Our performance experiment over a large selection from the real identity data set is shown in Figure 6. It is clear tha t the scalability of both Suffix Array techniques out-perform traditional blocking. Also of interest is the extremely low amount of extra processing required to carry out the group-ing aspect of Improved Suffix Array blocking, both in index construction and querying.

Results from our experiments where we changed the fea-ture set of fields used to construct the BKV are shown in Figure 7 and 8. Different BKV combinations show consis-tent results. Of interest here are the results achieved from the feature selections which contain  X  X uburb X  at the end of the concatenated BKV. For the synthetic data, the suburb field contained errors that may occur typographically if the field is captured in the real dataset using free text entry. However, our real dataset utilises a list of suburbs that con -tains fixed entries, and the operator must select the correct suburb from this list. Therefore, there are no typographi-cal errors in the suburb field of our real dataset. Therefore, when  X  X uburb X  is used as the last string to be concatenated into the BKV, most of the short suffixes generated from different records are exactly the same. The suburb field therefore does not have enough discriminating power to be used at the most important position in the concatenation of strings to form the BKV for the real dataset. Figure 2: Pairs completeness and pairs quality ob-tained while varying minimum suffix length ( l ms ) on the real identity data set. Figure 3: Pairs completeness and pairs quality ob-tained while varying minimum suffix length ( l ms ) on the synthetic data set. Figure 4: Pairs completeness and pairs quality ob-tained while varying maximum block size ( l mbs ) on the real identity data set. Figure 5: Pairs completeness and pairs quality ob-tained while varying maximum block size ( l mbs ) on the synthetic data set. Figure 6: Overall running time on a large set of real identity data. Both Suffix Array techniques require an index construction step as shown. Figure 7: Pairs completeness for different BKV com-binations, using the real identity data set. Figure 8: Pairs completeness for different BKV com-binations, using the synthetic data set.
Suffix Array blocking is highly efficient and able to out-perform traditional methods in scalability, at the cost of a significant amount of accuracy, depending on the attributes of the data used. Our improvement inherits these qualities, but significantly improves the accuracy at the cost of a very small amount of extra processing. The qualities of Improved Suffix Array blocking make it well-suited for large-scale ap-plications of record linkage. Our experimental results sho w that our approach is much more scalable than the traditional approach for data sets containing millions of records. This is a common situation in many industrial applications where many large data sets exist, both current and archival, and it is beneficial to bring data together from different sources in order to increase the amount of knowledge that is avail-able to inform and drive decisions. Scalability becomes a matter of feasibility for very large scale record linkage ta sks. It is also a critical property for high-performance and real -time applications. For this approach, the average query tim e is also important, which may fluctuate significantly when traditional blocking is used. A given example for identity matching was that querying  X  X ohn Smith X  will take much longer than some rare names, often by several orders of mag-nitude. Improved Suffix Array blocking is able to overcome this problem and can avoid excessive query times for records with common field values.

We have also shown that the accuracy or pairs complete-ness of Improved Suffix Array blocking is much higher than standard Suffix Array blocking for the data sets we used in our experiments. In fact, Improved Suffix Array is able to achieve a result highly similar to the highly accurate tra-ditional blocking method. This shows the strength of the additional grouping process that is carried out on the sorte d list of suffixes in the indexing structure, even when we limit our implementation to an efficient one that does not take into account the position of differences within the BKVs being compared, and does not compare suffixes more than one record away in the ordered suffix list. However, further improvements could be designed to utilise these additional sources of information.
Timothy de Vries and Sanjay Chawla acknowledge the financial support of the Capital Markets CRC. [1] A. Aizawa and K. Oyama. A fast linkage detection [2] C. R. Arvind Arasu and D. Suciu. Large-scale [3] R. Baxter, P. Christen, and T. Churches. A [4] P. Christen. Towards parameter-free blocking for [5] P. Christen. Febrl  X  An open source data cleaning, [6] H. L. Dunn. Record linkage. In American Journal of [7] M. Elfeky, V. Verykios, and A. Elmagarmid. Tailor: a [8] I. P. Fellegi and A. B. Sunter. A theory for record [9] L. Gill, M. Goldacre, H. Simmons, G. Bettley, and [10] L. Gu, R. Baxter, D. Vickers, and C. Rainsford. [11] M. A. Hernandez and S. J. Stolfo. Real-world data is [12] L. Huang, L. Wang, and X. Li. Achieving both high [13] M. A. Jaro. Advances in record-linkage methodology [14] L. Jin, C. Li, and S. Mehrotra. Efficient record linkage [15] J. T. Marshall. Canada X  X  national vital statistics [16] A. Mccallum, K. Nigam, and L. H. Ungar. Efficient [17] H. B. Newcombe and J. M. Kennedy. Record linkage: [18] W. E. Winkler. Overview of record linkage and [19] S. Yan, D. Lee, M.-Y. Kan, and L. C. Giles. Adaptive
