 It has been shown, both theoretically and empirically, that reasoning about large and expressive ontologies is computa-tionally hard. Moreover, due to the different reasoning algo-rithms and optimisation techniques employed, each reasoner may be efficient for ontologies with different characteristics. Based on recently-developed prediction models for various reasoners for reasoning performance, we present our work in developing a meta-reasoner that automatically selects from a number of state-of-the-art OWL reasoners to achieve op-timal efficiency. Our preliminary evaluation shows that the meta-reasoner significantly and consistently outperforms 6 state-of-the-art reasoners and it achieves a performance close to the hypothetical gold standard reasoner.
 I.2.4 [ Computing methodologies ]: Artificial Intelligence X  Knowledge representation and reasoning Meta-reasoner, OWL reasoner, Ontology, Prediction mod-els, The Semantic Web
Core reasoning services such as consistency checking and classification are at the heart of ontology-based applications. For ontologies in expressive logics, such reasoning services have a very high worst-case complexity [2, 8]. For instance, satisfiability checking for logic SROIQ has worst-case com-plexity of 2NExpTime -complete [2]. Recent works have also demonstrated empirically [3, 6, 9] that large and complex ontologies indeed pose a real computational challenge even for the state-of-the-art reasoners.

Ontology reasoners such as FaCT++, HermiT and Pellet implement different reasoning algorithms and employ differ-ent sets of preprocessing and optimisation techniques. As
The basic premise of the meta-reasoner lies in the auto-matic ranking of a number of reasoners and selection of one reasoner that is most likely to be the most efficient. The key components in building the meta-reasoner include the training of prediction models for individual reasoners and the training of ranking models (simply rankers ) to generate rankings of the reasoners, based on their reasoning efficiency as predicted by those models. The learning of such rankers follows the same idea under the realm of preference learn-ing [4] whose goal is to learn total orders (i.e. rankings) of all possible labels (i.e. prediction models) from a training example and predict an order to an unseen instance.
The ranking performance of the rankers is analyzed, and the best ranker that leads to the best ranking performance is selected. Then, given an unseen ontology, the selected ranker predicts the most efficient reasoner, which the meta-reasoner eventually invokes to perform reasoning on the on-tology. In the following, we elaborate on the above steps to train our meta-reasoner.

Let R = { r 1 ,...,r n } be a set of n reasoners, O = { o 1 ,...,o p } be a set of p ontologies and OM = { om 1 ,...,om q } be the set of q ontology metrics . Ontology metrics represent dif-ferent aspects of an ontology X  X  size and structural charac-teristics [9]. The ontology set O is divided into three dis-joint subsets: O p , O r and O t , for training of the prediction models, training of the ranking models and testing of the meta-reasoner, respectively.
As the first phase, for each reasoner r i [1 ,n ]  X  R , we train a prediction model M i in the spirit of [9], with the aim to estimate the discretized reasoning time. We employ a dis-cretization method similar to those used in [5, 9]: reasoning time is discretized into one of 4 bins of increasing difficulty: 0s &lt;  X  X  X   X  0.1s, 0.1s &lt;  X  X  X   X  10s, 10s &lt;  X  X  X   X  100s, and  X  X  X  &gt; 100s, with a 20,000-second timeout.

For each reasoner, we only train a single prediction model based on random forest (RF), since it leads to overall best prediction models for all reasoners as suggested in [9]. In-stead of using feature selection algorithms to select a subset of features to train the model [9], we use all the 27 metrics used in [9] as features, as we find the full metrics set leads to more accurate prediction models in the experiments of this work.

The performance of each prediction model M i for reasoner r is measured, based on 10-fold cross validation [7], using the micro-averaged F-measure [10] as the performance mea-sure, since it takes the sizes of the bins into account. The prediction models are trained using the entire dataset O . They are used to estimate the reasoning time of rea-soners for a given ontology. Such predictions will be used in generating a ranking matrix to train the rankers.
As the second phase, for the purpose of training the rank-ing models, we generate a ranking matrix that is the key ma-trix for building a meta-reasoner. Let O r  X  O = { o 0 1 ,...,o 0 m } be the set of m ontologies. Initially, we build an m  X  ( q + n ) data matrix M d (recall that m = | O r | , q = | OM | , n = | R | ),
For this work, we used 798 real-world ontologies collected from the Tones Ontology Repository and the BioOntology repository. 1 To build our meta-reasoner, 6 state-of-the art OWL 2 DL reasoners are included: FaCT++ (version 1.5.3),
HermiT (version 1.3.6), 3 JFact (version 0.9), 4 MORe (ver-sion 0.1.6, with HermiT as the underlying OWL 2 DL rea-soner), 5 Pellet (version 2.2.0), 6 and TrOWL (version 1.4). 7
We first measured the reasoning time (consistency check-ing and classification) of each reasoner for the 798 ontologies on a high-performance server running OS Linux 2.6.18 and Java 1.6 on an Intel Xeon X7560 CPU at 2.27GHz, with a maximum of 32GB memory allocated to the reasoner.

Of the 798 ontologies, 535 ontologies were successfully rea-soned by all the 6 reasoners, while the others encountered processing problems by at least one reasoner. These 535 ontologies constitute our dataset O . In O , 90% of the 535 ontologies (482) were randomly chosen to build our predic-tion models and meta-reasoner, where 60% (290 ontologies) are randomly chosen as O p for building the 6 prediction models of the 6 reasoners, and 40% (192 ontologies) as O r for generating the ranking matrix to build the rankers. The remaining 10%, O t , were used to assess the effectiveness of the meta-reasoner. We repeated this experiment procedure 3 times to alleviate the effect of randomness.

For each of the 3 experiments, we built the 6 predic-M
Pellet , M TrOWL } , on O p using the RF classifier with the 27 ontology metrics (i.e. features) used in [9]. Table 1 shows the effectiveness of M in terms of the micro-average F-measure (simply F-measure) scores obtained from 10-fold cross val-idation using O p in all the 3 experiments. As observed, although there are slight differences in the F-measure scores between prediction models, all the models are shown to be highly effective, achieving over 80% F-measure.
 Using the predicted reasoning time of the ontologies in O r obtained from prediction models in M , we trained 6 rankers [11]: kNN (based on a k-NN algorithm), BinaryPCT (based on predictive clustering trees), PairwiseComparison (based on binary pairwise classification models), BinaryART http://owl.cs.manchester.ac.uk/repository/ , http: //www.bioontology.org/ . https://code.google.com/p/factplusplus http://hermit-reasoner.com http://jfact.sourceforge.net http://www.cs.ox.ac.uk/isg/tools/MORe http://clarkparsia.com/pellet http://trowl.eu mance. For example, for a given ontology o , assume the most efficient reasoner r best has reasoning performance  X  X  X . Suppose that the meta-reasoner selects a less efficient rea-soner r 1 with actual reasoning performance  X  X  X . P@1 score does not distinguish r 1 with another wrong selection, say, r 2 , with actual performance  X  X  X . However, clearly, r 2 is much more inefficient than r 1 for o . Therefore, we further evalu-ate the performance of the meta-reasoner and the other rea-soners, taking into consideration their discretized reasoning time. As explained in Section 2.1, the reasoning time was discretized in a way that the width of the bins increase with their difficulty. Table 4 summarizes the reasoning time dif-ference between bins, calculated using the difference between upper-bound of the time intervals of pairs of bins. Table 4: Approximated time difference (in sec) be-tween discretized reasoning time labels (bins).

Finally, Table 5 presents the average reasoning perfor-mance difference (  X  rpd , in seconds), on the basis of Table 4, between each reasoner and the gold standard r best , the most efficient possible reasoner of the 6 reasoners. Hence, the smaller the value of  X  rpd , the more efficient the reasoner is. The smallest  X  rpd value of all reasoners in each experiment is highlighted in bold. As can be observed from Table 5, the meta-reasoner substantially outperforms all the other 6 rea-soners in all the 3 experiments with performance improve-ment of up to 3 orders of magnitude. The meta-reasoner is also near-optimal , with a small subsecond average reason-ing performance difference (  X  rpd ) from the gold standard. Evaluation on P@1 and average reasoning performance dif-ference shows that the meta-reasoner exhibits significant and consistent performance improvement over all of the 6 state-of-the-art reasoners.
 Table 5: Average reasoning performance difference (  X  rpd , in seconds) on the testing ontologies.

In this paper, we present a novel meta-reasoning approach that combines reasoners in an efficient way, by automatically selecting the reasoner that is most probably the most effi-cient for any given ontology. A key feature of our approach
