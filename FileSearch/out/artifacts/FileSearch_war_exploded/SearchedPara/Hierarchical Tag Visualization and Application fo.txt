 Social bookmarking sites typically visualize user-generated tags as tag clouds. While tag clouds effectively show the rel-ative frequency and thus popularity of tags, they fail to con-vey two aspects to the users: (1) the similarity between tags, and (2) the abstractness of tags. We suggest an alternative to tag clouds known as tag hierarchies. Tag hierarchies are based on a minimum evolution-based greedy algorithm for tag hierarchy construction, which iteratively includes opti-mal tags into the tree that introduce minimum changes to the existing taxonomy. Our algorithm also uses a global tag ranking method to order tags according to their levels of abstractness as well as popularity such that more abstract tags will appear at higher levels in the taxonomy. Based on the tag hierarchy, we derive a new tag recommendation al-gorithm, which is a structure-based approach that does not require heavily trained models and thus is highly efficient. User studies and quantitative analysis suggest that (1) the tag hierarchy can potentially reduce the user X  X  tagging time in comparison to tag clouds and other tag tree structures, and (2) the tag recommendation algorithm significantly out-performs existing content-based methods in quality. H.3.3 [ Information Storage and Retrieval ]: Informa-tion Search and Retrieval; I.5.3 [ Pattern Recognition ]: Clustering X  algorithms Algorithms, Visualization, Experimentation hierarchical tag visualization, tag recommendation, mini-mum evolution criteria
Social bookmarking systems, like Delicious 1 and Flickr 2 are one of the several systems that have been popularized by Web 2.0. Social bookmarking systems allow users to specify keywords or tags for web resources that are of interest to them, helping them to organize and share these resources with others in the community. The most prevalent way of displaying tags is using tag clouds, in which tags are orga-nized in a two-dimensional array with (often) no particular order. Popular tags are typically expressed using larger font sizes or different colors.

While tag clouds offer a natural way of visually exploring the popularity of tags, it has failed to express two impor-tant information: (1) the similarity between different tags, and (2) the concept abstractness of different tags. Firstly, tag clouds cannot directly show how closely two tags are in its two-dimensional representation, which is quite impor-tant when users try to find relevant tags when performing tagging. Currently, most machine learning-based models for tag recommendations require a similarity measure of tags, e.g., tag co-occurrence or tag correlation [15, 8]. Since the representation of tag clouds can rarely help discovering tag similarities in this case, a lot of approaches have to resort to the content of the resources (e.g., the text of documents, the low-level features of images and so on) for learning models, which is known to be time-consuming [3, 13, 16].

Secondly, although some preliminary studies have tried to improve the tag clouds by clustering-based methods [9, 2], they are unable to observe the underlying concept abstract-ness of tags. For example, programming is a more abstract concept than java , which is in turn more abstract than j2ee . While more concrete tags give more specific descriptions of resources (e.g., j2ee based applications), more abstract tags provide a higher-level view of the underlying topics in the resources, which usually attract more interests from the gen-eral population. On the other hand, more abstract tags can also be (ambiguously) related to multiple topics. e.g., dif-ferent users may use tools to annotate both resources about software or craftsman instruments. Consequently, simply clustering tags and displaying a 2-D array is not sufficient for abstract tags. As a result, it is important to understand the level of abstractness of tags, which can greatly help orga-nizing and managing the user-expressed knowledge of a par-ticular set of resources, as suggested by traditional approach of taxonomy learning. Since the unstructured representation of tag clouds is not capable of reflecting tag abstractness, http://delicious.com/ http://www.flickr.com/ having an alternative way of organizing and displaying tags is of imminent importance.

In this paper, we propose a novel framework to address both aforementioned issues in tag visualization. We suggest a hierarchical tag representation to visualize tags in a tree-structured taxonomy. In our representation, each node in the tree is a user-assigned tag, whose abstractness is con-trolled by its level in the tree. The edges in the tree reflect the closeness between tags, e.g., how often are they used to tag the same documents. We propose a principled way to construct the taxonomy by introducing the minimum evolu-tion (ME) criteria from phylogenetics [6]. This structure of-fers a much richer knowledge representation than traditional tag clouds, thus can be potentially leveraged for many appli-cations. As an extension to the tag hierarchy visualization, we derive an efficient tag recommendation algorithm using the tag taxonomy. The recommendation algorithm consid-ers both the tag abstractness and the similarity between tag taxonomy and the resource content. Our algorithm is novel in the sense that it does not require training models that contain knowledge from previous seen documents.

Figure 1 illustrates one of the examples we tested during a user study. The same document is given to two sets of users with tag cloud and tag hierarchy respectively. Users are asked to find the most relevant tags as quickly as possible. It turned out that once the users have located sports ,itonly takes them 1.79 seconds to select football using tag hierarchy. While for tag cloud, users spend 3.56 seconds on average to locate the next tag. Besides being more efficient, tag hierarchy also results in more relevant tags being chosen, according to the user rating in terms of NDCG scores.
Specifically, this paper makes the following contributions:  X  We propose an unsupervised approach to hierarchical tag visualization, which is a novel way to effectively exhibit both the similarity between tags, the level of abstractness and popularity for individual tags.  X  We introduce two novel methods for ranking tags: infor-mation theoretic-based and learning-to-rank-based approach-es, focusing on the popularity and the abstractness of tags respectively. We then combine the power of these two meth-ods to assign global rankings to tags.  X  We use a principled way to construct tree according to the minimum evolution (ME) criteria. Our greedy algorithm along with the minimum description length (MDL) guideline ensures that the tag hierarchy is calibrated to be balanced and well-organized into topics.  X  We conducted a preliminary user study comparing tag clouds with tag trees. Results showed that the tagging time using tag trees was lesser than using tag clouds.  X  Based on the hierarchical representation, we introduce a novel tag recommendation algorithm. Our algorithm is ca-pable of matching a target document with one or more sub-tree of tags, then sorting the tags according to their correla-tions with the document. Comparing to previous approaches that require the content of the training documents for model learning, our framework does not involve any features from document contents, making it efficient and scalable.
It should be noted that although our proposal contains both tag visualization and recommendation, we put much more emphasis on the first part in this paper. Our biggest contribution is the visualization framework, while recom-mendation algorithm is a plus. Due to space limitations, we are unable to show all the details of our recommendation algorithm. Also, many other tag suggestion methods that we compare to during our study are not shown, so that we pick two most representative ones to present in this paper. (ontology learning in folksonomies) Our work resem-bles that of ontology learning (OL) [4], where the objective is to construct a taxonomy of concepts as well as the rela-tionship between them. Traditionally, OL is often studied in the context of natural language processing by annotating the concepts with labels, calculating the similarity between concepts via lexical and contextual similarities, and lever-aging machine learning algorithms such as hierarchical clus-tering to construct a meaningful knowledge representation of concepts. Note that the labeling of terms in OL usu-ally requires external resources. e.g., many of the existing approaches leveraged WordNet to collect hypernyms or syn-onyms for all terms in the documents [18]. Apparently, this guided approach is not suitable for tags since tags can be any arbitrary (novel) words which may not be found in WordNet or any other dictionaries. Comparatively, our work is a ful-ly unsupervised approach that constructs the tag ontology by exploring the correlations between tags in an automatic fashion, without any human-labeling or external knowledge.
A similar tree-based tag hierarchy was studied in [5], where the authors used Jaccard coefficient to measure tag similar-ity and proposed a maximum spanning tree (MST) algo-rithm for tag hierarchy construction. While similar to our approach, the authors did not consider regularization which could cause the tree to be unbalanced. (tag visualization) The most prevalent format of tag visualization is called tag cloud which represents tags in an ordered or random list. Popular tags are usually highlighted with larger font or unique colors. This representation is simply yet quite intuitive, allowing users to quickly discover popular topics they are interested in. This format is used by most popular web2.0 websites like Delicious and Flickr.
An alternative view of tags that aims at visualizing tag topics over time was proposed in [7]. It was used for Flickr photo-tag visualization by associating photos with the most user-tagged words. The application offers two different views of photo-tag pairs: list view and stream view.

Neither of the aforementioned two approaches considered the relationships between tags. A closer proposal to our approach was found in [9], where the authors proposed a two-dimensional layout for improving the display of tags. In their proposal, both tag popularity and tag correlations are considered so that similar tags can be displayed close to each other. However, the levels of abstractness of different tags are still missing in their representation.

The closest approach to ours so far is the greedy algorithm from Heymann [10]. The authors combined graph centrality theorem with tag similarity measurement to derive a greedy hierarchical method for visualization. While these two ap-proaches seem identical, the underlying principle is quite different, as we shall discuss in Section 3.2.5. (tag recommendations) Besides discovering more effec-tive tag representations, extensive research attempts have been made recently on automatically recommending rele-vant tags based on the content of the resources (e.g., web pages, images, videos and so on) or the user interests.
Most of the existing tag recommendations methods are al-gorithmically equivalent to collaborative filtering techniques, where the goal is to first retrieve a set of similar documents from the corpus given a target document, then use the ex-isting tags from these documents for recommendations [16, 8, 3, 11]. A list of successful tag recommendation method-s have been posted on ECML/PKDD Discovery Challenge site 3 . The measurement of similarity between tags is usual-ly the tag co-occurrence, i.e., the number of times tags were used to tag the same document. Other metrics of similarity include tag correlations, random-walk-based similarity and etc.

Overall, existing approaches of tag recommendation main-ly aim at improving the relevance of recommended tags while ignoring the underlying structure of the tag taxonomy. This is mainly due to the heterogenous nature of tags where there is no principled way of modeling tag abstractness. In what follows, we propose our framework to address this issue.
This section describes our framework of hierarchical tag visualization which consists of mainly two steps: (1) rank the http://www.kde.cs.uni-kassel.de/ws/dc09/ tags across all documents, in which we combine the power of two ranking methods: information-theoretic based ranking and learning-to-rank based ranking, and (2) construct the tag tree iteratively given the ranked list of tags, where mini-mum evolution (ME) criteria is used to greedily add tags to the tree, with minimum description length (MDL) standard iscontroltheoptimalshapeofthetree.
The first step of our approach is to rank tags according to their level of abstractness as well as popularity. e.g.,  X  programming  X  has more abstract meaning than  X  java  X  X hus should be ranked higher so that  X  programming  X  can appear at a higher level in the tree. In this paper, we leverage the following features for constructing global tag ranking:
With the three features defined above, we introduce the informativeness of a tag t as follows: where the first term measures the specificity of t and the second term the popularity of t . Z is a normalization factor that ensures any I ( t ) to be in (0,1). The second term is sim-ilar to the measurement of document-frequency and term-frequency in text retrieval applications. We empirically take the logarithm of C ( t ) here to deemphasize its contribution since in practice, tag raw count is usually of several magni-tudes larger than tag distinct count. A direct multiplication will thus over-credit C ( t ). e.g., tag A is annotated 300 times but only in 2 documents, while tag B is annotated 50 times in 10 different documents. Directly multiply C ( t )and D ( t ) would put tag A ahead of B but apparently B is more pop-ular than A. Using our formula successfully eliminates this issue since tag B scores 56.72 while A only gets 16.46.
Other numbers of topics are also tested which showed sim-ilar tag ranking results. Figure 2: The relationship of two tags. Co is the co-occurrence. D ( t i | X  t j ) is the relative appearance.
While our information-theoretic approach bears some lev-el of similarity with Heymann X  X  entropy-based method [11], it should be noticed that the tag distinct count are appar-ently ignored in their approach. Since popular documents could potentially over-credit tags that appear many times in those documents but only a few times in less popular doc-uments, it is important to consider both tag raw count and tag distinct count for a more accurate tag ranking.
On the other hand, we construct an alternative tag rank-ing framework using the idea from learning-to-rank approach [14]. To be concrete, our training data are implicit feedbacks from users who performed tagging. Our objective is to learn an optimal weight vector for linear combination of features for ranking tags across all documents.

Figure 2 depicts a scenario of two tags X  relationship. The middle part, Co ( t i ,t j ) indicates the number of times two tags t i and t j were used to tag same documents, while D ( t means the number of times t i appeared in a document while t did not. Note that for any tag t i , D ( t i )= Co ( t i ,t D ( t i | X  t j ).

Intuitively, the relative occurrence of a tag t i given the absence of another tag t j is a strong indicator of higher ab-stractness of t i than t j . In our data set, we found that for ex-ample, Co ( programming,java ) = 200, D ( java | X  programming ) =29and D ( programming | X  java ) = 239. These statistics strongly indicates that  X  programming  X  is more abstract than  X  java  X , or say  X  java  X  X sakindof X  programming  X .
Consequently, we can construct positive and negative train-ing examples ( x, y ) automatically by discovering these pairs of tags from a given corpus T .Weconstruct
Here { t i  X  t j } is the difference between feature vectors of two tags. In our case, we use the three features defined above, so x k  X  R 3 . And since we only consider binary clas-sification, y k  X  X  X  1 , +1 } .Importantly, &gt; r and &lt; the pair-wise ranking preference. i.e., if the relative appear-ance of t i is significantly more than t j ,wetreatedthepair { t  X  t j } as a positive example. Likewise, a negative example is constructed if t i relatively appeared much less than t example, in the above case of  X  programming  X ( t i )and X  java  X  ( t cant. In our experiment, we set  X  = 2 empirically. Table 1: Top 10 ranked tags in Delicious using differ-ent ranking methods. IT = Information-theoretic, LETOR = Learning-to-rank. The combination uses a ratio of 0.5 as in eq.(5).

In our experiment, we constructed 532 training examples out of 3498 distinct tags in our experiment. For simplicity, we used a regularized logistic regression model for learning the optimal decision in this paper. Other advanced models such as Rank-SVM [14] remained to be explored in future work. Our model specifically maximizes the log-likelihood of the training data, Here x im is the value of the m th feature of the i th instance constructed using eq.(2), w m is the weight value of that par-ticular feature and the weight vector w is the parameter of the model. We used gradient decent to optimize w in model. After learning, we assign each tag a score and normalize Lr to be within (0 , 1).

It should be noted that although learning-to-ranking of-ten requires supervised learning methods, the way we derive labels in our approach does not require any explicit human annotations.
Since the aforementioned two ranking methods address d-ifferent perspectives of tag importance, it is natural to com-bine the power from both of them. A linear combination is proposed here to calculate the final score for each tag: Tags are then ranked according to S ( t ) in a descending order. Table 1 shows the top 10 ranked tags for each of the methods and the combination uses a ratio of 0.5. This parameter will be tuned in our experiment.
The proposed construction of tag hierarchy is an iterative process that contains two primary operations at each step: (1) select appropriate tags to be included in the tree 5 ,and (2) choose the optimal position for those tags.
Our method can be applied to other tag ranking methods as well, e.g., FolkRank [12].
We propose a greedy algorithm that iteratively includes tags from the ranked tag list to the hierarchical represen-tation. Following the minimum-evolution (ME) criteria in phylogentics [6], our objective is to minimize the change (cost) of an existing tree when adding a new tree node. As such, we define the cost of a tree R to be the sum of distances between every pair of the nodes: where the distance between two nodes are the shortest path (  X 
P ( t i ,t j )) that connects them, through their lowest common ancestor ( LCA ( t i ,t j )), For example, in Figure 3, d ( t 1 ,t 2 )=0 . 7 ,d ( t 3 ,t specify the distance between two tags, we define the edge weight between two directly-connected tag nodes i and j as the normalized correlations of the tags t i and t j , i.e., where  X  is a radius parameter.

In general, our algorithm works as follows. During each iteration, we greedily choose the highest-ranked tag for in-clusion to form a new tree. It is worth notice that not all tags are included in the tree. We discard those tags that have zero correlations with all existing tree nodes. The en-tire algorithm is sketched as in Algorithm 1.
The number of potential positions that a new tag can be added is equivalent to the number of existing nodes in the tree, which could become quite large when the tree grows. To limit the search space for the optimal insertion position, we make a restriction here that a new node t new can not be added at a higher level than the current leaf nodes, i.e., if the tree has depth L ( R ), t new can only be inserted at level L ( R ) or L ( R ) + 1. For example, in Figure 3, t 6 can only be added at the same level as of t 4 and t 5 or lower. Our restriction here is legitimate since the ranking of the tags are correlated to their abstractness, so that highly-ranked tags are more likely to be abstract. Under this guidance, we can reduce the search space significantly to N ( L ( R )  X  1) + N ( L ( R )), where N ( i ) indicates the number of nodes at level i .
Note that after the first step of tag ranking, more abstract tags are ranked at higher positions. Therefore, in our itera-tion insertion process, abstract tags will always be selected first and inserted at higher level of the tree, which makes our greedy selection process legitimate.

Once the search space is specified, it becomes easier to locate the optimal position for t new . Mathematically, the optimal position for t new thus can be found by minimizing the following criteria: where R = R  X  t new . (9)
The above optimization framework iteratively construct-ing an optimal tag hierarchy by adding one tag in each step. Figure 3: An example of tag representation of 5 tree nodes and a ROOT node.
 This greedy method is quite efficient. Since in practice, only the cost difference ( X  Cost ) between two trees are required to be computed, the majority part of the tree remains un-changed so that the cost among the rest of the nodes usually does not change at all.
There is an issue comes with the optimal selection criteria mentioned above. Since the nodes are chosen based on the co-occurrence with its parent node, it will then always have shorter distance to the parent node than others. Therefore, connecting the new node with its parent node always returns the optimal result. This could cause, in the worst case, all nodes to have only one child node while each level of the tree only contains one node (Such an example is shown in Figure 8(a), which will be discussed later).

To discourage such effort and make the tree more bal-ance and meaningful, we introduce the idea of regularization, where we penalize the depth of the tree while minimizing the cost of adding new nodes.

This idea is similar to minimum description length (MDL) criteria [1], which has been widely used for model selection. The MDL principle states that the optimal model to encode a set of data essentially minimizes the number of bits that is required to transmit the data. To be specific, the MDL principle minimizes the objective function L ( M | D )+ L ( M ), where the first term defines the explanation of the data D using model M , i.e., the likelihood. The second term corre-sponds to the length of the encoding under model M , i.e., model complexity. As a result, complex models are penal-ized. In our model, the first term controls the cost of intro-ducing a new node, while the second term aims at reducing the height of the tree. i.e., where L ( R )and | R | indicate the depth and the total num-ber of nodes in the new tree, respectively. Here  X  is a bal-ancing factor between the two terms. The second term of eq(10), while penalizes the dep th of the tree in general, en-courages the increase of depth when the number of tree n-odes at the same level becomes large. Since L ( R )remains a constant when the depth does not change, the insertion of new nodes will gradually increase the value of the denomi-nator log | R | and therefore decrease the value of the entire second term. Finally, the depth of the tree will increase when the second term exceeds a threshold.
We introduce a ROOT node for tree initialization. Be-cause none of the top-ranked node can be treated as a par-ent node of all others as they all belong to different topics. Algorithm 1 Tag Hierarchy Construction 2: Initialize tag tree R  X  ROOT 3: while L ( R )  X  2 4: addthetoptag t from T to R according to eq.(10) 5: update W ( e ( ROOT, child )) according to eq.(11) 6: T  X  T \ t 7: while sizeof( T ) &gt; 0 8: extract top tag t from T 9: check the correlation of t with R 11: continue; (throw away t ) 12: else 13: add t to R according to eq.(10) 14: T  X  T \ t 15: end while 16: end while 17: Output R Figure 4: The ROOT node and the first level nodes in our data set. The first level corresponds to the highest level of topics for all tags.
 Therefore, ROOT is a special node that controls the number of nodes at the initial level of the tree, which are real root nodes that break down the topics of a corpus at the highest level. Since there is no correlation between ROOT and oth-er tags, we dynamically adjust the weights between ROOT and other tags according to the weights between its child nodes. We specify the distance to be equal to the maximum distance between any two child nodes,
Under this criteria, once the first level of nodes are fixed and the tree grows to deeper levels, the distance between ROOT and other child nodes will not change. In practice, it helps constructing the initial topics in the tree hierarchy. Figure 4 shows the first level of nodes in our data set. A total of 6 topics are specified at the first level.
Heymann proposed a tag hierarchy construction algorith-m in [10] by leveraging graph centrality theorem and tag similarity measurement. Their method greedy X  X  choose the most central tags from tag graph and added to the hierar-chy. A tag can only be added as (1) a child node of the most similar tag node or, (2) a root node, depending on the sim-ilarity threshold. While the graph centrality theorem bears similarity with our global tag ranking algorithm, there are several fundamental differences:
While our tag visualization method offers an alternative view of underlying tag relationships to the traditional tag could representation, its hierarchial structure also provides a potential opportunity of recommending relevant tags for annotation. Traditional approaches of tag recommendations are usually equivalent to the idea of collaborative filtering, in which documents having similar contents with the targeting document are selected and their tags are used for recommen-dations. Although some researchers have proposed methods for real-time tag recommendations which work well in terms of efficiency [16], the cost of training the model and main-taining large amount of model parameters in memory is still quite expensive and may not be suitable for web-scale ap-plications. In this section, we propose to simplify this rec-ommendation process by leveraging the existing hierarchical tag structures without training an expensive model.
In our proposal, recommending relevant tags to a target-ing document is equivalent to (1) retrieve a candidate tag list (instead of retrieving a candidate document list in tradition-al way), and (2) rank retrieved tags given a scoring function in terms of document-tag relevance. Our framework break-s down the retrieval of candidate list into three scenarios which will be discussed in the next section. Given a tar-geting document d with its textual content, as well as a set prior tag information will be discussed later). We introduce a scoring function to measure document-tag relevance:
Score ( d, t i |{  X  t 1 , ...  X  t k } )=  X  where t i is one of the candidate tags, W (  X  t j ,t i )isthesim-ilarity between user-entered tag  X  t j and t i , N ( t i to the number of times tag t i appears in document d .  X  is a balancing factor between two terms, so  X   X  [0 , 1]. This function addresses the similarity between candidate tags and user tags, as well as the relevance between candidate tags and the content of document d . Note that for applications like images and videos which does not contain textual con-tents, we could leverage the side-information such as user comments, related titles and so on, as proposed in [16]. Or we could simply set the balancing parameter  X  =1toignore the second term of eq.(12).
In the first scenario, we assume that document d only Algorithm 2 Generating Candidate Tag List 1: Input tag tree R , target document d , 2: Initialize candidate( d )=  X  3: If sizeof( T )=0 4: T ( d )=top-K -words-in-d  X  R 5: end if 11: candidate( d ).Add( t c ) 12: end for 13: end for 14: Output candidate( d )[1 ... 50] has one user-entered tag t d . This basically involves find-ing the location of t d in the tree (or location of the most lexically-similar tag), and recommending nearby tags. As an example, for t d = X  programming  X , Figure 5 shows part ofthesubtreefor X  programming  X , as well as its path to the ROOT, where the numbers beside the edges indicate the number of tag co-occurrence. The challenge here is whether the child nodes (more specific tags, like  X  java  X ,  X  .net  X ) of t or parent nodes (more general tags, like  X  development  X ) should have the priority to be included in the list. In our framework, we set the size of candidate list to be sufficiently large (we empirically include 50 tags in the candidate list) to include as many relevant tags as possible. We treat par-ent nodes with higher priority and first include them to the candidate list, then perform a breadth-first search (BFS) to include the most relevant child nodes.

For documents with multiple user tags, e.g. technology and webdesign in Figure 5, we include the joint set of their candidate lists, and output the first 50 with the highest cor-relations. As a result, their common ancestors, programming and development will be more likely to be included.
The most common scenario, however, is a document d without any prior knowledge of user-tags. In this case, we leverage the top K most frequent words from d that appear in our tag list, and treat them as pseudo tags to extract candidate list. So that for each pseudo tag, we retrieve its candidate tree and combine all candidates together. Algo-rithm 2 summarizes our method for these three scenarios.
Note that we X  X e been quite generous towards the inclusion of candidate tags for recommendation, which is an arguable factor against efficiency. However, in fact, the correlations of tags are pre-computed and thus can be looked up in constant time. Constructing the word frequency table for a document with F words costs O ( F ) time. For each candidate tag, the total cost of ranking using eq.(12) is merely O ( K + F + F log F ), where K is the number of user-entered tags. Since both K and F are quite small in practice, our framework is fully capable of performing real-time tag recommendation with very little memory and computational cost.
In this section, we present empirical results which address (1) the efficiency of the tag hierarchy on helping users nav-igate through a large set of tags, and (2) the effectiveness of our tag recommendation algorithm on choosing the most relevant tags for annotations. In our experiments, we mainly Figure 5: Subtree of programming are used for tag recommendation for programming related docu-ments. The weights of the edge indicate the number of co-occurrence of two tags. focus on user tagging experience on web documents, includ-ing News articles, blog articles and so on.
The most well-known tag repository of web documents is the Delicious web site, which currently has more than 5 mil-lion registered users with over 150 million tagged web pages. We crawled Delicious for a period of two months data from May 2008 to July 2008. We first subscribe to the RSS feeds of the top 100 most popular tags in Delicious tag cloud, and continuously crawl these tags to get the associated URLs. For each individual URL, we also retrieve all user tags and download its HTML content. Finally, we ended up with 43,113 unique tags with 36,157 distinct URLs. The data set in evaluation is approximately 3.6 GB.

For user study purpose, we cannot enumerate all combi-nation of the parameters. Therefore, we set both r and  X  in eq.(5) and eq.(12) to be 0.5,  X  =0 . 7 in eq.(10). The sensi-tivity of parameters will be discussed later in this section.
As a preliminary user evaluation, we sought to compare the efficiency and effectiveness of tag hierarchies with tag clouds and Heymann X  X  tag tree representation [10]. First, efficiency was being measured by three time-related metric-s: (1) time-to-first-selection: The time between the times-tamp from showing the page, and the timestamp of the first user tag selection; (2) time-to-task-completion: the time re-quired to select all tags for the task; (3) average-interval-between-selections: the average time interval between adja-cent selections of tags. An additional metric considered is (4) deselection-count: the number of times a user deselects a previously chosen tag and selects a more relevant one.
We conducted a between-subjects design with 49 partic-ipants where half of the users were presented with one tag visualization method and the remaining users were present-ed with the other. The 49 users were selected randomly from the information and computer science departments at a large university, and engineers at a large software compa-ny. Technical users were sought because of their expected background in tagging. Table 2 shows the basic user infor-mation. Note that the size of users in our study is sufficient, Male Female Average Years Average Tagging Table 2: Statistics about user study. Tagging expe-riences are rated in 4-point Likert scale: none (0), a little (1), some (2) and alot(3) . which is supported by many previous literature [17] of user study, where much smaller sample sizes were used.
There were two phases to the experiment. In Phase I we compare our tag hierarchy and tag cloud. Users were asked to tag 10 random web documents from the Delicious data corpus. 15 tags were presented with each web document. Users were asked for select 3 tags that best described the web document. An example has been shown in Figure 1(b) at the beginning of the paper. Note that each user can only see either tag cloud or tag hierarchy. The 15 tags are selected based on Algo. 2. The exactly same set of tags are shown to users in both visualization methods so that quality of the recommended tags are same for all users. For the users chosen for tag cloud, we randomly serve them with (1) alphabetically ordered or (2) randomly ordered tag clouds. For the users chosen for tag hierarchy, we randomly serve them with (1) our result or (2) Heymann X  X  approach [10].
Table 3 and 4 list the results from Phase I study for all the 4 metrics. Overall, our method (MDL-Tree) outper-forms all other three ones in all categories with statistical significance ( p&lt; 0 . 01). Some observations can be made: (1) users were faster in selecting tags using tag hierarchy than tag cloud. Both Heymann X  X  tree and our method show sig-nificant less time in the time-to-task-completion metric. (2) assuming the two groups of users have similar reading speed, the time-to-first-selection metric indicates how fast the user can locate the most relevant tags once they finish reading. In this metric, we observe th at our MDL-Tree requires the least time. Meanwhile, tag cloud organized alphabetically (Cloud- X  ) also exhibits better performance than random tag cloud (Cloud-Rand) as well as Heymann X  X  Tree. (3) Once the user locates the first tag, tag tree shows better perfor-mance in average-interval-between-selections than tag cloud thanks to the tree structure . The reason behind this phe-nomenon is quite obvious: without the hierarchical structure which groups similar tags together, users have to scan the tag cloud every time when they need to find another rele-vant tag, which also explains why the deselection-count of tag clouds are much higher than tag trees.

Furthermore, we asked users to rate their tagging expe-rience on the following 4-point Likert scale: none, a little, some and alot . By removing the sophisticated users in the  X  X  lot X  category, the average time for users to tag using the MDL-Tree visualization was 494 seconds versus 643 seconds for the tag cloud visualization (Cloud- X  ), more than a 30% time advantage. Therefore, our experiment does show the potential for tag hierarchy visualization to be more efficient for tagging than the traditional tag clouds. Controlling for users X  tagging experience can be incorporated in a subse-quent experiment.
In Phase II of our experiments, we evaluate the perfor-mance of our tag recommendation algorithm as in Algorithm 2 and eq.(12). For comparison, we implemented a content-based collaborative-filtering (CF) algorithm which is similar to one of the state-of-the-art P-Tag algorithm [3]. Since our candidate selection algorithm is a content-free recommenda-tion method, we believe that the content-based CF method offers a strong baseline comparison. Specifically, the CF algorithm constructs a document-word matrix for all web pages in collection. For a new targeting web page, the algo-rithm finds the top m most similar web pages in collection by calculating the cosine similarities using word features. CF then recommends the most popular n tags within these m web pages to the targeting page. In our experiment, we fixed both m and n to be 5 for simplicity. Moreover, we also compare with one of the state-of-the-art algorithms in [16] which combines spectral clustering and mixture models for tag recommendation (noted as PMM).

To consider both the relevance of the recommended tags and their relative rankings, we use normalized discounted cumulative gain (NDCG) as the metric, which is a widely-used metric to evaluate the relevance of search engine result pages (SERP) given a set of user queries. We randomly sampled 10 pages from the data set and asked 49 users to measure the relevance of recommended tags. Each algorith-m returns top 5 tags for relevance measurement. The tags are randomly ordered so that the judgers are not aware of the underlying algorithm or the relevance scores. Each tag is measured as one of the 5 levels: Perfect (score 5), Ex-cellent (score 4), Good (score 3), Fair (score 2) and Poor (score 1). The NDCG of the tags given a web documen-t with relevance scores { s (1) , ...s (5) } from a judge can be is a normalization factor corresponding to the ideal discount-ed cumulative gain, so that the NDCG score will always be between 0 and 1. For each document, we average NDCG from 49 judgements to get the average score.

Figure 7 demonstrates the strength of our method against the strong baseline approach and PMM. Overall, our algo-rithm beats the baseline in 9 out of 10 questions, and beats PMM in 8 out of 10 questions. We performed paired T-test with null hypothesis that both algorithm performs equally well. The alternative hypothesis is that our algorithm per-forms better in terms of NDCG (one-sided). The p -value from the statistical significance test is 0.001 comparing with the baseline, which indicates that the result is statistically significant. On average, our algorithm outscores the base-line by 21%. The p -value is 0.02 comparing with PMM X  X till shows statistically significance.

Furthermore, we break down the evaluation into different positions of recommendation. Here, we consider the quality of recommendation for the 1 st tag as well as top-5 tags. Therefore, we measure the NDCG score at position 1 and 5, respectively. Figure 9 summarizes the two cases. Our algorithm has 0.79 NDCG score at position 1, indicating that in 79% of times, the first recommended tag by our algorithm is the most relevant one among all. Our algorithm also outperforms the baseline and PMM in both positions.
The regularization parameter  X  in eq.(10) plays an im-# Time-to-Task-Completion Time-to-First-Selection 10 46  X  19.8 45.8  X  17.9 37.8  X  19.0 29.8  X  11.1 38.52  X  18.8 38.67 Ave 56.65 57.49 46.815 39.83 37.661 39.705 44.991 37.326 MDL-Tree. Less time indicates better performance. (MDL-Tree) outperforms others in both criteria ( p&lt; 0 . 05 ). portant role in balancing the tree depth and the number of nodes at the same level. As mentioned above, without regularization, the tree could grow infinitely deep without expanding its child nodes at any level. By penalizing the tree depth properly following the MDL criteria, our model is capable of minimizing this issue. Figure 8 shows two bad tag tree construction due to the improper use of regulariza-tion. As indicated in the left graph (a), if the regularization is not leveraged (or slightly used), the objective function of eq.(10) will choose to grow the tree first instead of penaliz-ing its height. On the other hand, if we penalize the height too much as in (b), the tree stops growing deeper but keep adding nodes to the current level. We found the range of [0 . 4 , 0 . 7] best balancing these two factors. As our objective function in eq.(9) is equivalent to the maximum spanning tree (MST) used in [5], without proper regularization, MST is expected suffer the same problem in large taxonomy.
The next important parameter is the tuning parameter  X  in eq.(12), which controls the document-tag relevance by judging from two factors: the similarity between tags and the tag frequencies in the document content. To measure its sensitivity, we again resort to the NDCG measurement used above. Figure 6 gives the contour plot of NDCG @5 as a function of  X  and  X  , where higher NDCG scores indi-cate better performance. It can be observed that the high-est NDCG occurs when  X  =0 . 7and  X  =0 . 5. The best NDCG @5 is approximately 0.56. Recall that in our user study we set both parameters to be 0.5 which yielded a 0.51 Figure 6: Parameter sensitivity of  X  and  X  in terms of NDCG scores. The best combination is around  X  =0 . 7 and  X  =0 . 5 .
 NDCG , close to the optimal combination. Overall, when  X  is in [0.4, 0.8],  X  in [0.4, 0.7], our method performs well.
Besides, the combination parameter in eq.(5) that com-bines two ranking methods decides the global ranking of tags. However, we did not find this parameter to be sensitive in practice. Although the global ranking of a tag determines the order of its inclusion into the hierarchy, what really mat-ters, however, is the relative order between two tags. e.g., as long as  X  programming  X  is included in the tag hierarchy prior to  X  java  X , Algorithm 1 can optimize the objective function Figure 7: Comparison of algorithm performance in terms of NDCG scores. The improvement of our algorithm is significant to the baseline ( p &lt; 0.001). Figure 8: Two bad examples of tree visualization without proper regularization. (a)  X  =0 . 1 .Thetree grows too deep without expanding its children at thesamelevel.(b)  X  =0 . 9 . The model penalizes too much on the depth and the tree grows too slowly. and keep similar tags close with high probability. Due to space limitation this experiment is omitted here.
In this paper we proposed a novel visualization of tag hi-erarchy which addresses two shortcomings of traditional tag clouds: (1) unable to capture the similarities between tags, and (2) unable to organize tags into levels of abstractness. Our proposed method mimicked the principle of minimum evolution criteria and constructed tag hierarchy to overcome previous issues. A greedy algorithm was proposed to iter-ative select the best tags to be added in the tree. Along with the regularization method, our framework guaranteed a balanced tag hierarchy with well-organized topics.
We then proposed a taxonomy-based tag recommendation method which leveraged the tag hierarchy to find relevant tags. User study and quantitative analysis indicated that our visualization method can reduce the tagging time, es-pecially for users with limited tagging experiences. Also, our tag recommendation algorithm outperformed a content-based recommendation method in NDCG scores.

In the future, we plan to investigate more advanced rank-ing methods for global tag ranking and compare with exist-ing methods as well [12]. Document-specific tag hierarchy and personalized tag hierarchy are also under consideration. Figure 9: The performance of tag recommendation at position 1 and position 5.
