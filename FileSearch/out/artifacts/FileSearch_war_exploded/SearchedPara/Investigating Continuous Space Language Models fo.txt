 Quality Estimation (QE) is concerned with pre-dicting the quality of Machine Translation (MT) output without reference translations. QE is ad-dressed with various features indicating fluency, adequacy and complexity of the translation pair. These features are used by a machine learning al-gorithm along with quality labels given by humans to learn models to predict the quality of unseen translations.
 A variety of features play a key role in QE. A wide range of features from source segments and their translated segments, extracted with the help of external resources and tools, have been proposed. These go from simple, language-independent features, to advanced, linguistically motivated features. They include features that summarise how the MT systems generate transla-tions, as well as features that are oblivious to the systems. The majority of the features in the lit-erature are extracted from each sentence pair in isolation, ignoring the context of the text. QE performance usually differs depending on the lan-guage pair, the specific quality score being opti-mised (e.g., post-editing time vs translation ad-equacy) and the feature set. Features based on n-gram language models, despite their simplicity, are among those with the best performance in most QE tasks (Shah et al., 2013b). However, they may not generalise well due to the underlying discrete nature of words in n-gram modelling.

Continuous Space Language Models (CSLM), on the other hand, have shown their potential to capture long distance dependencies among words (Schwenk, 2012; Mikolov et al., 2013). The assumption of these models is that semantically or grammatically related words are mapped to simi-lar geometric locations in a high-dimensional con-tinuous space. The probability distribution is thus much smoother and therefore the model has a bet-ter generalisation power on unseen events. The representations are learned in a continuous space to estimate the probabilities using neural networks with single (called shallow networks) or multi-ple (called deep networks) hidden layers. Deep neural networks have been shown to perform bet-ter than shallow ones due to their capability to learn higher-level, abstract representations of the input (Arisoy et al., 2012). In this paper, we ex-plore the potential of these models in context of QE for MT. We obtain more robust features with CSLM and improve the overall prediction power for translation quality.

The paper is organised as follows: In Section 2 we briefly present the related work. Section 3 describes the CSLM model training and its vari-ous settings. In Section 4 we propose the use of CSLM features for QE. In Section 5 we present our experiments along with their results. For a detailed overview of various features and algorithms for QE, we refer the reader to the WMT12-14 shared tasks on QE (Callison-Burch et al., 2012; Bojar et al., 2013; Ling et al., 2014). Most of the research work lies on deciding which aspects of quality are more relevant for a given task and designing feature extractors for them. While simple features such as counts of tokens and language model scores can be easily extracted, feature engineering for more advanced and useful information can be quite labour-intensive.

Since their introduction in (Bengio et al., 2003), neural network language models have been successfully exploited in many speech and language processing problems, including auto-matic speech recognition (Schwenk and Gau-vain, 2005; Schwenk, 2007) and machine trans-lation (Schwenk, 2012).
 Recently, (Banchs et al., 2015) used a Latent Semantic Indexing approach to model sentences as bag-of-words in a continuous space to measure cross language adequacy. (Tan et al., 2015) pro-posed to train models with deep regression for ma-chine translation evaluation in a task to measure semantic similarity between sentences. They re-ported positive results on simple features; larger feature sets did not improve these results.
In this paper, we propose to estimate the prob-abilities of source and target segments with con-tinuous space language models based on a deep architecture and to use these estimated probabili-ties as features along with standard feature sets in a supervised learning framework. To the best of our knowledge, such approach has not been stud-ied before in the context of QE for MT. The result shows significant improvements in many predic-tion tasks, despite its simplicity. Monolingual data for source and target language is the only resource required to extract these features. A key factor for quality inference of a translated text is to determine the fluency of such a text and how well it conforms to the linguistic regularities of the target language. It involves grammatical correctness, idiomatic and stylistic word choices that can be derived by using n -gram language models. However, in high-order n -grams, the pa-rameter space is sparse and conventional mod-elling is inefficient. Neural networks model the non-linear relationship between the input features and target outputs. They often outperform con-ventional techniques in difficult machine learning tasks. Neural network language models (CSLM) alleviate the curse of dimensionality by projecting words into a continuous space, and modelling and estimating probabilities in this space.

The architecture of a deep CSLM is illus-trated in Figure 1. The inputs to a CSLM model are the ( K  X  1) left-context words ( w hot vector encoding scheme is used to repre-sent the input w i  X  k with an N -dimensional vec-tor. The output of CSLM is a vector of pos-terior probabilities for all words in vocabulary, output layer (vocabulary size), the complexity of a basic neural network language model is very high. Schwenk (2007) proposed efficient training strate-gies in order to reduce the computational complex-ity and speed up the training time. They process several examples at once and use a short-list vo-cabulary V with only the most frequent words.
Following the settings mentioned in (Schwenk et al., 2014), all CSLM experiments described in this paper are performed using deep networks with four hidden layers: first layer for the projec-tion (320 units for each context word) and three hidden layers of 1024 units with tanh activation. At the output layer, we use a softmax activation function applied to a short-list of the 32k most frequent words. The probabilities of the out-of-vocabulary words are obtained from a standard back-off n-gram language model. The projection of the words onto the continuous space and the training of the neural network is done by the stan-dard back-propagation algorithm and outputs are the converged posterior probabilities. The model parameters are optimised on a development set. In the context of MT, CSLMs are generally trained on the target side of a given language pair to ex-press the probability that the generated sentence is  X  X orrect X  or  X  X ikely X , without looking at the source sentence. However, QE is also concerned with how well the source segments can be trans-lated. Therefore, we trained two models, one for each side of a given language pair. We extracted the probabilities for QE training and test sets for both source and its translation with their respec-tive models and used them as features, along with other features, in a supervised learning setting.
Finally, we also used CSLM in a spoken lan-guage translation (SLT) task. In SLT, an auto-matic speech recogniser (ASR) is used to decode the source language text from audio. This creates an extra source of variability, where different ASR models and configurations give different outputs. In this paper, we use QE to exploit different ASR outputs (i.e. MT inputs) which in turn can lead to different MT outputs. We focus on experiments with sentence level QE tasks. Our English-Spanish experiments are based on the WMT QE shared task data from 2012 to ferent sizes and labels such as post-editing effort (PEE), post-editing time (PET) and human trans-lation error rate (HTER). The results reported in Section 5.5 are directly comparable with the of-ficial systems submitted for each of the respec-tive tasks. We also performed experiments on the the applicability of our models on n -best ASR (MT inputs) comparison. 5.1 QE Datasets In Table 1 we summarise the data and tasks for our experiments. We refer readers to the WMT and IWSLT websites for detailed descriptions of these datasets. All datasets are publicly available. WMT12: English-Spanish news sentence trans-lations produced by a Moses  X  X aseline X  statisti-cal MT (SMT) system, and judged for perceived post-editing effort in 1 X 5 (highest-lowest), taking a weighted average of three annotators (Callison-Burch et al., 2012).
 WMT13 (Task-1): English-Spanish sentence translations of news texts produced by a Moses  X  X aseline X  SMT system. These were then post-edited by a professional translator and labelled using HTER. This is a superset of the WMT12 dataset, with 500 additional sentences for test, and a different quality label (Bojar et al., 2013). WMT14 (Task-1.1): English-Spanish news sentence translations. The dataset contains source sentences and their human translations, as well as three versions of machine translations: by an SMT system, a rule-based system system and a hybrid system. Each translation was labelled by professional translators with 1-3 (lowest-highest) scores for perceived post-editing effort.
 WMT14 (Task-1.3): English-Spanish news sentence translations post-edited by a professional translator, with the post-editing time collected on a sentence-basis and used as label (in milliseconds). WMT15 (Task-1): Large English-Spanish news dataset containing source sentences, their machine translations by an online SMT system, and the post-editions of the translation by crowdsourced translators, with HTER used as label.
 IWSLT14: English-French dataset containing source language data from the 10 -best (sentences) ASR system output. On the target side, the 1 -best MT translation is used. The ASR system leads to different source segments, which in turn lead to different translations. METEOR (Banerjee and Lavie, 2005) is used to label these alternative translations against a reference (human) transla-tion. Both ASR and MT outputs come from a sys-tem submission in IWSLT 2014 (Ng et al., 2014). The ASR system is a multi-pass deep neural net-work tandem system with feature and model adap-tation and rescoring. The MT system is a phrase-based SMT system produced using Moses.

Table 1: QE datasets: # sentences and labels. 5.2 CSLM Dataset The dataset used for CSLM training consists of Europarl, News-commentary and News-crawl cor-pus. We used a data selection method (Moore and Lewis, 2010) to select the most relevant train-ing data with respect to a development set. For English-Spanish, the development data is the con-catenation of newstest2012 and newstest2013 of the WMT translation track. For English-French, the development set is the concatenation of the IWSLT dev2010 and eval2010. In Table 2 we show statistics on the selected monolingual data used to train back-off LM and CSLM.
 Table 2: Training data size (number of tokens) and language models perplexity (ppl). The values in parentheses in last column shows percentage de-crease in perplexity. 5.3 Feature Sets We use the QuEst 3 toolkit (Specia et al., 2013; Shah et al., 2013a) to extract two feature sets for each dataset:  X  BL : 17 features used as b asel ine in the WMT  X  AF : 80 a ugmented MT system-independent The resources used to extract these features (cor-pora, etc.) are also available as part of the WMT shared tasks on QE. The CSLM features for each of the source and target segments are extracted us-ing the procedure described in Section 3 with the
We trained QE models with following combina-tion of features:  X  BL + CSLM src,tgt : CSLM features for  X  AF + CSLM src,tgt : CSLM features for For the WMT12 task, we performed further exper-iments to analyse the improvements with CSLM:  X  CSLM src : Source side CSLM feature only.  X  CSLM tgt : Target side CSLM feature only.  X  CSLM src,tgt : Source and target CSLM fea- X  FS(AF) + CSLM src,tgt : CSLM features in 5.4 Learning algorithms We use the Support Vector Machines implementa-tion of the scikit-learn toolkit to perform re-gression (SVR) with either Radial Basis Function (RBF) or linear kernel and parameters optimised via grid search. To evaluate the prediction models we use Mean Absolute Error (MAE), its squared version  X  Root Mean Squared Error (RMSE), and Pearson X  X  correlation ( r ) score.
 Table 3: Results for datasets with various feature features are significantly better than BL and AF on all tasks (paired t-test with p  X  0 . 05 ). Table 4: Impact of different combinations of CSLM features on the WMT12 task. Figures with features are significantly better than BL and AF on all tasks (paired t-test with p  X  0 . 05 ). 5.5 Results Table 3 presents the results with different feature sets for data from various shared tasks. It can be noted that CSLM features always bring significant improvements whenever added to either baseline or augmented feature set. A reduction in both error scores (MAE and RMSE) as well as an increase in Pearson X  X  correlation with human labels can be observed on all tasks. It is also worth noticing that the CSLM features bring improvements over all tasks with different labels, evidencing that dif-ferent optimisation objectives and language pairs can benefit from these features. However, the im-provements are more visible when predicting post-editing effort for WMT12 and WMT14 X  X  Task 1.1. For these two tasks, we are able to achieve state-of-the-art performance by adding the two CSLM features to all available or selected feature sets.
For WMT12, we performed another set of ex-periments to study the effect of CSLM features by themselves and in combination. The results in Table 4 show that the target side CSLM fea-ture bring larger improvements than its source side counterpart. We believe that it is because the tar-get side feature directly reflects the fluency of the translation, whereas the source side feature (re-garded as a translation complexity feature) only has indirect effect on quality. Interestingly, the two CSLM features alone give comparable re-spite the fact that these 17 features cover many complexity, adequacy and fluency quality aspects. CSLM features bring further improvements on pre-selected feature sets, as shown in Table 3. We also performed feature selection over the full fea-ture set along with CSLM features, following the procedure in (Shah et al., 2013b). Interestingly, both CSLM features were selected among the top ranked features, confirming their relevance.
In order to investigate whether our CSLM fea-tures results hold for other feature sets, we ex-perimented with the feature sets provided by most teams participating in the WMT12 QE shared task. These feature sets are very diverse in terms of the types of features, resources used, and their sizes. Table 5 shows the official results from the shared task (Off.) (Callison-Burch et al., 2012), those from training an SVR on these features with and without CSLM features. Note that the official scores are often different from the results obtained with our SVR models because of differences in the learning algorithms. As shown in Table 5, we observed similar improvements with additional CSLM features over all of these feature sets. Table 5: MAE score on official WMT12 feature sets using SVR with and without CSLM features. We proposed novel features for machine transla-tion quality estimation obtained using a deep con-tinuous space language models. The proposed fea-tures led to significant improvements over stan-dard feature sets for a variety of datasets, outper-forming the state-of-art on two official WMT QE tasks. These results showed that different opti-misation objectives and language pairs can bene-fit from the proposed features. The proposed fea-tures have been shown to also perform well on QE within a spoken language translation task.

Both source and target CSLM features improve prediction quality, either when used separately or in combination. They proved complementary when used together with other feature sets and produce comparable results to high performing baseline features when used alone for prediction. Finally, results comparing all official WMT12 QE feature sets showed significant improvements in the predictions when CSLM features were added to those submitted by participating teams. These findings provide evidence that the proposed fea-tures bring valuable information into prediction models, despite their simplicity and the fact that they require only monolingual data as resource, which is available in abundance for many lan-guages.

As future work, it would be interesting to ex-plore various distributed word representations for quality estimation and joint models that look at both the source and the target sentences simulta-neously.
 This work was supported by the QT21 (H2020 No. 645452), Cracker (H2020 No. 645357) and DARPA Bolt projects.
