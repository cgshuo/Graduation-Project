 TETSUYA SAKAI, TOSHIHIKO MANABE and MAKOTO KOYAMA Knowledge Media Laboratory, Toshiba Corporate R&amp;D Center 1. INTRODUCTION
Pseudo-Relevance Feedback (PRF) is a widely used query expansion technique for enhancing Information Retrieval (IR) performance, which works as follows: 1. Perform an initial search with the original query, and obtain an initial 2. Assume that the top P documents in the initial ranked output are relevant a term selection criterion , and add these terms to the original query. ( Nega-tive feedback, e.g., downweighting the original query terms using assumed-nonrelevant documents, is also possible.) The original query terms can also be reweighted. 3. Perform a final search with the expanded query and obtain a final ranked output .
 Most top performers of the Monolingual and Cross-Language IR tasks at
TREC, 1 CLEF, 2 and NTCIR 3 have used some kind of PRF [e.g. Kishida et al. 2004; Voorhees 2004b]. However, even when PRF improves IR performance, av-eraged over a topic set, it may actually hurt performance for around one-third of the topics [e.g. Sakai et al. 2000; Xu and Croft 2000]. This is a serious prob-lem from a practical point of view: the user patiently waits for the system to perform a two-stage search, only to find a messed-up ranked output!
PRF is a form of unsupervised learning and there is no guarantee whatso-ever that the pseudo-relevant documents are truly relevant. For example, if the quality of the initial search is extremely low, then the top P documents are probably nonrelevant and PRF probably would not work. On the other hand, if the initial search is already extremely successful, PRF may hurt performance by  X  X iluting X  the original query. To make things worse, even true relevance feed-back is not altogether a reliable technique: For example, at the recent Reliable Information Access (RIA) Workshop [Buckley and Harman 2004; Harman and
Buckley 2004; Montgomery et al. 2004; Warren and Liu 2004], it has been re-ported that some truly relevant documents can hurt performance when used for relevance feedback. (These documents are casually known as  X  X oison pills. X )
Conversely, including some nonrelevant documents can actually be more ef-fective than using a 100%-pure set of relevant documents. In short, it is very difficult to predict whether PRF will work or not.

Traditional PRF uses fixed values of parameters, such as P and T , for all topics. However, what is truly desirable is Flexible PRF, that can optimize these parameters fo r each topic . (Note that Flexible PRF subsumes the binary decision problem of whether to apply PRF or not.) This paper explores a new approach to Flexible PRF, first proposed at the NTCIR-4 Workshop, called Selective Sam-pling [Sakai et al. 2004a]. It is unlike any other Flexible PRF method in that it selectively picks up pseudo-relevant documents from the initial ranked output for collecting a variety of document samples. 4
The remainder of this paper is organized as follows. Section 2 reviews pre-vious work related to the present study. Section 3 describes the monolingual
IR features of our Information Retrieval/Access system called BRIDJE [Sakai et al. 2003a], a top performer at the NTCIR-4 CLIR (Cross-Lingual Informa-tion Retrieval) Task [Sakai et al. 2004a]. Section 4 proposes Selective Sampling for Flexible PRF, as well as a new variation of Selective Sampling that was not tested at NTCIR-4. Section 5 describes the NTCIR Japanese/English test collec-tions and the evaluation metrics used in our experiments, Section 6 discusses the experimental results. Section 7 concludes this paper. 2. PREVIOUS WORK
This section reviews previous work that are related to Selective Sampling and/or Flexible PRF.

In order to vary the number of Pseudo-relevant documents ( P ) and the num-ber of expansion Terms ( T ) across topics, Sakai et al. [2000] proposed Flexible
PRF (or Flexible Local Feedback) by grouping topics based on evidence, such as the initial query length, the highest initial document score, the number of
J apanese case particles in the search requests, and so on. Sakai [2000, 2001] proposed a Flexible PRF method that determines P and T based on  X  X udden drops X  in the initial document score curve. At NTCIR-2, Sakai et al. [2001a, 2001b] used various statistics, such as the (normalized) document scores, query-term idf and expansion-term selection values for mapping test topics onto train-ing topics to achieve Flexible PRF. Sakai and Robertson [2001] revisited the approach used by Sakai et al. [2000], but used the average initial document score, the average term selection value, and so on, as evidence for grouping topics and reported that using the average initial document score for Flexible
PRF has a small positive effect that is consistent across different test collec-tions and languages. However, none of the above methods has significantly outperformed Traditional PRF. Other researchers have also tackled this prob-lem, but without clear success. For example, Xu and Croft [2000] reported that using the number of matched query terms for deciding whether or not to ap-ply Local Context Analysis , which is a kind of PRF, does not work; Billerbeck and Zobel [2003] studied the problem of predicting  X  X hen query expansion fails X  but did not provide a solution; More recently, Amati et al. [2004] have c laimed at the TREC 2003 Robust Retrieval Track [Voorhees 2004b] that they can effectively determine whether to apply PRF or not based on an informa-tion theory-based metric, but its effectiveness does not appear to be significant either. In short, even the binary decision version of Flexible PRF is a difficult problem.

Existing Flexible (and Traditional) PRF methods assume that the top P documents in the initial ranked output are relevant. In contrast, our proposed method goes beyond this assumption. Thus, with Selective Sampling , some doc-uments in the initial ranked output may be skipped .F or example, we may treat documents at Ranks 1, 2, 5, and 6 as relevant, but not those at Ranks 3 and 4 (See Section 4). In this respect, Selective Sampling is related to PRF methods that use document clustering .A t TREC-5, Lu et al. [1997] removed singleton document clusters from the initial ranked output, based on the assumpution that large clusters contain many relevant documents, while singleton clusters are nonrelevant documents. At TREC-6, Buckley et al. [1998] used a similar approach, but used the clusters that best match the query for PRF. However, the results of these document-clustering experiments were also inconclusive. (Similar experiments were conducted at the RIA Workshop as well [Buckley and Harman 2004].) Selective Sampling is philosophically different from these c lustering approaches in that it tries to pick up a small number of documents from every group of similar documents, so that a variety of document samples are obtained for PRF. This is based on the assumption that the initial top P doc-uments are not necessarily representative samples from the whole set of  X  X ood pseudo-relevant X  documents. Unlike clustering-based methods, Selective Sam-pling aims at collecting  X  X ovel X  documents rather than separating relevant doc-uments from nonrelevant ones, since experience suggests that even nonrelevant documents may be of some use in PRF, while even relevant documents may hurt performance, as was mentioned in Section 1. It should also be noted that docu-ment clustering involves similarity calculation between document pairs, while Selective Sampling is comptutationally inexpensive, as we shall see in Section 4.
Selective Sampling can be regarded as a kind of Flexible PRF because, if S denotes the set of pseudo-relevant documents obtained by Selective Sampling,
P =| S | can differ across topics. Of course, the idea of being  X  X elective X  in the broad sense is not new: Many researchers have tried to improve the reliability of Traditional PRF by using passages [e.g. Buckley and Harman 2004; Evans and Lefferts 1994; Sakai et al. 1999; Xu and Croft 2000] or summaries [e.g. Lam-
Adesina and Jones 2001; Sakai and Sparck Jones 2001] instead of documents; by selecting expansion terms based on term cooccurrence [e.g. Xu and Croft 2000]; by improving the initial search precision using Boolean filters [e.g. Mitra et al. 1998] or named entity recognition [e.g. Sakai et al. 2004].

Another line of research that is potentially related to Flexible PRF is the work on query clarity , which is the Kullback X  X eibler divergence between the query and collection language models [Cronen-Townsend et al. 2002]. If it is possible to predict initial query performance with sufficient accuracy, then it may help predict when PRF fails. Unfortunately, both the results from RIA [Buckley 2004;
Buckley and Harman 2004] and work by Billerbeck and Zobel [2003] suggest the contrary. The RIA results, on the other hand, suggest that it may be possible to predict query  X  X ardness X  by measuring the similarity of retrieval rankings of several different systems. This may also be a good way to start tackling the
Flexible PRF problem. 3. THE BRIDJE SYSTEM
This section briefly describes the monolingual IR features of the BRIDJE (Bidi-rectional Retriever/Information Distiller for Japanese and English) system, which we used for our participation at the NTCIR-4 CLIR task [Kishida et al. 2004]. Other features of BRIDJE that are not relevant to the present study, such as search request translation and document summarization/translation, are described elsewhere [Sakai et al. 2003a, 2004a].

BRIDJE can handle both Japanese and English texts, using Japanese mor-phological analysis for the former and part-of-speech tagging/stemming for the latter. The default retrieval algorithm of BRIDJE is the Okapi/BM25 term set of query terms) q , the document score for each document d ( score ( q , d )) is calculated based on the weight of each query term t  X  q as follows: where tf ( t , d ) = number of occurrences of t in d ; ndl ( d ) = normalised document length,
At the initial search, R and r ( t ) are set to zero. Our default PRF algorithm as-sumes the top P documents in the initial ranked output to be relevant (thus, we let R = P ) and selects T new expansion terms from the document texts based on a term-selection criterion . The term-selection criterion used throughout this study is the traditional offer weight , defined as:
Several variations of the above criterion are also available [Sakai and Robertson 2002; Sakai et al. 2003b].
 At the final search, each term in the expanded query is (re-)weighted using
Eq. (1). Moreover, the expansion terms are downweighted compared to the ini-tial query terms: In our experiments, the weights of the expansion terms are multiplied by a default downweighting factor, namely, 0.25. 4. FLEXIBLE FEEDBACK VIA SELECTIVE SAMPLING
This section describes our Selective Sampling algorithm, first proposed at the NTCIR-4 Workshop [Sakai et al. 2004a]. In addition, it proposes a new variation of Selective Sampling, which we call Selective Sampling with Memory Resetting .
 Let d ( r ) denote the document at Rank r in the initial ranked output, and let T ( d ( r )) denote the set of initial query terms found in d ( r ). Instead of freezing P as in Traditional PRF, we use three parameters:
P
P
P
The algorithm shown in Figure 1 returns a set of pseudo-relevant documents, namely S , obtained through Selective Sampling. Thus, the number of pseudo-relevant documents is P =| S | , such that P min  X  P  X  essence of the algorithm is that it tries to avoid collecting too many documents with the same T ( d ( r )). Thus, as was mentioned in Section 2, Selective Sampling aims at collecting a variety of document samples, based on the assumption that the initial top P documents are not necessarily representative samples from the whole set of  X  X ood pseudo-relevant X  documents. Stated another way, we assume that the initial top-ranked documents may be too similar, or redundant .
Once S has been obtained, the rest of the query expansion algorithm is identical to that of our default PRF (See Section 3).

Selective Sampling would be identical to Traditional PRF if T ( d ( r )) is unique for every retrieved document, that is, if the initial ranked output is already  X  X edundancy-free. X  At the other extreme, if there are few query terms, it is very likely that many of the top-ranked documents have the same T ( d ( r )), that is, there would be a huge  X  X luster X  at the top. In such a case, Selective Sampling may skip too many documents.

Based on the above observation, we propose a new variation of Selective Sam-pling, which we call Selective Sampling with Memory Resetting. When faced with a large  X  X luster X  of documents, the original Selective Sampling algorithm would take the first few and throw the rest away. In contrast, Memory Resetting takes the first few, discards the next few, then takes the next few again, etc.:
The exact algorithm is shown in Figure 2. With Memory Resetting, we have two global variables toprank and nskip . While the original  X  X s good sample X  function scans the ranked output from Rank 1 to obtain the number of previ-ously seen documents with the same T ( d ( r )), the new one scans the ranked output only from toprank and, thus,  X  X orgets X  about documents ranked above it. The value of toprank is updated every time nskip (the number of consecutive documents skipped) reaches P min .( We could have introduced another parame-ter for this purpose, but we used P min for simplicity.) For example, if P and when there is a large  X  X luster X  at the top, we take three documents, then discard three, then take three, and so on.

Our Selective Sampling algorithms are very simple and there may be more sophisticated ways to achieve the same goal. However, our attempt in this di-rection has not yet been successful. The variations of Selective Sampling we tried, but did not appear to be beneficial compared to the above two methods, include: 5. TEST COLLECTIONS AND EVALUATION METRICS
This section describes the NTCIR Japanese/English test collections and the evaluation metrics used in our experiments. 5.1 Test Collections
T able I provides some statistics on the NTCIR-3 Japanese test collection and the NTCIR-4 Japanese/English test collections from the NTCIR CLIR tasks [Kishida et al. 2004]. Throughout our experiments, the NTCIR-3 Japanese test collection was used as training data (i.e., for parameter tuning) and the
NTCIR-4 collections were used as test data. The NTCIR-4 Japanese and English topics are translations of each other, but the number of topics differs slightly due to exclusion of topics with few relevant documents. The table shows the number of documents (#docs) and the number of relevant documents (#reldocs) for each document source. Following the NTCIR traditions,  X  X elaxed X  means treating S-relevant, A-relevant, and B-relevant documents as  X  X elevant, X  while  X  X igid X  means treating only S-relevant and A-relevant documents as  X  X elevant. X 
Based on our training experiments with the NTCIR-3 Japanese test col-lection, we let P = 10 for Traditional PRF, and P min
P these runs used T = 40 expansion terms. As for the Okapi/BM25 parameters, all runs used the default values, namely, k 1 = 1 . 2 and b both DESCRIPTION runs and TITLE runs with the NTCIR-4 test collections.
After stopword removal, our DESCRIPTION/TITLE queries contained around six/four terms on average, respectively, for both Japanese and English.
At NTCIR-4, we claimed that Selective Sampling may be effective for ho-mogeneous document collections, because a homogeneous document collection may return an initial ranked output that contains many similar documents, i.e., a lot of redundancy. To test this hypothesis, we also performed subcol-lection experiments, by creating four separate document databases using the Y omiuri/Mainichi data from the Japanese collection and Xinhua/Hong Kong
Standard documents from the English collection. For example, with the Yomi-uri subcollection, only the relevant documents from the Yomiuri data were used for performance computation. 5.2 Evaluation Metrics
F ollowing the NTCIR traditions, we use Relaxed and Rigid Mean Average Pre-cision (MAP) as our primary evaluation metrics. However, it is known that taking the mean across a topic set does not tell us much about poor-performing topics, because the mean is mostly affected by high values of high-performing topics [Voorchees 2004a, 2004b]. As our ultimate goal is to devise a PRF method that never hurts a topic rather than to improve the average performance, below we introduce a supplementary metric that represents how reliably a technique can improve on the initial performance.
 Let Q denote a set of topics. Let n + and n  X  denote the number of topics from
Q improved and hurt by a technique, such as PRF, with respect to a baseline such as the Initial Search performance. (Thus n + + n  X   X | the Reliability of Improvement (RI) as ( n +  X  n  X  ) / | Q improves all topics, while RI = X  1i fi t hurts all topics. If a method does not affect any of the topics, or if it hurts topics exactly as often as it improves others,
RI = 0. 6. RESULTS AND DISCUSSIONS This section discusses the results of our experiments for comparing Traditional
PRF and Selective Sampling (with and without Memory Resetting). 6.1 Overall Results
T ables II and III summarize the results of our NTCIR-4 Japanese and En-glish experiments, respectively. Here, the label of each run is in the form  X  X anguage X  X opic field X  X earch strategy X : D stands for DESCRIPTIONs and T stands for TITLEs. INIT represents Initial Search, PRF represents Traditional
PRF, SS represents the original Selective Sampling, and SSR represents Selec-tive Sampling with Memory Resetting. Column (a) shows Relaxed/Rigid MAP values and column (b) shows how many topics were improved/hurt compared to INIT . Based on column (b), column (e) shows the Reliability of Improvement (RI) values. Columns (c) and (d) show per-topic performance comparisons with
Traditional PRF and the original Selective Sampling, respectively. Significant differences in terms of the Sign Test are indicated by  X   X  X  X  (  X  = 0 . 05). For example, Table II column (b) shows that J-D-PRF outperforms
J-D-INIT for 48 topics in terms of both Relaxed and Rigid Average Precision, while the opposite is true for only seven topics, which is a statistically highly siginficant difference.

The two tables show that, regardless of languages, the three PRF methods ( PRF , SS , and SSR ) are all successful: Their improvements over INIT are all statistically highly significant [indicated by the  X  X tars X  in column (b)]. The MAP values of the three methods are generally similar, and the differences are not statistically sigificant [except for J-T-SS vs J-T-PRF as indicated in Tables II column (c)]. The RI values of the three methods are also generally similar, although it can be observed that high average performance does not necessar-ily imply high reliability. For example, in terms of Relaxed Average Precision, note that the RI of E-T-SSR is higher than that of E-T-PRF (0.69 vs. 0.55), al-though the Relaxed MAP of E-T-SSR is lower than that of E-T-PRF (0.4243 vs. 0.4379). Thus, using RI along with MAP for evaluating PRF methods is probably useful.

While the results show that SS and SSR are only comparable in average performance and reliability to PRF , column (c) of each table shows some in-teresting results: For example, in terms of Relaxed Average Precision, J-D-SS outperforms J-D-PRF for 19 topics while the opposite is true for 18 topics; and
J-T-SSR outperforms J-T-PRF for 26 topics while the opposite is true for 25 top-ics. This means that treating the top P documents in the initial ranked output is often not the best strategy for PRF .

F igures 3 X 6 compares PRF , SS , and SSR per-topic in terms of absolute gain over INIT in Relaxed Average Precision. The abundance of dots above zero rep-resents the fact that all three methods improved performance for the majority of the topics. However, these graphs also show that the behavior of the three methods are not consistent. For some topics, Traditional PRF hurts while Se-lective Sampling helps, while for others, the opposite is true.

Given the above results, two questions arise: (1) How can we effectively c hoose between Selective Sampling and Traditional PRF? (1) How can we ef-fectively choose between Selective Sampling and no PRF? Unfortunately, these two questions seem to be as difficult as the long-standing question  X  X ow can we effectively choose between Traditional PRF and no PRF? X  We considered vari-ous measurable statistics as predictors for answering the above two questions, but none was effective for enhancing the reliability of PRF. The statistics we examined include the average document score of the initially top-ranked docu-ments, query length, the number of documents covered by the initial query (or  X  X uery footprint X ), the degree of overlap of documents covered by each query term (or  X  X uery sharpness X ), and the lowest document frequency in the initial query (or  X  X uery specificity X ).

What we did find was that the number of relevant documents is somewhat correlated with the reliability of the three PRF methods and possibly with the performance difference between SS / SSR and PRF .F igures 7 X 10 plots, for the three PRF methods, the gain over INIT in Relaxed Average Precision against the number of relevant documents. As the dots below zero represent failures by PRF, it appears that the three PRF methods are quite reliable when the number of relevant documents is large, say, over 200. Figures 11 and 12, on the other hand, plots the performance difference between SS / SSR and PRF for the Japanese DESCRIPTION runs. The two graphs suggest that PRF tends to outperform SS and SSR when the number of relevant documents is, say, less than 200, but that SS and SSR start to recover when there are more relevant documents. As we hypothesized in our NTCIR-4 site report, this may be because topics with many relevant documents are  X  X road X  and that Selective Sampling, which aims at collecting a variety of documents, may be suitable for such top-ics. However, similar graphs for the Japanese TITLE runs and the English DESCRIPTION/TITLE runs do not support this topic broadness hypothesis.
Even if the number of relevant documents is a useful predictor for solving the aforementioned two questions, it is an  X  X racle X  metric (as opposed to  X  X easur-able X ) and, therefore, we would have to estimate this statistic first. Unfortu-nately, this is yet another difficult problem. Another  X  X racle X  metric we consid-ered was the proportion of truly relevant documents in the pseudo-relevant set, but this was not useful either, as we shall illustrate in Section 6.2.
To summarize Selective Sampling outperforms Traditional PRF almost as of-ten as Traditional PRF outperforms Selective Sampling. However, it is difficult to predict when Selective Sampling outperforms Traditional PRF.
 6.2 A Closer Look at Some Topics
T ables IV X  X X compare PRF , SS , and SSR for six topics chosen from Figures 3 X  6, based on Relaxed Average Precision. We examine which documents were actually used with each PRF method, and how that affected performance. T able IV compares the three PRF methods for Topic 004 of the NTCIR-4
J apanese DESCRIPTION run (English DESCRIPTION:  X  X ind articles intro-ducing Florence Griffith Joyner X ). This topic has R = 15 relevant documents and the DESCRIPTION query initially contained ql en = topic, both PRF and SS had a negative effect, while SSR had a positive effect.
All three methods used P = 10 pseudo-relevant documents, but PRF and SSR used seven truly relevant documents, while SS used only six. The actual ranks of the pseudo-relevant documents in the initial ranked output are shown in the table, together with information as to which of them are S/A/B-relevant.
Thus, although both PRF and SSR used seven truly relevant documents, PRF hurt performance, while SSR improved it. As this example suggests, even the proportion of the truly relevant documents in the pseudo-relevant set is not necessarily a good performance predictor. (Note, however, that SSR used two
A-relevant documents while PRF used one A-relevant document. This may be one cause of the different behaviors.) T able V compares the three PRF methods for Topic 033 of the Japanese
DESCRIPTION run (English DESCRIPTION:  X  X ind articles on the research of protein for elimination of diseases X ). For this topic, the pseudo-relevant sets obtained by SS and SSR happened to be identical. The only difference between PRF and SS / SSR are the last two documents used: PRF used documents at
Ranks 9 and 10, while SS / SSR used those at Ranks 11 and 12. Although all of these four documents are nonrelevant, PRF wa s successful while SS and
SSR were not. This implies that either (1) Documents at Ranks 9 and 10 were  X  X ood pseudo-relevant X  documents, although nonrelevant; or (2) Documents at
Ranks 11 and 12 were  X  X ad pseudo-relevant X  documents; or (3) both. We have actually read the documents from Ranks 9 to 12, but even the human eye could not judge which was the case. For example, the document at Rank 10 was about a recall of a medical product, while that at Rank 11 was about Familial
Amyloidotic Polyneuropathy. Thus, one of the reasons why predicting the be-havior of PRF methods is difficult is that some nonrelevant documents help im-prove performance while others hurt it and what distinguishes between these two groups is not at all clear.
 T able VI compares the three PRF methods for Topic 027 of the Japanese
TITLE run (English DESCRIPTION:  X  X ind articles showing China X  X  military actions against Taiwan such as deployment of missiles, etc. X ). For this topic, all three methods used the same two A-relevant documents. The difference between PRF and SS is that the former used documents at Ranks 5 X 8, which are nonrelevant, while SS used documents at lower ranks instead, which are also nonrelevant. Despite this, SS had a positive effect while PRF had a negative effect. This example also supports our claim that taking the top P is not always the best strategy.
 T able VII compares the three PRF methods for Topic 041 of the Japanese
TITLE run (English DESCRIPTION:  X  X hat kind of unique internet ser-vices can be received on cellular phones? X ). This example illustrates the advantage of introducing Memory Resetting to Selective Sampling. For this topic, all of the top 57 documents in the initial ranked output contained the four initial TITLE query terms, namely,  X  keitai (cellular), X   X  denwa (phone), X  documents in this large  X  X luster, X  then stopped when P scope
In contrast, SSR started digging the large cluster again, from Rank 7. As a result, SSR and PRF had a positive effect, while SS had a negative effect. We now look at a few English IR results. Table VIII compares the three
PRF methods for Topic 022 of the English DESCRIPTION run ( X  X earching for articles with professional opinions about Kia Motors X  legal management X ). The success of SS may be attributable to the use of two A-relevant and one B-relevant documents, instead of three B-relevant ones. However, as with the
J apanese IR results, the proportion of the relevant documents does not explain the difference between PRF and SSR .
 T able IX compares the three PRF methods for Topic 034 of the English
DESCRIPTION run ( X  X ind articles on the election of the Tokyo provincial governor X ). (We have identical results with the TITLE run for this topic, as the initial queries were identical, both containing  X  X lection X ,  X  X okyo X ,  X  X rovin-cial X  and  X  X overnor. X ) For this topic, SS collected only eight pseudo-relevant documents. Both SS and SSR are much more successful than PRF ,soitap-pears that skipping documents at Ranks 4 X 6 helped. The table also shows that SSR obtained one extra B-relevant document as a result of skipping.
Our examples suggest that the performance of a PRF method is not neces-sarily bounded above by that of a true relevance feedback method, since the proportion of relevant documents in the pseudo-relevant set does not appear to explain the behavior of PRF and since some nonrelevant documents do help improve performance. This is in agreement with the findings from the RIA
W orkshop, where Traditional PRF was compared with true relevance feedback using only the truly relevant documents in the pseudo-relevant set [Warren and Liu 2004]. Our analyses differ from that of RIA, in that we considered PRF methods that can skip documents regardless of whether they are relevant or not and in that we took graded relevance into account. 6.3 Subcollection Results
As was mentioned in Section 5.1, we also conducted subcollection experiments using the Yomiuri ( X  X om X ), Mainichi ( X  X ai X ), Xinhua ( X  X ie X ), and Hong Kong
Standard ( X  X k X ) documents to test the hypothesis that Selective Sampling may outperform Traditional PRF for homogeneous document collections. For example, searching a 100% pure Xinhua data may return an initial ranked output that contain more similar documents at the top than searching the en-tire NTCIR-4 English collection, which is a mixture of data from six different sources.

T ables X and XI show the results of our Japanese and English subcollec-tion experiments in a way similar to Tables II and III, where, for example,
Jyom-D-PRF represents a Yomiuri subcollection run with Traditional PRF, us-ing DESCRIPTIONs. The results are generally quite similar to the ones using the original NTCIR-4 data and, therefore, the collection homogeneity hypothe-sis is not supported. However, it is worth mentioning that: 7. CONCLUSIONS
This paper explored a new, inexpensive Flexible PRF method called Selec-tive Sampling, which is unique in that it can skip documents in the initial ranked output to look for more  X  X ovel X  pseudo-relevant documents. While Se-lective Sampling is only comparable to Traditional PRF in terms of average performance and reliability, per-topic analyses showed that Selective Sampling outperforms Traditional PRF almost as often as Traditional PRF outperforms
Selective Sampling. Thus, treating the top P documents as relevant is often not the best strategy. However, predicting when Selective Sampling outperforms
Traditional PRF appears to be as difficult as predicting when a PRF method fails. For example, our per-topic analyses showed that even the proportion of truly relevant documents in the pseudo-relevant set is not necessarily a good performance predictor.
 As future work, we plan to explore more effective and reliable Flexible PRF methods. Moreover, we plan to conduct IR performance evaluations using performance metrics that really utilize the graded relevance available in the NTCIR test collections. This is not the same as using both Relaxed and Rigid
A verage Precision, as we have done in this paper, because the former ignores the relevance levels completely and the latter ignores, in addition, the B-relevant documents. If an ideal ranked output should have all S-relevant documents listed at the very top, followed by all A-relevant documents, and then all
B-relevant documents, then Relaxed and Rigid Average Precision are clearly inadequate for pursuing this goal. One promising IR metric based on graded rel-evance is Q-measure proposed at the NTCIR-4 Workshop [Sakai 2004a, 2004b, 2005], since it has been shown that it inherits the property of traditional Aver-age Precision and is extremely highly correlated with both Relaxed and Rigid
A verage Precision when used for ranking systems. With new metrics based on graded relevance, we may learn new lessons for improving IR performance. We thank the anonymous reviewers for their helpful comments, and Noriko Kando for her kind advice as a guest editor.

