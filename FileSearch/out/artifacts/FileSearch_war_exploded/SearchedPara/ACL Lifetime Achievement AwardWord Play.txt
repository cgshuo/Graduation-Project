 Palo Alto Research Center Stanford University
This article is a perspective on some important developments in semantics and in computational the field: semantics and morphology. The semantic part deals with issues from the 1970s such as discourse referents , implicative verbs , presuppositions , and questions . The second or the lack thereof, between computational and paper-and-pencil linguistics. The final section inference and question answering. 1. Prologue Thirty-eight years ago, in the summer of 1969 at the second meeting of
S  X  anga-S  X  aby in Sweden, I stood for the first time in front of a computational audience and started my talk on Discourse Referents by reading the following passage (Karttunen 1976): description to refer to an object introduced by an indefinite noun phrase. For example, in (1a), the pronoun It can refer to Bill X  X  car, but in (1b) it cannot. (1) a. Bill has a car i .It i /The car i is black.
Columbus, Ohio. The title of the invited talk was The Logic of English Predicate Comple-ment Constructions . It started off with the following declaration (Karttunen 1971b): complements, that -clauses and infinitival complements, based on whether the sentence commits the author to the truth or falsity of the complement clause. For example, all the sentences in (2) imply, for different reasons, that the complement is true while all the sentences in (3) imply that the complement is false. 2 (2) a. John forgot that Mary was sick. Mary was sick. (3) a. John pretended that Mary was sick. Mary was not sick. Neither one of these two papers would have been accepted at this 2007 There was no implementation, no evaluation, and very little discussion of related work.
In the happy childhood of computational linguistics even the most junior person in the field, like myself, was allowed X  X ven invited X  X o give a talk at the main session about uncharted linguistic phenomena. It was a small field then.
 Austin, who had arrived from Finland in 1964 by way of the University of Indiana at
Bloomington where he had just received a Ph.D. in Linguistics. Where did the young man acquire, and why was he spouting, that kind of computational rhetoric, when the record shows that for the next ten years he never laid his hands on a computer? down to do pure semantics in the 1970s. I wanted to do linguistics because of Syntactic Structures (Chomsky 1957) and when the Uralic and Altaic Studies Department in and managed to get into the Linguistics department as a graduate student. My job title turned out not to be accurate. During my first two years in Bloomington I was teaching computational linguistics taught by an excellent teacher and mentor, Robert E. Wall. Bob
Wall had participated in an early MT project at Harvard and in a project on automatic 444 summarization at IBM . In his course, we learned formal language theory from notes that eventually became a book (Wall 1972), a bit of Fortran and developed by Victor Yngve at MIT . I wrote a program on punched cards to randomly generate sentences from a small grammar of Finnish. Thanks to Bob, I was rescued from my indentured servitude in the Uralic and Altaic Studies. In my third and final year in
Bloomington, I worked as a research assistant in the Computer Center with no specific duties other than to be a liaison to the Linguistics Department. My only accomplishment in that role was to save piles of anthropological data from obsolescence by writing a program to transform rolls of 5-channel paper tape to 6-channel magnetic tapes. By doing that, I became one of the few linguists who could explain the joke, There are 10 kinds of linguists: those who know binary and those who don X  X . I suspect that the data on my tapes for the Control Data 3600 computer have now been lost. We still have the data on manuscripts hundreds of years old but much of the content created in the first decades of the computer age is gone forever.
 getting a one-year fellowship at the RAND corporation in Santa Monica, California, in the group headed by David G. Hays, the author of the first textbook in our field (Hays 1967), and the founder of the Association for Machine Translation and Computational Linguistics ( AMTCL , the predecessor of our ACL ). The main focus of Hays X  X  team was
Russian-to-English machine translation. Remarkably, Hays was also one of the authors of the infamous 1966 ALPAC report that inexorably caused the shutdown of all gov-ernmentally funded MT projects, including the one at RAND translation had acquired a bad odor, the 1968 meeting of AMTCL name and became ACL . At the 1970 meeting, the first one that I attended, people were still bitterly arguing about the matter.
 course on parsing at the 1966 Linguistic Institute at UCLA course had been on the computational analysis of Finnish morphology, a topic to which I would eventually return some fifteen years later. Happy to become Martin X  X  student again, I learned Algol, an elegant new programming language, and got an understanding of the beauty of recursive algorithms. Martin was running an exciting weekly colloquium series. I teamed up with an intern by the name of Ronald Kaplan for a small study project and we gave a joint presentation about our findings. Ron and
I agree that we did this together but neither one remembers what we said. It probably was about the similarities and differences between pronouns and logical variables, the topic of my first published paper (Karttunen 1969b).
 priate system for semantic representation within transformational grammar (McCawley 1970). But as I showed in the 1969 CSL paper, even cases as simple as (4a) and (4b) could not be treated adequately within the proposed framework. (4) a. The man i who loved his i wife j kissed her j .
The problem with (4a) is that the phrase his i wife has to be treated in situ as it cannot be replaced by another coreferential noun phrase, say Mary , without changing the meaning. (4a) implies that only one man in some group of men loved his wife, which is not the same as there being just one man who loved Mary even if the two noun phrases pick out the same individual. For this reason there was no way in a system such as unique cookie to serve as the referent of it . Being an anaphoric pronoun linked to an antecedent does not necessarily mean that the two are coreferential, at least not in any naive sense of coreference.
 tence, X  named after its inventors, Emmon Bach and Stanley Peters. As I was going to devote a chapter of my dissertation to this topic, I was lucky to run into them at a conference in San Diego. I found the two very intimidating in their suits and crew cuts.
They looked like Haldeman and Ehrlichman, a pair of Nixon aides. But at least I found out that the problem had not been solved.
 of time walking up and down the Santa Monica pier just down the cliff from my office thinking about pronouns, variables, reference, and definiteness. I went up to few times to discuss these issues with Barbara Partee and gave a talk in her seminar. The topic of my dissertation was Problems of Reference in Syntax , in principle due before Ileft RAND but finished half-a-year later (Karttunen 1969a).
 in the Linguistics Department at the University of Texas at Austin. Climate was one consideration, but, more importantly, Austin was where Emmon Bach and Stanley Peters were. My Indiana mentor, Bob Wall, had just moved into the same department.
For the next ten years I had very little contact with Martin Kay and Ronald Kaplan but they became very important people in Act II of my life. 2. Act I: Framing Problems
I started my career in Austin in the fall of 1968 and became a regular faculty member in 1970. My work on discourse referents was largely done when I arrived in Austin.
I went on to study so-called implicative verbs such as manage and fail , a subtopic in the discourse referents paper, and branched to other types of verbs that take sentential complements. One important semantic class of verbs with sentential complements, called factives , had already been identified and discussed by Zeno Vendler (1967) and
Paul and Carol Kiparsky (Kiparsky and Kiparsky 1971) at MIT to presuppose that the complement clause is true.

Fall of 1972. In the spring before I had a surprise phone call from Paul Kiparsky who said that the MIT Department was still looking for a one-year replacement for David
Perlmutter who was going on a sabbatical. Would I be interested? Of course I was. I had come to the U.S. seven years earlier to study linguistics because of Noam Chomsky there, I lost interest in transformational syntax. I found Chomsky X  X  Thursday lectures of that year, on themes later published as Conditions on Transformations (Chomsky 1973), uncompelling.
 1995) between generative (George Lakoff, John Ross, James D. McCawley, Paul Postal, and others) and interpretive semantics (Ray Jackendoff and others) had been won by
Chomsky for the interpretivists, although Chomsky himself was, and still is, skeptical of any kind of formal theory of meaning. My sympathies were with the losing side. But I sensed that both camps were essentially doing syntax, albeit in different ways. Barbara Partee had convinced me in our discussions about pronouns and variables at
UCLA that model theory and intensional logic was the right approach to semantics. But 446 it was going to take a while before I could do anything original within that emerging paradigm. At MIT I gave a  X  X ormal methods X  course for a few linguistic students starting with Bob Wall X  X  textbook (Wall 1972) and finishing with Montague Grammar that I was just learning about myself (Montague 1970a, 1970b, 1973; Partee 1995) and a seminar on my own topics: discourse referents, implicative verbs, and presuppositions. I had one star student in the seminar by the name of Mark Liberman, who wrote a Master X  X  Thesis poking holes in my emerging ideas about presuppositions.
 young semanticist. I learned tremendously from my colleagues there: Emmon Bach, Lee
Baker, Stanley Peters, Carlota Smith, and Robert Wall. We had some excellent semantics and syntax students. David Dowty, Per-Kristian Halvorsen, Roland Hausser, Orvokki Hein  X  am  X  aki, Jim McCloskey, and Hans Uszkoreit got their degrees from
I was there. Orvokki was my first Ph.D. student. She wrote an insightful thesis on the meaning of before and other temporal connectives (Hein  X  am  X  aki 1974). presuppositions, and questions. This is not an occasion to deep-end into any of these topics but I will discuss each of them briefly in the following sections to give a general idea of what I think my contributions were. 2.1 Discourse Referents
The obvious difference between definite and indefinite noun phrases is that garden-variety definite NP s such as the car in simple main clauses imply the existence of an individual or an object but indefinite noun phrases such as acar often do not. In that respect, definite NP s are similar to definite pronouns such as it in contexts where the pronoun does not play the role of a bound variable. The reason for the incoherence of (1b) is that it tells us explicitly that there is no such car.
 individual that it describes? This was the question I tried to answer in the 1969 paper on discourse referents, excerpted from the first chapter of my Indiana dissertation. The conclusion I came to was that a pronoun or a definite description could refer back to the individual was semantically implied by the text. Put in this simple way, the answer seems obvious but it gave rise to many problems some of which remain unsolved to this day. The novelty of the approach was that it rephrased the problem of pronom-discussion about anaphors and antecedents had been about the constraints on their syntactic configurations.
 and simple negative sentences such as Bill doesn X  X  have a car in (1b) imply the opposite.
The type of the verb matters, as seen in (5). (5) a. The director is looking at an innocent blonde i .She
In (5b), the phrase an innocent blonde may be understood in two ways. In the specific sense it describes a particular individual that we can refer to as she . But (5b) can also be interpreted nonspecifically, describing the type of girl the director is looking for. In that sense, the continuation is incoherent because there is not yet any individual to refer to. (5a) has no such ambiguity; it entails the existence of an innocent blonde in the actual world and we can talk about her.
 an innocent blonde does not establish a discourse referent in the actual world, we can nevertheless have one in a modal or hypothetical context, as in (6). (6) The director is looking for an innocent blonde i .She
There is another problem here. If we interpret an innocent blonde nonspecifically in (6), then must has a deontic reading. It is a requirement that she be 17 years old. However, on the specific reading must gets an epistemic interpretation. That is, we have made an inference about the age of the girl in question from her looks or other evidence. this topic including Bonnie Webber X  X  1978 dissertation (Webber 1978), Irene Heim X  X  file change semantics (Heim 1982), and the theory of discourse representation structures ( ( S ) Theory) proposed by Hans Kamp (1981) and Uwe Reyle (Kamp and Reyle 1993).
Looking back at my old paper, I am amused by the youthful innocence with which it approached the topic but I am also impressed by the fact that some of the problems it uncovered, such as the deontic/epistemic contrast in (6), apparently remain unsolved. 2.2 Semantics of Complementation
An indefinite noun phrase creates a stable discourse referent just in case the clause it is bound to is implied to be true by the context in which it appears. main idea in the 1969 paper. In the course of seeking evidence for this thesis, I came across an interesting class of verbs and constructions that give rise to such implications (Karttunen 1971a, 1971b). For example, the contrast between (7a) and (7b) is explained by the semantic properties of the two verbs, manage and fail . (7) a. John managed to get a sabbatical i .It i starts in September. (7a) entails that John got a sabbatical, (7b) entails that he didn X  X . The interesting fact about these verbs is that when we change the polarity from positive to negative we still get an entailment, but of the opposite polarity as seen in (8). (8) a. John didn X  X  manage to get a sabbatical i .*It i starts in September.
There exists quite a number of such two-way implicatives that yield an entailment in both positive and negative contexts. Verbs like manage yield a positive entailment in positive contexts ( ++ ) and a negative entailment in negative contexts ( constructions. 448 ++ /  X  X  X  implicatives +  X  /  X  + implicatives ment in one direction but not necessarily the other way.

For example, it is tempting to conclude from (9a) that the president attended the conclusion. Nevertheless, the author may take away that  X  X nvited inference X  (Geis and
Zwicky 1971) without contradicting himself as in (9b). (9) a. The president was able to attend the meeting.
The entailments of constructions involving more than one implicative verb have to be computed from  X  X op X  X own. X  The two examples in (10) establish a stable discourse referent because they both entail that a picture was taken. (10) a. John managed not to forget to take a picture.
The early version of Kamp X  X  Discourse Representation Theory did not include any mechanism for computing lexical entailments about existence. I found the disappointingly static at the time.
 tational linguists. Among the early adopters were Joshi and Weischedel (1973). Ralph
Weischedel X  X  Ph.D. dissertation (Weischedel 1975) showed that useful inferences can be computed directly by the parser, in contrast to the then prevailing view of the community that all inferences have to come from some giant inference engine. This was the starting point of Jerrold Kaplan X  X  work on  X  X ooperative responses X  in database systems (Kaplan 1977). 2.3 Presuppositions X  X onventional Implicatures
The semantics of two-way implicatives puzzled me greatly when I first discovered them (Karttunen 1971a). If the entailments in (11) both hold, in standard logic it would follow that the construction manage to is empty of meaning. In general, if p entails q and entails  X  q , then it logically follows that p and q are equivalent: p (11) a. John managed to speak. John spoke.
But this is of course wrong as far as (11) is concerned. Choosing the construction manage verbs in Table 1 bring in some additional commitment over and beyond what is entailed although it is difficult in some cases to pin down exactly what it is. Furthermore, the commitment remains the same regardless of whether the sentence is affirmative or negative. It is also present in questions and conditionals as shown in (12). (12) Did John manage to speak?
The extra bits of meaning attached to the two-way implicatives were yet another in-stance of a phenomenon that had already been discussed for some time under the term presupposition . The term came from philosophers who had been debating heatedly and for a long time whether The present king of France is false, meaningless, or lacking a truth value (Frege 1892; Russell 1905; Strawson 1950; Russell 1957; Strawson 1964).
When linguists got into the act in the late 1960s, being more systematic observers of language, within a span of just a few years they collected a large zoo of other types of constructions besides definite descriptions that seem to involve presuppositions (Fillmore 1971; Keenan 1971; Kiparsky and Kiparsky 1971). Unfortunately, they did not sort them into different habitats. Here are some examples:
In addition to vastly enlarging the presupposition population, the linguistic community also came up with a problem that had been ignored in the philosophical literature up to that point:
Projection problem: How are the presuppositions of a complex sentence derived from the presuppositions of the component clauses? 450
This question was first posed by Langendoen and Savin (1971). Their answer was (page 57):
They were badly mistaken. Although the consequent clause of (13) by itself presupposes the existence of a unique king of France, (13) as a whole obviously does not. (13) If France has a king, I bet the king of France speaks only French.

In a conditional sentence, a presupposition of the consequent clause can be  X  X iltered X  or  X  X ancelled X  away if it is entailed by the antecedent and general background knowledge.
If a presupposition is not filtered locally and is not part of the context of the discourse, the reader or hearer must in some way adjust his or her state of knowledge to incorpo-rate the new information. The idea is in Karttunen (1974, page 191):
Lewis (1979) called this process accommodation . There is a huge literature on the projection problem and accommodation. Among the papers often cited are Karttunen (1973), Stalnaker (1973), Karttunen (1974), Karttunen and Peters (1979), Gazdar (1979), Lewis (1979), Soames (1982), Heim (1983), van der Sandt and Geurts (1991), van der
Sandt (1992), Zeevat (1992), Beaver (1995), Geurts (1999), and Kamp (2001). Geurts (1999, page 5) sums up the early developments as follows:
I don X  X  disagree with that assessment. 5 It seems to me that by now the notions of pre-supposition projection and accommodation have outlived their usefulness. It is evident that no uniform theory can account for all the phenomena that historically have been lumped together under the label presupposition .
 dation strategy for definite descriptions is closely linked to anaphora resolution (van der
Sandt 1992). One motivation for Kamp X  X  DRS theory was to be able to handle  X  X onkey anaphora X  in sentences such as (14a). (14) a. If John has a donkey i ,hebeatsit i .
What van der Sandt observed was that the treatment of the anaphor in (14a) could be used in (14b) to eliminate the presupposition that John has children. right approach for definite descriptions and for iterative presuppositions triggered by prefixes such as re-in verbs like recalculate and particles such as too and again . However, verbs or factives.
 ples such as (11) and (12) commit the speaker to the view that it was difficult for John to speak. The audience may take note of that piece of information but it does not need to be accepted or accommodated for the discourse to proceed. Another phenomenon that does not call for any accommodation is it -clefts. As Ellen Prince (1978) showed, a sentence such as (15) does not covertly slip into the discourse a piece of new information disguised as being old. On the contrary, the rhetorical force of the it -cleft is to tell you something that presumably you did not know before in a manner that makes the new piece of information incontestable. (15) It was/wasn X  X  Barbara Partee who in a private conversation around
Peters and I proposed to do the sensible thing, namely to divide up the heterogeneous collection of phenomena that had been lumped together under this misbegotten label.
We suggested that many cases that had been called presupposition are best seen as instances of what Grice (1979) had called conventional implicature . Conventional im-plicatures are propositions that the speaker or the author of the sentence is committed to by virtue of choosing particular words or constructions to express himself or herself.
However, whether those implicatures are true or not does not have any bearing on commits the author to the view that Bill is an unlikely person to agree with Mary. (16) Even Bill agrees with Mary.

But the meaning contributed by even plays no role in determining the truth conditions of the sentence. (16) is true if Bill agrees with Mary and false otherwise.
Potts (2004) we see an attempt to build the sort of two-dimensional semantics Stanley and I sketched out that separates conventional implicatures from truth-conditional aspects of meaning. 2.4 Syntax and Semantics of Questions
My paper on questions (Karttunen 1977) was an ambitious effort to give a unified account in the framework of Montague Grammar of the meaning of all types of inter-rogative phrases including direct questions such as the examples in (17) and embedded interrogatives illustrated in (18). (17) a. Is it raining? 452 (18) a. John knows whether Bill smokes.
Examples (17a) and (18a) are yes/no questions . (17b) and (18b) are alternative ques-tions that pose two or more choices. As (17c,d) and (18c,d) illustrate, wh -questions may contain any number of interrogative quantifiers.
 distribution as embedded wh -questions. 6 For that reason a syntactician would prefer to have just a single category of embedded questions. In the framework of Montague
Grammar this is possible only if all types of embedded interrogatives have the same single type of meaning to both direct and embedded questions. For example, which when embedded under a verb such as find out . Finally, whatever meaning we assign to interrogatives, it should help us to elucidate the meaning of question-embedding verbs including the examples in (18) and the one in (19) that sets up a relation between two questions. (19) Whether Mary comes to the party depends on who invites her.

With these desiderata in mind, I came to the conclusion that the best solution would be to adopt an approach proposed by Hamblin (1973) for direct questions and carry it further. Hamblin X  X  idea was to let every direct question denote a set of propositions, namely, the set of propositions expressed by all the possible answers to the question.
For example, under Hamblin X  X  analysis Is it raining? denotes the set containing two propositions { It is raining, It is not raining } . My improvement of that idea was to make the meaning of a question be a function that in each possible world picks up the set of true answers to the question.
 with a that -complement to the meaning of know with an embedded question as in (18a).
If Bill in our actual world is a smoker, then in our world whether Bill smokes picks out the set consisting of the proposition that Bill smokes. In that case, what John knows is that Bill smokes. In examples such as (19) the meaning of depend on can be explicated as a function that in each world maps the true answers to who invites Mary onto the true answer(s) to whether Mary comes to the party . I worked out these ideas with all the rigor of a Montague grammarian. After years of apprenticeship I had finally become a competent formal semanticist. 7 it, I fell from the pinnacle of semantics into the low life of finite-state automata. My semanticist friends kept asking,  X  X hat happened to you Lauri? You were such a good semanticist. X  The politely unstated premise was that I had fallen onto skid row. 3. Interlude
Towards the end of the 1970s I began to think that I had stumbled on, and helped to create and frame, more problems in semantics than I could ever solve. It was time to move on and leave the mess for others to clean up. In a bold move I signed up to teach a course on computational linguistics. As every professor knows, teaching a course on something you know next to nothing about is a great learning opportunity. To get some idea of how the field had developed in the previous ten years I went to the 1979
ACL Annual Meeting in La Jolla, California, and immediately ran into two of my old colleagues from RAND , Martin Kay and Ron Kaplan, very surprised to see me.  X  X hat are you doing here? X  they asked. I said I had picked up a new hobby and was planning to do some computational work on Finnish morphology, the topic of my term paper for Martin X  X  course in 1966.
 ( CASBS ) at Stanford, 1981 X 1982, I often went to visit Martin at the Palo Alto Research Center ( PA R C ) just a short drive from CASBS . I learned about unification and InterLisp. I got to compute on the Alto personal computer and even had my own personal 1 floppy for it, about the size of a large briefcase. On the floppy was the project Martin and I were collaborating on, a unification based parser/generator for Finnish. Finnish was a good test case for Martin X  X  functional unification grammar ( FUG , constituents could be labeled by a syntactic category such as assigned functional roles such as CONTRAST and TOPIC . We showed how the constraints on Finnish word order could be described and implemented in those terms (Karttunen and Kay 1985a).
 make the move from academia to industrial research.
 computational linguists and AI people: Barbara Grosz, Jerry Hobbs, David Israel, Robert
Moore, Fernando Pereira, Ray Perrault, Stuart Shieber, and Hans Uszkoreit were there, among others. SRI  X  X  AI Center and Xerox PA R C were cofounders of the new Center for the Study of Language and Information ( CSLI ) at Stanford, funded by a generous grant from the System Development Foundation, an offshoot of the , Stuart Shieber had designed and implemented his influential for unification-based grammars (Shieber et al. 1983). I implemented it (Karttunen 1984;
Karttunen and Kay 1985b; Karttunen 1986) at CSLI in Interlisp on a Xerox Dandelion, a wonderful machine with Interlisp as the language of the operating system.
 interest: finite-state morphology . In making the crosstown transit from Menlo Park to
Palo Alto, I graduated from my lovely Dandelion to the top-of-the-line Xerox Dorado, still the best computing experience in my life. After all the years spent on theorizing and playing with formalisms, I wanted to do something practical that would have an impact on the real world. 4. Act II: Providing Solutions
In the early 1980s, morphological analysis of natural language was a challenge to computational linguists. Simple cut-and-paste programs could be written to analyze strings in particular languages, but there was no general language-independent method available. Furthermore, cut-and-paste programs for analysis were not reversible, they could not be used to generate words. 454 means of ordered rewrite rules introduced by Chomsky and Halle (1968). These rules are of the form  X   X   X / X   X  , where  X  ,  X  ,  X  ,and  X  can be arbitrarily complex strings or feature matrices. It was not understood how such rules could be used for analysis. was a visitor from Finland, Kimmo Koskenniemi, who was looking for a dissertation topic. Martin Kay and Ronald Kaplan were also there and it turned out that all four of us were interested in morphology. I demoed a small system I had built with my students for Finnish (Karttunen, Uszkoreit, and Root 1981). Martin and Ron reported that they had recently made a breakthrough discovery in computing with rewrite rules. Kimmo went on to California to visit them at PA R C to learn more. That was the beginning of our long collaboration. 4.1 Origins
The discovery Kaplan and Kay had made was actually a rediscovery of a result that had been published a decade before in a book that none of us knew about at that time, a
UC Berkeley dissertation by C. Douglas Johnson (1972). Johnson observed that although the same context-sensitive rule could be applied several times recursively to its own output, phonologists have always assumed implicitly that the site of application moves to the right or to the left in the string after each application. For example, if the rule  X   X   X / X   X  is used to rewrite the string  X  X  X  as  X  X  X  , any subsequent application of the same rule must leave the  X  part unchanged, affecting only  X  or  X  . Johnson demonstrated that the effect of this constraint is that the pairs of inputs and outputs produced by a phonological rewrite rule can be modeled by a finite-state transducer . transducers established by Sch  X  utzenberger (1961): for any pair of transducers applied ducers can in principle be composed into a single transducer that maps lexical forms directly into the corresponding surface forms, and vice versa, without any intermediate representations.
 Kay, but not convinced about the practicality of the approach for morphological analysis .
Traditional phonological rewrite rules describe the correspondence between lexical forms and surface forms as a one-directional, sequential mapping from lexical forms to surface forms. Even if it were possible to model the generation of surface forms efficiently by means of finite-state transducers, it was not evident that it would lead to an efficient analysis procedure going in the reverse direction, from surface forms to lexical forms. rewrite rules, N-&gt;m/ p and p-&gt;m/m . The corresponding transducers map the lexical form kaNpat unambiguously to kammat ,with kampat as the intermediate representation. However if we apply the same transducers in the opposite direction to the input kammat , we get the three results shown in Figure 1. This asymmetry is an inherent property of the generative approach to phonological description. If all the rules are deterministic and obligatory and if the order of the rules is fixed, each lexical form generates only one surface form. But a surface form can typically be generated in more than one way, and the number of possible analyses grows with the number of rules that are involved. 4.2 Two-Level Morphology
Back in Finland, Koskenniemi invented a new way to describe phonological alterna-tions in finite-state terms. Instead of cascaded rules with intermediate stages and the computational problems they seemed to lead to, rules could be thought of as statements that directly constrain the surface realization of lexical strings. The rules would not be applied sequentially but in parallel. Each rule would constrain a certain lexical/surface correspondence and the environment in which the correspondence was allowed, re-quired, or prohibited. For his 1983 dissertation, Koskenniemi (1983) constructed an ingenious implementation of his constraint-based model that did not depend on a rule compiler, composition, or any other finite-state algorithm, and he called it two-level morphology . Two-level morphology is based on three ideas:
Applying the rules in parallel does not in itself solve the overanalysis problem illus-trated in Figure 1. The two constraints just sketched allow kammat to be analyzed as kaNpat , kampat ,or kammat . However, the problem becomes manageable when there are no intermediate levels of analysis. In Koskenniemi X  X  1983 system, the lexicon was represented as a forest of tries (= letter trees), tied together by continuation-class links from leaves of one tree to the root of another tree or trees. 456 analysis of the surface form are performed in tandem. In order to arrive at the point the lexical string kaN . At this point, it only considers symbol pairs whose lexical side matches one of the outgoing arcs of the current state. It does not pursue analyses that have no matching lexical path. All the rule networks must accept every lexical:surface pair. In the case at hand, the p:m pair is accepted by the N:mRule that requires a p as the right context on the lexical side and by the p:mRule that requires an m as the left context on the surface side. In two-level rules, zero (epsilon) is treated as an ordinary symbol.
Because of this, a two-level rule represents an equal-length relation . Conceptually, the system in Figure 2 simulates the intersection of the rules and the composition of the rules with the lexicon.
 history of computational linguistics for the analysis of morphologically complex lan-guages. The language-specific components, the rules and the lexicon, were combined with a universal runtime engine applicable to all languages. 4.2.1 The Texas KIMMO System . I met Koskenniemi again in Finland around Christmas time in 1982. He had just finished the first implementation of a two-level system and home I unfolded the long printout on the floor of a corridor and spent quite a bit of time crawling up and down the code trying to understand what it did, and learning Pascal along the way. I was going to teach computational linguistics again in the spring. Hav-ing figured out Kimmo X  X  program, it occurred to me that doing a Lisp implementation of the two-level model would be a good class project.
 with our Lisp code (Gajek et al. 1983; Karttunen 1983). To make sure that Koskenniemi got the credit for the invention, we called it the KIMMO system. The name stuck and inspired many other KIMMO implementations. The most popular of these is a free C implementation from the Summer Institute of Linguistics (Antworth 1990). in several large systems for natural language processing such as the British Alvey project (Black et al. 1987; Ritchie et al. 1987, 1992), (Carter 1995), the ALEP Natural Language Engineering Platform (Pulman 1991), and the
MULTEXT project (Armstrong 1996). 4.2.2 A Compiler for Two-Level Rules . In his dissertation Koskenniemi (1983) introduced a formalism for two-level rules. The semantics of two-level rules was well-defined but there was no rule compiler available at the time. Koskenniemi and other early practitioners of two-level morphology constructed their rule automata by hand .Thisis tedious in the extreme and very difficult for all but very simple rules.

Koskenniemi to Stanford in the Summer of 1985. Although two-level rules are concep-tually quite different from the rewrite rules studied by Kaplan and Kay, the methods that had been developed for compiling rewrite rules were applicable to two-level rules
Kaplan and Kay had already solved by an ingenious technique for introducing and then eliminating auxiliary symbols to mark context boundaries. Another fundamental insight they had was the encoding of context restrictions in terms of double negation.
For example, a constraint such as  X  p must be followed by q  X  can be expressed as  X  X t is not the case that something ending in p is not followed by something starting with q . X  In Koskenniemi X  X  formalism, p=&gt; q .
 in InterLisp by Koskenniemi and me in 1985 X 1987 using Kaplan X  X  implementation of the finite-state calculus (Koskenniemi 1986; Karttunen, Koskenniemi, and Kaplan 1987). The current C-version two-level compiler, called TWOLC (Karttunen and Beesley 1992). It has extensive systems for helping the linguist to avoid and resolve rule conflicts, the bane of all large-scale two-level descriptions. 4.2.3 Two-Level Descriptions . Many languages have been described morphologically in the two-level framework. But in many cases the work has been done for companies tions have not been made public for obvious reasons. Here are some of the languages Finnish (Koskenniemi 1983), Estonian (Uibo 2006), German (Schiller 1996), Nothern
S  X  ami (Moshagen, Sammallahti, and Trosterud 2006), and Turkish (Oflazer 1994). 4.3 Lexical Transducers Soon after arriving at PA R C I made a serendipitous discovery. At the time collaborating with Microlytics, a company that marketed spell-checkers, the first suc-cess story of finite-state morphology. 10 Microlytics had licensed from Koskenniemi X  X  company, Lingsoft, the rights to the Finnish analyzer. I was asked to extract from the
Lingsoft two-level analyzer a network of surface forms that could be fed to Kaplan X  X  compression routine to make a Finnish spell-checker in the Microlytics format. For that task I designed an algorithm that simultaneously carried out the intersection of
Koskenniemi X  X  23-rule automata and the composition with the lexicon. I was surprised original source lexicon. Figure 3 is a sketch of that process. Just intersecting the rule automata by themselves was barely possible for us then because of the exponential worst-case complexity of the intersection algorithm. We assumed that the composition with a large lexicon might make the computation even harder to carry out. In fact the 458 opposite happened. The reason should have been obvious from the beginning. The intersection of a set of two-level rules explodes because it has to compute a result for any in the language, there is no blowup. The same applies to the composition of transducers derived from rewrite rules. If the rule cascade is computed starting with the lexicon, the  X  X veranalysis X  problem illustrated in Figure 1 never arises.
 extracted from the transducer, but we realized that keeping the lexical forms and their surface realizations in a single network would be even more valuable. I created a transducer for English with a small number of two-level rules. It consisted of mappings such as in Figure 4. Annie Zaenen and Carol Neidle created, with a large number of rules, a much more ambitious proof-of-concept, a lexical transducer for French, mapping lemmas such as vouloir+Verb+IndP+Sg+P3 to the corresponding surface form veut . Such a transducer is the ultimate  X  X wo-level model X  for a language as it compactly encodes
A comprehensive analyzer such as we built for English and French consists of tens of thousands of states and hundreds of thousands of arcs; but physically they can be quite small, a couple of megabytes in size. The same network can be applied in two ways: to provide an analysis for a surface form or to generate a surface from a lexical form in a tiny fraction of a second. Karttunen, Kaplan, and Zaenen (1992) and Karttunen (1994) are the first published reports on lexical transducers.

France. Annie Zaenen and I went there to launch the Center X  X  research on natural language. We started with a couple of employees in an unfinished building with three empty floors, an elevator, and a pile of Sun workstations stacked at the entrance. Not knowing a word of French made it a hardship assignment for me, but in every other respect it was a lucky break. Because XRCE was a start-up as a research center in need of visibility and recognition on the level of the Xerox Corporation, I got more resources and help for my work than I could possibly have had at PA R C made a contract with XRCE to produce morphological analyzers and disambiguators (=  X  X aggers X ) for six European languages. Kenneth R. Beesley, who had worked for
Microlytics, came to Grenoble to manage the development effort. I headed a small finite-state team of researchers and programmers charged with the mission of creating better development and run-time tools such as XFST (Xerox Finite State Tool) and (Lexicon Compiler).

We concluded that lexical transducers are easier to construct with sequentially applied rules than with the parallel two-level rules. Andr  X  e Kempe and I therefore developed a compiler for replace rules (Karttunen 1995, 1996; Kempe and Karttunen 1996).
XFST regular expression language now includes a large set of different types of replace expressions: parallel replacement, replacement with multiple contexts, replacement the usual finite-state operations union, intersection, composition, and negation. to researchers most of the tools that were developed in Grenoble for creating and useful NLP tasks such as tokenization and named-entity recognition. Ken and I wrote a book, Finite State Morphology (Beesley and Karttunen 2003), a pedagogical text that explains and documents the tools that come with the book. There have been many improvements in the software since then. A new edition of the book is in the making. company in California, was marketing finite-state morphological analyzers and stem-mers for about three dozen languages. From a computational point of view morphology was a solved problem. 4.4 Computational vs. Paper-and-Pencil Morphology
Historically, computational linguists and their  X  X aper-and-pencil X  counterparts in lin-guistics departments have been curiously out of sync in their approach to phonology and morphology. When computational linguists implemented parallel two-level models in the 1980s, paper-and-pencil linguists were stuck in the sequential Chomsky X  X alle paradigm. Many arguments had been advanced in the phonological literature in the 1970s to show that phonological alternations could not be described or explained ad-equately without sequential rewrite rules. The idea of rules as constraints between a realization of a lexical symbol could be constrained by the lexical side and/or by the 460 surface side. The standard arguments for rule ordering were based on the a priori assumption that a rule could refer only to the input context (Karttunen 1993). tools embraced the sequential model as the more practical approach, a two-level theory took over paper-and-pencil linguistics by storm in the guise of Optimality Theory ( (Prince and Smolensky 1993; Kager 1999; McCarthy 2002). In just a few years virtually all working phonologists switched into the OT paradigm. From my perspective two-level model where the ranking of the constraints plays the role that rule-ordering has in the sequential model.
 forms is basically a regular relation, then the choice between the two ways of decompos-ing it, either as a composed cascade of replace operations or as an intersection of parallel rules, has important practical consequences but it is not a deep theoretical divide. In fact, the two-level analyzer for French discussed in Karttunen, Kaplan, and Zaenen (1992) combined parallel rules with composition. It is unclear to me why my paper-and-pencil colleagues seem to think that it has to be absolutely one or the other.
 interested in, or at least aware of, what is happening in computational morphology (Karttunen 1993, 1998, 2003, 2006). I have not succeeded. Paper-and-pencil morphol-languages. They design formalisms for expressing generalizations about morphological phenomena commonly found in all natural languages. Practical issues that arise in the speed of applications are irrelevant from an academic morphologist X  X  point of view.
The main purpose of a morphologist writing for an audience of fellow linguists is to be convincing that his theory of word formation provides a more insightful and elegant account of this aspect of the human linguistic endowment than the competing theories and formalisms.
 finite-state implementation of Gregory Stump X  X  realizational morphology (Stump 2001; Karttunen 2003).
 tional knights have presented themselves at the Royal Court of Linguistics, rushed up to the Princess of Phonology and Morphology in great excitement to deliver the same message: And time after time, the put-down response from the Princess has been the same:
Because the most suitable suitor has always been rejected, I suspect that the Princess has a vested interest in making simple things appear more complicated than they really are. The good news that the computational knights are trying to deliver is unwelcome. The
Princess prefers the pretense that phonology/morphology is a profoundly complicated subject, believing herself to be shrouded by veils of theories. a different strategy. Instead of being the eternal rejected suitor at the Royal Court, they should adopt the role of the innocent boy in the street shouting
That was my conclusion in the 2003 paper. 5. Epilogue
I am very happy to see that the topics I worked on at the very beginning of my career have finally become relevant in NLP . To quote again the opening paragraph of my 1970 ACL presentation (Karttunen 1971b):
This 37-year-old prediction of semantics having practical value is becoming a reality in the context of automated question answering and reasoning initiatives such as the PASCAL Textual Entailment Challenge (Dagan, Glickman, and Magnini 2005) and the
ARDA -sponsored AQUAINT project (Karttunen and Zaenen 2005; Zaenen, Karttunen, and Crouch 2005). The first computational implementation of textual inferences arising from the six types of implicative constructions in Tables 1 and 2, and their interaction with factive verbs, is presented in Nairn, Condoravdi, and Karttunen (2006). We may soon see search engines that actually make use of semantic processing in addition to simple string matching. The ability to draw textual inferences will significantly improve the quality of question answering and Web searches.
 such as the classification of complement constructions. The availability of search engines actual usage. One question I have always had about the classification of implicative con-structions is whether the commitment to the truth or falsity of the complement clause is always based on a semantic entailment or whether some of these cases should be looked upon as a usage convention. For example, if you google the pattern didn X  X  hesitate to ,it is immediately evident that hesitate to really is one of the rare (20) a. Head Coach Jon Gruden didn X  X  hesitate to share interesting Buccaneer
When you see examples such as in (20) in their full context, it is obvious that the author presents the complement clause of hesitate to as a fact. But it is difficult to explain why things should be this way starting from the concept of hesitation or the semantics of the verb hesitate . 462 solve. The construction didn X  X  wait to is ambiguous. Here are a couple of examples from
Google to to illustrate the ambiguity. (21) a. Deena did not wait to talk to anyone. Instead, she ran home. (21a) implies Deena did not talk to anyone . But (21b) implies She told me something right away .

Question 1: How does it come about that X didn X  X  wait to do Y means either that X
When you look at examples with didn X  X  wait to in their full context, it is nearly always possible to tell which of the two meanings the author has in mind. In (21a), for instance, the negative polarity item anyone and the word instead are telltale indicators. In (21b), the cataphoric pronoun it indicates that a telling event took place. I am sure that it is possible to learn to pick the intended meaning by statistical techniques. But statistics alone will not give you an answer to Question 1, nor will it solve the related problem in Question 2.

Question 2: Why is it not possible to translate expressions such as Neil didn X  X  wait
In languages such as Dutch, Finnish, French, German, Hungarian, and Japanese, among others, it is of course possible to express the two meanings of X did not wait to Y ,butnot in one and the same sentence. My answer to these two questions will have to wait until my next semantics paper.
 Acknowledgments References 464 466
