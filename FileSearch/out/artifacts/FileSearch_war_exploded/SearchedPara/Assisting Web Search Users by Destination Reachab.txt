 Search engine users are increasingly performing complex tasks based on the simple keyword-in document-out paradigm. To assist users in accomplishing their tasks effectively, search engines provide query recommendations based on the user X  X  current query. These are suggestions for follow-up queries given the user-provided query. A large number of techniques have been proposed in the past on mining such query recom-mendations which include past user sessions (e.g., sequence of queries within a specified window of time) to identify most frequently occurring pairs, using click-through graphs (e.g., a bipartite graph of queries and the urls on which users clicked) and rank these suggestions using some form of frequency counts from the past query logs. Given the limited number of queries that are offered (typically 5) it is important to effectively rank them. In this paper, we present a novel approach to ranking query recommendations which not only consider relevance to the original query but also take into account efficiency of a query at accomplish-ing a user search task at hand. We formalize the notion of query efficiency and show how our objective function effec-tively captures this as determined by a human study and eliminates biases introduced by click-through based metrics. To compute this objective function, we present a pseudo-supervised learning technique where no explicit human ex-perts are required to label samples. In addition, our tech-niques effectively characterize preferred url destinations and project each query into a higher dimension space where each sub-spaces represents user intent using these characteristics. Finally, we present an extensive evaluation of our proposed methods against production systems and show our method to increase task completion efficiency by 15%.  X 
Chi-Hoon Lee participated in the work during his tenure at Yahoo! Labs.
 H.3.3 [ Information Search and Retrieval ]: Query for-mulation; H.3.5 [ Online Information Services ]: Web-based services Algorithms, Design Query Log mining, Query Recommendation, Search Assist Metric
Search engines are continuously looking into ways to im-prove user search experience and reduce user efforts in for-mulating their search tasks. One such user assistance method is query suggestions or query recommendations where search users are provided a suggestion for follow-up query based on a query they input. Query recommendation is a fun-damental tool in helping users complete their search task. Almost all state-of-the art search engines provide query rec-ommendations which largely focuses on providing as sugges-tion queries that are relevant to the user query. For instance, when a user submits the query  X  X lamo, X  the search engine may suggest  X  X lamo discount coupons X ,  X  X ertz X  or  X  X hrifty X .
Traditionally, studies on query recommendation have been largely focused on identifying sources or candidates for query suggestions. While relevance to original query is important, these approaches have not yet considered the efficiency of a suggestion in helping users to arrive at their desired desti-nation, as the following example shows.

Example 1.1. Consider the query q =  X  X lenty of fish X  for which candidates for query recommendations after min-ing query log sessions are: s1 =  X  X lenty of fish pof  X  s2 =  X  X lenty of fish login X  and s3 =  X  X lenty of fish dating site X  ranked in the order of relevance based on frequency. Inter-estingly, in our dataset we observed that sometimes relevant suggestions can  X  X low down X  users by competing with search results and can lead to in-efficient search sessions. Specifi-cally, user search query logs reveal that after submitting the query  X  X lenty of fish X  users arrive to the desired destination url www.plentyoffish.com but via several alternative paths. Figure 1 shows these alternative paths as edges between node q representing the query, node u representing the url, and s representing query suggestions s 1 ,s 2 ,s 3 . As seen in the fig-ure, sometimes users click on the desired search result after Figure 1: An example of paths from query to desti-nation URL which include algorithmic path as well as reformulated paths via recommended suggestions. submitting the query, whereas sometimes users follow the query recommendations and after multiple clicks on query suggestions arrive at the desired destination.

As illustrated by the above example, sometime users end up using query recommendations as  X  X teps X  towards their desired destination until they arrive at a suggestion that is highly indicative of their intent or the typical intent asso-ciated with the destination. For instance, users may con-sider  X  X acebook login page X  as a suggestion associated with their daily facebook activity as opposed to  X  X acebook X  even if search results for both suggestions contain the desired re-sult. We hypothesize that an effective query recommenda-tions approach should reduce such alternatives that compete with the search results and distract users.

Effectively ranking query recommendations presents sev-eral open challenges. First, commonly used user-feedback-based relevance metrics, e.g., click-through rate, prove to be insufficient and a biased estimator. The examples above illustrates how often users may click multiple query recom-mendations to arrive to their destination but unfortunately these clicks are not indicative of relevance or utility to search tasks; instead these are distractive clicks. Second, query logs typically consist of rich information for head queries and rel-atively sparser information for tail queries. However, query recommendations should be effectively produced for as many queries as possible. Finally, a search engine processes mil-lions of queries on a daily basis and thus the ranking mech-anism should not be computationally prohibitive.

Until now, the choice of query suggestions is largely based on relevance to original query and the impact of this choice on the efficiency of arriving at a destination page was ig-nored. However, considering the impact of suggestion effi-ciency is critical and with this in mind we propose a novel query suggestion ranking objective based on the efficiency with which a suggestion leads to the desired destination, i.e., url, and reduces alternatives that compete with the search results. For this, we first analyze past user query logs to understand the importance of efficiency and capture this by proposing a novel metric, called PGL for query q and sug-gestion s . We assert the effectiveness of our efficiency-based metric by conducting a first user-study. As we will see, this metric appropriately captures the efficiency in destination reachability but is computationally expensive and at times prohibitive at Web search scale. However, this analysis al-lows us to postulate necessary data requirements for char-acterizing the efficiency of query suggestions. Using these, we provide techniques to estimate the efficiency of a query suggestion conditioned on the user-provided query. In sum-mary, the contributions of this paper are: The rest of the paper is organized as follows: First, we present a detailed search logs analysis which forms the basis of our efficiency metric (Section 2). Then, we show how this metric can be computed for large scale query volumes ob-served by commercial search engine (Section 3). Finally, we present a detailed experimental evaluation (Section 4) and discuss related work (Section 5).
A critical goal for search engines is to help users in effi-ciently arriving at their destinations as quickly as possible.
As a first step towards building effective ranking algo-rithms for query recommendations, we performed an in-depth analysis of the past user query logs. Specifically, we  X  X liced X  the query logs to focus on the sequence of events that occur between users issuing a query to the search engine and reaching a destination page that meets the users needs. The problem of characterizing a destination page is an active re-search area and several methods have been proposed in the past [22, 7]. Without loss of generality, we used a simple destination page classifier based on the landing page dwell time , which is the total time a user spends on a webpage after clicking on one of the search results page. Intuitively, short dwell time could indicate that the page does not meet user X  X  needs and similarly, long dwell time indicates vice versa. In our experiments, we learned a threshold for the dwell time based on a user-study and set it to 100s. The details of this user-study are out of the scope of this paper but in brief, experienced annotators were asked to judge the quality of landing pages and these annotations were used to examine the page dwell time and pick a threshold based on precision and recall metrics. Applying this threshold allows us to efficiently process millions of queries in the logs, how-ever, other methods proposed in the literature can also be applied to this task.

Given this set of queries along with their destination pages, we denote the sequence of search events in between as a search path or simply path . Figure 1 shows one such query-destination graph. An interesting observation was made on analyzing these search paths. There were two types of search paths: (a) algorithmic paths , where users simply clicked on the search results directly, and (b) reformulated paths , where users reformulated their queries either by clicking on sug-gestions offered by the search engine or by themselves. An example algorithmic path would be (1: query= X  X mazon X , 2: url= www.amazon.com ) where the query was  X  X mazon X  and the user consumed www.amazon.com as their destination. The number (e.g., 1 and 2) preceding the colon symbol denotes the sequence number of observed search events. An ex-ample of reformulated path would be (1:query= X  X mazon X , 2:query= X  X mazon kindle X , 3:url= kindle.amazon.com ) where the first query  X  X mazon X  was reformulated to  X  X mazon kin-dle X  before the destination page was consumed. This indi-cates that oftentimes query recommendations compete with search results in that users parse the query recommenda-tions (instead of the search results), click on the recommen-dations and spend relatively more time in arriving to their destinations. This could be due to several reasons: first, oftentimes recommendations are placed in high-visibility re-gions (e.g., north of search results or south of search results before users click on the Next search page); second, some suggestions add additional context to the original query and may appear more promising even though the search results page may be near identical; third, users simply use these recommendations as milestones towards their search goals.
To gain further insights, we traced the timestamps for each search event and computed the path time defined as:
Definition 2.1. [Search path time] The search path time is the difference in timestamps for the first query event and last destination page event. Path times measure the overall time users spent in search before they arrive at their destination page. 2 We compared the algorithmic path time and reformulation path time and observed that for a large fraction of queries there exist reformation paths with shorter path times than algorithmic path times. This could be the case when refor-mulations can bring about the destination page at a higher rank position and thus easily visible to the user. Ideally, we would like to minimize the search path time for each queries and therefore surface query reformulations that are likely to reduce the search path time. Specifically, we characterize query suggestion efficiency as:
Definition 2.2. [Suggestion efficiency] Given a query suggestion, its efficiency is defined as the inverse of the search path time if a user were to accept this suggestion. 2 Our goal now is to rank query recommendations based on their efficiency and thus helping in arriving at their desti-nation pages quickly. For this, we show we can characterize suggestion efficiency based on the query-destination graphs such as that in Figure 1.
Given a query-destination graph, we characterize the ef-ficiency of each suggestion on a reformulated path using a novel metric called path gain-loss (PGL) . For a suggestion s , we mine the graph to estimate the amount of gain and loss in efficiency that will be introduced by s . Specifically, the process to compute the PGL of a suggestion s given an initial query, denoted as pgl(q,s) is shown in Algorithm 1. We split the search paths on the graph into the set P a algorithmic paths and the set P s of all reformulated paths that  X  X isited X  suggestion s . For each set P a and P s , we com-pute their median search path times, denoted m and  X  ( s ), respectively. We then apply two-sample t  X  test to  X  a and  X  ( s ) with the null hypothesis H 0 that both means are equal. When the null hypothesis is accepted, we conclude that us-ing suggestion s does not bring obvious gains or losses in the search path time. If otherwise, we use m , median search time for algorithmic paths, as an approximate time an aver-age search user takes to arrive to the destination page. For each path p  X  P s (eg., q  X  s 3  X  u from Figure 1), we further derive the gain G ( q,s,d,p ) and loss L ( q,s,d,p ) as follows: Input: Search Logs
Extract all competitive path sets , reformulated R and algorithmic A .
 R = p | first query ( p ) = q,last url ( p ) = u,s  X  p
A = p | first query ( p ) = q,last url ( p ) = u,s  X  p for each competitive sets R and A do end for Compute PGL by Equation 1 Output: PGL(q,s) where m t p indicates that m is longer than t p and t p de-notes the search time for path p . We would report gain if the reformulated path time t p is shorter than the median algo-rithmic path time m . Finally, we aggregate the G ( q,s,d,p ) and L ( q,s,d,p ) values for each query-destination graph that contains s and begins with the same query q but may lead to different destination urls (i.e., d ). Formally, PGL = where G ( q,s,d,p ) and L ( q,s,d,p ) are the gain and loss statis-tics for each reformulated path, s , over the query-destination graph ( q,d ). In this equation, we aggregate across all refor-mulated paths (i.e., p ) and destination urls to derive a PGL value for a pair of q and s . The smoothing parameters,  X  and  X  , deal with corner cases where the denominator is zero. Without loss of generality, we set both of them as 10 in our experiments. The pgl(q,s) values ranges from 0 to 1 with a value close to 1 indicates that suggestion s tends to reduce the search path time. Alternatively, PGL can be used to compute the overall quality of the suggestion set by aggre-gating pgl(q, s i ) for all suggestions s i to identify poor query recommendation cases.

With the PGL metric, we also attempt to address an in-herent bias introduced by CTR (click-through rate) of sug-gestions. CTR is a commonly used suggestion quality metric which ranks suggestions based on the click-through rates ob-served in the query logs. As discussed in the Introduction (see Example 1), oftentimes, suggestions compete with the search results and users may click on these suggestions to either arrive at a similar SERP or to further click on other suggestions. Such suggestions extend a user X  X  search ses-sion without contributing any new information, but can be ranked high if only CTR was considered. PGL on the other hand, only merits suggestions that are likely to bring users to their desired destinations faster.

As we will see in our experiments, PGL effectively cap-tures suggestion efficiency and ranking suggestions by PGL generates shorter search sessions. However, in practice de-riving pgl(q,s) for each query and its suggestion can prove to be prohibitively expensive. In particular, given a query q and suggestion s for which we want to derive pgl(q,s) , we need to examine all search paths (algorithmic and reformu-lated via s ) across all users that issued query q for all des-tination pages . Building query-destination graphs for each observed query q over terabytes of query logs is prohibitively expensive. To address this challenge and build a ranking al-gorithm amenable to production, we propose a novel method to estimate pgl(q,s) for each suggestion s without exhaus-tively building each query-destination graph. Before dis-cussing our estimation techniques, we give an overview of our suggestion ranking algorithm that incorporates both ef-ficiency and relevance of a suggestion.
While PGL captures the efficiency of a suggestion, rele-vance of the suggestion to the original user query cannot be ignored. To incorporate both efficiency and relevance into suggestion recommendation, we follow a two-phase ap-proach: 1. Given a query q , identify a set S of candidate sugges-2. For each suggestion s  X  S , assign an efficiency score. 3. Rank suggestions in S by their efficiency scores.
In Step 1, to identify a candidate set of relevant sugges-tions we mine search query logs sent to a major search engine for a period of approximately two years. The mining pro-cess constructs pairs of ( q,s ) where q is a query and s is a highly correlated query which can be offered as a suggestion. The mining process to generate these pairs is as follows: Given raw query logs data, we first identify a user session which is a sequence of queries issued within a time window of ten minutes. Typically, a user session consists of queries of a single intent 1 and therefore, queries q 1 ,q 2 sued within a query session could be considered as highly-correlated queries. However, due to the high level of noise in the query logs, several filters and proprietary algorithms are applied to these  X  X aw X  query-suggestion pairs to filter out low quality suggestions. For instance, queries that were is-sued by a robot, or had low responses on SERP, or had no meaningful SERP. We aggregate each pair to compute an association score C ( q,s ).

A simple approach deriving C ( q,s ) is to identify queries with high reformulation probability p ( s | q ) as suggestions. This can simply be measured over the frequency counts in the session logs: where f ( s,q ) is the frequency that these two queries are issued by the same user within a session. The problem with using reformulation probability as the merit for suggestion candidate generation is that a query that is not dependent on q might have a high reformulation probability just because
Note that this assumption has been empirically proven to be effective to serve 500 millions users per month. of its high marginal probability. To account for this, one can use point-wise mutual information (PMI) instead, where f ( s ) and f ( q ) are individual marginal counts 2 difference is that PMI normalizes the reformulation proba-bility by a factor of f ( s ); hence, it measures the dependency of these two queries. One weakness of PMI is that it might become very unstable for pairs of rare queries. As f ( q ) and f ( s ) get low, even a single coincidental co-occurrence might lead to a high PMI value. To account for this, we use a variant of PMI, namely, the log-likelihood ratio (LLR) instead [15]. We present further details of this algorithm is Section 4.1. A key advantage of this approach is that it is highly scalable when processing hundreds of terabyte log data, where each log record contains approximately 300 number of fields, using a grid computing technology such as Hadoop [10]. Specifically, we use Map/Reduce and Pig to process query logs.

For Step 2, we could rank each suggestion s  X  S by their pgl(q,s) but due to the reasons discussed above we compute ePGL, an estimate of the pgl(q,s) which we discuss next. In summary, given query q , our goal is to rank suggestions in S and select top k out of S , which is formalized as where ePGL is a scoring function with two input parameter queries q and s which is a suggestion candidate (typically, k is 5 or 12).
So far, we presented a novel metric pgl(q,s) to score a sug-gestion candidate s by its efficiency in arriving at a destina-tion page. We are now interested in building an estimator for PGL that is computationally feasible. We formulate the problem of estimating PGL as a  X  X seudo-supervised X  learn-ing problem which does not require explicit label samples. In our paper, our targets are generated by real search engine users X  responses, not by professional annotators who might miss real users X  intents with their own biases.

A fundamental observation in deriving pgl(q,s) is that ef-ficient suggestions (i.e., suggestions with high PGL scores) tend to be those that have led to clicks on search results (instead of clicks on other query suggestions). Therefore, a straightforward estimator would be to examine the rela-tion between the fraction of clicks generated after accepting suggestion s that were SERP clicks. Figure 2 shows the re-lation between PGL values and the fraction of SERP clicks per suggestions for our test set of 6000 suggestions (see Sec-tion 4.1 for details). Unfortunately, these two quantities are unrelated and thus this naive approach of click fraction is insufficient since it does not incorporate the quality of the suggestion in terms of its SERP.
A critical point captured by PGL scores is also that the search result pages of efficient suggestions are substantially
Without losing any generality, we approximate PMI by omitting term N from the nominator within log, which is the query and suggestion size from the query logs. Figure 2: Relation between PGL values and fraction of SERP clicks per suggestion clicks. different from the original query (e.g., the last  X  X op X  on the query-destination graph). Given a query q and a suggestion candidate s , we could capture difference in SERP by exam-ining the top-k results for both q and s . To capture this, we could examine the top-k results generated by q and s but this is naturally not feasible due to the query-per-second lim-itations of search engines. However, we can mine the query logs to reconstruct the SERP for both q and s . In particular, we use the clicked urls data for each query as a surrogate for the SERP and preferred destinations for a query, with the click frequency as an indicator of the rank position. To this end, we can compute a measure of similarity (or its dual di-versity) to assign a score to s with respect to q . The rest of this section presents our estimator which represents a query q and suggestion s using information from clicked-urls and computes a similarity Sim ( q,s ) between q and s . Later in Section 3.2 we discuss the relation between Sim ( q,s ) and pgl(q,s) values.

Our feature space to estimate PGL, which mines the clicked-urls data, projects queries into a higher dimension based on the url destination preferences of users for these queries. Several researchers have studied the problem of understand-ing destination preferences of a query, namely understand-ing the intent of the query based on the graph structural information [4, 13, 5], lexical characteristics and query log statistics [15, 23]. In this work, we exploit a new source of information  X  the contextual information encoded in the URLs that users clicked as derived from the query logs. The idea here is the oftentimes users design and structure their web pages to reflect the intent or action associated with the page and thus url strings act as a short summary of its con-tents. Table 1 shows some examples of queries and the urls that users clicked on. Intuitively, the tokens of a url string capture the intrinsic user intents and in turn the diversity between SERPs.

Given a query q and set U = { u 1 ,u 2 ,  X  X  X  ,u n } of clicked urls from the search logs, we construct a query-click-graph as shown in Figure 3. query q is generalized to n connectivities of urls, which is represented as vector q = ( f w 1 ,f w 2 where which is a weight of connectivity between q and url u i  X  U with their associated click counts c i when word w i is in bag of words S ( u i ) created from url u i . I ( E ) is an indicator Table 1: Examples of URL strings as summary of contents Figure 3: Connectivities between query q and its clicked-urls u i . Query q has connections with urls { u i } 3 i =1 and each connectivity i is associated with click counts c i used as an association weight. function: it returns 1, when E is true. A word w i minimum unit of terms from a url string, which determines a dimension in higher feature space 3 of q . A weight that measures the association between q and u i is represented by its clicks from the query logs.

Given the feature vector representations q and s of q and suggestion candidate s , respectively, we compute similarity in SERPs as a product of two vectors and a d  X  d matrix M that determines importance of each feature dimension, expressed as Much research has addressed to improve the effectiveness of a score Sim ( q,s ) such as learning matrix M [2, 12] and an approximation of a row matrix of q to discover latent structure [6, 11]. However, most of the work involves poly-nomial computations (e.g., inverse of a matrix), which is ex-tremely challenging in dealing with large scale of data such as a commercial search engine. Instead of learning matrix M , we assume the matrix as a diagonal (e.g., unit) matrix, which reduces Equation (3) to cosine similarity in our fea-ture space. As we will see the characteristics of PGL (its efficiency is strongly correlated with destination diversity), allow for our feature representation to be a good candidate as a predictor variable in estimating PGL.
Figure 4 illustrates the correlations between scores gener-ated from Equation (3) and PGL. Each data point in Fig-ure 4 plots pair of (score, PGL) values for two queries. As claimed from the previous section, the graph shows correla-tions: as our scores in X-axis approach to 1, PGLs values in Y-axis decreases. This strongly supports the utility of our score as an estimator of PGL. Note that the red curve in Figure 4 is a quadratic fit. Using this fit, we simply tested the quality of the quadratic fit. Basically, we convert a PGL value for pair ( q,s ) to 0 when they are less than 0.5. Other-wise 1. Here, s is a suggestion of q . Having 0 for pair ( q,s ) indicates that s is not a good suggestions since its efficiency has produced more loss paths, while 1 for a good sugges-tion. Accuracy of the quadratic fit is plotted in Figure 5. Red colored points indicate true positive (TP), where the es-
Note that the sparseness of the feature space can be further discussed using low rank approximation. Figure 4: Correlation between Sim ( q,s ) scores from Equation (3) (X-axis) and PGL (Y-axis). Each point in the plot is generated from a pair of two queries ( q,s ) (note that s can be considered as a suggestion of query q .) The red curve from the plot is a quadratic regression fit to estimate PGL with inputs of score from Equation(3). timates and PGL agree that suggestions from the pairs are good ones. The green colored points indicate true negative, where both of the estimates and PGL consider the sugges-tions are not useful to users queries. Blues are for false positive (FP) and blacks false negative. Based on the fit, the precision is 0.8127, derived from TP/(TP+FP), which is the ratio to correctly classifies items into a target class.
As an alternative, we also trained a Classification And Re-gression Tree (CART) [3], which is a non-parametric deci-sion tree learning algorithm, specifically using classification trees. This allows to learn a non linear decision boundary as opposed to the result from the previous quadratic fit. Al-though CART is one of the state-of-art in predictive models, it requires users X  input to start with such as the minimum number of data samples per node. We have used a held-out set to tune the parameter to avoid over-fitting. Figure 6(a) shows that the testing errors (Blue curve) are increased by increasing the model complexity (ie., reducing the number of minimum samples per nodes (X-axis)), as opposed to train-ing errors (Red curve) are decreased. In this exercise, it is obvious to observe the over-fitting due to our single predictor variable.

To compare accuracy between the quadratic fitting with a tree approach, we randomly select 6000 samples and split them into training and testing data as 60% and 40%, respec-tively with 100 iterations. Overall, a tree (an example shown in Figure 6(b)) shows better accuracy: 0.832 (tree) vs. 0.824 (quadratic fit), although the difference is marginal. When our Sim ( q,s ) is between 0.1965 and 0.6665 in Figure 6(b), it corresponds to high utility w.r.t. PGL. This is aligned with PGL X  X  characteristic to penalize suggestions that compete with algo results of query due to possibility of redundant paths to a same destination. It in turn implicitly favors sug-gestions that diversifies query X  X  reachability. In addition, it is interesting to observe that low scores from Equation (3) Figure 5: Performance of a quadratic estimator to PGL values mapped to { 0, 1 } and ePGL . Colors red, black, blue, and green correspond to true positive, false negative, false positive, and true negative, re-spectively. are estimated as high utility w.r.t. PGL, although the low score itself from Equation (3) implies that q and s are orthog-onally projected onto the feature space. However, this is a critical characteristic in understanding PGL since some sug-gestions that are not contextually related to a query could be good candidates to lead users to enhanced experiences (eg., suggestion  X  X ears X  would be a good candidate for query  X  X ry X  X  electronics X .) This insight is in turn captured by es-timators using scores from Equation (3) with projection on our feature representation.
We now present our experimental evaluation. We present our experimental settings in Section 4.1. Next, we estab-lish the effectiveness of PGL as an effective metric at im-proving user interaction with a search engine in Section 4.2. Finally, we evaluate our estimation techniques for PGL in Section 4.3. Data collection . We processed a fully anonymized set of query logs of a major search engine for a period of about two years to generate pairs ( q,s ) where q is a query for which s is a relevant suggestion. Each pair is given an association score based on their co-occurrence and relevance strength as discussed in Section 2.3. Of these, we used a random sample of 55K queries from the first three months of 2011 for our search log analysis presented in Section 2.2. Table 2 shows the distribution of the number of suggestions per query. Compared methods . To the best of our knowledge, we are not aware of any other method on ranking suggestions by their efficiency. However, as a strong baseline we use the suggestion recommendation system of a major search engine. We denote this method as BN . We implement our ranking algorithm discussed in Section 2.3 and denote that method (a) By increasing model complexity the training errors (Red curve) decreased, while the testing errors (Blue curve) in-creased. Errors are in Y-axis. Model complexities in X-axis are increased by decreasing minimum number of samples in a splitting node. As the model complexity moves to the right side, the min number of data sample for a node to split gets smaller. (b) An example of a learned tree showing non-linearity decision boundary. Precision of this decision tree is 0.8363.
It is important to note that when x1 (ie., our Sim ( q,s )) is between 0.1965 and 0.6665, it corresponds to high utility w.r.t. PGL, by being classified as 1. This is aligned with
PGL X  X  characteristic that favors a lateral move type of suggestions yielding high PGL values.

Table 2: Distribution of number of suggestions. as ePGL. In particular, for Step 2, we build an estimator for ePGL using the quadratic regression method discussed in Section 4.3 over a training set of 6000 ( q,s ) query-suggestion pairs randomly sampled from the query set.
 Evaluation method . To examine the effectiveness of our proposed metric, PGL, we ran a bucket test over live search traffic. To evaluate ePGL, we performed two types of tests: (a) an offline test which involved a team of professional an-notators that were asked to judge the overall quality of the suggestions produced by a suggestion algorithm, the details of which were hidden from the annotator; (b) an online test which involved live search traffic. We present the details of these studies shortly in the next sections.
 Evaluation metrics . For experiments involving live search traffic, we evaluated the performance of a suggestion rec-ommendation system using a variety of proprietary  X  X ser-engagement X  metrics. When necessary we will provide the intuition behind these metrics but leave out complete details of these metrics due to Intellectual Properties.
We start our evaluation by examining the effectiveness of our PGL metric. A particularly important challenge here is collecting the  X  X round-truth. X  Manually building ground-truth sets of evaluating the quality of suggestions is unfea-sible due to the following reasons: first, we would require queries that span across all quantiles of query frequencies (i.e., head as well as tail queries) and the annotators would need to have some background information about these top-ics which can be impractical; second, manual annotators of-ten introduce a bias similar to that introduce by suggestion CTR, e.g.,  X  X acebook login X  is considered a good suggestion candidate for the query  X  X acebook X  although it can introduce inefficiency in the search task.

We tested PGL using on-line tests (i.e., bucket tests on live search traffic). Specifically, we generated two mutually exclusive groups or buckets of search users. In one bucket, we presented our baseline suggestion algorithm BN . For the other bucket, we presented our test suggestion algorithm denoted PGL-OVR . As discussed earlier, computing PGL val-ues for every suggestion is unfeasible therefore to imple-ment PGL-OVR , we generated a  X  X lacklist X  of suggestions for each query. For this, we mined three months of query logs and generated 600K unique query-destination graphs and computed PGL values for each suggestion in these graphs. All suggestions with pgl(q,s) values below or equal 0.3 were added to the blacklist and at runtime, suggestions that be-long to the blacklist were not shown to the users. The black-list consisted of 18 thousand suggestions.

To avoid position bias, we placed suggestions in two sec-tions on the search result page, (a) Nqs , north of the search results, (b) Sqs , south of the search results. Table 3 shows the various search performance metrics for both algorithms where the improvements by PGL-OVR were statistically sig-nificant. As seen in the table, using PGL-OVR , the rate at which users are submitting manually reformulated queries (i.e., when suggestions are not useful to them), and the rate Table 3: PGL-OVR  X  X  relative percentage changes over baseline.
 at which users are abandoning the results decrease. Fur-ther, we also see an increase in SERP clicks and interestingly an increase in the advertisements displayed. This happens mostly due to the fact that PGL promotes suggestions that bring about diverse search result pages. Finally, we also show the average search path time (see Section 2.2) and using PGL-OVR , reduces the search path time as expected. Next, we examine the gain and loss values (see Section 2.2 for both Nqs and Sqs ; see Table 4. As we can see, the PGL utility improved about 9.3% for Nqs suggestions and 4.3% for Sqs suggestions; both improvements are statisti-cally significant. In summary, through tables 3 and 4, PGL correlates well with user experience and quicker search ses-sions.
We now present our evaluation of ePGL which is two-fold: Offline evaluation (Section 4.3.1) and Online evaluation (Section 4.3.2).
For our offline experiments, we conducted a user study which consisted of five professional search engine quality evaluators experienced in assessing the quality of query sug-gestions. We randomly selected 1500 queries and selected top-k suggestions generated by both BN and ePGL, and pre-sented them to evaluators. Evaluators were asked to com-pare both suggestions  X  X ide-by-side X  and provide a judgment on which side was preferred. In order to avoid a position bias that could be driven by a presentation for results, we ran-domly swapping a side of each result per query. Judgments were split into: left/right side is much better, left/right side is better, left/right side is slightly better, both are equal. The judgement criteria was to consider 1) poor ranking of suggestions (ie., highly relevant suggestions are ranked higher than non-relevant suggestions); 2) suggestions redun-dant to queries are included in a result set. (e.g., suggestion  X  X ahoo mail login X  is not a good candidate for query  X  X ahoo mail signin X ); 3) a suggestion set is disambiguated enough to cover uncertain query intents. As an example, Figure 7 shows suggestions produced by BN (left) and ePGL (right) for query  X  X ython X . BN produces a single suggestion where Figure 7: Ranked list of suggestions produced by BN (left) and ePGL (right) for query  X  X ython. X  ePGL delivers suggestions that cover a variety of SERPs, namely, programming language or a snake. Finally, Table 5 summarizes the grade distributions highlighting strong edi-torial preferences of ePGL X  X  ranking over BN  X  X  one. Accord-ing to the comments from the evaluators, ePGL X  X  ranking improves ranking quality itself as well as reduces redundancy with queries.
 Table 5: Preferences for BN and ePGL gathered from a user study.
Evaluations of our online experiments are based on two different families of performance metrics: suggestion-level metrics and search engine result page, SERP-level metrics. Suggestion-level metrics examine whether users utilize query recommendations and if they are assisting in reducing users X  querying efforts; similarly, SERP-level metrics examine if users are arriving at their desired destinations and their in-teraction with SERP. Table 6 shows results for these met-rics and shows the relative percentage difference between BN and ePGL. As seen in the table, CTR on suggestions, which captures total click counts for displayed suggestion, was improved by 43% when using ePGL, and Drop rate, which captures cases where users click on a suggestion but do not click on the SERP, decreases by -0.24%. Direct retry rate, which captures the case of users manually reformu-lating their queries (since query suggestions are not useful) decreased by -0.074%. On SERP, the CTR increased by 0.25% and good click rate, which captures the case where users are arriving at their desired destination after a SERP click, increased by 0.19%. Overall, using ePGL improves the quality of query suggestions by surfacing suggestions that capture users diverse needs and helping them arrive at their destination pages quickly.

To highlight the difference between ePGL and BN , Table 7 shows examples of suggestions provided for two queries in the online test setting. We pick as examples two queries with different typical intents and show how ePGL can produce interesting results for both cases. The query  X  X dvance auto parts store X  has a strong navigational intent to its author-ity url, http:// shop.advanceautoparts.com with emphasis on finding locations of the store. Our ePGL method effec-tively captures the utility based on the feature that explores the destination reachability, and thus it ranks  X  X tore locator X  Metric Relative Percentage change CTR on suggestion 43% Dropped rate -0.024% CTR on SERP 0.25% Good click rate 0.19% Direct retry rate -0.074% Table 6: Relative percentage change comparing users X  responses from ePGL with BN . Bolded statis-tics indicate statistical significance at level p &lt; 0 . 05 Figure 8: Daily ratio of click rates. A positive click rate in Y-axis for each day in X-axis indicates sug-gestions by ePGL attracts more users X  attentions than ones by BN at statistically significance level p &lt; 0 . 05 . suggestion to the top, which helps users efficiently find their destination. In addition, we also take an example for an intent of information exploration, which shows how ePGL effectively ranks suggestions when the destination of an in-tent typically does not have an authority. For query  X  X orld X  X  oldest cat X  in Table 7, more relevant suggestions are shifted to higher ranks in ePGL: that is, suggestion  X  X orld X  X  oldest cat creme puff X  directly leads users to destinations that pro-vide relevant information for the world X  X  oldest cat, whose name is Creme Puff. In addition, suggestion  X  X ow old is the world X  X  oldest cat X  lets users find a specific answer strongly associated with the query. Figure 8 visualizes ratio on the daily clicks of suggestions to highlight robustness of ePGL over BN . Overall, suggestions delivered by ePGL show sig-nificantly (at statistically significant level p &lt; 0 . 05) much higher attentions than ones to BN .

Finally, we also perform a post-mortem analysis on the query logs for this online traffic and compare BN and ePGL using PGL. As expected, users experiences from ePGL show significant improvement (  X  15.07% ) over ones from BN : that is, PGL value from ePGL is 0.84 and 0.73 from BN . We focus on subset of paths from PGL to see how many of sug-gestions degrades efficiency for destination reachability due to the algorithmic competing. Specifically, we count paths between a query and users X  destinations. If a destination is presented on SERP by a query offering a suggestion leading to the same destination would delay reachability. Therefore, the less number these incidents are, the more efficient users X  search experiences are. For ePGL, there are 8.3% of total
Table 7: Top 5 Suggestions using BN and ePGL. paths (  X  50 K ) that were delayed by suggestions while BN has 9.6%.
In summary, we observed that using PGL, even in the form of a blacklist of suggestions, can greatly improve the quality of interaction users have with a search engine. Furthermore, our estimator ePGL effectively captures the merits of PGL and shows improvements for performance metrics related to user engagement on the query recommendations as well as the search results page.
Ranking query suggestions: Many research has been working to improve the quality of suggestion by exploring ranking objective functions. Mei et al. [16] constructed a bipartite-graph between query and clicked urls with a ran-dom walk by  X  X itting time X  that estimates the expected first time to reach to a node. The lower from a query to a sugges-tion the hitting time is, the higher the suggestion is ranked. Several differences from our work are; 1) Mei X  X  ranking only considers the clicks between queries and urls, and thus in-tents implied from ulrs are not well incorporated; 2) their graph construction is a query dependent, where incremental query processing is computationally challenging.; 3) there is no clear way to distinguish if click on a url are good or an accidental ones. Dang and Croft [20] discussed a ranking ob-jective for suggestion recommendation. However, their work did not explicitly consider the associations between queries and suggestions, and often miss users X  intents.

Providing query suggestions: Several past research ef-forts have built methods to generate suggestions for follow-up queries. Typically, these methods mine observed queries and developed similarity measures: in [8] association rules are used to mine query recommendations from queries in individual users X  search sessions which are defined as fixed length periods of interaction. [23] builds a graph represen-tation of the of the sequential search queries and combines it with a content based similarity method to account for the sparsity of the query logs. These methods simply mine query reformulations from observed queries. Other methods lever-age the click information, as well. In [17], a query similarity measure is developed based on term-weight vector space rep-resentations of the queries and clicked URLs. Query term and click pattern overlap is used in [21] to develop query similarity measures in order to cluster user queries. Random walks on query-click graph is used by [4]. Slightly differently, Markov Random Field models are used over the query-click graph to generate bid terms in [9].

Earlier methods of generating alternative queries also in-cluded query expansion by pseudo-relevance feedback where additional query terms are obtained from the documents retrieved by the original query. These terms are used to expand the original query to retrieve more specific docu-ments [18, 1, 14, 15, 19].

Our work complements the above line of work in that we are interested in ranking suggestions with the goal of helping users reach their end goals efficiently.
This paper introduced a novel performance objective for query recommendation systems employed by search engines. Our proposed metric, PGL, examines the likelihood of a sug-gestion competing with a search result page and in turn de-laying the users in arriving at their destination, and captures the efficiency of a query suggestion. We establish the effec-tiveness of this metric over a real-world setting and show how accounting for efficiency improves search user experi-ence. We present a highly scalable algorithm to estimate PGL values that handle the large volume of queries pro-cessed by a commercial search engine. Our experiments show that our estimation technique correctly predicts the PGL values and also improves the search session quality of users.

Our work in the paper has so far only established the foun-dations of this area, and many interesting research problems remain open. First, while PGL captures the efficiency with which a user will arrive at their destination post clicking a suggestion, we still need to account for the efficiency with which a user will click on the suggestion pre suggestion click. Specifically, suggestions carry an inherent click appeal which also needs to be modeled which remains future work. Sec-ond, we would like to study the case of queries where no effective suggestions can be provided. For instance, navi-gational queries such as  X  X nn X  or  X  X itibank X  often result in a direct SERP click and make the case of no-query recom-mendations. Third, we plan on extending our estimation technique to incorporate an  X  X xplore-exploit X  model which observes how users interact with a suggestion over a period of time or over a fixed number of views. After exploring a suggestion for fixed window, we can identify suggestions that failed to generate any interest from the search users. This could be either due to poor suggestion or a simple temporal drift in the interesting topic pertaining the query.
We appreciate anonymous reviewers X  comments that were valuable in improving the quality of this paper. We also thank the editorial team at Yahoo! Inc. for their insightful comments. [1] P. Anick. Using terminological feedback for web search [2] F. R. Bach and M. I. Jordan. Learning spectral [3] L. Breiman et al. Classification and Regression Trees . [4] N. Craswell and M. Szummer. Random walks on the [5] H. Cui, J.-R. Wen, J.-Y. Nie, and W.-Y. Ma. Query [6] S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. [7] D. Downey, S. Dumais, D. Liebling, and E. Horvitz. [8] B. M. Fonseca, P. B. Golgher, E. S. de Moura, and [9] A. Fuxman, P. Tsaparas, K. Achan, and R. Agrawal. [10] Hadoop. http://hadoop.apache.org/ , 2009. [11] T. Hofmann. Probabilistic latent semantic indexing. In [12] T. Hofmann. Learning the similarity of documents: [13] Y. M. Idan Szpektor, Aristides Gionis. Improving [14] R. Jones and D. C. Fain. Query word deletion [15] R. Jones, B. Rey, O. Madani, and W. Greiner.
 [16] Q. Mei, D. Zhou, and K. Church. Query suggestion [17] C. H. Ricardo Baeza-Yates and M. Mendoza. Query [18] I. Ruthven. Re-examining the potential effectiveness of [19] E. Terra and C. L. Clarke. Scoring missing terms in [20] M. B. Van Dang and W. B. Croft. Learning to rank [21] J.-R. Wen, J.-Y. Nie, and H.-J. Zhang. Clustering user [22] R. W. White, M. Bilenko, and S. Cucerzan. Studying [23] Z. Zhang and O. Nasraoui. Mining search engine query
