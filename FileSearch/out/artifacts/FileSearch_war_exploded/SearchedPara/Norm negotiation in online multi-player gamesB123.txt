 Guido Boella  X  Patrice Caire  X  Leendert van der Torre Abstract In this paper, we introduce an agent communication protocol and speech acts for norm negotiation. The protocol creates individual or contractual obligations to fulfill goals of the agents based on the so-called social delegation cycle. First, agents communicate their individual goals and powers. Second, they propose social goals which can be accepted or rejected by other agents. Third, they propose obligations and sanctions to achieve the social goal, which can again be accepted or rejected. Finally, the agents accept the new norm by indicating which of their communicated individual goals the norm achieves. The semantics of the speech acts is based on a commitment to public mental attitudes. The norm negotiation model is illustrated by an example of norm negotiation in multi-player online gaming. Keywords Multi-player online games  X  Normative multi-agent systems  X  Dependence networks  X  Social delegation cycle  X  Agent communication protocols 1 Introduction Online multi-player games are online spaces in which human and artificial agents interact with each other every day [ 5 ]. For example, EverQuest has more than 400,000 subscribers and has developed into a complex system of social dynamics, like the Sims Online or Second become among the most complex and sophisticated online social spaces in existence [ 36 ]. One aspect in which online games move toward social spaces is that players become more independent and autonomous with respect to the game designers, in the sense that they start to play in ways unforeseen by the game designers.

Why, you might ask, would anyone waste four hours of their life doing this? Because a game said it couldn X  X  be done.

This is like the Quake freaks that fire their rocket launchers at their own feet to propel themselves up so they can jump straight to the exit and skip 90% of the level and finish in 2 seconds. Someone probably told them they couldn X  X  finish in less than a minute.
Games are about challenges, about hurdles or puzzles or fights overcome. To some players, the biggest hurdle or challenge is how to do what you (the designer) said couldn X  X  happen. If you are making a game, accept this [ 36 ].

An important aspect of autonomy, both for individuals and groups, is the creation of one X  X  own norms, as reflected by the literal meaning of autonomy: auto-nomos. In online games, example in this paper. Moreover, autonomy is a central concept in agent theory, and there-fore multi-agent systems technology can be used to develop online games. However, existing social mechanisms for norm creation and change in multi-agent systems are restricted in the sense that either norms are not explicitly represented, or there are no communication primi-tives, protocols and social mechanisms to create or change norms. Normative multi-agent societies coordinating their behavior via obligations, norms and social laws. A distinguishing feature from group planning is that also sanctions and control systems for the individual or contractual obligations can be created. Since agents may have conflicting goals with respect to the norms that emerge, they can negotiate amongst each other which norm will be created. process:  X  Propose the social goal and argue about that (instead of merging), and  X  Negotiate the norm and sanction.

The research question of this paper is how to define a communication protocol for online norm negotiation explains also what it means, for example, to recognize or to obey a norm, and how new norms interact with existing ones. This breaks down in the following three sub-questions: 1. How to use agent technology for the social delegation cycle referring to the individual 2. How do agents publicly accept a norm in the social delegation cycle based on public 3. How to use the communication protocol in multi-player online games like EverQuest
The semantics of the speech acts is based on public mental attitudes. To keep the formal but sketch informally their effects and illustrate them by a running example. Formalizations of speech acts with public mental attitudes can be found in [ 27 , 30 , 41 ].
An important subtask of the social delegation cycle is to break down goals in subgoals, using a goal hierarchy. However, to focus on the main social mechanism, in this paper, we do not detail the sub-protocol for subgoaling. Therefore, we assume that the individual goals communicated by the agents are such that they can be seen to by other agents. The extension with a protocol for subgoaling is left for further research.

The layout of this paper is as follows. In Sect. 2 , we discuss norm creation in online games and social spaces, and we introduce our running example. In Sect. 3 , we define our social X  cognitive conceptual model of multi-agent systems and in Sect. 4 , we discuss the kind of agent communication language in which we study and formalize the social delegation cycle. In Sect. 5 , we formalize individual goal and power communication. In Sect. 6 ,wedefinethe negotiation protocol, which we use in Sect. 7 to formalize goal negotiation, and in Sect. 8 we formalize norm negotiation. Finally, in Sect. 9 , we formalize the acceptance relation. In Sect. 10 , we discuss related research of an abstract model of the social delegation cycle in Tennenholtz X  game-theoretic artificial social systems, our own more detailed social X  X ognitive model, and we discuss why these models are less suitable for communication models in online games and social spaces than our power and dependence-based model. 2 Multi-player online games As a running example, we use in this paper, an example discussed by Ludlow [ 36 ] from Sony X  X  EverQuest. 2.1 EverQuest EverQuest is a multi-player online game where gamers are supposed to fight each other in a world of snakes, dragons, gods and the Sleeper. Sony intended the Sleeper to be unkillable and gave it an extreme high hit points. However, a combined combat of close to 200 players nearly succeeded to kill the animal. Unfortunately, Sony decided to intervene and rescue the monster. Most of the discussion on this example has highlighted the decrease in trust of the game players in Sony, despite the fact that the next day Sony let the game players beat the Sleeper. However, in this paper, we would like to highlight what this story tells us about the desires of game players, and its consequences for necessary technology in games. The following quote illustrates the excitement in killing the Sleeper.

A supposedly [player-vs.-player] server banded together 200 people. The chat channels across the server were ablaze, as no less than 5,000 of us listened in, with  X  X MG they X  X e attempting the Sleeper! Good luck d00dz! X  Everyone clustered near their screens, sharing the thrill of the fight, the nobility of the attempt and the courage of those brave 200. Play slowed to a crawl on every server as whispers turned to shouts, as naysayers predicted,  X  X t can X  X  be done X  or  X  X t will drop a rusty level 1 sword X  and most of us just held our breath, silently urging them forward. Rumors abounded:  X  X f they win, the whole EQ world stops and you get the text from the end of Wizardry 1, X  or  X  X f they win, the president of Sony will log on and congratulate them. X  With thousands watching and waiting, the Sleeper X  X  health inched ever downward. ... [EverQuest player] Ghenwivar writes, On Monday, November 17th, in the most amazing and exciting battle ever, [EverQuest guilds] Ascending Dawn, Wudan and
Magus Imperialis Magicus defeated Kerafyrm, also known as The Sleeper, for the first time ever on an EverQuest server. The fight lasted approximately three hours and about 170180 players from [EverQuest server] Rallos Zek X  X  top three guilds were involved.
Hats off to everyone who made this possible and put aside their differences in order to accomplish the impossible. Congratulations RZ!!! [ 36 ]
The example illustrates that the game had been so well wrought that a real community of players had formed, one that was able to set aside its differences, at least for a night, in pursuit of a common goal. This was not intended or foreseen by Sony, and getting 200 people to focus on accomplishing the same task is a challenge. In this paper, we study multi-agent technology to support these emerging cooperation in online games. 2.2 Multi-agent systems Computer games have traditionally implemented empirical solutions to many AI problems and are now turning to more traditional AI algorithms. Cavazza [ 19 ] introduced the role of AI in gameplay, reviewed the main techniques used in current computer games such as finite-state transition networks, rule-based systems and search algorithms, described the implementation of AI in several commercial computer games, as well as academic research in AI targeting computer games applications, and discussed future trends and proposing research directions.
Another important challenge for AI in computer games is the development of on-line and multi-player gaming. In theory, the availability of human opponents around the clock would seal the fate of AI in on-line games. However, this is unlikely to be the case, as for instance autonomous actors would still be needed for the most mundane tasks in large-scale simulations and on-line games, as human players are unwilling to play such parts.

The recent advances in graphic rendering will leave more room for the future develop-ment or AI in games. With the level of realism currently achieved, further improvements in realism will come from physics and AI. The development of AI should also foster the development of new game genres based on more sophisticated user interaction with artificial actors [ 19 ].

LairdandvanLent[ 32 ] proposed that artificial intelligence for interactive computer games is an emerging application area in which this goal of human-level AI can successfully be pursued. Interactive computer games have increasingly complex and realistic worlds and increasingly complex and intelligent computer-controlled characters. They motivated their proposal of using interactive computer games, reviewed previous research on AI and games, presented the different game genres and the roles that human level AI could play within these genres, described the research issues and AI techniques that are relevant to each of these roles. Their conclusion is that interactive computer games provide a rich environment for incremental research on human-level AI.

From a researcher X  X  perspective, even if you are not interested in human-level AI, com-puter games offer interesting and challenging environments for many, more isolated, research problems in AI. We are most interested in human-level AI, and wish to leverage computer games to rally support for research in human-level AI. One attractive aspect of working in computer games is that there is no need to attempt a  X  X anhattan Project X  approach with a monolithic project that attempts to create human-level intelligence all at once. Computer games provide an environment for continual, steady advancement and a series of increasingly difficult challenges. Just as computers have inexorably got-ten faster, computer game environments are becoming more and more realistic worlds, requiring more and more complex behavior from their characters. Now is the time for
AI researchers to jump in and ride the wave of computer games [ 32 ]. 2.3 Agent communication protocol We use the EverQuest example in this paper to illustrate our communication model. It consists of the following four steps. 1. Agents communicate their individual goals and powers. They say which monster they 2. They propose social goals which can be accepted or rejected by other agents, based on 3. They propose obligations and sanctions to achieve the social goal, which can again be 4. Finally, the agents who like to participate in the coordinated attack accept the new norm 3 Power viewpoint on normative multi-agent systems an agent is more powerful than another agent if it can achieve more goals.

For example, in the so-called power view on multi-agent systems [ 6 ], a multi-agent system sense that achieving some goals may make it impossible to achieve other goals, the function goals returns a set of set of goals for each set of agents. Such abstract structures have been studied as qualitative games by Wooldridge and Dunne [ 56 ], although they do not call the ability of agents to achieve goals their power. To model trade-offs among goals of agents, we introduce a priority relation among goals.
 Definition 1 Let a multi-agent system be a tuple A , G , goals , power ,  X  where:  X  the set of agents A and the set of goals G are two finite disjoint sets;  X  goals : A  X  2 G is a function that associates with each agent the goals the agent desires  X  po w er : 2 A  X  2 2 G is a function that associates with each set of agents the sets of goals  X   X : A  X  X  X  2 G  X  2 G is a function that associates with each agent a partial pre-ordering example.
 Example 1 The set of agents are the players that participate in the coordinated attack. We assume that the individual agents have various ways to attack the Sleeper, and they disagree about which is the best way to kill the beast. The goals of the agents are different individual ways to attack the animal. The powers of the agents are the goals each agent can achieve. The priorities of the agents are their preferences for the ways to attack the animal.
To model the role of power in norm negotiation, we extend the basic power view in a couple of ways. To model obligations we introduce a set of norms, we associate with each norm the set of agents that has to fulfill it, and for each norm we represent how to fulfill it, and what happens when it is not fulfilled. In particular, we relate norms to goals in the following two ways.  X  Second, we associate with each norm a set of goals V ( n ) which will not be achieved if
Since we accept norms without sanctions, we do not assume that the sanction affects at least one goal of each agent of the group the obligation belongs to.
 Definition 2 Let a normative multi-agent system be a tuple MAS , N , O , V extending a multi-agent system MAS = A , G , goals , power ,  X  where:  X  the set of norms N is a finite set disjoint from A and G ;  X  O : N  X  A  X  2 G is a function that associates with each norm and agent the goals  X  V : N  X  A  X  2 G is a function that associates with each norm and agent the goals that The normative multi-agent system is illustrated by our running example.
 Example 2 The obligations of each agent are the attacks each individual player must make. Sanctions can be that someone is removed from the coalition, has to pay a penalty (if this is possible in the game), gets a lower reputation, and so on.

An alternative way to represent normative multiagent systems replaces the function power by a function representing dependencies between agents. For example, a function of minimal dependence can be defined as follows. Agent a depends on agent set B  X  A regarding the g  X  power ( C ) . Note that dependence defined in this way is more abstract than power, in the sense that we have defined dependence in terms of power, but we cannot define power in terms of dependence. 4 Agent communication languages Agent communication languages share the idea that agents communicate by performing with preconditions and effects [ 3 , 21 ]. Longer stretches of dialog can be explained by plans variations of agent communication languages have been designed on these common speech act foundations, thus making the standardization effort difficult.

There is a distinction in terms of communication primitives between two main traditions in agent communication. The agent X  X  mental attitudes like beliefs and intentions are inclu-ded in preconditions and effects of speech acts used in information seeking dialogs using speech acts such as inform and request. This side has been popularized by the standardi-zation efforts of the Foundation for Intelligent Physical Agents [ 25 ], although the concepts of desire , goal and intention are not used uniformly, and desire and goal are often used interchangeably. Below the agent social semantics tradition, developed as a reaction to FIPA, is depicted [ 48 ]. The agent can be either a creditor or a debtor of his social commitments. The semantics of speech acts used in negotiation or persuasion refer to these commitments. Although both kinds of dialog use the terminology of commitment, requesting an action in negotiation means something else than defending a proposition in persuasion after a chal-lenge. We therefore distinguish action commitment from propositional commitment, where the former is used to give a semantics of speech acts like request or propose in negotia-tion [ 26 , 48 ], and the latter is used for speech acts like assert or challenge in persuasion [ 29 , 55 ].

In this paper, we use an approach bridging the two traditions based on a reinterpretation of the beliefs and intentions. The idea that meaning cannot be private, but is inter-subjective or based on a common ground, has been accepted for a long time in the philosophy of language. This idea of public meaning has been discussed, for example, by [ 33 ]or[ 49 ], who stress the importance of the common ground. The crucial point is that a private semantics does not make it possible for a language user to be objectively wrong about an interpretation from a third person point of view. In contrast to FIPA, beliefs and intentions are not interpreted as private mental attitudes, but as some kind of public mental attitudes, for example as grounded beliefs [ 27 ] or as ostensible beliefs or public opinions [ 41 ]. 5 Individual goal and power communication The first phase consists of communicating the individual goals and powers of the agents. Since existing agent communication languages do not cover inform speech acts whose content is a public goal, we introduce the speech act desire . The semantics of the speech act desire ( a , X ) is that agent a has the public goal  X  . It means that he is committed to this public goal, and, for example, he cannot say later that he desires the opposite.

If hundreds of agents have to communicate their goals, then the communication will become long and cumbersome. So, when an agent informs the others that he desires  X  ,the other agents can respond by a simple  X  X e too! X . Moreover, we do not have only speech acts implying that the speaking agent has a goal, but also speech acts implying that all agents have the same goal. Agent a says that everyone has the desire for  X  : desireall ( a , X ) The agents can respond to this speech act by saying  X  X ot me! X . If none of them answers in agents saying  X  X e too! X . The principle that by default all agents agree, is sometimes called  X  X ilence means consent X .

An optional parameter of these two speech acts is the priority of the individual goals. In this running example we assume that there are five priorities: essential, important, intermediate, useful, possible.
 Example 3 Suppose that three agents { a , b , c } are going to negotiate the attack strategy to kill the Sleeper. Moreover, we assume that there are various ways in which the individual the eyes, and so on. The three agents are negotiating for three kinds of game players with specific powers. For example, agent a is negotiating for all agents that have the power to hit the feet, throw stones, and have guns. In this example, each negotiator communicates goals they cannot see to by themselves. For example, agent a will communicate the desire to scare the Sleeper to death.

The negotiators begin by communicating their individual goals. Each player can type individual or shared goals by entering a text string into his computer. In the text box is also a list of goal priorities, which is default at intermediate. When someone types an individual goal, other agents can press the  X  X e too! X  button, When an agent enters a shared goal, other agents can press the  X  X ot me! X  button, or change the suggested priority of the goal. For example, when agent a enters his goal and agent b accepts it too, and agent c communicates a shared goal but agent d refuses it, then the system translates it into the following speech acts. desire ( b ,  X  X itting the feet X  ) desire ( b ,  X  X it him with stones X  , intermediate ) desire me too ( c ,  X  X it him with stones X  , important ) desire ( c ,  X  X hoot him in the eyes X  , possible ) desire ( a ,  X  X how him a picture of M. X  ) desireall ( a ,  X  X care him to death X  ) desire not me ( c ,  X  X care him to death X  ) ...

Each agent can indicate which goals it can see to. We use the new speech act power ( a , X ) , indicating that agent a communicates that he has the power to see to goal  X  . As before, the other agents can respond by  X  X e too! X  and there is a speech powerall which agents can respond to by saying  X  X ot me! X .
 Example 4 Consider the following three speech acts. power ( a ,  X  X itting the feet X  ) powerall ( a ,  X  X it him with stones X  ) power not me ( c ,  X  X it him with stones X  ) powerall ( c ,  X  X hoot him in the eyes X  ) ...
 The communicated goals and powers are represented in a single table for each player, as showninTable 1 . In case the agents are negotiating a norm for a large set of players, then not all individual names of the agents are given, but only aggregate information such as the number of players having the power to see to a goal. 6 Generic negotiation protocol A negotiation protocol is described by a set of sequences of negotiation actions which lead to either success or failure. In this paper, we only consider protocols in which the agents propose a so-called deal, and when an agent has made such a proposal, then the other agents can either accept or reject it (following an order  X   X  of the agents). Moreover, they can also end the negotiation process without any result.
 Definition 3 ( Negotiation Protocol ) A negotiation protocol is a tuple Ag , deals , actions , valid , finished , broken , ,where  X  the agents Ag , deals and actions are three disjoint sets, such that  X  valid , finished , broken are sets of finite sequences of actions .
 We now instantiate this generic protocol for negotiations in normative multi-agent systems. We assume that a sequence of actions (a history) is valid when each agent does an action respecting the order defined on agents. Then, after each proposal, the other agents have to accept or reject this proposal, again respecting the order, until they all accept it or one of them rejects it. When it is an agent X  X  turn to make a proposal, it can also end the negotiation by breaking it. The history is finished when all agents have accepted the last deal, and broken when the last agent has ended the negotiations.
 Definition 4 ( NMAS protocol ) Given a normative multi-agent system MAS , N , O , V extending a multi-agent system MAS = A , G , goals , power ,  X  , a negotiation protocol for NMAS is a tuple NP = A , deals , actions , valid , finished , broken , ,where:  X   X  A  X  A is a total order on A ,  X  a history h is a sequence of actions, and valid ( h ) holds if:
In theory we can add additional penalties when agents break the negotiation. However, In this respect norm negotiation differs from negotiation about obligation distribution [ 8 ], where it may be the interest of some agents to see to it that no agreement is reached. In such cases, sanctions must be added to the negotiation protocol to motivate the agents to reach an agreement.
 Example 5 Assume three agents and the following history. action 1 : pr opose ( a , d 1 ) action 2 : accept ( b , d 1 ) action 3 : r ej ect ( c , d 1 ) action 4 : pr opose ( b , d 2 ) action 5 : accept ( c , d 2 ) action 6 : accept ( a , d 2 )
We h ave v alid ( h ) , because the order of action respects , and we have accepted ( h ) , ( action 4 ).
 The open issue of the generic negotiation protocol is the set of deals which can be proposed. They depend on the kind of negotiation. In social goal negotiation the deals represent a social control system based on sanctions. This is illustrated in our running example in the following sections. 7 Social goal negotiation We characterize the allowed deals during goal negotiation as a set of goal which contains for each agent a goal it desires. Moreover, we add two restrictions. First, we only allow goals the agents have the power to achieve. Then, we have to consider the existing normative system, which may already contain the norms that look after the goals of the agents. We therefore restrict ourselves to new goals. Additional constraints may be added, for example excluding in some applications (e.g., one may delegate some tasks to a secretary even when one has the Definition 5 ( Deals in goal negotiation ) In the goal negotiation protocol, a deal d  X  deals is a set of goals satisfying the following restrictions: 1. d  X  power ( A ) 2. for all a  X  A there exists some g  X  d such that The following example illustrates social goal negotiation in the running example. Example 6 The agents proposing a deal select a couple of individual communicated goals from Table 1 by clicking on them. The system then send them to the other agents, which can either accept them, or reject them. When they reject a communicated goal, they can add a reason to it, which is communicated to the other agents.
 representation of Table 1 ,where Moreover, let NMAS = MAS , N , O , V be a normative multi-agent system with N g and g 2 have to be part of the social goal. Therefore, social goals (i.e., possible deals) are { g 2 , g 5 } and { g 2 , g 4 , g 5 } .

Finally, consider the negotiation. Assuming that agent a is first in the order ,hemay { g protocol no other proposals can be made.

The example illustrates that the negotiation does not determine the outcome, in the sense that there are multiple outcomes possible. Additional constraints may be added to the nego-tiation strategy to further delimit the set of possible outcomes. 8 Social norm negotiation We formalize the allowed deals during norm negotiation as obligations for each agent to see to some goals, such that all goals of the social goal are included. Again, to determine whether the obligations imply the social goal, we have to take the existing normative system into account. We assume that the normative system only creates obligations that can be fulfilled together with the already existing obligations.
 Definition 6 ( Fulfillable NMAS ) A normative multi-agent system MAS , N , O , V exten-ding a multi-agent system MAS = A , G , goals , power ,  X  can be fulfilled if there exists a G  X  power ( A ) such that all obligations are fulfilled  X  n  X  N , a  X  A O ( n , a )  X  G . Creating a norm entails adding obligations and violations for the norm.
 Definition 7 ( Add norm )Let NMAS be a normative multi-agent system MAS , N , O , V extending a multi-agent system MAS = A , G , goals , power ,  X  . Adding a norm n  X  N with a pair of functions o 1 , o 2 for obligation o 1 : A  X  2 G and for sanction o 2 : A  X  2 G leads to the new normative multi-agent system MAS , N  X  X  n } , O  X  o 1 ( n ), V  X  o 2 ( n ) .
Moreover, if every agent fulfills it obligation, then the social goal is achieved. Definition 8 ( Deals in norm negotiation ) In the norm negotiation protocol, a deal d  X  deals o : A  X  2 G satisfying the following conditions: 2. NMAS achieves the social goal,  X  a  X  A o 1 ( a ) = S . 3. If NMAS is fulfillable, then NMAS is too.
 The running example illustrates the norm negotiation protocol.
 solution here is that each agent sees to one of the goals.

Sanctions can be added in the obvious way. In the norm negotiation as defined thus far, the need for sanctions has not been formalized yet. For this need, we have to consider the acceptance of norms. 9 Norm acceptance An agent accepts a norm when it believes that the other agents will fulfill their obligations, and the obligation implies the goals the cycle started with. For the former we use the quasi-stability of the norm (e.g., if the norm is a convention, then we require that the norm is a Again we have to take the existing normative system into account, so we add the condition that all other norms are fulfilled. In general, it may mean that an agent does something which it does not like to do, but it fears the sanction more than this dislike. We use the trade-off among goals  X  .
 Definition 9 ( Stability ) A choice c of agent b  X  A in NMAS with new norm n is c  X  power ( b O ( c  X  ( b ) c with O ( n , b )  X  c .
 Finally, we have to test whether the new situation is better than the old one for all agents. For example, we may request that the outcome in both the original multi-agent system as in the new multi-agent system is a Nash equilibrium, and we demand that each Nash outcome in the new system is better than each Nash outcome in the original normative multi-agent acceptance in Definition 9 .
We ask that each agent communicates to the other agents why it accepts the norm. By acceptance. Agent a accepts the norm n for goal g : accept-norm ( a , n , g ) Norm acceptance is illustrated in our running example.
 Example 8 Let n stand for the norm accepted by the agents. Assume that every agent can can be communicated to the other agents using the following three speech acts. accept-norm ( a , n ,4) accept-norm ( b , n ,5) accept-norm ( c , n ,2) The resulting table is given in Table 4 . 10 Related work In this section, we discuss the existing three formalizations of the social delegation cycle. 10.1 Norms are a class of constraints deserving special analysis Meyer and Wieringa define normative systems as  X  X ystems in the behavior of which norms play a role and which need normative concepts in order to be described or specified X  [ 40 ,pre-face]. Alchourr X n and Bulygin [ 2 ] define a normative system inspired by Tarskian deductive systems:
When a deductive correlation is such that the first sentence of the ordered pair is a case and the second is a solution, it will be called normative. If among the deductive correlations of the set  X  there is at least one normative correlation, we shall say that the set  X  has normative consequences. A system of sentences which has some normative consequences will be called a normative system [ 2 , p. 55].

Jones and Carmo [ 31 ] introduced agents in the definition of a normative system by defining it as  X  X ets of agents whose interactions are norm-governed; the norms prescribe how the agents ideally should and should not behave. [  X  X  X  ] Importantly, the norms allow for the possibility that actual behavior may at times deviate from the ideal, i.e., that violations of obligations, we use the following definition.

A normative multi-agent system is a multi-agent system together with normative systems in which agents can decide whether to follow the explicitly represented norms, and the normative systems specify how and in which extent the agents can modify the norms [ 14 ]. Note that this definition makes no presumptions about the internal architecture of an agent or of the way norms find their expression in agent X  X  behavior. 10.1.1 Representation of norms Since norms are explicitly represented, according to our definition of a normative multi-agent system, the question should be raised how norms are represented. Norms can be interpreted occur. However, the representation of norms by domain-dependent constraints runs into the question what happens when norms are violated. Not all agents behave according to the norm, and the system has to deal with it. In other words, norms are not hard constraints, but soft constraints. For example, the system may sanction violations or reward good behavior. Thus, the normative system has to monitor the behavior of agents and enforce the sanctions. Also, when norms are represented as domain-dependent constraints, the question will be raised how to represent permissive norms, and how they relate to obligations. Whereas obligations and prohibitions can be represented as constraints, this does not seem to hold for permissions. For example, how to represent the permission to access a resource under an access control system? Finally, when norms are represented as domain-dependent constraints, the question can be raised how norms evolve.
 We therefore believe that norms should be represented as a domain independent theory. permissions, and more in particular violations and contrary-to-duty obligations, permissions from deontic logic can be used to represent and reason with norms in multi-agent systems. Deontic logic also offers representations of norms as rules or conditionals. However, there are several aspects of norms which are covered neither by constraints nor by deontic logic, Meyer and Wieringa explain why normative systems are intimately related with deontic logic.
Until recently in specifications of systems in computational environments the distinc-tion between normative behavior (as it should be ) and actual behavior (as it is )has been disregarded: mostly it is not possible to specify that some system behavior is non-normative (illegal) but nevertheless possible. Often illegal behavior is just ruled out by specification, although it is very important to be able to specify what should happen if such illegal but possible behaviors occurs! Deontic logic provides a means to do just this by using special modal operators that indicate the status of behavior: that is whether it is legal (normative) or not [ 40 , preface]. 10.1.2 Norms and agents research. On the one hand, they claimed that legal theory and deontic logic supply a theory of norm-governed interaction of autonomous agents while at the same time lacking a model that integrates the different social and normative concepts of this theory. On the other hand, they claim that three other problems are of interest in multi-agent systems research on norms: how agents can acquire norms, how agents can violate norms, and how an agent can be autonomous. Agent decision making in normative systems and the relation between desires and obligations has been studied in agent architecture [ 16 ], which thus explain how norms and obligations influence agent behavior. An important question in normative multi-agent systems is where norms come from. Norms are not necessarily created by legislators, but they can also be negotiated among agents, or they can emerge spontaneously, making the agents norm autonomous [ 53 ]. In electronic commerce research, for example, cognitive foundations of social norms and contracts are creations of norms in multi-agent systems: formal methods designed for protection against cooperation between agents for more effectiveness [ 42 ]. Moreover, agents like legislators playing a role in the normative system have to be regulated themselves by procedural norms [ 13 ], raising the question how these new kind of norms are related to the other kinds of norms.

When norms are created, the question can be raised how they are enforced. For example, when a contract is violated, the violator may have to pay a penalty. But then there has to be a monitoring and sanctioning system, for example police agents in an electronic institution. Such protocols or roles in a multi-agent system are part of the construction of social reality, and Searle [ 44 ] has argued that such social realities are constructed by constitutive norms. This raises the question how to represent such constitutive or counts-as norms, and how they are related to regulative norms like obligations and permissions [ 10 ]. 10.1.3 Norms and other concepts Not only the relation between norms and agents but also the relation between norms and other social and legal concepts must be studied. How do norms structure organizations? How do norms coordinate groups and societies? How about the contract frames in which contracts live? How about the relation between legal courts? though in some normative multi-agent systems there is only a single normative system, there can also be several of them, raising the question how normative systems interact. For example, in a virtual community of resource providers each provider may have its own normative system, which raises the question how one system can authorize access in another system, or how global policies can be defined to regulate these local policies [ 10 ]. 10.2 Social delegation cycle using artificial social systems The problem studied in artificial social systems is the design, emergence or more generally the creation of social laws. Shoham and Tennenholtz [ 45 ] introduced social laws in a set-ting without utilities, and they defined rational social laws as social laws that improve a social game variable [ 46 ]. We follow Tennenholtz X  presentation for stable social laws [ 50 ]. In Tennenholtz X  game-theoretic artificial social systems, the goals or desires of agents are represented by their utilities. A game or multi-agent encounter is a set of agents with for each agent a set of strategies and a utility function defined on each possible combination of strategies. The social goal is represented by a minimal value for the social game variable. Tennenholtz [ 50 ] used the maximin value as game variable. This represents safety level deci-sions, in the sense that the agent optimizes its worst outcome assuming the other agents may follow any of their possible behaviors.
 available to the agents. It is useful with respect to an efficiency parameter e if each agent an agent does not profit from violating the law, as long as the other agent conforms to the social law (i.e., selects strategies allowed by the law). Quasi-stable conventions correspond to Nash equilibria. The efficiency parameter can be seen as a social kind of utility aspiration or resource-bounded reasoning, and have led to the development of goals and planning in artificial intelligence; we therefore use a goal based ontology in this paper. The three steps as follows. Goal negotiation implies that the efficiency parameter is higher than the utility the agents expect without the norm, for example represented by the Nash equilibria of the game. Norm negotiation implies that the social law is useful (with respect to the efficiency parameter). The acceptance relation implies that the social law is quasi-stable. The game-theoretical model has several drawbacks as a basis of our communication model. Due to the uniform description of agents in the game-theoretic model, it is less clear how to distinguish among kinds of agents. For example, the unique utility aspiration level does not distinguish the powers of agents to negotiate a better deal for themselves than for the other agents. Moreover, the formalization of the social delegation cycle does neither give a clue on how the efficiency parameter is negotiated, nor on how the social law is negotiated. For example, the goals or desires of the agents as well as other mental attitudes may play a role in this negotiation. There is no sanction or control system in the model (adding a normative system to encode enforceable social laws to the artificial social system complicates the model social goals, and social laws) are formalized in three completely different ways. 10.3 Power and dependencies The second formalization of the social delegation cycle is the highly detailed model we have proposed within normative multi-agent systems [ 11 ]. The challenge to define social mecha-nisms, as we see it, is to go beyond the classical game theoretic model by introducing social and cognitive concepts and a negotiation model, but doing so in a minimal way. In the model proposed in this paper, we therefore keep goals and obligations abstract and we do not des-cribe them by first-order (or propositional) logic or their rule structure, we do not introduce decisions, actions, tasks, and so on. Similar concerns are also mentioned by Wooldridge and Dunne in their qualitative game theory [ 56 ].
 In [ 12 ] we introduce a norm negotiation model based on power and dependence structures. Power may affect norm negotiation in various ways, and we therefore propose to analyze the norm negotiation problem in terms of social concepts like power and dependence. Power has been identified as a central concept for modeling social phenomena in multi-agent systems by negotiation of the obligations with their control system. Roughly, the social goals are the benefits of the new norm for the agents, and the obligations are the costs of the new norm for the agents in the sense that agents risk being sanctioned. Moreover, in particular, when representatives of the agents negotiate the social goals and norms, the agents still have to accept the negotiated norms. The norm is accepted when the norm is quasi-stable in the the norm leads to achievement of the agents X  desires X  X .e., when the benefits outweigh the costs.

Our new model is based on a minimal extension of Tennenholtz X  game theoretic model of the social delegation cycle. We add a negotiation protocol, sanction and control, and besides acceptance also effectiveness. It is minimal in the sense that, compared to our earlier model [ 11 ] in normative multi-agent systems, we do not represent the rule structure of norms, we do not use decision variables, and so on. Also, as discussed in this paper, we do not add goal hierarchy, definitions, etc. The model therefore focuses on various uses of power: the power as ability to achieve goals and in negotiation. 11 Summary Massively, multi-player online games have now become among the most complex and sophis-become more and more connected with their offline lives such online spaces are shedding significant light on questions of liberty and responsibility, and on how an online existence creation is built in some of the games, such as for example A Tale in the Desert, but we envision in this paper a gaming environment in which the players can create arbitrary norms to coordinate their behaviors.

In this paper, we introduce an agent communication protocol and speech acts for norm on the social delegation cycle. The first problem is that the social delegation cycle refers to the individual goals of the agents, which are not accessible to other agents and which are not represented in the agent environment. This problem is solved by the first step of our communication protocol. 1. First, agents communicate their individual goals and powers. 2. Second, they propose social goals which can be accepted or rejected by other agents. 3. Third, they propose obligations and sanctions to achieve the social goal, which can again Consequently, the social delegation cycle does not refer to the individual private goals of the agents, but to the communicated public goals.

The second problem, to define norm acceptance, is solved by the final step of our norm negotiation protocol, in which each agent has to commit itself to the social norm by indicating which of its publicly communicated goals the norm achieves.

Finally, the solution of the third problem, that is, how to use the norm negotiation protocol in online multi-player games and social spaces like The Sims Online or Second Life, is to use an abstract power and dependence based representation of the social delegation cycle. Clearly the game players cannot detail their individual probabilities and utilities as required by Tennenholtz X  game-theoretic model, and they cannot detail the decision variables of the language to describe their goals, and detail their powers by selecting the relevant goals. This solution is illustrated by our running example of norm negotiation in multi-player online gaming, where the goals are lines of text entered in a table, powers are cross-tables, social goals are sets of individual goals, and norms are distributions of the goals over the agents.
An important subtask of the social delegation cycle is to break down goals in subgoals, using a goal hierarchy. However, to focus on the main social mechanism, in this paper we do not detail the sub-protocol for subgoaling. Therefore, we assume that the individual goals communicated by the agents are such that they can be seen to by other agents. The extension with a protocol for subgoaling is left for further research. References Author biographies
