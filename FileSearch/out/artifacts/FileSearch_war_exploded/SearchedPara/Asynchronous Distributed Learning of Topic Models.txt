 provides an immediate and practical motivation to develop l earning algorithms that are able take of artificial intelligence and cognitive science.
 ing via Gibbs sampling. The frameworks of LDA and HDP have rec ently become popular due to topic model in near real-time for tens of thousands of docume nts returned by a search-engine. heterogeneous machines with different processor speeds an d memory capacities can be used; (4) new processors and new data can be incorporated into the syst em at any time. LDA and HDP, based on local collapsed Gibbs sampling on each p rocessor. We assume an asyn-chronous  X  X ossip-based X  framework [6] which only allows pa irwise interactions between random While building towards an asynchronous algorithm for HDP, we also introduce a novel synchronous distributed inference algorithm for HDP, again based on col lapsed Gibbs sampling. We first review collapsed Gibbs sampling for LDA and HDP. Then we describe the details of our Before delving into the details of our distributed algorith ms, we first describe the LDA and HDP topic models. In LDA, each document j is modeled as a mixture over K topics, and each topic k is a multinomial distribution,  X  mixture over topics,  X  parameter  X  . In order to generate a new document,  X  from a Dirichlet distribution with parameter  X  . For each token i in that document, a topic assignment z specific word x is shown in Figure 1, and the generative process is below: can perform collapsed Gibbs sampling [7] by integrating out  X  assignments in the following manner: N The HDP mixture model is composed of a hierarchy of Dirichlet processes. HDP is similar to LDA mixture model. Let L be the number of mixture components, and  X  drawn from a Dirichlet distribution with parameter  X /L . The mixture for each document,  X  generated from a Dirichlet with parameter  X  X  from a base Dirichlet distribution with parameter  X  . As in LDA, z x Both  X  the only topics that are instantiated are those that are actu ally used. We consider the problem of learning an LDA model with K topics in a distributed fashion where w assigned topic for each token. N  X  p include the processor X  X  local counts. N p ), N p count matrix (derived from z p and w p ) which only contains the counts of data on the processor. Newman et al. [5] introduced a parallel version of LDA based o n collapsed Gibbs sampling (which counts N p Parallel-LDA can provide substantial memory and time savin gs. However, it is a fully synchronous gain the benefits of asynchronous computing, we introduce an asynchronous distributed version of LDA (Async-LDA) that follows a similar two-step process to t hat above. Each processor performs a local Gibbs sampling step followed by a step of communicati ng with another random processor. tribution, in a manner directly analogous to Equation 1, The combination of N  X  p (not including processor p  X  X  local counts), while N p Once the inference of z p is complete (and N p dated), the processor finds another finished processor and the case where memory and communication bandwidth are both limited. We also assume in the simplified gos-sip scheme that a processor can establish communication with every other processor  X  later in the paper we also discuss scenarios that relax these assumptions.
 In the communication step, let us consider the case where two processors, p and g have never met before. In this case, processors simply exchange their local N p local contribution to the global topic set), and processor p simply adds N g Consider the case where two processors meet again. The proce ssors should not simply swap and add their local counts again; rather, each processor should firs t remove from N  X  p meet from over-influencing each other. We assume in the gener al case that a processor does not Since the previous local counts of the other processor were a lready absorbed into N  X  p not retrievable, we must take a different approach. In Async -LDA, the processors exchange their N collection { N  X  p from which we pick N g to sampling from a multivariate hypergeometric distributi on.  X  N g N maximum entropy). Finally, we update N  X  p Pseudocode for Async-LDA is provided in the display box for A lgorithm 1. The assumption of  X  the cached N g connected, forwarding is necessary to propagate the counts across the network. Our approach can Inference for HDP can be performed in a distributed manner as well. Before discussing our asyn-chronous HDP algorithm, we first describe a synchronous para llel inference algorithm for HDP. Dirichlet Process (DP),  X  is the concentration parameter for the document level DP,  X  model for HDP is shown in Figure 1.
 sor maintains local  X  p nization step, the local word-topic counts N p N these parameters and the global count matrix are distribute d back to the processors. Algorithm 2 Parallel-HDP Motivated again by the advantages of local asynchronous com munication between processors, we propose an Async-HDP algorithm. It is very similar in spirit to Async-LDA, and so we focus on the since some probability mass is reserved for new topics: We resample the hyperparameters  X  p ,  X  p In Async-HDP, a processor can add new topics to its collectio n during the inference step. Thus, when two processors communicate, the number of topics on eac h processor might be different. One algorithm. However, performing this topic matching step im poses a computational penalty as the number of topics increases. In our experiments for Async-LD A, Parallel-HDP, and Async-HDP, we [5] also observed this same behavior occurring in Parallel-LDA.
 During the communication step, the counts N p and merged. Async-HDP removes a processor X  X  previous influe nce through the same MH technique used in Async-LDA. Pseudocode for Async-HDP is provided in t he display box for Algorithm 3. from the New York Times (nytimes.com); and PUBMED, a large co llection of PubMed abstracts hardware is used to measure speedup on larger data sets (NYT, PUBMED). Our simulation features using perplexity, a widely-used metric in the topic modelin g community.
 tiated average per-word log-likelihood. For each of our exp eriments, we perform S = 5 different Parallel-HDP, perplexity is calculated in the same way as in standard HDP: log p ( x test ) = X After the model is run on the training data,  X   X  s fixed. Perplexity is evaluated on the second half of each docu ment in the test set, given  X   X  s The perplexity calculation for Async-LDA and Async-HDP use s the same formula. Since each pro-each processor X  X  local model. In our experiments, we report the average perplexity among proces-sors, and we show error bars denoting the minimum and maximum perplexity among all processors. topic models learned on each processor are equally accurate .
 5.1 Async-LDA perplexity and speedup results Figures 2(a,b) show the perplexities for Async-LDA on KOS an d NIPS data sets for varying numbers (results not shown) which give essentially the same results across different test/train splits. two documents on each processor), and we found that performa nce was virtually unchanged (figure 2(c)). As a baseline we ran an experiment where processors ne ver communicate. As the number of processors P was increased from 10 to 1500 the corresponding perplexitie s increased from 2600 to 0.5, and one can see that Async-LDA converges much more quick ly than LDA. Figure 3(c) shows actual speedup results for Async-LDA on NYT and PUBMED, and t he speedups are competitive since communication overhead is dwarfed by the sampling tim e.
 In Figure 3(a), we also show the performance of a baseline asy nchronous averaging scheme, where global counts are averaged together: N  X  p of the setting for d .
 The rate of convergence for Async-LDA P =100 can be dramatically improved by letting each pro-cessor maintain a cache of previous N g improvement made by letting each processor cache the five mos t recently seen N g time. In this manner, one can elegantly make a tradeoff betwe en time and memory. 5.2 Parallel-HDP and Async-HDP results the model generated by Parallel-HDP has nearly the same pred ictive power as standard HDP. Figure 4(b) shows that Parallel-HDP converges at essentially the s ame rate as standard HDP on the KOS power of the model. The number of topics does stabilize after thousands of iterations. Topics are generated at a slightly faster rate for Async-HDP than for Parallel-HDP because Async-HDP take a less aggressive approach on pruning small topics, since processors need to be careful when pruning topics locally. Like Parallel-HDP, Async-HDP converges rapidly to a good solution. 5.3 Extended experiments for realistic scenarios our framework, if new data arrives, we simply assign the new d ata to a new processor, and then let that new processor enter the  X  X orld X  of processors with w hich it can begin to communicate. Our asynchronous approach requires no global initializati on or global synchronization step. We do assume a fixed global vocabulary, but one can imagine schem es which allow the vocabulary to grow as well. We performed an experiment for Async-LDA where we introduced 10 new processors is known, and every 100 iterations, an additional 10% of the d ata is added to the system through After 1000 iterations, the perplexity of Async-LDA has conv erged to the standard LDA perplexity. In the experiments previously described, documents were ra ndomly distributed across processors. Async-LDA X  X  behavior on a non-random distribution of docum ents over processors. After running LDA (K=20) on NIPS, we used the inferred mixtures  X  sets of documents corresponding to the 20 topics. We assigne d 2 sets of documents to each of 10 shows that Async-LDA performs just as well on this non-rando m distribution of documents. divided into 30 blocks of 100 documents and these blocks were assigned to 10 processors according take k units of time to complete one sampling sweep. Figure 5(c) sho ws that this load imbalance does not significantly affect the final perplexity achieved. More generally, the time T There exist pathological cases where the graph may be discon nected due to phase-locking (e.g. 5 has a stochastic component (e.g. due to network delays), a re asonable assumption in practice. network topologies. After running Async-LDA on both a 10x10 fixed grid network and a 100 node chain network on KOS K =16, we have verified that Async-LDA achieves the same perple xity as LDA as long as caching and forwarding of cached counts occurs between processors. and Newman et al. [5], who each propose parallel algorithms f or the collapsed sampler for LDA. et al. [8] examine asynchronous EM algorithms for LDA. The pr imary distinctions between our work and other work on distributed LDA based on Gibbs samplin g are that (a) our algorithms use purely asynchronous communication rather than a global syn chronous scheme, and (b) we have also extended these ideas (synchronous and asynchronous) to HDP . More generally, exact parallel Gibbs EM, a gossip algorithm for performing EM on Gaussian mixture learning [10]. Although processors perform local Gibbs sampling based on i nexact global counts, our algorithms We have proposed a new set of algorithms for distributed lear ning of LDA and HDP models. Our Acknowledgments This material is based upon work supported in part by NSF unde r Award IIS-0083489 (PS, AA), IIS-0447903 and IIS-0535278 (MW), and an NSF graduate fellowship (AA). MW was also supported by ONR under Grant 00014-06-1-073, and PS was also supported by a Google Research Award. References
