 The border concept has been introduced by Mannila and Toivonen in their seminal paper [20]. This concept finds many applications, e.g maximal frequent itemsets, minimal functional dependencies, emerging patterns between consec-utive database instances and materialized view selection. For large transactions and relational databases defined on n items or attributes, the running time of any border com-putations are mainly dominated by the time T (for standard sequential algorithms) required to test the interestingness , in general the frequencies, of sets of candidates.

In this paper we propose a general parallel algorithm for computing borders whatever the application is. We prove the efficiency of our algorithm by showing that: (i) it gener-ates exactly the same number of candidates as the standard sequential algorithm and, (ii) if the interestingness test time of a candidate is bounded by  X  then for a multi-processor shared memory machine with p cores, we prove that the to-tal interestingness time T p &lt; T/p + 2 X  n . We implemented our algorithm in the maximal frequent itemset (MFI) mining setting and our experiments confirm our theoretical perfor-mance guarantee.
 H.2.8 [ Database Management ]: Database Applications-Datamining Algorithm, Performance.
Let us first recall the definition of borders as it has been introduced in [20]. Let O be a set of objects, r be a database and q be a predicate evaluating the interestingness of O  X  X  This work has been partially supported by the project ANR ALADDIN from the French Ministery of Research.
 over r . In other words, q ( O, r ) = True iff O is interesting. On the other hand, let v be a partial order between the subsets of O . The border of O with respect to r , q and v is the set of subsets O  X  O such that (i) q ( O, r ) = True and (ii) there is no O 0 6 = O such that O v O 0 and q ( O True . We illustrate this notion by the following examples:
Other recent applications of borders can be found e.g. in [22] where it is used to characterize the emerging parts of a datacube between two consecutive states, in [15] for dat-acube partial materialization and in [21] for extracting the most interesting attributes w.r.t. a query log.

However, computing borders is a very time consuming task. Indeed, [13] shows that it is NP-Hard. Therefore, either we rely on heuristics to optimize the computation (for example, the way we traverse the search space), try to approximate the result (for example, by using proba-bility arguments) or exploit parallelism (data and/or task parallelism). [20] proposed a general level-wise algorithm for computing the borders when q is anti-monotone . Re-call that q is anti-monotone iff whenever q ( O, r ) = False then q ( O 0 , r ) = False for each O 0 w O . In the three exam-ples above, one may easily check that q is anti-monotone. The main parts of any border computations algorithms are the candidates management and the interestingness tests. Whatever is the underlying data structure (tables, FP-trees, . . . ), the interestingness test time for one candidate is be-tween 1 (atomic test) and O ( mn ) (scan of the whole table of m entries containing at most n values). It turns out that for large datasets, in practice, the interestingness tests is the most consuming task. The algorithm of [20], akin to A priori [2], turns to be inefficient in practice in that it tests much more candidates than the algorithms following a depth first strategy (DFS). The reason is that DFS exploits the anti-monotone property up and downward to prune the candidates. In order to parallelize DFS algorithms, the im-mediate solution would be to partition the data and each time we have to check the interestingness of a candidate, we test it in parallel in every part then we combine the results. The problem here is that we have no guarantee that doing so, the time required is not more than that of a sequential algorithm, e.g. for MFI mining, prefix trees [14] are often used to summarize the data and it may happen that each tree corresponding to a part has the same size as the global one. Therefore, the performance of the parallel version is much worse than the sequential one. To sum up, no exist-ing parallel computations of borders provides a theoretical guarantee to run faster than sequential executions.
Our starting point is a new proposition of a parallelization of DFS algorithms based on an on-line scheduling of inde-pendent sets of candidates. To our knowledge, it is the first parallel algorithm that provides some guarantee of perfor-mance with respect to the running time. More precisely, we present a partitioning of the candidates into at most 2 n  X  1 ordered equivalence classes then we provide an algorithm that picks candidates from one class at each iteration and evaluates the interestingness of them in parallel. The perfor-mance of the algorithm is proven by showing that during its execution, it generates exactly the same candidates as those considered by the baseline DFS algorithm (described in Sec-tion 3.1). Let T be the interestingness test time required by DFS . Let  X  be the maximal time needed for testing one candidate. Then for or a multi-processor shared memory machine of p cores, we prove that the total interestingness time T p &lt; T/p + 2 X  n . Since 2 X  n is in general much less than T/p , the speed up of our algorithm is in practice almost perfect.

To make our results more concrete and without loss of generality, in the rest of the paper we will concentrate on the MFI mining problem. This makes the comparison with related works easier.
In the next section we recall some related work on MFI extraction. After giving some notations, we describe more precisely the DFS algorithm in section 3.1. Then, we give our algorithm MineWithRounds in section 4 and prove some of its properties. We discuss data distribution in sec-tion 5.1 and we conclude this paper by section 6 where we show some of the experiments we conducted on real and synthetic data sets. Future work is described in section 7.
The algorithms for extracting MFI X  X  proposed so far can be divided into three categories: DFS algorithms (e.g Mafia [4], GenMax [11], DepthProject [1], FPmax* [12]and PADS [28]), BFS (e.g Pincer search [18] and MaxMiner [3]) and those based on dualization (e.g Dualize and Advance [13, 26]). Even if the MFI extraction problem is not recent, it continues to motivate the suggestion of new algorithms. After a flourishing number of such algorithms, the commu-nity of researchers decided to organize the FIMI workshops (2003 and 2004) especially devoted to compare these algo-rithms and their implementations. Indeed, the analytical comparison of all these proposals turned to be quite diffi-cult. The experiments have confirmed this difficulty since, for example, depending on data distribution and minimal support thresholds, almost each algorithm turned to behave better than the others under some circumstances. [9] pro-posed data classification depending on MFI and minimal infrequent itemsets distributions (also called the negative border) as a classification criterion of data sets and hence ex-plained some of the performance results obtained with state of the art algorithms.

To the best of our knowledge, two parallel algorithms [5, 7] have been proposed so far. [5] partitions the data and parallelizes the support computation of each candidate by using MaxMiner [3]. Due to its BFS strategy, MaxMiner does compute the support of more candidates than DFS al-gorithms. [7] guesses a set of MFI X  X  candidates by consider-ing all branches of the prefix tree summarizing the data. If a branch is frequent then it is an MFI otherwise it is inter-sected with other non frequent candidates to generate new ones. Due to the usage of the tree structure, this technique is hardly adaptable to other contexts, e.g. FD mining. The following is a summary of the limitations of each category:
DFS and BFS have the same worst case time complexity, even if in practice DFS behaves better. To illustrate this, let I = { A,B,C,D,E } be a set of items. The search space may be represented by the lexicographic tree of I which is de-picted in Figure 1. Suppose that the border is { ABCD,E } . Figure 1: The lexicographic tree of { A,B,C,D,E } DFS will compute the support of all supersets of E . Hence, for a border of size 2, DFS may compute O (2 n  X  1 ) supports where n is the number of items. For the same case, BFS does not test all supersets of E but does test all subsets of ABCD . Thus both algorithms make the same number of tests. Suppose now that the border is { ABCDE } . DFS will climb the left most branch until ABCDE . Hence, it com-putes n supports while BFS computes the support of all subsets of ABCDE thus 2 n supports. Note however that from a parallel programming point of view, since BFS is level wise, at each level it may compute the supports of all candidates in parallel. Therefore, from a theoretical point of view, if we have p processors where p is the maximal num-ber of candidates we may generate in a level i.e., n n/ 2 algorithms behave the same. So, from the number of data scans point of view, both DFS and BFS are similar.

The advantage of DFS over BFS algorithms can be eas-ily explained: the algorithms for extracting MFI X  X  try to exploit the anti-monotonicity in order to prune the search space in two ways:(i) Upward pruning: eliminating those itemsets which are supersets of infrequent itemsets and (ii) Downward pruning: no need to compute the frequency of an itemset whenever one of its supersets is already found frequent. Since BFS cannot exploit downward pruning, it generates more candidates than DFS. We first define a total order among the items.

Definition 1 (Item Rank). Let / be a total order on the items appearing in the transaction data base. If I is an item, then Rank ( I ) = r iff |{ J | J / I }| = r  X  1 . Example 1. Let I = { A,B,C,D } be the set of items. Suppose that / is the alphabetical order. Then Rank ( A ) = 1 ,Rank ( B ) = 2 ,...,Rank ( D ) = 4 .

Without loss of generality, in the rest of the paper the order / is equivalent to the alphabetical order. We also will consider only ordered itemsets i.e., if I = I 1 I 2 ...I k-itemset then j &lt; `  X  I j / I ` . For example, ACD is an ordered itemset while DAC is not. As usual, from the order / we can define the lexicographic order between itemsets as follows: Let I = I 1 I 2 ...I m and J = J 1 J 2 ...J ` two ordered itemsets. Then I  X  J iff there exists p such that I k = J for k &lt; p and I p / J p . For example ABD  X  B . Moreover, we say that I is an ancestor of J iff I  X  J and I is a parent of J iff I is a J 0 s ancestor and | I | = | J | + 1. For example, ABC is an ancestor of B while AB is a parent of B . With respect to an itemset I and to the order  X  we distinguish two kinds of ancestors (resp. parents): those that precede I , called left ancestors , and those that follow it, called right ancestors . For example, AB and BC are both parents of B . Since AB  X  B  X  BC , then AB is a left parent of B while BC is a right parent of B . We say that a set of itemsets S covers an itemset I iff S contains an ancestor of I . We define the position of an itemset I , denoted Pos ( I ), as the rank of I with respect to the order  X  . More precisely, Definition 2. Let (2 I ,  X  ) be the ordered set of itemsets. Pos ( I ) = |{ J | J  X  I }| .

For example, if I = { I 1 ,...,I n } and I 1 / I 2 /  X  X  X  / I then Pos ( { I 1 } ) = 1 ( I 1 follows  X  ), Pos ( { I n } ) = 2 Pos ( { I 1 ,I 2 } ) = 2. From these definitions, the structure of the lexicographic tree should be clear: T is the lexicographic tree of a set of items I iff the root of T =  X  (or apex) and for each itemset I  X  X  all the right ancestors of I belong to the subtree of T rooted at I .

Finally, let I = I 1 ...I m be an ordered itemset. The successor of I , denoted Successor ( I ), is the itemset J = I ...I m I k such that rank ( I k ) = rank ( I m ) + 1. Clearly, not all itemsets have a successor. For example, if I = { A,B,C,D,E } then Successor ( AC ) = ACD while AE does not have a successor (leaves of the lexicographic tree). We start with a simple algorithm for extracting MFI X  X . Despite its simplicity, this algorithm provides some nice prop-erties. It computes less supports than any BFS algorithm. Moreover, we claim that any candidate and borders update can be performed in constant time. This DFS algorithm tra-verses the so called lexicographic tree. We assume that the itemsets are coded by a binary vector V ( n ) where V [ j ] = 1 means that item I j is present and V [ j ] = 0 otherwise. We note the set of MFI X  X  as B where B stands for Border. The parameters n and  X  are respectively the total number of items and the support threshold. The integer i is a position index in the vector V . The algorithm is called by initializing i to 1, B as empty set and all positions of V as 0.
There are two critical operations in this algorithm: the covering test (line 3) and the support test (line 7). The cov-ering test may require the comparison of V to all elements of B . One heuristic has been proposed to optimize this step in [11] by maintaining a local subset of B where the test is performed. Furthermore, most DFS like algorithms use some optimizations in order to reduce the number of cover-ing tests, e.g. candidates reordering: the parents of an item-set are reordered by increasing support 1 i.e., the order / is changed dynamically [11, 4] or exploit previously obtained
Note that this makes these algorithms not pure DFS algo-rithms. Procedure DFS(integer i) Input : integer i
Output : The MFI set B 1 if i  X  n then 2 V [ i ]  X  1; 3 if B covers V then 4 DFS ( i + 1); 5 else 6 if support ( V )  X  s then 7 Add V to B ; 8 Remove from B the subsets of V ; 9 DFS ( i + 1); 10 V [ i ]  X  0; 11 DFS ( i + 1); MFI X  X  in order to prune some branches [28]. For support computation, data structures like FP-trees and FP-arrays [12], diffsets [11] and bitmaps/bitvectors [4] have been pro-posed. Finally, look ahead heuristic first proposed in [3] has been adopted by most algorithms.

In the next section, we introduce a new algorithm which traverses the search tree in such a way that the advantages of both depth and breadth strategies are kept i.e., efficiently mining of long patterns and  X  X mall X  number of transaction data base scans.
As we have seen in the previous section, DFS processes sequentially the set of itemsets following the lexicographic order  X  . From time to time, it makes jumps that corre-spond to upward pruning. In this section we propose a new algorithm which mimics DFS and whose rationale consists in triggering the processing of an itemset as soon as possible i.e., whenever we respect the  X  order, we will obtain no new information about the status of this itemset. More precisely, suppose that I has just been processed and let J be an item-set such that I  X  J . We want to start the processing of J whenever none of the itemsets K such that I  X  K  X  J is able to provide new information about I .

For example, consider the itemsets AB and B from figure 1. Once AB is processed, we can start the processing of B . Indeed, either AB is frequent and in this case B is covered or AB is not frequent and in this case none of the itemsets whose position is between AB and B i.e, [ ABC ...AE ], will provide a new insight about B . Figure 2 illustrates this. The shaded region contains the itemsets whose processing is useless for gaining information about itemset B once AB is processed.

The principle of the proposed algorithm consists in pro-cessing several itemsets in parallel. Roughly speaking, we partition the set of itemsets into rounds R 1 ,..., R that an itemset is processed during the i th round if it belongs to R i . Let us now formalize our partitioning procedure.
Definition 3 (Depth First Partitions). Let ( R ) i  X  1 be a partition of 2 I . R is a Depth First Partition (DFP) of 2
I with respect to  X  if and only if for each itemset I ,
Processing B in this situation means just discarding it as a candidate.
 Figure 2: Useless itemsets for B once AB is pro-cessed. 1. I  X  R k and I has a successor  X  Successor ( I )  X  R ` 2. I  X  X  k  X  X  X  left parent I 0 of I , I 0  X  X  l where l &lt; k . To simplify the notation, we will talk about the DFP of I instead of 2 I . Intuitively, the two conditions above simply say that (1) the successor relationship must be satisfied by a DFP (an itemset must be processed before its ancestor) and (2) the left parents of I must satisfy the  X  relationship (they must be processed before I ). The obvious DFP of I is the one with 2 n  X  1 parts and I  X  R j  X  Pos ( I ) = i . In order to maximize parallelism while respecting the DFS, we want the smallest partition in terms of the number of parts. We say that a DFP R is smaller than R 0 iff |R| &lt; |R 0 |R| denotes the number of parts in R .

Example 2. Let I = { A,B,C } . One can easily check that R = {{ A } , { AB } , { B } , { ABC } , { AC , BC } , { C }} and R 0 = {{ A } , { AB } , { B } , { ABC } , { AC } , { BC } , { C }} are both DFP X  X . R is smaller than R 0 .

The following theorem shows that there exists a unique smallest DFP.

Theorem 1. Let  X  be the set of DFP X  X  we can define over ( I ,  X  ) . Let R X   X  s.t R has the least number of parts, then there is no other R 0  X   X  such that |R| = |R 0 | and R6 = R 0 .

Let us denote by b R the smallest DFP of ( I ,  X  ). Now we give a constructive way to obtain b R .

Theorem 2. Let b R = {R 1 ,..., R m } and I = I 1 ...I k be an ordered k-itemset such that rank ( I k ) = r . Then I  X  R i  X  2 r  X  k = i .

From here on, Round ( I ) denotes the round where I be-longs. As a consequence of the above theorem, we have the following properties.

Corollary 1. Let b R be the smallest DFP of I where |I| = n . Then  X  the number of rounds | b R| is 2 n  X  1 .  X  Let J 1 ,...,J k be the right parents of I and H 1 ,...,H its left parents. Then  X  Round ( H i ) = Round ( I )  X  1 for 1  X  i  X  ` and  X  Round ( J i ) = Round ( I ) + (2 i  X  1) for 1  X  i  X  k Example 3. Let I = { A,B,C,D,E } be the set of items. Then, the round of each itemset is depicted in Figure 3. We now describe the MineWithRounds which uses the partitioning defined before. Intuitively, it uses a loop which considers at each iteration the itemsets of the corresponding round. We use the following data structures: C is an array of sets of itemsets sets. C [ i ] is the set of candidates to be processed during round i and B is the set of MFI X  X  found so far. The right children of ABC are AC and BC (just not consider its prefix). The right sibling of ABC is ABD (just replace the last item, here C , by its successor, here D ). Note that if Round ( I ) = i and J is the right sibling of I then Round ( J ) = i + 2. Intuitively, the algorithm proceeds
Algorithm 1: MineWithRound for mining MFI X  X  1 C [1]  X  X  I 1 } ; 2 k  X  1; 3 while k  X  2 n  X  1 and C [ k ] 6 =  X  do 4 foreach I  X  X  [ k ] do 5 // Loop executed in parallel 6 if support ( I )  X   X  then 7 Add I to B ; 8 Remove the left child I 0 of I from B ; 9 Add the right parent I 00 of I to C [ k + 1]; 10 else 11 Add the right children of I to Children ; 12 Add the right sibling of I to C [ k + 2]; 13 foreach I  X  X  [ k + 1] do 14 // Loop executed in parallel 15 if I is covered by B then 16 Remove I from C [ k + 1]; 17 foreach I  X  Children do 18 // Loop executed in parallel 19 if I is not covered by C [ k + 1]  X  X  then 20 Add I to C [ k + 1]; 21 k  X  k + 1; as follows: If a candidate I  X  C [ k ] is found frequent, then its parent is a candidate for the next iteration k + 1. If I is infrequent then (i) its right children are potential candidates for the next iteration k + 1 and (ii) its sibling is a potential candidate for iteration k + 2. To explain the covering tests, suppose AC and BC are candidates tested during the same round. Suppose AC is frequent but not BC . Thus AC generates its parent ACD as a future candidate and BC generates both C (its right child) and BD (its sibling). Note that since C is covered by ACD , there is no need to consider it as a future candidate.

In lines 15 and 19, we make a covering test with respect to the whole current B . We optimized this test as follows: we considered a prefix tree structure with a header table to manage the border 3 . Each new discovered MFI is a branch that has to be inserted in the tree. Doing so, we do not need to remove the left child of a new discovered MFI (line 8 is actually needless); we just use it as a prefix of the new branch, e.g. if ACE is just discovered, then there is certainly a branch in the tree whose prefix is AC , then inserting ACE by using this branch will  X  X emove X  the child AC . Now let H be the header table and I = I j 1 ...I jm be a potential candidate for which we want to make the coverage test. It suffices to traverse the nodes of the tree corresponding to I m starting from the last inserted one (newest) back to the older ones. Each time, I is compared to the path from the current node to the tree root. This traversal is stopped as soon as an inclusion is found ( I is covered) or a node whose depth is less than the size of I ( I is not covered). Figure 4 illustrates this. The underlined part represents the depth of each node. The nodes are sorted from right to left in the descending order of their depth. Fortunately, this list of nodes does not need to be sorted each time a new node has to be inserted. Indeed, we have the following lemma.
Lemma 1. Let I j be an item and let L ( I j ) be the list of nodes related to I j in the FP-tree serving for storing the border. Assume that d is the maximal depth of the nodes in L ( I j ) at iteration i . Now suppose that at iteration i + 1 we add a new element I 0 to the border and that I 0 contains I If the insertion of I 0 into the tree requires the creation of a new node  X  for I j then Depth (  X  )  X  d .
 Thus, it suffices to consider insertions of new nodes in the front of the list to keep it sorted in depth descending order making coverage tests faster.

In our implementation, the Foreach loops in lines 4, 13 and 17 are executed in a parallel fashion. The first one essentially tests the support, the second checks the coverage of the sibling candidates that have been generated during iteration k  X  1 and the last one tests the coverage of the
We do not recall this structure due to space limitation. The interested reader can find it in [14]. children that have been generated during iteration k . It is worthwhile to note that there are barriers between these loops, i.e. the second loop cannot start before the first has finished.

The following theorem, which is our main result, compares the performance of the above algorithm with DFS .

Theorem 3. DFS and MineWithRounds perform ex-actly the same number of support computations.

Proof. (Sketch) It suffices to note that DFS does com-pute the support of an itemset I if and only if (i) all left chil-dren of I are frequent and (ii) none of the left parents of I is frequent. These two conditions are verified by MineWith-Rounds .

As a consequence, we can state the theoretical speed up with respect to the number of available processing units.
Corollary 2. Let T and T p be respectively the compu-tations times of DFS and MinWithRounds when p pro-cessors are available. Then Where n is the total number of items and  X  is the maximal time needed for one support computation.

Proof. First, suppose that the time needed for comput-ing the interestingness of each candidate is the same and let us denote it by t . Let r i be the number of candidates tested during round i and r = P i r i . Clearly, T = t  X  r . When p processors are available, the computation time of iteration i is  X  i = d r i p e X  t . Hence, T p = P i  X  i  X  ( r/p + 2 n  X  1)  X  t = T/p + (2 n  X  1) t .

In the more realistic setting, the required testing time of a candidate interestingness is not always the same. Hence, we can bound the formula above by replacing t by the maximal time  X  and we obtain T p  X  T/p + (2 n  X  1) X .
 Note that, since in general the number of candidates r is much larger than n , (2 n  X  1)  X   X  can be neglected with re-gard to T/p . Hence, the speed up of MineWithRounds is almost perfect. The worst case is when r i mod p = 1, e.g. suppose p = 16 and r i = 33. Suppose that the 33 rd takes 2 sec and the others take 1 sec . Then, the sequential algorithm requires 34 sec . With p cores, the first 32 can-didates are processed within 2 sec while the last candidate requires 2 sec by itself so a total of 4 sec . Hence, the speed up is 34 / 4 = 8 . 5. Note that if we had 47 candidates and it is the 47 th one which requires 2 sec then the parallel total time is again 4 sec and the speed up is this time equal to 48 / 4 = 12.

The following example illustrates the MFI X  X  computation with our algorithm.

Example 4. Let I = { A,B,C,D,E } and suppose that the maximal frequent itemsets are { ABDE,BC } . Figure 5 shows candidates generation. The arrows show candidates generation. For example, A is found frequent in round 1, therefore its parent AB is generated as a candidate. ABC is found infrequent during round 3, so its sibling ABD is gen-erated together with its right children AC and BC . Dashed arrows show candidates that have been generated and imme-diately removed because they turned to be covered. For exam-ple, in round 4, AC is found infrequent so its right children C is generated but this one is covered by BC . The same remark holds for the sibling AD of AC which is first gen-erated for round 6 then removed because meanwhile (during round 5) ABD is found frequent. Note that in this small ex-ample just two iterations contain more than one candidate, namely iterations 4 and 5. Thus, with 2 processors just there rounds will benefit from parallelism. We will come back to this remark in Section 6 to explain the obtained speed ups. Figure 5: Rounds assignment and MineWithRounds MFI X  X  computation
In this section we discuss some extensions one can envision for our algorithm. The first one considers data distribution and the second concerns the application of the algorithm in the functional dependency mining setting.
An obvious implementation of our algorithm when data is distributed is to consider one distinguished node as the master and the others are slaves . The master is responsi-ble of the candidates and the border management while the slaves compute the interestingness on their respective local data. At each iteration, (1) the master sends all candidates to all slaves, (2) each slave computes the local support of all candidates, (3) each slave sends its results to the master, (4) the master combines the partial results to find the global interesting candidates, (5) it updates the border and (6) it generates new candidates In the MFI setting, the communication complexity is the total size of messages. Let r i be the the number of candi-dates processed during iteration i and p be the number of slaves. At each iteration i the master sends a message of size r i to the slaves and each slave notifies the master with a message of length r i . Thus the total communication cost is e.g., instead of computing the support of each candidate by every slave, we let the later to make this computation only if the already computed support is not sufficient to make the candidate frequent. This will reduce the overall com-putation time. As one can see, our proposal of candidates partitioning is orthogonal to the way data is distributed. In fact, it leads itself easily to Map-Reduce setting.
The adaptation of our algorithm to the case of MFD is straightforward. Given a relational schema R ( A 1 ,...,A and an instance r of R we want the dependencies of the form X  X  A i satisfied by r such that X is minimal, i.e., there is no Y  X  X such that r | = Y  X  A i . Let us first introduce some notations. Let A = { A 1 ,...,A n } and A i = A\{ A We also note  X  X i = A i \ X , i.e. the complement of X in A . It algorithm can be rewritten as shown in Algorithm 2. The reader may notice the very small modifications of Algorithm 1. Clearly, the result of this algorithm satisfies the following: X  X  B [ i ]  X  r | =  X  X i  X  A i and this dependency is minimal.
Algorithm 2: Minimal FD X  X  1 foreach A i  X  X  do 2 C [1]  X  X  X  ; 3 k  X  1; 4 while k  X  2 n  X  1 and C [ k ] 6 =  X  do 5 foreach X  X  X  [ k ] do 6 // Loop executed in parallel 7 if r | =  X  X i  X  A i then 8 Add X to B [ i ]; 9 Remove the left child X 0 of X from B [ i ]; 10 Add the right parent X 00 of X to C [ k + 1]; 11 else 12 Add the right children of X to Children ; 13 Add the right sibling of X to C [ k + 2]; 14 foreach X  X  X  [ k + 1] do 15 // Loop executed in parallel 16 if X is covered by B [ i ] then 17 Remove X from C [ k + 1]; 18 foreach X  X  Children do 19 // Loop executed in parallel 20 if X is not covered by C [ k + 1]  X  X  [ i ] then 21 Add X to C [ k + 1]; 22 k  X  k + 1;
We implemented our algorithm using C++ and the STL library together with OpenMP [24] a multi-shared program-ming API. OpenMP makes it very easy to exploit new multi-core architecture by just adding compilation directives. Our tests were conducted on a machine equipped with two quad-core Intel Xeon X5570 2.93GHz processors running under Redhat Linux enterprise release version 5.4. Thanks to their multi-threading ability, these processors can execute con-currently two threads per core. Therefore, we were able to launch up to 16 threads. The machine has 48G RAM while the caches are respectively L1=32K, L2=256K and L3=8G. We checked the correctness of our implementation by com-paring its results with those presented by Flouvat 4 .
We present the results we obtained with six well known datasets: Chess, Mushroom, T10I4D100K, T40I10D100K, Kosarak and Webdocs. Their respective characteristics are described in Table 1. With each dataset, we varied the min-pages.univ-nc.nc/  X  flouvat/rprojects.html T10I4D100K 10 5 10 3 10 T40I10D100K 10 5 10 3 40 imal support threshold  X  and with every such value, we var-ied the number of parallel threads executed for every Fore-ach loop in the algorithm (the same number of threads for the three loops). This number varies from 1 to 16 as a power of 2. In order to gain load balancing, we used the dynamic scheduling of openMP. This option means that if for exam-ple we have to compute in parallel the support of ` itemsets by using m threads, then the first m itemsets are assigned randomly to the m threads and as soon as one thread ter-minates its computation, the system will assign to it a new task, i.e., a new itemset for which the support computation has to be done.
For each data set, we measured the time devoted to each of the three loops present in our algorithm (cf. Algorithm 1). The first loop essentially computes the support of a set of candidates while the two remaining loops make covering tests: One of them (line 17) tests the coverage of the candi-dates that have been generated as children of non frequent itemsets and the other (line 13) tests the candidates that have been generated as siblings. In our main theoretical re-sult (Corollary 2, we claimed that when the time devoted to support computation or more generally to interestingness test is the dominating time, the speedup of our algorithm is almost perfect. The experiments not only confirm this result but also show that when this test is not very time consuming we still get interesting speed ups.

Figure 6 shows some of the obtained results. For each dataset, we consider two support thresholds and for each, there is one subfigure showing the time repartition over the three loops as well as the time of the sequential part of the algorithm. The second subfigure shows the speedup of each loop as well as the total speedup. The X axis of each figure represents the minimal support value concatenated to the number of threads. For example, in subfigure 6(a), 140K 16 means that the minimal threshold is 140K and the number of threads is 16.
Subfigures in the first and third columns of Figure 6 show that time repartition is quite different from one data set to another. In general, the larger is the dataset and more time is devoted to support computation. Time repartition depends also on data density as well as the support thresh-old. Another general remark is that sibling coverage test time is in general negligible w.r.t. the rest. This shows that its parallelization is not worthwhile in general. Despite our algorithm does not provide any performance guarantee on the candidate management part, we generally observe an interesting speedup for the covering tests. is consumed by the support computation (Figures 6(a) and 6(c)). Hence, the speed up of the overall execution is almost equal to the support computation speed up. A noticeable remark at this stage is the values mentioned in Figures 6(b) and 6(d). Indeed, Figure 6(b) shows a total speedup of 30 while the number of threads is 16. Recall that the number of physical cores is 8. This is explained by data locality. Indeed, when two threads access the same data, the system will first check the different levels of the cache before making an access to the memory.
 the proportion of time devoted to support computation is comparable to the time spent in covering tests when the minimal support  X  is set to 1 K (Figure 6(e)). This is not the case when  X  = 2 k . and 6(g)). We note however that the speedup for support computation is in both cases almost perfect(Figures 6(f) and 6(h)). Finally, even if the support calculation uses only half of the total time, the total speed up is interesting.
 than T10I4D100K. Figures 6(i) and 6(k) show that support computation time is important. Nevertheless, we do not reach the same speed ups as those with Webdocs. Never-theless, the total execution time is almost always divided by the number of threads.
 this data set is 10. Hence, maximal frequent itemsets are rapidly reached when DFS is used (note that in Kosarak, this average length is even lower). This makes support com-putation not very time consuming, even when the minimal support is set to 20) thanks to the summarization capacity of the FP-trees.
 it is not surprising to find that support computation is neg-ligible w.r.t. candidates management.
We conducted some experiments so that to check whether this simple DFS algorithm performs much worse than the theoretical lower bound [20]. It is proven that any algo-rithm which computes the set of MFI X  X  has to check the support of at least each MFI and each minimal infrequent itemset . This second set if also called the negative border. More precisely, if M + and M  X  are respectively the positive and the negative borders, then any algorithm computing the MFI X  X  needs to make more than |M +  X  X   X  | support com-putations. Figure 7 shows the ratios between the number of support computations made by DFS and |M +  X  X   X  | . In general the number of support computations is not much more than the theoretical lower bound. To our knowledge, there is no algorithm in the literature providing a proven linear or at least a polynomial upper bound of the support computations number w.r.t. |M +  X  X   X  | . As a final experiment, we present the execution time of PADS [28] which is, to our best knowledge, the most effi-cient implementation of a sequential MFI mining algorithm compared to that of MineWithRounds . PADS is not a pure DFS implementation in that it uses several heuristics to reduce computation time, e.g., each time a candidate is found frequent, all its right parents are evaluated and re-ordered w.r.t their increasing support. Hence, the  X  order is dynamically and continuously modified. Figure 8 shows the execution times w.r.t. support thresholds. Both im-plementations have been executed on the same machine as before. The time needed to load the data and to construct the FP-trees is not taken into account. One should however notice that our execution time is measured when 16 threads are executed in parallel. It however took about 2 minutes for both implementations to start the mining procedure. When Fig ure 8: Execution times for PADS and MineWith-Rounds with Webdocs the minimal support is less or equal to 140000 PADS had a segmentation fault. To be complete, we should mention that our implementation is naive in that we used the C++ STL library which makes it very easy to program prototypes but if we want to fine tune the optimizations, as in PADS, one should program his/her own data structures. We presented a general algorithm for computing borders. Our results are valid independently of the actual type of bor-der as soon as the mined patterns are sets of objects and the interestingness property is anti-monotone. With the gener-alization of multi-core CPU X  X  and their cache management, some proposals fine tuned the previous techniques so that data caching and pre-fetching become effective [10]. The proposal of an efficient parallel algorithm for building the FP-trees has already been proposed by [17] but the authors did not parallelize the mining process. With our work, we make a step further in exploiting modern multi-core archi-tectures. Our experiments show that without necessarily us-ing very sophisticated optimization techniques, we are able to have execution times much better than state of the art implementations. Parallelizing algorithms that extract other kinds of patterns e.g. sequences, trees or graphs is a natural extension of the present work, e.g. [29] use a lexicographic order for mining frequent graphs. With the Round func-tion, one may partition the subgraphs such that they can be processed in parallel. In a distributed setting, i.e. when the data itself is distributed, communications overhead becomes a bottleneck thus our algorithm could have to be adapted. Finally, extending our algorithm to mine in parallel condi-tional functional dependencies [8] is an interesting direction of future work.
 viewers for their valuable suggestions that helped us for ame-liorating the presentation of this work. [1] R. Agarwal, C. Aggarwal, and V. Prasad. Depth first [2] R. Agrawal and R. Srikant. Fast algorithms for mining [3] R. Bayardo. Efficiently mining long patterns from [4] D. Burdick, M. Calimlim, and J. Gehrke. MAFIA: A [5] S. M. Chung and C. Luo. Efficient mining of maximal [6] T. Eiter, G. Gottlob, and K. Makino. New results on [7] M. El-Hajj and O. Za  X   X ane. Parallel leap: Large-scale [8] W. Fan, F. Geerts, J. Li, and M. Xiong. Discovering [9] F. Flouvat, F. D. Marchi, and J. Petit. A thorough [10] A. Ghoting, G. Buehrer, S. Parthasarathy, D. Kim, [11] K. Gouda and M. Zaki. GenMax: An efficient [12] G. Grahne and J. Zhu. Fast algorithms for frequent [13] D. Gunopulos, R. Khardon, H. Mannila, S. Saluja, [14] J. Han, J. Pei, Y. Yin, and R. Mao. Mining frequent [15] N. Hanusse, S. Maabout, and R. Tofan. A view [16] Y. Huhtala, J. K  X  arkk  X  ainen, P. Porkka, and [17] E. Li and L. Liu. Optimization of frequent itemset [18] D. Lin and Z. Kedem. Pincer search: A new algorithm [19] S. Lopes, J.-M. Petit, and L. Lakhal. Efficient [20] H. Mannila and H. Toivonen. Levelwise search and [21] M. Miah, G. Das, V. Hristidis, and H. Mannila. [22] S. Nedjar, A. Casali, R. Cicchetti, and L. Lakhal. [23] N. Novelli and R. Cicchetti. Fun: An efficient [24] OpenMP. www.openmp.org. [25] S. B. Roy, S. Amer-Yahia, A. Chawla, G. Das, and [26] K. Satoh and T. Uno. Enumerating maximal frequent [27] C. M. Wyss, C. Giannella, and E. L. Robertson. [28] K. W. X. Zeng, J. Pei and J. Li. PADS: a simple yet [29] X. Yan and J. Han. gSpan: Graph-based substructure
