 vote classifier is maximized when all its errors are located on low margin examples. We show that our bound is tight when the associated Gibbs risk can accurately be estimated and when the Bayes classifier makes most of its errors on low margin examples.
 that the margin is above a given threshold.
 fix the margin threshold manually.
 compared to TSVM [7] and the same algorithm but with a manually fixed threshold as in [11] discuss the outcomes of this study and give some pointers to further research. probability distribution D over X  X Y and we denote the marginal distribution over X by D X . each x 0  X  X U , there is exactly one possible label that we will denote by y 0 . 1 B Bayes classifier is defined by distribution Q . We accordingly define the transductive risk of G Q over an unlabeled set by: y are then able to accurately bound the transductive risk of the Bayes classifier: This result follows from a bound on the joint Bayes risk: in a self-learning algorithm by iteratively assigning pseudo-labels to unlabeled examples having margin probability distribution over X U i.e. For any subset A,P ( A ) = 1 u card ( A ) . 2.1 Main Result classifier (3) and the joint Bayes risk (4).
 and b . c + denotes the positive part (i.e. b x c + = [[ x &gt; 0]] x ) .
 More generally, with probability at least 1  X   X  , for all Q and all  X   X  0 : with low margin. Proposition 2, together with the explanations that follow, makes this idea clearer. Proposition 2 Assume that  X  x  X  X U ,m Q ( x ) &gt; 0 and that  X  C  X  (0 , 1] such that  X   X  &gt; 0 : Then, with probability at least 1  X   X  : Where  X   X  = sup {  X  | P u ( B Q ( x 0 ) 6 = y 0  X  m Q ( x 0 ) =  X  ) 6 = 0 } of all the statements above and show in lemma 4 a simple way to compute the best margin threshold for which the general bound on the joint Bayes risk is the lowest. 2.2 Proofs X Denote moreover b i = P u ( B Q ( x 0 ) 6 = y 0  X  m Q ( x 0 ) =  X  i ) for i  X  X  1 ,...,N } . Then, Equation (8) is obtained from the definition of the margin m Q which writes as y defined the Gibbs risk (see equation 2 and the definition of m Q ).
 and the following inequality: 0  X  b i  X  P u ( m Q ( x 0 ) =  X  i ) and which satisfy equations (8) and (9). with probability 1  X   X  we have then solution of a linear program that can be solved analytically and which is attained for: For clarity, we defer the proof of equation (12) to lemma 4, and continue the proof of equation (6). from equations (11) and (12) with b I = K bound on R u  X   X  ( B Q ) : side of equation (13). Moreover, for  X  &lt;  X  I , we notice that  X  7 X  P u ( m Q ( x 0 ) &lt;  X  )+ K decreases.
 k  X  X  1 ,...,N } . Then, the optimal value of the linear program: is attained for q  X  defined by:  X  i  X  k : q  X  i = 0 and  X  i &gt; k,q  X  i = min p i , b B  X  P j&lt;i q in O , and that this solution is q  X  . In the rest of the proof, we denote F ( q ) = P N i = k +1 q i . q M6 =  X  . In this case, let K = min { i &gt; k | q  X  i &lt; p i } and M = I ( q,q  X  ) . feasible.
 an optimal solution in O must be the greatest feasible solution for ).
 and q  X  X  . Let  X  = min q M , g M g P g K &lt; g M and  X  &gt; 0 . Thus, q is not optimal.
 the greatest feasible solution for the lexicographic order in O to be optimal and which is q  X  . Proof of Proposition 2 First let us claim that Indeed, assume for now that equation (15) is true. Then, by assumption we have: Since F  X  u ( Q )  X  P u ( m Q ( x 0 ) &lt;  X   X  ) + 1  X   X  j K  X  u ( Q )  X  M &lt; Q (  X   X  ) k R u ( G Q )  X  R u ( G Q ) . Taking once again equation (16), we have P u ( m Q ( x ging back this result in equation (17) yields Proposition 2.
 P u ( m Q ( x 0 ) =  X  i ) . Finally, from equation (9), we have R u ( B Q ) = P that the Bayes classifier makes its errors mostly on low margin regions. This assumption constitutes the working hypothesis of margin-based self-learning algorithms in which a pseudo-labels to unlabeled examples having a margin above a fixed threshold (denoted by the set Z U\ in equivalently, are those which have a small conditional Bayes error defined as: For all these algorithms the choice of the threshold is a crucial point, as with a low threshold the risk to assign false labels to exam-ples is high and a higher value of the threshold would not provide enough examples to enhance the current decision function. In order to examine the effect of fixing the threshold or computing it automat-ically we considered the margin-based self-training algorithm pro-posed by T  X  ur et al. [10, Figure 6] (referred as SLA in the following), in which unlabeled examples hav-ing margin above a fixed threshold are iteratively added to the labeled mizing the conditional Bayes error (18) from equation (6) of theorem 1 is computed at each round of the the threshold in equation (18) was fixed to its worst value 0 . 5 . In our experiments, we employed a Boosting algorithm optimizing the following exponential loss 2 as the baseline learner (line (6), figure 1): Where H = P t  X  t h t is a linear weighted sum of decision stumps h t which are uniquely defined by an input feature j t  X  X  1 ,...,d } and a threshold  X  t as: training examples. We evaluated the performance of the algorithms on 4 collections from the benchmark and unlabeled training sets.
 unlabeled examples in each data set.
 Dataset d l + u l SLA SLA  X  TSVM l SLA SLA  X  TSVM the 20 trials that we ran for these collections.
 gorithm becomes competitive when the margin threshold is found automatically rather than if it is fixed that of TSVM in most cases, while it outperforms the initial method over all runs. for a fixed number of labeled training data l = 10 . After this fall, few examples are added because the learning algorithm does not increase the margin on We have shown that our bound is a good approximation of the true risk when the errors of the associated examples.
 The proof of the bound passed through a second bound on the joint probability that the voted classifier solving a linear program.
 allows to enhance the performance of a self-learning algorithm.
 References
