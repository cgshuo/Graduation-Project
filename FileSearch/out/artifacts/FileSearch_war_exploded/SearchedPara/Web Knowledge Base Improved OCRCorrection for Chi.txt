 The performance of Optical Character Recognition (OCR) is often impacted by weak illumination, noise, and skew. Most of the error correction methods address on Natural Language Processing (NLP) or Machine Learning techniques. How-ever, they might not perform well for business card. For instance, it X  X  difficult to determine the correct address among various candidates valid for models with-out resorting to additional information. Nowadays, there exist diverse Knowledge Bases (KBs) such as web encyclopedias (e.g. Wikipedia 1 ), and Point of Interest (POI) database. It is plausible that the  X  X inked X  knowledge within the KBs be explored to improve the OCR accuracy further.
 first work for the OCR correction of Chinese business cards from the point of view of KB. A pipelined framework of correction method including selectivity-aware pre-filtering, text-level correction and image-level correction was proposed. Selectivity-aware pre-filtering was tested to exclude irrelevant records quickly while retain the possible candidates. To be flexible for address integrity, a robust similarity measuring method integrating Dynamic Time Warping (DTW) with Jaccard/Levenshtein measure was developed. To make the company comparison more reliable, a strategy of weighting importance based on Wikipedia KB was conducted. To distinguish the candidates with similar text similarity, traditional Levenshtein distance was generalized to Image-based Levenshtein measure. We focused on the correction of company-address pair. The biggest challenge was similarity computation to handle the format diversity or importance imbalance. Related key techniques were emphasized below. 2.1 Address/Company Similarity To compute the similarity reasonably for addresses with different integrity, we decomposed address by its hierarchy at first. Then, DTW and Jac-card/Levenshtein measure were performed. Since different part within company name played different roles in comparison, Part of Speech Tagging was applied for segmentation and feasible weights were then assigned using Inverse Document Frequency(IDF) derived from Wikipedia KB. The details of weight computing is illustrated in Formula.1. An example was shown in Table.1. 2.2 Text-level/Image-level Correction and Pipelined Framework Although the incorrectly recognized characters are different from the correct ones in text, they might be similar in image. Hence, a pipelined correction method using both text similarity and image similarity was applied. For text-level com-parison, DTW combined with Jaccard/Levenshtein measure was applied. For image-level comparison, 2D Discrete Cosine Transform features and intersecting features were applied. To integrate the image similarity with text comparison, we proposed Image-based Levenshtein measure. The proposed distance between strings a, b is generalized in Formula.2. An example was shown in Table.2. Our pipelined correction method was illustrated in Fig.1. To leverage speed and accuracy, more latter the step was, more complicated the method would be. Baseline: A novel method based on Online Spelling Suggestion[ 1 ] which har-nesses an internal database containing a huge collection of terms and n-gram words statistics was used as the baseline. To be more effective on Chinese text, we improved the baseline to a strengthened  X  X id you mean X  correction method by using Baidu X  X  engine as the complement of Google X  X  engine.
 Schemas: Two schemas were conducted: 1) Schema.1: correction on original OCR outputs. 2) Schema.2: correction on customized OCR outputs (to be com-parable with current OCR system, we retained the error rate less than 10% for characters while kept the statistics of right/wrong OCR cases invariant). Dataset: 204 Shanghai business cards were collected for test. The POI KB derived from Baidu map covers 330 thousands records of Shanghai related points. Discussions: The OCR accuracy is relatively lower on schema 1. Besides our rigid definition of  X  X ight/Wrong X , insufficient trained data was another factor. On schema 2, even under less noisy condition comparable with current OCRs, the system could further improve the correction accuracy.
 Different from most of previous publications, this work performed data cor-rection from the perspective of linked knowledge and demonstrated its effective-ness in reducing errors further in both noisy and less-noisy condition.
