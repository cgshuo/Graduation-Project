 This paper proposes a novel framework for music content indexing and retrieval. The musi c structure information, i.e., timing, harmony and music region c ontent, is represented by the layers of the music structure pyra mid. We begin by extracting this layered structure information. We analyze the rhythm of the music and then segment the signal proportional to the inter-beat intervals. Thus, the timing information is incorporated in the segmentation process, which we call Beat Space Segmentation . To describe Harmony Events , we propose a two-layer hierarchical approach to model the music chords. We also model the progression of instrumental and vocal content as Acoustic Events . After information extraction, we propose a vector space modeling approach which uses these events as the indexing terms. In query-by-example music retrieval, a query is represented by a vector of the statistics of the n -gram events. We then propose two effective retrieval models, a hard-indexi ng scheme and a soft-indexing scheme. Experiments show that the vector space modeling is effective in representing the layered music information, achieving 82.5% top-5 retrieval accuracy using 15-sec music clips as the queries. The soft-indexing outperfor ms hard-indexing in general. H.3.1. [ Information Storage and Retrieval ]: Content Analysis and Indexing -Indexing methods, H.3.3 Information Search and Retrieval -Retrieval models Algorithms, Performance, Experimentation Music structure, beat space segmentation, harmony event, acoustic event, vector space modeling, n -gram, Over the past decades, increasingly powerful technology has made it easier to compress, di stribute and store digital media content. There is an increasing demand in tools for automatic indexing and retrieval of music recordings. The task of music retrieval is to rank a collection of music clips according to each one X  X  relevance to a query. In th is paper, we are particularly interested in music information retrieval (MIR) for popular songs that are recorded in the raw audio format. In general we aim at providing musicians and scholars tools that search and study different musical pieces of similar music structures (rhythmic structure, melody/harmony structure, music descriptions, etc); help entertainment service provide rs index and retrieve the songs of similar tones and se mantics in response to the user queries in the form of music clips, which is also referred to as query-by-example . The challenges of a MIR system include effective indexing of music information that supports run-time quick s earch, accurate query representation as the music descriptor, and robust retrieval modeling that ranks the music doc uments by relevance score. Many MIR systems have been reported in the survey articles [18][24]. MIR research community initially focused on developing text based systems wh ere both database and the query are in the MIDI format and the information is retrieved by matching the melody of query with the database [5][6][10][12] [17][19][25]. Since the melody information of both query and song database are text based (M IDI), the research has been devoted to database organiza tion of the music information (monophonic or/and polyphonic nature) and to text-based retrieval models. The retrieval models in those systems includes dynamic programming (DP)[15][22][25], n -gram-based matching [5][6] [25] and vector space model [17]. Recently, with the advances in information technologies, the community has started looking in to developing MIR systems for music in raw audio format. Successful examples towards this research objective includes the query-by-humming systems [10][22], which allows a user to input the query by humming a melody line via the microphone. To do so, research efforts have been made to extract the pitc h contours from the hummed audio, and to build a retrieval model that measures the relevance between the pitch contour of the query and the melody contours of the intended music signals. Autocorr elation [10], harmonic analysis [22] and statistical modeling via audio feature extraction [21] are some of the techniques that have been employed for extracting pitch contour from hummed queries . In [4][9][10], fixed length audio segmentation, spectral and pitch contour sensitive features are discussed to measure similarity between music clips. However, the melody-based retrieval model is insufficient for MIR because it is highly possible that different songs share an identical melody contour. The challenge for MIR of music in raw audio format is to represent the music content including harmony/melody, vocal and song structure information holistically. In this paper, we propose novel indexing and retrieval framework which describes a music signal with a multi-layer representation and it is continuation of our earlier research [15]. We incorporate timing information of the song with the music segmentation process. Then we detect the progression of both music chord and the contents in the music regions to describe the harmony events and the acoustic events respectively. Inspired by the success of vector space modeling in text-bas ed information retrieval, we index and retrieve the songs using vectors of n -gram statistics of those events. The proposed framework is illustrated in Figure 1. This paper is organized as follows. Conceptual music structure pyramid to visualize the information in the music structure is statistical modeling of layered mu sic information. In Section 4, we propose a vector space modeling framework for MIR with two retrieval models, the hard-indexi ng and the soft-indexing models. In Section 5, we describe the experiment results. Finally, we conclude in Section 6. As shown in Figure 2, we represent music information conceptually by a multi-layer pyramid structure. The 1 st layer is the foundation of the pyramid which dictates the timing of a music signal. As time elapses mixing multiple notes together in the polyphonic music, a harmony line is created which is the 2 nd layer of music information. Pure instrumental (PI), pure vocal (PV), instrumental mixed vocal (IMV) and silence (S) are the regions that can be seen in a song. PV regions are rare in popular music. Silence regions (S ) are the regions which have imperceptible music including unnoticeable noise and very short clicks. The content of the music regions are represented in the 3 layer. The 4 th layer and above depicts the semantics of the song structure, which describes the events or the messages to the audience. Out of all the layers, the most difficult task is to understand the information in the top layer, the semantics of a song from the song structure point of view. In the case of query-by-example MIR, we often have a partial clip instead of a full-length song as a query. Therefore, we believe that the lower layer music information is more informa tive than the top layer as far as MIR is concerned. As such, the top layer information is less critical. It is noted that popular songs are similar in many ways, for example, similar beat cycle  X  common beat patterns, similar harmony/melody -common chord patterns, similar vocal  X  similar lyrics and similar s emantic content  X  music pieces or excerpts that creates similar auditory scenes or sensation. In this paper, we will study the retrieval model that eval uates the song sim ilarities in the aspects of beat pattern, melody pattern and vocal pattern. The fundamental step for audio content analysis is the signal segmentation where the signal within a frame can be considered as quasi-stationary. With quasi-stationary music frames, we can extract features to describe the content and model the features with statistical techniques. The quality of signal segmentation has an impact on system level pe rformance of music information extraction, modeling and retrieval. Like in speech processing, earlier music content analysis [1][3][8] approaches have used fixed length signal segmentation. A music note can be considered as the smallest measuring unit of the music flow. Usually smaller not es (1/8, 1/16 or 1/32 notes) are played in the bars to align the melody with the rhythm of the information within the duration of a music note can be considered quasi-stationary. In this paper we segment the music into frames of the smallest note length instead of fixed length frames. Since the inter-beat interval of a song is equal to the integer multiples of the smallest note, this music framing strategy is called Beat Space Segmentation (BSS) . We will discuss the music segmentation in Section 3.1 that captures timing information (1 st layer in Figure 2) of the music structure. In Section 3.2 and 3.3 we will further discuss extraction of harmony and music region content descriptive features that model music information in the 2 the 3 rd layer. We illustrate the proposed onset detection and smallest note length calculation in Figure 3. As highlighted in [15], the spectral characteristics of the music sign als are enveloped proportional to octaves. So, we first decompose the music signal into 8 sub-bands whose frequency ranges are shown in Table 1. Figure 3: Onset detection and smalle st note length calculation Then the sub-band signals are segmented into 60ms frames with 50% overlap. Both the frequency and energy transients are analyzed using a method similar to that in [7]. We measure the frequency transients in terms of pr ogressive distances in sub-band 01 to 04 because fundamental freque ncies (F0s) and harmonics of music notes in popular music are strong in these sub-bands. The energy transients are comput ed from sub-band 05 to 08. Table 1: The frequency ranges of the octaves and the sub-bands Eq.(1) describes the computation of final onset at time  X  X  X , On(t) which is the weighted sum of sub-band onsets SO r (t) . been empirically found to be the best set for calculating dominant onsets in music signals. We run circular autocorrelation over the detected onsets to es timate the inter-beat pr oportional note length. By varying this estimated note le ngth, we check for patterns of equally spaced intervals between dominant onsets On(.) using a dynamic programming approach. The most frequent smallest interval, which is also an integer fraction of other longer intervals, is taken as the smallest note length. Figure 4(a) illustrates the process for a 10-second song clip. The detected onsets are shown in Figure 4(b). The autocorrelation of the detected onsets is shown in Figure 4(c). Inter-beat proportional smallest note level (183.11ms) measure is shown in Fi gure 4(d). We assume that the tempo of the song is constant. Th erefore the starting point of the song is used as the reference point for BSS. Similar steps are followed for computation of the smallest note length in the query song clip. However the first dom inant onset is used as the reference point to segment the clip back and forth accordingly. The reference onset is marked in dashed line in Figure 4(b). The smallest note length and its multiples form the tempo/rhythm cluster (TRC). By comparing the TRC of query clip with TRC of the songs in the database, we can narrow down the search space. Silence is defined as a segment of imperceptible music, including unnoticeable noise and very short clicks. We use short-time energy function to detect the silent frames. The progression of music chords describes the harmony event of music. A chord is constructed by playing set of notes (&gt;2) simultaneously. Typically there are 4 chord types ( Major, Minor, Diminished and Augmented ) and 12 chords per chord type that can be found in the western music. Fo r efficient chord detection, the tonal characteristics (F0s, harm onics and sub-harmonics) of the music notes which comprise a c hord should be well characterized by the feature. Goldstein (1973) [11] and Terhardt (1974) [23] proposed two psycho-acousti cal approaches: harmonic representation and sub-harmonic representation, for complex tones respectively. It is noted that harmonics and sub-harmonics of a music note are closely related to the F0 of another note. For and G6. Similarly 5 th and 7 th sub-harmonics of note E7 are closed to F0 of C5 and F#4 respectively. In our chord detection system, we place 12 filters centered on F0s of 12 notes in each octave covering 8 octaves (C2B2 ~C8B8) to capture the strengths of F0s, sub-harmonics and harmonics. The filter positions are calculated using Eq.(2) which first maps the linear frequency scale ( f linear ) into octave scale ( f N , F req are sampling frequency, number of FFT points and reference mapping point respectively . We set frequency resolution (Fs/N) equal to 1Hz, F req =64Hz (F0 of the note C2) and C =12 (12 pitches). The filter (rectangular filter in dashed line) position near note G in both octave and linear frequency axis is depicted in Figure 6. The reasons for using filters to extract tonal characteristics of notes are explained below. 1. Due to physical configuration of the instruments, the F0s of 2. Though the physical octave ratio is 2:1, cognitive experiments In our experiments, it is found that the tonal characteristics in an individual octave can even effectively represent the music chord. To model these tonal characteris tics in the octaves, we propose a 2-layer hierarchical model for music chord (see Figure 5). The feature vectors (12-dimensional) which are extracted from individual octaves. Due to poor chord detection accuracy in the C9B9 octave, only C2B2~C8B8 octaves are considered. The construction of PCP vector for n th signal frame and for each octave is explained in Eq.(3). F0 strengths of the  X  related harmonic and sub-harmonic strengths of other notes are summed up to form the  X  th coefficient of the PCP vector. In Eq.(3), S(.) is the frequency domain magnitude (in dB) signal frequency range varies with both octave index (OC) and  X  in the octave (OC). If the octave index is 1, then the respective octave is C2B2. The 2 nd layer model is trained with the outputs of the 1 models which are organized into a feature vector. In our implementation we use 4 Gaussian mixtures for each model in layer 1 and 2. Therefore input vectors to the layer 2 model are probabilistic vectors. This 2-layer modeling can be visualized as first transforming feature space represented tonal characteristics of the music chord into probabilistic space at the layer 1 and then modeling them at layer 2. We us e this 2 layer representation to model 48 music chords in ou r chord detection system. Figure 5: Two layers hierarchical representation of a music chord As discussed in Section 2, PV, PI, IMV and S are the regions types in a song (3 rd layer). However PV regions are comparatively rare in popular music. Therefor e both PV and IMV regions are considered as vocal (V) region. In this way, we can just focus on contents of 3 regions (PI, V and S). Silence detection has been discussed in Section 3.1. Sung vocal line carries more desc riptive information about the song than other regions. In the PI regions, the extracted feature must be able to capture the information generated by lead instruments (typically the tunes/melody). To this end, we examine Octave scale cepstral coefficient (OSCC) feature and Mel-frequency cepstral coefficient (MFCC) feature for their capabilities to characterize music region content information. MFCC have been highly effective characterizing subjective pitch and the frequency spectrum of speech signals [4]. OSCCs are computed by using a filter bank in frequency domain. Filter positions in the linear frequency scale ( f linear ) are computed by transforming linearly positioned filters in the octave scale ( f that 12 overlapping rectangular filters are positioned in each octave from C2B2 to C9B9 octave (64 ~ 16384) Hz. The Hamming shape of filter/window has sharp attenuation and it suppresses valuable information in the higher frequencies nearly by 3 fold as compared to the rectangular shape filter [4]. Therefore, a rectangular filter is better than Hamming filter for music signal analysis because they are wide band signals compared to speech signals. Figure 6 depicts octave to linear filter position transformation. The output Y(b) of the b computed according to Eq.(4) where S(.) is the frequency spectrum in decibel (dB), H b (.) is the b th filter, and m boundaries of b th filter. Eq.(5) describes the computation of  X  th cepstral coefficient where k , N f and Fn are center frequency of the b th filter, number of frequency sampling points and number of filters respectively ( Fn =12 in our case). Figure 6: Transformation of octave sc ale filter positions to linear frequency scale Singular values (SVs) indicate the variance of the corresponding structure. Comparatively high singular values describe the number of dimension in which th e structure can be represented orthogonally. Smaller singular va lues indicate the correlated information in the structure and considered to be noise. We perform singular value decomposition (SVD) over feature matrices extracted from PI and V regions. Figure 7 shows the normalized singular value variation of 20 OSCCs and 20 MFCCs extracted from both PI and V regions of a Sri Lankan Song  X  X a Bala Kale ( ) X . We use 96 filters for calculating MFCCs a nd OSCCs. It can be seen that singular values of OSCCs are higher than of MFCCs for both PV and PI frame. The average of 20 singular values per OSCCs for PV and PI frames are 0.1294 a nd 0.1325. However, for MFCC, they are as lower as 0.1181 and 0.1093 respectively. As shown in Figure 7, the singular values are in descending order with respect to the ascending coefficient numbers. The average of the last 10 singular values of OSCCs is nearly 10% higher than those of MFCCs, which means the last 10 OSCCs are less correlated than the last 10 coefficients of MFCCs. Thus we can conclude that the OSCCs are less correlated than MFCCs in representing content of music regions. Figure 7: Singular values from OSCCs and MFCCs for PV and PI frames. The frame size is a quarter note length (662ms) Unlike text document that uses words or phrases as indexing terms, a music signal is a c ontinuous digital signal without obvious anchors for indexing. The challenges of indexing music signal are two fold. First, what would be good indexing anchors; second, what would be good representation of music contents for indexing and retrieval. In Sec tion 3, we have discussed the statistical modeling of music information in a multi-layer paradigm as illustrated in Fi gure 2. Layer-wise information representation allows us to desc ribe a music signal quantitatively in a descriptive data structure. Ne xt, we will propose two indexing terms i.e. harmony event and acoustic event to describe the information in the 2 nd and 3 rd layers of the music structure pyramid. Progression of music chords describes the Harmony Event . In Section 3.2, we explained sub-band PCP feature extraction and a 2-layer hierarchical chord modeling. Note that it is relatively easy to detect beat spacing in a music signal. A beat space is a natural choice as a music frame, and thus the indexing resolution of a music signal. Suppose that we ha ve trained 48 frame-based chord models, as shown in Figure 5 (4 chord types Major, Minor, Diminish and Augmented in combination with 12 chords each type). Each chord model descri bes a frame-based harmony event which can serve as the indexing term. One can think of music as a chord sequence, with each chord spanning over multiple frames. A chord model space  X  = { C i } can be trained on a collection of chord-labeled data. We use the HTK 3.3 toolbox for training such a 2-layer chord model space. At run-time, a music frame O recognized and converted to a harmony event  X  h , and a music signal is therefore tokenized into a chord sequence. Pure instrumental (PI) and the vocal (V) regions contain the descriptive information about the music content of a song. A song can be thought of as a sequence of interwoven PI and V events, that we call Acoustic Events . We extract 20 OSCC features from each music frame. We train two Gaussian Mixture models (GMMs) of 64 mixtures. Each GMM is trained on a collection of features from one of the two events. We define the frame-based acoustic events as another type of indexing term in parallel with harmony events. Suppose we denote r 1 for PI and r 2 for V event. They are trained from a labeled database. At run-time, a music frame O n is recognized and converted to a V or PI event  X  and (7) can be seen as the c hord and acoustic event decoders. We index the contents in s ilence regions (S) with zero observation. Inspired by the idea in text categorization where we use lexical words as indexing term s to form a document vector for a text document, we attempt to use the events as indexing terms to design a vector for a music segment. Next let us formulate the indexing and retrieval problem cast in the vector space model framework. We will study two retrieval models, hard-indexing and soft-indexing n -gram MIR models. The harmony and acoustic decoders serve as the tokenizers for music signal. The tokenization process results in two synchronized streams of events, a chord and an acoustic sequence, for each music signal. An event is represented by a tokenization symbol. They are represented in a text-like format. It is noted that n -gram statistics has been us ed in many natural language processing tasks to capture short-term substring constraints such as letter n -gram in language identification [2] and spoken language identification [14]. If we think of the chord and acoustic tokens, as the letters of music, then a music signal is a document of chord/acoustic transcripts. Similar to the letter n -gram in text, we can use the token n -gram of music as the indexing term, which aims at capturing the short-term syntax of musical signal. The statistics of token themselves represent the token unigram. Vector space modeling (VSM) has b ecome a standard tool in text-based IR systems since its introduction several decades ago [20]. advantages of the method is th at it makes pa rtial matching possible. We can derive the distance between documents easily as long as the vector attributes are well defined characteristics of the documents. Each coordinate in the vector reflects the presence of the corresponding attribute, wh ich is typically a term. A chord/acoustic token in a music signal is just like a term in a document. Inspired by the idea of VSM in text-based IR, we propose using a vector to represent a music segment. If a music segment is thought of as an articl e of chord/acoustic tokens, then the statistics of the presence of the tokens or token n-grams describe the content of the music. Suppose that we have a token sequence, t 1 t 2 t 3 unigram statistics from the token sequence itself. We derive the bigram statistics from t 1 (t 2 ) t 2 (t 3 ) t 3 (t vocabulary is expanded over the toke n X  X  right context. Similarly, we derive the trigram statistics from the t 1 (#,t t (t 3 ,#) to account for left and right contexts. The # sign is a place holder for free context. In the in terest of manageability, we only use up to bigrams. In this way, for an acoustic vocabulary of | c |=48 token entries in the chord stream, we have 48 unigram Figure 8. f n i is equal to 1, if t n =c i otherwise it is 0 . Similarly we have 2 unigram frequency items in the acoustic vector for the acoustic stream. For simplicity, we only formulate the bigram representation for two consecutive frames. As such, we build a chord bigram vector of [48x48=2304] dimensions, ff f f  X  =
JJG f =1; otherwise f n i,j =0. Similarly an acoustic bigram vector of [2x2=4] dimensions can be formed. For a music segment of N f fff =
JJG the i th element as We can construct its chord bigram vector of [48x48=2304] (i,j) th element as The acoustic vector can be formul ated in a similar way with a 2-dimensional vector for unigram and [2x2=4] dimensional vector for bigram. Figure 9 show s schematically how an n -gram vector is constructed using N frames of unigram vector and how the relevance score is evaluated between a query and a music segment. A query (song clip) Figure 9: The music database is indexed by n -gram vector. Each harmony/acoustic event is associated with an indexing vector. The similarity between a query and a music segment in the database is measured for relevance ranking.
 Although we use 2-dimensional coor dinate for the bigram count, the vector can be treated as a 1-dimensional array. The process of deriving unigram and bigram vectors for a music segment involves minimum computation. In practice, we can compute those vectors at run-time dir ectly from the chord/acoustic transcripts resulting from the tokenization. Note that the tokenization process compares a music frame against all the chord/acoustic events at a higher computational cost. It can be done off-line. Following the text-based IR process, the MIR process computes the similarity between a query music segment and all the candidate music segments. For simplicity, let () i chord unigram vector (48 dimensions) and , () ij chord bigram vector (2304 dimensions) for a query of N frames. music database. The similarity between two n -gram vectors can be defined as With Eq.(10) and Eq.(11), we can rank the music segments by their relevance. The relevance is can be defined by the fusion of unigram and bigram similarity scores. Although it would be convenient to derive the term count from token sequences derived from a music query, we find that the tokenization is affected by many factors. For example, the tokenization does not always pro duce identical token sequence for two similar music segments. The difference could be due to the variation in beat detection, variation of music productions between the query and the intended music. The inconsistency between the tokenization of th e query and the intended music produce an undesired mismatch as far as MIR is concerned. Assuming that the numbers of beats in query and music are detected correctly, the inconsistency is characterized by substitutions of tokens between the desired label and the tokenization results. If a token is substituted, then it presents a mismatch between the query and the intended music segment. To address this problem, we propos e using the tokenizers as probabilistic machines that generate a posteriori probability for each of the chord and acoustic events. If we think of the n -gram counting as integer counting, then the posteriori probability can be seen as soft-hits of the events. For brevity, we only formulate the soft-hits for chord vector. According to Bayes X  rule, we have where p(c i ) be the prior probability of the event i c . Assuming no prior knowledge about the events, p(c i ) can be dropped from Eq.(12), which is then simplified as Let P(c i |o n ) be denoted as p n i . It can be interpreted as the expected continuous values as illustrated in Figure 8, which can be thought of a soft-indexing approach as opposed to the hard-indexing approach for music frame using n-gram counting. The soft-indexing reflects how a frame is represented by the whole model space while the hard-indexing estimates the n-gram count based on the top-best tokenization re sults. We have good reason to expect soft-indexing to provide higher resolution vector representation for a music frame. Figure 10: An expected frequency vector for a music frame Assuming the music frames are independent of each other, the joint posteriori probability of two events i and j between two frames, n th and (n+1) th can be estimated as frequency of unigram and bigr am can be estimated as Thus the soft-indexing vector for query and matching music f q
JJG Eq.(13), the similar relevance scores can be used for soft-indexing ranking. We first study the chord and acoustic modeling performance. Then we carry out MIR experiments. We established a 300 song database DB1 (44.1 kHz sampling rate, 16 bits per sample, mono channel) extracted from music CDs for MIR experiments. Songs in DB1 are sung by 20 artists as listed in Table 2, each on average contributing 15 songs. The tempos of the songs are in the rage of 60~180 beats per minute Harmony events are described by the progression of music chords. Each of the 48 chord models is a 2-layer representation of Gaussian mixtures (see Figure 5) and is trained with annotated samples in a chord database (CDB). The CDB includes recorded chord samples from original instruments (string type, bow type, blowing type, etc) as well as synthetic instruments (software generated). In addition, the CD B also includes chord samples extracted from 40 English songs (a s ubset of DB1), with the aid of music sheets and listening tests. Therefore we have around 10 minutes of each chord sample spanning from C2 to B8. 70% of the samples of each chord are used for training and the rest 30% for testing in cross validation se tup. Experimental results are shown in Figure 11. The results of the proposed 2-layer model (TLM) are compared with single layer model (SLM). Single layer chord model is constructed using 128 Gaussian mi xtures. General PCP features vectors (GPCP) are used for training and testing the SLMs. feature vectors. It is noted that the proposed TLM with feature extracted from BSS outperforms the SLM approach by 5% in absolute accuracy. We compare performance of OSCCs and MFCCs for modeling regions PI and V. SVD analysis depicted in Figure 7 highlights that OSCCs characterize music content more uncorrelatedly than MFCCs. In this experiment, we selected 100 English songs (10 songs per artist and 5 artists pe r gender) from DB1. We annotate the V and PI regions. Each V and PI class information is then modeled with 64 GMs. 100 songs are used by cross validation where 60/40 songs are used as training/testing in each turn. Table 3 shows correct region detection accuracies for optimized number of both the filters and coeffici ents of MFCC and OSCC features. We report the correct detection accuracy for PI-region and V-region, when the frame size is e qual to both beat space and fixed length (30ms). Both OSCC and MFCC perform better when the frame size is beat space. As OS CC outperforms MFCC in general, we use it for modeling acoustic events. Table 3: Correct Average classification of PI and V regions In DB1, we select 4 clips of 30-second music as queries from each belong to V region and other two ma inly belongs to PI region. For a given query, the relevance score between a song and the query is defined as the sum of the similari ty score between the top K most similar indexing vectors and the que ry vector. Typically, we set K to be 30. After computing the smallest note length in the query, we check the tempo/rhythm clusters of the songs in the data base. For song relevance ranking, we only cons ider the songs whose smallest note lengths are in the same range (with  X 30ms tolerance) as the smallest note length of the query or integer multiples of them. Then the surviving songs in the DB1 are ranked according to their respective relevance scores. Figure 12 shows the average accuracy of the correct song retrieval when the query length is varied from 2-sec to 30-sec. Both chord events and acoustic events are considered for constructing n -gram vectors. The average accuracy of correct song retrieval in top choice is around 60% for the query length varies fro 15 ~30-sec. For the similar query lengths, the retrieva l accuracy for top-5 candidates is improved by 20%. In Table 4 we study the chord event effect and the combined effect of chord and acoustic events on the retrieval accuracy. Table 4: Effects of chord and acoustic event information in MIR It can be found that soft-indexi ng outperforms hard-indexing (see Eq.(8), Eq.(9)). In general, co mbining acoustic events and chord events yields a better performance . This can be understood by the fact that similar chord patterns are likely to occur in different songs. The acoustic content helps differentiate one from the other. We have proposed a novel framew ork for MIR. We visualize music information (timing, harm ony and music region contents) in the form of a music structure pyramid. We incorporate timing information in the beat space segmentation of music signal. A two-layer hierarchical chord model has been proposed to describe the harmony events. Content progression of instrumental and vocal regions has also been mode led to describe acoustic events. After modeling layered music information in the vector space, we explored two retrieval models, hard-indexing and soft-indexing. Our experiments show that oc tave scale music information modeling followed by the inter-beat interval proportion segmentation is more efficient than with the fixed length music segmentation. We found the soft-indexing retrieval model is more effective than the hard-indexing one. The fusion of chord model and acoustic model statistics improves retrieval accuracy effectively. The overall experime ntal results convince our focus on layer-wise music processing and vector space retrieval model are promising research directions. We find that music information in different layers complements each other to achieve an improved MIR performance. The robustness in this retrieval modeling framework depends on how well the information is extracted. We will continue to focus on the extraction of uncorrelated music information. Even though music retrieval is the targeted applic ation in this paper, the proposed vector space music modeling framework is useful for developing many other applications such as music summarization, streaming, music structure analysis, and creating multimedia documentary using music semantics. In the future, we will work on extending it to other relevant applications. [1] Berenzweig, A., Logan, B., Ellis, D.P.W., and Whitman, B. A [2] Cavnar, W.B., and Trenkle, J.M. N-Gram-Based Text [3] Chai, W., Vercoe, B. Structure Analysis of Music Signals for [4] Deller, J. R., Hansen, J.H.L., and Proakis, H. J. G. Discrete-[5] Doraisamy, S., and R X ger, S. Robust Polyphonic Music [6] Downie, J.S., and Nelson, M. Evaluating a Simple Approach [7] Duxburg. C, Sandler. M., and Davies. M. A Hybrid Approach [8] Foote, J. Visualizing Music an d Audio Using Self-Similarity. [9] Fujishima, T. Real Time Chord Recognition of Musical Sound: [10] Ghias, A., Logan, J., Chamberlin, D., and Smith, B. C. Query [11] Goldstein, J. L. An Optimum Processor Theory for the Central [12] Kageyama, T., Mochizuki, K., and Takashima, Y. Melody [13] Lemstr X m, K., and Laine, P. Music Information Retrieval [14] Ma, B., and Li, H., A Phonotactic-Semantic Paradigm for [15] Maddage C. N., Xu, C., Kankanhalli, M.S., and Shao, X, [16] McNab, R.J., Smith, L.A., Witten, I.H., Henderson, C.L., and [17] Melucci, M., and Orio, N. Music Information Retrieval using [18] Pickens, J. A Survey of Feature Selec tion Techniques for Music [19] Pickens, J. and Iliopoulos, C. Markov Random Fields and [20] Salton, G. The SMART retrieval system. Prentice-Hall, [21] Shih, H.-H., Narayanan, S. S., and Kuo, C.-C. J. An HMM-[22] Song, J., Bae, S. Y., and Yoon, K. Mid-Level Music Melody [23] Terhardt, E. Pitch, Consonance and Harmony. In JASA, Vol. [24] Typke, R., Wiering, F., and Veltkamp, R. A Survey of Music [25] Uitdenbogerd, A. L., and Zobel, J. An architecture for effective [26] Ward, W. Subjective Music Pitch. In JASA, Vol. 26, 195
