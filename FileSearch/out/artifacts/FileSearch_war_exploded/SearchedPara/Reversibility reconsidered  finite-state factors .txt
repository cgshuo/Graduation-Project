 The objective of Natural Language Understanding (NLU) is to map linguistic utterances to semantic representations, that of Natural Language Genera-tion (NLG) to map semantic representations to lin-guistic utterances. In most of NLP practice, these two objectives are handled by different processes, and computational linguists rarely operate at the intersection of the two subdomains.

For a few years around the early nineties, based both on cognitive, linguistic, and engineering con-siderations, there was a surge of interest in so called reversible grammar approaches to NLP, where one and the same grammatical specification could serve both for parsing utterance x into logi-cal form z , but also for generating x from z (Strza-lkowski, 1994).

We start by a brief review of this historical non-probabilistic notion of reversibility and point out certain of its weaknesses, in particular regarding robustness; we then give in section 3 a new proba-bilistic definition of reversibility; then, in section 4 we argue for a reversibility model based on modu-lar weighted finite-state transducers. We end with a discussion of recent related work. The most direct approaches to NLU attempt to de-sign procedures for semantic parsing that, given an input utterance x , produce a semantic repre-sentation z , by following a number of interme-diate steps where the surface form is gradually transformed into semantic structure. Such  X  X ro-cedural X  approaches to semantic parsing are typ-ically very hard or impossible to invert: start-ing from a semantic representation z , there is no simple process that is able to find an x which, when given to the parser, would produce z . For-mally, a Boolean relation r ( x,z ) can be such that the question ?  X  z r ( x,z ) is decidable for all x  X  X , while the reciprocal question ?  X  x r ( x,z ) is unde-cidable for some z  X  X  (Dymetman, 1991). 1 One of the motivations for the emerging paradigm of uni-fication grammars at the end of the eighties was the clean separation they promised between spec-ifying well-formed linguistic structures, both on the syntactic and semantic levels, through a for-mal description of the relation r ( x,z ) , and pro-ducing efficient implementations of the specifi-cation; in particular, there was much hope that such formalisms would be conductive to effec-tive reversibility (by contrast to variable assign-ment, variable unification is inherently symmetri-cal), that is, to feasible (and if possible efficient) implementations of the parsing problem r ( x, ?) and of the generation problem r (? ,z ) .

To some extent, this hope was validated through a number of works at the time, mostly involving machine translation applications, and constrain-ing in more or less explicit ways the specifica-tion of r (van Noord, 1990). However, for the non-statistical approaches to parsing then strongly dominant, robustness was an issue: a parser had to either accept or reject a given input x , with no in-termediary options, and in order to be able to parse actual utterances, with all their empirical diversity, parsers had to be rather tolerant. In the procedural view of parsing, such robustness issues could of-ten be mitigated through engineering tricks such as ordering the rules from strict to lax, where gram-matical constructions were given preference over less conventional ones; however, when trying to move to reversible grammars, these tricks could not be reproduced: if the grammar was able to parse an x into z , then, by design, it was also able to generate x from z , and there was no obvious way, in these non-probabilistic approaches, to dis-tinguish between producing a linguistically correct x or producing a deviant or incorrect one. In the classical non-probabilistic case, a (relative) consensus existed around the fact that a reversible grammar should be, as we indicated above, a for-mal specification of the relation r ( x,z ) such that the problems r ( x, ?) and r (? ,z ) were effectively solvable.

Transposing this to the probabilistic world, we propose the following semi-formal Definition:
A probabilistic reversible grammar is a for-mal specification of a joint probability distribu-tion p ( x,z ) over logical forms z and utterance strings x such that the conditional distributions
Why such focus on sampling ? We could have chosen other definitions of parsing (and similarly for generation), for instance the ability to re-turn the most probable z given x , i.e. to return argmax z p ( z | x ) ; however sampling is the most di-rect way of providing a concrete view of the un-derlying probabilistic distribution, and has many applications to learning, so we think the definition Finite-state transducers have properties which make them uniquely suited to implementing re-versible linguistic specifications in the above sense. Consider a simple weighted string-to-string transducer  X  ( s,t ) , where s,t are strings, and where the underlying semiring is the  X  X robabilis-tic semiring X  over the nonnegative reals, addition and multiplication having their usual interpreta-tions. Such a transducer preserves regularity , both in the forward (resp. reverse) directions, meaning that the image through  X  of any weighted regular language over s (resp. over t ) is again a weighted regular language over t (resp. over s ). In partic-ular the forward (resp. reverse) image of a fixed string s 0 (resp t 0 ) can be computed in a compact form as a weighted finite-state automaton (FSA) over t (resp. s ), which we can denote by  X  ( s 0 ,  X  ) (resp.  X  (  X  ,t 0 ) ). A weighted FSA can be easily nor-probabilitic FSA exact samplers for the  X  X arser X   X  ( s 0 ,  X  ) and for the  X  X enerator X   X  (  X  ,t 0 )) are di-
In general, some of the properties that make weighted FSAs and FSTs  X  over strings or trees  X  specially relevant for probabilistic models of language are the following: (i) they allow com-pact representations of complex probability distri-butions over linguistic objects (automata) or pairs of linguistic objects (transducers), (ii) they permit efficient exact sampling (and efficient optimiza-tion over derivations (but not always over strings)), (iii) they support modularity: intersection of au-tomata, composition of transducers, projections of Conceptual architecture Armed with these general considerations, let us now propose a con-ceptual architecture based on a small number of finite-state modules, which attempts to satisfy the definition given above for probabilistic reversibil-ity, to address the problem of robustness that we described earlier, and can also support contex-tual preferences. We illustrate the approach with some simple examples of human-machine dia-logues (between a customer and a virtual agent), a domain for which reversibility has high relevance, due to effects such as self-monitoring (Neumann, 1998; Levelt, 1983), interleaving of understand-ing and generation (Otsuka and Purver, 2003), and lexical entrainment (Brennan, 1996). Figure 1: Reversible specification through finite-state factors.
 The conceptual architecture is shown in Figure 1. Formally, the figure represents a probabilistic graphical model in so-called factor form, where the factors are  X , X , X , X  (we have also indicated for future reference the  X  X ontextual X  factors  X , X  , that we ignore for now). The factors take as argu-ments three types of objects: z is a logical form, that is, a structured object which can be naturally represented as a tree, x is a surface string, and y is a latent  X  X nderlying X  string that corresponds to one of a small collection of  X  X anonical X  texts for realizing the logical form z (more about that later).
Each factor is realized through a weighted finite-state machine (acceptor or transducer) over strings or trees (Mohri, 2009; F  X  ul  X  op and Vogler, 2009; Maletti, 2010; Graehl et al., 2008).

The  X  factor is a string automaton that repre-sents a standard ngram language model (typically specific to domain), in other words a probability distribution over utterances x . Symmetrically , the regular tree automaton  X  represents a distribution over logical forms z , which can be seen as play-ing a similar role to the language model, but at the semantic level, namely telling us what are the pos-
The  X  X anonical factor X   X  is a weighted tree-to-string transducer (Graehl et al., 2008), which implements a relation between logical forms z and a small number of latent  X  X anonical X  texts y realizing these logical forms. For example,  X  may associate the logical form (dialog act) z = wad ( batLife , iphone6 )  X  with wad an abbrevi-ation for  X  X hat is the value of this attribute on this device? X , and batLife an abbreviation for  X  X at-tery life X   X , with such a canonical text (among a few others) as: What is the battery life of the Iphone 6? .

The  X  X imilarity factor X   X  is a weighted string-to-string finite state transducer which gives scores to x,y according to a notion of similarity. It has the role of  X  X ridging X  the gap between the actual utterances x and the latent canonical utterances y . The intention behind the similarity factor is to  X  X e-couple X  the task of modeling some possible real-izations of a given logical form from the task of recognizing that a given more or less well-formed input is a variant of such a realization. This fac-tor relates the two strings y and x , where y is a possible canonical utterance in the limited reper-tory produced by  X  , and x is an actual utterance, in particular any utterance that could be produced by a human speaker. So for instance suppose that the user X  X  utterance is x = What about battery du-ration on this Iphone 6? , we would like this x to have a significant similarity with the canonical ut-terance y = What is the battery life of the Iphone 6? but a negligible similarity with another canon-ical utterance such as y 0 = What is the screen size of the Galaxy Trend? .

Overall, the canonical factor  X  ( z,y ) concen-trates more on a core  X  X eneration model X , namely on producing some well-formed output y from a logical form z , while the similarity factor  X  ( y,x ) allows relating an actual user input x to a possi-ble output y of the  X  model. The main import of  X  is then to allow to use the core generation model defined by  X  to be exploited for robust semantic parsing .

Different instantiations of this scheme can be employed. In some preliminary experiments that we have performed, 7  X  is a simple edit-distance transducer (Mohri, 2003) which penalizes differ-ently the discrepancies between x and y : strongly for some salient content words or named entities of the domain, weakly for less relevant content words and for non-content words, with limited use of lo-cal paraphrases (which can also be implemented through  X  ). This strategy seems to work reason-ably well when the semantical repertory of the do-main is restricted, because a large number of pos-sible variants for x are  X  X ttracted X  to the same un-derlying semantics. In domains where small nu-ances of expression may result in distinct seman-tics, the division of work between  X  and  X  may be different.
 Parsing and Generation To understand the re-versibility properties of the model of Figure 1, let us first simplify the description by assuming that z , instead of being a tree, is actually a string. Then both  X  and  X  are string automata, and both  X  and  X  string-to-string transducers. Such a spec-ification satisfies our definition of probabilistic re-versibility, exploiting well-known compositional-ity properties of weighted finite-state machines over strings (Mohri, 2009). For parsing, we start from a fixed x 0 , and can project it through  X  into a weighted FSA over y ; in turn we can project this automaton onto an FSA over z , and finally intersect this automaton with  X  , obtaining a fi-nal weighted  X  x 0 -parser X  automaton over z , rep-resenting a probability distribution from which we eration works in exactly the reverse way, starting from a z 0 and eventually building a  X  z 0 -generator X  automaton over x .

In the actual proposal, z is a tree, meaning that  X  is a tree automaton, and  X  a tree-to-string trans-ducer. While finite-state tree automata correspond to a single concept, and share all the nice proper-ties of string automata (Comon et al., 2007), the situation with tree-to-tree or tree-to-string trans-ducers is more complicated (Maletti, 2010; Graehl et al., 2008): several variants exist, only some of which support the operations that our conceptual model requires (composition with the string trans-ducer  X  and intersection with the tree automaton  X  ). In particular, the  X  X inear non-deleting top-have the requisite properties.
 Contextual factors We now briefly come back to the factors  X  (tree automaton) and  X  (string automaton) of Figure 1, which highlight the use-fulness of our modular finite-state architecture. These factors play similar roles to  X  and  X  , but they evolve dynamically with the context. In dia-logue applications, utterances can often only be in-terpreted by reference to the current dialogue state (e.g.  X  X en hours X  in the context of a question about battery life), and the  X  factor can be used as a com-pact representation of the current expectations of the dialogue manager about the next logical form, to be combined with the actual customer X  X  utter-ance. Symmetrically, the  X  factor can be used to represent such phenomena as lexical entrain-ment (Brennan, 1996), where the agent X  X  utterance is oriented towards using similar wordings to the customer X  X . The unique formal properties of finite-state ma-chines, which favor modular decompositions of complex tasks, have long been exploited in Com-putational Linguistics. Tree transducers in partic-ular have gained popularity in Statistical Machine Translation, starting with (Yamada and Knight, 2001), as described in the surveys (Maletti, 2010; Razmara, 2011).

The reversibility properties of finite-state trans-ducers have been exploited to a more limited ex-tent, starting with applications of non-weighted string-to-string transducers to morphological anal-ysis and generation (Beesley, 1996).

Concerning the application of weighted finite-state tree machines to NLU/NLG reversibility, our proposal is strongly related on the one hand to the approach of (Jones et al., 2012), who ex-plicitely proposes tree-to-string transducers as a tool for modelling semantic parsing and for train-ing on semantically annotated data, and on the other hand to (Wong, 2007; Wong and Mooney, 2007), who focus more directly on the problem of inverting a semantic parser into a generator. Wong et al. do not explicitely use tree-based transducers, but rather a formalism inspired by SCFGs (syn-chronous context-free grammars), which essen-tially corresponds to a form of tree-to-string trans-ducer. In relation to reversibility considerations, presentations in terms of synchronous formalisms have the interest that they are intrinsically sym-metrical. Such formalisms have tight relations to tree-transducers (Shieber, 2004); one recently pro-posed generalization,  X  X nterpreted Regular Tree Grammars X  (Koller and Kuhlmann, 2011), allows multiple (possibly more than two) synchronized views of an underlying abstract derivation tree, and has the advantage of permitting a uniform treatment of strings and trees.

One important aspect in which our proposal dif-fers from these previous approaches is in propos-ing to decouple the  X  X ore X  task of mapping logical forms to well-formed latent canonical realizations from the task of relating these realizations to ac-tual utterances, through an additional  X  X imilarity X  transducer acting as a bridge.

This idea of a bridge is however close to another line of work in semantic parsing, not transducer based, namely (Berant and Liang, 2014; Wang et al., 2015). There, a simple generic grammar is used to generate canonical realizations from a repertory of possible logical forms (expressed in a variant of lambda calculus). Given an input to parse, simple heuristics are used to select a fi-nite list of potential logical forms which are then ranked according to the (paraphrase-based) simi-larity of their associated canonical realization with the input. Thus in this approach, a form of gener-ation plays an important role, not for its own sake, but as a tool for semantic parsing. Because of their unique compositional properties, finite-state modules are a natural choice for imple-menting our definition of reversibility as efficient bidirectional sampling from a common specifica-tion. In this piece we have argued in favor of an architecture realizing this definition and display-ing robustness and contextuality.
 Acknowledgments We thank the anonymous reviewers for their detailed comments and for pointing us to some relevant literature that we had overlooked.

