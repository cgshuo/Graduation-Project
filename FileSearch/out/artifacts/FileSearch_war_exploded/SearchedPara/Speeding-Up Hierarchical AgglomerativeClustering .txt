 In several domains, hierarchical agglomerative clustering algorithms are able to yield best-quality results. However, this class of algorithms is characterized by a high complexity which reduces the size of datasets which can be handled. In the most standard cases, such complexity is O ( dN 2 + N 2 log N ), N being the number of objects in the dataset and d the cost of computing the distance between two objects, which is the result of O ( N 2 ) distance computations followed by O ( N 2 ) selection steps, each having cost O (log N ). In typical settings, d is either a constant or very small w.r.t. log N , so that the algorithm complexity is usually simplified to O ( N 2 log N ).
 such as in the case of high-dimensional data or complex comparison functions, e.g., the edit distance between long strings. In these cases, the computation of all object-to-object distances dominates the overall cost of the clustering process, and so any attempt to improve performances should aim at saving a significant portion of distance computations. To the best of our knowledge, this aspect has not been explicitly studied in literature, yet, despite the fact that it has been marginally mentioned in several works (e.g., most of those described in Section 2.2).
 agglomerative algorithms family, namely the single-and complete-linkage ver-sions, and propose a simple pruning strategy that improves their performances by reducing the number of object-to-object distances to compute without af-fecting the results. A formal proof of its effectiveness under some assumptions will also be given, together with an extensive experimental session to test it on different contexts and conditions. In this section we will provide a short description of the general hierarchical ag-glomerative clustering schema, instantiating it to the two specific cases discussed in this paper. Finally, a brief summary of related work will follow. 2.1 Hierarchical Agglomerative Clustering (HAC) The objective of hierarchical clustering algorithms is to extract a multi-level partitioning of data, i.e., a partitioning which groups data into a set of clus-ters and then, recursively, partitions them into smaller sub-clusters, until some stop criteria are satisfied [3]. Hierarchical algorithms can be divided in two main categories: agglomerative and divisive. Agglomerative algorithms start with sev-eral clusters containing only one object, and iteratively two clusters are chosen and merged to form one larger cluster. The process is repeated until only one large cluster is left, that contains all objects. Divisive algorithms work in the symmetrical way. In this paper we will focus on the former class of algorithms. marized as in Figure 1. As we can notice, there are two key operations in the general schema which still need to be instantiated: the choice of the best couple of clusters, and the computation of the distances between the new cluster and the existing ones. Each different instantiation of these two operations results into a different agglomerative clustering algorithm. In this paper, the cluster selection in step 3 is performed by selecting the closest pair of clusters, while the distance computation in step 7 is performed in two alternative ways: by extracting the distance between the closest pair of objects (excluding couples belonging to the same cluster), which yields a so called Single-linkage algorithm; and by extracting the distance between the farthest pair of objects, which yields a Complete-linkage algorithm. In particular, the complete-linkage algorithm in general produces tightly bound or compact clusters, while the single-link algo-rithm, on the contrary, suffers from a chaining effect , i.e., it has a tendency to produce clusters that are straggly or elongated [3]. 2.2 Related Work Efficiency is a strong issue in hierarchical clustering, and it has been treated in literature in many different ways. In the following we summarize some of the main approaches to the problem.
 HAC problem. For example, [2] introduces a data structure for dynamic clos-est pair retrieval, which is directly applicable to hierarchical clustering, and which is shown to reach a O ( n 2 ) complexity for simple aggregation operators (e.g., maximum, minimum and average). For specific contexts, even faster solu-tions have been proposed, such as a sub-quadratic single-linkage method for low-dimensional data [4], and a O ( n log n ) complete-linkage solution for R d spaces ( d  X  1) with L 1 and L  X  metrics [5]. We remark that these approaches do not take into account the (pragmatic) possibility of having very expensive distance computations, which is exactly the context we will focus on in this paper. When some degree of approximation in the hierarchical clustering structure can be tol-erated, several approximation approaches can be followed, which mainly try to reduce the size of data: from data simple sampling methods to data aggregation solutions, such as (i) grid-based clustering solutions for vectorial datasets [3], and (ii) the data bubbles approach [1], which extends the grid-based approach to non-vectorial data. The basic assumption of our method is that our distance function is a metric. Then, the key idea is that from the exact distances of a limited number of couples it is possible to derive useful approximated values for all object-to-object distances. Such approximations can be easily updated at each iteration of the HAC algorithm, and can be used to effectively limit the number of exact distance computations needed along the whole process. 3.1 Distance Approximations As basic means for estimating unknown distances, we propose to use the tri-angular inequality, a property satisfied by all metrics:  X  a, b, p  X  D : d ( a, b )  X  d ( a, p )+ d ( p, c ), where d is a metric defined over a domain D . With some simple math and exploiting the symmetry property of metrics, we can rewrite it as which we will call pivot , the above formula can be directly used to provide a bounding interval for the distance between any couple ( a, b ) of objects. Hence-forth, we will refer to such bounding intervals as approximated distances or simply approximations . In particular, we notice that if some object a is very close to the pivot, the d ( p, a ) values in (1) will be very small, and therefore the approximation of any distance d ( a, b )from a will be very tight.
 performed at the beginning of HAC algorithms, is replaced by (i) the computa-tion of the | D | exact distances relative to a randomly chosen pivot, and (ii) the approximation of all other distances by following the method outlined above. 3.2 Enhanced Distance Management The method shown in the previous section can be used to provide an initial set of approximations aimed at replacing as much as possible the full matrix of distances. In the following we will describe: (i) how such approximations can be used to save exact distance computations in the couple selection phase (step 3 in Figure 1); (ii) how they can be composed to derive approximations for a newly created cluster (steps 6 X 7); and (iii) how to exploit them also in the on demand computation of exact distances between compound clusters, when they are required in the couple selection phase.
 Enhanced Couple Selection. Both the single-and complete-linkage algo-rithms, at each iteration find the couple of clusters with minimal distance, and merge them. A simple method for searching such couple exploiting the approxi-mated distances, is the following: 1. Select the couple ( a, b ) which has the lowest-bounded approximation; 2. if the approximation is perfect 3. then return ( a, b ); 4. else compute the exact d ( a, b ) and return to step 1; promising candidate couple is performed by means of the known approxima-tions; if the best approximation is perfect, then all other couples certainly have an equal or greater distance, and therefore we can safely choose the selected couple for the merging phase; otherwise, another step is necessary, i.e., the exact distance of the couple needs to be computed and checked to be still the best candidate. The last test is implicitly performed by immediately repeating the selection step. Deriving New Approximations. When two clusters are merged, all distances from the resulting new cluster have to be computed, exact or approximated, so that it can be considered in the next iterations of the selection-merging process. Analogously to the case of exact distances, the approximations for the new clus-ter can be derived by aggregating the already known approximations of the two clusters it originated from. In particular, we exploit a simple property of the max and min aggregation operators, that are used in the single-and complete-linkage HAC algorithms 1 : computed as the minimum of the object-to-object distances between elements provides a straightforward means for approximating all distances d ( c, c )from c , given that we know an approximation for both its components c 1 and c 2 . A completely symmetrical reasoning can be repeated for the complete-linkage algorithm, which makes use of inequality (3).
 Enhanced Distance Computation. In the (enhanced) selection step it is of-ten necessary to compute the exact distance between two clusters. That happens whenever the best candidate couple found is associated with only an approxi-mated distance. The trivial way to do it, consists in computing all distances between each object in the first cluster and each object in the second one and aggregating them with the proper operator (min or max). An obvious drawback distances, which is exactly what we wanted to avoid. A surprisingly effective enhancement can be obtained by exploiting the following simple fact: [ l recursively analyzing their components (i.e., the two sub-clusters they originated from), until we reach simple objects. At each step of the recursion, the above property allows to prune unnecessary distance computations. The process for single-linkage HAC can be summarized as in Figure 2. If the clusters to compare contain single objects, then the algorithm simply computes their distance (step 1), otherwise it breaks down one of the compound clusters into its components (steps 2 X 4), and recursively analyzes them. In the analysis of sub-components, priority is given to the most promising one, i.e., that with the smaller lower bound distance (step 5), to the purpose of maximizing the pruning opportunities offered by Proposition 2. Step 7 implements that by avoiding to compute the distance for the less promising component when it is not strictly necessary.
 be obtained by just modifying the conditions of step 5 and 7 with, respectively, ( u 1 &lt;u 2 ) and ( d 1 &gt;u 2 ), and by replacing min with max in step 9. 3.3 Selecting Pivots As we noticed in Section 3.1, the approximations computed before the clustering process can have variable tightness. In particular, the approximations for objects close to the pivot will be tight, while the others will be looser. A natural extension of the method consists in choosing more than one pivot, so that a larger number of objects will have a pivot near to them, and therefore a larger quantity of approximated distances will result tight. The expected consequence is that the pruning strategies described in the previous sections will be more effective. tance  X  one for each pivot  X  so they need to be composed together in some way. The approximation computed by means of each pivot represents a constraint that the real distance must satisfy. Therefore, the composition of approxima-tions corresponds to the conjunction of the constraints they represent, which is simply implemented by intersecting of the available approximations.
 random choice would be a possible solution, it provides no guarantee on the results. On the contrary, assuming that a dataset is really composed of a number of clusters, an optimal choice for pivots would assign at least one pivot to each cluster. The key idea of our pivot selection heuristics is the following: assuming to have very well defined clusters in our data, each point is expected to be far from the objects of other clusters, at least if compared with the distance from other objects in the same cluster. Therefore, given a set of pivots, we can reasonably search a new good pivot, i.e., a pivot which belongs to an uncovered cluster, among those objects which are far from all existing pivots. These are essentially the same ideas applied in [6], where a similar approach has been used for approximated clustering. Figure 3 shows our pivot selection method. are chosen as mentioned above. In particular, the furthest object from the exist-ing set of pivots is selected, i.e., the object which maximizes the distance from the closest pivot (step 4). This simple algorithm seems to capture reasonably well the cluster structure of data, at least for clean-cut clusters, as indicated by the property proven below.
 Definition 1 (  X  -separateness). Given a set of objects D and a distance d () , D is called  X  -separated if it can be split into at least two clusters, such that the following holds:  X  a, b, a ,b  X  D :if a and b belong to the same cluster while a and b do not, then d ( a ,b ) &gt; X   X  d ( a, b ) .
 ters is at least  X  times larger than the maximum diameter of clusters. Proposition 3. Let D be a 1-separated dataset composed of n clusters, and k  X  n . Then, PivotsSelection( D , k ) returns at least one object from each cluster. In this section we provide some experimental and theoretical evaluations of the performances of the HAC algorithms with enhanced distance management de-scribed in this work.
 4.1 Theoretical Evaluation While any realistic context usually shows some kind of irregularity, such as noise (i.e., objects that do not clearly belong to any cluster) and dispersion (i.e., largely dispersed clusters, possibly without clear boundaries), it is useful to have some theoretical estimation of performances also on ideal datasets: on one hand, it provides at least a comparison reference for empirical studies; on the other hand, it helps to understand where are the weak and strong points of the algorithm analyzed. In this section, we provide one of such theoretical hints. Definition 2 (k-HAC). Given a HAC algorithm and a parameter k ,wedefine the corresponding k -HAC algorithm as its variant which stops the aggregation process when k clusters are obtained. That corresponds to replace step 2 in the general HAC algorithm (Figure 1) with the following: while | C | &gt;k do . provide some a priori lower bound on the number of clusters we are interested in  X  obviously at least 2, but often it is much larger.
 Proposition 4. Given a 3 -separated dataset D with n clusters, and a parameter k  X  n , the execution of an optimized k-HAC algorithm over D with k initial pivots requires O ( N 2 1 +  X  X  X  + N 2 k ) object-to-object distance computations, where ( N i ) i =1 ,...,k are the sizes of the k top level clusters returned by the algorithm. limit the distance computations just to couples within the same cluster. That results in a considerable reduction factor, as stated by the following: Corollary 1. Under the assumptions of Proposition 4, if k = n and the clus-ters in D have balanced sizes (i.e.,  X  i : N i  X  N/k ), then the k-HAC algorithm with enhanced distance computation requires a fraction O (1 /k ) of the distances required by the simple HAC algorithm.
 ing capabilities of the Enhanced Distance Computation algorithm. As the next section will show, in some cases this second component allows to obtain much larger reduction factors. 4.2 Experimental Evaluation In order to study the effectiveness of our pruning heuristics, and to understand which factors can affect it, we performed several experiments over datasets of different nature with corresponding distance functions:  X  2D points: the dataset contains points in the R 2 space, and the standard  X  Trajectories: each element describes the movement of an object in a bi-to 3200 objects) and the single-and complete-linkage versions of a 10-HAC algorithm were applied, with a variable number of pivots (from 4 to 48). Figure 4 depicts the results of our experiments for single-linkage, which are evaluated by means of the ratio between the total number of distance computations required by the basic HAC algorithms and the number of distances computed by their enhanced version. We will refer to such ratio as gain factor , and each value is averaged over 16 runs. Due to space limitations, the results for the complete-linkage algorithm are not reported here, since they are quite similar to the single-linkage case. The interested reader can find them in [8], together with tests on other datasets. We can summarize the results as it follows:  X  For 2D data (Figure 4 left), a very high gain factor is obtained for all settings  X  For trajectory data (Figure 4 right), the gain factor is moderately high, and We summarize our results as follows: with 2D data the gain in running times is slightly negative, because the Euclidean metric is extremely cheap, and then, even though the overhead of our heuristics results to be small, the distances saved cannot compensate it; with other data, instead, the gain in running times is almost identical to the gain factor, since the distances are more complex, and saving even a few of them is enough to balance all the overhead. In this paper we introduced an optimization technique for two popular hier-archical clustering algorithms, and studied its potentialities and its limitations by means of both theoretical and empirical means. Our optimization technique tries to save as many distance computations as possible, which is particularly important for contexts where distances are time-consuming, and we showed that on reasonably dense datasets it is able to achieve good performances. standing more precisely which statistical properties of data influence the per-formances of our pruning heuristics (as suggested in the previous section and confirmed by additional tests in [8], dimensionality is one of them); (ii) to em-pirically evaluate the pruning power of the heuristics over several real world datasets, having different characteristics; and, finally, (iii) to extend the heuris-tics to other variants of HAC and, if possible, to other clustering approaches.
