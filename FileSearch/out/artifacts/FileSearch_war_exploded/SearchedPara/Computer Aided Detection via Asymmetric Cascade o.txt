 This paper describes a novel classification method for com-puter aided detection (CAD) that identifies structures of in-terest from medical images. CAD problems are challenging largely due to the following three characteristics. Typical CAD training data sets are large and extremely unbalanced between positive and negative classes. When searching for descriptive features, researchers often deploy a large set of experimental features, which consequently introduces irrel-evant and redundant features. Finally, a CAD system has to satisfy stringent real-time requirements.

This work is distinguished by three key contributions. The first is a cascade classification approach which is able to tackle all the above difficulties in a unified framework by employing an asymmetric cascade of sparse classifiers each trained to achieve high detection sensitivity and satisfac-tory false positive rates. The second is the incorporation of feature computational costs in a linear program formu-lation that allows the feature selection process to take into account different evaluation costs of various features. The third is a boosting algorithm derived from column genera-tion optimization to effectively solve the proposed cascade linear programs.

We apply the proposed approach to the problem of detect-ing lung nodules from helical multi-slice CT images. Our approach demonstrates superior performance in comparison against support vector machines, linear discriminant analy-sis and cascade AdaBoost. Especially, the resulting detec-tion system is significantly sped up with our approach. I.5.m [ Pattern Recognition ]: Miscellaneous Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00. Algorithms Computer aided detection, Cascading classification, Mathe-matical programming, Support vector machines, Sparse so-lutions
Over the last decade, Computer-Aided Detection (CAD) systems have moved from the sole realm of academic pub-lications, to robust commercial systems that are used by physicians in their clinical practice to help detect early can-cer from medical images. The growth has been fueled by the Food and Drug Administrations (FDA) decision to grant ap-proval in 1998 for a CAD system that detected breast can-cer lesions from mammograms (scanned x-ray images) [19]. Since then a number of CAD systems have received FDA approval. Virtually all these commercial CAD systems fo-cus on detection (or more recently diagnosis [7]) of breast cancer lesions for mammography. The CAD concept can be generalized to many other detection tasks in medical im-age analysis, such as lung nodule detection and colon polyp detection.

The typical workflow for a CAD system when used to identify structures in a new patient image is: 1. Identify candidate structures in the image :Mostmed-ical images, particularly image volumes generated by high-resolution computed tomograp hy (CT), are very large. Typ-ically, an efficient image processing algorithm considers each pixel (or voxel) in the image as a potential candidate  X  X eed X , and selects a fraction of the seeds as candidates. 2. Extract features for each candidate :Alargenumberof image features are usually calculated to describe the target structure. Some of the features can be irrelevant, or redun-dant, or computationally expensive. Thus, sparse feature selection is necessary in order to ensure a relatively small number of relevant features in the deployed CAD system. 3. Classify candidates as positive or negative : A previously-trained classifier is used to label each candidate. 4. Display positive candidates : Commonly, the digitized image is displayed with marks for inspection by physicians.
In the candidate identification stage, even a small fraction of the seeds is necessarily very large in order to maintain high sensitivity. High sensitivity (ideally close to 100%) is essential, because any cancers missed at this stage can never be found by the CAD system. Hence, a very large number of false positives are generated in this stage (typically, fewer than 1% of the candidates are positive), which makes the classification problem highly unbalanced. Moreover, CAD systems must run fast enough so that physicians can use them in the middle of their diagnostic analysis.
Our major contribution in this paper lies in a new cas-caded classification approach that solves a sequence of linear programs, each constructing a hyperplane classifier of the form sign( w x + b )where x is the feature vector and ( w ,b ) are model parameters to be de termined. The linear pro-grams are derived through piece-wise linear cost functions and the 1 -norm regularization condition. The resulting lin-ear program works in the same principle as for the 1-norm SVM. The 1 -norm regularization inherently performs fea-ture selection since penalizing on the 1-norm regularization of w drives the resulting optimal w to be sparse, meaning only a few features receive a non-zero weight w .Toincorpo-rate the feature computational complexity into the selection of features, a weighted 1 -norm is employed where weights are determined by the computational cost of each feature. Each linear program employs an asymmetric error measure that penalizes false negatives and false positives with differ-ent weights. An extreme case is that the penalty for a false negative is infinity, which is used in the early stage of the cascade design to alleviate the skewed class distribution and preserve high detection rates.

Previous cascade classification approaches are mostly based on AdaBoost [10, 20, 11]. Cascade AdaBoost serves as a great tool for building real-time robust applications [22, 24], especially for object detection systems. However, cascade AdaBoost works with two implicit assumptions: 1. a signif-icant amount of representative data is available for training the cascade classifier; 2. all features can be equally evaluated with a relatively low computational cost. These assump-tions, unfortunately, often do not hold in CAD systems. Col-lecting patient data is very expensive and time-consuming. Available data can be noisy and hardly represent all aspects of the target characteristics. One of the major concern about cascade classification approaches is if a classifier within the cascade does not generalize well and hence screens out more true positives than necessary, then these true positives will never be recovered at later stages. The more stages in the cascade, the more likely that the system becomes unstable. This observation motivates us to design a cascade that con-sists of significantly few stages. Furthermore, simple and low-cost image features are often not sufficient for detect-ing target structures in a CAD system. Advanced features are indispensable for performance enhancement, but they require great computation time. If these features need to be calculated for a large portion of the candidates at the early stage of the cascade, the system may become prohibitively slow. Cascade AdaBoost treats all features equally when selecting features for each individual stage classifier, which leads to a computation inefficiency.

Unlike cascade AdaBoost, the proposed approach incor-porates the computational complexity of features into the cascade design. Our cascading strategy brings advantages of multiple folds: 1. Easy classification: the detection prob-Figure 1: Shown in this figure are four sample nod-ules (top row) and four sample false positives (bot-tomrow)fromtheCGstep. lem becomes much more balanced at later stages, facilitating advanced classification algorithms to be applied and perform well at these stages when overall accuracy becomes more de-manding at later stages. 2. High computational efficiency: early stages weed out many non-target patterns, so most stages are not evaluated for a typical negative candidate. Computationally expensive features are only calculated for a small portion of the candidates at later stages. 3. Robust system: the linear program with a 1 -norm regularization at each stage is a robust system. Although no theoretical justification is derived, a cascade of very few stages is un-likely to harm the robustness of linear classifiers, as opposed to a cascade of over 20 stages as often obtained via cascade AdaBoost.
In this article, we focus upon an automatic lung nodule detection system. Lung cancer is the leading cause of cancer-related death in western countries with a mean 5 year sur-vival rate for all stages of only 14%. The prognosis of stage I cancer is more optimistic with a mean 5 year survival rate of about 49%. Although multi-slice CT scanners allow ac-quisition of the entire chest, only 15% of lung cancers are diagnosed at the early stage. The problem is that a single CT examination may acquire up to 700 axial images whose interpretation is tedious and perceptually demanding. CAD is considered to be a helpful diagnostic tool to handle this increasing amount of radiological data. It is well recognized that the use of CAD not only offers the potential to de-crease detection and recognit ion errors as a second reader, but also to reduce mistakes related to misinterpretation [1, 16]. Recently a variety of research has been dedicated to im-provement of automatic nodule detection performance using state-of-the-art machine learning techniques, such as convo-lution neural networks [13], support vector machines [15] or a combined system which uses simple rules to reduce the number of nodule candidates followed by linear discriminant analysis [2].

Our data was collected from multiple sites. The CT vol-umes are typically of size 512  X  512  X  350 (approximately), representing a slice thickness of about 1mm. We conduct a pre-processing step. The region of interest, which is the lung in our problem, is first extracted using segmentation tech-niques, so that all candidates generated will be guaranteed inside the lung. This also facilitates the detection of wall-attached nodules (an example is shown in Figure 1, top-left). The candidate generation (CG) algorithm employs a robust blob detection strategy that identifies all the blob-like struc-tures. The size of the blob-lik e structures vary in diameter starting from 3mm, which is the minimum size of interest for radiologists. The output of the CG step is the locations of candidates, along with some internal features that are generated as intermediate results of the candidate genera-tor. These features include simple gray scale statistics and shape based information of the blob. There are a total of 10 such features output by the candidate generator. The CG algorithm successfully identifies most of the nodules from the CT scans, including some non-obvious nodule examples, as illustrated in Figure 1 (first row): from left to right, wall-attached, elliptical, vessel-attached and very small nodules. However, the CG also generates a huge number of false posi-tive candidates as illustrated in Figure 1 (second row): from left to right, these false positives are due to lymph tissue, tissue scarring, unknown structure and motion.

After the CG step, a large number of image features are computed for each of the candidates to describe the shape (both 2D and 3D), size, intensity statistics (that describe texture), and template matching. The complexity of these features is around linear with respect to the size of the sub-volume where the feature computation takes place. This can be as large as 21  X  21  X  21 voxels, and the amount of computation may become prohibitively large if we need to compute them on every candidate.

Another set of more advanced and computationally de-manding features are calculated as follows. For each de-tected candidate region, the target is segmented by using a nodule segmentation technique developed in [18]. Twenty seven statistical features, computed in a remapped coordi-nate system, are derived from the segmented shape of the candidate. The feature set includes local intensity statis-tics and geometrical features, such as size and anisotropy of the gray values at multiple scales. Furthermore, the intensity homogeneity is represented in a set of entropic measures by using the generalized Jensen-Shannon diver-gence [14]. Jensen-Shannon divergence extends the well-known Kullback-Liebler divergence between a pair of prob-ability distributions in order to describe overall similarity of a set of distributions. Intensity homogeneity can be repre-sented with the Jensen-Shannon divergence by computing overall similarity of a set of intensity-based histograms de-rived from different sub-partitions. This approach allows a flexible extension of the entropy-based intensity homogene-ity index as a function of arbitrary data partition and/or sampling. The computation time of different sets of features are summarized in Table 1.
Cascade classifiers have previously been investigated in such works as [27, 12, 9]. Particularly, in the field of face detection, many classification cascading strategies [25, 21, 23] have shown good performance. Among those methods, the AdaBoost boosting algorithm provides a simple yet ef-fective stagewise learning approach for the cascade design. However, as discussed in Section 2, it has some disadvan-tages.
 (Most Complex) Multi-scale statistics 2010 351 Table 1: Statistics of the computation time of vari-ous features in milliseconds per candidate.

In this section, we investigate a linear programming frame-work for constructing a cascade of sparse linear classifiers w x + b . Each stage of the cascade solves a linear program which is formulated through the hinge loss and the 1 -norm penalty or weighted 1 -norm penalty where  X  i is a weighting factor related to the computational cost of the i -th feature assuming the cost information is available or otherwise it becomes the regular 1 -norm by setting all  X  = 1. Although the linear programs at each stage can be solved using any general-purpose linear pro-gram solver, we show in the next section that the column generation technique for linear programs, is equally suitable for optimizing each linear program in an incremental fash-ion as AdaBoost does. Moreover, the column generation boosting derivation can be applied to any linear program regardless of the choice of the trade-off factor between the detection rate and the false positive rate whereas AdaBoost needs to be revised for an asymmetric re-weighting scheme [25, 23].

The proposed cascade classification strategy provides a general framework for building a cascade, but a concrete cascade design is problem-specific. Prior knowledge or do-main insights may help identify good features to be included in various stages for a specific problem. Users can give high priority to more meaningful and interpretable features to use in early stages. If no such priori information exists, one philosophy is to have a preference for less complex and com-putationally cheaper features.

The cascade hierarchy for our nodule detection system has 3 stages as depicted in Figure 2. The first stage ( C 1 )consid-ers the internal features of the CG algorithm as discussed in Section 3 because these features come together with candi-dates and do not require any extra computational cost. As observed in our experiments, a significant amount of non-nodule candidates are eliminated at this stage. This stage in the classification cascade can also be viewed as an effort to optimize the CG algorithm itself. After the CG step, more advanced features are calculated to improve the over-all accuracy for both false negative and false positive rates. With these features, the second classifier ( C 2 )inthecas-cade often achieves an acceptable performance. However, to further improve from the acceptable performance to an excellent performance, it requires the third set of features which are computationally demanding. Our cascade evalu-Figure 2: Cascade of classifiers for nodule detection ate these features in the last stage ( C 3 ) for the candidates that have survived from the second stage. Bear in mind that each stage can take a single set of features or treat compu-tationally expensive features as an addition to the feature set which is already being used. Our system uses the accu-mulated set of features.
In a cascade, computation time and detection rate of the early stages are critically important to overall performance of the final system. As emphasized already in previous sec-tions, any nodules missed at the first stage can not be re-covered later by the system. Detection sensitivity needs to be extremely high, and often requires to be 100%. In most cascade classification methods, a reasonable classifier is trained at this stage and then the decision threshold is adjusted (manually or in a greedy fashion) to minimize the false negatives. We propose a more principled formulation that guarantees a 100% detection rate as well as optimizes the best possible false positive rate at 100% detection rate.
Denote { x i ,y i } ,i =1 ,  X  X  X  , as our candidate set gener-ated by the CG algorithm. We use X to denote the fea-ture matrix where each row represents a candidate feature vector x and each column specifies a feature, and use i to index the rows (or candidates) and j to index the various features. Notice that the feature vector x realizes different image features at different stages. Without loss of general-ity, we assume that the classification stage receives + pos-itive candidates and  X  non-nodule candidates, X contains d features, and C + and C  X  contain, respectively, the sets of indices of positive sample and negative sample.

Our linear program formulation at each stage seeks an optimal hyperplane classifier by minimizing a weighted sum of the empirical error measure and the regularization fac-tor. The classification error m easure approximated via the hinge loss is generally defined by unbalanced problem, we define the error measure as a con-vex combination of the false negative rate and false positive rate, i.e., where 0  X   X   X  1 is a tuning parameter. The asymmetric cascade classifier can be achieved by choosing an appropri-ate value of  X  close enough to 1. However, this does not guarantee a 100% detection rate at the first stage which is desired for our design. An extreme case of the asymmet-ric error measure is to give a penalty of infinity to a false negative so that the resulting classifier preserves all nodule candidates detected. This asymmetric error cannot be for-mulated as a convex combination of false positive and false negative rates, and it corresponds to imposing the constraint P X ij w j + b  X  0 ,  X  i  X  C + .

To form a linear program, we rewrite w j = u j  X  v j and require u j ,v j  X  0. The linear program is written as the following optimization problem with a regular 1 -norm reg-ularization: s.t.
 where  X &gt; 0 is the regularization parameter. Note that | w j | = u j + v j if either u j or v j has to be 0 so is replaced by gram yields optimal solutions to the formulation directly with variables u j and v j will be zero for all j =1 ,  X  X  X  ,d .
We now derive the dual problem for the above linear program since the dual will play a key role in the column generation derivation for the boosting algorithm which we shall discuss in the next section. There are two variables u ,v j corresponding to a feature X  X  j in problem (2). Cor-respondingly the Lagrangian dual problem has two con-straints for the feature X  X  j , i.e.,  X 
P  X   X   X  as:
The two linear programs (2) and (3) guarantee that all true positives remain to the next cascade stage. With all nodule candidates preserved, they reduce the most possible amount of false positives by minimizing the error 1  X 
In later stages of a cascade, 100% detection rate may not be realistic to maintain in order to attain a reasonably good false positive rate. The convex combination error measure (1) is thus used in the linear programs to allow the presence of false negatives. In later stages, more and more computa-tionally demanding features are included in the training of classifiers. One philosophy we hold is to allow cheaper fea-tures to do their best before reso rting to expensive features, thus leading to a computational efficiency. The weighted -norm regularization is employed to form linear programs where weighting factors  X  are each determined by the com-putational cost of the corresponding feature. Consequently, expensive features will be selected with greater penalty in the objective function of linear programs. The optimization problem can be formulated as the following linear program similarly by rewriting each w j = u j  X  v j : s.t. y i
Analogous to the duality analysis for problem (2), the dual to problem (4) can be derived and written as follows:
Determining an appropriate weight vector  X  based on the feature computational complexity can be problem specific, and can be an interesting topic for further research. In our implementation, we simply normalized the computation time in milliseconds by the Sigmoid function, so  X  ranges from 0.5 to 1.
We describe a column generation technique in this sec-tion. The column generation techniques have been widely used for solving large-scale LPs or difficult integer programs since 1950s [17], and have been introduced to the machine learning community, i.e., the so-called LPBoost [5, 6]. But the LPBoost procedure derived in [5, 6] does not directly solve our formulations. Although our formulations (2) and (4) can be optimized by any linear program solvers. The to-be-derived boosting scheme offers an on-line and incre-mental fashion solution, and provides as well insights into which features play roles during the training phase, facili-tating feature selection.

In the context of column generation, a feature X  X  viewed as a column. During the process, features are con-tinuously selected and classifiers are optimized based on the selected features corresponding to the columns generated. In the primal space, the column generation method solves linear programs on a subset of variables w ,whichmeans not all columns of the matrix X are generated at once and used to construct the classifier. Columns are generated iter-atively and added to the problem to achieve optimality. In the dual space, a column in the primal problem corresponds to a constraint in the dual problem. When a column is not included in the primal, the corresponding constraint does not appear in the dual. If a constraint absent from the dual problem is violated by the solution to the restricted prob-lem, this constraint (a cutting plane) needs to be included in the dual problem to further restrict the dual feasible region. Thus these techniques are also referred to as cutting plane methods [3].

The variables w j are then partitioned into two sets, the working set W used to build the model and the remaining set denoted as N that is eliminated from the model as the corresponding columns are not generated. Each generation step optimizes a subproblem over the working set W of vari-ables and then selects a column from N to add to W .At each iteration, w j (i.e., u j , v j )in N can be interpreted as w j = 0, or accordingly, u j ,v j = 0. Hence once a solu-tion  X  W = u W  X  v W to the restricted problem is obtained,  X   X  =(  X  W  X  N = 0) is feasible to the master linear program (4). The following statement examines when an optimal solution for the master problem is obtained in the column generation procedure.
 Remark 1 (Optimality of Column Generation).
 version of problem (4) with variable b always included in W . The solution is optimal to (4) if and only if for all j  X   X   X 
P To show the optimality is achieved, we need to confirm pri-mal feasibility, dual feasibility and the equality of primal and dual objectives. Recall how we define  X  u =( u W u N =0) and  X  v =( v W v N =0),so(  X  u ,  X  v ,  X   X  ) is feasible for LP (4). Since the solution is optimal to the restricted problems, the primal objective is equal to the dual objective. Now the key issue to evaluate is the dual feasibility. Since  X   X  is opti-mal for the restricted problem, it satisfies all constraints of the restricted dual. Hence the dual feasibility is validated if  X   X   X  proof, we can show a similar optimality condition for linear program (2).
 Any column that violates dual feasibility can be added. A common heuristic is to choose the column X  X  j that max-column X  X  j that solves will be included in the restricted problem. Compared with original LPBoost in [5], our method encloses negations of weak models X  X  j inthehypothesisset. Wesummarizethe column generation steps in Algorithm 1 where e is a vector of ones of appropriate dimension corresponding to the bias term b .

Algorithm 1. Column generation for LP (4) 1. Initialize the first column X 0 = e , 2. For t =1 to T ,do 3. Solve problem (4) with X t  X  1 , 4. Solve problem (6) to obtain  X  , 5. If  X   X   X  + tol , optimal, break from loop, 6. End of loop 7.  X  w = u t  X  v t .

Similarly, column generation for linear program (2) can be derived and is depicted in Algorithm 2.

Algorithm 2. Column generation for LP (2) 1. Initialize the first column X 0 = e , 2. For t =1 to T ,do 3. Solve problem (2) with X t  X  1 , 4. Solve problem to obtain  X  , 5. If  X   X   X  + tol , optimal, break from loop, 6. End of loop 7.  X  w = u t  X  v t .
We validate the cascade linear program (LP) classifica-tion algorithm with respect to its generalization performance and computational efficiency. We compared our cascade LP strategy to a single stage 1-norm SVM model constructed us-ing all features, and the commonly-used cascade AdaBoost. We also compared our approach to a greedy search algo-rithm [26] based on linear discriminant analysis (LDA) in the most recent lung nodule detection system.
A prototype version of our system (not commercially avail-able) was applied on a proprietary de-identified patient data set. The dataset consisted of 176 high-resolution CT images that were randomly partitioned into two groups: a training set of 90 volumes and a test set of 86 volumes. In total, 129 nodules were identified and labeled by radiologists, among which 81 appeared in the training set and 48 in the test set. The training set was then used to optimize the classification parameters, and construct the final classifier which was then tested on the independent test set of 86 volumes.
The candidate generation algorithm was independently applied to the training and test sets, achieving 98.8% de-tection rate on the training set at 121 FPs per volume and 93.6% detection rate on the test set at 161 FPs per vol-ume, resulting in totally 11056 and 13985 candidates in the respective training and test sets. There can exist multiple candidates pointing to one nodule, so 131 and 81 candi-dates were labeled as positive in the training set and test set, respectively. A total of 86 numerical image features were designed (10 of which came from the CG step with no extra computational burden), and used to train the first classifier. The feature set 2 contained size, shape, intensity, template matching features which required on average 15.6 millisec. cpu time per feature per candidate, and was used together with the CG features to learn the second classifier. The multi-scale statistical features depicting sophisticated higher-order properties of nodules comprised the feature set 3 and were used in the final classifier construction together with all other features. These features each on average need 2010 millisec. cpu time for a candidate.
During the training phase, the tuning parameters in the greedy LDA approach and the parameters (  X ,  X  )inourcas-cade LP approach as well as 1-norm SVM were optimized according to the leave-one-patient-out (LOPO) cross vali-dation performance [8]. The LOPO procedure is, in spirit, similar to leave-one-out. The parameter  X  was chosen from choices of { 0.01, 0.1, 1, 10, 100, 1000 } ,and  X  was chosen from a range [0.6, 0.98] with a stepsize 0.02. Parameters at each stage of our cascade LP were tuned to achieve the best LOPO performance. Notice that the first stage of cascade LP does not require  X  . The single 1-norm SVM was tuned to attain the best overall LOPO performance. The choice of  X  = 1 turned out to be the best option for both cas-cade LP and 1-norm SVM on the training data, and  X  was selected as 0.96 for 1-norm SVM, and 0.98 for the second and third stages in our cascade. Figure 3 depicts the 3 Re-ceiver Operator Characteristic (ROC) curves of the LOPO performance for classifiers, respectively, obtained by the 3 algorithms. Cascade LP outpermed the single 1-norm SVM, and dominated greedy LDA at the lower end of false positive rates.

In the experiments with cascade AdaBoost, we largely fol-lowed the procedure described in [24] 1 . Each stage classifier was learned using all available features and after a classifier was obtained, the decision threshold was adjusted to mini-mize false negatives since AdaBoost itself aimed to optimize overall classification accuracy whereas the cascade design requires high detection rates. Cascade AdaBoost does not have hyper-parameters to tune. Instead it requires a vali-dation set. The performance on the validation set is used to determine when to terminate the boosting steps at each stage. Hence other than using LOPO process as used by other approaches, we randomly sampled 30% of the train-ing patient cases and used them as a validation set. Cas-cade AdaBoost also requires a pre-specified target accuracy which we chose as the minimum acceptable detection rate of 88% at 4 false positives per volume. In our experiment, it reached the target accuracy on the validation set at the 4th stage, so 4 classifiers were constructed in the cascade. The overall training and validation performance is reported in Figure 5. We also tried to only include CG features in the first stage of AdaBoost cascade, but it failed reducing a reasonable amount of non-nodule candidates with a full detection rate (i.e., the CG sensitivity).

The four classifiers obtained respectively by cascade LP, 1-norm SVM, greedy LDA and cascade AdaBoost were eval-uated on the unseen test set. The performance is summa-rized in Figure 4. We see cascade LP and cascade AdaBoost generalize equally well. Greedy LDA seems to have overfit as LOPO and test ROC curves have a large gap. The two
The AdaBoost approach [11] used in [24] has been imple-mented by other sources. A MatLab version of the imple-mentation was downloaded from MatLab Statistics web page http://www.mathtools.net/MATLAB/Statistics/ Figure 3: ROC curves show the leave-one-patient-out performance for 3 classifiers on the training data. cascade approaches outpermed the other two methods sig-nificantly with a t-test p -value close to 0.
The numbers of features selected from different feature sets are listed in Table 2 together with the numbers of can-didates remained after corresponding stages or classifiers. Features listed at the columns corresponding to later clas-sifiers are the features selected different from those in pre-vious stages. Notice that the computational cost mainly came from feature evaluation since all the four algorithms adopted linear classifiers which cost ignorable time in com-parison with the time for feature calculation.
 Clearly, cascade LP demonstrates computational efficiency. Its 3 stages together selected 18 features, and only 2 of them were from feature set 3 which were evaluated only for 393 candidates in the last stage. The final system achieved 87.5% versus 0.7 FP rate. The first stage significantly re-duced the false positive rate from 161 to 34.1. Only 3011 candidates left for further evaluation of image features. The 1-norm SVM single model and greedy LDA approach both selected more features from set 3 and they computed these features for all the candidates, resulting in momentously longer running time (14  X  2010 + 12  X  15 . 6 = 28327 millisec. per candidate and totally 3 . 9  X  10 5 sec. on all candidates for  X  X VM X , and 6  X  2010 + 7  X  15 . 6 = 12169 millisec. per can-didate, and totally 1 . 7  X  10 5 sec. for  X  X DA X ) in comparison with a total execution time of 3011  X  8  X  15 . 6+ 393  X  (5 2  X  2010) millisec., and approximately 1986 sec. that the cascaded classifier achieved. Although cascade AdaBoost achieved similar generalization performance, it required a significant greater execution time of 9 . 7  X  10 4 sec.
We have proposed a novel cascade classification approach based on sparse linear programs for computer aided detec-tion systems. Our approach can handle very large training sets and produce an excellent generalization. In addition, it offers the advantage of producing highly sparse hyperplane Figure 4: ROC curves of 4 classifiers on the test data. Figure 5: ROC curves of cascade AdaBoost classifier on the validation set and training set. classifiers. The proposed cascade algorithm is relatively easy to implement since at each stage of the algorithm only a linear program has to be solved. In general, any linear pro-gram solver can be used to optimize the related linear pro-grams. We particularly presented an incremental solver via column generation optimization. The proposed approach prioritizes features with low computational cost to be at the top of the cascade and incorporates the feature compu-tational complexity into the selection of features, resulting into fast CAD systems. Comparisons to other existing lung CAD algorithms on a real dataset consisting of 176 high-resolution CT images illustrate the superiority of the new approach.

Our current approach optimizes hyper-parameters (  X ,  X  ) at each stage to achieve the best performance of that stage. A possible extension of this work is to develop automatic op-timization of hyper parameters of individual stages towards the final system performance. Theoretical examination of the system robustness is also an important extension for further research.
