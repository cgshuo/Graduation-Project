 Discriminati v e methods for training probabilistic models ha v e enjo yed wide popularity in natura l language pro-cessing, such as in part-of-speech tagging (T outano v a et al., 2003), chunking (Sh a and Pereira, 2003), named-entity recognition (Florian et al., 2003; Chieu and Ng, 2003), and most recently parsing (T askar et al., 2004). A discriminati v e proba bilistic model is traine d to maxi-mize the conditional probability p ( y | x ) of output labels y gi v en input v ariables x , as opposed to modeling the joint probability p ( y , x ) , as in generati v e models such as the Nai v e Bayes classifier and hidden Mark o v models. The pop ularity of dis criminati v e models stems from the great fle xibility the y allo w in defining features: because the distrib ution o v er input features p ( x ) is not modeled, it can contain rich, highl y o v erl apping features without making the model intractab le for training and inference.
In NLP , for e xample, useful features include w ord bi-grams and trigrams, prefix es an d suf fix es, membership in domain-specific le xicons, and information from semantic databases such as W ordNet. It is not uncomm on to ha v e hundreds of thousands or e v en m illions of features.
But not all fea tures, e v en ones that are carefully engi-neered, impro v e performance. Adding more features to a model ca n hurt its accurac y on unseen testing data. One well-kno wn reason for this is o v erfitting: a model with more features has more capacity to fit chance re gulari-ties in the training data. In this paper , ho we v er , we focus on anothe r , m ore subtle ef fect : adding ne w features can cause e xisting ones to be underfit . T raining of discrimi-nati v e models, suc h as re gularized logistic re gression, in-v olv es comple x trad e-of fs among weights. A fe w highly-indicati v e features can sw amp the contrib ution of man y indi vidually weak er features, e v en if the weak er features, tak en togethe r , are just as indicati v e of the output. Such a model is less rob ust, for the fe w strong features may be noisy or missing in the test data.

This ef fect w as memorably observ ed by Dean Pomer -leau (1995) when training neural netw o rks to dri v e v ehi-cles autonomously . Pomerleau reports one e xample when the system w as learning to d ri v e on a dirt road: The netw ork had features to detect the sides of the road, and these features were acti v e at training and test time, although weakly , because the dirt road w as dif ficult to detect. But the ditch w as so highly indicati v e that the netw ork did not learn the dependence between the road edge and the desired steering direction.

A natural w ay of a v oiding undertraining is to train sep-arate models for g roups of com peting features X  X n the dri ving e xample, o ne model with the ditch feature s, and one with the side-of-the-road features X  X nd then a v erage them into a single model. This is same idea behind lo g-arithmic opinion pools , used by Smith, Cohn, and Os-borne (2005) to reduce o v erfitting in CRFs. In this pa-per , we tailor our ensemble to reduce undertraining rather than o v erfi tting, and we introduce se v eral ne w combina-tion methods, based on whether the mixture is tak en ad-diti v ely or geometrically , and on a per -sequence or per -transition basis. W e call this general class of methods featur e ba g ging , by analogy to the well-kno wn bagging algorithm for ensemble lear ning.

W e t est these m ethods on co nditional random fields (CRFs) (Laf ferty et al., 2001; Sutton and M cCallum, 2006), which are discriminati v el y-trained undirected models. On t w o natural-language tasks, we sho w that feature bagging performs significantly better than train-ing a single CRF with all a v a ilable features. Conditional r andom fields (CRFs) (Laf ferty et al., 2001; Sutton a nd McCallum, 2006) are undirected graphical models o f a conditional distrib ution. Let G be an undi-rected graphical model o v er random v ectors y and x . As a typical special case , y = { y t } and x = { x t } for t = 1 , . . . , T , so that y is a labeling of an observ ed se-quence x . F or a gi v en c ollection C = {{ y c , x c }} cliques in G , a CRF models the conditional probability of an assignment to labels y gi v en the observ ed v ariables x as: where  X  is a potential function and the pa rtition function Z ( x ) = " o v er all possible label assig nments.

W e assume the potentials f actoriz e according to a set of features { f k } , which are gi v en and fix ed, so that The m odel parameters are a set of real weights  X  = {  X  k one weight for each feature.

Man y applications ha v e used the linear -c hain CRF , in which a first-order Mark o v assumpt ion is made on the hidden v ariables. In this case, the cliques of the condi-tional model are the nodes and edges, so that there are tion. (Here we write the feature functions as potentially Figure 1: Ef fect of a single strong featu re dro wning out weak er feature s in logistic r e gression on synthetic data. The x -axis indicates the strength of the strong f eature. In the top line, the strong featu re is present at training and test time. In the bottom line, the strong featu re is missing from the training data at tes t time. depending on the entire input sequence.) Feature func-tions can be arbitrary . F or e xample, a featu re function f ( y t  X  1 , y t , x , t ) could be a binary test that has v alue 1 if and only if y t  X  1 has the l abel  X  adjective  X , y t has the la bel  X  pr oper noun  X , and x t be gins with a capital lett er .
Linear -chain CRFs correspond to finite st ate machines, and can be roughly understood as conditiona lly-trained hidden Mark o v models (HMMs). This class of CRFs is als o a globally-normalized e xtension to Maximum En-tr opy Mark o v Models (McCallum et al., 2000) that a v oids the label bias problem (Laf f erty et al., 2001).
Note that the number of state sequences is e xponential in the input sequence length T . In linear -chain CRFs, the partition function Z ( x ) , the node mar ginals p ( y i | the V iter bi labeling can be calculated ef ficiently by v ari-ants of the dynamic programming algorithms for HMMs. In the section, we gi v e a simple demonstration of weight undertraining. In a discriminati v e classifier , s uch as a neural netw ork or logistic re gression, a fe w strong features can dro wn out the ef fect of man y i ndi vidually weak er features, e v en if the weak features are just as indicati v e put together . T o demonstrate this ef fect, we present an illustrati v e e xperiment using logistic re gres-sion, bec ause of its strong relation to CRFs. (Linear -chain c onditional random fields are the generalization of logistic re gression to sequen ce data.)
Consider random v ariables x 1 . . . x n , each distrib uted as indepe ndent standard nor mal v ariables. The output y is a binary v ariable whose probability depends on a ll the x i ; specifically , we define its distrib ution as y  X  Bernoulli ( logit ( " in thi s synthetic problem is the h yperplane tangent to the weight v ector (1 , 1 , . . . , 1) . Thus, if n is lar ge, ea ch contrib utes weakly to the output y . Finally , we include a highly indicati v e feature x S =  X  " 0 ,  X  2 = 0 . 04) . This v ariable alone is suf ficient to deter -mine the distrib ution of y . The v ariable  X  is a parameter of the problem that determines ho w strongly indicati v e x
S is; specifically , w hen  X  = 0 , the v ariable x S is ran-dom noise.

W e choose this synthetic model by ana logy to Pomer -leau X  s observ ations. The x i correspond to the side of the road in P omerleau X  s case X  X he weak features present at both testing and training X  X nd x S corresponds to the ditch X  X he strongly indicati v e feature that is corrupted at test time.

W e e xam ine ho w badly the learned classifier is de-graded when x S feature is present at training time b ut missing at test time. F or se v eral v alues o f t he weight pa-rameter  X  , we train a re gularized logistic re gress ion clas-sifier on 1000 instances with n = 10 weak v ariables. In Figure 1, we sho w ho w the amount of error caused by ablating x S at test time v arie s according t o the streng th of x S . Each point in Figure 1 is a v eraged o v er 100 randomly-generated data sets. When x S is weakly in-dicati v e, it does not af fect the predictions of the model at all, and the classifier X  s performance is the same whether it appears at test time or not. Wh en x S becomes strongly indicati v e, ho we v e r , the classifie r learns to depend on it, and performs much more poorly when x S is ablated, e v en though e xactly the same information is a v ailable in the weak features. In this section , we describe the feature bagging method. W e di vid e the set of features F = { f k } into a collec-tion of possibly o v erlapping su bsets F = { F 1 , . . . F which we call featur e ba gs . W e train indi vidual CRFs on each of the feature bags using standard MAP training, yielding indi vidual CRFs { p 1 , . . . p M } .

W e a v erage the indi vidual CRFs into a single com-bined model. This a v eraging ca n be performed in se v eral w ays: we can a v erage pro babilities of entire sequences, or of indi vidual transitions; and we can a v erage using the arithmetic mean, or the geometric mean. This yields four combination methods: 1. P er -sequence mixtur e . The distrib ution o v er label 2. P er -sequence pr oduct of e xperts. These are the lo g-
The pre vious tw o combination methods combine the indi vidual models by a v eraging probabilities of en-tire sequences. Alternati v ely , in a sequence model we can a v erage probabilities of indi vidual t ransitions p ( y t | y t  X  1 , x ) . Computing these transition proba-bilities requires performing probabilistic inferenc e in each of the original CRFs, because p i ( y t | y t  X  1 , x ) = "
This yields tw o other comb ination methods: 3. P er -tr ansition mixtur e . The t ransition probabilities 4. P er -tr ansition pr oduct of e xperts. Finally , we can
Of th ese four combination methods, Met hod 2, the per -sequence product of e xperts, is originally due to Smith et al. (2005). The othe r three combination methods are as f ar as we kno w no v el . In the ne xt se ction, we compare the fo ur combination methods on se v eral sequence label-ing tasks. Although for concreteness we describe them in terms of sequence models, the y may be generalized to arbitrary graphical structu res. W e e v aluate feature bagging on tw o natural language tasks, named ent ity recognition and noun-phrase chunk-ing. W e use the standard CoNLL 2003 English data set, which is tak en from Reuters ne wswi re and consists of a training set of 14987 sentences, a de v elopm ent set of 3466 sentences, and a testing set of 3684 sentences. The named-entity labels in this data set corresponding to peo-ple, locations, or g anizations and other miscellaneous en-tities. Our second task is noun-phrase chunking. W e use the standar d CoNLL 2000 data set, which cons ists of 8936 sentences for training and 2012 sen tences for test-ing, tak en from W all Street Journal articles annotated by the Penn T reebank project. Although the CoNLL 2000 data set is labeled with other chunk types as well, here we use only the NP chunks.

As is standard, we compute precision and recall for both tasks based upon the chunks (or named entities for CoNLL 2003) as W e report the harmonic mean of precis ion and recall as F 1 = ( 2 P R ) / ( P + R ) .

F or both tasks, we use per -seq uence produc t-of-e xperts feature b agging with tw o feature bags which we manu-ally choose based on prior e xperience with the da ta set. F or each e xperiment, we report tw o baseline CRFs, one trained on union of the tw o feature sets, and one trained only on the features that were present in both bags, such as le xi cal identity a nd re gular e xpre ssions. In both data sets, we trained the indi vidual CRFs with a Gaussian prior on parameters with v ari ance  X  2 = 1 0 .

F or the named entity task, we use tw o featur e bags based upon character ngrams and le xicons. Both bags contain a set of baseline features, such as w ord identity and re gular e xpression s (T able 4). The ngram CRF in-cludes binary features for character ngrams of length 2, 3, and 4 and w ord prefix es and suf fix es of length 2, 3, and 4. The le xicon CRF includes member ship features for a v ariety of le xicons containing people names, places, and compan y names. The combined model has 2,342,543 features. The mixture weight  X  is selected u sing the de-v elopment set.

F or the chu nking task, the tw o feature sets are selected based u pon part of speech and le xicons. Ag ain, a set of baseline features are used, similar to the re gular e xpres-sions and w ord i dentity featur es used on the named entity task (T able 4). The first bag also includes part -of-speech tags g enerated by the Brill tagger and the conjunctions of those tag s used by Sha and Pereira (2003). The second bag use s le xicon m embership features for le xicons con-taining names of people, places, and or g anizations. In ad-dition, we use part-of-speech le xicons generated from the entire T reebank, such as a list of all w ords that appear as nouns. These lists are also used by the Brill tagger (Brill, 1994). The combined model uses 536,203 features. The mixture weight  X  is selected using 2-fold cross v alida-tion. The ch osen model had weight 0 . 55 on the le xicon model, and weight 0 . 45 on the ngram model.

In both data sets, the bagged model performs better than th e single CR F trained with all of the features. F or the named entity task, bagging impro v es p erformance from 85 . 45% to 86 . 61% , with a substantial error reduc-tion of 8 . 32% . This is lo wer than the best reported results for this data set, which is 89 . 3% (Ando and Zhang, 2005), using a lar ge amount of unl abeled data. F or t he chunking task, bag ging impro v ed the performance from 94 . 34% to 94 . 77% , with an erro r reduction of 7 . 60% . In both data sets, the imp ro v ement is statis tically significant (McNe-mar X  s test; p &lt; 0 . 01 ).

On the chunking task, the bagged model also outper -forms the models of K udo and Matsumoto (2001) and Sha and P ereira (2003), and equals the currently-best re-sults of (Ando and Zhang, 2005), who use a l ar ge amount of unlabeled data. Although we use le xic ons that were not included in the pre v ious models, the additi onal fea-tures actually do not help the original CRF . Only with feature bagging do these le xic ons impro v e performance.
Finally , we compare the four bagging methods of Sec-tion 4: pre-transition m ixture, pre-transition product of e xperts, and per -sequence mixture. On the named en-tity data, all four models perform in a statistical tie, with no statistically significant dif ference in the ir p erformance (T able 1). As we mentioned in the last section, the de-T able 1: Comparison of v arious bagging metho ds on the CoNLL 2003 Named Entity T a sk.
 T able 2: Results for the CoNLL 20 03 Named Entity T ask. The bagged CRF per forms significantly better than a single CRF with all a v ailable features (McNemar X  s test; p &lt; 0 . 01 ). coding procedure for the per -sequence mixture is approx-imate. It is possible that a d if ferent decoding procedure, such as maximizing the node mar ginals, w ould yield bet-ter performance. In the machine learning lit erature, there is much w ork on ensemble methods such as stacking, boosting, and bag-ging. Generally , the ensemble of classifiers is generated by training on d if ferent subsets of data, rather t han dif-ferent features. Ho we v er , there is some literature within unstructured classified on combinin g m odels trained on feature subsets. Ho (1995) creates an e nsemble of de-cision trees by randomly choosing a fe ature subset on which to gro w each tree using standard decision tree learners. Other w ork along these lines include that of Bay (1998) using nearest-neigh bor classifiers, an d more re-cently Bryll et al (2003). Also, in Breim an X  s w ork on ran-dom forests (2001), ensembles of random decision trees are constructed by choosing a random feature at each node. This literature mostly has the goal of impro ving accurac y by reducing the classifier X  s v ariance, that is, re-ducing o v erfitting.

In contrast, O X  X ulli v an et al. (2000) specifica lly focus on increasing rob us tness by training classifiers to u se all of the a v ailable features. Their algorithm F eatureBoost is analogous to AdaBoost, e xcept that the me ta-learning algorithm main tains weights on feat ures instead of on in-stances. Feature subsets are automatic ally sampled based on which features, if corrupted, w ould most af fect the ensemble X  s pred iction. The y sho w that FeatureBoost is more rob ust than AdaBoost on synthetically corrupted UCI data sets. Their method do es not easily e xtend to se-quence models, esp ecially n atural-language models with hundreds of thousands of feat ures. T able 3: Results for the CoNLL 2000 Chunking T ask. The bagged CRF performs significant ly better than a sin-gle CRF (McNemar  X  s test; p &lt; 0 . 01 ), and equals the re-sults of (Ando and Zhang, 2005), who use a l ar ge amount of unlabeled data. T able 4: Baseline features used in all bags. In the abo v e w t is the w ord at position t , P t is t he POS tag at position t , w ranges o v er all w ords i n the training data, and P ranges o v er al l chunk tags supplied in the training data. The  X  X ppears to be X  features ar e based on ha nd-designed re gular e xpressions.

There is less w ork on ensembles of sequence models, as op posed to un structured classifiers. One e xample is Altun, Hofmann, and Johnson (2003), who describ e a boosting algorithm for sequence models, b ut the y boost instances, not features. In f act, the main adv antage of their technique is increased model sparsenes s, whereas in this w ork we aim to fully use mor e features to increase accurac y and rob ustness.

Most closely rela ted to the presen t w ork is that on log-arithmic opinion pools for CRFs (Smith et al., 2005), which we ha v e called per -sequence mix ture of e xperts in this paper . The pre vious w ork foc uses on reducing o v er -fitting, combining a model of man y featu res with se v eral simpler models. In contrast, here we apply feat ure bag-ging to reduce feature undertraining, combining se v eral models with complementary feature sets. Our current positi v e results are probably not due to reduction in o v er -fitting, for as we ha v e observ ed, all the models we test, including the bagged one, ha v e 99.9% F1 on the train-ing set. No w , feature un dertraining can be vie wed as a type of o v erfitting, because it arises when a set of fea-tures is more indicati v e in the training set than the test-ing set. Understanding this particular t ype of o v erfitting is usef ul, because it moti v ates the choice of feature bags that we e xplore in this w ork. Ind eed, one contrib ution of the present w ork is demonstrating ho w a careful choice of feature bags can yield sta te-of-the-art performance.
Concurrently and independently , Smith and Osborne (2006) prese nt similar e xperiments on the CoNLL-2003 data set, e xamining a per -sequence mix ture of e xperts (that is, a log arithmic opinion pool), in which the le xi-con features are trained separately . Thei r w ork presents more detailed error analysis than we do here, while we present results both on other combination methods and on NP chunking. Discriminati v ely-trained probabilistic models ha v e had much success in applications because of their fle xibil-ity in defining features, b ut somet imes e v en highly-indicati v e f eatures can f ail to increase performance. W e ha v e sho wn that this can be due to feature undertrain-ing, wher e highly-indicati v e features pre v ent training of man y weak er featur es. One solution to this is featu re bag-ging: repeatedly selecting feature subsets, training sepa-rate models on each subset, and a v era ging the indi vidual models.

On lar ge, real-w orld natural-language processing tasks, feature bagging significantly imp ro v es perfor -mance, e v en with only tw o feature subsets. In this w ork, we c hoose the subsets based on our intuition of which features are complementary for this task, b ut automati-cally determining t he feature subsets is an interesting area for future w ork.
 W e thank Andre w Ng, Hanna W allach, Jerod W einman, and Max W el ling for helpful con v ersations. This w ork w as supported in part by the Center for Intel ligent Infor -mation Retrie v al, in part by the Def ense Adv anced Re-search Projects Agenc y (D ARP A), in part by The Cen-tral Intelligence Agenc y , the National Secur ity Agenc y and National Science F oundation under NSF grant #IIS-0326249, and in part by Th e Central Intelligence Agenc y , the National Security Agenc y and National Science F oundation under N SF grant #IIS-0427594. An y opin-ions, findings and conclusions or recommendations e x-pressed in this material are the autho r(s) and do not nec-essarily reflect those of the sponsor . Y asemin Alt un, Thom as Hofmann, and Mark Johnson. 2003. Discriminati v e learning for label sequences via boosting. In Advances in Neur al Information Pr ocess-ing Systems (NIPS*15) .
 Rie Ando and T ong Zhang. 2005. A high-p erformance semi-supervised learning method for te xt chunking. In
Pr oceedings of t he 43r d Annual Meeting of the Asso-ciation for Computational Linguistics ( A CL  X 05) , pages 1 X 9, Ann Arbor , Michig a n, June. Association for Com-putational Linguistics.
 Stephen D. Bay . 1998. Combining nearest neighbor classifiers through multiple feature subsets. In ICML  X 98: Pr oceeding s of the F ifteenth International Con-fer ence on Mac hine Learning , pages 37 X 45. Mor g an Kaufmann Publishers Inc.
 Leo Breiman. 2001. Random forests. Mac hine Learn-ing , 45(1):5 X 32, October .
 Eric Brill. 1994. Some adv ances in transforma tion-based part of speech tagging. In AAAI  X 94: Pr oceedings of the twelfth national confer ence on Artifi cial intelli-g ence (vol. 1) , pages 722 X 727. America n Asso ciation for Artificial Intelligence .
 Robert Bryll, Ricardo Gutierrez-Osuna , and Francis
Quek. 2003. Attrib ute bagging: impro ving accurac y of classifier en sembles by using random feature sub-sets. P attern Reco gnition , 36:1291 X 1302.
 Hai Leong Chieu and H wee T ou Ng. 2003. Named en-tity r ecognition with a maximum entrop y approach. In
W alter Daelemans and Miles O sborne, editors, Pr o-ceedings of CoNLL-2003 , pages 160 X 163. Edmonton, Canada.
 Radu Florian, Abe Ittycheriah, Hongyan Jing, and T ong
Zhang. 2003. Named entity recognition thr ough clas-sifier combination. In Pr oceedings of CoNLL-2003 . G.E. Hinton. 2000. T raining products of e xper ts by mini-mizing contrasti v e di v er gence. T e chnical Report 2000-004, Gatsby Computational Neuroscience Unit.
 T . K. Ho. 1995. Random decision forests. In Pr oc. of the 3r d Int X  X  Confer ence on Document Analysis and
Reco gnition , pages 278 X 28 2, Montreal, Canada, Au-gust.
 T . K u do and Y . Matsumoto. 2001. Ch unking with sup-port v ector ma chines. In Pr oceedings of N AA CL-2001 . J. Laf ferty , A. McCallu m, and F . Pereira. 2001. Con-ditional rando m fields: Prob abilistic models for se g-menting and labeling seque nce data. Pr oc. 18th Int er -national Conf . on Mac hine Lea rning .
 Andre w McCallum, Dayne Freitag, and Fernando
Pereira. 2000. Maxi mum entrop y Mark o v models for information e xtraction and se gmentation. In Pr oc. 17th International Conf . on Mac hine Learning , pages 591 X 598. Mor g an Kaufmann, Sa n Francisco, CA.
 Joseph O X  X u lli v an, John Langford, Ri ch Caruana, and
A vrim Blum. 2000. Featureboost: A meta learning algorithm that impro v es model rob ustness. In Interna-tional Confer ence on Mac hin e Learning .
 Dean Pomerleau. 1995. Neural net w ork vision for robot dri ving. In M. Arbib, editor , The Handbook of Br ain Theory and Neur al Network s .
 Fei Sha and Fernando Pereira. 2003. S hallo w pars-ing with conditional random fields. In Pr oceedings of HL T -N AA CL 2003 . Association for Computational Linguistics.
 Andre w Smi th and Miles Osborne. 2006. Using g azetteers in discriminati v e information e xtraction. In
CoNLL-X, T enth Confe r ence on Computational Natu-r al Langua g e Learning .
 Andre w Smith, T re v or Cohn, and Miles Osborne. 2005.
Log arithmic opinion pools for conditional random fields. In Pr oceedings of the 43r d Annual Meet-ing of the Association for Computational Li nguistics (A CL  X 05) , pages 18 X 25, Ann Arbor , Michig an, Jun e. Association for Computati onal Linguistics.
 Charles Sutton and Andre w McCallum. 2006. An in-troduction to conditional random fields for r elational learning. In Lise Getoor and Ben T askar , edi tors, Intr o-duction to Statistical Relational Learning . MIT Press. T o appear .
 Ben T askar , Dan Klein, M ichael Collins, Daphne K oller , and Chri s Manning. 2004. Max-mar gin parsing. In
Empirical Methods i n Natur al Langua g e P r ocessing (EMNLP04) .
 Kristina T outano v a, Dan Klein, Christopher D. Manning, and Y oram Singer . 2003. Feature-rich part-of-speech tagging with a c yclic dependenc y netw ork. In HL T -
N AA CL 2003 .
