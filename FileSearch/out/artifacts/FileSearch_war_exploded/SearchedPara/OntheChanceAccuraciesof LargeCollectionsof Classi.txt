 Mark Palatucci markmp@cmu.edu Andrew Carlson acarlson@cs.cmu.edu Carnegie Mellon Univ ersit y, Pittsburgh, PA 15213 USA There are man y real world problems in whic h a large num ber of experts predict the outcome of a small num-ber of events. For example, we may ask one hundred football fans to predict the outcome of twenty games, or we may ask fty political pundits to predict the outcome of ten elections.
 With only a small num ber of events to predict, there may be a reasonable chance that some exp ert may pre-dict all the outcomes perfectly , even if the outcomes are chosen at random.
 For example, supp ose we ask a person to predict the outcome of ve coin ips where the probabilit y of ob-taining heads is 0.5. Since the ips are indep enden t, this person has a (0 : 5) 5 = 1 32 chance of guessing the outcome of all ips correctly . Now, if we ask ten people to predict the outcome of the ve ips, there is a much higher chance that some one will predict all outcomes perfectly . With thirt y-two people, someone would (in exp ectation) guess correctly eac h time.
 Supp ose we rep eated this exp erimen t again but ask ed our participan ts to predict the outcome of thirt y coin ips. In this case, the chance of obtaining a perfect prediction would be nearly 1 in 1 billion. Giv en any num ber of participan ts less than 1 billion, we would not exp ect any participan t to perfectly predict all the outcomes. But some participan t will predict a series of outcomes that is most similar to the true ips. How accurate should we expect this particip ant's pre-dictions to be? We consider this question and its relev ance to mac hine learning. In our setting, we consider experts that are not people, but rather classi cation algorithms that predict lab els for a set of examples.
 When a large num ber of classi ers predict lab els for a small num ber of examples, some classi ers will predict the lab els well purely by random chance. This may lead us to believ e that a subset of the classi ers are actually good predictors, when in fact they may be just guessing randomly .
 This e ect is commonly seen in discriminativ e feature selection, where a feature is selected based on the ac-curacy of a classi er trained on that single feature and tested on a held-out set of validation examples. In mo dern high-dimensional mac hine learning appli-cations suc h as fMRI or microarra y analysis, there are typically thousands of features with less than one hun-dred examples. Classi cation tasks in suc h settings of-ten have sparse solutions, meaning that only a small subset of the features are useful for predicting the cor-rect class.
 To determine whic h features are relev ant, it would be useful to kno w how well some classi er could perform if all classi ers just chose lab els at random. We would like to kno w how this accuracy changes with both the num ber of features and num ber of examples. This pa-per poses and answ ers the follo wing question: Given M classi ers that each produc e labels randomly for N examples, what is the highest accuracy that we would expect some classi er to achieve? 1.1. Related Work Our work is closely related to the multiple-testing problem in the statistics comm unit y. In statistics, hy-pothesis tests are the standard way to test if some assertion is true with high probabilit y. While a sin-gle test has a low probabilit y of making an error, when multiple hypothesis tests are performed sim ul-taneously , the probabilit y of at least one of the tests making an error can be much higher. It is common to correct the tests by making them more conserv ativ e to comp ensate.
 Tw o of the most popular metho ds for correcting mul-tiple tests are the Bonferroni Correction and the False Disco very Rate (Benjamini &amp; Hochberg, 1995). We can apply these metho ds to the problem of feature selection, but in practice they are often too conser-vativ e at standard signi cance levels (e.g. 5%). See Wong et al. (2002) and Frank and Witten (1998). With man y high-dimensional classi cation problems they may simply state that no feature is signi can t. This is not particularly helpful when building a classi-er.
 We could lower the signi cance level so more features are considered relev ant, but it is unclear what sig-ni cance level to choose. Since di eren t learning al-gorithms have di eren t tolerances to noisy , irrelev ant features, there is no single signi cance level that is ap-propriate for all learning algorithms.
 This fact, along with the large num ber of available tests and correction metho ds, mak es hypothesis test-ing a dicult task for non-exp erts.
 In our work, we approac h the problem of signi cance from a di eren t angle. Using order statistics , we ex-plicitly mo del small chance events in a group setting. These techniques are relativ ely unkno wn in the ma-chine learning literature although the multiple comp ar-ison procedures describ ed in Jensen (2000) are similar in spirit.
 We feel an order statistic approac h is much more in-tuitiv e than hypothesis testing, and is well suited to problems in mac hine learning.
 One suc h problem is discriminativ e feature selection. This feature selection technique is often called a wrap-per metho d in con trast to more recen t emb edde d meth-ods like the L 1 regularized Lasso (Tibshirani, 1996). While a full comparison of wrapp er and embedded metho ds is beyond the scop e of this pap er, we be-liev e that wrapp ed metho ds will con tinue to play a role in mac hine learning due to their simplicit y and tractabilit y. An excellen t overview of the feature se-lection literature is available in Guy on (2003). The work most similar to ours is by Li and Grosse (2003), whic h uses extreme value distribution theory to choose a signi cance threshold for selecting relev ant features. While the general theme is similar, we do not use asymptotic results of extreme value the-ory , nor do we use sim ulation to compute momen ts of order statistics. By con trast, we focus on classi cation problems and sho w exact solutions that do not require any sim ulation. 2.1. Order Statistics We use order statistics extensiv ely in this pap er, thus we begin with a small introduction to de ne some basic concepts and notation. Consider M samples (i.i.d.) dra wn from some distribution: X 1 ; : : : ; X M F X ( x ). If we order these samples from smallest to largest we obtain: and we use the notation X ( r ) to denote the r th smallest sample whic h we call the r th order statistic. X (1) and X ( M ) have special meaning whic h we call the extr eme values : Eac h order statistic X ( r ) is also a random variable and can be describ ed by a cum ulativ e distribution func-fer to an order statistic's parent distribution , whic h is the original distribution from whic h the M unordered samples were dra wn. In our example this is F X ( x ). We will use the notation r : M to denote the mean of the r th order statistic for M samples dra wn from the paren t distribution. Using order statistics we can now answ er the question we posed earlier: Given M classi ers that each produc e labels randomly for N examples, what is the highest accuracy that we would expect some classi er to achieve? To answ er this question, rst consider a classi er that lab els some collection of examples at random. If the classi er lab els an example incorrectly with probabil-ity p err , we can mo del the num ber of errors the clas-si er mak es as a binomial random variable. Formally , let X be de ned as the num ber of errors the classi er mak es on some true lab eling of N examples. Then: and the mean and variance of X are: Now, supp ose instead we have M indep enden t classi-ers where eac h pro duces a set of N lab els at random. Once again, the probabilit y that eac h classi er mak es a mistak e on a single example is p err . Let X i be the num ber of errors made by the i th classi er. We then have: One of these classi ers will have the minimal num ber of errors. Using our order statistic notation we have: and the exp ected minim um num ber of errors is: If we knew the densit y function of X (1) for M samples from a Binomial ( N; p err ) we could compute the mean If the paren t distribution were a con tinuous variable, obtaining f (1) would not be dicult and man y refer-ences sho w simple metho ds to compute the densit y for any order statistic of a con tinuous distribution (Casella &amp; Berger, 2002). Since our paren t distribution is the discrete binomial, computing f (1) and more imp or-tan tly 1: M is more dicult.
 We could resort to sim ulation to nd the mean, but this can be quite time consuming for large collections of variables. We will sho w later, however, that an exact solution does exist. 3.1. The Multiplicit y Gap For any problem with M classi ers and N examples there is a risk that some classi er will perform well by random chance. What is a good measur e of this risk? As we sho wed earlier, E [ X (1) ] is the minim um num ber of errors that we should exp ect some classi er to mak e. We also kno w that E [ X ] is the exp ected num ber of errors an individual classi er will mak e.
 Thus, one natural measure of this risk is the di erence between these two values. We de ne the multiplicity gap G M;N for M classi ers and N examples as: Reducing the num ber of examples N or increasing the num ber of classi ers M incr eases the risk. Theorem 4.1. Highest Chanc e Accuracy Consider a classi c ation problem with M classi ers and N examples. If the probability that a classi er makes a mistake on a single example is p err , the high-est expected accuracy A H of any classi er is given by: wher e I p ( a; b ) is the incomplete beta function 1 : Proof. Let X i , (1 i M ) be the total num ber of errors classi er i mak es on some true lab eling of N examples. If the probabilit y that a classi er mak es a mistak e on a single example is p err , then: Therefore, the exp ected minim um num ber of errors is: To compute the value of 1: M we utilize a useful result from Feller (1957) that relates the mean of a discrete random variable to its distribution function: therefore A result from David and Nagara ja (2003) sho ws that is equiv alen t to: Now, for a Binomial ( N; p err ), F X ( i ) = 1 when i N . Therefore, the upp er limit of the sum becomes N 1: Note that the incomplete beta function I p ( a; b ) has an expansion that looks similar to the distribution func-tion of a binomial:
I p ( a; b ) = Using this expansion and a few algebraic manipula-tions we can express the tail of the distribution func-tion in terms of the incomplete beta function 2 : 1 F X ( i ) = P ( X i + 1) Substituting this form into (2) we have: To put our answ er in terms of accuracy rather than errors we rearrange: 1
N Note that this theorem dep ends on the num ber of classes only through p err . It does not require any mo d-i cation to adapt to man y classes. Example 4.1 Predicting NFL games Consider an oc e football pool with 200 particip ants betting on the outc ome of 20 games. If each partici-pant sele cts the outc ome of a game according to a fair coin ip, how wel l would we expect the \winner" to perform? To answ er this question, we apply Equation 1 where M = 200, N = 20, and p err = 0.5. In this case, the highest exp ected accuracy of some participan t is 80%. Although the chance probabilit y of obtaining a per-fect lab eling is extremely small, in this case only 1 = 2 20 = 1 = 1 ; 048 ; 576, the chance of obtaining a very good lab eling is much higher. Exactly 1,048,576 partic-ipan ts would be needed for us to exp ect one to obtain a perfect lab eling. Yet, with only 200 participan ts, the exp ected accuracy of the top performer is 80%. This e ect can be seen by plotting Equation 1 for a two class problem where p err = 0 : 5 (see Figure 1). The graph sho ws the highest exp ected chance accu-racy (y-axis) for a given num ber of classi ers (x-axis). Eac h line represen ts a di eren t num ber of examples N . As we increase the num ber of examples, the multiplic-ity gap closes, and highest exp ected chance accuracy for some classi er approac hes the exp ected chance ac-curacy for a single classi er.
 With smal l numb ers of examples and large numb ers of classi ers, the chanc e of obtaining a very good labeling may be very high, even if the chanc e of obtaining a perfe ct labeling is very low .
 A simple and popular metho d for nding relev ant fea-tures in a classi cation task is discriminativ e feature selection. This metho d evaluates how well individual features discriminate between di eren t classes and se-lects features with high predictiv e accuracy . For example, if we have M features in a classi cation task, we train M distinct classi ers, where eac h clas-si er is trained using a single feature. After training, we evaluate all the classi ers on a set of validation examples and select the top performing features ac-cording to some criterion. A nal classi er is then trained using only these top performing features, and then evaluated on some set of test examples.
 This metho d is popular because it is simple to imple-men t and often performs well in practice. The main dicult y is: What are appr opriate criteria for sele ct-ing signi c ant featur es? One approac h is to run a cross-validation loop, test-ing di eren t signi cance thresholds to nd one that has high empirical performance. This loop is compu-tationally exp ensiv e and also requires additional vali-dation examples. To avoid these diculties in practice, it is common to choose some arbitrary threshold, and hop e that performance is sucien t for the classi ca-tion task.
 Besides being pedan tically unsatisfying, choosing an arbitrary threshold in a high-dimensional problem with a small num ber of examples is very risky . For example, a simple threshold migh t choose all features that perform better than 80% accuracy . As we sho wed earlier, man y features may exceed this seemingly high threshold purely by random chance.
 In high-dimensional problems with smal l numb ers of examples, the accuracy requir ed for statistic al signi -canc e is often much higher than intuition might sug-gest.
 A more principled approac h for determining signi -cance is to use a hypothesis test. With a hypothe-sis test, one tries to dispro ve a certain assertion. For example, one migh t assume that a classi er performs with a true accuracy of 50%. This assumption is called the null hyp othesis . The goal then is to reject the null hypothesis if the evidence (e.g. the discriminativ e ac-curacy) is sucien tly strong.
 Hyp othesis testing has a vast literature in the statis-tics comm unit y. A good introduction can be found in Wasserman (2005). The Wald, \t", binomial, per-mutation, and 2 tests are just a few of the possible testing metho ds available. It is dicult, however, for a non-exp ert to kno w when to apply a particular test. To complicate matters, adjustmen ts must be made when multiple tests are considered sim ultaneously . This is kno wn in the statistics comm unit y as the multiple test-ing problem . Sev eral metho ds suc h as the Bonferroni correction, family-wise error rate, and the false disco v-ery rate (FDR) are used to comp ensate for multiple tests (Benjamini &amp; Hochberg, 1995).
 For the problem of discriminativ e feature selection, the use of a binomial test along with a false disco very rate adjustmen t is an appropriate choice. As we men tioned earlier, however, hypothesis tests require the choice of a signi cance level . As is common in the scien ti c literature, the level = 0 : 05 is typically considered statistically signi can t.
 For the purp ose of feature selection, however, an ap-propriate choice of is highly dep enden t on the classi-cation algorithm used. Some classi ers are more tol-eran t to irrelev ant features than others. Thus, there is no single value appropriate for all classi ers. We could use a cross-v alidation loop to searc h for an ap-propriate , but then we could have avoided the hy-pothesis test altogether and searc hed empirically for an appropriate threshold. 5.1. The Multiplicit y Gap Midp oint (MGM) Earlier in Equation 1 we deriv ed the highest exp ected chance accuracy of some classi er assuming all classi-ers choose their lab els according to random chance. In some sense, this accuracy is a natur al signi c anc e threshold , since we would not exp ect any classifer to perform better than this threshold by random chance. While this may seem like an intuitiv e threshold for feature selection, in practice the threshold is overly conserv ativ e for sev eral reasons. First, this threshold assumes all features are indep enden t. This rarely holds in practice, and in man y high-dimensional datasets it is very common to see strong correlations between fea-tures.
 Further, the threshold assumes that all features are irrelev ant and pro duce lab els at random. In practice, some subset of the features will actually be signi can t, thereb y lowering the e ectiv e num ber of random fea-tures. There is also no guaran tee that errors for a fea-ture can be mo deled as a binomial random variable. These violations of indep endence and irrelev ance ef-fectiv ely lower the highest exp ected chance accuracy (and increase the exp ected minim um num ber of er-rors). While this threshold may be overly conserv ativ e, it e ectiv ely serv es as an upp er bound on the highest exp ected chance accuracy .
 At the other extreme, we migh t consider any fea-ture signi can t that performs better than the exp ected chance accuracy of a single feature. As we sho wed be-fore, this will clearly allo w man y irrelev ant features to be considered signi can t. Note that these two ex-tremes are the endp oints of the multiplicit y gap that we de ned earlier. If we mo del the num ber of errors made by a classi er as a binomial random variable, and we have M classi ers and N examples then the multiplicit y gap G M;N = E [ X ] E [ X (1) ] We conje ctur e that the optimal threshold should fall within the multiplicity gap .
 In practice, we can choose any threshold between these two extremes. If we believ e that our classi er is sensi-tive to irrelev ant features, we should choose a thresh-old closer to E [ X (1) ]. Similarly , if our classi er is ro-bust to irrelev ant features, we should choose a thresh-old closer to E [ X ].
 Without any kno wledge of the particular classi er it is imp ossible to kno w what the optimal threshold should be. Therefore, as a simple heuristic we prop ose the multiplicity gap midp oint metho d, whic h chooses the midp oint of the extremes of the multiplicit y gap. This yields a threshold MGM on the maxim um num ber of errors a classi er could mak e and still be considered signi can t: where E [ X (1) ] is computed as in Equation 1: and E [ X ] is the num ber of examples N multiplied by the probabilit y p err that a classi er mak es an error on a particular example: E [ X ] = N p err To use this threshold, we perform a discriminativ e fea-ture selection and select all features that mak e less than MGM errors on a validation set with N exam-ples. 5.2. Exp erimen tal Metho dology We perform discriminativ e feature selection exp er-imen ts on two high-dimensional classi cation tasks that have few relev ant features and limited training data: Task 1: Cognitiv e state classi cation us-ing functional magnetic resonance imaging (fMRI) In this task, we are given a time series of neural activit y from thirteen human sub jects. Eac h feature is the neuro-activ ation of a particular region of the brain at a given time. The goal is to distinguish between two cognitiv e states: reading a sen tence, and viewing a picture (Mitc hell et al., 2004). Eac h sub ject has 80,000 features and 40 examples.
 Task 2: Colon cancer patien t classi cation us-ing microarra y gene expression levels (Cancer) In this task, the goal is to predict whether a patien t is diagnosed with colon cancer. The data are microarra y gene expression levels from tissue samples (Alon et al., 1999). There are 2,000 features and 62 examples. Testing Metho d In eac h exp erimen t, we use a Gaus-sian Naiv e Bayes classi er and perform a leave-one-out-cross-v alidation. On eac h round, we leave out one example, and split the remaining examples into equal training and validation sets. We train using the rst set, and measure classi cation accuracy on the vali-dation set. We select the best performing features ac-cording to a speci c criterion. After selecting features, we retrain by com bining the validation and training sets. We then test the left out example. We rep eat the pro cess for eac h example.
 We tested ve di eren t feature selection criteria: 1. No feature selection Uses all features. 2. Highest Exp ected Chance Accuracy Selects 3. Binomial Hyp othesis Test with False Dis-4. Multiplicit y Gap Midp oint (MGM) 5. Oracle Threshold This is the threshold that 5.3. Results and Discussion In Figure 2, we see the classi cation results of ve discriminativ e feature selection metho ds for both the colon cancer and fMRI datasets (for the fMRI dataset, we averaged the results of the 13 sub jects together). In both datasets, the threshold E [ X (1) ] yields an im-pro vemen t over no feature selection. But the assump-tions made in calculating that threshold, namely that all features are indep enden t and irrelev ant, result in a very conserv ativ e threshold whic h admits few features. The multiplicit y gap midp oint (MGM) metho d relaxes these assumptions and performs signi can tly better. This metho d comes closest to the accuracy that could have been achiev ed had an oracle told us the optimal threshold to use 4 .
 As a state-of-the-art baseline, we tried a binomial hy-pothesis test with a false disco very rate correction. As is common in the statistical and scien ti c literature, we chose a signi cance level = 0.05. This metho d completely failed to select any features for the fMRI task, indicating that it is overly conserv ativ e for very high-dimensional problems. The metho d performed fairly well on the colon cancer dataset, but did so af-ter selecting few er than ten features.
 It is worth noting that we could tune the value of the false disco very rate test to admit more features and help performance. But the goal of the midp oint heuristic is to avoid this tuning (in fact, if we were to do tuning, it would mak e more sense to just tune the threshold for selecting features directly). Thus we feel the midp oint metho d pro vides a more appropriate default threshold than a speci c value of would in a classical test.
 We chose the Gaussian Naiv e Bayes classi er because it is extremely fast to train and test making it very appropriate for use in a wrapp ed featur e sele ctor . This classi er is also robust to noise but is not entirely im-mune to over tting. We found that adding additional features increased performance up to a point, but even-tually noisy features overwhelmed the classi er, and performance degraded.
 Figure 3 sho ws this e ect for three fMRI sub jects and the colon cancer dataset. The curv es sho ws test accu-racies at various feature selection thresholds. In eac h plot, the x-axis is the num ber of errors allo wed, and the y-axis is the test accuracy of the resulting clas-si er. We mark the extremes of the multiplicit y gap E [ X (1) ] and E [ X ] on eac h plot. On all thirteen sub-jects as well as the colon cancer dataset, the optimal (oracle chosen) threshold falls within this gap. 5.4. Future Work The goal of this pap er was to sho w how order statistics can be a useful tool for problems in mac hine learning. While our initial work focused on accuracy , we feel similar techniques can be applied to other measures suc h as information gain, entrop y, and AUC. Also, in our initial analysis we compute a signi cance threshold assuming that all features are indep enden t. One natural extension of this work is to dev elop a metho d that adjusts for correlations between features. We pro vided a theoretical analysis of the chance accu-racy of large collections of classi ers. We sho wed that on problems with small num bers of examples and large num bers of features, we should exp ect some classi er to be highly accurate by random chance. We deriv ed a theorem to directly calculate this accuracy . We used this theorem to pro vide a principled feature selection criterion for sparse, high-dimensional prob-lems. This criterion is theoretically well-motiv ated, simple to implemen t, and computationally inexp en-sive.
 We demonstrated this metho d on microarra y and fMRI datasets and sho wed that this metho d per-forms very close to the optimal oracle accuracy . We also sho wed that on the fMRI dataset this technique chooses relev ant features while another state-of-the-art metho d, the False Disco very Rate (FDR), completely fails at standard signi cance levels.
 We would like to o er our tremendous thanks to Haik ady Nagara ja, Larry Wasserman, Tom Mitc hell, and Geo Gordon for their useful commen ts. We would also like to ackno wledge the National Science Foundation, W.M Kec k Foundation, and Yaho o! for their generous nancial supp ort.

