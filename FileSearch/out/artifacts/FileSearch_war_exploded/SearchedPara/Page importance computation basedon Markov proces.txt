 Abstract This paper is concerned with Markov processes for computing page impor-tance. Page importance is a key factor in Web search. Many algorithms such as PageRank using different data sources, and with different assumptions. Then a question arises, as to whether these algorithms can be explained in a unified way, and whether there is a general guideline to design new algorithms for new scenarios. In order to answer these questions, we introduce a General Markov Framework in this paper. Under the framework, a Web Markov Skeleton Process is used to model the random walk conducted by the web surfer average value that the page gives to the surfer in a single visit. These two factors can be computed as the stationary probability distribution of the corresponding embedded Markov chain and the mean staying time on each page of the Web Markov Skeleton Process respectively. We show that this general framework can cover many existing algorithms including PageRank, TrustRank, and BrowseRank as its special cases. We also show that the framework can help us design new algorithms to handle more complex problems, by constructing graphs from new data sources, employing new family members of the Web Markov Skeleton Process, and using new methods to estimate these two factors. In par-ticular, we demonstrate the use of the framework with the exploitation of a new process, named Mirror Semi-Markov Process . In the new process, the staying time on a page, as a random variable, is assumed to be dependent on both the current page and its inlink pages. Our experimental results on both the user browsing graph and the mobile web graph validate that the Mirror Semi-Markov Process is more effective than previous models in several tasks, even when there are web spams and when the assumption on preferential attachment does not hold.
 Keywords Page importance PageRank BrowseRank Web Markov skeleton process Mirror semi-Markov process 1 Introduction Page importance is a critical factor in web search, since it plays a key role in crawling of Web pages, indexing of the crawled pages, and ranking of the indexed pages. Many effective algorithms have been proposed to compute page importance in the literature, such as PageRank (Brin and Page 1998 ; Page et al. 1998 ) and TrustRank (Gyongyi et al. 2004 ).
PageRank and its variations are usually modeled as a discrete-time Markov process on the web link graph for page importance computation, the process of which actually sim-ulates a random walk of a surfer along the hyperlinks on the web. By applying the above algorithms, people have solved many critical problems in Web search. However, these algorithms also have certain limitations as in the modeling for representing page impor-tance. For example, PageRank only models a random walk on the graph, but does not consider the lengths of time that the web surfer spends on the pages during the browsing process. The staying time information can be used as good indicators of the page quality, which is highly related to the importance of the pages.

To solve this problem, BrowseRank (Liu et al. 2008 ) was proposed to leverage the user staying time information on webpages for page importance computation. It collects the user behavior data in web surfing and builds a user browsing graph, which contains both user transition information and user staying time information. A continuous-time Markov process is employed in BrowseRank to model the browsing behaviors of a web surfer, and the stationary distribution of the process is regarded as the page importance scores.
However, there are still challenges that cannot be well handled by the aforementioned algorithms. Here, we give two examples.  X  In some new scenarios, the assumptions in existing algorithms may not hold. For example,  X  In some existing applications, the assumptions in existing algorithms may not be
With these challenges lying ahead (not limited to the ones mentioned above), it may be necessary to develop new technologies to address the problems in both existing and new scenarios. For this purpose, it is helpful to study whether there is a common theory behind designing new algorithms. This is just the motivation of this paper.

Considering that most previous works are based on random walks on a graph, and employ Markov processes in their mathematical modeling, we propose using a general Markov framework as the unified description of these algorithms. In the framework, we consider how to model page importance from the viewpoint of random surfer, assuming importance of a page means the value that the page can eventually provide to the random surfer. It can be considered that there are two factors that affect page importance: page reachability and page utility . The former represents the possibility that the surfer arrives at the page and the latter represents the value of the page given to the surfer in a visit. Web Markov Skeleton Processes can represent these two factors with the stationary probability distribution of their embedded Markov chain (EMC) 2 and the mean staying time. Spe-cifically, the larger the stationary distribution of the EMC of a page is, the higher reach-ability the page has; the longer mean staying time a page retains, the higher utility the page provides. We can then take the product of stationary distribution of the EMC and the mean staying time as the page importance. In many cases, it can be proved that the product is proportional to the stationary distribution of the Web Markov Skeleton Process itself, if it exists.

Existing algorithms such as PageRank and BrowseRank can be well covered by the framework. For example, PageRank is based on the Discrete-Time Markov Process which is a special Web Markov Skeleton Process. BrowseRank is based on the Continuous-Time Markov Process , another special Web Markov Skeleton Process.

Furthermore, the general framework also provides us a guideline of designing new algorithms. We can attain new methods by defining the graph on new data sources, employing new family members of the Web Markov Skeleton Process, or developing new methods to estimate the stationary distribution of the skeleton process and the mean staying time. To demonstrate the use of this framework, we propose employing a new process, which we call Mirror Semi-Markov Process . In the new process, the staying time on one page depends on not only this page but also the previous pages visited by the surfer. By doing so, we can address the aforementioned issues that existing algorithms suffer from. As for the mobile web graph, if we penalize the estimated staying time on a page when the transition is from the same website or a partner website, then the corresponding page utility will be decreased and the problem with the profit-oriented attachment will be tackled. And in the scenario of BrowseRank, if we assume that the distribution of staying time on one page varies when the transitions are from different websites, and perform some normali-zation among different source websites, then the calculated page importance will more accurately reflect the underlying truth even if there is spam or click fraud in the data.
We tested the Mirror Semi-Markov Process and the corresponding algorithms on both the mobile web graph and the user browsing graph. The experimental results show that the new algorithms can outperform existing methods in several tasks such as top ranked page finding and spam/junk page filtering. This well validates the effectiveness of the proposed framework.

To sum up, the proposed general framework for page importance computation has the following characteristics:  X  It can accurately represent the importance of web pages.  X  It can well explain existing models and even include existing models as special cases.  X  It has solid theoretical background.  X  It can effectively guide the development of new algorithms. Such algorithms can deal The rest of the paper is organized as follows. Section 2 makes a survey on related work. Section 3 describes the general Markov framework built on the Web Markov Skeleton Process. Section 4 explains how PageRank is generalized to BrowseRank. Section 5 explains how BrowseRank is generalized to BrowseRank Plus, in which the Mirror Semi-Markov Process method is introduced. Section 6 makes more discussions on the Mirror Semi-Markov Process, while Sect. 7 makes more discussions on the general Markov framework. The experimental results are reported in Sect. 8 . Section 9 makes discussions on the differences between the proposed framework and several other link analysis frameworks. Conclusion and future work are given in Sect. 10 . 2 Related work PageRank (Brin and Page 1998 ; Page et al. 1998 ) and its variants (Boldi et al. 2005 ; Haveliwala 1999 ; Haveliwala and Kamvar 2003 ; Haveliwala et al. 2003 ; Haveliwala 2002 ; Langville and Meyer 2004 ; McSherry 2005 ; Richardson and Domingos 2002 ) compute page importance by taking the web as a graph of pages connected with hyperlinks. The approach makes an assumption that the web link graph is used for the random surfer to carry out a random walk. A Discrete-Time Markov Process is employed to model the Other work such as (Bianchini et al. 2005 ; Kleinberg 1998 ) is also based on the random walk on a web link graph.
 As PageRank can be easily spammed by tricks like link farms (Gyongyi and Garcia-Molina 2004 ), some robust algorithms against link spam have been proposed. For instance, TrustRank (Gyongyi et al. 2004 ) takes into consideration the reliability of webpages when calculating the page importance. In this approach, a set of reliable pages are identified as seed pages at first. Then the trust scores of the seed pages are propagated to other pages along links on the web graph. As the propagation starts from the reliable pages, TrustRank can be more immune to spam than PageRank.

BrowseRank (Liu et al. 2008 ) is a recently proposed method, which computes the page from the web browsing history of users. The browsing graph contains richer information such as staying times on web pages by users. BrowseRank assumes that the more fre-quently users click on a page and the longer time the users stay on it, the more likely the page is important. A Continuous-Time Markov Process is employed to model the random walk on the user browsing graph. BrowseRank then takes the stationary distribution of the process as page importance. Other work such as (Berberich et al. 2004 ; Yu et al. 2005 ) also exploits time information to compute page importance. In the T-Rank algorithm (Berberich et al. 2004 ), freshness was defined to represent the timestamps of most recent updates of pages and links, and then freshness and its update rate were used to adjust the random walk and the resulting Markov chain in PageRank computation. In the Timed-PageRank algorithm (Yu et al. 2005 ), the inlinks of a Web page are assigned with different weights according to their creation time. A decaying rate was defined so that the latest links would get the highest weights. In another word, link information from different snapshots of graphs is compressed in one web graph, and then the PageRank algorithm is imple-weighted PageRank algorithm, which can also be covered by our proposed framework. 3 General framework We first consider key factors for computing page importance, and then we introduce the Web Markov Skeleton Process and describe our framework of using it to represent the factors. 3.1 Page importance factors We assume that there is a web surfer performing a random walk on the web graph. By random walk here we mean a trajectory consisting of successive random steps on the nodes of a graph. The web graph can be web link graph or user browsing graph. In the former, each node in the graph represents a web page, and each edge represents a hyperlink between two pages. In the latter, each node in the graph stands for a web page, and each edge stands for a transition between pages. The transition information can be obtained by random surfer is a persona combining the characteristics of all the web users 3 .
The importance of a page can be viewed as the average value that the page provides to the random surfers during the surfing process. Note that a visit of a page by a single surfer is random, and the value which a page can offer to the surfer in one visit is also random.
Therefore, there are two intuitive factors that can affect page importance.  X  Page reachability: the (average) possibility that a surfer arrives at the page.  X  Page utility: the (average) value that the page gives to a surfer in a single visit.
Page reachability is mainly determined by the structure of graph. For example, in a web link graph, if a page has a large number of inlinks, it is likely to be more frequently visited. pages the surfer visited before. That is, page utility may depend on not only the current page but also other related pages. 3.2 Web Markov Skeleton process Intuitively, a Web Markov Skeleton Process (WMSP) is a stochastic process which con-tains a Markov chain as its skeleton. It has been proposed and studied in probability theory (Hou and Liu 2005 ; Hou et al. 1998 ) and applied to many fields including queueing theory and reliability engineering. 3.2.1 An intuitive definition A WMSP is a stochastic process Z defined as follows. Note that we try to provide an intuitive definition here. A more rigorous definition can be found in Sect. 3.2.2 . Suppose that X is a Markov Chain with state space S and transition probability matrix P . Let X n = 0, 1, ... . Then, a Markov Skeleton Process Z is a Stochastic Process based on X and Y . A sequence of Z can be represented as on X n -1 and Y n depends on multiple states S n  X  n  X  1 ; 2 ; ...  X  .

Many existing stochastic processes are special cases of WMSP. We just list a few of them as examples.  X  Discrete-Time Markov Process: when Y n is constant, then Z is called a Discrete-Time  X  Continuous-Time Markov Process: when Y n only depends on X n following an  X  Semi-Markov Process: when Y n only depends on X n and X n ? 1 according to distribution
Furthermore, Mirror Semi-Markov Process, proposed in Sect. 5.1.1 , is also a special case of MSP.  X  Mirror Semi-Markov Process: when Y n only depends on X n and X n -1 according to 3.2.2 A rigorous definition in mathematics After an intuitive explanation, we would like to give a rigorous definition on WMSP. Definition 1 A stochastic process Z = { Z ( t ), t C 0} with life time f is called a Markov s  X  0 ; s 1 \ \ s n \ s n  X  1 \ \ f ; lim
In probability theory, MSPs have been intensively studied and applied to queuing 1998 ). Note that in (Hou and Liu 2005 ; Hou et al. 1998 ) the definition of MSP is slightly more narrow than ours, where it was required (among others) that for each n C 0, the Definition 1.1.1).

Let Z be a Markov Skeleton Process. We denote by X n  X  Z  X  s n  X  , and Y n  X  s n  X  1 s n ; for sequence of positive random variables. Clearly the pair of ( X , Y ) is uniquely determined by by ( X , Y ). Specifically, suppose that we are given a Markov chain X = { X n , n C 0} and a sequence of positive random variables Y = { Y n , n C 0}. Then a MSP Z satisfying Z ( t ) = X ( s n ) for s n t \ s n  X  1 can be uniquely determined by the following regime.
More precisely, a MSP Z with the above specified property can be uniquely determined by the pair of ( X , Y ) by setting s n  X  (X,Y) with the above regime. Further we introduce the following definition.
 Definition 2 Let Z = ( X , Y ) be a Markov Skeleton Process described by the regime ( 1 ). Then X is called the Embedded Markov Chain (EMC) of Z .

For our purpose of studying the random walk on web link graph or user browsing graph, we shall focus on a special class of Markov skeleton processes defined below. Definition 3 Let Z = ( X , Y ) be a Markov Skeleton Process on S determined by the regime ( 1 ). Suppose that: (i) the state space S is finite or discrete, (ii) the embedded Markov chain X = { X n , n C 0} is time-homogeneous, (iii) given the information of X , the positive random variables f Y n g n 0 are conditionally independent to each other. Then the process Z is called a Web Markov Skeleton Process (WMSP).

Note that when S is finite or discrete, then X = { X n , n C 0} is a Markov chain if and and the time-homogeneity of X means that the probability of single-step transition is Note also that the conditional independency of f Y n g n 0 given X means that Y = { Y n , n  X  0 ; 1 ; ... . In various applications, we may assume further that for each n , there exists a subset S n of f X k g k 0 such that P  X  Y n t jf X k g k 0  X  X  P  X  Y n t j S n  X  .
Let Z = ( X , Y ) be a WMSP defined as above, then intuitively X n denotes a state and Y n denotes staying time at state X n for each n . By the above explanation, X n depends on X n -1 , known stochastic processes used in studying web page importance (this is why we call it Web MSP). Below we just list a few of them as examples.  X  Example 1 . Discrete-Time Markov Process: a WMSP Z = ( X , Y ) such that Y n is  X  Example 2 . Continuous-Time Markov Process: a WMSP Z = ( X , Y ) such that Y n  X  Example 3 . Semi-Markov Process: a WMSP Z = ( X , Y ) such that Y n depends only on
Furthermore, Mirror Semi-Markov Process, which will be proposed later in this paper, is also a special case of WMSP. Below we list only its short description, for more details see Sect. 5.1.1 .  X  Example 4 . Mirror Semi-Markov Process: a WMSP Z = ( X , Y ) such that Y n depends only
Note that WMSP is different from the high-order Markov process. In general, in a high-there is no process Y . For example, in a two-order Markov chain, X n ? 2 is determined by the not related to the high-order Markov process. 3.3 Modeling page importance Since WMSP can naturally model the random walk of web surfer, in the rest of this paper we shall consider only the framework of WMSP. Let Z = ( X , Y ) be a WMSP on a state space S . In our model the state space S corresponds to Web pages, and process Z represents a random walk on pages. The surfer randomly chooses the next page to visit based on the current page, according to X . He/she further randomly decides the length of staying time on the current page based on the page, and several other pages he/she visited before, and/or several other pages he/she will visit, according to Y .
 Therefore, the aforementioned two factors are characterized by the two quantities in WMSP. Specifically,  X  Stationary distribution of X (which exists under some conditions) represents page  X  Mean staying time represents page utility.

Furthermore, given a page, we can define page importance as the product of the value of the stationary distribution of X on the page and the mean staying time on it. Note that there might be other forms for page importance besides the product defined above. The reason probability distribution of the Web Markov Skeleton Process if it exists.

WMSP is a very general model, which does not make any specific assumption on the distribution P  X  Y n t j S n  X  . That is, staying time can be assumed to depend on a large number of other states. Therefore, there is a large room for people to develop new algorithms based on this class of general processes.

Note that the proposed framework is mainly intended for entity importance computation in graphs. Though link analysis algorithms like PageRank can be used for different pur-graphs, we focus on entity importance computation and the general Markov framework in this paper. 4 From PageRank to BrowseRank The proposed framework is suitable for web page importance computation and is theo-algorithms. 4.1 PageRank and its variations In PageRank, a Discrete-Time Markov Process is used to model the random walk on WMSP.

According to Definition 2, we see that the embedded Markov chain X of a Dis-crete-Time Markov Process is equivalent to the Discrete-Time Markov chain Z .Under some conditions, EMC X has a unique stationary distribution denoted by ~ p , which satisfies
In PageRank, page reachability is computed by the above stationary distribution ~ p , and page utility is set to one for all pages under the assumption that all pages are equally useful for the random surfer, which is indicated by P ( Y n = 1) = 1.

As TrustRank is a modified PageRank by starting the iterative process from a reliable seed set, it can also be covered by the framework. Similarly, other PageRank-alike approaches such as (Nie et al. 2005 ; Poblete et al. 2008 ) can also be regarded as special cases of the framework. 4.2 BrowseRank In BrowseRank, the random walk is defined on the user browsing graph, and a Continuous-special WMSP,
Denote T j for the random variable of staying time on page j , and as Y n is a random variable following an exponential distribution P  X  Y n t jf X n g X  , we can get, Here k j is the parameter for the exponential distribution, F T j  X  t  X  is the cumulative probability distribution of random variable T j . The mean staying time on page j is calculated as, By Definition 2, we see that X is the embedded Markov chain of the Continuous-Time Markov Process, and its unique stationary distribution ~ p can also be calculated as ( 2 ).
In BrowseRank, it is equivalent to defining page reachability as the stationary proba-bility distribution of the Embedded Markov Chain of the Continuous-Time Markov Pro-cess, and defining page utility as the mean staying time on the page. According to Sect. 3.3 , we can define the following distribution to compute the BrowseRank scores, using ~ p and ~ T , where a is a normalized coefficient. probability distribution of the Continuous-Time Markov Process. 5 From BrowseRank to BrowseRank plus The framework can also help devise new algorithms. Particularly the dependency of staying time on multiple states can be leveraged to address issues that existing algorithms cannot cope with. Here we take the extension from BrowseRank to BrowseRank Plus 4 as an example to show how to design the computing mechanism under the guide of the framework.

As discussed in Sect. 1 , spam is pervasive on the web and how to effectively eliminate it is an issue we must consider when calculating page importance, especially for algorithms like BrowseRank that are based on user browsing graph. As discussed before, BrowseRank builds a Continuous-Time Markov Process model based on user browsing graph. Click fraud is the biggest challenge for BrowseRank. The reason is that BrowseRank fully trusts the user behavior data, and calculates the mean staying time directly from user behavior data. This leaves a big room for web spammers to conduct click fraud by manually or automatically clicking the pages they want to boost from their websites. As a result, we will observe a large volume of transitions to the spam pages from their websites. A straightforward application of BrowseRank will result in a heavily biased page importance calculation.

To solve the problem, we may treat the transitions from different inlink websites by different weights. If a big portion of transitions are from a specific inlink website, we may downgrade the weight of each of these transitions, so as to reduce the influence from click fraud. However, in this situation, the Continuous-Time Markov Process would not work well for it does distinguish from which websites the transitions come. Therefore, we need to design a new and specific model of WMSP, which is suitable to deal with the above task. We call the new model of WMSP as Mirror Semi-Markov Process (MSMP), and refer to the new algorithm derived from it as BrowseRank Plus. The major advantage of BrowseRank Plus is that it can deal with click fraud, which the original BrowseRank algorithm may suffer from. Note that the idea of generalizing BrowseRank to BrowseRank Plus is somehow similar to SimRank(Jeh and Widom 2002 ). 5.1 Mirror Semi-Markov Process method To better explain the BrowseRank Plus algorithm, we define the Mirror Semi-Markov Process and introduce an implementation of it for page importance computation. 5.1.1 MSMP model Mirror Semi-Markov Process (MSMP) is a new stochastic process that can be used in not only web search but also other applications.
 Definition 4 (cf. Example 4 of Sect. 3.2 ) A Web Markov Skeleton Process Z = ( X , Y )is called a Mirror Semi-Markov Process (MSMP), if Y n depends only on X n and X n -1 , and the X ; X n 1 g X  X  P  X  Y 1 t jf X 1 ; X 0 g X  for all n C 1.
 Obviously MSMP is a special case of Web Markov Skeleton Process. Furthermore, MSMP is similar to the Semi-Markov Process (see Example 3 of Sect. 3.2 ). In a Semi-MSMP Y n depends on the current state X n and the previous state X n -1 . The dependencies are in two opposite directions. That is why we call the new model Mirror Semi-Markov Process.

Let X be the Embedded Markov Chain of a MSMP Z , as defined in Sect. 3.2 . In what follows we assume that the state space S is finite, which represents all Web pages. We use X . Assume further that there exists a unique stationary distribution of process X , which will be denoted again by ~ p : Then ~ p satisfies also the ( 2 ), and can be calculated by the power method (Golub and Loan 1996 ).

As in Sect. 4.2 , we use j to represent a state  X  j 2 S  X  and use T j to represent staying time on state j . By Definition 4 we can get the following equation: staying time on state j from state i , and use F T j  X  t  X  to represent the the cumulative prob-ability distribution of staying time on state j . It is easy to get the following result: For easy of reference, we have the following definition of Contribution Probability. state i to state j .

Owing to the time-homogeneity of EMC X , we can find that contribution probability c ij varies with two successive states i and j no matter which time of the jump from i to j occurred.

Based on the definition of contribution probability and cumulative probability distri-bution of staying time T j , we can calculate the mean staying time on state j as follows.
We further calculate the following distribution, which is defined as the page impor-tance score as mentioned before, by using of the stationary distribution ~ p of EMC X and mean staying time ~ T j , where a is a normalized coefficient to make equation true. 5.1.2 Implementation of MSMP As explained above, we can apply MSMP to page importance computation. Given a web graph and its metadata, we build an MSMP model on the graph. We first estimate the stationary distribution of EMC X . We next compute the mean staying time using the metadata. Finally, we calculate the product of the stationary distribution of the EMC and the mean staying time on pages, which we regard as page importance. As the stationary distribution of the EMC can be conveniently computed by power method (Golub and Loan 1996 ), we will focus on the staying time calculation in the next subsection.

From ( 8 ), we obtain that the mean staying time is determined by two parts: contribution probability, and cumulative probability distribution from previous state. Hereafter, we analyze these two quantities in details. 5.1.2.1 Contribution probability Suppose that for page j there are n j pages linked to it: N that the surfer comes from page i when given the condition that he/she has come to page j , then we can easily obtain the following proposition.
 Proposition 1 Suppose c ij is the contribution probability from i to j, then (i) (ii) matrix of process X, and ~ p is the stationary distribution of process X .
 Proof (i) It is easy to get the following deduction: (ii) First, due to the time-homogeneity of process X , we get Second, process X has stationary distribution, that is, its limiting probability distribution exists, then, we calculate limit on the above equation,
From ( 11 ), we can easily calculate the contribution probability. In this paper, we use another heuristic method to compute such probability as a demonstration. Because in this paper, we just want to show the impact from different websites on the calculation of page importance.

Suppose that the n j inlinks of page j are from m j websites, and from website k  X  k  X  1 ; ... ; m j  X  there are n jk inlinks. Thus we have, Note that the website which page j belongs to might also exist in the m j websites.
Suppose that the m j sites that linked to page j are: U j  X f / j 1 ; / j 2 ; ... ; / jm probability of site.
In this work, we assume that the contribution probability of different webpages belong to the same website are identical, that is, 5.1.2.2 Cumulative probability distribution With the same reason as before, here we assume that the cumulative probability distribution from different webpages belong to the we have 8 i ; k 2 S ,if i ; l 2 / k ; k  X  1 ; 2 ; ... ; m , We use the denotation / from website / k .

By contrast with the other members of WMSP, without loss of generality, we further assume that the staying time T j follows an exponential distribution in which the parameter is related to both page j and inlink website / k , That is, staying time depends on page j and the website of inlink, not the inlink page itself. 5.1.2.3 Summary Based on the above analysis, hereafter, we use contribution probability of site and cumulative probability distribution from site to calculate the mean staying time, websites,
The major idea here is to assume that the staying time (utility) of a page is conditioned on the website of inlink page of it and calculate the mean staying time based on inlink sites . Moreover, we can change the mean staying time through the changes of two quantities: c / and k jk .

Intuitively, the utility of web page does change according to the previous websites visited by the surfer. In the link graph case, if the surfer comes to the current page from a website with high utility (authority, quality, etc), then the utility of the page will also be frequency, then the utility of the page will also be high.

The question then becomes how to calculate the contribution probabilities from dif-ferent sites c / observations of staying times, we can estimate the parameters k jk ; k  X  1 ; ... ; m . In other cases (insufficient observations or web link graph), we can employ heuristics to calculate mean staying times. For contribution probabilities c / calculate them. We will discuss more about these in specific applications in Sect. 5.2 .
Table 1 gives the detailed steps for creating the Mirror Semi-Markov Process model. 5.2 BrowseRank Plus After introducing MSMP, we explain how to design the BrowseRank Plus algorithm to deal with click fraud in page importance computation. BrowseRank Plus tackled the problem of click fraud by using MSMP. In this algorithm, the mean staying time of a page is computed based on its inlinked websites. Specifically, the samples of observed staying time are distinguished according to different inlink websites by estimating different method used in BrowseRank. Furthermore, in BrowseRank Plus, the contribution proba-bility c / j are: U j  X f / j 1 ; / j 2 ; ... ; / jm That is, we assume that the contributions from different inlink sites are all equal. Finally, different inlink websites will mostly differ from each other.

The main ingredient of BrowseRank Plus is the website normalization ( 18 ). We explain its effect through an example. Suppose that there are a large number of inlink sites to the page on the user browsing graph, that is, the sites from which users have made transitions spammer can cheat BrowseRank-like algorithms in order to maximize their spamming effect). (b) However, the visits usually come from a very limited number of websites. (This is because otherwise it will be very costly for the spammer). In BrowseRank Plus we make effective use of the fact (b) and normalize contributions from different sites. Note that this is only one simple and example way to calculate the mean staying time. One can certainly consider more advanced ways for performing the task. Note that the key point is to control the contributions from different sites, and MSMP provides a framework to do it in a principled approach.
 Proposition 2 If parameters k jk ; k  X  1 ; ... ; m are all set to the same value, then BrowseRank Plus will degenerate to BrowseRank .
 Proof If we have k j 1  X  k j 2  X  X  k jm , k j , then from ( 17 ) and ( 10 ) we obtain, ~ T is exactly the mean staying time used in BrowseRank. h 6 Beyond BrowseRank plus: further discussions on MSMP In this section, we skip out from the scope of BrowseRank Plus, and make more discus-sions on MSMP. First, we give a mathematical proof on the stationary distribution of MSMP, to make the new process more self-contained. Second, we provide another sce-nario of using MSMP to design page importance computation algorithm for Mobile web graph. Third, we give a brief analysis on the computational complexity of MSMP based algorithms. 6.1 Proof on stationary distribution of MSMP As mentioned in Sect. 3.3 , we use the product of stationary distribution of EMC X and the mean staying time on pages to model the page importance. The main reason is that in many cases it can be proved that such product is proportional to the limiting probability distri-bution of MSMP, if it exist. In this section, we will give the detailed explanation.
Generally, the limiting probability distribution of MSMP will not exist, except in some special conditions. We introduce them in the following lemma. Before it, we give some denotations here.

Let T jj be the period between two successive departures from state j in MSMP. That means that the random surfer walks a circle from state j to state j during the period. All the circlings follow the Staggered Renewal Process (Papoulis and Pillai 2001 ). Let E ( T jj ) stands for the mean time of the circling on state j .
 Lemma 1 Suppose Z is a Mirror Semi-Markov Process, if the transition probability matrix P of the embedded Markov chain X is irreducible, and probability distribution tribution of Z exists, and by applying the Central Limit Theorem, we have , which is independent of the initial state i of MSMP .

The lemma ensures the existence of the limiting probability distribution of MSMP, we Theorem 1 Suppose MSMP is defined on a finite state space S. If the Embedded Markov Chain X is ergodic, which implies both the limiting probability and the stationary distri-bution (denote as ~ p ) exist, and they are identical, i.e ., lim have , time on state j during the previous n transitions. According to the lemma, we get that p  X  lim Thus we obtain ( 21 ). h bution of the EMC and the mean staying time is proportional to the limiting probability distribution of MSMP. 6.2 MobileRank In this subsection, we discuss more about the MSMP model in specific applications, and design another algorithm whin this model for mobile web search.

Suppose that mobile graph data is available. Mobile graph contains web pages con-nected with hyperlinks and is for mobile phone accesses. Mobile web differs largely from general web in many aspects. For example, the topology of mobile web graph is signifi-cantly dissimilar from that of general web graph (Jindal et al. 2008 ). This is because the owners of websites on mobile web tend to create hyperlinks only to their own pages or pages of their business partners. As a result, there are more disconnected components in the connection. In such case, the scores computed by algorithms like PageRank may not reflect the true importance of the pages.

We propose a new algorithm called MobileRank 6 for computing page importance on mobile web using MSMP. We actually consider a new way of calculating the mean staying time.

Note that in MSMP implementation we assume that staying time depends on not only represent relation between websites and to utilize the information for promoting or demoting staying time (utility) of page. Specifically, if the inlink is from a partner website, then we can demote the staying time of visits from the website.

We define the contribution probability c / ( 18 )). We heuristically calculate the parameter k jk .

Suppose that for page j there is an observed mean staying time 1 k follow a partnership-based discounting function L jk ,
The discounting function can have different forms for different business relations between websites. For example, we use a Reciprocal Discounting function in this paper, where c denotes coefficient.

Therefore, we can calculate the mean staying time in MobileRank as below,
From ( 24 ) we can see that: (a) the larger number of inlink websites (i.e., m j ), the smaller penalty on the mean staying time; (b) given a fixed number of inlink websites (i.e., m j ), the larger number of inlink pages from the k th website (i.e., n jk ), the larger penalty on the mean staying time by visits from the website.

Note that this is only one simple and example way of coping with the partnership problem on mobile web. One can certainly think about other ways of doing it.
 Proposition 3 If parameters k j ; j 2 S are all set to the same value, and the discounting function is written as , then MobileRank will be reduced to PageRank .
 Proof Substituting ( 25 ) into ( 22 ), and ( 22 ) into ( 17 ), we obtain importance only depends on the stationary distribution of the EMC. Therefore, it becomes equivalent to PageRank. h 6.3 Computational complexity page reachability, estimation of page utility, and combination of page importance. The calculation of page reachability is a PageRank-alike power method, and thus its complexity is O ( n 2 ), where n is the number of pages. Considering the transition matrix is usually very sparse, the actual complexity might be much lower than O ( n 2 ). For the estimation of page utility, as k jk can be computed in parallel on different pages, we only need to check ( 17 ). usually a much smaller number than n . The combination of page importance is run by ( 2 ), from which we can see the complexity is O ( n ). Therefore, the overall computational complexity of the proposed general framework is comparable with the PageRank algo-rithm, and thus it can scale up for importance computation on large graphs. 7 Beyond MSMP: further discussions on WMSP In the previous sections, after introducing the general framework, we have made stepwise generalization following the line of PageRank, BrowseRank, BrowseRank Plus, and we have also discussed other related algorithms like MobileRank. In this section, we will stand at a high level and overseas all the discussed algorithms and Markov processes.
Figure 1 shows the relationship among the Markov processes and the corresponding page importance computation algorithms derived from them. In the figure, an solid directed edge from a to b means that a contains b as its special case, while a dashed directed edge from a to c means that c is an algorithm derived from a .
 For the processes we can see that, (i) the Semi-Markov Process and the Mirror Semi-Markov Process are special cases of the Web Markov Skeleton Process, (ii) the Contin-uous-Time Markov Process is a special case of both the Semi-Markov Process and the Mirror Semi-Markov Process, and (iii) the Discrete-Time Markov Process is a special case of the Continuous-Time Markov Process.

For the algorithms we can see that, (i) PageRank, TrustRank, and PopRank are derived from the Discrete-Time Markov Process, (ii) BrowseRank is derived from the Continuous-Time Markov Process, and (iii) BrowseRank Plus and MobileRank are derived from the Mirror Semi-Markov Process.

To give a detailed overview of the Markov processes and their related page importance computation algorithms, we organized them in Table 2 . It summarizes the conditional probability of y n and distributions of y n on different states of S n .  X  For Discrete-Time Markov Process, the distributions of y n on different states of S n is  X  For the other three cases of conditional probability of y n , if the distributions of y n on 8 Experimental results experiments to test the performances of the proposed algorithms (BrowseRank Plus and tering. Two datasets were used in the experiments, with the first one being user browsing behavior data and the second one being mobile web data. 8.1 Ranking on user browsing graph 8.1.1 Dataset and baselines We used a user behavior dataset from a commercial search engine for the experiments. All much as possible. There are in total over 3-billion records and 950-million unique URLs.
We first constructed the website-level user browsing graph as described in (Liu et al. 2008 ), by merging web pages in the same website, ignoring the transitions between the pages within the same website, and aggregating the transitions from (or to) the pages in the same website. This user browsing graph consists of 5.6-million vertices and 53-million edges. We then ran BrowseRank Plus and BrowseRank on this graph. We also obtained a link graph containing the 5.6-million websites from the commercial search engine, and computed PageRank and TrustRank from it as baselines.
 For PageRank, we used the uniform teleport vector. For TrustRank, we first ran inverse PageRank on the link graph and selected the top 2000 websites as seed candidates. Then we manually checked the 2000 sites and removed spam, mirror, and porn sites from them, resulting in a seed set with about 1700 reliable websites. TrustRank was computed by propagating trust scores from this seed set. For BrowseRank, we referred to the same setting as in (Liu et al. 2008 ). 8.1.2 Top-20 Websites Table 3 shows the top-20 websites ranked by the different algorithms. For ease of refer-ence, we use PR, TR, BR, and BR ? to denote PageRank, TrustRank, BrowseRank, and BrowseRank, respectively. We have the following observations from this table.  X  X R ? gives high ranking scores to Web 2.0 websites (marked in bold), such as  X  X R ? can better demote the websites with high local transitions than BR. For example, 8.1.3 Spam filtering From the 5.6 million websites, we randomly sampled 10,000 websites and asked human labelers to make spam judgments on them. In the labeling process, the pure advertisement 2,714 websites are labeled as spam and the rest 7,286 websites are labeled as non-spam.
To compare the performances of the algorithms, we draw the spam bucket distribution, which is similar to the setting in (Gyongyi et al. 2004 ; Liu et al. 2008 ). Specifically, for each algorithm, the 5.6-million websites are sorted in the descending order of the scores statistics of the spam websites over the fifteen buckets for different algorithms are sum-marized in Table 4 .

We can have the following observations: (i) TR performs better than PR, for TR considers the trustiness scores calculated from the human labels and the link graph. (ii) Among all the algorithms, BR ? pushes the largest number of spam websites to the tail buckets. (Though BR has a bit more spam websites than BR ? in the last bucket, BR ? still outperforms BR in the total spam numbers of the last 6,5,4,3,2 buckets.) For example, we have observed that in the data http://www.latestlotteryresults.com is ranked high by BR but very low by BR ? . Data analysis shows that 26% transitions to this website come from one single website which contains an advertisement of http://www.latest-lotteryresults.com on its homepage 7 . The data implies that the webmaster of the website seems to have purposely clicked the advertisement on the homepage. By website normalization, BR ? can effec-tively decrease the influence of this click fraud. 8.2 Ranking on mobile web graph 8.2.1 Dataset and baselines The mobile web graph we used in the experiments is provided by a Chinese mobile search engine. The graph crawling was done in October 2008. There are about 80% Chinese webpages and 20% non-Chinese webpages in the graph. The numbers of webpages and hyperlinks are about 158-million and 816-million respectively. By our statistical study, the basic properties of this graph are similar to those reported in (Jindal et al. 2008 ). We computed PageRank (denoted by PR) and MobileRank (denoted by MR) on this graph. 8.2.2 Top-10 websites We list the top-10 webpages ranked by the algorithms in Table 5 . We conducted a user study and found that MR performs better than PR. In Table 5 , wap.cnscu.cn hits the 2nd the homepage of the website has 78,331 inlinks from 829 other websites. The top 5 inlink websites contribute 8,720, 5,688, 3,764, 3,557, 2,920 inlinks respectively. That is, the top 5 contribute 31.5% inlinks. By MR, the effect from possible business models between wap.cnscu.cn and the above contributors on page importance is decreased. Another example is wap.motuu.com , which is ranked 11 by PR and 474 by MR. The homepage of the website has 12,327 inlinks only from 79 websites. Its top 5 contributors bring 3,350, 2,450, 1,662, 1,541, 1,144 inlinks respectively, occupying 82.3% of the 12,327 inlinks. MR was able to successfully decrease the effect from these websites and give it a low ranking.
 8.2.3 Junk filtering We selected 1,500 pages from the mobile web graph in a random manner, and asked human labelers to make judgment on whether they are junk pages or not. Finally, 441 pages are labeled as junk and the rest are regarded as non-junk pages.

Similar to the spam filtering experiments in Sect. 8.1.3 , we again use the bucket dis-tribution to compare the performance of the algorithms. For each algorithm, we sort the 158-million pages in the descending order of their scores, and then put the pages into ten buckets. Table 6 summarizes the statistics of the junk pages over buckets.
 We can see that MR performs better than PR in demoting junk pages to the tail buckets. For example, in the data a junk page (advertisement page) has a large number of inlinks to it. It appears that the links were added by a robot only from several forum sites . This falls into a very typical pattern, because the number of large forum sites which the spammers can use is limited. For MR (refer to formula ( 24 )), given a fixed number of inlink websites, than PR. 9 Discussion ( 2001 ) combined the concepts of PageRank and HITS into a unified framework from the viewpoint of matrix computation. Chen et al. ( 2002 ) extended HITS to analyze both hyperlinks embedded in the webpages and the interactions of the users with the webpages. They considered the two factors in a unified framework, and performed link analysis on hyperlink-click graph, and ran a random walk on the combined graph. It fused multiple types of information to compute a more reliable random walk. The works described above are all very different from the proposed framework in the paper. Some of them (Ding et al. 2001 ) stand in the point of view of matrix computation, and the others (Chen et al. 2002 ; Poblete et al. 2008 ) focus on multiple signal fusion. The proposed general Markov framework is based on Markov processes and explains the factors of page importance computation. It can cover (Poblete et al. 2008 ) as a special case.
 10 Conclusions and future work In this paper, we have proposed a general Markov framework for page importance com-putation. In this framework, the Web Markov Skeleton Process is employed to model the random walk by the web surfer. Page importance is assumed to consist of two factors: page reachability and page utility. These two factors can be respectively computed as transition probabilities between webpages and the mean staying times of webpages in the Web Markov Skeleton Process. The proposed framework can cover many existing algorithms as its special cases, and can also provide us with a powerful tool in designing new algorithms. We showcase its advantage by developing a new Web Markov Skeleton Process called Mirror Semi-Markov Process and applying it to two application tasks: anti-spam on general web and page importance calculation on mobile web. Our experimental results indicate that the framework allows modeling different notions of page importance, based on which we can obtain different ranked lists. However, a more thorough experimental evaluation will be necessary to draw final conclusions on the practical importance for IR systems in web and mobile search.

As future work, we plan to try new ways of calculating the mean staying time; develop new Markov processes and new algorithms within the framework; and apply the general framework to the cases of heterogeneous web graph and web graph series.
 References
