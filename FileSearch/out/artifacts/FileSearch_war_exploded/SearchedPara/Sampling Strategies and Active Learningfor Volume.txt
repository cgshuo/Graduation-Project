 This paper tackles the challenge of accurately and efficiently estimating the number of relevant documents in a collection for a particular topic. One real-world application is estimat-ing the volume of social media posts (e.g., tweets) pertaining to a topic, which is fundamental to tracking the popularity of politicians and brands, the potential sales of a product, etc. Our insight is to leverage active learning techniques to find all the  X  X asy X  documents, and then to use sampling techniques to infer the number of relevant documents in the residual collection. We propose a simple yet effective tech-nique for determining this  X  X witchover X  point, which intu-itively can be understood as the  X  X nee X  in an effort vs. recall gain curve, as well as alternative sampling strategies beyond the knee. We show on several TREC datasets and a collec-tion of tweets that our best technique yields more accurate estimates (with the same effort) than several alternatives.
Suppose we would like to estimate the number of relevant documents in a collection for a particular topic. We refer to this as the volume estimation problem. How would we go about doing this both accurately , such that our estimate is as close as possible to the actual value, and efficiently , with as little effort as possible? This problem presents an interesting twist on the problem of high-recall retrieval. For example, in electronic discovery [6], the litigants are interested in the actual documents, whereas we just want the volume of the relevant documents. Of course, if we can find all the rele-vant documents, we can just count them X  X o existing active learning techniques for high-recall retrieval provide a base-line. Alternatively, we could just randomly sample from the collection to estimate the prevalence and infer the volume. The question is: can we do better than either approach?
Even assuming we can, who cares? Under what circum-stances would we like to know the number of relevant doc-uments without identifying the actual relevant documents? The concrete instantiation of our problem is estimating the volume of social media posts (e.g., tweets) pertaining to a topic. Volume estimation in this case is fundamental to sev-eral real-world applications: tracking the popularity of politi-cians and brands, the box-office appeal of movies, the audi-ence for a sporting event, the potential sales of a product, and so on. The number of tweets pertaining to unfolding events is an often-quoted statistic in media coverage. Thus, volume estimation is not only academically interesting, but has significant real-world value.

The contribution of this paper is the development and evaluation of a technique for volume estimation based on active learning and sampling. Consider an active learning approach that can be characterized by a gain curve. The  X  X nee X  of that curve corresponds to the point where all the  X  X asy X  documents have been found. Our idea is to take ad-vantage of active learning until the knee, and then use sam-pling techniques to extrapolate on the remainder of the col-lection. We present a simple technique for finding the knee that works well in practice and explore three different sam-pling approaches past the knee. On several TREC datasets and a collection of tweets, we show that our stratified sam-pling technique yields the most accurate estimates compared to other techniques with the same amount of effort.
The volume estimation problem is related to retrieval tech-niques that focus on achieving very high recall X  X otivating application domains include legal eDiscovery, systematic re-views in evidence-based medicine, and locating prior art in patent search. The starting point of our work is the so-called baseline model implementation (BMI) that was pro-vided to participants of the TREC 2015 Total Recall track, whose principal purpose was to evaluate, through a con-trolled simulation, methods to achieve very high recall X  X s close as practicable to 100% X  X ith a human assessor in the loop. BMI is based on the  X  X utoTAR X  technique of Cormack and Grossman [3]. Starting with a query, AutoTAR applies continuous active learning using relevance feedback to pri-oritize documents for human assessment. As in standard active learning, relevance judgments incrementally refine an underlying relevance model X  X ut unlike the standard for-mulation of active learning, the point of AutoTAR is not to arrive at the best decision boundary between relevant and non-relevant documents, but rather to find all the relevant documents within a finite collection.

The effectiveness of techniques for high-recall retrieval is typically characterized by gain curves plotting effort ( x -axis) vs. recall ( y -axis). Most techniques, including the BMI and even manual approaches, exhibit gain curves that begin with a  X  X amp up X  phase, followed by a period where gain rises steadily, indicating the continuous discovery of  X  X asy to find X  relevant documents, and end with a region where the gain curves level off as the remaining relevant documents become more difficult to find. The  X  X nee X  is where the gain curve levels off, and this is a feature we exploit in our solution. In this work, we use the BMI from the TREC 2015 Total Recall track as-is, with the enhancement of a stopping criterion.
Suppose we would like to estimate the number of relevant documents in a particular collection consisting of D docu-ments. How might we go about doing this?
A na  X   X ve approach might be to randomly sample (without replacement) documents from the collection and assess them for relevance. We can approximate this as a Bernoulli pro-cess, for which the volume estimate R T is D  X  R E { E , where we find R E relevant documents after examining a total of E documents. Note that this does not form a complete al-gorithm because we still need to know how many samples to draw. We return to this issue later when explaining our paired experimental methodology.

An alternative starting point might be to leverage an ex-isting high-recall retrieval technique such as the BMI (Sec-tion 2). That is, we simply find all the relevant documents, which is a trivial way to determine the count. The problem, however, is that the BMI also does not come with a stop-ping criterion. Although various researchers have tackled this and related issues [7, 1], it is by no means solved. Regardless, let X  X  say we apply BMI to judge A documents. During this process, we will have explored some fraction of the collection that is more likely to contain relevant docu-ments, and say we discover R A documents. This value, of course, provides a lower bound on the number of relevant documents in the collection. But the problem is that we don X  X  know how many relevant documents there are in the documents we didn X  X  look at (what we refer to as the residual collection). Nevertheless, we can estimate an upper bound using the rate at which we X  X e finding relevant documents just before we stopped, and extrapolate to the remainder of the collection. However, this makes a terrible assumption because any active learning technique will prioritize docu-ments more likely to be relevant before documents less likely to be relevant, and so such an extrapolation would yield an unrealistically large overestimate.

One solution is to sample the residual collection. This requires answering two questions: First, how do we find the knee (i.e., the value of A )? Second, how do we sample after the knee? We tackle these two questions in turn.
In our approach, we employ the BMI augmented by the following the knee-finding method proposed by Cormack and Grossman [4]. In each iteration, the BMI selects exponen-tially larger batches of documents for human judgment. Af-ter receiving feedback for each batch, we can trace the gain curve described in Section 2; to be precise, the y axis is now the number of relevant documents found and not recall.
At the end of each iteration, we have a point that cor-responds to the total number of relevant documents found and the total number of documents judged thus far. We propose a candidate knee point as follows: find a point on the gain curve with maximum perpendicular distance from a line between the origin and the current (i.e., last) point of the curve. Let p 0 be the slope of the line from the ori-gin to the candidate knee, p 1 be the slope of the line from the candidate knee to the last point, and the slope ratio (1) the number of documents examined exceeds 1000 (to en-sure that the active learning process has gotten beyond the  X  X amp up X  phase), and (2)  X   X  6, if at least 150 relevant documents have been retrieved, or  X   X  156  X  r , if r  X  150 relevant documents have been retrieved. The second clause in (2) is a special case for handling topics with few relevant documents. The parameters for this technique were tuned on a private dataset.

Note that to be precise, this technique doesn X  X  actually discover the knee until we X  X e  X  X assed X  the knee, but the intu-ition nevertheless holds. The actual switchover point where we stop active learning and begin sampling corresponds to the point where we discovered the knee. However, for ex-pository convenience we still refer to  X  X topping at the knee X .
Based on the knee-finding algorithm described in the pre-vious section, we apply the BMI until the stopping criterion is met. At that point, we have judged A documents and found R A relevant documents. The next question is: what do we do with the residual collection that we have not yet examined? We present three sampling strategies: Negative Binomial Sampling. In this approach, we sam-ple from the residual collection until we encounter M rele-vant documents; let X  X  say this process requires us to examine S documents. Each sample can be modeled as a Bernoulli trial, and thus the sampling process can be characterized by a negative binomial distribution. Under this interpreta-tion, the minimum variance unbiased estimator for  X  p , the probability of success (i.e., probability of a document being relevant) is given as  X  p  X  p r  X  1 q{p r ` k  X  1 q , where r is the number of successes (relevant documents, which we set to M ) and k is the number of failures (non-relevant docu-ments) in our sequence of observations [5].

From this, our estimate of the total number of relevant documents, R T , is as follows (note S  X  r ` k ): In our experiments, we tried setting M P t 2 , 4 , 8 u . Natu-rally, higher values of M reduce the variance, but at the cost of requiring more assessment effort. The total effort required with this approach is A ` S , where S is the number of documents we must assess to find M relevant documents. The Horvitz-Thompson Estimator. The downside of the negative binomial sampling strategy is that for cases where the prevalence of relevant documents in the residual collection is low, it might require a lot of effort to find M relevant documents. An alternative is to use the BMI to score all documents in the residual collection at the point when it terminates, thus ordering all remaining documents in decreasing probability of relevance.

From here, we can apply a standard sampling technique called the Horvitz-Thompson Estimator (HT estimator) [8]: we compute a distribution over all documents in the resid-ual collection such that its probability of being sampled is proportional to its probability of relevance (as estimated by the BMI). This renormalized distribution is referred to as the inclusion probability, i.e.,  X  i refers to the probability that document i will be sampled. The Horvitz-Thompson estimate of the number of relevant documents is: where Y i is an indicator variable for relevance in each of the n sampled documents. Note that this does not form a com-plete algorithm because we are missing a stopping criterion. Once again, we address this issue in our paired experimental methodology below.
 Stratified Sampling. We propose a novel stratified sam-pling strategy, also based on a relevance ranking of the resid-ual collection at the point when the BMI terminates. This approach proceeds in iterations: at the i -th iteration, we randomly sample K S documents (  X  1000) from the next top ranking K documents (  X  10000) and judge those doc-uments. Suppose we find R i relevant documents: we can then estimate that there are K  X p R i { K S q in the top K hits. We then proceed to the next iteration and sample K S doc-uments from the next K top ranking documents, repeating as long as we find at least one relevant document. The total number of relevant documents can then be computed as: where n is the number of iterations. The total effort ex-pended is A ` n  X  K S .
To evaluate our volume estimation techniques, we used test collections from the TREC 2015 Total Recall Track [7]. In particular, we used three collections: the (redacted) Jeb Bush Emails (called  X  X thome1 X ), consisting of 290k emails from Jeb Bush X  X  eight-year tenure as the governor of Florida (10 topics); the Illicit Goods dataset (called  X  X thome2 X ) col-lected for the TREC 2015 Dynamic Domain Track, consist-ing of 465k documents from a web crawl (10 topics); and the Local Politics dataset (called  X  X thome3 X ) collected for the TREC 2015 Dynamic Domain Track, consisting of 902k documents from various news sources (10 topics). The rele-vance assessment process is described in the track overview paper [7], but for the purposes of our study, it suffices to say that the evaluation methodology has been sufficiently vali-dated for assessing the effectiveness of high-recall tasks (and thus these collections are suitable for our volume estimation problem). Finally, as a validation set, we evaluated our tech-niques on the Twitter collection described in Bommannavar et al. [2], who exhaustively annotated approximately 800k tweets from one day in August 2012 with respect to four topics: Apple (the technology company), Mars (the planet), Obama, and the Olympics. This dataset exactly matches our motivating application: how much  X  X uzz X  is there on so-cial media about a particular topic?
Note that random sampling and the HT estimator ap-proaches are not complete estimation algorithms since they lack a stopping criterion. Therefore, they are compared to negative binomial sampling and stratified sampling in a paired setup, where we evaluate how the techniques com-pare at the same level of effort. This models an A/B test-Figure 1: Box-and-whiskers plot characterizing 50 trials of each of our techniques on the Athome1 collection. ing scenario where we have two parallel efforts proceeding at exactly the same pace assessing documents. When one technique terminates, we also stop the other. At that point, we ask: how do the two estimates compare?
Our experimental procedure is as follows: for each topic in a collection, we ran our estimation technique (either nega-tive binomial sampling or stratified sampling) and recorded the total effort. We then ran a paired experiment with ei-ther random sampling or the HT estimator (or both) using exactly the same level of effort. We recorded the estimated volume for all techniques. For each collection, we report the average (relative) error across all topics and the root mean square error. The Athome1 collection was used as our train-ing set, on which we ran 50 trials of the above procedure to characterize the variability of estimates. The Athome2, Athome3, and Twitter collections were used as held-out test sets X  X e report the results of a single trial.
The results of 50 trials of our experimental procedure are shown in Figure 1, where the average relative error across the trials is characterized by a standard box-and-whiskers plot. We compared negative binomial sampling, M  X  t 2 , 4 , 8 u , with random sampling using the paired approach described above. We compared stratified sampling with the HT esti-mator and random sampling using exactly the same proce-dure. Each of these comparisons is shown by grouped bars (separated by dashed lines) in the figure.

As expected, the negative binomial sampling approach be-comes more accurate with increasing values of M (but re-quires correspondingly more effort). For reference, the en-tire collection contains 290k documents, so with M  X  8, on average we must examine nearly a quarter of the collec-tion. However, we see that negative binomial sampling is more accurate than random sampling at the same level of effort. It is clear that our stratified sampling approach is superior to all other techniques. On average, stratified sam-pling requires about half as much effort as negative binomial sampling with M  X  2 but gives much more accurate esti-mates. In fact, stratified sampling provides more accurate estimates than negative binomial sampling with M  X  8, at about a fifth of the effort. Stratified sampling also beats both the HT estimator and random sampling at the same level of effort.

Results for Athome2 and Athome3 are shown in Table 1. Table 1: Results of various volume estimation techniques on the Athome2, Athome3, and Twitter collections.
 Since these comprise our held-out test data, we only report the results of a single trial. In the table, rows are grouped in terms of different techniques at the same level of effort, e.g., the rows marked  X   X  sample X  denote accuracy with the same number of judged documents as the corresponding negative binomial or stratified condition. Due to the variability in-herent in our sampling strategies, in our particular trial we observe greater error with M  X  8 than with M  X  4 using negative binomial sampling. This, however, is not inconsis-tent with the results in Figure 1.

Overall, the results on Athome2 (465k documents) are consistent with the results from Athome1, our training set. Negative binomial sampling becomes more accurate with in-creasing M and is more accurate than random sampling with the same level of effort. However, our stratified sampling technique provides comparable error at far less cost, beat-ing both the HT estimator and random sampling.

Results on the Athome3 collection, which contains 902k documents, are quite poor. Table 2 shows why: for each topic in that collection, we list the total number of rele-vant documents, the effort expended in the active learning portion of our procedure, and the number of relevant doc-uments found at that point. For five of the topics (those in bold), with active learning we X  X e found either all or all but one of the relevant documents, which means that our termination condition for negative binomial sampling (e.g., with M  X  2) is never met and hence the procedure forces us to examine the entire collection . In contrast, with stratified sampling we examine 1000 of the top 10000 documents, find zero relevant, and terminate.
 The bottom of Table 1 shows our results for the Twitter a thome3089 2 55 1 105 2 54 a thome3133 1 13 1 105 1 12 a thome3226 2 094 3 478 2 022 a thome3290 2 6 2 316 2 6 a thome3357 6 29 1 526 5 99 a thome3378 6 6 1 105 6 6 a thome3423 7 6 1 232 4 0 a thome3431 1 111 1 232 1 106 a thome3481 2 036 3 478 1 924 a thome3484 2 3 1 105 2 3 Table 2: Relevant documents identified and effort when BMI terminates for Athome3. collection. Once again, we report results from a single trial. Overall, the findings are consistent with the other collec-tions: our stratified sampling technique clearly yields more accurate estimates than all other techniques. This gives us some degree of confidence that our algorithms, developed on email (Athome1), generalize to entirely different collec-tions (tweets). We have no explanation as to why negative binomial sampling with M  X  4 gives worse estimates than comparable random sampling, or why comparable random sampling with M  X  8 gives such poor results. We purposely decided against error analysis in order to preserve the sanc-tity of this validation set.
Estimating the number of relevant documents in a collec-tion presents an interesting twist to the high-recall retrieval problem. Our results show that actually finding the rele-vant documents is a good approach to counting the total volume. However, we develop and verify the insight that we should first identify the  X  X asy to find X  documents and then extrapolate via sampling on the rest. Our approach estab-lishes a baseline for future work on an important real-world application, particularly in the social media space. Acknowledgments. This work was supported in part by the U.S. National Science Foundation under awards IIS-1218043 and CNS-1405688, the Natural Sciences and Engi-neering Research Council of Canada (NSERC), Google, and the University of Waterloo. Any opinions, findings, conclu-sions, or recommendations expressed are those of the authors and do not necessarily reflect the views of the sponsors.
