 QCRI Qatar LMU Munich LMU Munich University of Edinburgh LMU Munich
In this article, we present a novel machine translation model, the Operation Sequence Model (OSM), which combines the benefits of phrase-based and N-gram-based statistical machine translation (SMT) and remedies their drawbacks. The model represents the translation process reordering operations. As in N-gram-based SMT, the model is: (i) based on minimal translation independence assumption, and (iv) avoids the s purious phrasal segmentation problem. As in unique properties of the model are (i) its strong coupling of reordering and translation where translation and reordering decisions are conditioned on n previous translation and reordering decisions, and (ii) the ability to model local and long-range reorderings consistently. Using
BLEU as a metric of translation accuracy, we found that our system performs significantly better than state-of-the-art phrase-based systems (Moses and Phrasal) and N-gram-based sys-tems (Ncode) on standard translation tasks. We compare the reordering component of the OSM to the Moses lexical reordering model by integrating it into Moses. Our results show that OSM be improved further by learning generalized representations with a POS-based OSM. 1. Introduction
Statistical Machine Translation (SMT) advanced near the beginning of the century from word-based models (Brown et al. 1993) towards more advanced models that take contextual information into account. Phrase-based (Koehn, Och, and Marcu 2003; Och and Ney 2004) and N-gram-based (Casacuberta and Vidal 2004; Mari  X  no et al. 2006) models are two instances of such frameworks. Although the two models have some common properties, they are substantially di fferent. The present work is a step towards combining the benefits and remedyin g the flaws of these two frameworks.
 chunks of translation called bilingual phrases. 1 Memorizing larger units enables the phrase-based model to learn local dependenc ies such as short-distance reorderings, idiomatic collocations, and insertions and deletions that are internal to the phrase pair. The model, however, has the following drawbacks: (i) it makes independence assumptions over phrases, ignoring the co ntextual information outside of phrases, (ii) the reordering model has difficulties in dealing with long-range reorderings, (iii) problems in both search and modeling require the use of a hard reordering limit, and (iv) it has the spurious phrasal segmentation problem, which allows multiple deriva-model scores.
 erated monotonically. Tuples are minimal translation units (MTUs) composed of source and target cepts. 2 The N-gram-based model has the following drawbacks: (i) only pre-calculated orderings are hypothesized during decoding, (ii) it cannot memorize and (iv) using tuples presents a more difficult searchproblemthaninphrase-basedSMT. tightly integrates translation and reorderin g into a single generative process. Our model source and target sentence in parallel, in a tar get left-to-right order. Possible operations explicit target positions for reordering operations, and (iii) forward and backward jump operations that do the actual reordering. Th e probability of a sequence of operations is defined according to an N-gram model, that is, the probability of an operation depends on the n  X  1 preceding operations. Because the translation (lexical generation) and reordering operations are coupled in a single generative story, the reordering decisions may depend on preceding translation decisions and translation decisions may depend 158 on preceding reordering decisions. This provides a natural reordering mechanism that is able to deal with local and long-distance reorderings in a consistent way. based on minimal translation units and takes both source and target information into account. This mechanism has several useful p roperties. Firstly, no phrasal independence assumption is made. The model has access to both source and target context outside of phrases. Secondly the model learns a uniqu e derivation of a bilingual sentence given its alignments, thus avoiding the spurious p hrasal segmentation problem. The OSM, however, uses operation N-grams (rather than tuple N-grams), which encapsulate both translation and reordering information. This allows the OSM to use lexical triggers for reordering like phrase-based SMT. Our reordering approach is entirely different from the tuple N-gram model. We consider all po ssible orderings instead of a small set of
POS-based pre-calculated orderings, as is used in N-gram-based SMT, which makes their approach dependent on the availability of a source and target POS-tagger. We show that despite using POS tags the reordering patterns learned by N-gram-based SMT are not as general as those learned by our model.
 units makes the search much more difficult because of the poor translation coverage, inaccurate future cost estimates, and pruning of correct hypotheses because of insuffi-cient context. The ability to memorize and produce larger translation units gives an edge to the phrase-based systems during decodi ng, in terms of better search performance and superior selection of translation units . In this article, we combine N-gram-based modeling with phrase-based decoding to benefit from both approaches. Our model is based on minimal translation units, but we use phrases during decoding. Through an extensive evaluation we found that this combination not only improves the search accuracy but also the BLEU scores. Our in-house phrase-based decoder outperformed state-of-the-art phrase-based (Moses and Phrasal) and N-gram-based (NCode) systems on three translation tasks.
 the state-of-the-art phrase-based system Moses (Koehn et al. 2007). Our aim was to directly compare the performance of the lexicalized reordering model to the OSM and to see whether we can improve the performan ce further by using both models together.
Our integration of the OSM into Moses gave a st atistically significant improvement over a competitive baseline system in most cases.
 of better modeling with MTUs in the OSM-augmented Moses system, we removed the reordering operations from the stream of ope rations. This is equivalent to integrating based decoder, as also tried by Niehues et al. (2011). Small gains were observed in most cases, showing that much of the improvement obtained by the OSM is due to better reordering.
 the lexicalized reordering model is its ability to take advantage of the wider contextual information. In an error analysis we found that the lexically driven OSM often falls back to very small context sizes because of data sparsity. We show that this problem can be addressed by learning operation sequences o ver generalized representations such as POS tags.
 review. We discuss the pros and cons of the phrase-based and N-gram-based SMT frameworks in terms of both model and search. Section 3 presents our model. We show how our model combines the benefits of both of the frameworks and removes their drawbacks. Section 4 provides an empir ical evaluation of our preliminary system, which uses an MTU-based decoder, against state-of-the-art phrase-based (Moses and Phrasal) and N-gram-based (Ncode) systems on three standard tasks of translating
German-to-English, Spanish-to-English, and French-to-English. Our results show im-provements over the baseline systems, but we noticed that using minimal translation units during decoding makes the search problem difficult, which suggests using larger units in search. Section 5 presents an extension to our system to combine phrase-based decoding with the operation sequence model to address the problems in search.
Section 5.1 empirically shows that information available in phrases can be used to improve the search performance and transl ation quality. Finally, we probe whether integrating our model into the phrase-based SMT framework addresses the mentioned drawbacks and improves translation quality. Section 6 provides an empirical evaluation of our integration on six standard tasks of translating German X  X nglish, French X  X nglish, and Spanish X  X nglish pairs. Our integration gives statistically significant improvements over submission quality baseline systems. Section 7 concludes. 2. Previous Work 2.1 Phrase-Based SMT
The phrase-based model (Koehn et al. 2003; Och and Ney 2004) segments a bilingual sentence pair into phrases that are continuous sequences of words. These phrases are then reordered through a lexicalized reordering model that takes into account the orientation of a phrase with respect to i ts previous phrase (Tillmann and Zhang 2005) or block of phrases (Galley and Manning 2008). Phrase-based models memorize local dependencies such as short reorderings, tra nslations of idioms, and the insertion and deletion of words sensitive to local context . Phrase-based systems, however, have the following drawbacks.

Handling of Non-local Dependencies. Phrase-based SMT models dependencies be-tween words and their translations insid e of a phrase well. However, dependencies across phrase boundaries are ignored because of the strong phrasal independence assumption. Consider the bilingual sentence pair shown in Figure 1(a).

Kampagne stimmen - X  X ote against your campaign X  and therefore represented by the trans-lation model. However, the model fails to co rrectly translate the test sentence shown Canada vote  X , failing to displace the verb. The language model does not provide enough 160 evidence to counter the dispreference of the translation model against jumping over the source words f  X  ur die Legalisieurung der Abtreibung in Kanada and translating stimmen - X  X ote X  at its correct position.

Weak Reordering Model. The lexicalized reordering model is primarily designed to deal with short-distance movement of phrases such as swapping two adjacent phrases and cannot properly handle long-range jumps. The model only learns an orientation of how a phrase was reordered with respect to its previous and next phrase; it makes independence assumptions over previously translated phrases and does not take into account how previous words were translated and reordered. Although such an inde-pendence assumption is useful to reduce spar sity, it is overly generalizing and does not help to disambiguate good reorderings from the bad ones.
 probability of orientation given phrase-pair estimates are based on a single observation.
Due to sparsity, the model falls back to use o ne-word phrases instead, the orientation of which is ambiguous and can only be judged based on context that is ignored. This drawback has been addressed by Cherry (2013) by using sparse features for reordering models.

Hard Distortion Limit. The lexicalized reordering model fails to filter out bad large-scale reorderings effectively (Koehn 2010). A hard distortion limit is therefore required during decoding in order to produce good translations. A distortion limit beyond eight words lets the translation accuracy drop because of search errors (Koehn et al. 2005). The use of a hard limit is undesirable for German X  X nglish and similar language pairs with significantly different syntactic structures. Several researchers have tried to address this problem. Moore and Quirk (2007) proposed improved future cost estimation to enable higher distortion limits in phrasal MT. Green, Galley, and Manning (2010) addition-ally proposed discriminative distortion models to achieve better translation accuracy than the baseline phrase-based system for a distortion limit of 15 words. Bisazza and
Federico (2013) recently proposed a novel method to dynamically select which long-range reorderings to consider during the hypothesis extension process in a phrase-based decoder and showed an improvement in a German X  X nglish task by increasing the distortion limit to 18.

Spurious Phrasal Segmentation. A problem with the phrase-based model is that there is no unique correct phrasal segmentation of a sentence. Therefore, all possible ways of segmenting a bilingual sentence consiste nt with the word alignment are learned and used. This leads to two problems: (i) phrase frequencies are obtained by counting all possible occurrences in the training corpus, and (ii) different segmentations producing the same translation are generated during decoding. The former leads to questionable parameter estimates and the latter may lead to search errors because the probability of a translation is fragmented across different s egmentations. Furthermore, the diversity in N-best translation lists is reduced. 2.2 N-Gram-Based SMT
N-gram-based SMT (Mari  X  no et al. 2006) uses an N-gram model that jointly generates the source and target strings as a sequence of bilingual translation units called tuples.
Tuples are essentially minimal phrases, atomic units that cannot be decomposed any further. The tuples are generated left to r ight in target word order. Reordering is not part of the statistical model. The parameters of the N-gram model are learned from bilingual data where the tuples have been arranged in target word order (see Figure 2). so that the translation can be done monotonically. The reordering is performed with
POS-based rewrite rules (see Figure 2 for an example) that have been learned from the training data (Crego and Mari  X  no 2006). Word lattices are used to compactly represent a number of alternative reorderings. Using parts of speech instead of words in the rewrite rules makes them more general and help s to avoid data sparsity problems.
 there is only one derivation for each aligned bilingual sentence pair. The model therefore avoids spurious ambiguity. The model ma kes no phrasal independence assumption and generates a tuple monotoni cally by looking at a context of n previous tuples, thus capturing context across phrasal boundaries. On the other hand, N-gram-based systems have the following drawbacks.

Weak Reordering Model. The main drawback of N-gram-based SMT is its poor re-ordering mechanism. Firstly, by linearizing the source, N-gram-based SMT throws away useful information about how a partic ular word is reordered with respect to the previous word. This information is instead stored in the form of rewrite rules, which have no influence on the translation score. The model does not learn lexical reordering triggers and reorders through the learned rule s only. Secondly, search is performed only on the precalculated word permutations created based on the source-side words. Often, evidence of the correct reordering is available in the translation model and the target-side language model. All potential reorderings that are not supported by the rewrite rules are pruned in the pre-processing step. T o demonstrate this, consider the bilingual sentence pair in Figure 2 again. N-gram-based MT will linearize the word sequence gegen ihre Kampagne stimmen to stimmen gegen ihre Kampagne ,sothatitisinthesame order as the English words. At the same time, it learns a POS rule: IN PRP NN VB
INPRPNN.ThePOS-basedrewriterulesservetoprecomputetheorderingsthatwillbe hypothesized during decoding. However, notice that this rule cannot generalize to the test sentence in Figure 1(b), even though the tu ple translation model learned the trigram &lt; sie  X  X  X hey X  w  X  urden  X  X  X ould X  stimmen  X  X  X ote X  &gt; and it is likely that the monolingual language model has seen the trigram they would vote .

Hard Reordering Limit. Due to sparsity, only rules with seven or fewer tags are ex-tracted. This subsequently constrains the reordering window to seven or fewer words, preventing the N-gram model from hypothesizing long-range reorderings that require 162 larger jumps. The need to perform long-distance reordering motivated the idea of using syntax trees (Crego and Mari  X  no 2007) to form rewrite rules. However, the rules are still extracted ignoring the target-side, and search is performed only on the precalculated orderings.

Difficult Search Problem. Using MTUs makes the search problem much more difficult because of poor translation option selection . To illustrate this consider the phrase pair schoss ein Tor  X   X  X cored a goal X , consisting of units schoss  X  X  X cored X , ein  X  X  X  X ,and Tor  X   X  X oal X . It is likely that the N-gram system does not have the tuple schoss  X  X  X cored X  X n its N-best translation options because it is an uncommon translation. Even if schoss  X   X  X cored X  is hypothesized, it will be ranked quite low in the stack and may be pruned, before ein and Tor are generated in the next steps. A similar problem is also reported in
Costa-juss ` a et al. (2007): When trying to reproduce the sentences in the N-best transla-tion output of the phrase-based system, th e N-gram-based system was able to produce only 37 . 5% of sentences in the Spanish-to-English and English-to-Spanish translation task, despite having been trained on the same word alignment. A phrase-based system, on the other hand, is likely to have access to the phrasal unit schoss ein Tor  X  X  X coreda goal X  and can generate it in a single step. 3. Operation Sequence Model
Now we present a novel generative model that explains the translation process as a forward and backward jump operations that do the actual reordering. The probability probability of an operation depends on the n  X  1 preceding operations. Because the translation (generation) and reordering op erations are coupled in a single generative story, the reordering decisions may depend on preceding translation decisions, and translation decisions may depend on preceding reordering decisions. This provides a natural reordering mechanism able to deal with local and long-distance reorderings consistently. 3.1 Generative Story The generative story of the model is motivated by the complex reordering in the
German-to-English translation task. The English words are generated in linear order, and the German words are generated in paralle l with their English translations. Mostly, the generation is done monotonically. Occasi onally the translator inserts a gap on the designated landing site for the translato r to jump back to. When the translator needs to cover the skipped words, it jumps back to one of the open gaps. After this is done, the translator jumps forward again and con tinues the translation. We will now, step by step, present the characteristics of the new model by means of examples. 3.1.1 Basic Operations. The generation of the German X  X nglish sentence pair Peter liest  X  without reordering:
Generate (Peter , Peter) Generate (liest , reads) requires the insertion of an additional German word ja , which is used as a discourse particle in this construction.
 Generate (Es , it) Generate (ist , is) Generate Source Only (ja) Generate (nicht , not) Generate (so , that) Generate (schlimm , bad)
Conversely, the translation Lies mit  X  X  X eadwithme X  X equiresthe deletion of an untrans-lated English word me .

Generate (Lies , Read) Generate (mit , with) Generate Target Only (me) 3.1.3 Reordering. Let us now turn to an example that requires reordering, and revisit the example in Figure 1(a). The generation of this sentence in our model starts with generating sie  X   X  X hey X , followed by the generation of w  X  urden  X  X  X ould X .Thenagapis inserted on the German side, followed by the generation of stimmen  X   X  X ote X . At this point, the (partial) German and E nglish sentences look as follows:
The arrow sign  X  denotes the position after the prev iously covered German word. The translation proceeds as follows. We jump back to the open gap on the German side and fillitbygenerating gegen  X   X  X gainst X , Ihre  X  X  X our X  X nd Kampagne  X   X  X ampaign X . Let us discuss some useful properties of this mechanism: 1. We have learned a reordering pattern sie w  X  urden stimmen  X  X  X hey 2. The model handles both local (Figure 1 (a)) and long-range reorderings 3. Learning the operation sequence Generate(sie, they) Generate(w  X  urden, 4. The model couples lexical generation and reordering information.
 164
Complex reorderings can be achieved by inserting multiple gaps and/or recursively (borrowed from Chiang [2007]). The generation of this bilingual sentence pair proceeds as follows: Generate(Aozhou, Australia) Generate(shi, is) Insert Gap Generate(zhiyi, one of) At this point, the (partial) Chines e and English sentences look like this:
The translator now jumps back and recursively inserts a gap inside of the gap before continuing translation: Jump Back (1) Insert Gap Generate(shaoshu, the few) Generate(guojia, countries) The rest of the sentence pair is generated as follows:
Jump Back (1) Insert Gap Generate(de, that) Jump Back (1) Insert Gap Generate(you, have) Generate(bangjiao, diplomatic relationships) Jump Back (1) Generate(yu, with) Generate(Beihan, North Korea)
Note that the translator jumps back and o pens new gaps recursively to exhibit a property similar to the hierarchical model. However, our model uses a deterministic algorithm (see Algorithm 1 later in this artic le) to convert each bilingual sentence pair given the alignment to a unique derivation, thus avoiding spurious ambiguity unlike hierarchical and phrase-based models.

Multiple gaps can simultaneously exist at an y time during generation. The translator decides based on the next English word to be covered which open gap to jump to.
Figure 4 shows a German X  X nglish subordinate clause pair. The generation of this example is carried out as follows: Insert Gap Generate(nicht, do not) Insert Gap Generate(wollen, want to)
At this point, the (partial) German an d English sentences look as follows: The inserted gaps act as placeholders for the skipped prepositional phrase  X  uber konkrete translator decides to generate any of the skipped words, it jumps back to one of the open gaps. The Jump Back operation closes the gap that it jumps to. The translator proceeds monotonically from that point until it needs to jump again. The generation proceeds as follows:
The translation ends by jumping back to the open gap and generating the prepositional phrase as follows:
Jump Back (1) Generate(  X  uber, on) Generate(konkrete, specific) Generate(Zahlen, figures) 5. Notice that although our model is based on minimal units, we can
Phrases Operation Sub-sequence nicht X wollen  X  X  X onotwantto X  Generate (nicht , do not) Insert Gap verhandeln wollen  X   X  X ant to negotiate X  Insert Gap Generate (wollen , want to)
X represents ,the Insert Gap operation on the German side in our notation. 166 3.1.4 Generation of Discontinuous Source Units. Now we discuss how discontinuous source cepts can be represented in our generative model. The Insert Gap operation discussed in the previous section can also be used to generate discontinuous source Figure 5. The gappy cept hat...gelesen  X   X  X ead X  can be generated as shown.
 generated as an incomplete translation of  X  X ead X . The second part gelesen is added to
Buch . Lastly, the second word ( gelesen )oftheunfinishedGermancept hat...gelesen is added to complete the translation of  X  X ead X  through a Continue Source Cept operation.
Discontinuous cepts on the English side cannot be generated analogously because of the fundamental assumption of the model that English (target-side) will be generated from left to right. This is a shortcoming of our approach, which we will discuss later in
Section 4.1. 3.2 Definition of Operations
Our model uses five translation and three reordering operations, which are repeatedly applied in a sequence. The following is a definition of each of these operations. 3.3 Translation Operations
Generate (X,Y): X and Y are German and English cepts, respectively, each with one or more words. Words in X (German) may be consecutive or discontinuous, but the words in Y (English) must be consecutive. This operation causes the words in Y and the first word in X to be added to the English and German strings, respectively, that were generated so far. Subsequent words in X are added to a queue to be generated later.
All the English words in Y are generated immediately because English (target-side) is generated in linear order as per the assumption of the model. second (and subsequent) German words in a multiword cept can be delayed by gaps, jumps, and other operations defined in the following.
Continue Source Cept: The German words added to the queue by the Generate (X,Y) operation are generated by the Cont inue Source Cept operation. Each Continue Source
Cept operation removes one German word from the queue and copies it to the German string. If X contains more than one German word, say n many, then it requires n translation operations, an initial Generate ( X 1 ... X n
Source Cept operations. For example kehrten...zur  X  uck  X   X  X eturned X  is generated by the operation Generate (kehrten zur  X  uck, returned) , which adds kehrten and  X  X eturned X  to the German and English strings and zur  X  uck to a queue. A Continue Source Cept operation later removes zur  X  uck from the queue and adds it to the German string. Generate Source Only (X): The words in X are added at the current position in the
German string. This operation is used to generate a German word with no cor-esponding English word. It is performed imme diately after its preceding German word is covered. This is because there is no evidence on the English side that indicates when to generate X. 5 Generate Source Only (X) helps us learn a source word deletion model.
It is used during decoding, where a German word X is either translated to some English word(s) by a Generate (X,Y) operation or deleted with a Generate Source Only (X) operation.
 Generate Target Only (Y): The words in Y are added at the current position in the
English string. This operation is used t o generate an English word with no cor-responding German word. We do not utilize this operation in MTU-based decoding where it is hard to predict when to add unaligned target words during decoding. We therefore modified the alignments to remov e this, by aligning unaligned target words (see Section 4.1 for details). In phrase-based decoding, however, this is not necessary, as we can easily predict unaligned target words where they are present in a phrase pair.
Generate Identical: The same word is added at the current position in both the German and English strings. The Generate Identical operation is used during decoding for the translation of unknown words. The probability of this operation is estimated from singleton German words that are translated to an identical string. For example, for a tuple QCRI  X   X  X CRI X , where German QCRI was observed exactly once during training, we use a Generate Identical operation rather than Generate (QCRI, QCRI) . 3.4 Reordering Operations We now discuss the set of reordering operations used by the generative story.
Reordering has to be performed whenever the German word to be generated next does not immediately follow the previously gene rated German word. During the generation process, the translator maintains an index that specifies the position after the previously
German word covered so far, and an index of the next German word to be covered ( j ). The set of reordering operations used in generation depends upon these indexes.
Please refer to Algorithm 1 for details. 168
Insert Gap: This operation inserts a gap, which acts as a placeholder for the skipped words. There can be more than one open gap at a time.

Jump Back (W): This operation lets the translator jump back to an open gap. It takes a parameter W specifying which gap to jump to. The Jump Back (1) operation jumps to the closest gap to Z , Jump Back (2) jumps to the second closest gap to Z , and so forth. After the backward jump, the target gap is closed.

Jump Forward: This operation makes the translator jump to Z . It is performed when the next German word to be generated is to the right of the last German word generated and does not follow it immediately. It will be followed by an Insert Gap or Jump Back (W) operation if the next source word is not at position Z . 3.5 Conversion Algorithm conversion is done. The values of the index variables are displayed at each point. 170 3.6 Model
Our model is estimated from a sequence of operations obtained through the transfor-mation of a word-aligned bilingual corpus. An operation can be to generate source and target words or to perform reordering by inserting gaps and jumping forward and backward. Let O = o 1 , ... , o J be a sequence of operations as hypothesized by the translator to generate a word-aligned bilingual sentence pair model is then defined as: where n indicates the amount of context used and A defines the word-alignment func-tion between E and F . Our translation model is implemented as an N-gram model of operations using the SRILM toolkit (Stolc ke 2002) with Kneser-Ney smoothing (Kneser and Ney 1995). The translate operations in our model (the operations with a name start-ing with Generate) encapsulate tuples. Tuples are minimal translation units extracted from the word-aligned corpus. The idea is similar to N-gram-based SMT except that the tuples in the N-gram model are genera ted monotonically. We do not impose the restriction of monotonicity in our model but integrate reordering operations inside the generative model.
 sentence pairs and operation sequences, that is, we get exactly one operation sequence per bilingual sentence given its alignments. The corpus conversion algorithm (Algo-rithm 1) maps each bilingual sentence pair given its alignment into a unique sequence of operations deterministically, thus maintaining a 1-to-1 correspondence. This property of the model is useful because it addresses t he spurious phrasal segmentation problem in phrase-based models. A phrase-based mode l assigns different scores to a derivation based on which phrasal segmentation is chosen. Unlike this, the OSM assigns only one score because the model does not suffer from spurious ambiguity. 3.6.1 Discriminative Model. We use a log-linear approach (Och 2003) to make use of standard features along with several novel features that we introduce to improve end-to-end accuracy. We search for a target string E that maximizes a linear combination of feature functions: dard features such as target-side language model, length bonus, distortion limit, and IBM lexical features (Koehn, Och, and Marcu 2003), we used the following new features:
Deletion Penalty. Deleting a source word ( Generate Source Only (X) ) is a common operation in the generative story. Because there is no corresponding target-side word, the monolingual language model score tends to favor this operation. The deletion penalty counts the number of deleted source words.
Gap and Open Gap Count. These features are introduced to guide the reordering decisions. We observe a large amount of reo rdering in the automatically word aligned training text. However, given only the sourc e sentence (and little world knowledge), it is not realistic to try to model the reasons for all of this reordering. Therefore we can use a more robust model that reorders less than humans do. The gap count feature sums to the total number of gaps inserted while producing a target sentence.
 tion ( Generate(X,Y), Generate Identical, Generate Source Only (X) ) performed whose value is the number of currently open gaps. This penalty controls how quickly gaps are closed.

Distance-Based Features. We have two distance-based features to control the reorder-ing decisions. One of the features is the Gap Distance, which calculates the distance between the first word of a source cept X and the start of the leftmost gap. This cost is paid once for each translation operation ( Generate, Generate Identical, Generate
Source Only (X) ). For a source cept covering the positions X value g j = X 1  X  S ,where S is the index of the left-most source word where a gap starts.
Another distance-based penalty used in our model is the Source Gap Width .This feature only applies in the case of a discontinuous translation unit and computes the distance between the words of a gappy cept. Let f = f 1 ... cept where x i is the index of the i th source word in the cept f . The value of the gap-width penalty is calculated as: 4. MTU-Based Search
We explored two decoding strategies in this work. Our first decoder complements the model and only uses minimal translation units in left-to-right stack-based decoding, similar to that used in Pharaoh (Koehn 2004a). The overall process can be roughly tion, (iii) hypothesis extension, and (iv) recombination and pruning. The last two steps are repeated iteratively until all the words in the source sentence have been translated. of the right-most source word covered so far ( Z ), the number of open gaps, the number of gaps so far inserted, the previously generat ed operations, the gene rated target string, and the accumulated values of all the features discussed in Section 3.6.1. The sequence of operations may include translation operations (generate, continue source cept, etc.) and reordering operations (gap insertions, jumps). Recombination hypotheses having the same coverage vector, monolingual language model context, and
OSM context. We do histogram-based prunin g, maintaining the 500 best hypotheses for each stack. A large beam size is required to cope with the search errors that result from using minimal translation units during decoding. We address this problem in Section 5. 172 4.1 Handling Unaligned and Discontinuous Target Words
Aligned bilingual training corpora often co ntain unaligned target words and discon-tinuous target cepts, both of which pose prob lems. Unlike discontinuous source cepts, discontinuous target cepts such as hinuntersch  X  uttete  X  X  X oured like den Drink hinuntersch  X  uttete  X  X  poured the drink down  X  cannot be handled by the operation sequence model because it generat es the English words in strict left-to-right order. Therefore they have to be eliminated.
 has difficulties predicting where to inser t them. Thus, we eliminate unaligned target words in MTU-based decoding.
 alignments and removes unaligned and discontinuous targets. If a source word is aligned with multiple target words that are not consecutive, first the link to the least frequent target word is identified, and the group (consecutive adjacent words) of links containing this word is retained while the o thers are deleted. The intuition here is to keep the alignments containing content word s (which are less frequent than functional words). For example, the alignment link hinuntersch  X  uttete  X   X  X own X  is deleted and only the link hinuntersch  X  uttete  X   X  X oured X  is retained because  X  X own X  occurs more frequently than  X  X oured X . Crego and Yvon (2009) used split tokens to deal with this phenomenon. each unaligned target word, we determine th e (left or right) neighbor that it appears more frequently with and align it with the same source word as this neighbor. Crego, p ( f | e ) obtained from IBM Model 1 (Brown et al. 1993) to decide whether to attach left or right. A more sophisticated strategy based on part-of-speech entropy was proposed by
Gispert and Mari  X  no (2006). 4.2 Initial Evaluation We evaluated our systems on German-to-English, French-to-English, and Spanish-to-
English news translation for the purpose of development and evaluation. We used data from the eighth version of the Europarl Corpus and the News Commentary made available for the translation task of the Eighth Workshop on Statistical Machine Translation . bilingual corpora contained roughly 2M bilin gual sentence pairs, which we obtained by concatenating news commentary (  X  184K sentences) and Europarl for the estimation of the translation model. Word alignments were generated with GIZA++ (Och and
Ney 2003), using the grow-diag-final-and heuristic 8 (Koehn et al. 2005). All data are lowercased, and we use the Moses tokenizer. We took news-test-2008 as the dev set for optimization and news-test 2009-2012 for testing. The feature weights are tuned with
Z-MERT (Zaidan 2009). 4.2.1 Baseline Systems. We compared our system with (i) Moses
Phrasal 10 (Cer et al. 2010), and (iii) Ncode 11 (Crego, Yvon, and Mari  X  no 2011). We used all these toolkits with their default setting s. Phrasal provides two main extensions to
Moses: a hierarchical reordering model (Galley and Manning 2008) and discontinuous source and target phrases (Galley and Manning 2010). We used the default stack sizes of 100 for Moses, 12 200 for Phrasal, and 25 for Ncode (with 2 guage model is used. Both phrase-based systems use the 20 best translation options per source phrase; Ncode uses the 25 best tuple translations and a 4-gram tuple sequence model. A hard distortion limit of 6 is used in the default configuration of both phrase-based systems. Among the other defaults, we retained the hard source gap penalty of 15 and a target gap penalty of 7 in Phrasal. We provide Moses and Ncode with the same post-edited alignments 13 from which we had removed tar get-side discontinuities.
We feed the original alignments to Phrasal because of its ability to learn discontinuous source and target phrases. All the systems use MERT for the optimization of the weight vector. 4.2.2 Training. Training steps include: (i) post-editing of the alignments (Section 4.1), (ii) generation of the operation sequence (Algorithm 1), and (iii) estimation of the N-gram translation (OSM) and language models u sing the SRILM too lkit (Stolc ke 2002) with
Kneser-Ney smoothing. We used 5-gram models. 4.2.3 Summary of Developmental Experiments. During the developent of the MTU-based decoder, we performed a number of experiments to obtain optimal settings for the system. We list here a summary of the results from those experiments: 4.2.4 Comparison with the Baseline Systems. In this section we compare our system ( OSM mtu ) with the three baseline systems. We used Kevin Gimpel X  X  tester, uses bootstrap resampling (Koehn 2004b) to test which of our results are significantly better than the baseline results. We mark a baseline result with  X * X  in order to indicate 174 that our model shows a significant improvem ent over this baseline with a confidence of p &lt; 0 . 05. We use 1,000 samples during bootstrap resampling.
 line systems in most cases. Our French-to-English results show a significant im-cases. The N-gram-based system NCode was better or similar to our system on the
French task. Our Spanish-to-English system also showed roughly the same trans-lation quality as the baseline systems, but was significantly worse on the WMT task. 5. Phrase-Based Search
The MTU-based decoder is the most straightforward implementation of a decoder for the operation sequence model, but it faces search problems that cause a drop in translation accuracy. Although the OSM captures both source and target contexts and provides a better reordering mechanism, t he ability to memorize and produce larger translation units gives an edge to the phras e-based model during decoding in terms of better search performance and superior selection of translation units. In this section, we combine N-gram-based modeling with phr ase-based decoding. This combination not only improves search accuracy but also increases translation quality in terms of BLEU.
 learn larger translation chunks by memori zing a sequence of operations. However, it often has difficulties to produce the same translations as the phrase-based system be-cause of the following drawbacks of MTU-based decoding: (i) the MTU-based decoder does not have access to all the translation units that a phrase-based decoder uses as part of a larger phrase, (ii) it requires a larger beam size to prevent early pruning of correct hypotheses, and (iii) it uses less-powerful fu ture-cost estimates than the phrase-based decoder. To demonstrate these problems, consider the phrase pair which the model memorizes through the sequence:
Generate(Wie, What is) Insert Gap Generate (Sie, your) Jump Back (1) Generate (heissen, name)
The MTU-based decoder needs three separate tuple translations to generate the same phrasal translation: Wie  X  X  X hatis X , Sie  X  X  X our X  X nd hei X en  X   X  X ame X . Here we are faced with three challenges.

Translation Coverage: The first problem is that the N-gram model does not have the same coverage of translation options. The English cepts  X  X hat is X ,  X  X our  X , and  X  X ame X  are not good candidate translations for the German cepts Wie , Sie ,and hei X en ,whichare usually translated to  X  X ow X ,  X  X ou X , and  X  X all X , respectively, in isolation. When extracting
Wie  X   X  X hat is X  is ranked 124th, hei X en  X   X  X ame X  is ranked 56th, and Sie  X   X  X our  X  is ranked 9thinthelistof n -best translation candidates. Typically, only the 20 best translation options are used, for the sake of efficiency, and such phrasal units with less frequent translations are never hypothesized in the N-gram-based systems. The phrase-based system, on the other hand, can extract the phrase Wie hei X en Sie  X   X  X hat is your name X  even if it is observed only once during training.

Larger Beam Size: Even when we allow a huge number of translation options and therefore hypothesize such units, we are faced with another challenge. A larger beam size is required in MTU-based decoding to prevent uncommon translations from getting pruned. The phrase-based system can generate the phrase pair Wie hei X en Sie  X   X  X hat is your name X  in a single step, placing it directly into the stack three words to the right. The MTU-based decoder generates this phrase in three stacks with the tuple translations Wie  X  X  X hatis X , Sie  X  X  X our X ,and hei X en  X   X  X ame X . A very large stack size is required during decoding to prevent the pruning of Wie  X   X  X hat is X , which is ranked
Although the translation quality achieved by phrase-based SMT remains the same when varying the beam size, the performance of our system varies drastically with different beam sizes (especially for the Ge rman X  X nglish experiments where the search is more difficult due to a higher number of reorderings). Costa-juss ` a et al. (2007) also report a significant drop in the performance of N-gram-based SMT when a beam size of 10 is used instead of 50 in their experiments.
 Future Cost Estimation: A third problem is caused by inaccurate future cost estimation.
Using phrases helps phrase-based SMT to better estimate the future language model cost because of the larger context availabl e, and allows the decoder to capture local (phrase-internal) reorderings in the future c ost. In comparison, the future cost for tuples hei X en Sie  X   X  X hat is your name X  is estimated by calculating the cost of each feature. 176
A bigram language model cost, for example, is estimated in the phrase-based system as follows:
The translation model cost is estimated as: Phrase-based SMT is aware during the preprocessing step that the words Wie hei X en
Sie may be translated as a phrase. This is helpful for estimating a more accurate future cost because the context is already availa ble. The same is not true for the MTU-based decoder, to which only minimal units are av ailable. The MTU-based decoder does not have the information that Wie hei X en Sie may be translated as a phrase during decoding. The future cost estimate available to the operation sequence model for the span covering
Wie hei X en Sie will have unigram probabilities for both the translation and language models.
The translation model cost is estimated as: p tm = p(Generate(Wie, What is))  X  p(Generate(hei X en,name))
A more accurate future cost estimate for the translation model cost would be: where C i is the context for the generation of the ith operation X  X hat is, up to m previous operations. For example C 1 = Generate(Wie, What is) , C 2
Gap, and so on. The future cost estimates computed in this manner are much more accurate because not only do they consider context, but they also take the reordering operations into account (Durrani, Fraser, and Schmid 2013). 5.1 Evaluating the Phrase-Based Decoder
We extended our in-house OSM decoder to use phrases instead of MTUs during decod-ing. In order to check whether phrase-based decoding solves the mentioned problems and improves the search accuracy, we evaluated the baseline MTU decoder and the phrase-based decoder with the same model parameters and tuned weights. This allows us to directly compare the model scores. We tuned the feature weights by running MERT with the MTU decoder on the dev set. Table 3 shows results from running both, the
MTU-based ( OSM mtu ) and the phrase-based ( OSM phr ) decoder, on the WMT
Improved search accuracy is the percentage of times each decoder was able to produce 200. Table 3 shows the percentage of times the MTU-based and phrase-based decoder produce better model scores than their cou nterpart. It shows that the phrase-based decoder produces better model scores for almost 48% of the hypotheses (on average) System German French Spanish Average OSM mtu 8.98% 8.88% 6.73% 8.2%
OSM phr 56.20% 37.37% 49.36% 47.64% across the three language pairs, whereas t he MTU-based decoder (using a much higher stack size [500]) produces better hypotheses 8.2% of the time on average.

This improvement in search is also reflected i n translation quality. Our phrase-based decoder outperforms the MTU-based decoder in all the cases and gives a significant improvement in 8 out of 12 cases (Table 4). 5.2 Handling of Unaligned and Discontinuous Target Words
In Section 4.1 we discussed the problem of handling unaligned and discontinuous target words in MTU-based decoding. An advantage of phrase-based decoding is that we can use such units during decoding if they appe ar within the extracted phrases. We use a Generate Target Only (Y) operation whenever the unaligned target word Y occurs in WMT 09 *20.47 *20.78 *20.52 *21.17 21.47 WMT 10 *21.37 *21.91 *21.53 *22.29 22.73 WMT 11 *20.40 *20.96 *20.21 *21.05 21.43 WMT 12 *20.85 *21.06 *20.76 *21.37 21.98 WMT 09 *25.78 *25.87 *26.15 26.22 26.51 WMT 10 26.65 *25.87 26.89 26.59 26.88 WMT 11 *27.37 *27.62 *27.46 27.75 27.91 WMT 12 *27.15 27.76 *27.55 *27.66 27.98 WMT 09 *25.90 26.13 *25.91 25.90 26.18 WMT 10 *28.91 *28.89 *29.02 *28.82 29.37 WMT 11 *28.84 *28.98 *28.93 *28.95 29.66
WMT 12 31.28 31.47 31.42 *30.86 31.52 178 a phrase. Similarly, we use the operation Generate (hinuntersch  X  uttete, poured down) when the discontinuous tuple hinuntersch  X  uttete  X   X  X oured ... down X  occurs in a phrase.
While training the model, we simply ignore the discontinuity and pretend that the word  X  X own X  immediately follows  X  X oured X . This can be done by linearizing the subsequent parts of discontinuous target cepts to appear after the first word of the cept. During decoding we use phrase-internal alignment s to hypothesize such a linearization. This generated in its original order. This heuristic allows us to deal with target discontinuities without extending the operation sequence model in complicated ways. It results in better BLEU accuracy in comparison with th e post-editing of the alignments method described in Section 4.1. For details and empirical results refer to Durrani et al. (2013a) (see Table 2 therein, compare Rows 4 and 5).

Manning 2010), allows all p ossible geometries as shown in Figure 7. However, because our decoder only uses continuous phrases, we cannot hypothesize (ii) and (iii) unless they appear inside of a phrase. But our mode l could be integrated into a discontinuous phrase-based system to overcome this limitation. 6. Further Comparative Experiments
Our model, like the reordering models ( Tillmann and Zhang 2005; Galley and Manning 2008) used in phrase-based decoders, is lexicalized. However, our model has richer conditioning as it considers both translati on and reordering context across phrasal boundaries. The lexicalized reordering mo del used in phrase-based SMT only accounts for how a phrase pair was reordered with respect to its previous phrase (or block of phrases). Although such an independence assumption is useful to reduce sparsity, it is overgeneralizing, with only three possible o rientations. Moreover, because most of the extracted phrases are observed only once, th e corresponding probability of orientation given phrase-pair estimates is very sparse. The model often has to fall back to short one-word phrases. However, most short phrases are observed frequently with all possible orientation should be picked d uring decoding. The model therefore overly relies on the language model to break such ties. The OSM may also suffer from data sparsity and the back-off smoothing may fall back to very short contexts. But it might still be able to disambiguate better than the lexicalized reordering models. Also these drawbacks can be addressed by learning an OSM over generalized word representation such as POS tags, as we show in this section.
 ized reordering model, we incorporate the OSM into the phrase-based Moses decoder.
This allows us to exactly compare the two mo dels in identical settings. We integrate the OSM into the hypothesis extension process of the phrase-based decoder. We convert each phrase pair into a sequenc e of operations by extracting the MTUs within the phrase pair and using phrase internal alignments. Th e OSM is used as a feature in the log-linear framework. We also use four supportive features: the Gap, Open Gap, Gap-distance, and Deletion counts, as described earlier (see Section 3.6.1). 6.1 Baseline Our Moses (Koehn et al. 2007) baseline systems are based on the setup described in
Durrani et al. (2013b). We trained our systems with the following settings: maximum sentence length 80, grow-diag-final and sy mmetrization of GIZA++ alignments, an in-terpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield 2011) used at runtime, distortion limit of 6, minimum Bayes-risk decoding (Kumar and Byrne 2004), cube pruning (Huang and Chiang 2007), and the no-reordering-over-punctuation heuristic. We used factored models (Koe hn and Hoang 2007), for German X  X nglish and
English X  X erman. We trained the lexicaliz ed reordering model (Koehn et al. 2005) with msd-bidirectional-fe settings. 6.2 Results
Table 5 shows that the OSM results in higher gains than the lexicalized reordering model on top of a plain phrase-based baseline ( Pb ). The average improvement obtained using the lexicalized reordering model ( Pb lex ) over the baseline ( Pb ) is 0.50. In comparison, the average improvement obtained by using the OSM ( Pb osm is 0.74. The average improvement obtained by the combination ( Pb average improvement obtained by adding the OSM over the baseline ( Pb tested for significance and found that in seven out of eight cases adding the OSM on top of Pb lex gives a statistically significant improvement with a confidence of p
Significant differences are marked with an asterisk. 6.3 Comparison with Tuple Sequence Model
In an additional experiment, we studied how much the translation quality decreases when all reordering operations are removed from the operation sequence model during FR-EN 30.19 30.73 30.74 30.89 30.77 *31.34 30.97 *31.48 EN-FR 28.45 29.62 28.98 30.06 29.16 30.46 *29.38 *30.54 ES-EN 33.64 29.86 34.07 30.25 34.24 *30.72 *34.43 *31.04 EN-ES 33.57 29.26 34.30 30.03 34.51 30.07 *34.71 *30.53
Avg 30.67 31.17 +0.50 31.41 +0.74 31.64 +0.97 180 DE-EN 22.95 25.26 *23.54 *26.01 23.18 25.51 EN-DE 17.95 20.16 18.10 20.43 17.90 20.20 FR-EN 30.74 30.89 30.97 *31.48 30.80 31.04 EN-FR 28.98 30.06 *29.38 *30.54 29.12 30.24 ES-EN 34.07 30.25 *34.43 *31.04 34.19 30.44 EN-ES 34.30 30.03 *34.71 *30.53 34.38 30.20
Avg 27.97 28.43 +0.46 28.10 +0.13 training and decoding. The resulting model is similar to the tuple sequence model than POS-based rewrite rules to do the sour ce linearization. Table 6 shows an average improvement of just 0.13 on top of the base line phrase-based system with lexicalized reordering, which is much lower than the 0.46 points obtained with the full operation sequence model.
 phrase-based systems before, either inside the decoder (Niehues et al. 2011) or to rerank the N-best candidate translations in the output of a phrase-based system (Zhang et al. 2013). Both groups reported improvements of similar magnitude when using a target-order left-to-right TSM model for German X  X nglish and French X  X nglish translation with shared task data, but higher gains on other data sets and language pairs. Zhang et al. (2013) showed further gains by combining models with target and source left-to-right and right-to-left orders. The assumption of generating the target in monotonic order is a weakness of our work that can be addressed following Zhang et al. (2013). By generating
MTUs in source order and allowing gaps and jumps on the target side, the model will be able to learn other reordering patterns that are ignored by the standard OSM. 6.4 OSM over Generalized Representations therefore often backs off to very small context sizes. Consider the example shown in
Figure 1. The learned pattern sie w  X  urden stimmen  X   X  X hey would vote X  cannot be generalized to er w  X  urde w  X  ahlen  X   X  X e would vote X . We found that the OSM uses only two preceding operations as context on average. This problem can be addressed by replacing words with POS tags (or any other generalized representation such as Morph tags, word clusters) to allow the model to consider a wider syntactic context where this model. Crego and Yvon (2010) and Niehues et al. (2011) have shown improvements in DE-EN 22.95 25.26 23.54 26.01 23.78 26.30 EN-DE 17.95 20.16 18.10 20.43 18.33 20.70
Avg 21.58 22.02 +0.44 22.28 +0.70 translation quality when using a TSM model over POS units. We estimate OSMs over generalized tags and add these as separate features to the loglinear framework. Experiments. We enabled factored sequence models (Koehn and Hoang 2007) in German X  X nglish language pairs as these have been shown to be useful previously.
We used LoPar (Schmid 2000) to obtain morphological analysis and POS annotation of German and MXPOST (Ratnaparkhi 1998), a maximum entropy model for English
POS tags. We simply estimate OSMs over POS tags 16 corresponding tags during training.

Table 7 shows that a system with an additional POS-based OSM ( Pb gives an average improvement of +0.26 over the baseline ( Pb an OSM over surface forms only. The overall gain by using OSMs over the baseline system is +0.70. OSM over surface tags cons iders 3-gram on average, and OSM over POS tags considers 4.5-grams on average, thus co nsidering wider contextual information when making translation and reordering decisions. 6.5 Time Complexities and Memory Usage
Table 8 shows the wall-clock decoding time (in minutes) from running the Moses decoder (on news-test2013) with and without the OSMs. Each decoder is run with 24 threads on a machine with 140GB RAM and 24 processors. Timings vary between experiments because of the fact that machines were somewhat busy in some cases. But generally, the OSM increases decoding time by more than half an hour. along with the OSMs. It also shows the model sizes when filtered on news-test2013.
A similar amount of reduction could be achieved by applying filtering to the OSMs following the language model filtering described by Heafield and Lavie (2010). 182 DE 61 88  X  27 143 158  X  15 FR 108 163  X  55 113 154  X  41 ES 111 142  X  31 74 109  X  35 Avg 93 131  X  38 110 140  X  30 7. Conclusion
In this article we presented a new model for statistical MT that combines the benefits of two state-of-the-art SMT frameworks, namely, N-gram-based and phrase-based SMT.
Like the N-gram-based model, it addresse s two drawbacks of phrasal MT by better handling dependencies across phrase boundaries, and solving the phrasal segmentation problem. In contrast to N-gram-based MT, our model has a generative story that tightly reorderings, unlike N-gram systems that perform search only on a limited number of pre-calculated orderings. Our model is able to correctly reorder words across large distances, and it memorizes frequent phrasal translations including their reordering as probable operation sequences.
 (MTUs) against the state-of-the-art phrase-based systems Moses and Phrasal and the
N-gram-based system Ncode for German-to-English, French-to-English, and Spanish-provements in 9 out of 12 cases in the German-to-English translation task, and 10 out of 12 cases in the French-to-English translation task. Our Spanish-to-English results are similar to the baseline systems in most of the cases but consistently worse than Ncode. cost estimates, and pruning of correct hypotheses. Phrase-based SMT, on the other hand, avoids these drawbacks by using larger translation chunks during search. We therefore extended our decoder to use phrases instead of cepts while keeping the statistical model unchanged. We found that combining a model based on minimal units with phrase-based decoding improves both se arch accuracy and translation quality. Our system extended with phrase-based decoding showed improvements over all the base-line systems, including our MTU-based decoder. In most of the cases, the difference was significant.
 ordering model and gives statistically significant gains over a very competitive Moses baseline system. We showed that considerin g both translation and reordering context is important and ignoring reordering conte xt results in a significant reduction in the performance. We also showed that an OSM based on surface forms suffers from data sparsity and that an OSM based on a generalized representation with part-of-speech tags improves the translation quality by considering a larger context. In the future we would like to study whether the insight of using minimal units for modeling and search based on composed rules would hold for hierarchical SMT. Vaswani et al. (2011) recently showed that a Markov model over the derivation history of minimal rules can obtain the same translation quality as using grammars formed with composed rules, which we believe is quite promising.
 Acknowledgments References 184
