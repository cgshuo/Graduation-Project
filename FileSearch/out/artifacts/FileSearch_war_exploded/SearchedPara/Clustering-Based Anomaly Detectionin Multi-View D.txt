 This paper proposes a simple yet effective anomaly detec-tion method for multi-view data. The proposed approach detects anomalies by comparing the neighborhoods in dif-ferent views. Specifically, clustering is performed separately in the different views and affinity vectors are derived for each object from the clustering results. Then, the anomalies are detected by comparing affinity vectors in the multiple views. An advantage of the proposed method over existing methods is that the tuning parameters can be determined effectively from the given data. Through experiments on synthetic and benchmark datasets, we show that the proposed method out-performs existing methods.
 H.3.3 [ Information systems ]: Information Storage and Retrieval X  Information Search and Retrieval Anomaly detection; multi-view data; affinity propagation
Anomaly detection methods aim at identifying anomalies, a.k.a. outliers, in a given dataset. Those methods are useful for various types of applications including fraud and intru-sion detection [ 2]. The standard anomaly detection methods use single-view data to identify anomalies. For example, the one-class support vector machine approach [ 7] finds a normal region containing a certain fraction of samples and regards the samples not included in the region as anomalies.
However, present-day data tends to be multi-view, i.e. the individual objects are described from several perspectives or This work was completed when the author was intern at NTT CS Labs.
 several views, each of which highlights different character-istics of the objects. An example of multi-view data is the data acquired by stereo cameras that capture the images from two different points of view. Another appealing ex-ample appears in social networks where multimedia content (texts, images, movies, etc.) can be described by its intrin-sic information (bag of words, image features, etc.) or by the actions of the users (comments,  X  X ike X , number of views, etc.). In order to adapt to this particular property, anomaly detection methods should be designed to take advantage of those multiple views.

Recently, Gao et al. [ 4] proposed a novel multi-view ano-maly detection algorithm, called horizontal anomaly detec-tion (HOAD), that exploits several different data sources, i.e. multi-view data, to detect outliers of the dataset. The main idea behind HOAD is to regard samples whose behav-ior is inconsistent among multiple information sources as anomalies. More specifically, HOAD first clusters objects si-multaneously in all the views with spectral clustering [ 6] and then classifies as anomalies the samples that belong to dif-ferent clusters in different views. Gao et. al experimentally showed the efficiency of their approach. However, HOAD has several tuning parameters, including the number of clus-ters, which is assumed to be the same for all views. As a consequence, the performance of HOAD highly depends on the values given to the tuning parameters.

To overcome the drawbacks of HOAD, we propose a multi-view anomaly detection method based on affinity propaga-tion (AP) [ 3]. More specifically, we first cluster the data in-dependently in each view with AP and then compute, from the clustering results, affinity vectors whose elements rep-resent the affinity between two objects. Finally, we detect anomalies by simply comparing anomaly scores computed from the affinity vectors. An advantage of the proposed met-hod over HOAD is that the tuning parameters can be easily determined by a simple, yet effective, heuristic. In addition, the number of clusters is automatically determined by AP, while HOAD needs to be given the number of clusters be-forehand. This is an important property for the practitioner. Through experiments on synthetic and benchmark datasets, the proposed method is shown to outperform HOAD.
In this section, we give a formal definition of the multi-view anomaly detection problem. Figure 1: Illustrative example: motivation of using multi-view data in anomaly detection: with only one view, some anomalies may remain hidden.

Suppose we are given n independent and identically dis-tributed (i.i.d.) objects, denoted by i = 1 , . . . , n , described by V views denoted by v = 1 , . . . , V . Each object i , when seen through view v , has a feature representation x v i such that: where X v is the domain of view v and d v is the dimensional-ity of that domain. In this case, x v i is the feature represen-tation of object i when seen from the particular perspective of view v . Note that each view sees different aspects of the object. Consequently, the object has V different feature representations, one for each view.

We define D as the set containing the V feature represen-tations of each object i :
Then, our multi-view anomaly detection strategy consists in computing an anomaly score for each object and in com-paring this anomaly score to a threshold  X  that controls the sensibility/robustness tradeoff.
In this section, we propose a method that detects anoma-lies in multi-view data.
In this paper, we propose a new multi-view anomaly de-tection method that consists in three steps: (1) perform clustering separately in each view, (2) for each view, create an affinity vector for each object based on the clustering re-sults of step (1), and (3) compute an anomaly score based on affinity vectors. The key idea here is to utilize clustering-based affinity information to detect anomalies. The pro-posed method is designed to detect the anomalous objects whose neighborhoods are inconsistent among multiple views.
Figure 1 illustrates our multi-view anomaly detection fra-m ework using an example with web images observed from two points of view: the image features and the web pages in which the images are contained. The leftmost graph shows the objects (clustered in  X  X nimals X  and  X  X ars X ) from the image features view, while the rightmost graph shows the objects (clustered in  X  X nimals.com X  and  X  X ars.com X ) from the web pages view.

If we consider only a single view, i.e. image features or web pages, then, only the image represented by a square can be identified as a clear anomaly while the rest of the data seems normal. However, if we consider the two views si-multaneously, the square is not an outlier anymore because its neighborhood is consistent in both views. In this case, the square is far away from the other objects in both views. On the other hand, when both views are considered, an-other point draws our attention. Indeed, we notice that the red triangle is clustered in  X  X nimals X  in the image features view, while it is clustered in  X  X ars.com X  in the web pages view. The clustering-based affinity information, or neigh-borhoods, in the image features view and in the web pages view are inconsistent . We define this behavior as anomalous and regard the red triangle as an anomalous image.
In this paper, we employ affinity propagation (AP) [ 3] as a c lustering method. AP is an exemplar based clustering algo-rithm that associates each object with its exemplar. In this context, an exemplar is an object that corresponds to the center of one cluster. We denote that object i is associated with exemplar j by c i = j . In this setting, all the config-urations are not allowed and some constraints are needed to ensure that the result is coherent. Indeed, if object i is associated with exemplar j ( c i = j ), then the exemplar of j should be j as well ( c j = j ), if not, this means that object j is actually not an exemplar.

Affinity propagation computes two messages for each pair ( i, j ) of objects: the responsibility r ij and the availability a . The responsibility r ij sent from i to j models how much object i wants object j as exemplar, whereas the availability a ij sent from j to i models how much object j wants to be the exemplar of object i . As AP is an iterative algorithm, the responsibilities and availabilities are updated at each iteration until convergence is reached or until a fixed budget of iterations is exhausted. The initial availabilities are set to 1 and the update equations are as follows: where L is the likelihood matrix such that L ij represents the affinity, or similarity, between objects i and j . A large value of L ij indicates that the objects are similar. Note that, for AP, we need to set the diagonal values of L . Smaller values on the diagonal tend to yield a smaller number of clusters. Frey et al. [ 3] propose to set those diagonal values to either t he median or the minimum of the non-diagonal elements of L . Those heuristics work very well in practice.

Once the algorithm has converged, the clustering solution is obtained  X  i by c i = { 1 , . . . , n } , such that where the value of c i represents the exemplar of object i .
In the proposed method, we apply affinity propagation s eparately in the different views. Consequently, we have V clustering results that we refer to as such that the clustering results obtained in view v are de-noted by c v i . R n of object i in view v . We propose to compute z v i as follows: where z v i ( j ) is the j -th element of vector z v i , Z is a nor-malization factor such that P j z v i ( j ) = 1, and L v ic likelihood between object i and the exemplar of j , c v j the view v . The key idea here is to incorporate clustering
If two objects i and j are similar, we expect their like-lihood L v ij to be large and their exemplar to be the same, thus giving a large value of z v i ( j ). On the other hand, if two objects are assigned to different clusters or if their like-lihood L v ij is rather small, then the value z v i ( j ) is small. Thus, this definition of z v i characterizes the affinity of the objects with respect to object i and to the current clustering structure. Indeed, when i 6 = j , z v i ( j ) can be regarded as a similarity score between object i and object j with respect to the clustering structure in the v -th view. We propose several types of anomaly score computation. Here, we consider the two-view case. However, the proposed method is much more general and can be adapted without effort to the case where more than two views are considered. In any case, object i is flagged as an anomaly if the score A z 1 i , z 2 i is smaller than the threshold  X  .
 Distance based: We define a first anomaly score as: P earson X  X  correlation based: A z 1 i , z 2 i can also be com-puted from the Pearson X  X  correlation between the affinity vectors z v i of object i in both views.
 Spearman X  X  correlation based: We define another ano-maly score A z 1 i , z 2 i as being the Spearman X  X  correlation between the affinity vectors z v i of object i in both views. Independence based: The last anomaly score is given by with Figure 2: Synthetic dataset:  X  X tructures X  ( 2 classes, 2000 objects, 4 dimensions). Figure 3: 2D Synthetic datasets:  X  X ircle X  (2 classes, 2000 objects) and  X  X piral X  (3 classes, 5000 objects). where I n is the identity matrix of dimension n and 1 n is a n -dimensional column vector with each element equal to 1. Note that the summation of all the anomaly scores A z 1 i can be interpreted as a variant of a kernel-based indepen-dence measure called the (empirical) Hilbert-Schmidt Inde-pendence Criterion (HSIC) [ 5]: where tr(  X  ) denotes the trace. HSIC always takes a non-negative value, and is zero if and only if two random vari-and z 2 i , i.e. the object in view 1 and view 2, are independent.
We now experimentally show the effectiveness of the pro-posed method on synthetic and benchmark datasets.
 Datasets: We consider seven datasets for our experiments: one 4D and two 2D synthetic datasets, namely  X  X tructures X ,  X  X ircle X  and  X  X piral X  shown in Figures 2 and 3 respectively, and f our benchmark datasets, namely  X  X ris X ,  X  X etter X ,  X  X aveform X  and  X  X oo X , selected by Gao et al. [ 4] from the UCI machine l earning repository [ 1]. Those datasets are not multi-view d atasets. We thus simulate two views by following the ap-proach in [ 4]. The approach consists in splitting the object f eature representation into two subsets, where each subset is considered as one view of the data. In order to generate an anomaly, we take two objects from two different classes and swap the subsets in one view but not in the other. We randomly perturb 10% of the data in that way. 1 I n theory, the independence is assured if we use a universal reproducing kernel such as the Gaussian kernel. Figure 4: Evolution of AUC with the number of clusters k and the penalty term m for HOAD met-hod. Those results are obtained for the  X  X tructures X  dataset and are averaged over 50 runs. The Gaussian kernel is used to compute the affinity matrix.
 Setup: We compare our affinity propagation based method, or  X  X P X , to the spectral clustering based method, or HOAD, proposed in [ 4]. The methods are compared based on the a rea under ROC curve (AUC) as proposed in [ 4]. For AP, we s et the values L v ii to the median of the non-diagonal elements of L v , as proposed in [ 3]. As for HOAD, we need to set two p arameters: k and m . Parameter k specifies the number of clusters in which HOAD should cluster the objects and m is a penalty term penalizing the clustering if one object is clustered differently in several views. For each dataset, we select, from a list of possible values, the pair of values ( k  X  , m  X  ) that give the best results on that dataset. Both AP and HOAD need a likelihood, or affinity, matrix as input. We compare two types of affinity measures. The first one is the negative L2-norm: and the second one is the Gaussian kernel: w here  X  is a kernel parameter whose value is given by  X  = 2 tic [ 8]. Each method is applied 50 times on randomly per-t urbed versions of each dataset by using the aforementioned trick to generate the anomalies.
 Results: Figure 4 first shows the influence of parameters k a nd m on the performance of HOAD on the synthetic dataset  X  X tructures X . The performance critically depends on the values given to k and m . Note that, when we explore a range of possible values of parameter k , parameter m is fixed to its optimal value m  X  . We proceed in the same way when we explore the range of possible values of m . Table 1 shows the AUC for the different settings and the different d atasets. We statistically compare HOAD to AP (HSIC). A bold font indicates that the method is significantly better than the other one according to a one-tailed Student X  X  t-test ( p = 0 . 01). From these results, we clearly see that our method outperforms HOAD in all the considered datasets.
In this paper, we developed a new anomaly detection ap-proach for multi-view data. The main idea behind the ap-proach is to compare the objects based on their neighbor-hoods in the different views. In order to achieve that, we first perform clustering separately in the different views, and then compute clustering-based affinity vectors for each ob-ject. Each affinity vector is computed such that it takes into account the clustering results in the current view. An ano-maly score is then computed for each object by comparing its affinity vectors with each other. Through experiments on several datasets, we showed that the proposed method out-performs existing methods. Moreover, the proposed method does not require a careful tuning of parameters and can au-tomatically adjust to different clustering structures with no outside intervention.

