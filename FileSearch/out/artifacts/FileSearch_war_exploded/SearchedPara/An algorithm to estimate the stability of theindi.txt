 Department of Bioinformatics, United Institute of Informatics Problems, Surganova Street 6, Minsk, 220012, Belarus Tel.: + 375 17 2842 092; E-mail: novosel@newman.bas-net.by 1. Introduction
Clustering is one of the important steps in a data-mining process and is widely used for the exploration of natural groupings in the analysed data. In the field of bioinformatics the clustering algorithms are extensively used to extract novel biological knowledge from a bio-molecular data, e.g. functions of the unknown genes and proteins [7], new subtypes of malignancies [6] etc. It is very important not only to obtain some data clustering, but also to asses the reliability of clusters. For this purpose, different validity measures were proposed [8,18]. Several measures are specially developed for the analysis of bio-molecular and genetic data [5,10], aiding in revealing the hidden data interrelations in the field of genomics and proteomics. Nevertheless, each validity measure must be always applied with caution because, as a rule, it has the internal bias to particular clustering parameters or a clustering algorithm objective function [9].

Recently, a new technique to the validation of clustering results made it possible to bypass the bias problem by applying the so-called notion of stability. The stability approach to cluster validation does not take into account the cluster appearance but assumes that the right clustering result is the one that is stable. This means that a clustering algorithm should obtain similar results if applied to several data sets from the same underlying model or of the same data generating process. The stability approach differs from the standard validation techniques [8] in the repetitive re-clustering of the data, generated by per-turbing or resampling the original dataset. After that, the consistency of the re-clusterings is estimated by the external validation measures and it serves as an indicator of significance of clusters from the original dataset. There are several approaches to the estimation of stability of clustering results with the emphasis on bio-molecular data [2,3,12 X 16]. Those approaches utilize several perturbation techniques to construct the data subsets for re-clustering and several external validity measures and stability in-dices [4]. All the stability techniques are based on the clustering results with a pre-defined number of clusters. For bio-molecular discovery it is very crucial to extend the stability technique to the estimation the number of clusters and is necessitated by the following reasons: 1) It is possible that the dataset cannot be exactly divided into several independent clusters as only 2) The clusters of bio-molecular data may be highly intersected with each other or even embedded 3) Bio-molecular data can have a very high number of possible clusters at different levels of hierarchy,
After the estimation of all the individual clusters at the different levels of hierarchy it is possible to select the most significant ones and analyze them from the biological point of view. The first approach in this direction, the stability-based algorithm, has been proposed in [1], wh ere the authors directly applied a stability meas ure that estimated the relia bility of each individual clust er in the dendr ogram computed by the hierarchical algorithm. In our view, the deficiencies of the stability-based algorithm in [1] are an unsuitable construction of the consensus (stability) matrix, which takes into account all the hierarchy levels to calculate the pairwise similarities of the data objects. Such an approach might not produce reliable estimations of the cluster stability indices at the higher levels of hierarchy. The main purpose of our research was to overcome the deficiencies of the previous work [1] and to develop a stability-based algorithm for the clustering hierarchy estimation. Such an algorithm must be capable of producing results corresponding to the real structure of the analyzed dataset. The idea of constructing several clustering hierarchies in order to estimate the stability of individual clusters is not original [1], but the calculation of the consensus matrix is innovative. Our HClusterV algorithm improves the reliability of the stability indices X  values, especially for the clusters at the higher levels of hierarchy. The experiments on the simulated datasets and the comparative analysis confirmed the advantages of our approach. We also applied the proposed algorithm to two real datasets and compared the results with ones in [3,14], where the stability technique was applied to estimate the individual clusters in the non-hierarchical context. 2. Backgrounds 2.1. Standard stability-based algorithm, consensus matrix and estimation of the individual clusters
In what follows, we will describe the standard stability-based algorithm to estimate the reliability of clustering results and to determine the suitable number of clusters usually consisting of the steps described in [4]. Under stability we understand the maintenance of the clustering result under mul-tiple data perturbations. Several perturbation techniques have been proposed, ranging from bootstrap techniques [2,14], to random projections to lower dimensional subspaces [3,15] and to noise injection procedures [13]. The clustering then is considered reliable if it is approximately maintained across mul-tiple perturbations [3]. The stability indices are designated to estimate the level of the above-mentioned repeatability and several stability indices have b een proposed in the l iterature [12 X 15]. According to taxonomy [3], they can be divided into indices that use statistics of the clustering similarity measures or their overall empirical distribution.

On a very general level standard stability-based algorithm works as follows: 1. For a pre-defined number k of clusters randomly perturb the data many times according to a given 2. Apply a given clustering algorithm to the perturbed data. 3. Apply a given clustering similarity measure (external validation measure) to multiple pairs of k -4. Use appropriate stability indices to assess the stability of a given clustering. 5. Repeat Steps 1 to 4 for multiple values of k and select the most stable clustering(s) as the most
One of the approaches to estimate the stability indices of both the whole clustering result and the individual clusters is to construct the consensus matrix, which is based on the repeated construction of the pairwise similarity matrix. Let X = { x 1 ,x 2 ,...,x n } be the dataset to be clustered, where x i  X  R d and n is the number of data objects. Let C ( X,k )= A 1 ,A 2 ,...,A k , k i =1 A i = X be a partition or clustering of the dataset X into k clusters. For each clustering C = C ( X,k ) , a pairwise similarity matrix is defined as
The construction of the consensus matrix M , described below, is based on the assumption that in order to create the different perturbed datasets the random projections to lower dimensional subspaces are used [4]. According to random projection technique, the mapping  X  from a high to a low-dimensional space preserves the distances between points. The mapping  X  : R d  X  R d ,where d &lt;d ,isdefinedby d  X  d matrices with random orthonormal vectors. In comparison to the resampling technique [16], the cardinality of the dataset remains unchanged  X  all the data objects are used for the re-clustering. The clustering algorithm is then applied to each of the perturbed datasets and the consensus among the multiple runs is assessed by the consensus matrix. The matrix M is constructed in several steps [4]: 1) The number of perturbed datasets D (1) ,D (2) ,...,D ( H ) are created by the random projections of 2) For each dataset D ( i ) ,i = 1 ,H , the clustering algorithm C is applied to construct the correspond-3) The elements of the consensus matrix M are calculated as follows: The consensus matrix M is symmetric and each element in M takes the value in the interval [0 , 1] . The perfect consensus matrix corresponding to the stable clustering of the perturbed datasets has only 0 or 1 as its entries.

Using the consensus matrix M , the cluster consensus or the stability index s for a cluster A i is calcu-lated as follows:
The stability index S ( k ) for the whole clustering result C ( X,k )= A 1 ,A 2 ,...,A k , k i =1 A i = X in the original space is defined as an average value of the stability indices for the individual clusters: where k is the number of clusters.
 to the clustering result maintained across all clusterings of the perturbed data; 0 corresponds to the fully unstable clustering, i.e. each cluster A i of the clustering C ( X,k )= A 1 ,A 2 ,...,A k is disjoint from all clusters across all clusterings C ( D ( i ) ,k )= B D 2.2. Algorithm to asses the stability of the hierarchical clusters
The stability-based algorithm for the estimation of hierarchical clusters in [1] follows the standard scheme, where the multiple hierarchical re-clustering of the perturbed data are performed in order to estimate the reliability of the individual clusters in the hierarchy. A particular concern is paid to the construction of the consensus matrix, which takes into account all the levels of hierarchy.
In the above algorithm from [1], each node of the hierarchical tree represents the individual cluster and, consequently, 2 n  X  1 clusters are formed after clustering each perturbed dataset. The authors consider only n  X  2 individual clusters, excluding the leaves of the dendrogram (singleton clusters) and the  X  X oot X  cluster. The latter clusters are always present in any hierarchical clustering and hence their stability is always 1. The construction of the consensus matrix in such a case takes into account the coexistence of the data objects in n  X  2 clusters or nearly all the levels of the hierarchy. Taking into consideration the sum of depths of the hierarchical trees for each perturbed data D ( i ) ,i = 1 ,H , the consensus matrix in [1] is calculated by the following algorithm: 1. Repeat Steps 2 X 5 for each perturbed dataset D ( i ) ,i = 1 ,H . 3. Generate the clusters of the form: C ( i ) = B 4. Modify the overall depth value d = d + depth ( C ( D ( i ) ))  X  1 . 5. Calculate the elements of the consensus matrix using the equation 6. Calculate the elements for the whole consensus matrix using the equation After forming the consensus matrix M , the stability indices for the n  X  2 hierarchical clusters lated by Eq. (4).
 The authors of [1] have considered the reliability threshold  X  in order to select the most stable clusters R  X  from the whole hierarchy such as R  X  = of the threshold value on the cluster size.

We have reproduced the algorithm from [1] in R-language using the cluster R package [4], which implements a set of functions to assess the reliability of clusters discovered by clustering algorithms. After several experiments on the simulated datasets, the results of which will be described in Section 3, we have noticed the discrepancy between the real structure of the simulated data and the stability indices of several clusters. To our opinion, the reason for such a mismatch might be the unsuitable calcula-tion of the consensus matrix, proposed in [1]. To correct the detected discrepancy we propose the new stability-based algorithm, which is based on the original approach to the consensus matrix calculation. The new algorithm demonstrates reliability in reflecting the known structure of the simulated datasets and therefore can be more trusted in experiments with real datasets. 3. Proposed stability-based algorithm and implementation
In our research, we rely on the algorithm for the construction of the hierarchical clustering ensem-bles in order to estimate the stability of the individual clusters at the different levels of hierarchy [1]. As opposed to the standard stability techniques, this approach does not require to set the number of clusters. The stability indices of the individual clusters in the dendrogram are directly estimated, taking into account all the levels of hierarchy. In such a case, several stable clusters at different levels can be distinguished at parallel, without the necessity to analyze the hierarchical tree at the several cut levels.
According to the proposed algorithm, the consensus matrix is constructed under a cardinally different to encode the results of a single hierarchical clustering. Given the hierarchical clustering of the dataset X = { x 1 ,x 2 ,...,x n } as C ( X )= A 1 ,A 2 ,...,A 2 n  X  1 , a matrix P is formed as follows: 1. Columns of the matrix P encode the content of individual nodes of the dendrogram, excluding the 2. Rows of the matrix P correspond to the individual data objects in X . For example, for the clustering shown in Fig. 1, the corresponding matrix P is as in Table 1.
The sum of elements in i th row of matrix P , corresponding to i th data object is equal to the number of clusters in the hierarchy containing this object.
 The main idea of our algorithm is the following: because we need to construct the consensus matrix M for multiple hierarchical clusterings, encoded in the form of the matrix P , we need to develop an approach allowing us to combine the matrices P in such a way that some criterion is optimized. Since the optimal criterion for simultaneous combination of H clustering results is hard to formalize, we adopt the heuristic approach, which iteratively seeks the optimal combination of the two matrices. The task of combining the two matrices P is considered as an assignment problem, which is one of the fundamental combinatorial optimization problems. It consists of finding a maximum weight matching in a weighted bipartite graph. Computa tional complexity for solving this tas k by the well-known Hungarian method is O( k 3 ) (see [16]). In particular, the following task is formulated:
Let C ( D (1) )= B clusterings (without leaves and root node) for two perturbations D (1) and D (2) of the original dataset X , which are characterized by matrices P (1) and P (2) , respectively. The optimization problem consists in finding such a permutation  X  of the columns of the matrix P (2) (the assignment of columns of the matrix P (2) to columns of the matrix P (1) ) that the cost function F is minimized. The value of the cost W encodes the cost of assignment of the j th column of the matrix P (2) to the i th column of the matrix P f ( i, j ) ofthepairofclusters ( i, j ) from different clusterings. The two terms are calculated as follows: where n is the cardinality of the dataset X . Let us define the elements of the cost matrix as follows:
Let col 1 = 1 ,n  X  2 and col 2 = 1 ,n  X  2 be the column vectors of the matrices P (1) and P (2) , respec-tively. So the permutation  X  setting the columns matching is one which minimizes the cost function: where Thus, the pseudocode of the proposed stability-based algorithm is as follows:
The input data for the algorithm is indicated in lines 1 X 7 of the pseudocode. In lines 4 X 6, several perturbed datasets D ( i ) ,i = 1 ,H , are formed. Then the clustering of the original dataset X = { x i  X  R d , 1 i n } is performed (lines 9 X 10) and the matrix P is constructed (line 11). In lines 14 X 32, the general consensus matrix M is iteratively constructed. The number of iterations is equal to the number P matrix are created, and then the assignment optimization problem is solved (lines 25 X 29). At the first iteration, the consensus matrix is equivalent to the matrix P (1) , which encodes the hierarchical clustering result for the dataset D (1) (line 20). At the end of the iteration cycle, the consensus matrix is updated by summing up the elements of the previous consensus matrix and the matrix P ( i ) ,i = 1 ,H (line 31), whose columns are permutated according to the optimal solution of the assignment problem for two matrices. The stability indices of the individual clusters of the original dataset are calculated in lines 34 X 42 of the algorithmic scheme. 4. Datasets and experiments 4.1. Datasets
We tested the proposed stability-based algorithm on the two simulated datasets and two real gene expression datasets.

The simulated data are generated according to the R-procedures of the clusterv R package [3]. The first simulated dataset Sample 1 is the normally distributed multivariate synthetic dataset. It consists of 3 clusters with 5 examples for each class. Each example is characterized by 2000 components (features). The first class has its components centered in 0 (vector of length 2000). The second class has its com-ponents centered in 4 (vector of length 2000). The third class has its components centered in  X  4(vector of length 2000). For all classes the covariance matrix is diagonal with values sigma =1 .

The second simulated data Sample 2 consists of 5 examples for each from 6 classes. All classes (each consisting of 5 examples) have 1000 components. The clusters have a hierarchical structure: 2 or 6 clusters may be detected. The two main clusters are centered in 10 and  X  10. Around each main cluster three other subclusters are generated using the displacement d = 3.

The first real Leukemia dataset includes the bone marrow samples obtained from acute leukemia patients at the time of diagnosis: 25 acute myeloid leukemia (AML) samples; 9 T-lineage acute lym-phoblastic leukemia (ALL) samples; and 38 B-lineage ALL samples. Each sample is described by the values of 100 features (genes) with the largest variation across samples. The second real DLBCL dataset consists of 58 patients with diffuse large B-cel l lymphoma (DLBCL) and 19 with follicular lymphoma (FL), described by the expression of 6286 genes after preprocessing.
 4.2. Experimental results and discussion
The main steps of our experimental design are as follows: 1) The proposed stability-based clustering algor ithm HClusterV and the one, proposed in [1] for 2) The proposed stability-based clustering algor ithm is applied to the Leukemia dataset, the most 3) The proposed stab ility-based clustering algorithm is applied to the DLBCL dataset, the most reli-
We used the hierarchical clustering algorithm to generate the partitions of the original X and the perturbed datasets D ( i ) ,i = 1 ,H . To generate the perturbed datasets and to construct the consensus matrix, we performed 50 repeated random projections to lower dimensional subspace, whose dimensions correspond to predicted distortions 1+ epsilon with epsilon equal to 0.2 [1]. 4.3. Results on simulated data
The hierarchical clustering result for the dataset Sample 1 is presented in Fig. 2, where the three reliable clusters can be visually observed.

In Tables 2 and 3, the consensus matrix and the stability indices of the clusters in hierarchy, obtained by applying the algorithm from [1], are presented.

According to Table 2, the values of the consensus matrix for the pairs of elements that belong to the same original cluster are higher than the others, but the stability indices for each of three clusters are very low (less than 0.4) for the possibility to consider them as reliable (see clusters No. 10, 11, 12 in Table 3). The result contradicts the pre-defined structure of the dataset Sample 1, according to which the stability indices of three clusters must be equal to 1. Such a result with a definite contradiction inspired us to develop the HClusterV algorithm. We applied the new algorithm to the dataset Sample 1andthe results of visual inspection of the stability indices were very encouraging. The consensus matrix and the of pre-defined clusters (see clusters No. 10, 11 and 12 in Table 5) are equal to 1. The consensus matrix M is constructed according to proposed algorithm, where the columns indicate the reproducibility of the individual clusters in the hierarchy.

We proved the compliance of the results of the HClusterV algorithm to the artificially simulated data structure of the second dataset Sample 2. The clustering of the dataset Sample 2 in the original feature space is presented in Fig. 3, where the two-level hierarchical data structure (two clusters at the first level and six clusters at the second level of the hierarchy) can be clearly seen.

In Tables 6 and 7, the stability indices of the clusters in hierarchy, obtained by applying the HClusterV algorithm and the algorithm from [1], are presented. Again, the discrepancy of the stability indices and the real data structure can be clearly seen using the algorithm from [1]. With our algorithm we obtained the acceptable results.

According to the algorithm from [1] (Table 7), the pre-defined clusters in the dataset Sample2 ,espe-cially the two clusters at the first level of hierarchy, have insufficiently low stability indices to consider them reliable. To our opinion, the reason for such low values of stability can be the utilizing of all the levels of hierarchy for the calculation the elements of the consensus matrix or the cooccurrences of the pairs of data objects.

It turned out that a further use of the consensus matrix to estimate the stability of clusters is more reliable for the lower-sized clusters than for the big ones. Such a conclusion gave rise to the idea to develop a new approach to the stability estimation. According to such an approach, implemented in the HClusterV algorithm, the values of the consensus matrix for the pairwise co-existence of the data objects differ for different levels of hierarchy. The proposed HClusterV algorithm clearly identified the eight pre-defined clusters in dataset Sample 2 (see clusters No. 15, 17 X 18, 23 X 24 and 27 X 28 with stability indices equal to 1 in Table 6). 4.4. Results on real data
We used the proposed HClusterV algorithm to reveal stable clusters in the Leukemia dataset. For this dataset, each data object is in itially assigned to one of the three subtypes of leukemia. With our algorithm, we proved the reliability of the predefined subtypes and revealed the more stable subclusters inside the predefined ones. The hierarchical clustering result for the Leukemia dataset is presented in Fig. 4, where the clusters corresponding to the cut of the dendrogram at 3 clusters level are framed. According to Fig. 4, each of the three marked clusters consists mostly of the data objects of one of the leukemia subtypes. The results of our algorithm assign the sufficiently high values of stability indices for these clusters (Table 8). The B-ALL cluster is not pure as it contains four AML cases. A further observation of the results revealed that for two subclusters of the B-ALL cluster (rightmost in Fig. 4), the stability indices have cardinally different tendency: the stability index of the subcluster with only B-ALL cases increases and is equal to 0.939, while the stability value of the another impure subcluster decreases and reaches the value of 0.22. One more interesting observation is the existence of more stable subclusters in the T-ALL cluster, where the stability index for the rightmost branch of the T-ALL cluster is equal to 0.907, and for the leftmost it is 0.97.

Our results confirmed the assumption from [14] that the class of acute lymphoblastic leukemia can be further partitioned into biologically meaningful sub-classes. Therefore, it is possible that the subclass structure we discover within the T-lineage ALLs reflects a biologically meaningful distinction, which is shown in Table 9.
 For the DLBCL dataset, each data object is initially assigned to one of two subtypes of lymphoma. The hierarchical clustering result for the DLBCL dataset is presented in Fig. 5, where the clusters corre-sponding to the cut of the dendrogram at the 4 th clusters level are framed (in order to separate the cluster of FL data objects).
 According to Fig. 5, all four clusters are sufficiently perfectly separated, where the first cluster is com-posed by the FL patients (marked with letter F), the second is composed by both the DLBCL (marked with letter D) and FL patients, while the third and fourth clusters contain exclusively the DLBCL pa-tients. According to the results of our algorithm, all but one cluster have the sufficiently high values of branch with DLBCL cases (stability index is equal to s =0 . 84) is divided into two subbranches (cluster No. 3 with the stability index s =0 . 9 and cluster No. 4 with s =0 . 814 in Table 10). Also, the cluster with the mixture of FL and DLBCL cases with low stability index s =0 . 36 (cluster No. 2 in Table 10) contains the subclusters with very high stability indices (see Table 11): cluster No. 3 ( s =0 . 93) ,cluster No. 5 ( s =0 . 96) and cluster No. 6 ( s =0 . 965) . Such results assume the existence of the reliable clus-ters both at the higher and the lower levels of hierarchy. This confirms the bio-medical findings on the existence of the DLBCL subclasses and the acquisition of the characteristics of the DLBCL subtype by some FL patients [3]. The list of the reliable clusters, obtained by the HClusterV algorithm according to the stability threshold,  X  =0 . 9 is shown in Table 11 (except the clusters with only two elements).
The main advantage of the proposed stability-based algorithm HClusterV to estimate the hierarchy of clusters consists in the possibility to simultaneously estimate the whole dendrogram and to extract the most reliable individual clusters without need to choose certain cut levels. 5. Conclusion
In this study, we proposed a new stability-based algorithm to estimate the reliability of the individual clusters in the hierarchy. We analyzed the deficiencies of the previous algorithm [1], designed to solve the same task, and tried to improve it by considering the new form of the consensus matrix and the new principle of its construction. Based on the idea of constructing several clustering hierarchies in order to estimate the stability of i ndividual clusters and considering the proposed consensus matrix, we developed the new algorithm HClusterV featuring the improved reliability of the stability indices X  values, especially for the clusters at the higher levels of hierarchy. This is confirmed by the experiments on the simulated datasets with the comparative analysis of our algorithm with the one from [1]. The application of our algorithm to the real datasets enables us to reveal the reliable clusters at the different levels of hierarchy without the necessity to determine the cut level of the dendrogram. Considering the stability indices of clusters calculated by our algor ithm for the DLBCL dataset and the corresponding stability values obtained by the algorithm from [3], we can observe the similarity of the results. Such an observation can serve as an evidence of the applicability of our approach to the estimation of the clusters in the whole hierarchy.

We believe that our study might provide new insights into the applications of different measures to estimate the stability of clustering results of bio-molecular data.
 References
