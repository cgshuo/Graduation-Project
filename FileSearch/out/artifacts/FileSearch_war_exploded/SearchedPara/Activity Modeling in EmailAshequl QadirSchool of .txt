 ahmed.awadallah@microsoft.com Activities are a prominent characteristic of a work-place, typically governed by people X  X  job roles and work responsibilities. Examples of workplace activ-ities can include organizing a conference, purchas-ing equipment, managing candidate interviews, etc. Activities can be viewed as a collaborative work practice involving a set of people each playing a different role in the activity (Dredze et al., 2006). Although emails are an integral part of workplace communication, current email clients offer little sup-port for the activity oriented use of email (Khous-sainov and Kushmerick, 2005). Discussions can get split across long email threads and communications about all activities can get intermixed, making activ-ity management difficult (Balakrishnan et al., 2010).
In this work, we model activities as latent proba-bility distributions personalized to the email sender. We present three variants of the activity model, in-corporating: (1) email recipients, (2) email recip-ient pairs which account for co-occurrence of the email recipients, and (3) email body and subject to-kens along with email recipient pairs. Additionally, we experiment with lexical (bag of words), syntac-tic (nouns and verb phrases), and semantic (things of interest in an email) representations of the body and subject tokens of an email. The parameters of the generative model are learned using an expectation maximization (EM) algorithm.

For evaluation, we formulate a real world task set-ting for email recipient recommendation, where we assume that all but the last recipient of an email has been entered by the sender, and we test the effective-ness of our activity model in recommending the last recipient. Such a system has practical applications, such as reminding an email sender about a poten-tially forgotten recipient or recommending the next recipient as the sender enters each recipient.
The main contributions of our research are: Prior research related to our work can be divided into the following three major areas presented below. 2.1 Activity in Emails Prior research has treated emails as a communica-tion tool for workplace activities (Moran, 2005) or a task management resource (Bellotti et al., 2003). Kushmerick and Lau (2005) formalized e-commerce activities as finite-state automata, where transitions among states represent messages sent between par-ticipants. Dredze et al. (2006) used user generated activity labels and classified emails into activities us-ing overlapping participants and content similarity. Minkov et al. (2008) modeled user created folders and TO-DO items as activities, and created a hetero-geneous graph to perform activity-centric search.
Shen et al. (2006) predicted tasks associated with an incoming email by leveraging email sender, re-cipients and distinct subject words. They found the body words to not provide additional prediction value. Although they used similar information as we do, they used a combination of generative and dis-criminative models toward task classification, and did not do recipient recommendation. Our activity model designs are closer to the model introduced by Dredze and Wallach (2008), who presented a Dirich-let process mixture model combined with author and thread information. Our designs differ as we use co-occurring recipients in the generative process, and use various linguistic representations of content. 2.2 Generative models for Emails Latent Dirichlet Allocation (LDA) (Blei et al., 2003) is a frequently used generative topic model. Assum-ing a Dirichlet prior, LDA models learn probability distributions of words as latent topics in a corpus. In emails, LDA models have been used for learn-ing summary keywords (Dredze et al., 2008), ana-lyzing how topics change over time (Wang and Mc-Callum, 2006), understanding entity relations (Bala-subramanyan and Cohen, 2011), analyzing commu-nication networks (Nguyen et al., 2013), for author-ship attribution (Seroussi et al., 2012), and discover-ing topics associated with authors (McCallum et al., 2005).

Other generative models have also been used for analyzing email communication behavior (Navaroli et al., 2012), identifying links between an email sender and a recipient to detect potential anoma-lous communication (Huang and Zeng, 2006), and resolving personal names in emails (Elsayed et al., 2008). Representing workplace activities of the emails with probabilistic inference in graphical models where observed information is personalized to the email senders is what sets our work apart from previous research in computational models for emails. 2.3 Email Recipient Recommendation For recommending email recipients, interactions among email participants and content similarity are the signals that have been explored most. Car-valho and Cohen (2007) leveraged content similar-ity by creating tf-idf centroid vectors and determin-ing k-nearest neighbors of a target email. Pal et al. (2007) presented a discriminative author recipient topic model that uses transfer learning. Desai and Dash (2014) used reverse chronologically arranged implicit groups determined from sent emails. Sofer-shtein and Cohen (2015) created a ranking function combining temporal and textual features.

Among the generative modeling based ap-proaches, Pal and McCallum (2006) learned prob-ability distributions of recipients, and words in body and subject to predict recipients in email cc lists. Dredze et al. (2008) evaluated the impact of sum-mary keywords generated using LDA, for email re-cipient prediction. More recently, Graus et al. (2014) predicted email recipients by estimating sender and recipient likelihood using a communication graph, email likelihood using content words, and evaluated performance on the Avocado email corpus. In our work, we use latent activity distributions, and iden-tify senders who engage in similar activities. We compare our recipient prediction results against the generative model of Graus et al. (2014). 3.1 Activity Modeling in Emails Our motivation for activity modeling in email stems from the assumption that in the workplace, people primarily use emails as a communication tool for their ongoing activities, and an email X  X  recipient list, content, and other context are governed by a given activity. For example, an employee attending a con-ference may write emails to the conference organiz-ers regarding registration or scheduling, or emails to a hotel for booking confirmation. The commu-nication may span multiple emails, involving many parties, but all under the same activity.

We model the activities as a latent probabilistic variable over the email recipients and content, per-sonalized to the email sender. Let D be the set of all emails in a corpus containing N emails, generated by S = { s i | 1  X  i  X  S D } senders, and sent to R = { r i | 1  X  i  X  R D } recipients. Let B = { b i | 1  X  i  X  B D } , and T = { t i | 1  X  i  X  T D } rep-resent the body and subject vocabulary of the emails respectively. Let K be the number of latent activities for each sender. We model the activities as probabil-ity distributions over email components S , R , B and T . 3.2 Corpus Description and Data Sets For our experiments, we use the Avocado email cor-contains emails from a defunct IT company referred to as  X  X vocado X . For learning the activity model, we extract emails from 7/1/2000  X  5/1/2001 to create a training data set, and from 5/1/2001  X  6/30/2001 for development/tuning. In this work, we did not con-sider threaded conversations, only retained the first email in a thread and discarded the rest.

As additional filtering steps, we only kept emails written by the Avocado employees, allowing us to confine the scope of the activities within the com-pany. To control sparsity and noise, for each email, we enforced a a minimum of two recipients, and a we consider it reasonable that a model only acti-vates after some history of email is sent by a user. We therefore removed emails by the senders hav-ing fewer than 25 emails in the training data. From email bodies and subjects, we removed stopwords, and words appearing fewer than 5 times or more than present multiple times, we took it only once, as well as removed a sender X  X  email alias from the recipi-ents list if it was present there. In this work, we focused on recipients from only the  X  X O X  field, and did not include recipients from the  X  X C X  or  X  X CC X  field. Table 1 presents statistics of the data sets. Our key assumption in modeling the activities in email is that different components of an email con-tain specific types of information that can help to characterize the activities that drive user behavior. In our generative process of the activity model, for an email d  X  D , a sender s  X  S is first gener-ated from a multinomial distribution with probabil-ity vector  X  , then an activity a is generated from a sender personalized multinomial distribution with probability vector  X  s . Let R d  X  R , B d  X  B and T d  X  T be the set 4 of recipients, body and subject tokens of d respectively. The generation of the email contexts (recipients and body/subject tokens) varies based on the specific design of each variant of our model. In a first simplistic model, we assume that recipient r  X  R d , body token b  X  B d and subject token t  X  T d for an email can be generated from the multinomial distributions with probability vec-tors  X  s,a ,  X  s,a , and  X  s,a respectively, that are condi-tioned s and a . Point estimates for  X  can be directly calculated from a training corpus, whereas  X  ,  X  ,  X  , and  X  are the unknown parameters of the model. 4.1 Model 1: Rec In our first model Rec , we assume that the latent activities can be learned as a probability distribution over the recipients alone. The generative process is: Figure 1 presents the plate diagram of the model. The joint probability of the Rec model is the product of the conditional distributions: P ( s,a,r |  X , X , X  ) = P ( s |  X  ) P ( a | s, X  )
The probability of a sender s , an activity a given s , and a recipient r given s and a are defined below 5 : P ( a =  X  a | s =  X  s ) =
Inference: Let d n be the n th email, where d n = distribution over the activities P n ( a | d ) , which is di-rectly proportional to the joint distribution P n ( a,d ) . We can exactly compute this distribution by evaluat-ing the joint distribution for every value of a and the observed document d n .

Learning: Point estimates for  X  can be directly obtained from the training corpus. We estimate the parameters  X  and  X  by maximizing the (log) proba-bility of observing D . We write the log( D ) as: log P ( D ) =
We use the Expectation-Maximization (EM) al-gorithm to set the parameters. Starting with a ran-dom initialization of the parameters (with Gaussian noise), EM iterates between the E-step in which P n ( a | s,R d ) is computed for each email with fixed parameter values computed in the previous M-step, and the M-step in which the parameters are updated with fixed P n ( a | s,R d ) values computed in the E-step. The parameter updates are obtained by taking the derivative of log P ( D ) with respect to each pa-rameter, and setting the resultant to 0, providing us with the following parameter updates:
We run EM until the change in log P ( D ) is less 4.2 Model 2: CoRec Using co-occurring recipients in generative mod-els for emails has been rarely explored in previ-ous work. Pal and McCallum (2006) modeled co-recipient information as a probability distribution of recipients conditioned on the other recipients, and noted that this information improved their email cc prediction performance. In our CoRec model, we model co-recipients as pairs of recipients generated from a probability distribution conditioned on the sender and the activity. Let L = { ( r i ,r j ) | 1  X  i  X  R
D , 1  X  j  X  R D } having L D pairs of recipients in the corpus. For an email d , L d  X  L is the set of re-cipient pairs in d . The CoRec model first generates a sender s from the probability distribution  X  , then an activity a from a distribution over latent activi-ties  X  s personalized to s , and finally recipient pairs r p  X  L d from a distribution over recipient pairs  X  s,a conditioned on s and a . The generative process is summarized below: The joint probability of the CoRec model is:
This adds over the Rec model the probability of a recipient pair r p given s and a , defined below:
The EM algorithm is applied in the same way as in the Rec model. During the M-step, update for  X  remains the same. The update for  X  is given below: 4.3 Model 3: CoRecBT Finally, in the CoRecBT model, we further incor-porate body and subject of emails. The generative process of the model is: The joint probability of the CoRecBT model: P ( s,a,r p ,b,t |  X , X , X , X , X  ) = P ( s |  X  ) P ( a | s, X  ) where the probability of a body token b and subject token t given s and a defined as:
During the M-step of the EM algorithm, updates for  X  and  X  remain the same as the CoRec model. The updates for  X  and  X  are given below: 4.4 Subject and Body Token Representations Previous work in modeling email content mostly ex-plored bag of words (e.g., (Graus et al., 2014)) or tf-idf vectors (e.g., (Carvalho and Cohen, 2007)) as the content representation of an email. For model-ing activities in emails, we experiment with different linguistic representations of the email content. They are: 4.5 Discussion Full Bayesian Treatment: In the above mod-els, we learn point estimates for the parameters (  X , X , X , X , X  ). One can take a Bayesian approach and treat these parameters as variables (for in-stance, with Dirichlet prior distributions), and per-form Bayesian inference. However, exact inference will become intractable and we would need to re-sort to methods such as variational inference or sam-pling. We found this extension unnecessary, as we had a sufficient amount of training data to estimate all parameters reliably. In addition, our approach enabled us to learn (and perform inference in) the model with large amounts of data with reasonable computing time. To evaluate the effectiveness of our activity model, we formulate a recipient recommendation task.
Task Definition: For a test email document d containing the list of recipients R d , a modified list cipient r  X   X  R d . Given d with R  X  is to recommend r  X  as the next recipient for d . 5.1 Our Methods To recommend a recipient for a test email document d written by sender s d , we first create a candidate re-cipient list by combining recipients who received an email from s d , and recipients who co-occurred with an observed recipient r  X  R  X  Sender s d and any r  X  R  X  candidate list. Next, we determine the probability distribution of the activities in d using:
Each candidate recipient r  X  is then ranked by a score using two different methods defined below. The ranked list is used as our final recommended recipients. The two scoring methods are:
Reg Method: In the Reg method, we score using
We smooth the above function using the following linear interpolation:
P ( r  X  ,r | a,s ) =  X  1  X  P ( r  X  ,r | a,s ) + (1  X   X  1 )  X 
Here, P ( r rare ) is the lowest probability of any re-cipient in the training data. We calculate  X  i with a sigmoid logistic function, allowing us to determine when to rely more on the learned probabilities:
For  X  1 , x is the pointwise mutual information (PMI) between s and r in training data, with steep-ness parameter k = 50 . For  X  2 , x is the frequency of r in training data, with k = . 5 . Sigmoid X  X  midpoint x 0 is the first quartile ( Q 1 ) of the PMI and recipi-ent frequency distributions respectively. The above values for k have been determined from the shape of the sigmoid curves in the training data.

Sim Method: In the Sim method, we explore the idea that the activity model can be used to iden-tify other senders with similar activities as s d , who we refer to as similar senders , S  X  similar senders, we evaluate senders who maximize the log likelihood of the test document d by calcu-lating log P ( s,d ) for all s  X  S , and identify the top sender s d is not included in S  X  P ( r  X  | d ) for each s  X  S  X  along with a weight w s : The final scoring function for the Sim method is:
P ( r  X  | d ) =  X P s
Here,  X  is determined with the frequency of s d in training data, using the sigmoid function with k = 0 . 5 and x 0 as the Q 1 of the frequency distribution. 5.2 Baseline Systems As simple baseline systems to compare with our methods, we use 1) a random recipient baseline; 2) ranked recipients by P ( r = r  X  ) ; and 3) ranked re-cipients by P ( r = r  X  | s = s d ) , where the proba-bilities are calculated from the training data. We evaluate two additional generative baselines using 4) P ( r = r  X  | R  X  applying Bayes X  theorem, and assuming conditional independence among r  X  R  X  we used similar interpolation smoothing as before.
We additionally implemented the generative model presented by Graus et al. (2014) for recipient recommendation, which for test email d uses:
Graus estimated P ( d | r  X  ,s d ) by P ( b | r  X  ,s d ) where b is an observed term in the email. The eval-uation task was different from ours as they pre-dicted all recipients of an email. In our evalua-tion task, we recommend the last recipient, allow-estimating P ( d | r  X  ,s d ) . Consequently, we present 3 additional baselines adopting Graus X  method: 6) GrausB ( BOW ) method uses body words, 7) GrausB ( V P  X  TOI ) uses the verb phrases and things of interest, and finally 8) GrausR method equivalent to how we calculate our fifth baseline, P ( r = r  X  | s = s d ,R  X  the smoothing function. 6.1 Experimental Setup To evaluate recipient recommendation, we create a test data set by extracting emails from 7/1/2001  X  8/31/2001 from the Avocado data set. First we train our activity model with the training data and de-termine the optimum number of activities for each method by evaluating recipient recommendation on the development data. The number of activities per model is shown in Table 3. We then combine train-ing and tuning data to create a new training data set in order to minimize the time difference between training and test emails. From the test data, we re-moved emails that had a sender or recipient never appearing in the training data. Although this lim-its the scope of the recipient recommendation evalu-ation task, predicting a recipient for a sender who never appeared in the training data is beyond our current modeling scope and practical settings. The final test set contains 1923 emails with 14.91 emails per sender.

With the ranked lists of recipients generated by each method, we calculate precision@X (X= 1, 2, 5, 10), and MRR (Mean Reciprocal Rank). Preci-sion@X is defined as percentage of emails having the actual recipient in the top X ranked recipients. 6.2 Recipient Recommendation Results Table 2 presents the recipient recommendation re-sults for different methods. The first 5 rows show that the generative baselines from row (4) and (5) performed much better than the simple baselines (row (1) X (3)), yielding up to .2871 MRR . Compar-atively, the GrausB(BOW) baseline in row (6) that uses body words, did not perform well, which is con-sistent with the finding by Shen et al. (2006) about body words not providing additional value in their task classification work. However, the GrausB(VP X  TOI) in row (7) shows that using body terms more selectively has the potential for improving perfor-mance. Comparatively, the use of observed recipi-ents (GrausR in row (8)) substantially improved rec-ommendation results, yielding the highest MRR , precision@1, and precision@2 scores, while the generative baseline in row (5) retained the highest precision@5 and precision@10 scores.

Next, rows (9) to (13) show results for the activ-ity models that use the Reg scoring. First, the Rec model outperformed the simple baselines from rows (1) to (3) as well as the GrausB methods from rows (6) and (7), but did not perform better than the gen-erative baselines from rows (4) and (5) or GrausR. All the CoRec models performed better at recom-mending a recipient at top of the ranked lists with higher precision@1 and precision@2 scores, which are more practically useful for recommendation pur-poses, and also resulted in higher MRR scores.
Finally, the rows (14) and (15) show the results with the Sim scoring, and we observe a substan-tial improvement across the board, with verb phrase and thing of interest as the body context yielding our best results. This model achieved 3.31% addi-tional improvements in MRR, and 3.38% additional improvements in precision@1 over the best baseline results. This demonstrates that the learned activity model can be used to identify senders who are likely to engage in similar activities, improving recipient recommendation performance further.

Figure 2 shows examples of activity tokens from the emails of a sender in our training corpus, using word clouds. This is meant to serve as a case anal-ysis, but it is not straightforward to interpret word clouds. When inspecting, we found that the names of potential customers (Nokia, Siemens, SAP) in the first example are prominent in some of the emails of the sender in the raw data. The recipients in these emails form a small cluster of people who are mainly involved in discussions around a particular event (Mobile Business Forum) where these compa-nies are amongst the sponsors.

The second example, from the same sender, shows a coherent set of recipients. But in this case, the model seemed to have conflated multiple topics (such as the Palm VII device and support issues). We suspect that the cause for this confusion lies in the strong and coherent cluster of recipients which forces divergent topics to coalesce. While the com-bined signals of co-recipients and topic words im-prove the overall activity model, in some of the indi-vidual cases it leads to one signal improperly domi-nating the other. We presented a latent activity model for workplace emails where the activities are modeled as proba-bility distributions over email recipients and other contexts, personalized to the email sender. Our model incorporates co-occurring recipients as part of the generative process, and can be used to iden-tify senders who participate in similar activities, re-sulting in improved performance in email recipient recommendation. Our experiments suggest that syn-tactic and semantic knowledge such as verb phrases and thing of interests in emails can model the activi-ties much better than bag-of-words, as demonstrated by the recipient recommendation results. Learning topics and sub-activities under workplace activities is a promising research direction which we will ex-plore in future work.
