 Tao Shi taoshi@stat.osu.edu Department of Statistics, Ohio State University Mikhail Belkin mbelkin@cse.osu.edu Bin Yu binyu@stat.berkeley.edu Department of Statistics, University of California Berkeley Gaussian mixture models are a powerful tool for vari-ous tasks of data analysis, modeling and exploration. The basic problem is to estimate the parameters of a Gaussian mixture distribution p ( x ) = P G g =1  X  g p g ( x ) , from sampled data x 1 , . . . , x n  X  R d , where the mixture component p g = N ( g ,  X  g ) has the mean g and the covariance matrix  X  g , g = 1 , . . . , G . Gaussian mix-ture models are used in a broad range of scientific and engineering applications, including computer vi-sion, speech recognition, and many other areas. However, effectiveness of modeling hinges on choosing the right parameters for the mixture distribution. The problem of parameter selection for mixture models has a long history, going back to the work of (Pearson, 1894, [9]), who introduced the Method of Moments and applied it to the study of a population of Naples crabs, deducing the existence of two subspecies within the population.
 The most commonly used method for parameter es-timation is Maximum Likelihood Estimation (MLE), which suggests choosing the parameters in a way that maximizes the likelihood of the observed data, given a model. In modern practice this is most commonly done through the iterative optimization technique known as Expectation Maximization (EM) algorithm ([3]), which is typically initialized using k -means clustering. Recently significant progress on understanding theo-retical issues surrounding learning mixture distribu-tions and EM has been made in theoretical computer science, e.g., [2, 4].
 Another set of methods for inferring mixture distri-bution is based on the Bayesian inference, which is done using a prior distribution on the parameters of the model. In recent literature ([7]) the Dirichlet pro-cess mixture models were used to produce posterior distribution for parameters of a mixture model. The inference procedure involves applying Markov Chain Monte-Carlo to draw samples from the posterior dis-tribution.
 In this paper we propose a new method for estimat-ing parameters of a mixture distribution, which is closely related to non-parametric spectral methods, such as spectral clustering (e.g., [8]) and Kernel Prin-cipal Components Analysis [11]. Those methods, as well as certain methods in manifold learning (e.g., [1]), construct a kernel matrix or a graph Laplacian ma-trix associated to a data set. The eigenvectors and eigenvalues of that matrix can then be used to study the structure of the data set. For example, in spec-tral clustering the presence of a small non-zero eigen-value indicates the presence of clusters, while the cor-responding eigenvector shows how the data set should be split. In particular, we note the work [12] where the authors analyze dependence of spectra on the input density distribution in the context of classification and argue that lower eigenfunctions can be truncated with-out sacrificing classification accuracy. We will develop the intuitions and analyses underlying these methods and take them a step further by offering a framework, which can be applied to analyzing parametric families, in particular a mixture of Gaussian distributions. We would like to study mixture distributions by build-ing explicit connections between their parameters and spectral properties of the corresponding kernel ma-trices. More specifically, we construct a family of probability-dependent operators and build estimators by matching eigenvalues and eigenfunctions of the op-erator associated to a probability distribution to those of the matrix associated to a data sample. Thus given integral operator which will be the principal object of this paper. Our framework will rely on three key observations about the spectral properties of this operator and its connec-tion to the sampled data.
 Observation 1. (Single component) For the Gaus-sian distribution p = N ( ,  X ), we can analytically ex-press eigenfunctions and eigenvalues of G  X  p in terms of the mean and the covariance  X . This will allows us to reverse this dependence and explicitly express and  X  in terms of the spectral properties of G  X  p . Observation 2. (Mixture of components) Let p be a mixture distribution p ( x ) = P G g =1  X  g p g Note that by linearity It can be seen (Theorem 1) that given enough separa-tion between the mixture components, top eigenfunc-tions of the individual components G  X  p g are approxi-mated by top eigenfunctions of G  X  p . That will allow us to connect eigenfunctions/eigenvalues of the mixture to eigenfunctions/eigenvalues of the individual com-ponents. A specific example of this is given in Fig. 2, which will be discussed in detail in Section 4. Observation 3. (Estimation from data) The eigenfunctions and eigenvalues of G  X  p can be approx-imated given data sampled from p ( x ) by eigenvectors and eigenvalues of empirical kernel matrices. To highlight the effectiveness of our methodology consider the distribution in Fig. 1, where the den-sity given by a mixture of two normal distributions tained by sampling 1000 points are shown. From the Table 1, we see that the spectroscopic estimator has no difficulty providing reasonably accurate estimates for the mixing coefficients  X  1 ,  X  2 , means 1 , 2 and variances  X  1 ,  X  2 for each component, despite the fact that the mixture is unbalanced. We also see that these estimates can be further improved by using the spec-troscopic estimate to initialize EM.
 We note that, while EM is a computationally efficient and algorithmically attractive method, it is a local op-timization procedure and the quality of the achieved maximum and accuracy of the resulting estimate are sensitive to initialization (see, e.g., [10]). If the ini-tial value happens to be close to the global maximum, fast convergence can be guaranteed. However, finding such  X  X ucky X  regions of the parameter space may be nontrivial. To emphasize that point, consider the bot-tom two rows of Table 1, where the results of k -means clustering ( k = 2) and EM initialized by k -means are shown. We see that k -means consistently provides a poor starting point as the energy minimizing configu-ration splits the large component, ignoring the small one. EM, initialized with k -means, stays at a local maximum and cannot provide an accurate estimate for the mixture. On the other hand, EM initialized with our method, converges to the correct solution. We should note that our method requires sufficient separation between the components to provide accu-rate results. However there does not exist a computa-tionally feasible method for estimating parameters of a mixture distribution in several dimensions without a separation assumption.
 The rest of the paper is structured as follows: in Sec-tion 2, we describe our approach in the simplest setting of a one-dimensional component in R . In Section 3, we analyze a single component in R d , in Section 4, we deal with a general case of a mixture distribution and state a basic theoretical result for the mixture. In section 5, we show some experimental results on a sim-ulated mixture distribution with three components in R 5 and show some experimental results on the USPS handwritten digit dataset. We conclude in Section 6. We start the discussion by demonstrating the basis of our approach on the problem of estimating pa-rameters of a single univariate Gaussian distribution p ( x ) = N ( ,  X  2 ). We first establish a connection be-tween eigenfunctions and eigenvalues of the convolu-tion operator G  X  p f ( y ) = R the parameters and  X  2 . We show these parameters can be estimated from sampled data. We will need the following Proposition 1 (Refinement of a result in [13]) Let  X  = 2  X  2 / X  2 and let H i ( x ) be the i -th order Hermite polynomial. Then eigenvalues and eigenfunctions of G p for i = 0 , 1 , are given by  X  =  X  i ( x ) = Since H 0 ( x ) = 1, and putting C = (1 + 2  X  ) 1 / 8  X  0 ( x ) = C exp  X  We observe that that the maximum value of |  X  0 ( x ) | is taken at the mean of the distribution , hence derive Thus we have established an explicit connection be-tween spectral properties of G  X  p and parameters of p ( x ). We now present Algorithm 1 for estimating and  X  2 from a sample x 1 , . . . , x n from p ( x ).  X  Step 1. Construct kernel matrix K n , ( K n ) ij =  X  Step 2. Construct estimators  X  and  X   X  2 for mean These estimators are constructed by substituting top eigenvector of K n for the top eigenfunction of G  X  p and eigenvalues of K n for the corresponding eigenvalues of G It is well-known (e.g., [6]) that eigenvectors and eigen-values of K n approximate and converge to eigenfunc-tions and eigenvalues of G  X  p at the rate 1  X  which implies consistency of the estimators. The ac-curacy of  X  and  X   X  2 depends on how well the empirical operator K n approximates the underlying operator G p . The Table 2 reports the average and the standard devi-ation of our spectroscopic estimators ( X  ,  X   X  2 ) compared the standard estimators (  X  x, s 2 ) for one hundred repeti-tions of the simulation. We see that our spectroscopic estimators are comparable to the standard estimators for mean and variance of a single Gaussian. In this section we extend our framework to estimat-ing a single multivariate Gaussian p = N ( ,  X ) in R d . the covariance matrix  X . As before we put G  X  p f ( x ) = R invariant under rotations, it follows that the operator G p can be decomposed as: G an 1-dimensional Gaussian with variance  X  2 i and mean h , u i i along the direction of u i .
 It is easy to see that given two operators F , H , the spectrum of their direct sum F  X  X  consists of pair-wise products  X  , where  X  and are their respective eigenvalues. The corresponding eigenfunction of the product is e [  X , X  ] ( x, y ) = e  X  ( x ) e  X  ( y ). Applying this result, we see that eigenvalues and eigen-functions of of G  X  p can be written as products Where [ i 1 , . . . , i d ] is a multindex over all components. It can be seen that  X  [0 ,..., 0] is (up to a scaling factor) a Gaussian with the same mean as the original dis-tribution p ( x ). Thus can be estimated as the point with maximum value  X  [0 ,..., 0] in the same way as for 1-dimensional distributions.
 Consider now  X  I , where I = [0 , . . . , 0 Since H 2 ( x ) = 2 x , Eq. 1 implies that  X  I ( x )  X  linear function in x with the gradient pointing in the direction of u i . That allows us to estimate the prin-cipal directions. The resulting Algorithm 2 for esti-mating and  X  is presented below: Step 1. Construct kernel matrix K n , ( K n ) st = e operator G  X  p . Compute eigenvalues  X  ( K n ) and eigen-vectors v ( K n ) of K n . Denote the top eigenvector by v 0 and the corresponding eigenvalue by  X  0 .
 Step 2. Identify each eigenvector v i , v i 6 = v 0 , i = 1 , . . . , d such that the values of v i v linear in x , that is The corresponding principal direction u i is estimated by  X  u i = a k a k . Let the corresponding eigenvalue be  X  Step 3. Construct estimators  X  and  X   X  for mean and variance as follows: We now extend our framework to the case of a mix-ture of several multivariate Gaussian distributions with potentially different covariance matrices and mix-ture coefficients. To illustrate our approach we sam-ple 1000 points from two different Gaussian distribu-tions N (2 , 1 2 ) and N (  X  2 , 1 2 ) and from their mixture ture density is shown in the top left panel of Fig 2, and histograms of each mixture component are shown in the right top panels. Taking the bandwidth  X  = 0 . 3, we construct three kernel matrices K 1 , K 2 and K for a sample from each of the components and the mixture distribution respectively. The middle and lower left panels show the top two eigenvectors of K , while the middle and lower right panels show the top eigenvector of K 1 and K 2 respectively.
 The key observation is to notice the similarity between the left and right panels. That is, the top eigenvectors of the mixture are nearly identical to the top eigenvec-tors of each of the components. Thus knowing eigen-vectors of the mixture allows us to approximate top eigenvectors (and the corresponding eigenvalues) for each of the components. Having access to these eigen-vectors and using our Algorithms 1,2, allows us to es-timate parameters of each of the mixture components. This phenomenon is easily understood from the point of view of operator theory. The leading eigenfunctions of operators defined by each mixture component are approximately the eigenfunctions of the operators de-fined on the mixture distribution. To be explicit, let us consider the Gaussian convolution operator G  X  p de-fined by the mixture distribution p ( x ) =  X  1 p 1 +  X  2 with Gaussian components p 1 = N ( 1 ,  X  2 ) and p 2 = N ( 2 ,  X  2 ) and the Gaussian kernel K ( x, y ) with band-width  X  . The corresponding operators are G  X  p 1 and G sider an eigenfunction  X  1 ( x ) of G  X  p 1 with eigenvalue  X  G G  X  p  X  1 ( y ) =  X  1  X  1  X  1 ( y ) +  X  2 It can be shown that eigenfunction  X  1 ( x ) of G  X  p 1 is centered at 1 and decays exponentially away from 1 . Therefore, assuming the separation k 1  X  2 k is large enough, the second summand  X  hence G  X  p  X  1  X   X  1  X  1  X  1 . When the approximation holds the top eigenfunctions of G  X  p are approximated by top eigenfunctions of either G  X  p 1 or G  X  p 2 . Theorem 1 Given a d -dimensional mixture of two Gaussians p ( x ) = P 2 i =1  X  i p i ( x ) where  X  i is mix-ing weight and p i is the density corresponding to N ( i ,  X  2 I ) . Define  X  = 2  X  2 /w 2 and  X  =  X  2  X / p with an eigenvalue  X  1 0 ) of G w p eigenfunction of G w p in the following sense: For any  X  &gt; 0 we have that for all y assuming that the separation satisfies We do not provide a proof of Theorem 1 for lack of space. A more general version of the theorem for sev-eral Gaussians with different covariance matrices can also be given along the same lines. Together with some perturbation analysis ([5]) it is possible to pro-vide bounds on the resulting eigenvalues and eigen-functions of the operator.
 We now observe that for the operator G  X  p g , the top eigenfunction is the only eigenfunction with no sign change. Therefore, such eigenfunction of G  X  p corre-sponds to exactly one component of the mixture distri-bution. This immediately suggest a strategy for iden-tifying components of the mixture: we look for eigen-functions of G  X  p that have no sign change . Once these eigenfunctions of G  X  p are identified, each eigenfunction of G  X  p can be assigned to a group determined an eigen-function with no sign change. As a result, the eigen-values and eigenfunctions in each group only depend on one of the component p g and mixing weight  X  g . By reversing the relationship between parameters and eigenvalues/eigenfunctions, parameter estimations for each mixing component can be constructed based only on the eigenvalues/eigenvectors in the corresponding group. 4.1. Algorithm for Estimation of a Mixture of Following the discussion above, we now describe the resulting algorithm for estimating a multidimensional mixture of Gaussians p ( x ) = P G g =1  X  g N ( g ,  X  g ) , from a sample x 1 , . . . , x n  X  R d , first giving the following Definition 1 For vectors d, e  X  R n ), we define 1.  X  -support of d is the set of indices { i : | d i |  X   X  , i = 1 , , n } . 2. d has no sign changes up to precision  X  , if d is X Y
Eigenvectors
Eigenvectors either positive or negative on the  X  -support of e . { i : | e i | X   X  }  X  { i : | d i | X   X  } .
 Algorithm 3 . Spectroscopic estimation of a Gaussian mixture distribution.
 Input : Data x 1 , . . . , x n  X  R d . Parameters: Kernel bandwidth  X  &gt; 0, threshold  X  &gt; 0. 1 Output: Number of components  X  G . Estimated mean  X  g  X  R d , mixing weight  X   X  g , g = 1 , . . . ,  X  G and covari-ance matrix  X  g for each component.  X  Step 1. Constructing K n , the empirical approx- X  Step 2. Estimating the number of components  X  Step 3. Estimating the mean g and the mixing To estimate the covariance matrix  X  g of each compo-nent p g : we first all eigenvectors such that v ( x s ) v approximately a linear function of x s on the  X  -support of v g 0 . Then we can apply the estimation methods de-scribed in Algorithm 2 , Step 3 on the  X  -support of v . Simulation: multivariate Gaussian mixture dis-tribution.
 A simulation on five dimensional data is carried out to test the proposed algorithm. The first two variables X 1 and X 2 are a mixture three Gaussian components p ( X ) = P 3 g =1  X  g N ( g ,  X  g ) with mixing weights and group means shown in Table 3 and covariance matri-ces:  X  1 = 0 . 5  X  0 . 25 The remaining three variables are Gaussian noise N (0 , 0 . 1 I ). In each simulation run, 3000 data points are sampled. The histogram of X 1 , two-dimensional histogram of X 1 and X 2 , and histogram of X 2 for one simulation run are shown in Figure 3. We see that it is impossible to identify the number of components by investigating the one-dimensional histograms. The Algorithm 3 with  X  = 0 . 1 was used to estimate the number of components G , mixing weights  X  g . The sim-ulation is run 50 times and the algorithm accurately estimated the number of groups in 46 of the 50 runs. Two times the number of groups was estimated as 2 and two times as 4. The average and standard devia-tion of the estimators of mixing weights and means for the 46 runs are reported in Table 3. We see that the es-timates for mixing weights are close to the true values and the estimated group means are close to the esti-mates from labeled data. Covariance estimates, which we do not show due to space limitations, also show reasonable accuracy.
 USPS ZIP code data .
 To apply our method to some real-world data we choose a subset of the USPS handwritten digit dataset, consisting of 16x16 grayscale images. In this experi-ment, 658  X 3 X  X , 652  X 4 X  X , and 556  X 5 X  X  in the training data are pooled together as our sample (size 1866). The Spectroscopic estimation algorithm using a Gaus-sian kernel with bandwidth 2 is applied to the sample . Here we do not use the algorithm to estimate mean and variance of each component, since we do not expect the distribution of the 256 dimensional data to like a Gaussian distribution. Instead, we investigate the We expect (1) the data corresponding to large absolute values of each of such eigenvectors present one mode (cluster) and (2) those data points are in the same digit group.
 In the output of our algorithm, three eigenvectors v 1 , v 16 and v 49 of K n satisfy the condition of no sign change over { x : | v ( x ) | &gt;  X  } with  X  = max( v ) /n . We first rank the data by an decreasing order of | v | and show the 1 st , 41 st , 81 st , , 361 st digits in Figure 4. All digits with larger value of | v 1 | belong to the group of  X 4 X  X , and other digits ( X 3 X  and  X 5 X ) correspond to smaller values of | v 1 | . Similarly, larger values of | v are in the group of  X 3 X  X  and | v 49 | for  X 5 X  X . By assigning digits to their component defined by one of the eigenvectors ( v 1 , v 16 , v 49 ) we obtain the cluster-ing results shown in the confusion Table 4. We see that 1 = 0 the overall accuracy of clustering is 93.46%. This clus-tering method can be thought of as an extension of the framework provided in this paper. While this method is closely related to spectral clustering, the procedures for choosing eigenvectors are different. In this paper we have presented Data Spectroscopy , a new framework for inferring parameters of certain fam-ilies of probability distributions from data. In particu-lar we have analyzed the case of a mixture of Gaussian distributions and shown how to detect and estimate its components under the assumption of reasonable com-ponent separation. The framework is based on the spectral properties of data-dependent convolution op-erators and extends intuitions from spectral clustering and Kernel PCA. We have developed algorithms and have shown promising experimental results on simu-lated and real-world datasets.
 We think that our approach provides new connections between spectral methods and inference of distribu-tions from data, which may lead to development of al-gorithms for using labeled and unlabeled data in prob-lems of machine learning. The authors thank Yoonkyung Lee for helpful dis-cussions and suggestions. Mikhail Belkin was par-tially supported by NSF Early Career Award 0643916. Bin Yu was partially supported by NSF grant DMS-0605165, ARO grant W911NF-05-1-0104, NSFC grant 60628102, a grant from MSRA, and a Guggenheim Fel-lowship in 2006.
 [1] M. Belkin, P. Niyogi , Laplacian Eigenmaps [2] S. Dasgupta , Learning mixtures of Gaussians , [3] A. Dempster, N. Laird, D. Rubin , Maximum-[4] R.Kannan, H.Salmasian, S.Vempala , The [5] T. Kato , Perturbation Theory for Linear Oper-[6] V. Koltchinskii and E. Gin  X  e , Random ma-[7] S. MacEachern and P. Muller , Estimating [8] A. Ng, M. Jordan, and Y. Weiss , On spec-[9] K. Pearson , Contributions to the mathematical [10] R. Redner and H. Walker , Mixture densities, [11] B. Sch  X  olkopf, A. J. Smola, and K.-R. [12] C.Williams, M.Seeger , The effect of the in-[13] H. Zhu, C. Williams, R. Rohwer, and
