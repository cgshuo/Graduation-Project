 CLEMENT H. C. LEUNG and ALICE W. S. CHAN, Hong Kong Baptist University The sharing and retrieval of social medi a provide an immense opportunity to ex-ploit the collective behavior of community users interacting in a common environment [John et al. 2008; Lerman 2007; Shaw and Schmitz 2006; Spertus et al. 2005]. The judgment and knowledge derived from user interactions through a single system can be capitalized in different ways by preprocessing or postprocessing analysis [Badjio and Poulet 2005; Bian et al. 2008; Leung and Liu 2007; Minetou 2005; Wong and Leung 2008], which can be used to adapt and evolve the system behavior to the user X  X  current needs [Milani et al. 2008, 2009]. For example, many social media allow users to rate or vote different resources (e.g., pictures, videos, other user profiles, etc.), [Bian et al. 2008; Lerman 2007]. The collected information is then eventually used to assess the interest of resources the community is search ing. Although this form of collective rat-ing is mainly aimed at building hot links to attract user attention (such as top-ten lists, best video of the day, most clicked profile, etc.), some examples exist [Baluja et al. 2008; Halvey and Keane 2007; Martin 2007; Mislove et al. 2007] of deploying user X  X  ratings to improve the relevance of search results. This perspective is the emerging model of a self-organizing search engine, where the rating is not given by a software agent [Cheng et al. 2006; Hargittai 2004; Jung et al. 2004; Khopkar et al. 2003] but directly by the users, who assess the relevance of objects in the database with respect to different dimensions and search parameters.

User assessment of objects is particularly suitable for multimedia objects where traditional text-based indexing techniques [Ferragina and Manzini 2005; Maass and Nowak 2005] cannot be directly applied; moreover, such an approach can better reflect the dynamical evolution of user preferences. Different communities of users can as-sign different relevance to a search term, d epending on time, geographic, and cultural interests. For instance, consider the three-term query  X  X hampion+football+player X  is-sued in a U.K. community of users and the same query issued in the U.S.A.; the users X  opinions about what documents are relevant to this query are probably different in different countries, and they probably change over time.

Although the idea of exploiting users X  feedback to rate relevance seems promising, it is quite hard to convince a community of users to spend their time explicitly rat-ing objects [Chakrabarti et al. 2008; Hoashi et al. 2002; Hoi and Lyu 2004; Iwayama 2000; Leung and Liu 2007; Vinay et al. 2005; Widyantoro et al. 2003; White and Kelly 2006; Zhang et al. 2005]. On the other hand, it can be observed that the history of user behavior during a query session contains feedback information which indirectly assesses the relevance of system answers with respect to user queries. Examples of these elements are the number of clicks, if any; the time spent on viewing and judging the result list before clicking; the time spent evaluating an object (such as watching a video preview before downloading it); possible further queries refining the search; etc.
Relevance feedback, originally developed for information retrieval, is an online learning technique used to improve the effectiveness of the information retrieval sys-tems. Since its introduction into image retrieval around the mid 1990s, it has at-tracted tremendous attention in the content-based image retrieval (CBIR) community. A particularly noteworthy work is that of Rui et al. [1998] which develops a relevance feedback-based interactive retrieval approach focusing on content-based aspects such as color, texture, and shape, and relevance feedback has since been shown to provide dramatic performance improvement [Cheng et al. 2006; Zhou and Huang 2003]. Recent works that apply relevance feedback techniques to image retrieval systems include that of Azimi-Sadjadi et al. [2009], which makes use of kernel machines and selec-tive sampling that adaptively modify the similarity measures and Tao et al. [2007], which develops kernel convex machines and exploits the idea that negative samples for relevance feedback form several subclusters, while positive ones group only in one cluster. In Tao et al. [2008], an orthogonal complement component analysis method was used that captures the concepts in all positive samples and demonstrates favor-able comparison with those of linear and kernel principal component analysis meth-ods. Here, we apply the user feedback media ranking in a different way. Generally, most of the user feedback media-ranking techniques are applied to the CBIR only. CBIR focuses on retrieval based on the visual feature of the image (e.g., color, tex-ture). However, our approach supports the searching of multimedia resources, such as images, videos, and audios, and it has the advantage of being able to focus on arbitrar-ily higher-level human properties and perceptive details which are not extractable by machines.

Particularly significant studies of relevance feedback as applied to cross-media re-trieval are given in Yang et al. [2010] and Zhuang et al. [2008]. In Zhuang et al. [2008], a uniform cross-media correlation graph (U CCG) is constructed for evaluating the cor-relation among media objects of different modalities, with the media objects repre-sented as vertices and their correlations as weights of the weighted edges. Unlike the relevance feedback used in Zhuang et al. [2008] which adopts a heuristic approach that modifies the edge weights of an underlying graph model, the algorithm used in Yang et al. [2010] adopts a statistical approach and refines the cross-media indexing space (CMIS) directly, where the mechanism of long-term relevance feedback is for-mulated as a minimization problem, which includes making use of data periodically extracted offline from a log file for produci ng overall improvement in cross-media re-trieval performance. Unlike their approach, which makes use of the concept of a multi-media document (MMD) consisting of a set of heterogeneous multimedia objects of the same semantics, our approach focuses on each media object separately, and our index update mechanism is carried out continuously online in the course of normal usage rather than done periodically offline. In addition, with respect to experimentation, we base our measurements mostly on concepts rather than content-based features and semantic categories. Rocchio X  X  similarity-based relevance feedback algorithm [Chen and Zhu 2002; Rocchio et al. 1971] is one of the classical query reformation methods in information retrieval. It is essentially an adaptive supervised learning algorithm from examples. Rocchio X  X  formula is also used in Zhang and Zhang [2007] in the con-text of image retrieval in which the query point vector moving strategy for relevance feedback is based. Van Uden [1998] comments that the Rocchio algorithm aims at finding a request that best suits the user X  X  information need (i.e., user input query) by using relevance feedback. The ranking of this algorithm relies heavily on the iterative explicit user feedback. On the other hand, our approach not only takes into account explicit user feedback but also implicit feedback, incorporating both positive and neg-ative feedback forms. Lin X  X  Web image retrieval reranking process [Lin et al. 2003] based on the relevance model utilizes global information from the image X  X  HTML doc-ument to evaluate the relevance of the image. This approach seems promising when text-based HTML documents associated with the Web images are available. However, this approach can only perform well when the Web images are rich in related text-based HTML information. Our proposed reranking approach focuses on each type of media separately and does not rely on any extra embedded information, and the media resources can be retrieved e ven though there is no supplementary information on the object. This work focuses on the user community feedback as the main source for rank-ing relevance; on the other hand, classical search methods, such as query expansion [Efthimiadis 1996] and pseudo relevance feedback [Buckley 1995], can be easily inte-grated in the search engine. These methods have proven to be particularly effective when additional textual information (e.g., annotations, abstracts, dialogue transcripts extracted from videos, etc.) is available on the multimedia objects [Rudinac et al. 2009; Torjmen et al. 2008; Yan et al. 2003].

The motivation of this work is to harness considerable user judgment and human evaluation of media objects in the course of users X  normal interactions with the search system, which collectively over the community and cumulatively over time can amount to highly substantial efforts. The basic idea of this work is that information derived by the queries issued by a community of users can be used to drive and adapt future systems X  answers to similar queries. In this work, we shall focus on a single-click feed-back model, that is, we assume that the only form of feedback the user can provide is to click on a single object link or not clicking at all when the answer lists are not sufficiently relevant to the query. While the analysis of a more specific session model is beyond the scope of the article, the single-click feedback model is quite universally ap-plicable to most search engine interfaces. The increasing relevance of privacy concerns and legal issues makes user profile and client-side monitoring of user tasks increas-ingly difficult. A single-click anonymous user feedback model is then a quite realistic hypothesis.

The key contributions of this article are the development of an adaptive search en-gine architecture and a robust adaptive index update strategy which enable the system to improve its performance over time. In the course of normal usage, the underlying index structure and contents are gradually and dynamically reorganized. This ap-proach realizes a form of machine learning which can be put into relationship with re-inforcement learning [Kaelbling et al. 1996] and genetic algorithms-based approaches [Goldberg and Holland 1988] where the feedback from the environment X  X ere repre-sented by the user community X  X s used to evolve the system behavior. A major differ-ence from a typical supervised learning scheme is the absence of a separate training phase. Here, the learning phase is a continuo us interactive process, since the object repository, the community of users, and user judgment of the relevance of objects to search terms dynamically evolve over time. For this reason, an adaptive algorithm has been devised to reflect the continuous change in the term/objects index. The adap-tive engine exploits some typical evolutionary techniques, such as mutation, random tournament, and elitism. Systematic simulation experiments on a large number of ob-jects and search queries have been held to tune parameters and to evaluate adaptive ability, performance, and scalability of the approach. In addition, experiments on real data with actual users have been performed. A search engine can be characterized as a set of structures and algorithms which in-dexes a database of resource objects with respect to search term relevance. Whenever a query consisting of one or more search terms is issued, the engine is expected to return a k -bounded list of objects relevant to the search terms. Compared with other approaches, a particular advantage of the present approach is that it is able to incor-porate highly subtle nuances, elusive attributes, and deep-level semantics that may be associated with a particular media object, and that search performance may be improved continuously and automatically without additional intervention. To ensure precision of meaning, it is useful to define some concepts accurately.

Definition of Indexing. Our approach supports searching of media resources, such as images, videos, and audios. As it is impossible to extract the semantics in the multi-media data automatically with the current technology, effective indexing and retrieval of multimedia resources are necessary for a successful search system [Leung and Liu 2007]. Since the concept-based (higher-level human perception) indexing of the multi-media resources are more meaningful than its low-level contents (e.g., color, texture, size), our collective indexing approach focuses on the concept-based approach. Through the iterative use of our model, knowledge from users are collected and novel indexes would be built gradually.

Definition of Static Search Engine. Given a set of n objects = { o 1 , ..., o n } ,asetof m search terms T = { t 1 , ..., t m } ,aquery Q  X  T , a bound k on the answer length, and a relevance indexing function I :2 T  X   X  R ,where R =[0 , 1], a static search engine E can be defined by a four-tuple E  X  ( , T , I , X  ), where  X  is an answer function  X  :2 T  X  I  X  k which returns  X  ( Q , I )= V =[ o 1 , ..., o k ] X  X  vector of k objects in ranked according to relevance I  X  X uch that o i o j  X  I ( Q , o i )  X  I ( Q , o j )where i , j  X  [1 , k ]. (Note: the notation 2 S denotes the power set of S , that is, the set of all subsets of S ). As an illustration, consider a specific instance of I shown in Table I. Assuming { t 4 , t 6 , t 7 } appears only in rows three and five in Table I, from the query random values to the relative relevance values for each term of each object. Those ran-dom values can be in [0,1] and follow the Gaussian distribution (normal distribution). Since the normal distribution is a well known model of quantitative phenomena in the natural and behavioral sciences, it is appropriate to use it for the relevance indexing function.
 Definition of User Feedback. The user feedback F QV in response to an answer vector V =[ o 1 , o 2 , ..., o k ]foraquery Q is an integer F QV  X  [0 , k ] which identifies the object that the user decides to click on, where 0 encodes negative feedback, that is, the fact that no object is clicked after a given amount of time.

As previously pointed out, we assume a single-click feedback model, and the user is expected to click on a single object because of its relevance or not clicking at all if the returned answer vector does not contain objects that the user considers to be relevant.
Issues such as monitoring user multiple queries sessions or explicit scoring of the queries are beyond the scope of this article. Most search engine interfaces allow one to easily implement the single-click feedback model, while a more complex detection of the user session behavior would require special software (such as plug-ins or client-side scripts) to monitor user activity.

Definition of Adaptive Search Engine. An adaptive search engine is a search engine which adapts its response to user feedback, that is, the answer to a query depends on the en-gine X  X  initial state and the query history. It can be seen as a process over time in which, and object insertion operations, the internal structures of the search engine is contin-uously being updated.

The adaptive engine can be characterized by describing the answer function  X  and by the methods used to update the relevance index I ,thesetofobjects ,andtheset of terms T , which are terms given in a query to specify the properties of the target data objects. The general architecture of the adaptive search engine is shown in Figure 1. A user submits a query Q through the user interface ( step 1 ); the  X  module processes the query by analyzing the relevance index I and the structure H which contains additional historical statistical information on previous queries, and a sorted vector V =[ o 1 , .., o k ] is then returned to the user ( step 2 ).

The user feedback F QV is used by the update module in order to update the rel-evance index I and the statistical data H ( step 3 ). The underlying intuition is that objects with positive feedback increase their relevance, while negative feedback pro-duces a relevance decrement. If the query Q introduces new terms or new objects are added to the systems, the index I and the statistics H are also updated accordingly ( step 4 ). In general, the aim of a typical search engine is to return the best k objects which are most relevant to the query terms [Akbarinia et al. 2007; Anh and Moffat 2006; Liu et al. 2006; Panda and Chang 2006; Shen and Zhai 2005; Soliman et al. 2008; Theobald et al. 2004, 2005, 2008; Vlachou et al. 2008] according to the current term/objects rele-vance index. In search engines for text-based documents [Diligenti et al. 2002; Dwork et al. 2001; Haveliwala 2002, 2003], a preprocessing analysis is crucial for two activ-ities: (1) identifying the potential index terms and (2) computing the relevance of ob-jects with respect to those terms. Once the index has been built after the preprocessing phase, it can be used for retrieval [Azzam et al. 2004, 2005; Over et al. 2004]. After-wards, the index is typically updated only when either new documents are indexed, or when document relevance indicators, such as reference links and document citations, have changed (e.g., these changes are usually detected by a spiderbot software agent [  X  Angeles Serrano et al. 2007; Bergmark 2002; Chau et al. 2003; Dimou et al. 2006; Kammenhuber et al. 2006; Kobayashi and Takeda 2000; Lee et al. 2008; Li et al. 2007; Lourenc  X o and Belo 2006; Sun et al. 2007]).

In an adaptive search engine, the computation of the relevance index relies instead on the interactive process of collecting users feedback, since the purpose is to reflect the relevance rating as evaluated by a community of users at a given period of time. We shall consider a number of strategies. 2.2.1. Na  X   X ve Greedy Strategy. A first strategy, which can be considered as candidate for object selection, is the na  X   X ve greedy strategy, consisting in returning the best k relevant objects which appear in the current index for a query term t .

This is the typical strategy used to build hot links, such as the mentioned  X  X op-ten list of most clicked links X  which can be found in many portals X  homepage. More precisely, if the probability of a top-ten object o j being clicked is p i ,then where r i is the rank of object o i . A question can be posed:  X  X ow reliable is a top-ten to really represent the best ten links? X  Th e problem is that since the top-ten are more likely to be seen because they are on the homepage, they are also more likely to be clicked. In other words, an initial bias in some of the top-ten links could randomly boost up the rate of not very relevant links.

It is easy to see that this na  X   X ve greedy strategy can easily lead to a local maximum problem when it is applied to an adaptive search engine. Let us assume, for instance, that the sorted vector V =[ o 1 , ..., o k ] contains the current best k objects for query that the objects in V are sufficiently relevant to produce a user click, that is, positive feedback. Then, any next query Q = { t } would eventually contain the same objects, although possibly in a different order, thus hiding potentially more relevant objects which have not had the chance to be shown to the user to receive positive feedback.
More precisely, let the probability of producing a click for object o i be p i with p 1  X  p the list V . Consider a particular object o j  X  V ; then for each appearance of o j ,the corresponding index relevance w ill increase by an average amount of where f pos &gt; 0and f neg &gt; 0 respectively signify the increase in index relevance due to positive feedback and decrease in index relevance due to negative feedback. For f have
Hence, over time, what already appears in V will keep on being shown as the cor-responding index relevance tend to increase. On the other hand, for a possibly highly relevant object o not belonging to V , it stands no chance of raising the corresponding index relevance. Thus the index relevance of the two groups V and O  X  V will tend to diverge, and the objects being shown to the users may not contain the most relevant object.

It is therefore desirable to devise a mechanism where the query answer  X  ( Q , I )= [ o allows underestimated or recently inserted objects to be submitted to the user for evaluation. 2.2.2. Randomized Strategy. In order to overcome the problem of local maximum and discover the hidden objects in the search domain, a randomized algorithm has been designed which selects the k objects in the answer vector by sequential random extrac-tions from the index. The algorithm gives proportionally higher chances to best rated objects, but it also gives a non-null probability of appearing in the answer vector to objects which have never been submitted to the evaluation of the user community.
The randomized query processing consists of a randomized tournament among the database objects, which is repeated until k distinct objects are selected. The vector of selected objects, ranked by decreasing relevance, is then returned as an answer to query Q .

Definition of Randomized Query Processing. Let A ( t , o ) be the number of times the ob-jects has appeared in the answer for a single term t ,andlet C ( t , o ) be the number of clicks which has been received by the queries, including the single term t .Let A ( Q , o ), C ( Q , o ), and I ( Q , o ) for each object denote, respectively, the cumulative values for appearances , clicks ,and relevance over the terms of the query Q = { t 1 , t 2 , ..., t m } .Let A Q , C Q ,and I Q be the total values over all the objects in the current domain. Then, an object o is assigned probability o / of being selected in the extraction tour-nament as an answer for a query Q where is a weighted combination of the object relevance and statistics; min A is the minimum nonzero value (but close to zero) for the number of times that the object has appeared in the answer for a query; and is a normalization term = o  X  o .
 It is worth pointing out the roles of different terms in the o expression: term I ( Q , o ) / I denotes the relative relevance of object o in the current index; term C ( Q , o ) / A ( Q , o ) denotes the success rate of an object, that is, how many times an object has been clicked with respect to the times it has appeared in an answer vector; term 1 / max { A ( Q , o ) , min A } is reciprocal of the number of answers in which it has ap-peared where max { A ( Q , o ) , min A } is used to avoid a zero divide error when object o has appeared zero times (i.e., A ( Q , o )=0); c 1 , c 2 , c 3 and are weights parameters, which are fixed such that a high value for relative relevance prevails over the success rate, which prevails over the inverse appearance term.

The idea underlying the randomized tournament with weights o / is that most successful objects are preferred among obje cts of similar relevance and among objects with the same success rate, and objects wit h the lesser appearance are given a greater chance to be evaluated by the user community. 2.2.3. Coverage and Mutation. One of the main features of the proposed randomized query processing is that on a large number of e xtraction tournaments, the objects are, on average, expected to be extracted proportionally to their relative relevance, that is, to term I ( Q , o ) / I . At the same time, all the objects still have a chance to be extracted and submitted for the evaluation of the user community, which eventually can boost up their relevance, thus eliminating possible bias in the current relevance index. This mechanism is similar to the technique of mutations in genetic algorithms [Jansen and Wegener 2006; Saha and Bandyopadhyay 2007; Yang and S  X ima Uyar 2006]; the ran-domized extraction guarantees a coverage of the whole object domain where the term 1 / max { A ( Q , o ) , min A } in expression o / is intended to take into account such domain coverage. A more complex technique of dynamic elitism is also used to determine the size of the mutation. 2.2.4. Dynamic Elitism. The randomized query processing guarantees to avoid local maxima and to limit the effect of initial bias in the relevance index. On the other hand, it also introduces some noise which tends to lower the overall system performance. While the set of query answers will tend, in the long run, to contain the best k objects, each single answer will actually contain some good relevant objects as well as some irrelevant ones, which downgrades the quality of the provided solution.

The risk of having irrelevant objects is intrinsic to the randomized method, but it is repaid back by the advantage of having genomic variety, which allows for the discovery of new objects with a good degree of relevance not yet evaluated by the user.
In order to reduce the noise due to randomized query processing, we introduce elitism , a technique taken from genetic algorithms [Bhattacharya 2004; Eskandari et al. 2005; Piszcz and Soule 2006], which consists of transmitting from one generation to another a certain amount e of best objects, that is, the elite . The problem of deciding the elitism degree e is concerned with the two extremes: e = 0, where no elitism guar-antees fast coverage of the domain but with maximum noise, and e = k ,wherethere is no noise, but the algorithm can trap into possible suboptimal local maximum. Note that elitism e = k is equivalent to applying the previously mentioned na  X   X ve greedy strategy.

It has been experimentally observed that low elitism is preferable in the first stages when the search engine is operating on new terms. Since there is no separate train-ing phase in our adaptive indexing search engine, our model will evolve continuously through the interactive learning phase initially. Our model will only accept a low level of noise objects in the query results. Subsequently, the index would become conver-gent after this phase, and the tolerance of the noise objects can be accepted. Therefore, the elitism degree will gradually increase with the usage of the model, that is, when coverage is important and initial biases could mislead the convergence. On the other hand, in an advanced phase when many queries have been eventually issued, a higher elitism degree would improve the quality of the query answer.

Our solution dynamically increases elitism e from the initial stage of 0% elitism toward a more stable situation where elitism is about 80% of k , that is, the minimal elitism is p min =0 . 2.

The dynamic progression of elitism values is computed for the q th query by e q = q  X  (1  X  p min )  X  k ) / Q C if q  X  Q C and by e q = (1  X  p min )  X  k )if q &gt; Q C ,where q is the number of issued queries, Q C is the number of queries which are estimated to be necessary for the index to converge, and p min is the minimal elitism.

The minimal elitism p min , that is, the minimal percentage of solution which can vary, together with the relative relevanc e values of objects can be regarded as pro-viding an analogous of probability of mutation [Jansen and Wegener 2006; Saha and Bandyopadhyay 2007; Yang and S  X ima Uyar 2006] with respect to the best k objects. The general idea of a collective search engine is to prize the relevance index of objects which receive positive feedback from users, while punishing negative feedback. The single-click feedback model assumes that positive feedback can be detected by a click, since it reflects an explicit user choice. The idea of prizing a clicked object is based on the reasonable assumption that the distribution of clicks, when accumulated over the time, will tend to reflect the distribution of user relevance. On the other hand, negative feedback can be detected only when no object is clicked in an answer list. The underlying hypothesis is that, as observed in real behaviors, users tend not to click if they receive a list of objects of little relevance, that is, not useful for their purposes.

Note that when an answer vector V receives a click on an object o , nothing nega-tive can be concluded about the rest of the objects which do not receive the click, for example, they could have a relevance slightly less than o but still good. The difference in relevance will eventually emerge in subsequent queries where they will collect, on average, a smaller number of clicks. On the other hand, if an answer vector V receives no clicks at all, it can be reasonably concluded that, on average, all the objects in the list are not very relevant and the fact can be annotated by punishing the associated relevance index. The feedback update mechanism reflects this criteria by appropriate increment/decrement of the index terms.

Definition of Positive Feedback. Positive feedback is a single click on an object in the answer list. The index relevance of the clicked object is increased by a quantity f pos ; appearance and click statistics are also updated.
Definition of Negative Feedback. Negative feedback takes place when no object is clicked in the answer list. The index relevance of all the objects in the answer vector is de-creased by a quantity f neg , appearance statistics are also updated.
Increment/Decrement Values. Since the absence of clicking is a random process, that is, answer vectors which contains relevant objects are not clicked with a low (but not zero) probability, the amount f neg should not punish the object relevance too much. It has been experimentally found that a good value for f neg is f neg = f pos / k ,thatis,to distribute a  X  f pos decrement over the k objects in the answer. The value of f pos is usually taken equal to 1. The insertion of new terms and/or new objects can be seen as moving from engine E  X  ( , T , I , k , X  )toengine E  X  ( , T , I , k , X  ). In the following, we will characterize the modification introduced in , T ,and I by insertion operations. 2.4.1. Object Insertion. When a new object o ne w is inserted in the index, its relevance with respect to all index terms is set to the initial value I init , and its statistics are initialized to 0. The inverse appearance component 1 / max { A ( Q , o ) , min A } in the randomized tourna-ment increases the chance for newly introduced objects to appear in a query answer. Initially, the new object will be randomly returned to the users in relation to different terms, as more queries are issued, either the new object will eventually increase its relevance with respect to some term, or t he same inverse appearance component will tend to discard the object if selected but not clicked. The introduction of new terms in the index is typically triggered by a single term query or a multiple terms query which contains some new terms t ne w the current domain for T .

When a new term is inserted into the index, the relevance index for all the objects with respect to the new term t ne w is set to an initial I init relevance value.
It is worth noting that the evaluation of a query Q = { t ne w 1 , ... t ne w k } completely based on new terms would produce a highly randomized answer with high chances of not being clicked.
 On the other hand, the evaluation of a multiple terms query, such as which mixes one or more new term t ne w with several old terms ( t 1 , ..., t q ), would produce a more meaningful answer based on the already evaluated index. In this latter case, the new term is indirectly exploiting its relationship with the other terms to increase the index relevance. One of the main problems in testing an adaptive search engine is to use a methodology which meets the scientific requirements of being systematic, scalable, and repeatable. Moreover, tests are also needed in order to refine and tune system parameters (such mal elitism p min , and other parameters). Last but not least, there is the problem of establishing reference points for system performance evaluation, that is, being able to assess the relevance indexes the system is producing.

Although testing the effectiveness of the adaptive search engine with real users on real data is a final and primary objective, some major problems exist for tests with real users: they are not repeatable due to the interactive nature of the tests; they cannot be used for a systematic tuning of the adaptive search engine parameters; and moreover, they cannot be done on a large scale to show asymptotic behaviors.

We have then decided to design a two-phase set of experiments consisting of simu-lated user tests and real user tests. In the first stage, in order to better evaluate the effectiveness of the adaptive engine on a large scale, a simulation approach based on hidden relevance values has been adopted. Simulated user tests have been prelimi-narily done to validate the theoretical expected behavior of the adaptive engine and to fine tune the engine parameters. In the second phase, real user tests, have been held to verify the theoretical expectations and the simulated results on a set of benchmark images from Flickr with a set of predesigned queries covering different semantic situ-ations. The real user tests have been run on a prototype version of the adaptive engine where volunteer student users of Hong Kong Baptist University have been using the system for a total of about 1,300 user sessions and 21,000 feedback evaluations. The main idea behind the simulation approach based on hidden relevance is that each term/object pair is assigned a hidden relevance value, representing an artificially gen-erated user relevance; the hidden values are used to generate positive/negative user feedback. The goal of the adaptive search engine in the simulated user tests is then to approach, as close as possible, these hidden values which are unknown to the adaptive algorithm.

The architecture of such a test model is shown in Figure 2. A blackbox user model provides a sequence of random queries to the search engine. The search engine, which has no access to the hidden relevance values, computes the query answers using its internal structures as described in the prev ious paragraphs. Answers are then sent to the user module which simulates user feedback on the basis of users X  hidden values.
The main advantage of such an approach is that the tests are repeatable and can be held on different problem sizes, that is, terms and objects, as well as different dis-tributions of hidden relevance. Moreover, objective performance indexes can be easily established, like measures of distance between the system computed relevance and the user hidden relevance.

In the following, we describe the main features of the user model used in the experiments. 3.1.1. Relevance Distribution Models. Let us assume that U : T  X   X  [0 , 1] is a func-tion which assigns a hidden relevance value to every term/object pair, where 0 means not relevant at all, and 1 means totally relevant; then, a relevance distribution model establishes how the hidden relevance values are distributed among the objects. In our tests, we have experimented normal distributions with different parameters and r-reduced normal distribution. The latter consists in setting to 0 the relevance of r percent objects in and assigning a normal distribution to the others. The r-reduced normal distribution models the more realistic situation in which each term has a cer-tain percentage of completely uncorrelated objects in the database. The assignment of a distribution is realized by a pseudorandom generator. 3.1.2. Feedback Model. The feedback model is the main part of the user model since it simulates a user evaluating the query answer and decides whether to click an object or not click any object in the answer list, and, in the first case, which object to click.
The user feedback model is based on the assumption that evaluation of a user com-munity is a randomized process with a bias toward user hidden relevance values. Since each user in the community which builds the index has his/her own mental model of relevance between terms and objects, we expect that, on average, the user tends to click more on objects with greater relevance, that is, greatest hidden values with respect to objects with lower relevance.

We also assume that the attitude of the user of not clicking at all is influenced by the most relevant objects on the answer list instead of being determined by the global relevance of the answer list. In other words, the attitude to click a list which has very good objects is much higher than to click a list with greater global relevance but not a good maximum. For example, assume list a 1of20objectswhichsumupatotal relevance of 1.490, where two objects have 0.7 relevance, while the other 18 objects have a very low 0.005 relevance; let a 2 be another list of 20 objects having a total relevance of 3.0 distributed uniformly on the 20 objects for a relevance of 0.15 each. It is apparent a 2 would result in being less appealing than a 1.

These criteria have been implemented by a model where the feedback F QV to the answer vector V of length k for a query Q is computed by making a randomized tour-nament among k + 1 elements, where choosing the ( k + 1)th element represents the choice of not clicking the answer list at all.

Let us assume that the user model preliminarily sorts the vector V in ascending order with respect to the hidden values; then the weight for not clicking in the random tournament is taken as where max VQ is the maximum hidden value and c 4 a tuning parameter, while the weights of the candidate elements to be clicked are computed by where U ( Q , V [ i ]) is the sum of hidden evaluations over t  X  Q for the i th object, and the square component amplifies the difference among elements. The effect is that the weight of elements with high relevance is greatly amplified when, for instance, they coexist with many low relevance elements. On the other hand, elements which differ a little still tend to have uniform chances.

The probability values used in the tournament are, respectively,
The modeled behavior is then (i) to have a higher probability of not clicking when the list does not contain good elements, (ii) to have a similar probability of clicking elements with similar hidden relevance, and (iii) to amplify the gradient between good and not good elements.

Let us suppose, for instance, a vector V of 20 elements where V [1] and V [2] have relevance 0.9, and V [3] ... V [20] has relevance 0.1. A random click tournament-based only on total hidden relevance would click 50% of the time on the first two elements and 50% of the times on the other 18 ones. On the other hand, the quadratic amplification of gradient would give V [1] and V [2] a probability of about 90% and a probability of about 10% of choosing one the others 18 elements. 3.1.3. Query Model. The query model is the component of the simulated user model responsible for generating a sequence of random queries Q 1 , ..., Q n the adaptive engine . In the case of single-term queries, the only relevant parameter is the single term t to test and the number of queries n q .

For multiple-term queries, the relevant parameters are the terms X  domain T and the bounds min q and max q , that is, the minimum and maximum number of terms re-spectively allowed in a query, and for each query, the following must hold: min q  X  Q terms in each query when min q &lt; | T | ; in the experiments, they have been tested single-term queries, fixed-size queries where | T | = min q = max q ,fixed-sizequeries where min q = max q &lt; | T | , and variable-length queries where max q is uniformly as-cending/descending over time. Real user tests have been designed in order to verify the results obtained from the theoretical tests both in terms of convergency properties and behavior determined by the parameters. The benchmark is represented by a set of 1,200 images downloaded from Flickr, a popular image and video storing and sharing platform ( www.flickr.com ) where the ground truth (GT) is represented by the relevance tags assigned by Flickr which have been successively filtered and checked manually.

Since real user feedback is a very rare and p recious resource for the experiment, the queries and the experimental settings have been carefully designed in order to point out different aspects of the engine and to asses different properties of domains and system settings.

Five different classes of queries with specific keywords covering different semantic situations have been proposed to the users. We have tested three noun keywords: apple, bicycle, flower and two adjective/noun keywords: green and orange . The idea is that the visual information connected with a noun describing an object, for example, a flower , is intrinsically less ambiguous than more smooth concepts which are usually linked to adjectives. Let us consider, for example, the notion of relevant to green :the green keyword, depending on the user X  X  intended meaning/context, can either refer to a vegetable or to the dominant color in the picture or to an environmentally friendly object in the picture, etc. Moreover, visual a mbiguity can also be associated to certain objects; for instance, we can be quite sure about the presence/absence of a bicycle in a picture, while an apple can be more easily confused with other round and red objects or fruits, like a red ball, a strawberry, or a peach. The presence of a fuzzy semantic and visual ambiguity can lead real users to make feedback errors with respect to the GT, thus introducing a form of noise into the system.

In order to assess the engine with the real user, a range of different settings, the maximum answer list size (i.e., the number of images returned by a query) has been fixed either to 20 (for tests 1, 2, and 4) or to 10 (for tests 3 and 5), and the number of ground truth-relevant images is set to 80 for each keyword, while a different number of total images ranging from 354 to 1,200 have been used. The other parameter varying in the tests is the elitism, where dynamic elitism has been tested in test 1, static elitism in tests 2 (20%) and 4 (40%), and no elitism in tests 4 and 5.
 Table II shows the detailed settings used in the real user tests.

Volunteer students at Hong Kong Baptist University have used the system for a total of about 1,300 user sessions and 21,000 feedback evaluations.

It is interesting to note that in all the experiments, the engine started with a com-pletely empty index, where all the terms have the same initial relevance value with the same default score, and the adaptive engine has no clue of image relevance. As the user feedbacks are collected, the relevance indexes are dynamically built by the randomized algorithm in order to converge toward the ground truth.
 Both in the hidden relevance tests and in the real user tests, the following quality measures for the system have been considered.  X  Relative answer relevance ( R tot ) is the ratio between the total (hidden/ground truth) relevance of a query answer and the best possible answer of length k ; the aim of this measure is to express how optimal the current query answer is with respect to the object currently in the database; if relevance is interpreted as a binary classification (relevant/not relevant) this metric can be seen as a measure of the precision of the query answer with respect to the class of relevant objects.  X  Global answer relevance is the sum over time of answer relevance normalized with respect to the maximum possible relevance in a given period of time. It is a mea-sure of the global performance of the system over a fixed period of time with values normalized for comparison purposes.  X  Relevance coverage measures the proportion of objects which have been assessed as relevant over the total number of relevant objects; it is similar to the concept of recall in classification problems.
 Another important parameter, convergence speed , is easily readable from diagrams which show the time evolution of the quality measures. Relative answer relevance is a measure of the single query optimality, and global relative answer relevance is a measure of the quality over the time of the system. The relevance coverage, on the other hand, tries to assess if the adaptive system focus only on the top relevant elements and how reliable is the randomized algorithm in measuring the relevance of the whole set of objects. The experiments focus on testing the convergence of the simulation model with respect to the total number of objects in the index, the total number of queries, the answer size (i.e., the number of objects returned by a query), and with respect to different accumulated relevance , success rate , inverse appearance , not clicking ), and different configurations of the static/dynamic elitism ratio. 3.4.1. Parameters Tuning. The first part of the experiments focused on system parame-objects in 5,000 queries of answer size ten were performed with different sets of the weight parameters value ( c 1 , c 2 , c 3 ). In these figures, the  X  X umber of queries X  refers to the number of queries issued against the collection of data objects. By comparing these runs (Figure 3(f)), the performance of the 3rd runs ( c 1 = 100, c 2 =0 . 1, c 3 =0 . 01) results is the best one and very similar to that of the 4th run ( c 1 =1, c 2 =0, c 3 =0), while the performance of the 5th run ( c 1 =1, c 2 = 10, c 3 = 100) is the worst. As already noted, the experiments confirm that the contribution of term c 1 , that is, accumulated relevance, should prevail on c 2 and c 3 , that is, on success rate and inverse appearance.
It is worth noting that the convergence is quite fast with the best configuration settings of Figure 3(c). After 500 queries, we already obtain results with over 90% relative relevance, with an average of 70%; eventually, after about 2,500 queries, the average relative relevance is over 90% and constantly converging to about 98%. The noise which can be noted in the graphs is due to the randomized component in the query answer and is essential to guaranteeing the exploration of the available object space. The poor result in Figure 3(e) is due to the fact that giving more weight to inverse appearance, the algorithm tends to distribute uniformly the appearance of objects. This tendency also is appare nt and present in Figures 3(a) and (b).
In comparing the performance of Figures 3(c) and (d), it may be concluded that the performance of the former is slightly supe rior. Therefore, we investigate these two cases further. We rerun these two cases five times, and in these two sets of runs, we keep all the variables unchanged except the values of the weight parameters c 1 , c 2 ,and c . In addition, the runs within the same set are performed with the same variables except the initial random relative relevance v alues. These initial random relative rele-vance values follow the same distribution with the same mean and standard deviation. In these runs, we have found that the first set of runs consistently outperforms those in the second set. Although the differences are sometimes marginal, in one case, the two exhibit noticeable diffe rences in performance. The results are given in Table III, where the difference can be as much as over 10%.

The tests used to tune the weight parameter c 4 , that is, weight for not clicking decrement, are shown in Figure 4. The tests have been held varying c 4 in the set 1, 0.1, 10, 0, 0.01, while keeping other weight parameters constant, as determined in the best configuration ( c 1 = 100, c 2 =0 . 1, c 3 =0 . 01). In these tests, the best performance results were when c 4 = 10, while the worst performance results were when c 4 =0.This result can be explained intuitively with the idea that not clicking should not drastically decrease the relative relevance o f an object with respect to a term. 3.4.2. Scalability and Queries/Objects Ratio. In the series of tests shown in Figure 5, the scalability of the adaptive search engine is evaluated with respect to an increasing number of objects (i.e., number of objects = 1,000, 2,000, 3,000, 4,000, 5,000), while keeping the number of queries and answer size constant, respectively, to 5,000 and 10. The results shown in Figure 5(f) are as expected: that while the number of objects increases, the general perfo rmance decreases. The intuitive reason is that the same amount of queries and answer size cannot guarantee adequate coverage to an increas-ing amount of objects. Nevertheless, after 5,000 queries, the index converges on 5,000 objects up to 90% relative relevance, with an average of 85% (Figures 5(e) and (f)). A more remarkable result shown in Figure 5(f) is that the time of index convergence seems to be linearly proportional to the number of queries over the number of objects ratio, #queries/#objects. 3.4.3. Static and Dynamic Elitism. The purpose of this series of tests was to investi-gate the influence of elitism degree on the convergence of the adaptive engine. The tests have been held with no elitism , static elitism degree ,and dynamic elitism .The other parameters settings remains unchanged. Figure 6 shows the results for static elitism degrees of 10%(b), 30%(c), 50%(d), 70%( e), and 90%(f). Figures 6(a X  X ) show that an increasing elitism degree does not p roduce any improvement with respect to no elitism. The static 90% elitism improves quickly in the early stage, but afterwards it does not produce any improvement in the long term. It is worth noting that the static 90% elitism corresponds to the greedy strategy of always keeping the best ele-ments; only 10% of the evolution is allowed. On the other hand, the best approach is dynamic elitism, which performs significantly better (also with respect to no elitism). In dynamic elitism, the elitism degree is low in the early stage when adaptation and coverage of objects are important and is gr adually increased when relevant objects have been focused on.

A further confirmation of this fact can be found by evaluating the global relative relevance of the experiments, shown in Table IV, where the relevance is accumu-lated and normalized over time. For each experiment, the first number is the global relevance for all 5,000 queries, while the number in bold face is the same measure limited to the last 1,000 queries. It can be observed that the general increment of the global relative relevance in the second case is due to the impact on the global per-formance of the preliminary convergence phase. Once more, it is worth to point out the results of the dynamic elitism run of Figure 6(f), which is very sensitive to the performance increment. 3.4.4. Real User Tests: Convergence. The tests with real users have been conducted with the c 1  X  c 4 best parameters combination determined in the first phase of simulated user tests. The convergence results for the experiments are shown in Figure 7.
The results with real user tests are quite encouraging, and they generally confirm the properties of convergence, scalability of the algorithm, as well as the benefits of the dynamic elitism observed in the simulated user tests.

It is also useful to point out that in the experiments with answer size ten, the num-ber of relevance feedbacks was about half of that obtained for that of answer size 20. The behavior with real users confirms that performance linearly depends on the total number of images and on the answer size, since they constraint the discovery of rele-vant images and the coverage capability of the algorithm. The test also confirms that static elitism negatively affects convergence performance, despite quickly reaching a relevance level corresponding exactly to the static elitism quota (respectively 25% in test 2 and 40% in experiment test 3); the static elitism acts against the explorative capability of the algorithm, especially in the early stage of the research, avoiding that other relevant objects can be assessed.

The greater variance which can be observed in the real user tests with respect to the simulated user tests has a twofold explanation: first, the ground truth (as provided by Flickr) is a crispy boolean value, while the ground truth artificially generated in the simulated tests was a real value distributed in [0,1]. In other words, even when not fully relevant images are extracted in the answer list, they can have intermediate rel-evance values which can make the simulated test graph smoother. The second reason is that this phenomenon is greatly amplified when there is no dynamic elitism control, like in test 4 (see Figure 7(e)), since occasionally, despite the average good performance, very bad answers are still possibly generated by a completely free random tournament.
On the other hand, having a certain amount of irrelevant objects is an intrinsic property of the proposed randomized method which guarantees its adaptive behavior, and due to the binary GT of Flickr and the small size of the generated query answer (i.e., a list of ten or 20 images), variations of few elements can result in great variations of the performance indicators. 3.4.5. Real User Tests: Coverage and Noise. Another important result which is worth pointing out is the good relevance coverage , that is, the ability of the engine to assess the relevance of the objects in the repository. The precision of such a relevance evalu-ation according to the ground truth is also remarkable. Table V shows that relevance coverage and precision are generally quite satisfactory. Another positive result is the ability of the algorithm to be noise tolerant; in Table V, we indicated as WNF and WPF , respectively, the wrong negative feedbacks and the wrong positive feedbacks, that is, when the user wrongly gave an image whose ground truth defines relevant a feedback rating as nonrelevant, and vice versa. The wrong user feedback is a form of data noise that can be due to the phenomenon of visual or semantic ambiguity, to an actual user error, to a user disagreeing with the community knowledge, or possibly to a bias in the ground truth. In Table V, answer size , WNF ,and WPF are given as numbers of units, while coverage, precision ,and noise are given as percentage values. It must be noted that even a high level of noise (12%) is not affecting the precision of classification, with the exception of orange , where the ambiguity can be a suitable explanation. Finally, note that as expected, the level of observed noise for wrong nega-tives accounts for most of the total noise, from a minimum of 82% for test 4 (green) to 95% for test 1 (apple). In other words, the cases of relevance not recognized by the user are largely greater than noise generated from wrong positives, that is, images which the users consider relevant while they are n ot. (Only two users apparently recognized a bicycle where there was not one!) The key contributions of this article are the development of an adaptive search engine architecture and a robust adaptive index update strategy which enable the system to improve its performance over time. A particular advantage of the present system is that the underlying index structure and contents are gradually and dynamically reorganized in the course of normal usage without the need to deliberately activate special procedures from time to time. We have presented an adaptive indexing search engine whereby the indexing of social media resources may be done systematically by keeping track of the users querying behavior. By analyzing the search, relevance feed-back, and results selection patterns of the community of users, our indexing engine allows advanced properties of media resources X  X hich otherwise are not automati-cally extractable X  X o be gradually indexed and discovered. Through this engine, the retrieval and consumption of multimedia objects, such as images, sounds, and videos, becomes possible and effective. Given that t he automatic capturing of the properties of most media resources, and hence their automatic indexing, is not possible, such an evolutionary approach will allow user intelligence and judgment to be progressively captured and transferred from the community to the index and will bring substantial benefits to the quality of query answers. In particular, this will obviate the need to perform time-consuming, intensive, dedicated manual cataloging and indexing, which has shown to be costly and X  X f done by a small unrepresentative group X  X an also pro-duce a biased and subjective indexing structure not shared by the social community. Although such indexing is not one-off or immediate, we have shown that a competent level of retrieval performance may be achieved over a reasonable time period. Our engine also incorporates genetic algorithms to enable the mining and discovery of oth-erwise obscured or hidden media and is able to respond dynamically to changing usage patterns caused by evolving community interests and social trends.

The self-organizing and the exploration capabilities of the algorithm X  X hich is able to do the indexing, continuously covering most of the objects X  X hile maintaining a good performance in terms of total relevance to the query answers X  X ogether with the noise tolerance behavior, are some of the remarkable benefits which result from the randomized approach. On the other hand, strategies based on straightforward  X  X romotion of the best X  focus on some very relevant objects, which prevent them from assessing the others which are basically ignored and have no chance of receiving feed-back from the users. Although they can obtain good performance in the short term, this lack of flexibility is a major drawback in domains where the user relevance eval-uation dynamically evolves over time (e.g., social trends) or when new more relevant objects enter the repository.

