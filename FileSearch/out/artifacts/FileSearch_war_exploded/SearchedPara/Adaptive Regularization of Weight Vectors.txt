 koby@ee.technion.ac.il Online learning algorithms are fast, simple, make few statistical assumptions, and perform well in a wide variety of settings. Recent work has shown that parameter confidence in-formation can be effectively used to guide online learning [2]. Confidence weighted (CW) learning, for example, maintains a Gaussian distribution over linear classifier hypotheses and uses it to control the direction and scale of parameter updates [6]. In addition to for-mal guarantees in the mistake-bound model [11], CW learning has achieved state-of-the-art performance on many tasks. However, the strict update criterion used by CW learning is very aggressive and can over-fit [5]. Approximate solutions can be used to regularize the update and improve results; however, current analyses of CW learning still assume that the data are separable. It is not immediately clear how to relax this assumption. In this paper we present a new online learning algorithm for binary classification that com-bines several attractive properties: large margin training, confidence weighting, and the capacity to handle non-separable data. The key to our approach is the adaptive regular-ization of the prediction function upon seeing each new instance, so we call this algorithm Adaptive Regularization of Weights (AROW). Because it adjusts its regularization for each example, AROW is robust to sudden changes in the classification function due to label noise. We derive a mistake bound, similar in form to the second order perceptron bound, that does not assume separability. We also provide empirical results demonstrating that AROW is competitive with state-of-the-art methods and improves upon them significantly in the presence of label noise. Online algorithms operate in rounds. In round t the algorithm receives an instance x t  X  R d linear prediction rules parameterized by a weight vector w :  X  y = h w ( x ) = sign( w  X  x ). Recently Dredze, Crammer and Pereira [6, 5] proposed an algorithmic framework for on-line learning of binary classification tasks called confidence weighted (CW) learning. CW learning captures the notion of confidence in a linear classifier by maintaining a Gaussian distribution over the weights with mean  X   X  R d and covariance matrix  X   X  R d  X  d . The values  X  p and  X  p,p , respectively, encode the learner X  X  knowledge of and confidence in the weight for feature p : the smaller  X  p,p , the more confidence the learner has in the mean weight value  X  p . Covariance terms  X  p,q capture interactions between weights. Conceptually, to classify an instance x , a CW classifier draws a parameter vector w  X  N (  X  ,  X ) and predicts the label according to sign( w  X  x ). In practice, however, it can be to the approach taken by Bayes point machines [9], where a single weight vector is used to approximate a distribution. Furthermore, for binary classification, the prediction given by the mean weight vector turns out to be Bayes optimal.
 CW classifiers are trained according to a passive-aggressive rule [3] that adjusts the dis-tribution at each round to ensure that the probability of a correct prediction is at least constraint, the algorithm makes the smallest possible change to the hypothesis weight dis-tribution as measured using the KL divergence. This implies the following optimization problem for each round t : Confidence-weighted algorithms have been shown to perform well in practice [5, 6], but they suffer from several problems. First, the update is quite aggressive, forcing the probability objective. This may cause severe over-fitting when labels are noisy; indeed, current analyses of the CW algorithm [5] assume that the data are linearly separable. Second, they are designed for classification, and it is not clear how to extend them to alternative settings such as regression. This is in part because the constraint is written in discrete terms where the prediction is either correct or not.
 We deal with both of these issues, coping more effectively with label noise and generalizing the advantages of CW learning in an extensible way. We identify two important properties of the CW update rule that contribute to its good performance but also make it sensitive to label noise. First, the mean parameters  X  are guaranteed to correctly classify the current training example with margin following each This aggressiveness yields rapid learning, but given an incorrectly labeled example, it can also force the learner to make a drastic and incorrect change to its parameters. Second, confidence, as measured by the inverse eigenvalues of  X , increases monotonically with every update. While it is intuitive that our confidence should grow as we see more data, this also means that even incorrectly labeled examples causing wild parameter swings result in artificially increased confidence.
 In order to maintain the positives but reduce the negatives of these two properties, we isolate and soften them. As in CW learning, we maintain a Gaussian distribution over weight vectors with mean  X  and covariance  X ; however, we recast the above characteristics of the CW constraint as regularizers, minimizing the following unconstrained objective on each round: two tradeoff hyperparameters. For simplicity and compactness of notation, in the following we will assume that  X  1 =  X  2 = 1 / (2 r ) for some r &gt; 0.
 The objective balances three desires. First, the parameters should not change radically on each round, since the current parameters contain information about previous examples (first term). Second, the new mean parameters should predict the current example with low loss (second term). Finally, as we see more examples, our confidence in the parameters should generally grow (third term).
 Note that this objective is not simply the dualization of the CW constraint, but a new formulation inspired by the properties discussed above. Since the loss term depends on  X  convex and differentiable in  X  , yield algorithms for different settings. 1 To solve the optimization in (1), we begin by writing the KL explicitly:
C (  X  ,  X ) = We can decompose the result into two terms: C 1 (  X  ), depending only on  X  , and C 2 ( X ), de-pending only on  X . The updates to  X  and  X  can therefore be performed independently. The squared-hinge loss yields a conservative (or passive) update for  X  in which the mean parameters change only when the margin is too small, and we follow CW learning by en-forcing a correspondingly conservative update for the confidence parameter  X , updating it only when  X  changes. This results in fewer updates and is easier to analyze. Our update thus proceeds in two stages. We now develop the update equations for (3) and (4) explicitly, starting with the former. Taking the derivative of C (  X  ,  X ) with respect to  X  and setting it to zero, we get and assuming 1  X  y t (  X  t  X  x t )  X  0, we get back in (6) to obtain the rule It can be easily verified that (7) satisfies our assumption that 1  X  y t (  X  t  X  x t )  X  0. Input parameters r The update for the confidence parameters is made only if  X  t 6 =  X  t  X  1 , that is, if 1 &gt; y the derivative of C (  X  ,  X ) with respect to  X  to zero: Using the Woodbury identity we can also rewrite the update for  X  in non-inverted form: Note that it follows directly from (8) and (9) that the eigenvalues of the confidence pa-appears in Fig. 1. We first show that AROW can be kernelized by stating the following representer theorem. Lemma 1 (Representer Theorem) Assume that  X  0 = I and  X  0 = 0 . The mean param-eters  X  t and confidence parameters  X  t produced by updating via (7) and (9) can be written as linear combinations of the input vectors (resp. outer products of the input vectors with themselves) with coefficients depending only on inner-products of input vectors. Proof sketch: By induction. The base case follows from the definitions of  X  0 and  X  0 , and the induction step follows algebraically from the update rules (7) and (9). We now prove a mistake bound for AROW. Denote by M ( M = |M| ) the set of example Other examples do not affect the behavior of the algorithm and can be ignored. Let X M = P Theorem 2 For any reference weight vector u  X  R d , the number of mistakes made by AROW (Fig. 1) is upper bounded by where g t = max 0 , 1  X  y t u &gt; x t .
 The proof depends on two lemmas; we omit the proof of the first for lack of space. Lemma 4 Let T be the number of rounds. Then Proof: We compute the following quantity: Using Lemma D.1 from [2] we have that Combining, we get
X We now prove Theorem 2.
 Proof: We iterate the first equality of Lemma 3 to get We iterate the second equality to get Using Lemma 4 we have that the first term of (13) is upper bounded by 1 r log det  X   X  1 T . For the second term in (13) we consider two cases. First, if a mistake occurred on example made an update (but no mistake) on example t , then 0 &lt; y t x t  X   X  t  X  1  X  1 and ` t  X  0, thus 1  X  ` 2 t  X  1. We therefore have Combining and plugging into the Cauchy-Schwarz inequality we get Rearranging the terms and using the fact that  X  t  X  0 yields By definition, so substituting and simplifying completes the proof: A few comments are in order. First, the two square-root terms of the bound depend on r in opposite ways: the first is monotonically increasing, while the second is monotonically decreasing. One could expect to optimize the bound by minimizing over r . However, the reduces to the bound of the second-order perceptron [2]. In general, however, the bounds are not comparable since each depends on the actual runtime behavior of its algorithm. We evaluate AROW on both synthetic and real data, including several popular datasets for document classification and optical character recognition (OCR). We compare with three baselines: Passive-Aggressive (PA), Second Order Perceptron (SOP) 2 and Confidence-Weighted (CW) learning 3 .
 Our synthetic data are as in [5], but we invert the labels on 10% of the training examples. curves for both full and diagonalized versions of the algorithms on these noisy data. AROW improves over all competitors, and the full version outperforms the diagonal version. Note that CW-full performs worse than CW-diagonal, as has been observed previously for noisy data.
 We selected a variety of document classification datasets popular in the NLP community, summarized as follows. Amazon : Product reviews to be classified into domains (e.g., books or music) [6]. We created binary datasets by taking all pairs of the six domains (15 datasets). Feature extraction follows [1] (bigram counts). 20 Newsgroups : Approximately 20,000 newsgroup messages partitioned across 20 different newsgroups 4 . We binarized the corpus following [6] and used binary bag-of-words features (3 datasets). Each dataset has between 1850 and 1971 instances. Reuters (RCV1-v2/LYRL2004) : Over 800,000 man-ually categorized newswire stories. We created binary classification tasks using pairs of labels following [6] (3 datasets). Details on document preparation and feature extraction are given by [10]. Sentiment : Product reviews to be classified as positive or negative. We used each Amazon product review domain as a sentiment classification task (6 datasets). Spam : We selected three task A users from the ECML/PKDD Challenge 5 , using bag-of-words to classify each email as spam or ham (3 datasets). For OCR data we binarized two well known digit recognition datasets, MNIST 6 and USPS , into 45 all-pairs problems. We also created ten one vs. all datasets from the MNIST data (100 datasets total). Each result for the text datasets was averaged over 10-fold cross-validation. The OCR experiments used the standard split into training and test sets. Hyperparameters (including Figure 2: Learning curves for AROW (full/diagonal) and baseline methods. (a) 5k synthetic training examples and 10k test examples (10% noise, 100 runs). (b) MNIST 3 vs. 5 binary classification task for different amounts of label noise (left: 0 noise, right: 10%). r for AROW) and the number of online iterations (up to 10) were optimized using a single randomized run. We used 2000 instances from each dataset unless otherwise noted above. In order to observe each algorithm X  X  ability to handle non-separable data, we performed each experiment using various levels of artifical label noise, generated by independently flipping each binary label with fixed probability. 5.1 Results and Discussion Our experimental results are summarized in Table 1.
 AROW outperforms the base-lines at all noise levels, but does especially well as noise increases. More detailed results for AROW and CW, the overall best performing baseline, are compared in Fig. 3. AROW and CW are comparable when there is no added noise, with AROW winning the majority of the time. As label noise increases (moving across the rows in Fig. 3) AROW holds up remarkably well. In almost every high noise evaluation, AROW improves over CW (as well as the other baselines, not shown). Fig. 2(b) shows the total number of mistakes (w.r.t. noise-free labels) made by each algorithm during training on the MNIST dataset for 0% and 10% noise. Though absolute performance suffers with noise, the gap between AROW and the baselines increases.
 To help interpret the results, we classify the algorithms evaluated here according to four characteristics: the use of large margin updates, confidence weighting, a design that acco-modates non-separable data, and adaptive per-instance margin (Table 2). While all of these properties can be desirable in different situations, we would like to understand how they interact and achieve high performance while avoiding sensitivity to noise.
 Based on the results in Ta-ble 1, it is clear that the com-bination of confidence informa-tion and large margin learning is powerful when label noise is low. CW easily outperforms the other baselines in such situ-ations, as it has been shown to do in previous work. However, as noise increases, the separa-bility assumption inherent in CW appears to reduce its performance considerably. Figure 3: Accuracy on text (top) and OCR (bottom) binary classification. Plots compare performance between AROW and CW, the best performing baseline (Table 1). Markers above the line indicate superior AROW performance and below the line superior CW per-formance. Label noise increases from left to right: 0%, 10% and 30%. AROW improves relative to CW as noise increases.
 AROW, by combining the large margin and confidence weighting of CW with a soft update rule that accomodates non-separable data, matches CW X  X  performance in general while avoiding degradation under noise. AROW lacks the adaptive margin of CW, suggesting that this characteristic is not crucial to achieving strong performance. However, we leave open for future work the possibility that an algorithm with all four properties might have unique advantages. AROW is most similar to the second order perceptron [2]. The SOP performs the same type of update as AROW, but only when it makes an error. AROW, on the other hand, updates even when its prediction is correct if there is insufficient margin. Confidence weighted (CW) [6, 5] algorithms, by which AROW was inspired, update the mean and confidence parameters simultaneously, while AROW makes a decoupled update and softens the hard constraint of CW. The AROW algorithm can be seen as a variant of the PA-II algorithm from [3] where the regularization is modified according to the data.
 Hazan [8] describes a framework for gradient descent algorithms with logarithmic regret in which a quantity similar to  X  t plays an important role. Our algorithm differs in several ways. First, Hazan [8] considers gradient algorithms, while we derive and analyze algo-rithms that directly solve an optimization problem. Second, we bound the loss directly, not the cumulative sum of regularization and loss. Third, the gradient algorithms perform a projection after making an update (not before) since the norm of the weight vector is kept bounded.
 Ongoing work includes the development and analysis of AROW style algorithms for other settings, including a multi-class version following the recent extension of CW to multi-class problems [4]. Our mistake bound can be extended to this case. Applying the ideas behind AROW to regression problems turns out to yield the well known recursive least squares (RLS) algorithm, for which AROW offers new bounds (omitted). Finally, while we used the [1] John Blitzer, Mark Dredze, and Fernando Pereira. Biographies, bollywood, boom-boxes [2] Nicol  X o Cesa-Bianchi, Alex Conconi, and Claudio Gentile. A second-order perceptron [3] Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer. [4] Koby Crammer, Mark Dredze, and Alex Kulesza. Multi-class confidence weighted [5] Koby Crammer, Mark Dredze, and Fernando Pereira. Exact convex confidence-weighted [6] Mark Dredze, Koby Crammer, and Fernando Pereira. Confidence-weighted linear clas-[7] Simon Haykin. Adaptive Filter Theory . 1996. [8] Elad Hazan. Efficient algorithms for online convex optimization and their applications . [9] Ralf Herbrich, Thore Graepel, and Colin Campbell. Bayes point machines. Journal of [10] David D. Lewis, Yiming Yang, Tony G. Rose, and Fan Li. Rcv1: A new benchmark [11] Nick Littlestone. Learning when irrelevant attributes abound: A new linear-threshold
