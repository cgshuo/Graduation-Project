 W e prop ose a data reduction metho d based on a probabilis-tic similarit y framew ork where tw o v ectors are considered similar if they lead to similar predictions. W e sho w ho w this t yp e of a probabilistic similarit y metric can b e de ned b oth in a sup ervised and unsup ervised manner. As a con-crete application of the suggested m ultidimensional scaling scheme, w e describ e ho w the metho d can b e used for pro duc-ing visual images of high-dimensional data, and giv e sev eral examples of visualizations obtained b y using the suggested scheme with probabilistic Ba y esian net w ork mo dels. Multidimensional scaling (see, e.g., [3, 2]) is a data com-pression or data reduction task where the goal is to re-place the original high-dimensional data v ectors with m uc h shorter v ectors, while losing as little information as p os-sible. In tuitiv ely sp eaking, it can be argued that a prag-matically sensible data reduction scheme is suc h that t w o v ectors close to eac h other in the original m ultidimensional space are also close to eac h other in the lo w er-dimensional space. This raises the question of a distance measure | what is a meaningful de nition of similarit y when dealing with high-dimensional v ectors in complex domains? T raditionally , similarit y is de ned in terms of some standard geometric distance measure, suc h as the Euclidean distance. Ho w ev er, suc h distances do not generally re ect prop erly the prop erties of complex problem domains, where the data typically is not co ded in a geometric or spatial form. In this typ e of domains, changing one bit in a v ector ma y totally change the relev ance of the v ector, and mak e it in some sense a quite di eren t v ector, although geometrically the di erence is only one bit. This issue is discussed in more detail in Section 2.
 In [9] w e prop osed and analyzed a sup ervised, probabilistic mo del-based data reduction sc heme where the similarit yof tw ov ectors w as determined b y using a formal mo del of the problem domain. In the suggested Ba y esian framew ork, two ve ctors ar ec onsider ed similar if they le ad to similar pr edic-tive distributions , when the corresp onding attribute-v alue pairs are giv en as input to the same probabilistic mo del. The idea is related to the Ba y esian distance metric suggested in [11] as a metho d for de ning similarit y in the case-based reasoning framew ork. The basic principles of the suggested sup ervised Ba y esian data reduction sc heme are review ed in Section 3.
 An ob vious dra wbac k with the approac h suggested in [9] is that the sc heme is inheren tly sup ervised in nature, and can-not b e applied in unsup ervised domains. T oo v ercome this limitation, in Section 4 w ein tro duce a no v el, unsup ervised Ba y esian distance measure based on an extension of the ear-lier sup ervised approac h. As a concrete application of the suggested unsup ervised Ba y esian data reduction sc heme, w e consider the problem of visualizing high-dimensional data on a 2D or 3D displa y . This t yp e of visualizations can b e exploited in nding regularities in complex domains, and applied in v arious data mining tasks, suc h as instance selec-tion and clustering. A formal description of the visualization problem is giv en in Section 2.
 In this pap er w e use probabilistic Bayesian networks [16, 14] as the formal mo del family required in our Ba y esian data reduction framew ork. In tuitiv ely sp eaking, a Ba y esian (b elief ) net w ork is a represen tation of a probabilit y distri-bution o v er a set of (usually) discrete v ariables, consisting of an acyclic directed graph, where the no des corresp ond to domain v ariables, and the arcs de ne a set of indep endence assumptions whic h allo w the join t probabilit y distribution for a data v ector to b e factorized as a pro duct of simple con-ditional probabilities. T ec hniques for learning suc h mo dels from sample data are discussed in [5]. One of the main ad-v an tages of the Ba y esian net w ork mo del is the fact that with certain tec hnical assumptions it is p ossible to marginalize (in tegrate) o v er all parameter instan tiations in order to pro-duce the corresp onding predictiv e distribution. As demon-strated in, e.g., [13], suc h marginalization impro v es the pre-dictiv e accuracy of the Ba y esian net w ork mo del, esp ecially in cases where the amoun t of sample data is small. Practi-cal algorithms for p erforming predictiv e inference in general Ba y esian net w orks are discussed for example in [16, 15, 7].
Permission to make digital or hard copies of part or all of this work or permission and/or a fee.

KDD 2000, Boston, MA USA  X  ACM 2000 1 -58113 -233 -6/00/0 8 ...$5.00 After pro ducing a compressed, t w o-or three-dimensional represen tation of our data, an ob vious question concerns the qualit y of the result: ho w do w e kno w whether the compressed data set represen ts the problem domain in a reasonable manner? This question can of course b e partly answ ered through the results of a data mining pro cess: if the user is capable of disco v ering new, in teresting regulari-ties in the data based on the visualization obtained through the data reduction sc heme, w e can sa y that the compression is in some sense reasonable, at least from a pragmatic p oin t of view. Ho w ev er, w e w ould lik e to come up with a more theoretically rigorous, statistical metho dology for estimat-ing the qualit y of di eren t compressions. This imp ortan t question is discussed in Section 5, where w e also giv e illustra-tiv e examples of visualizations obtained with public-domain classi cation data sets. Let X = f x 1 ;:::; x N g denote a collection of N v ectors, and let us assume that eac h v ector x i consists of v alues of m attributes X 1 ;:::;X m . F or simplicit y , in the sequel w e will assume the attributes X i to b e discrete.
 Let ~ X denote a new data matrix where eac h m -comp onen t data v ector x i is replaced b ya t w o-or three-comp onen t data v ector ~ x i . As this new compressed data matrix can easily b e plotted on a t w o-or three-dimensional displa y , the result can b e used for pro ducing a visual represen tation of a high-dimensional domain, and hence in the sequel w e will call the visualization of data X .
 F or pro ducing a visualization of a high-dimensional data set X , w e need to nd a transformation (function) whic h maps eac h data v ector x i in the domain space to a v ector ~ x i in the visual space. In order to guaran tee the usefulness of the transformation used, an ob vious requiremen t is that t w ov ectors close to eac h other in the domain space should also b e close to eac h other in the visual space. One w a yto express this condition formally is to aim at minimizing the follo wing ob jectiv e function F ( X ; ~ X ), where d ( x i ; x j ) denotes the pairwise distance b et w een v ec-tors x i and x j in the domain space, and ~ d ( ~ x i ; ~ x tance b et w een the corresp onding v ectors in the visual space. This approac h to the visualization problem is kno wn as Sam-mon's mapping (see [8]).
 Our practical goal is to apply the reduced data set ~ X for data visualization purp oses, so the geometric Euclidean dis-tance is a natural c hoice for the distance metric ~ d ( )inthe visual space. Nev ertheless, it is imp ortan t to realize that there is no a priori reason wh y this distance measure w ould mak e a go o d similarit y metric in the high-dimensional do-main space. As a matter of fact, in man y complex domains it is quite easy to see that geometric distance measures re ect p o orly the signi can t similarities and di erences bet w een the data v ectors. Handling discrete data is esp ecially di-cult: man y data sets con tain nominal or ordinal attributes, in whic h case nding a reasonable co ding with resp ect to the Euclidean distance metric is a dicult task. F urthermore, the results are highly dep endable on attribute scaling: as all attributes are treated as equal, it is ob vious that an attribute with a scale of, sa y ,bet w een -1000 and 1000, is more in uen-tial than an attribute with a range b et w een -1 and 1. What is more, although it seems at rst sigh t that Euclidean dis-tance is mo del-free in the sense that the similarities are not based on a an y sp eci c domain mo del, this view is a w ed: when summing o v er the pairwise distances b et w een di er-en t attribute v alues indep enden tly ,w eha v e already made an implicit global indep endence assumption, although w eha v e not stated this (and other) assumptions explicitly . F or these reasons, w e argue that although the Euclidean distance is an ob vious c hoice for the distance metric ~ d ( ), in general d ( ) should b e di eren t from ~ d ( ).
 There ha v e b een sev eral attempts to circum v en t the ab o v e w eaknesses b y using v arious co ding sc hemes or v arian ts of the Euclidean distance measure, suc h as the Mahalanobis distance (see, e.g., [2]). Ho w ev er, the prop osed approac hes either use ad ho c metho dologies with no theoretical frame-w ork to supp ort the solutions presen ted, or are based on relativ ely simple implicit assumptions that do not usually hold in practice. As an example of the latter case, it is easy to see that the Euclidean distance is based on an under-lying mo del with normally distributed, indep enden t v ari-ables, while the Mahalanobis distance assumes the m ulti-v ariate normal mo del. These mo dels are clearly to o simple for mo deling practically in teresting, complex domains, es-p ecially without the explicit, formal theoretical framew ork that can b e used for determining the mo del parameters. W e argue that that in order to o v ercome the problems listed in Section 2, our assumptions concerning the domain space should b e explicitly listed and exploited b y using a formal mo del of the problem domain. By a mo del M w e mean here a parametric mo del form so that eac h parameterized instance ( M; ) of the mo del pro duces a probabilit y distri-bution P ( X 1 ;:::;X m j M; ) on the space of p ossible data v ectors x . T o mak e our presen tation more concrete, for the remainder of the pap er w e assume that the mo dels M repre-sen t di eren t Bayesian network structur es (for an in tro duc-tion to Ba y esian net w ork mo dels, see e.g., [16, 15]). The general idea can b e summarized as follo ws: two ve ctors ar ec onsider e d similar if they le ad to similar pr e dictive dis-tributions , when the corresp onding attribute-v alue pairs are giv en as input to the same Ba y esian net w ork mo del M . T o mak e this idea more precise, w em ust rst de ne the predic-tiv e distribution used in the ab o v e informal de nition. In [9] this predictiv e distribution w as determined with resp ect to a sp ecial tar get variable X k , resulting in the conditional dis-tribution Data v ectors x i and x j are no w considered similar if the cor-resp onding predictiv e distributions are similar, i.e., P ( X x ;M ) P ( X k j x j ;M ), where x i denotes the attribute v alues in v ector x i without the v alue of the target v ariable X This t yp e of similarit y measures lead to sup ervised distance measures, and w e can easily c hange the fo cus of the met-ric b yc hanging the target v ariable X k . The sc heme is also scale in v arian tas w eha v emo v ed from the original attribute space to the probabilit y space where all the n um b ers lie b e-t w een 0 and 1. This also allo ws us to handle di eren tt yp e of attributes (discrete or con tin uous) in the same consisten t framew ork. F urthermore, the framew ork ful lls the require-men t stated earlier: the approac h is theoretically on a solid basis as all our domain assumptions m ust b e formalized in the mo del M .
 The ab o v esc heme still lea v es us with the question of de n-ing a similarit y measure bet w een t w o predictiv e distribu-tions. The standard solution for computing the distance b e-t w een t w o distributions is to use the Kullbac k-Leibler div er-gence. Ho w ev er, this asymmetric measure is not (in its basic form) a distance metric in the geometric sense. In the em-pirical exp erimen ts rep orted in [9] it w as observ ed that the simpler distance metric d k ( x i ; x j )=1 : 0 P (MAP k ( x MAP k ( x j )) yields go o d results in practice. Here MAP denotes the maximum p osterior pr ob ability v alue of target v ariable X k with resp ect to the predictiv e distribution (2). In this pap er w e ho w ev er use a sligh tly di eren t distance function d ( x i ; x j ) based on a straigh tforw ard logarithmic transformation of the predictiv e probabilities: As noted in [9], extending this general approac h to cases with t w o or more target v ariables is straigh tforw ard. The distance metric describ ed in Section 3 is inheren tly su-p ervised in nature, as it requires us to c ho ose one (or more) of the domain v ariables to b e used as the target v ariable. Consequen tly , this approac h cannot b e directly used cases where no natural candidate for suc h a target v ariable exist, and w ew ould lik e to visualize our data in a purely unsup er-vised manner. F or suc h unsup ervised domains, w e prop ose the follo wing unsup ervised extension of the suggested sup er-vised similarit y metric: Consequen tly , the distance b et w een t w ov ectors x i is computed b y taking eac h of the v ariables X k in its turn as the target v ariable, and summing the resulting m sup er-vised distance measures computed b y form ula (3). In tu-itiv ely sp eaking, this means that t w ov ectors x i and x considered similar, if the most probable outcome is the same in b oth cases in all the m individual sup ervised prediction tasks based on the m conditional distributions (2) for all k 2f 1 ;:::;m g . As w e are using a sum of logarithms in the de nition of our o v erall unsup ervised distance function (4), this means that w e are basically treating all these separate sup ervised prediction tasks indep enden tly .
 In the Ba y esian framew ork, the mo del structure M used in the conditional distributions required can b e determined b y maximizing the p osterior probabilit y P ( M j X ). Assuming the uniform prior for the mo del structures, this equals to using the mo del with the highest mar ginal likeliho o d or evi-denc e . The required conditional distribution (2) can then b e computed b y marginalizing the join t probabilit y distribution P ( x i j X ;M ) appropriately .
 Ho w ev er, as noted in e.g. [6], nding the maximal evidence mo del structure is not a feasible task in practice as the n um ber of p ossible Ba y esian net w ork structures is sup er-exp onen tial. This means that in practical situations w e are dealing with a mo del structure M that is p ossibly only a poor mo del of the \true" join t domain probabilit y distri-bution, and hence some of the probabilities obtained are not correct. As demonstrated in [12], instead of trying to ndagood mo del of the join t probabilit y distribution, in sup ervised classi cation domains it mak es sense to try to nd a mo del (or a set of mo dels) so that the errors a ect the accuracy of the conditional distribution (2) as little as p ossible, while w e can allo w the join t probabilit y distribu-tion to b e suc h that the predictions concerning some other v ariable w ould b e quite inaccurate. F or this reason, w e sug-gest that instead of using a single mo del structure M for determining the distance measure (4), w e should use m su-p ervised mo dels M 1 ;:::;M m , eac hc hosen with resp ect to the corresp onding predictiv e task. W e return to this issue in Section 5. T o illustrate the v alidit y of the suggested data reduction sc heme, w e p erformed a series of exp erimen ts with 20 pub-licly a v ailable classi cation data sets from the UCI data rep ository [1]. In the prepro cessing phase of the exp eri-men tal setup, all con tin uous attributes in the data sets w ere discretized b y using a straigh tforw ard application of the k-means algorithm. Consequen tly , with resp ect to the empir-ical study rep orted here, all the data sets w ere discrete. When pro ducing a visualization of a data set X , the pair-wise distances b et w een v ectors x i and x j w ere determined b y using the unsup ervised distance metric (4). This requires determining m predictiv e distributions P ( X k j x i ;M k Unfortunately , as discussed in [6, 4, 12], nding accurate Ba y esian net w ork mo dels for sup ervised prediction tasks is a dicult problem. On the other hand, as demonstrated in, for example, [17, 10], the structurally simple Naiv e Ba y es classi er p erforms surprisingly w ell in man y real-w orld clas-si cation domains, despite of the fact that the mo del is ex-tremely fast to construct and use. F or this reason, in this series of exp erimen ts the predictiv e mo dels M k used in the visualization sc heme w ere Naiv eBa y es mo dels, constructed with resp ect to the target v ariable X k .
 F or constructing a visualization ~ X for data X , in the ap-proac h presen ted here w e need an algorithm for nding a visualization ~ X so that the ob jectiv e function (1) is min-imized. This ob jectiv e function is t ypically quite complex and nding the optimal visualization in this sense is not p os-sible in practice, so w e are left with appro ximativ e solutions. Ho w ev er, it should b e p oin ted out that in this con text it is not necessary to aim at the absolutely optimal visualization ~ X | for visualization purp oses a reasonable appro ximation is usually quite sucien t. Ho w to nd e ectiv ely go o d ap-pro ximations of the optimal visualization is ho w ev er a wide Figure 1: The Th yroid Disease data set: an example of the unsup ervised visualizations obtained with the suggested metho d. researc h problem on its o wn, and is not discussed in detail here. In the exp erimen ts rep orted here w e used a simple iterativ e sto c hastic greedy algorithm where at eac h step the visual lo cations of t w o randomly c hosen data v ectors are optimized along the connecting line so that the ob jectiv e function (1) is optimized lo cally .
 The practical relev ance of a visualization ~ X can b e indirectly measured through a data mining pro cess, where domain ex-p erts try to capture in teresting regularities from the visual image. In our case, ho w ev er, no suc h domain exp erts w ere a v ailable, so w e had to ev aluate our visualization tec hnique in a di eren t manner. In this set of exp erimen ts, this w as done b y assuming that the clustering pro vided b y the class lab els in the UCI data sets is a reasonable clustering, in the sense that this clustering can b e regarded as something that w e should come up with, had w e not seen the class lab els originally . F ollo wing this line of reasoning, eac h classi ca-tion data set w as rst pruned b y remo ving the class lab els, i.e., the column con taining the v alues of the class v ariable X m . The remaining data X w as then visualized in t w o-dimensional space b y using the unsup ervised approac hed presen ted here. Finally , the pro duced visual images w ere colored according to the class lab els that w ere not used at all in the visualization pro cess. If the resulting image w as visually pleasing in the sense that the di eren t classes (dif-feren t colors) w ere nicely separated in the picture, it can b e said that w ew ere able to reco v er the original clustering in a totally unsup ervised manner, without using the class lab el information. The empirical results sho w that the suggested unsup ervised visualization metho d w ork v ery w ell: most of the pro duced visual images pass the class coloring clarit y test explained ab o v e. A library of colored 2D examples of the pro duced vi-sualizations can b e found at URL: h ttp://www.cs.Helsinki. FI/researc h/cosco/Pro jects/Visual/ KDD2000.
 There is ho w ev er a p ossible ca v eat in the ab o v e exp erimen-tal pro cedure: basically there is no a priori reason wh y the unsup ervised visual image pro duced should re ect the clus-tering pro vided b y the class lab els, esp ecially if the original clustering is p o or from the probabilistic mo deling p oin tof view. One w a y to measure the \go o dness" of the cluster-ing pro vided b y the class lab els is to ev aluate the predictiv e accuracy of the Naiv eBa y es mo del, as this mo del is essen-tially based on clustering the data according to the class v ariable X m . Lea v e-one-out crossv alidated classi cation re-sults of the Naiv eBa y es classi er can b e found in the second column of T able 1.
 W e can no w conjecture that the ab o v e \class-color clarit y test" for the resulted visual images ma y fail with data sets where the p erformance of the Naiv eBa y es classi er is p o or. The exp erimen tal results con rm this h yp othesis: in cases where the lea v e-one-out crossv alidated classi cation accu-racy of the NB classi er is poor in the absolute sense (as with the Liv er Disorders data set), or in the relativ e sense with resp ect to the default classi cation accuracy (as with the P ostop erativ e P atien t data set), the class lab eled col-ored images are somewhat blurred. Nev ertheless, w ew ould lik e to emphasize that this do es not mean that the unsu-p ervised visualization tec hnique has \failed" in these cases, but that in these (relativ ely few) cases the somewhat arti -cial empirical setup used here is not practically sensible in the rst place. This means that if the data w ould in these cases b e clustered according to the visual image pro duced, this could result in a probabilistic mo del pro ducing more accuracy predictions than the Naiv eBa y es classi er. This in teresting idea is ho w ev er not studied further here. W e b eliev e that most p eople agree that the pro duced im-ages (with the exception of the few cases discussed ab o v e) are visually pleasing in the sense that the original classes are clearly separable in the image. This ho w ev er raises the question of whether the qualit y of the visualization could b e measured more ob jectiv ely . In tuitiv ely ,w ew ould lik eto measure ho ww ell the data in the visual image is clustered according to the (hidden) class lab el. W e suggest that this can b e done b y using, for example, a simple k-NN (nearest neigh b or) metho d, where eac h data p oin t is classi ed as a mem b er of the class con taining the most represen tativ es in the k nearest data p oin ts. The results with this t yp e of k-NN (with k=9) classi cation metho d are summarized in T able 1. The metho d 9-NN refers to 9-nearest neigh bor classi ca-tion, where the distances of the (m-1)-dimensional v ectors are computed b y form ula (4). The metho d 9-NN 2 means the corresp onding metho d with the distances computed b y using the Euclidean distance in the 2-dimensional visual im-age pro duced b y the unsup ervised visualization metho d sug-gested here.
 F rom T able 1 w e can mak e the follo wing observ ations. First, the qualit y of the crossv alidated k-NN classi cation accu-racy do es not degrade signi can tly as w e mo v e from the high-dimensional space to the 2-dimensional space. This is further evidence for the fact that the pro duced visualizations are of go o d qualit y . Second, w e can see that the classi ca-tion accuracy is quite comparable (and in some cases ev en b etter) to the accuracy of the Naiv eBa y es classi er, ev en though the visualization w as done in a purely unsup ervised Data Default NB 9-NN 9-NN 2 Australian Credit 55.5 87.1 82.0 81.2 Breast Cancer (Wisconsin) 65.5 97.4 97.3 96.9 Breast Cancer 70.3 72.3 71.7 74.1 Credit Screening 55.5 86.2 82.9 83.0 Pima Indians Diab etes 65.1 77.9 72.7 72.1 German Credit 70.0 74.9 67.4 66.1 Heart Disease (Clev eland) 54.1 57.8 57.4 57.4 Heart Disease (Hungarian) 63.9 83.3 82.7 81.3 Heart Disease (Statlog) 55.6 85.2 83.7 82.2 Hepatitis 79.4 83.2 82.6 82.6 Ionosphere 64.1 92.9 91.2 87.2 Iris Plan t 33.3 94.0 88.7 87.3 Liv er Disorders 58.0 63.2 58.8 58.6 Lymphograph y 54.7 85.8 81.8 78.4 Mole F ev er 67.1 87.8 89.2 82.4 P ostop erativ eP atien t 71.1 67.8 71.1 68.9 Th yroid Disease 69.8 99.1 97.2 95.3 V ehicle Silhouettes 25.8 64.7 66.5 45.2 Congressional V oting Records 61.4 90.1 88.0 87.1
Wine Recognition 39.9 97.2 96.1 96.1 manner, and classi cation w as not at all the goal of the visu-alization pro cess. This is ev en more surprising considering the fact that the 9-NN classi er used here w as a random and naiv ec hoice for p erforming the classi cation, mean tto b e used only for illustrating the qualit y of the visual results. Consequen tly , the results suggest that the distance-based approac h used w ould o er an in teresting framew ork for pro-ducing accurate classi ers, if that w ould b e the primary goal of the researc h. W e ha v e describ ed a data reduction sc heme based on the idea of de ning a probabilistic distance metric with resp ect to predictions obtained b y a formal, probabilistic domain mo del. The concrete mo del used in this pap er w as a Ba y esian net w ork, or more precisely , a p o ol of sup ervised Naiv eBa y es classi ers, although the general approac h can b e used with an y t yp e of a probabilistic mo del. W e ga v e examples of ho w this t yp e of distance measures can b e de ned in b oth sup ervised and unsup ervised manner. As a concrete appli-cation of the suggested sc heme, w e considered the problem of pro ducing visual images of high-dimensional data. The metho d suggested in this pap er is based on a Sammon's mapping tec hnique with resp ect to the prop osed probabilis-tic distance measure.
 The suggested visualization metho d w as empirically tested b y using publicly a v ailable UCI data sets. T o study ho w w ell the resulting visual images can re ect the hidden struc-ture of the data, the class lab els w ere not used at all in the visualization pro cess, and the resulting images w ere colored afterw ards according to this laten t information. The results demonstrate that the suggested unsup ervised visualization metho d can b e used for e ectiv ely disco v ering the underly-ing structure of the data. W e also discussed more ob jectiv e metho ds for measuring the qualit y of the pro duced visual-izations, and p erformed exp erimen ts b y using a simple clas-si cation metho d for this purp ose. The results con rm the visual observ ations ab out the go o d qualit y of the resulting images. This researc h has b een supp orted b y the National T ec hnol-ogy Agency (T ek es), and b y the Academ y of Finland.
