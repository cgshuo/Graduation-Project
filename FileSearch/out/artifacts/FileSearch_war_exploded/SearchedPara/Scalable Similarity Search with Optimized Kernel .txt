 Scalable similarity search is the core of many large scale learning or data mining applications. Recently, many re-search results demonstrate that one promising approach is creating compact and efficient hash codes that preserve data similarity. By efficient, we refer to the low correlation (and thus low redundancy) among generated codes. However, most existing hash methods are designed only for vector data. In this paper, we develop a new hashing algorithm to create efficient codes for large scale data of general for-mats with any kernel function, including kernels on vectors, graphs, sequences, sets and so on. Starting with the idea analogous to spectral hashing, novel formulations and solu-tions are proposed such that a kernel based hash function can be explicitly represented and optimized, and directly applied to compute compact hash codes for new samples of general formats. Moreover, we incorporate efficient tech-niques, such as Nystr  X  om approximation, to further reduce time and space complexity for indexing and search, making our algorithm scalable to huge data sets. Another impor-tant advantage of our method is the ability to handle diverse types of similarities according to actual task requirements, including both feature similarities and semantic similarities like label consistency. We evaluate our method using both vector and non-vector data sets at a large scale up to 1 mil-lion samples. Our comprehensive results show the proposed method outperforms several state-of-the-art approaches for all the tasks, with a significant gain for most tasks. H.3 [ Information Storage and Retrieval ]: Information Search and Retrieval; H.4 [ Information Systems Appli-cations ]: Miscellaneous Algorithms, measurement, performance hashing, search, indexing, nearest neighbor, scalable, kernel method, structure data
Internet nowadays makes it very easy to download a huge amount of data of diverse modalities like texts, images, videos, or others. To make use of such data sets in machine learn-ing and data mining, a critical step is similarity search, i.e., finding nearest neighbors of a given query. The straightfor-ward solution that exhaustively computes similarity between every data point and the query obviously does not scale up due to the prohibitive cost associated with computation and storage. To handle the scalability issue, approximate sim-ilarity search techniques exploring tradeoffs between accu-racy and speed have been actively explored in recent liter-atures[1,2,3,6,16,7,12,11,9],wherewetradecertain accuracy for faster speed. In some early works, spatial par-titions of the feature space via various tree structures[1, 2] have been extensively studied. Despite good results for low dimensional data, the performance of such tree-based ap-proaches is known to degrade significantly when the data dimension is high, sometimes even worse than linear scan. Recently, many hash coding based algorithms have been pro-posed to handle similarity search of high dimensional data. These methods follow the idea that similar data points are expected to be mapped to binary codes within a small Ham-ming distance. When such hash codes are used as adequate indexing keys, often a sublinear time complexity becomes feasible for large scale similarity search.

One well known example is locality-sensitive hashing (LSH) [3]. In LSH, random vectors are utilized to generate codes, such that two points in database within a small distance are shown to have a higher probability of collision, i.e., having the same hash code. LSH algorithms with L p norms [6], inner products [5], and learned metrics [8] have been pro-posed in recent literature. Another well known method is spectral hashing[12], which rather than using randomized projections, generates much more compact codes by thresh-olding some nonlinear functions on the projections along the principal component directions. It has been shown to achieve much better performance than LSH in some appli-cations[12].

Despite of the aforementioned progress in similarity search, most existing hashing methods have one important limita-tion: they assume data is represented in a vector format and can be embedded as points in a vector space. Such as-sumption unfortunately are not compatible with many real Kernelized LSH yes low yes only kernel similarity on features Semantic Hashing no medium yes yes
Spectral Hashing no high yes only feature similarity
Kernel Hashing data types in the forms of graphs, trees, sequences, sets, or other formats existent in applications involving multi-media, biology, or Web. For such general data types, usu-ally certain complex kernels are defined to define and com-pute the data similarities. For instance, random walk kernel and subtree kernel [26] are proposed for graph data; pyra-mid matching and spatial pyramid matching kernels [24] are proposed for data with the format of a set of vectors; Earth mover X  X  distance kernel is proposed for sequence data. How to apply hashing algorithms to those non-vector data with complex kernels becomes an important problem. More-over, even if the data are stored in the vector format, many machine learning solutions benefit from the use of domain-specific kernel functions, for which the underlying data em-bedding to the high-dimensional space is not known explic-itly, namely only the pair-wise kernel function is computable. Kernelized locality sensitive hashing (KLSH) [13] is one of the very few methods that have been developed for simi-larity search based on kernels that are applicable to both vector and non-vector data. However, as a direct extension of LSH, KLSH may suffer from the same problem of produc-ing inefficient codes, and hence will not perform well when the number of bits is small.

In this paper, we aim at developing a hashing algorithm that can 1. work on general types of data with any kernel function. 2. generate efficient and compact codes. 3. achieve fast indexing and search speed. 4. preserve diverse types of similarities including both
These properties are motivated by practical requirements in large-scale problems. Properties 2 and 3 are critical for making the hashing algorithm scalable to huge data sets, in terms of space and time complexity. Property 1 is desired in order to support general types of data from different areas. Finally, property 4 is crucial since usually no single similar-ity measure is sufficient for achieving robust performance in tasks like classification, retrieval, and so on.

Comparison of several state-of-the-art hashing algorithms based on the above properties is shown in Table 1. None of them have properties 1 and 2 at the same time, while our objective is to design a novel hashing solution that satisfy all 4 properties.

Our algorithm follows the idea of generating data-dependent optimal hash codes similar to that used in spectral hashing. However, without an explicit hash function, the original op-timal hash code formulation in spectral hashing suffers from the inability in handling new data points. To derive approx-imate solutions, in spectral hashing some strict restrictions were used, which caused several limitations. To overcome such problems, in our solution a kernel hash function is ex-plicitly represented and learned via optimization, which can be directly applied to novel samples even in nonvector data format. In addition, several speed-up techniques, such as those based on landmark points or Nystrom approximation, are incorporated to reduce the time complexity involved in the indexing and search stages. Finally, our method does not make any assumption about the similarity terms. There-fore, diverse types of similarities such as feature similarity, label consistence, or other association relations can be eas-ily handled. Hence, our method can conveniently support unsupervised, supervised, or semi-supervised hashing.
We have evaluated our algorithm on several databases in-cluding vector data, graphs, and sets, with a data size rang-ing from several thousand to one million, for different tasks such as image retrieval, classification, and near duplicate de-tection. Compared to other stat-of-the-art approaches, our proposed method achieves significant performance gains for most of the data sets/tasks. In addition, our algorithm only needs to solve a small scale eigenvector problem, and hence is very easy to implement and reproduce.
Suppose we have N samples { X j ,j =1 , ...N } , W ij is the similarity between sample X i and sample X j .Asshownin spectral hashing, efficient hash codes can be obtained by the following optimization problem: Note here Y is a M  X  N bit matrix, and Y i is i th column of Y , which is hash bits of sample X i .
 Here, ity between original data points. In other words, on average, samples with high similarity, i.e., larger W ij , should have  X 
C ; O ( P 3 ) similar hash codes, i.e., smaller || Y i  X  Y j || 2 . The constraint is to make sure every single bit component of the M hash bits should be balanced, i.e., 50% to be 1 and 50% to be  X  while the constraint is to ensure low correlation among different bits. Both these two constraints are helpful to create compact hash bits[12].
Unfortunately, as pointed out in [12], the above problem is equivalent to balanced graph partitioning problem, and hence is NP-hard. In spectral hashing, first, the constraint Y i  X  X  X  1 , 1 } M is ignored to relax the problem, and the relaxed version turns out to be eigenvectors of graph Lapla-cian. However even after relaxation, the above solution does not produce hash functions that can be used to handle novel input samples. Therefore, several additional assumptions are made in [12] in order to obtain an approximate solution that can handle novel samples. First of all, the data are assumed to be uniformly distributed vectors. Moreover, the similarity matrix W has to be fixed as With these strict assumptions, spectral hashing obtains an approximate solution by thresholding some nonlinear func-tions on the projections along the principal component di-rections [12].
In order to obtain efficient hash codes, we start with an idea and formulation similar as shown above for spectral hashing. However, unlike spectral hashing where no explicit hash functions are included, we explicitly include hash func-tions based on kernels, so that the learned hash functions can be directly applied to novel input samples of generic types. Specifically, we adopt the following formulation min s.t.
 Here Y mi is the element in i th column and m th row in bit matrix Y , and hence is the m th bit for Y i . There are M hash functions { h m ,m =1 , ...M } in total, each of which is for one hash bit. Each hash function h m ( X i )= sign ( V m  X  ( X is represented in the kernel form, as in most kernel learning methods [20], where V m is the hyperplane vector in the ker-nel space,  X  is the function for embedding samples to the kernel space and usually is not computable,and b m is the threshold scalar. Since it is infeasible to define the hyper-plane vector V m directly in the kernel space, we use an ap-proach similar to that in Kernelized LSH [13] to represent V as a linear combination of landmarks in the kernel space with combination weights denoted as A pm . { Z p ,p =1 , ..., P landmark samples, which for example can be a subset ran-domly chosen from the original N samples. Moreover, if the data are in vector form, the landmarks can also be some  X  X asis X  vectors generated by projections like PCA, or some cluster centers. Note that the weight matrix A is a P  X  M matrix. b is M  X  1 vector, where b m is m th element in b .The term the smoothness of the kernel function.

In the following, we will derive the analytical solutions of the above optimization problem and analyze the complex-ity of the method. Specifically, we will show the optimal kernel hash functions can be found elegantly by solving an eigenvector problem. spectral hashing by ignoring the constraint of Y i  X  { X  1 , 1 } M , the above optimization problem is equiva-lent to the following: with where and Here K P  X  N is the kernel matrix between P lamdmarks and N samples. More specifically the element of i th row and j th column for K P  X  N is defined as K i is the i th column of K P  X  N ,and K
P  X  P is the kernel matrix among P landmarks. More specif-ically the element of i th row and j th column for K P  X  P defined as ( K P  X  P ) i,j = K ( Z i ,Z j ) ,i =1 ,  X  X  X  ,P, j =1 ,  X  X  X  and D is a diagonal matrix with D ii =( ( i =1 ,  X  X  X  ,N ).
 Proof of Proposition 1: The kernel hashing function Y mi = h m ( X i )= sign ( V m b ) can be reformulated as where A m is m th column of A or equivalently, and b =[ b 1 , ..., b M ] T .

With the same relaxation as in spectral hashing by ignor-ing the constraint of Y i  X  X  X  1 , 1 } M , we will have Hence, from the constraint of Moreover, since from the constraint of 1 N
And because
So the optimization problem in (2) becomes: where
Moreover, note that so which completes the proof of Proposition 1.

Note here C and G are both P  X  P matrix.
The above optimization problem in (3) can be further rewritten into an eigen vector problem for simpler imple-mentation.
 More specifically, suppose the SVD decomposition of G is and denote  X  A as where  X  is a diagonal matrix consisting of M largest ele-ments of  X  0 , while T is the corresponding columns of T 0
The problem in (3) equals to The solution  X  A is a M  X  M matrix, which is the M eigen vectors for matrix
Given  X  A , A can be obtained from equation (16). For a novel sample x ,its m th bit code y m can be computed as where namely, the kernel values between x and the landmark points. Equally, y = sign ( A k x  X  b ).
 A complete workflow of our algorithm can be found in Table 2 and 3. As shown in the above, kernel based hash functions { h m ,m =1 , ..., M } can be optimized by solving an eigen vector problem on a matrix with a size around M  X  M . (Recall (19), (18) and (16) ). After { h m ,m =1 , ..., M learned via optimization, they can directly hash new samples of any data format using properly defined kernel function, as shown in (19) and (20).
W in our algorithm does not need to be fixed as W ij = exp(  X  X | X i  X  X j || 2 / X  2 ) like in spectral hashing. Furthermore, the common requirements for similarity matrix, like positive semi-definite, or no negative elements, are unnecessary here either. Finally, from equation (18) and (4), it is easy to see even nonsymmetric W is applicable. Actually, any real W with reasonable physical meaning of some kinds of similarity can be applied in our method. So, besides the usual feature similarities, other kinds of similarities, e.g., those based on class label consistency, can also be used. Hence, our method can conveniently support unsupervised, supervised, or semi-supervised hashing.

Besides the formulation in equation (2), other variation or extension is possible. For instance, suppose no similarity score W ij is directly provided, and only some ranking tuples are available, i.e., we know sample j is ranked as more simi-lar to i than k is. In this case ranking information instead of similarity scores are supposed to be preserved via  X  X elative comparison X , i,e., if a pair of sample ( i, j ) is ranked as more similar than pair ( i, k ), then the distance of hash codes be-tween j and i are supposed to be smaller than that between k and i .

The cost function in equation (2) would be changed to where ( i, j ) &gt; ( i, k )means j is ranked as more similar to i than k is, and ples. In this case, the definition of C in equation (4) would become: Note here sample i does not necessarily come from training sets. For example, in the scenario of relevance feedback, sample i can be novel queries, and sample j and k could be some retrieved samples for the query.
In the above method, the bottleneck of computation is W and K P  X  N ( D  X  W ) K P  X  N .Since W is N  X  N matrix, when we have large scale data set, what may consist of millions of samples, W and K P  X  N ( D  X  W ) K P  X  N would be every expensive to compute, with a time complexity of N 2 and PN 2 respectively.
One way to overcome the computation complexity is to use a sparse W .

Sometimes a sparse W can be obtained with supervised information. For example, in the task of near-duplicate text (or image) detection, for every sample in the training set, we only need to consider the similarity between one sample and its near-duplicates, and hence W is very sparse. Another example is multi-class classification task, such as news topic categorization or image object categorization, only samples in the same class would be considered to compute similarity.
More generally, one can always obtain sparse W by sam-pling a small subset of training samples to compute similar-ity matrix.

If W is sparse, even when N is very large, we can still compute K P  X  N ( D  X  W ) K P  X  N directly.
Another approach to overcome the computation complex-ity is to apply a low rank representation/approximation for W to handle huge data set. First, assume W (or W + W T 2 if W is not symmetric) can be computed or approximated as where R is a N  X  L matrix and Q is L  X  L matrix. Usually L&lt;&lt;N .Inourexperiments, L is set to be O ( P ).
This low rank approximation is often possible. For in-stance, if the data are vectors, W can be defined as inner products W = XX , and hence R = X and Q = I .More generally, when other similarities such as some kinds of fea-ture similarities or kernel similarities is used, often Nystr  X  om algorithm [21] can be applied to get a low rank approxi-mation for W , such that R = W N  X  L and Q = W  X  1 L  X  L .(if W
L  X  L is not invertible, use the pseudo inverse). Here W N  X  L is the similarity matrix between N samples and L selected samples in Nystr  X  om algorithm, and W L  X  L is the similarity matrix among L selected samples.
 Using the aforementioned approximation W = RQR , where d = W 1= RQ ( R 1). Therefore, C can be computed as C =( which involves small matrix sizes only.
The detailed description and time complexity analysis for our algorithm are provided in Table 2 and 3. To maintain a sublinear time online searches, P are usually chosen as a number much smaller than N .Notethat T K is the time complexity to compute the kernel between two samples and N S is the number of non-zero elements in sparse W .
If W is sparse, we can compute C from (4), and the time complexity is O ( PN S ); otherwise if W is not sparse, we can compute C from (25) with a complexity of O ( P 2 N + PLN ). If W is sparse, N S &lt;&lt; N 2 , and usually we can assume N
S = O ( NP ). Moreover, L are often chosen as a number close to P . So in both cases, the complexity to compute C is O ( P 2 N ), which is much faster than O ( PN 2 ), the original complexity if without any speedup. Note that in step 2 of Table 2, we also eed to compute W . However, the time complexity of obtaining a sparse W or computing R and Q is negligible compared to the complexity of step 1 and 2 in Table 2.

As shown in Table 2 and 3, the complexity to obtain the optimized kernel hash functions and compute hash codes for training samples is O ( PNT K + P 2 N + NMP )andthecom-plexity to compute hash codes for one novel query sample is O ( PT K + PM ). Our complexity is actually the same as that of kernelized locality sensitive hashing. In practice, to index a data set consisting of around 1 million samples, spec-tral hashing, kernelized locality sensitive hashing, and our algorithm cost about the same amount of time, i.e., several hours.

The space complexity of our method is O ( NP ), no matter we use a sparse W or a low rank approximation for W .
As shown in equation (2), our algorithm needs to select a set of landmark samples. These landmark samples, for example, can be a subset randomly chosen from the original training data, some  X  X asis X  vectors generated by projections like PCA, or some cluster centers.
 Our algorithm only involves one parameter:  X  as in (2). Though in some preliminary experiments, we found the per-formance can indeed be improved by carefully selecting the parameter  X  . However tuning the parameter needs extra time. To reduce the learning time especially on the large scale data set, in the following experiments, we set  X  =0. As shown in the experiment results, such simplified method performs well, for example, better than other state-of-the-art methods.

We compare our algorithm with several state-of-the-art methods including locality sensitive hashing (LSH), spec-tral hashing (SH), and kernelized locality sensitive hashing (KLSH). All algorithms are compared using the same num-ber of hash bits. For the latter two, We used the codes provided by the original authors, which can be downloaded from Internet. For LSH, We use our own implementation according to [5], which is reported as one of the best varia-tion for LSH. For a fair comparison, we always use the same number of landmark samples and the same kernel for our method and KLSH.

To evaluate the above approximate methods, we need to obtain groundtruth of true nearest neighbors for each query sample. One way to define groundtruth near neighbors is to choose the top samples that has highest feature similarities to the query. Another way is to use side information other than features. For example, in the task of near-duplicate detection, near-duplicate groundtruth are known and can be used as groundtruth for nearest neighbors.
The first data set we use is Photo Tourism image patch set[25]. In our experiment, we use 100K patches, which are extracted from a collection of Notre Dame pictures. 10 K patches are randomly chosen as queries, the rest 90 K are used as training set to learn the optimized kernel hashing function. For each patch, 512 dimension gist features [18] are extracted. The task on this data set is to identify the neighbors, i.e., near-duplicate patches, in the training set for each query patch. The groundtruth neighbors are defined based on the 3 D information. More specifically, for each patch in the data set, there is a label provided to describe its 3 D position. Patches with the same label are defined as groundtruth neighbors, which are near duplicate image patches of the same place of Notre Dame, with variations in lighting, camera viewpoints, and etc.

In some papers, performance is measured in terms of re-call rate for the results with a hamming distance from the query smaller than a threshold. However, this kind of eval-uation may be biased sometimes. In an extreme case, if one hashing algorithm maps all data into the same code, then all training samples would have a 0 hamming distance to any query, and hence have a recall of 1 all the time. This hashing algorithm would be  X  X isstated X  as the best, but in-deed it is one of the worst cases. In our experiments, we report recall rate, i.e., percentage of groundtruth neighbors found, together with the number of retrieved samples within a hamming distance from the query smaller than a thresh-old. This evaluation metric contains information related to represent both search quality (recall rate) and search time (number of retrieved examples). Moreover, the precision-recall curve is also reported . The experiment results of our algorithm compared to locality sensitive hashing, spectral hashing, and kernelized locality sensitive hashing are shown in Figure 1. As we can see, with a similar number of re-trieved samples, our algorithm achieved significantly higher recall than all the other three methods. With the same re-call rate, the precision of our method is often several times higher than that of SH, hundreds of times higher than LSH and KLSH.

It is a little surprising that our method can create signif-icantly more efficient codes than spectral hashing in such vector data, considering both solutions are designed with similar objectives, e.g., balanced efficient codes. One possi-ble reason is that spectral hashing assumes a uniform dis-tribution for the data points, which may not be true in this data set. Moreover, another limitation of spectral hashing is that its similarity matrix W has to be fixed as W ij = color. exp(  X  X | X i  X  X j || 2 / X  2 ), which may not be suitable for the task here either. On the contrary, the similarity matrix W used in our algorithm here is defined as the label consistency directly, i.e., W ij =1,ifthe i th patch and j th patch in the training set have the same label, namely near-duplicate pairs; otherwise W ij = 0. This demonstrates a unique strength of our algorithm, unlike most existing hashing al-gorithms that can only preserve some kinds of feature sim-ilarity, our algorithm can preserve various type similarities other than feature similarity, or more specifically any kinds of similarities represented by a real matrix W . This prop-erty is one of the keys to the superior performance achieved by our algorithm. This is confirmed by the results shown in Figure 2, in which label similarity and feature similarity are directly compared.
 Sensitivity to landmark points Since our algorithm needs some landmark points, which might be randomly chosen. So a reasonable concern is that how sensitive the performance are affected by the landmark points. Two strategies of choosing landmark samples are ac-tually shown in Figure 1: random selection and determinis-tic generation via PCA. As we can see, these two strategies provide almost the same result, which demonstrates that our algorithm is quite robust to landmark samples. Actu-ally, from our preliminary observation which is not reported here, changing the number of landmark points within a wide range, for example from 200 to 1000, only affects the per-formance slightly.
NCI1[27] is a biological data set with 4K samples. Each sample in this data set is a compound represented via a graph, with a label to show whether or not the compound is active in an anti-cancer screen (http://pubchem.ncbi.nlm. nih.gov). Though the data set is not large, however, the ker-nel similarities between samples are very expensive to com-pute. For example, even some state-of-the-art graph kernels methods have to take several seconds or minutes to compute the kernel similarity between a single pair[26]. So approxi-mate nearest neighbor search via hashing on this median-size data set is still an important problem. More details on the data and the graph kernel can be found in [26].

Due to the non-vector data type, LSH and spectral hash-ing can not be applied to this graph data. So we can only compare our algorithms with the KLSH method. The ex-periments are repeated 5 times. In each time, 90% of the data are chosen as the training set, the other 10% are used as test queries, and KLSH and our algorithm use the same number of randomly chosen landmark points. For each test query sample, we find its top k nearest neighbors based on Hamming distance to the query bits. The label of the query sample is predicted by the majority of labels from top k -nearest neighbors. In table 4, the average accuracy over 5 runs is shown. We can see that our method is clearly better than the KLSH method, especially when k is small (10%  X  20% performance gain). Caltech101 is an image data set of about 10K samples [23]. In our experiment, local SIFT features [19] are extracted for each image, and Bag of visual words with geometry location are used. In other words, each image is represented by a set of visual words with geometry locations. Spatial pyra-mid matching (SPM) kernel [24] is used to compute the set similarity between two images. Since this data set consists of non-vector data, only KLSH and our algorithm are ap-plicable. 90% of the data are selected as training samples while the other 10% are used as queries. The groundtruth neighbors for each query is defined as top 300 neighbors in the training set found via linear scan with spatial pyramid matching kernel similarity. KLSH and our algorithm use the same parameters like the number of landmark samples and so on. The similarity matrix for our method is defined as label consistency, i.e., W ij =1,ifthe i th sample and j th sample have the same object class label; otherwise W ij =0. The precision-recall curves for our algorithm and KLSH are shown in Figure 3, confirming the superiority of our method.
We have downloaded around 1 M web images from flickr web site 1 . For each image, 512 dimension gist features[18] are extracted. The groundtruth neighbors for each query here is defined as top 1% samples in the training set found via linear scan of inner product. In this case, a factoriza-tion for W = XX T can be used and the tricks described in section 3 can be applied to handle a huge data set of this scale (1 million). Specifically, R = X and Q = I in our experiments.

For our method and KLSH, RBF kernel with the same parameter is used, and moreover, the number of landmark points are set close to the number of feature dimensions, such that KLSH, SH and our algorithm would have almost the same amount indexing time, i.e., several hours by using a regular workstation. The results are shown in Figure 4. Our algorithm provides better results compared to other two methods.
 We have also tried other kernels like linear kernel for KLSH and our method. With linear kernel, our method performs comparably or slightly better than KLSH and SH. This result is not shown due to space limit.
In this paper, we have proposed a novel and effective hash-ing algorithm that can create compact hash codes for general types of data with any kernel, and can be easily scaled to huge data set consisting of millions of samples.

Future works include study of how other large matrix ap-proximation methods can be incorporated with hash func-tion learning and how they will affect the performance of the integrated approach. In addition, we will study how to select suitable kernels and how to fuse multiple kernels for specific tasks.
This work is supported in part by National Science Foun-dation through Grant No CNS-07-51078 and CNS-07-16203, and Office of Naval Research.

We also thank Professor Nenghai Yu and his group for their assistance in providing the web images. www.flickr.com 0.05 0.1 0.15 0.2
Precision 0.01 0.02 0.03 0.04
Precision 0.05 0.1 0.15 0.2
Precision biological data set [1] J. Freidman, J. Bentley, and A. Finkel. An Algorithm [2] J. Uhlmann. Satisfying general proximity / similarity [3] P. Indyk and R. Motwani. Approximate nearest [4] A. Gionis, P. Indyk, and R.Motwani. Similarity search [5] M. Charikar. Similarity search in high dimensions via [6] M. Datar, N. Immorlica, P. Indyk, and V. Mirrokni. [7] K. Grauman and T. Darrell. Pyramid match hashing: [8] P. Jain, B. Kulis, and K. Grauman. Fast image search [9] R. Salakhutdinov and G. Hinton. Semantic hashing. In [10] R. Salakhutdinov and G. Hinton. Learning a nonlinear [11] A. Torralba, R. Fergus, and Y.Weiss. Small Codes and [12] Y. Weiss, A. Torralba, and R. Fergus. Spectral [13] Brian Kulis and Kristen Grauman. Kernelized [14] Brian Kulis and Trevor Darrell. Learning to hash with [15] M. Raginsky and S. Lazebnik. Locality sensitive [16] Kave Eshghi and Shyamsundar Rajaram. Locality [17] Xiaofei He, and Partha Niyogi. Locality Preserving [18] A. Oliva and A. Torralba. Modeling the shape of the [19] D. Lowe. Distinctive image features from [20] Vladimir N Vapnik. The Nature of Statistical Learning [21] Christopher Williams , Matthias Seeger. Using the [22] Mikhail Belkin and Partha Niyogi. Laplacian [23] L. Fei-Fei, R. Fergus, and P. Perona. Learning [24] S. Lazebnik, C. Schmid and J. Ponce. Beyond bags of [25] N. Snavely, S. Seitz, and R. Szeliski. Photo tourism: [26] Nino Shervashidze, Karsten M. Borgwardt. Fast [27] N. Wale and G. Karypis. Comparison of descriptor
