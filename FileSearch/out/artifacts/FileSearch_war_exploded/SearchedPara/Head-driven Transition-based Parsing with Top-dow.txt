 Transition-based parsing algorithms, such as shift-reduce algorithms (Nivre, 2004; Zhang and Clark, 2008), are widely used for dependency analysis be-cause of the efficiency and comparatively good per-formance. However, these parsers have one major problem that they can handle only local information. Isozaki et al. (2004) pointed out that the drawbacks of shift-reduce parser could be resolved by incorpo-rating top-down information such as root finding.
This work presents an O ( n 2 ) top-down head-driven transition-based parsing algorithm which can parse complex structures that are not trivial for shift-reduce parsers. The deductive system is very similar to Earley parsing (Earley, 1970). The Earley predic-tion is tied to a particular grammar rule, but the pro-posed algorithm is data-driven, following the current trends of dependency parsing (Nivre, 2006; McDon-ald and Pereira, 2006; Koo et al., 2010). To do the prediction without any grammar rules, we introduce a weighted prediction that is to predict lower nodes from higher nodes with a statistical model.

To improve parsing flexibility in deterministic parsing, our top-down parser uses beam search al-gorithm with dynamic programming (Huang and Sagae, 2010). The complexity becomes O ( n 2 b ) where b is the beam size. To reduce prediction er-rors, we propose a lookahead technique based on a FIRST function, inspired by the LL(1) parser (Aho and Ullman, 1972). Experimental results show that the proposed top-down parser achieves competitive results with other data-driven parsing algorithms. A dependency graph is defined as follows.
 Definition 2.1 (Dependency Graph) Given an in-put sentence W = n 0 : : : n n where n 0 is a spe-cial root node $ , a directed graph is defined as G
W = ( V W ; A W ) where V W = set of (indices of) nodes and A W V W V W is a set of directed arcs. The set of arcs is a set of pairs ( x; y ) where x is a head and y is a dependent of x . x ! l denotes a path from x to l . A directed graph G W = ( V W ; A W ) is well-formed if and only if: These conditions are refered to ROOT , SINGLE-HEAD , and ACYCLICITY , and we call an well-formed directed graph as a dependency graph . Definition 2.2 (PROJECTIVITY) A dependency graph G W = ( V W ; A W ) is projective if and only if,  X  : j n  X  : f p g  X  : j n  X  : f p g s  X  : j ::: j s  X  :  X  for every arc ( x; y ) 2 A W and node l in x &lt; l &lt; y or y &lt; l &lt; x , there is a path x ! l or y ! l . The proposed algorithm in this paper is for projec-tive dependency graphs. If a projective dependency graph is connected, we call it a dependency tree , and if not, a dependency forest . Our proposed algorithm is a transition-based algo-rithm, which uses stack and queue data structures. This algorithm formally uses the following state: where  X  is a step size, S is a stack of trees s d j ::: j where s 0 is a top tree and d is a window size for feature extraction, i is an index of node on the top of the input node queue, h is an index of root node of s 0 , j is an index to indicate the right limit ( j 1 inclusive) of pred y , and is a set of pointers to predictor states , which are states just before putting the node in h onto stack S . In the deterministic case, is a singleton set except for the initial state. deductive system of the top-down algorithm is shown in Figure 1. The initial state p 0 is a state ini-tialized by an artificial root node n 0 . This algorithm applies one action to each state selected from appli-cable actions in each step. Each of three kinds of actions, pred, scan, and comp, occurs n times, and this system takes 3 n steps for a complete analysis.
Action pred x puts n k onto stack S selected from the input queue in the range, i k &lt; h , which is to the left of the root n h in the stack top. Similarly, action pred y puts a node n k onto stack S selected from the input queue in the range, h &lt; i k &lt; j , which is to the right of the root n h in the stack top. The node n i on the top of the queue is scanned if it is equal to the root node n h in the stack top. Action comp creates a directed arc ( h  X  ; h ) from the root h  X  of s  X  0 on a predictor state q to the root h of s 0 on a current state p if h &lt; i 1 .

The precondition i &lt; h of action pred x means that the input nodes in i k &lt; h have not been flict with each other since their preconditions i &lt; h , i = h and h &lt; i do not hold at the same time. flict because both actions share the same precondi-tion h &lt; i , which means that the input nodes in 1 k h have been predicted and scanned. This parser constructs left and right children of a head node in a left-to-right direction by scanning the head node prior to its right children. Figure 2 shows an example for parsing a sentence  X  X  saw a girl X . To prove the correctness of the system in Figure 1 for the projective dependency graph, we use the proof strategy of (Nivre, 2008a). The correct deduc-tive system is both sound and complete .
 Theorem 4.1 The deductive system in Figure 1 is correct for the class of dependency forest.
 Proof 4.1 To show soundness, we show that G p 0 = ( V axiom, is well-formed and projective, and that every transition preserves this property.

To show completeness, we show that for any sen-tence W , and dependency forest G W = ( V W ; A W ) , there is a transition sequence C 0 ,m such that G p m = G W by an inductive method.
 The deductive sysmtem in Figure 1 is both sound and complete. Therefore, it is correct. 2 5.1 Stack-based Model The proposed algorithm employs a stack-based model for scoring hypothesis. The cost of the model is defined as follows: where s is a weight vector, f s is a feature function, and act is one of the applicable actions to a state  X  :  X  i; h; j; S  X  : . We use a set of feature templates of (Huang and Sagae, 2010) for the model. As shown in Figure 3, left children s 0 : l and s 1 : l of trees on Algorithm 1 Top-down Parsing with Beam Search stack for extracting features are different from those of Huang and Sagae (2010) because in our parser the left children are generated from left to right.
As mentioned in Section 1, we apply beam search and Huang and Sagae (2010) X  X  DP techniques to our top-down parser. Algorithm 1 shows the our beam search algorithm in which top most b states are preserved in a buffer buf [  X  ] in each step. In line 10 of Algorithm 1, equivalent states in the step  X  are merged following the idea of DP. Two states  X  When two equivalent predicted states are merged, their predictor states in get combined. For fur-ther details about this technique, readers may refer to (Huang and Sagae, 2010). 5.2 Weighted Prediction The step 0 in Figure 2 shows an example of predic-tion for a head node  X  $ 0  X , where the node  X  X aw 2  X  is selected as its child node. To select a probable child node, we define a statistical model for the prediction. In this paper, we integrate the cost from a graph-based model (McDonald and Pereira, 2006) which directly models dependency links. The cost of the 1 st-order model is defined as the relation between a child node c and a head node h : where p is a weight vector and f p is a features func-tion. Using the cost c p , the top-down parser selects a probable child node in each prediction step.
When we apply beam search to the top-down parser, then we no longer use 9 but 8 on pred x and many nodes as an appropriate child from a single state, causing many predicted states. This may cause the beam buffer to be filled only with the states, and these may exclude other states, such as scanned or completed states. Thus, we limit the number of pre-dicted states from a single state by prediction size implicitly in line 10 of Algorithm 1.

To improve the prediction accuracy, we introduce a more sophisticated model. The cost of the sibling 2 nd-order model is defined as the relationship be-tween c , h and a sibling node sib : The 1 st-and sibling 2 nd-order models are the same as McDonald and Pereira (2006) X  X  definitions, ex-cept the cost factors of the sibling 2 nd-order model. The cost factors for a tree structure in Figure 4 are defined as follows: This is different from McDonald and Pereira (2006) in that the cost factors for left children are calcu-lated from left to right, while those in McDonald and Pereira (2006) X  X  definition are calculated from right to left. This is because our top-down parser gener-ates left children from left to right. Note that the cost of weighted prediction model in this section is incrementally calculated by using only the informa-tion on the current state, thus the condition of state merge in Equation 2 remains unchanged. 5.3 Weighted Deductive System We extend deductive system to a weighted one, and introduce forward cost and inside cost (Stolcke, 1995; Huang and Sagae, 2010). The forward cost is the total cost of a sequence from an initial state to the end state. The inside cost is the cost of a top tree s 0 in stack S . We define these costs using a combina-tion of stack-based model and weighted prediction model. The forward and inside costs of the combi-nation model are as follows: where c fw s and c in s are a forward cost and an inside cost for stack-based model, and c fw p and c in p are a for-ward cost and an inside cost for weighted prediction model. We add the following tuple of costs to a state:
For each action, we define how to efficiently cal-culate the forward and inside costs 3 , following Stol-cke (1995) and Huang and Sagae (2010) X  X  works. In where In the case of scan, where In the case of comp, where performed based on the following linear order for the two states p and p  X  at the same step, which have ( c fw ; c in ) and ( c  X  fw ; c  X  in ) respectively: p  X  p  X  iff c fw &lt; c  X  fw or c fw = c  X  fw ^ c in &lt; c We prioritize the forward cost over the inside cost since forward cost pertains to longer action sequence and is better suited to evaluate hypothesis states than inside cost (Nederhof, 2003). 5.4 FIRST Function for Lookahead Top-down backtrack parser usually reduces back-tracking by precomputing the set FIRST ( ) (Aho and Ullman, 1972). We define the set FIRST ( ) for our top-down dependency parser: FIRST ( t X  ) = f ld : t j ld 2 lmdescendant ( Tree ; t X  ) where t X  is a POS-tag, Tree is a correct depen-dency tree which exists in Corpus, a function lmdescendant ( Tree ; t X  ) returns the set of the leftmost descendant node ld of each nodes in Tree whose POS-tag is t X , and ld : t denotes a POS-tag of ld . Though our parser does not backtrack, it looks ahead when selecting possible child nodes at the prediction where n i : t is a POS-tag of the node n i on the top of the queue, and n k : t is a POS-tag in k th position of there are no nodes which satisfy the condition, our top-down parser creates new states for all nodes, and pushes them into hypo in line 9 of Algorithm 1. Our proposed top-down algorithm has three kinds of actions which are scan, comp and predict. Each scan and comp actions occurs n times when parsing a sentence with the length n . Predict action also oc-curs n times in which a child node is selected from a node sequence in the input queue. Thus, the algo-rithm takes the following times for prediction: n + ( n 1) + + 1 = As n 2 for prediction is the most dominant factor, the time complexity of the algorithm is O ( n 2 ) and that of the algorithm with beam search is O ( n 2 b ) . Alshawi (1996) proposed head automaton which recognizes an input sentence top-down. Eisner and Satta (1999) showed that there is a cubic-time parsing algorithm on the formalism of the head automaton grammars, which are equivalently con-verted into split-head bilexical context-free gram-mars (SBCFGs) (McAllester, 1999; Johnson, 2007). Although our proposed algorithm does not employ the formalism of SBCFGs, it creates left children before right children, implying that it does not have spurious ambiguities as well as parsing algorithms on the SBCFGs. Head-corner parsing algorithm (Kay, 1989) creates dependency tree top-down, and in this our algorithm has similar spirit to it.
Yamada and Matsumoto (2003) applied a shift-reduce algorithm to dependency analysis, which is known as arc-standard transition-based algorithm (Nivre, 2004). Nivre (2003) proposed another transition-based algorithm, known as arc-eager al-gorithm. The arc-eager algorithm processes right-dependent top-down, but this does not involve the prediction of lower nodes from higher nodes. There-fore, the arc-eager algorithm is a totally bottom-up algorithm. Zhang and Clark (2008) proposed a com-bination approach of the transition-based algorithm with graph-based algorithm (McDonald and Pereira, 2006), which is the same as our combination model of stack-based and prediction models. Experiments were performed on the English Penn Treebank data and the Chinese CoNLL-06 data. For the English data, we split WSJ part of it into sections 02-21 for training, section 22 for development and section 23 for testing. We used Yamada and Mat-sumoto (2003) X  X  head rules to convert phrase struc-ture to dependency structure. For the Chinese data, we used the information of words and fine-grained POS-tags for features. We also implemented and ex-perimented Huang and Sagae (2010) X  X  arc-standard shift-reduce parser. For the 2 nd-order Eisner-Satta algorithm, we used MSTParser (McDonald, 2012).
We used an early update version of averaged per-ceptron algorithm (Collins and Roark, 2004) for training of shift-reduce and top-down parsers. A set of feature templates in (Huang and Sagae, 2010) were used for the stack-based model, and a set of feature templates in (McDonald and Pereira, 2006) were used for the 2 nd-order prediction model. The weighted prediction and stack-based models of top-down parser were jointly trained. 8.1 Results for English Data During training, we fixed the prediction size and beam size to 5 and 16 , respectively, judged by pre-liminary experiments on development data. After 25 iterations of perceptron training, we achieved 92 : 94 unlabeled accuracy for top-down parser with the FIRST function and 93 : 01 unlabeled accuracy for shift-reduce parser on development data by set-ting the beam size to 8 for both parsers and the pre-diction size to 5 in top-down parser. These trained models were used for the following testing.

We compared top-down parsing algorithm with other data-driven parsing algorithms in Table 1. Top-down parser achieved comparable unlabeled ac-curacy with others, and outperformed them on the sentence complete rate. On the other hand, top-down parser was less accurate than shift-reduce parser on the correct root measure. In step 0 , top-down parser predicts a child node, a root node of a complete tree, using little syntactic information, which may lead to errors in the root node selection. Therefore, we think that it is important to seek more suitable features for the prediction in future work.
Figure 5 presents the parsing time against sen-tence length. Our proposed top-down parser is the-oretically slower than shift-reduce parser and Fig-ure 5 empirically indicates the trends. The domi-nant factor comes from the score calculation, and we will leave it for future work. Table 2 shows the oracle score for test data, which is the score of the highest accuracy parse selected for each sen-tence from results of several parsers. This indicates that the parses produced by each parser are differ-ent from each other. However, the gains obtained by the combination of top-down and 2 nd-MST parsers are smaller than other combinations. This is because top-down parser uses the same features as 2 nd-MST parser, and these are more effective than those of stack-based model. It is worth noting that as shown in Figure 5, our O ( n 2 b ) ( b = 8 ) top-down parser is much faster than O ( n 3 ) Eisner-Satta CKY parsing. 8.2 Results for Chinese Data (CoNLL-06) We also experimented on the Chinese data. Fol-lowing English experiments, shift-reduce parser was trained by setting beam size to 16 , and top-down parser was trained with the beam size and the predic-tion size to 16 and 5 , respectively. Table 3 shows the results on the Chinese test data when setting beam size to 8 for both parsers and prediction size to 5 in top-down parser. The trends of the results are almost the same as those of the English results. 8.3 Analysis of Results Table 4 shows two interesting results, on which top-down parser is superior to either shift-reduce parser or 2 nd-MST parser. The sentence No.717 contains an adverbial clause structure between the subject and the main verb. Top-down parser is able to han-dle the long-distance dependency while shift-reudce parser cannot correctly analyze it. The effectiveness on the clause structures implies that our head-driven parser may handle non-projective structures well, which are introduced by Johansonn X  X  head rule (Jo-hansson and Nugues, 2007). The sentence No.127 contains a coordination structure, which it is diffi-cult for bottom-up parsers to handle, but, top-down parser handles it well because its top-down predic-tion globally captures the coordination. This paper presents a novel head-driven parsing al-gorithm and empirically shows that it is as practi-cal as other dependency parsing algorithms. Our head-driven parser has potential for handling non-projective structures better than other non-projective dependency algorithms (McDonald et al., 2005; At-tardi, 2006; Nivre, 2008b; Koo et al., 2010). We are in the process of extending our head-driven parser for non-projective structures as our future work. We would like to thank Kevin Duh for his helpful comments and to the anonymous reviewers for giv-ing valuable comments.
