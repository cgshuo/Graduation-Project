 Online learning refers to a paradigm where, at each time t , an instance x t  X  X is presented to a learner, which uses its parameter vector f t to predict a label. This predicted label is then compared The learner then updates its parameter vector to minimize a risk functional , and the process repeats. Kivinen and Warmuth [1] proposed a generic framework for online learning where the risk func-tional, J t ( f ) , to be minimized consists of two terms: a Bregman divergence between parameters  X  The parameter updates are then derived via the principle the more easily computable expression [1] obtain the familiar stochastic gradient descent update We are interested in applying online learning updates in a reproducing kernel Hilbert space (RKHS). To lift the above update into an RKHS, H , one typically restricts attention to f  X  X  and defines [2] where || X || H denotes the RKHS norm,  X  &gt; 0 is a regularization constant, and C &gt; 0 determines the penalty imposed on point prediction violations.
 Recall that if H is a RKHS of functions on X  X Y , then its defining kernel k : ( X  X Y ) 2  X  R satis-the conclusion that  X  f L ( x , y, f )  X  X  , and in particular there must exist coefficients  X  i,  X  y fully specifying f t +1 via In this paper we propose an algorithm ILK (implicit online learning with kernels) that solves (2) directly, while still expressing updates in the form (7). That is, we derive a technique for computing the implicit update that can be applied to many popular loss functions, including quadratic, hinge, and logistic losses, as well as their extensions to structured domains (see e.g. [3]) X  X n an RKHS. We also provide a general recipe to check if a new convex loss function is amenable to these implicit updates. Furthermore, to reduce the memory requirement of ILK, which grows linearly with the number of observations (instance-label pairs), we propose a sparse variant SILK (sparse ILK) that approximates the decision function f by truncating past observations with insignificant weights. replacing R ( x t , y t , f ) with (5), and setting G ( f ) = 1 2 || f || 2 H , one obtains Since L is assumed convex with respect to f , setting  X  f J = 0 and using an auxiliary variable  X  On the other hand, from the form (7) it follows that f t +1 can also be written as for some  X  j,  X  y  X  R and j = 1 , . . . , t . Since it follows from (9) and (10) that manner to work in our setting.
 in  X  t,  X  y alone, which sometimes can be solved efficiently. We now elucidate the details for some well-known loss functions.
 value of y . Furthermore, we assume that Y = R , and write which yields Substituting into (12) and using (9) we have After some straightforward algebraic manipulation we obtain the solution loss for binary classification can be written as where  X  &gt; 0 is the margin parameter, and (  X  ) + := max(0 ,  X  ) . Recall that the subgradient is a hinge loss is not differentiable at the hinge point, but its subgradient exists everywhere. Writing  X 
L ( x t , y t , f ) =  X  t k ( x t ,  X  ) we have: We need to balance between two conflicting requirements while computing  X  t . On one hand we want the loss to be zero, which can be achieved by setting  X   X  y t f t +1 ( x t ) = 0 . On the other constraints by appropriately clipping the optimal estimate of  X  t .
  X   X  y t ((1  X   X  ) f t ( x t ) +  X   X  t k ( x t , x t )) = 0 , which yields On the other hand, by using (15) and (12) we have  X  t y t  X  [0 , (1  X   X  ) C ] . By combining the two scenarios, we arrive at the final update The updates for the hinge loss used in novelty detection are very similar.
 Graph Structured Loss The graph-structured loss on label domain can be written as Here, the margin of separation between labels is given by  X ( y t ,  X  y ) which in turn depends on the graph structure of the output space. This a very general loss, which includes binary and multiclass hinge loss as special cases (see e.g. [3]). We briefly summarize the update equations for this case. x The updates are now given by Logisitic Regression Loss The logistic regression loss and its gradient can be written as respectively. Using (9) and (12), we obtain Although this equation does not give a closed-form solution, the value of  X  t can still be obtained by using a numerical root-finding routine, such as those described in [5]. 2.1 ILK and SILK Algorithms We refer to the algorithm that performs implicit updates as ILK, for  X  X mplicit online learning with kernels X . The update equations of ILK enjoy certain advantages. For example, using (11) it is easy to see that an exponential decay term can be naturally incorporated to down-weight past observations: Intuitively, the parameter  X   X  (0 , 1) (determined by  X  and  X  ) trades off between the regularizer and the loss on the current sample. In the case of hinge losses X  X oth binary and graph structured X  X he (16) and (18)).
 A major drawback of the ILK algorithm described above is that the size of the kernel expansion grows linearly with the number of data points up to time t (see (10)). In many practical domains, where real time prediction is important (for example, video surveillance), storing all the past obser-vations and their coefficients is prohibitively expensive. Therefore, following Kivinen et al. [2] and Vishwanathan et al. [3] one can truncate the function expansion by storing only a few relevant past observations. We call this version of our algorithm SILK, for  X  X parse ILK X .
 Specifically, the SILK algorithm maintains a buffer of size  X  . Each new point is inserted into the value is discarded to maintain a bound on memory usage. This scheme is more effective than the straightforward least recently used (LRU) strategy proposed in Kivinen et al. [2] and Vishwanathan et al. [3]. It is relatively straightforward to show that the difference between the true predictor and its truncated version obtained by storing only  X  expansion coefficients decreases exponentially as the buffer size  X  increases [2]. In this section we will primarily focus on analyzing the graph-structured loss (17), establishing relative loss bounds and analyzing the rate of convergence of ILK and SILK. Our proof techniques adopt those of Kivinen et al. [2]. Due to the space constraints, we leave some details and analysis to the full version of the paper. Although the bounds we obtain are similar to those obtained in [2], our experimental results clearly show that ILK and SILK are stronger than the NORMA strategy of [2] and its truncated variant. 3.1 Mistake Bound We begin with a technical definition.
 f quences is denoted as F ( T, B, D 1 , D 2 ) .
 { ( f 1 , . . . , f T )  X  X } , the number of errors M is defined as of mistakes M made by ILK by the cumulative loss of an arbitrary sequence of hypotheses from F ( T, B, D 1 , D 2 ) .
 bitrary sequence of hypotheses ( g 1 ,  X  X  X  , g T )  X  F ( T, B, D 1 , D 2 ) with average margin  X  = ber of mistakes of the sequence of hypotheses ( f 1 ,  X  X  X  , f T ) generated by ILK with learning rate  X  =  X  ,  X  = 1 B X  q D 2 T is upper-bounded by where S = X 2 4 ( B 2 + BD 1 + B hypothesis g t .
 When considering the stationary distribution in a separable (noiseless) scenario, this theorem allows us to obtain a mistake bound that is reminiscent of the Perceptron convergence theorem. In partic-cumulative loss K = 0 , we obtain a bound on the number of mistakes 3.2 Convergence Analysis of the hypothesis sequence produced by ILK converges to the minimum risk of the batch learning counterpart g  X  := argmin g  X  X  P T t =1 R ( x t , y t , g ) at a rate of O ( T  X  1 / 2 ) . where U = CX  X  , a = 4  X C 2 X 2 + 2 U 2  X  , and b = U 2 2  X  are constants. In particular, if we obtain Essentially the same theorem holds for SILK, but now with a slightly larger constant a = 4  X  (1 + 2  X  ) C 2 X 2 + 2 U 2  X  . In addition, denote g  X  the minimizer of the batch learning cumula-the sample size T grows, T  X  X  X  , we obtain g  X   X  f  X  in probability. This subsequently guarantees the convergence of the average regularized risk of ILK and SILK to R ( f  X  ) .
 The upper bound in the above theorem can be directly plugged into Corollary 2 of Cesa-Bianchi et al. [7] to obtain bounds on the generalization error of ILK. Let  X  f denote the average hypothesis hindsight plus a term which grows as O q 1 T . We evaluate the performance of ILK and SILK by comparing them to NORMA [2] and its truncated variant. On OCR data, we also compare our algorithms to SVMD, a sophisticated step-size adap-tation algorithm in RKHS presented in [3]. For a fair comparison we tuned the parameters of each algorithm separately and report the best results. In addition, we fixed the margin to  X  = 1 for all our loss functions.
 Binary Classification on Synthetic Sequences The aim here is to demonstrate that ILK is better than NORMA in coping with non-stationary distributions. Each trial of our experiment works with 2000 two-dimensional instances sampled from a non-stationary distribution (see Figure 1) and the task is to classify the sampled points into one of two classes. The central panel of Figure 1 compares the number of errors made by various algorithms, averaged over 100 trials. Here, ILK and SILK make fewer mistakes than NORMA and truncated NORMA. We also tested two other algorithms, ILK(0) obtained by setting the decay factor  X  to zero, and similarly for NORMA(0). As expected, both these variants make more mistakes because they are unable to forget the past, which is crucial for obtaining good performance in a non-stationary environment. To further compare the perfor-mance of ILK and NORMA we plot the relative errors of these two algorithms in the right panel of Figure 1. As can be seen, ILK out-performs NORMA on this simple non-stationary problem. Novelty Detection on Video Sequences As a significant application, we applied SILK to a back-ground subtraction problem in video data analysis. The goal is to detect the moving foreground objects (such as cars, persons, etc) from relatively static background scenes in real time. The chal-lenge in this application is to be able to cope with variations in lighting as well as jitter due to shaking of the camera. We formulate the problem as a novelty detection task using a network of classifiers, one for each pixel. For this task we compare the performance of SILK vs. truncated NORMA. (The ILK and NORMA algorithms are not suitable since their storage requirements grow linearly). A constant buffer size  X  = 20 is used for both algorithms in this application. We report further implementation details in the full version of this paper.
 The first task is to identify people, under varying lighting conditions, in an indoor video sequence taken with a static camera. The left hand panel of Figure 2 plots the ROC curves of NORMA and SILK, which demonstrates the overall better performance of SILK. We sampled one of the initial frames after the light was switched off and back on. The results are shown in the right panel of Figure 2. As can be seen, SILK is able to recover from the change in lighting condition better than NORMA, and is able to identify foreground objects reasonably close to the ground truth. Our second experiment is a traffic sequence taken by a camera that shakes irregularly, which creates a challenging problem for any novelty detection algorithm. As seen from the randomly chosen frames plotted in Figure 3 SILK manages to obtain a visually plausible detection result. We cannot report a quantitative comparison with other methods in this case, due to the lack of manually labeled ground-truth data.
 Binary and Multiclass Classification on OCR data We present two sets of experiments on the MNIST dataset. The aim of the first set experiment is to show that SILK is competitive with NORMA and SVMD on a simple binary task. The data is split into two classes comprising the digits 0  X  4 and 5  X  9 , respectively. A polynomial kernel of degree 9 and a buffer size of  X  = 128 is of errors on the examples seen so far divided by the iteration number. As can be seen, after the initial oscillations have died out, SILK consistently outperforms SVMD and NORMA, achieving a lower average error after one pass through the dataset. Figure 4 (b) examines the effect of buffer size on SILK. As expected, smaller buffer sizes result in larger truncation error and hence worse performance. With increasing buffer size the asymptotic average error decreases. For the 10-way multiclass classification task we set  X  = 128 , and used a Gaussian kernel following [3]. Figure 4 (c) shows that SILK consistently outperforms NORMA and SVMD, while the trend with the increasing buffer size is repeated, as shown in Figure 4 (d). In both experiments, we used the parameters for NORMA and SVMD reported in [3], and set  X  = 0 . 00005 and C = 100 for SILK. In this paper we presented a general recipe for performing implicit online updates in an RKHS. Specifically, we showed that for many popular loss functions these updates can be computed effi-ciently. We then presented a sparse version of our algorithm which uses limited basis expansions to approximate the function. For graph-structured loss we also showed loss bounds and rates of convergence. Experiments on real life datasets demonstrate that our algorithm is able to track non-stationary targets, and outperforms existing algorithms.
 For the binary hinge loss, when  X  = 0 the proposed update formula for  X  t (16) reduces to the PA-I algorithm of Crammer et al. [8]. Curiously enough, the motivation for the updates in both cases seems completely different. While we use an implicit update formula Crammer et al. [8] use a Lagrangian formulation, and a passive-aggressive strategy. Furthermore, the loss functions they handle are generally linear (hinge loss and its various generalizations) while our updates can handle other non-linear losses such as quadratic or logistic loss.
 Our analysis of loss bounds is admittedly straightforward given current results. The use of more sophisticated analysis and extending our bounds to deal with other non-linear loss functions is on-going. We are also applying our techniques to video analysis applications by exploiting the structure of the output space.
 Acknowledgements We thank Xinhua Zhang, Simon Guenter, Nic Schraudolph and Bob Williamson for carefully proof reading the paper, pointing us to many references, and helping us improving presentation style. National ICT Australia is funded by the Australian Government X  X  Department of Communications, Information Technology and the Arts and the Australian Research Council through Backing Aus-tralia X  X  Ability and the ICT Center of Excellence program. This work is supported by the IST Program of the European Community, under the Pascal Network of Excellence, IST-2002-506778.
