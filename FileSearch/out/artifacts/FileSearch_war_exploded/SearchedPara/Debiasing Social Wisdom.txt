 With the explosive growth of social networks, many applica-tions are increasingly harnessing the pulse of online crowds for a variety of tasks such as marketing, advertising, and opinion mining. An important example is the wisdom of crowd effect that has been well studied for such tasks when the crowd is non-interacting. However, these studies don X  X  explicitly address the network effects in social networks. A key difference in this setting is the presence of social influ-ences that arise from these interactions and can undermine the wisdom of the crowd [17].

Using a natural model of opinion formation, we analyze the effect of these interactions on an individual X  X  opinion and estimate her propensity to conform. We then propose efficient sampling algorithms incorporating these conformity values to arrive at a debiased estimate of the wisdom of a crowd. We analyze the trade-off between the sample size and estimation error and validate our algorithms using both real data obtained from online user experiments and synthetic data.
 H.2.8 [ Database Management ]: Database Applications -Data Mining Algorithms, Experimentation Wisdom of crowd, Social Networks, Opinion Formation
The  X  X isdom of crowd X  effect refers to the phenomenon in which the average opinion of a diverse group of individuals is often closer to the truth than the opinion of any single member of the group [10]. The wisdom of the crowd is in-creasingly finding use in a plethora of contexts ranging from the traditional online surveys to query predicates involving human computation in database applications [20, 19]. In many of these applications, the underlying assumption is that the crowd does not interact and that individuals in the crowd form their opinions independently. With the explo-sive growth of social networks and online question-answer websites 1 , these platforms are becoming good sources for harnessing the collective opinions of online users. However, one significant difference in this new setting is the interacting crowd wherein the user can interact with her neighborhood to arrive at an opinion which might not necessarily reflect her original opinion. Crowdsourcing applications that rely on getting an unbiased opinion from the user will not work effectively in this setting. Lorenz et. al. [17] study how so-cial influence can undermine the wisdom of a crowd for a variety of estimation tasks.

The presence of interactions between a group of online users brings up two important problems that we address in this paper.

The first is that of characterizing the effect of these in-teractions on the dynamics of opinion formation in online social networks or ad hoc settings such as social crowds. In a social setting, a user can be associated with both an innate opinion and an expressed opinion [5, 11, 3, 22] for any given topic or question. Her innate opinion is typically formed in-dependent of online social interactions, while her expressed opinion could be shaped by the opinion of her online neigh-bors. This shaping of her expressed opinion is attributed to her propensity to conform . There is a line of impressive work that studied the consensus and fragmentation of the expressed opinions at steady state using different models of opinion formation [9, 13, 3, 2]. In this study, we adopt the model due to Friedkin and Johnsen [9], wherein the effect of social influence is captured by a user X  X  propensity to con-form to her neighborhood X  X  opinion. However, our focus is fundamentally different from [9] in that we rely the underly-ing opinion formation dynamics to extract the latent innate opinions from the network.

The second problem is that of factoring out the effect of social influence when estimating the collective wisdom of a crowd. In the presence of social interactions, this wisdom now corresponds to the average innate opinion of the crowd h ttp://www.quora.com (as opposed to the average expressed opinion after inter-a ctions). One important point to note is that the innate opinions of people are not known in general, and the knowl-edge of the users X  propensity to conform is either incomplete or noisy. This brings us to the two questions that we need to address for estimating the average innate opinions in the social network. First, we need efficient sampling algorithms that can obtain good estimates of the true average innate opinion using a small number of samples. Second, and more importantly, since we can only work with the expressed opin-ion of the nodes and cannot directly observe their innate opinion, our algorithms need to take care of debiasing the expressed opinions of the nodes that they sample.
In this paper, we study the above two problems using both an analytical opinion formation model that accounts for so-cial influence, and a real-world experimental setup based on online surveys among participants that can interact with each other. While these social interactions in our experi-ments are limited solely to a user being able to share her opinions with a random set of other online users, we show that even these relatively simple interactions can cause users to move away from their original innate opinions for a given question.
 We make the following contributions in this work
The findings suggesting that the collective opinion of a group is as good as, and often better than, the answer of an individual to a question are well established. Over a century ago, Galton [10] observed that the median of eight hundred participants X  responses to a weight guessing contest was accurate within 1% of the true answer. Surowiecki [23] surveys numerous case studies and experiments from stock markets, political elections, and quiz shows supporting the above statement, highlighting that the independence of indi-viduals X  opinions is a key requirement to form a wise crowd  X  an assumption undermined by the very nature of social networks [12, 21].

Lazer and Friedman [16] take an agent-based computer simulation approach to argue for a tradeoff between diver-sity and information flow, showing that the connectedness (in moderation) may improve the social wisdom and perfor-mance in several contexts. Lorenz et al. [17] demonstrate through human-subject lab experiments that the connected-ness may harm more than benefit by diminishing the diver-sity and also by false-boosting the confidence of the crowd.
Our problem somewhat relates to consensus formation [14] in the sense that each node aggregates the opinion of her neighborhood. One notable example in this setting is the work by DeGroot [5] which studies how consensus is formed and reached when individual opinions are updated using the average of the neighborhood of a fixed network. Work of Friedkin and Johnsen (FJ) [9], is perhaps the first study to extend the DeGroot model [5] to include both disagreement and consensus by associating with each node an innate opin-ion in addition to her expressed opinion. In their model, they propose a certain degree g i with which a user adheres to her initial opinion and by a susceptibility of 1  X  g i is socially in-fluenced by others in her network. French [8] used a similar model to empirically estimate the susceptibility values.
Budgeted actions and inquiries over social networks have been studied before in the context of influence maximiza-tion [7, 15], vaccination [6] and expectation polling [4]. This theme is increasingly motivated by the explosive growth of social networks and the inhibitive cost of covering their members. Our work differs from [4] in the sense that we do not assume that the individuals may explicitly provide an answer to an expectation poll, yet their expressed opinion reflects what they observe in their neighborhood as well as their innate opinion. Moreover, every individual -to express an opinion -take all her neighbors X  expressed opinions who take all their own neighbors expressed opinion and so on.
We consider an online social network graph G = ( V, E ) with nodes { v 1 , v 2 , . . . , v n }  X  V . For ease of notation, we will frequently interchange node v i and index i . The nodes v correspond to individuals, and edges E = [ e ij ], denote social interactions between the individuals. For a node v its set of neighbors is denoted by N ( i ) = { j : e ij = 1 } , and its degree is denoted by d i = | N i | . We denote the total number of edges in the graph as | E | . We define D  X  R n  X  n to be Diag( d 1 , d 2 , . . . , d n ).

We wish to estimate the average wisdom or opinion held in the social network about a particular topic or question of interest. As mentioned earlier, due to social interactions between the individuals in the social network, an opinion stated by an individual might not be the same as the original or true opinion held by the individual, and is often influenced by the expressed opinions in the individual X  X  social neighbor-hood. All that we are likely to observe in the social network at any instant of time is therefore the stated or expressed opinions held by individuals at the time, and not their origi-nal innate opinions. We refer to node v i  X  X  expressed opinion at time t as Y i ( t ), and its innate opinion as Y i (0) (which is assumed to be the same as the initial expressed opinion of the node at time 0). We assume that an opinion is expressed (or encoded) by a single real quantity, hence Y i ( t )  X  R f or all i and t . For each individual v i , we define a conformity parameter  X  i , 0  X   X  i  X  1, which is a measure of how strong her innate opinions are, and how likely will she be influenced by her neighborhood opinions. An  X  i value close to 1 implies that the individual is highly opinionated, and her expressed opinion is similar to her innate opinion, while a value close to 0 implies that the individual has a very weak innate opinion and consequently her expressed opinion is largely governed by the opinions of neighbors around her. Thus, the value 1  X   X  i represents agent i  X  X  propensity to conform. We define  X  D  X  R n  X  n to be Diag(  X  1 ,  X  2 , . . . ,  X  n ). We analyze a natural model for opinion formation due to Friedkin and Johnsen [9] (under weaker assumptions than in [9]) and show existence and convergence to a unique equi-librium.

In this model, individuals update their expressed opin-ion in discrete time steps by taking a convex combination of their innate opinion and the expressed opinions of their neighbors. As mentioned earlier, the weights in the convex combination depends on a user X  X   X  value. For simplicity, we assume that the individuals don X  X  distinguish between their neighbors and take their opinions equally important. Equation 1 captures the above model for all individuals i .
We show that the above opinion formation model defines a unique equilibrium as long as all  X   X  X  are non zero: that is, individuals hold an innate opinion that has some impact on what they express.
 Lemma 1 The above model has a unique equilibrium if  X  i &gt; 0 for all i .
 Proof. Consider any equilibrium Y  X  = { Y  X  1 , . . . , Y Equation 1. Then, for i = 1 , 2 , . . . , n , Y  X  satisfies O r, where Y 0 = { Y 1 (0) , Y 2 (0) , . . . , Y n (0) } T , and Thus, we have where I is the n  X  n identity matrix.
 We show that the matrix I  X  M  X  R n  X  n is non-singular. For any row i of I  X  M , the sum of absolute values of its non element is 1. Thus, using the Gersgorin Disc Theorem, every eigenvalue  X  of I  X  M lies within one of the discs { z : | z  X  1 |  X  1  X   X  i } for i = 1 , 2 , . . . n . Since  X  i &gt; 0, the eigenvalues of I  X  M cannot include 0. Thus, I  X  M is invertible, and the equilibrium is unique.

Hence Y  X  = ( I  X  M )  X  1  X  D Y 0 is the unique equilibrium of the opinion formation model defined in Equation 1. We now prove that after sufficient iterations, the model will indeed converge to this equilibrium.
 Lemma 2 If  X  i &gt; 0 for all i , then the above opinion for-mation model converges to its unique equilibrium Y  X  . Proof. Note that Y k +1 , the social opinion state at time k + 1 can be formulated by the following system: where M is the iteration Matrix. Define  X  k so that Y k = Y  X  +  X  k . By definition  X  k +1 = M X  k . Again we use the property that the sum of all the entries of each row in M is a non-negative quantity smaller than one to show that the error term goes to zero as k grows. Let  X  max k be the largest coordinate in  X  k . Hence, it is sufficient to show that  X  for some node i as long as  X  j &gt; 0 for all j .
Our goal is to estimate the average innate opinions in the social network (denoted by  X  Y 0 = P n i =0 Y i (0) /n ), by factor-ing out the social influences in the expressed opinions of the social network users. There are several reasons why this esti-mation of the average innate opinion of the social network is more important than the average expressed opinions. First, for many opinion polls and surveys, pollsters care about the true opinions held by an individual which might be quite different from their expressed opinions. Second, as shown in [17], the wisdom-of-crowd effect can break down when using individual opinions that are not independent and are influ-enced by social interactions. Hence it is important to use the original innate opinions of individuals when estimating answers using the wisdom-of-crowds phenomenon.

As mentioned earlier, there are two problems that we need to tackle when estimating the average innate opinions in the social network. Firstly, we need efficient sampling algo-rithms that can obtain good estimates of the true average innate opinion using a small number of samples. More im-portantly, since we can only work with the expressed opin-ions and cannot directly observe the innate opinions of in-dividuals, we need to take care of debiasing the expressed opinions of the sampled nodes.
We now describe three sampling algorithms that might be used to estimate the average innate opinion in the network.
The simplest and perhaps the most prevalent sampling method is uniform sampling , in which a sampling budget is decided and each node is sampled with a uniform probability ( with replacement) until the budget is exhausted. This naive algorithm is oblivious of any differences between innate and expressed opinions in the social network.

With the goal of eliciting the innate opinion and assuming that we have access to the  X  values, one may prefer to sample the nodes with large values of  X  as they are expected to retain their innate opinion in their expressed opinion to a Algorithm 1 U niformSampling 1: Choose a random sample S  X  V o f size r with replace-2: Output  X  Y 0 = 1 r P i  X  S Y i ( t ) . large extent. We call this c onformity sampling . and formally define it as follows: Algorithm 2 C onformitySampling 1: Choose a random sample S  X  V o f size r with re-
We use both sampling criteria as baselines and propose a n ew algorithm that outperforms the baselines both theoret-ically and empirically.
Our approach is reminiscent of the social sampling ([4]) algorithms where, instead of only using the opinions of the sampled nodes to estimate the true average opinion, the algorithm uses the opinions of the neighborhood of each sampled node via expectation polling. This might how-ever arguably be a strong assumption, since individuals are expected to accurately report their neighborhood X  X  average opinion. Furthermore, the authors do not distinguish be-tween innate and expressed opinions in the social network. As described in [4], the authors analyze sampling algorithms that sample each node v i with probability proportional to its degree d i to obtain a sample set S of size r . Assuming that the opinion of each node v i is Y i and the goal is to estimate the average  X  Y = P n i =0 Y i /n , the authors then propose us-a probabilistic bound for the estimation error |  X  Y  X   X  a sample size r .

In our case, the goal is to estimate the average innate opinion  X  Y 0 = P n i =0 Y i (0) /n by sampling a set S ; however we do not have access to the Y i (0) of the sampled set S or its neighbors, and can only observe the Y i ( t ) values instead, at time t . If we knew the Y i (0) values, the problem would de-generate to simple uniform sampling of the nodes, for which the following result holds.
 Fact 3 If we choose r = 1  X  2 l og(1 / X  ) samples, then with probability 1  X   X  , a uniform sampling strategy will give an estimate  X  Y 0 , such that |  X  Y 0  X   X  Y 0 |  X   X 
In our case however, since we have estimates for the  X  i values at all nodes in the graph, we would like to incorpo-rate these to guide our sampling strategy and our choice of estimator function  X  Y 0 . Note that since solving Equation 2 to obtain the Y 0 i values requires explicit knowledge of all the Y i in the network, we cannot directly use the equation to compute the Y i (0) values from the observed Y i ( t ).
Our approach therefore is to use the Y i ( t ) values directly in our estimator function  X  Y 0 , and to construct  X  Y 0 such that the Y i ( t ) of the sampled nodes and its neighbors lead to good approximations for the corresponding Y i (0) values. We will then carefully choose our sampling strategy in a way that tends to minimize these approximation errors.

In order to derive an  X  X ptimal X  sampling algorithm we rewind the opinion formation process as shown in the fol-lowing lemma. The following lemma shows how to get the average innate opinion from the expressed opinions Lemma 4  X  Y 0 = P n i =0 c i Y  X  i where P roof. Using Equation 2, we have Y 0 =  X   X  1 D ( I  X  M ) Y Thus E xpanding the right hand side of the last step into a sum form completes the proof
Note that the c i a bove might be negative. Guided by the above lemma, we now define the sampling probabilities p i our InfluenceSampling algorithm as Algorithm 3 I nfluenceSampling 1: Choose a random sample S  X  V o f size r with replace-
We now show that estimator  X  Y 0 o f the InfluenceSam-pling algorithm is an unbiased estimator of  X  Y 0 Lemma 5 The InfluenceSampling estimator  X  Y 0 is an unbiased estimator of  X  Y 0 Proof. Since we sample each node with probability pro-portional to | c i | , E [  X  Y 0 ] = ( P n j =1 | c j | )  X  P Theorem 6 U sing  X ( 1 H timator  X  Y 0 output by the InfluenceSampling algorithm satisfies |  X  Y 0  X   X  Y 0 | &lt;  X  with probability 1  X   X  , when the c are non-negative 2 . Furthermore, this number of samples is optimal up to constant factors.
 Proof. Note that we sample each node with probability proportional to | c i | . Let D c denote this sampling distri-bution. The number of samples required follows from the Hoeffding inequality [18]. This is because  X  Y 0 is the mean of i.i.d. random variables of the form ( P n j =1 | c j | )  X  Y where i  X  D c . Each of these is a bounded random vari-able in [  X  l, l ] where l = P n i =1 | c i | . Hence, using the Ho-effding inequality and Lemma 5, Prob( |  X  Y 0  X  E [  X  Y 0 ] | &gt;  X  ) = w e address the case when the c i are negative in Remark 7. samples. Thus, 2 l 2  X  2 l og(1 / X  ) is sufficient (and necessary [18]) to get an additive  X  approximation estimate with confidence 1  X   X  . For non-negative c i , l  X  P n i =1 1 n X  follows.

Note that the number of samples in the above theorem m atches (up to a constant factor of 1 H we get from Fact 3. Thus, InfluenceSampling performs almost as well as this optimal ideal uniform sampling algo-rithm, even though the latter has a huge advantage in terms of assuming accessing to the innate opinion. This is due to the fact that InfluenceSampling can take advantage of implicitly accessing neighbors X  innate opinions encoded in the node X  X  expressed opinion, as long as it is debiased properly. This shows the optimality of our algorithm. Remark 7 If the c i values are negative, then the number of samples needed in Theorem 6 could be larger (since l is nodes with negative c i correspond to subgraphs having large star topologies with node i in the center. We observe that in our experiments (and social networks in general) the number of such nodes is never too large. In fact, in our experiments, the c i values that we empirically calculate are always posi-tive.

A insightful special case is when all degrees are equal to that we give advantage to weakly opinionated nodes that are surrounded by strongly opinionated ones as they aggregate their innate opinions efficiently.
 Remark 8 Note that in practice we may not have the exact values of  X  i , but only estimates that are appromations of the true value. If these estimates are within an  X  additive factor of the true values then our estimator for the mean Y 0 i will also have a bias within at most  X  additive factor of the true value.

Also note that if we can compute c i for a small fraction of the nodes, that can be used to approximately estimate P i that is needed in InfluenceSampling .

A word about the assumption in our sampling algorithm on knowing the estimates of  X  values of users: it is not entirely unrealistic to get reasonable estimates for a user X  X  propensity to conform in real social networks. For example, on the Twitter Social Network, one might use a function of a user X  X  tweet and retweet frequency as a proxy for her  X  values. Similarly, on online sites that support discussion threads [1], a user X  X  participation history could be used to obtain an approximate estimate of her  X  values. We leave the characterization of algorithms that mine users X  social posts to obtain an estimate of her  X  value for future research. Furthermore, as the above remark shows, our techniques gracefully handle scenarios when these  X  values are either missing or noisy for some users.
We now validate our opinion formation model and our sampling algorithms using synthetic and real datasets. We first describe our datasets.
Figure 1: Screenshots of the DotsRegular survey
Our first set of experiments consisted of an interacting net-work of online users built using Amazon Mechanical Turk (mturk) and a personal website used to host the experi-ments. Subjects were recruited from Amazon Mechanical Turk and asked to take part in three online surveys hosted on the external website. Figure 1 illustrates the one of the online experiments. Figure 1(a) shows the first set of ques-tions asked to users while the subsequent screen in the exper-iment showing the  X  X nteractions X  is captured in Figure 1(b). We label these surveys DotsRegular , DotsRandom and TabletsRegular . The aim was to ask subjects X  opinions on questions in the surveys both before and after interac-tions with other subjects, and analyze their innate (initial) and expressed (final) answers. To show that our opinion models are not specific to a particular type of questions or topics, we ran our experiments using two different topics for our questions -the first one dealt with questions about properties of an image, and the second one dealt with the opinions about a class of consumer products. We detail the surveys next.
T he DotsRegular survey consisted of three questions about three images containing a set of dots. The first im-age consisted of 1000 randomly distributed black dots in a circle. The second image consisted of 3000 randomly dis-tributed black dots in a circle, and the third image consisted of a mixture of 900 red dots and 1800 blue dots randomly distributed in a circle. There were three questions in the surveys. The first two questions were about guessing the number of dots in the first and second image respectively. The third question was about guessing the percentage of red dots in the third image. In all the images, the dots were finely spaced enough to discourage explicit counting of dots by the participants to estimate the answers.

The survey participants were asked to log onto the sur-vey site and complete the survey only during a pre-specified 30-minute time window. This ensured roughly simultaneous participation by all the subjects in the online survey. In this time duration, the subjects were first asked to provide their answers to the three questions (which was treated as their innate opinions about the questions). Then, for each partic-ipant we randomly assigned a set of 5 other participants as her  X  X eighbors X , and showed her their current answers. We then gave her an opportunity to update her current answers. This process was repeated for each survey participant for up to 3 iterations (the answers of the participants changed very little after 3 iterations, hence we chose to report the answers provided by the users after the third iteration.) Thus we created a social graph among the survey participants that is 5-regular, within which each participant interacts with her neighbors during the process of converging to her final an-swer. Figure 2(a) illustrates the resulting graph. It has 125 nodes corresponding to the number of participants in the experiment and 625 edges.

The DotsRandom survey was identical to the DotsReg-ular survey, except that instead of fixing 5-neighbors for each participant, we randomly selected a varying number of neighbors (from 1 through 15) for a given participant. This experiment elicited response from 63 participants resulting in graph with 63 nodes and 584 edges. We note that in both the surveys, the original answers provided by the partici-pants (before any interactions with other participants) were treated as their innate opinions and the final answers were treated as their expressed opinions.

The final survey, TabletsRegular , had a setup simi-lar to DotsRegular , except that we asked a different set of questions related to two recently introduced tablet com-puter products (A and B) in the market. The first question asked about the opinion of the participant regarding the potential success of a tablet computer in the market. The participant was asked to choose between 5 options: Strongly Positive, Positive, Neutral, Negative and Strongly Negative. The question also asked the participant to provide a sentence justifying their choice. Thus, these justifications could in-clude positive, negative, and neutral comments about the product corresponding to the star rating provided by the user. Another unrelated question asked participants about what they thought would be the total sales volume of an-other tablet in the month of January 2013. These questions are markedly different from the dots surveys in the sense that they are more subjective. In fact, as part of the survey associated with the first question, we showed a user with the justifications written by her randomly selected set of neigh-bors as a proxy for the influence she might experience as part of her opinion forming process. As before, the original answers for the first and second provided by the partici-pants (before any interactions with other participants) were treated as their innate opinions and the final answers were treated as their expressed opinions. Again, the total number of participants for this survey was 125 users resulting in a graph with 125 nodes and 625 edges.
The second set of experiments were performed using syn-thetic data. We generated random graphs with 200000 nodes and two different types of degree distributions: regular graphs with degree 20, and power-law graphs degree of each node was obtained from a Zipf distribution truncated to lie be-tween 20 and 200. For each graph, we generated the  X  pa-rameters for each node from a uniform distribution in [0 , 1]. We also generated an initial (innate) opinion for each node from a Gaussian distribution with mean 50 and varying val-ues of variance ranging between 0 and 1000. We then ran our opinion formation dynamics on the graph until the ex-pressed opinions converged to the equilibrium value and used the starting (innate) values and the converged values as the expressed opinions in our analysis of the model and the sam-pling algorithms.
For the social graphs corresponding to both the survey and the synthetic datasets, we then ran the various sampling algorithms using different sample sizes r , read the expressed opinions of the sampled nodes, and output an estimate of the average innate opinion in the graph as a function of these expressed opinions. We compute the estimation error to be the absolute difference between this estimate and the true average innate opinion. We repeat the sampling process over 100 runs and report both the average mean and variance of the estimation error over all the runs. We began by validating the opinion formation model from Section 3. Note that the  X  parameter is central to the model as it captures the intrinsic behavior of a user when it comes to interacting with her neighborhood in the process of form-ing an opinion. Specifically, we set out to measure how in-nate is this parameter to the user, i.e., how consistent was the user in her behavior for different questions. This consis-tency would reflect in a rather stable value of the  X  parame-ter. To study this, we use the opinion model equation (1 and the initial and final answers for each of the survey questions, to obtain an estimate of the  X  parameter for each partici-pant: we denote them by  X  1 ,  X  2 and  X  3 . We then measure how similar these three estimated  X  values are for each par-ticipant. For every (  X  i ,  X  j ) pair for a participant, we plot the relative difference in values (to handle to case where one the relative difference) Figure 3(a) plots the relative pairwise difference in the computed  X  values from the three ques-tions in the DotsRegular Survey (the plots corresponding to the DotsRandom and TabletsRegular experiments were similar). As the figure illustrates, the relative differ-ence in the computed  X  values are indeed small (less than 0 . 2) for almost 60% of the participants, which shows that the  X  values indeed are largely independent of the questions asked. In particular, the average relative difference over all users and all (  X  i ,  X  j ) pairs was observed to be 28%.
Figure 3(b) plots the distribution of  X  1 values among the survey population of DotsRegular . It is interesting to note that while a majority of the users have a high  X  value, there is a non-trivial fraction of users that value the opinion of their neighbors or have a higher propensity to conform.
Figures 4(a) and 4(b) plots the corresponding relative pairwise difference in computed  X  values and the distribu-tion of the  X  1 values for the TabletsRegular Survey. No-tice that the distribution is very similar to the DotsReg-ular case, which again seems to indicate that the  X  values are largely independent of the questions for a given topic.
Next, we move to the question of estimating the average innate opinion of the survey population (i.e., the wisdom of the social crowd) by using only a small sample of the ex-pressed opinions. Using the  X  parameter value associated with the first question of each of the three surveys ( Dot-sRegular , DotsRandom , and TabletsRegular ) we run the three sampling algorithms described in Section 4, viz. , InfluenceSampling , UniformSampling and Conformi-tySampling , and compare the estimates (  X  Y 0 ) output by each of the algorithms against the true average innate opin-ion (  X  Y 0 ). For a given sample size, we run each sampling algorithms over 100 runs and plot the mean and variance of the estimation error |  X  Y 0  X   X  Y 0 | .

Figure 5 plots the average mean and variance of the es-timation errors for each sampling algorithm as a function of the samples sizes for the DotsRegular , DotsRandom and TabletsRegular experiments. As seen in the figure, InfluenceSampling outperforms both UniformSampling and ConformitySampling in all three surveys in terms of the mean estimation error, by margins ranging from 10% to almost 30%. This validates our theoretical results showing optimality of the InfluenceSampling algorithm. More in-tuitively, our algorithm outputs a more accurate estimate by picking nodes that implicitly aggregate their innate neigh-borhood opinion.

Again, in terms of the standard deviation of the estima-tion error, InfluenceSampling achieves significantly lower standard deviations than either UniformSampling and Con-formitySampling . Furthermore, the corresponding curves for InfluenceSampling are much better behaved (in terms of the expected monotonic decay as the number of sam-ples increase) compared to the other sampling algorithms. The standard deviation plots for UniformSampling and ConformitySampling algorithms does not decay smoothly with the number of samples, and the reason for this was not intuitively obvious to us.
For the synthetic datasets, our main focus was to evalu-ate the performance of the sampling algorithms at scale. As mentioned earlier, we use a uniform distribution for the  X  values, and a Gaussian distribution (with variance ranging from 0 to 1000) for the innate opinions at each node. We con-duct our experiments for both regular graphs and power-law graphs. For each type of graph, we report our estimation er-ror results both for a high-variance case and a lower-variance case for the innate opinion distribution. We compare the es-timates (  X  Y 0 ) output by each of the algorithms against the true average innate opinion (  X  Y 0 ). For a given sample size, we run each sampling algorithms 100 times and report the mean and variance of the estimation error |  X  Y 0  X   X  Y 0 | .
Figure 6 plots the mean and variance of the estimation er-rors for each sampling algorithm as a function of the samples sizes for regular graphs and for power-law graphs.
Again, as with the survey experiments, InfluenceSam-pling outperforms both UniformSampling and Confor-mitySampling in all three surveys in terms of the mean estimation error, by as much as 200% for 10 samples, and around 30% for 100 or more samples. As Figure 6 illus-trates, the gap in performance of InfluenceSampling is less pronounced as the number of samples increases.
In terms of the standard deviation of the estimation error too, InfluenceSampling achieves significantly lower stan-dard deviations than either UniformSampling and Con-formitySampling for a small number of samples, though this gap goes down as the number of samples increases.
Finally, we studied the convergence of the opinion forma-tion dynamics of Equation 1 in terms of the average change in the expressed opinion of nodes over two successive iter-ations. As seen from Figure 7, expressed opinions even for regular graphs of size 200000 converges to the equilibrium values in as little as 3 steps something which was also ob-served in our real-world survey experiments.

As might be expected, the improved performance of In-fluence Sampling over the other sampling algorithms is even more significant in these large scale synthetic graphs, than for the small-scale survey experiments on Mechanical Turk.
In this paper, we considered the problem of analyzing and debiasing the  X  X isdom of crowd X  phenomenon in the pres-ence of online social interactions. We adopted a natural opinion formation model that depends on users X  propensity Figure 7: Convergence of the opinion formation pro-c ess for synthetic regular graphs to conform (as characterized by their  X  parameters), and de-signed a provably-efficient sampling algorithm ( Influence-Sampling ) that uses these  X  values to estimate the average innate opinion of the social crowd with a small number of samples. We validated the opinion formation model on User Opinion Surveys, and evaluated our sampling algorithm on both real and synthetic data.

One direction of future work is to validate the model and sampling techniques on large social networks under more richer social interactions. [1] L. Backstrom, J. Kleinberg, L. Lee, and [2] K. Bhawalkar, S. Gollapudi, and K. Munagala.
 [3] D. Bindel, J. Kleinberg, and S. Oren. How bad is [4] A. Dasgupta, R. Kumar, and D. Sivakumar. Social [5] M. H. DeGroot. Reaching a consensus. J. American [6] Z. Dezs  X o and A.-L. Barab  X asi. Halting viruses in [7] P. Domingos and M. Richardson. Mining the network [8] J. French. A formal theory of social power.
 [9] N. E. Friedkin and E. C. Johnsen. Social influence and [10] F. Galton. Vox populi. Nature , 75:450, 1907. [11] S. Goel, W. Mason, and D. Watts. Real and perceived [12] M. Granovetter. Threshold models of collective [13] R. Hegselmann and U. Krouse. Opinion dynamics and [14] S. Judd, M. Kearns, and Y. Vorobeychik. Behavioral [15] D. Kempe, J. Kleinberg, and E. Tardos. Maximizing [16] D. Lazer and A. Friedman. The network structure of [17] J. Lorenz, H. Rauhut, F. Schweitzer, and D. Helbing. [18] R. Motwani and P. Raghavan. Randomized algorithms . [19] A. G. Parameswaran, H. Garcia-Molina, H. Park, [20] H. Park, H. Garcia-Molina, R. Pang, N. Polyzotis, [21] T. Schelling. Micromotives and Macrobehavior . [22] B. Shiv. The lonely shopper -ttp://www.gsb. [23] J. Surowiecki. The Wisdom of Crowds . Anchor, 2005.
