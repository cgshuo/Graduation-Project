 In recent years, sentiment analysis, which mines opinions from large-scale subjective information available on the Web such as news, blogs, reviews and tweets, has at-tracted much attention [7, 15]. It can be used for a wide variety of applications, such as opinion retrieval, product recommendation, political polling and so on. 
In such applications, polarity lexicons consisting of either positive or negative words/phrases are usually important resources for practical systems. They can be con-resources such as WordNet to induce positi ve/negative words [2]; or learning senti-ment-bearing words from large-scale corpora, such as news corpora [2] or even the Web [15]. 
Graph models have been recently used in sentiment analysis for various tasks, such as polarity lexicon induction, ranking the word senses by polarity properties, or document-level sentiment analysis. However, most work has been done based on ei-ther on WordNet or English documents. Although these methods can be applied on Chinese, Chinese has its own special characteristics, i.e. Chinese words are composed of characters or morphemes, the smallest meaning blocks. In Chinese, each morpheme has its own meaning, and the polarity of a Chinese word is influenced or even deter-mined by the polarities of its component morphemes. Ku et al. [5, 6] proposed the character-based methods to use sentiment sc ores of Chinese characters to compute the sentiment scores of Chinese words. 
However, either of these two kinds of models, namely graph models and character-other hand, these two kinds of models are complementary to each other, since they models respectively the external relations and internal structures of one Chinese word: resources or real texts, while morphological features denote the internal formation or structure of individual Chinese words. 
We first build word graphs based on lexical sources, and then induce more posi-tive/negative words from seed words by using semi-supervised graph models. We also propose to use machine learning for polarity classification of Chinese words based on morphological features. Then, we integrate the external relations and internal struc-tures of Chinese words (i.e. graph models and morphological feature-based models) achieves significantly better performance than the baselines. The rest of the paper is organized as follows. Sec. 2 introduced related work. In Sec. 3, we describe our method for learning Chinese polarity lexicon. Sec. 4 gives the experiments, followed by the discussion in Sec. 5. Finally, we conclude in Sec. 6. Some related works have tackled the automated determination of term polarity based on either corpora or lexical resources such as WordNet. Hatzivassiloglou and McKe-own [2] learned polarity of adjectives by exploiting co-occurence of conjoined adjec-tives. Turney and Littman [14] used two statistical methods, namely PMI-IR and LSA, to calculate the polarity of individual terms by calculating mutual information between words and seed words via search engines or corpora. Kamps and Marx [4] proposed the WordNet-based method to compute the word polarity by calculating the semantic distance between words and seed words good and bad . 2.1 Graph Models for Polarity Lexicon Induction Recently, graph models have also been trie d on polarity lexicon induction. Esuli and Sebastiani [2] presents an application of PageRank to rank WordNet synsets in terms of how strongly they possess a given semantic property, e.g. positivity and negativity. Rao and Ravichandran [8] treated polarity detection as a semi-supervised Label Propagation problem in a graph. Their results indicate that Label Propagation im-proves significantly over the baseline and other semi-supervised learning methods like Mincuts and Randomized Mincuts for this task. 
Velikovich et al. [15] described a new graph propagation framework by construct-English lexicon that is significantly larger than those previously studied. They evalu-ated the lexicon, both qualitatively and quantitatively, and show that it provides supe-rior performance to previously studied lexicons. 2.2 Chinese Polari ty Lexicon Induction For Chinese, Yuen et al. [16] proposed a method, based on [14], to compute the polar-ity of Chinese words by using their co-occurrence with Chinese morphemes. It was noted that morphemes are much less numerous than words, and that also a small num-ber of fundamental morphemes may be used to get great advantage. Zhu et al. [18] tried to compute the polarity of Chinese wo rds using the semantic distance or similar-ity between words and seeds in HowNet 1 based on [14]. 
Ku et al. [5] measured sentiment degrees of Chinese words by averaging the senti-ment scores of the composing characters, called the bag-of-character (BOC) method. The sentiment score of each character is calculated by using the observation probabili-ties of the character in positive and negative seed words. Ku et al. [6] further consid-ered the internal morphological structures of Chinese words for opinion analysis on words. Chinese words were classified into eight morphological types by the proposed classifiers, and then heuristic scoring rules were manually defined for each morpho-logical type based on the character scores obtained by the BOC method. 2.3 Analysis of the Two Kinds of Models Graph models and the morpheme-based or character-based models provide different perspectives of Chinese words, and have different characteristics. Word graph encode the external relations of one word with ot hers, while morphological features represent the internal formation or structures of Ch inese words. Graph models would need ex-ternal resources, such as thesauri, lexical resources, or large corpora to construct word arbitrary word without any thesauri or la rge corpora. However, the character-based methods could have the following problems: (1) The polarities of many Chinese words cannot directly be derived from its com-ponent characters, such as  X  X  X  (fail),  X  X  X  (in panic),  X  X  X  (malicious), etc. For ex-characters have salient polarity, but the whole word is negative; (2) A character may have many possible senses with different polarities, but the character-based methods only compute one polarity score for each character. For in-stance, the character  X  has many senses in HowNet: a) SelfMoveInManner|  X  X  X  X  X  X   X  or alter|  X  X  X  , e.g. the  X  in  X   X  (turmoil) and  X   X  (unrest) is negative; b) excite|  X  X  X  , e.g. the  X  in  X   X  (making you feel emotional or sympathetic) and  X   X  (pleasant to the ears) is positive; c) use|  X  X  X  , e.g. the  X  in  X   X  (utilize) is neutral; (3) To cover most Chinese characters, the character-based methods will need a large amount of training data, i.e. Chinese words annotated with polarities. 
The problem of graph models could be the need of large-scale lexical resources or corpora to construct the word graphs and to achieve good performance, and even with such large-scale resources it sometimes cannot cover the words concerning. But they could more easily adapt to different domains and compute domain-dependent polarity score based on different corpora. Meanwhile, once the word graphs are constructed, graph models can do semi-supervised learning with a small number of seed words. 
From the above analysis, we can know that the two kinds of model have different advantages. Thus it would be very attractive to integrate graph models and the mor-pheme-based or character-based methods to get better performance. 3.1 Graph Models for Polarity Lexicon Induction the nodes are all words, both labeled and unlabeled, and the edge between nodes i, j is weighted so that the closer the nodes are, the larger the weights w ij . Intuitively, words that are close should have similar labels, and thus the labels of a node could be propa-gated to all nodes through the edges. We as sume as input an undirected edge weighted sume as input two sets of seed words, denoted P for the positive seed set and N for the negative seed set. After constructing the graph, we can use Label Propagation or Pag-eRank to derive Y U from X and Y L based on the graph. 
Label Propagation (LP) is an iterative algorithm for classification or regression [17], where each node takes on the weighted average of its neighbor X  X  values from the previous iteration. The result is that nodes with many paths to seeds get high polarities due to the influence from their neighbors. LP is known to have many desirable prop-erties including convergence, a well defined objective function (minimize squared error between values of adjacent nodes), and an equivalence to computing random walks through graphs. We use LP taking a form slightly different from the algorithms sets. The neutral words are manually chosen from the top 200 most frequent words in the LIVAC 2 corpus, and many of them are monosyllabic, include  X  (of),  X  (at),  X  (one),  X  (and),  X  (he), etc. The neutral words are used as stop words to prevent po-larity propagate into them and also to prevent flow from passing through them into other related words. 
PageRank is also a random walk model [1], but used for ranking problem. It allows we can normalize the scores for each ranked list, and then use the score in the positive ranking minus that in the negative ranking to get the final score for each word. If the final score is positive, then the word can be classified as positive; otherwise, negative. By this means, we are actually using PageRank for classification. Although Label Propagation and PageRank originally were proposed for different tasks, namely clas-sification and ranking, respectively, they actually are closely related and have theo-retical connection [8]. 3.1.1 Building Word Graphs The word graph could also be built by different means from a wide variety of re-sources, such as news corpora [2], WordNet [8], and the Web documents [15]. In this paper, we use Tongyici Cilin (Cilin) [12] and a combined bilingual lexicon to con-struct word graphs. All the entries in Cilin are organized in a hierarchical tree, and the vocabulary is divided into different categories, i.e. 12 large categories and 1, 400 sub-categories. There are some synonym groups within each subcategory, and the words in the same group either have the same or similar meaning or have high relevance. The total number of synonym groups in Tongyici Cilin is 13,440. Following are two synonym group examples: 
Ed03A01={  X  ,  X  ,  X  ,  X  ,  X  ,  X  ,  X  X  ,  X  X  ,  X  X  X  ,  X  X  X  , ...} standing, wonderful, ...} 
Ed03B01={  X  ,  X  ,  X  ,  X  ,  X  ,  X  ,  X  X  X  ,  X  X  X  ,  X  X  ,  X  X  , ...} {bad, bad, inferior, weak, shallow, rubbishy, not good, not fine, poor, bad, ...} 
Another lexical resource we used for word graph construction is a combined bilin-different words in another language. For example, beautiful could be translated into  X   X  ,  X  X  X  , or  X  X  X  in Chinese, while ugly could be translated into  X  ,  X  X  X  or  X  X  X  . In such cases, the different translations of the same word could be seen as synonyms. We LDC_CE_DIC2.0 3 constructed by LDC, bilingual terms in HowNet and the bilingual lexicon in Champollion 4 . In total, there are about 251K bilingual entries in the combined dictionary. By using the English words as the pivot, we get 45,448 synonym groups. 
In our constructed word graphs, the nodes are all words, the edge between nodes indicates a synonym relation and each edge has a weight w of 1 initially. We assume there are n nodes in the graph which could be represented as a n  X  n transition matrix T derived by row-normalizing edge weights. Then, Labe Propagation and PageRank can be run on the constructed matrix. 3.2 Polarity Lexicon Induction with Morphological Features As words are the basic building blocks for texts, most researches on sentiment analy-sis in English have been done based on words. However, when it comes to Chinese, the situation is rather different. The majori ty of Chinese words in a corpus are disylla-bic or polysyllabic, where each syllable is normally represented by a single logograph, or usually a morpheme. The meaning of the predominant polysyllabic words can be seen as derived from the meanings of its component morphemes, which are consid-ered to be the smallest meaningful linguistic unit 5 . 
In the BOC method [5], the opinion score of a word is determined by the combina-tion of the observation probabilities of its composite characters in positive and nega-tive words. In our implementation of the BOC method, we make a small modification by considering negation markers, such as  X  ( no ) ,  X  ( no ) ,  X  ( no ). For the calculation of the sentiment score of a character c , if a negation marker neg occurs before some other characters, the characters following the neg would be considered as occurring in a word with an opposite polarity. For instance, when computing the frequency of the character  X  (good) as a positive or negative character, our modified method would consider the  X  in the negative word  X  X  X  (not good) as a positive occurrence because of the negation marker  X  before the character  X  ; while in the original BOC method, word. Negation markers are processed with the similar method for calculating senti-ment scores of test words by the scores of its component characters. 
One problem of the BOC method is that it only assigns a sentiment value for each character without considering character co ntexts, and cannot easily integrate other characters in the word, into the model. Therefore, we propose to learn word polarity using machine learning by integrating more morphological features in addition to its component characters as basic features. The polarity lexicon induction is considered a classification problem and we use machine learning to solve it by using morphologi-shown in Table 1. 
The POS of each Chinese word is obtained by querying the POS of that word with most senses in HowNet. The features mined for each Chinese word with templates are converted into a vector in which each dimension has the weight of 1. 3.3 Integrating Graph Models and Morphological Features Since graph models and morphological features provide two individual and independ-ent perspectives (i.e. external and internal) of Chinese words, we propose to integrate them to achieve better performance. After obtaining different classifiers based on graph models and morphological features, we could exploit different ensemble meth-ods to combine the results of individual cla ssifiers. According to theoretical analysis [8], the effectiveness of ensemble learning is determined by the diversity of its com-models and morphological features could satisfy this diversity requirement. number of classifiers, and f k ( x )  X  [-1, 1]. We exploit the following ensemble methods for deriving a new value from the individual values: average of the values in F . (2) Weighted Average: This combination method improves the average method by associating each individual value with a weight, indicating the relative confidence in the value. The weights are experimented to be set in the following two ways: individual classifier obtained on the development data. individual classifier obtained on the development data. (3) Majority Voting: This combination method relies on the final polarity tags polarity tags based on the individual analysis results in the p classifiers. The polarity tag receiving more votes is chosen as the final polarity tag of the Chinese word. (4) SVM Meta-classifier: Motivated by the supervised hierarchical learning, we also propose to use SVM to automatically adjust the weights for each component framework (namely SVM) as features, and thus a weight model for individual classifiers is learned from the training and development data. By this strategy, we actually use a two-level classification model with a higher-level meta-classifier to learn the corresponding weights for the individual lower-level classifiers. Two manually constructed polarity lexicons are used as gold standard for evaluation: The Lexicon of Chinese Positive Words [13] consisting of 5,045 positive words, and The Lexicon of Chinese Negative Words [19] consisting of 3,498 negative words. Thus, we The bag-of-character method [5] and Label Propagation [8] are used as baselines. 
We use the standard precision (Pre), recall (Rec), and F-measure (F1) to measure the performance of positive and negative class, respectively, and used the MacroF1 and MicroF1 to measure the overall performance. The metrics are defined the same as in general text categorization. The SVM light package is used for training and testing, with all parameters set to their default values. The evaluations are shown as follows. 4.1 Experiments with Graph Models In this section, we evaluate the performance of PageRank and Label Propagation (LP) on the word graphs built from two resources, namely Tongyici Cilin (Cilin) and the bilingual lexicon (BiLex) introduced in Sec. 3.2.1, and the graph built from their combination (Cilin+BiLex). The residual probability of PageRank is set to 0.85. Since we do not have annotated ranking data of Chinese polarity lexicon to evaluate PageR-ank, we use the converted classification results introduced in Sec. 3.1 for the evalua-tion. We run both algorithms for 10 iterations, and show the results in Table 2. 
From Table 2, we can observe that: the graph models show better performance on the word graph built from the combination of Cilin and BiLex than on the graphs built from either of the resources; and the graph models show better performance on Cilin than on BiLex. Meanwhile, PageRank and Label Propagation show similar perform-ance, and the differences are not so remarkable for the word graphs built from Cilin, BiLex or their combination. 4.2 Experiments with Models of Morphological Features In this section, we investigate the performances of different models with morphologi-cal features, including the BOC (Ku) method [5], our modified BOC method with negation processing introduced in Sec. 3.2, and our proposed SVM models with dif-ferent kinds of features introduced in Sec. 3.2. The SVM-All method uses all the fea-tures introduced in Table 1. The results are shown in Table 3. 
The SVM models outperform the BOC models. Although the improvement of about 1% from BOC to SVM-ALL seems not remarkable, the t-test shows that the MacroF1 and MicroF1 differences are statisti cally significant at the 90% level, and the difference between BOC and SVM-Uni+Bi, or between SVM-All and BOC is with position, and POS into SVM, we can improve by about 0.5% compared SVM with only unigrams. Meanwhile, our modified BOC method achieves slightly better result than the original BOC method [5]. 
We also tried to integrate the features of morphological types in [6] into our mor-phological feature-based SVM model. Since we do not have the words annotated with morphological types, we just use the unsupervised heuristic rules to compute the mor-phological type for each Chinese word, and then use the heuristic rules in [6] or inte-grate it into the SVM model. But it did not improve the performance. 4.3 Experiments on Integration In this section, we investigate the performance of the integration of graph models and morphological features. Different classifier combinations are tried based on the SVM strategy introduced in Sec. 3.3. Since the graph built from the combination of two lexical resources show better performance than the graph built from the individual resources, we use only the combination graph for the graph models in this section. The development data are used to adjust the parameters of each model. The results are shown in Table 4. The LPBOC method denotes the BOC method based on the posi-tive and negative word lists generated by the LP model. 
All of the ensembles in Table 4 significantly outperform the baselines: the bag-of-character method in Table 3, and Label Propagation in Table 2, which shows that graph models and models with morphological features have their own evidences for polarity classification, and thus the integration of models could significantly improve performance. The best performance is obtained by the integration of all the four methods: LP, ML-ALL, BOC and LPBOC, i.e. it improves MacroF1 to 94.56% from 88.69% of BOC or 76.42% of LP, and improves MicroF1 to 94.75% from 89.16% of BOC and 76.56% of LP, which are both significant improvements. Even without graph models, we can also significantly improve performance by integrating BOC and SVM-ALL. The integration of LP with BOC shows better performance than that of LP with SVM-ALL, but the integration of these three methods outperforms the integration of any two methods. We then investigate the other ensemble methods introduced in Sec. 3.3 to integrate LP, BOC, SVM-ALL, and LPBOC. Table 5 gives the comparison results. We can see that all the ensemble methods outperform the constituent individual method, while SVM performs the best, followed by the precision-weighted average. The results further dem-onstrate that 1) the good effectiveness of the ensemble combination of individual analy-sis results for Chinese word polarity classification, 2) the SVM strategy seems to be able to find better weights compared with other simpler combination methods. tion. Fig. 1 shows the influence of iteration numbers of Label Propagation on the com-(Com). We can observe that the precisions of Label Propagation for Cilin and Com show little difference, both above 90%, but the recalls with Com are much higher with those with Cilin, and thus the F1s are much higher with Com consequently. training words) on Label Propagation (LP), the BOC method and the SVM Meta-classifier based integration of LP+BOC+SVM-ALL+LPBOC (Integration) introduced in Sec. 4.3 . From Fig. 2, we can see that (1) The precision of BOC improves steadily with more training data, from 75% of 100 training words to above 90% of 5K+ words; while the precision of LP remains quite high (i.e. always above 86%), even with only 100 seed words; (2) The recall of BOC improves even much faster the precision when the training data increases, from 23% of 100 training words to above 87% of 5K+ words; while the recall of LP improves much slowly. From Fig. 3, we can observe that the SVM-based integration of the four methods has been always significantly outperforming the individual methods of BOC and LP, and even with only 100 seed/training data, we can achieve 82% MicroF1. 
To summarize Fig. 2 and 3, when the training data is small, LP outperforms BOC, but when the training data becomes large, BOC outperforms LP inversely. However, no matter how much training data, the integration of graph models and morphological fea-tures could significantly improve the performance compared to individual methods. This paper proposes a novel approach to integrate both internal structures and external relations of Chinese words for polarity lexicon induction via graph models and mor-phological features. The polarity detection is first treated as a semi-supervised learn-ing in a graph, machine learning, namely Support Vector Machine (SVM), is used based on morphological features, and then we integrate morphological features with the graph models to further improve the performance. The results show that the inte-gration could significantly improve the performance. 
In future work, more resources could be explored to further improve the results, especially large-scale corpora or even th e Web. Since a word could have different word senses in Chinese, instead of only the word level. Meanwhile, evaluation of the ranking problem of word polarities could be another direction. Acknowledgements. We wish to thank Prof. Jingbo Zhu from Northeastern University, China, Dr. Preslav Nakov from National University of Singapore, and anonymous reviewers for their valuable comments, as well as the Harbin Institute of Technology X  X  IR Lab for their sharing of the extended version of Tongyici Cilin.
