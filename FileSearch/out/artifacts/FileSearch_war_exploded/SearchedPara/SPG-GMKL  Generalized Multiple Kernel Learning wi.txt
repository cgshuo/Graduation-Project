 Multiple Kernel Learning (MKL) aims to learn the kernel in an SVM from training data. Many MKL formulations have been proposed and some have proved effective in certain applications. Nevertheless, as MKL is a nascent field, many more formulations need to be developed to generalize across domains and meet the challenges of real world applications. However, each MKL formulation typically necessitates the development of a specialized optimization algorithm. The lack of an efficient, general purpose optimizer capable of handling a wide range of formulations presents a significant challenge to those looking to take MKL out of the lab and into the real world.

This problem was somewhat alleviated by the develop-ment of the Generalized Multiple Kernel Learning (GMKL) formulation which admits fairly general kernel parameteriza-tions and regularizers subject to mild constraints. However, the projected gradient descent GMKL optimizer is ineffi-cient as the computation of the step size and a reasonably accurate objective function value or gradient direction are all expensive. We overcome these limitations by develop-ing a Spectral Projected Gradient (SPG) descent optimizer which: a) takes into account second order information in selecting step sizes; b) employs a non-monotone step size selection criterion requiring fewer function evaluations; c) is robust to gradient noise, and d) can take quick steps when far away from the optimum.

We show that our proposed SPG-GMKL optimizer can be an order of magnitude faster than projected gradient descent on even small and medium sized datasets. In some cases, SPG-GMKL can even outperform state-of-the-art special-ized optimization algorithms developed for a single MKL for-mulation. Furthermore, we demonstrate that SPG-GMKL can scale well beyond gradient descent to large problems in-volving a million kernels or half a million data points. Our code and implementation are available publically. I.2.6 [ Artificial Intelligence ]: Learning Algorithms, Performance, Experimentations Multiple Kernel Learning, Support Vector Machines, Spec-tral Projected Gradient Descent
Support Vector Machines (SVMs) have become ubiquitous in diverse areas ranging from computer vision to natural lan-guage processing to bioinformatics. One of the reasons for their widespread adoption is the availability of general pur-pose, efficient optimizers capable of handling various SVM formulations as demanded by real world applications.
The success of non-linear SVMs also depends on the choice of a good kernel. Multiple Kernel Learning (MKL) tech-niques aim to learn such a kernel from training data often as a combination of given base kernels. MKL can be used for various tasks such as learning a similarity measure tailored to the given application, heterogeneous feature combination and learning models with sparse structures. For instance, MKL techniques have yielded state-of-the-art results on very challenging object recognition [22] and object detection [30] problems in computer vision. They have also yielded supe-rior results as compared to many state-of-the-art filter and wrapper methods for feature selection [28]. However, MKL techniques have not proved to be as popular as the baseline SVMs upon which they hope to improve.

The primary reason for this is that MKL is still a nascent field and, while a few formulations have shown promising results in certain contexts, many more need to be devel-oped that are robust to over fitting and generalize across domains. At the same time, existing formulations need to be tried out on many real world applications in order to de-termine regimes in which they perform well. Unfortunately, there does not exist an efficient, general purpose optimizer which gives practitioners the flexibility to quickly prototype novel MKL formulations or try out existing ones on large scale applications. As things stand, the practitioner faces the significant challenge of having to come up with a differ-ent optimization technique for every novel MKL formulation that she might wish to explore.

This problem was somewhat alleviated by the develop-ment of the Generalized Multiple Kernel Learning (GMKL) formulation [28]. The formulation allows learning fairly gen-eral kernel parameterizations, including linear and non-linear kernel combinations, subject to general regularization. The only restriction that is placed on the kernel and regularizer is that their derivative with respect to the kernel param-eters must exist and be jointly continuous with the SVM dual variables  X  . Most of the kernel learning formulations proposed in the literature satisfy these constraints.
However, the general purpose, projected gradient descent based GMKL optimizer is not efficient and doesn X  X  scale well. In particular, it has two significant shortcomings. First, the gradient descent step size is calculated via the Armijo rule to guarantee convergence. This necessitates the solving of many expensive, single kernel SVMs on the entire data set in order to take a single step. Second, the objective function value and the gradient descent direction are also obtained by solving an SVM. To avoid noise in these estimates the SVM Quadratic program (QP) has to be solved to a very high precision. Unfortunately, high precision solvers don X  X  scale beyond toy problems, while large scale specialized SVM QP solvers based on chunking and decomposition [6] typically produce low precision results. Optimizing GMKL via pro-jected gradient descent can thus be significantly slower than solving a single SVM.

Our objective, in this paper, is to speed up GMKL op-timization by an order of magnitude in many cases. We achieve this by carefully designing an alternative optimizer based on Spectral Projected Gradient (SPG) descent [5]. SPG is a well known technique in the optimization commu-nity. Our contribution is to adapt it for the GMKL problem. Towards this end, we use a non-monotone line search crite-rion which allows the objective function to increase once in a while rather than strictly decrease at each step. This allows us to accept initial step size proposals based on the spectral step length, in many cases even the first, whereas gradient descent would need to try out many step sizes before accept-ing one that satisfies the Armijo rule. SPG steps are thus much quicker as they require fewer objective function and SVM evaluations.

We also engineer SPG to be more robust to the objec-tive function and gradient noise encountered in large scale GMKL problems. Gradient descent will often get stuck due to inaccurate gradient computation. Only miniscule step sizes will satisfy the Armijo rule since the gradient is point-ing in the wrong direction and gradient descent will either time out or take many small steps before recovering. SPG, on the other hand, can recover much faster due to the non-monotone line search criterion. We further exploit this prop-erty by deliberately computing objective function values and gradients at low precision when we are far away from the op-timum. This allows our initial steps to be much quicker than gradient descent X  X . As the optimum is approached, we auto-matically increase the precision of the objective function and gradient computation according to a schedule based on the duality gap and the projected gradient. In many cases, due to this effective scheduling, we need to solve SVMs with a precision of only 10  X  1 at the beginning and at most 10  X  3 wards the end. On the other hand, we found that projected gradient descent needed to solve SVMs with a precision of 10  X  6 from the very start in order to converge.

We carried out extensive experiments comparing the per-formance of our proposed SPG optimizer to projected gra-dient descent . We observed that SPG was much faster on even small data sets, optimizing GMKL in a matter of seconds, while gradient descent often struggled to converge. Furthermore, SPG scaled well to large data sets where gradi-ent descent based optimization was infeasible. For instance, without using any parallelization, we were able to train on the Sonar data set with a million kernels and on Covertype with over half a million data points. Note that Covertype is one of the largest data sets on which a single kernel SVM can be trained without parallelization and that our train-ing time was much the same as that of a single SVM with a kernel defined to be the uniformly weighted sum of the given base kernels. In contrast, sequential MKL techniques proposed in the literature have mostly reported results on thousands of kernels or data points (with the exception of SMO-MKL [31]). Finally, in some cases, SPG even outper-formed state-of-the-art specialized optimizers designed for a specific MKL formulation. Thus, not only can SPG han-dle diverse MKL formulations it is also the most efficient optimizer for many of them.

We have implemented SPG in C and use LibSVM [6] as the inner SVM solver. Our implementation builds on top of the LibSVM code base. Our code is easy to modify and new kernel parameterizations and regularizers can readily be plugged in. We therefore expect the code to be useful for trying out new MKL formulations as well as optimizing existing ones. Our objective is to provide a flexible and efficient tool for kernel learning and the SPG-GMKL code can be downloaded from [17].
Research in Multiple Kernel Learning has focused on both developing new MKL formulations as well as their opti-mization. Different formulations are required to address the needs of different applications. Early work focused on learn-ing the kernel as a linear combination of given base kernels subject to l 1 or l 2 regularization [12, 19]. This was later extended to handle any l p&gt; 1 regularizer [18], mixed block regularizers [1] as well as radius-margin based regulariza-tion [14]. Non-linear kernel combinations [11, 28], such as products of kernels and mixtures of polynomials, have also been shown to be appropriate in certain domains. Other formulations include multi-class MKL [34], learning over ex-ponentially large kernel combinations [2], learning a different kernel combination per data point [15,20] and MKL discrim-inant and subspace analysis [9,32]. Many of these formula-tions can be easily cast in the GMKL framework.

As regards optimization, specialized algorithms have been developed for many of these formulations. For instance, lin-ear MKL subject to l 1 regularization has been optimized via semi-definite programming [19], M.-Y. regularization [3], semi-infinite linear programming [26] and mirror descent [1]. Sparse MKL models have also been learnt by direct, greedy minimization of the l 0 norm [25]. Linear MKL with l p regu-larization has been optimized using the SMO algorithm [31], stochastic gradient descent [21] and semi-infinite linear pro-gramming [18]. Stochastic gradient descent has also been used for optimizing multi-class MKL [21,22] and second or-der cone programming [27] for learning hyper-kernels.
Unfortunately, most of these specialized optimization tech-niques are limited to their own formulation and do not gen-eralize well. Gradient descent, on the other hand, can be used to optimize many formulations. It has been used to train linear MKL with l 1 regularization [8,23,29], multi-class MKL with shared parameters [23], for learning local ker-nel combinations per data point [15], for hierarchical kernel learning over exponentially large kernel combinations [2] and for learning non-linear kernel combinations [28]. All of these formulations, even if they are not covered under GMKL, can benefit by replacing their gradient descent based optimizer by our proposed SPG algorithm.
We review the GMKL formulation in this section and dis-cuss how it can be optimized via gradient descent. We focus on the binary classification problem though other convex loss functions can be substituted as desired.

Our objective is to learn a function of the form f ( x )= w t  X  d ( x )+ b with the kernel k d ( x i , x j )=  X  t d ( x resenting the dot product in feature space  X  parameterized by d . The goal in training an SVM is to learn the globally optimal values of w and b from training data { ( x i ,y i addition, MKL also estimates the kernel parameters d .The primal formulation of GMKL is where T ( d )=Min where both the regularizer r andthekernelcanbeany general differentiable functions of d with continuous deriva-tive and l could be one of various loss functions such as l = C max(0 , 1  X  y i f ( x i )) for classification. The constraint d  X  0 can be strengthened or relaxed depending on the cost of projecting back into the feasible set so as to ensure that the parameterized kernel remains positive definite. Allowing the regularizer, kernel and loss function to have such gen-eral forms permits GMKL the flexibility to directly model, or be easily extended to, most of the formulations discussed in Section 2. For instance, the kernel can be learnt to be a linear combination of given base kernels, product of kernels or mixture of polynomials, etc . Similarly, the regularizer can be set to the 1-norm, any p&gt; 1-norm, a mixed block norm, the log determinant of the learnt kernel matrix, etc .
The GMKL primal was deliberately formulated as a nested optimization. The key idea is that once the kernel parame-ters d are fixed, an efficient, single kernel SVM solver, such as LibSVM, can be used to solve the inner optimization in T while T itself can be optimized via gradient descent.
In order to optimize T using gradient descent, we need to prove that  X  d T exists and calculate it efficiently. This is best achieved by looking at T  X  X  dual given by where K d is the kernel matrix for a given d and Y is a diagonal matrix with the labels on the diagonal.
Note that we can write T = r + P and W = r + D with strong duality holding between P and D . Therefore, T ( d )= W ( d ) for any given value of d , and it is sufficient for us to show that W is differentiable and calculate  X  d Proof of the differentiability of W comes from Danskin X  X  Theorem [13]. Since the feasible set is compact, the gradi-ent can be shown to exist if k , r ,  X  d k and  X  d r are smoothly varying functions of d and if  X   X  ,thevalueof  X  that opti-mizes W , is unique. Furthermore, a straight forward exten-sion of Lemma 2 in [8] can be used to show that W have derivatives given by where H = YKY . Thus, in order to take a gradient step, all we need to do is obtain  X   X  . Note that since W is equivalent to the single kernel SVM dual with kernel matrix K d ,  X   X  can be obtained by any SVM optimization package. The projection operator in our case is P ( d )=max( 0 , d ).
To guarantee convergence, the step size s n is chosen ac-cording to the Armijo rule so as to satisfy where g is the chosen direction of descent, the negative gra-dient  X   X  W ( d )inourcase,and  X  is a tolerance parameter.
Spectral gradient descent iteratively approximates the ob-jective function with a quadratic model and then optimizes the model at each iteration. It is particularly well suited to large scale problems since it builds a coarse approximation very efficiently and without any memory overhead. Early work by Barzilai and Borwein [4] laid the foundation for choosing the step size as the spectral step length. Grippo X  X  classic non-monotone line search criterion for Newton meth-ods [16] was incorporated into the algorithm by Raydan in [24]. The technique was extended to handle constrained optimization problems in [5] who recommended descending according to the spectral projected gradient rather than the regular gradient so as to reduce the number of projection operations. We refer to this algorithm as Spectral Projected Gradient (SPG) descent and efficiently adapt it for optimiz-ing GMKL based on gradient precision tuning. In addition, we replace the classic non-monotone line search criterion by the one proposed in [33] based on averaging. Proofs of con-vergence for each of the methods can be found in their re-spective publications. A few attempts have been made to use SPG for optimizing machine learning problems [10]. How-ever, SPG appears not to be a widely used technique in machine learning or data mining.
 Our SPG-GMKL pseudo code is given in Algorithm 1. The three main components are the spectral step length and the spectral projected gradient computation, the non-monotone line search and the SVM solver precision tuning. Algorithm 1 SPG-GMKL n  X  0
Initialize d 0 randomly repeat until converged We discuss these components in detail and also mention our convergence criterion and kernel caching strategy. Spectral step length and projected gradient.

We describe how to move from the current iterate d n to the next iterate d n +1 . The step size and the descent direc-tion are chosen so as to reduce the number of SVM eval-uations and projection operations as compared to regular projected gradient descent.

SPG-GMKL chooses it X  X  step size based on second order information. At each iteration, it approximates the GMKL objective function W ( d ) by a quadratic in d . The step size is then based on the eigenvalues of the approximated Hes-sian. A particularly simple choice of the Hessian,  X   X  1 I ,is made so as to be able to efficiently scale to large problems. Imposing the constraint that the directional derivatives of the objective and the approximated functions match at the current point leads to the following choice for  X  If  X  n is negative then it is set to  X  Max , else it is clipped between  X  Min =10  X  30 and  X  Max = 10 so as to ensure that the steps are always bounded.

Standard methods would have taken a projected gradi-entstepofsize  X  n . However, in SPG, we instead step in the direction of the negative spectral projected gradient. In particular, the next iterate is obtained as where s n is selected from { 1 , 1 / 2 , 1 / 4 ,... } according to the non-monotone line search criterion, p is the spectral pro-jected gradient and P is the projection operator in our case P ( d )=max( d , 0 ). This choice of descent direction ensures that the projection operator needs to be applied only once per iteration. While the cost of projection is negligible in our case it might become significant if the constraints d are replaced by more complicated ones.
 Non-monotone line search criterion.

We employ a non-monotone line search criterion so as to reduce the number of SVM evaluations and be robust to the objective function and gradient noise inherent in GMKL optimization. The classical SPG non-monotone line search criterion selects step sizes s n that satisfy
W ( d n  X  s n p n )  X  max So as to allow the objective function to increase once in a while. However, [33] observed that, for unconstrained quasi-Newton methods, maximizing over the previous function val-ues could be unstable in some cases and that averaging could be a much better alternative. We therefore incorporate their criterion into SPG-GMKL by choosing s n satisfying where we set C 0 = W ( d 0 ), Q 0 =1and  X  =10  X  4 .The parameter  X  influences the number of iterations over which W is averaged. Setting  X  = 0 reduces the criterion to the Armijo rule while setting  X  = 1 implies averaging W over all previous iterations. We follow the heuristic of [33] and vary  X  dynamically depending on the behavior of the objective function and the approximated quadratic. If the approxi-mation is good (bad) then we increase (decrease)  X  by 0 . 025 making sure to clip it between [0 . 1 , 1] else leave it unchanged. Tuning the tolerance of the SVM solver.

The SPG-GMKL optimizer is robust to imprecisions in calculating the objective function and the gradient. We ex-ploit this by solving SVMs with a tolerance of only 10  X  1 the initial iterations. This is automatically decreased as we move closer to the optimum or the step size becomes too small. The SVM solver X  X  tolerance is set as = where u is the duality gap (whenever available) and v is the projected gradient norm.
 Convergence Criterion and Kernel Caching.

We declare the SPG-GMKL algorithm to have converged whenever the duality gap reduces below 10  X  3 or, if the du-ality gap is unavailable, whenever the l 2 or l  X  norm of the projected gradient reduces below 0 . 04. The reason the stop-ping criterion is so loose in terms of the projected gradient norm is because we do not assume that our inner SVM solver produces high quality solutions, and hence there is signifi-cant noise in the gradient estimates. In fact, the condition on the projected gradient seems stricter since, in many of the small data sets, the duality gap fell below 10  X  3 before the projected gradient norm reduced below 0 . 04. However, on some of the larger data sets, the projected gradient norm was found to become small even when the duality gap was large. We therefore use the duality gap as a stable stopping criterion whenever it is available (for sum of kernels in our experiments).

Kernel caching strategies can have a big impact on perfor-mance since kernel computations can dominate everything else in some cases. While a few different kernel caching tech-niques have been explored for SVMs, we stick to the stan-dard one used in LibSVM [6]. A Least Recently Used (LRU) cache is implemented as a circular queue. Each element in the queue is a pointer to a recently accessed (common) row of each of the individual kernel matrices.
 Advantages over projected gradient descent.

Our SPG-GMKL implementation has a number of advan-tages over regular projected gradient descent. First, we are able to efficiently bring second order information into play by choosing our step size according to the spectral step length. This was found to be very beneficial for many data sets. Second, we can significantly reduce the number of SVM evaluations by combining the spectral step length compu-tation with a non-monotone line search criterion. While gradient descent might take ten or more SVM evaluations to find a step size satisfying the Armijo rule, SPG-GMKL will frequently take only a single SVM evaluation as the first step size proposal is accepted. The non-monotone line search might also have beneficial side effects. Non-convex kernel parameterizations might benefit as such criteria have been shown to sometimes help escape poor local optima. Furthermore, non-monotonicity can also be helpful in cases when the objective function starts resembling a long, nar-row valley where standard gradient descent techniques can get stuck. Third, our SPG-GMKL implementation is robust to noise in the calculation of the gradient and the objective function. Gradient descent can require hundreds of SVM evaluations in a line search when the gradient is not point-ing in the right direction. SPG-GMKL is typically more robust and can recover much faster. Fourth, by starting with a low SVM precision and then dynamically increasing it as required, we can make the cost of evaluating each SVM much lower. This was found to give an order of magnitude improvement over gradient descent on some large data sets where the cost of evaluating an SVM is very high.
We carry out extensive experiments to test the perfor-mance of SPG-GMKL along various dimensions. We bench-mark its performance on small, medium and large scale data sets. We show that SPG-GMKL consistently outperforms projected gradient descent by a wide margin and that SPG-GMKL can often be more than ten times faster. We also demonstrate that SPG-GMKL can scale well to large prob-lems involving a million kernels on Sonar or over half a mil-lion data points on Covertype. This is particularly signifi-cant as Covertype is one of the largest data sets on which an RBF SVM can be trained without parallelization.
We also optimize four different MKL formulations to demon-strate the flexibility of SPG-GMKL. We learn two different types of kernel combinations  X  sum of kernels with K = k d k K k and product of kernels with K = k K k ( d k )= while combining distance functions D k . We regularize these with the l 1 and l p&gt; 1 norms. We focus on the linear MKL formulations in Subsections 5.1 and 5.2. Specialized optimiz-Data sets # Train # Dim #Kernels Sum Product Leukemia 38 7129 7129 Wpbc 194 34 455 34 Sonar 208 60 793 60 Liver 345 6 91 6 Ionosphere 351 34 455 34 Breast-Cancer 683 10 143 10 Australian 690 14 195 14 Diabetes 768 8 117 8 Letter 20000 16 16 RCV1 20242 47236 50 Adult-8 22696 123 50 42 Web-7 24692 300 50 43 Poker 25010 10 10 Adult-9 32561 123 50 42 Web-8 49749 300 50 43 KDDCup04-Physics 50000 71 50 Cod-RNA 59535 8 50 8 Real-Sim 72309 20958 25
Covertype 581012 54 5 ers have been developed for learning a sum of base kernels subject to l 1 regularization as it is one of the most popular formulations. We show that the performance of general pur-pose SPG-GMKL can be much better than state-of-the-art implementations of some of these specialized optimizers. We then use SPG-GMKL to learn products of kernels in Subsec-tions 5.3 and 5.4 which cannot be optimized using any other solver (apart from projected gradient descent, of course). We show that SPG-GMKL scales well to many problems where projected gradient descent fails to converge.
Finally, in Subsection 5.5, we study the scaling properties of SPG-GMKL and the contribution of each of the individual components. On the data sets that we tried, SPG-GMKL exhibited a linear dependence on the number of kernels and a sub-quadratic dependence on the number of training points. We also show that each of the three major components of SPG-GMKL, namely the spectral step length, the non-monotone line search and the SVM solver precision tuning, are necessary and that removing any of them can lead to substantial degradations in performance.

We briefly describe the various implementations that we tested. We refer to our algorithm as SPG-GMKL and the projected gradient descent algorithm as PGD. The SPG-GMKL implementation is obtained by starting from the PGD codebase and adding spectral step length computation, the non-monotone line search criterion and SVM solver tol-erance tuning. This ensures an absolutely fair comparison between the two implementations. Another projected gradi-ent descent implementation is available in the form of Sim-pleMKL [23]. The code was downloaded from the authors X  website. However, this implementation is meant only for lin-ear MKL subject to l 1 regularization and hence we compare to it only in Subsection 5.2. For this formulation, a very efficient implementation of the SILP-MKL [26] optimizer is available in the Shogun toolbox. We downloaded the latest version of Shogun (v 0.9.3) and used the highly optimized CPLEX solver to solve their outer loop LP. Note that the linear MKL formulations can be made convex with a simple change of variables. As such, SPG-GMKL and PGD con-verge to the same solution. SimpleMKL and SILP-MKL can also be made to converge to the same solution with an ap-propriate choice of parameters. We therefore report only the training time in our experiments as the objective function value, number of kernels selected and prediction accuracy on the test set are similar for all the methods. SPG-GMKL and PGD might potentially converge to different solutions for the non-convex formulations involving products of ker-nels. We did not observe this behavior though, and both implementations returned very similar solutions. Table 1 summarizes the statistics of the data sets that were tried. For linear MKL, kernels were generated as recommended in [23] for the small data sets. We generated RBF kernels with ten bandwidths for each individual dimension of the feature vector as well as the full feature vector itself. Sim-ilarly, we also generated polynomial kernels of degrees 1, 2 and 3. Kernel matrices were pre-computed and normalized to have unit trace. A fixed number of kernels were computed on the fly for the medium and large data sets. All kernels were computed on the fly for products of kernels where we defined one RBF kernel per feature for the small data sets and a fixed number for the large data sets. We perform 5 fold cross validation on the small data sets. This gives both the mean training time and its variance so as to better judge 329 . 76 . 9  X  1 . 223 . 5  X  2 . 36 . 0  X  0 . 2 2 . 50 . 7  X  0 . 10 . 7  X  0 . 10 . 6  X  0 . 1 39 . 84 . 2  X  0 . 47 . 7  X  1 . 13 . 1  X  0 . 3 2 . 43 . 3  X  0 . 16 . 6  X  0 . 62 . 7  X  0 . 1 2 . 43 . 0  X  0 . 13 . 6  X  0 . 12 . 5  X  0 . 1 1 . 81 . 3  X  0 . 51 . 7  X  0 . 21 . 2  X  0 . 1 the significance of our results. The SVM parameter C was chosen by validation so as to maximize the prediction accu-racy. All experiments were carried out on a single core of a 2.1 Ghz AMD 6172 processor with 48 Gb RAM. The results on Covertype were obtained on a similar machine with only 14 Gb RAM.
We learn a linear combination of base kernels subject to l p&gt; 1 regularization. This formulation has yielded state-of-the-art results [22] for combining multiple, heterogeneous features for recognizing objects on the challenging Caltech image data sets. We focus on comparing SPG-GMKL to PGD in this Subsection. Table 2 summarizes the results on the small data sets. For small values of p ,theoptimiza-tion problem is hard, and SPG-GMKL can be two orders of magnitude faster than PGD. As the value of p increases, the optimization problem becomes much simpler, and the two techniques become more comparable though SPG-GMKL canstillbetwiceasfastasPGD.

Results on the larger data sets are given in Table 3. PGD is again much slower and fails to converge on KDDCup04-Physics with fifty thousand points, Covertype with nearly six hundred thousand points and Sonar with a million ker-nels. SPG-GMKL, without any parallelization, converged on Covertype with 26 SVM evaluations in 64 hours which is reasonable given that solving just the first SVM, equivalent to solving a single kernel SVM obtained by linearly summing the five base kernels with uniform weights, took 44 hours. Note that we were caching only 0 . 19% of the support vec-tors and thus the training time can be considerably speeded up by utilizing a larger kernel cache. Similar trends were observed for Sonar with a million kernels.

Figure 1 plots the decrease in objective value versus time on a log-log scale. SPG-GMKL X  X  initial steps are much quicker than PGD X  X  due to the combination of the spectral step length, the non-monotone line search and the coarse SVM precision. SPG-GMKL therefore gets close, and con-verges, to the optimum much faster than PGD. The non-monotone behavior of SPG-GMKL is prominent on both the Web and Sonar20K datasets. Note that, on Web, the objective function seems to dip below the global minimum. This is due to noise in the objective function because of the coarse SVM precision. PGD would have gotten stuck here as it insists on a strict decrease in the objective function at each iteration. SPG-GMKL can recover due to its non-monotone criterion and SVM precision tuning. This demonstrates that SPG-GMKL is more robust to noise than PGD.
One of the most popular MKL formulations is to learn a sum of given base kernels subject to l 1 regularization on the combination coefficients. The joint winner of the highly competitive PASCAL VOC object detection challenge [30] employed such a formulation to learn a sparse combina-tion of features for efficient object detection in images. A number of optimizers have been developed for this formu-lation. The state-of-the-art is defined by SILP-MKL and SimpleMKL. Table 4 shows that SPG-MKL can not only outperform PGD on this formulation, it is also much bet-ter than SILP-MKL and SimpleMKL. For instance, SPG-GMKL is always more than twice as fast as SILP-MKL and can sometimes be orders of magnitude faster as the number of kernels is increased. This demonstrates that our gen-eral purpose SPG-GMKL solver can be more efficient than state-of-the-art optimizers designed for a specific formula-tion. SPG-GMKL is also an order of magnitude faster than SimpleMKL and PGD which are two different implementa-tions of the projected gradient descent algorithm. Hessian-MKL [7] provides a second order method for optimizing this formulation. While we found HessianMKL to be an improve-ment over SimpleMKL on small data sets, it X  X  performance was still much worse than that of SPG-GMKL.
We demonstrate the flexibility of SPG-GMKL by using it to learn products of kernels subject to l p&gt; 1 regularization. Note that this formulation presents a challenging non-convex optimization problem. Tables 5 and 8 detail performance on small and large data sets respectively. PGD failed to con-verge in many cases and, when it did, it was often ten to a hundred times slower than SPG-GMKL. We found that the main reason for PGD not converging was that its step size was approaching zero due to inaccurate gradient com-putation coupled with the Armijo rule. As a result, PGD required many SVM evaluations to take a miniscule step and timed out after a considerable period. On the other hand, 7 . 738 . 2  X  17 . 66 . 2  X  4 . 2 1 . 257 . 9  X  85 . 15 . 1  X  0 . 6 621 . 629 . 5  X  7 . 110 . 1  X  0 . 8 18 . 8 1392 . 4  X  824 . 239 . 2  X  6 . 8 65 . 0  X  273 . 4  X  64 . 0 167.9 0.6  X  0.2 0.3  X  0.1 0.4  X  0.1 691.5 3.3  X  0.8 377.8  X  712.0 3.2  X  0.5 718.3 2.5  X  0.5 2.1  X  0.4 2.1  X  0.1 41.2 3.7  X  1.4 209.3  X  390.8 2.1  X  0.4 SPG-GMKL generally required only a single SVM evalua-tion per iteration and converged quickly to the optimum.
We finally use SPG-GMKL to learn products of kernels subject to l 1 regularization. This formulation was observed to be very good for feature selection [28] as it was signifi-cantly better than leading filter and wrapper methods. Ta-bles 9 and 10 compare the performance of SPG-GMKL and PGD on small and large data sets respectively. The trends, and explanations, are much the same as for l p regularized product of kernels but SPG-GMKL X  X  gains over PGD are even bigger. For instance. On many of the small data sets, SPG-GMKL was a thousand times faster than PGD.
We now investigate how SPG-GMKL scales with the num-ber of kernels and data points. Figure 2 plots the results for linear MKL with various values of p on a log-log scale. The number of training points is varied on the Adult and Web data sets while the number of kernels is varied on Sonar. We estimate, from the plots, that SPG-GMKL, on aver-age, scales linearly with the number of kernels on Sonar and as n 1 . 98 and n 1 . 52 with the number of training points on Adult and Web. The scaling factors are very similar for PGD though the constants involved are much bigger.
We also determine the contribution of each of the three major components of SPG-GMKL, namely the spectral step length, the non-monotone line search and the SVM precision tuning. In Table 7, we start with the PGD implementa-tion as the baseline. We then turn on either the spectral step length computation (PGD+S), or the non-monotone line search (PGD+N), or both (PGD+N+S) and assess the impact on both training time and the number of SVM evalu-ations. As can be seen, adding either component in isolation does not lead to the best gains and, in fact, can sometimes degrade performance below the PGD baseline. It is only when both the components are combined that one consis-tently gets improvements in performance.

The full impact of tuning the SVM precision is best seen on larger data sets where the cost of evaluating an SVM is Table 11: Effects of individual components on large data sets significant. Table 11 shows that adding spectral step length computation and the non-monotone line search criterion to the PGD baseline cuts down its training time to at least half on Web and by more than fifteen times on Sonar. SPG-GMKL adds SVM precision tuning to the combination to further reduce the training time by a factor of two. Thus, each of the three major components that we proposed must be present in order for SPG-GMKL to be an efficient solver that scales well to large problems.
In this paper, we developed an efficient, spectral projected gradient descent based optimizer for the Generalized Mul-tiple Kernel Learning framework. Starting from the pro-jected gradient descent algorithm, we added three compo-nents based on the spectral step length, the non-monotone line search criterion and SVM precision tuning. We demon-strated that each of the components is necessary to the suc-cess of the SPG-GMKL optimizer.

We carried out extensive experiments benchmarking the performance of SPG-GMKL. We showed that SPG-GMKL routinely gives order of magnitude improvements over PGD and scales well to problems where PGD does not converge. For instance, SPG-GMKL was able to train on problems with a million kernels or half a million data points. This redefines the state-of-the-art in kernel learning optimiza-tion. Furthermore, SPG-GMKL, even though it is a gen-eral purpose solver, was able to outperform leading special-ized solvers developed for a specific formulation. As such, SPG-GMKL is one of the most efficient techniques for kernel learning.

We tested the flexibility of SPG-GMKL by having it op-timize four different MKL formulations. Our code is easy to modify and new formulations can readily be plugged in. Our objective is to provide an efficient tool for rapidly prototyp-ing new MKL formulations and trying out existing ones on real world applications.
 We are grateful to Kamal Gupta and to the Computer Ser-vicesCenteratIITDelhi. [1] J. Aflalo, A. Ben-Tal, C. Bhattacharyya, J. S. Nath, [2] F. R. Bach. Exploring large feature spaces with [3] F. R. Bach, G. R. G. Lanckriet, and M. I. Jordan. [4] J. Barzilai and J. Borwein. Two-point step size [5] E. G. Birgin, J. M. Martinez, and M. Raydan. [6] C.-C. Chang and C.-J. Lin. LIBSVM: a library for [7] O. Chapelle and A. Rakotomamonjy. Second order [8] O. Chapelle, V. Vapnik, O. Bousquet, and [9] J. Chen, S. Ji, B. Ceran, Q. Li, M. Wu, and J. Ye. [10] D. Cores, R. Escalante, M. Gonzalez-Lima, and [11] C. Cortes, M. Mohri, and A. Rostamizadeh. Learning [12] N. Cristianini, J. Shawe-Taylor, A. Elisseeff, and [13] J. M. Danskin. The Theorey of Max-Min and its [14] K. Gai, G. Chen, and C. Zhang. Learning kernels with [15] M. Gonen and E. Alpaydin. Localized multiple kernel [16] L. Grippo, F. Lampariello, and S. Lucidi. A [17] A. Jain, S. V. N. Vishwanathan, and M. Varma, 2012. [18] M. Kloft, U. Brefeld, S. Sonnenburg, P. Laskov, K.-R. [19] G. R. G. Lanckriet, N. Cristianini, P. Bartlett, L. El [20] C. S. Ong, A. J. Smola, and R. C. Williamson. [21] F. Orabona and L. Jie. Ultra-fast optimization [22] F. Orabona, L. Jie, and B. Caputo. Online-batch [23] A. Rakotomamonjy, F. Bach, Y. Grandvalet, and [24] M. Raydan. The barzilai and borwein gradient method [25] V. Sindhwani and A. C. Lozano. Non-parametric [26] S. Sonnenburg, G. Raetsch, C. Schaefer, and [27] I. W. Tsang and J. T. Kwok. Efficient hyperkernel [28] M. Varma and B. R. Babu. More generality in efficient [29] M. Varma and D. Ray. Learning the discriminative [30] A. Vedaldi, V. Gulshan, M. Varma, and A. Zisserman. [31] S. V. N. Vishwanathan, Z. Sun, [32] J. Ye, , S. Ji, and J. Chen. Multi-class discriminant [33] H. Zhang and W. W. Hager. A nonmonotone line [34] A. Zien and C. S. Ong. Multiclass multiple kernel
