 We propose a new ranking method, which combines rec-ommender systems with information search tools for better search and browsing. Our method uses a collaborative filter-ing algorithm to generate personal item authorities for each user and combines them with item proximities for better ranking. To demonstrate our approach, we build a prototype movie search and browsing engine called MAD6 ( M ovies, A ctors and D irectors; 6 degrees of separation). We conduct offline and online tests of our ranking algorithm. For offline testing, we use Yahoo! Search queries that resulted in a click on a Yahoo! Movies or Internet Movie Database (IMDB) movie URL. Our online test involved 44 Yahoo! employees providing subjective assessments of results quality. In both tests, our ranking methods show significantly better recall and quality than IMDB search and Yahoo! Movies current search.
 H.3.5 [ Information Systems ]: On-line Information Ser-vices; H.3.4 [ Information Systems ]: Systems and Soft-ware; H.3.3 [ Information Systems ]: Information Search and Retrieval Experimentation, Measurement, Performance collaborative filtering, search ranking, recommender systems, information retrieval, movie search
Two types of technologies are widely used to overcome in-formation overload: information retrieval and recommender systems . Information retrieval systems (e.g., general web Copyright 2007 ACM 978-1-59593-609-7/07/0008 ... $ 5.00. search engines such as Google 1 and Yahoo! Search 2 ) accept a query from a user and return the user relevant items against the query. Since the number of returned documents can run into the millions, a good ranking algorithm, which ensures high precision in the top ranked documents, is important for the success of an information retrieval system.

In general, the ranking of returned documents in web search engines is the combination of the document prox-imity and authority. Document proximity, sometimes called document relevance, denotes the document X  X  similarity or relevance to the given query. Document authority denotes the importance of a document in the given document set. PageRank [19] measures global importance of documents on the Web while HITS [16] measures local authorities and hubs in the base-set documents extracted by the given query. However, even though item authority and proximity are widely used together in general search engines for better document ranking, item authority is often ignored or par-tially used in many specialized search systems, for example product search in an ecommerce site. For example, search results may be sorted based only on item relevance against the given query.

There are several challenges for adapting item authority in these information retrieval systems due to the different characteristics of documents like item or product informa-tion documents in commercial sites, as compared to web documents. The power of PageRank and HITS stems from the feature of links between web documents. PageRank and HITS assume that a link from document i to j represents a recommendation or endorsement of document j by the owner of document i . However, in item information pages in commercial sites, links often represent different kinds of relationships other than recommendation. For example, two items may be linked because both items are produced by the same company. Also, since these item information pages are generally created by providers rather than users or cus-tomers, the documents may contain the providers X  perspec-tive on the items rather than those of users or customers.
On the other hand, recommender systems are widely used in ecommerce sites to overcome information overload. Note that information retrieval systems work somewhat passively while recommender systems look for the need of a user more actively. Information retrieval systems list relevant items at higher ranks only if a user asks for it (e.g. when a user sub-mits a query). However, recommender systems predict the need of a user based on his historical activities and recom-http://www.google.com http://search.yahoo.com mend items that he may like to consume even though the user does not specifically request it.

In this study, we propose a new approach to combine informational retrieval and recommender system for bet-ter search and browsing. More specifically, we propose to use collaborative filtering algorithms to calculate personal-ized item authorities in search. This approach has several benefits. First, user ratings or behavior information (e.g., user click logs) better represent user X  X  recommendation than links in the item information pages. Second, this informa-tion is biased to the customers X  perspectives on items rather than those of providers. Third, many ecommerce sites pro-vide users both information retrieval and recommender sys-tems. Calculating item authorities using these already ex-isting recommender systems in ecommerce sites does not require much additional work and resources. Fourth, using both item authorities and proximities, search results can be improved. Last, since collaborative filtering algorithms pro-vide personalized item authorities, the system can provide a better personalized user experience.

To demonstrate our approach, we build a prototype per-sonalized movie search engine called MAD6. The name is an acronym for M ovies, A ctors, and D irectors with 6 degrees of separation. 3 MAD6 combines both information retrieval and collaborative filtering techniques for better search and navigation. MAD6 is different from general web search en-gines since it exploits users X  ratings on items rather than the link structures for generating item authorities. More-over, using the users X  historical preference data and expected preferences on items, MAD6 provides a personalized search ranking for each user.

To evaluate our ranking algorithms, we conduct offline and online tests. We use search click logs from Yahoo! Search for the offline test. We extract queries that resulted in a user clicking on a movie page in either the Internet Movie Database (IMDB) or Yahoo! Movies. We compare the per-formance of five search ranking systems, including IMDB search, Yahoo! Movies current search, and three of our own algorithms, according to hit rate and reciprocal ranking. We also conduct an online test involving 44 Yahoo! employees using our test demo. The users subjectively rate the recall and quality of six ranking systems. Our approach performs better than IMDB search and Yahoo! Movies current search in both the offline and online tests, according to both recall and quality of search results.
Recommender systems can be built in three ways: content-based filtering, collaborative filtering, and hybrid systems. Content-based recommender systems, sometimes called in-formation filtering systems, use behavioral user data for a single user in order to try to infer the types of item attributes that the user is interested in. Collaborative filtering com-pares one user X  X  behavior against a database of other users X  behaviors in order to identify items that like-minded users are interested in. Even though content-based recommender systems are efficient in filtering out unwanted information and generating recommendations for a user from massive 6 degrees of separation is a well-known phrase from so-ciology adapted more recently to the movie domain in the form of a  X  X arty game X  called  X  X ix degrees of Kevin Bacon X , where the goal is to identify as short a path as possible from a given actor to Kevin Bacon, following co-actor links. information, they find few if any coincidental discoveries. On the other hand, collaborative filtering systems enables serendipitous discoveries by using historical user data.
Collaborative filtering algorithms range from simple nearest-neighbor methods [5, 25, 27] to more complex machine learn-ing based methods such as graph based methods [1, 15], linear algebra based methods [4, 26, 10, 24, 7], and proba-bilistic methods [14, 22, 23, 8]. A few variations of filterbot-based algorithms [11, 21] and hybrid methods [23, 18, 3] that combine content and a collaborative filtering have also been proposed to attack the so-called cold-start problem. Tapestry [9] is one of the earliest recommender systems. In this system, each user records their opinions (annota-tions) of documents they read, and these annotations are accessed by others X  filters. GroupLens 4 [25], Ringo [28] and Video Recommender [30] are the earliest fully automatic recommender systems, which provide recommendations of news, music, and movies. PHOAKS (People Helping One Another Know Stuff) [29] crawls web messages and extracts recommendations from them rather than using users X  ex-plicit ratings. GroupLens has also developed a movie rec-ommender system called MovieLens 5 [11, 26, 27]. Fab [2] is the first hybrid recommender system, which use a combina-tion of content-based and collaborative filtering techniques for web recommendations. Tango [6] provides online news recommendations and Jester [10] provides recommendations of jokes.

Page et al. [19] and Kleinberg [16] first proposed a new concept of document relevance, often called document au-thority, and developed the PageRank and HITS algorithms, respectively, for better precision in web search. Both al-gorithms analyze the link structure of the Web to calcu-late document authorities. Haveliwala [12] proposed topic-sensitive PageRank , which generates multiple document au-thorities biased to each specific topic for better document ranking.

Note that our approach is different from general web search engines since we use user ratings rather than link structure for generating item authorities. Also, our approach is differ-ent from topic-sensitive PageRank since we provide person-alized item authorities for each user rather than topic-biased item authorities. Also, our approach is different from rec-ommender systems since it uses predictions of items as a ranking function for information search rather than gener-ating recommendation.
Note that we only focus on movie title search rather than people (actor or director) search in our study. Thus, the term  X  X tem X  is equivalent to  X  X ovie X  or  X  X ovie title X . Like general web search engines, our ranking algorithm consists of two main components: item proximity and item authority.
Most movie search engines index only titles or few key-words on items. Thus, item relevance for the given query against a database are often measured by relevances of ti-tles and keywords for the query. In other words, they are
GroupLens, http://www.grouplens.org/ MovieLens, http://movielens.umn.edu/ most useful when users already know what they are looking for. Search queries are assumed to be part of movie titles, or names of actors or directors. We define these type of queries as navigational queries.

However, when a user searches for something, in many cases he does not know much about the object, and that is one of main reasons why he searches for it. Sometimes, searching means trying to find unknown (or unfamiliar) in-formation, which may be interesting. Thus, search tools should anticipate that some queries will be ambiguous or inexact. Even for niche search engines, the situation is not changed. Imagine a scientific literature search. Even though a scientist is very familiar with her research field, sometimes she is searching for articles that she might have missed. In this case, we cannot expect that she already knows the titles of the articles she is looking for.

A fan of  X  X rnold Schwarzenegger X  may try to find a list of the actor X  X  movies with a query such as  X  X rnold action X , expecting to find movies such as The Terminator or Conan the Barbarian . We define these type of queries as infor-mational queries. However, the Internet Movie Database (IMDB) and Yahoo! Movies, for example, currently do not return these movies, since their titles do not contain any of the query words. Since both systems X  basic search supports title and name matching only, they suffer from poor recall when a user does not know the exact titles of the target movies. Another example of a poorly supported query type is for character names. Users may want to find The Lord of the Rings series with queries such as  X  X andalf X  or  X  X rodo X . IMDB does provide a character name search option in their advanced search, but only one name is allowed and gender information is required. Thus, the query  X  X eo trinity X  (look-ing for The Matrix ) is not supported. Yahoo! Movies does not support character name search at this time.

To address these limitations, we build our own database of more extensive metadata for better recall in search results. In addition to movie titles, we index a large amount of meta-data including genres, names of actors, directors, characters, plots, MPGA ratings, award information, reviews of critics and users, captions from trailers and clips, and so on. To measure item relevance for the given query, we use MySQL X  X  match-against function in each indexed field. The function returns matched items with relevance scores in each field and we calculate item relevances for the query by calculating the weighted sum of all fields. A few heuristics are used to bal-ance the weight of each field. For example, we give more weight on the title field, so that items with title matches will have higher relevance scores. Relevance scores of the returned documents are normalized such that the highest score in each search becomes 13. (In our system, relevance scores are integers from 1 to 13, corresponding to ratings grades F through A+.)
To show the possible increase in recall, we conducted a recall test comparing our extensive indexing system with IMDB and Yahoo! Movies current search. We downloaded movie data from IMDB 6 and generated queries for the 100 most popular movies, where popularity is measured by the number of user ratings. We use five movie metadata fields: names of actors, directors, characters, plots, and genres. The two highest TF/TFIDF words (except stopwords) of each of the top 100 popular movies are selected as a query http://www.imdb.com/interfaces Table 1: Hit ratios of three movie search engines for the top 100 most popular movies; Only the top 10 returned movies are considered.  X  X B X  denotes our base system with an extensive index. Two top TF/TFIDF terms from five metadata, includ-ing names of actors, directors, characters, plots, and genres, are selected as a query for each movie in the top 100 popular movies. Then each query is sub-mitted to three systems, IMDB, Yahoo! Movies and our base system. The popularities of movies are measured by the number of user ratings. We down-loaded the IMDB movie content data and conducted this test in April 2006.
 current Yahoo! 2 94 2 95 and only the top 10 returned movies are analyzed. We con-sider only movies which exist in our database.

Only a few queries succeed in extracting the target movie within top 10 highest position when IMDB and current Ya-hoo! Movies are used. All of the successful queries con-tain at least one title word. Table 1 shows the improve-ment of our base search engine with an extensive index. Note that queries are somewhat biased toward IMDB, since they were generated based on IMDB data. Our system suc-cessfully returned the target movie about 1/3 of the time, whereas IMDB and current Yahoo! Movies returned the tar-get movie less than 6% of the time. Note that IMDB con-ducts OR matching search and returns many matches in most cases. Yahoo! Movies conducts AND matching search and our system conducts AND matching search for items but OR matching search for the names. 7
The performance gain of our system is much smaller when tested with actual user queries on Yahoo! Movies instead of synthetic queries. This is because most queries on Ya-hoo! Movies current search are navigational: all or part of movie titles or actor names. However, it is impossible to know whether users inherently prefer navigational queries, or if users have learned through experience that informa-tional queries are not supported by today X  X  search systems at IMDB and Yahoo! Movies. We expect the frequency of informational queries to increase significantly when movie search engines start to support such queries. Though we em-ploy synthetic queries here, we report results of performance comparisons with real queries and real users in Sections 4 and 5.
We also use an additional concept of item relevance for the given query that we call web relevance , which takes into account the relevance as judged by a general purpose algo-rithmic search engine like Yahoo! or Google. We find that users often provide some extra information of items which
OR matching search returns any items that match at least one query term while AND matching search returns items that match all query terms.
 Table 2: The effect of web relevance.  X  X lo X  is sub-mitted to each system as a query. Bold represents items relevant to Jennifer Lopez. The results are as of April 2006.
 do not exist in our database. For example,  X  X lo X  X  X he nick-name of actress Jennifer Lopez X  X s often found in the users X  reviews of the movies she has starred. Moreover, the perfor-mance of general search engines are constantly improving, reflecting a huge number of person-hours of development. In fact, performing a Yahoo! or Google search restricted to a particular movie site (e.g., querying a search engine with  X  X rnold action site:movies.yahoo.com X ) often works better than using the site-specific search on the movie site itself. One of the advantages for this approach is that we can take advantage of any improvements made by general search en-gines without delay, without re-inventing all the tools and tricks that they use.

We use the Yahoo! Search API 8 for getting web informa-tion. Each time our system gets a query from a user, it conducts a site-limited search through the API and gets the top 50 results. Then, our system grabs the item ids from the document URLs and extracts information of corresponding items from our database. The web relevance score of each returned item is given based on its relative first position in the web search result. More specifically, if L ( i,q ) is the highest rank of an item i in the web search result for the query q , its web relevance score is given by the following equation.
 where N and  X  are the maximum number of returns from the search engine and a normalized factor. We set  X  = 13 http://developer.yahoo.com/search/web/
Note that the returned items can be duplicated. One may be a  X  X ast and Credit X  page and another may be a  X  X ovie Details X ,  X  X howtimes &amp; Tickets X , or  X  X railers &amp; Clips X  page of the same movie. such that the web relevance score of the top ranked item becomes 13. We set N = 50. We ignore all items that do not appear in our database. Table 2 shows the effect of Web relevance for the query  X  X lo X . Our web relevance system returns eight items and five of them are relevant to Jennifer Lopez. IMDB does not return any relevant titles in its top 10 search results.
The item proximity score of a returned document is cal-culated as: where DB ( i,q ) and Web(i,q) denote DB and Web relevances of an item i for the given query q . We tested several heuristic weighting schemes such as averaging two relevance scores or selecting worst relevance score as the proximity scores and found that this heuristic method seems to be the best among them. Note that the definition of a good ranking is very sub-jective and we do not have clear metric for this matter. Thus we always depend on the judgment of the first author when parameters are chosen. In other words, our implemented ranking system is very biased to the taste of the first user. We do not claim that our ranking is the best performing in-stance among all possible implementations. Our goal of this study is showing that collaborative filtering can be useful for ranking when PageRank can not be used for the item authority.
We first generate global item authorities. The global item authorities can be generated based on the items X  average ratings over all users. However, we add some heuristics for calculating global item authorities, which emphasize both the popularity and quality of items. Note that the quality of items do not always match with the need of users. For ex-ample, even though some old movies have very good quality, most users may not look for those 40 X  X  or 50 X  X  movies since they prefer recently produced movies. In fact, only 57 users in our database have rated Citizen Kane (1941) . Thus, we calculate global item authorities using the following equa-tion:
Auth i = r i + log  X  where U i is the set of users who have rated item i , r the average rating of item i over all users, c i is the average critic rating of item i , aw i is the number of awards that item i has won, an i is the number of awards that item i has been nominated for, and  X  is a normalization factor such that the maximum global item authority is 13. Also, we set  X  such that the maximum value of log  X  | U i | is 13. We use award scores and average critic ratings on items for assigning better authorities to the classic movies than the movies of which users have frequently rated but their average ratings are low.
We use an item-based collaborative filtering (CF) algo-rithm [27] to calculate a user X  X  expected ratings on the re-turned items. We have tested several collaborative filter-ing algorithms including user-based CF and a few machine Table 3: Weak &amp; strong generalization: The aver-age normalized mean absolute errors and standard deviations on three sample sets are shown. Smaller numbers (lower errors) are better.
 learning algorithms and found that item-based was the best performing collaborative filtering algorithm among them over three movie data sets including Yahoo! Movies, MovieLens, and EachMovie. Table 3 shows the normalized mean ab-solute error of item-based CF and maximum margin ma-trix factorization (MMMF), the best algorithm among ten tested machine learning approaches in [17, 24], according to the weak and strong generalization tests with the MovieLens and the EachMovie data. The results of MMMF are copied from [24]. More motivations and details of the weak and strong validation test can be found in [17, 24].

Item-based CF first calculates item similarities using ad-justed cosine similarity : sim ( i,j ) = where r u,i is the rating of user u for item i and r u is user u  X  X  average item rating. It helps to penalize similarity scores that are based on the small number of common users in order to reflect less confidence, yielding a modified similarity score sim X (i,j) as follows [13]: where U i denotes a set of users who have rated the item i . We set  X  = 50. We use user rating information from Yahoo! Movies 10 to calculate item similarities. The prediction of the target item for the user is given by the sum of the average rating of the target item and the weighted average of its neighbors: where r i and I u denote the average rating of the item i over all users and a set of items the user u has rated.
We assign item authorities for each search result based on the following procedure. We assign global item authorities as item authorities when the target user is unknown. When a user logs in our system, we partition returned items in each search result into two groups: items which the user has rated and others that the user has not rated. We assign the user X  X  own ratings as item authorities for the first group and the user X  X  expected ratings calculated by item-based
User ratings of movies consist of a small sample generated by Yahoo! Movies on November 2003. The data contains 211,327 ratings, 7,642 users and 11,915 items. All users rate at least 10 movies. algorithm for the second group. If we cannot calculate the user X  X  expected ratings for any items in the second group due to lack of information, global item authorities are assigned for those items. Then the ranking score of document i for the given query q and user u is: MADRank ( i,q,u ) =  X   X  Auth ( i,q,u ) + (1  X   X  )  X  Prox ( i,q ) where  X  is an weighting factor for item authorities. We set  X  = 0 . 5. In addition, we set the MADRank score to 13 if the title of an item exactly matches to the given query.
Table 4 shows the top 10 title search results of six movie search systems, including the current Yahoo! Movies search, IMDB search, and four of our own search systems, for the query  X  X rnold action X . DB denotes one variant of our sys-tems with an extensive index and DB relevance based rank-ing. Web denotes a system using the Yahoo! Search API and web relevance ranking. GRank denotes a system us-ing MADRank as a ranking system and item authorities are based on global item authorities. PRank denotes a sys-tem with MADRank and personal item authorities. Table 5 shows the profile of the test user used in the PRank.
Note that the current Yahoo! Movies search does not re-turn any titles due to AND matching within a limited in-dex. IMDB does not return any Arnold Schwarzenneger movies in the top ten results. In the DB system, Arnold Schwarzenneger DVD 2-Pack -The Sixth Day/The Last Ac-tion Hero(2003) is shown first since the title contains both  X  X rnold X  and  X  X ction X . However, the results still show the need of better ranking for informational search since many famous titles such as Terminator (1984) and Terminator 2: Judgment Day (1991) do not appear in the first search results. The result of Web seems to be better than that of DB. Note that several famous Schwarzenneger movies in-cluding Terminator (1984) and Terminator 3 (2003) appear in the top 10 results. However, Arnold Schwarzenneger DVD 2-Pack (2003) is still shown first. Several items including Terminator (1984) and Terminator 2 (1991) are boosted in the GRank due to their higher item authorities while Arnold Schwarzenneger DVD 2-Pack (2003) disappears from the top 10 due to its low global item authority. In PRank, Terminator (1984) , Terminator 2 (1991) and Total Recall (1990) are boosted further since either the user has rated the items higher or his expected ratings for those items are high. Similarly, Terminator 3 (2003) , Eraser (1996) and End of Days (1999) disappear due to their low personal-ized item authorities. By applying item authorities in the ranking function, we believe that search results can be sig-nificantly improved.
We measure the effectiveness of our ranking algorithms by comparing against two existing systems X  X MDB search and the current Yahoo! Movies search X  X n both online and offline tests. In this section, we discuss our offline test. We use search click data from Yahoo! Search containing search queries and clicked URLs. We select data stored on the first day of each month from October 2005 to May 2006. We extract all queries that resulted in a click on a movie page in either IMDB or Yahoo! Movies. Specifi-cally, we extract queries containing clicked-URLs starting with imdb.com/title or http://movies.yahoo.com/shop?d=hv . We use some heuristics to extract additional movie pages Table 4: Top 10 results of different ranking methods for the query  X  X rnold action X . The results are as of April 2006.

Ranking Top 10 movie results current No items return
Yahoo!
IMDB 5. Armed for Action (1992)
DB 4. Last Action Hero (1993)
Web 5. Eraser (1996)
GRank 5. End of Days (1999)
PRank 5. The Terminator (1984) Air Force One (1997) (F) Commando (1985) (C+) Hulk (2003) (C-) Lord of the Rings: The Fellowship of the Ring (2001) (A) Lord of the Rings: The Return of the King (2003) (A) Matrix (1999) (A+) Raiders of the Lost Ark (1981) (A) Return of the Jedi (1983) (B-) Saving Private Ryan (1998) (A) Shawshank Redemption (1994) (A+) Star Wars (1977) (A+) Terminator (1984) (A)
Terminator 2: Judgment Day (1991) (A+) and remove cast and crew pages. We extract 110 K  X  170 K queries for each day.

We choose 500 randomly selected instances from each day for a total of 4,000 query-URL pairs. Next, we extract movie ids from the URLs. If a URL is a Yahoo! Movies page, we extract the yahoo movie id from the URL and find the movie title in our database. If a URL is an IMDB page, we submit the URL to Yahoo! Search and find the title, then find the matching Yahoo! movie id. If this fails, we use some additional heuristics. For example, if we cannot find a corresponding movie by title match (i.e. Bom yeoreum gaeul gyeoul geurigo bom (2003) in IMDB), we submit a query such as  X  X ite:movies.yahoo.com Bom yeoreum gaeul gyeoul geurigo bom (2003) X  to Yahoo! Search and select the first returned movie as its counterpart in Yahoo! Movies (e.g. Spring, Summer, Fall, Winter... and Spring (2004) in Yahoo! Movies). Then, the first author manually inspected the match list and removed a few incorrect instances. After removing all instances without either Yahoo! or IMDB IDs, we are left with 2,179 query-movieID pairs.

For the purpose of the test, we assume that the clicked movie URL is the user X  X  desired target movie for that query. We test five ranking systems, including IMDB search, Ya-hoo! Movies current search, and three of the methods de-scribed in the Section 3.3. We cannot test the personal-ized algorithm PRank, because we do not have access to the movie ratings of users in our search click log data set.
Our database includes movie meta data and user rating information from Yahoo! Movies, including 21M user rat-ings, 3.3M users and 150K movies. This data is also used for the current version of MAD6 implementation and online test discussed in Section 5.

We use hit rate ( HR ) and average reciprocal hit rank ( ARHR ) [8] as performance metrics for the offline tests. Hit rate measures how many times a system returns the target movie using following equation: where N is a set of test instances and H is a set of hit instances within the top 10 results.

Note that our test data contains, for each query, a list of potentially several movies clicked by the same user. When a user clicks on several movies from the same query, we count a hit if the system returns any of the visited movies. Then average reciprocal hit rank is measured by the follow-ing equation: where r i is the actual rank of the target movie i . When a user visits several movies with the same query, we only consider the highest ranked item. We only consider the top 10 results with this metric. Note that hit rate is used to capture recall of search results while average reciprocal hit rank is used to measure the quality of search results. The results are shown in Table 6. In general, we find that Web and GRank perform better than the others. GRank performs best on hit rate while Web performs best on ARHR. Even though DB shows a much better hit rate than Yahoo!, DB performs worst on ARHR. Yahoo! performs worst in terms of hit rate due to its AND matching search. IMDB performs better than Yahoo! and DB, but worse than Web and GRank in general.
 One may be somewhat surprised by the high hit rate of IMDB and Web. This happens since most queries are nav-igational queries. In other words, most queries are either movie titles, part of movie titles, or movie titles with ex-tra terms such as the word  X  X ovie X . We believe this hap-pens because most users understand from experience that informational queries are not supported by existing search facilities, and thus mainly submit only supported (i.e., nav-igational) queries. Also note that our offline test is biased to Web ranking, since we use Yahoo! Search click data. If a target movie is not found by a search engine, a user refines his/her query and submits it again to the search engine. In our test, the first failure would not be captured, and queries that tend to work well in web search (i.e., that result in clicks on movie pages) are tested. Also our test may be somewhat biased to IMDB, since most URLs in our test data come from IMDB. In our test data, 1,297 unique URLs point to IMDB pages while 231 unique URLs point to pages in Yahoo! Movies.
We also conduct an online evaluation using MAD6 [20], our prototype personalized movie search and browsing en-gine. In this section, we first briefly explain MAD6, then discuss our online test procedure and results.
The architecture of MAD6 is shown in Figure 1. It has four internal components (User Interface (UI) Module, Database, Web Analyzer and Ranker) and two external components (Search Engine and Collaborative Filtering (CF) Module). Note that the two external components are modular and can be exchanged with other systems. The User Interface (UI) Module gets a query from a user and presents the user the search results from the Ranker. When the UI Module ob-tains a query from a user, it passes the query to the Web Analyzer and Database. The Web analyzer extracts the web search result from the associated search engine and gener-ates web relevance scores of the returned items. Then, this information is submitted to the Database. The Database ex-tracts items relevant to the given query and generates their DB relevance scores. It also extract all information of items extracted by Web Analyzer or Database itself. The informa-tion contains item contents, global item authorities and DB and Web relevances. Then, this information is submitted to the Ranker, which requests the CF Module expected ratings of items for the given user. Note that CF Module has its own user rating database. Then, items are sorted based on the ranking scheme the user has requested. MAD6 provides users three information features; Search , Item Presentation , and Personal User Profile pages. In Search pages, MAD6 presents two search results for a given query: movie search results and people search results. Search ranking can be personalized if a user logs in the sys-tem. The user can choose to rank results according to global MADRank, personalized MADRank, Web relevance, DB rel-evance, or item authorities. Each returned item shows rat-ings based of four methods (MADRank, Web relevance, DB relevance and item authorities) and matched fields against the given query. An example search result is shown in Figure 2.

There are two types of Item Presentation pages: movies pages and people (actor and director) pages. Each page shows details of the item (title, synopsis, cast, release date, ratings, posters, etc.) along with two lists of relevant items, one showing neighboring items in the collaboration graph of actors and directors, the other based on similarities inferred from user preferences.

The Personal User Profile page presents the user personal information such as: (1) What queries has the user submit-ted most frequently? (2) What movies, actors and directors has the user visited most frequently, either directly or indi-rectly? 11 (3) What are the user X  X  favorite genres? (4) What movies are most recommended for the user? Park et al. [20] describe and illustrate MAD6 in greater detail.
By an indirect visit we mean visiting a movie or person that links to the movie or person in question via the movie graph.

Our online test consists of 44 Yahoo! employees submit-ting 180 relevance judgments between October 25 2006 and November 7 2006. The test is conducted with our test demo shown in Figure 3.

Each participant is asked to rate at least 10 movies us-ing MAD6 before they participate in the test. When a user comes to our test demo, the user is asked to either submit any free-form query, or to select one of 60 suggested queries provided by our system. The 60 queries are randomly se-lected from about 5,000 pre-selected queries, including the 500 most popular movie titles, 1,000 actors and 1,000 char-acters (the top 2 actors/roles from each of the top 500 most popular movies) and 2,400 randomly selected Yahoo! Search queries that resulted in a click on a movie link (300 randomly selected queries from each day in our data set).

After a participant submits a query, the test demo re-turns ranked results of six systems: IMDB search, Yahoo! Movies current search, and our four algorithms. The demo does not tell users which result is generated by which sys-tem, and the location of each system is randomly selected whenever a query is submitted. After reviewing the six dif-ferent search results, participants are asked to answer fol-lowing three questions: (1) Which system or systems finds the movie most relevant to your query? (2) Overall, which system or systems seems to be the most useful for you? and (3) What movie is most relevant to your query? A partici-pant may select multiple systems for the first two questions. The first question is asked to measure recall of search re-sults and the second question is asked to measure quality of search results. The third question requires a free form text answer.
In general, we find that Web, GRank and PRank per-form better than IMDB, current Yahoo! Movies, and DB, as shown in Table 7. PRank performs best on recall while GRank performs best on quality of search results when all 180 relevance judgments are analyzed. We also classify queries by comparing them with the user X  X  stated target movie (ques-tion 3). If a query exactly matches with the title of the tar-get movie, we consider it a navigational query. Otherwise, we consider it an informational query. This classification is manually done by the first author. We classify 49 queries as navigational queries.

A feature of IMDB search hampers our ability to eval-uate IMBD results for navigational queries. For exact ti-tle matches, IMDB often (but not always) returns the in-ferred target movie page directly, rather than listing one or more movies on a search results page. Our wrapper did not properly handle this case and the online test showed empty results for IMDB when this happened. Thus, we exclude IMDB when results of navigational queries are evaluated. Note that we did make the proper correction for the offline test in the previous section.

When navigational queries are submitted, PRank per-forms best while DB performs worst both on recall and quality of search results. It is somewhat surprising that the recall of most systems remains about 50  X  60% even when the titles of the target movie is submitted as a query. We find that many participants often check only one system in the first question even though all six systems return the target movie, probably because participants either did not inspect results carefully enough or misinterpreted the intent of question 1. Thus, we want to point out that our recall analysis on the online test may contain some noise. When informational queries are submitted, PRank shows the best recall while GRank performs best on quality.

It is interesting that even though there is not much dif-ference between informational and navigational queries on recall, we find a big difference on quality of search results. When navigational queries are submitted, participants are more satisfied with PRank and Web than GRank. How-ever, when informational queries are submitted, participants prefer GRank rather than PRank and Web. One possible explanation is that, when participants submit navigational queries, they may have very clear target movies in their minds. These movies may be their favorites and are more likely rated before the test. In this case, the authority of the target movies are very accurate since they are real ratings provided by the participants. However, when informational queries are submitted, participants may not have clear tar-get movies and returned items may have less probabilities to be rated. Also, users may search for somewhat unfamiliar items. For example, a user may search for some good roman-tic comedy movies for dating, even though he has mostly watched and rated action movies. Then item authorities calculated by the item-based algorithm may be inaccurate due to the lack of user information.

Note that only 6 participants rated 20 or more movies and most participants rated fewer than 15 movies. So our online test environment may be considered a cold-start user setting where all test users are relatively new users. This cold-start environment may reduce the quality of PRank X  X  personalized search results, since the item-based algorithm suffers from a cold-start problem. We believe that users X  satisfaction of PRank will increase as users provide more ratings. To support this argument, we also measure recall and quality of search results based on different participant groups. We find that participants who have rated at least 20 movies are more satisfied with PRank while participants with less than 20 ratings prefer GRank. It is also interesting that the quality of Web results are almost always similar to that of PRank.
We plan to incorporate some of the search and brows-ing features of MAD6 in Yahoo! Movies and related Yahoo! properties. We plan to develop a pseudo natural language query interface ( X  X hortcuts on steroids X ) for supporting sim-ple question and answering. For example, we would like to be able to handle queries like:  X  X ho won the best actor Oscar in 1995? X , or  X  X ighly rated comedy starring Arnold Schwarzenegger X . Moreover we would like to answer some personalized questions such as  X  X ecommend me an action movie from 2005 X  or  X  X ho is my favorite 90s actress? X . We plan to use MAD6 as a online research platform for testing various search, browsing, personalization, and recommen-dation algorithms and interfaces. We also plan to utilize our hybrid recommendation algorithm [21] to provide bet-ter cold-start predictions and recommendations.
In this paper, we discuss our new ranking method, which combines recommender systems and search tools for bet-ter informational search and browsing. To evaluate our ap-proach, we have built MAD6, a personalized movie search engine with some unique features. In both offline and online tests, MAD6 seems to provide users better search recall and quality than IMDB search and Yahoo! Movies current search by combining proximities and authorities of the returned Table 7: Online experiment results. The  X  X ecall X  column lists the percentages of users who selected the corresponding system in answering question 1.
 The  X  X uality X  column lists the percentages for ques-tion 2.
 20 ratings (152 feedbacks DB 38.2 21.1 items. Even though MAD6 is one application in the movie domain, we believe that our approach is general enough to apply other domains including music, travel, shopping and web search.
We thank Yahoo! Movies for providing movie content and user rating information. We thank Dennis DeCoste, Gunes Erkan, Rosie Jones, Bernard Mangold, and Omid Madani. We thank the Yahoo! employees who participated in our online test.
