 The goal of relation extraction is to identify the semantic relations of entities in a document. For example, consider the following example sentence.
 In this sentence, a successful result of relation extraction must indicate that there is a semantic relationship regarding birthplace between Barack Obama and Honolulu .
Many supervised machine learning approaches were successfully applied to relation extraction tasks [Bunescu and Mooney 2005; Culotta and Sorensen 2004; Kambhatla 2004; Zelenko et al. 2003; Zhang et al. 2006; Zhou et al. 2005]. Most approaches dealt with relation extraction as a classification problem and primarily focused on learning more effective classifiers through provided training examples.

However, applications of these approaches are still limited, because researchers as-sume that there are sufficient training examples to achieve good extraction results. Several datasets that provide manual annotations of semantic relationships are avail-able for the Automatic Content Extraction project [Doddington et al. 2004] or the Mes-sage Understanding Conference [Grishman and Sundheim 1996], but these datasets contain labeled training examples in only a few major languages, including English, Chinese, and Arabic. Although these datasets encourage the development and evalua-tion of statistical relation extractors for these major languages, there are few labeled training samples for learning new systems in other languages, such as Korean.
Because manual annotation of semantic relations for such resource-poor languages is very expensive, we instead consider a weakly-supervised learning technique to learn the relation extractor without significant annotation efforts. Weakly-supervised ap-proaches which have been applied for relation extraction can be classified into two categories. The approaches belonging to the first category aim to achieve better per-formance with fewer seed examples. To this end, several pattern induction-based [Agichtein and Gravano 2000; Brin 1999; Riloff and Jones 1999] and graph-based [Chen et al. 2006; Zhou et al. 2009] weakly-supervised techniques were sought for relation extraction and yielded meaningful results. However, these techniques still face cost problems when preparing quality seed examples, which plays a crucial role in obtaining good extractions; indeed, it could be worse to apply these techniques to a language for which this approach has never been attempted.

Conversely, the other subset of prior work attempted to appropriately use exter-nal resources not specially constructed for relation extraction instead of using task-specific training or seed examples. They demonstrated that external resources, such as treebank [Banko et al. 2007], which is built for other natural language processing tasks, and Wikipedia [Wu and Weld 2010], which is a general encyclopedia constructed by the collective intelligence, can help build competitive relation extraction systems. Moreover, these approaches benefit from the external resources X  availability in more languages than the task-specific resources.

In this article, we propose to leverage parallel corpora as a new external resource for weakly-supervised relation extraction. To obtain training examples in the resource-poor target language, this method exploits parallel corpora by projecting the annota-tions generated by a relation extraction system in a resource-rich source language. We call our primary hypothesis regarding the use of parallel corpora for learning the relation extraction system a cross-lingual annotation projection . Early studies in cross-lingual annotation projection were accomplished for various natural language process-ing tasks [Fu et al. 2011; Hwa et al. 2005; Merlo et al. 2002; Pado and Lapata 2009; Yarowsky and Ngai 2001; Yarowsky et al. 2001; Zitouni and Florian 2008]. However, to the best of our knowledge, no work has reported on the relation extraction task.
Using our proposed approach, the foundations for successfully building a relation extraction system for a new language are presenting sufficient parallel texts between the source and target languages and having an available quality relation extraction system for the source language. We assume the availability of these pre-required resources for our approach, because a large number of parallel corpora in various language pairs were constructed and distributed to develop statistical machine trans-lation systems, and several monolingual relation extraction systems were developed especially for English.

A simple direct projection method propagates the relations in source language sentences to word-aligned target sentences, and a target relation detector can boot-strap from these projected annotations. However, this automatic annotation can be unreliable because of source text misclassification and word alignment errors; thus, it can cause a critical falling-off in annotation projection quality. To alleviate this prob-lem, we introduce noise reduction strategies, including alignment refinement and in-stance filtering into the direct projection approach. Furthermore, we also present a novel graph-based projection approach considering both instance and context informa-tion for relation extraction.

Using our cross-lingual annotation projection approaches, we developed a Korean relation extraction system. We used an English-Korean parallel corpus to project the results of an English relation extraction system onto training examples for the target Korean system. Experiments demonstrate that our cross-lingual annotation projection approaches are beneficial in building a relation extraction system for a new language and outperform other systems based on monolingual resources in the target language.
The remainder of this article is structured as follows. We outline a cross-lingual an-notation projection approach for relation extraction in Section 2, describe details of two projection approaches, direct projection in Section 3 and graph-based projection in Section 4, provide the implementation details for the developed Korean relation extrac-tion system based on our proposed approaches in Section 5, report system evaluation results in Section 6, introduce related work in Section 7, and conclude this article in Section 8. including words or chunks, relation extraction can be considered as a classification problem. where e k is the k th entity in S .

An imbalance problem in resources for relation extraction causes the performance gap between extractors f s and f t for a resource-rich source language L poor target language L t , respectively.

Our annotation projection approach intends to learn the f without significant efforts in building the resources for L tomatically creates a set of annotated text for f t , utilizing a well-made extractor f a parallel corpus of L s and L t . The parallel corpus consists of bi-sentence pairs which are sententially aligned between their translational counterparts. The annotation pro-jection for each bi-sentence S s , S t in the parallel corpus can be performed, as shown in Figure 1. (1) Annotation . Given an input sentence S s , a set of extracted relations O (2) Projection . The annotations O t for the sentence S t We will describe the details of both phases. The first step in projecting annotations from L s onto L t the sentences in L s , as follows. (1) A set of entities e 1 s ,  X  X  X  , e N s in the given sentence S (2) Each instance is composed of a pair of entities e i s From the annotation step, we can obtain a set of extracted results formularized as As an example of annotation projection for relation extraction with a bi-text in the L t Korean and L s English (Figure 2), the annotation of an English sentence shows that the entities Barack Obama and Honolulu have a semantic relationship. To use annotation projection from the sentences in L s onto the sentences in L utilize alignments for linguistic unit (word or chunk) pairs sharing the translational semantics obtained by an aligner: where u s  X  S s and u t  X  S t .

Using the aligner A , the annotations in the target language sentence S from annotations in the source language sentence S s as follows. (1) As in the annotation phase, each instance is composed of a pair of base noun (2) For each instance e i t , e j t , its translational instance e (3) The existence of semantic relationship in e i t , e j
In Figure 2, an instance e 1 K , e 3 K = beo-rak-o-ba-ma, ho-nol-rul-ru in the Korean sentence is aligned with the instance e 1 E , e 2 E = Barack Obama, Honolulu in the English sentence. Because e 1 E , e 2 E is annotated as a positive instance in the annotation phase, e 1 K , e 3 K is considered a semantically related instance. The most important problem in projection is how to determine the counterpart instance in target language for a given instance in source language. As an intuitive ap-proach, we can just consider the alignments among entities organizing the instances to be projected. In the example in Figure 2, the annotation on Barack Obama, Honolulu is projected onto beo-rak-o-ba-ma, ho-nol-rul-ru , because  X  X arack Obama X  and  X  X onolulu X  are aligned to  X  X eo-rak-o-ba-ma X  and  X  X o-nol-rul-ru X  , respectively. We call this approach a direct projection . The result of direct projection can be formularized as Since a projection is performed independently for each single instance, it has O ( n ) time and space complexities, where n is the number of instances.

Our direct projection approach itself is likely to produce noisy outputs because of the technically imperfect submodules, including preprocessors, an extractor, and a word aligner. Thus, we must consider the way of reducing these erroneous projections which can be seriously affected by errors accumulated from intermediate submodules. To solve this problem, we introduce two alignment refinement strategies and an instance filtering strategy. The success of annotation projection highly depends on the quality of alignment; Therefore, the harmful effects of erroneous alignments should be minimized to ob-tain quality results. To reduce noises on word alignment, we propose two refinement strategies: chunk-based reorganization and dictionary-based correction. 3.1.1. Chunk-Based Alignment Reorganization. Fundamentally, we utilize word alignment information which is an important component in statistical machine translation techniques. The objective of the word alignment task is to identify translational rela-tionships among words in a bi-sentence and to produce a bipartite graph G with a set of edges E between words in S s and S t with a translational relationship. The simplest aligner A w , directly utilizing the results of word alignment, can be defined as where w s  X  S s ,and w t  X  S t . However, the results of automatic word alignment may include incorrect alignments because of technical difficulties.

In this work, we propose using alignments between pairs of base phrase chunks instead of pairs of words. For a given bi-sentence S s , S a corresponding language produces the base phrase chunk lists C C t = c 1 t ,  X  X  X  , c m t from S s and S t , respectively.

To identify the translational alignment between each pair of chunks c based reorganization of the word alignment is performed based on the overlap score o ( c s , c t ) . To compute the score, a pseudo chunk aligner is defined as which yields a set of aligned words from the original word-based result for all words in c . The Jaccard coefficient between c s and c t is then computed as which measures the ratio of words occurring in both the result of pseudo chunk aligner and the target chunk. The overlap score o c ( c s , c t ) is the arithmetical mean of the bidi-rectional Jaccard coefficients of J c ( c s , c t ) and J
In this work, a simple constraint that each chunk has only one edge connected to its counterpart to maximize the overlap score is considered to align between pairs of chunks. Following the constraint, the aligner based on the chunk-based reorganization is defined as
The key hypothesis of chunk-based reorganization is that the word alignment qual-ity can be improved by considering linguistic constraints in both languages. For example, the word alignments in Figure 3(a) have some errors, such as
Honolulu, ui , COMMA, neun ,and PERIOD, da . These erroneous word alignments are then removed by chunk-based reorganization (Figure 3(b)). 3.1.2. Dictionary-Based Alignment Correction. The stated chunk alignment strategy is an approach to reduce noisy alignments utilizing given bi-sentences themselves. To ob-tain further refined results of alignment, we propose an alignment correction method based on a selected bilingual dictionary for entity candidates as an external knowl-edge source. Each entry in the dictionary is a pair: an entity in L or transliteration in L t . We define D ( e s ) as a lookup function in the bilingual dictio-nary that yields the translational counterpart of e s in L with this bilingual dictionary are performed on the results of chunk-based reorganiza-tion rather than the original word-based alignments, in order to improve the coverage of the strategy by considering not only the equivalent word sequences to dictionary entries, but also the similar candidates to them in a chunk level. We first introduce a new Jaccard coefficient between a chunk pair c s and c t , defined as which numerically evaluates the overlap of words in the lookup result and the target chunk.

The overlap score o , which plays a crucial role in reducing noisy alignments, is also replaced by the linear combination of the original o c and the dictionary score o is the mean of bidirectional Jaccard coefficients J d , which is computed as where  X  is a real-valued parameter between 0 and 1, the degree of dictionary use in refining alignments.

After computing the overlap scores for all pairs of chunks in a given bi-sentence, the refined alignments are determined as which follows the same progression as A c .

We expect that dictionary, as external knowledge, helps the system handle erroneous alignments, which are not easily distinguishable with only internal resources. The other noise reduction strategy is to consider the reliabilities of the results of sub-modules for determining whether a projection should be filtered or not. This instance filtering strategy is based on the confidence score for each projected instance into our approach. Following the pipeline in our approach, we define two confidence scores:  X  for the annotation step and  X  p for the projection step.
  X  a is obtained based on the assumption that the extractor f score for each extracted result, formularized as where  X  f
The overlap scores o , defined in Section 3.1.1 and Section 3.1.2, are utilized for  X  The confidence score for a projection of an instance comprised of a pair of entities is computed by multiplying the o values of the entities and their aligned counterparts.
These two confidence scores are multiplied by each other to obtain the final confi-dence score, defined as Following the definitions of  X  a and  X  p values,  X  also returns a real value between 0and1.

Because the higher  X  value means that the corresponding projection is more reliable, the instances which have  X  values lower than a given threshold  X  are filtered out of the final annotation projection. Finally, we obtain the refined instance set: Although some noise reduction strategies were proposed, the direct projection ap-proach is still vulnerable to the erroneous inputs generated by submodules. This lim-itation is caused by the fact that each target instance is independently affected by a single aligned instance in the source language, and what X  X  worse is that it doesn X  X  consider any contextual information of the instances during projection.
To solve this limitation, we propose a graph-based projection approach for cross-lingual relation extraction. This approach utilizes a graph constructed with both in-stance and context information; and it is operated in an iterative manner to propagate the annotations among contextually related instances along the graph structure (Figure 4). By contrast to direct projection, all the related instances in graph-based projection are linked to each other through contextual units as intermediate nodes. Our hypothesis on graph-based projection is that this interconnected structure has the merits of improving the quality of projections with noisy features compared to direct approach. Even if an instance has totally incorrect features and they inevitably cause the noisy projection with direct approach, it can be rescued by iteratively propagated information from contextually similar instances which have correct projections. The most crucial factor in the success of graph-based learning approaches is how to construct a graph which is appropriate for the target task. For the purpose of mono-lingual weakly-supervised learning, most previous work [Chen et al. 2006; Zhou et al. 2009] used a graph which consists of the vertices denoting labeled and unlabeled exam-ples and the weighted edges with similarities between vertices. Das and Petrov [2011] successfully extended it to bilingual projection of part-of-speech tagging by considering the tagged words in source language as labeled examples and connecting them to the unlabeled words in target language referring to the word alignment.

Graph construction for projecting the semantic relationships is more complicated than part-of-speech tagging, because the unit instance of projection is a pair of entities, not a word or a morpheme, which is equivalent to the alignment unit. 4.1.1. Graph Vertices. To construct a graph for relation projection, we define two types of vertices: instance vertices V and context vertices U .

Instance vertices are defined for all pairs of entity candidates in source and target languages. Each instance vertex has a soft label vector Y abilities of the instance to be positive and negative, respectively. The larger the y value, the more likely the instance has a semantic relationship. The initial label val-ues of an instance vertex v ij s  X  V s for the instance e as follows: where f s is an extractor in source language and  X  f Since the sum of y + and y  X  is 1.0, the instances having larger y considered to be semantically related. For this, the ranges of  X  for positive instances are rescaled between 0.5 to 1.0 in Equation (19).
 As for the target language, every instance vertex v ij t  X  of 0.5 in both y + and y  X  , because they are considered as unlabeled examples in the beginning of graph-based learning algorithm.

The other type of vertices, context vertices, is used for identifying the relation de-scriptors which are contextual subtexts representing semantic relationships of the pos-itive instances. Since the characteristics of these descripting contexts vary depending on the language, context vertices should be defined to be language-specific.
In the case of English, the semantic relationship of an instance is likely to be rep-resented by the words between the entity pair and the surrounding words of two en-tities in the original sentence. Thus, we define the English context vertex for each trigram which is located from the second word before the left entity ARG 1 to the second word after the right entity ARG 2 , where both entities are semantically re-lated to each other. To concentrate on contextual clues rather than entities, each base noun phrase considered as an entity candidate is replaced with the corresponding tag among ARG 1 , ARG 2 for the entity pair of the instance, and NP for the other noun phrases. For example, the sentence in Figure 2 is converted into the sequence for the instance Barack Obama , Honolulu . From this sequence, we can generate following five context vertices: ARG 1 | was | born , was in | ARG 2 | , ,and ARG 2 | , | NP .

If the context vertices U s for source language sentences are defined, the units of con-text in target language can be also created based on word alignment. For each source language vertex u i s , the corresponding context vertex u organized with the aligned words with all three units in u guage vertex, the base noun phrases in u i t are replaced with NP tags. Among these, each one aligned with ARG 1 or ARG 2 in u i s are considered as an entity for a target language instance and labeled with the corresponding tag respectively. For example, the trigram ARG 1 | was | born of the previous examples in English is aligned to a set Figure 3(a).

While only contiguous sequences are considered for the source language units, some context vertices in target language can have non-contiguous sequences, because of the differences in word order between two languages. To encode the non-contiguous vertices, we introduce a wildcard character  X * X  that can be substituted for any other subsequences. Since these words are located non-contiguously in the original sen-tence, a wildcard should be inserted between  X  neun  X  X nd X  tae -eo -na  X , so the sequence ARG 1 | neun | X  X  tae -eo -na | at is generated for a context vertex in Korean.
If an argument in u i s is aligned with multiple noun phrases in u phrases are basically labeled with the corresponding argument tags. It means that a single context vertex can have more than one ARG 1 or more than one ARG 2 in the unit sequence. To avoid ambiguity of matching context sequences in target language, each context vertex which has multiple same argument labels is transformed into the non-ambiguous form by following policies: if the noun phrases labeled with the same argument tag are contiguous with each other, they are merged into a single argument; otherwise, the vertex is duplicated as the number of conflict arguments and only one argument candidate for each duplicated vertex is labeled with the corresponding tag.
Each context vertex u s  X  U s and u t  X  U t also has y + and y likely the context is to denote semantic relationships. The probability values for all context vertices in both languages are initially assigned to y have no labels for the context vertices before performing the projection. 4.1.2. Edge Weights. The graph for our graph-based projection is constructed by con-necting related vertex pairs by weighted edges. If a given pair of vertices is likely to have the same label, the edge connecting these vertices should have a large weight value.
 We define three types of edges according to combinations of connected vertices. The first type of edges consists of the connections between an instance vertex and a context vertex in the same language. We consider this connecting as a kind of n-gram pattern matching problem by regarding each context vertex as a pattern. For a pair of instance vertex v i , j and context vertex u k , these vertices are connected if the context sequence of v i , j contains u k as a subsequence. For example, the vertex for the instance Barack Obama , Honolulu in Figure 2 is connected to was to was | killed | in . The edge weight connecting v i , j
Another edge category is for the pairs of context vertices in a language. Since each context vertex is considered as an n-gram pattern in our work, the weight value for each edge of this type represents the pattern similarity between two context vertices. To compute the similarity between u k and u l , we use the Jaccard coefficient formula-rized as
While the previous two categories of edges are concerned with monolingual connec-tions, the other type is to deal with bilingual alignments of context vertices between source language and target language. It is relatively straightforward to determine whether a given pair of vertices is connected or not, because each context vertex in target language is directly generated from the source language vertices from the first. However, even a single vertex can be aligned to more than one counterparts over all the corpus. Thus, we define the weight for a bilingual edge connecting u relative frequency of alignments as follows. where count ( u s , u t ) is the number of alignments between u parallel corpus. As mentioned in Section 4.1.1, every context vertex in target lan-guage is generated from a source language context considering word alignments. Thus, w ( u k s , u l t ) denotes the ratio of u l t in the set of generated contexts from u corpus. The less diverse the generated contexts, the higher the edge weights. As shown in an example graph in Figure 4, each instance vertex in a source language English and a target language Korean is connected to its corresponding context ver-tices. While there are no connections between instance vertices, all the context vertices in the same language are connected each other. As for the bilingual edges of context vertices in English and Korean, there exist connections between aligned counterparts. Each context vertex can be connected to more than one instance vertices and more than one context vertices in the other language. To induce the labels of all unlabeled vertices on the graph constructed in Section 4.1, we utilize label propagation algorithm [Zhu and Ghahramani 2002], which is a graph-based semisupervised learning algorithm (Algorithm 1). This algorithm iteratively propagates the label distribution of each vertex to its adjacent vertices in a graph through the weighted edges. The larger the weight value on the edge connecting two vertices is, the more likely both vertices have similar labels to each other.
This procedure can be implemented by multiplying following two input matrices.  X  T is an n  X  n matrix representing transition probabilities for all vertex pairs. Each  X  Y 0 is an n  X  m matrix with initial label distributions. Each Y In this work, these matrices T and Y are initialized by the values described in Section 4.1.2 and Section 4.1.1, respectively. Since the target task aims to predict whether a given instance has a semantic relationship or not, each row Y of two values for both positive and negative probabilities. Thus, Y has the size of n After assigning all the values on both T and Y , we normalize the matrices for each row to make the element values as probabilities.

For these input matrices T and Y , label propagation is performed by multiplying these two matrices to update Y matrix. This multiplication is repeated until Y con-verges or the number of iteration exceeds a certain number. The Y matrix after finish-ing iterations are considered as the result of this algorithm.

This algorithm runs in O ( cn 2 ) time complexity and requires O ( n where n is the number of vertices and c is the number of iterations.

We utilize the preceding algorithm for three phases executed sequentially to project the semantic relationships from source language to target language. In the first step, the label propagation is performed for the vertices in source language. The soft labels on the context vertices are induced from the instance vertices in source language. Then, the bilingual projections are performed by applying the algorithm on the con-text vertices in both source and target languages. The results of the first phase are assigned to the initial labels at the beginning of this second phase. Finally, the algorithm yields the propagated labels of the instance vertices in target language from the context labels generated in the previous step.

While the fundamental goal of label propagation algorithm is to obtain the labels of unlabeled instances from seed instances, we use this algorithm to induce the target language labels from the source language annotations in this work. This approach can be also considered as a graph-based semisupervised method using the automatically generated labels on the source language instances as a seed set. As label propagation algorithm has outperformed, other non-graph-based approaches for semisupervised monolingual relation extraction [Chen et al. 2006; Zhou et al. 2009], we expect that it will also help to improve the performances of cross-lingual annotation projection by reducing the noises compared to direct projection. To demonstrate the effectiveness of our cross-lingual annotation projection approach for relation extraction, we developed a Korean relation extraction system (Figure 5) trained with projected annotations from English resources. The system consists of four parts: (a) annotation, (b) projection, (c) learning, and (d) extraction. To build a system based on our approach, we first obtained the source language sen-tences X  annotations in a given parallel corpus. We used an English-Korean parallel corpus 1 , which contains 266,892 bi-sentence pairs in English and Korean. The English sentences in the parallel corpus were preprocessed by the OpenNLP toolkit provides a set of analyzed results, including morpheme segmentations, part-of-speech tag sequences, and base phrase chunks for a given sentence.

For each preprocessed sentence, a set of positive instances with semantic relation-ships was identified by an off-the-shelf relation extraction system, ReVerb et al. 2011]. ReVerb extracts binary relationships from English sentences based on open information extraction techniques. The system outputs pairs of base noun phrases predicted as positive instances with their confidence scores. Of the 1,082,108 base noun phrase pairs in 266,892 English sentences, ReVerb annotated 155,409 instances as positive. The English sentences X  annotations in the parallel corpus were then propagated into the corresponding Korean sentences. For this projection, the alignments between linguistic unit pairs with translational relationships in a given bi-sentence were re-quired. Because we considered each morpheme in both language as a unit of align-ment, the Korean sentences in the parallel corpus were also preprocessed to obtain morpheme segmentations by the Espresso POS-K tagger. 4 To obtain the word align-ments for each bi-sentence in the parallel corpus, we used the GIZA++ software and Ney 2003] in its standard configuration in both the English-Korean and Korean-English directions. The bidirectional alignments were joined by the grow-diag-final algorithm [Koehn et al. 2003], which is widely used in bilingual phrase extraction for statistical machine translation.

Then, the noise reduction strategies described in Section 3.1 were utilized to re-fine the result of word alignment. Chunk-based reorganizations were performed be-tween each pairing of a base phrase and an eojeol in English and Korean sentences, respectively. An eojeol is a syntactic unit in Korean, which consists of one or more morphemes; and each eojeol is separated by space from the adjacent eojeols. For the dictionary-based alignment correction strategy, we constructed an English-Korean bilingual dictionary, including 304,212 entries. For each base noun phrase in English or noun eojoel in Korean sorted from the parallel corpus, we deduced its phrasal trans-lation by executing Google Translate API. 6
To project the annotations based on these post-processed alignments, we imple-mented two types of projection modules. The first module was based on direct pro-jection (Section 3). In this module, only the instances containing at least one proper noun argument were considered as candidates for projection. We obtained 110,807 in-stances from Korean sentences on the parallel corpus. From the distribution of entire projections, the instances likely to be unreliable were filtered out based on the thresh-old value  X  =  X  + k  X  , where  X  and  X  are the mean and standard deviation of confidence scores for all projected instances.

For the other projection module using graph-based approach (Section 4), we con-structed a bilingual graph described in Section 4.1 from the result of alignment and annotation for the parallel corpus. The graph consisted of 266,892 English in-stance vertices and 110,807 Korean instance vertices. Then, 14,159 context vertices in English were generated from the trigrams occurred more than five times in the contexts of positive instances. To reduce the number of non-contiguous context vertices in Korean, we utilized the refined alignments with chunk-based reorganization, rather than word alignment as it is. The number of Korean context vertices was 32,565. For this generated graph, we performed projection by the label propagation algorithm us-ing Junto toolkit. 7 We set the maximum number of iterations to 10 for each execution. Projected instances were utilized as training examples to learn the Korean relation extractor. We built a tree kernel-based support vector machine model using SVM-Light 8 [Joachims 1998] and Tree Kernel tools 9 [Moschitti 2006]. In our model, we adopted the subtree kernel method for shortest path dependency kernel [Bunescu and Mooney 2005]. The dependency analysis for Korean sentences was performed by Espresso-P. 10 We input the shortest dependency path between two arguments of each instance and its projected label to train the binary classifier. During the execution phase, each given sentence must be converted to the appropriate input form for the model trained in Section 5.3. We used the Espresso-P toolkit, which we also used in the learning phase as the preprocessor of our system for POS tagging and dependency analysis. From the preprocessed information, the shortest dependency path for each instance, here a pair of noun eojeols, was produced and entered into the model for prediction. The output of the system was set of noun eojeol pairs classified as positive by the model. To evaluate our proposed methods, we performed following experiments using the sys-tem described in Section 5. (1) Experiment 1 investigated the effectiveness of direct projection approach with (2) Experiment 2 evaluated the performance of graph-based projection approach and (3) Experiment 3 compared our approaches to the systems based on other resources (4) Experiment 4 sought the merits of our approaches compared to supervised base-All experiments were performed on manually annotated dataset built following Bunescu and Mooney X  X  [2007] approach. For each argument pair in Table I, the Ko-rean sentences containing the arguments were collected from a Web search engine. We then manually annotated the presence of a semantic relationship in each pair of arguments represented in a given sentence. The dataset consists of 600 sentences for four relation types. We divided this dataset into two sets: the test set and the develop-ment set. While the 500 sentences in the test set were used to evaluate and compare the performances of the systems, the other 100 sentences in the development set were utilized to assign the parameters for noise reduction strategies. The numbers of posi-tive instances in the test set and the development set are 278 and 58, respectively.
We evaluated the performance of each model in Precision, Recall, and F-measure to measure the ratio of agreement between these gold standard labels and the model X  X  predictions for the dataset X  X  sentences. The first experiment aimed to evaluate the models based on the direct projection ap-proach. We built five models, which are learned with direct projections, distinguished by the applied noise reduction strategies. (1) M w was built based on the word alignments processed by GIZA++ as they were. (2) M c utilized chunk-based reorganization described in Section 3.1.1. (3) M d originated from the corrected alignments using the dictionary-based strategy. (4) M f adopted the confidence-based instance filtering strategy. (5) M df utilized both strategies: dictionary-based alignment correction and confidence-The word alignment module used in M w achieved 65.1/41.6/50.8 in Precision/ Recall/ F-measure in our evaluation of 201 randomly sampled English-Korean bi-sentences with manually annotated alignments. After the chunk-based reorganization, the align-ment performance increased to 66.3/42.0/51.5 in Precision/Recall/F-measure for the same evaluation set; and these refined alignments were used to build M
In M d , the linear combination parameter  X  mentioned in Section 3.1.2 has a direct influence on the system outputs. To find the  X  value which maximizes the effectiveness of dictionary-based alignment correction, we evaluated the performances of M the development set with  X  values varied from 0.0 to 0.9 with increments in tenths. Table II shows the alignment and extraction performances with each  X  value. The similar aspects of changes in performance of both evaluations for the  X  values indi-cate that the quality of alignments were directly connected to the projection. When the  X  parameter was set to 0.6, the system achieved the highest performances in both alignment and extraction evaluations.
 The other parameter to be controlled is the threshold parameter k introduced in Section 5.2 for the confidence-based filtering. We evaluated the performances of M M df with five k values in value was used, the higher precision and the lower recall were achieved. The setting with k = X  0.2 achieved the best performances in F-measure for both M models on the development set.

Table IV compares the performances on the test set and the number of positive pro-jections of five models. The parameters  X  and k were set to 0.6 and which achieved the best performances on the development set. The results indicate the following.  X  While the word-based model M w achieved better recall, the chunk-based model M  X  The dictionary-based corrections to the chunk-based projections boosted both pre- X  The confidence-based instance filtering helped improve the F-measure performance  X  The model M df incorporating both alignment refinement and instance filtering In this experiment, we evaluated two models constructed by the graph-based approach. (1) M g was trained with the instances projected by graph-based algorithm. (2) M gd used corrected alignments by dictionary for graph-based projection.
Table V shows the performances of relation extraction of two models. Compared with the results of direct projection, these results indicate the following.  X  Both models M g and M gd achieved better performances in precision and recall than  X  Without dictionary-based alignment correction, M g outperformed M  X  The dictionary-based refinement strategy was also effective to improve the perfor- X  When the dictionary-based correction was enabled, the difference in performance
In the preceding experiment, we set the parameter  X  for M the best performance for direct projection. To identify the influence of  X  value in graph-based projection approach, we also performed the evaluation of M varied from 0.0 to 0.9. The extraction performances of M gd presented in Figure 6. We obtained the best performance of M of 0.6, which also maximized the performances of alignment and direct projection. However, the graph shows that M gd had less variation in performance than M the difference between the maximum and minimum performances of M F-measure, M gd had the value of 1.37 as the maximum difference. This suggests that graph-based projection may be less sensitive to the result of alignment than direct projection.

In this experiment, the average running time of graph-based projection was 1h 42m 03s on Linux with XEON 2.4GHz processor and 24.0GBytes of main memory; and the maximum size of allocated memory during projection was 16.9GB when we loaded the full graph on the memory. Although it requires much more time and memory compared to direct projection which has linear complexity in both time and memory, there is still much room for improvement in efficiencies of graph-based projection by distributed processing or simplified graph structure.

In addition to the quantitative results on the relation extraction in Korean as the final product, we also performed the qualitative analysis on the intermediate projec-tions obtained from each approach. First, 200 instances were randomly selected from the results of annotation on the English sentences in the parallel corpus. Then, the erroneous projections in Korean produced by each approach were manually sorted out and categorized according to the cause of error into following three categories. The first category is for the instances which have a wrong annotation even in English part; another one contains the errors caused by incorrect alignments; and the other covers the cases of false outputs of projection approach itself in spite of faultless annotations and alignments.

Figure 7 compares the error distributions with three projection approaches: direct projection without any refinement strategy, direct projection with our proposed refine-ment strategies, and graph-based projection. The comparison between the distribu-tions of two approaches based on direct projection shows the merits of our proposed refinement strategies with the decreased number of errors by 19.8% after applying the strategies. However, most of the corrected instances with refinements on direct projection belong to the category caused by the faults in alignments. On the other hand, the third distribution has established the effectiveness of graph-based projec-tion not only on the alignment errors, but also on the extraction errors. Overall, 43.2% of the extraction errors were rescued by graph-based projection as well as 44.9% of the alignment errors were also resolved. These differences result from the use of contex-tual information in graph-based projection compared to direct approaches. The goal of this experiment was to demonstrate the merits of our proposed approaches based on bilingual resources against other monolingual resources. Recently, several studies tried to utilize external resources to reduce annotation effort in relation ex-traction. As comparisons to our approach, we selected two types of resources which were reportedly effective for unsupervised relation extraction: (a) a treebank and (b) Wikipedia in Korean. 6.4.1. Resource 1: Treebank. The first baseline utilized the structural information en-coded in the treebank dataset to build a training dataset for relation extraction. Fol-lowing the work of Banko et al. [2007], we obtained a set of training examples from a Korean treebank corpus by automatically annotating each instance e the following structural semantic relationship constraints.  X  There exists a dependency chain between e i and e j .  X  The dependency chain between e i and e j contains at least one verb.  X  The parent of e i is not e j , and vice versa.  X  The path from e i to e j does not cross any sentence-like boundaries in the constituent  X  X ither e i or e j is the subject of the sentence or clause.

We obtained 194,170 instances from 77,121 sentences in the Sejong treebank cor-pus [Kim 2006], with 19,896 instances deemed positive for passing all constraints. Using these training examples, a baseline model was trained following the procedure described in Section 5.3. 6.4.2. Resource 2: Wikipedia. Wikipedia presents a large number of articles consisting of unstructured text and a structured infobox, which is a set of tuples summarizing the key attributes described in the Wikipedia article. The other baseline model for this experiment took advantage of Wikipedia following the self-supervision approach [Wu and Weld 2010], matching the primary entity and a set of attribute values in the infobox of each Wikipedia article. We first collected Korean Wikipedia articles processed them as follows. (1) Each Wikipedia article was divided into sentences by a sentence splitter using a (2) Only the sentences containing the primary entity, the entry of the article, were (3) In the selected sentences, each attribute value in infobox of the article was sought. (4) Each pair of the primary entity and an attribute value within a single sentence We obtained 46,305 instances with 19,340 positive matchings from 62,173 sentences. Training the model with this dataset also kept the same schemes as previous models. 6.4.3. Comparison Results. We also evaluated two models using treebank and Wikipedia following the settings described in Section 6.1. Table VI compares the per-formances of the two models and our proposed approaches for four relation types on the test set. It shows the following.  X  The model with treebank achieved poor performance due to extremely low recall,  X  The Wikipedia-based model achieved much performance levels than the treebank- X  Our proposed direct and graph-based projection approaches got better per-
The results show that our proposed approaches are competitively effective when building a relation extraction system in a new language without additional annotation efforts. Moreover, they also have the potential for great improvement if the required modules for relation extraction in source language sentences and word alignment of bi-sentences are further refined. The last experiment aimed to compare our proposed approaches to supervised tech-niques that have been widely studied for relation extraction. As mentioned earlier, the use of fully supervised approaches can be limited for most languages, including Korean, due to lack of manually annotated data. Even in our experimental settings, there are only 100 annotated instances in development set that are available for train-ing the models.

To evaluate the effectiveness of these limited amount of manual annotations, we built three baseline models with both supervised and semisupervised approaches. (1) The first baseline model was trained on the development set as it is. According to (2) Another baseline was based on a semisupervised approach by incorporating much (3) The last baseline made use of a graph-based semisupervised approach using la-
We evaluated these baseline models and showed the comparisons to our proposed methods (Table VII).  X  The model learned with fully-supervised manner achieved the lowest performances  X  The results of semi-supervised approaches indicate that unlabeled instances helped  X  Nevertheless, these baselines with manual annotations failed to show their merits
The results show that the baseline approaches with manual annotations are not effective when the amount of training instances is insufficient. Although the per-formances of these approaches can be improved by incorporating additional anno-tated data, collecting and annotating the instances manually is time consuming, labor intensive, and expensive. One the other hand, our proposed approach succeeded to demonstrate the comparative advantages in performance to the baseline approaches even without additional human efforts. Many supervised machine learning approaches successfully applied when solving tra-ditional relation extraction tasks have considered how to improve the model learned with a given training dataset by introducing more helpful features [Kambhatla 2004; Zhou et al. 2005] or developing more effective kernel methods [Bunescu and Mooney 2005; Culotta and Sorensen 2004; Zelenko et al. 2003; Zhang et al. 2006]. However, regardless of the incorporated detailed techniques, these approaches require a large number of training examples to achieve high performance levels.

To reduce the annotation cost, several weakly-supervised techniques have been pro-posed. The development of weakly-supervised relation extraction started from the use of context patterns iteratively induced from a small number of seed examples [Agichtein and Gravano 2000; Brin 1999; Riloff and Jones 1999]. This bootstrapping approach was also adapted to learn the statistical model for relation extraction by re-peating multiple iterations [Zhang 2004]. As an alternative weakly-supervised relation extraction approach to these bootstrapping methods, graph-based weakly-supervised machine learning techniques [Chen et al. 2006; Zhou et al. 2009] were introduced to solve the relation extraction tasks.

Currently, self-supervised approaches emerged as constructed relation extraction systems because they do not require a large number of training examples for fully supervised methods, and they require far fewer seed examples for traditional weakly-supervised methods. The key to their success was using the information obtained from external resources such as a treebank [Banko et al. 2007], Wikipedia [Wu and Weld 2010], and the Web [Bunescu and Mooney 2007]. The main difference between our work and the previous self-supervised approaches is that we used cross-lingual re-sources, while previous approaches only depended on monolingual resources.
Our proposed approach was designed using cross-lingual annotation projection tech-niques. Early studies of cross-lingual annotation projection considered lexically-based tasks, for example, part-of-speech tagging [Yarowsky and Ngai 2001; Yarowsky et al. 2001], and verb classification [Merlo et al. 2002]. Recently, annotation projection ap-plications, such as dependency parsing [Hwa et al. 2005], named entity recognition [Fu et al. 2011; Zitouni and Florian 2008], and semantic role labeling [Pado and Lapata 2009], have been studied. Some researchers have proposed utilizing parallel corpora for bilingual named-entity recognition [Chen et al. 2010; Li et al. 2012]. To the best of our knowledge, our work is the first to introduce a cross-lingual annotation projection approach to the relation extraction task.

While most previous studies have focused on direct projection approaches, one of the latest work [Das and Petrov 2011] proposed a graph-based projection method which is also considered in our work. Compared with Das and Petrov X  X  [2011] work, the con-tribution to our work is to tackle a more complicated task X  X elation extraction X  X han part-of-speech tagging which is the target task of previous work. An annotation for part-of-speech tagging is assigned to each word; thus a projections for the part-of-speech annotation is also performed in word-level. Since the unit of projection is same to the word alignment, the approach works on a relatively simple graph structure which encodes a set of words as nodes and their relationships as edges. However, there are some difficulties of utilizing this previous approach to relation extraction, because a unit of projection is not a word, but a pair of word sequences. To solve this problem caused by the inconsistent definitions of unit between word alignment and relation projection, we proposed a novel scheme of graph construction with multiple layers and three-phase propagation framework for projecting semantic relationships. In this work, we demonstrated the effectiveness of our proposed approach for Korean as a resource-poor target language. Some studies have been conducted on Korean language processing tasks including part-of-speech tagging [Lee et al. 2002], parsing [Chung 2004; Chung et al. 2010], text categorization [Kim et al. 2011; Ko and Seo 2011], and named entity recognition [Lee et al. 2007]. However, to the best of our knowledge, no work has reported on weakly-supervised relation extraction in Korean. This article presented a weakly-supervised approach for relation extraction. Our ap-proach uses cross-lingual annotation projection to automatically obtain training ex-amples for a resource-poor target language by propagating annotations generated by an existing relation extraction system for a resource-rich source language via a par-allel corpus between two languages. To relieve the bad influence of noisy projections, we focused on strategies for reducing the noises generated during direct projection. Furthermore, we proposed a novel graph-based projection technique which is more ro-bust than previous direct projection approach. The feasibility of our approaches was demonstrated by our Korean relation extraction system. The system was developed using an off-the-shelf English relation extraction system and an English-Korean par-allel corpus. Experimental results show that the projected instances help improve the performance of the task when our noise reduction strategies are adopted, and our sys-tem outperforms the other systems incorporating monolingual resources as well as supervised approaches learned with a limited number of training instances.
However, we expect that our method can be further improved. First, we can consider the way of determining the parameters for the noise reduction strategies in the current system. Although the evaluations demonstrated that these parameters directly influ-enced the performance of the system, we operated the system with a set of empirically defined values for those parameters. If we discover much more optimized combinations of parameters for the system, they can raise the quality of the extractions.
Another plan to make our approach better is to improve the efficiency of graph-based projection method. In this work, we operated the label propagation algorithm under very restricted conditions, including the small number of iterations, the large threshold frequency of trigram for context vertices, and unidirectional propagation from source to target languages. Most of these constraints were imposed to reduce the size of the graph, because the algorithm spends more time and memory as the number of vertices increases. To improve the performance of our graph-based projection approach, we plan to expand the bilingual projection graph; thus we have to relieve the complexity problem first.

The other direction of our future work is to investigate a hybrid approach to weak supervision considering not only cross-lingual projected annotations, but also various external resources such as treebank, Wikipedia, and the Web. We expect that this combined approach can help improve the quality of extracted results because the ef-fectiveness of each resource has been demonstrated by previously reported relation extraction systems.

