 We study the problem of scoring and selecting content-based features for a collaborative filtering (CF) recommender sys-tem. Content-based features play a central role in mitigat-ing the  X  X old start X  problem in commercial recommenders. They are also useful in other related tasks, such as recom-mendation explanation and visualization. However, tradi-tional feature selection methods do not generalize well to recommender systems. As a result, commercial systems typ-ically use manually crafted and selected features. This work presents a framework for automated selection of informa-tive content-based features, that is independent of the type of recommender system or the type of features. We evalu-ate on recommenders from different domains: books, movies and smart-phone apps, and show effective results on each. In addition, we show how to use the proposed methods to generate meaningful features from text.
 H.2.8 [ Database Management ]: Data Applications -Data Mining .
 Feature Selection, Recommender Systems, Collaborative Fil-tering, Content Based, Cold Start
Collaborative Filtering (CF) recommender systems have become an essential component in many in e-commerce sites and digital markets such as Amazon, Ebay and the Xbox Marketplace [8, 12, 15]. CF algorithms are typically fa-vored over content-based algorithms [11] because of their overall higher accuracy in predicting common purchase pat-terns. However, by their nature, CF algorithms face chal-lenges in modeling and recommending items with little or no usage (the  X  X old-start X  problem [16]). Items meta-data in the form of content-based features, has been used to al-leviate the cold-start problem and improve accuracy [1, 3, 4, 5]. Features are also useful for providing explanations to recommendations [17] and for visualization [7].

This work studies the problem of evaluating the quality of meta-data features. We present an algorithmic frame-work which is independent of the specific recommendation algorithm for which the selection is made. Instead, the rec-ommendation algorithm and other parameters are pluggable variables. Two types of selection are discussed: one for scor-ing meta-data attributes, and another for scoring meta-data labels (to be defined later on). Both have been success-fully used to enhance recommendations in the Xbox mar-ketplace, serving recommendations to more then 50 million users worldwide [8, 12, 14, 13].

We evaluate our methods and show their high effectiveness by experimenting with books, movies and data from win-dows Phone 8 smart-phone apps. In addition, we show how to use the framework to automatically generate informative labels from item descriptions and present the extracted la-bels for a qualitative evaluation.
Item catalogs in e-commerce marketplaces typically in-clude meta-data features in the form of attributes . The at-tributes may be numerical, categorical, ordinal, binary, etc. For example, some common attributes are price , brand and is-on-sale . Another common type of features are labels, or tags, assigned to items by consumers, experts, or extracted from text by an algorithm. A label is typically a word, an n-gram, or a short phrase describing the item. Labels follow the  X  X ag-of-words X  model. They form a closed dictionary, and every label may or may not be assigned to any item. Some examples of movie labels are: location-usa , horror , kids and funny .

In most catalogs there exists a subset of features that is highly informative with regard to the recommendation task at hand. However, many other features are often redun-dant or irrelevant with regard to CF. For example, in the context of books recommendations, author is informative, while cover-color is not. Despite a substantial body of work on feature selection for Data Mining algorithms, for most algorithms it is unclear how to extend them to the case of a recommendation system.
We briefly survey the three main categories of feature se-lection algorithms:
Wrapper methods evaluate different subsets of features by training a model for each subset and then evaluating each subset X  X  contribution on a validation dataset. As the number of all possible subsets is factorial in the number of features, different heuristics are used to choose  X  X romis-ing X  subsets (forward-selection, backward-elimination, tree-induction, etc.). Wrapper methods are independent of the prediction algorithm.

Filter methods are typically based on heuristic measures, such as Mutual Information or Pearson Correlation , to score features based on their information contents w.r.t. the pre-diction task. Similar to wrapper methods, filter methods are also independent of the algorithm in use. However, they do not require training many models and therefore scale well for large datasets. Yet, filter methods can not be naturally extended to recommender systems, in which the prediction target varies and depends both on the user X  X  history and on the item under consideration. This work proposes a frame-work and algorithms to address the above difficulties.
Embedded methods are a family of algorithms in which the feature selection is performed in the course of the train-ing phase. Unlike wrapper methods, they are not based on cross-validation and therefore scale with the size of the data. However, since the feature selection is an inherent property of the algorithm, an embedded method is tightly coupled with the specific model: If the recommendation algorithm is replaced, features selection needs to be revisited (e.g., in the context of recommendation systems see [9]).
As explained above, we distinct between two types of item features: attributes and labels . For attributes, we denote by s.a the value that item s has for the attribute a . For labels, we denote by s.L the set of labels associated with the item s ( X  X ag-of-words X ).

Our algorithms depend on a pluggable features similarity function sim (  X  ,  X  ) between two attribute values or between two labels. The function sim (  X  ,  X  ) may be a content-based function that depends on features values. Alternatively, sim (  X  ,  X  ) can be a CF similarity function based on users who purchased items with features f 1 and f 2 . An example of a simple content-based similarity is sim ( f 1 ,f 2 ) =  X  ( f which equals 1 if f 1 = f 2 and 0 otherwise. An example of a CF similarity function is the cosine similarity based on the users of items with f 1 and f 2
We denote by H u = { h u 1 , h u 2 , ... ,h un } the set of n items in user u  X  X  history. The set H u is used by an implicit-feedback recommender to produce a set of k recommended items denoted by R k ( H u ) = { r u 1 , r u 2 , ...,r explicit-feedback case, H u is also associated with numeric ratings.

The recommendation algorithm, R k (  X  ), is also pluggable to the framework. Different applications may benefit from different R k (  X  ) X  X  and sim (  X  ,  X  ) X  X . In Section 3, we discuss some of the concrete similarity measures and the recom-mender system we use.
We perform feature selection by computing a relevance score for each feature and then selecting the highest-scoring (i.e., most informative) features. Algorithm 1 and Algo-rithm 2 describe our algorithms for computing a relevance score for attributes and labels, respectively. Both algorithms are based on the ratio between two variables b 1 and b 2 : b is proportional to the similarity of the feature under con-sideration w.r.t. relevant items according to R k (  X  ). b Algorithm 1 ScoreAttribute(Attribute a ) Algorithm 2 ScoreLabel(Label l )
Figure 1: Computing attribute and label scores a normalizer which is proportional to the similarity of the feature under consideration w.r.t. random items. The ratio b /b 2 measures the normalized relevance of the feature with regard to recommended items.
Interestingly, our methods can be seen as a generaliza-tion of the lift measure commonly used outside the con-text of recommendation systems. For the particular case where sim ( v 1 ,v 2 ) =  X  ( v 1 ,v 2 ), the parameter b 1 occurrences of the feature in user histories and in their rel-evant recommended items. The parameter b 2 counts co-occurrences of the feature in user histories and in random items. Let E 1 be the event where a recommended item r is a  X  X ood X  recommendation to a user with history H u (i.e., appears in the top-k recommendations). Let E 2 be the event where a recommended item r , not necessarily  X  X ood X , has the same feature as an item in H u . It can be easily shown that under this particular settings, ranking according to b coincides with the empirical lift ( E 1  X  E 2 ):
Feature scoring for recommendation systems is not well studied. We thus evaluate the feature scoring algorithm us-ing a novel cold item representation task for Matrix Factor-ization (MF) models. MF models represent items and users by trait vectors in a low dimensional latent space. The in-ner product between a user vector and an item vector is Table 1: Reconstruction results for the books dataset. proportional to the affinity between the user and the item [10]. We perform our evaluation as follows: After training a MF model we iteratively remove a random item vector from the model and attempt to reconstruct its trait vector based on other item vectors having a similar features (labels or attribute values). We repeat this reconstruction process for N items, each time reconstructing a different item vector.
Formally, let q i  X  R D be an item vector of a removed item i from a trained MF model with dimensionality D . We measure the ability of each single feature f to reconstruct q based on other items which are similar to i according to the feature f . For each test item i , we find a set of items S whose f values are similar to that of item i  X  X  value for f . We then compute a reconstructed vector  X  q f i for i according to S f ( i ) as follows: Finally, we measure the quality of a reconstruction according to the Root Mean Squared Error (RMSE) and score the features quality by averaging all reconstructed item vectors: where N is the number of reconstructed vectors and ||  X  || denotes the L 2 norm.

This reconstruction process is designed to cope with the cold-start problem for items. When a new item i is intro-duced into the catalog, we wish to construct a trait vector in order to integrate i into an existing model even before having any usage data for i . This process is successfully used in the Xbox movies recommender. We note that while this evaluation process is specific to the cold-start repre-sentation problem for items in a MF model, the presented feature scoring framework is general to any task and any recommendation algorithm.
We evaluate with two datasets: The book-crossing dataset [18] and a dataset from the Xbox Movies recommender (smart-phone apps are considered in Section 4). As explained, we are interested in implicit-feedback. We use the algorithm of [12] to learn trait vectors for items, as well as for ranking recommended items in Algorithms 1 and 2.
 Books: Each book in [18] is associated with three attributes: author , publisher and year . For scoring attributes, we choose sim (  X  ,  X  ) to be the cosine similarity between attribute values. For example, for author , sim ( v 1 ,v 2 ) is the cosine similarity on users who read author v 1 and author v 2 . We reconstruct a sample of 500 books based on each of these attributes. Table 1 summarizes the reconstruction results. As intuitively ex-pected, author has the highest score, followed by publisher , and then year . RMSE results coincide with these scores (i.e., higher RMSE for low scores). Movies: Movies in the Xbox movies dataset are associated with labels. Each label is associated with a category, e.g., Audience , Mood , Plot . Every movie in the catalog has zero or more labels from each category. The Audience category has labels such as Kids , Girls Night and Family ; The Look category has labels such as 3D , Black and White and Ani-mation ; The Time-Period category has labels indicating the time in which the plot takes place. We use this dataset for evaluating both attribute and label ranking as follows: Movies Attributes. We treat each of the label categories as a distinct attribute, and the unordered set of labels which belong to this category as the attribute value. Here we de-fined sim ( v 1 ,v 2 ) to be Jaccard similarity on labels belonging to movies v 1 and v 2 . We then reconstruct a sample of 1,500 movies, and compute RMSE for each category. Figure 2 depicts the RMSE results vs. the attribute scores. As ex-pected, categories like Audience and Look were found to be more informative than categories like Time-Period . Notice the negative correlation between the scores and RMSE. Movies Labels. In this experiment, we evaluate our la-bel scoring algorithm. Our experience shows that different labels from the same category may have a different infor-mative value. For example, the Place category is in general non-informative, as for most movies it simply take the la-bel USA . Nevertheless, for a small subset of movies, this category carries a more informative label such as Ghetto which highly predicts whom might watch the movie. Hence, we evaluate labels separately, ignoring categories. Here, we used sim ( i,j ) =  X  ( i,j ). Figure 3 depicts the RMSE results vs. the label scores. Again, we see a clear trend line indicat-ing a negative correlation between the scores and RMSE.
Crafting a list of descriptive features and assigning them to catalog items is a labor-intensive process, typically per-formed manually by content specialists. Next, we demon-strate how to use the presented framework to automatically generate (and select) label features extracted from text de-scriptions of items. We model text descriptions using the
Table 2: Ranked labels from apps descriptions binary bag of words model, and perform a simple filtering process, in which we remove stop words, words with very high inverse-document frequency, and words with very low frequency [2]. The remaining terms are normalized with a stemmer. We treat each word as a label and perform feature selection using Algorithm 2.

We present a ranked list labels from Windows Phone 8 apps descriptions 1 . After filtering, a total of 1335 labels were scored using Algorithm 2 with sim ( f 1 ,f 2 ) =  X  ( f and R k (  X  ) from [12]. Table 4 presents the 10 top ranking and the 5 low ranking labels. Observe that high ranking labels describe apps genres ( music , video , and ball , which indicates a game) and demographic features ( kid , girl ), that are naturally informative for CF. Labels such as app , version and time , that clearly not informative, are ranked low.
Avail. on http://www.windowsphone.com/en-us/store
Feature selection for recommendation systems is a rela-tively new field. This paper presents a generalized frame-work for selecting informative features for CF. The frame-work is successfully utilized to enhance recommendation in the Xbox Marketplace. We hope this work will inspire future research on feature selection for recommendation systems. [1] Fab: content-based, collaborative recommendation. M. [2] R. Baeza-Yates and B. Ribeiro-Neto. Modern [3] Hybrid Recommender Systems: Survey and [4] Combining content-based and collaborative filters in [5] Yahoo! Music Recommendations: Modeling Music [6] The Yahoo! Music Dataset and KDD-Cup X 11. G.
 [7] Map Based Visualization of Product Catalogs. M. [8] The Xbox Recommender System. N. Koenigstein, N. [9] Xbox Movies Recommendations: Variational Bayes [10] Matrix Factorization Techniques for Recommender [11] Content-based Recommender Systems: State of the [12] One-class Collaborative Filtering with Random [13] Sage -Recommender Engine as a Cloud Service. R. [14] Automatic Feature Selection for Recommender [15] J. B. Schafer, J. A. Konstan, J. Riedl. E-Commerce [16] Methods and metrics for cold-start recommendations. [17] Designing and Evaluating Explanations for [18] Improving recommendation lists through topic
