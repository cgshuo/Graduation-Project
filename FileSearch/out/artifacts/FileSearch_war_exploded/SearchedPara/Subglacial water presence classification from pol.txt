 1. Introduction surveys, attempt to acquire data in order to infer properties of the subsurface from a remote location such as the surface, air, or space. A prominent example is the use of radar sensors to gather data from the polar ice sheets, specifically internal layers and the ice X  X edrock interface, from the surface of an ice sheet. Other examples are satellite-based imagery and identification of land-cover and events, and the observation of light and its character-istics to infer properties of distant galaxies and stars.
Antarctic ice sheets have been used for many years to determine characteristics such as ice thickness, subglacial topography, and mass balance of large bodies of ice. Radar sounding of ice sheets is challenging due to the rough surface interface, various stages of melting both on top of and within the ice sheet, and spatial variation of ice thickness and bedrock topography. Processing the data, including the incorporation of knowledge about the sensing medium, is important for proper interpretation and dissemination of accurate data to the scientific community. For example, in
Greenland, water is present over continuous finite distances which are smooth, but do not qualify as large lakes that are evident in Antarctica.
 At the University of Kansas, the Center for Remote Sensing of Ice
Sheets ( CReSIS, 2009 ) performs polar research to gather data and model ice sheets to better understand global climate changes and the possible effects, including sea level impacts. We have designed, built, and utilized mobile robo ts to autonomously trav erse polar terrain in
Greenland and Antarctica and su pport radar remote sensing data acquisition ( Gifford et al., 2009 ; Stansbury et al., 2004 ; Harmon et al., 2004 ; Akers et al., 2004 , 2006a , 2006b ). Airborne systems have been developed to offer large-scale studies of polar regions. One primary technologyforthisresearchisthedevelopmentofsophisticated radars. CReSIS researchers have p rovided first views of the ice sheet bed in fast-flowing areas and certain internal ice sheet layers, and continue to provide detailed subsu rface images and models describ-ing their behavior and dynamics. CReSIS has a wide variety of radar data from various radar designs o ver many years. All of these data are available to the public, provid ing information such as latitude, longitude, radar travel times, bed echo intensity, and ice thickness for extended flight segments. Thi s data repository can then be used for advanced machine learning, data mining, and modeling efforts for polar environments.
 In this paper, we utilize ice-pen etrating radar data collected in
Greenland in May 1999 and September 2007 as part of a model-creation effort for subglacial water presence classification. Using radar data from ice sheets, the goal is to learn a model of water presence or absence for each radar measurement . This enables the production of predicted water presence maps for the scientific community without needing to drill resource-intensive ice cores. Specifically, a detailed study of ensemble learning and decision combination has been conducted. Experimental results a re presented, including successful ensemble compositions, individual learning algorithm contribution, team size, and team diversity. Finally, classification results as well as ensemble learning heuristics discovered as part of this investigation are discussed. 2. Background and related work
Machine learning techniques have seen limited application to ice sheet and polar subsurface data. Most efforts involve identification or tracing of specific layers that hold historical importance. Internal layers of ice sheets have been investigated to predict the depth and thickness of certain layers. For example, initial efforts to predict the depth and thickness of the Eemian Layer in the Greenland ice sheet utilized a Monte Carlo Inversion of the flow model to estimate unknown para-meters constrained by the internal layers ( Buchardt and Dahl-Jensen, 2007a , 2007b ). One of these parameters is the basal melt rate, an important parameter for ice sheet models. Similar work has been done to classify the presence of bottom cre-vasses, and to estimate their height using radar data of the Ross Ice Shelf in Antarctica ( Peters et al., 2007 ). A physical model was developed by studying the radar data and long echo tails that were found to be characteristic of bottom crevasses. That work largely involved studying the data to determine power reflection coefficients, and did not incorporate any regression or machine learning methods.

Learning classifiers have been recently employed for autono-mous polar event detection via satellites. As part of NASA X  X  New Millennium Program, the Autonomous Sciencecraft Experiment (ASE) has been used for detecting dynamic events  X  such as volcanic eruptions, floods, and cryospheric events  X  using onboard science algorithms. One such focus was on the detection of ice-based surface events (lakes freezing/thawing and sea ice breakup) ( Castano et al., 2005 , 2006 ). As part of that work, four pixel-based classifiers (manually constructed classifier, exhaustive threshold band ratio search classifier, decision tree, and SVM) were employed to generate models to recognize events using hyper-spectral images. The SVM classifier, which has seen regular use onboard the EO-1 spacecraft, successfully classified sea ice breakup near Antarctica which autonomously triggered a space-craft reaction. Such a classifier can be used to identify high priority data and reduce the bandwidth for data communication back to Earth. In Srivastava and Stroeve (2003) , preliminary results are presented for unsupervised discovery of geophysical processes (including snow, ice, and clouds) from a spaceborne instrument over Greenland. Their results showed that regional classifiers provide higher classification accuracy for polar regions, but still had difficulty differentiating between certain clouds and snow/ice. Others have taken a knowledge-engineering approach to build an intelligent satellite sea ice classifier by creating rules from sea ice experts ( Soh and Tsatsoulis, 2000 ).

Efforts to classify subsurface layers, occasionally called facies in the literature, using ground penetrating radar (GPR) are abundant. Together with seismic surveying, GPR allows the remote sensing of subsurface properties to guide decisions of where to drill for oil or natural gas, as well as help explain observed surface changes. Radar reflection patterns can be used to distinguish echo returns in different regions, which lends well to machine learning methods to associate such patterns with distinct classes ( Moysey et al., 2005 ). GPR signal interpretation is one of the more studied applications of subsurface classification, as interpretation of subsurface materials or layers requires coring or drilling to determine ground truth. Experts are required to manually study the materials to segment them into distinct rock or object classes. These data can then be used by classifiers to learn a model between GPR return signals and the excavated ground truth. As GPR has been used for many applications, machine learning can aid in automating these tasks by learning a model, interpolating between measurement sites, and charac-terizing the subsurface at other unknown locations.

From these works, it has been shown that using machine learning to create models utilizing polar radar data analysis is not only feasible, but also can provide high levels of accuracy while offering a significant increase in efficiency. In the following sections, we discuss the approach, using multiple learning algo-rithms, to further increase accuracy and efficiency for the applica-tion of subglacial water presence classification from radar data acquired on the Greenland ice sheet. This approach could also be applied to the Antarctic and other large bodies of ice, but is not explicitly discussed in this paper. 3. Radar, coring, and subglacial water
As part of NASA X  X  Program for Arctic Regional Climate Assess-ment (PARCA) and others, the University of Kansas and CReSIS have collected ground and airborne radar data in Greenland which cover the majority of the continent X  X  ice sheet. CReSIS now hosts these data for public use. Other major efforts, such as ice coring, have produced data that complement these radar data sets to study subglacial activity of the Greenland ice sheet.
Studying the extent/presence of subglacial water is important, as the state of the bed of the ice sheet (wet or frozen) provides information about flow, friction, and roughness of the underlying bedrock interface. The presence of water (wet state) means that the interface is likely slippery, or that heat has caused a downflow of water from upper or nearby portions of the ice sheet. Portions of the ice with larger amounts of water exhibit faster flow properties and dynamic changes of the ice in those regions due to lubrication at the bed. The lack of water means that the interface is frozen, likely exhibiting more friction and therefore little movement.
A smooth bedrock interface lends more to sliding of the ice sheet along its surface, whereas a rough interface introduces more friction between the ice sheet and bedrock on which it rests.
If water is present at the bed, then the interface between the ice and water will be smoother, compared to a mixed water and rock interface (which will exhibit the shape of the underlying rock).
Transport of material, however, may cause this lower rock inter-face to change over time.

Ice coring offers an additional method to study, among many other things, the state of the ice X  X edrock interface by manually examining the bottom of the cored ice column. Thus, ice cores represent the only ground truth available for large-scale ice sheet modeling and remote sensing validation. In Greenland, there are two primary drill sites which we focus on in this work, namely,
GRIP and N-GRIP. The Greenland Ice Core Project (GRIP) drilled a core from 1989 to 1992 to a depth of 3029 m, located at 72.58 N, 37 : 63 W ( NCDC, 2009 ). The subglacial state of this core was found to be frozen. The North GRIP (N-GRIP) drilled a core from 1999 to 2003 to a depth of 3085 m, located at 75.1 N, 42 : 32 W ( NGRIP, 2009 ). The subglacial state of this core was found to be wet (high melt).

As a radar transmits energy down to and into the ice  X  typi-cally from one or more transmitting antennas mounted on the wings of an aircraft  X  changes in dielectric properties cause the signal energy to reflect and refract. Larger changes in dielectric properties (such as air to ice, and ice to rock) cause strong reflection intensities to be received by the radar X  X  receiving antenna(s). Fig. 1 shows an example radar echogram over an ice sheet, illustrating the reflection of internal layers and the bedrock interface beneath the ice sheet. Each column of the echogram is a trace (right), representing sensor measurements with time (and therefore depth). Water is a comparatively strong reflector when surrounded by ice/rock and affects the signal return intensity, whether internal to or at the bottom of the ice sheet. Frozen ice against bedrock also represents a strong reflecting interface. Thus, other properties of the signal and the return envelope can be analyzed to determine whether water is present or not. 4. Combining decisions from multiple classifiers offered advances in classification accuracy for complex data sets.
It has been termed differently in the literature, namely, classifier fusion, mixture of experts, committees, ensembles, teams, pools, collective recognition, composite systems, etc. Several methods have been developed to combine predictions from multiple classifiers, the most popular being voting, boosting, bagging, and stacking.
 rated homogeneous learning algorithms. Fewer works have focused on combining heterogeneous learners, which is the focus of our study. The term  X  X  X eterogeneous X  X  is used in different contexts in the literature. Popular classifier combination approaches such as boosting and bagging involve homogeneous learning algorithms and manipulation of the training data set.
Produced models are therefore homogeneous in representation; although the learners may output slightly different predictions.
Use of different learning algorithms, and therefore heterogeneous model representations, is considered purely heterogeneous in the contexts of algorithm and model.
 be a mixture of weak (high error) and strong (low error) classifiers. Weak classifiers are typically simple to create, at the expense of their accuracy on complex data sets. Strong classifiers are typically time-consuming and expensive to create, as their parameters are fine-tuned for maximum performance. Further-more, some classifiers perform better than others on the same data set due to their algorithmic nature. Combining weak/strong or homogeneous/heterogeneous classifiers offers the benefit of encompassing different levels of expertise and knowledge bases.
Exploiting and studying these properties as advantages for classi-fication is a driving force for multi-agent machine learning.
An important aspect related to difference in learners is their level of error correlation relative to one another, and as a whole.
The more correlated (less disjoint) individual learners are, the less complementary they may be. If uncorrelated, they likely will misclassify different instances. Combining them better enables the system to correctly classify more instances. A significant improve-ment over a single classifier can only occur if the individual classifier theories are substantially different. It is desired to obtain a balance between high performance and complementarity in a team which combines decisions. If one learner does not predict correctly, the other learners should be able to do so. Diverse models are therefore more likely to produce error in different ways. Methods to accom-plish this typically involve introducing diversity in terms of learning paradigms, feature subsets, or training sets ( Tumer and Ghosh, 1996 ).

Although selection of the best individual classifier is easier and occasionally effective, combination techniques scale better to larger and more complex learning problems. Combining all classifiers in an ensemble can be improved upon by selecting for combination only those that perform significantly better than others, termed Selective Fusion ( Tsoumakas et al., 2005 ;
Kuncheva, 2002 ). Other efforts have studied the use of training multiple classifiers on different feature subsets prior to their combination ( Chen et al., 1997 ). Focusing on differing and potentially overlapping feature subsets creates additional diver-sity, which could lead to an improved combined model. Techni-ques such as boosting and stacking can also be chained together to achieve high accuracy ( Ebrahimpour, 2007 ). These works represent examples of how heterogeneity and diversity can lead to more robust, high-performance classifiers. Multi-classifier learning systems offer a medium for additional study on how knowledge from learners with different, potentially overlapping feature and data subsets can be combined. Interaction between the learners during the learning process may prove to increase learning efficiency, robustness, and accuracy. 5. Implementation and processing
The work presented here is based on a method that recovers radar reflection characteristics at the bed of the ice sheet that discriminate between frozen and wet subglacial properties for each trace of a radar survey ( Oswald and Gogineni, 2008 ). The method is said to be most appropriate for Greenland, but a similar method could be applied to the Antarctic ice sheet. It is indepen-dent of the radar characteristics, as the two primary signal return attributes are radar independent if computed properly and normalized. Numerical and statistical analyses of radio-echo profiles and interface geometries, as well as ice and water physics, represent the basis of the method. Results were validated using known basal water presence at the GRIP and N-GRIP core sites.
In Oswald and Gogineni (2008) more details of the method are provided, including related work on radio-echoing and depth-sounding of ice sheets.

A total of 13 strategic radar flights covering various portions of the Greenland ice sheet were flown throughout May 1999. These flight lines are shown in Fig. 2 , with one line/color per flight. The radar data set used to develop the original method was collected from an aerial survey over Greenland on May 14, 1999.
 Specifications of the radar used to acquire these data, named Improved Coherent Antarctic and Arctic Radar Depth Sounder (ICARDS), is shown in Table 1 . Uncertainty of the radar ice thickness measurements have been estimated to be 7 10 m from comparisons to ice coring sites. We reproduced the results, with minor differences, of the published method, with minor modifica-tions, on the same data set.

GPS technology allows the same flight lines to be flown for data acquisition over several years to study regional ice sheet changes due to climate and dynamics. In this work, we use the method X  X  radar independence to generate a subglacial water presence model for radar data acquired in 2007 from a new radar system, called the Multi-Channel Radar Depth Sounder (MCRDS), over an identical extended flight segment covering the GRIP and N-GRIP core sites. These flight lines (May 14, 1999 and September 17, 2007), which comprise the primary data set for our study, are shown in Fig. 3 . Table 2 presents the specifications of the MCRDS radar system.

Using the extracted basal water presence information by the published method for the 1999 ICARDS radar data, training and testing sets for classification were constructed for our machine learning study. This assumes regional similarity in water content and that the water presence has not drastically changed (promi-nently wet or frozen) at the GRIP and N-GRIP sites in the years from 1999 to 2007. Given that these sites are located near the
Greenland ice divide, this assumption can be made without loss of generality.

This section presents the implementation details associated with processing the radar data for classification. The following sections discuss the implementation and results of a detailed multi-classifier machine learning study for the application of radar-independent model generation for polar subglacial research. The implementation incorporates additional informa-tion to that of the original method for more robust water presence classification and transfer of the concept to radar data acquired with new technology in the midst of global warming and climate change. Matlab ( The MathWorks, 2006 ) was utilized for proces-sing and manipulating the radar data into attributes for classifica-tion using the described method with slight modifications. 5.1. Smooth and filter data
Pre-processing of raw data consists of presumming (averaging) in hardware during acquisition, as well as offline coherent integration, range compression, frequency filtering, initial inco-herent integration, azimuth compression, and waveform combi-nation. This results in pre-processed data products, which are then passed through several cleaning stages before attributes can be computed using the received radar returns.

First, the data are incoherently integrated (using both magnitude and phase information for averaging) over an along-track distance of 200 m for smoothing purposes. N ext, individual traces are removed which exhibit the following  X  X  X ad data X  X  aspects:
Latitude or longitude is erroneous (well away from traversed flight line, possibly due to poor GPS visibility during aircraft turns)
Visually smeared surface or bed portions of preliminary-processed echogram
Non-characteristic reflection magnitudes at bed, likely due to aircraft banking, nearby mountains or exposed rock, or large amounts of surface water
Ice thickness is o 300 m (surface and bed reflections difficult to distinguish) Bed reflection is not visible
Ice thickness or bottom depth could not be computed due to lack of visible bed reflection
Portions of the airborne survey that contain banks/turns of the aircraft may produce results that are not reliable or indicative of the true subglacial state. These portions are therefore removed prior to attribute calculation and processing. 5.2. Data calibration and boundary alignment
Most airborne ice sheet surveys begin on a known smooth, frozen path (runway) or portion of the ice sheet. This represents another known aspect of the ice sheet, to accompany ice cores, with which can be used for calibrating and bounding the intensity distributions received by the radar. These portions of the data are assumed to be indicative of typical frozen subglacial interfaces.
The GRIP (frozen) and N-GRIP (wet) core sites provide reflection intensities that are known to range from frozen to wet. By focusing on these sources of ground truth, encompassing the entire spectrum of subglacial water extent (fully wet to fully frozen), the method can be applied to the entire ice sheet. used as a reference. The upper intensity bound in the received intensities over flights represents wet interfaces with varying fractions of water content. Using the lower intensity bound, all extended flight segments have their intensity distributions shifted to exhibit matching lower bounds. This, if done correctly, removes the data X  X  dependence on radar settings and sensitivities, yielding radar independence.
 from varying over 40 dB down to 20 dB for more reliable basal reflection coefficient determination. Intense returns accompanied by high abruptness are indicative of the water presence. Large internal layer disturbances in the ice (ice stress and deformation) are also indicative of locations which likely contain water at the base. The original method verified this using the coherence index and echograms exhibiting such distortion ( Oswald and Gogineni, 2008 ). The method underestimates the extent of water presence, producing a decision of frozen, where intensity is high but abruptness falls below threshold. 5.3. Radar signal attributes
Three primary radar data attributes are utilized by the method at each trace: relative signal intensity, signal abruptness index, and signal coherence index ( Oswald and Gogineni, 2008 ). The first two are used to determine the binary result of wet or frozen, where relative signal intensity is the primary determinant. They employ coherence to further analyze and explain the results.
These attributes are now discussed in more detail. 5.3.1. Relative intensity
This attribute represents the radar signal return intensity at the basal interface, including the first return and echo tail. The strength of the reflection intensity degrades with depth, as the signals attenuate and experience loss as they travel through solid ice and other media. Thus, the intensity variations can be modeled by an equation similar to the following to depth-adjust the received reflection due to rate of absorption in ice:
B  X  2 : 3 3000 H  X  2000  X  1  X  where H represents the surface altitude, which governs intensity variation as a function of depth. The intensity after adjustment still requires recalibration for variations of the system, but is otherwise representative of the reflection coefficient at the bed of the ice. Pre-processed radar data go through several stages of processing and adjustment to arrive at the final relative intensity value of the bed reflection. 5.3.2. Abruptness index
This attribute provides information on whether the echo tail is beamwidth-limited. Abruptness constrains the angular spectrum of the received signal. The abruptness index can be computed as follows:
Abruptness  X  P peak  X  X  X  P
The numerator in the calculation is the peak (max) basal reflection intensity ( P window ) for a specific trace/position. The denominator is the aggregate (sum) basal reflection intensity, or the sum of return intensities over depth ( X ) bins in the basal return window ( P window ). High values of abruptness are correlated with high aggregate intensity, but is not a clear discriminator of wet versus frozen on its own. Thus, high abruptness that is accompanied by intense returns is indicative of the presence of water. For the pulse length and sample rate used by the radar, the reflection abruptness is typically between 0.05 and 0.5. The method ( Oswald and Gogineni, 2008 ) uses a value of 0.25 to identify locations that are significantly smooth, which is half of the maximum expected abruptness index. 5.3.3. Coherence index This attribute provides information on whether the first Fresnel zone is determined by roughness or wavelength. Thus, coherence is another measure of smoothness at a smaller scale. Coherence can be calculated using the following equation: Coherence  X 
The numerator sums the complex echo signal ( R complex  X  D , X  X  ) over the distance along-track ( D ) for coherent and incoherent integrations, which is evaluated for a slope that maximizes the coherent integral. The denominator is the signal power summed over the distance ( D ) and depth ( X ) bins. Coherence is calculated from the complex received signal and abruptness, independent of the received signal intensity. Coherent integrations are performed over a distance of 200 m. A coherence value of 0.3 or larger is said to indicate surface facets that are flat to within 0.2 m over an integration distance of 200 m. As suggested in Oswald and Gogineni (2008) , a coherence value of 0.3 and above, coupled with interface and surface slopes, produces a regression gradient that directly links smooth interface facets with physical charac-teristics of water.

Coherent integration is in the along-track direction, using only the magnitudes (no phase information) for averaging. Incoherent integration is also in the along-track direction, but uses both magnitude and phase information for averaging. Received inten-sities are incoherently averaged over a distance of 200 m to achieve smoothing and reduce signal intensity variation. For smooth interfaces, the response is more impulse-like; whereas slightly rough interfaces exhibit more intensity variation; and rough interfaces produce intensity variations more evenly dis-tributed. The intensity distribution is largely caused by the scatter of energy as a function of interface roughness. Variance of the received signal is further reduced by aggregating the basal reflection envelope over all beam pattern directions ( Oswald and Gogineni, 2008 ). 5.4. Computing data attributes
Prior to computing data attributes, each trace has the bed reflection (peak intensity depth from expected bed region of A-scope, where visible) selected by a radar expert, assisted by surface-and bed-picking software. Fig. 4 demonstrates this process using the custom software, showing echograms surface returns fully picked and the bed reflections partially picked. The surface can be easily selected, as it typically represents the first and largest reflection intensity spike in the trace X  X  return profile. Using the corresponding radar echo time, peak depth and ice thickness can be calculated.

A five-sample window is created around the peak bed reflec-tion for each trace, containing two samples above and below the determined bedrock reflection depth. Abruptness is computed using the peak reflection and the sum of the bed reflection window intensities (the depth bins). The received reflections are depth-adjusted, as previously described, using the ice thickness at each trace. Peak reflection intensity is converted to a relative intensity value in dB by dividing the peak at each trace by the maximum peak intensity in the data X  X  basal reflection window.
An important aspect of this method is the alignment of the lower boundary of the intensity distributions. The upper bound-ary corresponds to the wet interface, while the lower boundary corresponds to the rock/frozen interface. By studying the ground-truth GRIP (frozen) and N-GRIP (wet) coring sites, this lower/ frozen reflection intensity boundary can be found. For radar data acquired over these core sites in 1999, this lower bound was found to be 31 : 37 dB. Each flight X  X  lower boundary is aligned using this reference value to perform the final step of removing the effects of radar settings.

Fig. 5 plots the relative intensity distribution over the May 14, 1999 flight segment containing wet and frozen bed characteris-tics, illustrating a bimodal distribution (one hypothetical
Gaussian distribution for the wet intensities and another for the frozen intensities). Some portions of the 20 X 30 dB variation in intensity overlap at lower probabilities. The overlapping area is where it can be distinguished between wet (higher relative reflection intensities) and frozen (lower relative reflection inten-sities) areas by computing a threshold which separates the bimodal distribution into wet and frozen portions. Observed intensities showed a separation of 13 dB between the mean of each Gaussian mode with high confidence ( Oswald and
Gogineni, 2008 ). Once the thresholds and lower bound of the intensity distribution are found, the method can be applied to radar data acquired over the entire Greenland ice sheet to recover the extent of basal water presence. Finally, coherence is calcu-lated for each trace.

Fig. 6 presents the attributes, thresholds, and binary basal water presence classification for the extended flight segment 42 X 70 from the May 14, 1999 flight. Along with the echogram for this segment, the bedrock reflection window is shown, on which relative intensity calculations are based. The empirically-found thresholds of 15 dB (relative intensity) and 0.27 (abruptness) dictate water presence for each radar trace. Coher-ence values above 0.3 represent smoother surfaces, which are utilized as additional signal information for classification. The referenced study ( Oswald and Gogineni, 2008 ) uses a value of 0.25 to identify locations that are significantly smooth. We found an abruptness value of 0.27 (slightly more than half of the maximum abruptness index value) more closely matched our ground truth data, provided that the referenced method did not explicitly specify the relative intensity used for experimentation.
Our empirical results suggest a relative intensity value of 15 dB, coupled with 0.27 for abruptness and 0.3 for coherence, provided a best fit to our ground truth water presence data, as well as alignment with results obtained by others ( Oswald and
Gogineni, 2008 ). 6. 1999 ICARDS radar data set
Each trace is marked as being wet or frozen using the calculated abruptness and relative intensity for each trace. A trace, t , is determined to contain water at the bedrock if:  X  RelIntensity  X  t  X  Z 15 : 0  X  and  X  Abruptness  X  t  X  Z 0 : 27  X  The binary (wet/frozen) 1999 ICARDS data set is then generated by saving the following attributes for each trace: Latitude/longitude Relative intensity ( dB) Abruptness ([0,1]) Coherence ([0,1]) Ice thickness (m) Subglacial state: wet (1), frozen (0) Ice thickness is included to allow the learning algorithms to take into account depth when fitting a model to the data for classifica-tion. All three primary attributes are used, plus others, for learning a model connecting the radar data and subglacial water presence. The original method X  X  paper reports results using only relative intensity and abruptness attributes, with some analysis using coherence ( Oswald and Gogineni, 2008 ).

Each trace of the radar survey represents an instance which can be used for training or testing a classifier. The above information represents the attributes for each instance. Finally, the method X  X  output represents the instance X  X  class, used as the target for training a classifier or for computing classification accuracy during testing. This represents a binary classification problem. The data set X  X  statistics are shown in Table 3 .
Using the described method, water presence has been classi-fied for the entire May 1999 Greenland radar survey. The method classifies that, out of a total of 7995 traces for this segment, 1565 are wet and 6430 are frozen. The GRIP and N-GRIP sites are classified as frozen and wet, respectively, and water tends to be present in continuous regions. Fig. 7 shows the results spatially.
Icons marking water presence over flight lines are sized for visualization purposes, and should not be interpreted as contain-ing water beneath the entire icon. 7. 2007 MCRDS radar data set
As the data and method were studied and developed in detail, the May 1999 ICARDS radar data set represents ground truth for further studies. This portion of the 1999 data set encompasses some of the limited pure ground truth available in Greenland.
This data set is used in our work to train teams of classifiers to learn a model between radar surveys acquired with different radars over the Greenland ice sheet.
 The corresponding flight segment from the 2007 MCRDS
Greenland radar survey was flown on September 17, 2007. In particular, the D-K data segments form a flight path that overlaps the same flight path flown over GRIP and N-GRIP in 1999. We can thus use the method X  X  output from 1999 data at those locations to provide the target class (wet or frozen) for training using the 2007 data.

The 2007 MCRDS radar data are processed using the same method as the 1999 data, with the following minor differences:
Use of seven traces on each side of the current trace (15 total traces) for incoherent integration and computing coherence (as opposed to 1 on each side, 3 total traces, for the 1999 data).
This is due to the higher resolution, more dense radar data in the 2007 survey from advanced radar and computing technologies.

Differences between the oscillators used in the 2007 radar, which affects the coherence attribute. This is due to improved technology.

Alignment of the lower boundary of the reflection distribu-tions of the 2007 data is performed using reflection intensity bounds from the MCRDS data.
 Fig. 8 plots the relative intensity distribution over the
September 17 flight segment from the 2007 Greenland MCRDS radar survey. Again, a bimodal distribution is observed: one hypothetical Gaussian distribution for the wet intensities, and another for the frozen intensities. However, compared to the 1999
ICARDS relative intensity distribution, it is more difficult to distinguish between wet and frozen basal states, as the distribu-tion exhibits more overlap.

The 2007 MCRDS data set is created using attributes computed from the MCRDS radar data and the geographically closest location X  X  class (wet or frozen) from the 1999 ICARDS model.
Fig. 9 shows the transferred water presence (marked with icons) for training and testing purposes. This assumes regional similarity in subglacial state, and the general stability of this state over the time difference between data acquisition between these systems.
Thus, the ground truth water presence output from the described method is used to label the 2007 MCRDS data for each trace X  X  water presence. The reason that this is possible, even with heterogeneous radars, is that attributes of the 2007 processed data are normalized independently, and independent of the 1999 data. Even though the reflection intensity, abruptness, and coher-ence magnitudes are different from radar to radar and with different radar settings, a common classification target and normalization facilitate model transfer from one year X  X  survey to another.
ICARDS data set. Fig. 10 presents the attributes and binary basal water presence classification for the extended flight segment D-K from the September 17, 2007 flight. Along with the echogram for this segment, the bedrock reflection window is shown, on which relative intensity calculations are based. Training and testing sets are created by randomly splitting the data into two-third training and one-third testing portions. The statistics of this data set and its training and testing portions are shown in Table 4 . Out of a total of 45,477 traces, 7819 are wet and 37,658 are frozen. that of wet training instances (nearly fivefold), creating a highly unbalanced training problem. As the number of ground truth sites were limited (i.e., where drilling had occurred from surface to bedrock to determine wetness, coupled with multiple radar passes acquired over time), we approached the training problem from a site-based perspective. Further, radar data processing and experimentation suggested that there were varying levels of  X  X  X rozen X  X  from radar signature and basal wetness/melt perspec-tives. As such, we aimed to capture as much of the variation as possible with a larger amount of frozen bedrock examples from various tracks, while including all wet bedrock instances for model creation. Our primary concern was identifying water presence with confidence. Therefore, the levels of  X  X  X rozen X  X  were accumulated into a single class.

In the process of constructing the classifiers to approach this problem, experiments were conducted with other training set compositions which employed sampling techniques to reduce the size of the training set (e.g., number of total training instances, due to the resolution of the employed radars) and attempt to avoid overfitting. It was found that training classifiers with more radar instances, specifically for those traces labeled as frozen in the ground truth data, resulted in a more robust and accurate classifier set. This became apparent when testing the classifiers with radar data that was spatially (i.e., regionally) similar or exhibited characteristics similar to the sites o f interest (i.e., GRIP and N-GRIP).
Although the data set was unbalanced, little adverse overfitting was observed. As ground truth sites are drilled and more data becomes available via drill site analysis and radar transects, the need to sample for training purposes will be explored.

The 2007 data set is provided to ensembles of classification algorithms to create a combined model of the radar data against the determined subglacial state for all traces. Classification results offer probability estimates for each trace X  X  inferred state (wet, frozen). Probability estimates from multiple learning algorithms, using different model fitting techniques, provide further informa-tion with which to determine the extent of the ice sheet X  X  subglacial water presence. 8. Research methodology 8.1. Multi-Classifier Collaborative Learning Architecture
The WEKA machine learning suite ( Witten and Frank, 2005 )is utilized as a base for the implementation of the proposed Multi-
Classifier Collaborative Learning Architecture ( Gifford, 2009 ). The primary uses of WEKA in this architecture are to prepare the data for experimentation and provide implementations of various classification algorithms. Additional Java and script/batch file implementations act as a wrapper for machine learning experi-ments involving single or multiple learners (i.e., for teams of any size), homogeneous or heterogeneous team composition, independent or collaborative learning (with the ability to vary the number of collaboration events during learning), and combin-ing the decisions of the learners using a variety of accuracy-and vote-based combination techniques. This offers a robust and flexible architecture for machine learning studies involving multi-ple classifiers.

The Multi-Classifier Collaborative Learning Architecture was utilized to study aspects of team learning for the subglacial water presence classification data set. By default, each learner is provided a randomized version (different orders of instances for each learner) of the full training data set. Learning takes place by training each classifier using the training data set. At this time, each learner tests its model on the data on which it was trained.
Once training is complete, the testing data set is passed through each learner X  X  model and the corresponding class probabilities (predictions) are recorded. The final individual testing predictions are then used for combining decisions from multiple learners.
This acts as the final collaboration step, which fuses the knowl-edge from multiple learners to a single team classifier via accuracy-and vote-based mechanisms.

Decision combination utilizes each learner X  X  classification for each testing instance to arrive at a single team classification per combination method. A learner X  X  prediction consists of a probability that the testing instance belongs t o each of the possible classes. The highest probability represents the predicted class for each classifier.
The following vote-based combination methods are used for evalua-tion. Selective variations of all methods listed below, with the exception of Select Best, were als o examined. Selective variations for each method only take into account learners that have a testing accuracy Z the mean team testing accuracy. Ties are broken by using the class selected by the learner with the highest accuracy on the testing data, unless otherwise stated. The combination method resulting in the best classification accuracy is selected, reflecting overall team performance.

Max : Selects the class with the absolute maximum probability value out of all learners.

Average : Averages all predictions for each class from all learners and selects the class corresponding to the highest average value.
Multiply : Multiplies all prediction values for each class from all learners and selects the class corresponding to the highest resulting value.
 probability) for each class from all learners. The class with the most votes wins. Ties are broken by using the first occurrence of the highest sum of testing accuracies from voters of the class receiving the most votes.
 by its accuracy on the training data (as a percentage). All votes are then summed, and the class with the highest vote tally wins. This essentially represents the learner with the highest accuracy on the training data having the most influence in which class is selected. Ties are broken by using the first occurrence of the maximum vote value.
 training data is used for all final predictions. 8.2. Experimental setup employed to perform a comprehensive study on all aspects of ensemble learning for this application. Team configurations exhibiting the highest testing accuracy move on to the next round, where larger teams are constructed from the best teams from the previous round.
 individual machine learning algo rithms, each with a few different settings and initialization seeds, to determine which algorithms perform well in general for the data set. The best settings and corresponding 10-fold cross-validation testing accuracy for each algorithm are recorded, and the top five algorithms are selected to advance to the next round, consisting of teams of size two. settings are then run. This inherently includes homogeneous and heterogeneous team compositions. Similarly, the top teams of size two are selected based on combined testing accuracy. The same process takes place for teams of size four, where the top teams of size four are selected to advance to the final round of size eight teams. Once the round of size eight teams is complete, all results are compiled and a full study can take place.
 applying them to other real-world data sets, we have found that teams larger than size eight typically do not offer a significant accuracy versus cost tradeoff. What we found to be more important, as illustrated by Fig. 11 in the later section, is the diversity of the team. As team size scales up, diversity appears to drive the construction of a robust team of classifiers. Some classifiers exhibit built-in diversity due to their construction process (e.g., Random Forest), which we find consistently among the most successful classifier teams. With this said, experiments were performed with team sizes of 16 and 32; however, results were found to not offer a statistically significant improvement in performance. Average accuracy as team size increased exhibited a plateau at and beyond teams of size eight for this classification problem.
 studies is to study whether teams are better than an individual, combining decisions from multiple learners is useful, and that heterogeneous learning algorithms leads to higher task perfor-mance/accuracy. The following sections present experimental results and discuss general aspects of team learning discovered as part of this research. 9. Experimental results aspects of team learning from the subglacial water presence classification study. The notation used in the subsequent figures and analyses are abbreviations of learning algorithms. The abbre-viations for the 11 learning algorithms are Naive Bayes (NB) ( John and Langley, 1995 ), Decision Tree (DT) ( Quinlan, 1993 ), Instance-Based KNN (IBK) ( Aha et al., 1991 ), Neural Network (NN), Logistic Regression (LGR) ( le Cessie and van Houwelingen, 1992 ), Radial Basis Function Network (RBF), K* (KST) ( Cleary and Trigg, 1995 ),
Decision Table (DTB) ( Kohavi, 1995 ), RIPPER Rule Learner (JRP) ( Cohen, 1995 ), PART Rule Learner (PRT) ( Frank and Witten, 1998 ), and Random Forest (RFT) ( Breiman, 2001 ). The reader is referred to the referenced papers for the algorithms for detail on their underlying structure and theory. When listing team compositions, the number of learners for each learning algorithm of the team is listed, separated by  X  X   X   X  X . For example,  X  X 3ibk  X  2nn  X  2dt  X  1lgr X  X  represents a team of size eight composed of three IBK, two NN, two DT, and one LGR classifiers. The order of classifiers is indicated by the team X  X  listed configuration. Default WEKA classifier parameters were used for all experiments.

A total of 73 learning experiments were performed, including the variation of team size and composition. Experiments are broken down as follows: 11 size one, 19 size two, 24 size four, and 19 size eight 26 homogeneous and 47 heterogeneous teams 9.1. Team rankings
First, team compositions that are generally successful were investigated. Table 5 shows the best and worst 10 teams overall, based on testing accuracy, accompanied by the associated team sizes. Teams of size eight produce the highest team classification accuracy. In general, larger team sizes produce higher testing accuracies for this data set, while smaller teams produce the lowest testing accuracies. Certain pairings of learning algorithms perform very poorly in comparison to the majority of the teams as well. Increasing the number of learners in the team can help stabilize team accuracy. Team composition makes a difference in team accuracy, as each learning algorithm models the data in a different manner. Combining these different models in different ways can increase accuracy.

Two primary trends are apparent when analyzing the best performing teams: (1) heterogeneous teams consistently produce high team classification accuracy, and (2) larger teams (domi-nantly size eight in the top 10) consistently perform better. Smaller team size is indicated as a prominent trend for the worst performing teams. The majority of poor-performing teams are combinations of poorly performing individual classifiers, or homogeneous teams of the same poor individual classifier on this data set. There is a clear separation between those learning algorithms that offer high accuracy, and those that do not.
It can additionally be seen that multiple teams can produce the same testing accuracy. For example, a base team of six RFT classifiers is made better by adding one or two other classifiers. DT, KST, and DTB can be added to offer additional viewpoints and decisions, all producing the same combined testing accuracy. In this case, these classifiers are required to achieve higher accuracy than a team of only six RFT classifiers. The worst teams perform only 3% less than the best teams, which are largely composed of individual classifiers. Thus, it can be concluded that larger team sizes, coupled with heterogeneity, provide a small improvement for this binary classification problem. 9.2. Team diversity
Team diversity, or the composition of a team in terms of multiple heterogeneous learning algorithms, is central to a study of ensemble learning dynamics. As different learning algorithms model the data and error differently, combining them can be beneficial for increas-ing classification accuracy. Teams composed of the same learning algorithm may not experience these advantages, especially when collaboration is involved. Here, we specifically focus on comparing results from homogeneous and h eterogeneous teams. When dis-cussing a team X  X  diversity level, i t represents the number of unique learning algorithms in that team.

One method for analyzing team diversity is to observe its effect on testing accuracy as a function of the diversity level, as shown in Fig. 11 . No team existed which contained five, six, or seven unique learning algorithms, and the testing accuracies are assumed linear between diversity level four and eight. First, the tournament-style experimentation process produced many teams of diversity levels 1, 2, and 3. Teams see a steady increase in testing accuracy coupled with an increase in diversity up to a diversity level of three, after which accuracy levels off. Thus, a team consisting of three heterogeneous learning algorithms is desirable in this application, as it maximizes testing accuracy.
Further, heterogeneous teams (i.e., those containing more than one unique learning algorithm) perform 3% better on average compared to homogeneous teams. Homogeneous team composi-tions perform best at larger team sizes, and worst as individual classifiers. Increasing team diversity has proven necessary to offer an increase in testing accuracy. These results again stress the importance of team composition as a driving factor for higher classification accuracy. 9.3. Single versus multiple classifiers
A study of ensemble learning inherently involves the question,  X  X  X ow large should the ensemble be? X  X  Specifically, it is desirable to study how scaling the team size affects different dynamics (successful team composition, combined testing accuracy, etc.).
The objective is to study what can be gained by using multiple learning algorithms, as opposed to a single learning algorithm.
The following table sheds more light on these aspects, and is intended to supplement the results presented as part of other studies in this paper.

Table 6 lists the five most successful and least successful teams as a function of team size. This table shows the direct result of using a tournament-style experimentation setup, as the best individual learners are heavily used in all successful teams of larger sizes. A tradeoff does exist between team size and classi-fication accuracy. Team size inherently involves a difference in training and testing time. Some of the learning algorithms require more resources and time to perform their modeling effort. For example, decision trees do not require the memory space and time that neural networks do. The K* algorithm is the most memory-intensive algorithm utilized in the experiments. Thus, if similar accuracy can be achieved with a smaller team size and the application permits, it may not be efficient to utilize larger, more resource-intensive teams for a minimal amount of accuracy gain.

There exists a coupling between team size and combined testing accuracy: testing accuracy increases with team size. Again, evidence that RFT is the best individual classifier is apparent, also representing a strong base classifier with which to construct an ensemble. This suggests, for this application, that a strong base classifier is more important than team diversity. A heuristic which follows from these results is to construct a base homogeneous team of a strong classifier, and add one or two other potentially complementary classifiers. For large teams, this could translate to a heterogeneous mixture of homogeneous teams.
 generally do not follow this heuristic. Some of the poor teams exhibit diversity equality (team size divided by diversity level of 1 to 2). This was unexpected, as the theory of combining multiple heterogeneous learners as part of a highly diverse team of classifiers suggests higher accuracies and increased learning stability. In general, however, homogeneous teams offer compar-able, but slightly lower, combined testing accuracy. Teams of size five to seven were not tested as part of this study. 9.4. Decision combination method classifications from multiple learning algorithms to formulate the ensemble X  X  collective decision for an instance requiring classifica-tion. In the following table, each count for a combination method represents an experiment which resulted in that combination method providing the highest testing accuracy. Ties (in terms of combined decision testing accuracy from all learners) count as a win for each method. The reported win percentages have been normalized, as there are varying numbers of teams of different sizes and more heterogeneous teams, due to heterogeneous teams generally offering higher classification accuracy in the tournament-style experimentation process.
 methods over all experiments for this data set. In general, team size dictates the combination method to choose. Multiply, Aver-age, and their Selective variations are successful methods for these experiments. It was found that Max performs best for size two, Selective Average for size four, and Selective Multiply for size eight teams. Select Best, Majority Vote, and Weighted Vote performed comparatively poor for all team sizes. Max, Average, and Multiply are best for homogeneous teams, while
Selective methods perform best for heterogeneous teams. These results suggest that the Multiply method is superior for this classification problem, followed closely by Average and their
Selective variations. 9.5. Algorithm contribution
The contribution and success of individual machine learning algorithms were investigated. By analyzing a learning algorithm X  X  contribution to success, learning algorithms that are generally successful for this application, and those which also perform well together, can be extracted. If teams that contain a certain learning algorithm consistently perform better than others not containing that learning algorithm, then that algorithm can be considered valuable (or a major contributor) to a team X  X  success. This is measured in two primary ways. First, as shown in
Fig. 12 , average testing accuracy is compared for each learning algorithm over all teams in which that algorithm participated. On average, RFT, IBK, KST, and NN perform the best. Teams contain-ing NB, DTB, JRP, and RBF learners are less stable, as they can produce very poor classification accuracy when included in a team. On the other hand, learning algorithms such as RFT, IBK, and KST are stable learning algorithm choices, as their minimum team accuracies are still comparably high. Combining these algorithms is encouraged, to produce a team that has a slightly heterogeneous mixture of strong, complementary classifiers.
Second, as shown in Fig. 13 , the number of total teams which each learning algorithm participa ted in can be compared. This figure shows individual learning algorithm 10-fold cross-validated accura-cies, and learning algorithm participation over all team learning experiments as a percentage. The byproduct of the tournament-style experimental setup is the use of the best performing algorithms in more teams. It is also evident that the RFT, IBK, and KST classifiers are used in 20% more teams than the next best set of classifiers. This is a result of combining teams of size two to form teams of size four, and combining teams of size four to form teams of size eight. NB, DTB, JRP, and RBF classifiers were utilized least overall. 9.6. Classification comparison: 1999 X 2007
Using the established thresholds for the 1999 radar data and the classification model created for the 2007 radar data set, the accuracy of the 2007 water presence to the accepted 1999 ground truth results surrounding the GRIP and N-GRIP core sites can be compared. Fig. 14 presents the comparison of water presence classification for the geographically closest 1999 and 2007 flight segments. The model corresponding to the best team of classifiers (7RFT  X  1DT, 85.72% combined testing accuracy) was utilized to produce 2007 water presence.

The ice sheet X  X  bedrock interface topography is very similar between the two data sets, except where the 2007 flight line gets further away. The 2007 survey has much higher data resolution compared to the 1999 survey, and the icon size used to indicate water presence is not to scale. The GRIP and N-GRIP sites are accurately classified as dominantly frozen and wet, respectively. The majority of misclassified radar traces are in predominantly wet areas, occasionally being incorrectly classified over steeper bedrock slopes. The northernmost portion of the 2007 flight segment is also classified as containing several areas of subglacial water presence, which were not present at a similar scale in the 1999 data.
 subglacial water presence, utilizing 1999 ground truth data as a means to transfer water presence to 2007 radar data. These results demonstrate that pattern recognition techniques can be important tools for analyzing radar and other data from the Greenland ice sheet. By integrating more attributes about the ice sheet from various sensors and existing models, water presence indicators can be discovered and employed in the creation of maps showing the inferred subglacial state. 10. Conclusion fication problem, attempting to associate remotely sensed radar data properties to subglacial water presence and transfer the model to different radar systems. To our knowledge, this is the first attempt to classify water presence from radar data acquired over the polar regions using multiple machine learning algo-rithms. Knowledge of ice dynamics, ground truth core data, and attribute extraction from airborne radar returns have allowed us to learn a model of subglacial water presence to facilitate in the creation of water presence maps of the entire Greenland ice sheet.
Using methods to achieve radar independence, attributes were extracted to create a binary (wet or frozen) data set from radar data acquired in 1999 and 2007. It is planned to extend this concept to Antarctic radar data as well, such that water presence maps can be generated and monitored from new radar data as they become available.
 general study could, in part, contribute to the similar accuracy of most classifier ensembles. Specifically, the radar data were acquired over the ice sheet eight years apart. With global warm-ing and climate change being of increased focus and research in recent years, this time difference could have allowed for small changes on top of, within, and at the base of the ice. These changes could, in turn, affect the radar data and therefore the method utilized to achieve radar independence in a manner which affects classification. For instance, temporal temperature, snow accumulation, and/or melting alter the ice sheet and there-fore the data recorded by the sensor. Radar settings and hardware differences may also contribute to small-scale changes in the radar data and measured intensity at the subglacial interface.
However, at the locations where these data were acquired, subglacial water presence is assumed to be relatively static over the time difference between data sets.

Architecture, 86 % classification accuracy was obtained. Several heuristics were discussed for constructing teams of multiple classifiers for this application. A tradeoff between team size and team composition, including the time required to complete the learning process, also became apparent. The Random Forest (RFT) classifier was the best individual classifier, as well as a necessary base for increased classification accuracy in heterogeneous teams.
Strong teams were composed of a base homogeneous team of RFT classifiers accompanied by one or two other classifiers which were complementary. Larger teams proved to be well suited for this data set, where heterogeneous teams with low diversity performed best on average. Heterogeneous algorithms offer the advantage to integrate different error models. Contrary to our expectations, equally diverse teams (team size divided by diver-sity level of 1 to 2) were not among the best for this data set.
There are many facets of team learning that require further investigation. Some real-world applications are restricted by lim-itations of computing power, working memory, and decision time.
Additional future work could be performed which focuses on groups of classifiers that have various limitations on memory, time, and processing. General heuristics could be produced for specific system constraints for real-world problems. For example, missile defense or target classification requires real-time classifica-tion. Space applications also have restrictions such as computation power and memory, due to the radiation-hardening constraint on spaceflight processors and systems. Such problems cannot always leverage traditional ensemble learning and conventional machine learning algorithms.

Team learning introduces social interaction, team-building, and knowledge transfer that are inherent in human teams.
In order to achieve true team-based learning and collaboration, models are needed for how humans cognitively learn, interact, and teach one another. New and novel machine learning methods are also needed for information sharing and collaboration. Incor-porating concepts such as trust, confidence, and negotiation, along with using them to intelligently reason to arrive at a collective decision, may allow for higher accuracy classification or task performance. Investigating other next-generation learning architectures, such as Deep Learning (incorporate both substance and meaning) and hierarchical systems, is also important to advance the state-of-the-art in multi-agent machine learning. Acknowledgments Author C.M. Gifford performed this research while part of the
Center for Remote Sensing of Ice Sheets (CReSIS) at the University of Kansas. The authors would like to thank Anthony Hoch at the
CReSIS for helpful discussions about radar theory, and assistance in constructing the data set for this study. This material is based upon work supported by the National Science Foundation under
Grant No. ANT-0424589. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.
 References
