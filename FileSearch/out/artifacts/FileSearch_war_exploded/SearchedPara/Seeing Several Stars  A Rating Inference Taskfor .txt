 As the World Wide Web rapidly grows, a huge number of online documents are easily accessible on the Web. Finding in formation relevant to user needs has become increasingly important. The most important information on the Web is usually contained in the text. We obtain a huge number of review documents that include user X  X  opinions for produc ts. Buying products, users usually sur-vey the product reviews. More precise and effective methods for evaluating the products are useful for users. Many rese archers have recently studied extraction and classification of opinions [6, 10, 11, 12, 14, 15].

There are many research areas for sentiment analysis; extraction of sentiment expressions, identification of sentiment polarity of sentences, classification of review documents and so on. In this paper we address a new sentiment analysis task of review documents. Most of existing studies for classification of review documents have handled two polarities: positive and negative opinions [10, 12]. On the other hand, several researchers have challenged not only p/n classification but also rating inference, namely seeing stars in a review document [8, 9]. We also handle a rating inference task in this paper.

The previous studies, p/n classification and rating inference, contain a prob-lem; a document includes only one polarity (or stars). They did not discuss a task handling several polarities in a documen t. However, reviewers judge not only the overall polarity for a product but also details for it. For example, they are  X  X er-formance X ,  X  X ser-friendliness X  and  X  X ortability X  for laptop PCs and  X  X cript X ,  X  X asting X  and  X  X usic X  for movies.
 In this paper we deal with a document cont aining several sentiment polarities. It is a new task for sentiment analysis: seeing several stars in a document. This is a primary experiment for the task. To estimate several ratings in a document is beneficial for users. Furthermore it is important for sentiment analysis tasks to extract words or phrases that relate to each polarity (evaluation criteria). Zhuang et al. have reported a method of movie review mining and summariza-tion using the discovered p/n information [15]. If significant words or phrases for an evaluation criteria and their strength as positive or negative opinions are ex-tracted, a system with knowledge that consists of them can recommend products for users appropriately. For example, the system can output a detailed summary from review documents: it generates not only a simple summary  X  X his movie is good X , but also a more detailed summary  X  X he story of this movie is excellent (five stars), but the music might be substandard (two stars) X .

In this paper we compare several methods for the rating inference task. Also we compare some feature sets for SVR in this task and discuss solutions for the improvement of accuracy. The experimental results are useful for new researchers who try this new task. There are many review documents of various products on the Web. In this paper we handle review documents about game softwares. Figure 1 shows an example of a review document. The review documents consist of evaluation criteria, their ratings, positive opinions, negative opinions and comments for a product. The number of evaluation criteria is 7:  X  X riginality X ,  X  X raphics X ,  X  X usic X ,  X  X ddic-tion X ,  X  X atisfaction X ,  X  X omfort X , and  X  X ifficulty X . The range of the ratings, e.g. stars, is zero to six points.

We extract review documents from a Web site 1 . The site establishes a guide-line for contributions of reviews. In addition, the reviews are checked by the administrator of the site. As a result, the reviews unfitting for the guideline are rejected. Therefore the documents on the site are good quality reviews. 3.1 The Methods In this section we describe 4 methods, which are SVM, SVR, Maximum entropy and a similarity based method, for inferring the ratings in a document. SVM and SVR. SVMs are a machine learning algorithm that was introduced ME. Maximum entropy modeling (ME) is one of the best techniques for natural SIM. The 4th method is based on a similarity measure. We use the cos measure 3.2 Feature Selection For the features of the methods, we use words appearing in positive and negative opinions in review documents. We do not use words in comment areas because the accuracy with them in a preliminary experiment was lower than that without them. Here we distinguish words in the positive opinion areas and the negative opinion areas. In other words, for a word w i , the word in the positive opinion areas is w p i and the word in the negative opinion areas is w n i . A vector of an evaluation criterion y for a document d x is as follows: where j is the number of words appearing in review documents. We select words belonging to  X  X oun X ,  X  X erb X ,  X  X djective X  and  X  X dverb X . We use ChaSen for the morphological analysis 4 . The value of the features is based on the word frequency.
Next we consider two extensions for th e feature selection. One approach is to use more complex information. In this paper, we use a word sequential pattern between two words in each sentence, namely cooccurrence. In the pattern ex-traction, we allow a skip between words. We extract word pairs within a length that we define. For example, we obtain the patterns  X  X ighting::WiFi, Fight-ing::excited, Fighting::me, WiFi::excited, WiFi::me, excited::me X  from a sentence  X  X ighting with WiFi excited me. X 
Another approach for improvement of t he accuracy of a classifier is to select effective and significant features for the feature space. Furthermore it seems un-likely that all words in a document contri bute to all evaluation criteria. In other words some words that are significant to estimate the rating of an evaluation criterion exist in a review document. To extract the words, we compute a confi-dence measure of each word. The confidence measure in this paper is variance of words concerning each evaluation criter ion. We measure whether a word appears frequently with the same point regarding an evaluation criterion. It is computed as follows: where c j is an evaluation criterion. m and n are the document frequency ( df )of aword w (or a word pair) and the number of documents respectively. real ( d i ,c j ) and ave ( w c j )aretheactualratingof c j in d i and the average score of w for c j . We use w of which the var is a threshold or less.
 Furthermore we apply two conditions to the feature selection.
 Frequency (F). The frequency of a word is n times or more.
 Evaluation value (E). If a word w appears in  X  X ositive opinion area X , the In this section, we explain datasets and criteria for the experiment first. Then we evaluate our method with a dataset and discuss the experimental results. 4.1 Dataset and Criteria for the Experiment We evaluated this new sentiment analysis task with a dataset that consists of 1114 review documents that consist of different kinds of game softwares such as RPGs and action games of Nintendo DS, namely a mixed dataset. In this experiment we evaluated the dataset with 5-fold cross-validation.

In this experiment, we evaluated the outputs of each method with the follow-ing criteria: the mean squared error (MSE) between actual ratings and outputs of each method, the standard deviation (SD) of the MSE, and the accuracy . The mean squared error (MSE) is computed as follows: where i and j denote a review document and an evaluation criterion in the document respectively. out and real are the output of a method and the actual rating in a document respectively. We converted the outputs of the SVR and the similarity based method into integral value with half adjust because it was continuous. The MSE is one of important criteria for the rating inference task because not all mistakes of estimation with the methods are equal. For example, assume that the actual rating of a criterion is 4. In this situation, the mistake of estimating it as 3 is better than the mistake of estimating it as 1.
In this experiment, we used two types o f accuracy. The first accuracy is simple accuracy, that is to say the correspondence between real ratings and outputs. The second one is PNN accuracy (P ositive-Neutral-Negativ e). For the PNN accuracy, we defined 4 and 5 points as  X  X ositive X , 3 points as  X  X eutral X  and 0, 1, 2 points as  X  X egative X . 4.2 Results First we compared the methods with bag-of-words (Bows) features only. We ran the SVR and SVM with all default parameters in this experiment. For the Maximum entropy we estimate parameters by using the generalized iterative scaling algorithm.

Table 1 shows the result.  X  X ll-3 X  in the table is the MSE in the assumption that the ratings of all criteria are 3.  X  X ve X  is the MSE computed from actual ratings and average values of each evaluation criterion in the training data. The average values are discretized fo r the MSE computation. These MSEs are baselines for this task. As you can see, al l methods outperformed the baselines 5 . In this experiment, the SVMs produced the best accuracy. However the MSE of the SVR was the smallest of them. The SD of the SVR was also small. As a result, we arrived at the conclusion that the SVR was the most suitable in this experiment because the MSE is the most important criterion in this task.
Next we compared the resul ts concerning the extensi ons for the feature se-lection, namely word sequential patterns and a confidence measure var based on the variance. In this experiment, we used the SVR only for the evaluation. Here we applied the extension with var to word sequential patterns only. Table 2 shows the comparison of the value of var . In this experiment, the length for the pattern extraction was 4. The value of the condition of the frequency (F) in Section 3.2 was 1 6 . Table 3 shows the comparison of the length for the pattern extraction. The value of the var was 0.5. As you can see, there is no difference in the MSE and the accuracy.

Here we need to discuss a problem for this task. In this task, there is a possi-bility that humans even can not infer a rating in a document because a document contains many evaluation cr iteria. In other words, words or phrases for an evalua-tion criterion do not exist in a document occasionally. Therefore we inquired into 30 documents selected from review documents randomly. We judged whether we could infer each criterio n in the documents or not. The criterion of the judg-ment was whether the document contained words or phrases for an evaluation criterion or not 7 . As a result, approximately 75% of all criteria could be inferred by humans. We think that this is one reaso n that the accuracy was low. However, the judgment of the possibility of inference was examined by one test subject only. We need to discuss the reliability of the judgment process with some test subjects by using a conco rdance rate such as the Kappa coefficient [2]. 4.3 Discussion In this section we discuss this task on the basis of the experimental results. The accuracy in the experiment was insufficient; approximately 41% for the 5-fold cross-validation. These res ults show the difficulty of this  X  X eeing several stars X  task (6 grades for 7 criteria ). We need to discuss the im provement of the accuracy and the MSE. We think that dictionaries obtained from opinion extraction or word polarity estimation tasks [5, 6, 14] are useful to infer the ratings in our task.
 In this experiment, we used SVR to estimate the ratings in a document. The SVR is often utilized in rating inference tasks [8, 9]. However Koppel and Schler [7] have discussed a problem of use of regr ession for multi-class classification tasks and proposed a method based on optimal stacks of binary classifiers. Pang et al. [9] have proposed a method based on a metric labeling formulation for the rating inference problem. The results of these studies denote that SVR is not always the best classifier for this task. We need to consider other methods for the improvement of the accuracy. W e have proposed high accuracy classi-fiers for a p/n classification task [11]. The method incorporated three classifiers: SVMs, Maximum Entropy and score calculation. In the movie review classifica-tion task [10], this multiple classifier improved the accuracy as compared with the single classifiers. Applying this method to this task is one of our future work.
 The size of the dataset in this experiment was not large: 1114 documents. To generate a high accuracy classifier, we need a large amount of training data. Goldberg and Zhu [3] have argued the significance of training data acquisition from unlabeled data. As an additional experiment, we evaluated the SVR-based method with bows and patterns based on the value of var computed from 1114 review documents 8 . As a result, the accuracy increased by 11% 9 . We think that one reason for the improvement is the increase of training data for the var calculation. Therefore, we need to consider a trainin gdataextractionmethod. In this paper we described a novel sentim ent analysis task of rating inference. The documents in this task include 7 evaluation criteria that contain 6 rating points: seeing several stars in a document. As a primary experiment for this task we inferred the ratings in each document and compared some machine learning techniques. As a result, the support vect or regression (SVR) produced the best performance. We also explained the feature selection based on variance of words and the use of word sequential patterns. The experimental results show that this is a difficult task of sentiment analysis and we need more training data. Future work includes (1) extraction of mo re effective features for a classifier, (2) evaluation with other classification methods.

