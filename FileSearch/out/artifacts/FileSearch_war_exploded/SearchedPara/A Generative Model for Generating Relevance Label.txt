 Lack of high quality relevance labels is a common challenge in the early stage of search engine development. In media search, due to the high recruiting and training cost, the labeling process is usually conducted by a small number of human judges. Consequently, the generated labels are often limited and biased. On the contrary, the click data that is extracted from a large population of real users is massive and less biased. However, the click data also contains considerable noise. Therefore, more and more researchers have begun to focus on combining those two resources to generate a better ground-truth approximation. In this paper, we present a novel method of generating the relevance labels for media search. The method is based on a generative model that considers human judgment, position, and click status as observations generated from a hidden relevance with multinomial prior. The model considers the position bias with a requirement that the click status depends on both the hidden relevance and the position. We infer the model parameters by using a Gibbs sampling procedure with hyper-parameter optimization. From experiments on the Xbox X  X  data, the newly inferred relevance labels significantly increase the data volume for ranker training and have demonstrated superior performance compared to using the limited human labels only, the click-through-rates only, and the heuristic combination of the two. H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval; I.2.6 [Artificial Intelligence]: Learning Algorithms, Experimentation Search; relevance; labeling; generative model To generate the relevance labels in search problems is essentially to approximate the ground truth, which can only be revealed from the information of many users. To be able to train a good ranker, the most important thing is to have enough labeled query-document pairs that truly reflect the users X  preference. However, in reality, it X  X  very difficult to obtain a large number of high quality labels directly from users especially when a search engine is in its early phase. Therefore a common approach is to employ a small number of human judges to do the labeling work under limited budget. One problem is that it's hard to generate enough labeled data for ranker training. Another problem is that the labels provided by a small number of judges are biased toward a small group of users. Compared with the human labels, the click-logs are cheap, scalable, and much less biased. For these reasons, mining the click-logs for relevance information has been an appealing topic in information retrieval and tremendous research efforts have been put into modeling click data during the past decade. Although the click-logs are very informative, they are also noisy. Chen et al. [3] presented an interesting statistical summary of a combined web-search data that contains 474,185 human-judged query-document pairs and 16.18 million clicks on these query-document pairs. The judgments have five levels: Perfect , Excellent , Good , Fair , and Bad . Among all the clicks the portion under Excellent is less than 35% and the portion under Bad or Fair is more than 28%. This indicates that the reasons for a click event are much more than just the relevance. Probably one of the mostly discussed click factors is the position bias, which was first noticed by Granka et al. [7] and many researchers such as Richardson et al. [12] and Craswell et al. [5] quickly proposed methods to correct it. Later on more complicated models [6][8][9][2] were proposed to address the position bias in various forms. While those models intensively consider the position factor, they don X  X  handle other potential reasons for clicks. In our search practices in the media domain, we found that a noticeable number of clicks are not based on relevance but on the popularity of the returned content, especially when the irrelevant content is returned due to the poor quality of the search engine. We also found that some users are more likely to click than others and a lot of clicks seem to be very random. For a very relevant result, a user might not click simply because it has been read before and many reasons for click/nonclick, it may not be enough to depend solely on the click-logs for relevance retrial when it X  X  hard to distinguish the high-quality clicks that lead to user consumption and the low-quality clicks that look like random browsing. A solution is to utilize the limited human judgments and find a way to combine them with the click-logs. The recent noise-aware click model [3] is such an attempt. There can be many noise-aware models, as the generalizations of previous click models. Based on the literature and our practice in media searches, we also made an effort to combine the usage of human judgments and click-logs. Just like the peer reports, we also found that the human labels are of higher quality than the clicks. However, due to the fact that there are only a very limited number of human judges, it has been found that even if a result is judged at the highest relevance level at the beginning, it can still be confirmed to be are easily identified, the human judgments should not be treated as the ground truth, rather it may be treated as observations in addition to the click-logs. In practice we observed that the human judgment and the click-through-rate (CTR) can correct each other. For example, if a result is very relevant, then the chance that both the human rating and the CTR are low is less than the chance that at least one of them is high. If one is observed as high and the other is observed as low, then probably the higher one shouldn X  X  be that high and the lower one shouldn X  X  be that low. Starting from here, we explored the idea of adding the human judgment as less noisy observation and imagining there is a true relevance behind. We formulated the idea into a new generative model that views the observable human judgment, position, and click status as being generated from the hidden relevance. The rest of the paper is organized as follows: we first formulate our generative model; we then present our Gibbs sampling method with hyper-parameter optimization for inference; after introducing the evaluation metrics, we present some experimental test results of applying our inferred relevance to the Xbox X  X  ranker training; finally we conclude by highlighting our model and pointing out the directions of future work. The basic idea is that for a query-document pair in a query session, the human judgment of the relevance, the position of the document, and the click-status of the document are all generated by the hidden true relevance. We require that the click-status of the document also depends on its position in the session. In addition, the hidden true relevance is a multinomial sample. In our notations, we let ( X  , , ) denote the triplet of the document under the -th (out of ) session of the query , where  X   X  {1, ... , } ,  X  {1, ... , } , and  X  {1,0} respectively are the human judgment, the position, and the click status. is number of human judgment categories including the missing judgment and is the position truncation level. We let  X  {1, ... , } denote the hidden true relevance level that has the multinomial prior parameterized by = ( , ... , ) , where is number of the true relevance levels and level represents the -th highest level. The following Figure 1 illustrates the structure of the model. Figure 1: The graphical model representation of the (human judgment, position, click)-generation In practice, the human judgments are very sparse. Hence it X  X  commonly observed that a lot of  X  are empty. Also in most sessions, there might be just one single rating as Excellent , Good , or Bad etc. However, the data of ( , ) is abundant. Our goal is to infer the parameters from the observations  X  = ( X  ,  X  , ... ) , = ( , , ... ) , and = ( , , ... ) . This can be achieved through the maximum likelihood estimation method. Under the structure of our model, the joint likelihood of observing  X  , , can be computed via conditioning on the latent relevance sequence = ( , , ... ) . That is ! X  , , | = # !( X  , , |)!(| ) $ = # % !( X  | ) % !( , | ) % !( | ) $ = # % &amp;!( X  | )!( , | )!( | )' $ = % # !( X  |)!( , |) $ $( . (1) Hence the joint likelihood is determined by several basic probabilities. They include !( X  X ) : the probability of observing human judgment  X  given the relevance level , ! ( , | ) : the probability of observing position and click status given the relevance level , and $ : the probability of the relevance level . Note that !( X  X ) measures the quality of the human judgment, ! ( , | ) can be factorized into the product of !(|, ) and !(|) , which respectively measures the position bias and the search engine X  X  quality. To maximize the joint likelihood (1), our strategy is a Gibbs sampling procedure that is similar to the one used in [11][13], but without using the Dirichlet prior for the purpose of simplicity. According to our model, the full conditional posterior for sampling given \ ,  X  , , , and is ! = *\ ,  X  , , , =  X  !( = , \ ,  X  , , | ) = !( X  , , | = , \ )!( = , \ | ) We start from initial guesses of !( X  X ) , ! ( , | ) , and and sample using the full conditional posterior (2). Given , we can re-estimate !( X  X ) , ! ( , | ) , and via smoothed counting. The smoothing is based on the pre-counts that are the results of aggregating over all the -pairs. After a certain number of sweepings, we perform a hyper-parameter optimization step for . The optimization is to minimize a loss function of , ... , under the constraints: 0  X  $  X  1 for = 1, ... ,  X  1 and $(  X  1 . By substituting the estimates ! 5 ( X  |) and ! 5 ( , |) into (1) and taking the negative of the natural logarithm, we obtain the loss function of , ... , 2 as 6 0 ( , ... , 2 ) = A nice property of this loss function is that it X  X  convex. Hence we can apply the gradient descent method. Let : &gt; 0 denote the learning rate (usually 0.01). We start from a feasible . If 0  X  $(  X  1 we then perform the update: $  X  $ &lt; for all = 1, ... ,  X  1 . Since there could be hundreds of thousands of -pairs, a practical implementation is to use the map-reduce [4] for parallelization. Practically the quality of the labels is tested by training a ranker and evaluating its performance on the test data sets. We consider the future performance by click-metrics and NDCG (normalized discounted cumulative gain) [10] measurements. Ideally, a good ranker should have good test performance under both the click-metrics and NDCG measurements if the click-signals are consistent with the human judgments. We use the click-happening-rate (CHR) and the last-click-rate (LCR) of the top positions for = 1,2,4 as our click-metrics. We also use the NDCG of the top positions for = 1,2,4 as our NDCG measurements. The CHR of the top positions is calculated as The LCR of the top positions is calculated as The NDCG of the top positions is calculated as NDCG J = qr p , qr p , ... , which respectively are the relevance gains of the documents at positions 1,2, ... under the query that does not have all zero-gain results, and u is total number of such queries. In this section, we present some experimental results of comparing the relevance labels generated by our method with several other types of relevance labels. We used an Xbox X  X  data set that contains the human judgments and click-logs from February 2013 to May 2013 to generate the training data for deriving rankers. Among 260,126 unique query-document pairs that appeared in no less than 30 sessions, about 16.9% were judged with three levels of judgments: Excellent , Good , Bad . We built six training data sets with each of them having the same feature set but different label type. They are named and explained as the follows: ii. l CTR: Training data with l CTR labels. The l CTR is iii. t CTR: Training data with t CTR labels. The t CTR is iv. t CTR  X  HJ: Training data with t CTR  X  HJ labels. The v. GM2: Training data with GM2 labels. The labels are vi. GM3: Training data with GM3 labels. The labels are For the training data with the HJ labels, there are 6838 unique queries and 43961 unique query-document pairs; for each of the other five sets of training data, there are 31,075 unique queries and 260,126 unique query-document pairs. For calculating the t CTR, we truncate at position 50. For inferring , we also truncate at position 50 so that our generative model can fit the same range of positions. For ranker training, we used the same learning algorithm as LambdaRank [1]. After the six rankers were trained, we used the click-logs in June 2013 to test the future click-performance and used a set of human judgments that contains 6654 unique queries and 36,000 unique query-document pairs in movie and TV domains to test the future NDCG-performance. Those query-document pairs were judged from June 2013 to August 2013. For each ranker, to calculate both the click-metrics and the NDCG values, the documents under each query are ordered by the corresponding ranker-scores. To score each query-document pair in the test sets, the same feature set in the training data was used. The test results are summarized in the following Tables 1, 2, where the performance of HJ at the top 1 position is treated as baseline represented by value 1 and all the other values are the relative ones. An observation is that our model in trinary case ( K = 3) has the superior performance at the top 1 position under both the click metrics and the NDCG measures. This is very encouraging since the NDCG values drop a lot possibly because for any query-document pair, the Excellent with gain = 15 and the Good with gain = 7 in human labels are grouped together as relevant so that the gain 15 and the gain 7 are not differentiated. However, it still has the second best click performance at the top 1 position and its values of the click-metrics are very good at the top 2 and top 4 positions compared to the other label types. The l CTR in overall has the worst performance that indicates certain effect of noise. The t CTR has much better click-performance, but its NDCG-performance is relatively poor. This implies that there exists some discrepancy between the human judgments and the click signals. The t CTR  X  HJ has better-balanced results in that the click-performance is very similar to that of the t CTR, but the NDCG-performance is much better. The benefit of combining human judgments and click signals was confirmed again by our generative model under K = 2 and K = 3. The purpose of the t CTR  X  HJ is the same as the goal of our generative model, but it X  X  more like an ad hoc heuristic. On the other hand, our generative model is based on a solid foundation of Bayesian statistics. In this paper, we introduce a novel method of generating relevance labels from human judgments and click-logs. The method is based on a generative model that views the human judgment, the position, and the click status as the observations generated from the hidden relevance. Additionally, the latent relevance level is assumed to have the multinomial prior. Hence the model is Bayesian and can be inferred using the Gibbs sampling procedure with the hyper-parameter optimization steps. The model considers the relevance, the quality of the human judgment, the position bias, and the quality of the search engine behind. We performed experiments on the Xbox X  X  data and generated relevance labels to construct the training data that was significantly larger than the training data with only human labels. We also derived a superior ranker measured by the future click-performance and NDCG metrics. Our ranker comparison is fair in the sense that in the ranker training, we kept the same feature set and the same learning algorithm as LambdaRank and in the ranker testing, we used the same future click data and the same future human judgment data under the same feature set. We found superior performance of our method at the top 1 position, which is very important in Xbox X  X  search. Compared with the ad hoc heuristics of combining human judgments and click information, our method is built on a solid statistical foundation and can be generalized to incorporate more click factors. Our suggestion for immediate next work is to add the popularity and recentness of the returned content as observable factors that affect both the human judgments and the clicks into the probabilistic graphical model. We thank Microsoft X  X  Aeather team and Cosmos team for providing powerful computing tools and resources. We also thank those colleagues in Microsoft X  X  IPE media discovery team for their helps in preparing the data. [1] Burges, C. J. 2010. From ranknet to lambdarank to [2] Chapelle, O., &amp; Zhang, Y. (2009, April). A dynamic [3] Chen, W., Wang, D., Zhang, Y., Chen, Z., Singla, A., &amp; [4] Chu, C. T., Kim, S. K., Lin, Y. A., Yu, Y., Bradski, G., Ng, [5] Craswell, N., Zoeter, O., Taylor, M., &amp; Ramsey, B. (2008, [6] Dupret, G. E., &amp; Piwowarski, B. (2008, July). A user [7] Granka, L. A., Joachims, T., &amp; Gay, G. (2004, July). Eye-[8] Guo, F., Liu, C., &amp; Wang, Y. M. (2009, February). Efficient [9] Guo, F., Liu, C., Kannan, A., Minka, T., Taylor, M., Wang, [10] J X rvelin, K., &amp; Kek X l X inen, J. 2002. Cumulated gain-based [11] McCallum, A., &amp; Kachites, A. 2002.  X  X ALLET: A [12] Richardson, M., Dominowska, E., &amp; Ragno, R. (2007, May). [13] Yao, L., Mimno, D., &amp; McCallum, A. 2009. Efficient 
