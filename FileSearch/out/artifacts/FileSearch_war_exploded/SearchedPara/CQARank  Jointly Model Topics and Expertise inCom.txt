
Community Question Answering (CQA) websites, where peo-ple share expertise on open platforms, have become large reposi-tories of valuable knowledge. To bring the best value out of these knowledge repositories, it is critically important for CQA services to know how to find the right experts, retrieve archived similar questions and recommend best answers to new questions. To tackle this cluster of closely related problems in a principled approach, we proposed Topic Expertise Model (TEM), a novel probabilistic gen-erative model with GMM hybrid, to jointly model topics and exper-tise by integrating textual content model and link structure analysis. Based on TEM results, we proposed CQARank to measure user in-terests and expertise score under different topics. Leveraging the question answering history based on long-term community reviews and voting, our method could find experts with both similar topi-cal preference and high topical expertise. Experiments carried out on Stack Overflow data, the largest CQA focused on computer pro-gramming, show that our method achieves significant improvement over existing methods on multiple metrics.

H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  information filtering, selection process, retrieval models ; H.3.5 [ Information Storage and Retrieval ]: On-line In-formation Services X  Web-based services Algorithms, Experimentation, Performance
Community Question Answering; Latent Topic Modelling; Gaus-sian Mixture Model; Expert Recommendation; Link Analysis
The recent boom of Web 2.0 has seen the emergence and flour-ishing of many knowledge sharing community services such as Wikipedia 1 , Stack Overflow 2 and Quora 3 . The huge success of the concept of Community Question Answering(CQA), which enables people to post questions and answers in various domains, drives home the enormous power of online community activities in satis-fying users X  professional and personal knowledge quest.
However, existing question answering mechanism in CQA sites still falls short of users X  expectation for several reasons: (1) Poor expertise matching : A new question, in many cases, may not find its way to the right people with the best-matching interest and ability to answer it, resulting in suboptimal answers and pro-longed latency. (2) Low-quality answers: CQA sites may contain low-quality answers such as mischievous answers and spams [16]. These answers often receive low ratings or voting from community members. (3) Under-utilized archived questions: Many ques-tions from different users are in fact similar. Before posting a new question, a user may benefit from browsing related archived questions and their answers first. Not surprisingly, these issues are closely related. In fact, a common fundamental question underly-ing all these tasks is how to model topics and expertise in CQA sites.

Previous research efforts along this line include expert user min-ing [35, 5], relevant answer retrieval [2, 16] and similar question finding [33, 28, 15]. In this paper, our contribution is to push the research frontier along two dimensions: (1) Horizontally, we pro-pose to jointly model topics and expertise in a unified framework; and (2) Vertically, we achieve better understanding of both user topical interest and expertise by leveraging tagging and voting in-formation, important pieces of information that have so far been neglected in the modeling.
 Our Contributions First, to the best of our knowledge, we propose the first extensive study to jointly model topics and expertise. Traditionally, topics and expertise have been modeled separately. On one hand, for top-ics, latent topic models such as LDA[4], when applied to CQA, can measure the semantic similarity between questions and answers, and thus help find relevant answers or related questions given a new question. They can also model a user X  X  topical interests based on the user X  X  posting history, and hence match users and questions based on their topical similarity. On the other hand, for expertise, each user X  X  ability in answering questions can be modeled as an expertise level, by which we can better recommend candidate an-swerers. By modeling the relationships among users, questions and answers in CQA as a linked network, existing work often relies on link analysis techniques such as PageRank[24] and HITS[19] to find authoritative users. http://www.wikipedia.org/ http://www.stackoverflow.com/ http://www.quora.com/ questions.

Despite their success in each aspect, there is evidently a strong need in real-life CQA services to integrate these two aspects to-gether to enhance user experience. After all, no one is expert in all topical interests, which means one X  X  expertise level should be eval-uated with respect to the corresponding topics. On other hand, ev-ery new question falls into some particular topics, and they should be routed to answerers interested in those particular topics with the right level of expertise. We take both user topical interest and ex-pertise evaluation into our model, enabling our method to find ex-perts with both similar topical preference and matching topical ex-pertise.

Secondly, we achieve better understanding of both user topical interest and expertise by leveraging tagging and voting informa-tion. Since both topics and expertise are latent factors, i.e. we do not directly observe their values from CQA sites, existing work solve their inference based on their textual content and the link-age structure among them. However, we notice that two important types of information have not been well utilized: (I) Tagging infor-mation  X  Tags are important user-generated category information for many Q&amp;A communities, e.g., technical forums, that achieves fine-grained and dynamic topic representation. Users who use a particular tag when posting questions or answers might prefer topic summaries most relevant to that tag[29]. Consequently, incorporat-ing tags of questions and answers into textual content aids in better discovery of user topical interest. (II) Voting information  X  Votes indicate a CQA community X  X  long term review for a given user X  X  expertise level under a specific topic [1]. Users with high expertise tend to receive high votes for their Q&amp;A posts. This motivates us to exploit the votes for a user given specific topics to model user topical expertise.

We propose a probabilistic Topic Expertise Model (TEM) which uses tagging information to help learn topics and a Gaussian mix-ture hybrid to model voting information . Based on the model re-sults of TEM, we propose CQARank, an extension of PageRank algorithm, to aggregate user topical expertise base on Q&amp;A link structures, combining both textual content model results and link structure to simultaneously measure user topical expertise and in-terests.

Finally, we perform a thorough experimental study on a large real data set from Stack Overflow, the largest CQA focused on com-puter programming. The evaluation results show that CQARank achieves significant improvement over existing methods on multi-ple metrics.
 Roadmap. The rest of our paper is organized as follows. We give our method overview in Section 2. In Section 3 we define sev-eral important notations and present our Topic Expertise Model for jointly modelling user topical interest and expertise. In Section 4 we propose CQARank to combine both textual content model re-sults in Section 3 and link structure to estimate user expertise and interests under various topics. Section 5 is a systematic experimen-tal analysis using real data from Stack Overflow. Section 6 is on related work and we conclude our study in Section 7.
In this section, we provide an overview of our method referred as CQARank, which is shown in Figure 1. We first introduce some concepts.
 User: We use user to refer to the askers and answerers in CQA. Table 1 shows a snapshot of typical Q&amp;A posts with votes and tags in Stack Overflow. Every question has a tag set assigned by the asker. Both questions and answers have vote scores given by users in CQA. Users can vote-up or vote-down posts. The value of vote score equals the difference between times of vote-up and vote-down.
 Topical Interest: We use Topical Interest to refer to user prefer-ence for specific topics in CQA. For example, some users prefer to post content related to  X  X ava X , while others are more interested in  X  X atabase X .
 Topical Expertise: We use Topical Expertise to refer to their level of expertise on specific topics in CQA. Different users have dif-Table 1: Sample Q&amp;A posts with tags and votes in Stack Overflow. ferent topical expertise. Moreover, one user could have different expertise levels for different topics. For example, a user may be a guru for the  X  X ava X  topic but a novice for  X  X atlab X .
 Q&amp;A Graph: We use Q&amp;A Graph to refer to the network based on user posting behavior in CQA. Nodes denote users and a directed edge exists between two users if one of them has answered ques-tions by the other, where the edge direction is from the asker to the answerer.
 We first construct a Q&amp;A graph from user posting behavior in CQA corpus. We then jointly model Q&amp;A textual content with votes and tags using our probabilistic Topic Expertise Model. Fi-nally, we apply our CQARank to combine learning results from TEM with link analysis of Q&amp;A graph to discover user topical in-terests and expertise. For each topic, different users exhibit differ-ent topical interests and expertise in Q&amp;A graph, so we get user lists ranked by their interests and expertise. We also have top tags and words for each topic as model results. For new questions, us-ing recommendation score functions, we process the model outputs along with the question to generate ranked experts, answers and similar questions.

In Section 3, we will explain in detail how we jointly model top-ics and expertise in CQA with a generate probabilistic model with GMM hybrid. In Section 4, we will present CQARank which com-bines learning results of TEM with link structure analysis to make recommendations for given new questions. The recommendation score function for each output is explained in Section 5.3.
We now present Topic Expertise Model(TEM) to jointly model user topical interests and expertise. Table 2 shows the set of nota-tions and descriptions of our model parameters.

In our model, the user  X  X opical expertise X  e is the level of knowl-edge and ability of a user u under a topic z . To model this informa-tion, we assume there exist E expertise levels, each with a Gaussian distribution on vote scores. The reason why we choose Gaussian distribution is that it is with a high range of scores, and the exper-tise level can be reflected by looking at mean of its corresponding Gaussian distribution. Specifically, a high expertise level is often associated with high vote scores which can be modeled by a Gaus-sian distribution with high mean. On the contrary, a low expertise level is with a Gaussian distribution with low mean. To model user topical expertise, we assume each user u has an expertise level dis-tribution on each topic z , denoted as  X  z,u . In this case, if this user is an expert in topic z , the probability proportions  X  z,u high values for expertise levels which correspond to Gaussian dis-tributions with high mean.

For each Q&amp;A post, we observe its vote, multiple words and tags. We assume that each post has latent variables e and denote the expertise and topic of this post respectively. For each Q&amp;A post of a given user u i , topics are generated from a user spe-cific topic distribution  X  u and its expertise is generated from the user topical expertise distribution  X  z,u . For each topic generated from a topic specific word distribution  X  z and tags are generated from a topic specific tag distribution  X  z . Note that we as-sume tags of answers are the same with the corresponding question. For each expertise e , votes are generated from an expertise specific Gaussian distribution N (  X  e ,  X  e ) with Normal-Gamma distribution priors. The E expertise specific Gaussian distributions compose a Gaussian Mixture Model (GMM) component for modeling the generation of votes. The other distributions are Multinomial dis-tributions with symmetric Dirichlet priors. The plate notation is in Figure 2.
 Figure 2: The plate notation of Topic Expertise Model for user topical interests and expertise discovery in CQA. Dashed variables will be collapsed out in Gibbs Sampling.
The generative process of Q&amp;A posts of users can be described as follows:
We use collapsed Gibbs sampling to obtain samples of the hidden variable assignment and estimate the model parameters of TEM. The Gibbs Sampling process is described in Algorithm 1. Algorithm 1 Gibbs Sampling for TEM. 1:
We jointly sample topic z u,n and expertise e u,n for each user and post n , where we assume (  X ,  X  ) for all the expertise levels are known. Let c denotes { u, n } ,  X  denotes all the Dirichlet priors and Normal-Gamma priors, we can drive the Gibbs update rule for and e u,n as follows:  X  p ( Z , W , E , V , T |  X ) =  X ( C k u +  X  ) = where  X (  X  ) is a  X  X irichlet delta function X  which can be seen as a multidimensional extension to beta function [14], N (  X  ) distribution.

To estimate parameters (  X  e ,  X  e ) for an expertise level to consider all the votes associated with e and derive the posterior distribution. We report the derived formula in the following, one can refer to [9, 23] for the detailed derivations. where  X  e , X  e , X  e , X  e are defined as follows: where  X  v e is the average vote score for expertise e , n number of votes with expertise level e .

Given Eqn. 2 and Eqn. 3, we can update (  X  e ,  X  e ) as follows:
With Gibbs Sampling, we can make the following parameter es-timation:
TEM is a latent variable model for modeling textual contents and voting information to discover user topical interests and expertise. It does not make use of user network structure built from user Q&amp;A graph. However, user network structure will be helpful for topical expertise learning because users who provide answers to high ex-pertise level users tend to also be with a high expertise. Inspired by this intuition, we consider to extend PageRank to measure user topical expertise. The expertise of users under a specific topic in CQA can be interpreted as the  X  X uthority X  of web pages in hyper-link environment. We propose CQARank to combine user topical interests and expertise learning results in TEM with link structure to enforce user topical expertise learning. CQARank could find experts not only with similar topical interests, but also with high topical expertise based on Q&amp;A voting history in communities. First of all, we construct Q&amp;A graph G =( V,E ) in CQA. a set of vertex representing all users. E is a set of directed edges. An edge exists between two users if one of them answers questions of the other. The direction is from the asker to the answerer. For edge e =( u i ,u j ) where u i  X  V,u j  X  V . The weight w number of all answers provided by u j for questions of u i
A random surfer on Q&amp;A graph G visits each user vertex with random walk and teleportation operation, which results in a unique distribution of steady-state visiting probabilities. To let the ran-dom surfer visits user nodes with higher topical expertise and in-terest with larger probability, we incorporate the results from TEM into the transition matrix and teleportation vector computation of CQARank. Given a topic z , the transition probability of a random surfer from asker u i to answer u j is defined as: P ( i  X  j )= where sim z ( i  X  j ) is the similarity between u i and u z , which is defined as:  X  is row normalized U  X  K matrix learnt as user specific topic distribution in TEM.
 The transition matrix M is defined as:
In this definition, the more u j answer questions of u i , the higher expertise u j will gain, which corresponds to a higher transition probability from u i to u j . Also u j is more likely to answer ques-tions of u i if they share similar topical interests.

Given topic z , the CQARank saliency score R z ( u i ) of formulated in a recursive manner as follows: R z ( u i )=  X  where  X   X  [0 , 1] is a damping factor to control the probability of teleportation and random walk.  X  u i ,z is the estimated user topical interest score of u i under topic z , which is learnt as user specific topic distribution in TEM.  X  is the mean of the expertise specific vote Gaussian distribution. E z,u i (  X  ) is the estimated user topical expertise score of u i under topic z , which is defined as the expec-tation of user topical expertise distribution as follows: Thus  X  u i ,z  X  E z,u i (  X  ) defines the teleportation vector of the random surfer under topic z in CQARank. In original PageRank algorithm, the random surfer teleport to all nodes with the equivalent probabil-ity 1 /V where V is total number of vertex in graph. [35] propose a Topic-Sensitive PageRank for expert finding method which incor-porates user topical interest into teleportation vector computation. Tapping the value of excellent Q&amp;A performance based on com-munity voting information, we take both user topical interest and expertise into definition of teleportation vector, which enable the random surfer tend to teleport to user nodes with both similar topic preference and professional topic expertise.

Note that we can estimate each user X  X  topical expertise score by just using TEM results with Eqn. 13. However, for CQARank, we measure user topical expertise by the final saliency score when the iterated algorithm converges. The advantage of which is to combines results from TEM and link analysis of Q&amp;A graph to further improve the user topical expertise discovery. We design rec-ommendation experiments in Section 5.3 to compare performance of CQARank and TEM to reveal the effectiveness of incorporating Q&amp;A graph information. We use real data from Stack Overflow for experiments. Stack Overflow is the most popular question answering community fo-cusing on computer programming. The data of Stack Overflow is publicly available through Creative Commons Data Dump Ser-vice 4 . We download the complete dataset of two years which is from its launch in August 2008 to August 2010 . We select all posts in three months from May 1 st 2009 to August 1 st 2009 and then use all the posts of users who have asked and answered no fewer than 80 times for the training of TEM. In training data, we have 8 , 904 questions and 96 , 629 answers posted by 663 users. The data set contains 85 , 527 unique words, 10 , 689 unique tags and 135 unique votes. Our testing data for expert users and answers recommendation experiments in Section 5.3 is all posts of the same set of users in training data from August 2 nd 2009 to April 2010 . So training and testing data do not have overlap. We re-move testing questions which have no, or only one, answer. The testing data set contains 1 , 173 questions and 9 , 883 answers. For data preprocessing, we tokenize text and discard all code snippets. Then we remove the stop words and HTML tags in text.

The most frequent tags and votes with their counts in training data set are shown in Figure 3. We observe votes count distribution is a power-law distribution which means most votes are relatively small. Figure 3: The most frequent tags and votes and their counts in train-ing data set. We only visualize positive votes in this figure. http://blog.stackoverflow.com/category/ cc-wiki-dump/
For all experiments, we empirically set Dirichlet hyperparame-ters  X  =50 /K,  X  =0 . 01 , X  =0 . 01 , X  =0 . 001 according to suggestions in [10]. For Norma-Gamma parameters, we set  X  the mean of votes from our data set,  X  0 as 1,  X  0 as 1, and the mean distance between randomly sampled 1000 votes. We run TEM with 500 iterations of Gibbs sampling. With some trails on the number of topics and expertise, we set topic number K =15 expertise number E =10 as they provide meaningful topics and vote Gaussian distributions for our data set. For damping factor in CQARank, we set  X  =0 . 2 after we conduct multiple experiments to determine the best value of it from 0 . 1 to 0 . 9 .
In this section, we illustrate top tags and words for 10 randomly selected topics discovered by TEM in Table 3 and Table 4. We observe clean top words and tags for each topic. Moreover, top words have strong correlation with top tags under the same topic. For example, top tags in topic 6 are about  X  X ersion control X , corre-sponding to which TEM discovered topic words like  X  X it X ,  X  X epos-itory X ,  X  X ranch X ,  X  X ersion X ,  X  X ontrol X ,  X  X ommit X , etc. which are frequently mentioned by users when they talk about this topic. Fur-thermore, top tags like  X  X areer-development X ,  X  X est-practices X ,  X  X phone-sdk X ,  X  X emory-management X , etc. provide phrase level instead of bag-of-words features to distill richer and better interpreted topic information from Q&amp;A text.
One of our motivations in this work is to model user topical ex-pertise. Recall that TEM learns different user expertise levels by clustering votes using GMM component. The mean and precision of different expertise specific vote Gaussian distributions learnt by TEM are shown in Table 5. First, we observe 10 Gaussian dis-tributions with various means ranging from 0 . 40 to 40 . 17 generation of votes in data. The mean of each Gaussian distribu-tion can be used to denote expertise score for each expertise level. Based on this notation, we can estimate user topical expertise score according to Eqn. 13. Secondly, the higher the mean, the lower the precision. The variance becomes larger when the mean goes higher, which aligns with the power-law vote count distribution in Figure 3.
One important task in CQA sites is to make  X  X ecommendations X  for new questions, the idea of which is to either direct questions to the right expert users or answers, or to find similar questions for the asker to further explore similar answers. In particular, the three important tasks studied in CQA sites are the following: (1)To recommend expert users [35, 11, 5, 8](2)To find answers [2, 16], and (3)To find similar questions to new questions [33, 28, 15, 31]. In this section, we discuss how our model tackles these tasks.
The first task we consider is to recommend expert users where our aim is to find users who can provide answers with high vote scores for a given question, i.e. users with high expertise for the question. Note that this setting is different from related works like [11] which treats all actual answerers for questions in testing data as the ground-truth since they mainly model user topical interests and try to recommend responders for questions. For our experiments, we want to recommend users who not only would like to respond to the question, but also have real expertise to provide high quality answers. So in our experiments, all methods evaluated would find expert users and give the rank for each user in the recommendation list. We evaluate the rank list with ground truth from the answerer rank list ordered by votes in testing data.

Task: Given a question q and a set of test users U , the target is to rank all these users by their interests and expertise to answer the question q . We score each user u by considering user topic similar-ity with the question Sim ( u, q ) and user expertise in the question Expert ( u, q ) , where the intuition is that if the user is interested and have a high expertise for the question, then the user tends to pro-vide a good answer wining high votes. The recommendation score function is defined as follows: where Expert ( u, z ) is the expertise of user u under topic TEM, we compute it according to Eqn. 13. For CQARank, we set it as the final saliency score R z ( u i ) when CQARank achieves convergence.  X  q is the question X  X  topic distribution and JS JS-divergence distance.

Note that  X  u and  X  z,u can be obtained from our model results.  X  need to be estimated by computing its posterior probabilities. Specifically, we compute  X  q,z as follows: where w and t are the set of all the words and tags in question q . Here  X  u,z ,  X  ( z,w ) and  X  ( z,t ) can be obtained from our model results. After we score each user in U , we rank them in decreasing order of the score.

Baselines: To evaluate the effectiveness of CQARank, we com-pare against some previous related works including probabilistic topic models, link analysis techniques and mixture methods com-bining both as follows:
Evaluation Criteria: For ground truth, we consider all the an-swerers for each question q as the target user set U , and their av-eraged votes for each question are the ground truth vote scores -expert answerers tend to get more votes. Note that out task is not to predict the exact vote of each user but rank them in terms of votes.
We use the commonly used n DCG measure to evaluate model results, which is defined as follows where Q is the set of questions, M q,j is the j -th expert gener-ated by method M for question q , score ( M q,j )=2 v ( M where v ( M q,j ) is the ground truth score for the expert IdealScore ( K, q ) is the ideal ranking score of the top question q .

We also adopt Pearson and Kendall rank correlation coefficients which are two of the most frequently used correlation measures be-tween ranked variables as metrics. We compare rank lists of expert users by all methods with rank list in ground truth and then use cor-relation coefficients to measure the strength of correlation between the two rank lists.

Results: The results of expert user recommendation for new ques-tions are presented in Table 6. We summarize our observations as follows: (1) CQARank and TEM perform well in the task. Es-pecially looking at nDCG, both methods achieve at least a score of 0 . 89 . (2) In terms of correlation based criteria, CQARank can provide a user rank list with higher correlation coefficient with the ground truth rank list than all the other rivaling methods. (3) CQARank significantly outperforms TSPR, which shows the ad-vantage of considering vote and tag information for user topical ex-pertise discovery. (4) The result of CQARank is better than TEM, which proves the effectiveness of considering Q&amp;A link structure to enforce the expertise learning. Overall, in this expert users rec-ommendation task, our method significantly outperforms all base-line methods, with at least 10%significance level by Wilcoxon signed rank test. Table 6: Results on expert user recommendation for new questions. means the result is better than others except TEM in the same column at 5% significance level measured by Wilcoxon signed rank test and  X  is at 10% level.
The second task we consider is to recommend answers for a given question. Our task is defined as follows.

Task: For a given question q and a set of answers A , each method needs to rank all the answers in A . Similar to expert ranking task, we score each answer by considering its similarity to the question and the expertise of the answerer. Similar to Eqn. 14, we define the recommendation score function as: Note that  X  a and  X  q can be learnt from Eqn. 15.

Baselines: The baselines we consider for this task is the same as the task in Section 5.3.1.

Evaluation Criteria: We use each answer X  X  vote as its ground truth score. The metrics used here are the same as in Section 5.3.1.
Results: We present the results in Table 7. We observe simi-lar trends as in expert recommendation. (1) CQARank and TEM show good results in the task, in terms of the correlation based criteria. CQARank provides an answer rank list with higher cor-relation coefficient with the ground truth rank list than all the com-paring methods. (2) CQARank significantly outperforms TSPR in terms of all criteria, at least 10%significance level by Wilcoxon signed rank test, which shows the advantage of considering vote and tag information for user topical expertise discovery. (3) We find in this task, to consider Q&amp;A link structure is important as link analysis based approaches achieve better results than topic analysis based approach. Our method also shows clear advantage over TEM. Overall, in this task, CQARank outperforms all baseline methods. Table 7: Results on answers recommendation for new questions. means the result is better than others except TEM in the same column at 5% significance level measured by Wilcoxon signed rank test and  X  is at 10% level.
The third task we consider is to find similar questions for a given new question, which is defined as follows.

Task: We observe that in CQA forum, when a user asks a new question (referred as query question hereafter), the user will of-ten get replies from other users who provide links to other similar questions. These query questions serve as an ideal question set with ground truth similar questions. We crawl 1 , 000 questions to form our query question set whose similar questions exist in the training data set and serve as the ground truth. For each query question with n similar questions, we randomly select another ( m =1 , 000 ) questions from our training data set to form candidate similar questions. Each comparing method would gener-ate a rank list of these m + n candidate similar questions according to their topic similarity to the query question. Among these can-didate questions, the higher the similar questions are ranked, the better the performance of the method. The recommendation score is defined by 1  X  JS ( . ) , where JS ( . ) is the JS-divergence between topic distributions of two questions. Note that CQARank uses topic distributions learnt by TEM. Hence in this task, they are equivalent.
Baselines: Any topic analysis based approach can be used as baselines because the main task here is to find those questions top-ical similar with the query question. We consider TSPR [35] and UQA [11] discussed in Section 5.3.1 as our baselines. Note that topics learnt by TSPR [35] are equivalent to compare with LDA [4], as TSPR uses LDA to learn topics by aggregating all posts of a user to form a  X  X ocument X . To measure the usefulness of tags, we con-sider a simple baseline, SimTag , which recommends questions by looking at tag similarity. We use Jaccard Index (recommendation score) to measure tag similarity, where the idea is the more tags two questions share the more similar they are.

Evaluation Criteria: We compare CQARank with baselines in terms of four criteria: precision, the average rank of the similar questions  X  r , mean reciprocal rank (MRR) and cumulative distribu-tion of ranks (CDR). Let q be the query question and Q s be the ground truth similar questions. The average rank of the similar questions is defined as:  X  r ( q )= 1 |Q s | q s  X  X  s r ( q is the rank of the similar question q s from q  X  X  m + n candidate questions. The MRR and CDR are defined as follows.
 where CDR @ p is the percentage of users whose similar questions are ranked at least at rank p . For example, CDR @2 = 10% 10% of query questions whose similar questions are ranked at least the second. Thus a higher value means a more successful recom-mendation at top p rank.

Results: We present the results in Table 8. CQARank shows a better performance than all baselines in terms of all measures. As CDR score is a single numeric value, we cannot perform signifi-cant test on it. For the rest criteria, significant test shows a very low p-value, which are all less than 1E-10. This result indicates that our method significantly outperforms the comparing methods. We would like to stress that the task of recommending similar questions is difficult as the candidate question set is very large. Another chal-lenge is that most of time, we only observe one similar question in our data set which is one question that appears as a link in the post replying to the query question. In this case, the task is essentially to rank this question among more than 1000 candidate question set. It is therefore not surprising to observe that the precision of all the methods are not high. Yet, our method shows much better preci-sion among all. Furthermore, all the comparing methods show a low CDR score. In CDR @50 , less than 5% of query questions are observed with similar questions being ranked at least in top-50 po-sition. Our method performs significantly better, with more than 40% of query questions. Moreover, the results show the effective-ness of considering tags to measure topics as the SimTag baseline has a better performance than TSPR and UQA. Our method outper-forms all the baselines mainly because of two factors: (1) using tags to help learn topics; (2) jointly models topics and expertise, where the interplay between them can affect the formation of topics. We further give parameter sensitivity analysis for our proposed CQARank and Topic Expertise Model. CQARank is based on the topics and expertise model results of TEM, hence the choice of pa-Table 8: Results on similar question recommendation for new ques-tions. means the result is better than other methods at 0.0001% significance level measured by Wilcoxon signed rank test. rameters settings such as topic number, K and expertise number, E in TEM will also influence the performance of CQARank. We choose expert users recommendation task for analysis. We vary the number of expertise and topics in TEM and observe the change of CQARank performance in expert users recommendation exper-iments. The topic number K is from 10 to 30 and the expertise number E is from 4 to 15 . Figure 4 and Figure 5 demonstrate the change of multiple metrics with the number of expertise and topics varied. Figure 4: Performance in expert users recommendation of CQARank by varying the number of expertise ( E ). Figure 5: Performance in expert users recommendation of CQARank by varying the number of topics ( K ).

We can see that for n DCG @1 , n DCG @5 and n DCG metrics, the performance of CQARank is stable when the number of expertise or topics varies, which demonstrates the robustness and stability of CQARank with respect to the expertise or topic number when recommending expert users for new questions. For Pearson and Kendall correlation coefficients, we can see slight fluctuations with the increasing number of expertise or topics. Overall, CQARank has stable performance with varying setting of the topic number and expertise number. Expert Identification. Current methods for expert finding in CQA are mainly based on link analysis and latent topic modeling tech-niques. Bouguessa et. al. [5] proposed a model based on Indegree which is the number of best answers provided by users to discover experts. Jurczyk and Agichtein [18] applied HITS[19] algorithm on the underlying graph of CQA for estimating user ranking scores. Zhang et. al. [34] proposed expertise ranking and evaluated link al-gorithms on a specific domain dataset. They also proposed Z-score to measure the relative expertise of a user. Although link analy-sis technique helps find authoritative users, a given new question on some specific topics might not interest these global experts or match their expertise and skills.

To find topic-level experts, topic-model-based methods are pro-posed for user topical interests analysis. Guo et. al. [11] proposed a generative model for questions and answers by exploring the cat-egory information to discover latent interests of users and recom-mend question answerers for new arrival questions. Liu et. al. [22] used mixture of language model and LDA for best answerer pre-diction. While latent topic analysis could find users interested in a given new question, these approaches fail to capture to what extent these users X  expertise and skills match the questions with similar topical interest.

Furthermore, some approaches try to combine topical similar-ity and link analysis techniques for finding authoritative users. A typical work is [32] which proposed TwitterRank, an extension of PageRank algorithm to measure the influence of users in Twit-ter. Zhou et. al. [35] proposed Topic-Sensitive PageRank(TSPR) for expert finding. They also proposed a User-Topic Model, where they aggregate all posts of a user as a document. These approaches are inspired by the pioneering work of [13] which proposed orig-inal TSPR approach. Instead of computing a single global PageR-ank value for every page, this method computes multiple TSPR scores on topic level. Zhao et. al. [30] modeled user roles using topic models that can incorporate users contribution dynamically for generating experts and topics simultaneously. [7] modeled the user reputation in comment rating environment and proposed a la-tent factor model for multi-context rating prediction. Their work studied the rating information towards user comments, which is dif-ferent from our problem setting focusing on community question answering that includes factual contents without opinions. Com-petition technique based on pair-wise approach is prosed by [21]. Techniques based on gaussian mixture models are used in similar studies such as [27, 25, 26].

Our study differs from these works in that we jointly model top-ics and expertise, taking in consideration both user topical interest and expertise evaluation. We also better integrate data components of CQA into our proposed model. Tagging information helps learn clean and rich topics and a Gaussian mixture hybrid can model vot-ing information from community members which has not been well utilized in CQA for user topical expertise discovery.
 Relevant Answers Retrieval. For answer retrieval, Berger et. al. [2] proposed a lexicon correlation method to build an answer finding system from FAQ whereas, Jeon et. al. [16] evaluated semantic features of answers such as author activity, number of clicks, and average length of posts to find the best answers for a given ques-tion using maximum entropy. Both methods are built on supervised techniques whereas, we propose unsupervised approach based on probabilistic generative models to find answers. Topical modeling approach for question retrieval has been proposed by [17] where the lexical gap between question-answer pairs is reduced using the topics and proved the advantage of topic models over translation-based techniques. Similar to us, Cai et. al. [6] incorporated the category information for better learning of latent topics. Apart from tags, we incorporated the topical expertise information to aid the ranking of the answers. [3] proposed a semi-supervised cou-pled mutual reinforcement framework for simultaneously calculat-ing the quality scores of Q&amp;A posts, which requires relatively few labeled examples to initialize the training process. User profile in-formation such as pictures, levels and points has been exploited by [36] for ranking answers in CQA, whereas we exploited the user expertise using the voting information which can aid in detecting more detailed user topical expertise. Our study also differs from these works in that we consider both topical expertise of authors of answers and topic similarity between questions and answers for finding answers for new questions.
 Similar Question Recommendation. For similar question rec-ommendation, Jeon et. al. [15] proposed a statistical approach that explores the semantic features to measure the question simi-larity. Pattern-based approach [12] depends on seed patterns with a semi-supervised approach. [20] proposed an approach based on machine translation that goes beyond the simple cosine similar-ity approaches and Wu et. al. [33, 28] proposed probabilistic la-tent semantic analysis approach that exploits both user interest and feedback, using historical data for deriving user interests. Their work showed the benefits of PSLA over the translation methods. Our study is similar to some of these works in that we explore the semantics of the questions using topic models. However, in our method, we also consider the tagging information associated with the question that aids in effective similar question retrieval task. Furthermore, to alleviate the problem of lexical gap, we jointly model topics and expertise as the interplay between them can af-fect the formation of topics.
In this paper, we proposed Topic Expertise Model to jointly model topics and expertise in CQA services. Based on its model results we proposed CQARank to combine textual content learning with link analysis for deriving the user expertise and interests score under various topics. Our model is generalized and applicable for vari-ous CQA tasks including expert finding, relevant answers retrieval , and similar questions recommendation. Our extensive experimen-tal studies on Stack Overflow data sets demonstrates the effective-ness of our model when compared to other existing methods.
In the future we expect to further study the temporal aspect of users in CQA. In real world, the interests and expertise of users change with time. Capturing such temporal information could be more beneficial in recommendation tasks of CQA. Another inter-esting aspect is the social influence of users on CQA. The answerer profile might influence the voting behavior of users and hence im-pacts the recommendation methods. It is an interesting problem to analyze the correlated components in CQA for adaptive recom-mendation systems.
This work was done during Liu Yang X  X  visit to Singapore Man-agement University. The authors would like to thank the reviewers for their valuable comments on this work.
