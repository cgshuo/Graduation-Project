 BOXING CHEN, MIN ZHANG, and AI TI AW Institute for Infocomm Research, Singapore 1. INTRODUCTION The significant progress of Automatic Speech Recognition (A SR) and Statis-tical Machine Translation (SMT) technologies in the past tw o decades has inspired the research community to take on the challenge of s poken language translation (SLT). Internet and the expansion of borderles s communities, such as the increase in the number of European Union countries, ha ve accelerated research activities on speech-to-speech translation tech nology. Many research projects have been carried out to advance this technology, s uch as JANUS 1 [Waibel et al. 1991], DIPLOMAT 2 [Frederking et al. 1997], VERBMOBIL 3 [Wahlster 2000], EuTrans [Aiello et al. 1999], NESPOLE! 4 [Lavie et al. 2001], PF-STAR, 5 TC-STAR 6 [Hoge 2002], DARPA BABYLON project 7 [Eurospeech 2003], Digital Olympics [Sebastian et al. 2006], etc.
 translation of speech or text between two spoken languages. It differs from written language translation in several ways. Compared to w ritten language, spoken language usually has a high variability in syntactic structures. More-over, translation of spontaneous speech has to deal with hum an noise such as filler words ( um, well ), partial words, and lip smacks, which can be included as mid-utterance corrections and bad word placement. In add ition, it is very challenging to collect real spoken language data. For examp le, the content of the most widely used spoken language data corpora, the Basic Traveling Expression Corpus (BTEC) [Takezawa et al. 2002], are only collected from phrase books for tourists, instead of the desired spontaneo us speech. of-the-art, has shown many advantages over other machine tr anslation (MT) approaches. Phrase-based method specializes in modeling l ocal reorderings and translations of multiword expressions. While syntax-b ased decoder is en-hanced by using syntax and linguistic knowledge, which can b etter model long word reorderings, discontinuous phrases, and linguis tic syntactic struc-tures. To leverage on the strength of these two methods, we pr opose a mecha-nism which integrates multiple methods in a two-stage hypot heses generation framework for spoken language translation.
 eration. During decoding, we use two algorithms: phrase-ba sed decoding and syntax-based decoding. In the regeneration stage, we emplo y three methods: redecoding [Rosti et al. 2007a], n-gram expansion [Chen et a l. 2007a] and con-fusion network-based regeneration [Fiscus 1997]. All the g enerated hypothe-ses from these two stages are combined and the final translati on output is selected in a reranking pass. This mechanism not only levera ges the strength of phrase-based and syntax-based methods, but also explore s more translation hypotheses and features that are not able to be captured by ea ch individual decoding algorithm.
 on translating transcriptions of spontaneous speech showe d that our mecha-nism significantly outperformed Moses [Koehn et al. 2007], a state-of-the-art, phrase-based SMT system. Our system with the proposed appro ach repre-sented the best performing BLEU measured system in the Chine se-to-English open data track in IWSLT 8 -2007 [Chen et al. 2007b].
 Section 3 presents the structure of our proposed mechanism a nd the details of the two decoding algorithms and the three regeneration meth ods. The rescor-ing modules are described in Section 4, and Section 5 reports on the experimen-tal setups and results. Section 6 discusses the results obta ined, while Section 7 presents some conclusions on our work. 2. RELATED WORK Many previous studies have addressed spoken language trans lation (SLT) and they can be classified from the viewpoint of hypotheses ge neration strategies. The four major approaches are analysis-based, interlingua-based, example-based, and statistics-based. The analysis-based method includes rule-based methods and knowledge-based methods. Rayner and Boui llon [1995] and Rayner and Carter [1997] used a hybrid transfer model, wh ich com-bined unification-based rules and a set of trainable statist ical preferences. Nirenburg et al. [1992] and Hovy [1994] built knowledge-bas ed translation systems. Woszczyna et al. [1998], Lavie et al. [2001], Lee et al. [2001], Besacier et al. [2001], and Gao et al. X  X  [2002] MT components applied a n interlingua-based approach. Horiguchi and Franz [1997], Takezawa et al. [1998], Sugaya et al. [1999], Sumita et al. [1999], Amengual et al. [2000], a nd Seligman [2000] used an example-based scheme to translate source utterance s to the target language. The statistics-based method includes phrase-ba sed and syntax-based methods. Vidal [1997], Bangalore and Riccardi [2002] , and Casacuberta et al. [2002] used a finite-state translation model. Ney et al . [2000], Ney [2003], Vogel et al. [2004], and Hsiao et al. [2006] proposed phrase-based statistical models to generate translation hypotheses. Zhou et al. [200 2] used a maxi-mum entropy (ME) modeling-based sentence-level natural la nguage generator to generate sentences in the target language from parser out put. Zollmann et al. [2006] used syntax-augmented statistical models and Chen et al. [2007b] and He et al. [2007] used syntax-based statistical translat ion models to gener-ate translation hypotheses for spoken language. Each appro ach has its advan-tages and limitations. There is no single best MT solution fo r spoken language translation problems.
 Zong et al. [2002] generated translations by using multiple strategies: an interlingua-based, an example-based, and a slot-based met hod, but the final output translations were selected through human interacti on. Sumita et al. [2003] and Paul et al. [2005] generated hypotheses using exa mple-based and phrase-based statistical methods and then selected the 1-b est translation by an SMT-based method.
 [Frederking and Nirenburg 1994; Bangalore et al. 2001; Matu sov et al. 2006; Sim et al. 2007; Rosti et al. 2007a; Rosti et al. 2007b; Huang a nd Papineni 2007; Macherey and Och 2007; Chen et al. 2007b], which combin ed hypo-theses generated by different MT systems (using the same or d ifferent hy-potheses generation strategies) improved the performance over individual systems. Various system combination methods have been deve loped, where re-decoding [Rosti et al. 2007a], n-gram expansion [Chen et al. 2007a], and con-fusion network-based consensus decoding [Fiscus 1997] wer e used to generate hypotheses on top of the original ones.
 the statistical phrase-based and syntax-based approaches is presented. The proposed method is able to improve the performance of spoken language translation by generating translation hypotheses through multiple hypotheses generation strategies. 3. TWO-STAGE HYPOTHESES GENERATION FRAMEWORK The hypotheses generation framework as depicted in Figure 1 comprises two stages: decoding and regeneration. In the first stage, decod ing algorithms are applied to generate N-best translation hypotheses (N-b est1); while in the second stage, new translation hypotheses (N-best2) are gen erated from the first N-best hypotheses (N-best1). All the N-best lists are t hen combined, and the final translation is selected by a reranking pass using ad ditional feature functions.
 potheses. In the first stage, phrase-based and syntax-based statistical methods are applied. In the second stage, three methods are implemen ted to regen-erate new hypotheses based on the original N-best translati ons: redecoding, n-gram expansion, and confusion network-based regenerati on. The following paragraphs contain more details on these hypotheses genera tion methods. 3.1 Decoding Algorithms Two statistically motivated decoding methods, phrase-bas ed and syntax-based algorithms, are employed in the first stage to generate the N-best basic trans-lation hypotheses. Since these two methods represent diffe rent statistical con-straints, we have good reason to expect that they will produc e different N-best results. For example, the phrase-based approach may be more suitable for handling the flexibility in spoken language sentences while the syntax-based approach captures long-distance word-ordering patterns i n a more effective manner. guage, a statistical machine translation system generates the string e in the target language which maximizes the posterior distributio n Pr( e | f ). In phrase-based translation, words as the units of translation are com plemented by phrases, where phrase is defined as any string of consecutive words with no constraints on syntax and semantics. Phrase-based statist ical machine trans-lation systems are usually modeled through a log-linear fra mework [Och and Ney 2002] by introducing the hidden word alignment variable a [Brown et al. 1993]. Formally, functions, weights  X  m are typically optimized to maximize the scoring function [Och 2003].
 2007] with word alignment obtained from GIZA++ [Och and Ney 2 003]. IBM word reordering constraints [Berger et al. 1996] are applie d during decoding to reduce the computational complexity. The models and feat ure functions h (  X  e , f , a ) which are employed by the decoder are:  X  X ranslation models, which model the phrase-or word-based translation probabilities on direct and inverse directions.  X  X istortion models, which assign a cost linear to the reorde ring distance. The cost is based on the number of source words which are skipped w hen trans-lating a new source phrase.  X  X exicalized word reordering models [Koehn et al. 2005], wh ich are condi-tioned on both source and target phrases and learn different reordering probabilities for each phrase pair.  X  X anguage models, which are trained using the SRILM toolkit [Stolcke 2002] with modified Kneser-Ney smoothing method [Chen and Goodman 1998].  X  X ord and phrase penalties, which count the numbers of words and phrases in the target string.
 sively adds phrases  X  e to the target string, by covering corresponding source phrases according to the IBM word reordering constraints. T he translation model generates target translations  X  e and measures the adequacy of  X  e with respect to the source phrases; the distortion and reorderin g models measure the movement of the source and target phrases; and the langua ge model mea-sures the fluency of  X  e with respect to its left context. sion of the tree-to-tree alignment model proposed in Zhang e t al. [2007]. It is formally a probabilistic synchronous tree-sequence sub stitution grammar (STSSG) that is a collection of aligned elementary tree-seq uence pairs with mapping probabilities (which are automatically learned fr om word-aligned bi-parsed parallel texts). The difference between the two mode ls lies in the fact that Zhang et al. [2007] used an elementary tree as a basic tra nslation unit while we use an elementary tree-sequence as the basic transl ation unit. The elementary tree-sequence has stronger expressive ability than the elementary tree since the tree-sequence can model nonsyntactic phrase s with syntactic structure information.
 6 t , Ns , Nt , Ss , St , P &gt; is a septet, where  X  6 s and 6 t are source and target terminal alphabets (POSs or lexical wo rds), respectively.  X  Ns and Nt are source and target nonterminal alphabets (linguistic ph rase tag, i.e., NP/VP. . . ), respectively.  X  S s  X  N s and S t  X  N t are the source and target start symbols (roots of source and target parse trees).  X  X  is a production rule set, where a production rule is a pair o f elementary forests (  X  s  X   X  t ) with a linking relation between leaf nodes in the source elementary forest (  X  s ) and leaf nodes in the target elementary forest (  X  t ). An elementary forest is an ordered subtree sequence covering a consecutive tree fragment whose leaf nodes are either nonterminal or termina l symbols. ( PEF ) will be extracted as the forest transfer rules.
 tary forests of the source sentence are used to match their po ssible transfer rules. Lastly, based on a featured log-linear model, the opt imal target tree is generated. and K is the number of applied forest transfer rules. 3.2 Regeneration Methods If N-best translations are first generated by running the abo ve two decoding algorithms, then regeneration combines the substrings occ urring in the origi-nal N-best translations which are produced by these two deco ding algorithms from the first stage, to generate M new target hypotheses whic h are not in the original N-best translations.
 one selected by the decoder, because it could be the case that the best scoring translation in the list is inadequate and that a correct tran slation could be obtained by replacing some of its words with portions taken f rom other trans-lations in the N-best lists. Here, we introduce three method s to accomplish regeneration. Huang and Papineni 2007; Macherey and Och 2007] regeneratio n generates new hypotheses by running the decoder with test set-specific models. In this work, we use the phrase-based decoder (Moses) to produce new M-best hypotheses, employing the same feature functions as in Sect ion 3.1.1 during redecoding, but with new translation models and reordering models trained over the word-aligned source input and original N-best targ et hypotheses. Although the source-to-target phrase alignments are avail able in the original N-best hypotheses, we realign the words using GIZA++ to lear n new phrase-pairs and increase the difference between the new M-best hyp otheses and the original N-best hypotheses. The translation table and reor dering models are generated over the whole development or test set. The same la nguage models which are employed in the decoding stage are also used in the r edecoding. process of redecoding-based regeneration is shown as follo ws: 1. Run GIZA++ to align the words between the source input and t arget N-best 2. Estimate translation and reordering models. 3. Optimize the weights of the decoder with the new models. 4. Decode the source input by using new models and new weights to generate 5. Select M-best translations from the newly generated N+M h ypotheses 6. Merge the M-best translations with the original N-best tr anslations. without the tuning step (step 3). 2007a] generates new hypotheses through a generative n-gra m language model which is trained on the original N-best translations g enerated in the first decoding stage. Firstly, all n-grams from the original N-best translations are collected and the n-grams which start each entry of N-bes t translations are set as the initial partial hypotheses. Then the partial hypo theses are continu-ously expanded by appending a word through the n-grams colle cted in the first step. We explain this method in more detail using the followi ng example. collect all the 3-grams from the original hypotheses. The fir st 3-grams of all original entries in the N-best list are set as the initial par tial hypotheses. They are:  X  X t X  X  5 minutes, it is 5, it X  X  about 5 and i walk 5 X . Expans ion of a partial hypothesis starts by computing the set of n-grams matching i ts last n-1 words. As shown in Figure 3, the 3-gram  X 5 minutes on X  matches the las t two words of the partial hypothesis  X  X t X  X  about 5 minutes. X  So the hypo thesis is expanded to  X  X t X  X  about 5 minutes on. X  The expansion continues until t he partial hypoth-esis ends with a special end-of-sentence symbol that occurs at the end of all N-best strings. The algorithm returns when all partial hypo theses end with this end-of-sentence symbol.
 in Figure 2, which are taken from our development data. One re ference is also given in Figure 4. Note that the first newly generated hypothe sis is the same as this reference, while no such hypothesis occurred in the o riginal N-best translations.
 source sentence are computed through a beam-search algorit hm with a log-linear combination of the feature functions. The feature fu nctions which are employed in the search process are:  X  X -gram frequency [Chen et al. 2005].  X  X -gram posterior probability [Zens and Ney 2006].
  X  X anguage model.  X  X irect and inverse IBM model 1.  X  X ord penalty.
 are generated. regeneration builds a confusion network over the original N -best hypotheses, and then extracts M-best hypotheses from it. To build a confu sion network, first the N-best hypotheses are word-aligned against each ot her to create a graph with word alternatives ( null is also an alternative) for each alignment position; then, each word is scored according to the frequen cies derived from the N-best hypotheses. The new hypotheses are generated by c hoosing differ-ent words in each alignment position along the network.
 to choose a hypothesis with the  X  X ost correct X  word order as t he confusion net-work skeleton (alignment reference), then align and reorde r other hypotheses to this word order. Some previous works compute the consensu s translation under MT system combination, which differ from our method in the manner of choosing the skeleton and aligning the words. Matusov et a l. [2006] let every hypothesis play the role of the skeleton once and used G IZA++ to get word alignment. [Bangalore et al. 2001; Sim et al. 2007; Rost i et al. 2007a] chose the hypothesis that best agreed with other hypotheses on average as the skeleton. Bangalore et al. [2001] used a Word Error Rate base d alignment and Sim et al. [2007], Rosti et al. [2007a], and Rosti et al. [2007 b] used minimum Translation Error Rate-based (TER) [Snover et al. 2006] ali gnment to build the confusion network.
 Minimum Bayes Risk (MBR) decoding [Kumar and Byrne 2004]. On ly the top n hypotheses from each first-stage decoding system are consid ered as a skeleton. TER is used as the loss function in the MBR decoding . Due to heavy cost of computing the expected loss function, normally, a sm all n is set. one alignment, we developed our algorithm based on the one-t o-one assump-tion and used the competitive linking algorithm [Melamed 20 00] for our word alignment. First, an association score is computed for ever y possible word pair of the skeleton and sentence to be aligned. Then a greedy algo rithm is applied to select the best word-alignment. In this article, we used a linear combination of multiple association scores, as suggested in Kraif and Ch en [2004]. As the two sentences to be aligned are in the same language, the asso ciation scores are computed based on the following four clues:  X  X ssociation score based on cognate ( S 1 ) which is determined by counting the length of the longest common substring.  X  X ssociation score based on word class ( S 2 ), by setting S 2 = 1 if the two con-sidered words are in the same word class; otherwise, S 2 = 0. Word classes are clustered using the GIZA++ toolkit.
  X  X ssociation score based on a list of synonyms ( S 3 ), if the two considered words are synonyms, S 3 = 1; otherwise, S 3 = 0. In this work, we use a synonym list extracted from WordNet 9 [Fellbaum 1998].  X  X ssociation score based on position difference ( S 4 ), the position difference probability considers the chance of observing a certain pos ition difference be-tween two randomly drawn positions inside two sentences of e qual lengths. S 4 is computed by taking the logarithm of this probability.
 of each score is determined by additional experiments throu gh observing the performance on a development set where e i is the word in the skeleton, and e  X  j is the word in the sentence to be aligned with the skeleton.
 are reordered to match the word order of the skeleton. The ali gned words are reordered according to their alignment indices. The una ligned words are reordered using one of these two strategies: either moved wi th its previous word or with its next word. Additional experiments suggeste d that moving the unaligned word with its previous word achieves better pe rformance. In the case that the first word is unaligned, it will be moved with its next word. Figure 5 shows an example of a confusion network.
 confusion network. We again use the beam-search algorithm t o derive new hy-potheses. The same feature functions as proposed in Section 3.2.2 are used to score the partial hypotheses. Moreover, we use the position -based word prob-ability (i.e., in Figure 5, for the words in position 5,  X  X n X  s cored a probability of 0.5, and  X   X   X  scored a probability of 0.25). Figure 5 shows some examples of new hypotheses generated through confusion network regene ration. 4. RESCORING MODELS After generating the new list of hypotheses, the final transl ation is selected through the rescoring/reranking pass. Since the final joine d hypotheses are produced from different decoding algorithms or regenerati on methods, the local feature functions (scores) of each hypothesis are not comparable and cannot be used in reranking. We thus exploit rich global feat ure functions in the rescoring models to compensate for the loss of local fe ature functions. Weights of feature functions are optimized through minimum error rate train-ing (MERT) [Och 2003]. 4.1 Translation Models We use six scores in the rescoring pass to compensate for the l oss of trans-lation models. They are direct and inverse IBM model 1 and 3 le xicons, and two word-pair cooccurrence association scores: hyper-geo metric distribution probabilities and mutual information. These are used to rat e the translation adequacy of a target hypothesis. Equation (4) shows how to co mpute the fea-ture scores for direct IBM model 1 or 3. The other scores can be computed similarly where p ( f j | e i ) is the translation probability of the IBM translation mode l lexicon. 4.2 Reordering Probabilities Lexicalized word/block reordering rules which were propos ed in Chen et al. [2006] are used to compensate for the loss of reordering mode ls. These rules record the probability of a possible reordering for a source word-based pattern and are automatically extracted from word aligned training data and weighted according to observed statistics. The reordering feature f unction score is com-puted on the basis of the set of matching rules for a given sour ce (input) and target (hypothesis) pair: where r k is a matching rule, Pr( r k ) is its probability observed on the train-ing data and K is the number of reordering patterns matching the given source/target pair. The reordering rules are extracted on a competitive link-ing algorithm CLA-based word-aligned training data set and word order of the hypothesis is also given by CLA as suggested in Chen et al. [20 07a]. 4.3 Sentence Length Ratio The length ratio between source and target sentences is used to compensate for the loss of a word penalty. 4.4 Language Models In rescoring, we use the same language models as in decoding. Moreover, a target word-class-based language model is also applied; th e word classes are clustered by using the GIZA++ toolkit over the target traini ng data. 4.5 Translation Confidence Three feature functions which are learned from the hypothes es list and mea-sure the translation confidence of the hypothesis are comput ed as rescoring feature functions. They are  X  X inear sum of n-grams ( n = 1 , 2 , 3 , 4) relative frequencies within all trans-lations, which favors the hypotheses containing popular n-grams of higher order [Chen et al. 2005], it is computed as follows:  X  X ord and n-gram ( n = 2 , 3 , 4) posterior probabilities [Zens and Ney 2006;
Ueffing and Ney 2007].  X  X entence length posterior probabilities [Zens and Ney 200 6].
 et al. 2005] on the following two conditions: ( A ) the sentence ends with a question mark; ( B ) the sentence starts with a typical question starting word as found in the training data. The score is assigned as follow s: 5. EXPERIMENTS AND RESULTS 5.1 Experimental Settings We carried out the experiments on the Chinese-to-English sp oken language translation tasks on the Basic Traveling Expression Corpus (BTEC) Chinese-to-English data [Takezawa et al. 2002]. BTEC is a multilingu al speech corpus which contains sentences coming from phrase books for touri sts. Only 40,000 sentence-pairs were publicly available and used in our expe riment. Addition-ally, the English sentences of the Tanaka corpus 10 were also used to train our language model.
 of spontaneous speech (set Dev1 and Test1 in Table I) as our de velopment and test set. For comparison purposes, we also conducted anothe r experiment for which we used the IWSLT X 06 development clean text set (set De v2 in Table I) as development set and IWSLT X 06 test sets (set Test1, Test2, and Test3 in Table I) as test sets. The original source sentences of these sets do not contain punctuation, but we added this before feeding the sets to the decoder. Follow-ing the instructions provided by the IWSLT X 06 organizers, 11 the punctuation insertion was performed using the hidden-ngram command in SRILM toolkit. Table I details the statistics of the training, development and test data for the IWSLT-2006 experiment.
 could be adopted to general text-to-text machine translati on. Only the phrase-based decoder was used to generate the N-best translations i n the decoding stage for this experiment. We ran the experiments on the FBIS corpus of newswire domain data. We used the NIST 2002 MT evaluation set as our devel-opment set, and the NIST 2003, 2005 test sets as our test sets. Table II details the statistics of the training, development, and test data f or this experiment. which uses bootstrap resampling [Koehn 2004]. 5.2 Results For the IWSLT-2006 experiment, we extracted 600 distinct hy potheses for each source input from the phrase-based decoder; and 300 distinc t hypotheses from the syntax-based decoder that did not occur in the former 600 hypotheses; and generated 300 new hypotheses for each regeneration system: redecoding (RD), n-gram expansion (NE), and confusion network (CN). Finally , all the generated hypotheses were combined and formed the system COMB.
 lations from the two decoders is also computed. The RESC1 sys tem contained 1,200 distinct hypotheses which were extracted from the phr ase-based decoder (PBD). The RESC2 system contained the 800-best distinct hyp otheses from the phrase-based decoder (PBD) and the 400-best from the syntax -based decoder (SBD). The RESC3 system contained 1,800 distinct hypothese s which were ex-tracted from the phrase-based decoder (PBD). The RESC4 syst em contained the 1,200-best distinct hypotheses from the phrase-based d ecoder (PBD) and the 600-best from the syntax-based decoder (SBD).
 source input from the phrase-based decoder, and generated 8 00 new hypothe-ses for each regeneration system.
 score [Doddington 2002], which were used to perform case-in sensitive match-ing of n -grams up to n = 4. The translation results are reported in Table III, IV, and V respectively. The row  X  X BD X  shows the scores of the t ranslations produced by the phrase-based decoder (Moses);  X  X BD X  shows t he scores for the syntax-based decoder;  X  X ESC1/2/3/4 X  shows the scores w here rescoring is applied on the original N-best translations.  X  X D, X   X  X E, X  and  X  X N X  are the results of the regeneration systems based on redecoding, n-gram expansion and confusion network respectively.  X  X OMB X  used the 1-best translation from the combination of N-best and all M-best hypotheses lists. N ote that on top of the same global feature functions as mentioned in Section 4, the local feature functions used during decoding were also involved in rescor ing RESC1/3. the parameters were tuned on the clean text development set ( Dev2), test set Test1 achieved better performance as compared to when they w ere tuned on the transcription of the spontaneous speech development se t (Dev1) in all the systems except system SBD. Since a clean text development se t is not difficult to obtain, we will focus our discussion on the results report ed in Table IV. rescoring pass, especially on the BLEU score which improved from 20.18 (PBD) to 20.98 in RESC1. By increasing the size of N to 1,800 (RESC3) , a slight addi-tional improvement could be observed, from 20.98 (RESC1) to 21.05 (RESC3), but the NIST score was slightly worse. This means adding hypo theses gener-ated from the same decoder cannot guarantee further improve ment. However, by adding hypotheses generated from the syntax-based syste m SBD to the rescoring, further improvements were found: the BLEU score increased from 20.98 (RESC1) to 21.51 (RESC2) and from 21.05 (RESC3) to 21.6 3 (RESC4). It is worth noting that RESC2/4 only employed the global featur e functions in-troduced in Section 4, but RESC1/3 also employed the local fe ature functions which were used during the decoding presented in Section 3.1 .
 absolute improvement of more than 1.0 BLEU score was obtaine d for all three regeneration systems when compared to system RESC2: 1.22 BL EU ( p &lt; 0 . 05, from 21.51 to 22.73) for system RD, 1.18 BLEU ( p &lt; 0 . 05, from 21.51 to 22.69) for system NE, 1.12 BLEU ( p &lt; 0 . 05, from 21.51 to 22.63) for system CN. Please note that the total number of final N+M-best hypothese s for RESC1/2 and each regeneration system RD/NE/CN are the same.
 three regeneration methods has shown the greatest improvem ents. In total, COMB outperforms RESC4 on the IWSLT X 06 test set Test1 by 1.35 BLEU ( p &lt; 0 . 05, from 21.63 to 22.98).
 performance on test sets Test2 and Test3 which are clean text and transcrip-tion of read speech, by 0.96 ( p &lt; 0 . 05, from 28.28 to 29.24) BLEU for Test2 and 1.12 ( p &lt; 0 . 05, from 23.39 to 24.51) BLEU for Test3.
 Compared with RESC2 (rescoring on 4,000-best hypotheses ou tput by phrase-based SMT decoder), system COMB obtained an absolute improv ement of 0.57 ( p &lt; 0 . 05, from 27.21 to 27.78) BLEU score on NIST X 03 and 0.61 ( p &lt; 0 . 05, from 25.43 to 26.04) BLEU score on NIST X 05.
 some translation examples obtained from system RESC3 which generated hypotheses using only one approach and COMB which generated hypotheses using our presented five approaches. The examples are shown i n Table VI. 6. DISCUSSION In this report we have shown that regeneration of hypotheses from multiple SMT systems can significantly improve the performance of spo ken language translation. To better understand why this is the case, we mu st look at the con-tribution of each method to the final translation output (tra nslations of COMB) as shown in Table VII. The phrase-based decoder performed be tter (occupied more of the final translations) than the syntax-based decode r, with approxi-mately 45% of the final output hypotheses coming from the phra se-based de-coder during the first decoding stage. The syntax-based deco der contributed only about 2-3% of the final hypotheses. It is however worth no ting that the first 600-best hypotheses were always selected from the phra se-based decoder, while the next 300-best hypotheses were taken from the synta x-based system. This means that all hypotheses that were already present in t he phrase-based 600-best were not counted as part of the syntax-based contri bution. Thus the contribution from the syntax-based system may not be well ac counted for as it plays the complementary role to augment the hypotheses se lection for the phrase-based system.
 of the two decoding algorithms, more than 50% (e.g., for IWSL T test set Test1, (144 + 50 + 83) / 500 = 55 . 4%) of best scored outputs were generated by this method, showing that newly generated translations wer e quite often the reranking winner.
 turned out that more than 25% (NIST X 05 test set, (95+110+82) / 1082 = 26 . 5%) of the hypotheses for the NIST test sets were generated by reg eneration methods. This further proved that the newly generated hypot heses can be better than the original ones.
 mechanism can provide better translations, we must look at e ach single hypotheses generation method. The phrase-based decoder sp ecializes in mod-eling local reorderings and translations of multiword expr essions. The syntax-based decoder is enhanced by using syntax and linguistic kno wledge, which can better model long word reorderings, discontinuous phra ses, and linguis-tic syntactic structure. The regeneration methods can take advantage of each individual decoding method to select the best phrase-pairs between them. Moreover, considering each method, the redecoding method m ay introduce new and better phrase-pairs which were not available in the trai ning data and thus cannot be generated by either method. N-gram expansion can e xploit a much larger search space for target strings; as a result, it can pr oduce alternative translations which contain word reorderings and phrase str uctures beyond the ability of the decoder in the decoding stage [Chen et al. 2 007a]. Confu-sion network-based regeneration reinforces the word choic e by considering the posterior probabilities of words occurring in the N-best tr anslations. 7. CONCLUSIONS In this article, we propose a mechanism which integrates mul tiple methods in a two-stage hypotheses generation framework for spoken l anguage trans-lation. The hypotheses are generated in two stages: decodin g and regenera-tion. During decoding, we use two algorithms, that is, phras e-based decoding and syntax-based decoding. In the regeneration pass, we emp loy three meth-ods: redecoding, n-gram expansion, and confusion network b ased regeneration. Finally, all the generated hypotheses in the previous two st ages are combined and the final translation outputs are selected in a rescoring pass. This mecha-nism not only leverages on the strength of the phrase-based a nd syntax-based methods, but also explores more translation hypotheses and features that are not captured by the individual decoding algorithms.
 based decoders and carried out on the IWSLT-2006 Chinese-to -English chal-lenge task, which translates the transcription of spontane ous speech and the NIST Chinese-to-English task. It was shown that this mechan ism improved the performance significantly by working with a rich feature function rescoring model. Results also showed that using more hypotheses gener ation methods can lead to better performance. We conclude that it is an effe ctive approach to combining multiple translations from decoders of differ ent nature in spoken language translation. The proposed technique benefits from an increased num-ber of diverse translation candidates produced by multiple hypotheses genera-tion strategies.

