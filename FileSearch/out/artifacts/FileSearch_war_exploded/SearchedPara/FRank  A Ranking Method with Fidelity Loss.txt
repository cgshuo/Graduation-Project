 Ranking problem is becoming important in many fields, es-pecially in information retrieval (IR). Many machine learn-ing techniques have been proposed for ranking problem, such as RankSVM, RankBoost, and RankNet. Among them, RankNet, which is based on a probabilistic ranking frame-work, is leading to promising results and has been applied to a commercial Web search engine. In this paper we conduct further study on the probabilistic ranking framework and provide a novel loss function named fidelity loss for mea-suring loss of ranking. The fidelity loss not only inherits effective properties of the probabilistic ranking framework in RankNet, but possesses new properties that are helpful for ranking. This includes the fidelity loss obtaining zero for each document pair, and having a finite upper bound that is necessary for conducting query-level normalization. We also propose an algorithm named FRank based on a generalized additive model for the sake of minimizing the fidelity loss and learning an effective ranking function. We evaluated the proposed algorithm for two datasets: TREC dataset and real Web search dataset. The experimental re-sults show that the proposed FRank algorithm outperforms other learning-based ranking methods on both conventional IR problem and Web search.
 H.3.3 [ Information Search and Retrieval ]: Retrieval models, Search process Algorithms, Experimentation  X 
This work was conducted when the first author was visiting student at Microsoft Research Asia.
 Copyright 2007 ACM 978-1-59593-597-7/07/0007 ... $ 5.00. FRank, Fidelity Loss, Learning to Rank
In this information explosion era, accurately locating rel-evant information based on user information needs has be-come important. The IR problem can be formulated as a ranking problem: provided a query and a set of documents, an IR system should provide a ranked list of documents. Several methods have been proposed for solving the prob-lem in the literature, such as Boolean model, vector space model, probabilistic model, and the language model; these methods can be regarded as empirical IR methods.

In addition to traditional IR approaches, machine learning techniques are becoming more widely used for the ranking problem of IR; the learning-based methods aim to use la-beled data for learning an effective ranking function. There are several different ways to use learning-based methods for solving IR problems. One may regard IR as a binary classi-fication problem in which documents are categorized as rel-evant or irrelevant. However, for real Web search engines, pages cannot simply be classified into two types relative to user information needs; they should have multiple grades, such as highly relevant, partially relevant, definitely irrel-evant, and so on. Therefore, several learning-based meth-ods, such as RankBoost [8], RankSVM [12], and RankNet [4], view the problem as a ranking problem. These meth-ods mainly construct pairs between documents and use ma-chine learning techniques for minimizing the number of mis-ordered pairs.

According to [4], the probabilistic ranking framework has many effective properties for ranking, such as pair-wise dif-ferentiable loss, and can better model multiple relevance lev-els; the proposed RankNet algorithm performs well in prac-tice and has been used in a commercial Web search engine. However, the loss function in RankNet still has some prob-lems: for instance, it has no real minimal loss and appro-priate upper bound. These problems may cause the trained ranking function inaccurate and the training process biased by some hard pairs. Therefore, we conduct further studies on the probabilistic ranking framework, and propose a novel loss function named fidelity loss. The fidelity loss is inspired by the concept of Fidelity [14], which is widely used for measuring the difference between two states of a quantum in the field of physics [2]. The fidelity loss function inherits those properties of the probabilistic ranking framework, and has several additional useful characteristics for ranking. For instance, the fidelity loss can achieve zero and is bounded between 0 and 1 for each document pair; this property is necessary for conducting query-level normalization.
For the sake of efficiently minimizing the fidelity loss func-tion, we further propose a ranking algorithm based on a generalized additive model. Our ranking algorithm named Fidelity Rank (FRank) combines the probabilistic ranking framework with the generalized additive model. We evalu-ated the proposed FRank algorithm on both TREC dataset and a large-scale Web search dataset, which encompassed 19,600 queries collected from a commercial Web search en-gine. The experimental results show that the proposed ap-proach outperforms other ranking methods on both conven-tional IR problem and Web search, and that the improve-ments are statistically significant.

The remainder of this paper is organized as follows. We briefly review the previous research in Section 2. In Section 3, we revisit the probabilistic ranking framework and the fi-delity measure, and describe the FRank algorithm. Section 4 reports experimental results and discusses the relationship between different methods. We conclude our paper and dis-cuss future work in Section 5.
Manymachinelearningtechniques[1,4,5,6,8,12,13] have been studied for IR. In [13], Nallapati treats IR as a binary classification problem that directly classifies a doc-ument as relevant or irrelevant with respect to the given query. However, in real Web search, a page has multiple relevance levels, such as highly relevant, partially relevant, and definitely irrelevant. Therefore, some studies regard IR as a ranking problem: highly relevant web pages are ranked higher than partially relevant ones, and partially relevant ones are above irrelevant ones. In the literatures, many learning-based ranking algorithms are proposed; typical ex-amples include RankBoost, RankSVM, and RankNet.

Freund et al. [8] use the Boosting approach for learning a ranking function and proposed RankBoost to solve the problem of combining preferences. Their method aims to minimize the weighted number of pairs of instances that are mis-ordered by the final ranking function. The algorithm can be summarized as follows. For each round t ,RankBoost chooses  X  t and the weak learner h t so as to minimize the pair-wise loss, which is defined by where x i is the instance ranked higher than x j , D t ( x the weight of pair ( x i ,x j ). After the optimal weak learner h is selected, the weight D t ( x i ,x j ) is adjusted with respect to  X  t and h t . The rule of adjustment is to decrease the weight of pairs if h t gives a correct ranking ( h t ( x i ) &gt;h increase otherwise. Finally, this algorithm outputs the rank-ing function by combining selected weak learners. Owing to the greedy search nature of Boosting, RankBoost can be implemented in parallel and thus scale up to a large-scale dataset.
 Joachims [12] proposes RankSVM algorithm, which uses Support Vector Machines for optimizing search performance using click-through data. RankSVM aims to minimize the number of discordant pairs, which is similar to RankBoost, and to maximize the margin of pair. The margin maxi-mization is equal to minimize the L 2-norm of hyperplane parameter  X  . Given a query, if the ground truth asserts that document d i is more relevant than d j , the constraint of RankSVM is  X   X ( q, d i ) &gt;  X   X ( q, d j ), where  X ( q, d )isthefea-ture vector calculated from document d relative to query q . Then, the ranking problem can be expressed as the following constrained optimization problem. min : V (  X ,  X  )= 1 s.t.
From the theoretical perspective, RankSVM is well-founded in the structure risk minimization framework, and is veri-fied in a controlled experiment. Moreover, the advantage of RankSVM is that it needs no human-labeled data, and can automatically learn the ranking function from click-through data.

Burges et al. [4] have proposed a probabilistic ranking framework and used neural network for minimizing a pair-wise differentiable loss in a method named RankNet. As-sume that the ranking model is f : R d  X  R , so that the rank order of a set of samples is specified by the real value that the model takes. Therefore, f ( d i ) &gt;f ( d j )istakento mean that the model asserts that d i d j . Given a set of pairs of training samples ( d i ,d j ) together with the target probability P  X  ij of d i being ranked higher than d j , a logis-tic function can be used for mapping the output of ranking function to a probability as follows: where o ij = f ( d i )  X  f ( d j ), and P ij = P ( d i d j cross entropy is adopted as a loss function for training, which can be represented as: where C ij is the cross entropy loss of a pair ( d i ,d j the desired probability, and P ij is the modeled probability.
RankNet uses back-propagation neural network for mini-mizing the criterion during tra ining process, and then learn a ranking function. As compared to RankBoost and RankSVM, the loss function in RankNet is pair-wise differentiable, which can be regarded as an advantage in cases in which ground truths come from several annotators that may disagree. So, RankNet performs well in practice and has been successfully applied to a commercial search engine.
Inspired by the success of RankNet, we investigate the probabilistic ranking framework and propose a novel loss function named fidelity loss. In this section we briefly de-scribe the probabilistic ranking framework, and discuss the fidelity loss and its useful properties. We also present a ranking algorithm named FRank for minimizing the loss and learning an effective ranking function.
 Figure 1: Loss Function: (a) Cross Entropy Loss Function; (b) Fidelity Loss Function.
According to [4], the mapping from the output of ranking function to a probability based on the probabilistic ranking framework has the following properties: 1. The ranking model puts consistency requirements on 2. Any set of adjacency posteriors can uniquely identify 3. The expected strengthening (or weakening) of confi-
When the outputs are mapped to probabilities, the gen-eral measure of probability distribution can be applied as the criterion for training, such as cross entropy, KL-divergence, and information radius. In RankNet [4], cross entropy is adopted for this purpose; the loss function of a pair can be represented by C ij =  X  P  X  ij o ij +log(1+ e o ij ), which is shown in Figure 1(a).

The cross entropy loss function provides a principal way to make the samples have the same rank of ground truths. Previous works show that the loss function is effective and the corresponding RankNet algorithm performs well in prac-tice. However, if we carefully investigate Figure 1(a), we find there are still some problems within this loss function. Considering these problems, it is meaningful to investigate the other measures of probability distribution that can pro-vide better properties for ranking on the probabilistic rank-ing framework.
After investigating many different measures of probability distributions such as KL-divergence and information radius, we find that an optimal candidate for the criterion of ranking problem is Fidelity [14]. Fidelity is originally a distance met-ric in physics to measure the difference between two states of a quantum. The fidelity of two probability distributions is defined by where { p x } and { q x } are the probability distributions. Note that when the distributions { p x } and { q x } are identical, F ( p x ,q x )= x p x = 1. A better geometric interpretation of the fidelity is that the fidelity is the inner product be-tween vectors with components a unit sphere.

The fidelity is a useful measurement for mathematics and physics. Moreover, it is a meaningful measure candidate for the loss function of probabilistic ranking framework. We adapt the fidelity measure so that the loss of a pair is mea-sured by where P  X  ij is the given target value for the posterior prob-ability and P ij is the modeled probability; then, after the logistic function is adopted, F ij becomes F Figure 1(b) plots F ij as a function of o ij for three values P information available as to the relative rank of the two sam-ples, then F ij has its minimum at the origin. Note that the loss of this pair can obtain zero. This also gives us a princi-pal way of training samples that we desire to have the same rank as the ground truth. Actually, the fidelity loss inherits all three properties of the probabilistic ranking framework in RankNet. In addition, it also possesses new properties that are helpful for ranking. We describe these properties in detail as follows. 1. The fidelity loss is capable of obtaining the real mini-2. The fidelity loss of a pair is bounded between 0 and 1. 3. The fidelity loss of a query can easily be considered by
Although the third property of query-level fidelity loss looks natural, the similar extension for the previous methods is non-trivial. Each query is not guaranteed to contribute equally to the total loss since their loss function do not have an appropriate upper bound. With the new properties, we regard our proposed fidelity loss as a more suitable mea-sure of ranking loss. In the next subsection, we discuss how to efficiently minimize this loss function so as to learn an effective ranking function.
Considering the efficiency and scalability for large-scale datasets, we choose a generalized additive model similar to Boosting for minimizing the fidelity loss. The primary con-sideration is that the additive model is easier to implement in parallel since the examination of features is independent in the training process.

In the additive model, the final ranking function is defined as: where h t ( x ) is a weak learner and  X  t is the combination co-efficient of the weak learner, and t is the index of iterations. Consider the ranking function after the k -th iteration,
H k ( x )= Then, the fidelity loss of H k ( x ) over all queries is where Given H k ( x )= H k  X  1 ( x )+  X  k h k ( x ) and denoted the fidelity loss J ( H k ) can be formulated as h ( x j ).

Setting the derivative of Equation (2) with respect to  X  k to zero, we have the equation as follows:
For a general weak learner h k ( x ), solving the close form of  X  k from the above equation is quite difficult. For simplicity, we adopt the same choice of weak learners as in [8], which are binary weak learners, to construct the final ranking func-tion. When a binary weak learner is introduced, the above equation can be simplified because h i,j k only takes values -1, 0, and 1. Therefore, the equation can be expressed by = With further derivations and relaxations, we eventually ob-tain where
W i,j k = D ( i, j )
Consequently, the algorithm operates in this way. In the k -th iteration, a new weak learner h k ( x ) with the minimal fidelity loss J ( H k ) is obtained according to the current pair weight W i,j k , and then it is combined with the previous ones using the combination coefficient  X  k . In a stepwise manner, we eventually can obtain the final ranking function H ( x ). We name this algorithm Fidelity Rank (FRank), for which details are summarized in Algorithm 1.
Precision is an IR performance measure that quantifies the fraction of retrieved documents which are known to be relevant. P @ n is capable of measuring the accuracy within top n results of the returned rank list for a query. In general, most of popular Web search engines display 10 re-turned documents for each page and many users only browse the results in the first page. Therefore, we use precision within ten returned documents to evaluate the performance of each ranking algorithm.
 Algorithm 1 Learning Algorithm of FRank
Input: Pairs over all training queries and weak learner candidates h c ( x ), 1  X  c  X  m ,where m is the number of features
Initialization: Pair weight by Eq. (1) and the number of weak learners k for t =1to k do end for Output: H ( x )= k t =1  X  t h t ( x )
In addition, MAP [17, 18] is also considered as evaluation metric for evaluating ranking methods. Given a query q i its average precision (AP) is calculated as: where N is the number of documents retrieved, P ( j )isthe precision value at position j ,and pos ( j ) is a binary function to indicate whether the document at position j is relevant. Then, we obtain MAP by use of averaging the AP values of all the queries.

Considering that the web search dataset has multiple rat-ing grades, we also use the normalized discount cumulative gain (NDCG) [9, 10] to evaluate the performance of ranking algorithms. For a given query, the NDCG value for a ranked document at position i is computed as follows: 1. Compute the gain 2 r ( j )  X  1ofeachdocument j ; 2. Discount the gain of each document j by its ranked 3. Cumulate the discounted gain for document j at posi-4. Normalize the discounted cumulative gain for docu-where r ( j ) is the rating of the j -th document in the ordered list, and the normalization constant n i is chosen so that a perfect ordering gets NDCG value 1. We use NDCG within ten returned documents to evaluate the performance in the following experiments.
Three learning-based ranking algorithms are compared in this study: RankBoost [8], RankNet [4], and RankSVM [12]. For RankBoost, we implemented binary weak learner whose output is 0 and 1. For RankNet, we use linear and two-layer perceptrons, and those are denoted as RankNet Linear and RankNet TwoLayer. For RankSVM, we directly use the binary code of SVMlight [11]. In addition, non-learning based IR model, BM25 [18], is utilized as a baseline for comparison with conventional IR approach.
 Table 1: The extracted features of TREC dataset Figure 2: Experimental Results on TREC Dataset.
TREC dataset is crawled from the .gov domain in early 2002, and has been used as the data collection of Web Track since TREC 2002. There are totally 1,053,110 pages with 11,164,829 hyperlinks in it. W hen conducting the experi-ments on this corpus, we used the topic distillation task in the Web Track of TREC 2003 [7] as query set, which has 50 queries in total. The ground truths of this task are provided by the TREC committee with binary judgment: relevant and irrelevant. The number of relevant pages for each query spans from 1 to 86. There are 14 features extracted from TREC dataset for training, as listed in Table 1.

We use four-fold cross validation to evaluate performance owing to the size of TREC dataset. The experimental re-sults are plotted in Figure 2 in which the values are averaged scores over the four trials. The proposed algorithm, FRank, is a relatively effective ranking algorithm, as observed from a comparison with other ranking algorithms in Figure 2. All learning-based methods outperform BM25; our proposed FRank algorithm even obtains about 40% improvement over non-learning based method, BM25, in MAP. This would in-dicate that learning to rank is a promising direction in IR.
The proposed FRank algorithm gains more improvement at the top position of retrieved results, especially precision at position 1, as indicated from Figure 2. This result indicates that the proposed method is also suitable for Web search, which emphasizes the top position of result. Therefore, an experiment on real Web search is conducted in the following subsection.
The dataset consists of 19,600 queries and more than 3,300,000 web pages within a commercial Web search en-gine. These web pages are partially labeled with ratings Training dataset 12,000 385,293 Validation dataset 3,800 663,457
Testing dataset 3,800 693,180 Figure 3: Training Phase of FRank on Web Search Training Dataset. from 1 (irrelevant) to 5 (highly relevant) by annotators. Be-cause the dataset is too large to label completely, the unla-belled pages are given the rating 0. Given a query, a page is represented by query-dependent (e.g., term frequency) and query-independent (e.g., PageRank) features extracted from the page itself and the preprocessed indices. The total num-ber of features is 619. Considering that these features reflect the business secrete of that search engine company, we would not describe them in detail. However, this does not influ-ence our experimental study since all the methods under investigation operate on the same feature set.

For ease of experimental study, we divide the dataset into three sets: 12,000 queries for training, 3,800 queries for vali-dation, and 3,800 queries for testing. Some unlabeled pages would be relevant, so that performance descend if we use the whole unlabeled pages for training. Therefore, 20 unla-beled documents are randomly selected as the documents with rating 0 for training. So, our training dataset has 385,293 documents; it totally contains 2,635,976 pairs for training. Table 2 summarizes the details of the training, validation, and testing datasets.
The training process is shown in Figure 3, in which the number of weak learners starts from 5. The proposed FRank algorithm indeed decreases the fidelity loss with the increas-ing number of weak learners, and at the same time it in-creases the value of NDCG@10, as illustrated in Figure 3. These values change dramatically in top selected 30 weak learners, and then they vary smoothly in the following weak learners.
Only three experimental results of FRank, RankBoost, and RankNet on Web Search dataset are reported, since RankSVM runs out of memory in this dataset. After the training phase, the validation dataset is used for selecting the best parameter setting for each method, such as the number of weak learners of FRank and RankBoost, and the number of epochs of RankNet. This procedure is taken to guarantee the better generalization of these ranking meth-ods. Here, we use NDCG@10 as the criterion to select the parameter setting.
 Figure 4 plots the NDCG@10 curves of FRank, Rank-Boost, and RankNet on validation data. FRank outper-forms RankBoost for each weak learner, as demonstrated in Figure 4(a) where the number of weak learners starts from 10. RankBoost performs worse when the number of weak learners is few than 20; however, when the number of weak learners is over 20, RankBoost eventually obtains better performance. This is because the power of represen-tation of RankBoost is quite limited with few weak learners. Accordingly, we set the number of weak learners as 224 for FRank and 271 for RankBoost, as observed from the middle of flat tail in Figure 4(a).

In Figure 4(b), RankNet TwoLayer is observed to be more effective than RankNet Linear; however, its performance seems sensitive to the validation dataset. The performance of RankNet TwoLayer dropped when the number of epochs was more than 10; this would be the problem of overfit-ting. In contrast, ranking algorithms, such as FRank and RankBoost, based on the generalized additive model are more robust against the problem of overfitting; this robust-ness is also an essential characteristic of the generalized additive model. We set the number of epochs as 25 for RankNet Linear and 9 for RankNet TwoLayer, as observed from Figure 4(b).

After setting the parameter of models, we use the settings to examine the performance of the ranking algorithms on the testing dataset. Conventional IR model [3, 17, 18], i.e., BM25, is also conducted for a comparison of learning-based and non-learning based methods. In addition, an empirical approach of using linear combination of BM25 and PageR-ank is also performed, where the parameter is 0.8 for BM25 and 0.2 for PageRank (i.e., 0 . 8  X  BM25 + 0 . 2  X  PageRank); this parameter is obtained after tuning.

Figure 5 plots NDCG values from 1 to 10 of ranking algorithms on the testing dataset. Simply using conven-tional IR model is inadequate for Web search, as indicated from a comparison of BM25 with those learning-based meth-ods. This is because the learning-based methods is ca-pable of leveraging various features in a large-scale Web search dataset. The proposed FRank algorithm is more ef-fective than the other learned-based ranking algorithms from NDCG@1 to NDCG@10; RankNet TwoLayer also performs well on this large-scale dataset. The results indicate that the probabilistic ranking framework is a suitable framework for learning to rank. Although both FRank and RankNet are based on the framework, FRank is capable of ranking more accurately than RankNet; this is consistent with the discussion about the superiority of fidelity loss in Section 3.2. Moreover, the FRank algorithm performs well at the top position of ranking lists, as observed from a comparison of NDCG@1 with RankNet TwoLayer in Figure 5.

A significance test for FRank and RankNet TwoLayer with a confidence level of 98% is performed to verify whether the improvement is statistically significant. The corresponding p -values are 0.0114 for NDCG@1, 0.007 for NDCG@5, and 0.0056 for NDCG@10. This result indicates that, as to Web search, FRank is significantly better than RankNet, and also significantly outperforms other ranking algorithms. with Linear and Two-Layer Perceptrons. Figure 5: Experimental Results of Ranking Algo-rithms on Web Search Testing Dataset.
Considering RankSVM ran out of memory on the Web search training dataset, we further conducted several exper-iments on smaller-scale training datasets to provide more insight about RankSVM. In addition, investigating how the number of training queries affects performance is also essen-tial for Web search. For this purpose we separately trained these referenced ranking algorithms on 1,000, 2,000, 4,000, 8,000, and 12,000 queries. However, RankSVM ran out of memory with 8,000 queries in these experiments. This is mainly because RankSVM has to construct as many con-straints as the number of document pairs, and then the number of variables in the dual problem becomes enormous when training on large-scale dataset. Linear kernel is used for those case that RankSVM can operate upon, and the pa-rameter c of RankSVM is tunned on the validation dataset, which is similar to the procedure in Section 4.4.3.
Figure 6 plots the results of NDCG@10 for each ranking algorithm trained on different numbers of queries; Table 3 also summaries these results. We list several observations as follows. Table 3: NDCG@10 on Web Search Testing Dataset using Different Size of Training Dataset.
 RankNet TwoLayer 0.699 0.711 .720 0.727 0.729
This paper presents an approach for learning to rank with the goal of improving the accuracy of conventional IR and Web search. On the basis of the probabilistic ranking frame-work, we propose a novel loss function named fidelity loss Figure 6: NDCG@10 on Web Search Testing Dataset using Different Size of Training Dataset. to measure the loss of ranking, and accordingly present a ranking algorithm named FRank based on a generalized ad-ditive model. Experiments with significance test show that the FRank algorithm performs well in practice, even for con-ventional TREC dataset and real Web search dataset. Fur-ther research directions remain for future work:
Research of this paper was partially supported by Na-tional Science Council, Taiwan, under the contract NSC95-2752-E-001-001-PAE. [1] E. Agichtein, E. Brill, and S. Dumais. Improving web [2] N. Birrell and P. Davies. Quantum fields in curved [3] A. Bookstein. Outline of a general probabilistic [4] C. Burges, T. Shaked, E. Renshaw, A. Lazier, [5] Y. Cao, J. Xu, T. Liu, H. Li, Y. Huang, and H. Hon. [6] K. Crammer and Y. Singer. Pranking with ranking. [7] N. Craswell, D. Hawking, R. Wilkinson, and M. Wu. [8] Y. Freund, R. Iyer, R. Schapire, and Y. Singer. An [9] K. J  X  arvelin and J. Kek  X  al  X  ainen. IR evaluation methods [10] K. J  X  arvelin and J. Kek  X  al  X  ainen. Cumulated gain-based [11] T. Joachims. Making large-Scale SVM Learning [12] T. Joachims. Optimizing search engines using [13] R. Nallapati. Discriminative models for information [14] M. Nielsen and I. Chuang. Quantum computation and [15] L. Page, S. Brin, R. Motwani, and T. Winograd. The [16] T. Qin, T. Liu, X. Zhang, Z. Chen, and W. Ma. A [17] S. Robertson. The probability ranking principle in IR. [18] S. Robertson and S. Walker. Some simple effective [19] R.Song,J.Wen,S.Shi,G.Xin,T.Liu,T.Qin, [20] G. Xue, Q. Yang, H. Zeng, Y. Yu, and Z. Chen.
