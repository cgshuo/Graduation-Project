 The adoption of hashtags in major social networks including Twit-ter, Facebook, and Google + is a strong evidence of its importance in facilitating information di ff usion and social chatting. To under-stand the factors ( e.g., user interest, posting time and tweet con-tent) that may a ff ect hashtag annotation in Twitter and to capture the implicit relations between latent topics in tweets and their cor-responding hashtags, we propose two PLSA-style topic models to model the hashtag annotation behavior in Twitter. Content-Pivoted Model ( CPM ) assumes that tweet content guides the generation of hashtags while Hashtag-Pivoted Model ( HPM ) assumes that hash-tags guide the generation of tweet content. Both models jointly in-corporate user, time, hashtag and tweet content in a probabilistic framework. The PLSA-style models also enable us to verify the impact of social factor on hashtag annotation by introducing social network regularization in the two models. We evaluate the pro-posed models using perplexity and demonstrate their e ff ectiveness in two applications: retrospective hashtag annotation and related hashtag discovery. Our results show that HPM outperforms CPM by perplexity and both user and time are important factors that af-fect model performance. In addition, incorporating social network regularization does not improve model performance. Our experi-mental results also demonstrate the e ff ectiveness of our models in both applications compared with baseline methods.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Information filtering Twitter, Hashtag, Topic model, Hashtag annotation
Twitter is one of the most popular social networking and micro-blogging platforms. It has accumulated a tremendous amount of text data; as at January 2014, on average 58 million tweets are posted per day by more than 645 million active Twitter users. 1 These tweets cover a larger number of diverse topics, including comments on recent or ongoing events and emerging topics, per-sonal activities, politics and many others. Due to the informal writ-ing style and the 140-character length constraint, tweets are short, noisy, and are often posted with very limited context.

Hashtag ( i.e., keyword prefixed with # symbol) has demonstrated its e ff ectiveness in bringing organization to the sparse information in Twitter. Hashtags associated with tweets enhance information di ff usion and tweet search as well as facilitate social chatting. Re-ported in a recent survey by RadiumOne [20], 58% of Twitter users utilize hashtags on a regular basis. Because of its e ff ectiveness, hashtag has been adopted as a key feature in other micro-blogging services like Tumblr and Sina Weibo, and recently has been o ffi -cially supported in Google + and Facebook. 2
The e ff ectiveness of hashtags in tweets, however, is limited by the freedom of users in deciding (i) whether or not to annotate tweets with hashtags, and (ii) which hashtags to use ( e.g., #cikm , #cikm14 , or #cikm2014 ). In 2010, only about 11% of tweets were annotated with one or more hashtags [11]. Detailed in our literature survey, lots of studies related to hashtags in Twitter have been car-ried out. However, there is a lack of study on the formal modeling of the latent relationship between the important factors in a ff ect-ing hashtag annotation. In this study, we consider and model user interest, posting time, tweet content, and hashtag in a probabilis-tic framework for better understanding hashtag annotation at topic level. A topic-level modeling of these factors in hashtag annotation benefits many applications such as retrospective hashtag annota-tion, related hashtag discovery, hashtag summarization, etc..
Each tweet contains at least three attributes, i.e., tweet content, author (also known as user in this paper), and posting time. As previously mentioned, tweets cover a large number of diverse top-ics, such as personal activities and comments on recent or ongoing events. As a form of high-level topic abstraction, hashtags in a collection of tweets directly reflect these topics. In other words, hashtags reflect topics related to personal interests / activities of in-dividual users, and also reflect the popular or trending topics in Twitter at that time period. We therefore aim to model the latent topical relationship between tweet content , user , time , and hashtag . Tweet content . As an annotation, a hashtag is a high-level abstrac-tion of the content of a tweet. Among all factors, tweet content is the most important factor a ff ecting the usage of hashtags. However, there could be two kinds of possible associations between a hash-tag and a tweet: (i) a user composes a tweet and then finds one or more appropriate hashtags to describe the tweet. In other words, before user finishing writing this tweet, she has no particular hash-tag in mind to use. A hashtag is chosen because it best describes the tweet content. (ii) a user composes a tweet with a specific hash-ta g in mind. In this case, the tweet content could be considered as a detailed elaboration of the pre-chosen hashtag or comment on the event indicated by the hashtag. In this paper, we propose two mod-els to model the two di ff erent generation processes between tweet content and hashtag.
 User . In general, a large portion of tweets from a common user are about her personal interests / activities ( e.g., music , sports , food , travel ). The hashtags adopted by a user often reflect such interests and activities. Some of the common hashtags ( e.g., #nowplaying , #nba ) adopted by a large number of users sharing similar inter-ests lead to informal social communities through these common hashtags as well as mention mechanism. It is reported that social network formed through mentions among users is essential for in-teraction in Twitter [12]. Intuitively, users who often mention each other are more likely to share similar interests (or similar topics). We therefore consider user as a factor in a ff ecting hashtag annota-tion and also evaluate the impact of social factor on a ff ecting hash-tag annotation.
 Time . Twitter is a real-time social media. Many of the tweets are about recent or ongoing events. Many tweets, hence their associ-ated hashtags, published in a time period are about hot events at that time period. Take the royal wedding as an example, on April 29, 2011, hashtags like #royalwedding and #bbcwedding were used to annotate thousands of tweets reporting the wedding of Prince William and Catherine Middleton. The usage of both hashtags re-duces significantly in a week X  X  time. The time factor enables our models to better associate time-sensitive hashtags with tweets.
Considering the three factors and the two generation processes, we propose two PLSA-style models, namely, Content-Pivoted Model ( CPM ) and Hashtag-Pivoted Model ( HPM ), to jointly model the relationship between user, time, tweet content and hashtag, at topic level. CPM assumes that a user composes a tweet and then finds the appropriate hashtags to describe the tweet. HPM assumes that a user composes a tweet with pre-selected hashtag(s) in mind. We further incorporate and evaluate the impact of social factor in our models. Specifically, we evaluate CPM sn (resp. HPM sn ) by in-troducing social network regularization to CPM (resp. HPM ) with the assumption that users mention each other more often are more likely to adopt similar hashtags.

As case studies, we utilize our models in two example applica-tions. Retrospective hashtag annotation aims to annotate existing tweets with the most appropriate hashtags because 90% of hash-tags are not tagged, observed from our data and reported in other studies [11]. Related hashtag discovery is to search for most re-lated hashtags of a given query hashtag. The related hashtags help in hashtag query refinement, query extension and query recommen-dation. To summarize, contributions arising from this paper are: 1. To the best of our knowledge, we are the first to model the re-2. Through extensive experiments, we evaluate our models by 3. We define the problems of retrospective hashtag annotation
The rest of the paper is structured as follows. We survey the re-lated work in Section 2. Section 3 describes the proposed models and their inference algorithms for model parameter estimation. In Section 4, we evaluate the performance of the proposed models by perplexity and the discovered topics. Two applications,retrospective hashtag annotation and related hashtag discovery, are presented and evaluated in Section 5. Section 6 concludes this paper.
In this section, we begin with a brief overview of the studies on hashtags in Twitter. We then survey the related work on hashtag recommendation, followed by topic models proposed for Twitter. Hashtags in Twitter . The wide adoption of hashtags in Twitter has attracted significant research attention. Hashtags have been studied from many di ff erent perspectives in the literature, such as hash-tag adoption prediction [29], hashtag popularity prediction [15,27], hashtag di ff usion [24], and hashtag sentiment analysis [2,28].
In the study of hashtag adoption prediction [29], Yang et al. stated that there are two main purposes for hashtag adoption, book-marking tweet content and joining a community on the same topic or trend. The features used in the prediction include (i) relevance and preference derived from tweets annotated with hashtags, and (ii) prestige and influence derived from social graph formed by users who adopt a hashtag. In [27], Tsur and Rappoport predicted hashtag popularity on weekly basis using regression model. Both features derived from hashtag itself ( e.g., orthography, number of characters in a hashtag) and features derived from tweet content are used in the prediction. Their experiments showed that content fea-tures improve prediction performance. Both studies reveal a strong relationship between hashtag adoption and tweet content. In our work, we jointly model hashtag and tweet content to capture their relations at topic level.

Romero et al. [24] categorized hashtags into 8 classes ( e.g., pol-itics , celebrity , and game ) and analyzed the di ff erences in the me-chanics of information di ff usion of hashtags from di ff erent classes. They reported that hashtags on politics are adopted for a longer time period and the exposure times of a hashtag ( i.e., how many times a user observes this hashtag in her Twitter stream) plays an important role in hashtag di ff usion. A user graph based on mention relation-ship was constructed in their work to trace information di ff usion. Our work also considers the impact of user factor on hashtag adop-tion and models the social network as a regularization, assuming that users who often mention each other share similar topics. Hashtag Recommendation . Tag recommendation has established itself into an important research topic. Many techniques like tensor factorization [22,23,26] and graph model [5,7] have been proposed and applied to di ff erent social tagging systems like Flickr and De-licious. For Twitter, both user-based recommendation [4, 14] and tweet-based recommendation have been proposed for hashtag rec-ommendation [13,16,25,32]. Next, we briefly survey tweet-based recommendation for being more relevant to our work.
To recommend hashtags for a tweet, Zangerle e t al. [32] searched for similar tweets to the given tweet by content similarity, then ranked the hashtags by their usage on the similar tweets. Mazzia et al. [16] also utilized tweet content for hashtag recommendation using a Bayesian model. Kywe et al. [13] further incorporated user preference into the model in [32]. That is, hashtags to be recom-mended to a tweet d by user u are the hashtags used to annotated many similar tweets to d and the hashtags adopted by many similar users to u . In our experiments, we use this method to be our base-line method in the retrospective hashtag annotation application. Topic Models for Twitter . Topic models, including Probabilistic Latent Semantic Analysis (PLSA) [9] and Latent Dirichlet Alloca-tion (LDA) [1], are widely employed in text mining and informa-tion retrieval. PLSA [9] is a classic topic model and has been used to model various types of data, with or without regularization. Yin et al. [30] proposed a model to discover regional topics in Flickr and incorporated GPS information into PLSA with the assumption that topics of nearby regions are more coherent. In [8], a PLSA-style model was presented to mine topics of search queries. The authors incorporated regularization in the model with the assump-tion that topic distribution of two users are similar if they click on similar documents retrieved. Recently, PLSA models have been applied to mine Twitter data [10,31]. In [10], a PLSA-style model was proposed to discover topics and identify user X  X  interests from geo-tagged tweets for both topic tracking and location estimation. The PLSA model proposed in [31] further incorporated time factor in addition to geo-location information for location prediction.
Many LDA extensions have been proposed to Twitter. Due to the shortness of tweets, it is often assumed that each tweet has one unique topic [33]. To find bursty topics in Twitter, Diao et al. [3] proposed a TimeUserLDA model. This model assumes that tweets posted around similar time are more likely to share similar topics and tweets posted by the same user are more likely to share similar topics. Labeled LDA, a semi-supervised learning model, was pro-posed in [21], to model the latent relationship between users and tweets in Twitter. Topic models have also been applied to hash-tag recommendation [6]. Given a tweet, Godin et al. employed LDA to generate its topic distribution, and then recommended top keywords from the dominant topics to this tweet as hashtags. In our proposed solution, we recommend existing hashtags rather than keywords to tweets.

Although both PLSA and LDA extensions have been used to model tweet data, we choose to adopt the PLSA framework for its flexibility in introducing social network regularization. In this section, we present the two hashtag annotation models: Content-Pivoted Model ( CPM ) and Hashtag-Pivoted Model ( HPM ). Both models jointly model tweet content, user, time, and hashtag, but with di ff erent assumptions on the generation of hashtag and tweet content. In the following, we start with the notations used in our models and the intuitions in our models. We then present the two models and their inference algorithms. Lastly, we detail the inference algorithms considering social network regularization in the two models CPM and HPM . The models with social network regularization are denoted by CPM sn and HPM sn respectively. Notations . Let d be a tweet and D be a collection of tweets. Let U be a collection of users each of which has published at least one tweet. We partition time into a sequence of time slots of fixed length and map the publication time of a tweet to a time slot t . 3 Let T be the collection of time slots, V be the word vocabulary, and E be the hashtag vocabulary. A tweet d is a 4-tuple d = { u , t , w u  X  U is the author of the tweet; t  X  T is the time slot within which d was published; w d is the word collection in d , where the words are drawn from V ; and h d is the set of distinct hashtags annotated to tweet d , where the hashtags are drawn from E . Note that, a tweet may have more than one hashtag and even duplicated hashtags. In our work, we only consider distinct hashtags for the same tweet. Intuitions and Assumptions . All our models are designed based on the following two intuitions:
As discussed in Section 1, a hashtag is a high-level abstraction of the tweet content. Among all factors, words in a tweet is the most import factor a ff ecting hashtag annotation. However, when com-posing a tweet with hashtag(s), there could be two possible cases: (i) user composes the tweet first and then finds appropriate hash-tags to annotate this tweet, or (ii) user has a hashtag ( e.g., a hashtag created for a popular event) in mind and writes a tweet for the hash-tag. To model the di ff erence in the order of generating tweet con-tent and hashtags, we propose two models: Content-Pivoted Model which assumes the tweet content is drafted first and the generation (or selection) of the hashtag is guided by the tweet content, and (ii) Hashtag-Pivoted Model which assumes that the user has selected the hashtag and then drafts the tweet content based on her under-standing of this hashtag. In both models, we assume that each tweet has only one topic due to its short length. The same assumption has been adopted in many other works [3,33]. In the following, we de-tail the two models and their inference algorithms.
Figure 1(a) (without dotted line) illustrates the Bayesian graph-ical representation of CPM . Some of the notations used in the model are summarized in Figure 1(c).

The topic z of tweet d is generated from personal interest of u or topic distribution of time slot t . That is, when user u publishes a tweet d in time slot t , she first decides whether to write anything related to her personal interests / activities or to comment on some hot topics in that time slot. More specifically, the topic z of tweet d can be generated from the user topic distribution p ( z | u ) and the time topic distribution p ( z | t ). We use a parameter  X  to balance the importance between p ( z | u ) and p ( z | t ): After a topic z is generated, all words w d in the tweet d are sampled from p ( w | z ). Then the hashtags of tweet d , h d , are sampled from p ( h | z ). The generative process of CPM is summarized as follows: HPM sn (with dotted line), and the notations used in the models. Observe that CPM model incorporates all factors user, time, tweet content, and hashtag into a PLSA framework. Further, the tweet content and hashtag are generated after a topic has been determined based on the user interests and (popular) topics of that time slot.
Similar to CPM, the HPM model also jointly considers user, time, tweet content and hashtag. However, as illustrated in Fig-ure 1(b) (without dotted line), HPM models hashtags as a high-level feature partially guiding the generation of tweet content. That is, when drafting a tweet, a user may choose to report her personal interests or comment on some hot events in that time slot as in CPM; in HPM a user may also choose to directly comment on a specific hashtag. In short, the topic z of a tweet may be drawn from user topic distribution p ( z | u ), time topic distribution p ( z | t ), or hashtag topic distribution p ( z | h d ). Note that, one tweet might have multiple hashtags. We assume that all hashtags of a tweet h share equal importance to the tweet d : C onsidering the three factors p ( z | u ), p ( z | t ), p ( z | h a tweet d written by a user u at time slot t with hashtag(s) h mind is:
Here, the two parameters  X  and  X  balance the importance of the three factors in selecting the topic of the tweet. Similarly, after generating the topic z of tweet d , all words w d are sampled from p ( w | z ). The generative process of HPM is as follows:
For both CPM and HPM , there is one latent variable topic z to be inferred. The exact inference algorithm is intractable. We pro-pose an Expectation-Maximization ( EM ) algorithm for appropri-ately inferring z in both models. Next, we first detail the inference algorithm for CPM . In CPM , the joint probability over tweet d and topic z can be represented as: where Accordingly, the log-likelihood in CPM is L = P d log P z We train the model using EM algorithm as follows:
The joint probability for HPM over tweet d and topic z is de-fined in the following equation, where p ( z | u , t , h d Equation 1:
In Equation 11, p ( h d ) = Q h  X   X  h d p ( h  X  ). The inference algo-rithm for HPM is similar to that of CPM . Specifically, the E-steps for both models are the same. In M-step, the estimations of p ( z | u ) , p ( z | t ), and p ( w | z ) in CPM also apply to HPM . The additional parameter p ( z | h ) in HPM is estimated as follows:
As discussed in Section 1, users who often mention each other are more likely to share similar topics. With the aim of obtain-ing more accurate topics in Twitter data, we utilize the mention relationship in Twitter as a regularization R over the topic distri-bution of a pair of Twitter users who have mentioned each other. More specifically, we minimize the proximity of topic distributions p ( z | u ) and p ( z | v ) of two users u and v who have mentioned each other for C uv number of times in their tweets (regardless u men-tions v or v mentions u ):
The two models CPM and HPM with social network regulariza-tion are denoted by CPM sn and HPM sn respectively. The dotted lines in Figures 1(a) and 1(b) denote the social network regular-ization. Regularized log-likelihood is expressed as RL = L  X   X  R , where  X  is the regularization parameter (  X  = 10 in our evaluation following the setting in [8]). We maximize the regularized log-likelihood using Generalized EM algorithm [18].

Except for p ( z | u ), all other parameters in CPM sn and HPM are estimated in the same way as their corresponding models CPM and HPM . Next, we use CPM sn as an example to estimate p ( z | u ) and the same applies to HPM sn . Let p i ( z | u ) be the estimation obtained in the i -th iteration of CPM sn , p i + 1 ( z | u ) in the ( i + 1)-th iteration is computed using Equation 13 based on the Newton-Raphson method [19]. Note that p 0 ( z | u ) is the p ( z | u ) estimated in CPM (see Equation 7). In the above equation,  X  is the step parameter (  X  = 0 . 1 in our im-plementation following the setting in [8]) and C uv is the number of times users u and v who have mentioned each other in their tweets. More details of the algorithm can be found in [8].
We conduct experiments to evaluate the performance of CPM and HPM using perplexity and show example topics discovered by the two models. We also evaluate the impact of introducing social network regularization in both models.
The tweets used in our evaluation are published by Singapore-based usersfrom January 1, 2011 to August 31, 2011. 4 Because our work focuses on the modeling of hashtag annotation, tweets with-out hashtags are not considered in our evaluation. In other words, each tweet used in our experiments contains at least one hashtag. Stopwords and non-English words are also removed from all tweets and tweets with empty content are then dropped. To ensure that each hashtag has a reasonable number of tweets for topic modeling, tweets annotated with extremely infrequent hashtags ( i.e., each is used to annotate fewer than 5 tweets in the whole collection) are (a) H ashtag frequency distribution Figure 2: Hashtag frequency distribution and number of hash-tags per tweet Number of tweets | D | 1,217,928 N umber of distinct hashtags | E | 14,055 N umber of distinct words or vocabulary size | V | 61,274 N umber of users | U | 13,711
N umber of time slots (days) | T | 243 removed from our collection. As the result, every hashtag in our fi-n al collection has been used to annotate at least 5 non-empty tweets written in English.

After preprocessing, the data set used in our experiments con-tains more than 1.2 million tweets published by over 13 thousand users in 243 days. The tweets are annotated by more than 14 thou-sand distinct hashtags. Table 1 reports the statistics of our pro-cessed data set.

Plotted in Figure 2(a), the hashtag frequency distribution follows a power-law like distribution. That is, most hashtags are used few times by few users, while a small number of hashtags are extremely popular and have been used to annotate many tweets. Observe that 82.2% of tweets in our collection are associated with one hashtag each (see Figure 2(b)). The remaining 17.8% of tweets, each is annotated by more than one hashtag. A small number of tweets are annotated by more than 10 hashtags each. Perplexity is a standard metric for evaluating topic models [1]. Defined in Equation 14, perplexity measures the ability of a model in generating unseen data ( i.e., D t est in the equation, which is a set of documents not used in model training). In this equation, p ( w d ) indicates the probability of generating all the words in a test document d  X  D t est , and N d denotes the number of words in document d . Lower perplexity indicates better model performance.
In our evaluation, we randomly select 200,000 tweets to be the testing data set, and the remaining 1,017,928 tweets are used to train the models. Next, we first examine the impact of user factor, time factor and the number of topics on the model performance of CPM and HPM respectively by perplexity. We then evaluate the e ff ectiveness of social network regularization on the two models by comparing their perplexity with that of CPM sn and HPM sn all our experiments, the number of iteration in training the models is fixed to 100.
 CPM Model Performance . Recall that in CPM , the topic z of a tweet d is generated from the user topic distribution p ( z | u ) and the time topic distribution p ( z | t ), balanced with a parameter  X  : user interest p ( z | u ) and time factor p ( z | t ), we vary  X  from 0 to ization on CPM and HPM (Figure (d)). 1, with a step of 0.1. Observe that when  X  = 0 topic z is gener-ated from time topic distribution only; and when  X  = 1, topic z is generated purely based on user interest. Figure 3(a) plots the per-plexity of CPM with varying  X  from 0 to 1 for four topic number settings K = { 25 , 50 , 100 , 150 } . We make three observations from this result.

First, parameter  X  has a significant impact on the perplexity of the model which is in the range from 3800 to 5400. With the four di ff erent topic number settings, the perplexity values follow very similar trends against the varying of  X  . When  X  = 0 . 9, the lowest perplexity is achieved for all the four topic number settings. Ei-ther  X  = 0 or  X  = 1 results in much poorer model performance, indicating that (i) both user interest and time are important factors a ff ecting the topic of tweets, and (ii) user interest is often the dom-inant factor in determining the topics of the tweets from a user. Second, regarding the choice of number of topics, K = 25 or K = 150 leads to poorer performance than K = 50 or K = 100. Particularly, K = 100 and  X  = 0 . 9 delivers the best perplexity in this set of experiments. In all our following experiments, we therefore set K = 100 and  X  = 0 . 9 as the default settings.
Third, when tweet topics are purely drawn from time topic dis-tributions ( i.e.,  X  = 0), the number of topics K has a limited impact on the perplexity. However, when tweet topics are solely generated based on user interest ( i.e.,  X  = 1), the smaller the number of topics ( i.e., K = 50), the better the perplexity. This observation suggests that a common user usually does not show interests in too many di ff erent topics.
 HPM Model Performance . Compared with CPM , HPM con-siders one more factor p ( z | h d ) in generating the topic of a tweet.  X  ) p ( z | h d ). Note that  X  = 0 leads to tweet topic generation solely based on hashtags p ( z | h d ).

Based on the results of CPM , we first set  X  = 0 . 9 and evaluate the perplexity of HPM against the varying of  X  from 0 to 1 with a step of 0.1. Demonstrated in Figure 3(b) the impact of  X  on HPM is not significant when  X   X  0 . 1 for all K values. When K = 100, HPM achieves the best perplexity when  X  = 0 . 6. However, it is observed that the perplexity is much poorer when  X  = 0, i.e., the topic of a tweet is purely generated based on hashtags.
 Next, we fix  X  = 0 . 6 and vary the values of  X  from 0 to 1 (see Figure 3(c)). Similar to that in CPM , the perplexity of HPM is best when  X  = 0 . 9 for all the four di ff erent numbers of topics. Compared with CPM , HPM performs better by perplexity, with perplexity ranging from 3600 to 4100. One reason is that HPM treats hashtags as topic vectors which could better cluster the words in tweets leading to better topic cohesion.
 Social Network Regularization . We now evaluate the impact of considering social factor in the two models. In this set of experi-ments, we set number of topics K = 100 for all four models: CPM , CPM s n , HPM and HPM sn . For both HPM and HPM sn ,  X  is set to 0.6 based on earlier experimental results. The two additional parameters  X  and  X  in CPM sn and HPM sn are experimentally set to  X  = 0 . 1 and  X  = 10 (see Section 3.4).

Figure 3(d) shows the perplexity of all four models with  X  vary-ing from 0 to 1. Note that when  X  = 0, user interest is not con-sidered in the model and therefore no social factor is considered as well. As shown in Figure 3(d), the introduction of social network regularization makes both models much worse in terms of perplex-ity. One possible reason is that, two users may mention each other because of common interests in some but not all the topics. The assumption that a pair of users who mention more about each other are more likely to share similar topic distributions might be too strong. However, on the other hand, predetermining a subset of common topics for a given pair of users is infeasible in generative models. We now present 8 sample topics discovered by the two models CPM and HPM . For both models, we set the number of topics to be 100. From the 100 topics, we select 8 topics as examples.
Table 2 lists these 8 topics. We further manually label these 8 topics to better explain them. For each topic CPM generates word probability p ( w | z ) and hashtag probability p ( h | z ) (see Sec-tion 3.1). We therefore list both the top words and the top hashtags according to their generative probabilities for each of the 8 sample topics. For clarity, we name these two kinds of topics word topic and hashtag topic respectively. For HPM , the model only gen-erates word topic based on p ( w | z ) (see Section 3.2). Twitter top-ics can be categorized into exogenous topics and endogenous top-ics [17]. Exogenous topics ( e.g., #earthquake and #flood ) are orig-inated outside of Twitter and endogenous topics ( e.g., #10thingsi-hate and #nowplaying ) are originated within Twitter. As shown in Table 2, both CPM and HPM models capture the major topics dis-cussed by Singapore users in Twitter from January to August 2011. Among them Singapore General Election 5 and Japan earthquake 6 are major exogenous events in our data set. J obs , music and daily life are example continuous endogenous topics discussed in Twit-ter. In short, CPM and HPM are able to explore both exogenous topics and endogenous topics.

Next we discuss the relationship between word topic and hash-tag topic generated by CPM (see rows labeled by  X  CPM Hash-tag X  and  X  CPM Word X  in Table 2). Observe that most top-ranked hashtags of hashtag topic are well associated semantically with the corresponding word topics. Take the first topic job as an example, the top-ranked hashtags ( i.e., #job , #jobs , #career , #interview ) and the top-ranked words ( i.e., questions, interviewer, job, di ff erence) are closely associated semantically. Generally speaking, topical words of CPM are relatively more specific while topical hashtags of CPM are more general. However, because a topic is usually an-notated by few dominant hashtags only, the top-10 hashtags listed for each topic in Table 2 might not all describe the corresponding topic. For instance, hashtag #oscars is not very relevant to Japan earthquake and #royalwedding is irrelevant to Harry Porter movie . Some hashtags are extremely popular ( e.g., #fb , #singapore ) and are often used to annotate many di ff erent topics.
 HPM only generates word topics (see rows labeled by  X  HPM Word X  in Table 2). Some of the top-ranked topical words of CPM and HPM are very similar. The topic labeled digital devices is an example. However, HPM discovers several topics which can not be found in CPM , listed in Table 3. These topics include royal wedding , food , business , and shopping . HPM is more powerful in finding less popular topics like business and shopping , which also partially explains why the perplexity of HPM is better than that of CPM .

Next, we show the topic distribution generated by HPM for three example hashtags: #sgelections , #royalwedding and #pray-forjapan . For each example hashtag, Figure 4(a) lists their top-10 topics ranked by probability p ( z | h ) in descending order. Observe that both #prayforjapan and #royalwedding were popular for about two weeks, a relatively short time period. For each of the two hash-tags, there is one dominant topic, with the highest probability. For instance, the probability of the top topic for #prayforjapan is nearly
F igure 4: Topic distribution of top-3 most relevant topics 50%. For #sgelections , it was popular for a few months and was adopted to annotate tweets for two elections (parliamentary gen-eral election and presidential election). Three topics are observed to have high probabilities for this hashtag. For all the three example hashtags, the topical keywords of the top-3 topics with the highest probabilities are listed in Figure 4(b).
In this section, we present two applications as case studies to illustrate the e ff ectiveness of our models in addressing practical problems in Twitter. We first motivate the two problems, namely R etrospective Hashtag Annotation and Related Hashtag Discovery , and then present experimental results.
Hashtag facilitates tweet search and information di ff usion. How-ever, only about 10% of tweets are annotated by hashtags, observed from our data and also reported in other studies [11]. As surveyed in Section 2, many studies have been carried out on hashtag rec-ommendation. Most hashtag recommendation methods target on online recommendation ( i.e., to recommend one or more hashtags when a user posts a new tweet) because Twitter is widely accepted as a real-time media. However, the historical data accumulated in Twitter remains an important and rich information source for more advanced tweet search options and other applications like retro-spective event detection. Annotating historical tweets also helps to finding relevant tweets for less popular hashtags. As a case study, we evaluate the e ff ectiveness of our models in Retrospec-tive Hashtag Annotation which aims to annotate existing tweets without hashtags. More specifically, given a tweet d published by user u at time t , the task of retrospective hashtag annotation is to annotate this tweet with the most appropriate hashtag(s). That is, we recommend hashtags to historical tweets. Next, we present the baseline method proposed in [13] and discuss the solutions using our models.
 Baseline methods: CF and CFU . A collaborative-filtering ( CF ) based method proposed in [13] recommends hashtags to a tweet by considering both the tweet content and the user. Given a tweet d , the method finds the top-x most similar tweets with hashtags by content similarity ( e.g., cosine similarity). The most frequent hash-tags used by these top-x tweets are recommended. We name this method the CF method. The authors in [13] also propose a method which considers user factor, which we call the CFU method. In CFU , each user is represented as a hashtag vector. This hashtag vector is weighted by the TF  X  IDF scheme where the TF is the number of times this user has used a hashtag in all her tweets, and IDF is computed from the number of distinct users who have used this hashtag. With this hashtag vector, the top-y most similar users to a user u are retrieved. Then the hashtag to be recommended to a tweet d by user u is based on (i) the number of times a hashtag is used to annotate the top-x most similar tweets (from all users), and (ii) the number of times a hashtag has been adopted by the top-most similar users.
 Our proposed methods: CFU + CPM and CFU + HPM . Both CPM and HPM model the three factors user, time, and tweet content in hashtag annotation. Given a tweet d written by user u at time t , the two models are able to directly estimate p ( h | u , t , w most straightforward method for retrospective hashtag annotation is therefore to rank hashtags by this probability. This method, how-ever, delivers poorer accuracy than the baseline methods. The rea-son is that many hashtags are under-represented because of their very limited usage in tweets. Recall that, the usage of hashtag follows a power-law like distribution (see Figure 2(a)) and most hashtags are used to annotate a small number of tweets, making the estimation p ( h | u , t , w d ) less accurate for these hashtags.
To address this issue, we combine the recommendation by our models and the recommendation by the baseline methods. Gener-ally speaking, the combined method recommends hashtags by con-sidering both the global factors ( i.e., the latent relationship between hashtag and user, time, and tweet content based on our models) and the local factors ( i.e., the most similar tweets and most similar users based on the baseline methods). In this following, we use CFU + CPM as an example to illustrate the combined method.
Let r h be the number of times a hashtag h is recommended by the baseline method CFU for tweet d . Let p n ( h | u , t , w normalized recommendation score from CPM : w here the joint probability p ( u , t , w d , h ) = P  X  z p ( u , t , z , w d , h ) can be estimated with Equation 2 by replacing h with h in the equation. For HPM , p ( u , t , w d , h ) is computed in a similar manner based on the joint probability p ( u , t , h defined in Equation 11.

The recommendation score of hashtag h , denoted by Score ( h ), by the combined method CFU + CPM is: In the above equation, the logarithm function is introduced to re-duce the impact of extremely popular hashtags. Note that, if a hashtag h does not receive any recommendation from CFU , then Score ( h ) = 0 and this hashtag will not be recommended. Experimental Setting . We randomly select 200,000 tweets as test set and the hashtags adopted by these tweets are considered as the ground truth. We use Hit Rate to evaluate the annotation accuracy. Given a tweet, a hit occurs if at least one of the top-n recommended hashtags matches the ground truth hashtags of the tweet. The hit rate for a method is computed by the number of hits divided by the number of test tweets. We report the hit rate for top-5 and top-10 recommendations for all methods. We evaluated six methods in total: CF , CFU , CFU + CPM , CFU + CPM sn , CFU + HPM , and CFU + HPM sn .
 Experimental Results . Recall that in CFU , top-x most similar tweets and top-y most similar users are retrieved for hashtag rec-ommendation. In our experiments, we set x and y to be the same and evaluated 4 settings: x = y = 5, 10, 15, or 20. The hit rates of top-5 and top-10 recommendations are reported in Figures 5(a) and 5(b) respectively for the six methods. We make the following three observations from the results.

First, for both top-5 and top-10 hashtag recommendations, the methods with either CPM or HPM perform better than both base-line methods CF and CFU . In particular, in terms of hit rate for top-5 hashtag recommendation with 5 similar tweets / users, CFU + CPM outperforms CFU by 6.72% and CF by 14.34% respectively. We also observe that CFU + HPM yields very similar results as CFU + CPM , despite that HPM achieves better perplexity than CPM in our earlier experiments.
 Second, CFU + CPM sn performs slightly worse than CFU + CPM and the same observation holds for CFU + HPM sn against CFU + HPM . In other words, considering social network regular-ization does not improve the hit rate for hashtag recommendation. One possible reason is that the social network regularization intro-duces noises in estimating p ( z | u ). Consequently, the poorer esti-mation of p ( z | u ) results in less accurate p ( h | u , t , w is consistent with the results reported in Section 4.2 where the con-sidering social network regularization leads to poorer perplexity to both models.

Third, evaluated by hit rate of top-5 hashtag recommendation, the hit rate for all methods decreases along with increasing the number of similar tweets / users. This observation suggests a larger number of similar tweets / users likely brings in irrelevant hashtags to the given tweet, particularly when the ground truth hashtag is an infrequent hashtag. Recall that hashtag frequency distribution fol-lows a power-law like distribution and a large number of hashtags appear only 5 times in our dataset (see Section 4.1). F igure 5: Hit rate of the four methods for top-5 / top-10 hashtags
Hashtags are chosen by Twitter users from an uncontrolled vo-cabulary. For the same event or the same topic, multiple hashtags might be chosen by users, e.g., #cikm , #cikm14 ,or #cikm2014 for the same conference. Hashtags might also be related because of other types of relationships such as subsumption relation. For ex-ample the hashtag #sgelections has been used to annotate tweets related to both the Singapore Parliamentary General Election 7 in May 2011 and the Singaporean Presidential Election 8 in August 2011, while a more specific hashtag #sgpresident was also widely adopted for the latter. Discovering related hashtags helps users in refining, extending or reformulating hashtag-based queries.
Specifically, given a hashtag h and hashtag vocabulary E , related hashtag discovery is to locate the top-n hashtags from E (without h itself) that are most related to h . In this set of experiments, we evaluate four methods for their e ff ectiveness in finding most related hashtags of a given hashtag.
 Co-occurrence ( COO ) . A straightforward method in finding the related hashtags is through co-occurrence. If a hashtag h co-occurs with the given hashtag h in tweet annotation, then h believed to be related with h .
 Content-based Similarity ( CBS ) . Two hashtags are related if they share similar semantic meanings defined by the sets of tweets an-notated by them. Given a hashtag h , all tweets annotated by h combined together form a virtual document. Then the similarity
Table 4: Kappa scores between three pairs of volunteers ( Table 5: Precision for related hashtag discovery with the best r esult in boldface between two hashtags is computed based on the cosine similarity o f the two corresponding virtual documents.
 Content-and Topic-based Similarity ( CTS ) . In this method, we use the topic-based feature representation to enhance the hashtag similarity computation. More specifically, each hashtag can be rep-resented by a topic vector, where each dimension is one of the K topics and is weighted by p ( z i | h ), 0  X  i  X  K . Let S the content-based similarity between hashtags h and h  X  computed in CBS, and let S t ( h , h  X  ) be the cosine similarity between the topic vector representations of the two hashtags. The CTS similarity be-tween the two hashtags is: S ct ( h , h  X  ) =  X   X  S c ( h , h S ( h , h  X  ), where  X  is a parameter for the combination. The follow-ing question is: how to compute p ( z | h ) using the two models? To summarize, we have four methods for evaluation: COO , CBS , CTS C P M , and CTS H P M where for the latter two CPM and HPM denote the model for computing the topic vector for hashtags.
To evaluate the e ff ectiveness of the four methods in finding re-lated hashtags, we randomly selected 50 hashtags among the top-500 most popular hashtags to be the query hashtags. 9 For each of the 50 query hashtags, a method returns the top-5 most related hashtags for manual assessment. In CTS,  X  is set to 0.6 in our ex-periments based on observations using a few sample hashtags (not included in the 50 query hashtags). We employ three volunteers to label the relatedness of the top-5 hashtags returned by each method and each hashtag receives a binary score: 0 for not-related and 1 for related. The kappa scores of the agreement between any pair of the volunteers are reported in Table 4. The average kappa score is 0.723 suggesting substantial agreement between our volunteers.
The average precision for the 50 query hashtags from the three volunteers is reported in Table 5. Observe that COO results in the poorest precision. This is because 82.2% of tweets each is anno-tated with only one hashtag (see Section 4.1). Consequently, there might be too few co-occurring hashtags for a given query hashtag. Among the other three methods, which utilize content similarity, CTS C P M and CTS H P M outperform the method not using topic vector. This demonstrates that the e ff ectiveness of using topic vec-tor as additional information in enhancing related hashtag discov-ery. Observe that CTS H P M achieves the highest precision, prob-ably because HPM discovers more meaningful topics reflected by the lowest perplexity (see Section 4.2).

We now use two examples hashtags #prayforjapan and #movies to illustrate the di ff erence between the most related hashtags found Table 6: Top-5 most related hashtags to # prayforjapan and #movies , discovered by the four methods by the four methods, listed in Table 6. Among the top-5 most re-la ted hashtags for #prayforjapan found by COO , #fb and #sleague are not related. All the remaining three methods CBS , CTS and CTS H P M are able to find related hashtags for #prayforjapan . Interestingly, the two methods with topic-level representation rec-ommend the same set of hashtags in slightly di ff erent orders. An-other example is #movies . All top-5 hashtags by CTS H P M relevant to #movie . The hashtags from the other three methods all contain some irrelevant hashtags such as #sg , #xinmsn and #east-boundanddown .
In this paper, we propose two PLSA-style topic models to model the latent relationship between tweet content, user interest, time, and hashtag at topic-level. We also evaluate the impact of consid-ering social network regularization based on mention relationship in Twitter. Through extensive experiments, we show that Hashtag-Pivoted Model outperforms Content-Pivoted Model in terms of per-plexity measure. We also show that the social network regulariza-tion based on mention relationship hurts the performance of both models. We further demonstrate the e ff ectiveness of the two models in addressing two practical applications ( i.e., retrospective hashtag annotation and related hashtag discovery). The utilization of both models improves the e ff ectiveness in addressing both applications compared to their corresponding baselines.

Recall that the two models follow di ff erent assumptions to sim-ulate the two possible generation processes of hashtag and tweet content. However, given a tweet, there is no mechanism to predict which model best reflects the generation process between its hash-tag and content. Research on such predicting mechanism is part of our future work. Another piece of future work is to evaluate the impact of social network regularization to the models based on other types of user relationships other than mention relationship. Furthermore, we will continue to apply our models to practical ap-plications in tweets such as hashtag summarization.
