 Active Learning is a machine learning and data mining technique that selects the most informative samples for labeling and uses them as training data; it is especially useful when there are large amount of unlabeled data and labeling them is expensive. Recently, batch-mode active learning, where a set of samples are selected concurrently for labeling, based on their collective merit, has at-tracted a lot of attention. The objective of batch-mode active learn-ing is to select a set of informative samples so that a classifier learned on these samples has good generalization performance on the unlabeled data. Most of the existing batch-mode active learn-ing methodologies try to achieve this by selecting samples based on varied criteria. In this paper we propose a novel criterion which achieves good generalization performance of a classifier by specifi-cally selecting a set of query samples that minimizes the difference in distribution between the labeled and the unlabeled data, after annotation. We explicitly measure t his difference based on all can-didate subsets of the unlabeled data and select the best subset. The proposed objective is an NP-hard integer programming optimiza-tion problem. We provide two optimization techniques to solve this problem. In the first one, the problem is transformed into a convex quadratic programming problem and in the second method the problem is transformed into a linear programming problem. Our empirical studies using publicly available UCI datasets and a biomedical image dataset demonstrate the effectiveness of the pro-posed approach in comparison with the state-of-the-art batch-mode active learning methods. We also present two extensions of the pro-posed approach, which incorporate uncertainty of the predicted la-bels of the unlabeled data and transfer learning in the proposed for-mulation. Our empirical studies on UCI datasets show that incor-poration of uncertainty information improves performance at later iterations while our studies on 20 Newsgroups dataset show that transfer learning improves the performance of the classifier during initial iterations.
 H.2.8 [ Database Management ]: Database Applications -Data Min-ing Algorithm Active learning, marginal probability distribution, Maximum Mean Discrepancy
Classification has been an active research topic in data mining and machine learning. The availa bility of a l arge amount of digi-tal data in recent years, has greatly expanded the opportunities of automated data classification using data mining and machine learn-ing techniques. One of the prerequisites for any data classification framework is the availability of labeled examples. Annotating large quantities of data for developing automated classifiers is a time con-suming and expensive process. Hence there is a need to select an optimal set of instances from the pool of unlabeled data for label-ing, such that a classifier learned on the selected instances performs well on the unlabeled data and also on unseen data belonging to the same distribution. Randomly selecting a set of unlabeled instances may result in selection of redundant and non-informative instances. Active learning methodologies enable selection of a set of most in-formative unlabeled instances from enormous amount of unlabeled data for manual labeling, with the intention of developing a good classifier with low generalization error. Specifically, the goal of active learning is to label as little data as possible, to achieve a certain classification performance, thus saving considerable anno-tation cost for training a good learner.

Active learning methodologies iteratively select the most infor-mative data. Informativeness of a data sample or a set of data sam-ples is measured by their potentiality in increasing the performance of a classifier, once their label is known [32]. Many researchers have addressed the active learning problem in various ways [23]. Most have focused on selecting a single most informative unla-beled instance to query each time. The most popular approaches include query-by-committee [24, 7, 9] where a number of distinct classification models are generated and an instance having the most disagreement among the classification models in predicting the la-bel is selected for querying. Another popular approach is querying an instance with maximum uncertainty of labeling measured by the distance from the classification boundary [6, 22, 28] or by the en-tropy in the predicted label [17, 16].
Most single instance selection methods require to retrain the clas-sifier with each single instance being labeled. The retraining pro-cess between queries can make the process very slow. Furthermore, if a parallel labeling system is ava ilable, e.g., multiple annotators working parallely, single instance selection would not be able to make the effective use of the resources. A batch-mode active learn-ing strategy, that selects multiple instances each time is more ap-propriate under these circumstances. However, the challenge in batch-mode active learning is the formulation of the selection cri-teria for multiple instance selection. Using a single instance selec-tion strategy to select a batch of queries in each iteration by ranking them based on their individual merit may not give good results, as this strategy fails to take into account the information overlap be-tween multiple instances. Hence principles for batch-mode active learning to select a set of instances simultaneously based on their collective merit, need to be developed to address the multi-instance selection specifically .

The batch-mode active learning methods are particularly suitable for large scale applications where the data has high redundancy such as text classifications [11], content based image retrieval [14] and image recognition [13] due to high frame rate. The greatest challenge in selecting a set of instances simultaneously is two fold. The first challenge lies in the formulation of a right objective which will be optimized to select the most informative set of samples and the second challenge is concerned with the computational complex-ity of the NP hard combinatorial integer programming problem for obtaining a good local solution.

Recently, several sophisticated batch-mode active learning meth-ods based on optimizing an information measure have been pro-posed for classification. Guo [11] selected a batch of query sam-ples based on maximum mutual information with the unlabeled set of data, while Hoi et al. [13] applied Fisher information matrix to select a set of informative instances. Yu et al. [31] selected a set of instances closest to the basis vectors that approximate the set of unlabeled data and Guo and Schuurmans [12] proposed a discrimi-native approach.

In this paper, we propose a novel batch-mode active learning ap-proach that selects a batch of query instances such that the distribu-tion represented by the selected query set and the available labeled data is closest to the distribution represented by the unlabeled data. The motivation behind this approach is to ensure that a classifier learned by minimizing loss on the selected set of labeled data has good generalization capabilities on t he unlabeled data and also on the unseen data coming from the same distribution. In other words, in order to learn a classifier with a budgeted number of labeled data we select a set of samples S from the unlabeled set of data, denoted by U , such that the probability distributions represented by L and U \ S ,where L is the set of available labeled data, are similar to each other.

We measure the difference in the probability distribution between the two sets of data using the Maximum Mean Discrepancy (MMD) proposed by Borgwardt et al [4, 1, 26]. Maximum Mean Discrep-ancy is a statistical test based on the fact that two distributions are different if and only if there exists at least one function in a char-acteristic RKHS [26] having different expectations on the two dis-tributions. MMD has been proven to be very accurate in finding samples that were generated from the same distribution and outper-forms its best competitors. MMD has been widely and successfully used in various classification tasks to ensure similarity in distribu-tion between training and test data, specifically in the context of transfer learning applications [15, 20]. We use this measure in an optimization formulation to select a subset S out of all candidate subsets, based on minimum distribution difference between L and U \ S . To the best of our knowledge, this is the first work that uses MMD in the active learning context.

Figure 1 shows the data points selected by the proposed method (red triangles) under three different distributions of unlabeled data (dark green squares). We created six dense regions of two differ-ent densities and kept a budget of nine query points which were selected in batches of three, in three iterations. We started with two randomly selected data samples (blue circles). We observe that the proposed method selects points from every dense region. It is also interesting to note that the number of points that get selected from each dense region is approximately proportional to the den-sity of the region i.e., comparatively more data samples get selected from denser regions, A, B [Figure1 (a)], C, D, E, F [Figure1 (b)], and G, H [Figure1 (c)], thus preserving the distribution of the un-labeled data. We also observe that since available labeled data is considered at every iteration, hence diversity with respect to avail-able data is maintained in query selection, as shown in Figures 1 (b) where no query data gets selected from the small regions in the center as an instance from those regions is already available in the initial labeled set. More details about the properties of query points are provided in Section 2.2. We also observe that the proposed ap-proach decreases MMD monotonically as more data samples are selected from the unlabeled data and the decrease in MMD value corresponded to the increase in classification accuracy on the test set, discussed in detail in Section 4.

The subset selection problem is an NP-hard combinatorial inte-ger programming problem. The proposed formulation is an inte-ger quadratic programming problem. We show that the quadratic formulation can be reformulated as an integer linear programming problem. We then provide two optimization techniques to solve this problem. In the first method, we solve a continuous quadratic pro-gramming problem (by relaxing the integer constraint) on a convex function, providing a global solution. This is unlike most of the state-of-the art batch-mode active learning methods which provide a local solution following a gradient descent method [12, 11] or a greedy algorithm [13, 5, 30]. In the second method, we solve a continuous integer programming problem.
We tested our method on publicly available UCI 1 datasets and on a biomedical image dataset, Fly-FISH [18], consisting of im-ages representing different developmental stages in the life cycle of Drosophila . The manual annotation of developmental stages of the images of Drosophila is an expensive task. Hence active se-lection of an optimal number of images is crucial for the develop-ment of an automatic classifier. The empirical results on UCI and Fly-FISH datasets show that the proposed batch-mode active learn-ing approach achieves superior or comparable performance to the state-of-the-art batch-mode active learning methods. The proposed method is also significantly time efficient compared to the state-of-the-art batch-mode active learning methods.

We further extend the proposed method by incorporating uncer-tainty of the predicted labels of the unlabeled data and transfer learning in the proposed formulation. Our empirical studies on UCI datasets show that incorporation of uncertainty information improves performance at later iterations while our studies on pub-licly available 20 Newsgroups dataset 2 show that transfer learning improves the performance of the classifier during initial iterations. The source codes along with synthetic data are available online (www.public.asu.edu/  X  rchattop/code/MP-AL).

The remainder of the paper is organized as follows. Section 2 in-troduces the proposed batch-mode active learning framework. Sec-tion 3 compares the proposed method with the state-of-the-art ac-tive learning methods. Empirical studies are presented in Section 4. We present two extensions of the proposed approach in Section 5, and Section 6 concludes this paper.
The key hypothesis in active learning is that if the learning al-gorithm is allowed to choose the data from which it learns, it will perform better even with less annotation [23]. Given a parametric classification model, the learning algorithms often learn the param-eters  X  by maximizing the joint probability P ( X,Y |  X  )= P ( X P ( Y | X, X  ) where X and Y are represented empirically by the training data X tr = { x 1 ,x 2 ,  X  X  X  ,x n } and their corresponding labels Y tr = { y 1 ,y 2 ,  X  X  X  ,y n } and P ( X ) and P ( Y the marginal and conditional probability distribution of X and Y respectively. Traditional data mining and machine learning algo-rithms are based on the assumption that the training data ( X represents the true underlying distributions of X and Y and hence a model learned on this data works well for the test data ( X which is also drawn i.i.d. from the same distribution. When the distributions on the training and test set are different the classifica-tion model learned on the training data performs poorly on the test data due to model mismatch.

The proposed active learning method addresses this issue by it-eratively selecting a set of query instances from the unlabeled data such that the distribution represented by the queried and labeled data ( X tr ,Y tr ) , is similar to the probability distribution of the un-labeled data set. In other words, in order to learn a classifier with a budgeted number of labeled data, the proposed method iteratively selects a set of samples S from the unlabeled set of data, denoted by U , such that the joint probability distributions P ( X,Y ) repre-sented by X tr = L  X  S and X tst = U \ S , where L is set of avail-able labeled data, are similar to each other. Since the labeling func-tion or the conditional probability P ( Y | X ) remains the same for both S and U \ S as they are drawn from the same underlying dis-tribution, the problem reduces to selecting S such that the marginal probability P S  X  L ( X ) is similar to P U \ S ( X ) . In this paper, we
Available at http://www.ics.uci.edu/mlearn/MLRepository.html
Available at http://www.ai.mit.edu/ jrennie/20Newsgroups/ measure the difference in the marginal probability distribution be-tween the two sets empirically using Maximum Mean Discrepancy (MMD) [4, 1, 26]. The difference between the empirical means of two distributions after mapping onto a reproducing kernel Hilbert space, called Maximum Mean Discrepancy, has been shown to be an effective measure of the difference in their marginal probability distributions. We review the basics of MMD below.
Let X = { x 1 ,  X  X  X  ,x m } and Z = { z 1 ,  X  X  X  ,z n } be two sets of samples drawn randomly from a target population. Let p and q be the probability distributions defined on the basis of sample sets X and Z respectively. The Maximum Mean Discrepancy (MMD) proposed by Borgwardt et al [4, 1, 26] is a statistical tool that pro-vides a method for testing whether two distributions p and q from which X and Z have been drawn respectively are similar or not.
The principal underlying the Maximum Mean Discrepancy is to find a function that assumes different expectations on two different distributions so that when evaluated empirically on samples drawn from the different distributions it would tell us whether the distribu-tions are similar or not. Let F be a class of functions f : and let X , Z , p , q be defined as above. Then the Maximum Mean Discrepancy and its empirical estimate are defined as:
MMD [ F ,X,Z ]:=sup
Intuitively if F is  X  X ich enough X , MMD [ F ,X,Z ] will vanish if and only if p = q . A class of functions for which MMD may eas-ily be computed, while retaining the ability to detect all discrepan-cies between p and q without making any simplifying assumptions is the complete inner product space H (i.e., a reproducing kernel Hilbert space (RKHS) [27]) of functions f : X X  X  ,where X is a nonempty compact set and for all x  X  X  , the linear point eval-uation functional mapping f  X  f ( x ) exists and is continuous. In this case, f ( x ) can be expressed as an inner product via where  X  : X X  X  is known as the feature space map from X to [4]. When F is the unit ball in a characteristic RKHS [26], MMD is defined as the difference between the means of two distributions after mapping onto the characteristic RKHS. An empirical estimate of MMD is then obtained as follows: For more details about Maximum Mean Discrepancy, the related theoretical proofs and comparison with related methods, interested readers may refer to [4, 1, 26]. The proposed batch-mode active sampling method, referred to as Marginal Probab ility based Active Learning (MP-AL) iteratively selects batches of query instances which represent best the distribu-tion of the unlabeled instances so that a classifier learned by min-imizing risk on the queried data after labeling, has good general-ization performance on the unlabeled data set and on future unseen data that comes from the same distribution. We formulate the prob-lem as an integer quadratic programming problem which can be reformulated as an equivalent integer linear programming problem.
The proposed framework uses MMD to measure the distribution difference between two sets of samples. Let us assume that we have n u instances of unlabeled data U and n l instances of labeled data L and we would like to select a batch S of b instances such that the distribution of L  X  S is similar to the distribution of U case, the MMD between the sets L  X  S and U \ S defined by f ( S ) , can be computed using the expression in Equation (4), as follows: f ( S )= Since we want to select a set S that minimizes the mismatch be-tween L  X  S and U \ S we propose to select a subset S of U that minimizes f ( S ) . Next we define a binary vector  X  of size n each entry  X  i indicates whether the data x i  X  U is selected or not. If a point is selected, the corresponding entry  X  i is 1 else 0 . Thus the minimization problem reduces to finding  X  that minimizes the cost function f ( S ) : where 1 is a vector of the same dimension as  X  with all entries 1 and symbol T is used to represent the matrix or vector trans-pose operation. Evidently, the cost function in Equation (6) is an alternative (equivalent) representation of the cost function f ( S ) in Equation (5). The first term denotes the mean of the mapped fea-tures of the labeled and selected points. Note that if a point x not selected in the current set then  X  i will be 0 and this term would not get added in the summation. The second term is mean of the mapped features of the unlabeled data set minus the selected query set. The first constraint ensures that each entry in  X  is either 0 or 1 and the second constraint ensures that exactly b entries of  X  are 1 , meaning exactly b instances are selected from the unlabeled data set, where b is specified a priori by the user. The above formulation can be represented as: The various terms in the above expression are given as follows. We denote G as the ( n u + n l )  X  ( n u + n l ) kernel Gram matrix over the unlabeled data U and labeled data L , arranged in order, using a kernel function K such that G ( i, j )= K ( x i ,x K k
Based on the above expressions, we can draw the following ob-servations regarding the properties of the selected query set: Thus the proposed method selects examples which meet all the de-sirable properties for batch mode active learning. The proposed method can be easily extended to add any other evaluation criteria ( M ) by adding a corresponding linear term M T  X  , while still main-taining the quadratic form (see Section 5 for more details). Also the proposed method does not depe nd on the availability of labeled data to initiate the process of selecting a query set, in which case n =0 , and the third term k T 3  X  in Equation (7) vanishes.
The above optimization formulation is an integer quadratic pro-gramming problem. Next, we show that it can be reformulated as an equivalent integer linear programming (ILP) problem.
Due to the binary constraint  X  i  X  X  0 , 1 } ,  X  i , the linear terms in the objective defined in Equation (7) can be absorbed into the quadratic term by subtracting k 2 and adding k 3 terms to the diago-nal entries of matrix K 1 , forming a D matrix given by: D ( i, j )= K 1 ( i, j )  X  k 2 ( i )+ k 3 ( i ) for i = j and D ( i, j )= K wise. We can thus rewrite the optimization problem in (7) as: We next introduce a binary matrix Z =( z ij ) with z ij =  X  Thus the optimization in Equation (8) becomes: The quadratic equality constraint z ij =  X  i  X  j makes the problem difficult to solve. We show that this can be represented by a set of linear inequalities. Since d ij can have both negative and positive values, we rewrite the quadratic constraints as follows:
The first constraint ensures that z ij equals zero if the value of  X  or  X  j (or both) is zero. If  X  i and  X  j both equal to one, the value of z ij is free to be either 0 or 1 ; however, the minimization forces the value of z ij to be 1 since d ij is negative. Similarly, we derive the second constraint when d ij is positive. The second constraint makes the value of z ij equals to 1 if  X  i and  X  j both equal to one. If any of the  X  i and  X  j equals zero, then the value of z ij either 0 or 1 ; nevertheless, at optimality, z ij must equal 0 since d has positive contribution to the objective, which is a minimization of the cost function. Consequently, the relation between z the pair  X  i and  X  j at optimality is as follows: z i,j =1 , if and only if  X  i =1 and  X  j =1 . Thus, the formulation in Equation (10) is equivalent to the original integer quadratic programming problem.
Next, we present two algorithms to solve the integer quadratic and integer linear optimization problems defined in Equation (7) and Equation (10) respectively as quadratic programming (QP) and linear programming (LP) problems by relaxing the binary constraints.
The binary constraint on  X  i makes the integer quadratic problem defined in Equation (7) NP-hard. A common strategy is to relax the constraints to make it a continuous optimization problem, which can be solved in polynomial time: The minimization problem in (11) is a standard quadratic prob-lem (QP) and can be solved efficiently by applying many existing solvers. We used the  X  X uadprog X  function in MATLAB to solve this QP problem. The overall algorithm for selecting the query set S at any iteration is given in Algorithm 1.
We relax the integral constraint and obtain a linear program (LP) formulation which is a relaxation of the ILP formulation in (10) as follows:
The LP formulation can be further simplified by incorporating the first constraint into the objective function. Since this is a min-imization problem when d ij &lt; 0 , at optimality, z ij = lowing the first equality constraint. On the other hand, the second equality constraint for d ij  X  0 , may not hold at optimality. Since removing or relaxing a constraint of a minimization program does not reduce the optimal value, the formulation in Equation (12) can be reformulated as follows:
The problem in Equation (13) is a standard linear programming problem, and can be solved efficiently using any standard LP solver. Since the Hessian matrix K 1 in Equation (7) is a kernel Gram ma-trix which is positive semi-definite hence both formulations are convex. We used CVX [10] to solve the LP problem. The over-all algorithm for selecting the query set S at any iteration using the LP formulation is given in Algorithm 1.
 Algorithm 1 MP-AL 2: Output: S : query set; 4: if QP Problem then 5: Compute  X  by solving (11). 6: end if 7: if LP Problem then 9: Compute  X  by solving (13). 10: end if 12: Update sets L and U : L  X  L  X  S , U  X  U \ S .
We compared the performance of the proposed method with state-of-the-art batch-mode active learning methods including Matrix [11], Disc [12] and Fisher [13] which selected a set of instances that are together maximally informative, similar to the proposed approach. We also compared our approach with state-of-the-art batch-mode active learning methods which selected a set of instances in each iteration based on their individual merits such as svmD [5] and a multiple criteria based instance selection method [30], referred to in this paper as MCS for convenience. Besides, we also compared our method to one transductive experimental design method [31], referred to as Design , which is based on regression models. A brief review of each of these methods is presented below.

The Disc method selects a set of instances by maximizing the likelihood of labeled and selected instances, while minimizing the uncertainty of unlabeled instances, based on a classifier learned on the labeled and selected instances. The problem formulation is non-convex and a local solution is obtained using the gradient descent method. The Matrix method selects a batch of queries S in each iteration by maximizing a mutual information criterion between the selected and labeled instances ( L = L  X  S ) and unlabeled instances ( U = U \ S ) as follows: where  X  U U and  X  L L are covariance matrices of U and L computed using Gaussian kernel. Similar to the Disc method, this formulation is non-convex and a local solution is obtained using the gradient descent method. Similar to proposed approach, this method does not depend on any classifier model. The Fisher [13] method selects samples using Fisher information as the criterion. It selects a set of instances such that the difference in Fisher informa-tion between the selected set and unlabeled examples is minimum. The formulation is solved using a greedy algorithm.

The svmD method [5] selects a set of uncertain and diverse in-stances for query by ranking each instance in the unlabeled data based on their distance from the margin and maximum angle with the already labeled samples. The angle between two samples is measured using cosine of the angles between the hyperplanes cor-responding to the samples. Similar to svmD , MCS [30] evaluates instances based on their individual merit using multi-criteria, but added a third term to measure the representativeness of each un-labeled data based on average cosine similarity with the unlabeled data. The proposed MP-AL method based on distribution matching, addresses both diversity and representativeness, besides addressing redundancy as well. It is also different from these two selection methods as it selects a batch of instances simultaneously which are together maximally informative based on their collective merit.
The Design method [31] proposes an experimental design in a transductive setting, where the focus is on the predictive perfor-mance on known test data. This method selects a set of instances closest to the basis vectors that approximate the set of unlabeled data. This method does not consider already labeled data, when selecting the next set of query, unlike the proposed method. Datasets. We evaluated the empirical performance of the proposed MP-AL algorithm using eight datasets from the UCI machine learn-ing repository (both binary and multi-class), and a biological image dataset (Fly-FISH). The biological image dataset consists of 1016 images of 7 developmental stages of Drosophila , commonly known as fruit-fly. Each stage forms a class. Each image is represented by 3850 textural features that are extracted using Gabor filters [19]. Competing Methods. We compared the performance of the pro-posed approach with state-of-the-art batch-mode active learning methods which selected a set of instances based on their collec-tive merit including Matrix [11], Fisher [13] and Disc [12]. We also compared our method with state-of-the-art batch-mode active learning methods which selected a set of instances based on their individual merit such as svmD [5] and MCS [30], besides compar-ing to one transductive experimental design method, referred to as Design [31]. We used the sequential design code downloaded from the authors X  webpage and denote this method here as Design (s), that selects a set of instances sequentially based on their individual merits. A detailed description of each of these methods is provided in Section 3. Comparative performance of a random instance se-lection algorithm denoted as Rand , is also presented for reference. Experimental Setup. We randomly divided each dataset into two sets. Batch selection based on active learning methodologies was performed on one set referred to as unlabeled set (65%) and the effectiveness of the selection methodologies was measured based on classification accuracy on the other unseen fixed set (35%) re-ferred to as the test set. Table 2 summarizes the sizes of each of the datasets used. We subsampled some of the datasets due to the computational complexity of the several competing methods. We consider a hard case of active learning, where we start with two randomly selected labeled instances per class. All the algorithms start with the same initial labeled set, unlabeled set and test set. For a fixed batch size b , each algorithm repeatedly selected b in-stances for labeling at each iteration and evaluated the performance of a classifier learned on labeled instances, on the fixed test set. The size of b was fixed at 10 for all datasets except for Vehicles and Iris, where it was fixed at 5 due to their small sizes. The experiments were repeated 10 times and the average results are reported.
We compared the QP and LP formulations by the values of the objective function in Equation (7) and the classification accuracies obtained by the selected query set on the fixed test set. We ob-served that the values of the objective function obtained by LP were slightly lower than QP, though accuracies obtained were similar. We also noted that the execution time of QP was generally lower than LP. This can be attributed to the larger number of constraints in LP and the specific software package used. As part of the future work,weplantoexplorewaystoimprovetheefficiencyoftheLP formulation. The performance values of the proposed method in-cluded in this section are based on the QP formulation. We used a Gaussian kernel with the parameter value selected via cross valida-tion and Support Vector Machines as classification model to evalu-ate the effectiveness of the queried instances.
 Comparative Studies. The comparative performance of the pro-posed approach on UCI datasets, is shown in Figure 2. We observe that the proposed MP-AL performed better than the state-of-the-art batch-mode active learning methods for 6 out of 8 datasets and had comparable performance for the remaining two datasets: Musk and Wine. We also note that for 6 out of 8 datasets the nearest com-petitors were Matrix , Fisher and Disc , except for Iris and Vehicles where svmD , MCS and Design (s) were nearest competitors.
We conducted another set of expe riments on a multi-class, high dimensional biological image dataset (Fly-FISH) for classifying different developmental stages of Drosophila . We randomly sam-pled 511 samples (divided equally among all 7 classes). The batch size b and number of iterations were fixed at 10 and 9 respectively. Figure 4 reports the results of the proposed method MP-AL com-pared to the other active learning methods. We observe that MP-AL outperformed all the other active learning methods followed by Ma-trix , Fisher and Disc . Table 1 presents the results of 2-sided paired t-test of MP-AL vs Matrix , Disc and Fisher methods on UCI and Fly-FISH datasets. We compare the accuracies over 10 runs at each evaluation point and present the percentage of evaluation points at which MP-AL significantly outperforms or under-performs the compared algorithm, denoted as win % and loss% respectively. Variation in MMD Vs Number of Selected Samples. We inves-tigated the variation in MMD value between the training set and the unlabeled data at each iteration for all the datasets. Figure 3 present s the results for some of the representative UCI datasets. Ta ble 1: Win-Loss % o fMP -AL i n 2 -sided pairedt-t est( p&lt; 0 . 05 ) . The fraction not reported (e.g., 60% for MP-AL vs. Ma-trix on Heart) corresponds to the cases where the two algo-rithms are not significantly different at the level of p&lt; 0 . 05 . Similar patterns were observed for the other datasets. We observe that our algorithm decreases MMD value monotonically as more data samples are selected from the unlabeled data and that the de-crease in MMD value corresponds to the increase in classification accuracy on the test data as shown in Figure 2. The decrease in MMD value during the initial iterations is more than the decrease towards the later iterations, resulting in the higher increase in accu-racy values during the initial iterations than later iterations. We ob-serve for the Vehicles dataset the accuracy value sharply increases between the second and third iteration points. We also observe a sharp decrease in MMD value for the Vehicles dataset at the corre-sponding iteration points. Figure 4: Comparative performance of different active learn-ing methods on the Fly-FISH dataset.
 Efficiency Comparison. We compared the average time taken to select a batch of unlabeled points by the proposed MP-AL versus the nearest competitors Matrix , Fisher and Disc . All algorithms were implemented using MATLAB on a four-core Intel processor with 2.66 GHz CPU and 8 GB RAM. Table 2 presents the compar-ative run times on different UCI and Fly-FISH datasets. We note that MP-AL is much more efficient than the other three batch-mode active learning methods for all datasets. Matrix method involved solving a quadratic programming problem multiple times, per batch MP-AL vs. Matrix , Fisher and Disc are presented in Table 1. of query points selection. Fisher involved training of a classifier multiple times per query batch selection. The Disc method involved training of a classifier followed by solving a quadratic program-ming problem multiple times per selection of a query batch and the MP-AL on the other hand, required solving a quadratic program-ming problem once per batch of query points selection.
The proposed MP-AL framework can be readily extended to in-corporate uncertainty of prediction of unlabeled data in the query selection process. We experimented with the following three al-ternative algorithms: (1) At each iteration a classifier is learned on the available labeled data, and the unlabeled data is ranked based on uncertainty of predictions M ( x i ) measured for each unlabeled data x using entropy of predicted labels as in [12]. The MP-AL method is then applied only on the most uncertain set of unlabeled data (top 70%) instead of on the complete unlabeled data, referred to as MP-AL (U). (2) Query selection is based on MP-AL at initial iterations and during later iterations based on the MP-AL (U), referred to as MP-AL (D+U). (3) Add the prediction uncertainty vector M as a separate linear term (  X  M T  X  ) in the formulation in Equation (11), referred to as MP-AL+U . The base MP-AL method is referred to as MP-AL (D). Figure 5 presents the comparative results obtained on some of the representative UCI datasets. Similar patterns were ob-tained on other datasets. We observe that for initial iterations MP-AL (D) and MP-AL (D+U) outperform MP-AL (U) and MP-AL+U . However the performance of these methods improve during later iterations, as the classifier becomes more reliable when learned on a larger number of labeled data. Indeed we can observe that MP-AL (D+U) performs best for most cases as it combines the strengths of both MP-AL (D) and MP-AL (U).
The problem of insufficient labeled data (in a target domain) is addressed in Active learning by querying labels of most informa-tive instances. An alternative to address the same problem is by borrowing samples from an already labeled dataset belonging to a related domain (source domain), known as Transfer Learning [21]. Different transfer learning methodologies have been developed to address the distribution difference between a source and a target do-main, so that the source domain data can be efficiently used to label the target domain data. Re-weighting source domain data to match the marginal probability distributions is a commonly used strategy [15, 3, 25] in transfer learning. In our experiments we used an ex-isting re-weighting method [15], to re-weight the source domain data to match the distribution of the unlabeled set (target domain). We incorporate transfer learning in our MP-AL framework as fol-lows: At each iteration, we combine the re-weighted source sam-ples with the queried and labeled samples from the unlabeled set (in the target domain) and compute the classification accuracy on the other unseen fixed test set, similar to earlier experiments. We evaluated the proposed transfer learning extension of MP-AL on the 20 Newsgroups dataset for document categorization. We built three sets of source domain data vs. unlabeled data (target domain) as follows: (1) Sports: rec.sport.hockey vs. rec.sport.baseball; (2) Hardware: comp.sys.mac.hardware vs. comp.sys.ibm.pc.hardware and (3) Scientific: sci.med vs. sci.electronics. The positive class of each source and target domain data consists of 200 documents randomly sampled from the respective categories and the negative class consists of a random mixture of 200 samples from other cat-egories as suggested in [8]. We represented each document as a binary vector consisting of the 200 most discriminating words de-termined by Weka X  X  info-gain filter [29], after removing stop words and using a document frequency of 5. We start with no selected la-beled instances and use a batch size of 10 . The experiments were repeated 10 times and the average results are reported. Figure 6 shows the accuracies obtained by the extended method denoted as MP-AL+TL andbythebase MP-AL method without using the source domain data, on the 20 Newsgroups dataset. We observe that during intial iterations MP-AL has poorer performance, due to insufficient labeled data. We observe that combining transfer learn-ing with active learning ( MP-AL+TL ) improves the classification accuracy significantly during the initial iterations. However as the number of labeled data from the unlabeled set (target domain) in-creases, the performance of MP-AL improves and outperforms MP-AL+TL for Scientific and Hardware test cases. It has been shown both theoretically and empirically in [2] that if there are enough data from the target domain then no source data are needed, and in fact using additional source data may degrade the performance. We also observe that improvement in classification accuracies due to incorporation of transfer learning is more for Sports and moder-ate for Scientific and Hardware. This can be attributed to the extent of difference in distribution between the source and target domains in each of these test cases; one way to measure the distribution difference is to compute the MMD value between the source and target domains. The MMD value is 0 . 0121 , 0 . 0237 and 0 . 0239 for Sports, Hardware and Scientific respectively. This is consistent with our observation in Figure 6.
In this paper, we propose a novel batch-mode active learning method that selects a set of query samples from the unlabeled data so that the marginal probability distribution represented by the la-beled data after annotation, is si milar to the marginal probability distribution represented by the unlabeled data. The motivation be-hind this approach is to ensure that a classifier learned on labeled data with similar distribution has good generalization performance on the unlabeled data and also on the unseen data coming from the same distribution. The proposed method fully explores the avail-able unlabeled and the already labeled data and demonstrates sensi-ble data selection properties. It is formulated as an integer quadratic programming problem; besides, we also present an equivalent lin-ear programming formulation. Our empirical studies show that the proposed approach achieves superior or comparable performance, besides being computationally highly efficient, compared to the ex-isting batch-mode active learning methods. In addition, we present two extensions by incorporating uncertainty of the predicted labels of the unlabeled data and transfer learning in the proposed formu-lation. Our empirical studies on UCI and 20 Newsgroup datasets show that incorporating uncertainty information in the proposed formulation improves performance at later iterations and including transfer learning improves the performance of the classifier during initial iterations. In future work, we plan to study the theoretical properties of the proposed formula tion. In addition, we plan to ex-tend MP-AL to the multi-label setting.
