 1. Introduction Traditional information retrieval models have mainly focused on finding documents relevant to an information need. query.
  X  Jones X  X .
 subtask does not have any existing straightforward solution, and is thus more challenging. back method.
 performance.

The rest of the paper is organized as follows. We formulate the problem of related entity finding and present a general
Section 7 . 2. Problem formulation input entity and the relation between the input and related entities.

Formally, let D ={ d 1 , d 2 , ... , d n } denote a supporting document collection, E entity type t , and r ( e i , e j ) 2 {0,1} denote whether there exists a relation r between two entities e as a query with three components for the related entity finding task, where t is the type of related entities, e query q , the goal is to find a set of entities E ( q ) that have type t and have relation r with e which is to find all the candidate entities with the specified type, i.e., e entity, i.e,. e t 2 E t , where r ( e in , e t )=1.
 models and feedback method for the structured queries. The details are given in Section 3 and Section 4 . 3. Generative models for candidate ranking
With a list of extracted candidate entities and the corresponding structured query, we now discuss how to assign a rel-3.1. Basic idea
Given a query q ={ t , e in , r } and a related entity candidate e
Zhai, 2007; Lafferty &amp; Zhai, 2003 ) we may use the odds ratio to rank candidates and then apply Bayes X  rule for the estimation: where  X  rank means the two values are equivalent for ranking entity candidates e
Now the challenge is how to factor the conditional probability P  X  e corresponds to a generative process among these variables. As an example, RG (relation generation) model assumes that relations are generated from the input and target entities.
 By comparing these models, we can make the following observations:
The top three models (i.e. RG , IEG and QG ) share a similar property that e us to incorporate a candidate-related prior for e t , while the other models cannot utilize the candidate priors. in the ranking functions because one component either generates the other two or is generated by the other two. 3.2. Derivation of generative models for RG model and briefly explain the derivations for the other models. 3.2.1. Relation generation model (RG)
The RG model assumes that the relation between two entities is generated by a probabilistic model based on the entities (as shown in Fig. 2 a). Thus, the conditional probability in Eq. (2) can be factored in the following ways:
It is reasonable to assume that conditioned on the event R Under this assumption, we get
We now discuss how to estimate P  X  r j e in ; e t ; R  X  1  X  in Eq. (3) , which is the likelihood of e ables (i.e., e in , e t and r ) as follows:
One assumption we have made in this estimation process is that r is independent of e Fig. 3 shows a graphical representation of the relation generation model under the assumption. There is another component in Eq. (3) that we need to estimate: based on their co-occurrence in the supporting documents, i.e., uments that mention both entities in the supporting document collection D and E with the type t . The supporting document collection contains top ranked documents for the query. 3.2.2. Entity generation model (EG) (as shown in Fig. 2 b). Thus, the conditional probability in Eq. (2) can be factored as follows:
Here we make the similar assumption as in the derivation of the RG model (i.e., e
R estimated as:
Here we drop P d 0 2 D P  X  r j d 0 ; R  X  1  X  as it does not affect the ranking of e 3.2.3. Input entity generation model (IEG)
The IEG model assumes that an input entity is generated by a probabilistic model based on the candidate and their rela-didates using: The two component functions can be estimated as: 3.2.4. Related entity and relation generation model (RERG)
The RERG model assumes that both relation and candidate entities are generated by a probabilistic model based on an the candidates using: where P  X  e t ; r j e in ; R  X  1  X  can be estimated as: 3.2.5. Query generation model (QG)
The QG model assumes that a query with an input entity and a relation is generated by a probabilistic model based on a candidate entity (as shown in Fig. 2 e). Applying similar assumptions, the model is shown as follows: where P  X  e in ; r j e t ; R  X  1  X  can be estimated as: and 3.2.6. Related entity generation model (REG)
The REG model assumes that a related entity candidate is generated by a probabilistic model based on the input as follows: where P  X  e t j e in ; r ; R  X  1  X  can be estimated as: 3.3. Discussions
Table 1 summarizes all the six derived models when leveraging supporting documents for estimation. By comparing QG and REG for example. The main difference to distinguish them is that QG contains a candidate normalizer, i.e.,
P date prior may over-favor noisy candidates that occur less frequently in the supporting documents, which would lead to the hypotheses above.
 We now describe how to estimate the conditional probability P  X  X j d ;  X  X  X enerated X  X  from the observed document d . Here X may be individual query components (i.e. e tions between any two query components (i.e. { e in , r }, { e term sets from two query components. The conditional probability can be estimated with language modeling approach mation to better estimate P  X  r j d ; R  X  1  X  ; P  X  e in ; r j d ; Recall that we have discussed two methods to estimate the prior probabilities such as
Which one should we choose? It depends on whether the assumption (i.e., popular candidate entities tend to be relevant not hold because those noisy candidates with common words might also occur very frequently. However, when a candidate and the OccPrior is expected to improve the performance.
 3.4. Connections with other entity track models We now discuss the connections between the derived models and methods used in the top performed runs in the TREC Entity Track.
 ple, Fang et al. proposed to estimate the relevance scores based on not only documents but also passages ( Fang et al., proximity in relevance modeling ( Wu &amp; Kashioka, 2009; Wu, Hori &amp; Kawai, 2010 ). the specified type.
 ones. Although some of these models have been studied, it remains unclear which ranking models are more effective based 2009; Zheng et al., 2010 ), but the performance were quite different (e.g., the results measured with nDCG @ N they use different ways of entity candidate extraction ranging from simply applying off-the-shelf NER taggers ( Zheng and understanding the effectiveness of different ranking models. 4. Relation feedback method
Feedback is a commonly used technique to improve retrieval performance through finding more terms that are related to the given topic. In the scenario of document retrieval, a new query model can be estimated using the specified with the entity name. However, there may be several different expressions for the relation between the input entity and target entity, the original relation r may not cover all of them. Consider the example query in Fig. 1 . The relevant entity  X  X  X eborah Estrin X  X  may be described as  X  X  X eborah Estrin received the ACM Athena Award X  X  or  X  X  X he ACM Athena Award was granted to Deborah Estrin X  X  instead of saying  X  X  X eborah Estrin is the winner of ACM Athena
Award X  X  in many documents. If we use only the original relation description, we may fail to retrieve such relevant entities.
 We thus propose a relation feedback method to estimate an enriched relation model h these feedback entities to estimate an enriched model for the relation specified in the query.
To address the challenge, we propose a novel mixture model based relation feedback method. For each feedback entity, feedback documents F d for every generated new query. Intuitively, these feedback documents contain information about a mixture model of input entities, candidate entities, their relations and a background model: where c ( w ; d ) is the occurrences of term w in document d , h model of candidate entity e t ; h R f is the enriched language model of relation r ; plains the background information or non-relevant topics and h h mate the expanded query model, while C and h F can be estimated based on the term frequency distribution over the whole document collection and supporting documents, respectively. Note that b polation and they represent the likelihood that a term w is generated by each of the language models. We will set these three parameters to some constants and estimate only the enriched relation model h Laird, &amp; Rubin, 1977 ).

The EM updates for p  X  w j ^ h R f  X  are: guage model by eliminating the collection background noise as well as language models of e
Since we may use several top entities as feedback and each of them will derive one estimation of enriched relation lan-guage model, we take the average of them to get the final relation language model we get ^ h R f , we may interpolate it with the original relation model where a is the coefficient of linear interpolation and it works to control the influence of the feedback model.
Finally, we can incorporate the enriched relation model ^ likelihood p  X  r j d ; R  X  1  X  as the cross entropy of the enriched relation model and the document model, i.e., mated as follows:
The likelihood p  X  e in ; r j d ; R  X  1  X  can be estimated in a similar way. 5. Experiments 5.1. Experiment setup We evaluate the proposed methods on two standard collections developed in the TREC Entity track. and (3) the judgement file with relevant entities and homepage for every query, where each query has around 17 relevant entities on average.
 homepage for every query, where each query has around 16 relevant entities on average.
 ties per query for different related entity types.
 The performance is primarily measured with the official measure used in the track: nDCG @ N relevant, and the corresponding gains are 2, 1, 0, respectively.

The smoothing parameter l is set to 1000 for all the experiments. 5.2. Comparison of derived ranking models simple strategies for entity candidate selection.
 back method to rank the entity candidates.

Table 3 shows the performance comparison on the NER generated entity candidate set. The performance of all the models entity candidate prior, models on the top row of Fig. 2 have comparable performance with the other models in the bottom row of same column (e.g. comparing RG + OccPrior with EG).

By analyzing the results, we find that using only existing NER taggers cannot generate a high quality set of candidates are relevant, and among 23,418 entity candidates in Ent10 , 519 (2.2%) are relevant.

DBPedia URI and belong to the type specified in the query according to the ontology. higher precision with acceptable loss of recall, showing it is an effective heuristic.

We then conduct another set of experiments to evaluate the performance of our entity ranking models on NER + DBPedia , noises are filtered out.

To better understand the impacts of two entity candidate sets on the performance of entity ranking, we choose one query removing non-type-match entities like  X  X  X oeing X  X ,  X  X  X hina X  X , etc.). 5.3. Effectiveness of relation-based feedback methods We conduct experiments to examine the effectiveness of the proposed relation feedback method. Specifically, we choose RG and QG model with OccPrior for evaluation because QG is the one that performs best, and RG is the one that can more eter tuning of our related-based feedback methods and report the optimized results as OPTIMIZED . proposed relation feedback method can significantly improve the performance on both data sets under optimized parameter the relation  X  X  X inners of X  X .

There are five parameters in the relation-based feedback method: the number of feedback entities n would like to see whether the optimized parameter setting on one data set can lead to optimized performance on oth-ers. We use one data set to train the optimized parameter setting and apply it to the other. The results are denoted as
TRAINED . By conducting experiments to examine the parameter sensitivity of the relation feedback method, we find that the performance is relatively robust ( X 3%) with regard to the variation of most parameters. Therefore, we recommend a efforts, as shown in Table 8 . The results associated with the recommended parameter setting are denoted as FIXED . Ta-ble 9 shows the results comparison for OPTIMIZED , TRAINED and FIXED . We find that both the trained parameter set-ting and the recommended fixed parameter setting can lead to near-optimized performance for our related-based feedback method. 5.4. Comparison with TREC results
We compare our models with the methods in TREC 2009 Entity Track ( Balog et al., 2010 ). With the fixed parameter set-posed method could be further improved if combined with these additional techniques. 6. Related work the sense that both tasks rank a list of information items, i.e., entities or documents, based on their relevance to a given query. Related entity finding has attracted much attention due to the launch of the TREC Entity track ( Balog et al., 2010 ).

Most participants first extracted entity candidates using either off-the-shelf or custom-made named entity recognizers, links from the page of input entity to the page of target entities to model the relevance score between them. Serdyukov de Vries, and Kamps (2010), Kamps, Kaptein, and Koolen (2010) leveraged category information and external entities on in Fang et al. (2009), Wu, Hori and Kawai (2010), Wu and Kashioka (2009) are essentially very similar to our REG model. and Serdyukov (2009) used the PF (PathFinder)/Tijah system which combines database and information retrieval technology tain them then. Balog, Bron, De Rijke, and Weerkamp (2009), Balog, Bron, and de Rijke (2010), Balog, Bron, and De Rijke (2011) performed entity ranking using a KL-Divergence based generation model which integrates both term-based and cat-form entity prior, but the estimation of generation probability is different from ours: they used Wikipedia documents queries and entities are represented by term-based and category-based model which essentially are probability distribu-tions. The ranking of entities can be done by measuring similarities between the probability distributions. focused on expanding query term model and category model directly from the documents associated with feedback entities. the document.

Compared with the previous work, our main contributions include the following: (1) We derive six generative models for compare these models. (2) We propose a novel relation-based feedback method that can better capture the information mentary to the heuristics used in the Entity track and INEX and can be combined to further improve the performance. 7. Conclusions and future work proposed feedback method is effective to improve the performance.
 be interesting to study how to combine our proposed ranking strategies with existing QA methods to improve the performance.
 References
