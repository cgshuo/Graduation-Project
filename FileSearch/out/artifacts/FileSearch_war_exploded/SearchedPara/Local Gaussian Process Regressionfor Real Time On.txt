 Precise models of technical systems can be crucial in technical applications. Especially in robot tracking control, only a well-estimated inverse dynamics model can allow both high accuracy and compliant control. For complex robots such as humanoids or light-weight arms, it is often hard to model the system sufficiently well and, thus, modern regression methods offer a viable alternative [7,8]. For most real-time applications, online model learning poses a difficult regression problem due needs to take place at a speed of 20-200Hz and prediction at 200Hz to a 1000Hz). Secondly, the learning system needs to be capable at dealing with large amounts of data (i.e., with data arriving at 200Hz, less than ten minutes of runtime will result in more than a million data points). And, thirdly, the data arrives as a continuous stream, thus, the model has to be continuously adapted to new training examples over time.
 These problems have been addressed by real-time learning methods such as locally weighted pro-jection regression (LWPR) [7, 8]. Here, the true function is approximated with local linear functions covering the relevant state-space and online learning became computationally feasible due to low computational demands of the local projection regression which can be performed in real-time. The major drawback of LWPR is the required manual tuning of many highly data-dependent metaparam-eters [15]. Furthermore, for complex data, large numbers of linear models are necessary in order to achieve a competitive approximation.
 A powerful alternative for accurate function approximation in high-dimensional space is Gaussian process regression (GPR) [1]. Since the hyperparameters of a GP model can be adjusted by maxi-mizing the marginal likelihood, GPR requires little effort and is easy and flexible to use. However, the main limitation of GPR is that the computational complexity scales cubically with the training examples n . This drawback prevents GPR from applications which need large amounts of training data and require fast computation, e.g., online learning of inverse dynamics model for model-based robot control. Many attempts have been made to alleviate this problem, for example, (i) sparse Gaussian process (SGP) [2], and (ii) mixture of experts (ME) [3, 4]. In SGP, the training data is approximated by a smaller set of so-called inducing inputs [2, 5]. Here, the difficulty is to choose an appropriate set of inducing inputs, essentially replacing the full data set [2]. In contrast to SGP, ME divide the input space in smaller subspaces by a gating network, within which a Gaussian process expert, i.e., Gaussian local model, is trained [4, 6]. The computational cost is then significantly re-duced due to much smaller number of training examples within a local model. The ME performance depends largely on the way of partitioning the training data and the choice of an optimal number of local models for a particular data set [4].
 In this paper, we combine the basic idea behind both approaches, i.e., LWPR and GPR, attempting to get as close as possible to the speed of local learning while having a comparable accuracy to Gaussian process regression. This results in an approach inspired by [6, 8] using many local GPs in order to obtain a significant reduction of the computational cost during both prediction and learning step allowing the application to online learning. For partitioning the training data, we use a dis-tance based measure, where the corresponding hyperparameters are optimized by maximizing the marginal likelihood.
 The remainder of the paper is organized as follows: first, we give a short review of standard GPR in Section 2. Subsequently, we describe our local Gaussian process models (LGP) approach in Section 3 and discuss how it inherits the advantages of both GPR and LWPR. Furthermore, the learning accuracy and performance of our LGP approach will be compared with other important standard methods in Section 4, e.g., LWPR [8], standard GPR [1], sparse online Gaussian process regression (OGP) [5] and  X  -support vector regression (  X  -SVR) [11], respectively. Finally, our LGP method is evaluated for an online learning of the inverse dynamics models of real robots for accurate tracking control in Section 5. Here, the online learning is demonstrated by rank-one update of the local GP models [9]. The tracking task is performed in real-time using model-based control [10]. To our best knowledge, it is the first time that GPR is successfully used for high-speed online model learning in real time control on a physical robot. We present the results on a version of the Barrett WAM showing that with the online learned model using LGP the tracking accuracy is superior compared to state-of-the art model-based methods [10] while remaining fully compliant. forming the input vector x i into the target value y i given by y i = f ( x i ) + i , where i is Gaussian noise with zero mean and variance  X  2 n [1]. As a result, the observed targets can also be described by y  X  N 0 , K ( X , X ) +  X  2 n I , where K ( X , X ) denotes the covariance matrix. As covariance function, a Gaussian kernel is frequently used [1] distribution of the observed target values and predicted value for a query point x  X  is given by The conditional distribution yields the predicted mean value f ( x  X  ) with the corresponding variance V ( x with k  X  = k ( X , x  X  ) , K = K ( X , X ) and  X  denotes the so-called prediction vector. The hyperpa-rameters of a Gaussian process with Gaussian kernel are  X  = [  X  2 n , X  2 f , W ] and their optimal value for a particular data set can be derived by maximizing the log marginal likelihood using common optimization procedures, e.g., Quasi-Newton methods [1].
Algorithm 1: Partitioning of training data and model learning.
 The major limitation of GPR is the expensive computation of the inverse matrix ( K +  X  2 n I )  X  1 which regions and, subsequently, train the corresponding GP models on these local clusters. The mean prediction for a query point is then made by weighted prediction using the nearby local models in the neighborhood. Thus, the algorithm consists out of two stages: (i) localization of data, i.e., allocation of new input points and learning of corresponding local models, (ii) prediction for a query point. 3.1 Partitioning and Training of Local Models Clustering input data is efficiently performed by considering a distance measure of the input point x to the centers of all local models. The distance measure w k is given by the kernel used to learn the local GP models, e.g., Gaussian kernel where c k denotes the center of the k -th local model and W a diagonal matrix represented the kernel width. It should be noted, that we use the same kernel width for computing w k as well as for training of all local GP models as given in Section 2. The kernel width W is obtained by maximizing the log likelihood on a subset of the whole training data points. For doing so, we subsample the training data and, subsequently, perform an optimization procedure.
 During the localization process, a new model with center c k +1 is created, if all distance measures w k fall below a limit value w gen . The new data point x is then set as new center c k +1 . Thus, the number of local models is allowed to increase as the trajectories become more complex. Otherwise, if a new point is assigned to a particular k -th model, the center c k is updated as mean of corresponding local data points. With the new assigned input point, the inverse covariance matrix of the corresponding local model can be updated. The localization procedure is summarized in Algorithm 1.
 The main computational cost of this algorithm is O ( N 3 ) for inverting the local covariance matrix, where N presents the number of data points in a local model. Furthermore, we can control the complexity by limiting the number of data points in a local model. Since the number of local data points increases continuously over time, we can adhere to comply with this limit by deleting old data point as new ones are included. Insertion and deletion of data points can be decided by evaluating the information gain of the operation. The cost for inverting the local covariance matrix can be further reduced, as we need only to update the full inverse matrix once it is computed. The update can be efficiently performed in a stable manner using rank-one update [9] which has a complexity of
O ( N 2 ) . 3.2 Prediction using Local Models The prediction for a mean value  X  y is performed using weighted averaging over M local predic-P k =1  X  y k p ( k | x ) . According to the Bayesian theorem, the probability of the model k given x can be expressed as p ( k | x ) = p ( k, x ) / local model k where the measure metric w k is used as given in Equation (4). Thus, each local prediction  X  y k , determined using Equation (3), is additionally weighted by the distance w k between the corresponding center c k and the query point x . The search for M local models can be quickly done by evaluating the distances between the query point x and all model centers c k . The prediction procedure is summarized in Algorithm 2. We have evaluated our algorithm using high-dimensional robot data taken from real robots, e.g., the 7 degree-of-freedom (DoF) anthropomorphic SARCOS master arm and 7-DoF Barrett whole arm manipulator shown in Figure 1, as well as a physically realistic SL simulation [12]. We com-pare the learning performance of LGP with the state-of-the-art in non-parametric regression, e.g., LWPR,  X  -SVR, OGP and standard GPR in the context of approximating inverse robot dynamics. For evaluating  X  -SVR and GPR, we have employed the libraries [13] and [14]. 4.1 Dynamics Learning Accuracy Comparison For the comparison of the accuracy of our method in the setting of learning inverse dynamics, we use three data sets, (i) SL simulation data (SARCOS model) as described in [15] (14094 training points, 5560 test points), (ii) data from the SARCOS master arm (13622 training points, 5500 test points) [8], (iii) a data set generated from our Barrett arm (13572 training points, 5000 test points). and using the corresponding joint torques y = [ u ] as targets, we have a proper regression problem. For the considered 7 degrees of freedom robot arms, we, thus, have data with 21 input dimensions (for each joint, we have an angle, a velocity and an acceleration) and 7 targets (a torque for each joint). We learn the robot dynamics model in this 21-dim space for each DoF separately employing LWPR,  X  -SVR, GPR, OGP and LGP, respectively.
 Partitioning of the training examples for LGP can be performed either in the same input space (where the model is learned) or in another space which has to be physically consistent with the approximated function. In the following, we localize the data depending on the position of the robot. Thus, the partitioning of training data is performed in a 7-dim space (7 joint angles). After determining w k for all k local models in the partitioning space, the input point will be assigned to the nearest local model, i.e., the local model with the maximal value of distance measure w k . Figure 2: Approximation error as nMSE for each DoF. The error is computed after prediction on the test sets with simulated data from SL Sarcos-model, real robot data from Barrett and SARCOS master arm, respectively. In most cases, LGP outperforms LWPR and OGP in learning accuracy while being competitive to  X  -SVR and standard GPR. It should be noted that the nMSE depends on the target variances. Due to smaller variances in the Barrett data, the corresponding nMSE has also a larger scale compared to SARCOS.
 Figure 2 shows the normalized mean squared error (nMSE) of the evaluation on the test set for each of the three evaluated scenarios, i.e., the simulated SARCOS arm in (a), the real SARCOS arm in (b) and the Barrett arm in (c). Here, the normalized mean squared error is defined as nMSE = Mean squared error / Variance of target. During the prediction on the test set using LGP, we take the most activated local models, i.e., the ones which are next to the query point. Figure 3: Average time in millisecond needed for prediction of 1 query point. The computation time is plotted logarithmic in respect of the number of training examples. The time as stated above is the required time for prediction of all 7 DoF. Here, LWPR presents the fastest method due to simple regression models. Compared to global regression methods such as standard GPR and  X  -SVR, local GP makes significant improvement in term of pre-diction time.
 subset of about 1000 data points.
 LGP generalizes well using only few local models for prediction. In all cases, LGP outperforms LWPR and OGP while being close in learning accuracy to global methods such as GPR and  X  -SVR. The mean-prediction for GPR is determined according to Equation (3) where we precomputed the prediction vector  X  from training data. When a query point appears, the kernel vector k T  X  is evaluated for this particular point. The operation of mean-prediction has then the order of O ( n ) for standard GPR (similarly, for  X  -SVR) and O ( NM ) for LGP, where n denotes the total number of training points, M number of local models and N number of data points in a local model. 4.2 Comparison of Computation Speed for Prediction Beside the reduction of training time (i.e., matrix inversion), the prediction time is also reduced significantly compared to GPR and  X  -SVR due to the fact that only a small amount of local models in the vicinity of the query point are needed during prediction for LGP. Thus, the prediction time can be controlled by the number of local models. A large number of local models may provide a smooth prediction but on the other hand increases the time complexity.
 The comparison of prediction speed is shown in Figure 3. Here, we train LWPR,  X  -SVR, GPR and LGP on 5 different data sets with increasing training examples (1065, 3726, 7452, 10646 and 14904 data points, respectively). Subsequently, using the trained models we compute the average time needed to make a prediction for a query point for all 7 DoF. For LGP, we take a limited number of local models in the vicinity for prediction, e.g., M = 3 . Since our control system requires a minimal prediction rate at 100 Hz (10 ms) in order to ensure system stability, data sets with more than 15000 points are not applicable for standard GPR or  X  -SVR due to high computation demands for prediction.
 The results show that the computation time requirements of  X  -SVR and GPR rises very fast with the size of training data set as expected. LWPR remains the best method in terms of computational complexity only increasing at a very low speed. However, as shown in Figure 3, the cost for LGP is significantly lower than the one  X  -SVR and GPR and increases at a much lower rate. In practice, we can also curb the computation demands of single models by deleting old data points, if a new ones are assigned to the model. As approach to deleting and inserting data points, we can use the information gain of the corresponding local model as a principled measure. It can be seen from the results that LGP represents a compromise between learning accuracy and computational complexity. For large data sets (e.g., more than 5000 training examples), LGP reduces the prediction cost considerably while keeping a good learning performance. Figure 4: Schematic showing model-based robot control. The learned dynamics model can be up-dated online using LGP.
 model has a stronger effect on computing the predicted torque u FF and, hence, a better learning performance of each method results in a lower tracking error.
 For comparison with the learned models, we also compute the feedforward torque using rigid-body (RB) formulation which is a common approach in robot control [10]. The control task is performed Figure 5: (a) Tracking error as RMSE on test trajectory for each DoF with Barrett WAM. (b) Track-ing error after online learning with LGP. The model uncertainty is reduced with online learning using LGP. With online learning, LGP is able to outperform offline learned models using standard GPR for test trajectories. in real-time on the Barrett WAM, as shown in Figure 1. As desired trajectory, we generate a test trajectory which is similar to the one used for learning the inverse dynamics models in Section 4.1. Figure 5 (a) shows the tracking errors on test trajectory for 7 DoFs, where the error is computed as root mean squared error (RMSE). Here, LGP provides a competitive control performance compared to GPR while being superior to LWPR and the state-of-the art rigid-body model. It can be seen that for several DoFs the tracking errors are large, for example 5., 6. and 7. DoF. The reason is that for these DoFs the unknown nonlinearities are time-dependent, e.g., gear drive for 7. DoF, which can not be approximated well using just one offline learned model. Since it is not possible to learn the complete state space using a single data set, online learning is necessary. 5.1 Online Learning of Inverse Dynamics Models with LGP The ability of online adaptation of the learned inverse dynamics models with LGP is shown by the rank-one update of the local models which has a complexity of O ( n 2 ) [9]. Since the number of training examples in each local model is limited (500 points in average), the update procedure is fast enough for real-time application. For online learning the models are updated as shown in Figure 4. For doing so, we regularly sample the joint torques u and the corresponding robot trajectories [ q ,  X q ,  X q ] online. For the time being, as a new point is inserted we randomly delete another data point from the local model if the maximal number of data point is reached. The process of insertion and deletion of data points can be further improved by considering the information gain (and infor-mation lost) of the operation. Figure 5 (b) shows the tracking error after online learning with LGP. It can be seen that the errors for each DoF are significantly reduced with online LGP compared to the ones with offline learned models. With online learning, LGP is also able to outperform standard GPR. We combine with LGP the fast computation of local regression with more accurate regression meth-ods while having little tuning efforts. LGP achieves higher learning accuracy compared to locally linear methods such as LWPR while having less computational cost compared to GPR and  X  -SVR. The reducing cost allows LGP for model online learning which is necessary in oder to generalize the model for all trajectories. Model-based tracking control using online learned model achieves su-perior control performance compared to the state-of-the-art method as well as offline learned model for unknown trajectories. [1] C. E. Rasmussen and C. K. Williams, Gaussian Processes for Machine Learning . Mas-[2] J. Q. Candela and C. E. Rasmussen,  X  X  unifying view of sparse approximate gaussian process [3] V. Treps,  X  X ixtures of gaussian process, X  Advances in Neural Information Processing Systems , [4] C. E. Rasmussen and Z. Ghahramani,  X  X nfinite mixtures of gaussian process experts, X  Advances [5] L. Csato and M. Opper,  X  X parse online gaussian processes, X  Neural Computation , 2002. [6] E. Snelson and Z. Ghahramani,  X  X ocal and global sparse gaussian process approximations, X  [7] S. Schaal, C. G. Atkeson, and S. Vijayakumar,  X  X calable techniques from nonparameteric [8] S. Vijayakumar, A. D X  X ouza, and S. Schaal,  X  X ncremental online learning in high dimensions, X  [9] M. Seeger,  X  X ow rank update for the cholesky decomposition, X  Tech. Rep., 2007. [Online]. [10] J. J. Craig, Introduction to Robotics: Mechanics and Control , 3rd ed. Prentice Hall, 2004. [11] B. Sch  X  olkopf and A. Smola, Learning with Kernels: Support Vector Machines, Regularization, [12] S. Schaal,  X  X he SL simulation and real-time control software package, X  Tech. Rep., 2006. [14] M. Seeger, LHOTSE: Toolbox for Adaptive Statistical Model , 2007, [15] D. Nguyen-Tuong, J. Peters, and M. Seeger,  X  X omputed torque control with nonparametric
