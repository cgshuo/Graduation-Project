 Existing approaches used for training and evaluating search engines often rely on crowdsourced assessments of document relevance with respect to a user query. To use such assess-ments for either evaluation or learning, we propose a new framework for the inference of true document relevance from crowdsourced data X  X ne simpler than previous approaches and achieving better performance. For each assessor, we model assessor quality and bias in the form of Gaussian dis-tributed class conditionals of relevance grades. For each doc-ument, we model true relevance and difficulty as continuous variables. We estimate all parameters from crowdsourced data, demonstrating better inference of relevance as well as realistic models for both documents and assessors.
A document-pair likelihood model works best, and it is extended to pairwise learning to rank. Utilizing more infor-mation directly from the input data, it shows better perfor-mance as compared to existing state-of-the-art approaches for learning to rank from crowdsourced assessments. Exper-imental validation is performed on four TREC datasets. Categories and Subject Descriptors: H. Information Systems; H.3 Information Storage and Retrieval; H.3.3 In-formation Search and Retrieval Keywords: ordinal label aggregation; learning to rank; crowdsourcing; informativeness; assessor modeling
To assemble training and testing collections for informa-tion retrieval, ground-truth relevance assessments were tra-ditionally obtained from expensive expert assessors, as in numerous TREC efforts. The advent of Amazon Mechanical Turk and other crowdsourcing services has given rise to an alternative and far less expensive source of relevance assess-ments. However, while crowdsourced relevance assessments are far cheaper, they are also less reliable and more difficult to accurately obtain for graded relevance assessments, where a common grade scale is g  X  X  0 , 1 , 2 , 3 , 4 } . This work supported by NSF grant IIS-1421399.

In this paper, we propose a new and simpler framework for the inference of true document relevance from crowd-sourced assessments which achieves a performance exceed-ing state-of-the-art techniques, sometimes markedly so. In our proposed framework, both assessor and document pa-rameters are estimated. Assessor quality is measured as the usefulness for inference, precisely computed as the reduction in uncertainty for relevance inference; thus, assessor qual-ity is closely integrated with how much the assessor X  X  grade counts. Assessor grades are modeled as Gaussian distributed conditionals for each available relevance grade. Each asses-sor is modeled individually, accounting for: (1) the notion that higher assessor grades, for a good quality worker, corre-spond on average with higher document true relevance (ran-dom and adversarial assessors are modeled within our frame-work); (2) realistic decomposition into Gaussians of condi-tional densities; (3) suitability for crowdsourced grades in terms of amount of data collected; (4) the ability to repre-sent grade variance as a function of the assessor, the actual grade, and the document.

For each document, we model its true relevance and dif-ficulty as continuous variables. The true relevance scale (as opposed to an imposed grade scale) allows us to understand and model the perception of each assessor for each grade separately. Naturally, the continuous scale allows for aggre-gation of crowd-answers on inconsistent scales, for example a NIST assessor with two grades and Mechanical Turkers with five grades. Finally, modeling document difficulty allows for variance in assessor grade due to the documents themselves, as opposed to limiting this variance to be a function of only the assessor and the nominal grade, (e.g., through the use of true-relevance/assessed-grade confusion matrices).
We describe two methods, one based on data likelihood over individual documents and the other based on likelihood over pairs of documents. We estimate all the parameters of our model from crowdsourced data, demonstrating the in-ference of assessor parameters and quality as well as doc-ument difficulty and relevance. Both document-likelihood and document-pair-likelihood optimizations use information directly from the input data and show better performance than existing approaches such as simple averages or majority vote, variants of expectation-maximization aggregation [7, 10, 22], and Polytomous Rasch aggregation [1, 15].
Learning to Rank (LTR). We next consider the use of noisy crowdsourced assessments for training learning to rank algorithms. Learning to rank is a common technique in Information Retrieval using labeled documents over a set of training queries in order to obtain a model that can be used to rank documents for new testing queries. For it to work, each document-query datapoint must be represented by a consistent set of features.

We extend the pairwise aggregation model into a relevance scoring model as a function of document features. The train-ing input consists of document features together with crowd-sourced assessments ( X  X udgments X ,  X  X rades X  or  X  X abels X ). The likelihood is defined on pairs of documents, including all crowdsourced assessments and estimated parameters of user models and document difficulty. A likelihood optimization procedure gives the relevance scoring function, in a form of gradient-boosted ensemble of regression trees.
In summary, our contributions are as follows: (1) A new model for assessors that includes a novel measure of quality for each assessor, is easier to learn, and is more suitable for crowdworkers (Section 2). (2) A new model for documents that incorporates a continuous relevance scale and a docu-ment difficulty parameter (Section 2). (3) A new method for the aggregation of crowdsourced assessments based on likelihood of documents , including the inference of document relevance and difficulty as well as assessor quality. (Sec-tion 3.1). (4) A new method for aggregation of crowdsourced assessments based on likelihood of pairs of documents (sec-tion 3.2). (5) A new measure of assessor quality ( X  X nforma-tiveness X ) based on a sound information-theoretic measure of reduction in uncertainty due to the assessor (Section 3.7). (6) A novel analysis of aggregation quality as function of number of assessments, by means of confidence intervals for inferred document relevance (Section 3.6). (7) An analysis of document difficulty vs. crowd disagreement (Section 3.8). (8) A novel method for learning to rank with crowd assess-ments (Section 4).

We demonstrate significant, at times marked, improve-ments for all methods proposed on four TREC datasets. Our crowdsource aggregation results are presented in Section 3.5; learning to rank results are presented in Section 4.3.
In the literature, there are two main directions of research related to conducting inference about unknown quantities of interest, given human observations about them, that may potentially be biased and inaccurate. The first direction deals with so called latent class models , where the unknown quantity is discrete. One of the most popular examples is EM algorithm [7, 10], where a human assessor quality is modeled as a latent confusion matrix with off-diagonal ele-ments representing the probability of mislabeling each item they provide an assessment for. The true labels and the con-fusion matrices are estimated through an iterative procedure that maximizes the likelihood of observing the assessments given the parameters of the model. As a result, the es-timate of the true label is determined mostly by accurate workers, while  X  X ad X  workers are weighted out. There are further extensions of this method within Bayesian inference framework aimed to regularize the problem by assuming var-ious prior distributions over confusion matrices and also to improve the quality of inference [20, 14, 26, 12]. An exten-sion modeling item difficulties and solving the problem via minimax entropy principle was proposed in [31], and later further extended to the case of ordinal labels by regulariz-ing the problem with additional ordinality constraints [30]. Another extension of EM by modeling item difficulties was presented in [29], for the case of 2x2 confusion matrices.
For our domain of application (i.e., inferring document relevances), however, the second direction of research deal-ing with latent trait models [25] seems more promising. In these models, each item (document in our case) is assumed to have a continuous latent trait (true unknown document relevance), while each assessor is assumed to have a set of individual thresholds roughly defining the boundaries, on the scale of latent traits, between adjacent classes (relevance classes in our case). For each item and each assessor, the ob-served labels are assumed to be generated by a probabilistic model that depends on both item latent trait and assessor thresholds, and additionally may account for variable mea-surement errors for both assessor (his quality) and item (its difficulty). Polytomous Rasch model [1, 15] can be seen as a special case of a latent trait model when all the assessors have equal measurement errors. Originally, all the items were modeled to have similar measurement errors as well. However, there is known to exist an extension [17] of the basic Polytomous Rasch model for the case of variable diffi-culties of items (the qualities of workers are, however, mod-eled to be similar). The methods that we investigate in this paper resemble the one described in [25] which accounts for variable difficulties of assessors and has been shown to work well on medical datasets, the difference is that we choose a more flexible family of functions to model class probability curves, and we also accommodate for item difficulty. Other applications of latent trait models to label aggregation that consider both variability in labeler accuracies and item diffi-culties [21, 11] are likely limited in scalability due to MCMC simulation they have to rely on to conduct inference. In an-other recent application [13] a more computationally efficient method (via Approximate Inference) is proposed, however it may be limited by the assumption of same (fixed) thresholds being shared among all the assessors (also authors show no improvement over EM [7] in terms of nDCG).

Other related work includes methods for preference ag-gregation [27, 6] and ranked lists aggregation [18]. While the problem of ordinal assessment aggregation could be po-tentially reformulated within either aforementioned frame-work, it is clear that this transformation would be lossy as the information about degree of confidence with respect to pairwise preferences (encoded with assessor-specific distance between relevance grades) is discarded.

A method for measuring assessor performance in units of information (bits) based on information-theoretic approach was proposed in [28]. Since it was defined within the frame-work of latent class models, we adapted it to the continuous case, and devised a suitable measure of the informativeness.
Learning to Rank Researchers have shown in the past that the noise (and sometimes the adversarial behavior) of the cheap crowdworkers can be countered with variations of the Expectation Maximization algorithm [10]. For LTR per-formance we compare against LambdaMART [3], a state-of-the-art learning to rank algorithm that uses gradient boost-ing to produce an ensemble of weak models (regression trees) that together form a strong model; in the Yahoo! learning to rank challenge [5], the winner used ensembles of Lamb-daMART models. However LambdaMART takes as training labels a single assessment per document, so we have to make a choice from the multiple crowd-assessments collected; each label choice leads to a particular LambdaMART baseline: the random crowdsourced label, the average crowdsourced label, the EM-average crowdsourced label. We also compare with a previously modified LambdaMART [16] that uses as training labels EM-smoothed probabilistic preferences over pairs of documents obtained from crowsourced data.
CR O WD AGGREG Trec8 Ent07 CW12 RF10 (crowd assessments) Queries 10 33 9 100 Rel grades, expert 2 3 4 3 Rel grades, crowd 5 5 4 3 Documents total 18.2K 3.0K 3.2K 20.0K Doc per query mean 1826 91 359 200 Doc per query med 1798 80 368 199 Crowdworkers 661 132 328 764 Assessments, total 134K 18.5K 56.3K 98.3K Assess/worker mean 203 140 172 129 Assess/wkr med 39 40 30 18 Assess/wkr/query mean 106 39 47 16 Assess/wkr/query med 39 38 10 5 Assess/doc mean 7.3 6.2 17.4 4.9 Assess/doc med 6 5 15 5 Queries/worker mean 1.9 3.6 3.6 7.9 Queries/worker med 1 1 2 3
TE S TING LTR MODEL Trec8 Ent07 CW12 MQ09 (expert assessments) Queries 40 17 41 584 Documents, total 68.8K 11.3K 11.9K 15.5K Table 1: Top: datasets for Label Aggregation and training rankers. Bottom: datasets for testing learn-ing to rank models.
We run experiments over four datasets with slightly differ-ent characteristics: TREC 8, TREC Enterprise 2007, ClueWeb 2012 (subset of it that was used in TREC 2013 Crowdsourc-ing Track [23]) and ClueWeb 2009 (subset of it that was used in TREC 2010 Relevance Feedback track + Million Query 2009). We perform the label aggregation task and train learning to rank algorithms on a subset of the queries (see Table 1, top for statistics), while using the rest of the queries to test the performance of trained rankers (see Table 1, bot-tom). In each dataset, the  X  X old X  labels are TREC assess-ments obtained from expert annotators, present over both training and testing sets. For the former three collections, for each document from the training set we have collected at least 5 additional assessments from crowdworker annotators, such as Amazon Mechanical Turkers; for the latter collection we reused existing crowdsourced judgments collected previ-ously in [4]. The overall goal is both to (1) aggregate these non-expert labels in a meaningful way to estimate the actual relevance, and also (2) train a ranking algorithm with these crowdsourced assessments. For learning to rank, we extract a typical set of features (language models and web-graph properties) for each query-document, on all four collections.
Each assessment observation 1 consists of a document d , an assessor a , and the grade observed g , in short ( g,d,a ). We are assuming a uni-dimensional relevance scale (typical for IR relevance assessments), but continuous as opposed to discrete. Each document d is assumed to have  X  X rue rele-vance X  r d on this scale, and a  X  X ifficulty X  d &gt; 0 centered around 1. A fixed relevance prior is used on documents, centering document relevance to a common reference value (arbitrarily chosen to be 0 in our experiments), and having
Here we assume all assessments belong to one query. The generalization to multiple queries case is straightforward a unit variance. The actual document model  X  d = ( r d , to be inferred from assessments, starting from this prior and using crowdworkers assessments to optimize the likelihood.
Assessor model. We are modeling each assessor a with a set of sigmoid-like density functions p ( grade | r d ), one for each relevance grade (Figure 1). Intuitively, we want the densities to reflect probability of an assessor grade condi-tional to the true relevance, but we also want them to be decomposable into a set of underlying parametrizable simple functions. The model assumption is that, for any document d , there is an underlying set of grade-gaussians, such that each density corresponds to a normalized gaussian compo-nent. The mean of each gaussian is invariant in d (fixed per assessor-grade), while the variance of each gaussian compo-nent depends both on assessor variance w g for the grade g , and on document difficulty d :  X  2 = w g d . The difficult documents increase the variance already modeled by the as-sessor grade variance. Depending on the likelihood model, the means and variances will appropriately correspond to ei-ther grades (document likelihood, section 3.1) or difference in grades (document-pair likelihood, section 3.2)
The assessor model parameters are thus  X  a = (  X  a g , X  a for each grade g available to assessors (for example g  X  { 0 .. 4 } ), where  X  a g are weighting coefficients among the com-ponents 2 . These parameters, like the document parame-ters, are to be inferred from assessments, in our case crowd-sourced relevance judgments. The assessor informativeness, not a formal parameter, is calculated as reduction of un-certainty for each likelihood model for each assessor grade ( a,g ) pair; it is a function of the assessment grade, since an assessor can be differently accurate for different grades. Figure 1: Assessor grade model (conceptual illus-tration for  X  X DA X  method): given a true relevance scale (continuous real x-axis) and a set of grades (for example  X  X oor X ,  X  X edicore X ,  X  X ood X ), each assessor is modeled by its gaussian parameters.

We can enforce strict ordinality constraints on the asses-sor grade means, unless we believe there may be adversarial assessors (who intentionally provide out-of-order labels) and we want to use the  X  X eversed X  information out of their assess-ments. If ordinality constraints are enforced, the algorithm will automatically disregard the adversarial judgments  X  like in the case of random, or uninformative, judgments.
In our experiments we restrict the assessor variance to w w 1 = ... = w a 4 to reduce number of free parameters and to enforce unimodality of distributions p ( grade | r d )
Given set of assessments set OBS = { ( g,d,a ) } , a likeli-hood objective is defined, followed by an optimization pro-cedure which finds the assessor and document parameters  X  , X  d that achieve the maximum likelihood. Generally, the optimization is based on Newton-Raphson procedure [19], often far more efficient than Gradient Descent. We present two likelihood objectives: (1) a likelihood defined on indi-vidual documents; (2) a likelihood on pairs of documents.
To define a likelihood for observations based on individual documents, the probability of observation (grade g , docu-ment d , assessor a ) is given by a density of the grade condi-tional on the assessor and document models 3 : where Z ad = P g  X  a g N ( r d |  X  a g ,w a g d ) is a normalizing sum of gaussian components. Note that variance of each gaussian component is the product of assessor variance and document difficulty: w a g d . The log likelihood is Maximizing likelihood with numerical optimization (using analytically derived gradients), we obtain  X   X  a = (  X  a g for each assessor and  X   X  d = ( r  X  d ,  X  d ) for each document.
Knowing assessor model  X  a we can reason about the qual-ity of the assessor a in terms of informativeness of his judg-ments. We can compute how much information about the document relevance we gain by obtaining a judgment g from him. To this end, we take the difference between entropy of the prior distribution over relevance and the entropy of pos-terior distribution given the judgment: where p 0 ( r ) is the prior distribution over relevance for any unjudged document that we choose to be a Gaussian with zero mean and unit variance; p ( g | r, X  a ) is given by our model (eq. 1) assuming the document is of average difficulty ( d 1); p a ( g ) is the normalizing constant with the meaning of overall frequency of the grade g by assessor a :
Since g is not known in advance, we should take expecta-tion of I g (  X  a ) over all possible g , with (eq. 3) and (eq. 4):
I (  X  a ) is the expected informativeness of a single judgment of assessor a , it is measured in nats (natural units for infor-mation entropy, 1 nat  X  1.44 bits) and depends only on the parameters of the user model  X  a .
For a likelihood of observations on pairs of documents, the probability of two observations ( g , d , a ) and ( g 0 , d
The name  X  X DA X  is due to resemblance of the main ex-pression (eq. 1) to Gaussian Discriminant Analysis given by density of the grade conditionals of assessors a and a and the two documents models: where Z aa 0 dd 0 is the normalization constant. Note that gaus-sian means are in this model the difference between assessor grade means, and variance of each gaussian component is composed of assessor variance parameters (for both grades) and document difficulty (for both documents): q w a g w a
Denoting by K the overall number of judgments, we define the log likelihood 4 :
For the pairwise model, we can define informativeness of the assessor as well: it becomes the expected reduction of uncertainty of the relative relevance for a pair of documents  X  r = r d  X  r d 0 , given a pair of judgments g and g 0 :
I where p 0 ( X  r ) is the prior distribution over difference of rel-evance for any unjudged pair of documents (we choose it to be a Gaussian with zero mean and variance of 2); we model p ( g,g 0 |  X  r , X  a )  X  p p ( g | g 0 ,  X  r , X  a , X  a )  X  p ( g 0 | g,  X  compute it using (eq. 7) (again, we assume d = d 0 = 1); p ( g,g 0 ) is the normalizing constant:
Like before we take the expectation of I gg 0 (  X  a possible g and g 0 . Also, we divide the result by 2, to report the informativeness per judgment, not pair of judgments. With (eq. 9) and (eq. 10):
In the EM algorithm, the observed judgments from crowd-source workers are the input; worker qualities are unknown model parameters to be estimated, the document  X  X rue X  grades are the latent variables. We take a similar approach as in [10]. Let G be the number of relevance grades and M be the number of workers. We assign each worker a G x G latent confusion matrix: element C j lg is the probability that worker j labels the document as l , given ground truth relevance g . Let n j il = 1 if the judge j assigns a label l to the document i , otherwise n j il = 0. After computing confusion matrices for all workers, for each document we estimate a relevance grade based on all workers X  judgments and matrices. The probability of document i having true grade r :
While shown to work well on some datasets, in general this method is not very robust to sparse input: it requires
Alternatively, one could consider in (eq. 8) only pairs judged by the same assessor a = a 0 grades c ro wd only c ro wd only c ro wd only c ro wd only M et hod M A P nDCG K  X  M A P nDCG K  X  M A P nDCG K  X  M A P nDCG K  X 
Ra n dom 0 .0 40 * 0.432 * 0.000 * 0 .5 53 * 0.669 * 0.000 * 0 .7 61 * 0.815 * 0.000 * 0 .4 20 * 0.618 * 0.000
MV 0 .1 83 * 0.615 * 0.441 * 0 .6 78 * 0.751 * 0.295 * 0 .8 85 * 0.902 * 0.387 * 0 .5 06 * 0.660 * 0.151
Average 0 .3 09 * 0.724 * 0.685 * 0 .7 20 * 0.777 * 0.382 * 0 .9 15 * 0.923 * 0.476 * 0 .6 12 * 0.708 * 0.444
EM 0 .3 68 * 0.753 * 0.714 0 .7 67 * 0.810 * 0.446 * 0 .9 16 * 0.920 * 0.475 * 0 .6 09 0.697 * 0.372
Rasch 0 .3 73 * 0.763 * 0.713 * 0 .7 62 * 0.809 * 0.449 * 0 .9 20 * 0.928 * 0.501 * 0 .6 18 * 0.714 * 0.420 those of GDA-PW are marked with *.
 M et hod M A P nDCG K  X  M A P nDCG K  X  M A P nDCG K  X  M A P nDCG K  X 
M V 0 .2 19 * 0.647 * 0.519 * 0 .7 44 * 0.804 * 0.477 * 0 .8 90 * 0.899 * 0.436 * 0 .5 57 * 0.676 * 0.235
Average 0 .3 71 * 0.771 * 0.755 * 0 .8 40 * 0.872 * 0.624 * 0 .9 36 * 0.942 * 0.568 * 0 .6 63 * 0.733 * 0.541
EM 0 .5 03 * 0.814 * 0.897 * 0 .8 57 * 0.877 * 0.671 * 0 .9 53 * 0.946 * 0.647 * 0 .6 59 * 0.708 * 0.494
Rasch 0 .4 15 * 0.785 * 0.780 * 0 .8 65 * 0.889 * 0.680 * 0 .9 45 * 0.948 * 0.604 * 0 .6 70 * 0.743 * 0.538
G D A 0 .6 55 0.896 0.941 * 0 .8 87 * 0.900 * 0.732 * 0 .9 76 0.970 0.773 0 .7 63 0.795 * 0.668 many judgments per worker to reliably estimate worker pa-rameters (typically, proportional to G 2 or GT when number of  X  X rue X  classes T is chosen different from G ). Also, this EM method was not specifically designed for problems deal-ing with continuous quantities (like relevance), but rather with discrete classes. So it is not quite clear how many dis-crete latent classes for relevance we have to pick (with more classes we expect more accurate approximation of continu-ous relevance, but also increased number of free parameters making the model even more prone to overfitting).

Another shortcoming of EM in the current setup is the lack of guarantee for ordinality: it is not the case that if the latent class t + 1 corresponds to a higher degree of relevance than the class t , then class t + 2 should always be expected more relevant than class t + 1.
In Polytomous Rasch model, the probability of assessor a assigning a label g to the document d is modeled as follows: where r d is a scalar latent relevance associated with the doc-ument d , and  X  a is a vector of latent thresholds associated with assessor a . When r d =  X  a g for some g , the worker is equally likely to assign labels g  X  1 and g (other labels have some likelihood as well). The parameter d &gt; 0, supposed to be equal to 1 originally, extends the basic model and rep-resents document difficulty [17]. This model has very few free parameters, as compared to EM, and thus the inference is more robust when the input is sparse. On the downside, it works well only when the assessors are reliable. Notice that an assessor providing random responses (whether uni-formly across the grades or not) cannot be accurately mod-eled within this framework. Therefore, this method (in the current form) is not very appropriate for crowdsourcing, as we expect a massive number of unreliable judgments.
In our experiments, we use the extended version of Poly-tomous Rasch model (accounting for document difficulty), and estimate all the parameters by maximizing their joint likelihood in a very similar way we perform optimization of the models we propose (GDA and GDA-PW).
The task in this experiment was to aggregate crowdsourced labels for datasets from Table 1, rank the documents for each query by estimated relevance based on aggregated la-bels, and then evaluate these rankings by using expert la-bels in terms of common IR measures: MAP, nDCG and Kendall  X  . We compare the performance of proposed label aggregation methods, namely GDA (eq. 2) and GDA-PW (eq. 8), with performance of several baselines: (a) randomly ordered documents which doesn X  X  rely on training labels at all, (b) majority vote, by ordering documents by the most frequent label (or one of them if there are several), (c) order-ing documents by straight average of crowdsourced labels, (d) expectation-maximization method, see Section 3.3, and (e) Polytomous Rasch model, see Section 3.4. To measure the effect of modeling document difficulty, we run GDA-PW in two modes: with and without it (fixed = 1). The results are presented in Table 2. First, we observe that our meth-ods, GDA and GDA-PW, outperform the baselines on all datasets by all measures, in many cases by a wide margin 5 Between these two, the pairwise method (GDA-PW) per-forms better on all datasets. Modeling document difficulty seems to help, especially in case of TREC 8 and ClueWeb 2012. It should be noted that EM method is not perform-ing very well: it is comparable to Straight Average on two datasets, similar on third and even worse on the fourth, ClueWeb 2012. Perhaps that can be explained by analyzing the dataset statistics (see Table 1): to reliably estimate as-sessor models (confusion matrices in case of EM) we have too few assessments per worker per query (particularly, on the worst performing dataset, ClueWeb 2012). To partly mit-igate this problem, we minimized the number of unknown parameters by assuming each assessor has the same model across all queries (which may not be very realistic), and also by reducing the number of discrete  X  X rue X  classes of rele-vance (3 true classes worked best in almost all cases); we report the performance of best working configuration, but nevertheless the algorithm suffers from data sparsity. Poly-tomous Rasch model is less prone to overfitting on sparse datasets as it has fewer free parameters to optimize, so it tends to perform better than EM.

As an additional experiment, we decided to inject labels of expert assessors along with randomly generated labels of a virtual assessor, into the set of labels to be aggregated, with-out giving the algorithms a hint about the special properties of these two additional sources. We observe (see Table 3) that the previous conclusions are generally still valid. Our methods do a good job at recognizing the reliable sources, picking up the expert labels to further improve the perfor-mance over the rest of the methods.
When collecting crowdsourced relevance judgments for a document, we not only can estimate its relevance (assuming we already have some estimates of the qualities of asses-sors who contributed judgments), but more generally, we also can compute the posterior probability distribution of the true relevance. Having the posterior distribution can be very useful, for instance, to compute confidence intervals for the estimated relevance. If confidence intervals are too wide, then we may want to collect additional assessments for that document in order to obtain more reliable estimates. For our first method (eq. 2) the posterior has a simple expres-sion (assuming we have K independent judgments for that document): p ( r d |{ g 1 ,g 2 ...,g K } , X  )  X  p 0 ( r d )  X 
At the same time, the posterior for the second method (eq. 8) doesn X  X  seem to have a closed form expression. As an alternative, we can readily apply the Laplace approximation for the posterior (refer to [2]), given that we anyway have to compute the second derivatives of likelihood function with respect to the model parameters while performing Newton-
We test for significance of improvement using Fisher X  X  two-sided paired randomization test [24].
 Figure 2: Simulation of online experiment for 4 doc-uments of ClueWeb 2012 collection. Relevance judg-ments are collected one by one, each time improving the knowledge about true relevance. For each docu-ment, the true relevance estimates are plotted along with 50% confidence intervals.
 Raphson optimization procedure. A more general and po-tentially more accurate approach would be to employ the methods of approximate inference [2] to approximate joint posterior distribution for all the parameters of the model.
On Figure 2 we plot the results of simulating online ex-periment when we collect assessments for a few documents with different relevance (as per expert assessor), one by one, using (eq. 12). One can see how our knowledge about the initially unknown relevance improves over time. The more judgments we collect, the tighter confidence intervals be-come. We can reason about assessor reliability indirectly, by looking at the reduction of uncertainty caused by each assessment: as a consequence of bayesian inference, high quality assessors will reduce uncertainty more than it will do low quality workers. Abrupt changes in the expected value will also correspond to high-quality assessments.
On Figure 3 we can observe the user models inferred from the crowdsourced data using document likelihood (eq. 2). These models correspond to several very different assessors. To verify the validity of inference, we analyze the results of experiment when labels of expert TREC assessors, along with randomly generated labels of a virtual  X  X andom X  asses-sor, were included into the set of crowdsourced labels. Thus, we can analyze the model for two assessors for which we have a priori knowledge about their skills. So, the upper row on the plots corresponds to an expert, then we show models for three real crowdworkers in the decreasing order of their inferred quality, while the bottom row is taken by a random virtual assessor. Starting with the left column where we plot the inferred raw Gaussian for each assessor and each grade, we proceed to the middle column where these Gaussians are combined into category probability curves, for each grade. Figure 3: User model parameters inferred from crowdsourced judgments for ClueWeb 2012 dataset query #246: components of Gaussian mixture (left); grade probability curves (middle); joint probability distributions for relevance and grades (right). From top to bottom on each plot: (a) NIST assessors com-bined; (b) a good quality crowdworker; (c) an aver-age quality crowdworker; (d) a poor quality crowd-worker, (e) simulation of an assessor who provides random judgments (not uniform across grades).
 On the right plot we can see the joint probability distribu-tions for true relevance and observed grades. The right plot should be interpreted as follows: pick some assessor and look at, let X  X  say, the black curve, corresponding to documents labeled  X 0 X . It shows how all the documents labeled  X 0 X  by that assessor are expected to be distributed over the true relevance, while the area under that curve is proportional to the overall likelihood of this assessor assigning this grade. Intuitively, the better separated the curves corresponding to different grades, the better is the assessor able to distin-guish between these grades, meaning the more informative. The reported results match our intuition: we have well sepa-rated curves for expert assessors (top), while we have almost indistinguishable curves (up to normalization constant) for the random one (bottom). Besides, we see that the separa-bility of these curves for the actual crowdworkers vary in a wide range, suggesting we are indeed dealing with assessors of very different quality. We conclude that the family of functions we employ in our aggregation framework is flexi-ble enough to model assessors with a wide range of qualities and biases.

Next, we measure the informativeness of each assessor not just visually, but quantitatively, computing it using (eq. 11). We demonstrate that the inferred informativeness of each assessor makes sense, by plotting it against the measure of how consistent the judgments of that assessor are with the ones of the expert, in terms of Kendall  X  coefficient for rank correlation. By looking at Figure 5, we observe that there is indeed high correlation between two plotted quantities (it is particularly true for the cases when we have many judgments made by an assessor). Moreover, we observe almost zero inferred informativeness for random virtual workers and a considerably high informativeness for expert assessors.
An interesting fact: if we rank expert assessors for differ-ent collections by their informativeness, the order would be (from lower to higher): TREC 8, TREC 2010 RF, TREC Enterprise 2007 and finally ClueWeb 2012. That can be .
 Figure 4: Top plots: the distribution of crowd-workers by their inferred informativeness, for each dataset. Bottom plots: the distribution of collected assessments by their expected informativeness. explained by the granularity of the judgments provided by these experts: binary judgments, 3, 3 and 4 possible rele-vance grades for these collections, respectively. While per-haps all these assessors are good in ordering relevant docu-ments vs non-relevant, the judgments on 4 grades scale have, intuitively, much more discriminatory power than binary rel-evance judgments (moreover, most of the judgments are  X 0 X  and only small fraction are  X 1 X  for the latter case). Also, note that informativeness of crowdworkers for TREC 2010 RF, on average, is lowest among all the collections: it may be because they were asked to report relevance on 3-grade scale, the least discriminative one.

We conclude the analysis of informativeness by showing macro-level statistics for each dataset (see Figure 4). The main observation is that we have too many poor quality workers bringing in too many poor quality judgments (par-ticularly for ClueWeb 2012 dataset; for TREC 8 and TREC Enterprise 2007 we had a stricter rejection mechanism based on redundant trap questions). This is a good illustration of why measuring the informativeness of the assessors is so important in detecting unconscientious workers as early as possible, and perhaps to incentivize workers by paying for the amount of useful information they bring in to the sys-tem [28, 22].
The label aggregation models that we propose in this pa-per include a per-document parameter d which captures the degree of inter-assessor disagreement (adjusted for as-sessor reliability). Previously, we demonstrated empirically that this parameter may help improving the overall qual-ity of aggregation. Now we explore its properties in more details, on the example of aggregating judgments of crowd-workers (not including expert labels) for TREC 8 collection. First, on Figure 6 (left) we explore the pattern of interac-tion between document difficulty and document relevance, both inferred from judgments (for convenience, the points are colored according to independent NIST labels). One can notice that relevant documents (as per NIST) tend to have a higher spread of difficulty (and several of them be-ing the most difficult); the easy documents tend to be ei-ther certainly relevant or certainly non-relevant ones; the in-between documents are more difficult than average (as they cause more disagreement). Secondly, we show that the document difficulty we infer indeed represents an intrinsic property of the document, not just an assessing effect: we split the crowdworkers into two separate groups, so that to the number of judgments made by the assessor for that query. most of the documents have judgments from both groups. Then we train two separate models, one for each group, each model having independent estimates for each document dif-ficulty. Few of them are plotted on Figure 6 (right): there is a positive correlation between two estimates of difficulty.
Separately, we report a weak correlation between inferred document difficulty and the time it took a crowdworker to complete the judgment (we recorded duration of each judg-ment while collecting crowdsourced labels). Figure 6: Document difficulties for TREC 8. Left: document difficulties plotted against estimated rele-vances. Right: document difficulties estimated from labels provided by one group of crowdworkers being consistent with those of another group. Most of popular ranking algorithms (e.g. RankBoost, SVMRank, RankNet/LambdaRank/LambdaMART [3]) are designed to train on a collection of documents with reliable, high quality relevance labels. These algorithms internally learn from pairwise preferences between documents, usually these preferences being deterministically derived from nom-inal (ordinal) labels. That is, suppose the document d has nominal label l , and the document d 0 has nominal label l if l &lt; l 0 , d will be considered as being certainly less relevant than d 0 .

We can imagine scenarios when we would like to use such powerful ranking algorithms, but it is not possible to cer-tainly prefer one document to another. This is the case with document assessments in general, when there are multiple judgments for each document available, and these judgments can exhibit some degree of contradiction. It is particularly the case when the judgments are obtained from unreliable sources and therefore noise in the labels have to be processed prior to running the learning algorithm.

In this section we present a new LTR method based on ex-tension of our label aggregation method, GDA-PW (eq. 8). For comparison, an overview is given for adaptation of ex-isting learning to rank method, LambdaMART, to work on pairs of documents. For a given query and for each pair of documents ( d,d 0 ), LambdaMART [3, 16] models the probability P dd 0 that the document d should be ranked higher than document d 0 via a sigmoid function applied on the difference of the relevance scores, s d  X  s d 0 , returned by the model: Along with these modeled probabilities of pairwise prefer-ences P dd 0 , we have the desired or target ones:  X   X  P 0 d . The cost function of LambdaMART: where  X  Z dd 0 is the change in the IR measure of interest (e.g. nDCG) given by swapping the rank positions of documents d and d 0 in the ranked list sorted by current model scores. In other words, we are interested in minimizing the aver-age cross-entropy between target and modeled probability distributions, weighted by a list-wise component |  X  Z which directs the ranking algorithm to focus more on the pairs of documents that are more influential with respect to a particular IR metric.

LambdaMART uses gradient boosting [8] to produce an ensemble of weak models (regression trees) that together form a strong model. The ensemble F ( x ), mapping an input feature vector x  X  R n to the score s (such that for each document d : s d = F ( x d )), is built in a greedy stage-wise manner by performing gradient descent in functional space (using the first and second derivatives of C with respect to the score of each document d :  X  X   X  X  m , the ensemble F m  X  1 ( x ) from previous step is augmented with a new regression tree h m ( x ): where h m ( x ) is picked greedily so as to minimize cost func-tion (13) which assures better ranking performance on the training set, in terms of a particular IR metric. While Lamb-daMART has been typically used to train on pairs of doc-uments for which it is known that  X  P dd 0 = 0 or (these  X  X ard X  preferences being usually based on ordinal judgments of expert assessors), it has been shown in [16] how to train LambdaMART on unreliable crowdsourcing assess-ments. The trick is to run EM (see section 3.3) to aggregate the assessments and compute the posterior probability dis-tribution over true grades for each document d ; then, hav-ing these probability distributions separately for documents d and d 0 , one can compute the convolution probability that d is less/more relevant than d 0 : p ( d  X  d 0 ), p ( d 0 use them as target probabilities  X  P dd 0 and  X  P d daMART. Here we present how to adapt our label aggregation method GDA-PW (eq. 8) for learning to rank purposes by embed-ding it into gradient boosting framework [8]. Actually, the new method GDA-PW-MART is very similar to previously described LambdaMART with the difference it uses a cost function similar to GDA-PW:
C =  X  X   X  Z dd 0 | X where  X  d ,  X  d 0 ,  X   X  a and  X   X  a 0 are the estimates of document difficulty and assessor model parameters obtained as output of GDA-PW (eq. 8). The components of the cost function: where Z aa 0 dd 0 is the normalization constant, and s d = F ( x is the scoring function for document d that appears instead of modeled true relevance r d in (eq. 7). Intuitively, we would like to get, for each document d , a relevance score s close as possible to the estimate of the true relevance r that we would have gotten by optimizing for (eq. 8). How-ever, the exact equality is rarely achievable, as the relevance scores are constrained by the structure of the scoring func-tion F ( x ) (namely, by parameters of individual regression trees F is composed of). As such, one can think of minimiz-ing (eq. 14) as a constrained version of (eq. 8), where the volatility of s d (how much it can deviate from its optimal value, r  X  d ) is determined by the confidence we have about r , being implicitly encoded in (eq. 8). For implementation, we rely on the existing LambdaMART framework (we build upon open source implementation, JForests [9]) to perform optimization for the new cost function (eq. 14).
In our experiments we compare the performance of our learning to rank method, GDA-PW-MART, with different LambdaMART models trained on the same training set but with differently aggregated crowdsourced labels. We evalu-ate the models on the same testing set (see Table 1, bottom) with the same expert judgments, in terms of nDCG. As one can easily see from Figure 7, our method largely outperforms the rest of LambdaMART baselines trained using various la-bel aggregation methods.

The better performance of GDA-PW-MART, as compared to LambdaMART, we explain with the ability to handle un-certainties by the former algorithm. To have more intuition, consider the following example. Suppose we have four doc-uments: d 1 ,d 2 for some query, and d 3 , d 4 for another query, along with multiple assessments of their relevance. Suppose that after label aggregation phase, we obtain estimates of document relevance (along with standard deviations), let X  X  say r d 1 = 0 . 0  X  1 . 0 ,r d 2 = 0 . 0  X  1 . 0 ,r d 3 r 4 = 0 . 5  X  0 . 1. In this example, we are much more con-fident about relevance of d 3 and d 4 , while having a very little information (and high uncertainty) about relevance of d 1 and d 2 . Since we can X  X  prefer d 1 to d 2 , nor can we pre-fer d 3 to d 4 , all the target probabilities for LambdaMART result, LambdaMART will  X  X ush X  equally hard on all four documents with the intention to equate scores s d 1 and s d 3 = s d 4 , despite a very little evidence of d being equally relevant in reality. Unlike LambdaMART, our algorithm GDA-PW-MART will handle this problem by fo-cusing on equating the scores s d 3 = s d 4 , while leaving s and s d 2 loose.
We have demonstrated a new framework for inference of true document relevance, difficulty, and assessor quality from crowdsourced assessments. It is simpler than previous ap-proaches and with better performance. A document-pair likelihood model works best, via fast Newton-Raphson op-timization. Both assessor and documents are realistically modeled. We estimate all the parameters from crowdsourc-ing data, demonstrating better inference of true relevances and for assessor quality. Our analysis shows that assessor quality can distinguish good crowdworkers, bad crowdwork-ers, experts, or random assessors.

An extension to pairwise aggregation method to learning to rank is given, and it shows a better performance compared to state of the art learning to rank algorithm LambdaMART on crowdsourced assessments, including a prior modification of the LambdaMART to better use crowdsourced assess-ments processed by EM algorithm.

Another appealing property of the proposed model with continuous latent relevance is that it could easily be ex-tended to incorporate additional (continuous) signals of rel-evance, including machine-generated ones (retrieval model scores, output of pre-trained ranking algorithms involving many potentially useful features). The benefit would be in reduction of human efforts needed to achieve the same qual-ity of results. [1] D. Andrich. A rating formulation for ordered response [2] Christopher M. Bishop. Pattern Recognition and [3] C.J.C. Burges. From ranknet to lambdarank to [4] M. Lease C. Buckley and M. D. Smucker. Overview of [5] O. Chapelle and Y. Chang. Yahoo! learning to rank [6] X. Chen, P. N. Bennett, K. Collins-Thompson, and [7] A. P. Dawid and A. M. Skene. Maximum likelihood [8] J. H. Friedman. Greedy function approximation: A [9] Y. Ganjisaffar, R. Caruana, and C. V. Lopes. Bagging [10] M. Hosseini, I. J. Cox, N. Milic-Frayling, G. Kazai, [11] V. E. Johnson. On bayesian analysis of multirater [12] Chao L. and Y.-M. Wang. Truelabel + confusions: A [13] B. Lakshminarayanan and Y. W. Teh. Inferring [14] Q. Liu, J. Peng, and A. T Ihler. Variational inference [15] G. N. Masters. A rasch model for partial credit [16] P. Metrikov, J. Wu, J. Anderton, V. Pavlu, and J. A. [17] Paul Mineiro. Ordered values and mechanical turk. [18] S. Niu, Y. Lan, J. Guo, X. Cheng, L. Yu, and [19] W. H. Press, S. A. Teukolsky, W. T. Vetterling, and [20] V. C. Raykar, S. Yu, L. H. Zhao, G. H. Valadez, [21] S. Rogers, M. Girolami, and T. Polajnar.
 [22] V. Sheng, F. Provost, and P. Ipeirotis. Get another [23] M. Smucker, G. Kazai, and M. Lease. Overview of the [24] M. D. Smucker, J. Allan, and B. Carterette. A [25] J. S. Uebersax and W. M. Grove. A latent trait finite [26] M. Venanzi, J. Guiver, G. Kazai, P. Kohli, and [27] Maksims N. Volkovs and Richard S. Zemel. New [28] T. P. Waterhouse. Pay by the bit: An [29] J. Whitehill, P. Ruvolo, T. Wu, J. Bergsma, and J. R. [30] D. Zhou, Q. Liu, J. C. Platt, and C. Meek.
 [31] D. Zhou, J. Platt, S. Basu, and Y. Mao. Learning
