 e-Science, enabled by the emerging Grid computing paradigm [28], tightly couples scientists, their instruments (e.g., telescopes, synchrotrons, and networks of sensors), massive data storage devices and powerful computational devices. This new disci-instruments and their data, even across geographic separations, thereby ameliorating the tyranny of distance that often hinders research. Data can be captured, shared, interpreted and manipulated more efficiently and more reliably and on a far greater scale than previously possible. Data can be presented for interpretation in new ways using scientific visualization techniques and advanced data mining algorithms. These new technologies enable new insights to be derived and exploited. The data may also drive simulation models that support prediction and  X  X hat-if X  analyses. The models and their results may be archived for later use and analysis, and shared securely and reliably with scientific collaborators across the globe. The resulting network of people and devices is empowered to interact more productively and to undertake experiments and analyses that are otherwise impossible . 
In spite of tremendous advances in middleware and internet software standards, creating Grid applications that harness geographically disparate resources is still iffi-cult and error-prone. Programmers are presented with a range of middleware services, and many other incompatible development tools that often deal with only part of the Grid programming problem. So, a scientist might start with an idea for an innovative experiment but quickly become distracted by technical details that have little to do with the task at hand. Moreover, the highly distributed, heterogeneous and unreliable nature of the Grid makes software development extremely difficult. If we are to capitalize on the enormous potential offered by Grid computing, we must find more efficient and effective ways of de veloping Grid based applications. A critical ingredient for success in e-Science is appropriate Grid-enabled software which, to date, has lagged behind the high-performance computers, data servers, in-struments and networking infrastructure. ences in the details of the various phases in the lifecycle to make traditional tools and techniques inappropriate. For example, traditional software development tools rarely support the creation of virtual applications in which the components are distributed across multiple machines. In the Grid, these types of virtual applications are the norm. Likewise, traditional methods of debugging software do not scale to the size and het-importance, development, deployment, testing and debugging and execution. 2.1 Development Initially, software is developed using the most appropriate tools and programming languages for the task at hand. The process involves the specification, coding and compilation of the software. In the Grid, there is a very strong focus on building  X  X ir-tual applications X , or workflows, that consist of a number of interoperating compo-nents distributed across multiple resources. Grid workflows are powerful because they support the integration of computations, data, scientific instruments and visualization-software while leveraging multiple Grid resources. Grid workflows have been speci-fied for many different scientific domains including physics [31] gravitational wave physics [25], geophysics [40] , astronomy [15] and bioinformatics [36]. Accordingly, there have been many projects to date that support Grid workflows, to name a few, Triana [47], Taverna [37][42], Kepler [35], GrADS [17] and P-Grade [33]. 
A specialized form of workflow allows the creation of  X  X arameter sweeps X , where a computational model is run repeatedly with different input parameters. Using this approach it is possible to explore different design options, and perform more robust science than previously possible. A number of systems support parameter sweep workflows, including APST [21], the NASA IPG (Information Power Grid) parameter process specification tool [50] and our own Nimrod/G [1][4][5][7]. 
Apart from these specific environments, programmers can adopt any one of a num-ber of techniques for building distributed applications. These might build on standard languages like Java, and may use special message passing libraries like MPICH/G. Further, tools like Ninf-G [46] and NetSolve [22] provide powerful remote procedure call systems that can invoke arbitrary proced ures as well as specific services such as linear algebra. Finally, Web Services provide a generic and standards based mecha-nism for building large distributed applications, and these underpin the newly devel-oped Web Services Resource Framework (WSRF) [24]. 2.2 Deployment Traditionally, deployment is often combined with development, and may involve little more than copying the executable program to some partition in the file system, and perhaps registering the software with the operating system. The difficulty in deploy-ment is often underestimated because modern installation techniques, such as those used by Microsoft and Apple, appear to simplify the process enormously. However, in a Grid environment, deployment is much more complex because of the need to install the code on a range of heterogeneous machines, geographically distributed and in different administrative domains. In the Grid deploying an application means build-ing and installing the software on a range of different platforms, taking account of issues such as different instruction sets, operating systems, file system structures and software libraries. To date, this phase is often performed manually, which is both error-prone and does not scale to large Grids. For example, in order to deploy an ap-500 sequentially, compiling, linking and installing the software. Our own experiences at deploying a quantum chemistry package over a handful of resources have identified this as a serious bottleneck [43][44]. 
Surprisingly, there has been little work on the deployment problem, and none of the current middleware projects addresses deployment across heterogeneous re-sources. Some researchers have suggested solving the problem by taking a system-centric view, as is done in systems that configure a homogeneous compute cluster [18][14][29][30][13]. In this model, an application image is produced for the entire Grid, and then copied from a central resour ce. However, this central approach is in conflict with the philosophy of the Grid, which favours a decentralized approach. Moreover, it does not handle significant hete rogeneity because each resource could, in the worst case, require a tailored version of the software. One of the few systems that views deployment in a decentr alized way is GridAnt [48]. 2.3 Testing and Debugging Testing and debugging software is already challenging on a local workstation. In the Grid this phase is particularly difficult, because th e software must be tested and debugged on a range of different platforms, distributed geographically and in different administrative domains. Traditional testing and debugging tools are unable to provide the support re-quired. At present, the only feasible way of debugging a piece of Grid software is for the programmer to log into the remote system and run a conventional debugger such as gdb [41]. This technique does not scale to large Grids, and is not practical for workflows that partly because an application must often execute on a range of different platforms. In this environment, it is not uncommon for a program to work correctly on one machine, but fail in subtle ways when the software is ported or moved to another platform. Traditional debugging techniques usually force the programmer to debug the ported application from scratch, machine the task complex and time consuming. 2.4 Execution This phase typically means scheduling and coordinating execution, using a variety of resources concurrently. Of the phases discussed to date, execution has attracted the most attention. There are many different ways of starting, scheduling and controlling the execution of Grid software, ranging from direct interface to the middleware through to sophisticated scheduling and orchestration systems. 
Web Services and Globus [27] provide rudimentary mechanisms for starting jobs on a given remote, and these services can build more complex multi-resource sched-ules. For example, Kepler contains methods for scheduling and controlling the execu-tion of a workflow and uses the Globus GRAM interface to execute the various work-flow actors [12]. Other middleware, such as Condor-G [26][34] and APST [21], use sophisticated scheduling algorithms to enforce quality of service metrics. For exam-ple, APST can minimize the total execution time of a workflow. Systems such as Cactus [11] provide techniques for migrating computations across Grid resources, thus the computation adapts to the variability in the resource availability. Figure 2 shows a traditional software hierarchy. Here, e-Science applications use services that are exposed by both the platform infrastructure and middleware such as Globus and Unicore [39]. In our experience, whilst powerful, these services are typi-cally too low level for many e-Science applications. As a result, there is a significant  X  X emantic gap X  between them , because the application needs are not matched by the underlying middleware services. Moreover, they do not support the software lifecycle thereby making software development difficult and error-prone. 
To solve these problems, we propose a new hierarchy as shown in Figure 3. The existing middleware is renamed lower-middleware, and an upper middleware layer is inserted. This upper middleware layer is designed to narrow the semantic gap between existing middleware and applications. Importantly, it hosts a range of interoperating tools that will form the e-Scientists workbench, thus supporting the major phases of the software development lifecycle as well as the applications themselves. Our research group has built a number of software tools that address some of the challenges sited in Section 2, as shown in Figure 4. In particular, Nimrod and Grid-dleS target software development; Guard focuses on debugging; Grid Work Bench and DistAnt target deployment and Nimrod, GriddLeS, Active Sheets, REMUS and the Nimrod Portal all focus on execution. In this section we provide a very brief over-view of these tools. 4.1 Development Nimrod/G and GriddLeS [6][8], address some of the challenges in creating of Grid software. Nimrod/G manages the execution of studies with varying parameters across distributed computers. It takes responsibility for the overall management of an ex-periment as well as the low-level issues of distributing files to remote systems, performing the remote computations, and gathering the results. When users describe an experiment to Nimrod/G, a declarative plan file is developed that describes the parameters, their default values, and the commands needed to perform the work. Apart from this high-level description, users are freed from much of the complexity of the Grid. As a result, Nimrod/G has been very popular among application scientists. Nimrod/O is a variant of Nimrod/G that performs a guided search of the design space rather than exploring all combinations. Nimrod/O allows users to phrase questions such as:  X  X hat set of design parameters will minimize (or maximize) the output of my model? X  If the model computes metrics such as cost and lifetime, it is then possi-ble to perform automatic optimal design. A commercial version of Nimrod, called EnFuzion, has been produced [16]. 
GriddLeS, on the other hand, provides a very flexible input-output model that makes it possible to build workflows from legacy applications (written in Fortran, C, etc) thereby leveraging the enormous amount of scientific software that already exists. GriddLeS allows existing programs to tran sparently access local and remote files, as the Storage Resource Broker [38] and the Globus Replica Location Service [23]. It also allows workflows to pipe data from one application to another without any changes to the underlying data access model. In order to support scientific workflows, we have coupled GriddLeS with the Kepler workflow system. Kepler is an active open source cross-project, cross-institution collaboration to build and evolve a scien-tific workflow system on top of the Ptolemy II system. Kepler allows scientists from multiple domains to design and execute scientific workflows. It includes two data-flow-based computation models, Process Networks (PN) and Synchronous Data Flow (SDF), and these can be used to define the  X  X rchestration semantics X  of a workflow. Simply by changing these models, one can change the scheduling and overall execu-tion semantics of a workflow. By combining Kepler and GriddLeS, a user has signifi-cant flexibility in choosing the way data is transferred between the individual compo-nents, and this can be done without any changes to the application source. 4.2 Deployment We are currently developing a few different tools to solve the deployment problem, specifically DistAnt and GWB. DistAnt provides an automated application deploy-ment system with a user-oriented approach [ 30]. It is targeted at users with reasonable knowledge of the application they are deploying, but strictly limited grid computing knowledge, resource information and importantly, resource authority. DistAnt pro-vides a simple, scalable and secure deployment service and supports a simple proce-dural deployment description. 
DistAnt supports application deployment over heterogeneous grids by virtualizing certain grid resource attributes to provide a common application deployment gateway, deployment description, file system structure and resource description. To manage remaining resource heterogeneity DistAnt supports sub-grids, to divide an unmanage-able heterogeneous grid into manageable se ts of like resources, categorized by re-source attributes that can be queried. Sub-grids provide a framework to execute envi-ronment specific remote build routines, compile an application over a set of resource platforms and redistribute binaries to the entire grid. DistAnt also supports definition and deployment of application dependencies. DistAnt enables deployment of a com-plex native application over an uncharacter ized heterogeneous grid, assuming nothing about grid resources. 
Furthermore, integration of DistAnt into Nimrod/G, provides an overall environ-ment enabling grid scale application development, deployment and execution. 
In addition to DistAnt, we are building a rich interactive development environment (IDE), called Grid Work Bench (GWB). GWB is based on the public domain platform Eclipse [32], and supports the creation, management, distribution and debugging of Grid applications. GWB provides specific functionality to help programmers manage the complexity and heterogeneity of the Grid. 4.3 Testing and Debugging The Guard debugger targets the process of testing and debugging in the Grid [2][3]. Specifically, it solves some of the problems discussed in Section 2.3 concerning pro-grams that fail when they are ported from one Grid resource to another. We use a new methodology called relative debugging, which allows users to compare data between two programs being executed. Relative Debugging is effectively a hybrid test-and-debug methodology. While trad itional debuggers force the programmer to understand the expected state and internal operation of a program, relative debugging makes it possible to trace errors by comparing the c ontents of data structures between pro-grams at run time. In this way, programme rs are less concerned with the actual state of the program. They are more concerned with finding when, and where, differences occur between the old and new code. The methodology requires users to begin by observing that two programs generate different results. They then move back itera-answers appear. Guard supports the execution of both sequential and parallel pro-grams on a range of platforms. It also exists for a number of different development environments. Because Guard uses a client-s erver architecture, it is possible to run a debug client on one Grid resource and have it debug an application running on an uses a platform neutral data representation called AIF [49] which means the client and debug servers can run on different types of architecture. 
We are concurrently developing a WSRF compliant debug service that will allow high level tools like the GRB to debug applications across multiple Grid resources. This debug service will interoperate with the Globus GRAM interface, thus jobs launched by the GRAM can be debugged in a secure and efficient way using the addi-tional interface. 4.4 Execution Nimrod provides significant support during the execution of parameter sweeps, in-cluding a sophisticated scheduler that enforces real time deadlines. This economy allows users to specify soft real time deadlines th at are enforced by trading units in a computational economy [19][20]. Using this approach the system can provide a qual-ity of service that is proportional to the amount of currency a user wishes to expend their deadline at the expense of another user. The Nimrod scheduler supports two types of inter-task constraints, namely pa rallel and sequential dependencies. Parallel tasks are executed concurrently providing there are sufficient computational re-sources. Typically, these tasks pertain to different parameter values in a parameter quential parameters (called seqameters, as opposed to parameters) that force the order of the execution to be sequential. This means that one task may be dependent on the output from another, and its execution can be stalled until the data is available. The Nimrod Portal and Active Sheets address the execution phase of the life cycle. The Nimrod Portal allows users to create Nimrod experiments from a web interface. It supports the creation of the plan files discussed above using a graphical user interface, the management of the test bed (and associated Globus issues such as certificate man-agement), and control of the experiment as it executes. Active Sheets [10] allows users to set up and execute an experiment from a familiar spreadsheet interface. Indi-vidual cells can invoke Nimrod/G to perform one simulation run; multiple data inde-pendent cells can be used to specify an entire  X  X hat if X  experiment. Because the sys-tem is embedded in Microsoft Excel, all normal data manipulation and charting tools are available for post analysis (a feature that is popular with users). 
REMUS is an execution environment that helps users build complex Grid applica-tions across firewalls and different administrative domains [45]. REMUS provides mechanisms that reroute traffic through approved channels without compromising the security of any site. It effectively handles heterogeneity in security mechanisms, al-lowing applications to communication when there is no common security framework. In this paper we have provided a very br ief overview of the challenges in building software for the Grid. We have focused on four phases of a software lifecycle, namely development, deployment, testing and debu gging. We have shown that it is possible higher application focused layer. This latter layer can support software tools that make the software development task easier. We have discussed a number of tools developed by the author that simplify the software development task. The author wishes to acknowledge a number of people who have contributed to the work described in this document, incl uding Shahaan Ayyub, Ra jkumar Buyya, Phillip Chan, Clement Chu, Colin Enticott, Jagan Kommineni, Donny Kurniawan, Slavisa Garic, Jon Giddy, Wojtek Goscinski, Tim Ho, Andrew Lewis, Tom Peachey, Jeff Tan and Greg Watson. 
The projects are supported by a variety of funding agencies, including the Austra-lian Research Council, the Australian Department of Communications, Arts and In-formation Technology (DCITA), the Australian Department of Education, Science and Technology (DEST), Microsoft, IBM and Hewlett Packard. 
