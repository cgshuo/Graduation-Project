 Cost-sensitive learning addresses the issue of classification in the presence of varying costs associated with different types of misclassification. In this paper, we present a method for solving multi-class cost-sensitive learning problems us-ing any binary classification algorithm. This algorithm is derived using three key ideas: 1) iterative weighting; 2) ex-panding data space; and 3) gradient boosting with stochastic ensembles. We establish some theoretical guarantees con-cerning the performance of this method. In particular, we show that a certain variant possesses the boosting property , given a form of weak learning assumption on the component binary classifier. We also empirically evaluate the perfor-mance of the proposed method using benchmark data sets and verify that our method generally achieves better results than representative methods for cost-sensitive learning, in terms of predictive performance (cost minimization) and, in many cases, computational efficiency.
 I.2.6 [ Artificial Intelligence ]: Learning Algorithms cost-sensitive learning, multi-class classification, boosting
Classification in the presence of varying costs associated with different types of misclassification is important for prac-tical applications, including many data mining applications, such as targeted marketing, fraud and intrusion detection among others. A body of work on this subject has become known as cost-sensitive learning , in the areas of machine learning and data mining.

Research in cost-sensitive learning falls into three main categories. The first category is concerned with making particular classifier learners cost-sensitive, including meth-ods specific for decision trees [15, 4], neural networks [14] and support vector machines [13]. The second category uses Bayes risk theory to assign each example to its low-est expected cost class [8, 19]. This requires classifiers to output class membership probabilities and sometimes re-quires estimating costs [19] (when the costs are unknown at classification time). The third category concerns meth-ods that modify the distribution of training examples before applying the classifier learning method, so that the classifier learned from the modified distribution is cost-sensitive. We call this approach cost-sensitive learning by example weight-ing . Work in this area includes stratification methods [7, 6] and the costing algorithm [20]. This approach is very gen-eral since it reuses arbitrary classifier learners and does not require accurate class probability estimates from the clas-sifier. Empirically this approach attains similar or better cost-minimization performance.

Unfortunately, current methods in this category suffer from a major limitation: they are well-understood only for two-class problems. In the two-class case, it is easy to show that each example should be weighted proportionally to the difference in cost between predicting correctly or incorrectly [20]. However, in the multi-class case there is more than one way in which a classifier can make a mistake, break-ing the application of this simple formula. Heuristics, such as weighting examples by the average misclassification cost, have been proposed [6, 16], but they are not well-motivated theoretically and do not seem to work very well in practice when compared to methods that use Bayes risk minimiza-tion [8].

In this paper, we propose a method for multi-class cost-sensitive learning based on an iterative scheme for example weighting. There are a number of key techniques we em-ploy in this method: 1) an iterative process which empiri-cally adjusts the example weighting according to the perfor-mance of the learning algorithm; 2) data space expansion for multi-class labels; 3) gradient boosting [17] with stochastic ensembles. The first two ideas are combined in a unifying framework given by the third.

We establish theoretical performance guarantee for a vari-ant of our algorithm. In particular, we show that this variant possesses the so-called boosting property , given that the com-ponent classification algorithm satisfies a certain weak learn-ing assumption . We test our method on several multi-class data sets and discover excellent predictive performance (i.e. cost minimization) as compared to existing cost-sensitive algorithms. Moreover, our results show that when the dis-tribution of costs is skewed (as is common in many data mining applications) our method has the added advantage that it uses drastically smaller sample sizes and hence re-quires much less computational resources.
We begin by introducing some general concepts and no-tation we use in the rest of the paper.
A popular formulation of the cost-sensitive learning prob-lem is via the use of a cost matrix. A cost matrix, C ( y specifies how much cost is incurred when an example is pre-dicted to belong to class y 1 when its correct label is y the goal of a cost-sensitive learning method is to minimize the expected cost. Zadrozny and Elkan [19] noted that this formulation is not applicable in situations in which misclas-sification costs depend on particular instances, and proposed a more general form of cost function, C ( x,y 1 ,y 2 ), that allows dependence on the instance x . Here we adopt a formulation based on this (although slightly more general).

Once we allow the costs to depend on each example, it is natural to assume that the costs are generated according to some distribution, along with the examples, which leads to the following formulation. In (multiclass) cost sensitive classification, examples of the form ( x, C ) are drawn from a distribution D over a domain X  X  R + k . (Throughout the paper, we will let k denote | Y | .) Here, for each label y  X  Y , C y equals the cost of misclassifying instance x as i.e. C y = C ( x,y,y  X  )where y  X  is the minimum cost label for x .

Given a set of examples, S =( x, C ) m , the goal is to find a classifier h : X  X  X  1 ,...,k } which minimizes the expected cost of the classifier: We can assume without loss of generality that the costs are normalized so that Note that with this normalization, the above formulation is equivalent to the common formulation in terms of misclas-sification cost, i.e., whereweused I (  X  ) to denote the indicator function which takes on the value 1whenever the statement is true, and the value 0 otherwise.

Normally a learning method attempts to do this by min-imizing the empirical cost in the given training data, given some hypothesis class H : Here the empirical expectation notation,  X  E, refers to the averaged empirical cost.

As a building block of our method, we make use of meth-ods for solving importance weighted classification problems, which we define below. In importance weighted classifica-tion, examples of the form ( x,y,c ) are drawn from a distri-bution D over a domain X  X  Y  X  R + . Given a set of examples S =( x,y,c ) m , the goal is to find a classifier h : X  X  Y ing minimum importance-weighted misclassification error: Again, usually, a learning method attempts to meet this goal by minimizing the empirical weighted error in some hypothesis class H :
We emphasize that the importance weighted formulation critically differs from the per example formulation of multi-class cost-sensitive learning in that there is a single weight associated with each instance x , whereas in multi-class cost-sensitive learning there is a weight (misclassification cost) associated with each label y . We note that importance weighted classification can be solved very well with a classi-fier learning method, by use of weighted rejection sampling techniques [20].
In the above, we assumed that the hypotheses output by a cost-sensitive learner is a functional hypothesis h , i.e. X  X  Y. It is also possible to allow hypotheses that are stochastic ,namely subject to the stochastic condition: With stochastic hypotheses, stochastic cost-sensitive learn-ing is defined as the process of finding a hypothesis mini-mizing the following expected cost: In general, we sometimes use the following short-hand no-tation for the expected cost of a stochastic hypothesis h an instance x .

Note that in the special case that h is deterministic, the above formulation is equivalent to the definition given in Eq. 1. Also, this is a convexification of the standard ob-jective function that we usually expect a stochastic cost-sensitive learner to minimize, i.e.
 We also consider a variant of cost-sensitive learning in which relational hypotheses are allowed. Here relational hypothe-ses h are relations over X  X  Y , i.e. h : X  X  Y  X  X  0 , 1 } general h is neither functional nor stochastic, and in partic-ular it may violate the stochasticity:
Our methodology can be interpreted as a reduction ,which translates a multi-class cost-sensitive learning problem to a classifier learning problem.

This methodology is derived using three key ideas: 1) iterative weighting; 2) expanding data space; and 3) gradient boosting with stochastic ensembles. The first two ideas are combined in a unifying framework given by the third.
Below we will explain the first two key ideas by exhibiting a prototypical method based on each, and then derive our main learning method that makes use of them in a gradient boosting framework.
We note that the weighting scheme proposed in [20], called costing , exploits the following observation: For the binary class case, the above formulation in terms of per example cost for each class can be further reduced to a formulation in terms of a single importance number per example. This is possible by associating a number indicating the importance of an example ( x,y ), given by | C 0  X  C 1 | .Thisconversion allows us to reduce the cost-sensitive learning problem to a weighted classifier learning problem, but it is not immedi-ately obvious how that would be done for the multi-class sce-nario. It is therefore natural to consider iterative weighting schemes, in which example weights are iteratively modified in search for the optimal weighting. The technique, which we call IW (Iterative Weighting) , presented in Figure 1, is an example of such a scheme. We can show that the final weights of IW are the optimal weights in some sense, pro-vided the algorithm converges.

Theorem 1. Assume IW converges and the final hypoth-esis is h . Then, the following holds: Figure 2: Method DSE (Data Space Expansion)
This theorem says that, if the iterative algorithm con-verges, then the loss of the component learner with respect to the final weights (left hand side) is equal to the expected cost for the problem that we wish to solve (right hand side). Proof
At convergence, for every example ( x,y,c ) that the h errs on, we must have: and thus Noting that when h does not err on x ,wehave I ( h ( x ) = y ) = 0, Thus, it follows that Q.E.D
One drawback to iterative weighting is an inability to di-rectly take into account the different costs associated with multiple ways of misclassifying examples. This translates to non-convergence of the method in practice. We address this issue by the technique of expanding data space, as we describe below.

Given a labeled sample S consisting of ( x, C )ofsize m , we define an expanded sample S of size mk for weighted classification, where k is the size of the label set, i.e. as follows.
 Note here that the newly defined weights are more like ben-efits than costs, since larger costs are mapped to smaller weights.

It turns out that minimizing the importance weighted loss, on this new data set also minimizes the cost on our origi-nal sample. The algorithm DSE (Data Space Expansion), shown in Figure 2, is based on this observation, which is summarized as theorem below.

Theorem 2. With the definitions given in Figure 2, a hypothesis h minimizing the weighted classification error on the expanded weighted sample S , also minimizes the cost on the original sample S ,
Proof Q.E.D.
Having described two key ideas, namely iterative weight-ing and data space expansion, we now apply them together to arrive at our main method. We do so by casting the stochastic multiclass cost-sensitive learning in the framework of gradient boosting [17], with the objective function defined as the expected cost of the stochastic ensemble , obtained as a mixture of individual hypotheses, on the expanded data set. As we stated in Section 2, a functional hypothesis of the form h : X  X  Y can be viewed as a special case of a stochastic hypothesis. We then define a stochastic en-semble hypothesis H , given multiple functional hypotheses, h ,t =1 ,...,T , as the conditional distribution defined as the mixture of the component hypotheses, namely,
Let H t denote the mixture hypothesis of the learning pro-cedure at round t . The procedure is to update its current combined hypothesis by the mixture of the previous com-bined hypothesis and a new hypothesis, i.e. by setting Thus, the expected cost of H t on x is If we now take a derivative of this function with respect to  X  ,weget:
Note that this is the difference between the average cost of the current ensemble hypothesis and the new weak hy-pothesis assigning probability one to the specified label.
We then take the expectation of this derivative with re-spect to all data points ( x,y ) in the expanded data set and thus the gradient is mk -dimensional. The weak learner is to find a hypothesis h whose inner-product with the neg-ative gradient is large. That is, the output h of the weak learner seeks to maximize the following sum. This leads to the following example weighting on the ex-panded sample:
Note that these weight updates are similar to those used in IW and DSE. In fact, the IW weight update rule is es-sentially equivalent to the GBSE rule, except IW has, for each instance x , the weight for only the best (least cost) label, and hence C y = 0 holds. This is because the IW up-date rule mixes the weights from earlier iterations, which is equivalent to taking the average over the stochastic ensem-ble as is done in GBSE. The DSE update rule differs from theGBSEruleinthatmax y C y is used in place of C H t  X  1 ( which ensures that the weight is always non-negative, even though DSE has weights for all labels. Thus, the GBSE weights can be viewed as IW weights, applied on the ex-panded data set, as in DSE. As a consequence, the GBSE necessarily the best label. This means that the weak learner now receives both positive and negative weights. While the minimization of weighted misclassification with positive and negative weights makes perfect sense as an optimiza-tion problem, its interpretation as a classification problem is not immediately clear. In particular, it prohibits the use of weighted sampling as a means of realizing the weighted classification problem.

We deal with this problem by converting a relational ver-sion of the weighted multi-class classification problem (i.e. of finding h to maximize Eq. 10) in each iteration to a weighted binary classification problem. Specifically we convert each example pair ( x,y )to(( x,y ) ,l ), and set l =1iftheweight on ( x,y ) is positive, and l = 0 if the weight is negative. The output hypothesis of the binary classifier is in general rela-tional, so it is converted to a stochastic hypothesis by the procedure Stochastic showninFigure4. (Theparticular way this procedure is defined is motivated by the theoretical guarantee, which will be shown in the next subsection.) The overall process, consisting of multiple iterations of such a re-duction, constitutes a reduction of the stochastic multi-class cost-sensitive classification to binary weighted classification.
With the foregoing definitions, we can now state our main method, GBSE (Gradient Boosting with Stochastic Ensem-bles), which is shown in Figure 3.
It turns out that a strong theoretical performance guaran-tee can be proved on a variant of this method. The variant is obtained by simply replacing the weight updating rule of GBSE by the following: The resulting variant, which we call GBSE-T (Gradient Boost-ing with Stochastic Ensembles -Theoretical version), is sum-marizedinFigure5.

We can show that GBSE-T has a boosting property given a version of weak learning condition on the component clas-sifier. This weak learning condition, which we make precise below, is one that is sensitive to class imbalance.
Definition 1. We say that an algorithm A for the bi-nary importance weighted classification problem, as defined Figure 3: Method GBSE (Gradient Boosting with Stochastic Ensembles)
Stochastic ( h : a relational hypothesis, H : a stochastic hypothesis) in Section 2, satisfies the weak learning condition for a given classification sample S =( x,y ) m , if for all weighted samples S =( x,y,c ) m for it, its output h satisfies the following, for some fixed  X &gt; 0 : Intuitively, this weak learning condition requires that the weak learner achieve better weighted accuracy than that at-tainable trivially by assigning all examples to the negative class.

Theorem 3. Suppose that the component learner A sat-isfies the weak learning condition for sample S as defined by GBSE-T. If we set  X  t =  X  for all t , the output of GBSE-T
Identical to Method GBSE of Figure 3, except for the following change: 3.(a) Set w x,y := Figure 5: Method GBSE-T (Gradient Boosting with Stochastic Ensembles -Theoretical variant) satisfies:
This theorem shows that the empirical cost of the output hypothesis of GBSE-T converges exponentially fast, given the weak learning assumption.
 Proof We first establish the following simple correspondence be-tween the weak learning conditions on the relational multi-class classification problem that we wish to solve in each it-eration, and the weighted binary classification problem that is given to the component algorithm to solve it.
 Definition 2. Let S be a weighted sample of the form S =( x,y,c ) m , where weights c can be both positive and neg-ative. Then define a transformed sample S for weighted classification as S =(( x,y ) ,I ( c&gt; 0) , | c | ) m . 1. The relational weighted multi-class classification prob-2. The weighted binary classification problem for the trans-Note that, in a relational weighted classification problem as defined in Definition 2, the goal of a learner is to try to assign 1to pairs with positive weights and assign 0 to those with negative weights as much as possible.

Lemma 1. For all h :
Proof of Lemma 1 Hence the lemma follows. Q.E.D.

This lemma establishes that getting positive weighted ac-curacy on the original relational weighted multi-class classi-fication problem is equivalent to the weak learning condition on the transformed weighted binary classification problem. Proof of Theorem 3 First, note that applying Stochastic to h t can increase the expected cost only for x  X  X  such that |{ y | h t ( x,y )=1 and for such x  X  X  the cost of the output function f equals that of H t  X  1 by the definition of Stochastic .Hence,the average empirical cost of f on the original sample S ,satisfies the following: Now recall that the expected empirical cost of H t equals the following, where we drop the subscript t from  X  t . Hence, by combining Eq. 13 and Eq. 14, we can show the following bound on the decrease in empirical cost in each iteration. Here, we also drop the subscript t on h . =  X  E =  X   X  E  X   X   X  E  X   X   X  E =  X   X  E  X   X   X  E =  X   X  E Here Lemma 1was applied to get the last equality. Next apply the weak learning assumption on the induced measure over ( x ,y ,c ) defined by: x =( x,y ), y = I and c = The last inequality follows because for all yC y  X  0, there exists y such that C y = 0, and the sum is bounded below by its largest term.

Since the expected cost is convex (in fact linear), this implies convergence to the global optimum. Noting that in each iteration, the empirical cost is reduced at least by a factor of 1  X   X  X  k , the theorem follows. Q.E.D.

Note that at earlier iterations, the binary classifier used as the component learner is likely to be given a weighted sam-ple with balanced positive and negative examples. As the number of iterations increases and progress is made, how-ever, it will receive samples that are increasingly more neg-ative. (This is because the positive examples correspond to labels that can further improve the current performance.) It therefore becomes easier to attain high weighted accu-racy by simply classifying all examples to be negative. The weak learning condition of Eq. 12 appropriately deals with this issue, as it requires that the weak learner achieve bet-ter weighted accuracy than that attainable by assigning all examples to the negative class, as we mentioned earlier.
We use the C4.5 decision tree learner [18] as the base clas-sifier learning method, because it is a standard for empirical comparisons and it was used as the base learner by Domin-gos for the MetaCost method [8].

We compare our methods against three representative meth-ods: Bagging [5], Averaging cost [7, 8] and MetaCost. The Averaging cost method was also used for comparison in [8]. Note that Bagging is a cost-insensitive learning method. Here we give a brief description of these methods, and refer the reader to [5, 8] for the details.
There are some deviations from these methods in our im-plementation, which we clarify below. The main deviation is that we use rejection sampling for all methods (includ-ing bagging), while other sampling schemes such as resam-pling with replacement are used in the original methods. 1
In weighted rejection sampling, the original data are We do this for two reasons: (1) inadequacy of resampling with replacement (or over-sampling), especially for C4.5, has been noted by various authors [20, 9]; (2) since our proposed methods use rejection sampling, we do the same for the other methods for fairness of comparison. We stress that this devi-ation should only improve their performance. Another devi-ation is that we use a variant of MetaCost that skips the last step of learning a classifier on the relabeled training data set, but directly minimizes the expected risk on the test data. It has been observed that this variant performs at least as well as MetaCost, in terms of cost minimization. (This vari-ant has been called BagCost by Margineantu [16].) Also, in our implementation of AvgCost, we perform weighted sam-pling multiple times to obtain an ensemble of hypotheses, then output their average as the final hypothesis. (In the original implementation, only a single iteration of weighted sampling was performed.) We note that, due to our normal-ization assumption that the minimum cost for each instance x is always zero, our version of AvgCost is identical to a more sophisticated variant in which the difference between the average cost and the minimum cost is used for sampling weights. Our experience shows that this variant of AvgCost performs better than the original method.

The methods were applied to five benchmark data sets available from the UCI machine learning repository [3] and one data set from the UCI KDD archive [2]. These data sets were selected by the criterion of having approximately 1000 examples or more, besides being multi-class problems. A summary of these data sets is given in Table 1. Here class ratio is defined as the class frequency of the least frequent class divided by that of the most frequent one. We note that the KDD-99 data set is actually a larger data set. We used the so-called 10% training data set, which consists roughly of 500 thousand instances, and further sampled down by random sampling 40% of them, to get the data set of size 197,710 which we used for our experimentation. Emphati-cally, we only used data from the original training set, and not data from the test set. We do this because of the idiosyn-cratic property of this data set that the test data are gener-ated from a considerably different data distribution. While this property is both realistic and interesting for empirical evaluation of a method for intrusion detection, we judged it not to be desirable for the current purpose of evaluating general purpose cost-sensitive classification algorithms.
Except for the KDD-99 data set, these data sets do not have standard misclassification costs associated with them. For this reason, we follow Domingos and generate cost ma-trices according to a model that gives higher costs for mis-classifying a rare class as a frequent one, and the lower costs for the reverse. (Note therefore that our experiments do not exploit the full generality of the instance-dependent cost for-mulation presented in Section 2.) This reflects a situation that is found in many practical data mining applications, including direct marketing and fraud detection, where the rare classes are the most valuable to identify correctly.
Our cost model is as follows: Let  X  P ( y 1 )and  X  P ( y 2 empirical probabilities of occurrence of classes y 1 and y the training data. We choose the non-diagonal entries of the cost matrix C ( y 1 ,y 2 ), y 1 = y 2 with uniform probability scanned once (without replacement), and each example is accepted with probability equal to (or proportional to) its weight.
 Data Set #ofexamples # of classes Class ratio Annealing 898 5 0.01316 Solar flare 1389 7 0.002562 Table 1: Data set characteristics: data size, number of classes, and the ratio between the frequency of the most common class to the least common. entries were then chosen from the interval [0 , 1000], which often leads to cost matrices in which the correct label is not the least costly one. Besides being unreasonable (see Elkan [10]), these cost matrices can give an unfair advantage to cost-sensitive methods over cost-insensitive ones. We there-fore set the diagonal entries to be identically zero, which is consistent with our normalization assumption.

In all experiments, we randomly select two thirds of the examples in the data set for training and use the remaining one third for testing. Also, for each training/test split we generate a different cost matrix according to the rules above. Thus, the standard deviations that we report reflect both variations in the data and in the misclassification costs.
We remark on certain implementation details of the pro-posed learning methods in our experimentation. First, we note that in all of the methods used for comparison, ex-cept IW, C4.5 was used as the component algorithm with weighted rejection sampling, and the final hypothesis is ex-pressed essentially as an ensemble of output decision tress of C4.5. IW, as a meta-method, does not use ensembles; in-stead we used an ensemble method of costing [20] on C4.5, as the component algorithm. Its output hypothesis is there-fore also an ensemble of decision trees. DSE, as stated in its definition in Figure 2, is not an ensemble method, but analogously to AvgCost, we performed multiple iterations of weighted sampling according to the weighting scheme of DSE and averaged the resulting hypotheses to define the fi-nal hypothesis. Finally, the choice of the mixture weight was set at 1 /t for all methods.

The results of these experiments are summarized in Ta-ble 2 and Table 3. Table 2 lists the average costs attained by each of these methods on the 6 data sets, and their stan-dard errors. These results were obtained by averaging over 20 runs, each run consisting of 30 iterations of the respec-tive learning method. These results appear quite convinc-ing: GBSE outperforms all comparison methods on all data sets, except on Splice, for which it ranks second after Meta-Cost. Also, GBSE is the best performing among the pro-posed methods in the paper, confirming our claim that the combination of various techniques involved is indeed neces-sary to attain this level of performance.

Table 3 lists the average total data size used by each of the methods in 30 iterations. The data size for IW is not listed, since it consists of 30 iterations of 10 rounds of costing and the direct comparison of total data size does not seem to make as much sense for this method. Examining these re-sults in conjunction with the data characteristics in Table 1 reveals a definite trend. First, note that the data sets are di-vided into two groups: those having very large skews, or very low class ratios (Annealing, KDD-99 and Solar flare), and . 8 67 . 38  X  9 . 22 127 . 1  X  14 . 9 33 : 72 4 : 29 . 34 50 . 43  X  10 . 0 46 . 68  X  10 . 16 1 : 69 0 : 78 . 44 247 . 7  X  4 . 15 114 . 0  X  1 . 43 84 : 63 2 : 44 05 67 . 26  X  4 . 18 135 . 5  X  14 57 . 50  X  4 . 38 . 43 140 . 1  X  18 . 2 116 . 8  X  6 . 28 93 : 05 5 : 57  X  9 . 84  X  3795 . 5  X  688 1260 . 2  X  224  X  14 . 4  X  2112 . 8  X  276 486 . 45  X  53 . 3  X  143  X  12512  X  2450 4181  X  783 . 6  X  41  X  479130  X  2710 363001  X  5557  X  21  X  52123  X  592 50284  X  3659  X  127  X  218870  X  6516 140810  X  3335 error. those having moderate skews (Satellite, Splice and Letter). It is evident that the methods based on example weighting (AvgCost, GBSE, DSE) use magnitudes smaller data sizes for the 3 data sets in the first group (i.e. with large skews), as compared to the other methods (Bagging and MetaCost). The performance of GBSE is especially impressive on this group, achieving much lower cost while requiring very small data sizes. It is worth mentioning that it is these data sets in the first group with large skews, that require cost-sensitive learning the most.
It is not the first time that the issue of incorporating cost-sensitivity to boosting has been addressed. For example, AdaCost [11] suggested a way of modifying AdaBoost X  X  ex-ponential loss using a function (called cost adjustment func-tion ) of the cost and confidence. The rational choice of this cost adjustment function, however, appears not to be well-understood. The stochastic ensemble that we employ in the present paper provides a straightforward but reasonable way of incorporating cost and confidence, i.e. in terms of expected cost . An interesting future direction is to investi-gate the relationship between these alternative approaches to cost-sensitive boosting. Also note that AdaCost, being a modification of AdaBoost, is restricted to two-class prob-lems. Comparing and studying possible relationships be-tween GBSE and other (both cost-sensitive and insensitive) multi-class extensions of boosting, such as AdaBoost.M2 [12], is another interesting topic. Finally, GBSE can also be viewed as a reduction from multi-class classification to binary classification. Comparison with existing methods for such reductions (e.g. [1]) is another important research issue.
We thank Saharon Rosset of IBM Research for fruitful discussions on related topics. [1] E. L. Allwein, R. E. Schapire, and Y. Singer. [2] S.D.Bay.UCIKDDarchive.Departmentof [3] C.L.BlakeandC.J.Merz.UCIrepositoryof [4] J. Bradford, C. Kunz, R. Kohavi, C. Brunk, and [5] L. Breiman. Bagging predictors. Machine Learning , [6] L.Breiman,J.H.Friedman,R.A.Olsen,andC.J.
 [7] P. Chan and S. Stolfo. Toward scalable learning with [8] P. Domingos. MetaCost: A general method for making [9] C. Drummond and R. C. Holte. C4.5, class imbalance, [10] C. Elkan. Magical thinking in data mining: Lessons [11] W. Fan, S. J. Stolfo, J. Zhang, and P. K. Chan. [12] Y. Freund and R. E. Schapire. A decision-theoretic [13] G. Fumera and F. Roli. Cost-sensitive learning in [14] P. Geibel and F. Wysotzki. Perceptron based learning [15] U. Knoll, G. Nakhaeizadeh, and B. Tausend.
 [16] D. Margineantu. Methods for Cost-Sensitive Learning . [17] L. Mason, J. Baxter, P. Barlett, and M. Frean. [18] J. Quinlan. C4.5: Programs for Machine Learning . [19] B. Zadrozny and C. Elkan. Learning and making [20] B. Zadrozny, J. Langford, and N. Abe. Cost-sensitive
