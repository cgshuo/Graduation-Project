 Yuxin Chen yuxin.chen@inf.ethz.ch Andreas Krause krausea@ethz.ch ETH Zurich, Universit  X atstrasse 6, 8092 Z  X urich, Switzerland Active learning, i.e., sequential selection of unlabeled examples for labeling, can lead to dramatic (potentially exponential) reduction in labeling effort as compared to passive learning. In many practical settings, however, fully sequential selection, where the choice of the next example depends on all previous labels, is infeasible. For example, when recruiting workers on Amazon Mechanical Turk for crowdsourcing annotation, one usually generates tasks comprising several unlabeled examples. Similarly, in high-throughput experimental design, it is often more cost-effective to perform several experiments in parallel. Such problems have been stud-ied from the perspective of batch-mode active learning. While several heuristics have been proposed, little is known about their theoretical performance. More generally, in many sequential decision problems, we would like to choose multiple actions to be performed in parallel, and receive feedback only after all actions have been carried out. This feedback then informs the next batch of actions. For example, consider a viral marketing problem (Kempe et al., 2003), where we wish to spur demand for a new product by influencing a set of nodes in a social network. In such a setting it is natural to conduct a multi-stage marketing campaign, where each stage is informed by the observed effec-tiveness of the previous stage. Similar problems arise in resource allocation in computational sustainability (Golovin et al., 2011), and vaccination problems in epidemiology (Anshelevich et al., 2009).
 In this paper, we study information-parallel learning and decision making . In particular, we tackle batch-mode active learning and more general stochastic optimization problems, such as influence maximization in social networks, that exhibit adaptive submodularity (Golovin &amp; Krause, 2011), a natural diminishing returns condition. We prove that, for such problems, a simple BatchGreedy approach, which greedily selects examples within a batch, and assembles batches in a greedy manner, is competitive with the optimal batch-mode policy. Furthermore, we prove that surprisingly, in some natural settings, the price of parallelism is bounded: the use of batches incurs competitively low cost irrespective of the batch size, even when compared to a fully sequential policy. We demonstrate the effectiveness of our approach on active learning tasks, as well as adaptive influence maximization in social networks. Our approach is the first to provide both strong guarantees and compelling empirical performance for the important practical problem of batch mode active learning, where Batch-Greedy improves on random selection by  X  48% more than the state of the art does on our test sets. In summary, our main contributions are:  X  We consider a general approach for information  X  prove strong performance guarantees for a simple  X  provide practical algorithms for batch-mode active  X  demonstrate the empirical effectiveness of the We first describe two different applications that moti-vate our research. Then, in Section 2.2 we introduce a formalism that captures both of them, and then prove results for this more general model in Section 3. 2.1. Motivating applications Pool-based batch mode active learning Con-sider a simple model of pool-based Bayesian active learning . We are given a pool V of unlabeled examples x ,..., x n . We use y 1 ,...,y n  X  { +1 ,  X  1 } , where y the (initially unknown) label 1 of example x i . Our goal is to learn a classifier h : V  X  { +1 ,  X  1 } out of a finite set H of hypotheses, each corresponding to distinct la-belings of the pool V , and containing the true labeling, i.e., a hypothesis h such that h ( x i ) = y i for 1  X  i  X  n . For now let us assume that we have a uniform prior P ( h ) = 1 |H| over the hypotheses. We later show that our results also hold for more general priors, as well as for the prior-free (non-Bayesian) setting.
 Suppose we have already observed the labels y A for a subset A  X  V of the pool. In this case, some of the hypotheses h  X  H will be inconsistent with the observations y A , and we use the notation H ( y A ) = { h  X  X  : i  X  X  X  y i = h ( x i ) } to refer to the version space (set of hypotheses) consistent with the observation y A . We wish to actively select a minimum number of unlabeled examples and obtain their labels y
A , such that these allow us to uniquely identify h (i.e., infer the labels of all unlabeled examples), so that |H ( y A ) | = 1. An optimal active learning strategy is one that minimizes the expected number of labels re-quested, in expectation over our prior P ( h ). Similarly, we can consider strategies for batch-mode active learn-ing, which pick batches of k unlabeled examples at a time, then request all labels for the selected batch in par-allel, and then proceed to pick the next batch given the labels obtained so far. See Figure 1 for an illustration. Finding such an optimal policy is a formidable task. In fact, even representing an optimal batch policy may require exponential space. In the following, we will describe a general class of batch mode optimization problems, and present a simple greedy algorithm that is provably competitive with the optimal batch policy. Multi-stage influence maximization in social networks Suppose we would like to stimulate demand for a novel product. The idea behind viral marketing is to utilize the social network structure con-necting the potential customers: By giving the product to a subset of target people for free, these may influence their friends, potentially creating a cascade of influence motivating many more consumers to adopt the product. This problem was formalized by Kempe et al. (2003), who show that many natural models of influence (such as the independent cascade, or linear threshold models) can be modeled stochastically. Formally, let V = { 1 ,...,n } be the set of nodes in the social network, and let Y s  X  V be the (random) set of nodes eventually influenced if s is initially targeted. If a set A of nodes is initially targeted, the eventual influence is S s  X  X  Y s with probability 2 P ( Y A ) = Q s  X  X  P ( Y Instead of committing to all target nodes in advance, it is natural to consider conducting a multi-stage advertising campaign: In each stage certain nodes are targeted, then the effect of the campaign is observed, then the next target nodes are chosen, and so on. Implementing such a procedure may be much more practical if many nodes can be selected in each stage, to be influenced in parallel. In the following, we propose a simple greedy approach that is competitive not only with the optimal multi-stage strategy but even with an optimal fully sequential strategy. 2.2. General Problem Statement We now formalize a class of interactive optimization problems generalizing the two examples of Section 2.1. Adaptive Submodular Optimization We wish to adaptively select items A out of a finite set of n items V = { 1 ,...,n } (unlabeled examples; target nodes). Each item s  X  V is associated with a random variable Y s , taking values in a (finite) set O of outcomes (labels; sets of nodes eventually influenced). We use Y
V = [ Y 1 ,...,Y n ], to refer to the collection of all { x ,x 2 } { x 7 ,x 8 } { x 4 ,x 5 } variables, and assume that Y V is distributed according to a joint distribution P ( Y V ). Whenever an item s is selected, the corresponding variable Y s = y s is revealed. This information can be used to select subsequent items. We model the value associated with a set of items A , and corresponding observations y A  X  X  X  O by means of an objective function 3 f : 2 V X  O  X  N . In our active learning example, we can use f ( y A ) = |H| X  X H ( y A ) | , i.e., the number of the hypotheses eliminated through the labeled examples y A . In our viral marketing example, we choose f ( y A ) = | S s  X  X  y s | , i.e., the number of nodes eventually influenced. Furthermore, let S  X  X  X  O be a set of observations. Note that while technically y A and S do not denote the same objects ( S denotes a set of item/observation pairs, y A denotes the observations corresponding the item set A ), we will sometimes use these notations interchangeably to refer to observations. In both applications, f satisfies the following natural four properties 4 : 1. Normalized: f (  X  ) = 0, i.e., we derive no utility 2. Monotonic: Whenever S  X  S 0  X  V  X  O , then 3. Submodular : whenever S  X  S 0  X  V  X  O and 4. Adaptive submodular: Consider the conditional Our goal will be to find a policy  X  for selecting items (and associated observations) y A , such that we achieve a certain quota of value Q  X  0, i.e., f ( y A )  X  Q , while at the same time minimizing the number of items A used. In the active learning example, Q = |H|  X  1: achieving this quota is a necessary and sufficient con-dition for identifying the true hypothesis. In influence maximization, Q may be a certain fraction of the size of the social network. In the following, w.l.o.g. 5 , we assume f ( y V ) = Q for all y V  X  supp( P ).
 Formally, a policy  X  : 2 V X  O  X  V is a partial mapping from observations y A  X  V  X  O to the next item to be picked (or to stop, if y A /  X  dom(  X  )). Therefore, if the variables Y V are in state Y V = y V , the policy obtains a set of observations, which is denoted as S (  X , y V )  X  V  X  O . We define the expected and worst-case cost of policy  X  as cost ac (  X  ) = E y Our goal, is to find, out of a set  X  of candidate policies a feasible policy  X   X  with minimum cost, min Batch selection Based on this notation, we can study how different classes of policies compare in terms of their cost. On the one extreme, we have fully sequential policies  X  seq , where the choice of each item may depend on the labels of all previous items selected. On the other extreme, we have constant , or non-adaptive policies  X  const which commit to items picked in advance, before making any observations. However, fully sequential and constant policies are only two extremes on a spectrum. We are interested in poli-cies  X  [ k ] that sequentially pick batches of size k . Any policy  X   X   X  [ k ] starts selecting a fixed set A 1  X  V of k items. It then obtains all labels y A stops. Otherwise, if batches A 1 ,..., A `  X  1 have already been selected, it picks batch A `  X  X  of k items, obtains the labels, and stops if f ( y A Obtaining an optimal batch policy is a formidable task: There are n policy assembles such batches into a decision tree of possibly exponentially large branching factor. In the following, we describe a simple greedy algorithm, and prove that it implements a batch policy with cost com-petitive to that of the optimal batch policy. Moreover, we prove that under some additional conditions on the distribution P ( Y V ), the greedy algorithm is even competitive with the optimal fully sequential policy. We consider a simple, greedy approach towards con-structing batch policies. This policy, BatchGreedy , selects items within a batch in a greedy manner, then receives observations for all items in the batch, then selects the next batch in a greedy manner, conditional on all observations made so far, and so on. An important challenge in batch selection is the fact that the value of items (e.g., unlabeled examples) selected depends on observations (e.g., labels) obtained only after the entire batch is selected. In active learning for example, one wishes to select examples within a batch that are likely to be informative individually, but also diverse (minimize redundancy). BatchGreedy addresses this challenge by using a suitable notion of marginal benefit of an item, that takes into account all observations made so far, as well as items that have already been selected within the batch (but no obser-vation has been obtained yet). Formally, we generalize the conditional marginal benefit (2.1) of item s by  X  f ( s |A , y B ) = E y Thus,  X  f ( s | A , y B ) reflects the expected marginal gain of item s , when items B have been selected and the corresponding observations y B have been made, and items A have already been selected, but no obser-vations have yet been made about them. Therefore, (3.1) captures possible redundancy (diminishing gains) of candidate item s w.r.t. to labels already obtained, as well as labels that will likely be obtained within the Algorithm 1 The BatchGreedy algorithm.

Input: Quota Q . Objective f and prior P ( y V ) y B  X  X  X  repeat until f ( y B )  X  Q batch. Hence it encourages diversity among the items selected in the batch.
 Using this notation, the BatchGreedy policy will greedily select the i -th element in the j -th batch where y B is the set of observations (labeled examples) from batches up to j  X  1. After a batch is completed, all labels are requested and added to the observations y . Pseudocode is presented in Algorithm 1.
 If we set the batch size k to 1, BatchGreedy reverts back to a fully sequential, greedy active learning scheme. In particular, for the active learning example from Section 2.1, this algorithm is known as General-ized Binary Search , studied extensively in the literature (see e.g., Dasgupta (2004)). In fact, it is known that this simple greedy algorithm is near-optimal: its cost is upper-bounded by O (log |H| ) times that of the optimal sequential policy. More generally, Golovin &amp; Krause (2011) prove that this result can be generalized to any adaptive optimization problems that are adaptive submodular . As our first main theoretical contribution, we generalize their results, which only hold for fully sequential policies, to the batch setting.
 We first show that BatchGreedy is near-optimal as compared to the optimal batch selection policy. Theorem 1. Let OPT ac,k be the expected cost and OPT wc,k be the worst-case cost of an opti-mal policy selecting batches of size k . Further let  X  = min y policy  X  G implementing BatchGreedy it holds that Note that the guarantee of Theorem 1 matches (up to a small constant factor) hardness results known for the fully sequential ( k = 1) setting, which it general-izes, therefore BatchGreedy is near-optimal under computational constraints. Further note that for the active learning application, Theorem 1 guarantees that in the non-Bayesian setting (i.e., without any prior 6 ), BatchGreedy requires at most a factor of O (ln |H| ) more batches than the optimal batch-mode policy. As our second main theoretical result, we also prove that, perhaps surprisingly, under certain conditions BatchGreedy is not just competitive with the optimal policy that is restricted to selecting batches of examples: It is competitive with respect to an optimal fully sequential policy, which is not required to obey such a restriction. The additional condition needed is that the variables Y 1 ,...,Y n are independent. This assumption is satisfied in the influence maximization application, but not in the active learning problem. Theorem 2. Fix  X  &gt; 0 . Let OPT wc be the worst-case cost of an optimal sequential policy  X   X  , constrained to picking a number of items which is a multiple of k . Further suppose that the variables Y 1 ,...,Y n are independent. Then for the cost of the policy  X  implementing BatchGreedy , run until it achieves f (  X  G )  X  Q  X   X  it holds that Moreover, it holds that P f ( S (  X  G , y V ))  X  Q  X  1  X   X  . The proofs are given in the supplementary material. The key technical insight behind the proof is that a bound on the adaptivity gap for stochastic submodular maximization of Asadpour et al. (2008), adapted and generalized to our setting, allows us to interpret the BatchGreedy policy as an approximate implemen-tation of the fully sequential greedy policy. Note that Theorem 2, for technical reasons, is of a slightly dif-ferent flavor than Theorem 1: it compares the optimal policy  X   X  always achieving quota Q with one achieving the quota Q only with probability 1  X   X  . By choosing  X  &lt; min y in fact f ( S (  X  G , y V )) = Q for all y V with nonzero prob-ability. In this case, the bound on the worst-case cost of Theorem 2 is only a factor of e/ ( e  X  1)  X  1 . 58 larger than that of Theorem 1, irrespective of the batch size. As is, BatchGreedy is not immediately practical for the applications from Section 2.1: Computing the marginal gains (3.1) requires computing expectations Algorithm 2 Hit-and-run hypothesis sampler for lin-ear separators and noisy observations
Input: Labeled examples y B  X  R d , N , number of mixing iterations T , noise level  X   X  [0 , 0 . 5).
Output: Hypotheses set  X  H . w 0  X  a random point on d -dim. unit sphere S d for i = 1 to N do end for that may be intractable. In the influence maximization application, it is possible to perform Monte-Carlo sampling of the influence process to evaluate (3.1) up to arbitrarily small multiplicative error (1 +  X  ) (Kempe et al., 2003). Furthermore, with a slight generalization of the arguments of Golovin &amp; Krause (2011), using such an approximation of (3.1) increases the cost by at most the same factor (1 +  X  ).
 To obtain a practical algorithm for batch mode active learning, further challenges arise: BatchGreedy as is requires that H is finite, and its running time depends polynomially on |H| . Furthermore, it requires that observations are noise free. As a practical implemen-tation, we focus on active learning of linear separators, i.e., h ( x ) = sign( w T x ). In our experiments we use a Markov-Chain Monte Carlo sampler in order to generate samples from the posterior distribution over hypotheses P ( w | y S ). In particular, we build on the hit-and-run sampler (Smith, 1984; Lovasz, 1998), which is known to lead to a provably efficient near-optimal es-timation for the fully sequential active learning problem (Gonen et al., 2011). To handle noise, at each iteration, we generalize the hit-and-run sampler by sampling the Algorithm 3 Approximate implementation of Batch-Greedy for batch-mode active learning.
 Input: Hypotheses H , batch size k , noise level  X  Sample  X  H = { h 1 ,...,h N } from H using Algo. 2 Define  X  P ( H ) = 1 repeat until (1  X   X  ) of all hypotheses in the support of  X  P induce same labeling on the unlabeled pool entire version space, while varying the sample density according to a likelihood function. In the case of sym-metric binary channel (i.e., labels are flipped with prob-ability  X  ), we sample hypotheses with probability re-lated to the number of mistakes. This way, our method can handle data that are not linearly separable. Algo-rithm 2 presents details of our sampler, and our final batch-mode active learning algorithm is formalized in Algorithm 3. The time complexity of random sampling (Algorithm 2) is O ( TN ), where T is the number of mix-ing iterations. Once we discretized the hypothesis space with N samples, it takes O ( knN ) steps for Algorithm 3 to select a batch of k items. Hence the time complexity of Algorithm 3 selecting one batch is O ( N ( T + kn )). Furthermore, in both applications, we can use lazy evaluations to speed up the BatchGreedy algorithm (as used in Golovin &amp; Krause (2011) for the fully sequential setting). Lazy evaluations utilize the fact that the marginal gains  X  f ( s | A , y B ) are monoton-ically decreasing in both A and y B . This insight can be exploited by utilizing priority queues to accelerate selection of the next greedy choice. We empirically evaluate BatchGreedy on several data sets and on both applications discussed in Sec-tion 2.1. Our emphasis is on comparing BatchGreedy with baselines, as well as empirically quantifying the price of parallelism.
 Batch mode active learning of linear separators One natural way to perform batch mode active learn-ing is to select batches comprising the k most uncertain examples. As one baseline, we use  X  X atch mode margin-based active learning X  ( k -batch SVM) to greedily select batches of examples, as considered in Jain et al. (2010) for large-scale active learning. In this method, we ran-domly chose examples until there are two distinct la-bels, and we train SVM classifiers based on the labeled examples at the end of each batch. The next k un-labeled examples with the smallest distances from the decision boundary w T x + b = 0 are selected for label-ing. Another baseline approach we employ is the state of the art batch mode active learning algorithm (KLR-BMAL) of Hoi et al. (2006) that selects batches of k examples that are informative w.r.t. the Fisher informa-tion matrix. To see how well the parallelization of the se-lection process approximates the sequential algorithm, we compare with the fully sequential active learning algorithm, where only one example is selected and ob-served at each iteration, as well as a  X  X assive learning X  approach, where we make no observations during the learning process (corresponding to infinite batch size). We also compare BatchGreedy against the sequential active learning algorithm with purely random selection. For fair comparison, we use SVM as classifier for all competing algorithms, so the methods only differ by the set of examples chosen for labeling. We implement the KLR-BMAL algorithm using class membership probabilities inferred from the hypothesis sampler, and set the smoothing parameter  X  to be 0 . 1 (Hoi et al., 2006). As for our sampler, we set  X  to be 0 . 1. We normalize the data so that each feature has mean 0 and standard deviation 0 . 5, and place independent normal priors on each dimension. The results for all the batch mode active learning experiments are obtained from 150 random starts.
 We run our the first set of experiments on two UCI data sets 7 , WDBC (569 instances, 32-d) and Australian (690 instances, 16-d), using a fixed number of 5000 sampled hypotheses in each random trial. Figure 3(a) and Figure 3(b) depict the 150-trial average percentage of mistakes made by each algorithm when predicting the labels of the corresponding data set, for a batch size of k = 10. Figure 3(b) shows an improvement of BatchGreedy over both KLR-BMAL and the 10-batch SVM algorithm. On both datasets, surprisingly, BatchGreedy is competitive with the fully sequential greedy algorithm, with only minor differences. We also evaluate BatchGreedy on the MNIST data set. For each of the 14780 instances, we reduced the dimensionality down to 10 via PCA, and compare BatchGreedy with the sequential, KLR-BMAL, 10-bacth SVM, passive and random algorithms through 150 random trials. We observe that, even using 5000 sample hypotheses for each iteration, BatchGreedy is significantly faster than KLR-BMAL, as the cost of BatchGreedy grows linearly w.r.t. the number of hy-potheses and number of examples, while KLR-BMAL costs quadratically w.r.t. the number of examples. In fact, for the same settings, it takes KLR-BMAL approx. 50 seconds to select one example, compared to approx. 10 seconds for BatchGreedy .
 We also study the impact of the discretization param-eter M (i.e., number of hypotheses used to sample the version space), varying it from 300 to 5000, and we plot the results for each setting in Figure 3(d). For the WDBC data set, we can observe a statistically significant performance improvement across the 150 trials when increasing the number of sampled hypotheses used from 300 to 2000. Starting from 3000 samples, however, the advantage of introducing more samples begins to decrease dramatically. As there is a linear increase in running time as we employ more samples, we suggest to pick a moderate M to balance efficiency and accuracy. Multi-stage influence maximization in Social Networks We also apply BatchGreedy to the multi-stage influence maximization problem described in Section 2.1. We use two data sets from the SNAP repository 8 : the Epinions social network (with 75879 nodes and 508837 directed edges, where members of the site can decide whether to  X  X rust X  each other) and the Slashdot social network (with 82168 nodes and 948464 directed edges, where users are allowed to tag each other as friends or foes). For each network, we take the subgraph induced by the top 1000 nodes with largest outdegree. We use the independent cascade model (Kempe et al., 2003). In our simulations, we assume that each person has a certain, fixed probability to influence its neighbors. We choose this probability t according to the edge density of the target network, in our case to 0.05 and 0.03, respectively.
 We evaluate the performance of BatchGreedy while varying the size of batches picked at each stage. We repeat the experiments 100 times for all batch sizes (the non-adaptive method corresponds to infinite batch size). In each experiment, we initialize 100 random realizations of the target network based on the edge activation probability, and greedily select the best node in expectation. The results are summarized in Figure 3(e) and Figure 3(f). We observe that for the Epinions network, the sequential greedy policy covers 99% of the target network by selecting 244 nodes, while the 10-batch greedy policy, 100-batch greedy policy and non-adaptive greedy policy cost 241, 284, and 584 nodes respectively, to achieve the same coverage. Similarly, for the Slashdot network those numbers are 330, 319, 343, 765. After incorporation of the first batch of labels, BatchGreedy performs surprisingly well, even competitively with the fully sequential policy. Active Learning Fully sequential active learning, where one single unlabeled example is selected to be labeled at a time, has received much attention in the machine learning community. Several heuristic approaches have been proposed that perform well in some applications (e.g., MacKay (1992); Settles (2010)), and theoretical investigations (e.g., Balcan et al. (2006); Dasgupta (2006)) prove bounds on the label complexity. While in some cases exponential reduction is possible, in other settings little reduction over passive learning can be achieved (Dasgupta, 2006). Therefore, it is interesting to study active learning as an optimization problem with the goal to efficiently identify a sufficient yet near-minimal number of informative labels. For both noise-free (Dasgupta, 2004) and noisy (Golovin et al., 2010) active leaning problems, simple greedy policies have been shown to be provably competitive with the optimal (intractable) policy. This paper builds on these approaches, and generalizes the results to batch mode active learning. Batch mode active learning and submodularity Due to its practical importance, several approaches for batch mode active learning have been proposed. How-ever, previous work has focused on efficiently selecting a single batch comprising examples that are both diverse and informative. Interestingly, the classical notion of submodular set functions has proven useful (Hoi et al., 2006; Guillory &amp; Bilmes, 2011). In Hoi et al. (2006), both individual diversity and informativeness are evaluated w.r.t. the Fisher information matrix of the estimated linear separator. Cesa-Bianchi et al. (2010) and Guillory &amp; Bilmes (2011) investigate the problem of active learning on graph structured data, and provide near-optimal solutions for such problems. In contrast to the prior work, this paper focuses on analyzing batch-mode policies, i.e., sequential construction of batches with the goal to minimize the overall number of labels needed across batches. A work closely related in spirit is that of Desautels et al. (2012), who consider batch mode bandit problems, where the goal is to trade exploration and exploitation, and performance is measured w.r.t. the cumulative regret. In spite of a different context and formalism, a greedy algorithm works provably well in that setting. Adaptive and interactive submodular opti-mization For a general introduction to adaptive submodular optimization, see Golovin &amp; Krause (2011). Asadpour et al. (2008) study a special case of the adaptive submodular maximization problem where the random variables must be independent (which is not the case in active learning). Guillory &amp; Bilmes (2010) consider a different formalism for interactive submodular maximization and its applications, ana-lyzing the worst-case cost of fully sequential policies. In contrast to these previous approaches, which focus on the fully sequential case, in this paper we analyze the batch-mode setting, for which we provide approximation bounds as well as practical algorithms. Influence maximization This problem was first introduced by Domingos &amp; Richardson (2001), and Kempe et al. (2003) prove that the problem of (non-adaptively) finding the optimal k individuals in the network to target requires submodular maximization. Recently, by using adaptive submodularity, Golovin &amp; Krause (2011) generalize the problem to the fully sequential setting where observations are made after each selection. Our results generalize theirs and ad-dress the multi-stage setting, which, to our knowledge, is the first attempt to address this natural variant of the influence maximization problem. We presented a general framework for batch mode active learning and stochastic optimization. We analyzed BatchGreedy , an intuitive adaptive greedy approach, and proved its competitiveness with the op-timal batch-mode policy. For some problem instances (e.g., multi-stage influence maximization) we proved that, perhaps surprisingly, using batches only incurs a bounded increase of cost as compared to allowing fully sequential selection. In addition to new theoretical results, we empirically demonstrate the effectiveness of BatchGreedy on two real-world applications: Batch mode active learning of linear separators (where BatchGreedy outperforms the state of the art), and multi-stage influence maximization (where we observe a surprisingly small increase in cost compared to the fully sequential strategy). A natural question for future work is to understand more generally for which problems the price of parallelism, i.e., the increase in cost by re-stricting to information-parallel decisions, is bounded. We believe that our results provide an important step in characterizing the (approximate) tractability of practical active learning and optimization problems. Ageev, Alexander A. and Sviridenko, Maxim. Pipage rounding: A new method of constructing algorithms with proven performance guarantee. J. Comb. Optim. , 8(3):307 X 328, 2004.
 Anshelevich, Elliot, Chakrabarty, Deeparnab, Hate,
Ameya, and Swamy, Chaitanya. Approximation al-gorithms for the firefighter problem: Cuts over time and submodularity. In Algorithms and Computation . Springer Berlin / Heidelberg, 2009.
 Asadpour, Arash, Nazerzadeh, Hamid, and Saberi, Amin. Stochastic submodular maximization. In WINE , pp. 477 X 489, Berlin, Heidelberg, 2008. Springer-Verlag.
 Balcan, Maria Florina, Beygelzimer, Alina, and Lang-ford, John. Agnostic active learning. In ICML , pp. 65 X 72, 2006.
 Cesa-Bianchi, Nicol`o, Gentile, Claudio, Vitale, Fabio, and Zappella, Giovanni. Active learning on trees and graphs. In COLT , pp. 320 X 332, 2010.
 Dasgupta, Sanjoy. Analysis of a greedy active learning strategy. In NIPS , 2004.
 Dasgupta, Sanjoy. Coarse sample complexity bounds for active learning. In Weiss, Y., Sch  X olkopf, B., and Platt, J. (eds.), Advances in Neural Information Processing Systems 18 , pp. 235 X 242. MIT Press, Cambridge, MA, 2006.
 Desautels, Thomas, Krause, Andreas, and Burdick,
Joel. Parallelizing exploration-exploitation tradeoffs with gaussian process bandit optimization. In ICML , 2012.
 Domingos, Pedro and Richardson, Matt. Mining the network value of customers. In KDD , pp. 57 X 66, 2001. Golovin, Daniel and Krause, Andreas. Adaptive sub-modularity: Theory and applications in active learn-ing and stochastic optimization. Journal of Artificial Intelligence Research (JAIR) , 42:427 X 486, 2011. Golovin, Daniel, Krause, Andreas, and Ray, Debajyoti.
Near-optimal bayesian active learning with noisy observations. In NIPS , December 2010.
 Golovin, Daniel, Krause, Andreas, Gardner, Beth, Con-verse, Sarah, and Morey, Steve. Dynamic resource allocation in conservation planning. In AAAI , 2011. Gonen, Alon, Sabato, Sivan, and Shalev-Shwartz,
Shai. Active learning halfspaces under margin assumptions. CoRR , abs/1112.1556v3, 2011.
 Guillory, Andrew and Bilmes, Jeff. Interactive submodular set cover. In ICML , 2010.
 Guillory, Andrew and Bilmes, Jeff. Active semi-supervised learning using submodular functions. In UAI , 2011.
 Hoi, Steven C. H., Jin, Rong, Zhu, Jianke, and Lyu,
Michael R. Batch mode active learning and its appli-cation to medical image classification. In ICML , 2006. Jain, Prateek, Vijayanarasimhan, Sudheendra, and
Grauman, Kristen. Hashing hyperplane queries to near points with applications to large-scale active learning. In Advances in Neural Information Processing Systems 23 , pp. 928 X 936. 2010.
 Kempe, David, Kleinberg, Jon, and Tardos,  X  Eva.
Maximizing the spread of influence through a social network. In KDD , pp. 137 X 146, 2003.
 Lovasz, Laszlo. Hit-and-run mixes fast. Math. Prog , 86:443 X 461, 1998.
 MacKay, David J.C. Information-based objective func-tions for active data selection. Neural Computation , 4(4):590 X 604, 1992.
 Nemhauser, George L., Wolsey, Laurence A., and
Fisher, Marshall L. An analysis of approxima-tions for maximizing submodular set functions -I. Mathematical Programming , 14(1):265 X 294, 1978. Settles, Burr. Active learning literature survey. Tech-nical Report 1648, University of Wisconsin-Madison, 2010.
 Smith, Robert L. Efficient monte carlo procedures for generating points uniformly distributed over bounded regions. Operations Research , 1984.
 Vondrak, Jan. Submodularity in Combinatorial Optimization . PhD thesis, 2007.
 problem instance.
 Lemma 3. Let V = { 1 ,...,n } , O be finite sets; f : 2
V X  O  X  N monotonic and submodular, and P ( Y V ) such A , z ) } ) = ularity, fix element i  X  X  , corresponding to A i = { j 1 ,...,j } , and let B  X  X   X O 0 . Since Q ( B ) &gt; 0,  X  ( B ) cannot contain two elements ( i,o 1 ) and ( i,o 2 ) with o 1 6 = o Consider the marginal gain, and let C =  X  ( B X  ( i, z i )) \  X  ( B ) = { ( i 1 ,o 1 ) ,..., ( i ( i |  X  ( B )  X  C j  X  1 ) |  X  ( B )] .
 be the collection of all m = n and Q denote the problem instance induced by sets A the greedy policy  X  (w.r.t. ( g,Q )) satisfies ac,k ln Q + 1 .
 ) h f ( y A  X  y B )  X  f ( y B ) i .
 already been made, identifying an optimal batch i  X  X  s.t. h ( A , y B ) .
 5.8 of Golovin &amp; Krause (2011) thus proves that e e  X  1 Theorem 4 (Adapted from Asadpour et al. (2008)) . Let f : 2 approximates the optimal (sequential) adaptive policy  X   X  of length k within a factor of 1  X  1 /e , i.e., 1 e a policy: benefit of a policy  X  , denoted  X  f (  X  | B ), is  X  f (  X  | B ) := E [ f ( B X  X  (  X , y V ))  X  f ( B ) |B ] , where the expectation is computed w.r.t. P ( Y V |B ).
 ) by w (  X  0 ) X  f (  X  0 |B ), and therefore Note that each policy  X  0 contributes cost w (  X  0 ) k to cost (  X   X  |B ). Therefore, Therefore, |B )  X  cost ac (  X   X  |B ) max respect to the distribution P ( Y V ) , and  X  is the BatchGreedy of items divisible by k , and positive integers ` and m adaptive setting by Golovin &amp; Krause (2011). We derive a sequence of inequalities: factor e/ ( e  X  1) to  X  .
 where @ denotes policy concatenation 9 .
 1  X  1  X  , where for this last inequality we have used the fact that 1  X  x &lt; e  X  x for all x &gt; 0. Thus f avg (  X   X  )  X  f f
G , y V ))  X  Q  X  1) &gt;  X  . Then a contradiction.
 f y R X  X   X  O when i is included in R with probability y i . Formally,
Y polytope of P k corresponds to a non-adaptive policy. Let ~w  X  := arg max the utility of the optimal non-adaptive policy f avg (  X   X  ), as  X   X  const corresponds to the policy which constantly picks the optimal set, such that for all ~w in P k , f avg ) = F ( ~w  X  )  X  F ( ~w ).
 Lemma 8. (Vondrak, 2007) There exists a 0/1 vector ~w in P Proof. Suppose C  X  X  0  X  X   X O . For all j  X  X  , we have  X  X  )  X  g ( C 0  X  X  ) C 0 adaptive submodular with respect to P ( Y V ). expected utility f avg (  X  ) is upper bounded by f  X  , where f : [0 , 1] n  X  R is defined as y h f ( R X  X  j } )  X  f ( R ) i )) )  X  X  ) ) g ( S (  X , y V )  X  X  )  X  g ( B ) (C.1) an item-observation pair ( j,y ), as S seen by  X  just before it selects item j under y V . Formally, Based on the previous definitions, we have p ( y
V ) X  h (( j,y ) |Q (  X , y V ,j )) ) p ( y | y V\{ j } ) X  h (( j,y ) |Q (  X , y V ,j )) )
X By definition  X  h ( j |Q (  X , y V ,j )) = P y p ( y | y V\{ j } Therefore, according to Lemma 9, since  X  h ( j |Q (  X , y V of Equation C.2 is upper bounded by [ g ( B X  X  ( j,y ) } )  X  g ( B ))] . (C.3) [ g ( B X  X  ( j,y ) } )  X  g ( B ))] (C.4)
E [ g ( B X  X  ( j,y ) } )  X  g ( B ))] y
E y [ g ( B X  X  ( j,y ) } )  X  g ( B )) y [ f avg ( R X  X  j } )  X  f avg ( R )] which completes the proof.
 ) f  X  ( ~y ) .
 y process, R (1) contains item j independently with probability R E [ f ( R (1))]  X  F ( ~y ) by monotonicity.
 f ( R ( t )) is y dt [ f ( R ( t )  X  X  j } )  X  f ( R ( t ))] ( ~y )  X  f ( R ( t ))] (C.6) where step C.6 follows the definition of f  X  . Let  X  ( t ) = [ f ( R ( t ))], inequality C.6 can be written as ( ~y )  X   X  ( t ) (C.7) inequality completes the proof.
 Now, we are ready to prove Theorem 4 (the adaptivity gap): f
 Yuxin Chen yuxin.chen@inf.ethz.ch Andreas Krause krausea@ethz.ch ETH Zurich, Universit  X atstrasse 6, 8092 Z  X urich, Switzerland Active learning, i.e., sequential selection of unlabeled examples for labeling, can lead to dramatic (potentially exponential) reduction in labeling effort as compared to passive learning. In many practical settings, however, fully sequential selection, where the choice of the next example depends on all previous labels, is infeasible. For example, when recruiting workers on Amazon Mechanical Turk for crowdsourcing annotation, one usually generates tasks comprising several unlabeled examples. Similarly, in high-throughput experimental design, it is often more cost-effective to perform several experiments in parallel. Such problems have been stud-ied from the perspective of batch-mode active learning. While several heuristics have been proposed, little is known about their theoretical performance. More generally, in many sequential decision problems, we would like to choose multiple actions to be performed in parallel, and receive feedback only after all actions have been carried out. This feedback then informs the next batch of actions. For example, consider a viral marketing problem (Kempe et al., 2003), where we wish to spur demand for a new product by influencing a set of nodes in a social network. In such a setting it is natural to conduct a multi-stage marketing campaign, where each stage is informed by the observed effec-tiveness of the previous stage. Similar problems arise in resource allocation in computational sustainability (Golovin et al., 2011), and vaccination problems in epidemiology (Anshelevich et al., 2009).
 In this paper, we study information-parallel learning and decision making . In particular, we tackle batch-mode active learning and more general stochastic optimization problems, such as influence maximization in social networks, that exhibit adaptive submodularity (Golovin &amp; Krause, 2011), a natural diminishing returns condition. We prove that, for such problems, a simple BatchGreedy approach, which greedily selects examples within a batch, and assembles batches in a greedy manner, is competitive with the optimal batch-mode policy. Furthermore, we prove that surprisingly, in some natural settings, the price of parallelism is bounded: the use of batches incurs competitively low cost irrespective of the batch size, even when compared to a fully sequential policy. We demonstrate the effectiveness of our approach on active learning tasks, as well as adaptive influence maximization in social networks. Our approach is the first to provide both strong guarantees and compelling empirical performance for the important practical problem of batch mode active learning, where Batch-Greedy improves on random selection by  X  48% more than the state of the art does on our test sets. In summary, our main contributions are:  X  We consider a general approach for information  X  prove strong performance guarantees for a simple  X  provide practical algorithms for batch-mode active  X  demonstrate the empirical effectiveness of the We first describe two different applications that moti-vate our research. Then, in Section 2.2 we introduce a formalism that captures both of them, and then prove results for this more general model in Section 3. 2.1. Motivating applications Pool-based batch mode active learning Con-sider a simple model of pool-based Bayesian active learning . We are given a pool V of unlabeled examples x ,..., x n . We use y 1 ,...,y n  X  { +1 ,  X  1 } , where y the (initially unknown) label 1 of example x i . Our goal is to learn a classifier h : V  X  { +1 ,  X  1 } out of a finite set H of hypotheses, each corresponding to distinct la-belings of the pool V , and containing the true labeling, i.e., a hypothesis h such that h ( x i ) = y i for 1  X  i  X  n . For now let us assume that we have a uniform prior P ( h ) = 1 |H| over the hypotheses. We later show that our results also hold for more general priors, as well as for the prior-free (non-Bayesian) setting.
 Suppose we have already observed the labels y A for a subset A  X  V of the pool. In this case, some of the hypotheses h  X  H will be inconsistent with the observations y A , and we use the notation H ( y A ) = { h  X  X  : i  X  X  X  y i = h ( x i ) } to refer to the version space (set of hypotheses) consistent with the observation y A . We wish to actively select a minimum number of unlabeled examples and obtain their labels y
A , such that these allow us to uniquely identify h (i.e., infer the labels of all unlabeled examples), so that |H ( y A ) | = 1. An optimal active learning strategy is one that minimizes the expected number of labels re-quested, in expectation over our prior P ( h ). Similarly, we can consider strategies for batch-mode active learn-ing, which pick batches of k unlabeled examples at a time, then request all labels for the selected batch in par-allel, and then proceed to pick the next batch given the labels obtained so far. See Figure 1 for an illustration. Finding such an optimal policy is a formidable task. In fact, even representing an optimal batch policy may require exponential space. In the following, we will describe a general class of batch mode optimization problems, and present a simple greedy algorithm that is provably competitive with the optimal batch policy. Multi-stage influence maximization in social networks Suppose we would like to stimulate demand for a novel product. The idea behind viral marketing is to utilize the social network structure con-necting the potential customers: By giving the product to a subset of target people for free, these may influence their friends, potentially creating a cascade of influence motivating many more consumers to adopt the product. This problem was formalized by Kempe et al. (2003), who show that many natural models of influence (such as the independent cascade, or linear threshold models) can be modeled stochastically. Formally, let V = { 1 ,...,n } be the set of nodes in the social network, and let Y s  X  V be the (random) set of nodes eventually influenced if s is initially targeted. If a set A of nodes is initially targeted, the eventual influence is S s  X  X  Y s with probability 2 P ( Y A ) = Q s  X  X  P ( Y Instead of committing to all target nodes in advance, it is natural to consider conducting a multi-stage advertising campaign: In each stage certain nodes are targeted, then the effect of the campaign is observed, then the next target nodes are chosen, and so on. Implementing such a procedure may be much more practical if many nodes can be selected in each stage, to be influenced in parallel. In the following, we propose a simple greedy approach that is competitive not only with the optimal multi-stage strategy but even with an optimal fully sequential strategy. 2.2. General Problem Statement We now formalize a class of interactive optimization problems generalizing the two examples of Section 2.1. Adaptive Submodular Optimization We wish to adaptively select items A out of a finite set of n items V = { 1 ,...,n } (unlabeled examples; target nodes). Each item s  X  V is associated with a random variable Y s , taking values in a (finite) set O of outcomes (labels; sets of nodes eventually influenced). We use Y
V = [ Y 1 ,...,Y n ], to refer to the collection of all { x ,x 2 } { x 7 ,x 8 } { x 4 ,x 5 } variables, and assume that Y V is distributed according to a joint distribution P ( Y V ). Whenever an item s is selected, the corresponding variable Y s = y s is revealed. This information can be used to select subsequent items. We model the value associated with a set of items A , and corresponding observations y A  X  X  X  O by means of an objective function 3 f : 2 V X  O  X  N . In our active learning example, we can use f ( y A ) = |H| X  X H ( y A ) | , i.e., the number of the hypotheses eliminated through the labeled examples y A . In our viral marketing example, we choose f ( y A ) = | S s  X  X  y s | , i.e., the number of nodes eventually influenced. Furthermore, let S  X  X  X  O be a set of observations. Note that while technically y A and S do not denote the same objects ( S denotes a set of item/observation pairs, y A denotes the observations corresponding the item set A ), we will sometimes use these notations interchangeably to refer to observations. In both applications, f satisfies the following natural four properties 4 : 1. Normalized: f (  X  ) = 0, i.e., we derive no utility 2. Monotonic: Whenever S  X  S 0  X  V  X  O , then 3. Submodular : whenever S  X  S 0  X  V  X  O and 4. Adaptive submodular: Consider the conditional Our goal will be to find a policy  X  for selecting items (and associated observations) y A , such that we achieve a certain quota of value Q  X  0, i.e., f ( y A )  X  Q , while at the same time minimizing the number of items A used. In the active learning example, Q = |H|  X  1: achieving this quota is a necessary and sufficient con-dition for identifying the true hypothesis. In influence maximization, Q may be a certain fraction of the size of the social network. In the following, w.l.o.g. 5 , we assume f ( y V ) = Q for all y V  X  supp( P ).
 Formally, a policy  X  : 2 V X  O  X  V is a partial mapping from observations y A  X  V  X  O to the next item to be picked (or to stop, if y A /  X  dom(  X  )). Therefore, if the variables Y V are in state Y V = y V , the policy obtains a set of observations, which is denoted as S (  X , y V )  X  V  X  O . We define the expected and worst-case cost of policy  X  as cost ac (  X  ) = E y Our goal, is to find, out of a set  X  of candidate policies a feasible policy  X   X  with minimum cost, min Batch selection Based on this notation, we can study how different classes of policies compare in terms of their cost. On the one extreme, we have fully sequential policies  X  seq , where the choice of each item may depend on the labels of all previous items selected. On the other extreme, we have constant , or non-adaptive policies  X  const which commit to items picked in advance, before making any observations. However, fully sequential and constant policies are only two extremes on a spectrum. We are interested in poli-cies  X  [ k ] that sequentially pick batches of size k . Any policy  X   X   X  [ k ] starts selecting a fixed set A 1  X  V of k items. It then obtains all labels y A stops. Otherwise, if batches A 1 ,..., A `  X  1 have already been selected, it picks batch A `  X  X  of k items, obtains the labels, and stops if f ( y A Obtaining an optimal batch policy is a formidable task: There are n policy assembles such batches into a decision tree of possibly exponentially large branching factor. In the following, we describe a simple greedy algorithm, and prove that it implements a batch policy with cost com-petitive to that of the optimal batch policy. Moreover, we prove that under some additional conditions on the distribution P ( Y V ), the greedy algorithm is even competitive with the optimal fully sequential policy. We consider a simple, greedy approach towards con-structing batch policies. This policy, BatchGreedy , selects items within a batch in a greedy manner, then receives observations for all items in the batch, then selects the next batch in a greedy manner, conditional on all observations made so far, and so on. An important challenge in batch selection is the fact that the value of items (e.g., unlabeled examples) selected depends on observations (e.g., labels) obtained only after the entire batch is selected. In active learning for example, one wishes to select examples within a batch that are likely to be informative individually, but also diverse (minimize redundancy). BatchGreedy addresses this challenge by using a suitable notion of marginal benefit of an item, that takes into account all observations made so far, as well as items that have already been selected within the batch (but no obser-vation has been obtained yet). Formally, we generalize the conditional marginal benefit (2.1) of item s by  X  f ( s |A , y B ) = E y Thus,  X  f ( s | A , y B ) reflects the expected marginal gain of item s , when items B have been selected and the corresponding observations y B have been made, and items A have already been selected, but no obser-vations have yet been made about them. Therefore, (3.1) captures possible redundancy (diminishing gains) of candidate item s w.r.t. to labels already obtained, as well as labels that will likely be obtained within the Algorithm 1 The BatchGreedy algorithm.

Input: Quota Q . Objective f and prior P ( y V ) y B  X  X  X  repeat until f ( y B )  X  Q batch. Hence it encourages diversity among the items selected in the batch.
 Using this notation, the BatchGreedy policy will greedily select the i -th element in the j -th batch where y B is the set of observations (labeled examples) from batches up to j  X  1. After a batch is completed, all labels are requested and added to the observations y . Pseudocode is presented in Algorithm 1.
 If we set the batch size k to 1, BatchGreedy reverts back to a fully sequential, greedy active learning scheme. In particular, for the active learning example from Section 2.1, this algorithm is known as General-ized Binary Search , studied extensively in the literature (see e.g., Dasgupta (2004)). In fact, it is known that this simple greedy algorithm is near-optimal: its cost is upper-bounded by O (log |H| ) times that of the optimal sequential policy. More generally, Golovin &amp; Krause (2011) prove that this result can be generalized to any adaptive optimization problems that are adaptive submodular . As our first main theoretical contribution, we generalize their results, which only hold for fully sequential policies, to the batch setting.
 We first show that BatchGreedy is near-optimal as compared to the optimal batch selection policy. Theorem 1. Let OPT ac,k be the expected cost and OPT wc,k be the worst-case cost of an opti-mal policy selecting batches of size k . Further let  X  = min y policy  X  G implementing BatchGreedy it holds that Note that the guarantee of Theorem 1 matches (up to a small constant factor) hardness results known for the fully sequential ( k = 1) setting, which it general-izes, therefore BatchGreedy is near-optimal under computational constraints. Further note that for the active learning application, Theorem 1 guarantees that in the non-Bayesian setting (i.e., without any prior 6 ), BatchGreedy requires at most a factor of O (ln |H| ) more batches than the optimal batch-mode policy. As our second main theoretical result, we also prove that, perhaps surprisingly, under certain conditions BatchGreedy is not just competitive with the optimal policy that is restricted to selecting batches of examples: It is competitive with respect to an optimal fully sequential policy, which is not required to obey such a restriction. The additional condition needed is that the variables Y 1 ,...,Y n are independent. This assumption is satisfied in the influence maximization application, but not in the active learning problem. Theorem 2. Fix  X  &gt; 0 . Let OPT wc be the worst-case cost of an optimal sequential policy  X   X  , constrained to picking a number of items which is a multiple of k . Further suppose that the variables Y 1 ,...,Y n are independent. Then for the cost of the policy  X  implementing BatchGreedy , run until it achieves f (  X  G )  X  Q  X   X  it holds that Moreover, it holds that P f ( S (  X  G , y V ))  X  Q  X  1  X   X  . The proofs are given in the supplementary material. The key technical insight behind the proof is that a bound on the adaptivity gap for stochastic submodular maximization of Asadpour et al. (2008), adapted and generalized to our setting, allows us to interpret the BatchGreedy policy as an approximate implemen-tation of the fully sequential greedy policy. Note that Theorem 2, for technical reasons, is of a slightly dif-ferent flavor than Theorem 1: it compares the optimal policy  X   X  always achieving quota Q with one achieving the quota Q only with probability 1  X   X  . By choosing  X  &lt; min y in fact f ( S (  X  G , y V )) = Q for all y V with nonzero prob-ability. In this case, the bound on the worst-case cost of Theorem 2 is only a factor of e/ ( e  X  1)  X  1 . 58 larger than that of Theorem 1, irrespective of the batch size. As is, BatchGreedy is not immediately practical for the applications from Section 2.1: Computing the marginal gains (3.1) requires computing expectations Algorithm 2 Hit-and-run hypothesis sampler for lin-ear separators and noisy observations
Input: Labeled examples y B  X  R d , N , number of mixing iterations T , noise level  X   X  [0 , 0 . 5).
Output: Hypotheses set  X  H . w 0  X  a random point on d -dim. unit sphere S d for i = 1 to N do end for that may be intractable. In the influence maximization application, it is possible to perform Monte-Carlo sampling of the influence process to evaluate (3.1) up to arbitrarily small multiplicative error (1 +  X  ) (Kempe et al., 2003). Furthermore, with a slight generalization of the arguments of Golovin &amp; Krause (2011), using such an approximation of (3.1) increases the cost by at most the same factor (1 +  X  ).
 To obtain a practical algorithm for batch mode active learning, further challenges arise: BatchGreedy as is requires that H is finite, and its running time depends polynomially on |H| . Furthermore, it requires that observations are noise free. As a practical implemen-tation, we focus on active learning of linear separators, i.e., h ( x ) = sign( w T x ). In our experiments we use a Markov-Chain Monte Carlo sampler in order to generate samples from the posterior distribution over hypotheses P ( w | y S ). In particular, we build on the hit-and-run sampler (Smith, 1984; Lovasz, 1998), which is known to lead to a provably efficient near-optimal es-timation for the fully sequential active learning problem (Gonen et al., 2011). To handle noise, at each iteration, we generalize the hit-and-run sampler by sampling the Algorithm 3 Approximate implementation of Batch-Greedy for batch-mode active learning.
 Input: Hypotheses H , batch size k , noise level  X  Sample  X  H = { h 1 ,...,h N } from H using Algo. 2 Define  X  P ( H ) = 1 repeat until (1  X   X  ) of all hypotheses in the support of  X  P induce same labeling on the unlabeled pool entire version space, while varying the sample density according to a likelihood function. In the case of sym-metric binary channel (i.e., labels are flipped with prob-ability  X  ), we sample hypotheses with probability re-lated to the number of mistakes. This way, our method can handle data that are not linearly separable. Algo-rithm 2 presents details of our sampler, and our final batch-mode active learning algorithm is formalized in Algorithm 3. The time complexity of random sampling (Algorithm 2) is O ( TN ), where T is the number of mix-ing iterations. Once we discretized the hypothesis space with N samples, it takes O ( knN ) steps for Algorithm 3 to select a batch of k items. Hence the time complexity of Algorithm 3 selecting one batch is O ( N ( T + kn )). Furthermore, in both applications, we can use lazy evaluations to speed up the BatchGreedy algorithm (as used in Golovin &amp; Krause (2011) for the fully sequential setting). Lazy evaluations utilize the fact that the marginal gains  X  f ( s | A , y B ) are monoton-ically decreasing in both A and y B . This insight can be exploited by utilizing priority queues to accelerate selection of the next greedy choice. We empirically evaluate BatchGreedy on several data sets and on both applications discussed in Sec-tion 2.1. Our emphasis is on comparing BatchGreedy with baselines, as well as empirically quantifying the price of parallelism.
 Batch mode active learning of linear separators One natural way to perform batch mode active learn-ing is to select batches comprising the k most uncertain examples. As one baseline, we use  X  X atch mode margin-based active learning X  ( k -batch SVM) to greedily select batches of examples, as considered in Jain et al. (2010) for large-scale active learning. In this method, we ran-domly chose examples until there are two distinct la-bels, and we train SVM classifiers based on the labeled examples at the end of each batch. The next k un-labeled examples with the smallest distances from the decision boundary w T x + b = 0 are selected for label-ing. Another baseline approach we employ is the state of the art batch mode active learning algorithm (KLR-BMAL) of Hoi et al. (2006) that selects batches of k examples that are informative w.r.t. the Fisher informa-tion matrix. To see how well the parallelization of the se-lection process approximates the sequential algorithm, we compare with the fully sequential active learning algorithm, where only one example is selected and ob-served at each iteration, as well as a  X  X assive learning X  approach, where we make no observations during the learning process (corresponding to infinite batch size). We also compare BatchGreedy against the sequential active learning algorithm with purely random selection. For fair comparison, we use SVM as classifier for all competing algorithms, so the methods only differ by the set of examples chosen for labeling. We implement the KLR-BMAL algorithm using class membership probabilities inferred from the hypothesis sampler, and set the smoothing parameter  X  to be 0 . 1 (Hoi et al., 2006). As for our sampler, we set  X  to be 0 . 1. We normalize the data so that each feature has mean 0 and standard deviation 0 . 5, and place independent normal priors on each dimension. The results for all the batch mode active learning experiments are obtained from 150 random starts.
 We run our the first set of experiments on two UCI data sets 7 , WDBC (569 instances, 32-d) and Australian (690 instances, 16-d), using a fixed number of 5000 sampled hypotheses in each random trial. Figure 3(a) and Figure 3(b) depict the 150-trial average percentage of mistakes made by each algorithm when predicting the labels of the corresponding data set, for a batch size of k = 10. Figure 3(b) shows an improvement of BatchGreedy over both KLR-BMAL and the 10-batch SVM algorithm. On both datasets, surprisingly, BatchGreedy is competitive with the fully sequential greedy algorithm, with only minor differences. We also evaluate BatchGreedy on the MNIST data set. For each of the 14780 instances, we reduced the dimensionality down to 10 via PCA, and compare BatchGreedy with the sequential, KLR-BMAL, 10-bacth SVM, passive and random algorithms through 150 random trials. We observe that, even using 5000 sample hypotheses for each iteration, BatchGreedy is significantly faster than KLR-BMAL, as the cost of BatchGreedy grows linearly w.r.t. the number of hy-potheses and number of examples, while KLR-BMAL costs quadratically w.r.t. the number of examples. In fact, for the same settings, it takes KLR-BMAL approx. 50 seconds to select one example, compared to approx. 10 seconds for BatchGreedy .
 We also study the impact of the discretization param-eter M (i.e., number of hypotheses used to sample the version space), varying it from 300 to 5000, and we plot the results for each setting in Figure 3(d). For the WDBC data set, we can observe a statistically significant performance improvement across the 150 trials when increasing the number of sampled hypotheses used from 300 to 2000. Starting from 3000 samples, however, the advantage of introducing more samples begins to decrease dramatically. As there is a linear increase in running time as we employ more samples, we suggest to pick a moderate M to balance efficiency and accuracy. Multi-stage influence maximization in Social Networks We also apply BatchGreedy to the multi-stage influence maximization problem described in Section 2.1. We use two data sets from the SNAP repository 8 : the Epinions social network (with 75879 nodes and 508837 directed edges, where members of the site can decide whether to  X  X rust X  each other) and the Slashdot social network (with 82168 nodes and 948464 directed edges, where users are allowed to tag each other as friends or foes). For each network, we take the subgraph induced by the top 1000 nodes with largest outdegree. We use the independent cascade model (Kempe et al., 2003). In our simulations, we assume that each person has a certain, fixed probability to influence its neighbors. We choose this probability t according to the edge density of the target network, in our case to 0.05 and 0.03, respectively.
 We evaluate the performance of BatchGreedy while varying the size of batches picked at each stage. We repeat the experiments 100 times for all batch sizes (the non-adaptive method corresponds to infinite batch size). In each experiment, we initialize 100 random realizations of the target network based on the edge activation probability, and greedily select the best node in expectation. The results are summarized in Figure 3(e) and Figure 3(f). We observe that for the Epinions network, the sequential greedy policy covers 99% of the target network by selecting 244 nodes, while the 10-batch greedy policy, 100-batch greedy policy and non-adaptive greedy policy cost 241, 284, and 584 nodes respectively, to achieve the same coverage. Similarly, for the Slashdot network those numbers are 330, 319, 343, 765. After incorporation of the first batch of labels, BatchGreedy performs surprisingly well, even competitively with the fully sequential policy. Active Learning Fully sequential active learning, where one single unlabeled example is selected to be labeled at a time, has received much attention in the machine learning community. Several heuristic approaches have been proposed that perform well in some applications (e.g., MacKay (1992); Settles (2010)), and theoretical investigations (e.g., Balcan et al. (2006); Dasgupta (2006)) prove bounds on the label complexity. While in some cases exponential reduction is possible, in other settings little reduction over passive learning can be achieved (Dasgupta, 2006). Therefore, it is interesting to study active learning as an optimization problem with the goal to efficiently identify a sufficient yet near-minimal number of informative labels. For both noise-free (Dasgupta, 2004) and noisy (Golovin et al., 2010) active leaning problems, simple greedy policies have been shown to be provably competitive with the optimal (intractable) policy. This paper builds on these approaches, and generalizes the results to batch mode active learning. Batch mode active learning and submodularity Due to its practical importance, several approaches for batch mode active learning have been proposed. How-ever, previous work has focused on efficiently selecting a single batch comprising examples that are both diverse and informative. Interestingly, the classical notion of submodular set functions has proven useful (Hoi et al., 2006; Guillory &amp; Bilmes, 2011). In Hoi et al. (2006), both individual diversity and informativeness are evaluated w.r.t. the Fisher information matrix of the estimated linear separator. Cesa-Bianchi et al. (2010) and Guillory &amp; Bilmes (2011) investigate the problem of active learning on graph structured data, and provide near-optimal solutions for such problems. In contrast to the prior work, this paper focuses on analyzing batch-mode policies, i.e., sequential construction of batches with the goal to minimize the overall number of labels needed across batches. A work closely related in spirit is that of Desautels et al. (2012), who consider batch mode bandit problems, where the goal is to trade exploration and exploitation, and performance is measured w.r.t. the cumulative regret. In spite of a different context and formalism, a greedy algorithm works provably well in that setting. Adaptive and interactive submodular opti-mization For a general introduction to adaptive submodular optimization, see Golovin &amp; Krause (2011). Asadpour et al. (2008) study a special case of the adaptive submodular maximization problem where the random variables must be independent (which is not the case in active learning). Guillory &amp; Bilmes (2010) consider a different formalism for interactive submodular maximization and its applications, ana-lyzing the worst-case cost of fully sequential policies. In contrast to these previous approaches, which focus on the fully sequential case, in this paper we analyze the batch-mode setting, for which we provide approximation bounds as well as practical algorithms. Influence maximization This problem was first introduced by Domingos &amp; Richardson (2001), and Kempe et al. (2003) prove that the problem of (non-adaptively) finding the optimal k individuals in the network to target requires submodular maximization. Recently, by using adaptive submodularity, Golovin &amp; Krause (2011) generalize the problem to the fully sequential setting where observations are made after each selection. Our results generalize theirs and ad-dress the multi-stage setting, which, to our knowledge, is the first attempt to address this natural variant of the influence maximization problem. We presented a general framework for batch mode active learning and stochastic optimization. We analyzed BatchGreedy , an intuitive adaptive greedy approach, and proved its competitiveness with the op-timal batch-mode policy. For some problem instances (e.g., multi-stage influence maximization) we proved that, perhaps surprisingly, using batches only incurs a bounded increase of cost as compared to allowing fully sequential selection. In addition to new theoretical results, we empirically demonstrate the effectiveness of BatchGreedy on two real-world applications: Batch mode active learning of linear separators (where BatchGreedy outperforms the state of the art), and multi-stage influence maximization (where we observe a surprisingly small increase in cost compared to the fully sequential strategy). A natural question for future work is to understand more generally for which problems the price of parallelism, i.e., the increase in cost by re-stricting to information-parallel decisions, is bounded. We believe that our results provide an important step in characterizing the (approximate) tractability of practical active learning and optimization problems. Anshelevich, Elliot, Chakrabarty, Deeparnab, Hate,
Ameya, and Swamy, Chaitanya. Approximation al-gorithms for the firefighter problem: Cuts over time and submodularity. In Algorithms and Computation . Springer Berlin / Heidelberg, 2009.
 Asadpour, Arash, Nazerzadeh, Hamid, and Saberi, Amin. Stochastic submodular maximization. In WINE , pp. 477 X 489, Berlin, Heidelberg, 2008. Springer-Verlag.
 Balcan, Maria Florina, Beygelzimer, Alina, and Lang-ford, John. Agnostic active learning. In ICML , pp. 65 X 72, 2006.
 Cesa-Bianchi, Nicol`o, Gentile, Claudio, Vitale, Fabio, and Zappella, Giovanni. Active learning on trees and graphs. In COLT , pp. 320 X 332, 2010.
 Dasgupta, Sanjoy. Analysis of a greedy active learning strategy. In NIPS , 2004.
 Dasgupta, Sanjoy. Coarse sample complexity bounds for active learning. In Weiss, Y., Sch  X olkopf, B., and Platt, J. (eds.), Advances in Neural Information Processing Systems 18 , pp. 235 X 242. MIT Press, Cambridge, MA, 2006.
 Desautels, Thomas, Krause, Andreas, and Burdick,
Joel. Parallelizing exploration-exploitation tradeoffs with gaussian process bandit optimization. In ICML , 2012.
 Domingos, Pedro and Richardson, Matt. Mining the network value of customers. In KDD , pp. 57 X 66, 2001. Golovin, Daniel and Krause, Andreas. Adaptive sub-modularity: Theory and applications in active learn-ing and stochastic optimization. Journal of Artificial Intelligence Research (JAIR) , 42:427 X 486, 2011. Golovin, Daniel, Krause, Andreas, and Ray, Debajyoti.
Near-optimal bayesian active learning with noisy observations. In NIPS , December 2010.
 Golovin, Daniel, Krause, Andreas, Gardner, Beth, Con-verse, Sarah, and Morey, Steve. Dynamic resource allocation in conservation planning. In AAAI , 2011. Gonen, Alon, Sabato, Sivan, and Shalev-Shwartz,
Shai. Active learning halfspaces under margin assumptions. CoRR , abs/1112.1556v3, 2011.
 Guillory, Andrew and Bilmes, Jeff. Interactive submodular set cover. In ICML , 2010.
 Guillory, Andrew and Bilmes, Jeff. Active semi-supervised learning using submodular functions. In UAI , 2011.
 Hoi, Steven C. H., Jin, Rong, Zhu, Jianke, and Lyu,
Michael R. Batch mode active learning and its appli-cation to medical image classification. In ICML , 2006. Jain, Prateek, Vijayanarasimhan, Sudheendra, and
Grauman, Kristen. Hashing hyperplane queries to near points with applications to large-scale active learning. In Advances in Neural Information Processing Systems 23 , pp. 928 X 936. 2010.
 Kempe, David, Kleinberg, Jon, and Tardos,  X  Eva.
Maximizing the spread of influence through a social network. In KDD , pp. 137 X 146, 2003.
 Lovasz, Laszlo. Hit-and-run mixes fast. Math. Prog , 86:443 X 461, 1998.
 MacKay, David J.C. Information-based objective func-tions for active data selection. Neural Computation , 4(4):590 X 604, 1992.
 Settles, Burr. Active learning literature survey. Tech-nical Report 1648, University of Wisconsin-Madison, 2010.
 Smith, Robert L. Efficient monte carlo procedures for generating points uniformly distributed over
