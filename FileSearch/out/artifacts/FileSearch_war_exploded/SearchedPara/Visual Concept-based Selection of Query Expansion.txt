 In this paper we present a novel approach to semantic-theme-based video retrieval that consider s entire videos as retrieval units and exploits automatically detected visual concepts to improve the results of retrieval based on s poken content. We deploy a query prediction method that makes use of a coherence indicator calcu-lated on top returned documents and taking into account the in-formation about visual concepts presence in videos to make a choice between query expansion methods. The main contribution of our approach is in its ability to exploit noisy shot-level concept detection to improve semantic-the me-based video retrieval. Strik-ingly, improvement is possible us ing an extremely limited set of concepts. In the experiments performed on TRECVID 2007 and 2008 datasets our approach shows an interesting performance improvement compared to the best performing baseline. Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval General Terms: Algorithms, Performance, Experimentation Keywords: Semantic-theme-based vide o retrieval, video-level retrieval, query expansion, conc ept-based video indexing, query performance prediction The semantic theme (or subject matter) of a video is encoded in both its speech track and visual channel. This paper presents a novel multimodal retrieval approach aiming at videos covering the semantic theme expressed by the query. The approach improves the results of a speech-based retrieval system by exploiting con-cepts (e.g. human , car , house , female , children , indoor ) detected in the visual channel. In contrast with most previous works on video retrieval, which focus on shot -level retrieval, e.g., [2, 5], our approach is designed to retrieve entire videos. These larger re-trieval units are more appropriate than shots in cases where the searcher is looking for informa tional material or for entertain-ment, typical for the semantic-theme-based retrieval scenario (e.g., find a video about archaeology or psychology). Recent work on retrieval beyond the shot level includes [1]. Moving from the shot to the video level requires the combination of multiple shot-level concepts into an effe ctive video-level representation. The novel contribution of our work is the successful use of such a video-level representation to combine the output of automatic speech recognition and visual-concept detection, both known to be noisy, and achieve an overall improvement in retrieval of vid-eos on the basis of semantic theme. The key insight motivating our approach is that the presence, frequency and co-occurrence of visual concepts are potentially powerful indicators of the similar-ity of videos with respect to thei r semantic themes. In this initial realization, we focus on exploiting the effects of concept presence and frequency. cepts, we select a coherence-ba sed query prediction framework [4]. This technique automatically chooses between results lists produced by a range of different query expansions by making use of a coherence-based indicator calculated over top documents of the results list returned by each expansion. Our specific innova-tion is use of concept-based repres entations of videos for calcula-tion of the query prediction indicator. In the following, we first introduce our concept-based video representations and our query prediction method. Then we report on results of experiments that confirm the viability of our approach for effectively exploiting visual concepts to refine spoken c ontent retrieval of videos at the semantic theme level. The input for creating video-level representations is a vector in which each component represents a single concept. In order to weight each concept, we make use of term frequency (TF) and inverse document frequency (IDF) analogously to their use in text retrieval. However, concept det ectors do not output binary con-cept occurrences, but rather shot-level lists of confidence scores. Each score represents the probability that a particular concept occurred in each shot. Our concept weights are created by accu-mulating the confidence scores fo r each concept over all shots in the video and then normalizing with the total shot count. the next (feature selection) step the concepts that are potentially the most helpful in discriminating between videos with respect to their semantic themes. Because the output of concept detectors is notoriously noisy (in TRECVID 2009 the best performance failed to exceed 0.25 in the terms of MAP [5]), feature selection is a key aspect of our approach since it introduces a noise control effect. We developed our feature selec tion method by performing ex-ploratory experimentation. Our experiments make use of TREC-VID 2007 dataset and 46 semantic theme labels introduced by the VideoCLEF 2009 (www.multimedi aeval.org) benchmark. The labels are manually assigned by professional archivists from the Netherlands Institute for Sound and Vision. We use a set of pub-licly available concept detection sc ores, generated for a set of 374 concepts selected from the LS COM (www.lscom.org) ontology. In order to determine the most representative concepts, we trained classifiers that can identify videos related to each semantic theme and ranked the concepts according to their usefulness to the classi-fier. A simple voting approach was then applied to merge the lists into a single list that ranked the concept from most to least valu-able for semantic discri mination. Our investigation revealed that it is the most frequently occurring concepts in the videos that best support discrimination between videos in terms of semantic class. We use this result to select features in the coherence-based query prediction step. The fact that the most frequent concepts are most discriminative suggests that it is not so much the occurrence of a particular visual concept in a vide o that distinguishes that video's semantic class, but rather its relative frequency. and returns, as the final output, the results list with the highest coherence score over the Top N videos. The coherence score is calculated as: Here  X  is a function defined on the pair of videos ( v equal to 1 if their similarity is higher than the similarity of particu-larly close video pairs from the collection (i.e., closer than TP % of pair-wise similarities, where TP is the threshold defined below). As a similarity measure between vectors of concept frequencies we used the cosine similarity. Use of alternative similar-ity/distance measures such as Kullback-Leibler divergence and Euclidean distance yielded similar results. We test our approach on TRECVID 2007 and TRECVID 2008 datasets, using the 46 VideoCLE F 2009 semantic labels as que-ries. We index the Dutch speech recognition transcripts and the English machine translation and car ry out retrieval using the Le-mur toolkit. The initial results lis ts are produced using the original query and three query expansions: 1) Conventional PRF, where the number of feedback document s and terms used for expansion are selected for the optimal performance, 2) WordNet (http://wordnet.princeton.edu/) expansion and 3) Google Sets (http://labs.google.com/sets) expans ion, where each query is ex-panded with a set of up to 15 related items (words or multi-word phrases). For each query, the results list yielding the highest co-herence indicator is selected. In the (rare) cases of the same indi-cator values the priority is given to the baseline or the expansions following the ordering as above. We use the concept detection output provided by [3], as mentioned above. In the experiments on both collections we swept the parameter space for the following parameters: number of most frequent (discriminative) semantic concepts ( NC ), number of documents from the top of results list ( N ) and the threshold TP used to calculate the coherence score. We reported the results for the optimal parameter setting. The quality of the initial results list for the original query and three expansion methods is reported in terms of Mean Average Precision (MAP) in Table 1. TRECVID 2007 0.326 0.332 0.260 0.120 TRECVID 2008 0.245 0.265 0.268 0.142 Table 2 contains the MAPs of our concept-based selection of query expansion for TRECVID 2007 and TRECVID 2008 data-sets. Statistical significance w. r.t. Wilcoxon Signed Rank Test, p &lt; 0.02 is indicated with  X  X  X . Best Baseline 0.332 0.268 for refining the output of spoken-c ontent-based video retrieval at the level of a semantic theme. Recall that our feature selection approach was optimized using TRECVID 2007 as a development set. The fact that TRECVID 2 008 yielded similar performance demonstrates the ability of our feature selection method to gener-alize to new data. The optimal parameter settings for TRECVID 2007 and TRECVID 2008 are not the same for both datasets, but are in the similar range: N =5-10, TP =80-90%, NC = 5-10. We have proposed a multimodal approach to semantic-theme-based retrieval of entire videos that exploits frequencies of (se-mantic) concepts detected in a video to enhance the initial re-trieval result obtained at the spoke n-content level. We have dem-onstrated that our approach can be effectively used to decide whether the query should be expa nded and which of several query expansions to use. Further, we are making use of only a fraction (5-10) of the set of available concepts (374). This result suggests that concept detectors that focu s on a very small number of con-cepts have large potential to be useful for improving the results of semantic-theme-based video retrieval. In our future work we will further study the characteristics of concept detector output that contribute to effective performance of our approach, investigating, for example, whether the relatively larger performance improve-ment achieved on the TRECVID 2008 set (cf. Table 2) can be attributed to better performing concept detectors. We will also work to take into account concept co-occurrences and to combine proposed concept-based and text-based indicators to further im-prove query prediction. The research leading to these results has received funding from the European Commission's 7t h Framework Programme (FP7) under grant agreement n X  216444 (NoE PetaMedia). [1] Aly, R., Doherty, A., Hiemstra, D., a nd Smeaton, A. 2010. [2] Hsu, W. H., Kennedy, L. S. , and Chang, S. 2006. Video [3] Jiang, Y-G., Yanaga wa, A., Chang, S-F., and Ngo, C-W. [4] Rudinac, S., Larson, M., and Hanjalic, A. 2010. Exploiting [5] Snoek, C. G. M., van de Sande, K. E. A., de Rooij, O., et al. 
