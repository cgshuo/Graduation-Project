 Norbert Fuhr  X  Norbert G  X  overt
Abstract Content-only queries in hierarchically structured documents should retrieve the most specific document nodes which are exhaustive to the information need. For this problem, we investigate two methods of augmentation, which both yield high retrieval quality. As approximations to the  X  X orrect X  retrieval result may yield higher effectiveness. We present a classification scheme for algorithms addressing this issue, and adopt known algorithms from standard document retrieval for XML retrieval. As a new strategy, we propose incremental-interruptible retrieval , which allows for instant presentation of the top ranking documents.
We develop a new algorithm implementing this strategy and evaluate the different methods with the INEX collection.

Keywords XML retrieval . Content-only search . Ranked retrieval
Incremental algorithm 1. Introduction
With the increasing availability of documents in XML format, there is a growing need for retrieval methods exploiting the specific features of this type of documents. In 2002, INEX (initiative for the evaluation of XML retrieval) started, with the goal of defining standard retrieval tasks for XML documents and developing appropriate evaluation methods (Fuhr et al., 2003, 2004).

Since XML documents contain explicit information about their logical structure, XML retrieval methods should take into account the structural properties of the documents to be retrieved. One of the two tasks considered in INEX deals with content-only queries , where only the requested content is specified. Instead of retrieving whole documents, the IR system should aim at selecting document components that fulfill the information need. the FERMI model (Chiaramella et al., 1996), these components should be the deepest components in the document structure, i.e. most specific, while remaining exhaustive to the information need.

In this paper, we call this task also  X  X pecificity-oriented search X . Like for any kind of retrieval, there are two major issues to be addressed: Retrieval quality: How can we retrieve relevant documents, by avoiding non-relevant ones?
Whereas the FERMI model is easily applicable in the case of binary indexing (select the smallest component containing all query terms), the usage of weighted indexing requires a method which balances between indexing weights and component specificity. For this purpose, we have developed the method of augmentation , which gave us very good results in the first two INEX rounds.

Efficiency: How can we retrieve the top-ranking answers in very short time? Given the fact that not only complete documents, but (in principle) any XML element may be retrieved (and any element containing at least one of the search terms is a candidate answer), we need appropriate methods for dealing with this extended search space.

In this paper, we take an integrated view on retrieval quality and efficiency, by focusing achieving it. For example, in interactive IR, a user can achieve a higher retrieval quality by reformulating the query and/or by giving relevance feedback information (as an extreme case, maximum quality can be achieved by scanning through the whole collection). In most IR are proportional, and so often the distinction between these two concepts is dropped. Here we assume that the user X  X  effort is proportional to the retrieval time, which may be substantial for structured, multimedia (see e.g. the work on approximate similarity search by Amato et al. (2003)) or hyper-media collections; given the fact that today X  X  PCs already can store hundreds of gigabytes, there also should be methods for searching these data volumes in an effective way. For these reasons, the variance in user effort resulting from the response time cannot be neglected. In such a setting, a high retrieval effectiveness can be achieved either by improving retrieval quality or by reducing retrieval time.

In this paper, we address both issues: 1. First, we describe a method which gives high retrieval quality for content-only queries in XML retrieval; this method achieved the best results among all the submitted runs in
INEX 2002, and was (in unmodified form) among the best runs in INEX 2003. 2. Then we investigate several methods for reducing retrieval time, without sacrificing too much retrieval quality. Thus, we can improve retrieval effectiveness.

Although users are interested only in the top n documents of the result ranking only, algorithms focusing on these elements achieve small efficiency gains over standard methods computing the full ranking (see e.g. Buckley and Lewit 1985). Thus, we are looking for approximation methods, accepting small losses of retrieval quality, but expecting big gains in by dealing with the tradeoff between retrieval quality and efficiency. atomic (complete) documents, adapt them for XML retrieval and compare their performance.
In addition, we propose a new strategy for dealing with quality vs. efficiency: In interrupt-driven retrieval the user does not have to wait for the system to provide an answer; instead, s/he may request immediate delivery of the best answer currently available. We present a new retrieval algorithm following this strategy, and propose an appropriate evaluation method. The experimental evaluation of the different approaches is based on the INEX test bed.
The remainder of this paper is structured as follows: In the next section, we briefly describe our augmentation method and present two variants that both yield high retrieval quality. The and efficiency, and gives a classification scheme for these methods. Section 4 describes how these algorithms can be applied for retrieval of structured documents. Experimental results are presented in Section 5, followed by the final conclusions. 2. An augmentation-based approach for high-quality retrieval
The goal of specificity-oriented search is to retrieve the most specific document components that satisfy the query. For this purpose, two issues have to be addressed: 1. Irrespective of the query, what are possible retrieval answers? Given the fine-grained 2. How can we combine the criterion of specificity of an answer with weighted indexing? For the first problem, we have adopted the notion of index nodes from the FERMI model:
Given the XML document type definition (DTD), only certain elements are to be considered as possible retrieval answers. Thus, we assume that the database administrator has defined these element types beforehand.

For computing indexing weights, we want to apply standard methods developed for atomic documents. For this purpose, we have to split each document into disjoint parts. On the other hand, index nodes may be nested. Thus, we start with the most specific index nodes, treating their textual content like an atomic document. For the higher-level index nodes, only the text that is not contained within the other index objects is indexed. As an example, assume that we have defined section, chapter and book elements as index nodes. Then the title and introduction of a chapter would form this additional text chunk, since they do not belong to the sections included in the chapter.

When performing retrieval, the index weights of the most specific index nodes are given directly. For retrieval of the higher-level nodes, we have to combine the weights of the dif-ferent text units contained therein. For example, assume the following excerpt of a document structure, where we list the weighted terms instead of the original text: &lt;chapter&gt; 0.3 XPath &lt;section&gt; 0.5 example &lt;/section&gt; &lt;section&gt; 0.8 XPath 0.7 syntax &lt;/section&gt; &lt;/chapter&gt;
A straightforward possibility would be the logical disjunction of the different weights for a single term. As an example, the Boolean query  X  X yntax AND example X  can be answered by the complete chapter only, with a weight of 0 . 5  X  0 . 7 = 0
However, searching for the term  X  X Path X  in this example would retrieve the whole chapter a lower weight (assuming that term occurrences in different nodes are also independent).
Obviously, this strategy always assigns the highest weight to the most general element, thus contradicting the FERMI model mentioned before. As a solution that is in line with this model, we adopt the augmentation strategy from Fuhr et al. (1998), where index term weights have to be down-weighted when they are propagated upwards to the next index node.
The most obvious method for propagation is multiplication by a propagation weight, as proposed in Fuhr and Gro X johann (2004). As a probabilistic interpretation, we assume that for each term weight being propagated to the next higher index node, there is an additional
Therefore, we call this method conditional propagation . In our example, using a propagation weight of 0.3, the section weight of the term  X  X Path X  would yield 0 chapter level. Combining this propagated weight with the indexing weight of the chapter would yield an augmented weight of 0 . 3 + 0 . 24  X  0 . 3 term would rank the section ahead of the chapter.

As an alternate method of augmentation, we assume an exponential form, which has some nice mathematical properties (see below); here we take the counter-probability to the power of a propagation weight  X  . This method is called potential propagation here. When propagating a weight u of a term upwards to the next index node level, the resulting weight is computed as 1  X  (1  X  u )  X  . In our example document, the section weight 0  X  X Path X  combined with a propagation weight of 0 . 2 would yield 1 thus resulting in a combined weight of 0 . 3 + 0 . 275  X  0
In order to describe the augmentation process more formally, let us assume that a document relation symbol  X  with  X  X  X  N  X  N , where n j  X  n k iff n j there is a function  X  : N  X  IN 0 giving the level of a node, where the root has level 0 and  X  ( n ) =  X  ( n k ) + 1iff n j is a child of n k .

Let u im = P ( t i | n m ) denote the probabilistic indexing weight of term t we are dealing with logical disjunctions of independent probabilistic events (occurrences of the same term in different indexing nodes), we will mainly consider counter-probabilities term occurrences with weights u il and u jm can be computed as  X  u and the propagated weights for t i from its descendants. For this purpose, let us assume a propagation function p ( u ij , n j , n m ) which propagates the weight of term t node n m (provided that n m  X  n j ). Thus, the augmentation function a ( n the weight of node n m for term t i can be defined as  X  w = a ( n m , t i ) =  X  u im (where  X  w im = 1  X  w im ). The general forms of the two propagation functions discussed for our example document above can be defined as follows: p ( u p ( u
In case the propagation weight  X  2 ( n m , n j ) is additive, (i.e.  X  such that we must consider only the augmentation weights w and not the indexing weights of all its ancestors: a ( n m , t i ) =  X  u im specific, taking into account e.g. the number of children/descendants of n length of the path between n m and n j . Here we assume a constant factor (  X   X  X aking the latter additive) for propagating from one index node level to the next one.
Thus, the propagation functions can be simplified to p p
In the remainder of this paper, we only consider these special forms of conditional ( p and potential ( p 2 ) propagation. 3. Efficiency vs. retrieval quality quality, in order to optimize effectiveness. Especially, we are looking at fast approximation method) retrieval result, but require less computational effort. With regard to this problem, we can classify retrieval algorithms along two dimensions, namely processing mode and control mode:
Processing mode describes how the result of a query is produced: block: The complete result is computed as one block. incremental: Results are produced iteratively, such that top-ranking documents are de-
Control mode: specifies how the production of results (and especially the tradeoff between efficiency and retrieval quality) can be controlled: tunable: There is a tuning parameter which affects the quality of the approximation to interruptible: At any point in time, the user may ask for the next result element, and the
In principle, one can imagine algorithms for all possible combinations of values for these two dimensions; e.g., a block-interruptible algorithm would yield a complete ranked list at the moment the user requests it.

In the following, we will briefly describe some approaches (for retrieval of atomic doc-uments) published in the literature and classify them according to this scheme. Since most approaches in information retrieval are based on linear retrieval functions, we will restrict our discussion to this type of function. Furthermore, like most algorithms, we will only allow for accesses to the inverted file (and thus ignore approaches involving direct access methods, for retrieving the weight of a specific document for a given term, see e.g. Fagin (1999)).
First, let us introduce some additional notations: a query q consists of a list of search terms ( t 1 ,..., t l ), which are ordered by decreasing query term weights c document d m , let u im denote the indexing weight for term t is defined as r ( q , d m ) =
Assuming an inverted file structure, there is an inverted list for each term t of document d m and u im gives the weight of term t i for this document. In most cases, the entries are sorted by increasing document number, to allow for run-length compression and more efficient processing. However, we will also consider other orderings below.
The standard algorithm for computing the  X  X orrect X  result first creates an accumulator A file entries are processed term-wise, adding the product c entries are processed, the accumulators containing the top k retrieval status values (RSVs) are determined and the corresponding documents are retrieved. So the standard algorithm is a block-fixed algorithm according to our classification
Moffat and Zobel (1996) describes two algorithms for computing approximate retrieval results, which take as tuning parameter the number of documents (accumulators) to be con-sidered. Here accumulators are created only when a new document number is encountered, and inverted lists are accessed in the order of increasing query term weights. After each inverted list, the algorithm checks if the specified number of accumulators has been reached. stops processing of inverted file entries and ranks the accumulators created so far. In con-trast, the continue algorithm processes also the remaining inverted lists, but does not create any additional accumulators; thus, the correct RSVs are computed for those documents seen before, which, in turn, form the input to the ranking step. The continue algorithm is more efficient than than the standard algorithm due to a new data structure and algorithm which is described in Buckley and Lewit (1985), which uses a different set of tuning parameters: besides the number k of requested documents, also a value m algorithm stops when it can guarantee that m of the top k documents have been computed correctly. All these algorithms are block-tunable according to our classification scheme.
In Pfeifer and Pennekamp (1997), a general class of algorithms for incremental compu-tation of the ranked list is described. For each accumulator A terms T m from the query for which inverted file entries have been read for this document.
During processing, upper and lower bounds for the RSV in each accumulator are computed, and accumulators are ordered by their upper bound. Whenever there is a document whose lower bound is greater or equal to the upper bound of the current top-ranking of the remaining documents, it can be written to the output list. So these algorithms can be characterized as incremental-fixed. By modifying the output criterion, also incremental-tunable algorithms are possible.
 As a specific instance of this class of algorithms, the Nosferatu algorithm (Pfeifer and
Pennekamp 1997) assumes that inverted list entries are sorted by decreasing indexing weights processed in parallel, and entries are read in the order of decreasing RSV increments. Table 1 gives a survey over the algorithms discussed in this section.

In this paper, we will investigate an incremental-interruptible version of the Nosferatu algorithm which we call Nosferatu  X  . For this purpose, we simply sort accumulators according to their current values. Whenever a document is requested, we output the current top-ranking element and exclude it from further computations. So this algorithm consists of the following steps: 1. Start with an empty set of accumulators and an empty output list. 2. Read the inverted list entry ( o m , u im ) with the highest RSV increment c 3. Create an accumulator A m : = 0 and term set T m : = X  5. If there are any unread inverted list entries, proceed with step 2; otherwise stop. 6. Whenever a document is requested, output the top-ranking one and delete it from the set
Methods for efficient ranking are also discussed in the context of multimedia databases. In approaches assume that the system also has random access to the documents in order to get the indexing weights for the different query conditions (terms). Fagin (1996) has presented a theoretically optimum strategy for this case. By using additional information, the quick-combine algorithm described in G  X  untzer et al. (2000) is able to improve response times by an order of magnitude. These algorithms are block-fixed. Another line of research deals with similarity search (i.e. given a query object, find the k most similar objects in the database) involving complex similarity measures; here several researchers have addressed the issue of approximate similarity search, where the system may miss some answers, but computes the result more efficiently (see e.g. Amato et al. 2003). Most of these methods have been presented as block-tunable algorithms, but other variants also would be possible. 4. Retrieval algorithms for structured documents specificity-oriented search in XML documents, we also have to consider the document struc-ture, and store appropriate information in the inverted list entries. Since we do not consider ture only, without considering XML element types. For describing the hierarchical structure, it is sufficient to enumerate the nodes at each node level; in order to support augmentation during indexing time, we must specify the complete path from the document root to the current node, by giving the list of corresponding node indexes.

Assuming that all entries for a document are stored together, a group of entries for all occurrences of a term in a document has the structure ( o ing the current term. For each of these nodes, there is an entry e where l j specifies the length of the path ( n 1 ,..., n l gives the indexing weight for this node. The entries within a document are stored in inorder sequence. This way, augmentation can be performed easily at retrieval time. As an exam-ple, the following document entry lists 5 occurrences of terms, with path lengths between 1to4: (1234, 5, (1,(1),0.5), (3,(1,1,2),0.3), (3,(1,1,4),0.2), (4,(1,2,7,3),0.4), (2,(2,3),0.6)) . A method for compressing this type of inverted lists is described in Thom et al. (1995)
This basic structure can be varied in a number of ways: weights stored: We can store either indexing weights or augmentation weights. The latter is only possible for potential propagation, or in case we store weights for all nodes to which indexing weights are propagated. Storing only indexing weights allows for choosing the propagation method and weight at retrieval time, whereas storage of augmentation weights can speed up the retrieval process. degree of augmentation: In case we store augmentation weights, we can choose for which nodes we create an occurrence entry: At least, we must consider all nodes which have been assigned indexing weights. The other extreme would be the storage of entries for all nodes to which indexing weights are propagated. Intermediate solutions are also possible, where different criteria can be used for choosing the nodes to be considered. ordering of entries Usually, inverted file entries are stored in the order of increasing doc-ument numbers. For applying the Nosferatu  X  algorithm, we also test two other sorting orders. As additional data, we compute the maximum augmentation weight of a node in the document, and then sort document entries according to decreasing values of these weights. However, retrieval with this sorting order requires the consideration of all nodes with nonzero weights within a document where possibly only one node has a high weight.
Therefore, we also consider an inverted file organization where we give up document-wise grouping of occurrence entries, and store each occurrence along with its document num-ber. Then we sort occurrence entries of the complete collection according to decreasing weights; of course, we can combine this organization with both types of weights and all degrees of augmentation. 5. Experiments 5.1. Test setting
For our experiments, we used the INEX test collection (Fuhr et al. 2003). The document collection is made up of the full-text, marked up in XML, of 12 107 articles of the IEEE
Computer Society X  X  publications from 12 magazines and 6 transactions, covering the period of 1995 X 2002, and totalling 494 megabytes in size. Although the collection is relatively small compared with TREC, it has a suitably complex XML structure (192 different content models
XML nodes, where the average depth of a node is 6.9. A more detailed summary can be found where the following XML elements formed the roots of index nodes: article, section, ss1, ss2 (the latter two elements denote two levels of subsections).

As queries, we used the 24 content-only queries from INEX for which relevance judgments are available. Figure 1 shows an example query. Like in TREC, an INEX topic consists of the words from the topic title, the description and the keywords section as query terms.
For relevance assessments, INEX uses a two-dimensional, multi-valued scale for judging about relevance and coverage of an answer element. In our evaluations described below, recall and precision figures are based on a mapping of this scale onto a one-dimensional, binary scale where only the combination  X  X ully relevant/exact coverage X  is treated as relevant and all other combinations as non-relevant. This choice pinpoints the general goal of finding the most specific, highly relevant elements. 2 For each query, we considered the top ranking 1000 answer elements in evaluation. 3 5.2. Retrieval based on augmentation
In the first series of experiments, we evaluated the retrieval quality of our augmentation-based approach. For computing the indexing weights of terms in index nodes, we applied the standard BM25 formula (Robertson et al. 1992). As query term weights, tf weights were used.

When comparing the retrieval quality of conditional and potential augmentation, it turned out that both methods yield nearly identical results; they only differ in the absolute RSV values. However, the choice of the propagation weight has a substantial effect on performance, as can be seen in the recall-precision curves (Figure 2) as well as in the average precision for 100 recall points (Table 2). In INEX 2002, we submitted 2 official runs with conditional augmentation using weights of 0.3 and 0.6. The former gave the best performance among all official runs. From Table 2, we learn that we can even improve our performance by choosing a propagation weight of 0.2. Since INEX is the only available XML test collection for which weight is collection-dependent. However, the study (Hatano et al. 2004) suggests that the optimum propagation weight may also depend on the type of the query. So this will be an
Furthermore, since there are no differences between conditional and potential augmentation, we only consider potential augmentation in the following.
 5.3. Predictors of retrieval time timings depend on a large variety of factors related to both hardware (e.g. CPU, main memory, disk) and software parameters (e.g. implementation language, operating system). Therefore, of measuring retrieval times directly (with all the related problems of stochastic experiments and comparability of implementations), we were looking for other parameters that are good predictors of retrieval time. For this purpose, we measured retrieval times for the 30 INEX
CO queries and plotted the actual timings against two parameters: 1. sum of all document frequencies of query terms, 2. sum of all index node frequencies of query terms.

Moreover, we ran these experiments with two different indexing schemes: Besides the stan-dard scheme with four levels of index nodes as described above, we considered a scheme with six levels, where also body and paragraph elements were treated as index node roots. in Table 3 indicate a strong linear relationship between retrieval times and both types of term frequencies. Thus, any of these frequencies is a good predictor of retrieval time. In the following, index node frequencies are used for this purpose. 5.4. Quit and continue algorithm
We implemented a variant of the quit and continue algorithms mentioned in Section 3. Instead of specifying the number of accumulators to be used, our tuning parameter is the percentage of query terms to be considered; here query terms are ordered by their tf gives the corresponding figures.

For the quit algorithm, efficiency depends linearly on the number of document postings read from the inverted lists; thus, we have a nice tradeoff between efficiency and retrieval quality, which provides substantial savings in terms of retrieval time in combination with small performance degradation.

For the continue algorithm, we implemented two versions: In the continue/document vari-ant, skipping was performed at the document level only; for a document to be considered, all occurrence entries were taken into account; thus weights in accumulators could also be affected through propagated weights. In contrast, the continue/inode variant only considered occurrence entries that contained indexing weights for nodes for which there was an accu-mulator, and weight propagation to other nodes was suppressed.

The performance figures for both variants show that the continue strategy is much better than the quit method. Among the two continue variants, retrieval quality of continue/inode is not reduced for up to 92% of postings ignored, with even substantial gains when only 60 X  80% of the query terms are considered in the first phase of the algorithm. difference in terms of retrieval quality between the two variants, we must keep in mind that continue/document is the correct one, whereas continue/inode does not perform propagation to even better retrieval results.

Since we are only considering the number of postings to be skipped, we cannot guarantee documents described in Moffat and Zobel (1996) has been tested for a collection where the number of documents was two orders of magnitudes higher, but with unstructured documents.
Thus, only experimentation with an appropriate implementation can solve this issue. 5.5. Nosferatu  X 
As described in Section 4, the Nosferatu* algorithm can be applied for different inverted file organizations. In our experiments, we tested the following variants: inode/aweights: Inverted list entries contain augmentation weights (full augmentation) and are ordered by decreasing values of these weights (no document-wise grouping of occur-rence entries). document/iweights: Inverted list entries contain indexing weights (without augmentation), and entries are grouped document-wise. The document entries are sorted by maximum index node indexing weight. document/aweights: Like before, but instead of indexing weights, augmentation weights are stored and used as sorting criterion.

The inode/aweights variant is a kind of best case for the Nosferatu* algorithm, but has high could e.g. be based on partial augmentation).

For comparison, we considered the standard algorithm, which gives us the upper bound of the performance, but the fixed-block mode requires that the user cannot view any document before retrieval is completed. As baseline , we regard an incremental-interruptible version of the standard algorithm: Here query terms are processed in the order of decreasing weights tf
As Nosferatu  X  is an incremental-interruptible algorithm, we need an appropriate metrics for evaluating this type of approach. Since there is no standard measure for effectiveness, we assume the following setting here: after submitting the query, the user issues interrupts assume that s/he waits for some time before starting the first interrupt, but our results show second, which is approximately the time a fast reader would need for skimming the title of a retrieved document.

As evaluation measure, we consider precision after n documents retrieved this way; in addition, we also consider average precision (for 100 recall points) as a measure of overall performance.

The retrieval results for the different methods are listed in Table 5. The most surpris-ing result is the poor performance of the inode/aweights method, which is even below the baseline run. In contrast, the two Nosferatu  X  variants using document-wise grouping (docu-ment/iweights and document/aweight) perform quite well. Here we also performed statistical tests in order to find out which of these differences were significant: With a Wilcoxon signed rank test, the document/  X  methods are significantly better (at the 95% level) than both the baseline and the inode/aweights method.

One reason for the poor result of the inode/aweight variant is the fact that this approach partly ignores the document context: Since our queries contain 13.7 terms on average, we should retrieve index nodes that contain several of these terms. Without document-wise group-when using document-wise grouping, we get co-occurrences even when the high weights oc-cur in different index nodes of the same document. This hypothesis is confirmed by the statis-tics about the average number of term hits per retrieved index node shown in Table 6. Using a Wilcoxon signed rank test, in 7 of the 8 cases the number of term hits for the document/ methods is significantly higher (at the 95 % level) than for the inode/aweights method.
When comparing the two variants with document-wise grouping with the standard method, the performance loss is about 5% for the first 5 X 20 documents. The difference is significant only for document/iweight at 10 documents and document/aweight at 10 and 15 documents.
Given an average response time of about 30 seconds for the standard method in our current the top 45 documents with Nosferatu  X  at the moment the standard method comes up with the Nosferatu  X  is clearly more effective.

The two document-oriented methods have similar performance: document/iweights per-forms better in the beginning (significant difference at 5, 10 and 20 documents), whereas document/aweights seems to be superior when more time is available.

Overall, we can conclude that the Nosferatu  X  algorithm in combination with document-algorithm. With the interrupt rate tested, it yields instant responses when a query is submit-ted, thus supporting highly interactive retrieval. The efficiency gains of Nosferatu retrieval effectiveness.

Finally, we would like to comment on the absolute retrieval times given above, which are relatively high in comparison to those of other retrieval methods. However, besides the higher complexity of the task studied here, major parts of our algorithms were implemented in Perl, in order to allow for faster prototyping; we estimate that a full implementation in C or C ++ would speed up the retrieval process by a factor of about 2. On the other hand, our test collections has only a moderate size, so the speedup would be compensated if the size of the collection would be increased e.g. by an order of magnitude (assuming that retrieval time grows logarithmically with collection size, as in databases). 6. Conclusion
In this paper, we have presented effective methods for specificity-oriented search in XML retrieval. For our augmentation-based approach, the experimental evaluation of two variants gave us a very good retrieval quality. Currently, we are developing methods for learning node-specific propagation weights, for which potential propagation is more suitable.
Instead of regarding only retrieval quality, we have emphasized an effectiveness-oriented scheme for approaches in this area, and then investigated two types of algorithms in more successfully for XML retrieval, giving even better results than for unstructured documents.
As a new class of approaches, we have proposed incremental-interruptible methods. As an instance of these methods, we have developed the Nosferatu So this algorithm yields very high effectiveness.
 We think that algorithms of the latter kind are very important for interactive retrieval.
Whereas usability-oriented evaluation approaches are necessary for judging about the overall quality (see e.g. Beaulieu and Robertson (1996)), system-oriented approaches as considered here are necessary for tuning of components.
 methods are still at their infancy. With the major concepts presented in this paper  X  augmen-tation, effectiveness vs. quality, taxonomy of algorithms  X  we have addressed major issues for further research in this area.
 References
