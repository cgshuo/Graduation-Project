
Toyota Central R&amp;D Labs. Inc., Yokomichi, Nagakute, Aichi, Japan Kyoto University, Yoshida-Honmachi, Sakyo-ku, Kyoto, Japan 1. Introduction the other class data from a given training data set. The problem is a type of unsupervised learning complexity of learning the proposed classifier is approximately linear with regard to the number of training data.

In the proposed method, each datum in a training data set is converted into a logical formula after The proposed method has a threshold parameter, which is the magic parameter of the proposed method, that adjusts the degree of the over-approximation. We propose a method to tune the parameter of the region is considered to be a model that encodes the training data set.

The proposed method is evaluated with a synthetic data set and some realistic data sets. The experi-constructed parameter-freely by the proposed method. The detection accuracy achieved by the proposed vector machine.

This paper is organized as follows: Section 2 defines the problem considered in this paper and intu-of the proposed method based on the minimum description length principle. In Section 5, the proposed method is compared with other works. Section 6 shows some experimental results, and Section 7 sum-marizes this work and discusses future work. 2. Intuitive explanation of the proposed method their efficient implementation are presented in Section 3. 2.1. Problem setting target class. However, the collected data may contain some attacking connections as noise data. The one-class classification problem is defined as follows: construct a one-class classifier occ such that
For example, a classifier that distinguishes attacking connections from normal connections is learnt from the training data set mentioned above for a network intrusion detection problem. 2.2. Projection and hashing of data
Before explaining the proposed method, we define some notations. Let H be a u -dimensional hyper-cube whose side length is 2 m ,where m is an arbitrary positive integer.
 x i ) ( i =1 ,...,N ) in a training data set D .Forall x  X  R u ,  X  ( x ) is denoted by z .
An example of  X  is given as follows: where is an arbitrary small positive number and which are calculated from x ( i ) | i =1 ,...,N .
 Definition 3 .The neighborhood function  X  is a function that returns a u -dimensional unit hypercube that subsumes z  X  R u , which is defined as follows: where  X  is the floor function.
  X  (  X  )=1 . Algorithm 1 Construction of the initial region vector G from a training data set D Input: A training data set D and an example normalizer  X  .
 Output: The initial region vector G . 1: for i =1 to M do 3: for j =1 to N do 6: return G = { G 1 ,...,G M } 2.3. Constructing th e initial reg ion vector G is returned as a set of G i ( i =1 ,...,M ) .

Given the initial region vector G , a one-class classifier occ G can be constructed as follows: However, occ G works rarely if we use the initial region vector G obtained by Algorithm 1 directly, too large. In our approach, we set m large enough, and over-approximate G obtained by Algorithm 1 according to its hierarchical local density, which is mentioned in the next section. 2.4. Over-approximating the initial region vector of G is defined as follows: G = { G 1 ,...,G M } if the following condition is satisfied:
We propose Algorithm 2 that over-approximates G based on a threshold  X  whose dimensionality is C k is focused recursively until C k becomes a unit hypercube or the volume ratio of G i  X  X  k toward C returned by Algorithm 2. Figure 2 shows an example of the initial region vector G ,where u =2 , m =3 and M =1 , and its over-approximated results obtained by Algorithm 2 with various  X  .InFig.2,the smaller the threshold  X  becomes, the larger the approximated region becomes monotonically, as can be
Although the above mentioned algorithms seem computationally complex, it is possible to implement them efficiently by using binary decision diagram-based techniques, which is mentioned in the next section. Furthermore, we propose a method for tuning parameter  X  based on the minimum description length principle, which is mentioned in Section 4. Algorithm 2 Over-approximation of the initial region vector G with a threshold  X  Input: The initial region vector G and a threshold  X  .
 Output: The over-approximated region vector G  X  . 1: for i =1 to M do 3: return G  X  = G  X  1 ,...,G  X  M 4: procedure O VER A PPROX ( G i , C ,  X  ) 5: if C is a unit hypercube then 6: return G  X  i = G i 7: if D ENSITY ( G i , C )  X  then 9: return G  X  i 11: for k =1 to 2 u do 13: return G  X  i 14: procedure D ENSITY ( G i , C ) 15: return 3. Implementation based on binary dicision diagrams by using binary decision diagram-based techniques. 3.1. Binary decision diagram
A Binary decision diagram (BDD) is a compressed representation of a Boolean function. A reduced function and variable order. Hereafter, we refer to an ROBDD as a BDD, for short. A BDD also have an important feature that most logical operations between Boolean functions that are expressed as BDDs can be performed by polynomial-time algorithms [4]. For illustrative purposes, we consider Boolean variable nodes and function nodes, respectively. Solid arrows, dashed arrows and double-lined arrows the path that goes from the root node to the terminal 1 through Node A, Node B and Node D in Fig. 3
In some implementation that are used to handle BDDs, such as CUDD [17], complement edges are used to express BDDs more efficiently. Although the BDD-based algorithm mentioned in the following sections cannot directly be applied when complement edges are used in BDDs, it is possible to adapt them to BDDs with complement edges by modifying the algorithm slightly. For the sake of simplicity, we explain our algorithms without complement edges in the following sections. 3.2. Constructing the i nitial boolean function
In order to use a Boolean function for representing domains, we introduce a special function which codes every continuous data into a logical formula.
 to an integer, and then, coded in the manner of an unsigned-integer-type coding by using m Boolean method, data z that is normalized by an example normalizer  X  , which is mentioned in Section 2.2, is coded by C ODE Z. By using Boolean formulas, we can also treat categorical data. This means that we can combine a coding function for categorical data with C ODE Z.
 if the given data have no categorical attribute.
 { { d We propose Algorithm 3 that constructs the initial Boolean function F from a training data set D . In Algorithm 3, F is defined upon L + mu Boolean variables. BDDs are used to represent Boolean Algorithm 3 Construction of the initial Boolean function F from a training data set D Input: A training data set D and an example normalizer  X  .
 Output: The initial Boolean function F . 2: for i =1 to N do 5: return F the following condition: where the variables inside the square brackets can be in arbitrary order.
 Theorem 1 . The initial Boolean function F that is constructed from a training data set D through through Algorithm 1.
 variable appears once. Also, F can be derived from G in an opposite manner.
 common pattern in G is expressed as a node in a BDD. For example, the minterms of F defined by Eq. (3) are tively, the minterms mentioned above correspond to [0 , 2]  X  [2 , 4] and [2 , 4]  X  [0 , 2] , which is expressed as Node E in Fig. 3.
Given the initial Boolean function F , a one-class classifier occ F can be constructed as follows: which corresponds to a classifier defined by Eq. (2).
 3.3. The relationship between over-approximations The over-approximation of the initial Boolean function is defined as follows: following condition is satisfied: equivalent of G .
 We prove that  X  G over-approximates G .Let ( y,z ) be data that satisfy the following: Since F is the informational equivalent of G , F includes the following minterm: from  X  F , the following satisfies: From the fact that z  X   X  ( z ) and Eq. (8), it follows From Eqs (6) and (9), it is proved that  X  G over-approximates G .
 is a union of unit hypercubes. From Theorem 2, we can over-approximate the initial region vector G algorithm to over-approximate the initial Boolean function. 3.4. Calculation of the density and the level of a BDD node Let F be the initial Boolean function obtained by Algorithm 3 and T be a BDD representation of F . Each node in T corresponds to a Boolean function and the number of minterms of each node can be calculated recursively by the following equation [18]: where  X  X is the number of minterms of node X ,and T and E are the destination nodes of the true and Definition 8 . The density of node N , denoted by  X  N ,isdefinedas Definition 9 . The level of node N , denoted by  X  N ,isdefinedas where w N is a Boolean variable to which node N corresponds.
 nodes in Fig. 3 represents where bold squares mean common patterns caused by don X  X -care variables. the path that corresponds to b 11  X  b 21 in Fig. 3.
 Algorithm 4 Over-approximation of the initial Boolean function F with a threshold  X  Input: ABDD T that represents the initial Boolean function F and a threshold  X  . Output: The BDD T  X  that represents the over-approximated Boolean Function F  X  . 3: B DD O VER A PPROX (the root edge of T  X  ,0,  X  ) 4: return T  X  5: procedure B DD O VER A PPROX ( e , l ,  X  ) 6: N X  the destination node of e 7: if N is a terminal node then 8: return 9: if  X  N  X  and  X  N &gt;l then 10: Modify the destination of e from N to the terminal 1. 11: return 12: else 13: B DD O VER A PPROX ( the true edge of N , X  N , X  ) 14: B DD O VER A PPROX ( the false edge of N , X  N , X  ) 15: return 3.5. The BDD based over-approximation
We propose Algorithm 4 that over-approximates the initial Boolean function F through the direct manipulation of its BDD representation T based on the densities and the levels of BDD nodes, which T  X  , which is a BDD that represents an over-approximated Boolean formula F  X  . Algorithm 4 is a BDD-Fig. 5(b) is equal to  X  , the hypercube is added to the initial region.

A one-class classifier can be constructed by replacing F in Eq. (5) with the over-approximated func-tion obtained by Algorithm 4. 3.6. Computational complexity
The computational complexity of constructing the initial Boolean function by Algorithm 3 is ap-proximately O ( MN ) ,where M is the maximum size of the created BDDs, because logical operations because the approximation is directly manipulated on a compressed form of a BDD. Consequently, the proposed method can deal with a large training data set practically, unless the created BDD become intractably huge. 4. Parameter tuning with the MDL principle
The proposed method described in the previous sections has parameter  X  and it is necessary to tune  X  can be viewed as a probabilistic model. Cross-validation-based methods have another drawback that parameter of the proposed method based on the minimum description length (MDL) principle [15]. 4.1. MDL for the proposed method
In the MDL principle, parameters are tuned so as to minimize the MDL that is the sum of the code model. The MDL is defined as is one-sided.
 Let T be the BDD that represents the initial Boolean function F obtained by Algorithm 3, and let T  X  be the BDD that represents the over-approximated Boolean function F  X  obtained by Algorithm 4. hypercube H uniquely. Considering T  X  as a model that encodes the training data, the parameters in Eq. (13) are calculated as follows: T  X  .Thevolumes V and V  X  are calculated as the number of minterms of F and F  X  , respectively. The by Eq. (14) because we have  X  training data that are allocated to unit hypercubes. The number of the each node to store a BDD.
 If we want to evaluate MDL with various  X  , it is enough to execute Algorithm 3 and procedure D
ENS A ND L EVS in Algorithm 4 just once, because these procedures do not depend on  X  . Then, we ex-ecute procedure B DD O VER A PPROX in Algorithm 4 repeatedly with various  X  and evaluate the MDL of mentioned in Section 2. 5. Related works 5.1. One-class classifiers
Many methods have been proposed in relation to outlier detection, anomaly detection, novelty de-sification problems, with or without modifications. The features of some of the existing methods are listed in Table 2, comparing with the proposed method. In Table 2, the column model settings contain performance and for which there exists a common choice, for example, the Gaussian kernel for kernel-based methods, and the Euclidean distance for distance-based methods. The column magic parameters In the proposed method, the magic parameter is determined based on the MDL principle, and it does Another motivation for our research comes from the fact that some methods in Table 2 become compu-or more, where N is the number of data in the training data set. In comparison, the proposed method mentioned in Section 3.6. 5.2. BDD-based learning methods
Few applications use BDDs as their basis in the fields of data mining and machine learning. For example, Minato and Arimura [12] applied the zero-suppressed BDD (ZDD), which is a modification of the past.
 5.3. Over-approximating methods
Some algorithms that approximate a Boolean function have been proposed in the literature. Ravi et manipulating edges so as to minimize the ratio of minterms toward the number of nodes. Although we 6. Experimental results
In this section, we present experimental results to demonstrate the proposed method. First, we show Shuttle data set and KDD Cup 99 data set from the UCI machine learning repository [7]. We imple-mented the proposed method as a C program with the help of CUDD [17]. Although the algorithms to apply complement edges, because complement edges are used in CUDD. Hereafter, we refer to the proposed classifier as OCBDD. We use  X  in Eq. (1) as an example normalizer in the experiment. For comparison, real problems were also solved by the one-class support vector machine (OCSVM) [16], where e1071 [6] was used to solve the OCSVM. The Gaussian kernel is used in the OCSVM. The exper-iment was performed on a 32 bit Microsoft Windows XP machine with an Intel Core i7 CPU (2.80 GHz, 3.5 GB RAM). 6.1. Synthetic data set 6.1.1. Data overview named MoonStar. The data that are inside the moon-shaped region are considered to belong to the moon set consists of N = 10000 data: 95% of which are randomly sampled from the moon class, and the remaining 5% of which are randomly sampled from the star class. 6.1.2. Results
We first set m =16 , which means that sixteen Boolean variables are used to code each contin-uous attribute. The in itial Boolean function F is constructed from the MoonStar data set through even though the training data set contains 5% of the star data, which means that the proposed method is robust toward the noise in a training data set. Table 3 shows the number of variable nodes of the BDD, the size of the region, and the MDL of each over-approximated result in Fig. 6. From Table 3, F by Algorithm 3 was about 0.08 seconds, and it took about 0.03 seconds to over-approximate F by Algorithm 4 with  X  =10  X  4 . 9 .

The MDL optimal over-approximations are also calculated for m = 8, 10, 32 respectively to assess this experiment m 10 . 6.2. Shuttle data set 6.2.1. Data overview set by checking if they can distinguish the class 1 data from the other class data. 6.2.2. Results of the proposed method
We set m =16 . The initial Boolean function is constructed from the training data set, and then, the set. 6.2.3. Comparison with other methods MDL principle, is compared with that of the OCSVM. Although the OCSVM has two magic param-the other hand, the result of the proposed method is comparable to the best result of the OCSVM. The showing the efficiency of the proposed method.
 named Cudd_RemapOverApprox . In the experiment, Cudd_RemapOverApprox is used instead of B DD O VER A PPROX in Algorithm 4. Cudd_RemapOverApprox has a parameter q that controls the al. [14] is not suitable for the purpose of the one-class classification.
 6.3. KDD cup 99 data set 6.3.1. Data overview
The KDD Cup 99 data set is related to a network intrusion problem. It consists of seven categorical five million data and 310 thousand data, respectively. In reference to Yamanishi [21], we employ the around 0, each of them are transformed by log( x +0 . 1) . 6.3.2. Results of the proposed method
We set m =16 . The initial Boolean function is constructed from the training data set by Algorithm 3 shows the MDL, the TPR and the FPR of each over-approximated result. From Fig. 9(a), the MDL is Cup99dataset. 6.3.3. Comparison with the OCSVM
The classification result of the proposed method with the tuned parameter  X  =10  X  4 . 6 is compared with that of the OCSVM. The OCSVM are evaluated with  X  =0 . 1 , 1 , 10 ,where  X  is fixed to 0.01, method achieves the lowest FPR, and the TPR of the proposed method is high comparable to the results parameters for the OCSVM in advance. The computational time are listed in Table 8. It took only less than 35 seconds to evaluate the MDL of the 150 candidates of parameter  X  in the proposed method as increases rapidly as the training data set becomes large, as seen in Table 8. 6.4. Memory usage of the proposed method
Table 9 shows the memory usage peak of the proposed method for each data set mentioned above. The Table 9. The memory usage peaked while constructing the initial Boolean function by Algorithm 3 for every data set. From Table 9, the memory usage peak seems to be larger as the number of continuous a BDD is a kind of compressed form of data and can treat the overlapped data very efficiently. 7. Conclusions
In this work, we proposed a novel approach for the one-class classification problem. The region of diagram become intractably huge.

This work can be extended in many directions. We have considered only an over-approximation al-gorithm in this paper, which is an approximation in which the approximated region always becomes class classification problems by developing an evaluation scheme that is suitable for these problems based on the minimum description length principle.
 Some of the data set used in the experiments can be considered to face an imbalanced situation. other imbalanced situations.

Although the proposed method is scalable with regard to the number of training data, it may be-come intractable when the number of attributes is very large, because the size of the created binary decision diagram tends to be huge in such a case. Moreover, another serious issue, called the curse every subregion of the whole hypercube. One simple solution toward these problems is to embed some dimension-reduction techniques, such as principal component analysis, into an example normalizer  X  . Another possible solution is to select attributes based on the minimum description length principle. References
