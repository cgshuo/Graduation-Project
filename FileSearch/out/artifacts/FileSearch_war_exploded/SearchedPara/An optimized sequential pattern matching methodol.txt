 Themis P. Exarchos  X  Markos G. Tsipouras  X  Costas Papaloukas  X  Dimitrios I. Fotiadis Abstract In this paper we present a novel methodology for sequence classification, based on sequential pattern mining and optimization algorithms. The proposed methodology auto-matically generates a sequence classification model, based on a two stage process. In the first stage, a sequential pattern mining algorithm is applied to a set of sequences and the sequential patterns are extracted. Then, the score of every pattern with respect to each sequence is cal-culated using a scoring function and the score of each class under consideration is estimated by summing the specific pattern scores. Each score is updated, multiplied by a weight and the output of the first stage is the classification confusion matrix of the sequences. In the second stage an optimization technique, aims to finding a set of weights which minimize an objective function, defined using the classification confusion matrix. The set of the extracted sequential patterns and the optimal weights of the classes comprise the sequence classification model. Extensive evaluation of the methodology was carried out in the protein classification domain, by varying the number of training and test sequences, the number of patterns and the number of classes. The methodology is compared with other similar sequence classifica-tion approaches. The proposed methodology exhibits several advantages, such as automated weight assignment to classes using optimization techniques and knowledge discovery in the domain of application.
 Keywords Sequential pattern mining  X  Sequential pattern matching  X  Sequence classification  X  Optimization 1 Introduction Sequential data are sequences of ordered  X  X vents X , representing a situation, where each event might be described by a set of predicates. Examples of sequential data include text, bio-sequences (DNA, proteins), web-usage data, multiplayer games, and plan-execution traces. Classification is the procedure in which given a collection of training records, each one con-taining a set of attributes, with one of them being the class, to find a model for the class attribute as a function of the values of other attributes. The result of the classification is that new records are assigned to a class as accurately as possible.

Sequence classification is an important problem that arises in many real-world applica-tions, such as protein function prediction, text classification, speech recognition, intrusion detection, etc. [ 1 ]. Given a sequence (constructed from letters drawn from a finite alphabet; i.e. 20-letter alphabet of amino acids in the case of protein classification; a vocabulary of English words in text classification), a sequence classifier assigns a class label (typically drawn from a finite set of mutually exclusive class labels) to the sequence. Data mining and machine learning algorithms offer an effective approach to design sequence classifiers, when a training set of labeled sequences is available [ 2 ].

The problem of sequence classification has been addressed in the past; however, it has not received too much attention. The earliest approaches employed hidden Markov models [ 3 ], finite automata and entropy based approaches [ 4 ]. The most recent techniques treat the prob-lem of sequence classification as a feature mining problem [ 5  X  7 ], i.e. they mine sequential patterns from a set of training sequences and then use these patterns for classification. The FeatureMine algorithm uses these patterns as features; in this way the sequences are vec-torized based on the matched sequential patterns and then standard classification algorithms such as na X ve Bayes and winnow are applied to the vectorized sequences [ 5 , 6 ]. The Clas-sify by Sequences (CBS) algorithm mines sequential patterns from the sequences and then assigns a score to each sequence for each class, using a scoring function [ 7 ], which is based on the length of the matched sequential patterns. Tseng et al. [ 7 ] presents two approaches, the CBS_ALL and the CBS_CLASS. Experimental results show that CBS_CLASS outperforms both CBS_ALL and FeatureMine [ 7 ].

Recently, many data mining techniques like association rules, sequential patterns, cluster-ing and classification, emerged in various research topics [ 8  X  12 ]. Most of the existing data mining methods are designed for solving some specific problem independently. On the other hand, some few compound methods integrate two or more types of data mining techniques to solve complex problems. These compound methods can effectively utilize the advantages of each individual mining technique to improve the overall performance in the data mining tasks. For example, the CBA [ 13 ] method delivers higher accuracy than traditional classi-fication methods such as C4.5 [ 11 ]. Hence, it is a promising direction to integrate different kinds of data mining methods to form a new methodology for solving complex data mining problems.

In this work we propose a novel methodology for sequence classification that is able to work in many sequential domains. The methodology can be considered as a compound data mining method that uses sequential pattern mining for sequence classification. The input to our methodology is a set of labeled training sequences, and the output is a function mapping from a new, unknown sequence to a class. The classification of an unknown sequence is realized automatically. The methodology employs a sequential pattern mining algorithm, a scoring function that uses the sequential patterns for classification and an optimization tech-nique, in order to automatically assign weights to the classes. The proposed methodology extends a previously reported sequence classification algorithm [ 7 ] by introducing a set of weights and obtaining optimal values for them using optimization algorithms.

The proposed methodology consists of two stages. In the first stage a sequence clas-sification model based on sequential patterns is created. This first stage is similar to the CBS_CLASS algorithm, which also builds a sequence classification model from the extracted sequential patterns. The innovation of the proposed methodology is the introduction of weights, which are applied to sets of sequential patterns, and their tuning through opti-mization, during the second stage, which is proposed for the first time in the literature. The methodology, through the optimization stage, assigns optimal values for these weights to improve the sequence classification performance. The introduction and optimization of the weights is motivated by the fact that the sequential patterns, extracted from the sequences, do not describe all classes with the same adequacy; some classes are over described from the sequential patterns while for some others this description is rather poor. The weights are introduced to balance this trend, and subsequently, an optimization technique is used to automatically calculate optimal values for them. In addition, the weights integrate the avail-able information for each class, since the description of classes with more information (i.e. having a large number of training sequences) is more reliable. The results indicate that the employment of the optimal weights highly increases the classification accuracy of the simple sequential pattern based classifier. Furthermore, the methodology provides to the domain experts knowledge for their domain (by means of the extracted sequential patterns and the optimal weights). Finally, the proposed methodology is generic and can incorporate different algorithms/approaches in any of its stages.

The rest of the manuscript is organized as follows. In Sect. 2 , the two stages of the proposed methodology are described in detail. Section 3 presents the experiments from the protein clas-sification domain used to evaluate the methodology, the comparative work and the obtained results, for all experiments. Finally, in the discussion section, qualitative conclusions are derived and quantitative comments concerning the obtained results are addressed. 2 Methods The proposed methodology includes two stages (Fig. 1 ). In the first stage, a sequence clas-sification methodology is defined. For the realization of the first stage, a dataset D = {
S , c i } , i = 1 ,..., l S ,where S i is a sequence and c i is its class, with l c different clas-ses ( c i = { 1 ,..., l c } ) and l S is the number of sequences in the dataset ( | D | = l S ) ,anda vector of class weights w( | w | = l c ) are required. This stage is realized in four steps and its outcome is the classification confusion matrix. The second stage is an optimization tech-nique which is based on an objective function, defined by the classification confusion matrix, aiming to find a set of weights, w , which minimize the objective function. 2.1 Stage 1: Sequence classification Figure 2 presents the pseudo code for the realization of the four steps of the first stage. The list of symbols used and their explanation is presented in Table 1 .
 Step 1 Sequential pattern mining
The training sequences are divided into l c subsets, each one containing all sequences belonging to the same class S j , j = 1 ,..., l c . Then, sequential pattern mining ( SPM ) [ 9 ] is applied to each subset, generating l c sets of sequential patterns P j , satisfying the user-defined constraints. The above is followed by the CBS_CLASS [ 7 ] algorithm reported to outperform CBS_ALL, who mines the whole database of sequences for sequential pat-terns. This step closely resembles the feature mining problem [ 5 , 6 ]. For this reason, even if SPM is an unsupervised technique, we employ it in a supervised manner, since we generate sequential patterns for each class separately. The output of this stage is the sets of sequential patterns P j , j = 1 ,..., l c , characterizing the l c classes.
In the SPM procedure, we can incorporate several constraints, that allow for flexible gap of the extracted sequential patterns. Several algorithms have been reported in the literature which implement the SPM procedure [ 9 , 14  X  16 ]. However, little work has been done on con-strained SPM [ 17  X  20 ]. An algorithm that performs efficient and effective constrained SPM is the cSPADE algorithm [ 17 ]. The cSPADE algorithm is based on the SPADE algorithm [ 16 ] which uses efficient lattice search techniques and simple join operations on id-lists. As the length of a frequent sequence increases, the size of its id-list decreases, resulting in very fast joins. All sequences are discovered with only three database scans, one for frequent 1-sequences, another for frequent 2 sequences, and one more for generating all frequent k -sequences. The performance of the cSPADE algorithm has been proven superior, com-pared to other constrained SPM approaches [ 18 , 19 ].
 Step 2 Pattern score matrix calculation
After the extraction of the sequential patterns, for each class we create a pattern score matrix PSM j , j = 1 ,..., l c (a PSM matrix is created for each class). Each PSM j includes a score that defines the implication of a pattern that belongs to class j with all S i sequences; thus, its size is l j  X  l S . This implication is defined through a scoring function: if the P j m pattern i.e. the m th pattern of the j th class is contained in the S i sequence then the ( m , i ) element of the PSM j matrix is equal to the value length ( P j m )  X  1 divided by the number of patterns describing the j th class. If the P j m pattern is not contained in the S i sequence then PSM j ( m , i ) is equal to 0.

We subtract 1 from the length of the pattern, in order to assign the minimum score, which is 1, to the minimal pattern, whose length is 2. The length of the pattern in the numerator makes the longer sequential patterns more significant than shorter ones. Also, the score of a sequence with respect to a class is divided by the number of sequential patterns extracted from this class set. Thus, the smaller the number of patterns describing a class, the more significant is each one of these patterns. The above scoring function assigns higher scores to the longer sequential patterns, by adding their score along with the score of all their subse-quences. This has been taken into consideration since, longer sequential patterns are much more important than shorter ones.
 Step 3 Class score matrix calculation and update
From the PSM j matrices, we derive the class score matrix ( CSM ) . Each ( j , i ) element of this matrix is the score of the i th sequence for the j th class, and it is defined as the sum of the scores of all patterns belonging to the j th class for the i th sequence. This is shown schematically in Fig. 3 . Since the score of a specific pattern for a sequence is 0 if this sequence does not contain the pattern, only the patterns included in a sequence contribute to the class score; thus, the size of the CSM matrix is l c  X  l S . Then, each row of the matrix CSM is multiplied by a parameter, which denotes the weight of a specific class; thus the matrix CSM is updated as follows: row j = w ( j )  X  row j , where row j is the j throwofthe CSM matrix and w ( j ) is the j th element of the class weight vector.
 Step 4 Confusion matrix calculation
For each sequence S i , a class is predicted ( pc i ) , based on the updated CSM matrix. This predicted class is defined as the class that obtains the highest score for the i th sequence: pc i = confusion matrix ( CM ) for all the sequences is calculated, as it is shown in step 4 of the pseudocode (Fig. 2 ). 2.2 Stage 2: Optimization In the second stage, we try to calculate a set of weights w that derive the highest classification accuracy of the sequences in the dataset. Initially, a cost function is defined, based on the CM : Equation ( 1 ) can be formulated as an optimization problem, minimizing f ( D ,w ) with respect to w . Different values for the weights have an impact on the cost function, since they affect CM . This cost function has been selected since its minimum value is 0, and this value is obtained if (with the appropriate w ) all sequences are correctly classified ( l S = trace ( CM ) ) . Local optimization strategy is preferred, since an initial point is available (w = 1 ) and it is significantly faster than global optimization. In addition, it is very difficult to calculate analytically the derivatives of the objective function. Based on the above, we employed the Nelder-Mead simplex search method [ 21 ] to solve the optimization problem. This is a direct search method for multidimensional unconstrained minimization which does not use numerical or analytic gradients.

The Nelder-Mead simplex search method initially defines a simplex  X  R n in the X  n dimensional space, which is characterized by the n + 1 distinct vectors which are its vertices. At each step of the search, a new point in or near the current simplex is generated. The function value at the new point is compared with the function X  X  values at the vertices of the simplex and, usually, one of the vertices is replaced by the new point, giving a new simplex. This step is repeated until the diameter of the simplex is less than a specified tolerance. The result of the optimization procedure is a set of optimal class weights w  X  . 3Results The proposed methodology has been evaluated using an appropriate sequence dataset. Results of the proposed methodology are presented for both cases with and without the use of the optimization stage ( w = 1 ) [ 22 , 23 ]. Our methodology is also compared with a previously reported sequence classification method, the CBS algorithm [ 7 ]. 3.1 Dataset The sequence classification domain that was selected is the classification of protein primary structures into folds and classes. The formulation of SPM covers almost any categorical sequential domain [ 24 ]. In order to apply SPM to a specific domain, the following notions are required: a database of sequences D , a set of items (alphabet) I , a definition of the trans-action id ( tid ) and a definition of an itemset. In what concerns our problem, the database D consists of protein primary structures and each one of them has a sequence id .Thesetof items I is the 20 amino acids that compose the protein primary structures plus one for the unknown amino acid. An itemset in a transaction consists of a single item (one of the 21 letters) while the tid is the position of the amino acid in the protein primary structure, rather than the time.

A group of primary protein sequences were taken from the Protein Data Bank (PDB) [ 25 ]. All members of this group correspond to a specific fold of the Structural Classification of Proteins (SCOP) database [ 26 ]. Specifically the 17 SCOP folds, with at least 30 mem-bers, from classes A and B were used to derive the training and test data. From the resulted 1,000 proteins, the two thirds from each category were used for training, while the rest for evaluation. 3.2 Evaluation From the above sequences, four datasets were derived and four different classification exper-iments were performed. Each dataset was divided into training and test sets.  X  In the first experiment (Exp. 1) we used sequences from 17 categories (class A and class  X  In the second experiment (Exp. 2) we use sequences from 10 categories (class B folds).  X  In the third experiment (Exp. 3) we use sequences from 7 categories (class A folds). The  X  In the fourth experiment (Exp. 4) we use sequences from 2 categories (All sequences of
In all experiments, we set the minimum support to 50%, meaning that a pattern should above experiments, we varied the number of max_gap from 1 to 5, since values greater than 5 made the number of the extracted sequential patterns prohibitively large. Thus we created 20 experiments; in all experiments, the training set was used for sequential pattern mining and for calculating w  X  . In the testing phase, the sequential patterns and w  X  are used to classify the sequences of the test set. 3.3 Comparative work The CBS algorithm has also been tested using the same experimental procedure. We have implemented the CBS_CLASS variation of the CBS algorithm. In the CBS_CLASS algorithm the training set is divided into subsets of sequences belonging to the same category. These subsets are mined using sequential pattern mining and a set of sequential patterns is derived for each class. Then, the score of every sequential pattern for each sequence is calcu-the PSM j matrix is equal to the value length ( P j m ) l j m = 1 length ( P j m ) ,elseitisequalto0. After the creation of the PSM matrix, the CBS_CLASS algorithm employs the same steps with the stage 1 of the proposed methodology, for the creation of the CSM matrix and the classification of a sequence in a predicted class.

In addition, we compared our method with a well known and widely used method for sequence classification, which is based on hidden Markov models, the Sequence Alignment and Modeling method, SAM [ 27 , 28 ]. SAM employs the Baum-Welch algorithm [ 29 ]for training a hidden Markov model and classifies sequences using two approaches: either rank-ing of the scores obtained for each sequence (SAM_1) or ranking of the E-values obtained for each sequence (SAM_2). Currently, we tested both SAM_1 and SAM_2, with the same training and test sets with the proposed methodology.
 3.4 Classification performance Ta b l e 2 presents the obtained results for all experiments and for all the different values of the max_gap . It is mentioned that the accuracy results are presented both for the training and the test sets. In the first experiment, the number of patterns varies from 1,568 to 38,557 for max_gap = 1 X 5. Similarly, in the second experiment the number of patterns varies from 426 to 10,954, in the third experiment this number varies from 1,142 to 27,603 while for the fourth experiment, the number of patterns varies as in the first experiment since the patterns from the 17 classes were used as patterns for the two classes. Figure 4 presents the classification accuracies (for the test set) for all 4 experiments and for all 5 different values of max_gap . Ta b l e 3 presents the accuracy results in the test sequences for SAM_1, SAM_2, CBS, SPM and OSPM. It should be mentioned that in Table 3 , the accuracies of CBS, SPM and OSPM are the maximum ones for each different experiment (Exp. 1, Exp. 2, Exp. 3 and Exp. 4) for the corresponding max_gap value of Table 2 . 4 Discussion In the current work we presented a novel methodology for the automated generation of sequence classification models. Initially, sequential patterns are extracted from a set of (train-ing) sequences. The scores for each sequential pattern and each class are computed. In addi-tion, optimal weights for each class are calculated, using an optimization technique. The obtained optimal class weights along with the extracted sequential patterns compose the sequence classification model, which is used to classify the test sequences.

The proposed methodology introduces several innovative features. To our knowledge, the automatic assignment of weights to sets of sequential patterns using optimization tech-niques, for classification purposes is proposed for the first time in the literature. Other similar approaches use the extracted sequential patterns either as input features [ 5 , 6 ] to standard classification algorithms, or employ a scoring function, similar to the one reported in the cur-rent work [ 7 ]. The weight assignment to the classes and their tuning through optimization, is a major advantage of our methodology, since it adjusts the descriptive ability of the set of patterns for each class, thus leading to high classification accuracy, superior to previous works. Also, the results of the simple sequential pattern based classifier are significantly improved, when the optimal weights are applied.

The methodology can be applied to other domains of application; the application of the methodology is straightforward, since only fundamental information related to the domain of application is needed: the dataset, consisting from the set of sequences, the class of each sequence, and the alphabet that is used to form the sequences. For example, in the domain of document classification where there exists a set of documents, each of them belonging to a predefined class, our methodology can be applied as follows: the alphabet is defined as the total number of (stemmed) words existing in all documents and thus, each document is considered as a sequence of words. Then, based on the class of each document, the set of documents is divided into subsets, where each subset contains documents of the same class. After that, sequential pattern mining (step 1 of the first stage) is applied to each of the subsets of documents (sequences) to generate a set of sequential patterns describing each one of the document classes. Steps 2 X 4 of the first stage are then applied. In these steps, each set of sequential patterns describing a document class is considered of equal importance with all others; the importance of each set is defined through the introduction of the weights (one weight for each class) and initially the values of all weights are set to 1 (i.e. they are considered of equal importance). Finally, Stage 2 is applied to assign optimal values to each weight in order to increase the correctly classified documents. This is performed by defining an appropriate objective (error) function, in our case based on the document classification confusion matrix, and then minimizing its value using an optimization technique/strategy.
Additionally, the methodology is generic since different components can be used for any of its stages, i.e. different SPM algorithm, alternative objective function and/or optimization method (local or global). Also, the scoring function could be modified to integrate different preferences, e.g. in case the sequences are composed by itemsets with multiple items, the scoring function could be modified to use either the length (number of itemsets) or the size (number of items) of the pattern in the numerator (currently, itemsets are composed by single items and thus the length and the size of a pattern are the same) of the scoring function.
The SPM approach, employed in this work, is suitable for analyzing sequences and is able to discover strong sequential dependencies (patterns). In addition, the use of sequential pattern mining leads to pattern discovery in the specific sequential domain of application. Furthermore, the training phase of the method, i.e. the determination of the sequential pat-terns, is a fast procedure due to the use of the cSPADE algorithm. In general, SPM is a time consuming process and requires high computational load which is increased exponentially as longer sequences need to be mined. The lattice search techniques and the simple joins that the cSPADE algorithm employs, handle the two above aspects effectively.

It should be mentioned that the employed scoring function is selected heuristically, obtained after a series of experiments. Its basic design (i.e. provide higher score to sequential patterns of higher length by adding their score along with the score of all their subsequences) was obtained from the CBS algorithm. In addition, we utilized also as scoring function (in the numerator) the times a sequential pattern is contained in the sequence raised in the power of n ( n = 1 , 2 ,...) , the logarithm of the length of the pattern, the length of the pattern raised in the power of n ( n = 1 , 2 ,...) , the support of the pattern and others. All the above, including the scoring function of the CBS algorithm, reported lower classification results when they were used in our sequence database. More specifically (Table 2 ), the accuracy obtained in almost all classification problems (in 19 out of 20 experiments) is improved during training, when the proposed scoring function is employed, instead of the one used in the CBS algo-rithm. This improvement also holds in the testing (in 15 out of 20 experiments). The average accuracy for all experiments is 46.1% for training using the CBS algorithm and 61.3% using SPM (improvement of 15.2%), while the average accuracy in testing is 35.5% for the CBS and 40.5% for SPM (improvement of 5%).

The proposed methodology has been evaluated systematically, using 20 different eval-uation experiments (four datasets multiplied by five different values of the max_gap ). In the design of the classification experiments, special attention was given to create classifi-cation experiments with different properties and classification difficulty; the length of the employed sequences ranges from 36 to 590 letters, using a 21 letter alphabet, while the num-ber of classes, is 17, 10, 7 and 2. Also the number of sequential patterns extends from 426 to 38,557. This large number of different evaluation experiments resulting from the wide range of parameters, ensures the reliable evaluation of the proposed methodology in protein classification domain.

Comparing the computational complexity and the running times of the three algorithms (SPM, OSPM and CBS), SPM and CBS have the same computational complexity and run-ning time both in training and in testing, since for every different value of max_gap, the same number of sequential patterns are both mined in training and matched in testing. OSPM presents higher computational complexity and running time than the other two algorithms in the training, since it employs two stages, the first stage which is common with SPM and CBS and the second stage, which employs an optimization technique. Thus, the additional complexity and running time of the OSPM is due to the optimization stage and depends on the selection of the optimization technique. In the current work, the Nedler-Mead simplex search method is employed, which is a local optimization technique with low computational complexity. This complexity and training time depend on the number of classes, since its value defines the number of parameters for optimization. The running time for testing depends only on the classification difficulty of the experiment (number of patterns, number of classes, number of test sequences) and thus all three algorithms report the same running time in terms of each different experiment. It should be mentioned that the number of extracted sequential patterns, highly depends on the max_gap value, thus as max_gap increases the number of sequential patterns and subsequently the running time for all three algorithms, increase.
In addition, the proposed optimization stage significantly improves the ability of the sequential patterns to classify sequences, by adjusting the relative importance of each class according to the obtained optimal weights. More specifically, in all 20 experiments, both in training and testing, the proposed methodology (mentioned as OSPM in Table 2 )presents higher accuracy. The average accuracy for all experiments is 75.2% in training and.50.8% in testing, improving the accuracy of the CBS in training by 29.1% and SPM (which is the first stage of the OSPM methodology) by 13.9%. The respective improvement in testing is 15.3% and 10.3%, compared to the CBS and the SPM, respectively. It should be noted that the best accuracy for OSPM in Exp. 1,2,3 experiments is achieved for max_gap = 4, while for Exp. 4, the best accuracy is achieved for max_gap =5 (Fig. 4 ). Thus, patterns with up to 3 intervening amino-acids between two consecutive amino-acids constituting a pattern are the most descriptive in the cases of Exp. 1, Exp. 2 and Exp. 3, while patterns with up to 4 intervening amino-acids between two consecutive amino-acids, are more suitable for Exp. 4. This can be attributed to the higher homology between the folds in Exps. 1 X 3, since Exps. 1 X 3 are fold prediction problems. On the other hand, Exp. 4 is a class prediction problem, and between classes, there exists lower homology. Thus, a lower value of max_gap (4) is more appropriate for (higher homology) fold recognition, while for (lower homology) class prediction, a higher value of max_gap (5) reports the best results.

The proposed methodology is compared with the Sequence Alignment and Modeling sys-tem [ 27 , 28 ], which is a well known and widely used method for sequence classification that is based on hidden Markov models (Table 3 ). The comparison shows that the proposed meth-odology outperforms both SAM_1 and SAM_2 in terms of accuracy in the test sequences, for all experiments. The average accuracy in Table 3 is 41.7, 45.5, 40, 47.4 and 55% for SAM1, SAM2, CBS, SPM and OSPM, respectively. It should be mentioned that the methodology and all comparative methods, provide low classification results in terms of  X  X aw accuracy numbers X  in Exp. 1, Exp. 2 and Exp. 3. However, the performance of all methods becomes evident when compared with the respective performance of a classifier that makes random predictions. In this case, the proposed methodology and all comparative methods report an accuracy of more than 50% accuracy in Exp.4, where there are two classes, and the accuracy of a classifier that makes random predictions is 50%. In Exp.1, Exp. 2 and Exp. 3, the num-ber of classes is 17, 10 and 7 respectively, and thus the accuracy of a classifier that makes random predictions is 5.89, 10 and 14.29%, respectively. It is worth mentioning that, the relative literature presents comparable results when the number of the target classes (folds) for prediction is similar [ 30 ].

The proposed methodology is based on sequential pattern mining. A drawback of the methodology is that a large number of patterns is discovered which increases exponen-tially with max_gap . Although, the extraction of sequential patterns is relatively fast (due to the cSPADE algorithm), the overall processing time increases. In addition, SPM , besides discovering valid and causal relationships in the sequential data, will also find spurious and particular relationships among the data in the specific dataset. The above issues could be treated by employing a pattern reduction/selection algorithm; this feature will be addressed in the future. Additionally, the optimization stage, although significantly improves the classi-fication accuracy of the approach without the optimization stage, increases the computational effort and the overall time for the training. For this reason, a local optimization strategy was selected, which however does not ensure the best results. 5 Conclusions A novel sequence classification methodology has been presented along with an extensive evaluation in the domain of protein classification and the obtained results indicate its effec-tiveness. Application of the methodology in other discrete sequential domains will fully reveal its potential. Our work in the future must focus on: (1) using different techniques for sequential pattern mining (i.e. mine sequential patterns with sequence alignment [ 31 , 32 ]), (2) using methods for sequential pattern selection, or the use of specific types of patterns like closed [ 33 ] or maximal [ 34 ] sequential patterns, or minimal distinguishing sequential patterns [ 35 ], (3) employing different scoring function, and/or optimization strategies and (4) extending the methodology in order to handle time series.
 References Author Biographies
