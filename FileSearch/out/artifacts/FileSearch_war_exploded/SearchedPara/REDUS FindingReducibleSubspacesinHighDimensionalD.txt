 Finding laten t patterns in high dimensional data is an im-portan t researc h problem with numerous applications. The most well kno wn approac hes for high dimensional data anal-ysis are feature selection and dimensionalit y reduction. Be-ing widely used in man y applications, these metho ds aim to capture global patterns and are typically performed in the full feature space. In man y emerging applications, however, scien tists are interested in the local laten t patterns held by feature subspaces, whic h may be invisible via any global transformation.

In this pap er, we investigate the problem of nding strong linear and nonlinear correlations hidden in feature subspaces of high dimensional data. We formalize this problem as iden-tifying reducible subspaces in the full dimensional space. In-tuitiv ely, a reducible subspace is a feature subspace whose in-trinsic dimensionalit y is smaller than the num ber of features. We presen t an e ectiv e algorithm, REDUS, for nding the reducible subspaces. Tw o key comp onen ts of our algorithm are nding the overall reducible subspace, and unco vering the individual reducible subspaces from the overall reducible subspace. A broad exp erimen tal evaluation demonstrates the e ectiv eness of our algorithm.
 Categories and Sub ject Descriptors: H.2.8 [Database Applications]: Data Mining General Terms: Algorithm, Performance Keyw ords: Reducible Subspace, High Dimensional Data
Man y real life applications deal with high dimensional data. In bio-medical domain, for example, adv anced mi-croarra y techniques [2, 11, 17] allo w to monitor the expres-sion levels of hundreds to thousands of genes sim ultaneously . By mapping eac h gene to a feature, gene expression data can be treated as data points distributed in a very high dimen-sional feature space. To mak e sense of suc h high dimensional data, extensiv e researc h has been done in nding the laten t structures hidden in the large num ber of features. Tw o well kno wn approac hes in analyzing high dimensional data are feature selection and dimensionalit y reduction.

The goal of feature selection metho ds [6, 22, 31, 33] is to nd a single represen tativ e subset of features that are most relev ant for the data mining task at hand, suc h as classi -cation. The selected features generally have low correlation with eac h other but have strong correlation with the target feature.

Dimensionalit y reduction [4, 8, 18, 27, 29] is widely used as a key comp onen t of man y approac hes in analyzing high dimensional data. The insigh t behind dimensionalit y reduc-tion metho ds is that a high dimensional dataset may exhibit interesting patterns on a lower dimensional subspace due to correlations among the features. Though very successful in nding the low dimensional structures embedded in a high dimensional space, these metho ds are usually performed in the full feature space. They aim to mo del the global la-ten t structure of the data and do not separate the impact of any original features nor iden tify laten t patterns hidden in some feature subspaces. Please refer to Section 2 for a more detailed discussion of the related work.
In man y emerging applications, the datasets usually con-sist of thousands to hundreds of thousands of features. In suc h high dimensional dataset, some feature subsets may be strongly correlated, while others may not have any corre-lation at all. In these applications, it is more desirable to nd the correlations that are hidden in feature subspaces. For example, in gene expression data analysis, a group of genes having strong correlation is of high interests to bi-ologists since it helps to infer unkno wn functions of genes [11] and gives rise to hypotheses regarding the mec hanism of the transcriptional regulatory net work [17]. We refer to suc h correlation among a subset of features as a local cor-relation in comparison with the global correlation found by the full dimensional feature reduction metho ds. Since suc h local correlations only exist in some subspaces of the full dimensional space, they are invisible to the full dimensional reduction metho ds. In [32], an algorithm is prop osed to nd local linear correlations in high dimensional data. However, in real applications, the feature subspace can be either lin-early or nonlinearly correlated. The problem of nding lin-ear and nonlinear correlations in feature subspaces remains open.

For example, Figure 1 sho ws a data sets consisting of 12 features, f f 1 ; f 2 ; ; f 12 g , and 1000 data points. Em bedded Figure 2: Applying dimensionalit y reduction meth-ods to the full dimensional space of the example dataset in the full dimensional space, features subspaces f f 1 ; f and f f 4 ; f 5 ; f 6 g are nonlinearly correlated, f f 7 linearly correlated. Features f f 10 ; f 11 ; f 12 g con tain random noises.

Performing dimensionalit y reduction metho ds to the full dimensional space cannot unco ver these local correlations hidden in the full feature spaces. For example, Figure 2(a) sho ws the result of applying Principal Comp onen t Analy-sis (PCA)[18] to the full dimensional space of the example dataset sho wn in Figure 1. In this gure, we plot the point distribution on the rst 3 principal comp onen ts found by PCA. Clearly , we cannot nd any pattern that is similar to the patterns embedded in the dataset. Similarly , Figure 2(b) sho ws the results of applying ISOMAP [29] to reduce the dimensionalit y of the dataset down to 3. There is also no desired pattern found in this low dimensional structure.
How can we identify these local correlations hidden in the full dimensional space?
This question is two-fold. First, we need to iden tify the strongly correlated feature subspaces, i.e., a subset of fea-tures that are strongly correlated and actually have low di-mensional structures. Then, after these locally correlated feature subsets are found, we can apply the existing dimen-sionalit y reduction metho ds to iden tify the low dimensional structures embedded in them.

Man y metho ds have been prop osed to address the second asp ect of the question, i.e., given a correlated feature space, nding the low dimensional embedding in it. The rst as-pect of the question, however, is largely untouc hed. In this pap er, we investigate the rst asp ect of the question, i.e., iden tifying the strongly correlated feature subspaces. (1) In this pap er, we investigate the problem of nding correlations hidden in the feature subspaces of high dimen-sional data. The correlations can be either linear or nonlin-ear. To our best kno wledge, our work is the rst attempt to nd local linear and nonlinear correlations hidden in feature subspaces.

Man y metho ds for mo delling correlations can be found in the literature, suc h as mutual information [10], Pearson correlation [26], and rank correlation [19]. However, the commonly used measuremen ts are for correlations between two features. In high dimensional data, the correlation may involve a large num ber of features, i.e., a high order corre-lation. Note that a strong high order correlation does not necessarily imply that there are strong correlations between feature pairs. For example, the 3 features, f f 1 ; f 2 ; f Figure 1, are strongly correlated, since the 3-dimensional Swiss roll structure is actually on a 2-dimensional manifold. Figures 3(a) to 3(c) sho w the pro jections of the Swiss roll onto the spaces of two features. As we can see from the gures, there are no clear pairwise correlations between any two features.

We adopt the concept of intrinsic dimensionalit y [13] to mo del the high dimensional correlation. We formalize this problem as nding reducible subsp aces in the full dimen-sional space. Informally , a feature subspace is reducible if its intrinsic dimensionalit y is smaller than the num ber of fea-tures. Various intrinsic dimensionalit y estimators have been dev elop ed [9, 14, 21]. Our problem formalization does not dep end on any particular metho d for estimating the intrin-sic dimensionalit y. We sho w two necessary prop erties that any estimator should satisfy in order to be a generalization of the well-de ned concepts in linear case. (2) We dev elop an e ectiv e algorithm REDUS 1 to nd the reducible subspaces in the dataset. REDUS consists of the follo wing two steps.

It rst nds the union of all reducible subspaces, i.e., the over all reducible subsp ace . The second step is to unco ver the individual reducible subspaces in the overall reducible sub-space. The key comp onen t of this step is to examine if a feature is strongly correlated with a feature subspace. We dev elop a metho d utilizing point distributions to distinguish the features that are strongly correlated with a feature sub-space and those that are not. Our metho d achiev es similar accuracy to that of directly using intrinsic dimensionalit y estimators, but with much less computational cost.
Extensiv e exp erimen ts on syn thetic and real life datasets demonstrate the e ectiv eness of REDUS.
REDUS stands for REDU cible S ubspaces. (a) f 1 and f 2 Figure 3: Pairwise correlations of the Swiss roll in the example dataset
Feature Selection Feature selection metho ds [6, 22, 31, 33] try to nd a subset of features that are most rel-evant for certain data mining task, suc h as classi cation. In order to nd the relev ant feature subset, these meth-ods searc h through various subsets of features and evaluate these subsets according to some criteria. Feature selection metho ds can be further divided into two groups according to their evaluation criteria: wrapp er and lter. Wrapp er mo dels evaluate feature subsets by their predictiv e accuracy using statistical re-sampling or cross-v alidation. In lter techniques, the feature subsets are evaluated by their infor-mation con ten t, using statistical dep endence or information-theoretic measures.

Feature selection nds one feature subset for the entire dataset. The selected feature subset usually con tains the features that have low correlation with eac h other but have strong correlation with the target feature. The correlation measuremen ts are usually de ned for feature pairs, suc h as mutual information and Pearson correlation. Our work, on the other hand, is to nd the subsets of features, in whic h the features are strongly correlated. Moreo ver, the correlations are not limited to feature pairs.

Dimensionalit y reduction Dimensionalit y reduction metho ds can be categorized into linear metho ds, suc h as Multi-Dimensional Scaling (MDS) [8] and Principal Com-ponen t Analysis (PCA)[18], and non-linear metho ds, suc h as Local Linear Em bedding (LLE) [27], ISOMAP [29], and Laplacian eigenmaps [4]. For high dimensional datasets, if there exist low dimensional subspaces or manifolds embed-ded in the full dimensional spaces, these metho ds are suc-cessful in iden tifying these low dimensional embeddings.
Dimensionalit y reduction metho ds are usually applied on the full dimensional space to capture the indep enden t com-ponen ts among all the features. They are not designed to address the problem of iden tifying correlation in feature sub-spaces. It is reasonable to apply them to the feature spaces that are indeed correlated. However, in very high dimen-sional datasets, di eren t feature subspaces may have di er-ent correlations, and some feature subspace may not have any correlation at all. In this case, dimensionalit y reduction metho ds should be applied after suc h strongly correlated feature subspaces have been iden ti ed.

Correlation Clustering The goal of correlation cluster-ing metho ds is to nd clusters hidden in pro jected feature spaces [1, 7, 30]. They can be view ed as com binations of clustering metho ds and dimensionalit y reduction metho ds. Both OR CLUS [1] and 4C [7] can be treated as PCA-lized clustering metho ds. To nd the low dimensional clusters, OR CLUS and 4C apply PCA on subsets of data points in the full dimensional space and merge the point subsets hav-ing similar orien tations. Therefore, they do not touc h the problem of nding reducible subspaces. Instead, they im-plicitly assume the full dimensional space is reducible for certain subsets of data points. CURLER [30] nds clusters having non-linear correlations in subspaces. It rst applies EM clustering algorithm to nd a large num ber of micro-clusters, and then merges clusters with large overlaps. Since in its rst step, micro-clusters are formed by applying EM to the full dimensional space, CURLER also does not address the problem of nding the reducible subspaces.

Intrinsic Dimensionalit y Due to correlations among features, a high dimensional dataset may lie in a subspace with dimensionalit y smaller than the num ber of features [9, 14, 21]. The intrinsic dimensionalit y can be treated as the minim um num ber of free variables required to de ne the data without any signi can t information loss [13]. For ex-ample, as sho wn in Figure 1, in the 3-dimensional space of f f 1 ; f 2 ; f 3 g , the data points lie on a Swiss roll, whic h is actually a 2-dimensional manifold. Therefore, its intrinsic dimensionalit y is 2.

The concept of intrinsic dimensionalit y has man y appli-cations in the database and data mining comm unities, suc h as clustering [3, 15], outlier detection [24], nearest neigh-bor queries [23], and spatial query selectivit y estimation [5, 12]. Di eren t de nitions of intrinsic dimensionalit y can be found in the literature. For example, in linear cases, ma-trix rank [16] and PCA [18] can be used to estimate intrin-sic dimensionalit y. For nonlinear cases, estimators suc h as box counting dimension , information dimension , and corre-lation dimension have been dev elop ed. These intrinsic di-mensionalit y estimators are sometimes collectiv ely referred to as fractal dimension . Please see [25, 28] for good coverage of the topics of intrinsic dimensionalit y estimation and its applications.

Local Linear Correlation In [32], the CARE algo-rithm has been prop osed for nding local linear correlations. Adopting a similar criterion used in PCA, the strongly cor-related feature subsets are formalized as feature subspaces having small residual variances. However, this work only focuses on linear correlations. The problem of nding non-linear local correlations remains unin vestigated.
In this section, we utilize intrinsic dimensionalit y to for-malize the problem of nding strongly correlated feature subspaces.
 Supp ose that the dataset consists of N data points and M features. Let P = f p 1 ; p 2 ; ; p N g denote the point set, and F = f f 1 ; f 2 ; ; f M g denote the feature set in resp ectiv ely. We use ID ( V ) to represen t the intrinsic dimensionalit y of the feature subspace V 2 F .

Intrinsic dimensionalit y pro vides a natural way to examine whether a feature is correlated with some feature subspace: if a feature f a 2 F is strongly correlated with a feature sub-space V F , then adding f a to V should not cause much change of the intrinsic dimensionalit y of V . The follo wing de nition formalizes this intuition. Definition 3.1. (Str ong Correla tion) A featur e subsp ace V F and a featur e f a 2 F have strong correlation, if
In this de nition, is a user speci ed threshold. Smaller value implies stronger correlation, and larger value implies weak er correlation. If V and f a have strong correlation, we also say that they are strongly correlated.
 Definition 3.2. (Redund ancy) Let V = f f v 1 ; f v 2 ; ; f v m g F . f v i 2 V is a redundant featur e of V , if f v i has strong correlation with the featur e subsp ace consisting of the remaining featur es of V , i.e.,
We say V is a redundant feature subspace if it has at least one redundan t feature. Otherwise, V is a non-r edundant feature subspace.

Note that in De nitions 3.1 and 3.2, ID ( V ) does not de-pend on a particular intrinsic dimensionalit y estimator. Any existing estimator can be applied when calculating ID ( V ). Moreo ver, we do not require that the intrinsic dimensional-ity estimator re ects the exact dimensionalit y of the dataset. However, in general, a good intrinsic dimensionalit y estima-tor should satisfy two basic prop erties.

First, if a feature is redundan t in some feature subspace, then it is also redundan t in the sup ersets of the feature sub-space. We formalize this intuition as the follo wing prop erty.
Proper ty 3.3. For V 2 F , if ID ( V; f a ) , then 8 U ( V U F ) , ID ( U; f a ) .

This is a reasonable requiremen t, since if f a is strongly correlated with V U , then adding f a to U will not greatly alter its intrinsic dimensionalit y.

From this prop erty, it is easy to see that, if feature sub-space U is non-redundan t, then all of its subsets are non-redundan t, whic h is clearly a desirable prop erty for the fea-ture subspaces.

Cor ollar y 3.4. If U F is non-r edundant, then for 8 V U , V is also non-r edundant.

The follo wing prop erty extend the concept of basis [20] in a linear space to nonlinear space using intrinsic dimension-alit y. In linear space, supp ose that V and U con tain the same num ber of vectors, and the vectors in V and U are all linearly indep enden t. If the vectors of U are in the subspace spanned by the vectors of V , then the vectors in V and the vectors in U span the same subspace. (A span of a set of vectors consists of all linear com binations of the vectors.) Similarly , in Prop erty 3.5, for two non-redundan t feature subspaces, V and U , we require that if the features in U are strongly correlated with V , then U and V are strongly correlated with the same subset of features.
 Proper ty 3.5. Let V = f f v 1 ; f v 2 ; ; f v m g F and U = f f u 1 ; f u 2 ; ; f u m g F be two non-r edundant featur e subsp aces. If 8 f u i 2 U , ID ( V; f u i ) , then for 8 f F , ID ( U; f a ) i ID ( V; f a ) .

Intuitiv ely, if a feature subspace Y ( Y F ) is redun-dan t, then Y should be reducible to some subspace, say V ( V Y ). Concerning the possible choices of V , we are most interested in the smallest one that Y can be reduced to, since it represen ts the intrinsic dimensionalit y of Y . We now give the formal de nitions of reducible subspace and its core space.
 Definition 3.6. (Reducible Subsp ace and Core Space) Y F is a reducible subsp ace if ther e exists a non-r edundant subsp ace V ( V Y ) , such that (1) 8 f a 2 Y , ID ( V; f a ) , and (2) 8 U Y ( j U jj V j ) , U is non-r edundant.
 We say V is the core space of Y , and Y is reducible to V .
Criterion (1) in De nition 3.6 says that all features in Y are strongly correlated with the core space V . The meaning of criterion (2) is that the core space is the smallest non-redundan t subspace of Y with whic h all other features of Y are strongly correlated.

Among all reducible subspaces, we are most interested in the maxim um ones. A maxim um reducible subspace is a re-ducible subspace that includes all features that are strongly correlated with its core space.
 Definition 3.7. (Maximum Reducible Subsp ace) Y F is a maximum reducible subsp ace if (1) Y is a reducible subsp ace, and (2) 8 f b 2 F , if f b 62 Y , then ID ( V; f b ) &gt; , wher e V is the core space of Y .

Let f Y 1 ; Y 2 ; ; Y S g be the set of all maxim um reducible subspaces in the dataset. The union of the maxim um re-ducible subspaces OR = S S i =1 Y i is referred to as the over all reducible subsp ace .

Note that De nition 3.7 works for the general case where a feature can be in di eren t maxim um reducible subspaces. In this pap er, we focus on the special case where maxim um reducible subspaces are non-o verlapping, i.e., eac h feature can be in at most one maxim um reducible feature subspace. To nd the maxim um reducible subspaces in the dataset, REDUS adopts a two-step approac h. The rst step is to nd the overall reducible subspace. Then, from the overall reducible subspace, it iden ti es the individual maxim um re-ducible subspaces. In the next section, we presen t the algo-rithm for nding the overall reducible subspace. In Section 5, we discuss the metho d for nding individual maxim um reducible subspaces.
In this section, we presen t the algorithm for nding the overall reducible subspace. We rst give a short introduction to the intrinsic dimensionalit y estimator. Then we presen t the algorithm for nding the overall reducible subspace and the pro of of its correctness.
To nd the overall reducible subspace in the dataset, we adopt correlation dimension [25, 28], whic h can measure both linear and nonlinear intrinsic dimensionalit y, as our intrinsic dimensionalit y estimator since it is computation-ally more ecien t than other estimators while its qualit y of estimation is similar to others. In practice, we observ e that correlation dimension satis es Prop erties 3.3 and 3.5, although we do not pro vide the pro of here. In what follo ws, we give a brief introduction of correlation dimension. Let Y be a feature subspace of the dataset, i.e., Y
F . Supp ose that the num ber of points N in the dataset approac hes in nit y. Let dis ( p i ; p j ; Y ) represen t the distance between two data points p i and p j in feature subspace Y . Let B Y ( p i ; r ) be the subset of points con tained in a ball of radius r cen tered at point p i in subspace Y , i.e., The average fraction of pairs of data points within distance r is The correlation dimension of Y is then de ned as
In practice, N is a nite num ber. C Y is estimated us-ing 1 gro wth rate of the function C Y ( r ) in log-log scale, since log[ C Y ( r ) =C Y ( r 0 )] relation dimension is estimated using the slop e of the line that best ts the function in least squares sense.
The intuition behind the correlation dimension is follo w-ing. For points that are arranged on a line, one exp ects to nd twice as man y points when doubling the radius. For the points scattered on 2-dimensional plane, when doubling the radius, we exp ect the num ber of points to increase quadrat-ically . Generalizing this idea to m -dimensional space, we mensionalit y of feature subspace Y can be simply treated as the gro wth rate of the function C Y ( r ) in log-log scale.
The follo wing theorem sets the foundation for the ecien t algorithm to nd the overall reducible subspace.

Theorem 4.1. Supp ose that Y F is a maximum re-ducible subsp ace and V Y is its core space. We have 8 U Y ( j U j = j V j ) , U is also a core space of Y .
Proof. We need to sho w that U satis es the criteria in De nition 3.7. Let V = f f v 1 ; f v 2 ; ; f v m g and U = f f
Since U Y , from the de nition of reducible subspace, U is non-redundan t, and for every f u i 2 U , ID ( V; f u For every f a 2 Y , we have ID ( V; f a ) . Thus from Prop erty 3.5, we have ID ( U; f a ) . Similarly , for every f 62 Y , ID ( V; f b ) &gt; . Thus ID ( U; f a ) &gt; . Therefore, U is also a core space of Y .

Theorem 4.1 tells us that any subset U Y of size j V j is also a core space of Y .

Supp ose that f Y 1 ; Y 2 ; ; Y S g is the set of all maxim um reducible subspaces in the dataset and the overall reducible subspace is OR = S S i =1 Y i . To nd OR , we can apply the follo wing metho d. For every f a 2 F , let RF f a = f f b
F ; b 6 = a g be the remaining features in the dataset. We calculate ID ( RF f a ; f a ). The overall reducible subspace OR = f f a j ID ( RF f a ; f a ) g .We now pro ve the correct-ness of this metho d.
 Algorithm 1 : REDUS Input : Dataset , input parameters , n , and ,
Output : Y : the set of all maxim um reducible
OR = ; ; for each f a 2 F do end sample n points P = f p s 1 ; p s 2 ; ; p s n g from . for d = 1 to j OR j do end return Y. Cor ollar y 4.2. OR = f f a j ID ( RF f a ; f a ) g .
Proof. Let f y be an arbitrary feature in the overall re-ducible subspace. From Theorem 4.1, we have 8 f y 2 Y i OR , 9 V i Y i ( f y 62 V i ), suc h that V i is the core space of Y Thus ID ( V i ; f y ) . Since f y 62 V i , we have V i RF From Prop erty 3.3, we have ID ( RF f y ; f y ) . Similarly , if f y 62 OR , then ID ( RF f y ; f y ) &gt; . Therefore, we have OR = f f y j ID ( RF f y ; f y ) g .
The algorithm for nding the overall reducible subspace is sho wn in Algorithm 1 from Line 1 to Line 7. Note that the pro cedure of nding overall reducible subspace is linear to the num ber of features in the dataset. In this section, we presen t the second comp onen t of RE-DUS, i.e., iden tifying the maxim um reducible subspaces from the overall reducible subspace found in the previous section.
From De nition 3.7 and Theorem 4.1, we have the follo w-ing prop erty concerning the reducible subspaces.

Cor ollar y 5.1. Let Y i OR be a maximum reducible subsp ace, and V i Y i be any core space of Y i . We have
Therefore, to nd the individual maxim um reducible sub-spaces Y i OR (1 i S ), we can use any core space V i Y i to nd the other features in Y i . More speci cally , a candidate core space of size d is a feature subset C OR ( j C j = d ). From size d = 1 to j OR j , for eac h candidate core space, let T = f f a j ID ( C; f a ) ; f a 2 OR; f a 62 C g . If T 6 = ; , then T is a maxim um reducible subspace with core space of size d . The overall reducible subspace OR is then updated by remo ving the features in T . Note that the size of j OR j decreases whenev er some maxim um reducible subspace is iden ti ed. We now pro ve the correctness of this metho d. Cor ollar y 5.2. Any candidate core space is non-r edundant.
Proof. It is easy to see any candidate core space of size 1 is non-redundan t. Now, assume that all candidate core spaces of size d 1 are non-redundan t, we sho w all candidate core spaces of size d are non-redundan t. We pro ve this by con tradiction.

Let V = f f v 1 ; f v 2 ; ; f v d g be an arbitrary candidate core space of size d . Without loss of generalit y, assume that f is the redundan t feature in V . Let V 0 = f f 1 ; f 2 ; f We have ID ( V 0 ; f v d ) . Since j V 0 j = d 1, V 0 is non-redundan t according to the assumption. Moreo ver, we have T = f f a j ID ( V 0 ; f a ) ; f a 2 OR; f a 62 V 0 g 6 = ; , since f d 2 T . Therefore, f v d 2 T would have been remo ved from OR before the size of the candidate core spaces reac hes d . This con tradicts the assumption of f v d being in the candi-date core space V . Therefore, we have that any candidate core space is non-redundan t.

Cor ollar y 5.3. Let C be a candidate core space. If 9 f a OR such that ID ( C; f a ) , then C is a true core space of some maximum reducible subsp ace in OR .

Proof. Let Y = f f y j ID ( C; f y ) ; f y 2 OR g . Fol-lowing the pro cess of nding OR , we kno w that Y includes all and only the features in F that are strongly correlated with C . Thus 9 C Y , suc h that C satis es Criterion (1) in De nition 3.6, and Criterion (2) in De nition 3.7. More-over, according to Corollary 5.2, C is non-redundan t. Hence C also satis es Criterion (2) of De nition 3.6. Thus Y is a maxim um reducible subspace with core space C .

In this metho d, for eac h candidate core space, we need to calculate ID ( C ) and ID ( C [f f a g ) for every f a 2 OR in order to get the value of ID ( C; f a ). However, the intrin-sic dimensionalit y calculation is computationally exp ensiv e. Since the intrinsic dimensionalit y estimation is inheren tly appro ximate, we prop ose in the follo wing section a metho d utilizing the point distribution in feature subspaces to dis-tinguish whether a feature is strongly correlated with a core space.
After nding the overall reducible subspace OR , we can apply the follo wing heuristic to examine if a feature is strongly correlated with a feature subspace. The intuition behind our heuristic is similar to the one behind the correlation dimen-sion.

Assume that the num ber of data points N in the dataset approac hes in nit y, and the features in the dataset are nor-malized so that the points are distributed from 0 to 1 in eac h dimension. Let p s 2 P be an arbitrary point in the dataset, and 0 &lt; l &lt; 1 be a natural num ber. Let sy represen t the in-terv al of length l on feature f y cen tered at p s . The exp ected num ber of points within the interv al sy is lN . For d fea-tures C = f f c 1 ; f c 2 ; ; f c d g , let Q sC be the d -dimensional hypercub e formed by the interv als sc i ( f c i 2 C ). If the d features in C are totally uncorrelated, then the exp ected num ber of points in Q sC is l d N . Let f m be another feature in the dataset, and C 0 = f f c 1 ; f c 2 ; ; f c d ; f m lated with C , then C 0 has intrinsic dimensionalit y d . The Figure 4: Point distributions in correlated feature subspace and uncorrelated feature subspace exp ected num ber of points in the d -dimensional hypercub e, Q sC 0 , whic h is embedded in the ( d + 1)-dimensional space of C 0 , is still l d N . If, on the other hand, f m is uncorrelated with any feature subspace of f f c 1 ; f c 2 ; ; f c d g , then C dimensionalit y d + 1, and the exp ected num ber of points in the ( d + 1)-dimensional hypercub e Q sC 0 is l ( d +1) N . The dif-ference between the num ber of points in the cub es of these two cases is l d (1 l ) N .

Figure 4(a) and 4(b) sho w two examples on 2-dimensional spaces. In both examples, d = 1 and C = f f a g . In Fig-ure 4(a), feature f b is strongly correlated with f a . Feature f c is uncorrelated with f a , as sho wn in Figure 4(b). The randomly sampled point p s is at the cen ter of the cub es Q is clearly much higher than the point densit y in cub e Q due to the strong correlation between f a and f b .
Therefore, for eac h candidate core space, we can chec k if a feature is correlated with it in the follo wing way. We randomly sample n points P = f p s 1 ; p s 2 ; ; p s n g from the dataset. Supp ose that C = f f c 1 ; f c 2 ; ; f c d g is the curren t candidate core space. For feature f a 2 OR ( f a 62 C ), let C 0 = f f c of points in the cub e Q s i C 0 . P 0 = f p s i j s i C 0 the subset of the sampled points suc h that the cub e cen tered at them have more points than exp ected if f a is uncorrelated with C . We say f a is strongly correlated with C if j P 0 j where is a threshold close to 1.
Concerning the choice of l , we can apply the follo wing reasoning. If we let l = ( 1 N ) 1 d +1 , then the exp ected num ber of points in the cub e Q s i C 0 is 1, if f a is uncorrelated with C . If f a is correlated with C , then the exp ected num ber of points in the cub e Q s i C 0 is greater than 1. In this way, we can set l according to the size of the candidate core space. The second step of REDUS is sho wn in Algorithm 1 from Line 8 to Line 18. Note that in the worst case, the al-gorithm needs to enumerate all possible feature subspaces. However, in practice, the algorithm is very ecien t since once an individual reducible subspace is found, all its fea-tures are remo ved. Only the remaining features need to be further examined.
To evaluate REDUS, we apply it on both syn thetic datasets and real datasets. REDUS is implemen ted using Matlab 7.0.4. The exp erimen ts are performed on a 2.4 GHz PC with 1G memory running Windo wsXP system.
As sho wn in Algorithm 1, REDUS generally requires three input parameters: , n , and . In the rst step of nding the overall reducible subspace, is the threshold to lter out the irrelev ant features. Since features strongly correlated with some core space can only change intrinsic dimensionalit y a small amoun t, the value of should be close to 0. According to our exp erience, a good starting point is 0.1. After nd-ing the reducible subspaces, the user can apply the standard dimensionalit y reduction metho ds to see if the are really cor-related, and the adjust value accordingly to nd stronger or weak er correlations in the subspaces. In all our exp eri-men ts, we set between 0.002 to 0.25. In the second step, n is the point sampling size and is the threshold to de-termine if a feature is strongly correlated with a candidate core space. In our exp erimen ts, n is set to be 10% of the total num ber of data points in the dataset, and is set to be 90%.
To evaluate the e ectiv eness of the REDUS, we generate two syn thetic datasets.

Syn thetic dataset 1: The rst syn thetic dataset is as sho wn in Figure 1. There are 12 features, f f 1 ; f 2 ; ; f and 1000 data points in the dataset. 3 reducible subspaces: a 2-dimensional Swiss roll, a 1-dimensional helix-shap ed line, and a 2-dimensional plane, are embedded in di eren t 3-dimensional spaces resp ectiv ely. The overall reducible sub-space is f f 1 ; f 2 ; ; f 9 g . Let c i (1 i 4) represen t con-stan ts and r j (1 j 3) represen t random vectors. The generating function of the Swiss roll is: t = 3 2 (1 + 2 r rotated 45 coun ter clockwise on feature space f f 2 ; f 3 helix-shap ed line is generated by: f 4 = c 1 r 3 , f 5 = c f 6 = c 2 cos( r 3 ). The 2-dimensional plane is generated by f 9 = c 3 f 7 + c 4 f 8 . The remaining 3 features f f 10 ; f are random vectors consisting of noise data points. Figure 5: Examples of embedded correlations in syn-thetic dataset 2 Table 1: Accuracy of nding the overall reducible subspace when varying
In the rst step, with = 0 : 25, REDUS successfully un-covers the overall reducible space. The parameter setting for the second step is = 90%, and point sampling size 10%. We run REDUS 10 times. In all 10 runs, REDUS success-fully iden ti es the individual maxim um reducible subspaces from the overall reducible subspace.

Syn thetic dataset 2: We generate another larger syn-thetic dataset as follo ws. There are 50 features, f f 1 ; f and 1000 data points in the dataset. There are 3 reducible subspaces: Y 1 = f f 1 ; f 2 ; ; f 10 g reducible to a 2-dimensional space, Y 2 = f f 11 ; f 12 ; ; f 20 g reducible to a 1-dimensional space, and Y 3 = f f 21 ; f 22 ; ; f 30 g reducible to a 2-dimensional space. The remaining features con tain random noises. Fig-ures 5(a) and 5(b) sho w two examples of the embedded cor-relations in 3-dimensional subspaces. Figure 5(a) plots the point distribution on feature subspace f f 1 ; f 2 ; f 9 g of Y Figure 5(b) plots the point distribution on feature subspace f f 0.94 f 1 ; 2 ; 3 ; 4 ; 5 ; 6 ; 7 ; 8 ; 9 ; 10 g 0.92 f 1 ; 2 ; 3 ; 4 ; 5 ; 6 ; 7 ; 8 ; 9 ; 10 g 0.90 f 1 ; 2 ; 3 ; 4 ; 5 ; 6 ; 7 ; 9 g 0.88 f 1 ; 2 ; 3 ; 4 ; 5 ; 6 ; 7 ; 8 g n=N maxim um reducible subspaces iden ti ed Table 2: Accuracy of iden tifying the maxim um re-ducible subspaces from the overall reducible sub-space when varying and n
We apply REDUS on this syn thetic dataset using various parameter settings. Table 1 sho ws the accuracy of nding the overall reducible subspace when taking di eren t values. The recall is de ned as T P= ( T P + F N ), and the precision is de ned as T P= ( T P + F P ), where T P represen ts the num ber of true positiv e, F P represen ts the num ber of false positiv e, and F N represen ts the num ber of false negativ e. As we can see, REDUS is very accurate and robust to .

Tables 2(a) and 2(b) sho w the iden ti ed maxim um re-ducible subspaces when varying and n . Table 2(a) sho ws results under di eren t settings of . The point sampling size n in this table is the default value, i.e., 10% of the total num-ber of data points. Although changing may cause some mis-classi ed features, Table 2(a) still sho ws that REDUS achiev es reasonably high accuracy under di eren t settings of . The reason for di eren t decomp ositions of maxim um reducible subspaces is that features in di eren t maxim um reducible subspaces may still have mo derate correlations. If these correlated features are iden ti ed, they will be remo ved from the overall reducible subspace, since REDUS focuses Figure 6: Eciency evaluation of nding the overall reducible subspace on the case where the maxim um reducible subspaces are non-o verlapping. If we allo w eac h feature to be in di eren t maxim um reducible subspaces, the ndings under di eren t should be more similar to eac h other. Finding overlap-ping reducible subspaces is computationally more demand-ing, and is an interesting problem worth further exploration.
Table 2(b) sho ws the iden ti ed maxim um reducible sub-spaces when varying the num ber of the sampled points n . is set to be 0 : 92 in this table. As sho wn in the table, REDUS is not sensitiv e to the size of the sampled data points.
To evaluate the eciency and scalabilit y of REDUS, we apply it to syn thetic dataset 2. The default dataset for e-ciency evaluation con tains 1000 points and 50 features if not speci ed otherwise. The default values for the parameters are the same as before.

Figure 6(a) sho ws the run time of nding the overall re-ducible subspace when varying the num ber of data points. The run time scales roughly quadratically . This is because when computing the correlation dimensions, we need to cal-culate all pairwise distances between the data points, whic h is clearly quadratic to the num ber of points.

Figure 6(b) sho ws that the run time of nding the overall reducible subspace is linear to the num ber of features. This is because REDUS only scans every feature once to examine if it is strongly correlated with the subspace of the remaining features. This linear scalabilit y is desirable for the datasets con taining a large num ber of features.

Figures 7(a) and 7(b) sho w the run time comparisons be-tween using the correlation dimension as intrinsic dimen-sionalit y estimator and the point distribution heuristic to iden tify the individual maxim um reducible subspaces from the overall reducible subspaces. Since the calculation of in-trinsic dimensionalit y is relativ ely exp ensiv e, the program Figure 7: Eciency evaluation of iden tifying maxi-mum reducible subspaces from the overall reducible subspace often cannot nish in a reasonable amoun t of time. Using the point distribution heuristics, on the other hand, is much more ecien t and scales linearly to the num ber of points and features in the dataset.
We apply REDUS on the NBA statistics dataset. The dataset can be downloaded from http : ==spor ts:espn:g o:com= nba=teams=stats ? team = Bos &amp; year = 2007&amp; season = 2. It con tains the statistics of 28 features for 200 players of sea-son 2006-2007. Since the features have di eren t value scales, we normalized eac h feature suc h that points are distributed between 0 and 1. We rep ort two interesting correlations found in the dataset in Figures 8(a) and 8(b). Note that the features sho wn in the gures are mean-cen tered.

The correlation sho wn in Figure 8(a) says that the fea-ture subspace of three features: defence reb ounds (DEF), the o ense reb ounds (OFF), and the total num ber of re-bounds (TOT), is strongly correlated and reducible to a 2-dimensional space. This is an obvious correlation that one would exp ect. As sho wn in the gure, the points are linearly distributed on a 2-dimensional plane in the 3-dimensional subspace.
 Figure 8(b) sho ws a nonlinear correlation iden ti ed by REDUS. The feature subspace of three features: eld goal made (FGM), eld goal attempted (FGA), and the percen t-age of eld goal (FG%) is strongly correlated and reducible to 2-dimensional space. Clearly , the data points on this 3-dimensional space are distributed on a 2-dimensional mani-fold. Figure 8: Correlations iden ti ed in the NBA dataset
Figure 9: A linear correlation in the wage dataset
The wage dataset from the 1985 Curren t Population Sur-vey consists of 11 features in 534 data points. The dataset is available at http : ==l ib:stat:cmu:edu=datasets=C P S 85 W ages . The numerical features are age, years of education, years of work exp erience, and wage. We apply REDUS on this dataset to nd the strongly correlated feature subsets.
REDUS iden ti es one correlation between the features: age, years of education, and wage. Figure 9 sho ws the point distribution of the data points in this 3-dimensional feature subspace. From this gure, we can see that wage is clearly a linear function of age and years of education. Therefore, this 3-dimensional space is reducible to the 2-dimensional plane embedded in it. Figure 10: A correlation in the breast cancer dataset
We apply REDUS to the breast cancer dataset whic h is available at the UCI Mac hine Learning Arc hiev e. There are 569 data points and 30 features in this dataset. The features include the statistics of radius, texture, perimeter, area, smo othness, compactness conca vity, conca ve points, symmetry , and fractal dimension. These features are com-puted from a digitized image of a ne needle aspirate of a breast mass. They describ e characteristics of the cell nuclei presen t in the image.

Figure 10 sho ws one of the nonlinear correlations iden ti-ed by REDUS. The three features are the mean of radius, largest radius, and the mean of texture. From the gure, we can see these three features are strongly correlated and reducible to 1-dimensional space.
In this pap er, we investigate the problem of nding strongly correlated feature subspaces in high dimensional datasets. The correlation can be linear or nonlinear. Suc h correla-tions hidden in feature subspace may be invisible to the global feature transformation metho ds, suc h as PCA and ISOMAP . Utilizing the concepts of intrinsic dimensionalit y, we formalize this problem as the disco very of maxim um re-ducible subspaces in the dataset. An e ectiv e algorithm, REDUS, is presen ted to nd the maxim um reducible sub-spaces. The exp erimen tal results sho w that REDUS can e ectiv ely and ecien tly nd these interesting local correla-tions.

Our work rep orted in this pap er focuses on the case where the maxim um reducible subspaces are non-o verlapping. For future work, one interesting direction is to extend curren t work to the general case where a feature can be in multiple maxim um reducible subspaces. Another interesting direc-tion is nding the feature subspaces that are strongly cor-related on a subset of data points. This is a more general problem and has wider applications. This researc h was partially supp orted by EPA gran t STAR-RD832720, NSF gran ts IIS-0448392, IIS-0534580, and a Mi-crosoft New Facult y Fello wship.
