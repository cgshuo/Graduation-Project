 An idiom is a combination of words that has a fig-urative meaning which differs from its literal mean-ing. Idioms pose a great challenge to many NLP tasks, such as machine translation, word sense dis-ambiguation, and sentiment analysis (Volk, 1998; Korkontzelos et al., 2013; Zanzotto et al., 2010; Williams et al., 2015). Previous work (Salton et al., 2014) has shown that a typical statistical ma-chine translation system might achieve only half of the BLEU score (Papineni et al., 2002) on sentences that contain idiomatic expressions than on those that do not. Idioms are also problematic for second lan-guage learners. In a pilot study we have surveyed seven non-native speakers on 100 Tweets containing idioms; we have found that, on average, the partici-pants had trouble understanding 70% of them due to the inclusion of idioms.

This work explores the possibility of automati-cally replacing idiomatic expressions in sentences. The full pipeline of a successful system has to solve many problems. First, it has to determine that an expression is, in fact, being used as an idiom in a sentence (Fazly et al., 2009; Korkontzelos et al., 2013; Sporleder and Li, 2009). Moreover, the sys-tem has to sense disambiguate the idiom  X  it has to pick the correct interpretation when more than one is possible. Second, it has to generate an appropriate phrasal replacement for the idiom using literal En-glish. Third, it has to ensure that the replacement phrase will fit seamlessly back into the original sen-tence. This paper focuses on the second and third problem, which have not been studied as extensively in previous works.

We propose to extract the phrasal replacement for an idiom from its definition, assuming the existence of an up-to-date dictionary of broad coverage and long, it cannot directly serve as a replacement for the idiom. A major challenge of our work is in identi-fying the right nugget to extract from the definition. Another major challenge is the smooth integration of the substitution phrase into the sentence. We con-sider both grammatical fluency as well as references resolution in our automatic Post-Editing technique. These phrasal challenges set our goals apart from related work on lexical simplification and substitu-tion (Specia et al., 2012; Jauhar and Specia, 2012; McCarthy, 2002; McCarthy and Navigli, 2007) and from general sentence simplification (Wubben et al., 2012; Siddharthan, 2014; Zhu et al., 2010; Coster and Kauchak, 2011) methods.

We validate the plausibility of the proposed meth-ods with empirical experiments on a manually an-uations and user studies show that the proposed ap-proach can generate high-quality paraphrases of sen-tences containing idiomatic expressions. A success-ful idiom paraphrase generator may not only bene-fit non-native speakers, but may also facilitate other NLP applications. The main idea of this work is to produce a fluent and meaningful paraphrase of the original sentence similar to how a human non-native reader might ap-proach the problem. Suppose the reader encounters the following sentence: If they do not understand the expression gets my blood up , they may look it up in a dictionary: Then they might try to reconcile the definition with the context of the sentence and arrive at:
In the example above, only a portion of the full definition is needed. One possible way to iden-tify this relevant nugget is to apply sentence com-pression techniques (McDonald, 2006; Siddharthan, 2011;  X  Stajner et al., 2013; Filippova et al., 2015; Filippova and Strube, 2008; Narayan and Gardent, 2014; Cohn and Lapata, 2009). However, all these methods have been developed for standard texts with complete sentences, and it is not clear whether they are suited to dictionary definitions. Consider Ta-ble 1, in which a corpus of 1000 randomly selected Corpus Average length Punctuation density CLspoken 17 2.16 CLwritten 18 2.07 Definition 12 2.86 idiom definitions is compared with samples from two normal text corpora (CLwritten and CLspoken) used by Clarke and Lapata (2008). The CLwritten corpus comes from written sources in the British National Corpus and the American News Text cor-pus; the CLspoken corpus comes from transcribed broadcast news stories. We see that on average, definitions are shorter than complete sentences; ar-guably, each word in a definition carries more in-formation. The density of punctuation per sentence shows that definitions are more fragmented. These factors are problematic for sentence compression techniques that rely heavily on the syntactic parse trees of complete, well-formed sentences (Cohn and Lapata, 2009; Narayan and Gardent, 2014). One re-cent compression method that does not rely as heav-ily on syntax is the work of Filippova et al. (2015). However, their approach requires a training set of considerable size, which is not practical for the do-main of idiom definitions. The most likely to suc-ceed text compression method for our domain is the work of McDonald (2006) as they only use syntac-tic information as soft evidence to compress target sentences. We choose this method as a comparative baseline in our experimental evaluation.

After obtaining an appropriately shortened defini-tion, get someone angry , more operations are needed to properly replace the idiom with it in the origi-nal sentence. First, we need to convert get to gets to make the tense consistent. Second, we need to resolve the reference someone to the appropri-ate person in the context of the original sentence: me . These operations are important to fit the short-ened definition seamlessly into the original context, which will be covered in the Post Editing section. As outlined in the previous section, our proposed method consists of two components: substitution generation and post editing . 3.1 Substitution Generation This component aims to extract relevant replace-ment phrases from an idiom X  X  dictionary defini-tion. Rather than using generic sentence compres-sion techniques, we argue that the taxonomy of a definition follows certain conventions that can be exploited. In most definitions, the core meaning is presented first; it is then optionally followed by ad-ditional information that supports, explains, and/or exemplifies the main point. The relationship be-tween the core meaning and different types of ad-dition information is akin to relationships between nucleus and surrounding sentences as described by the rhetorical structure theory (Mann and Thomp-son, 1988). Using a development set of idiom def-inition, we have identified four types of additional information:
Coordination to discover or apprehend some-
Supplement time is very important. (Used es-
Below, we present two methods for extracting the core meaning from a full definition. We first con-sider a rule-based approach, under the assumption that definitions can be fully described by a small set of regular patterns. We also present a supervised machine-learning approach, showing that these reg-ular patterns do not have to be predefined, thus open-ing up for possibilities of adapting the method to dif-ferent dictionaries and languages. 3.1.1 A Rule-based Method
Analyzing the development set, we observe that additional information are often signaled by a small sets of lexical cues 5 ; we call them boundary words . Using these boundary words and some shallow syn-rules to pare down the definition. Below are five main types of rules: 1. Delete coordinated phrase after the word 2. Delete subordinate clause after  X  X ecause X  or 3. Delete the clause after words such as  X  X o 4. Delete sentence after words such as  X  X or in-5. Delete sentences in bracket . This is often just If multiple rules are applicable, we start from the rule that covers the widest range first, then to rules covering smaller ranges. After all these steps, if the output has more than one sentence, we always keep the first sentence for simplicity.

There is also a case of keyword ambiguity with respect to the word  X  X s: X  it could signal an explana-tion (like  X  X ecause X ) or an example (like  X  X uch as X ). Because TheFree Dictionary rarely use  X  X s X  by itself to signal an explanation, we have only encoded the  X  X uch as X  sense in our rule-set to avoid the ambigu-ity. 3.1.2 A ML-based Method
Not every definition follows the schema expected by the rule-based system. To generalize the patterns, we cast substitution generation as a binary classifi-cation problem. The most straightforward way is to decide whether each word in a definition should be deleted or kept, but this will degrade the sequential fluency of the shortened definition.

A better alternative is to segment the definition into syntactic chunks such as non-embedded NP, VP, ADJP, ADVP and PP phrases using off-the-shelf shallow parsers (e.g., NLTK). Chunks have been shown to minimize the generation of discontinuous sentences in previous works in machine translation (Zhang et al., 2007; Watanabe et al., 2003). We ap-ply a trained binary Support Vector Machine (SVM) classifier to each chunk to predict whether it should be kept or discarded. The shortened definition con-sists of only chunks that are kept.

Lexical and syntactical features are extracted from definition chunks as well as the sentence con-taining the original idiom. We have also incorpo-rated features that related previous works have found to be beneficial (  X  Stajner et al., 2013; Narayan and Gardent, 2014). The following is a brief description of our feature set.

Features from the Sentence : These features en-code the syntactic context of the idiom. One feature is the constituent label of the entire idiom from the sentence X  X  full parse. It aims to show the big picture of the grammatical function of the idiom in the orig-inal sentence. Another feature is the part-of-speech (POS) tag of the word preceding the idiom. These features help to select definition chunks that fit bet-ter into the sentence context in which the idiom is used. Due to data sparsity and overfitting concerns, we do not extract lexical features from the sentences.
Features from the Definition : These features en-code the syntactic information extracted from all the chunks that made up the definition. In addition to chunking, we also apply a full parser on the defini-tion to obtain its dependency and constituency tree. Although the parse trees may not be reliable enough to serve as hard constraints, they offer useful syn-tactic information as soft evidences. For example, the dependency tree helps us to identify the head word of every chunk (denoted here as w h ). The con-stituency tree helps us to determine whether w h is a node in a subordinate clause (subtree with its root la-beled as  X  X BAR X ). This feature is useful because two adjacent chunks in a relative clause tend to be kept or discarded together. We also include features in-dicating the relation of the typed dependency of the chunks. Thus, if a verb chunk is kept, its arguments are also likely to be kept. Other features includes whether w h is the root, whether w h is the leaf node in the dependency tree. Since certain adjacent words tend to be discarded or kept together, we reinforce this property by adding a bigram POS feature of w h to encode its context. Additionally, we extract var-ious surface features from the chunks such as their lengths, their positions in the definition, POS of w h , etc. Some definitions are very long and have several sub-sentences, while a good shortened definition is usually extracted from one sub-sentence. Thus, we have also included a feature indicating whether the definition has more than one sub-sentence, and if the definition has more than one sub-sentence, whether the chunk is in the first sub-sentence.

Features adapted from the Rule-Based method : These include: whether the chunk con-tains a boundary word, whether the preceding word of the chunk is a boundary word, whether the following word of the chunk is a boundary word, whether the chunk is in a bracket. 3.2 Post Editing To ensure that the shortened definition is a fluent re-placement for the idiom in the context of the original sentence, we must make grammatical adjustments, resolve references, and smooth over the replacement boundaries. 3.2.1 Grammatical Adjustments
We perform several agreement checks. For exam-ple, when replacing a noun phrase idiom, we need to make sure that the grammatical number of the re-placement phrase agrees with how it is used in the sentence. Similarly, when replacing a verb phrase idiom, we need to perform verb tense, person and number agreement checks, such as converting get someone angry to gets someone angry in the exam-ple mentioned in Section 2. 3.2.2 Reference Resolution
Reference expression is common in definition of idiom. For example, the idiom see eye to eye has a shortened definition of they agree with each other . The referent they has to be resolved when we sub-stitute the idiom with it. The general reference resolution problem is a long-standing challenge in NLP (Mitkov, 1998; Hobbs, 1978; Hobbs, 1979); even in the limited context of our idiom substitution problem, it is not trivial. While regular expression matching may work for idioms that contain simple slot replacements (e.g., the idiom lather something up with the definition to apply thick soapsuds to something ), further analyses on the idiom X  X  senten-tial context are necessary for many idioms (e.g., see eye to eye has no obvious slot).

Typical reference expressions in a definition in-clude something , someone , somebody , you , they , which often refer to noun phrases (NPs) in and around the idiom in the sentence. When the sentence context contains multiple NPs, we need to choose the right one to resolve the reference. To do so, we rely on two commonly used factors: recency and syntactic agreement (Lappin and Leass, 1994). Sim-ilar to the work of Siddharthan (2006), we extract all NPs in the original sentence with their agree-ment types and grammatical functions; for each NP, we assign it a score with equal weights of recency and syntactic factors. We choose the NP that satisfy the agreements and grammatical functions with the highest score, breaking ties by selecting the closest NP. When no contextual NP is suitable, we replace the reference expression with generics such as  X  X t, X   X  X eople, X  or  X  X erson X  instead.

There is one subtle difference between reference resolution in our work and typical cases. In addi-tion to deriving the correct interpretation of a ref-erence expression, our system has to actually insert the referent to the shortened definition and make the paraphrased sentence grammatical. This means that we need to make the appropriate PRP (personal pro-noun) and PRP$ (possessive pronouns) conversions. Consider the example from Section 2 again. the someone in the shortened definition is initially re-solved to my in the original sentence, but to make the substitution grammatical, it has to be transformed to me . In addition, special processing is also needed when the substitution is in the form of subordinate clause . For example: Although someone refers to he in the original sen-tence, no pronoun substitution is plausible. There-fore, someone is replaced by a generic expression,  X  X  person. X  3.2.3 Boundary Smoothing Boundary smoothing is the last step of the Post-Editing process to improve the fluency of the result-ing sentence. We rely on a standard n-gram language model to evaluate the  X  X moothness X  of the transi-tions between the original sentence and the substi-tution phrase. For the left boundary, we begin by checking the bigram probability of the word imme-diately before the substitution and the first word of the substitution. If it is 0, we would drop the first word and recheck until we find a bigram with non-zero probability or until we have reached the fourth word, whichever occurs first. If a non-zero bigram cannot be found within the first three words, we sub-stitute the original shortened definition as is, without any word deletion. The range of three word is cho-sen based on our analysis of the development set. A mirror image process is applied to the right bound-ary. The language model is trained via NLTK using To determine the performance of the definition shortening methods and post editing operations, we have carried out two experiments. The first (Section 4.2) evaluates the quality of the substitution gen-eration methods; we also argument the evaluation with statistical analysis of post-editing as a reference for future work. The second (Section 4.3) evaluates whether the resulting paraphrased sentence is gram-matical and preserves the original meaning. 4.1 Corpus To evaluate our method on real data, we chose to select Tweets that contain idioms. The reasons are twofold. First, the inspiration for our problem for-mulation was to help non-native speakers under-stand social media contents. The limited context of a Tweet makes it harder for someone who does not know an embedded idiom to induce its meaning from the rest of the text. Second, Tweet are self-contained, making the paraphrase task as well as its evaluation (by human judges) more stand-alone. The short context limits the set of mentioned en-tities, which helps with pronoun resolution; other-wise, we foresee no significant hurdles in applying our system to regular sentences.

To build the dataset, we randomly selected 200 idioms (100 for train and 100 for test) and automat-ically collected tweets in which they appeared using no exact match was found; so we included the usage examples from TheFreeDictionary.com instead. We presented these sentences along with each idiom X  X  definition and asked a volunteer native speaker (An-notator #1) to manually shorten the definition. Af-ter filtering out sentences that do not exemplify the ing and 84 for testing. Next, a near-native speaker (Annotator #2) also performed the same task so that we may compute the inter-annotator agreement. The shortened definitions from Annotator #1 are used as the gold standard.

Table 3 shows the agreement between two annota-tors. The overall average edit distance is 2.17 words; since the average length of the definitions is about twelve words long (cf. Table 1), the annotators have significant overlaps with each other (the Cohen X  X  kappa is 0.64, suggesting that the inter-annotator agreement is within an acceptable range (Viera et al., 2005)). However, although the annotators extracted the exact same phrase 34.8% of the times, in gen-eral they do not completely agree. Some people may select more words to convey a more precise mean-ing while others sacrifice some precision in meaning for a greater fluency. Thus, in addition to measuring against the gold standard (Annotator #1) using au-tomatic metrics, we also need to perform a human evaluation to directly judge the qualities of the para-phrases. 4.2 Automatic evaluation In this experiment, we compare different approaches for substitution generation using automatic metrics. We wish to determine: 1) How well does each method replicate human annotators X  phrasal extrac-tions? 2) Do we need specialized methods for ex-tracting core meanings from idiom definitions? 3) Is the ML-based method more general and flexible?
The training data contains 88 definitions for a to-tal of 645 chunks that have been labeled as  X  X eep X  or  X  X iscard X  according to the gold standards. The test data consists of 84 unique idioms used in tweets. The evaluation metric is the minimum edit distance of each proposed substitution from the gold stan-dard. We also calculate the compression rate , the ra-tio between numbers of tokens kept with total num-bers of tokens in original sentence.

We compare our proposed methods with McDon-ald (2006). Specifically we use an adapted version described in Filippova et al. (2015). We also imple-mented two simpler baselines: Equal-POS : Extract those words from the definition that have the same POS tags as the idiom. For exam-ple, if the idiom consists of a VB and an NN, then the first two words tagged as VB and NN in the def-inition are returned as the substitution. When POS matching fails, the whole definition is returned. First-Six : Always return the first six words. We choose six because the average length of the gold standard extractions from the training set is six words long.

From the results presented in Table 4, we see that the problem of extracting the core explanation from a long definition is not trivial. The average mini-mum edit distances from the gold standard are high for the two simple baselines (6.29 for First-Six, 4.92 for Equal-POS). The text compression baseline, Mc-Donald, is only a little better, at 4.86. Because the proposed methods are developed especially for id-iom definitions, they are closer to the gold stan-dard. Considering the inter-annotator agreement as an upper-bound (with an average minimum edit dis-tance of 2.15 for the test set), the ML-based ap-proach comes the closest to the upper-bound (with an average distance of 2.75).
Figure 1 plots a distribution of each method X  X  minimum edit distances for the instances in the test set. While the rule-based approach has a similar dis-tribution as the trained classifier, it is almost always slightly worse. We see that half of the extractions based on the keep/discard classification are within a one word difference from the gold standard, and there are fewer than five instances for which the edit distance is at least 10; in contrast, the rule-based ap-proach has fewer cases of (nearly) perfect matches and more cases of large mismatches (with the ex-ception of an edit distance of 9). These results sug-gest that specialized methods are necessary for pro-cessing idiom definitions and that an ML-based ap-proach is more general and flexible.

We have also measured the compression rate (CR) of each method; however, this may not be an appro-priate metric for our domain in the sense that lower is not necessarily better. The CR of gold standard is Def Rule-Based 4.92 4.71 Sen Rule-Based 4.25 4.31 45%, while the ML-based method is 51%. Although the McDonald method has the lowest CR, at 22%, it is lower than that of the gold standard; this suggests that its approach is too aggressive.

To evaluate the contribution of the post-editing component, we have performed data analyses on each step individually: grammatical adjustment, ref-erence resolution and boundary smoothing (using the outputs of the ML-based method). In terms of grammatical adjustments, there are two cases of noun number adjustment and five verb related ad-justment.

In terms of reference resolution, we need to ad-dress not only the typical reference expressions, but also special cases relating to PRP and PRP$ conver-sions and subordinate clauses that was discussed in section 3.2.2. We have found fifteen cases of typi-cal reference resolution and nine special cases, out of which, seven were related to subordinate clause (cf. Example 2 in Table 6). Finally, there are three cases for which reference expression cannot be re-solved due to the lack of an appropriate noun phrase (cf. Example 5 in Table 6).
With respect to boundary smoothing, there are many more cases of left boundary smoothing than right boundary smoothing (39 vs. 2 cases). Al-though many of the left boundary cases simply in-volve deleting the word  X  X o X  from the shortened def-initions, some boundary smoothing cases do address the more severe redundancy disfluencies (cf. Exam-ple 1 in Table 6). 4.3 Human evaluation Minimum edit distance to the gold standard can-not fully indicate the grammaticality and meaning preservation of the extracted phrase. In this exper-iment, we follow standard human evaluation proce-dures (McDonald, 2006) to verify our findings from the first experiment. The results will answer two questions: 1) Are the shortened definitions gram-matical and are they representative of the core mean-ings? 2) Are the final paraphrased sentences gram-matical and do they retain their original meanings?
We used the same 84 idioms which were the test set in the automatic evaluation. Four native speak-ers were recruited to evaluate the grammaticality and meaning of the shortened definitions and para-phrased sentences on a five-point scale. Each person took approximately 90 minutes to finish the study. We did not evaluate the simple baselines (First-Six and Equal-POS) because their qualities were obvi-ously low; including them may bias the human sub-jects to give inflated scores to the better methods. The results are presented in Table 5.

In terms of shortening the definition, the rule-based method obtains the highest scores in both grammaticality and meaning; this is because it tends to be relatively conservative. The compression rate is 59%, while the ML-based method is 51%. Keep-ing more words in the definition reduces the chance of introducing grammar error and meaning loss; however, a longer definition makes poorer substitu-tion in the full sentence because it introduces redun-dancy and thwarts post-editing efforts. This is val-idated in our experimental results  X  in terms of the paraphrased sentences, the rule-based method is out-performed by the ML-based method, which achieves the best result, with 4.61 in grammaticality and 4.64 in meaning.

Table 6 shows some typical examples of the para-phrases produced using substitution generation from the ML-based method, the rule-based method, and the McDonald method followed by processing with the proposed post-editing techniques. The first ex-ample features the effect of boundary smoothing. The shortened definition from the ML-based method is work together to . Direct replacement into the orig-inal sentence creates a disfluent bigram  X  X orking work X , which has a probability of 0; thus the first word in the shortened definition ( work ) is deleted au-tomatically. Similarly, the word to is deleted for the right boundary. In the third example, an automatic grammar adjustment is applied during substitution: a temporary solution is converted to Temporary so-lutions to keep the number consistent. In the forth example, the reference they is successfully resolved to you in the definition. The fifth example features a challenging rare case that results in a failed refer-ence resolution.

Shortening the definition is a trade off between length and meaning. In these examples, the rule-based method keeps as many words as possible from the definition and leads to redundancy in the final output. It has a negative impact on the readabil-ity of the paraphrase. The McDonald method is too aggressive for short text such as definition, so the outputs are often discontinuous. The ML-based method offers a reasonable balance between length and meaning, and produces paraphrases that people seem to prefer. We have proposed a phrasal substitution method for paraphrasing idiomatic expressions. Our system ex-tracts the core meaning of an idiom from its dictio-nary definition and replaces the idiom with it. Em-pirical evaluations shows that the proposed method produces grammatical paraphrases that preserves the idioms X  meanings, and it outperforms other methods such as sentence compression. In the future, we will explore the uses of the idiom paraphrases in NLP applications such as machine translation and intelli-gent tutor for second-language learners.
 We would like to thank Ric Crabbe, Xiaobing Shi and Huichao Xue for the helpful discussions and suggestions. We also would like to thank the anony-mous reviewers for their feedback.
