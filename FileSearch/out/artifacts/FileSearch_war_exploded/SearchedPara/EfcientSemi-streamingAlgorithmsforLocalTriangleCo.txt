 In this pap er we study the problem of local triangle coun t-ing in large graphs. Namely , given a large graph G = ( V; E ) we want to estimate as accurately as possible the num ber of triangles inciden t to every node v 2 V in the graph. The problem of computing the glob al num ber of triangles in a graph has been considered before, but to our kno wledge this is the rst pap er that addresses the problem of local tri-angle coun ting with a focus on the eciency issues arising in massiv e graphs. The distribution of the local num ber of triangles and the related local clustering coecien t can be used in man y interesting applications. For example, we sho w that the measures we compute can help to detect the presence of spamming activit y in large-scale Web graphs, as well as to pro vide useful features to assess con ten t qualit y in social net works.

For computing the local num ber of triangles we prop ose two appro ximation algorithms, whic h are based on the idea of min-wise indep enden t perm utations (Bro der et al. 1998). Our algorithms operate in a semi-streaming fashion, using O ( j V j ) space in main memory and performing O (log j V j ) sequen tial scans over the edges of the graph. The rst al-gorithm we describ e in this pap er also uses O ( j E j ) space in external memory during computation, while the second al-gorithm uses only main memory . We presen t the theoretical analysis as well as exp erimen tal results in massiv e graphs demonstrating the practical eciency of our approac h.
Luca Becc hetti was partially supp orted by EU Inte-grated Pro ject AEOLUS and by MIUR FIRB pro ject N. RBIN047MH9: \Tecnologia e Scienza per le reti di prossima generazione" . Paolo Boldi was partially supp orted by the MIUR COFIN Pro ject \Linguaggi formali e automi" and by EU Integrated Pro ject DELIS.
 H.3.3 [ Information Systems ]: Information Searc h and Re-triev al Algorithms, Measuremen ts Graph Mining, Semi-Streaming, Probabilistic Algorithms
Graphs are a ubiquitous data represen tation that is used to mo del complex relations in a wide variet y of applica-tions, including bio chemistry , neurobiology , ecology , social sciences, and information systems. De ning new measures of interest on graph data and designing novel algorithms that compute or appro ximate suc h measures on large graphs is an imp ortan t task for analysing graph structures that rev eal their underlying prop erties.

In this pap er we study the problem of coun ting the local num ber of triangles in large graphs. In particular, we con-sider undirected graphs G = ( V; E ), in whic h V is the set of nodes and E is the set of edges. For a node u we de ne S ( u ) to be the set of neigh bors of u , that is, S ( u ) = f v 2 V : e uv 2 E g , and let the degree of u be d u = j S ( u )) j . We are then interested in computing, for every node u , the num ber of triangles inciden t to u , de ned as: The problem of coun ting triangles also translates into com-puting the local clustering coecien t (also kno wn as tran-sitivit y coecien t). For a node u , the local clustering co-the num ber of triangles and the largest possible num ber of triangles in whic h the node could participate.

Note that the problem of estimating the overall (global) num ber of triangles in a graph has been studied already , see e.g. [2, 10]; here we deal with the problem of estimating the (local) num ber of triangles of all the individual nodes in the graph sim ultaneously .
We motiv ate our problem de nition by sho wing how the local triangle computation can be used in a num ber of in-teresting applications. Our rst application involves spam detection: we sho w that the distribution of the local clus-tering coecien t can be an e ectiv e feature for automatic Web-spam detection. In particular, we study the distribu-tion of the local clustering coecien t and the num ber of tri-angles in large samples of the Web. Results sho w that these metrics, in particular the former, exhibit statistical di er-ences between normal and spam pages and are thus suitable features for the automatic detection of spam activit y in the Web.

Next we apply our techniques to the characterization of con ten t qualit y in a social net work, in our case the Yaho o! Answ ers comm unit y. Follo wing a suggestion from the study of social net works in reference [32], that the type and qual-ity of con ten t pro vided by the agen ts is related to the de-gree of clustering of their local neigh borho ods, we perform a statistical analysis of answ ers pro vided by users, studying the correlation between the qualit y of answ ers and the local clustering of users in the social net work.

In addition to the ones we consider, the ecien t compu-tation of the local num ber of triangles and local clustering coecien t can have a larger num ber of other poten tial ap-plications, ranging from the analysis of social or biological net works [29] to the unco vering of thematic relationships in the Web [16].

For computing the local num ber of triangles we prop ose two appro ximation algorithms, whic h rely on well estab-lished probabilistic techniques to estimate the size of the intersection of two sets and the related Jaccard coecien t [6, 8, 9]. Our algorithms use an amoun t of main memory in the order of the num ber of nodes O ( j V j ) and mak e O (log j V j ) sequen tial scans over the edges in the graph.

Our rst algorithm is based on the approac h prop osed in [7, 8, 9], whic h uses min-wise indep enden t hash functions to compute a random perm utation of an ordered set. In our case, this is the (lab eled) set of nodes in the graph. In practice, to increase eciency , instead of hash functions we simply use a random num ber generator to assign binary la-bels to nodes. Doing this can in principle lead to collisions (i.e., we migh t have subsets of nodes with the same lab el). We pro vide a quan titativ e analysis of this approac h, char-acterizing the qualit y of the appro ximation in terms of the Jaccard coecien t and the role of collisions. A similar anal-ysis had been sketched in [6].

We then prop ose a second algorithm that main tains one coun ter per node in main memory|as opp osed to the rst algorithm, whic h requires one couter for eac h edge. In prac-tice, our second algorithm allo ws to perform the computa-tion in main memory , thus achieving a considerable speed up. In particular, the pro cessing time is almost halv ed, while the accuracy is still comparable or sometimes even better than the rst algorithm. This is achiev ed by using a new, simpler, linear function to appro ximate the Jaccard coe-cien t of two sets. As a theoretical con tribution, we assess the performance of this second algorithm in the framew ork used to analyze the rst one.

We supp ort our ndings and analysis by exp erimen tal re-sults. In particular, we use our algorithms to estimate the distributions of the num ber of triangles and of the clustering coecien t in medium and large samples of the Web graph. To the best of our kno wledge, this is the rst time ecien t (semi-streaming) appro ximation algorithms for coun ting tri-angles are describ ed.

The rest of the pap er is organized as follo ws. In the next section we review the related work and in Section 3 we intro-duce the mo del of computation and the notation that we will be using throughout the pap er. Section 4 describ es how to appro ximate the intersection of two sets using pairwise inde-penden t perm utations, as describ ed in [8]. Section 5 presen ts our rst algorithm, and Section 6 the main-memory-only al-gorithm. The last section presen ts our conclusions and out-lines future work.
Computing the clustering and the distribution of triangles are imp ortan t to quan titativ ely assess the comm unit y struc-ture of social net works [29] or the thematic structure of large, hyperlink ed documen t collections, suc h as the Web [16].
There has been work on the exact computation of the num ber of triangles inciden t to eac h node in a graph [1, 3, 25]. The brute-force algorithm for computing the num ber of triangles simply enumerates all j V j 3 triples of nodes, and thus it requires O ( j V j 3 ) time. A more ecien t solution for the local triangle coun ting problem is to reduce the problem to matrix multiplication, yielding an algorithm with running time O ( j V j ! ), where curren tly ! 2 : 376 [14]. If in addition to coun ting one wants to list all triangles inciden t to eac h node in the graph, varian ts of the \no de iterator" and \edge-iterator" algorithms can be used. A description and an ex-perimen tal evaluation of those \iterator" algorithms can be found in [30]; however, their running time is O ( j V j d O ( P v 2 V d 2 v ), resp ectiv ely. For the datasets we consider| very large num ber of nodes and high-degree nodes due to skewed degree distributions|suc h exact algorithms are not scalable, thus in this pap er we resort to appro ximation al-gorithms.

In [13] the authors prop ose a streaming algorithm that estimates the global num ber of triangles with high accuracy , using an amoun t of memory that decreases as the num ber of triangles increases. This result has been impro ved in [10]. We remark that, di eren tly from [13, 10], in this pap er we are interested in estimating the local clustering coecien t (and the num ber of triangles) for all vertices at the same time.

Min-wise indep enden t perm utations have been prop osed by Bro der et al. as a way to estimate the size of the intersec-tion of two sets and the related Jaccard coecien t. Together with the technique of shingles they pro vide a powerful tool to detect near duplicates in large documen t collections and the Web in particular [9, 6, 7]. Implemen ting min-wise in-dep enden t perm utations is infeasible in practice, since they require exp onen tial space [8]. In recen t years, families of linear hash functions have been prop osed that implemen t min-wise indep enden t perm utations appro ximately [24, 4]. As explained further in this pap er, in order to save com-putational time we do not use hash functions directly , but rather a pseudo-random generator. This can bring to colli-sions, but we sho w that their impact is negligible in practice.
The probabilistic estimation techniques we use have been considered in the past to solv e related problems. In [20], the authors use the techniques of shingles and linear hashing to disco ver subsets of Web pages that share signi can t subsets of their outlinks, thus extending and making the disco very of cyb er-comm unities in the Web computationally more e-cien t, in the spirit of [27]. Finally , in [19], the authors apply similar techniques to pro duce indices of page similarit y that extend SimRank [26]. Giv en the very large size of the data sets used in Web Information Retriev al, eciency considerations are very im-portan t. For concreteness, the total num ber of nodes N = j V j in the Web that is indexable by searc h engines is in the order of 10 10 [21], and the typical num ber of links per Web page is between 20 and 30.

This fact imp oses sev ere restrictions on the computational complexit y of feasible algorithmic solutions. A rst ap-proac h to mo deling these restrictions migh t be the streaming model of computation [23], whic h however imp oses limita-tions that are too sev ere for the problem at hand. Instead, we focus on building algorithmic solutions whose space and time requiremen ts are compatible with the semi-str eaming model of computation [17, 15]. This implies a semi-external memory constrain t [31] and thus re ects man y signi can t limitations arising in practice. In this mo del, the graph is stored on disk as an adjacency list and no random access is possible, i.e., we only allo w sequen tial access. Every compu-tation involves a limited num ber of sequen tial scans of the data stored in secondary memory [22].

Our algorithms also use an amoun t of main memory in the order of the num ber of nodes, whereas an amoun t of memory in the order of the num ber of edges may not be feasible. We assume that we have O ( N log N ) bits of main (random access) memory , i.e., in general there is enough memory to store some limited amoun t of data about eac h vertex, but not to store the links of the graph in main memory . We imp ose as a further constrain t that the algorithm should perform at most O (log N ) passes over the data stored on secondary storage.

For comparison, supp ose we want to measure the num-ber of triangles in a graph in a na  X  ve way. This would im-ply loading the lists of neigh bors of eac h node in the graph in main memory to be able to coun t the num ber of trian-gles directly . This would need O ( j E j log j V j ) bits of main memory whic h is impractical in general. As to this point, note that in man y data sets arising in practice, in particular some of the ones we consider in the exp erimen ts, we have j E j = ( j V j log j V j ).
Considered an undirected graph (possibly a symmetrized version of a Web graph) and a vertex u , denote by S ( u ) the set of u 's immediate neigh bors. Now notice that, for every edge uv 2 E , the num ber of triangles to whic h both u and v belong is j S ( u ) \ S ( v ) j . So, the overall num ber of As a result, the basic building blo ck of our approac h is an algorithm to estimate the size of the intersection of two sets.
In the next section, we revisit the general technique [8, 9, 6, 7] to estimate the Jaccard coecien t and thus the size of the intersection of two sets A and B , de ned over the same Collection Domain Year Nodes Edges WEBBASE-2001 various 2001 118M 1737M IT-2004 .it 2004 41M 2069M EU-2005 .eu.int 2005 862K 33M UK-2006-05 .uk 2006 77M 5294M
Answ ers social net 2007 6M 277M univ erse whic h we assume, without loss of generalit y, to be [ n ] = f 0 ; : : : ; n 1 g and where n = 2 k for some suitable k .
We ran most of our exp erimen ts on three medium-sized crawls gathered by the Lab oratory of Web Algorithmics, Univ ersit y of Milan (http://la w.dsi.unimi.it/); the graphs were symmetrized and loops were not considered in the com-putations. We used the WebGraph framew ork [5] to manip-ulate the graphs in compressed form. The particular col-lections we used are listed in Table 1. Note that, at least for some of the collections we consider, j E j is exp ected to gro w as ( j V j log j V j ). Furthermore, consisten tly with the empirical observ ations in [28], the average num ber of edges per node increases over the years. The dataset UK-2006-05 is the crawl that was lab eled by a team of volun teers for creating a Web-spam collection [11] so we have lab els of non-spam/spam for a large set of hosts in that collection. The distribution of the num ber of triangles in the smaller graph EU-2005 is sho wn in Figure 1 and follo ws a power law.
In addition to the graphs from web crawls, we also used a subgraph from Yaho o! Answ ers (http://answ ers.y aho o.com/), a question-answ ering portal. In the graph, eac h node rep-resen ts a user, and a link between two users indicates that one of the users has answ ered a question ask ed by the other user. In the system, users can choose among the answ ers receiv ed whic h one is the best answ er, and in the graph, we have iden ti ed the users who pro vide a high prop ortion of \best answ ers" to the questions they answ er. Figure 1: Distribution of the num ber of triangles in the EU-2005 graph.
Without loss of generalit y, we consider subsets of the uni-verse [ n ] = f 0 ; : : : ; n 1 g . We measure the overlap of two sets using the Jaccard coecien t: J ( A; B ) = j A \ B j j A [ B j
A very simple and elegan t technique to estimate the Jac-card coecien t has been prop osed in sev eral equiv alen t forms by Bro der et al. [6, 7, 8, 9]. Assume we are able to choose a perm utation ( ) mapping [ n ] onto itself uniformly at ran-dom. For every X [ n ], denote by ( X ) the set of the im-ages of elemen ts in X when ( ) is applied and let min( ( X )) denote their minim um. Then it can be sho wn [7] that ( i ) for every a 2 A [ n ], Pr [ a = arg min( ( A ))] = 1 = j A j ; ( ii ) for every A , B [ n ]: Pr [min( ( A )) = min( ( B ))] = J ( A; B ). This prop erty immediately yields a technique to estimate J ( A; B ) . The algorithm consists in performing m passes over the data. At eac h pass, one perm utation ( ) among the n ! possible ones is picked uniformly at random and then min( A ) is computed and compared with min( B ). Whenev er they matc h, a coun ter is updated. Let C m be the coun ter's value after m passes. Our estimation of J ( A; B ) is C m
Unfortunately , generating perm utations uniformly at ran-dom requires exp onen tial space [8]. In practice, suitable families of linear hash functions are used (e.g. see [24, 4]).
In this pap er, in order to increase the speed of computa-tion, we use a sligh t mo di cation of this approac h, simply assigning random lab els to the graph's vertices. As long as lab els are sucien tly random and collisions not too fre-quen t, we are able to appro ximate the Jaccard coecien t satisfactorily . In practice, we used the Mersenne Twister, a pseudo-random num ber generator, whic h is a fast gener-ation algorithm for obtaining high-qualit y pseudo-random num bers.

Figure 2 describ es the algorithm's pseudo-co de, whic h is exactly the standard one given for example in [7], except for the use of random lab els. As to the notation used in the pseudo-co de, l ( j ) is a k -bit integer lab el for every item j 2 [ n ] while, for A [ n ], L ( A ) = min j 2 A l ( j ). Require: sets A; B [ n ], integer m , k bits 1: for i : 1 : : : m do 2: For every j 2 [ n ], set l ( j ) to a value dra wn uniformly 3: COMPUTE L ( A ) AND L ( B ) 4: if ( L ( A ) == L ( B ) ) then 5: count count + 1 6: return estimate (count/(count + m ))( j A j + j B j ) Figure 2: Basic algorithm for the estimation of the intersection of two sets.

De ne the follo wing variables: W i = 1 if and only if, in the i -th iteration, L ( A ) = L ( B ) and W = P m i =1 Set X = j A \ B j . Our estimator of X is X = W= ( W + m )( j A j + j B j ). In fact, the lab eling step migh t assign the same lab el to multiple vertices. This means that, in eac h iteration of the algorithm above, the probabilit y that L ( A ) = L ( B ) is not exactly equal to J ( A; B ), as would be the case if we used min-wise indep enden t perm utations [8]. For the sak e of completeness, we sho w that, as long as lab els are reasonably random, the trivial lab eling scheme we use allo ws us to estimate J ( A; B ) with good accuracy , collisions having a negligible impact. This is stated in the next result, whose pro of follo ws the lines of those given in [9, 6, 7] and will be given in the full version of the pap er. We presen t this result here for the sak e of completeness, since it considers the role of collisions (an asp ect only sketched in [6]).
Theorem 1. For every &gt; 0 and for every numb er m of iter ations:
In practice, this result states that our estimation of j A \ B j di ers from the true value by more than a constan t fac-tor with a probabilit y that exp onen tially deca ys with m and J ( A; B ), while the worst case impact of collisions is summarized in the second term, whic h is o (1) as long as k = (log n + log m ), m typically being in the order of a few ten ths.

In the next section, we describ e how to apply the same techniques for estimating the num ber of triangles.
In this section we describ e an appro ximating algorithm for coun ting the num ber of triangles for eac h node in the graph. The idea is to compute an appro ximation T ( u ) of the num ber of triangles T ( u ) for all vertices in the graph.
The algorithm for computing the num ber of triangles is written in pseudo-co de in Figure 3 and explained in the next paragraphs. The notation used in the pseudo-co de is as follo ws: G = ( V; E ) is an undirected graph, S ( u ) is the set of neigh bors of vertex u , h p ( u ) denotes the random k -bit lab el for node u .
 Require: graph G = ( V; E ), num ber of iterations m , num-1: Z 0 2: for p : 1 : : : m do f This reads the graph 2 m times g 3: for u : 1 : : : j V j do f Initialize node lab els and min g 4: h p ( u ) k random bits 5: min( u ) + 1 6: for src : 1 : : : j V j do f Compute minima g 7: for all links from src to dest do 8: min( src ) min(min( src ) ; h p ( dest )) 9: for src : 1 : : : j V j do f Compare minima g 10: for all links from src to dest do 11: if min( src ) == min( dest ) then 13: for src : 1 : : : j V j do f Compute num ber of triangles g 14: T ( src ) 0 15: for all links from src to dest do 17: T ( src ) T ( src ) = 2 18: return T ( ) Figure 3: Algorithm for estimating the num ber of triangles of eac h node. The coun ters Z ; are kept on external memory and updated sequen tially .

The algorithm performs m passes. At the beginning of eac h pass p , a new random vector h p ( ) is created. Eac h pass consists of two reads of the graph. In the rst read of the graph, at eac h node we store the minim um lab el among those of the neigh bors of that node. In the second read of the graph, we chec k, for eac h edge, if the two minima at the endp oints of the edge are equal; in suc h a case, one coun ter Z ; for eac h edge is increased.

After the m passes, an estimation of the num ber of trian-gles of eac h node is computed as:
The algorithm is feasible because the coun ters Z uv , whic h mak e most of the memory usage, are accessed sequen tially and can be kept on secondary memory . The time complex-ity of the algorithm is O ( m j E j ). The main memory usage is O ( k j V j ) bits, basically for storing the node lab els and the minima; a natural choice for k is log( j V j ). The secondary memory usage is O ( j E j log m ) bits of temp orary space whic h is less than the space required to store the graph in uncom-pressed form. The space required in secondary memory is read and written sequen tially once for eac h pass.
The qualit y of the appro ximation only dep ends on local prop erties of the graph, and does not vary as the graph gro ws in size. In particular, every term in the sum above has an accuracy that is describ ed by Theorem 1, where A = S ( u ) and B = S ( v ). So, as stated in the previous section, the appro ximation impro ves with the num ber of passes, and it dep ends on the Jaccard coecien t so that for nodes with higher Jaccard coecien t the error is smaller.
 Remark. The value of m dep ends on the desired per-no de accuracy . As Theorem 1 sho ws, a value of m in the order of a few ten ths suces to satisfactorily estimate the size of the intersection of any two neigh bourho ods that overlap signi can tly.
We rst computed the exact num ber of triangles for a large sample of nodes in main memory . To do this, we pro ceeded by blo cks, keeping in main memory the neigh bors of a set of vertices, coun ting triangles, and then moving to the next blo ck of nodes. We did this for a sample of 4M nodes in eac h graph (except in the small one EU-2005 in whic h we were able to sample all the 800K nodes).

We use two similarit y measures: Pearson's correlation co-ecien t ( r ) and Spearman's rank correlation coecien t ( ) between the appro ximation and the real value. We also mea-sured the average relativ e error:
As a baseline appro ximation, we assume a constan t clus-tering coecien t C in the graph, kno wn in adv ance, and esti-For two of the metrics we use for measuring the qualit y of the appro ximation below, the value of C is not relev ant: Pearson's correlation coecien t assumes a linear relation-ship and Spearman's rank correlation coecien t is not af-fected by multiplicativ e factors.
 Next we computed the distribution using our algorithm. For a xed num ber of bits k , the accuracy of the appro xi-mation increases with the num ber of passes. In Figure 4 we sho w the error of these appro ximations in one of the Web graphs; the result for the other Web graphs in Table 1 are equiv alen t.

Already at 20 passes, involving only 40 sequen tial reads of the graph, the appro ximation has r 0 : 90 and 0 : 90.
Looking at Spearman's rank correlation, whic h is 0 : 85 with 50 iterations for our algorithm, we can see that the baseline algorithm pro vides a better appro ximation of the ordering of the nodes by num ber of triangles in IT-2004, EU-2005 and UK-2006-05. This fact indicates that the overall ordering is dominated by the degree of the nodes involved. However, the correlation coecien t of the baseline appro x-imation is very low (below 0.5, and below 0.1 in UK and WebBase) while the correlation coecien t of the prop osed algorithms is above 0.9.
 Remark. For the sak e of brevit y, we only men tion here that our algorithms sho w that the distribution of the num ber of triangles follo ws a power law, as sho wn in Figure 1. The same observ ation was also made in [16] for Web samples of smaller size.
This section describ es a mo di cation of previous algo-rithm that does not mak e use of external memory for the computation.

Observ e that, in the nal step of the algorithm presen ted in Section 5, we computed an estimation of the num ber of triangles of a node as: in whic h Z uv is the num ber of minima that were the same between u and v during the m passes, so 0 Z uv m .
To avoid the use of external memory , instead of keeping one coun ter for eac h edge, we can use one coun ter for eac h node, by appro ximating the num ber of triangles inciden t to a vertex u as: The algorithm that uses this appro ximation is given in Figure 5 and it is explained in the next paragraphs. The pro of that it estimates the triangle coun t with good accuracy is given in the next subsection.
 This algorithm is similar in spirit to the one sho wn in Figure 3, but remo ving Z uv from the denominator in the expression of T ( u ) allo ws to main tain one coun ter per node instead of one coun ter per edge. The algorithm does m passes, eac h pass consisting of two reads of the graph. In the rst read of the graph, at eac h node we store the minim um hash value of the neigh bors of that node. In the second read of the graph, we chec k, for eac h edge, if the two minima at the endp oints of the edge ( src; dest ) are equal, and if so a per-no de counter Z src is increased by j S ( src ) j + j S ( dest ) j .
After the m passes, an estimation of the num ber of trian-gles of eac h node is computed as:
The time complexit y of the algorithm is O ( m j E j ). The main memory usage is O ( k j V j ) bits, basically for storing the hash functions, minima, and the per-no de coun ters. Sec-ondary memory is accessed only to read the graph. rank correlation coecien t. Righ t: average relativ e error. Require: graph G = ( V; E ), num ber of iterations m , num-1: Z 0 2: for p : 1 : : : m do f This reads the graph 2m times g 3: for u : 1 : : : j V j do f Initialize node lab els and min g 4: h p ( u ) k random bits 5: min( u ) + 1 6: for src : 1 : : : j V j do f Compute minima g 7: for all links from src to dest do 8: min( src ) min(min( src ) ; h p ( dest )) 9: for src : 1 : : : j V j do f Compare minima g 10: for all links from src to dest do 11: if min( src ) == min( dest ) then 12: Z src Z src + j S ( src ) j + j S ( dest ) j 13: for u : 1 : : : j V j do f Compute num ber of triangles g 14: T ( src ) 1 3 m Z u 15: return T ( ) Figure 5: Algorithm for estimating the num ber of triangles of eac h node in main memory .
We can give a result similar to that of Theorem 1. Namely , for u; v 2 V , set X = j S ( u ) \ S ( v ) j and de ne W as in Section 4. In particular, W = P m i =1 W i , with W i = 1 if, during the i -th iteration of the algorithm, the if at line 14 of the algorithm of Figure 5 is true for nodes u and v . Finally , de ne We have Theorem 2.

Proof. For i = 1 ; : : : ; m , let E i = 1 if at the i -th it-eration there is more than one elemen t achieving the mini-mum, 0 otherwise and let E = P m i =1 E i . Also, set W ( i ) = ( W i j E = 0) and W = P m i =1 W ( i ). We have:
Set ^ X = ( X j E = 0). Since 0 E W m , it is easy to see that we have: We have: where the last inequalit y follo ws from the de nition of Now: where the inequalit y follo ws from the above given bounds on E h ^ X i in terms of X . Recalling that, by de nition, W ( j S ( u ) j + j S ( v ) j ) = (1 : 5 m ) we immediately have: The rest of the pro of now pro ceeds exactly as in Theo-rem 1.
 Remark. As to the choice of m , considerations analogous to those at the end of Section 5 hold.
In practice, we observ e that the second algorithm saves 40% to 60% of the running time. We ran the exp erimen ts for the large graphs in a quad-pro cessor Intel Xeon 3GHz with 16GB of RAM. The wall-clo ck times required for m = 50 iterations we observ ed were: Graph Nodes Edges (ext. mem.) (main mem.) WB-2001 118M 1.7G 10 hr 20 min 3 hr 40 min IT-2004 41M 2.1G 8 hr 20 min 5 hr 30 min UK-2006 77M 5.3G 20 hr 30 min 13 hr 10 min
The exp erimen tal results obtained sho w that, surprisingly , in man y cases the accuracy of the main-memory algorithm is even better than the algorithm that uses secondary memory . Figure 4 depicts the results for the case of IT-2004 (exp er-imen ts on the other datasets have been omitted for lack of space, but have essen tialy the same beha vior).

In the implemen tation, the num ber of bits necessary to store eac h coun ter dep ends on the num ber of iterations and on the link densit y of the graph. For instance, for WB-2001 we used a Java int (32-bits including the sign), but for IT-2004 and UK-2006, a long (64 bits including sign) was necessary to avoid over o w. We started to observ e over o w after 60 passes in IT-2004 and after 20 passes in UK-2006. We point out that this is indep enden t from the num ber of nodes in the graph.
An ecien t algorithm for local triangle coun ting is not only interesting as an algorithmic con tribution. This section describ es two applications of the algorithm for helping in information retriev al tasks in large graphs.
Spam and non-spam pages exhibit di eren t statistical prop-erties, and this di erence can be exploited for Web Spam Detection [18]. In this section we test if the num ber of tri-angles is a relev ant feature for this task.

We used the WEBSPAM-UK2006 spam collection [11], a pub-lic Web Spam dataset annotated at the level of hosts. First we computed the num ber of triangles for eac h host in this dataset and plotted the distribution for the non-spam and spam hosts. This is sho wn in Figure 6. A two-tailed Kolmo-goro v-Smirno v test indicates that both the num ber of trian-gles and the clustering coecien t have distributions that are substan tially di eren t in both classes: the larger di erences in the cum ulativ e distribution function plot are D = 0 : 32 and D = 0 : 34 resp ectiv ely.

We also compared the num ber of triangles and clustering coecien t with a kno wn set of link-based and con ten t-based features for the hosts in this collection [12]. We sorted all the features by computing the -squared statistics of eac h of them with resp ect to the class lab el. Using this ranking, the appro ximated num ber of triangles was rank ed as feature num ber 60 out of 221, and the appro ximated clustering co-ecien t as feature num ber 14 out of 221; suc h remark ably high positions mak e both features well worth being tested as part of a spam detection system.

To complemen t these results, we estimated the num ber of triangles at a page level , and considered the average and maxim um num ber of triangles in every host; in all cases we had to use the memory-based appro ximation algorithm to obtain the estimation, since an exact coun ting was in this case out of question. The results are sho wn in Figure 7. Also in this case, a two-tailed Kolmogoro v-Smirno v pro ved that the spam and non-spam distributions actually di er from eac h other: for example, the test in the case of average gave D = 0 : 09 with a p-v alue of 1 : 54 10 7 . Figure 6: Separation of non-spam and spam hosts in the histogram of triangles, measured using the exact algorithm (top), the appro ximated algorithm with 50 passes (middle) and the appro ximated algorithm in main memory with 50 passes (bottom).
In [32] it is sho wn that the amoun t of triangles in the self-cen tered social net work of a user is a good indicator of the role of that user in the comm unit y.

Here we perform an exploration trying to verify whether the qualit y of con ten t pro vided by a user in a social net-work is correlated with the local structure of the user in the net work. For our dataset, we use a social net work ex-tracted from the Yaho o! Answ ers site. Yaho o! Answ ers is a comm unit y-driv en kno wledge sharing system that allo ws users to ( i ) ask questions on any sub ject and ( ii ) answ er questions of other users. One notable characteristic of the system is that one answ er for eac h question is selected as the best answer , and one of the user attributes is the fraction of the best answ ers given by that user. Figure 7: Separation of non-spam and spam hosts in the histogram of triangles computed at page level and maximized/a veraged on eac h host.

We consider an undirected graph G = ( V; E ), where V is a set of users in the system, and an edge ( u; v ) 2 E denotes that the user u has answ ered a question posted by user v , or vice versa. For this graph we apply our coun ting algorithms and we obtain an estimate of the num ber of triangles at eac h node, as well as the local clustering coecien t. We focus on a small subset of randomly chosen questions and answ ers whic h have been lab eled by human judges as \high qualit y" or \normal" . These questions/answ ers have originated from a subset of about 9,500 users. Let H V be the subset of users who have pro vided a question or answ er of high qualit y in our sample, corresp onding to roughly 30% of the users in this case, and let N = V n H be the rest.

As a pro of of concept, we rst chec k if the fraction of best answ ers for the users di ers between the sets H and N . The two distributions are sho wn in Figure 8, in whic h one sees that users in the high qualit y set tend to have higher frac-tions of best answ ers. The two-tailed Kolmogoro v-Smirno v di erence of the two distributions is 0.26, and the null hy-pothesis is rejected with corresp onding p -value equal to 1 : 1
Next we explore the correlation of local structure in the user graph with resp ect to the lab eling of users in the classes H and N . In particular, we examine if the distribution of the num ber of triangles and the distribution of the lo-cal clustering coecien t di er between the sets H and N . The distributions in the case of the num bers of triangles are di eren t. The Kolmogoro v-Smirno v test rejects the null hypothesis with di erence value equal to 0.12 and p -value equal to 2 : 9 10 29 . Figure 8: Separation of users who have pro vided questions/answ ers of high qualit y with users who have pro vided questions/answ ers of normal qualit y in terms of fraction of best answ ers.

The distributions for the local clustering coecien t are sho wn in Figure 9. The separation in this case is better than with the num ber of triangles. In this case the Kolmogoro v-Smirno v di erence is 0.17 and the p -value for rejecting the null hypothesis is 7 : 9 10 54 . In general, the users in the set of high qualit y questions/answ ers have larger num ber of triangles and smaller local clustering coecien t. Figure 9: Separation of users who have pro vided questions/answ ers of high qualit y with users who have pro vided questions/answ ers of normal qualit y in terms of local clustering coecien t.

Notice that the partitioning of users into the sets H and N migh t not be very accurate since for eac h user there is usu-ally only one question or answ er that is evaluated. Thus, to obtain additional validation of our results we perform a second exp erimen t, in whic h we partition the users into two sets: H ba is the set of user who have fraction of best an-swers more than 30%, and N ba is the set of the rest of the users. Then, as in the previous exp erimen t, we examine if the distribution of the num ber of triangles and the distribu-tion of the local clustering coecien t di er between the sets H ba and N ba . For the num ber of triangles, the Kolmogoro v-Smirno v test rejects the null hypothesis with di erence value equal to 0.11 and p -value equal to 4 : 5 10 1 . The separation is again more clear for the case of local clustering coecien t. The Kolmogoro v-Smirno v di erence is 0.27 and the p -value for rejecting the null hypothesis is 1 : 8 10 59 . We remark that using only the degree of eac h user in the graph is not sucien t to distinguish between the two distributions.
We have presen ted ecien t semi-streaming algorithms for coun ting the local num ber of triangles in a large graph. To the best of our kno wledge, these are the rst suc h algorithms describ ed in the literature. We believ e that there are man y applications for suc h algorithms to Web-scale problems, and we have demonstrated two suc h applications.

For future work, exploring varian ts of the rst algorithm that relax the semi-streaming constrain t but still use a small amoun t of memory is promising. Giv en that the distribution of the num ber of triangles is very skewed, the coun ters Z could be compressed. For instance, if the coun ters follo w a power-la w, a suitable coding could be used to store them. Note that eac h coun ter will use a variable num ber of bits dep ending on the value being stored. This may cause a drop in performance if done in external memory , but could be a good choice if done in main memory .

Data and code. The data graphs we used in this pap er can be freely downloaded from http://webgraph.dsi.unimi.it/ ; the graph from Yaho o! Answ ers cannot be released publicly for priv acy reasons. The Java code used for computing all the estimations, implemen ting the algorithms we have de-scrib ed, is freely available under a GPL license at http://law.dsi.unimi.it/satellite-softwar e/ .
 Ackno wledgmen ts: we thank Massimo San tini and Se-bastiano Vigna for valuable commen ts and feedbac k about a preliminary version of this work.
