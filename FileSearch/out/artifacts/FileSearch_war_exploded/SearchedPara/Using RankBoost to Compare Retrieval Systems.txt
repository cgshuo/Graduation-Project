 This paper presents a new pooling method for constructing th e as-sessment sets used in the evaluation of retrieval systems. O ur pro-posal is based on RankBoost, a machine learning voting algor ithm. It leads to smaller pools than classical pooling and thus red uces the manual assessment workload for building test collections. Exper-imental results obtained on an XML document collection demo n-strate the effectiveness of the approach according to diffe rent eval-uation criteria.
 H.3.3 [ Information Search and Retrieval ]: Retrieval Models; I.2.6 [ Artificial Intelligence ]: Learning X  parameter learning Algorithms, Experimentation, Measurement Pooling, RankBoost, XML retrieval evaluation
For evaluating retrieval systems, relevance assessments o f test collections are essential. In order to reduce the manual wor kload, collection sampling -or pooling -of a limited number of docu -ments to be assessed is performed. In TREC conference, the po ol is created by gathering the top n documents (e.g. n = 100 the much longer list delivered by each participating system for each topic. The collected documents are then judged by humans, do cu-ments outside the pool are assumed non relevant. This docume nt subset is considerably smaller than the whole collection. H owever, as the document corpus becomes larger and larger, more robus t pooling strategies are necessary. This is particularly tru e for re-cently emerging information retrieval (IR) paradigms like XML IR or Web Topic Distillation where very few documents are relev ant due to the task nature.

This paper proposes a new pooling method based on RankBoost [ 2], a machine learning algorithm that selectively combines dif ferent Figure 1: MAP of systems on Depth-5 pool (left) and on RBoost-80 pool (right) vs on the full assessment set. Kendal l X  X  value is in parentheses. rankings. At the same performance level, it allows to use sma ller pools compared to the classical pooling method. The algorit hm has several properties which make it specially suitable for the sys-tem pooling task. First, RankBoost operates on relative jud gments, hence there is no need to normalize the relevance scales of th e dif-ferent retrieval systems. Second, it automatically select s systems to be combined and previous studies indicate that the chosen sy stems are highly independent one from the other. Finally, RankBoo st is easy to use in practice, the only parameter to be set is the num ber of boosting iterations. The proposed method is general enou gh to be used in any ad hoc retrieval, particularly in the aforementioned situations. We present here tests performed on the INEX XML c ol-lection. retrieval systems since 2002. Evaluation at INEX being perf ormed at the level of XML elements, the assessing procedure is part icu-larly heavy. This is a representative example which motivat es the development of efficient pooling strategies.

The INEX X 03 CO collection contains approximately 12,000 XM L documents with 32 judged topics and the pool was built from 54 participating systems. Since the assessment workload depe nds much more on the number of documents the assessor has to check than on the number of XML elements, the baseline pool is limited to around 500 XML documents per topic [3]. For the pooling tests , we consider here a document retrieval scenario where a docum ent is considered relevant if it contains a relevant element in t he INEX sense.

RankBoost pooling will be compared to the classical round ro bin pooling method which has been used in TREC and also adopted in the full assessment set (two horizontals correspond to  X  = 0  X  = 0 . 99 ); (c) size of group A calculated by Tukey grouping on arcsine-transformed data. The figures along the dot curves a re the n -values in Depth-n . The pool size is presented on the abscissa. INEX. TREC-style pool of the first n documents retrieved by each INEX submission will be denoted Depth-n . This pooling produces on average a set of m documents per topic. The corresponding pool consisting of the top documents extracted by RankBoost will be denoted RBoost-m . System performance is quantified by mean average precision (MAP).

A training step is needed for RankBoost. The learning data se t was collected from the Depth-5 pool. The average size of this pool-ing set is m = 80 per topic. We used the leave-one-out cross val-idation paradigm in evaluation: a model is trained with 31 to pics and tested with the remaining one. There are 32 models in tota l. The learning process is iterated for 1000 RankBoost rounds l ike in [2].

Figure 1 illustrates the scatterplots of system scores with the pooling methods Depth-5 and RBoost-80 compared to the score s calculated from the original pool. The scatterplot alone is how-ever not easily interpreted, we need more synthetic statist ics for that. Unfortunately, there is no standard way to assert the q ual-ity of a pool sample. Three indicators are taken into conside ration here. The first is the pool recall, that is the ratio of relevan t doc-uments found in the small pools wrt those in the original asse ss-ment set (Figure 2.a). The second is Kendall X  X   X  -the correlation value of system ranking produced by a small pool with that by t he original assessment set (Figure 2.b). Lastly, we use the dis crimina-tion power suggested in [4], namely the maximum number of top ranked systems which are not statistically different accor ding to a multicomparison test (Tukey test in this case). This quanti ty, called size of group A, is shown in Figure 2.c. The recall curves in Figure 2.a show the higher performance o f RankBoost for identifying relevant documents. This sugges ts that smaller pool size could be used with RankBoost compared to th e classical pooling method. We note that the improvement here is not as significant as reported in TREC circumstances [1]. The nat ure of data collections and the higher recall values of Depth-n p ools in our case may be the principal cause of the observed differenc e.
Figure 2.b reveals that the RankBoost system ranking is much more similar to the reference ranking than round robin ranki ng is. However, since Kendall X  X   X  averages all item swaps between the considered list and the reference ranking, this quantity al one does not provide enough information about system comparison. A s wap between adjacent systems is not important if their performa nces are not statistically distinct. It is however problematic i f there is a swap of two really different runs, especially if they are lo cated at the two ends of the ranking list. Deeper analysis in conjun ction with the use of scatterplots like Figure 1 indicate that cont rarily to the TREC case [1], there is no typical misranking associated with the top runs in the INEX collection for both pooling methods, even with small pools.

Figure 2.c shows another evidence in favour of RankBoost poo l-ing. For small pools, the group A is much larger for round robi n than for RankBoost. In other words, the latter produces much more discriminative pools than the former does.
Empirical results obtained on the INEX collection validate the feasibility of reducing the pool size, either by shallow TRE C-style pools or by the RankBoost method, the latter being the best in our experiments. Since the INEX collection is relatively small , the in-crease obtained with RankBoost is however limited. The expe ri-ments also show that the top runs are quite homogeneous and an y effective methods will probably have a similar behavior to t hese top runs. In order to enlarge the collection size for reliable sy stem eval-uation, our experimental results suggest adding more topic s rather than increasing the pool size. Finally, our proposal should still be validated on other collections with very different charact eristics (in terms of size, of relevant document proportion, etc) to confi rm its effectiveness in practice.
 Thanks to M.-R. Amini, N. Usunier for the source code of RankB oost, J. Blustein for IR-STAT-PAK, B. Piwowarski for proof-readi ng. [1] J. A. Aslam, V. Pavlu, and R. Savell. A unified model for [2] Y. Freund, R. Iyer, R. E. Schapire, and Y. Singer. An effici ent [3] B. Piwowarski and M. Lalmas. Providing Consistent and [4] J. Tague-Sutcliffe and J. Blustein. A statistical analy sis of the
