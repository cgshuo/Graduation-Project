 Based on a report by Australasian Corrosion Association (ACA), the Australian water industry faces many challenges, particularly in the areas of asset manage-ment of aging infrastructure and the training required to support the prevention of corrosion 1 . Failures of critical water mains typically bring severe consequences. Closing highways, massive street flooding and damages, traffic and giant sink-holes are just some of the impacts of water main failures. Based on official reports, these costs have been estimated at $91 million per annum to the Australian urban water industry. Moreover the total cost of corrosion in Australia is $982 million per year, which equates to an approximate annual cost of $60 for every adult in the country.
 The maintenance process is a very critical process in water main industry. The maintenance process starts by prioritizing high risk pipes. Then proceeding to physically inspect these assets to confirm their actual condition, before finally deciding on whether to renew them or not. Hence, forecasting and ranking the pipelines according to their risk (i.e., likelihood to fail) are critical to reducing maintenance and repair costs, to save the repair and/or rehabilitation time and for safety.
 Therefore, it is very useful to be able to rank accurately the water pipelines to find the most risky pipelines in each year. However, ranking water pipeline data has shown mixed results due to the characteristics of water data. On one hand, the data is sparse so that most pipes do not fail or fail just once during the observation period. On the other hand, the data is heterogeneous which means that their attributes have different types and/or scales and they are coming from different sources. In this paper, we consider the ranking problem in a pointwise manner. It is viewed as a regression or classification problem of predicting the specific relevance score for each category. In such cases, where the data is large, imbalanced and heterogeneous, learning a distance as a crucial step in classifi-cation is a very challenging issue.
 Motivated by this problem, a lot of research has been done to identify similari-ties/distances between instances by proposing efficient metric learning techniques determines the importance of different features and their correlations. However, when the problem is more complex, like in the water pipeline case, a global met-ric may not be able to fit the distance over the data manifold very well [ 1 ]. Local metric learning methods address this problem by learning a metric on each neigh-bourhood, or learning different metrics across the space [ 2 , 18  X  20 ]. Although local metric methods outperform global ones in many complex cases, they are prone to overfitting when they learn one metric per instance or when they learn completely independent metrics for different areas in data space. This overfitting problem has been addressed recently, by considering the rela-tionships between the different metrics learnt [ 13 , 16 ]. However, these proposals still learn different metrics for different chosen areas, and their performance still depends on the way that the most appropriate areas cover the different data dis-tributions are chosen. Secondly, same as the most local metric learning methods, they come with high CPU and memory costs [ 1 ].
 Based on these observations, we exploit the properties of fuzzy c-means (FCM) clustering technique to learn one metric per cluster and parameterize the metrics by considering the membership degree of each instance for different clusters. This process will overcome the overfitting problem by parameterizing the metric matrix of each instance as a linear combination of basis metrics of cluster centres (as mentioned in [ 16 ]). To further speed up the process, ball trees are used to search for target neighbours and imposters that will be described later. As shown by experiments, the proposed fuzzy based local metric learning method is faster than the state-of-the-art local.
 not spherical or/and when the similarities are not linear [ 5 ]. It is also practically desirable for a metric learning method to support heterogeneous types of data like water pipeline data. Most metric learning methods do not address these issues [ 4 ]. Therefore, in this paper, a new non-linear framework is proposed to enhance FCM with kernel density functions. In this framework, all the features are mapped into a non-linear feature space based on their density quantities, so that it can handle heterogeneous datasets with numerical, categorical and mixed features. We call our approach: kernel density fuzzy-based metric learning (KDFuzzyML). Wang et al. [ 17 ] presented a robust statistical analysis by developing five multiple regression models and applied them on 15-year water pipe failure data of a Canadian water distribution network. To develop regression models, this research focused on individual water pipes rather than similar groups of pipelines and considered a wide variety of factors that may affect the break rate. Tebesh et al. [ 14 ] presented two failure prediction models based on artificial neural network (ANN) and neuro-fuzzy systems to improve the accuracy of water pipeline failure rate prediction. The proposed models are applied to real data of one of the water distribution networks in Iran. The results showed that failure prediction using ANN model is more accurate and realistic than using neuro-fuzzy and multivariate regression models.
 prediction to estimate the rate of failure and find the best time to replace the pipelines. They tested the performance of six ANN models constructed using a cross-validation approach on an urban water distribution system in France. This study approved the efficiency of using the ANN approach to make a decision on the maintenance of water mains to reduce water pipe breaks.
 area. Four different models including a heuristic model, a na  X   X ve bayesian classifi-cation model, a logistic regression based model and a probabilistic model based on non-homogeneous poisson process (NHPP) have been developed. The pro-posed models have been used to rank the failure probability of some of the pipes which are expected to fail in the future. The models were applied to six various datasets from three different water distribution networks. The results of compar-ing these four models showed that NHPP-based model outperforms other models in terms of the prediction of the average number of failures for each pipe each year. However, it is not possible to select a model that is superior in terms of its ranking ability. In fact, different models had different but reasonable results on different scenarios.
 and proposed a model to help decision makers to have a cost-effective plan for water pipe maintenance. They first developed a pipe break prediction model based on the failure data of the Beijing water distribution network between 2008 and 2011 using genetic programming and then set up an economically efficient pipe replacement model.
 Recently Li et al. [ 9 ] proposed a non-parametric model using hierarchical beta process (HBP) to enhance the performance of pipe condition assessment. They addressed the limitations of parametric models since they usually have a fixed model structure based on a set of assumptions on the data behaviour. Prioritization of risky pipelines can bring significant savings to water utilities. Hence in this paper, a local metric learning based model is presented to further increase the accuracy of this process to avoid critical main failures and also to avoid replacing any pipe that is still in working condition. To the best of our knowledge, there is no work in literature that has used metric learning based method to assess water pipeline conditions. 3.1 Data Collection We use the data from two different Australian regions. Region A is a highly urbanised commercial area, and Region B is a lower density suburban area. We have access to two datasets for each region. The information of pipelines in each area is accessible through the first dataset. Attributes include: identifica-tion number, laid date, length, material, diameter size, location, protective coat-ing and surrounding soil type. The second dataset contains the failure records between 1998 and 2012. The failure information includes failure date and time, failure type, failure location, etc.
 In these regions, the oldest pipes were laid from 1800, with an average age of 60 years for Region A and 38 years for Region B . The fourteen years observation period was relatively short compared to the life cycle of water pipes such that most (about 99 %) pipes have not failed or failed just once during the observation period Hence the data is sparse or imbalanced in the other words. The soil types were similar for all the water mains belonging to the same region. Region A and B have 14 , 765 and 17 , 877 water pipes and 1 , 696 and 6 , 672 failure records respectively. 3.2 Fuzzy-Based Local Metric Learning In this paper, we focus on supervised learning. We assume that we have a labelled dataset S =( x 1 ,y 1 ) , ( x 2 ,y 2 ) , ..., ( x n ,y n ), where x and y i  X  X  are class labels from a set of c =1 , 2 , ..., C . First, we assume that all the features are numeric. Mahalonobis distance between two instances is given by: A linear metric learning method learns the positive semidefinite metric matrix M , based on some constraints. The most popular one is the large margin nearest neighbour method that is based on triplet constraints. Considering a triplet constraint ( x i ,x j ,x l ), in a projected space based on M , the distance between x i and x j will be smaller than the distance of x i and x l problem, like when the data is large and heterogeneous, learning a single metric will not be able to model complexity of the problem. To address this problem, local metrics are learned for each learning instance or set of learning instances. Therefore, in this paper, we will learn a set of metrics for a set of instances defined by FCM clustering method. A local metric function can be approximated by a linear combination of weighted metrics { M 1 ,M 2 , ..., M of each instance x i is parameterized by M i = w is a n  X  k matrix where its w ik entry is the membership degree of instance x to the cluster k .The w ik is calculated as: where ( z k ) is centre of the k th cluster.
 degree w ik . Although this metric is used to calculate the distance of x other instances, the metric parameterization avoids overfitting. We use a similar objective function as in [ 16 ] to learn the basis metrics Speed up Using Ball Trees. Storing data samples in a hierarchical structure can make the nearest neighbour search faster. Ball trees are one of the hierarchi-cal structures that have been used by Liu, et al. [ 10 ]forfast k -nearest neighbours ( k NN) search. Ball tree partitions data points into a set of D -dimensional hyper-spheres called balls. Each ball contains a subset of data points. The distance of each data sample to each ball is easily computed. The distance of a test point x t to any trained point x i in a ball tree S , has a bound as follows: where c s is the centre and r s is the radius of the ball s.
 training samples of each class in each cluster. The created ball trees are used to search for target neighbours and the imposters. Searching for target neighbours and imposters are the most time consuming processes in the proposed method. Note that for each data sample x i , the imposters are data samples with different class labels that are closer to the x i than its target neighbours. The target neighbours are the k nearest neighbours of x i with the same class labels. If x the k th closest point of x t and its distance to the x t is less than the bound from Eq. 4 , all the training samples inside the ball can be left unexplored. 3.3 The Proposed Kernel Density-Based Fuzzy Metric Learning This subsection is our main contribution in this paper where we propose a new fuzzy clustering method based on the kernel density and show how to learn the local metrics for the data in the new non-linear feature space. First, we need to know how to calculate the distance between instances in the new space. Considering two inputs x i and x j , Euclidean distance after mapping the inputs to the density probability space  X  ( x )is: And Mahalonobis distance is formulated as d 2 ( x i ,x  X  ( x )) T M (  X  ( x i )  X   X  ( x j )), where M is a positive semidefinite matrix that we are trying to learn in different areas of the space.
 Based on FuzzyML framework, we should first cluster the data using fuzzy clustering technique. Hence, we need to know how to calculate the cluster centre and the membership degree matrix. Let the cluster centre be Z [ z , ..., z k,d ] , 1  X  d  X  D, for clusters 1  X  k  X  K, where each attribute of Z is a vector of probabilities z k,d =[ z k (1 ,d ); ... ; z calculated as: More formally, each feature value of a cluster centre is a vector of conditional probability densities. Each item of the vector will be mean of the feature con-ditional probability density of all the points, weighted by degree of belonging of the points to the cluster. With fixed centres, weight matrix w is updated as Eq. 2 where: We call this clustering approach as kernel density fuzzy c-means (KDFCM). After clustering, all the basis metrics are learned. Considering U as dual forms of M k and  X  ijl in Eq. 3 , the dual formulation of objective function is obtained by substituting the following expression into the Lagrangian form of the objective function: where this expression is obtained by setting the first deviation of Lagrangian of the objective function over M k and  X  ijl to zero. Hence the dual formulation is defined as: where A ij is equal to the outer product of each instance pair, and G x ij x ij . The outer product of each pair x i ,x j in the new feature space should be calculated as: where x ( c, ij ) is a vector of x ij over all the D dimensions. x vector of concatenation of all the D ( x i,d  X  x j,d )whereeach( x vector of concatenation of all the C ( P d ( c, x i,d )  X  calculated as: Eq. 9 can be simplified to: Defining a symbol A =  X  2 which is the positive part of A . So we will have: U  X  k =( A ) We conduct extensive experiments to evaluate the proposed method and com-pare our results to the competing methods for predicting water pipe failures. and non-failed pipelines. Table 1 depicts the results of applying different methods for classifying these water pipes. We observe that KDFuzzyML has the lowest error rate (7 . 94 %). The water pipeline dataset is very imbalanced, so FScore and GMean values are better indictors than error rate. The methods with higher Pre-cision, Recall, FScore and AUC have better performance. As shown in Table 1 , k NN has the worst performance on the water pipe data. After that, BoostMetric is the second worst. Overall, our proposed method has the least error rate and is also at least slightly better than other methods based on all measurements. Since identifying every water pipe failure (in advance) saves lots of money, according to this experiment, our method is the best choice for pipe classification in the water pipeline condition assessment application.
 KDFuzzyML outperforms other methods in water pipe classification as shown in Table 1 . Next, we need to rank the problematic water pipelines according to their likelihood to fail, so inspectors can inspect the more problematic pipelines according to their urgency and available resources. The pipelines that are in ceptible pipelines using KDFuzzyML and two other popular survival analysis methods, Weibull [ 6 ]andHBP[ 9 ], using some real water pipeline data from two Australian regions. We choose Weibull, as a baseline for comparison, and HBP as a non-parametric method that has been very recently proposed for water pipeline condition assessment. The results are shown in Fig. 1 . Y-axis indicates the percentage of susceptible pipelines actually found, and x-axis indicates the amount of inspection carried out in the water pipeline network in the region. It shows that KDFuzzyML allows more accurate, targeted inspection on susceptible pipelines than HBP and Weibull. For example, by following KDFuzzyML rank-ing of the susceptible pipelines in Region A, inspectors can identify more than 60 % susceptible pipelines by inspecting less than 20 % of the water pipelines in the region. When using the other two methods, they will need to inspect more than 40 % and 70 % of the pipelines, respectively, to achieve the same result. This is practically very significant, as a huge amount of resources (esp. money) can be saved for water pipeline maintenance.
 Similar as Fig. 1 , Table 2 uses different measurements to compare the per-formance of KDFuzzyML, Weibull and HBP use the real water pipeline data. As shown, for example, using AUC, KDFuzzyML is at least 20 % better than Weibull and HBP.
 To investigate the significance of the results shown in Table 2 , Friedman and Nemenyi tests [ 3 ] have been chosen to further analyse the results. Friedman test is used to compare different classification results according to their similarity. Fried-man test, as a non-parametric test, is more suitable than parametric tests when more than two classifiers are involved [ 3 ]. Friedman test compares the average ranks of algorithms under a null -hypothesis. The null -hypothesis assumes that all the algorithms perform the same and hence their output ranks should be the same. Table 3 (a) presents the results of Friedman test and hence we reject the hypothesis, i.e., the algorithms do not perform the same. Friedman test has been performed considering the results of 10 times performing all three methods on water data in both regions. The  X  2 F values namely Chi-Square more than 0 with p&lt; 0 . 05 for all the performance diagnostics suggest that there are significant differences between performance measurements values of these three methods (KDFuzzyML, HBP and Weibull). In other words, these results illustrate that even if we perform these methods several times on different data samples, the performance will not be similar and their performances can be ranked.
 Nemenyi tests are used here to show the difference between each pair of the algorithms and rank the methods based on their performance. In other words, Nemenyi test is a post-hoc test for pairwise comparisons (Table 3 (b)) that are used after Friedman test. Nemenyi test is to test the same hypothesis as Friedman X  X  but just between two methods at a time. As shown in the compari-son between KDFuzzyML and HBP in Table 3 (b), critical value ( q )morethan0 with p&lt; 0 . 05 rejects the null -hypothesis. This means that KDFuzzyML performs better than HBP on all measurements. Similarly, as shown in the comparison between KDFuzzyML and Weibull in Table 3 (b), q more than 0 with p&lt; 0 . 05 implies that KDFuzzyML outperforms Weibull. The higher the critical value is, the bigger difference between their performances is. For instance, for AUC, the critical value from Nemenyi on KDFuzzyML and Weidbull is 8 . 65, which is larger than the critical value from Nemenyi on KDFuzzyML and HBP (i.e., 7 . 98). This implies that for AUC, KDFuzzyML performs the best, followed by HBP and then Weidbull. This is consistent with the results shown in Table 2 . In addition, the scalability of KDFuzzyML is shown by experiments on water pipeline datasets with different sample sizes. Table 4 shows the speed (in CPU time) of different algorithms on water pipeline datasets with different sizes. For every experiment, 10-fold cross validation is performed and average over 10 runs is obtained. Table 4 is divided into 3 sections. Each section shows the perfor-mance of one algorithm on various sizes of the water pipe dataset. Based on the results, the proposed KDFuzzyML works much faster than LMNN on the water pipeline dataset with any sample size. For example, it takes just 755 s for the proposed KDFuzzyML on the dataset with 350000 samples, whereas it takes more than a day for LMNN to finish the computation. Even when the sample size is less, like 15000 samples, KDFuzzyML is still about 200 times faster than LMNN with the same error rate. Hence, it can be concluded that the proposed KDFuzzyML is scalable to the dataset with hundred thousands of data samples. This is practically very useful for the task of water pipe condition assessment. tasks, different datasets are employed for comparisons. All the selected data sets have been collected from UCI Machine Learning Repository 2 iments have been done 10 times using 10-fold cross validation on each data KDFuzzyML is able to handle the categorical features by its nature. For all other techniques, we mapped all the categorical values to binary numbers. For example, for a categorical feature with n different values, all the values are transformed to a n bit binary. For each particular value, 1 digit will be equal to 1 and all the other digits will be equal to 0. Table 5 shows that the proposed KDFuzzyML method has a comparable performance against other popular metric learning methods on various heterogeneous data sets. We observe that KDFuzzyML has the lowest error rate and higher precision and recall on all the data sets. We have proposed an efficient local metric learning approach to predict and rank water pipelines according to their likelihood to fail. Compared to traditional sta-tistical modelling approaches, metric learning based methods are more flexible and able to adapt to the data complexity. When the data is large (heterogeneous and possibly imbalanced), global metric learning methods do not perform well in most cases. While local metric learning techniques perform better in those cases, they are expensive in terms of time and space. In this paper, we have proposed a local metric learning called KDFuzzyML that uses fuzzy clustering to make it fast and yet with at least similar performance as other metric learning methods. Experiments have shown that our proposed approach outperforms pre-vious parametric and nonparametric approaches for water pipe condition assess-ment. In practice, this represents major financial savings through more targeted inspections.

