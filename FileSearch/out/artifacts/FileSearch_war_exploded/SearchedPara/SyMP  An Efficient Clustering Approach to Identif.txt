
There exist a plethora of clustering algorithms which have been formulated differently depending on the research area where they have originated such as statistics, pattern recog-nition, optimization, neural networks, and graph theory. Unfortunately, only few of these algorithms can cluster large data sets. In Ill], Kaufman and Rousseeuw proposed CLARA, which is based on finding k medoids. Ng and Han [14] pro-posed a variation of CLARA called CLARANS, that makes the search for the k medoids more efficient. The Scalable K-Means[2, 6] and Scalable EM[3] are two other scalable parti-tional algorithms. DBSCAN [5] is a density-based algorithm that was developed to identify arbitrarily shaped clusters. DBSCAN is not suitable for high dimensional data since the intrinsic structure of all clusters cannot be characterized by global density parameters. Other density-based algorithms include DBCLASD[20], a density-based algorithm that as-sumes that points within a cluster are uniformly distributed, STING[19], an enhancement of DBSCAN, WaveCluster[18], a method based on wavelets, and DENCLUE[10] which uses influence functions to model the points density. CLIQUE[l] is a region-grouping algorithm that can find clusters em-bedded in subspaces of the original high dimensional data space. BIRCH[21] and CURE[8] are two hierarchical algo-rithms that use region grouping techniques. BIRCH first performs a pre-clustering phase in which dense regions are identified and represented by compact summaries. Second, it treats each of the sub-cluster summaries as representative points and applies an agglomerative hierarchical clustering. 
CURE represents each cluster by a given number of "well-scattered" points to capture its shape and extent. During the merging step, the chosen scattered points are "shrunk" towards the centroid of the cluster, and the clusters with the closest pair of representative points are merged. The mul-tiple representative points allow CURE to recognize non-spherical clusters, and the shrinking process reduces the ef-fect of outliers. To handle large databases, CURE employs a combination of random sampling and partitioning. 
In [15, 7], we introduced a model that combines synchro-nization and clustering concepts and resulted in a clustering approach that is efficient, robust, and can cluster very large data sets. This approach is described in the next section. 
Let ~ = {YsIJ = 1,.-. , N} be a set of N objects, where each object, yl E R p, is characterized by p attributes. We represent each yi by an Integrate-and-Fire (IF) oscillator (Oi) which is characterized by a phase  X i and a state variable xi, given by 
The function f : [0, 1] ~ [0, 1] is smooth, monotonically increasing and satisfies f(0)=0 and f(1)=l, b is a constant (usually=2) that measures the extent to which f is concave down. Whenever xi reaches a threshold at xi = 1, the i th oscillator fires and  X i and xl are instantaneously reset to zero, after which the cycle repeats. As a consequence, the phases of all other oscillators Oj (j ~ i) will change by an amount e~( X s) , i.e., where B is a clipping function that is used to guarantee that xs(t ) is confined to [0,1], and ei( X j) is a coupling function In (3), dis is the relative dissimilarity measure between Oi and Os, CE and Cl are the maximum excitatory and in-hibitory coupling. If dis &lt; 60, then oscillators Oi and Oj are considered similar, and the coupling would be excita-tory. On the other hand, if dis &gt; 50, then oscillators Oi and O s are considered dissimilar, and the coupling would be inhibitory. The parameter 60 can be regarded as a resolu-tion parameter. 61 (typically 61=2x50) is a constant that is used to indicate that if an oscillator is too far, then it should simply be maximally inhibited. 
The Self-Organization of Oscillators Network (SOON) is summarized at a high level as follows: 1 Whenever a given oscillator reaches the threshold, it ex-cites similar oscillators and inhibits dissimilar ones. 2 Initially, oscillators begin to clump together in small groups. 
Within each group, oscillators fire simultaneously. As the system evolves, new groups keep forming, and existing groups keep getting bigger. 3 The system will reach a stable state where the N oscillators are organized into C stable groups. Oscillators belonging to different groups are phase-locked, and oscillators belonging to the same group are synchronized and form a cluster. 
In [7], we integrated the Mahalanobis distance into SOON to identify hyper-ellipsoidal clusters. The distance between two oscillators (i.e., clusters), Ok and Ol, is computed using where (cl X , Ck) and (c~, C~) are the center and covariance matrix of oscillators (or group of oscillators) Ok and Ol. If the clusters are assumed to come from a multivariate Gaus-sian distribution, then d~ has a X 2 probability distribution with p degrees of freedom. This desirable feature allows us to (i) automate the choice of the resolution parameter 60; (ii) make the neighborhood of the excitatory region dynamic and cluster dependent. We use that is, the a tu percentile of the X~ probability distribution, 
Initially, each data point is represented by one oscillator and constitute a cluster by itself. The initial center of the oscillator (i.e., cluster) is the point itself, and the covariance matrix is initialized to ff  X  Ip X p, where Ip X p is the identity matrix, and ~ is a constant that depends on the dynamic range of the data. Whenever a group of oscillators syn-chronizes and result in a group k, the center and covariance matrix of this group are updated using the features of the synchronized oscillators. 
During the evolution of SOON, once a group of oscillators synchronize they will share the same center and covariance matrix until the algorithm reaches a stable state. Thus, a group of synchronized oscillators can be treated as a single oscillator. ScaleSOON[7] was designed to exploit this fact and efficiently cluster huge data sets while operating within a limited memory (RAM) buffer in one scan of the data set. ScaleSOON, is outlined as follows: 1 Get a sample from the data, and fill memory buffer. 2 Apply SOON to the data contents in the buffer. 3 Identify synchronized groups, summarize each group by a single oscillator, and purge synchronized oscillators. 4 If there are any points that have not been previously loaded, go to step 1. 
Let G be a group of synchronized oscillators. If the dis-tance defined in (4) is used, then the sufficient statistics for G are the triplet (N~, Sc, SSG), where Nc is the number of oscillators in group G, Sa=~yec y, and SSck =~yeC yyT. 
Initially, each feature vector (or oscillator) yj has its initial sufficient statistics, i.e., (Nu~,Su~,SSu~) = (1,yj,y~y~). 
After performing SOON on the data set resident in the mem-ory buffer, each group of synchronized oscillators, G~, will be summarized by an oscillator that has the following suffi-cient statistics: Nak = Z Nu; Sak = Z Sy; SSak = Z SS~. (6) 
Next, oscillators that belong to Gk are purged from the memory buffer. In (6), y can represent a single oscillator or a group of oscillators that have been synchronized and sum-marized in previous iterations. Finally, the memory buffer is filled with feature vectors that have not been previously loaded. The new oscillators that summarize groups of syn-chronized oscillators will inherit the phases of the groups they are summarizing. This is one important feature of our incremental approach since information accumulated in the phases will be propagated as new data gets loaded. 
To identify clusters of arbitrary shapes, we propose mod-eling each cluster by multiple prototypes, where each pro-totype (center and covariance matrix) represents one Gaus-sian component. Our choice for Gaussian components, as opposed to multiple point representatives (as in [8]) is mo-tivated by two factors. First, the number of prototypes re-quired to model each cluster can he automatically deter-mined using the X 2 probability distribution of the distances within each Gaussian component. Second, fewer Gaussian components than spherical components (single point) are needed to model complex shapes. The Synchronization with Multiple Prototypes (SyMP) algorithm is an extension of 
SOON and involves the following modifications. 
Let ,k' = {yl,'-. ,YM} be the set of oscillators that are being processed by SyMP at a given iteration. Each yi can represent a single oscillator or a group of oscillators that have been synchronized and merged in previous iterations. 
We assume that each cluster yi is represented by ki Gaus-sian components {yi 1 k~ ,'" , Yi }, where ki is unknown. Each 
Gaussian component y~ has a center c~ and a covariance matrix C~. The Multiple Prototype distance, d~tp, between two clusters y~ and Ym is computed using where d~(yt, Ym) is a function that measures the distance between two hyper-ellipsoidal components. The distance in (7) is similar to the single linkage hierarchical[17] and CURE [8]. The difference is that in traditional hierarchi-cal algorithms, the distance is computed between pairs of data points, in CURE, the distance is computed between few selected representative points, and in (7) the distance is computed between few Gaussian representatives. 
There are several measures that can be used to assess the similarity between two hyper-ellipsoidal components. In this paper, we report the results using a variation of the Maha-lanobis distance. This choice is motivated by the computa-tional simplicity and the ability to find the optimal number of components needed to model each cluster in an unsuper-vised manner. We use a distance that is similar to the one used in [7] (see equation (4)). Instead of taking the mini-mum of the Mahalanobis distance between cluster yi and the center of cluster yj and the Mahalanobis distance between cluster yj and the center of cluster yi, we take a convex combination of the two distances. We use is the Mahalanobis distance between the i th Gaussian com-ponent of cluster l and the center of the jth component of cluster m. In all experiments we fix/~ to 0.98. 
Let y = {Yl,..' ,yT} C ,~' be the subset of T oscilla-tors that have synchronized in the current iteration. Each yl has kl prototypes 1 . {Yi," " ,Y~}. The T oscillators are then merged into one cluster, say y,~, with km prototypes computed using the prototypes of the synchronized oscilla-tors. These km prototypes are selected to be well scattered and to reflect the shape of the merged clusters. We use the following procedure. The first component is selected as the center of all synchronized components. Then, we repeatedly select the furthest (using the distance in (8)) component from the already selected components. This process is ter-minated when the maximum number of allowed components (CMAx) is reached or when the furthest component has a distance less than 6~c. This condition ensures that all se-lected components have a pair-wise distance larger than 6we. 
We refer to 5~c as the within-cluster (or intra-cluster) reso-lution, and we set it using the X 2 probability distribution of the distances within each component. That is, 
In other words, if the probability that the furthest compo-nent belongs to an already selected component is greater than c~, then there is no need for an additional com-ponent. This idea is similar to the Minimum Description Length (MDL) principle I161. 
We should note here that 6w~ is different from 60 used in the coupling function in (3). 60 is needed to decide if two clusters (as opposed to two components within a cluster) are similar and if they should excite or inhibit each other when one fires. We will refer to 60 as the inter-cluster resolution or 6i,~t~,. and rewrite equation (5) as 509 to make this distinction clear. We require that 5~o~&lt;6i~t~ (or o~, X &lt;ai,~t,,~). When ~o~=~int,,-, then SyMP is equiva-lent to SOON and only one component would be selected to model any cluster. The SyMP algorithm is summarized below Initialize phases  X i randomly for i = 1,... , N; Each point is a cluster with I Gaussian components; Repeat 
Until (Synchronized groups stabilize); 
SyMP can be extended to handle very large data sets using an incremental approach similar to the one used to extend 
SOON. That is we proceed by loading only a sample from the data set, apply SyMP, summarize and purge synchro-nized groups. In ScaleSOON, each cluster C was modeled by one Gausian component and its sufficient statistics are the triplet (NG,Sc,SSc). ScaleSyMP on the other hand uses k Gaussian components to model each cluster. Thus, the sufficient statistics of each synchronized group would be the set (Nb,S~,SSb,... ~ k , NS, So, SSc). 
We evaluate the performance of SyMP with several syn-theticMly generated datasets. We demonstrate the ability of SyMP to identify clusters of arbitrary shapes in a com-pletely unsupervised fashion. Since CURE was shown to outperfom BIRCH and MST when the clusters do not have spherical shapes [8], we only compare the performance of 
SyMP to CURE. We use the basic algorithm without the random sampling. The random sampling scales-up CURE but introduces other parameters and can degrade the perfor-mance when the clusters' sizes vary significantly. Moreover, the random sampling technique is generic and can be used by SyMP as well. For all experiments shown in this section, we use the incremental version, i.e. ScaleSyMP. However, to provide a fair comparison, when we compare the exe-cution time with CURE, we use smaller data sets and the non-scalable SyMP. Unless stated differently, the parame-ters are set to their default values specified in Table 1. All experiments were performed on an Ultra Sparc IIi 300 Mhz workstation with 256 MB RAM. The data sets used in this experiment are shown in Fig. 1. 
For illustrative purpose, we use 2-dimensional data sets so that we can display the clusters' representation. The image containing DS1 is generated at four different resolutions and Symbol ~inter (11) resulted in data sets that have 10454 (DSI-a), 41843 (DS1-b), 107343 (DSI-c), and 199003 (DSI-d) data points. These data will be used to illustrate the scalability of our approach. DSI-a will also be used to compare the execution time of SyMP and CURE. DS2, DS3, and DS4 have 46028, 10007, and 32850 data points respectively. The results of clustering DS1 is shown in Fig. 2, where SyMP identifies the 5 clusters correctly. Each cluster is mod-eled by a different number of Gaussian components depend-ing on its complexity. For instance, the spherical and ellip-soidal clusters are modeled by a single component, while the "W" shaped cluster required 10 components. Fig. 2(b) dis-plays the results using CURE when the number of clusters is specified as 5 and 10 representatives per cluster were used. As can be seen, the partition is not correct, as the "crescent" shaped cluster grabs part of the "W" shaped cluster. This is because 10 representative points are not sufficient for the "W" shaped cluster. Using 20 representative per cluster, CURE partitions the data correctly. However, the represen-tation is not efficient. Obviously 20 representatives are not needed for the spherical and ellipsoidal clusters. 
Fig. 3 illustrates the robustness of ScaleSyMP. Noise points do not affect the partition or the components that model the clusters. This is because noise points are located in non-dense regions and they get inhibited by most of the other points. Even if these points reach the threshold, they will excite a few if any other oscillators and thus, do not Figure 2: Clustering Data set DSI: (a) using SyMP, (b) using CURE with I0 Rap per cluster 
Figure 4: Clustering DS3 and DS4 using ScaleSyMP synchronize. In some cases, few of these noise points syn-chronize and form tiny clusters. The results using CURE depends on the shrinking factor (c~) even when a large num-ber of representatives is specified. If o~ is small, then CURE becomes sensitive to noise and cannot partition the data correctly. On the other hand, if o~ is large, CURE splits the non-spherical clusters[8]. 
Data set DS3 is similar to the one used in CURE to il-lustrate the need for multiple representatives to avoid the "chaining effect". Using MST, where every points is con-sidered representative, would group all the data into one cluster. On the other hand, using a single representative (centroid), would not allow the detection of ellipsoidal clus-ters. CURE overcomes this problem by using few represen-tatives and shrinking them towards the centroid. When the shrinking factor is too small (close to 0), CURE degener-ates to the MST, and when the shrinking factor is too large (close to 1), CURE degenerates to traditional centroid-based algorithms (we do not show these results here as similar ex-periments were provided in [8]). SyMP on the other hand, is robust and does not require fixing the number of clusters a priori. Thus, some of the "bridging points" end up in tiny clusters while others do not synchronize at all. In any case, these points can be easily classified as noise. The clusters identified by ScaleSyMP are shown in Fig. 4(a). 
DS4 contains 2 spherical clusters with large differences in their sizes. Centroid-based methods would cause the big cluster to be fragmented. Moreover, since these methods are biased towards clusters with smaller variances, some points that belong to the big cluster will be assigned to the small cluster if they are close enough to its center. The multiple representative approach adopted by CURE solves this prob-lem to a certain degree. If enough representatives (compared to the ratio of the sizes) are used, then the data can be par-titioned correctly. However, since the size of the clusters is not known a priori, CURE may not be reliable for this case. Fig. 4(b) shows the results using ScaleSyMP. 
The example provided in DS4 is also not trivial for scal-able algorithms that are based on random sampling. This is because, due to the size contrast of the two clusters, not enough data points may be selected from the small cluster at any given step to form an initial cluster. As a result, it is possible for this type of algorithms to miss the small clus-ter. ScaleSyMP is also based on loading a random sample into memory. However, the few points (if any) that belong to the small cluster will remain in the buffer if they do not synchronize. Eventually, if these points are not noise, they will synchronize with other points in the subsequent buffers (the example in Fig. 5 will illustrate this point). 
Table 2 compares the CPU time of SyMP and CURE with 20 representatives. DS1 (a) was used in this experiment. As can be Seen, SyMP is much more efficient than CURE. 
One reason for the disparity in performance is that CURE maintains 20 representatives for each cluster, and comput-ing the distances for all pairs of representatives can be very expensive. SyMP on the other hand, starts with one rep-resentative per cluster and representatives are added only when needed. In [7], the scalability of ScaleSOON has been investigated. 
It has been shown that ScaleSOON scales linearly with re-spect to the number of data points and the number of at-tributes. It has also been shown that SealeSOON is more efficient with smaller buffer size. However, the buffer should include enough samples to maintain a diverse population and allow smooth agglomeration of the clusters. The be-havior of ScaleSyMP is almost identical to ScaleSOON, and due to lack of space, we cannot show all the results in this paper. We only illustrate two experiments. 
ScaleSyMP is based on loading and processing a random sample at a time. However, unlike random sampling, it is not sensitive to the size and distribution of the random sam-ple. If at a given step, only few scattered points from a given cluster are sampled, these points would remain in the buffer if they do not synchronize. Eventually, if these points are not noise, they will synchronize with other points when sub-sequent samples get loaded. This behavior is illustrated in Fig. 5 where 2 intermediate steps of ScaleSyMP are shown. 
In this experiment, for illustrative purposes, DSi-b is sam-pled in a raster order. Fig. 5(a) shows the results after processing 10 samples (10 X 1000 points). The processed and unprocessed data are shown with different gray values. Since the loaded samples of the "W" shaped cluster are scattered, these samples are assigned to 3 independent clusters. Af-ter loading and processing 20 more samples, these clusters are still disconnected. However, they have expanded and used more Gaussian components. Other clusters have also expanded, and new clusters were created. These results are shown in Fig. 5(b). Eventually, after loading all points, the sub-clusters of the "W" shaped cluster get merged and be-come components of the same cluster. The final result is the same as the one shown in Fig. 2. 
In the second scalability experiment, we used the different sizes of DS1, and recorded the CPU time for each data set. The results are shown in Fig. 6, where ScaleSyMP scales linearly with respect to the number of data points. 
We introduced a new clustering algorithm that can iden-
Figure 5: Intermediate steps of ScaleSyMP. Results after processing (a)10, (b)30 samples tify clusters of arbitrary shapes in a completely unsuper-vised fashion. Unlike CURE, which uses a specified number of point representatives to model each cluster, SyMP uses 
Gaussian components and requires fewer representatives to model complex clusters. Moreover, SyMP can determine the optimal number of components to model each cluster using a dynamic within-cluster resolution that depends on the X 2 probability distribution of each component. As a re-sult, simple clusters would be modeled by few components while more complex clusters would be modeled by a larger number of components. Another advantage of SyMP is that it does not require a priori knowledge about the number of clusters present in the data. This number is determined using an adaptive inter-cluster resolution that depends on the distribution of the components that model each clus-ter. The robustness of SyMP is an intrinsic property of the synchronization mechanism. Noise points will either syn-chronize and form tiny clusters or will not synchronize at all. 
Our proposed approach is very efficient even when compared with clusters that can detect only spherical or ellipsoidal clusters and require the specification of the number of clus-ters. To cluster very large data set with a limited memory buffer, we proposed the ScaleSyMP algorithm. ScaleSyMP is based on loading only a sample of the data set at a time, and using SyMP to cluster it. Our incremental approach is more efficient and less sensitive than random sampling. 
This is because the random samples are not treated inde-pendently. Information that is accumulated in the phases of the oscillators is propagated as new data gets loaded. This material is based upon work supported by the National 
Science Foundation under Grant No. IIS-0133415. [1] R. Agrawal, J. Gehrke, D. Gunopulos, and [2] P. Bradley, U. Fayyad, and C. Reina. Scaling [3] P. Bradley, U. Fayyad, and C. Reina. Scaling EM [4] A. P. Dempster, N. M. Laird, and D. B. Rubin. [5] M. Ester, 1t. P. Kriegel, J. Sander, and X. Xu. A [6] F. Farnstrom, 3. Lewis, and C. Elkan. Scalability for [7] H. Frigui and M. Rhouma. A synchronization based [8] S. Guha, R. Rastogi, and K. Shim. CURE: An [9] A. Hinneburg and D. Keim. Clustering techniques for [10] A. Hinneburg and D. Keim. An efficient approach to [11] L. Kaufman and P. Rousseeuw. Finding Groups in [12] L. Kanfman and P. J. Rousseeuw. Finding Groups in [13] R. Krishnapuram, H. Frigui, and O. Nasraoui. Fuzzy [14] R. T. Ng and J. Ham Efficient and effective clustering [15] M. Rhouma and H. Frigui. Self-organization of a [16] J. Rissanen. Stochastic Complexity in Statistical [17] R.O.Duda and P. E. Hart. Pattern Classification and [18] G. Sheikholeslami, S. Chatterjee, and A. Zhang. [19] W. Wei, J. Yang, and R. Muntz. Sting: A statistical [20] X. Xiaowei, M. Ester, H. Kriegel, and J. Sander. A [21] T. Zhang, R. Ramakrishnan, and M. Livny. BIRCH: 
