 S  X ebastien Gigu`ere sebastien.giguere.8@ulaval.ca Fran  X cois Laviolette francois.laviolette@ift.ulaval.ca Mario Marchand mario.marchand@ift.ulaval.ca Khadidja Sylla khadidja.sylla.1@ulaval.ca Structured output prediction is a supervised learning problem where the goal of the learner is to predict the correct output y associated to some given input x . Here, the output y can be a complex structure such as a sequence of symbols, a parse tree, or a graph. The predictor generally consists of a vector w of real-valued weights and each input-output example ( x,y ) is mapped to a high-dimensional feature vec-tor Z ( x,y ). The output predicted by w on input x is then the output y that maximizes the inner prod-uct  X  w | Z ( x,y )  X  . However, as emphasized by G  X artner &amp; Vembu (2009), this pre-image problem is often NP -hard. Consequently, any learning algorithm that needs to solve this pre-image problem, for several weight vec-tors and every training example, will often take a pro-hibitive running time. This is probably the most im-portant problem facing several state-of-the-art struc-tured output learning algorithms such as max-margin Markov networks (Taskar et al., 2004) and the struc-tural SVM (Tsochantaridis et al., 2005).
 One of the first attempts to design a learning algorithm that avoids the pre-image problem is due to Cortes et al. (2007). They have proposed to find the pre-dictor that minimizes an ` 2 -regularized regression ob-jective (which does not depend on the predicted out-put for a given input) and have obtained empirical results that compare favorably to those of structural SVM and max-margin Markov networks on the word-recognition data set used by Taskar et al. (2004). In this paper, we provide guarantees for such a regres-sion approach by first showing that the quadratic loss function used by Cortes et al. (2007) provides a con-vex upper bound on the original prediction loss (that depends on the predicted output) provided that the output kernel satisfies some condition with respect to the prediction loss. We also provide two PAC-Bayes upper bounds (McAllester, 2003; Langford, 2005) for the prediction risk that depend on the quadratic em-pirical loss used by Cortes et al. (2007). The minimizer of the first bound turns out to be the same as the one proposed by Cortes et al. (2007) while the minimizer of the second bound, valid for arbitrary reproducing kernel Hilbert spaces (RKHS), is proposed for the first time. Both predictors are compared on practical tasks. PAC-Bayes theory has also been applied re-cently (McAllester, 2007) to structured output predic-tion for a stochastic predictor that aims at minimizing the expected prediction risk. The resulting learning al-gorithms need to solve the pre-image problem on each example of the training set for each update of the pre-dictor. In contrast, we present here risk bounds for learning algorithms that avoid solving the pre-image problem and that produce a deterministic predictor instead of a stochastic one.
 In the supervised learning setting, the learner has ac-ing examples where each example consists of an input-output pair ( x,y )  X  X  X Y . The input space X and the output space Y are both arbitrary but we assume the existence of both an input feature map X : X  X  H X and an output feature map Y : Y  X  H Y , where both H X and H Y are high-dimensional vector spaces and, more generally, reproducing kernel Hilbert spaces (RKHS). In H Y , we use  X  Y ( y ) | Y ( y 0 )  X  to denote the in-squared norm. The same notation is used in H X . Given access to a training set S , the task of the learner is to construct a structured predictor which is repre-sented by a linear operator W that transforms vectors of H X into vectors of H Y . For any x  X  X  and any W , the output y w ( x ) predicted by W is given by Note that y w ( x ) = argmax y  X  X   X  Y ( y ) | W X ( x )  X  when-ever k Y ( y ) k is the same for all y  X  X  . In this case, we recover the usual structured output prediction method when the joint feature vectors Z ( x,y ) are tensor prod-ucts X ( x )  X  Y ( y ). Since finding y w ( x ) given x and W is generally NP -hard (G  X artner &amp; Vembu, 2009), we want to avoid solving this pre-image problem.
 We consider feature maps that are defined by kernels such that K Y ( y,y 0 ) =  X  Y ( y ) | Y ( y 0 )  X   X  ( y,y K that the proposed solutions for W will have the prop-erty that W X ( x ) = P m i =1 P m j =1 Y ( y i ) A i,j for some m  X  m matrix A . Consequently, the predicted output y w ( x ) only requires the use of the kernels K X and K Y (instead of the feature maps X and Y ). We assume that each example ( x,y ) is generated inde-pendently according to some unknown distribution D . Given a function L : Y X Y  X  R that quantifies the loss incurred on ( x,y ) when the predicted output is y w ( x ), the task of the learner is to find the predictor that min-We refer to L as the prediction loss .
 Note that the output kernel K Y , being a similarity measure on Y 2 , induces a loss function L K Y defined as
K Y ( y,y ) + K Y ( y w ( x ) ,y w ( x )) We refer to L K Y as the output kernel loss .
 Both the prediction loss and the output kernel loss on ( x,y ) depend on the predicted output y w ( x ). This is in sharp contrast with the quadratic loss k Y ( y )  X  W X ( x ) k 2 which does not depend on y w ( x ). However we can show that the quadratic loss provides an upper bound to the output kernel loss.
 Lemma 1. For any structured predictor W giving pre-dictions as defined by Equation (1) , for any ( x,y )  X  X  X Y , we have Proof. From the triangle inequality, we have, for all W and for all ( x,y ), From Equation (1), we have k Y ( y w ( x ))  X  W X ( x ) k  X  k Y ( y )  X  W X ( x ) k for all W and for all ( x,y ). From these two inequalities, we have k Y ( y )  X  Y ( y w ( x )) k  X  2 k Y ( y )  X  W X ( x ) k , which gives the lemma. Lemma 1 has far-reaching consequences whenever we use an output kernel K Y such that L ( y,y 0 )  X  L
K Y ( y,y 0 ) for all ( y,y 0 )  X  Y 2 because, in that case, we have for all predictors W and all ( x,y )  X  X   X Y .
 Under these circumstances, a predictor W having a small quadratic risk E ( x,y )  X  D k Y ( y )  X  W X ( x ) k minimize the structured prediction risk, we need to solve the (usually hard) pre-image problem of find-ing the predicted output y w ( x ) for every example in the training set and for all predictors W tried by the learning algorithm. Thanks to Lemma 1, we can avoid this computational burden by performing the simpler regression task of minimizing the quadratic risk when-ever we use an output kernel K Y for which the output kernel loss L K Y upper bounds the prediction loss L . Consider the case when the prediction loss L is the zero-one loss. In that case any output kernel K Y for which there exists two different outputs y and y 0 hav-output kernel loss L K Y which upper bounds L . But the Dirac kernel for which K Y ( y,y 0 ) = 1 if y = y 0 and 0 otherwise gives an L K Y which is identical to L . In the case where the prediction loss L is the Ham-ming distance, the Hamming kernel provides an out-put structured loss L K Y identical to L but one could also use any output kernel giving an L K Y which upper bounds the Hamming distance at the expense of intro-ducing an additional slackness between the quadratic risk and the prediction risk.
 A predictor achieving a small quadratic risk also achieves a small prediction risk when the output kernel K
Y gives an L K Y which upper bounds L . However, their exists data-generating distributions where the predictor achieving the smallest possible quadratic risk has a substantially larger prediction risk than the pre-dictor achieving the smallest possible prediction risk. In other words, there is no consistency guarantee for the regression approach to structured output predic-tion because no such guarantee exists for the particular case of binary classification 1 . However, the regression approach avoids the computational burden of dealing with the pre-image problem and, under some distri-butions, there might be some kernels for which there exists predictors achieving a small quadratic risk. Thanks to Lemma 1, any upper bound on the quadratic risk also provides a bound on the predic-tion risk (provided that there exists an output kernel loss that upper bounds the prediction loss). Conse-quently, the upper bounds proposed by Caponnetto &amp; De Vito (2007); Baldassarre et al. (2012) also pro-vide bounds on the prediction risk for predictors min-imizing the ` 2 -regularized least-squares. However, in-stead of focussing explicitly on such predictors, we pro-vide bounds that hold simultaneously for any predic-tor W and that depend on the empirical quadratic risk achieved by W on the training data. Values of the prediction loss L ( y w ( x ) ,y ) are always be-tween zero an one. However, this is clearly not the case for the quadratic loss k Y ( y )  X  W X ( x ) k 2 . Theoretically attainable very large loss values are well known to give very loose concentration inequalities and, unavoidably, very large risk bounds. Therefore, to obtain a tighter risk bound, we use the following lemma which upper bounds the prediction loss in terms of a bounded func-tion of the quadratic loss.
 Lemma 2. For any prediction loss L upper-bounded by the output kernel loss L K Y , for any ( x,y ) , any W , and any a  X  1 , we have
L ( y w ( x ) ,y )  X  Proof. For any 0  X  x  X  1, we have x  X  e e  X  1 (1  X  e  X  x ). Therefore 1 a where the last equality follows from Lemma 1 and the fact that L is upper-bounded by L K Y . 3.1. The Risk Bound We propose here an upper bound on the prediction risk that uses PAC-Bayes theory to upper bound E However, PAC-Bayes theory does not directly provide bounds on deterministic predictors such as W . In-stead, it provides guarantees for stochastic Gibbs pre-dictors that are described in terms of a posterior dis-tribution Q over deterministic predictors. More pre-cisely, PAC-Bayes theory provide bounds for Gibb X  X  risk defined as the Q -average of the risk of determin-istic predictors. The following theorem, due to Zhang (2006), provides an example of such a bound 2 . Theorem 3. (from Zhang (2006)) Let  X  be any loss function, and let P be any prior distribution on V . Then, for any D on X  X Y , with probability at least 1  X   X  over all training sets S sampled according to D m , we have, simultaneously for all distributions Q on V , where KL( Q,P ) denotes the Kullback-Leibler diver-gence between distributions Q and P .
 To use Theorem 3, we restrict ourselves (in this sec-tion) to the case where both feature spaces H X and H Y are finite-dimensional vector spaces of dimensions N
X and N Y respectively. The set of predictors thus coincides with the set of N Y  X  N X matrices. Each posterior is chosen to be an isotropic Gaussian of vari-ance  X  2 and expectation W . If Q W , X  ( V ) denotes the density at matrix V of this posterior, we have (also called the Frobenius norm). For the Prior P , we chose the (non-informative) isotropic Gaussian centered at the origin, i.e. , P = Q 0 , X  . In that case, we have The next theorem provides an upper bound on the risk of the (deterministic) predictor W which depends on its empirical quadratic risk X  X ot on the empirical risk of a stochastic (Gibbs) predictor. This new result was made possible by performing Gaussian integrals over functions of the quadratic loss and by observing that we can choose a value for  X  such that the noise of the empirical quadratic risk is cancelled by the noise of the true quadratic risk whenever K X ( x,x ) is the same for all x  X  X  .
 Theorem 4. Consider any input kernel K X and any output kernel K Y inducing finite-dimensional feature spaces. Suppose that K X ( x,x ) = 1 for all x  X  X . Let D be any distribution on X  X Y . Then, for any pre-diction loss L upper-bounded by the output kernel loss L K Y , with probability at least 1  X   X  over all training sets S sampled according to D m , we have, simultaneously for all predictors W ,
E 5 e e  X  1 Proof. If we use Theorem 3 in the case of the quadratic loss with the proposed posterior Q W , X  and prior P , and if we use equations (4) and (5) and exploit the convexity of  X  ln x , we then have that, with probability at least 1  X   X  , 1 m In the supplementary material we provide proofs of the following Gaussian integrals: Since, by hypothesis, k X ( x ) k is a constant indepen-dent of x , with probability at least 1  X   X  , 1  X  e For k X ( x ) k = 1, the value of  X  2 satisfying  X  = 0 is monotonously increasing with N X ; going from  X  2 = 0 , 6752 ... for N X = 1 to  X  2 = 1 when N X  X   X  . Consider Inequality (7) when  X  = 0. In that case 2 / 3 &lt;  X  2  X  1. Then its right-hand side can be upper-bounded by the same quantity but with  X  2 replaced by 2 / 3, and its left-hand side can be lower-bounded by the same quantity but with  X  2 replaced by 1. The theorem then follows by applying Lemma 2 for a = 5. 3.2. The Risk Bound Minimizer The predictor W that minimizes the risk bound of Theorem 4 is the one that minimizes the multiple-output ridge regression objective F rr , where for some value of C &gt; 0. Note that F rr is exactly the objective to minimize that was proposed by Cortes et al. (2007). At optimality, the gradient of F rr must vanish. As shown by Cortes et al. (2007), the solution W  X  is unique for finite C and is given by where X | ( x ) denotes the transpose of vector X ( x ), K
X denotes the input kernel matrix, and I denotes the m  X  m identity matrix.
 Since W  X  is the minimizer of the ` 2 -regularized least squares F rr , the convergence rates established by Caponnetto &amp; De Vito (2007) also apply to W  X  . Note that the predictor minimizing the ridge regres-sion objective is a linear combination of simple pre-dictors Y ( y i ) X | ( x j ) that are identified by two train-ing examples. Inspired by some recent work on PAC-Bayes sample-compression (Laviolette &amp; Marchand, 2007; Germain et al., 2011), we want to establish a guarantee on the true risk for arbitrary linear combi-nations of these simple structured output predictors. In contrast with Theorem 4, the obtained risk bound will be valid for feature spaces H X and H Y that are arbitrary RKHS (of possibly infinite dimensionality). For that purpose, let X  X  ( x ) denote the dual of vec-tor X ( x ). The dual X  X  ( x ) is a map from H X R such that  X  ( x,x 0 )  X  X 2 we have X  X  ( x ) X ( x 0 S of m examples, we consider predictors that can be written as where A i,j  X  R  X  ( i,j )  X  X  1 ,...,m } 2 . In this case, the quadratic loss k Y ( y )  X  W X ( x ) k 2 is now given by
Y ( y )  X  To connect with PAC-Bayes sample-compression, let us write A in terms of a distribution q over 2 m 2 pre-dictors. For this purpose, let q + i,j  X  0 be the weight on predictor Y ( y i ) X  X  ( x j ) and let q  X  i,j  X  0 be the weight on the opposite predictor  X  Y ( y i ) X  X  ( x j ) such that Now, w.l.o.g., for all ( i,j ), let A i,j =  X   X  ( q + for some  X  &gt; 0. For notational brevity, let R ( q ,x,y ) be the quadratic loss obtained from R ( A ,x,y ) when I = { 1 ,...,m } 2 denote the set of all pairs of indices and let W def = { X  1 , +1 } . We then have where, for i = ( i,i 0 ) and j = ( j,j 0 ), we have An upper bound on R ( q ) def = E ( x,y )  X  D R ( q ,x,y ) also provides an upper bound on the prediction risk E ( x,y )  X  D L ( y w ( x ) ,y ) since, by Lemma 1, we have L ( y w ( x ) ,y )  X  4 R ( q ,x,y ) whenever A i,j is replaced L is upper-bounded by L K Y . Our goal is thus to find a tight upper bound on R ( q ) and then design an al-gorithm that finds q (hence, the predictor W ) that minimizes this upper bound. 4.1. The Risk Bound The proposed risk bound follows from PAC-Bayes the-ory and depends on how far is the posterior distri-bution q from a prior p . For p , we choose the uniform distribution over I def = { 1 ,..., 2 m } 2 so that p i = 1 / (2 m The posterior q is chosen to be quasi-uniform . By this we mean that for all i  X  X  we have q + i + q  X  i = 1 /m 2 . In that case, each q s i  X  [0 , 1 /m 2 ] and, consequently, the KL-divergence KL( q , p ) is always at most ln 2. Such a small upper bound on KL( q , p ) contributes signif-icantly at reducing the risk bound closer to the em-pirical risk R ( q ,S ) def = (1 /m ) P m i =1 R ( q ,x i over, restricting q to quasi-uniform distributions does not restrict the class of predictors considered by the learner. Indeed, for any predictor W described by some matrix A in Equation (9), there exists  X  &gt; 0 and a quasi-uniform q such that A i,j =  X   X  ( q + i,j  X  q Theorem 5. Let a  X  ` s,t i , j ( x,y )  X  b  X  ( x,y )  X  X  X Y , Let D be any distribution on X X Y . Let m  X  8 . Then, with probability at least 1  X   X  over all training sets S sampled according to D m , we have, simultaneously for all quasi-uniform distributions q on I ,
R ( q )  X  R ( q ,S ) + Proof. Given the uniform prior p , consider the Laplace transform L ples that are used for the predictors described by ( i ,s ) and ( j ,t ). To obtain an unbiased estimator, let S i , j { ( x i ,y i )  X  S : i /  X  i  X  j } and let m i , j def = | S ` our unbiased estimator of ` s,t i , j . It is then straightfor-ward to show that ` s,t i , j ( S i , j )  X  4( b  X  a ) /m  X  ` ` i , j ( S i , j ) + 4( b  X  a ) /m and, consequently, for m  X  8
Now, if we use 2( q  X  p ) 2  X  kl( q,p ) def = q ln( q/p ) + (1  X  q ) ln[(1  X  q ) / (1  X  p )], we obtain L Since S i , j is the arithmetic mean of m i , j iid random variables, the lemma of Maurer (2004) tells us that the last expectation (over S i , j ) is at most 2 consequently, L p  X  2 L p is the expectation (over S ) of a positive random variable, we can use Markov X  X  inequality which states that, with probability of at least 1  X   X  over the random draws of S , we have ln  X  By turning the expectation over p 2 into an expecta-tion over q 2 , and by using Jensen X  X  inequality on the concavity of the logarithm, the last inequality implies that we have for all q . The theorem then follows by using Jensen X  X  inequality on the convexity of ( q  X  p ) 2 and by using KL( q 2 , p 2 ) = 2KL( q , p )  X  2 ln 2 for quasi-uniform posteriors.
 Hence, for quasi-uniform posteriors q , the upper bound on R ( q ) is very close to R ( q ,S ) whenever ( b  X  a ) m . From Equation (11), we can see that ( b  X  a ) is at most 2 B Y (1 +  X B X ) 2 when | K X ( x,x B
X  X  ( x,x 0 )  X  X  2 and | K Y ( y,y 0 ) | X  B Y  X  ( y,y 0 )  X  X  4.2. The Risk Bound Minimizer The posterior q that minimizes the upper bound of Theorem 5 is the posterior minimizing R ( q ,S ) under the constraint that q is quasi-uniform. In that case, each q s i,j  X  [0 , 1 /m 2 ]. Since A i,j =  X   X  ( q + i,j quasi-uniform constraint on q implies that | A i,j |  X  C for all ( i,j )  X  I and for some C &gt; 0. Instead of han-dling these m 2 constraints, it is computationally much cheaper to replace them by the single ` 2 constraint P | A i,j |  X  R for all ( i,j ) whenever this ` 2 constraint is satisfied. Hence, given any R &gt; 0, let us solve Theorem 6. Let A  X  denote the set of solutions of problem (13) . Let K X and K Y denote, respectively, the input and output kernel matrices. Let v 1 ,...,v and  X  1 ,..., X  m denote, respectively, the eigenvectors and eigenvalues of K X . Let u 1 ,...,u m and  X  1 ,..., X  denote, respectively, the eigenvectors and eigenvalues of K Y . Let J def = { ( i,j )  X  I :  X  i  X  j &gt; 0 } . Then P if X where  X  &gt; 0 is the solution of P Proof. Let L ( A , X  ) def = R ( A ,S )+  X  k A k 2  X  R 2 . Con-vex optimisation theory tells us that if there exists  X   X  0 and A : k A k 2  X  R 2 , that satisfies  X  X / X  A = 0 and  X   X  k A k 2  X  R 2 = 0, then A  X  X   X  . Here we have Since K X and K Y are symmetric positive semi-definite m  X  m matrices, their eigenvalues are all non-negative and their eigenvectors constitute an orthonormal basis of R m . Thus, { u i v | j } ( i,j )  X  X  is an orthonormal basis of R m 2 . Consequently, w.l.o.g., any m  X  m matrix A can of  X  i,j . Hence, we have k A k 2 = P m i =1 P m j =1  X  Equation (14) becomes  X  i,j (  X  i  X  2 j + m X  ) =  X  i  X  j ( u When  X  = 0, that equation is solved for  X  i,j = 0 when  X  i  X  j = 0 and  X  i,j = ( u | i v j ) / X  j when  X  (a solution where the ` 2 constraint is not active 4 When  X  &gt; 0, that equation is solved for  X  i,j = (  X  k A k 2 = R 2 with a unique non-zero solution for  X  . Note that the eigenvectors and eigenvalues of K X and K
Y can be obtained from their singular value decom-positions in O ( m 3 ) time. The solution for  X  can be obtained with Newton X  X  method requiring  X ( m 2 ) time for each iteration. Finally, we can obtain A in O ( m 3 ) by using A = u  X   X   X  v | where u and v are matrices ob-tained by concatenating the the column eigenvectors of K Y and K X respectively and where  X   X   X  denotes the matrix of  X  i,j values. Hence, the proposed solution of (13) is reached in O ( m 3 ) time whenever Newton X  X  method requires at most O ( m ) iterations. We have compared the solution given by Equation (8) (Structured Output by Ridge Regression X  X ORR) with the one given by Theorem 6 (Structured Output by Sample-Compression X  X OSC) on the word recog-nition task studied by Taskar et al. (2004); Cortes et al. (2007) and the enzyme classification task stud-ied by Rousu et al. (2006). All hyper parameters ( C ,  X  , and kernel parameters) were selected with 10-fold cross-validation (CV) on the training sets where we have relied on the pre-images (using Equation (1)) for that purpose only.
 The word recognition task consists of predicting the correct word (a sequence of letters) associated to a manuscript picture of the same word. The metrics used for this data set is usually the 0/1-risk (the frac-tion of errors on words) and the letter risk (the fraction of errors on letters). Hence, following Equation (2), we have used the Dirac kernel ( K Y ( y,y 0 ) = I ( y = y and the Hamming kernel (which is given by the length of the largest string minus the Hamming distance be-tween the two strings). The polynomial kernel of de-gree d was used for the input kernel. All experiments were done using the protocol described in Taskar et al. (2004); Cortes et al. (2007). According to Cortes et al. (2007), SORR achieved better performance than struc-tural SVM and max-margin Markov networks. Our empirical results are shown in Table (1). The error bars are the standard deviation of the corresponding risk over the different CV folds given by Taskar et al. (2004). Clearly, SORR and SOSC achieved very simi-lar generalization performance (with overlapping error bars) on both the 0/1-risk and the letter risk. The enzymes hierarchical classification task consists of predicting a path in a enzyme classification scheme used by biologist to classify amino acid sequences of enzymatic proteins. As in Rousu et al. (2006), the 4-gram kernel was used in the input space. Focussing on the hierarchical risk (the length of the incorrect sub-path from the root to the enzyme leaf) as the most natural metric for this data set, we have used the hi-erarchical kernel of (Jacob et al., 2008) (given by the length of the common sub-path between two paths) on the output space. All experiments were done using the protocol described in Rousu et al. (2006) and our empirical results are shown in Table (2). We have also included the results obtained by Rousu et al. (2006) for H  X  M 3  X  `  X  H and H  X  M 3  X  `  X  , which are variants of the max-margin Markov networks. In the case of the 0/1 risk (the fraction of misclassification errors), we have computed the 90% confidence intervals from the bino-mial tail inversion method of Langford (2005). From Table (2), we see that the 0/1 risk differences between all algorithms are significant (at 0.9 confidence level), with SORR being the best algorithm. For the hierar-chical risk, note that from the central limit theorem, the standard deviation of this metric is given by  X / for a testing set of n = 1755 examples when the hierar-chical loss variance is  X  2 . Since  X   X  3 for the hierarchy of 4 levels, the hierarchical risk differences between all algorithms appear to be significant, with H  X  M 3  X  ` being the best algorithm. We have shown that the quadratic regression loss is a convex surrogate of the prediction loss when the pre-diction loss is upper-bounded by the output kernel loss. We have provided two PAC-Bayes upper bounds of the structured prediction risk that depend on the em-pirical quadratic risk of the deterministic predictor. The second bound, based on the PAC-Bayes sample-compression approach, is more general than the first bound as it holds for feature spaces that are arbitrary RKHS. The minimizer of the first bound, SORR, turns out to be the predictor proposed by Cortes et al. (2007) while the minimizer of the second bound, SOSC, is a  X  `
H SORR SOSC predictor that has never been proposed so far. Both predictors have been compared on practical tasks. Fi-nally, although it would be time-consuming, it would be interesting to see if we can improve SOSC by using the full set of m 2 constraints instead of the single ` 2 constraint used in optimization problem (13).
 Baldassarre, Luca, Rosasco, Lorenzo, Barla, Annal-isa, and Verri, Alessandro. Multi-output learning via spectral filtering. Machine Learning , 87:259 X 301, 2012.
 Caponnetto, A. and De Vito, E. Optimal rates for regularized least-squares algorithm. Foundations of Computational Mathematics , 7(3):331 X 368, 2007. Cortes, Corinna, Mohri, Mehryar, and Weston, Jason.
A general regression framework for learning string-to-string mappings. In Bak X r, G  X okhan, Hofmann, Thomas, Sch  X olkopf, Bernhard, Smola, Alexander J., Taskar, Ben, and Vishwanathan, S. V. N. (eds.), Predicting Structured Data , chapter 8, pp. 143 X 168. MIT Press, Cambridge, MA, 2007.
 G  X artner, Thomas and Vembu, Shankar. On structured output training: hard cases and an efficient alterna-tive. Machine Learning , 79:227 X 242, 2009.
 Germain, Pascal, Lacoste, Alexandre, Laviolette, Fran  X cois, Marchand, Mario, and Shanian, Sara. A
PAC-Bayes sample-compression approach to kernel methods. In Getoor, Lise and Scheffer, Tobias (eds.), Proceedings of the 28th International Conference on Machine Learning , ICML  X 11, pp. 297 X 304, New York, NY, USA, June 2011. ACM.
 Jacob, Laurent, Hoffmann, Brice, Stoven, Veronique, and Vert, Jean-Philippe. Virtual screening of gpcrs:
An in silico chemogenomics approach. BMC Bioin-formatics , 9(1):363, 2008.
 Langford, John. Tutorial on practical prediction the-ory for classification. Journal of Machine Learning Research , 6:273 X 306, 2005.
 Laviolette, Fran  X cois and Marchand, Mario. PAC-
Bayes risk bounds for stochastic averages and major-ity votes of sample-compressed classifiers. Journal of Machine Learning Research , 8:1461 X 1487, 2007. Maurer, Andreas. A note on the PAC Bayesian theo-rem. CoRR , cs.LG/0411099, 2004.
 McAllester, David. PAC-Bayesian stochastic model selection. Machine Learning , 51:5 X 21, 2003.
 McAllester, David. Generalization bounds and consis-tency for structured labeling. In Bak X r, G  X okhan, Hofmann, Thomas, Sch  X olkopf, Bernhard, Smola, Alexander J., Taskar, Ben, and Vishwanathan, S.
V. N. (eds.), Predicting Structured Data , chapter 11, pp. 247 X 261. MIT Press, Cambridge, MA, 2007.
 Rousu, Juho, Saunders, Craig, Szedmak, Sandor, and
Shawe-Taylor, John. Kernel-based learning of hier-archical multilabel classification models. J. Mach. Learn. Res. , 7:1601 X 1626, December 2006.
 Taskar, Ben, Guestrin, Carlos, and Koller, Daphne. Max-margin markov networks. In Thrun, Sebastian,
Saul, Lawrence, and Sch  X olkopf, Bernhard (eds.), Ad-vances in Neural Information Processing Systems 16 . MIT Press, Cambridge, MA, 2004.
 Tsochantaridis, I., Joachims, T., Hofmann, T., and Al-tun, Y. Large margin methods for structured and in-terdependent output variables. Journal of Machine Learning Research , 6:1453 X 1484, 2005.
 Zhang, Tong. Information theoretical upper and lower bounds for statistical estimation. IEEE Transaction
