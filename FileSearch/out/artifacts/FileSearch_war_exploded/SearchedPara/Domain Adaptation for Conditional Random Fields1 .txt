 Conditional Random Fields (CRFs) are undirected graphical models that were devel-oped for labeling relational data [1]. A CRF has a single exponential model for the joint probability of the entire sequence of labels given the observation sequence. Therefore, CRFs modeling technique has received a great amount of attentions in many fields, such bioinfomatics [5], Chinese word segmentation [6,7], and Information Extraction [8]. It achieves good results in them.

Similar to most of the classification algorithms, CRFs also have the assumption that training and test data are drawn from the same underlying distributions. However, a that is related but not identical with the training data X  X . For example, one may wish to use a POS tagger trained with WSJ corpus to label email or bioinformatics research papers. This typically affects negatively the performance of a given model. From the experimental results we can know that the performance of the chunker trained with WSJ corpus can achieve 96.2% in different part of WSJ corpus. While performance of the same chunker in BROWN corpus is only 88.4%.

In order to achieve better results in a specific domain, labeled in-domain data is needed. Although large scale in-domain labeled corpus is hard to get, a small number of in-domain labeled data( adaptation data ) and a large number of domain related la-beled data( background data ) is easier to get. For example Penn Treebanks [9] can be used as background training data for POS tagging, chunking, parsing and so on. This kind of adaptation technique is used in many fields, such as language modelling [10], capitalization [11], automatic speech rec ognition [12], parsin g [13,14] and so on.
Directly combining background and adaptation data together is a way to use the in-domain data. But if the scale of adaptation data is much smaller than the background data, the adaptation data X  X  impact would be low. It can be seen from the experimen-tal results. Another disadvantage of this method is that this technique need to retrain the whole model. It would waste a lot of time. In order to take advantage of the in-domain labeled data, a maximum a-posteriori (MAP) adaptation technique for Condi-tional Random Fields models is developed, following the similar idea with adaptation of Maximum Entropy [11]. The adaptation procedure proves to be quite effective in further improving the classification result on different domains. We evaluate the perfor-mance of this adaptation technique in chunking, capitalizing. The relative chunking X  X  performance improvement of the adapted model over the background model is 56.9%. In capitalization task, the adapted model achieves 29.6% relative improvement.
The remainder of this paper is organized as follows: Section 2 describes the related works. The CRFs modeling technique is briefly reviewed in Section 3. Section 4 de-scribes the MAP adaptation technique used for CRFs. The experimental results are presented in Section 5. Conclusions are presented in the last section. Leggetter and Woodland [12] introduced a method of speaker adaptation for continuous density Hidden Markov Models (HMMs). Adaptation statistics are gathered from the available adaptation data and used to calculate a linear regression-based transformation for the mean vectors.

Several recent papers also presented their w orks on modifying learning approaches-boosting [15], naive Bayes [16], and SVMs [17] -to use domain knowledge in text classification. Those methods all modify the base learning algorithm with manually converted knowledge about words.

Chelba and Acero [11] presented a technique for maximum a posteriori (MAP) adap-tation of maximum entropy (MaxEnt) and maximum entropy Markov models (MEMM). The technique was applied to the problem of recovering the correct capitalization of uniformly cased text. Our work has similarities to Chelba and Acero X  X .

Daume and Marcu [18] presented a framework for domain adaptation problem. They treat the in-domain data as drawn from a mixture of  X  X ruly in-domain X  distribution and a  X  X eneral domain X  distribution. Similarly, the out-of-domain are also drawn from a  X  X ruly out-of-domain X  distribution and a  X  X eneral domain X  distribution. Then they apply EM method to estimate parameters. However, this framework used in CRF is computation-ally expensive. Conditional Random Fields (CRFs) are undirected graphical models trained to maxi-mize a conditional probability [1]). CRFs a void a fundamental limitation of maximum entropy Markov models (MEMMs), which can be biased towards states with few suc-cessor states.

Let X = x 1 ...x n and Y = y 1 ...y n represent the generic input sequence and label sequence. The cliques of the graph are now restricted to include just pairs of states ( y  X  1 ,y i ) that are neighbors in the sequence. Linear-chain CRFs thus define the condi-tional probability of a state sequence given an input sequence to be trary feature function over its arguments, and  X  k (ranging from  X  X  X  to  X  ) is a learned or a transition feature t ( y i  X  1 ,y i ,x,i ) .

Then, the CRF X  X  global feature vector for input sequence X and label sequence Y is given by by which can be found by Viterbi algorithm. 3.1 Parameter Estimation CRFs can be trained by the standard maximum likelihood estimation, i.e., maximizing the log-likelihood L  X  of a given training set T = { &lt;X j ,Y j &gt; } N j =1 . where
To perform the optimization, we seek the zero of the gradient forward-backword algorithm.
 where  X  i and  X  i are the forward and backward state-cost vectors defined by To avoid over fitting, we also use Gaussian weight prior [19]: with gradient The optimal solutions can be obtained by using traditional iterative scaling algorithms (e.g., IIS or GIS [20]) or quasi-Newton methods(e.g., L-BFGS [21]). The overview of adaptation stages is sho wn in Figure 1. A simple way to accomplish this is to use MAP adaptation using a prior distribution on the model parameters [11]. A Gaussian prior for the model parameters  X  has been previously used to smooth CRFs models. The prior has 0 mean and diagonal covariance:  X   X  X  (0 ,diag (  X  2 i )) .Inthe adaptation part, the prior distribution is centered at the parameter  X  0 estimated from were tied to  X  i =  X  whose value was determined by line search on development data drawn from the background data or adaptation data.
 Different from the Chelba and Acero X  X  method [11], we use both  X  a and  X  m here. In their method,  X  is used not only to balance the background and adaptation data, but of circumstance. In order to overcome this problem we use two  X  in adaptation step. The log-likelihood L  X  of the given adaptation data set becomes: Therefore the gradient becomes: where F background is the features generated from the background data, F adaptation is the features generated only from the adaptation data,  X  a represents the variance of the adaptation data, and  X  m is used to balance the background and adaptation model. A small variance  X  m will keep the weight  X  m close to the background model, while a large variance  X  m will make the model sensitive to adaptation data. With L  X  and  X  X   X  ,  X  can be iteratively calculated through L-BFGS.
 To evaluate the MAP adaptation of CRFs, we did several experiments on chunking and comes from Tipster corpus [22]. We will introduce the detail steps and features used in the following parts. 5.1 Experiments on Chunking The goal of chunking is to group sequences of words together and classify them by syntactic labels. Various NLP tasks can be seen as a chunking task, such as English base noun phrase identification (base NP chunking), English base phrase identification (chunking), and so on. Because c hunking technique is used in ma ny different fields, we choose chunking task to evaluate the adaptation methods.

The background data used for chunker is generated from WSJ data(wsj 0200.mrg -wsj 2172.mrg). The in-domain test data is from wsj 0000.mrg to wsj 0199.mrg. The others are used to tune parameters. Bracket ed representation is converted into IOB2 representation [23,24].
 For adaptation experiments we use BROWN data in Penn Treebanks III. As Brown Corpus dataset contains eight types of articles, we extract one article from each type(C* 01.mrg), which are used as adaptation dat a. The second articles from each type(C* 02. mrg) are used as development data. The others are used for evaluations. The templates used in chunking experiments are shown is Table 1.

Results of both in-domain and out-of-domain are shown in Table 2. The  X  2 used in background model is selected by in-domain development data.  X  2 a ,and  X  2 m are selected by development data extracted from BROWN data. From the result we observe that the performance of background model in in-domain data is significantly better than in out-of-domain data. Adaptation improves the performance on Brown data by 56.9% relative.

Figure 2 shows the result of the impact of the adaptation data X  X  size. X axis represents the percentage of the adaptation data in BROWN corpus(B-ada). Y axis represents the accuracy. Two lines represent the results of test data set on BROWN (B-tst) and WSJ (wsj-tst) corpus. The result in 0% is got by the background model. The result in 10% is got by the model adapted by 10 percents B-ada data. We observe from the result that the larger adaptation data are used the highe r accuracy in this domain could be get. When We use two sentences extracted from B-ada data to adapt the background model. The adapted model also achieves 30.2% relative improvement.

Then we evaluate the impact of  X  2 m to the performance. The result is shown in Fig- X  m result little adaptation. When the significant changes. Therefore this parameter can be easily set in the real system. 5.2 Experiments on Capitalizing Capitalization can be converted to sequence tagging problem. Each low case words re-ceive a tag which represents its capitaliza tion form. It X  X  also domain dependent. For example, in bioinformatics domain  X  X ene X  is almost low case form. It represents a con-cept. While in some domains,  X  X ene X  is usua lly capitalized, which represents a human name. Therefor we did some experiments to s how the impact of model adaptation tech-nique on this task.

The TIPSTER copra are used to generate both background and adaptation data for the capitalizer. The background data is WSJ data from 1987 -files from WSJ7 001 to WSJ7 127 in TIPSTER Phrase I. The in-domain test data is WSJ 0402 and WSJ 0403, which belong to WSJ 1990 in TIPSTER Phrase II. WSJ 0404 and WSJ 0405 are in-domain development data. The out-of-domain adaptation data is the combination of AP880212 and AP880213, which belong Associated Press 1988 in TIPSTER Phrase II. Files AP880214 and AP880215 are out-of-domain test data.

We use the same tag set with the set used in [11]. Each word in a sentence is labeled with one of the tags:  X  LOC lowercase  X  CAP capitalized  X  MXC mixed case; no further guess is made as to the capitalization of such words.  X  AUC all upper case  X  PNC punctuation; The feature templates we used are shown in table 3.

Table 4 shows results of in-domain and out-of-domain data. The  X  2 in background model we use in this experiment is 5, which is selected by development data. The  X  2 a ing X  X  results. The adapted model gives 29.6% relative improvement. The size of adap-tation data is less than 1% of the background WSJ data X  X  size.

Then we combine adaptation data(AP-ada) with the background data(WSJ) and train accuracy is 94.7%. Comparing with the resu lts got by background model, the capitalizer trained by combined data couldn X  X  significantly improve the performance. In this paper we present a novel technique for maximum a posteriori (MAP) adaptation of Conditional Random Fields Model. Through experimental results,we observe that this technique can effectively adapt a background model to a new domain with a small amount of domain specific labeled data. We did several experiments in three different fields: chunking and capitalizing. The relative chunking X  X  performance improvement of the adapted model over the background model is 56.9%. With two in-domain sen-tences, it also can achieve 30.2% relative improvement. The relative improvement of capitalizing experiment is 29.6%. The experimental results prove that the MAP adap-tation of Conditional Random Fields Model technique can benefit the performances in different tasks.
 This work was partially supported by Chinese NSF 60435020 and 60673038. The au-thors would like to thank Bingqing Wang and Lin Zhao for their carefully proofreading. Thanks the anonymous reviewers for their valuable comments and suggestions.
