 In empirical evaluation of Machine Translation (MT), automatic metrics are widely used as a sub-stitute for human assessment for the purpose of measuring differences in MT system performance. The performance of a newly proposed metric is it-self measured by the degree to which its automatic scores for a sample of MT systems correlate with human assessment of that same set of systems. A main venue for evaluation of MT metrics is the an-nual Workshop for Statistical Machine Translation (WMT) (Bojar et al., 2015) where large-scale hu-man evaluation takes place, primarily for the pur-pose of ranking systems competing in the transla-tion shared task, but additionally to use the resulting system rankings for evaluation of automatic metrics. Since 2014, WMT has used the Pearson correla-tion as the official measure for evaluation of metrics (Mach  X  a  X  cek and Bojar, 2014; Stanojevi  X  c et al., 2015). Comparison of the performance of any two metrics involves the comparison of two Pearson correlation point estimates computed over a sample of MT sys-tems, therefore. Table 1 shows correlations with hu-man assessment of each of the metrics participat-ing in the Czech-to-English component of WMT-14 metrics shared task, and, for example, if we wish to compare the performance of the top-performing metric, R ED S YS S ENT (Wu et al., 2014), with the popular metric B LEU (Papineni et al., 2001), this in-volves comparison of the correlation point estimate of R ED S YS S ENT , r = 0.993, with the weaker corre-lation point estimate of B LEU , r = 0.909, with both computed with reference to human assessment of a sample of 5 MT systems.

When a new metric achieves a stronger correla-tion with human assessment over a baseline metric, such as the increase achieved by R ED S YS S ENT over B
LEU , it is important to consider the uncertainty sur-rounding the difference in correlation. Confidence intervals are very rarely reported in metric evalua-tions, however, and when attempts have been made, the most appropriate method has unfortunately not been applied. For example, although WMT consti-tutes a main authority on MT evaluation, and have made the best attempt to provide confidence inter-vals for metric correlations we could find, when we closely examine results of WMT-14 Czech-to-English metrics shared task, reproduced here in Ta-ble 1, a discrepancy can be identified. For the nine top-performing metrics participating in the shared task, upper confidence interval limits are reported to exceed 1.0.

Confidence intervals reported in the metrics shared task unfortunately also risk inaccurate con-clusions about the relative performance of metrics for other less obvious reasons and risk conclusions that over-estimate the presence of significant dif-ferences. False-positives are problematic in metric evaluations because, if a given metric is mistakenly concluded to significantly outperform a competing metric, it is possible that had a larger sample of MT systems been employed in the evaluation, that the re-verse conclusion should in fact be made. We demon-strate how this can occur for metrics, showing that in reality in current metric evaluation settings, it is only possible to identify a very small number of signifi-cant differences in performance. A main cause is the small number of MT systems employed in evalua-tions, and we propose a new sampling technique, hy-brid super-sampling, that overcomes previous chal-lenges and facilities the evaluation of metrics with reference to a practically unlimited number of MT systems. Alongside the correlation sample point estimates achieved by metrics, WMT reports confidence in-tervals for correlations that unfortunately risk over-estimation of significant differences in metric per-formance, reasons for which we outline below (Mach  X  a  X  cek and Bojar, 2013; Mach  X  a  X  cek and Bojar, 2014; Stanojevi  X  c et al., 2015). 2.1 Sampling Distribution Assumptions As shown in Table 1, confidence intervals are re-ported for metric correlations using  X  notation. The use of the  X  notation implies that the sampling dis-tribution is symmetrical. Since the sampling distri-bution of the Pearson correlation, r , is skewed, how-ever, this means that, for a non-zero correlation, it is not possible for the portion of the confidence interval that lies above the correlation sample point estimate and the portion that lies below it to be equal. Ad-
Sampling Dist. Density ditionally, since the correlation sample statistic, r , cannot take on values greater than 1.0, the closer r is to 1.0 the more extreme the skew of its sampling
To demonstrate how the skew of the sampling dis-tribution of r impacts on upper and lower confidence interval limits for metrics, in Figures 1 and 2, we simulate a possible population and sampling distri-bution for B LEU  X  X  correlation with human assess-ment, r = 0.91, achieved in WMT-14 Czech-to-English shared task, where the sample size, n , was 5 MT systems. Figure 1 depicts a hypothetical  X  X op-ulation X  of 10,000 MT systems and B LEU scores, where hypothetical B LEU scores for systems corre-spond with human assessment scores in such a way that a correlation of 0.91 is achieved. Figure 2 de-picts the sampling distribution for r yielded by re-peatedly drawing sets of 5 systems at random from the larger  X  X opulation X  of 10,000 systems, where the negative skew can be clearly observed. Figure 2 also depicts the 95% confidence interval (CI), within which 95% of sampled correlations lie, where the width of the lower portion of confidence interval is substantially wider than the upper portion, and the overly conservative confidence interval reported for B
LEU in WMT-14, where upper and lower portions of the confidence interval are incorrectly assumed to be equal in size. 2.2 Application of Bootstrap Resampling A conventional approach to bootstrap resampling for the purpose of computing confidence intervals for a correlation sample point estimate is to create a cor-relation coefficient pseudo-distribution by sampling (at random with replacement) human and automatic scores for n MT systems from the set of n systems for which we have genuine metric and human scores. Instead, however, reported confidence intervals are the result of creating pseudo-distributions of human assessment scores for systems. The method unfor-tunately does not produce accurate confidence inter-vals for correlation sample point estimates, as con-fidence intervals produced in this way can unfortu-nately only inform us about the certainty surround-ing human assessment scores for systems rather than the more relevant question of the certainty surround-ing the correlation point estimates achieved by met-rics. Confidence intervals computed in this way are substantially narrower than confidence intervals computed using the standard Fisher r-to-z transfor-mation, that can also be directly applied to corre-lations of metrics with human assessment without application of randomized methods.
 metric correlations for English-to-Czech in WMT-15, and those computed using the standard Fisher r-to-z transformation, where confidence intervals of the latter are substantially wider. An extreme ex-ample occurs for metric D REEM , where the differ-ence between its original reported lower confidence interval limit and the correlation point estimate is 0.006, more than 34 times narrower than that com-puted with the Fisher r-to-z transformation, 0.206. 2.3 Difference in Dependent Correlations When reporting the outcome of an empirical evalua-tion, along with sample point estimates, such as the mean or, in the case of metrics, correlation, we only ever have access to a sample of the actual data that would be needed to compute the corresponding true value for the population . Confidence intervals pro-vide a way of estimating the range of values within which we believe with a specified degree of cer-tainty that the corresponding true value lies. Gener-ally speaking, they can also provide a mechanism for drawing conclusions about significant differences in sample statistics. If, for example, mean scores are used to measure system performance, and the confi-dence intervals of a pair of systems do not overlap, a significant difference in sample means and subse-quently system performance can be concluded.
Although confidence intervals for individual cor-relations do provide an indication of the degree of certainty with which we should interpret reported correlation sample point estimates, they unfortu-nately cannot be used in the above described way to conclude significant differences in the performance of metrics, however. All we can gain from confi-dence intervals for individual correlations with re-spect to significance differences is the following: if the confidence interval of a correlation sample point estimate does not include zero, then it can be con-cluded (with a specified degree of certainty) that this individual correlation is significantly different from zero . Confidence intervals for individual metric cor-relations with human assessment do not inform us about the certainty surrounding a difference in cor-relation with human assessment, the relevant ques-tion for comparing performance of competing MT metrics.

When computing confidence intervals for a dif-ference in correlation, it is important to consider the nature of the data. For MT metric evaluation, data used to compute correlation point estimates for a given pair of metrics is dependent , as it in-cludes three variables (Human, Metric a , Metric b ), and, for each MT system that is a member of the sample, there is a value corresponding to each of these three variables. Besides the two correlations we are interested in comparing, r (Human, Metric a ) and r (Human, Metric b ), there is a third correla-tion to consider, therefore, the correlation that ex-ists directly between the metric scores themselves, r (Metric a , Metric b ). Graham and Baldwin (2014) provide detail of Williams test, a test of significance of a difference in dependent correlations, suitable for evaluation of MT metrics. Confidence intervals are more informative than the binary conclusions that can be inferred from p-values produced by sig-nificance tests, however, and Zou (2007) presents a method of constructing confidence intervals for differences in dependent correlations also suitable for evaluation of MT metrics. We provide an im-plementation of Zou (2007) tailored to metric eval-uation at https://github.com/ygraham/ MT-metric-confidence-intervals .

Table 3 includes confidence intervals for differ-ences in dependent correlations (Zou, 2007) for the seven top-performing German-to-English metrics in WMT-15. Besides providing an indication of the degree of certainty surrounding a given difference in correlation for a pair of metrics, confidence inter-vals that do not include zero can now be used to in-fer a significant difference in performance for a pair of metrics. For example, the 95% confidence inter-val for the difference in correlation between the top-performing metric, U PF C OBALT ( r = 0.981) and M
ETEOR W SD ( r = 0.953), [ 0.005, 0.123 ] , in Table 3, does not include zero and subsequently implies a significant difference in performance.

Figure 3 depicts the contrast in conclusions for WMT-15 German-to-English metrics drawn from (a) a likely interpretation of confidence intervals originally reported in WMT, where the non-overlap of individual correlation confidence intervals of a pair of metrics is used to infer a significant dif-ference, and (b) those drawn from the non-overlap of confidence intervals for differences in dependent correlations with zero (Zou, 2007), highlighting the over-estimation of significant differences in metric performance risked by current WMT confidence in-tervals. For example, for German-to-English with interpretation (a) a total of 91 significant differ-ences are implied that are not identified accord-ing to our corresponding approach. For instance, the non-overlap of confidence intervals of the top-performing metric, U PF C OBALT , with those of all but one other metrics in the original report risks the interpretation of a significant increase in perfor-mance for that metric with all but one other compet-ing metrics, but with the more appropriate method of Zou (2007), however, confidence intervals of this metric X  X  difference in correlation with four of those competing metrics in fact include zero, with no sig-nificant difference identified. It is worth noting that original WMT reports do not state that the confi-dence intervals they provide should be interpreted in the way we have done here, where the non-overlap of individual correlation confidence intervals of a pair of metrics implies a significant difference, but this is nonetheless a very likely interpretation. Results of past metric evaluations have been highly inconclusive with relatively few significant differ-ences in performance possible to identify for met-is mainly caused by the small number of systems used to evaluate metrics. For example, in the origi-nal experiments used to justify the use of automatic metric B LEU , reported correlations with human as-sessment were for a sample size of as small as 5, comprising three automatic systems and two human translators (Papineni et al., 2001). WMT have im-proved on this for some language pairs at least, as in the past four evaluations sample sizes have ranged from 5 (Czech-to-English WMT-14) to 22 systems (German-to-English WMT-12/WMT-13). Even at the maximum sample size of 22 systems, however, correlation point estimates are computed with a high degree of uncertainty. 3.1 Hybrid Super-Sampling In an ideal world, MT metric evaluations would em-ploy a much larger sample of systems than those relied upon in past evaluations, subsequently yield-ing correlation sample point estimates that can be relied on with more certainty. Although not imme-diately obvious, data sets currently used to evalu-ate MT metrics potentially contain data for a very large number of systems. If we consider the fact that, given the output of as little as two MT systems, there exists a very large number of possible ways of combining their translated segments to form a hy-brid system, this opens up the evaluation of metrics to a vastly larger pool of systems. For example, even if we restrict the creation of hybrid systems to com-binations of pairs of the n MT systems competing in a translation shared task (as opposed to hybrids cre-ated by sampling translations from several different MT systems at once), the number of potential hybrid systems is exponential in size of the test set, m : For instance, even for a language pair for which hu-man scores are available for as few as 5 MT systems, by super-sampling translations from every pair of brid systems. Including all possible hybrid systems is of course not necessary, and to make the approach feasible, we sample a large but manageable subset of 10,000 MT systems.

Obtaining automatic metric scores for this larger number of MT systems is feasible for any metric that is expected to be useful in practice, since automatic metrics must already be highly efficient to be used for optimizing systems. Obtaining human assess-ment of this large set of hybrid systems may seem more challenging, but the method of human evalua-tion we employ facilitates the straight-forward com-putation of human scores for vast numbers of sys-tems directly from the original human evaluation of only n systems. Graham et al. (2013) provide a hu-man evaluation of MT that elicits adequacy assess-ments of translations, independent of other transla-tions on a fine-grained 100-point rating scale. After score standardization to iron-out differences in indi-vidual human assessor scoring strategies, the overall human score for a MT system is simply computed as the mean of the ratings attributed to its translations, and this facilitates the straight-forward computation of a human score for any hybrid system from the original human evaluation of n systems.
 To demonstrate, we replicate a previous year X  X  WMT metrics shared task, constructing a hybrid super-sample of 10,000 MT systems each with a corresponding metric and human score. Since we do not have access to all document-level metrics that participated in the original shared task, we use segment-level metric scores as pseudo document-level metrics by taking the average of segment-level scores of the segments that comprise the test set doc-ument. This allows retrospective computation of au-tomatic metric scores for the large set of 10k hy-brid MT systems. For the purpose of comparison, in addition to averaged segment-level metrics, we also include document-level B LEU and an analysis of the correlation it achieves in the context of hy-brid super-sampling. Human evaluation scores were computed using the mean of a minimum of 1,500 crowd-sourced human ratings per system, where strict quality-controlling of crowd-sourced workers was applied.

Table 4 shows correlations achieved by metrics when evaluated on the original 12 and 10k systems, as well as confidence intervals of the difference in correlation achieved by each metric with that of the pected, confidence intervals for differences in corre-lation are substantially reduced for the larger sample of metrics. Importantly, the change in rank order of metrics when evaluated with reference to a sample of 10k MT systems, as opposed to 12, highlights the risk of concluding an increase in performance from evaluations that include only a small sample of sys-tems.

Figure 4 plots super-sampled human and auto-matic metric scores for B LEU providing insight into how B LEU scores correspond with human assess-ment. Worryingly for the range of systems with scores below 20 B LEU points, the plot shows an almost horizontal band of systems spread across a wide range of quality according to human assessors despite extremely similar B LEU scores. The top-performing automatic metric, T ERROR C AT , on the other hand, impressively sustains its high correla-tion with human assessment when evaluated on as many as 10k MT systems, evidence that this metric is indeed highly consistent with human assessment of Spanish-to-English.

Due to space limitations, it is not possible to include pairwise confidence intervals for all pairs of metrics, and instead we include in Figure 5 a heatmap of significant differences in performance, where a significant win is inferred for the metric in a given row over the metric in a given column if the confidence interval of the difference in correla-tion for that pair did not include zero. Results show the super-sampled evaluation facilitates not only the identification of an outright best-performing met-ric, T ERROR C AT , it also yields an almost total-order ranking of metrics, as significant differences are pos-sible to identify for all but one pairs of competing metrics. Finally, we repeated the metric evaluation with ten distinct super-samples of 10k MT systems with all replications resulting in precisely the same ranking of metrics as shown in Table 4. Analysis of evaluation methodologies applied to au-tomatic MT metrics was provided and the risk of over-estimation of significant differences in metric performance identified. Confidence intervals for differences in dependent correlations were recom-mended as appropriate for evaluation of MT met-rics. Hybrid super-sampling was proposed, evaluat-ing metrics with reference to a substantially larger sample of MT systems, achieving genuinely highly conclusive metric rankings. We wish to thank the anonymous reviewers and Ond  X  rej Bojar for valued feedback and WMT or-ganisers for provision of data sets. This project has received funding from the European Union Horizon 2020 research and innovation programme under grant agreement 645452 (QT21) and the ADAPT Centre for Digital Content Technology ( www.adaptcentre.ie ) at Dublin City Uni-versity funded under the SFI Research Centres Pro-gramme (Grant 13/RC/2106) co-funded under the European Regional Development Fund.

