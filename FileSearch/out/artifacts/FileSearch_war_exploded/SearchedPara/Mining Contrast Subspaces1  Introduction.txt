 Imagine you are a medical doctor facing a patient having symptoms of being overweight, short of breath, and some others. You want to check the patient on two specific possible diseases: corona ry artery disease and adiposity. Please note that clogged arteries are among the top-5 most commonly misdiagnosed diseases. You have a set of reference samples of both diseases. Then, you may naturally ask  X  X n what aspect is this patient most similar to cases of coronary artery disease and, at the same time, dissimilar to adiposity? X 
The above motivation scenario cannot be addressed well using existing data mining methods, and thus suggests a novel data mining problem. In a multidi-mensional data set of two classes, give n a query object and a target class, we want to find the subspace where the quer y object is most likely to belong to the target class against the other class. We call such a subspace a contrast subspace since it contrasts the likelihood of the query object in the target class against the other class. Mining contrast subspaces is an interesting problem with many important applications. As another example, when an analyst in an insurance company is investigating a suspicious claim, she may want to compare the sus-picious case against the samples of frauds and normal claims. A useful question to ask is in what aspects the suspicious case is most similar to fraudulent cases and different from normal claims. In other words, finding the contrast subspace for the suspicious claim is informative for the analyst.

While there are many existing studies on outlier detection and contrast min-ing, they focus on collective patterns t hat are shared by many cases of the target class. The contrast subspace mining problem addressed here is different. It fo-cuses on one query object and finds the customized contrast subspace. This critical difference makes the problem formulation, the suitable applications, and the mining methods dramatically different. We will review the related work and explain the differences syst ematically in Section 2.

To tackle the problem of mining contra st subspaces, we need to address several technical issues. First, we need to have a s imple yet informative contrast measure to quantify the similarity between the query object and the target class and the difference between the query object and the other class. In this paper, we use the ratio of the likelihood of the query object in the target class against that in the other class as the measure. This is essentially the Bayes factor on the query object, and comes with a well r ecognized explanation [1].

Second, the problem of mining contrast subspaces is computational challeng-ing. We show that the problem is MAX SNP-hard, and thus does not allow poly-nomial time approximation methods unless P=NP. Therefore, the only hope is to develop heuristics that may work well in practice.

Third, one could use a brute-force method to tackle the contrast mining prob-lem, which enumerates every non-empty subspace and computes the contrast measure. This method, however, is very c ostly on data sets with a non-trivial dimensionality. One major obstacle preventing effective pruning is that the con-trast measure does not have any monotonicity with respect to the subspace-superspace relationship. To tackle the problem, we develop pruning techniques based on bounds of likelihood and contrast ratio. Our experimental results on real data sets clearly verify the effect iveness and efficiency of our method. The rest of the paper is organized as follows. We review the related work in Section 2. In Section 3, we formalize the problem, and analyze it theoretically. We present a heuristic method in Section 4, and evaluate our method empirically using real data sets in Section 5. W e conclude the paper in Section 6. Our study is related to the existing work on contrast mining, subspace outlier detection and typicality queries. We r eview the related work briefly here.
Contrast mining discovers patterns and models that manifest drastic differ-ences between datasets. Dong and Bailey [ 2] presented a comprehensive review. The most renowned contrast patterns include emerging patterns [3], contrast sets [4] and subgroups [5]. Although their definitions vary, the mining methods share heavy similarity [6].

Contrast pattern mining identifies patterns by considering all objects of all classes in the complete pattern space. Orthogonally, contrast subspace mining focuses on one object, and identifie s subspaces where a qu ery object demon-strates the strongest overall similarity to one class against the other. These two mining problems are fundamentally different. To the best of our knowledge, the contrast subspace mining problem has not been systematically explored in the data mining literature.

Subspace outlier detection discovers objects that significantly deviate from the majority in some subspaces. It is very different from our study. In contrast subspace mining, the qu ery object may or may not be an outlier. Some recent studies find subspaces that may contain substantial outliers. B  X  ohm et al. [7] and Keller et al. [8] proposed statistical approaches CMI and HiCS to select sub-spaces for a multidimensional database, where there may exist outliers with high deviations. Both CMI and HiCS are fundamentally different from our method. Technically, they choose subspaces for all outliers in a given database, while our method chooses the most contrast ing subspaces for a query object.

Our method uses probability density to estimate the likelihood of a query object belonging to different classes. There are a few density-based outlier de-tection methods, such as [9 X 12]. Our method is inherently different from those, since we do not target at outlier objects at all.

Hua et al. [13] introduced a novel top-k typicality query , which ranks ob-jects according to their typicality in a data set or a class of objects. Although both [13] and our work use density estimation methods to calculate the typical-ity/likelihood of a query object with respect to a set of data objects, typicality queries [13] do not consider subspaces at all. In this section, we first formulate the problem. Then, we recall the basics of kernel density estimation, which can estimate the probability density of objects. Last,weinvestigatetheco mplexity of the problem. 3.1 Problem Definition Let D = { D 1 ,...,D d } be a d -dimensional space, where the domain of D i is R , the set of real numbers. A subspace S  X  D ( S =  X  ) is a subset of D . We also call D the full space . Consider an object o in space D .Wedenoteby o.D i the value of o in dimension D S is o S =( o.D i 1 ,...,o.D i l ). For a set of objects O = { o j | 1  X  j  X  n } ,the projection of O in S is O S = { o S j | o j  X  O, 1  X  j  X  n } .

Given a set of objects O , we assume a latent distribution Z that generates the objects in O . For a query object q ,denoteby L D ( q |Z ) the likelihood of q being generated by Z in full space D . The posterior probability of q given O , denoted by L D ( q | O ), can be estimated by L D ( q |Z ). For a non-empty subspace S ( S  X  D , S =  X  ), denote by Z S the projection of Z in S .The subspace likelihood of object q with respect to Z in S , denoted by L S ( q |Z ), can be estimated by the posterior probability of object q given O in S , denoted by L S ( q | O ). In this paper, we assume that the objects in O belong to two classes, C + and C  X  , exclusively. Thus, O = O +  X  O  X  and O +  X  O  X  =  X  ,where O + and O  X  are the subsets of objects belonging to C + and C  X  , respectively. Given a query object q , we are interested in how likely q belongs to C + and does not belong to C  X  . To measure these two factors c omprehensively, we define the likelihood
Likelihood contrast is essentially the Bayes factor 1 of object q as the obser-vation. In other words, we can regard O + and O  X  as representing two models, and we need to choose one of them based on query object q . Consequently, the ratio of likelihoods indicates the plausibility of model represented by O + against that by O  X  . Jeffreys [1] gave a scale for interpretation of Bayes factor. When respectively, the strength of the eviden ce is negative, barely worth mentioning, substantial, strong, very strong, and decisive.
 We can extend likelihood contrast to subspaces. For a non-empty subspace S  X  D , we define the likelihood contrast in the subspace as LC To avoid triviality in subspaces where L S ( q | O + ) is very small, we introduce a minimum likelihood threshold  X &gt; 0, and consider only the subspaces S where L
S ( q
Given a multidimensional data set O in full space D , a query object q ,and a minimum likelihood threshold  X &gt; 0, and a parameter k&gt; 0, the problem of mining contrast subspaces is to find the top-k subspaces S ordered by the subspace likelihood contrast LC S ( q ) subject to L S ( q | O + )  X   X  . 3.2 Kernel Density Estimation We can use kernel density estimation [14] to estimate likelihood L S ( q | O ). In this paper, we adopt the Gaussian kernel, which is natural and widely used in density estimation. Given a set of objects O , the density of a query object q in subspace S , denoted by  X  f S ( q,O ), can be estimated as where dist S ( q,o ) 2 = Silverman [15] suggested that the optimal bandwidth value for smoothing nor-A ( K )= { 4 / ( | S | +2) } 1 / ( | S | +4) .

As the kernel is radially symmetric and the data is not normalized in sub-spaces, we can use a single scale parameter  X  S in subspace S and set h S =  X  the average marginal variance in S .
 Using kernel density esti mation, we can estimate L S ( q | O )as Correspondingly, the likelihood contrast of object q in subspace S is given by We often omit O + and O  X  and write LC S ( q )if O + and O  X  are clear from context. 3.3 Complexity Analysis We have the following theoretical result. It can be proved by a reduction from the emerging pattern mining problem [3], which is MAX SNP-hard [16]. Limited by space, we omit the details here.
 Theorem 1 (Complexity). The problem of mining contrast subspaces is MAX SNP-hard.

The above theoretical result indicates that the problem of mining contrast subspaces is even hard to approximate  X  it is impossible to design a good ap-proximation algorithm. In the rest of the paper, we turn to practical heuristic methods. In this section, we first describe a baseline method that examines every possible non-empty subspace. Then, we present a bounding-pruning-refining method that expedites the search substantially. 4.1 A Baseline Method A baseline method enumerates all possible non-empty spaces S and calculates the exact values of both L S ( q | O + )and L S ( q | O  X  ). Then, it returns the top-k subspaces S with the largest LC S ( q ) values. To ensure the completeness and efficiency of subspace enumeration, the baseline method traverses the set enumeration tree [17] of subspaces in a depth-first manner.

L S ( q | O + ) is not monotonic in subspaces. To prune subspaces using the min-imum likelihood threshold  X  , we develop an upper bound of L S ( q | O + ). We sort all the dimensions in their standard deviation descending order. Let S be the set of children of S in the subspace set enumeration tree using the standard deviation descending order. Define L  X  S ( q | O + )= 1 | where  X  min = min {  X  S | S  X  X } , h opt min = min { h S opt | S  X  X } ,and h Theorem 2 (Monotonic density bound). For a query object q ,asetof objects O ,andsubspaces S 1 , S 2 such that S 1 is an ancestor of S 2 in the sub-space set enumeration tree using the standard deviation descending order in O + , L Using Theorem 2, in addition to L S ( q | O + )and L S ( q | O  X  ), we also compute L
S ( q super-spaces of S can be pruned.

Using Equations 1 and 2, the baseline algorithm computes the likelihood con-trast for every subspace where L S ( q | O + )  X   X  , and returns the top-k subspaces. The time complexity is O (2 | D |  X  ( | O + | + | O  X  | )). 4.2 A Bounding-Pruning-Refining Method For a query object q and a set of objects O ,the -neighborhood ( &gt; 0) of q in subspace S is N S ( q )= { o  X  O | dist S ( q,o )  X  } . We can divide L S ( q | O ) into two parts, that is, L S ( q | O )= L N is contributed by the objects in the -neighborhood, that is, L N -neighborhood, that is, L rest S ( q | O )= 1 |
Let dist S ( q | O ) be the maximum distance between q and all objects in O in subspace S .Wehave,
Using the above, we have the following upper and lower bounds of L S ( q | O ) using -neighborhood.
 Theorem 3 (Bounds). For a query object q ,asetofobjects O and  X  0 , where
LL S ( q | O )= and We obtain an upper bound of LC S ( q ) based on Theorem 3 and Equation 2. Corollary 1 (Likelihood Contrast Upper Bound). For a query object q ,a set of objects O + ,asetofobjects O  X  ,and  X  0 , LC S ( q )  X  UL S ( q | O + ) LL
Using Corollary 1, for a subspace S ,ifthereareatleast k subspaces whose spaces of the largest likelihood contrast.
 Using the -neighborhood, L  X  S ( q | O + ) is computed by Our bounding-pruning-refining method, CSMiner (for Contrast Subspace Miner), conducts a depth-first search on the subspace set enumeration tree. For a candidate subspace S , CSMiner calculates UL S ( q | O + )and LL S ( q | O  X  ) using the -neighborhood. If UL S ( q | O + ) is less than the minimum likelihood threshold, S cannot be a contrast subspace. Otherwise, CSMiner checks whether the likelihood contrasts of the current top-k subspaces are larger than the up-per bound of LC S ( q ). If not, CSMiner refines L S ( q | O + )and L S ( q | O  X  )by involving objects that are out of the -neighborhood. S will be added into the current top-k list if its likelihood contrast is larger than one of the current top-k ones. Algorithm 1 gives the pseudo-code of CSMiner. Due to the hardness of the problem shown in Theorem 1 and the heuristic nature of this method, the tive baseline method. However, as shown by our empirical study, CSMiner is substantially faster than the baseline method.

Computing -neighborhood is critical in CSMiner. The distance between ob-jects increases when dimensionality increases. Thus, the value of should not be Algorithm 1. CSMiner ( q,O + ,O  X  , X ,k ) fixed. The standard deviation expresses the variability of a set of data. For sub-space S ,weset = the marginal variances of O + and O  X  , respectively, on dimension D i ( D i  X  S ), and r is a system defined parameter. Our experiments show that r can be set in the range of 0 . 3  X  0 . 6, and is not sensitive. In this section, we report a systematic empirical study using real data sets to verify the effectiveness and efficiency of our method. All experiments were con-ducted on a PC computer with an Intel Core i7-3770 3.40 GHz CPU, and 8 GB main memory, running Windows 7 operating system. All algorithms were implemented in Java and compiled by JDK 7. 5.1 Effectiveness We use 6 real data sets from the UCI machine learning repository [18]. We remove non-numerical attributes and all instances containing missing values. Table 1 shows the data characteristics.

For each data set, we take each record as a query object q ,andallrecordsex-cept q belonging to the same class as q forming the set O 1 , and records belonging to the other classes forming the set O 2 . Using CSMiner, we compute for each record (1) the inlying contrast subspace taking O 1 as O + and O 2 as O  X  ,and(2) the outlying contrast subspace taking O 2 as O + and O 1 as O  X  .Inthisexperi-ment, we only compute the top-1 subspace. For clarity, we denote the likelihood contrasts of inlying contrast subspace by LC in S ( q ) and those of outlying contrast subspace by LC out S ( q ). The minimum likelihood threshold is set to 0.001. interestingly a good portion of objects have strong outlying contrast subspaces. For example, in CMSC, more than 50% of the objects have outlying contrast subspaces satisfying LC out S ( q )  X  10 3 . Moreover, we can see that, except PID, a non-trivial number of objects in each data set have both strong inlying and outlying contrast subspaces (e.g., LC in S ( q )  X  10 4 and LC out S ( q )  X  10 2 ).
Figures 1, 2 show the distributions of dimensionality of inlying and outlying contrast subspaces, respectively. The dimen sionality distribution is an interesting feature characterizing a data set. For example, in most cases the dimensionality of contrast subspaces follows a two-side bell-shape distribution. However, in BCW and PID, the outlying contrast subspaces tend to have low dimensionality. 5.2 Efficiency To the best of our knowledge, there is no previous method tackling the exact same mining problem. Therefore, we evaluate the efficiency of only CSMiner and the baseline method. Limited by space, we report the results on the Waveform data set only, since it is the largest one with the highest dimensionality. We randomly select 100 records from Wavef orm as query objects, and report the average runtime. The results on the other data sets follow similar trends.
Figure 3(a) shows the runtime (in logarithmic scale) with respect to the mini-mum likelihood threshold  X  .As  X  decreases, the runtime in creases exponentially. However, the heuristic pruning techniques in CSMiner expedites the search sub-stantially in practice. Figures 3(b) and 3(c) show the scalability on data set size and dimensionality. CSMiner is substantially faster than the baseline method.
CSMiner uses a user defined parameter r to define -neighborhood. Figure 4 shows the relative runtime with respect to r . The runtime of CSMiner is not very sensitive to r in general. Experimentally, the shortest runtime of CSMiner hap-pens when r is in [0 . 3 , 0 . 6]. Figure 5 illustrates the relative runtime of CSMiner with respect to k , showing that CSMiner is linearly scalable with respect to k . In this paper, we studied a novel and interesting problem of mining contrast subspaces to discover the aspects that a query object most similar to a class and dissimilar to the other class. We showed theoretically that the problem is very challenging, and cannot even be approximated in polynomial time. We presented a heuristic method based on upper and lower bounds of likelihood and likelihood contrast. Our experiments on real dat a sets show that our method expedites contrast subspace mining substantially comparing to the baseline method.
