 We consider online learning in finite Markov decision processes (MDPs) with a fixed, known dy-algorithm in T time steps is O T 2 / 3 (ln T ) 1 / 3 .
 major difference between our work and that of Even-Dar et al. (2005, 2009) is that they assume is a bound on the total expected regret, which scales with the square root of the number of time information problem is due to Yu et al. (2009) who proposed new algorithms and proved a bound on algorithm ( X  X -FPL X ) and they have shown a sublinear ( o ( T ) ) almost sure bound on the regret. There are only three papers that we know of where the bandit situation was considered. ( X  X xploratory FPL X ) for this setting and have shown an o ( T ) almost sure bound on the regret. Recently, Neu et al. (2010) gave O twice) and so they do not generalize to our setting.
 Another closely related work is due to Yu and Mannor (2009a,b) who considered the problem of  X  X o-regret X  result.
 by a section about our assumptions (Section 3). The algorithm and the main result are given in Section 4, while a proof sketch of the latter is presented in Section 5. reader is referred to, for example, Puterman (1994). 2.1 Online learning in MDPs r this sequence. { r r from a fixed distribution P 0 .
 The goal of the learning agent is to maximize its expected total reward as (in short, the regret) is defined as policy). recall that we assume that the rewards are bound to [0 , 1] .
 the stationary distribution of a policy  X  is a distribution  X  which satisfies  X P  X  =  X  . Assumption A1 Every policy  X  has a well-defined unique stationary distribution  X   X  . inf  X ,x  X   X  ( x )  X   X  for some  X  &gt; 0 .
 Assumption A3 There exists some fixed positive  X  such that for any two arbitrary distributions  X  and  X  0 over X , where k X k 1 is the 1 -norm of vectors: k v k 1 = P i | v i | . Note that Assumption A3 implies Assumption A1. The quantity  X  is called the mixing time under-lying P by Even-Dar et al. (2009) who also assume A3. to be found, for example, in the book by Puterman (1994). After the definitions, we specify our of the proposed algorithm. 4.1 Preliminaries r by Bellman equations: from P 0 , and define following notation: Note that q t , v t satisfy the Bellman equations underlying  X  t , P and r t . between time steps t  X  N and t :  X  t (  X | x,a ) instead of  X  4.2 The algorithm we propose to use the following estimate: average reward MDP defined by ( P,  X  t ,  X  r t ) : Note that if N is sufficiently large and  X  t changes sufficiently slowly then almost surely, for arbitrary x  X  X ,t  X  N + 1 . This fact will be shown in Lemma 4. Now, assume generated by the history u t  X  N : Then also  X  t  X  1 ,...,  X  t  X  N  X   X  ( u t  X  N ) and  X  N t can be computed using row vector corresponding to x (and we assumed that X = { 1 ,..., |X|} ). Moreover, a simple but is, It then follows that X  X A , As a consequence, we also have, for all ( x,a )  X  X   X A ,t  X  N + 1 , the memory needed by our algorithm scales with N |A||X| . The computational complexity of the  X  t (  X | x t  X  N , a t  X  N ) ). The cost of this is O N |A||X| be taken into account in the proper tuning of the algorithm X  X  parameters. 4.3 Main result Our main result is the following bound concerning the performance of Algorithm 1. Theorem 1. Let N = d  X  ln T e , Algorithm 1 Algorithm for the online bandit MDP.
 Set N  X  1 , w 1 ( x,a ) = w 2 ( x,a ) =  X  X  X  = w 2 N ( x,a ) = 1 ,  X   X  (0 , 1) ,  X   X  (0 , X  ] . For t = 1 , 2 ,...,T , repeat Then the regret can be bounded as sharp in the sense that it may be possible, in agreement with the standard bandit lower bound of Auer et al. (2002), to give an algorithm with an O use the following decomposition of this difference (also used by Even-Dar et al., 2009): The first term is bounded using the following standard MDP result.
 Lemma 1 (Even-Dar et al., 2009) . For any policy  X  and any T  X  1 it holds that propositions.
 Proposition 1. Let N  X d  X  ln T e . For any policy  X  and for all T large enough, we have
X  X  (4  X  + 10) N + Proposition 2. Let N  X d  X  ln T e . For any T large enough, Note that the choice of N ensures that the second term in (10) becomes O (1) . The proofs are broken into a number of statements presented in the next section. Due to space the paper. 5.1 General tools  X  holds for 1  X  s  X  t  X  1 . Then we have In the next two lemmas we compute the rate of change of the policies produced by Exp3 and show that for a large enough value of N ,  X  N t,x,a can be uniformly bounded form below by  X / 2 . Lemma 3. Assume that for some N + 1  X  t  X  T ,  X  N t, x Let c = 2  X   X  1  X  + 4  X  + 6 . Then, ately, the policies will change slowly and  X  N t will be uniformly bounded away from zero. Lemma 4. Let c be as in Lemma 3. Assume that c (3  X  + 1) 2 &lt;  X / 2 , and let Then, for all N &lt; t  X  T , x,x 0  X  X and a  X  A , we have  X  N t,x,a ( x 0 )  X   X / 2 and instants, one can proceed by induction, using Lemmas 2 and 3 in the inductive step. 5.2 Proof of Proposition 1 tion 1 for T &gt; N .
 Lemma 5. (cf. Lemma 4.1 in Even-Dar et al., 2009) For any policy  X  and t  X  1 , ceding lemma shows that in order to prove Proposition 1, it suffices to prove an upper bound on E [ Q T ( x,a )  X  V T ( x )] .
 l
E [ Q T ( x,a )  X  V T ( x )] delay.
 Let algorithm), one can prove that  X  t ( a | x ) |  X  q t ( x,a ) | X  4  X  (  X  + 2) and Then, following the proof of Auer et al. (2002), we can show that using (8) and (13), that desired result.
 Proof of Proposition 1. Under the conditions of the proposition, combining Lemmas 5-6 yields
X  X  2 N + X  X  (4  X  + 10) N + proving Proposition 1.
 National Office for Research and Technology (OTKA-NKTH CNK 77782), the PASCAL2 Network of Excellence under EC grant no. 216886, NSERC, AITF, the Alberta Ingenuity Centre for Machine Learning, the DARPA GALE project (HR0011-08-C-0110) and iCore. bandit problem. SIAM J. Comput. , 32(1):48 X 77.
 pages 401 X 408.
 Even-Dar, E., Kakade, S. M., and Mansour, Y. (2009). Online Markov decision processes. Mathe-matics of Operations Research , 34(3):726 X 736.
 lem. In COLT-10 .
 Puterman, M. L. (1994). Markov Decision Processes: Discrete Stochastic Dynamic Programming . Wiley-Interscience.
 IEEE Conference on Decision and Control and 28th Chinese Control Conference . IEEE Press. changing rewards and transitions. In GameNets X 09: Proceedings of the First ICST international conference on Game Theory for Networks , pages 314 X 322, Piscataway, NJ, USA. IEEE Press. Yu, J. Y., Mannor, S., and Shimkin, N. (2009). Markov decision processes with arbitrary reward processes. Mathematics of Operations Research , 34(3):737 X 757.
