 Ensembles for unsupervised outlier detection is an emerging topic that has been neglected for a surprisingly long time (al-though there are reasons why this is more difficult than su-pervised ensembles or even clustering ensembles). Aggarwal recently discussed algorithmic patterns of outlier detection ensembles, identified traces of the idea in the literature, and remarked on potential as well as unlikely avenues for future transfer of concepts from supervised ensembles. Comple-mentary to his points, here we focus on the core ingredients for building an outlier ensemble, discuss the first steps taken in the literature, and identify challenges for future research. Outlier detection is the process of identifying those observa-tions which deviate substantially from the remaining data. Many definitions of outliers exist in the statistics literature, usually tied to specific assumptions on the underlying data distribution. The most common general definitions remain rather vague, such as these classic examples: The point of all these definitions is the idea that any pro-cess, whether it is a traffic network, web server traffic, credit card data, sensor data in some scientific experiment, or the human metabolism, offers characteristic observations that could even be predicted if the process was well-understood. Any unpredicted observation indicates a lack of understand-ing of the particular process, or is produced by a different process (such as a traffic accident, a network intrusion at-tack, credit card fraud, sensor failure, or a disease affecting human health), and therefore probably is worth further in-vestigation.
 Outlier detection algorithms aim to automatically identify those valuable or disturbing observations in large collections of data. Because there is no rigid definition of which obser-vation exactly is an outlier, every algorithm is based on a model that is relying on certain assumptions of what qual-ifies as an outlier. Clearly, the applicability of each model depends on the nature of the data. Sophisticated algorithms do not only label observations as outlier or inlier, but assign scores to observations, representing degrees or probabilities of outlierness. Some popular models are based on the dis-tance between objects [37; 60; 4; 74], or on the density of the neighborhood of an object [9; 56; 34; 38; 42], or based on the variance of angles between object vectors [42; 58], or on other principles of outlierness in various domains [12; 13; 3]. These methods represent different attempts to make the rather vague intuition about what outliers are more con-crete, typically in an implicit, procedural way [65]. Because every model is specialized for different characteris-tics of observations and therefore fits only to some aspects of the  X  X hole truth X , it might be a good idea to integrate vari-ous different outlier detection results, producing a consensus of judgements. The key idea of such an approach, which is called an  X  X nsemble X , is that the combination of individual judgements, or outlier detection results, is beneficial if those judgements do not contain all the same errors. One might think of it as a majority vote of a jury (as in Condorcet X  X  Jury theorem [47]): One or another judgement about an observation might be wrong, but the majority might still be right, as long as the judgements are, overall, somewhat reliable and every member decides independently from the others.
 Aggarwal [2] recently proposed a categorization of ensemble approaches to outlier detection by algorithmic patterns or strategies. He distinguishes  X  X equential ensembles X  vs.  X  X n-dependent ensembles X , and  X  X odel-centered ensembles X  vs.  X  X ata-centered ensembles X . This is helpful for identifying aspects of the ensemble approach in the literature. Accord-ingly, he points out that before the first paper was explicitly talking about  X  X utlier ensembles X  [45], traces of the very idea of combining different models have appeared earlier in the literature, and also several times later without dis-cussing a potential relationship to ensemble techniques ex-plicitly. When reading the literature through these glasses of ensembles, we can undoubtedly find many hints on the ensemble idea without explicit discussion. However, not ev-erybody has to wear these glasses. To discuss the problem of, e.g., subspace outlier detection based on the combination of several models [36; 51; 52] without discussing ensemble techniques is perfectly fine. In fact, the subspace outlier problem is a hard problem in its own right and the typical conference paper cannot accommodate a broader discussion for reasons of space restrictions.
 Furthermore, the subspace outlier problem could be seen as a problem analogous to the multiview or alternative cluster-ing problem [77] where it is not intended to find the con-sensus clustering; instead, different clustering solutions in different subspaces can each be interesting, valid solutions. Likewise, different outliers in different subspaces could each be meaningfully reported. This is reflected in recent research addressing the explanation of subspace outliers [14]. Seen this way, subspace outlier detection would even be orthogo-nal to the  X  X nsemble X  or  X  X onsensus X  idea.
 Nevertheless, discussing the subspace outlier problem while taking into account reasoning on ensemble techniques would seem promising of finding more principled solutions to the subspace outlier problem [76]. Likewise, it would seem that ensemble techniques such as feature bagging [45], i.e., using different subspaces as a means to learn diverse models, could also benefit from insights in the area of subspace outlier detection.
 Complementary to Aggarwal [2], we would like to discuss here the specific challenges, the first steps taken so far in the literature, and overall the important questions in research regarding ensembles for outlier detection.
 Transferring basic principles from supervised learning, the two key principles of ensemble construction would be accu-racy and diversity . Casting outlier detection as an unsu-pervised problem, however, there is nothing known about the accuracy of individual outlier detectors during learn-ing. This is a very fundamental problem and, as Aggar-wal [2] pointed out, probably one of the main reasons why the state of the art in research on ensembles for unsuper-vised outlier detection is not very advanced. But obviously this problem would also affect ensemble clustering where we have a lot more of research presented in the literature. Therefore, we should have a closer look on the differences between ensemble clustering and ensemble outlier detection beyond their common characteristic of being unsupervised ensembles. How to assess the diversity of outlier detection results does not have a straightforward answer either, but at least it found some attention recently.
 In the remainder of this paper, we will first have a look at the research area of ensemble clustering in Section 2, detail-ing why ensembles for outlier detection are quite a different issue. We will discuss the crucial research questions for out-lier detection ensembles, reflecting the literature as sparse as it is so far, in Section 3, Section 4, and Section 5. Com-mon approaches to assess the accuracy of outlier detection results are far from satisfying. We sketch the problem in Section 3. The diversity of models, besides their accuracy, is the most important ingredient for ensemble construction. We will discuss the issue of diversity of models for outlier detection in Section 4. Another central question is how to actually construct the ensemble, i.e., how to combine the models. The challenges in combining different models and preliminary findings in the literature will be discussed in Section 5. Finally, we summarize our positions in Section 6. Using ensemble techniques to improve classification is based on a sound theory [16; 70; 10; 44; 62]. In the unsupervised area of clustering, using ensemble techniques has at least a history of many empirical studies [67; 25; 55; 24; 33]. Fur-thermore, the idea of using several different clustering results is important not only in ensemble clustering as an explicit technique but also in related approaches such as multi-view clustering, subspace clustering, and alternative clustering [11; 59; 31; 50; 77]. The ensemble idea has also been used when clustering evaluation measures are combined [72]. By a simple transfer of ideas from these research results in the area of ensemble clustering (and related areas), we can assume that a combination of outlier detection models would also show potential to improve considerably over the com-bined individual models. Also, we can assume, by analogy, that diversity of models would be helpful in outlier detection as it is in clustering or classification.
 Surprisingly, for outlier detection there have not been many attempts to use ensemble techniques for improvement in a principled way, let alone investigations of the theoretical ba-sis of doing so. When comparing the tasks of outlier detec-tion and clustering, we can name several reasons for this sur-prising fact  X  reasons, that, at the same time, highlight the research issues that are different for the design of ensemble methods for outlier detection than for ensemble clustering. 1. The first issue is the question of how to measure ac-2. The second issue that is important for building good 3. The third issue, when given individual models (that If given a ground truth dataset where we know, for each object, whether it actually is an outlier or not, two ways of measuring the quality of the outlier detection result are commonly used in the literature [76].
 The first, more widely used measure of success is based on receiver operating characteristic (ROC) curves. ROC curves plot the true positive rate against the false positive rate. The resulting, monotone curves are usually turned into a measure by computing the area under this curve (AUC). This allows to display several results in a single graph and to compare the results numerically.
 For a random ranking result, both rates (true positive rate and false positive rate) will grow at the same rate, resulting in an area that approximately fills half of the space. For a perfect result, returning all outliers first and only then re-turning the inliers (i.e., we have 100% true positives before we even get the first false positive), the area under the cor-responding curve will cover the available space completely, i.e., the maximal ROC AUC value is 1 . 0. Intuitively, the ROC AUC value can be seen as the probability that a pair of two randomly chosen objects, one positive example (out-lier) and one negative example (inlier), is sorted correctly (i.e., the outlier is ranked before the inlier) [29]. ROC curves and ROC AUC analysis inherently treat the class imbalance problem by using the relative frequencies which makes them particularly popular for evaluation of outlier detection. Sometimes, additionally or alternatively to ROC analysis, the precision of the result is assessed for a given number k of top outliers: How many of the top k ranked data objects are actually outliers? This is known as  X  X recision at k  X . As an evaluation measure, this is a bit more problematic, as it involves a parameter.
 Both quality measures require data with known, annotated outliers, or, to put it in terms of classification, a binary, yet typically highly imbalanced classification task (very few outliers vs. many inliers).
 Although the task of outlier detection is practically ubiq-uitous, these practical tasks are tasks because the ground truth is unknown. There is nothing like established bench-mark data sets in this field, required to study and compare the behavior of algorithms for outlier detection. What peo-ple do, for example, is using classification data sets such as available in the UCI repository [7]. To prepare an outlier de-tection task from such classification tasks, one can, e.g., pick some class as outlying and keep only a small sample of this outlier class while the other classes remain complete and are treated as inliers. This procedure is sometimes called  X  X own sampling X  and has been used, with different variants, in many studies designing new outlier detection methods [1; 73; 42; 74; 36; 15; 14]. A recent study is dedicated to develop a more systematic approach [18], but this is also merely a wider step in the same direction  X  a direction that probably is debatable.
 Let as note, however, that all these problems regarding ex-ternal evaluation are not specific for outlier detection en-sembles but are inflicting the research on outlier detection in general. To the best of our knowledge, there are no insights whatso-ever in the literature on outlier detection regarding internal validation measures. As challenges for future research on the aspect of quality assessment of outlier detection results, we see the following issues and questions for research: Actually, since the effectiveness of internal criteria them-selves will need to be evaluated, the first challenge posed above, namely, to provide better, more principled and ob-jective possibilities for external evaluation of results, will be an important prerequisite for the second challenge. Typical unsupervised methods for outlier detection return a score of  X  X utlierness X  for each object. When assuming a fixed order of the objects in the dataset, an outlier detec-tion result can be thought of as a vector consisting of the outlier scores for each object [63]. This way, we can define the space of all possible outlier detection results for a given dataset, where each dimension represents the possible out-lier scores of a particular observation. Let us, for the sake of simplicity, consider some outcomes for two objects, resulting in a two-dimensional plot as illustrated in Figure 1 (for n objects, the space would be n -dimensional and not suitable for visualization). The green circle represents the (usually unknown) ground truth, while the red crosses are individual results generated by somehow diverse outlier models. Figure 1(a) shows how the combination of the six individ-ual result vectors by the simple component-wise mean pro-duces another result, represented by the blue X. This com-bined result is, in this case, a better approximation of the ground truth (green circle). All individual solutions are al-ready quite accurate, that is, they are close to the ground truth. This is a necessary condition for assembling these individual solutions to make a good ensemble, which can be illustrated by the following reasoning: It is known that the ground truth is located somewhere in the result space, but it could be anywhere. The generation of multiple individ-ual, more or less accurate (i.e., at least better than random) results restricts the space of where the true result most prob-ably lies: if they are accurate to some extent, the true result will be close to them. The motivation for combining the in-dividual results by, for example, computing the mean score for each observation is the expectation that the true result will be somewhere between them. In fact, for combination techniques like the mean, the convex hull of the individual results already restricts the result space to an area where the true result is expected to be, and where ensembles generate their integrating results.
 Figure 1(b) illustrates the limited effects of accuracy when diversity is missing. It can easily be seen that, again, the in-dividual results are quite accurate. However, the combined result gets rather attracted towards the majority of the sin-gle results. If the upper right, rather deviating, result would not exist, the combined result would lie completely inside the tight cluster of remaining results and would be even more distant to the true result. This is the effect of miss-ing diversity. All single results of that tight cluster make the same error: They underestimate the outlier score for the object that is represented by the x -axis. In comparison, Figure 1(a) shows results which make different errors, each of them over-and underestimating a score, resulting in an accurate ensemble result. Figure 1: Diverse and clustered outlier detection results. Of course, diversity is not the only criterion. Ignoring accu-racy and maximizing diversity would scatter the individual results all across the result space without any restriction. Both the true result and a combined result could reside any-where in the complete result space, not necessarily being close to each other.
 Seeing the outlier detection results as vectors in a vector space, spanned by the observations of a given dataset, as depicted in Figure 1, allows us to see both components, ac-curacy and diversity: accuracy of individual ensemble mem-bers (red crosses) is represented by the absolute distances from the true result (green circle), while the diversity is re-flected in the relative distances , taking into account also the direction of deviation , from the true result. Clearly, both cri-teria, accuracy and diversity, are antagonistic to a certain degree. The more accurate the individual results are, the tighter they are packed and therefore the less diverse they are. And the more diverse the individual results are, the less accurate most of them can possibly be. If we just transfer the intuition on ensemble learning from supervised ensem-bles, the essential requirement is that individual ensemble members would commit errors different from the other en-semble members whenever they are committing errors at all. As long as they are correct they also should be in ac-cordance with the other ensemble members. Let us note that, interestingly, the vector space intuition of outlier de-tection results would also allow us to talk about subspaces (i.e., subsets of objects) where result vectors would cluster. So maybe it is possible to transfer insights from subspace clustering [41; 66; 77] to the area of outlier ensembles. So far, this is an intuition basically transferred from su-pervised ensemble learning without any further theoretical understanding. Questions tackled in the literature are how to induce diversity among models (Section 4.1) and how to assess diversity along with the impact of diversity on the ensemble performance (Section 4.2). If we are given diverse models, another interesting question is if we can select some of them to build better ensembles than if we selected all of them. We sketch a greedy approach to this problem (Sec-tion 4.3) and suggest challenges and issues for future re-search w.r.t. to the diversity of models (Section 4.4). Mostly in analogy to methods for inducing diversity in clas-sification ensembles [10] or clustering ensembles [24], there have been studies on combining outlier scores (or rankings) (1) learned in different subsets of attributes (i.e., different subspaces), (2) learned on different subsets of objects, (3) learned by randomized methods, (4) learned by the same method but using different parametrization, and (5) learned by different models. 1. Combining outlier scores or rankings learned on dif-2. The orthogonal approach, combining models learned 3. The approach of  X  X solation forests X  [46] designs a ran-4. Combining models learned using different parameters 5. Combining outlier scores of different algorithms (i.e., Overall, these studies highlight three important aspects for outlier ensembles: assessment of diversity, normalization of scores, and combination procedures, which we discuss in Sec-tions 4.2, 5.1, and 5.2, respectively.
 The greedy combination strategy [63] also raises an inter-esting challenge for outlier ensembles: how to choose good ensemble members or how to train improved ensemble mem-bers based on previously learned and evaluated models in the absence of a ground truth for evaluation. This challenge has also been noted by Aggarwal [2]. The heuristic of the greedy ensemble will be discussed in Section 4.3.
 At first sight, thinking about the transfer of techniques from classification or clustering ensembles, it may seem that with the five mentioned categories of heuristics for inducing di-versity the obvious opportunities have been studied in the literature. However, all these studies leave room for deeper understanding of these heuristics and there are probably more methods for inducing diversity waiting to be explored that perhaps do not have a counterpart in classification or clustering ensembles. For example, diverse models could be learned by using different distance measures. This has only been studied partly [63], assessing the resulting diversity of models but not the quality of ensembles combining these models. Having seen different methods for inducing diversity that are known from classification or clustering ensembles, the question arises how well these methods work in the context of outlier detection. This question was addressed by a re-cent study [63], proposing the vector space of outlier scores that we sketched above and weighted Pearson correlation as a similarity measure for these score vectors. This study dis-cussed two use cases of such an assessment of diversity: (1) studying the suitability of methods for inducing diversity and (2) selecting the most diverse models for combination. The latter we discuss in Section 4.3, as mentioned earlier. Let us discuss the first aspect now.
 The idea of using a weighted similarity measure, such as weighted Pearson, to compare score vectors, is motivated by the relative importance of outlier scores while differences in inlier scores should not matter that much. Given a ground truth (i.e., using some dataset with known outliers), the weights for the similarity measure comparing score vectors can be adjusted to this ground truth. Studying some meth-ods, some distance measures, and some datasets using such a weighted similarity measure, the findings reported by Schu-bert et al. [63] are: The finding of weakly correlated results by feature bagging is also reflected in the finding of feature bagging being rather unstable [75]. From the perspective of building ensembles, instability is not necessarily a bad thing although the com-bination of very different models is not bound to lead to a good ensemble. Diversity, after all, is only one aspect be-sides accuracy and too much of diversity is bound to limit accuracy. Aggarwal [2] pointed out that analogues of  X  X oosting X  or  X  X ucket of models X   X  established concepts for supervised ensemble learning  X  are unlikely to be developed for unsu-pervised outlier detection. We pointed out (Section 3), that internal validity measures for outlier detection results are still missing in the literature and would be very important. Yet this does not mean that model selection is impossible: at least a greedy heuristic, optimizing diversity in an en-semble, has been discussed recently [63]. This rather crude heuristic (see a sketch in Algorithm 1) relies on an accuracy estimate based on all learned potential ensemble members. The method first takes the union of the top k points of all results as preliminary outliers for determining weights for the similarity measure (weighted Pearson), assessing the di-versity between results. Then the ensemble is composed, starting with the result that is closest to this consensus re-sult. Next the remaining outlier detectors are sorted by the lowest correlation to the result of the current ensemble (initially, the ensemble consists only of one outlier detec-tor) and test if including the next detector would improve Algorithm 1: Greedy Model Selection /* individual outlier detectors: */ I := list of individual outlier detectors; K := union of top-k outliers  X  I ; /* K are the preliminary  X  X utliers X  */ v := target vector; /* ( v i = 1 if object i  X  K , v i = 0 , otherwise) */ E :=  X  ensemble; sort I by weighted Pearson correlation to v ; E := E S getFirst( I ); p := current prediction of E ; sort I by weighted Pearson to p (decreasing order); while I 6 =  X  do end return E ; the correlation of the ensemble result with the (preliminary) target vector (i.e., the estimated ground truth). If yes, this detector is included in the ensemble and the list of remain-ing detectors is reordered. If no, the detector is discarded and the algorithm continues with the next detector. Note that the whole process works in a completely unsupervised manner in that no actual ground truth is used.
 This heuristic is based on the assumption that the union of the complete set of individual outlier detectors is somehow accurate but can be improved by dropping those detectors that are strongly correlated with others. This assumption serves to overcome the limitations of unsupervised ensem-ble learning by unavailability of training data. Although we cannot see  X  so far  X  analogues of boosting for unsuper-vised learning either, very likely better heuristics than this greedy model selection are possible. In terms of the issue of accuracy (Section 3), this heuristic is using internally con-structed means of validation as if it were externally available ground truth. As challenges for future research on the aspect of diversity for outlier ensembles, we see the following research ques-tions: Having derived a couple of outlier detection results, or vec-tors of outlier scores, that are  X  ideally  X  diverse and ac-curate to some extent, the third central question is how to combine them to derive a consensus or ensemble result. The two issues we discuss here in particular are the require-ment of score normalization for a meaningful combination of scores (Section 5.1) and the different possibilities to com-bine (normalized) score vectors (Section 5.2). Some prefer to combine the rankings instead of the score vectors which we will touch upon (Section 5.3). We suggest challenges and issues for future research w.r.t. the combination of models (Section 5.4). Any meaningful combination of score vectors relies heavily on the scores provided by the individual outlier detectors being comparable. This problem practically rules out the combination of different base methods or, for many methods, different parametrizations of the same method (e.g., differ-ent k for a k NN-distance-based method, as the result with the largest k would dominate the distance values). Even when using the same method as base outlier detector and identical parametrization, outlier scores obtained from dif-ferent subspaces could vary considerably, if some subspaces have largely different scales. The ensemble could then be dominated by just one of the feature bags.
 Several of the papers discussing outlier detection ensem-bles focused on the issue of comparability of scores for score combinations. The first approach was to use sigmoid func-tions and mixture modeling to fit outlier scores, provided by different detectors, into comparable probability values [23]. The second approach was scaling by standard devia-tion [54]. Finally, statistical reasoning about typical score distributions by different methods enabled normalizations tailored to particular properties of different methods [40]. Although the solutions provided so far probably leave room for improvements, the important thing is to realize the prob-lem and to use some normalization when combining outlier scores. To provide a good estimate of the actual probability of some object being an outlier is something valuable for supporting the user in the interpretation of the individual outlier detection result. For combination of several results into an ensemble, this calibration is perhaps not equally im-portant. But normalization of scores is important, to avoid a bias of the decision to the individual result with the largest scale. This distinction is somewhat analogous to the dis-tinction between class probability estimates and classifica-tion decisions based on these probability estimates that has been emphasized for the understanding of the performance of the na  X  X ve Bayes classifier [17]: the outlier scores need not be good absolute outlier  X  X robabilities X  in order to make sense for a combination but their relative scale needs to re-flect the actual ratio of outlierness for compared objects. Let us assume we are provided with accurate, diverse, and normalized outlier scores, where normalization includes reg-ularization [40], i.e., without loss of generality we can as-sume that the larger score denotes  X  X ore X  outlierness while inliers should have been assigned small score values. Now the question remains how to combine the scores. Aggar-wal [2, Section 4.2] discusses this issue as well, mentioning several interesting possibilities. As the most commonly used methods he names the maximum and the average function but which combination function is best remains an open question. We do not intend to answer this question, but rather to contribute to the debate. In our opinion, from the point of view of ensembles, using the maximum of scores has some decisive disadvantages whereas the average does seem to make more sense.
 To understand the disadvantage of using the maximum score, consider Figure 2, depicting the result of the maximum func-tion (blue X) as combination of the individual score vectors (red crosses), in comparison to the average in Figure 1. The maximum as a combination function results in the upper bound of all individual scores and, hence, has a tendency to overestimate the outlierness. This also means that a single result that is far off, overestimating the scores for some ob-jects, will determine the ensemble result (e.g., Figure 2(b)). Errors for different objects, contributed by different individ-ual outlier score vectors, can lead the maximum combination actually more off while all individual outlier scores are not too bad overall (e.g., Figure 2(a))  X  remember that this is a toy example for the rankings of two objects only. For a realistic scenario with n 2 objects, it would be even more likely that some individual score vector is off for some single object (i.e., in a one-dimensional subspace) and all the other score vectors would not matter at all for this object. This contradicts the very intuition of building ensembles. Errors of each individual score vector for single objects are strongly emphasized, and an error of a single ensemble mem-ber assigning a high outlier score to some object cannot be compensated for, even if all other detectors would be cor-rect. This drawback counteracts one of the most funda-mental benefits that one can expect from using an ensemble method: the correction of errors committed by single ensem-ble members. Let us note that keeping the maximum scores from different outlier models, if these models are learned in different subspaces, could be an approach to the problem of  X  X ultiview X  outliers that we mentioned in the introduc-tion  X  and that is somehow orthogonal to the ensemble or consensus idea.
 On the other hand, using the average of scores has been theo-retically advocated [75] for the combination of outlier scores based on (local) density estimates (as used by many classic methods such as LOF and its variants [9; 56; 38; 39] or the k NN outlier model and its variants [60; 4; 74]). 1 Building the average of different density estimates allows to abstract from the individual errors of these density estimates and, instead, to reason about the expected error. This reasoning
Note that the set of methods using local density estimates is not restricted to the so-called local outlier detection meth-ods. These are two different notions of  X  X ocal X , as elaborated by Schubert et al. [65]. Figure 2: The maximum for score combination results in an upper bound of the result vectors. might open up possibilities to improve our theoretic under-standing of the benefit of ensembles for outlier detection. However, the choice of a particular combination function will also remain application dependent. If the cost of missing a single outlier is much higher than the cost for a high false alarm rate, using the maximum combination is certainly worth considering. On the other hand, in an application scenario where the cost for false negatives is very high but missing some outliers might not hurt too much, maybe even the minimum as a combination method for score methods may be a good choice. This would mean that all individ-ual methods would have to assign a high outlier score to an object in order to actually count this object as an outlier. If just one of the ensemble members assigns a small outlier score, the minimum ensemble would use this smallest score for this object. Combining rankings provided by outlier methods, ignoring the actual outlier scores, could be seen as a particular way of normalization. But there is a considerable amount of litera-ture in databases and in information retrieval on the combi-nation of rankings that opens up possibilities for transferring known results from these areas to the particular problem of outlier detection. The feature bagging method [45], for ex-ample, was discussed in combination with a breadth-first traversal rank combination, i.e., taking the top ranked ob-ject from each individual ranking, then the second rank and so on. This is almost equivalent to using the maximum as score combination function 2 and, thus, has the same pros and cons.
 Most methods, however, use the outlier scores and not only the rankings. This might be motivated by the assumption that the scores and their relative differences have at least some meaning (an assumption that actually might be de-batable, in particular for high dimensional data due to an effect analogous to the concentration of distances [76]). See also the discussion of normalization issues by Aggarwal [2, Section 4.1]. As challenges for future research on the aspect of combining several outlier detection results to a consensus or ensem-ble ranking or score vector, we see the following issues and questions for research:
Different from the maximum score combination, this breadth-first rank combination introduces a discretization and the resulting ranking depends on the order of the indi-vidual rankings for the traversal. Aggarwal [2] discussed algorithmic patterns, identified traces of the ensemble idea in the literature, and touched upon more or less likely options for future transfer of concepts from supervised ensembles for classification to ensembles for unsupervised outlier detection. Complementing his overview, we focused on the fundamental ingredients for success in building ensembles for unsupervised outlier detection. These are (1) learning accurate but (2) diverse models and (3) com-bining these models (or a selection thereof). For all these aspects, the literature provides not more than some first steps and insights which we sketched in this paper. As we point out, there are many opportunities to improve, for all aspects, we listed some challenges and issues for future re-search. It is our hope to stimulate research on the surpris-ingly neglected but very interesting and promising topic of ensembles for outlier detection. [1] N. Abe, B. Zadrozny, and J. Langford. Outlier detec-[2] C. C. Aggarwal. Outlier ensembles [position paper]. [3] C. C. Aggarwal. Outlier Analysis . Springer, 2013. [4] F. Angiulli and C. Pizzuti. Fast outlier detection in high [5] H. Ayad and M. Kamel. Finding natural clusters us-[6] J. Azimi and X. Fern. Adaptive cluster ensemble selec-[7] K. Bache and M. Lichman. UCI machine learning repos-[8] V. Barnett and T. Lewis. Outliers in Statistical Data . [9] M. M. Breunig, H.-P. Kriegel, R. Ng, and J. Sander. [10] G. Brown, J. Wyatt, R. Harris, and X. Yao. Diversity [11] R. Caruana, M. Elhawary, N. Nguyen, and C. Smith. [12] V. Chandola, A. Banerjee, and V. Kumar. Anomaly de-[13] V. Chandola, A. Banerjee, and V. Kumar. Anomaly de-[14] X. H. Dang, I. Assent, R. T. Ng, A. Zimek, and E. Schu-[15] X. H. Dang, B. Micenkova, I. Assent, and R. Ng. Out-[16] T. G. Dietterich. Ensemble methods in machine learn-[17] P. Domingos and M. Pazzani. Beyond independence: [18] A. F. Emmott, S. Das, T. Dietterich, A. Fern, and W.-[19] X. Z. Fern and C. E. Brodley. Random projection for [20] X. Z. Fern and W. Lin. Cluster ensemble selection. Sta-[21] A. L. N. Fred and A. K. Jain. Combining multiple [22] I. F  X arber, S. G  X unnemann, H.-P. Kriegel, P. Kr  X oger, [23] J. Gao and P.-N. Tan. Converting output scores from [24] J. Ghosh and A. Acharya. Cluster ensembles. Wiley In-[25] A. Gionis, H. Mannila, and P. Tsaparas. Clustering ag-[26] F. E. Grubbs. Procedures for detecting outlying obser-[27] S. T. Hadjitodorov and L. I. Kuncheva. Selecting diver-[28] S. T. Hadjitodorov, L. I. Kuncheva, and L. P. Todorova. [29] J. A. Hanley and B. J. McNeil. The meaning and use of [30] D. Hawkins. Identification of Outliers . Chapman and [31] M. S. Hossain, S. Tadepalli, L. T. Watson, I. David-[32] L. Hubert and P. Arabie. Comparing partitions. Jour-[33] N. Iam-On and T. Boongoen. Comparative study of [34] W. Jin, A. K. H. Tung, J. Han, and W. Wang. Rank-[35] L. Kaufman and P. J. Rousseeuw. Finding Groups in [36] F. Keller, E. M  X uller, and K. B  X ohm. HiCS: high contrast [37] E. M. Knorr and R. T. Ng. A unified notion of out-[38] H.-P. Kriegel, P. Kr  X oger, E. Schubert, and A. Zimek. [39] H.-P. Kriegel, P. Kr  X oger, E. Schubert, and A. Zimek. [40] H.-P. Kriegel, P. Kr  X oger, E. Schubert, and A. Zimek. In-[41] H.-P. Kriegel, P. Kr  X oger, and A. Zimek. Subspace clus-[42] H.-P. Kriegel, M. Schubert, and A. Zimek. Angle-based [43] L. I. Kuncheva and S. T. Hadjitodorov. Using diver-[44] L. I. Kuncheva and C. J. Whitaker. Measures of diver-[45] A. Lazarevic and V. Kumar. Feature bagging for out-[46] F. T. Liu, K. M. Ting, and Z.-H. Zhou. Isolation-based [47] M. J. A. N. C. Marquis de Condorcet. Essai sur [48] M. Meila. Comparing clusterings  X  an axiomatic view. [49] D. Moulavi, P. A. Jaskowiak, R. J. G. B. Campello, [50] E. M  X uller, S. G  X unnemann, I. F  X arber, and T. Seidl. Dis-[51] E. M  X uller, I. Assent, P. Iglesias, Y. M  X ulle, and K. B  X ohm. [52] E. M  X uller, M. Schiffer, and T. Seidl. Statistical selec-[53] M. C. Naldi, A. C. P. L. F. Carvalho, and R. J. G. B. [54] H. V. Nguyen, H. H. Ang, and V. Gopalkrishnan. Min-[55] N. Nguyen and R. Caruana. Consensus clusterings. In [56] S. Papadimitriou, H. Kitagawa, P. Gibbons, and [57] D. Pfitzner, R. Leibbrandt, and D. Powers. Charac-[58] N. Pham and R. Pagh. A near-linear time approxi-[59] Z. J. Qi and I. Davidson. A principled and flexible [60] S. Ramaswamy, R. Rastogi, and K. Shim. Efficient al-[61] W. M. Rand. Objective criteria for the evaluation of [62] L. Rokach. Ensemble-based classifiers. Artificial Intel-[63] E. Schubert, R. Wojdanowski, A. Zimek, and H.-P. [64] E. Schubert, A. Zimek, and H.-P. Kriegel. Generalized [65] E. Schubert, A. Zimek, and H.-P. Kriegel. Local outlier [66] K. Sim, V. Gopalkrishnan, A. Zimek, and G. Cong. A [67] A. Strehl and J. Ghosh. Cluster ensembles  X  a knowl-[68] A. Topchy, A. Jain, and W. Punch. Clustering ensem-[69] A. P. Topchy, M. H. C. Law, A. K. Jain, and A. L. Fred. [70] G. Valentini and F. Masulli. Ensembles of learning ma-[71] L. Vendramin, R. J. G. B. Campello, and E. R. Hr-[72] L. Vendramin, P. A. Jaskowiak, and R. J. G. B. [73] J. Yang, N. Zhong, Y. Yao, and J. Wang. Local pecu-[74] K. Zhang, M. Hutter, and H. Jin. A new local distance-[75] A. Zimek, M. Gaudet, R. J. G. B. Campello, and [76] A. Zimek, E. Schubert, and H.-P. Kriegel. A survey [77] A. Zimek and J. Vreeken. The blind men and the ele-
