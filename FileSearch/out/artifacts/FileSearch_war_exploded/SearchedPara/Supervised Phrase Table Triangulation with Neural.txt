 Phrase-based statistical machine translation sys-tems require considerable amounts of source-target parallel data to produce good quality trans-lation. However, large amounts of parallel data are available for only a fraction of language pairs, and mostly when one of the languages is English.
Phrase table triangulation (Utiyama and Isa-hara, 2007; Cohn and Lapata, 2007; Wu and Wang, 2007) is a method for generating source-target phrase tables without having access to any source-target parallel data. The intuition behind triangulation (and pivoting techniques in general) is the transitivity of translation: if a source lan-guage phrase s translates to a pivot language phrase p which in turn translates to a target lan-guage phrase t , then s should likely translate to t . Following this intuition, a triangulated source-target phrase table  X  T can be composed from a source-pivot and pivot-target phrase table (  X  2).
However, the resulting triangulated phrase table  X  T contains many spurious phrase pairs and noisy probability estimates. Therefore, early triangula-tion work (Wu and Wang, 2007) already realis-tically assumed access to a limited source-target parallel data from which a relatively high-quality source-target phrase table T can be directly esti-mated. The two phrase tables were then combined, resulting in a higher quality phrase table that pro-poses translations for many source phrases not found in T . Wu and Wang (2007) report that in-terpolation of the two phrase tables T and  X  T leads to higher quality translations. However, the trian-gulated phrase table  X  T is obtained without using the source-target bilingual data, which suggests that the source-target data is not used as fully as it could be.

In this paper, we develop a supervised learning algorithm that corrects triangulated word transla-tion probabilities by relying on word translation distributions w sup derived from the limited source-target data. In particular, we represent source and target words using word embeddings (Mikolov et al., 2013) and learn a transformation between the two embedding spaces in order to approxi-mate w sup , thus down-weighting incorrect trans-lation candidates proposed by triangulation (  X  3). By representing words as embeddings, our model can generalize the information contained in the source-target data (as encoded in the distributions sign lexical-weighting probabilities to most of the phrase pairs in  X  T .

Fixing English as the pivot language (the most realistic pivot language choice), on a low-resource Spanish-to-French translation task our model gains + 0.7 B leu on top of standard phrase table interpolation. On Malagasy-to-French trans-lation, our model gains + 0.5 B leu on top of tri-angulation when using only 1k Malagasy-French dictionary entries (  X  4). Let s , p , t denote words and s , p , t denote phrases in the source, pivot, and target languages, respec-tively. Also, let T denote a phrase table estimated over a parallel corpus and  X  T denote a triangu-lated phrase table. We use similar notation for their respective phrase translation features  X  , lexical-weighting features lex , and the word translation probabilities w . 2.1 Triangulation (weak baseline) In phrase table triangulation, a source-target phrase table T st is constructed by combining a source-pivot and pivot-target phrase table T sp , T pt , each estimated on its respective parallel data. For each resulting phrase pair ( s , t ), we can also com-pute an alignment  X  a as the most frequent align-ment obtained by combining source-pivot and pivot-target alignments a sp and a pt across all pivot phrases p as follows: { ( s , t ) |  X  p : ( s , p )  X  a sp ( p , t )  X  a pt } .

The triangulated source-to-target lexical weights, denoted c lex st , are approximated in two steps: First, word translation scores  X  w st are ap-proximated by marginalizing over the pivot words: Next, given a (triangulated) phrase pair ( s , t ) with weighting probability is (Koehn et al., 2003): The triangulated phrase translation scores, de-noted  X   X  st , are computed by analogy with Eq. 1.
We also compute these scores in the reverse direction by swapping the source and target lan-guages. 2.2 Interpolation (strong baseline) Given access to source-target data, an ordinary source-target phrase table T st can be estimated di-rectly. Wu and Wang (2007) suggest interpolating phrase pairs entries that occur in both tables: Phrase pairs appearing in only one phrase table are added as-is. We refer to the resulting table as the interpolated phrase table. While interpolation (Eq. 3) may help correct some of the noisy triangulated scores, its e ff ect is lim-ited to phrase pairs appearing in both phrase ta-bles. Here, we suggest a discriminative supervised learning method that can a ff ect all phrase pairs.
Our idea is to regard word translation distri-butions derived from source-target bilingual data (through word alignments or dictionary entries) as the correct translation distributions, and use them to learn discriminately: correct target words should become likely translations, and incorrect ones should be down-weighted. To generalize be-yond the vocabulary of the source-target data, we appeal to word embeddings.

We present our formulation in the source-to-target direction. The target-to-source direction is obtained simply by swapping the source and tar-get languages. 3.1 Model Let c sup st denote the number of times source word s was aligned to target word t (in word alignment, or in the dictionary). We define the word transla-c word translation probabilities we wish to learn and consider maximizing the log-likelihood function: Clearly, the solution q (  X  | s ) : = w sup (  X  | s ) maxi-mizes L . However, we would like a solution that generalizes to source words s beyond those ob-served in the source-target corpus  X  in particular, those source words that appear in the triangulated phrase table  X  T , but not in T .

In order to generalize, we abstract from words to vector representations of words. Specifically, we constrain q to the following parameterization: Here, the vectors v s and v t represent monolingual features and the vector f st represents bilingual fea-tures. The parameters A and h are to be learned.
In this work, we use monolingual word embed-dings for v s and v t , and set the vector f st to con-tain only the value of the triangulated score, such that f st : =  X  w st . Therefore, the matrix A is a lin-ear transformation between the source and target embedding spaces, and h (now a scalar) quantifies how the triangulated scores  X  w are to be trusted.
In the normalization factor Z s , we let t range only over possible translations of s suggested by either w sup or the triangulated word probabilities. That is: This restriction makes e ffi cient computation pos-sible, as otherwise the normalization term would have to be computed over the entire target vocab-ulary.

Under this parameterization, our goal is to solve the following maximization problem: 3.2 Optimization The objective function in Eq. 4 is concave in both A and h . This is because after taking the log, we are left with a weighted sum of linear and concave (negative log-sum-exp) terms in A and h . We can therefore reach the global solution of the problem using gradient descent.

Taking derivatives, the gradient is where the scalar m st = c sup st  X  c sup s q ( t | s ) for the current value of q .

For quick results, we limited the number of gra-dient steps to 200 and selected the iteration that minimized the total variation distance to w sup over a held out dev set: We obtained better convergence rate by us-ing a batch version of the e ff ective and easy-to-implement Adagrad technique (Duchi et al., 2011). See Figure 1. 3.3 Re-estimating lexical weights Having learned the model ( A and h ), we can now use q ( t | s ) to estimate the lexical weights (Eq. 2) of any aligned phrase pairs ( s , t ,  X  a ), assuming it is composed of embeddable words. Figure 1: The (target-to-source) objective function per iteration. Applying batch Adagrad (blue) sig-nificantly accelerates convergence.

However, we found the supervised word trans-lation scores q to be too sharp, sometimes assign-ing all probability mass to a single target word. We therefore interpolated q with the triangulated word translation scores  X  w : To integrate the lexical weights induced by q  X  (Eq. 2), we simply appended them as new features in the phrase table in addition to the existing lexi-cal weights. Following this, we can search for a  X  value that maximizes B leu on a tuning set. 3.4 Summary of method In summary, to improve upon a triangulated or in-terpolated phrase table, we: 1. Learn word translation distributions q by super-2. Smooth the learned distributions q by interpo-3. Compute new lexical weights and append them To test our method, we conducted two low-resource translation experiments using the phrase-based MT system Moses (Koehn et al., 2007). 4.1 Data Fixing the pivot language to English, we applied our method on two data scenarios: 1. Spanish-to-French: two related languages 2. Malagasy-to-French: two unrelated languages
Table 1 lists some statistics of the bilin-gual data we used. European-language bitexts were extracted from Europarl (Koehn, 2005). For Malagasy-English, we used the Global Voices par-and the small Malagasy-French tune / test sets were Table 1: Bilingual datasets. Legend: sp = Spanish, fr = French, en = English, mg = Malagasy.

Table 2 lists token statistics of the monolin-French, Spanish and Malagasy word embeddings. The French and Spanish embeddings were (in-dependently) estimated over their combined to-the Malagasy Wikipedia and the Malagasy Com-French language model over the French monolin-gual data. Table 2: Size of monolingual corpus per language as measured in number of tokens. 4.2 Spanish-French Results To produce w sup , we aligned the small Spanish-French parallel corpus in both directions, and symmetrized using the intersection heuristic. This was done to obtain high precision alignments (the often-used grow-diag-final-and heuristic is opti-mized for phrase extraction, not precision). We used the skip-gram model to estimate the Spanish and French word embeddings and set the dimension to d = 200 and context window to w = 5 (default). Subsequently, to run our method, we filtered out source and target words that either did not appear in the triangulation, or, did not have an embedding. We took words that appeared more than 10 times in the parallel corpus for the training set (  X  690 words), and between 5 X 9 times for the held out dev set (  X  530 words). This was done in both source-target and target-source directions.
In Table 3 we show that the distributions learned by our method are much better approximations of
Method source  X  target target  X  source triangulation 71.6% 72.0% our scores 30.2% 33.8% Table 3: Average total variation distance (Eq. 5) to the dev set portion of w sup (computed only over words whose translations in w sup appear in the tri-angulation). Using word embeddings, our method is able to better generalize on the dev set.
We then examined the e ff ect of appending our supervised lexical weights. We fixed the word level interpolation  X  : = 0 . 95 (e ff ectively assigning very little mass to triangulated word translations to maximize B leu on the tuning set.

Our MT results are reported in Table 4. While interpolation improves over triangulation alone by + 0.8 B leu , our method adds another + 0.7 B leu on top of interpolation, a statistically significant gain ( p &lt; 0 . 01) according to a bootstrap resampling significance test (Koehn, 2004).
Method  X  tune test source-target  X  26.8 25.3 triangulation  X  29.2 28.4 interpolation 0.7 30.2 29.2 interpolation + our scores 0.6 30.8 29.9 Table 4: Spanish-French B leu scores. Append-ing lexical weights obtained by supervision over a small source-target corpus significantly out-performs phrase table interpolation (Eq. 3) by + 0.7 B leu . 4.3 Malagasy-French Results For Malagasy-French, the w sup distributions used for supervision were taken to be uniform distri-butions over the dictionary translations. For each training direction, we used a 70% / 30% split of the dictionary to form the train and dev sets.

Having significantly less Malagasy monolin-gual data, we used d = 100 dimensional embed-dings and a w = 3 context window to estimate both Malagasy and French words.

As before, we added our supervised lexical weights as new features in the phrase table. How-ever, instead of fixing  X  = 0 . 95 as above, we imize B leu on a small tune set. We report our re-sults in Table 5. Using only a dictionary, we are able to improve over triangulation by + 0.5 B leu , a statistically significant di ff erence ( p &lt; 0 . 01).
Method  X  tune test triangulation  X  12.2 11.1 triangulation + our scores 0.6 12.4 11.6 Table 5: Malagasy-French B leu . Supervision with a dictionary significantly improves upon simple triangulation by + 0.5 B leu . In this paper, we argued that constructing a trian-gulated phrase table independently from even very limited source-target data (a small dictionary or parallel corpus) underutilizes that parallel data.
Following this argument, we designed a super-vised learning algorithm that relies on word trans-lation distributions derived from the parallel data as well as a distributed representation of words (embeddings). The latter enables our algorithm to assign translation probabilities to word pairs that do not appear in the source-target bilingual data.
We then used our model to generate new lexi-cal weights for phrase pairs appearing in a trian-gulated or interpolated phrase table and demon-strated improvements in MT quality on two tasks. This is despite the fact that the distributions ( w sup ) we fit our model to were estimated automatically, or even na  X   X vely as uniform distributions. The authors would like to thank Daniel Marcu and Kevin Knight for initial discussions and a sup-portive research environment at ISI, as well as the anonymous reviewers for their helpful comments. This research was supported in part by a Google Faculty Research Award to Chiang.

