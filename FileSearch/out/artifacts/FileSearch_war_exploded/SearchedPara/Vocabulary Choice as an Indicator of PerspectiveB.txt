 We address the task of perspective classification. Apart from the spatial sense not considered here, perspective can refer to an agent X  X  role (doctor vs patient in a dialogue), or understood as  X  X  par-ticular way of thinking about something, espe-cially one that is influenced by one X  X  beliefs or experiences, X  stressing the manifestation of one X  X  broader perspective in some specific issue, or  X  X he state of one X  X  ideas, the facts known to one, etc., in having a meaningful interrelationship, X  stress-ing the meaningful connectedness of one X  X  stances and pronouncements on possibly different issues. 1
Accordingly, one can talk about, say, opinion on a particular proposed legislation on abortion within pro-choice or pro-life perspectives; in this case, perspective essentially boils down to opi-nion in a particular debate. Holding the issue con-stant but relaxing the requirement of a debate on a specific document, we can consider writings from pro-and con-perspective, in, for example, the death penalty controversy over a course of a period of time. Relaxing the issue specificity somewhat, one can talk about perspectives of people on two sides of a conflict; this is not opposition or sup-port for any particular proposal, but ideas about a highly related cluster of issues, such as Israeli and Palestinian perspectives on the conflict in all its manifestations. Zooming out even further, one can talk about perspectives due to certain life con-tingencies, such as being born and raised in a par-ticular culture, region, religion, or political tradi-tion, such perspectives manifesting themselves in certain patterns of discourse on a wide variety of issues, for example, views on political issues in the Middle East from Arab vs Western observers.
In this article, we consider perspective at all the four levels of abstraction. We apply the same types of models to all, in order to discover any common properties of perspective classification. We contrast it with text categorization and with opinion classification by employing models rou-tinely used for such tasks. Specifically, we con-sider models that use term frequencies as features (usually found to be superior for text categoriza-tion) and models that use term absence/presence (usually found to be superior for opinion classi-fication). We motivate our hypothesis that pre-sence/absence features would be as good as or better than frequencies, and test it experimentally. Secondly, we investigate the question of feature redundancy often observed in text categorization. A line of inquiry going back at least to Zipf strives to characterize word frequency distributions in texts and corpora; see Baayen (2001) for a sur-vey. One of the findings in this literature is that a multinomial (called  X  X rn model X  by Baayen) is not a good model for word frequency distri-butions. Among the many proposed remedies (Baayen, 2001; Jansche, 2003; Baroni and Evert, 2007; Bhat and Sproat, 2009), we would like to draw attention to the following insight articulated most clearly in Jansche (2003). Estimation is im-proved if texts are construed as being generated by two processes, one choosing which words would appear at all in the text, and then, for words that have been chosen to appear, how many times they would in fact appear. Jansche (2003) describes a two-stage generation process: (1) Toss a z -biased coin; if it comes up heads, generate 0; if it comes up tails, (2) generate according to F (  X  ) , where F (  X  ) is a negative binomial distribution and z is a parameter controlling the extent of zero-inflation.
The postulation of two separate processes is effective for predicting word frequencies, but is there any meaning to the two processes? The first process of deciding on the vocabulary, or word types, for the text  X  what is its function? Jansche (2003) suggests that the zero-inflation component takes care of the multitude of vocabulary words that are not  X  X n topic X  for the given text, including taboo words, technical jargon, proper names. This implies that words that are chosen to appear are all  X  X n topic X . Indeed, text segmentation studies show that tracing recurrence of words in a text permits topical segmentation (Hearst, 1997; Hoey, 1991). Yet, if a person compares abortion to infan-ticide  X  are we content with describing this word as being merely  X  X n topic, X  that is, having a certain probability of occurrence once the topic of abor-tion comes up? In fact, it is only likely to occur if the speaker holds a pro-life perspective, while a pro-choicer would avoid this term.

We therefore hypothesize that the choice of vo-cabulary is not only a matter of topic but also of perspective, while word recurrence has mainly to do with the topical composition of the text. Therefore, tracing word frequencies is not going to be effective for perspective classification beyond noting the mere presence/absence of words, dif-ferently from the findings in text categorization, where frequency-based features usually do better than boolean features for sufficiently large voca-bulary sizes (McCallum and Nigam, 1998). Partial Birth Abortion (PBA) debates : We use transcripts of the debates on Partial Birth Abor-tion Ban Act on the floors of the US House and Senate in 104-108 Congresses (1995-2003). Simi-lar legislation was proposed multiple times, passed the legislatures, and, after having initially been ve-toed by President Clinton, was signed into law by President Bush in 2003. We use data from 278 legislators, with 669 speeches in all. We take only one speech per speaker per year; since many serve multiple years, each speaker is repre-sented with 1 to 5 speeches. We perform 10-fold cross-validation splitting by speakers, so that all speeches by the same speaker are assigned to the same fold and testing is always inter-speaker.
When deriving the label for perspective, it is im-portant to differentiate between a particular leg-islation and a pro-choice / pro-life perspective. A pro-choice person might still support the bill:  X  X  am pro-choice, but believe late-term abortions are wrong. Abortion is a very personal decision and a woman X  X  right to choose whether to ter-minate a pregnancy subject to the restrictions of Roe v. Wade must be protected. In my judgment, however, the use of this particular procedure can-not be justified. X  (Rep. Shays, R-CT, 2003). To avoid inconsistency between vote and perspective, we use data from pro-choice and pro-life non-governmental organizations, NARAL and NRLC, that track legislators X  votes on abortion-related bills, showing the percentage of times a legislator supported the side the organization deems consis-tent with its perspective. We removed 22 legisla-tors with a mixed record, that is, those who gave 20-60% support to one of the positions. 2
Death Penalty (DP) blogs : We use University of Maryland Death Penalty Corpus (Greene and Resnik, 2009) of 1085 texts from a number of pro-and anti-death penalty websites. We report 4-fold cross-validation (DP-4) using the folds in Greene and Resnik (2009), where training and testing data come from different websites for each of the sides, as well as 10-fold cross-validation performance on the entire corpus, irrespective of the site. 3
Bitter Lemons (BL) : We use the GUEST part of the BitterLemons corpus (Lin et al., 2006), con-taining 296 articles published in 2001-2005 on http://www.bitterlemons.org by more than 200 dif-ferent Israeli and Palestinian writers on issues re-lated to the conflict.

Bitter Lemons International (BL-I) : We col-lected 150 documents each by a different per-son from either Arab or Western perspectives on Middle Eastern affairs in 2003-2009 from http://www.bitterlemons-international.org/. The writers and interviewees on this site are usually former diplomats or government officials, aca-demics, journalists, media and political analysts. 4 The specific issues cover a broad spectrum, includ-ing public life, politics, wars and conflicts, educa-tion, trade relations in and between countries like Lebanon, Jordan, Iraq, Egypt, Yemen, Morocco, Saudi Arabia, as well as their relations with the US and members of the European Union. 3.1 Pre-processing We are interested in perspective manifestations using common English vocabulary. To avoid the possibility that artifacts such as names of senators or states drive the classification, we use as features words that contain only lowercase letters, possibly hyphenated. No stemming is performed, and no stopwords are excluded. 5 For generative models, we use two versions of Naive Bayes models termed multi-variate Bernoulli (here, NB -BOOL ) and multinomial (here, NB -COUNT ), respectively, in McCallum and Nigam (1998) study of event models for text cate-gorization. The first records presence/absence of a word in a text, while the second records the num-ber of occurrences. McCallum and Nigam (1998) found NB -COUNT to do better than NB -BOOL for sufficiently large vocabulary sizes for text catego-rization by topic. For discriminative models, we use linear SVM, with presence-absence, norma-lized frequency, and tfidf feature weighting. Both types of models are commonly used for text clas-sification tasks. For example, Lin et al. (2006) use NB -COUNT and SVM -NORMF for perspective clas-sification; Pang et al. (2002) consider most and Yu et al. (2008) all of the above for related tasks of movie review and political party classification. We use SVM light (Joachims, 1999) for SVM and WEKA toolkit (Witten and Frank, 2005; Hall et al., 2009) for both version of Naive Bayes. Param-eter optimization for all SVM models is performed using grid search on the training data separately for each partition into train and test data. 6 Table 2 summarizes the cross-validation results for the four datasets discussed above. Notably, the SVM -BOOL model is either the best or not signif-icantly different from the best performing model, although the competitors use more detailed textual information, namely, the count of each word X  X  ap-pearance in the text, either raw ( NB -COUNT ), nor-malized ( SVM -NORMF ), or combined with docu-ment frequency ( SVM -TFIDF ).
 Table 2: Classification accuracy. Scores sig-nificantly different from the best performance (p 2 t &lt; 0.05 on paired t-test) are given an asterisk.
We conclude that there is no evidence for the relevance of the frequency composition of the text for perspective classification, for all levels of venue-and topic-control, from the tightest (PBA debates) to the loosest (Western vs Arab authors on Middle Eastern affairs). This result is a clear indication that perspective classification is quite different from text categorization by topic, where count-based features usually perform better than boolean features. On the other hand, we have not observed that boolean features are reliably better than count-based features, as reported for the sen-timent classification task in the movie review do-main (Pang et al., 2002).

We note the low performance on BL-I, which could testify to a low degree of lexical consolida-tion in the Arab vs Western perspectives (more on this below). It is also possible that the small size of BL-I leads to overfitting and low accuracies. How-ever, PBA subset with only 151 items (only 2002 and 2003 speeches) is still 96% classifiable, so size alone does not explain low BL-I performance. We explore feature redundancy in perspective classification.We first investigate retention of only N best features, then elimination thereof. As a proxy of feature quality, we use the weight as-signed to the feature by the SVM -BOOL model based on the training data. Thus, to get the per-formance with N best features, we take the N 2 highest and lowest weight features, for the posi-tive and negative classes, respectively, and retrain SVM -BOOL with these features only. 8 Table 3: Consolidation of perspective. Nbest shows the smallest N and its proportion out of all features for which the performance of SVM -BOOL with only the best N features is not sig-nificantly inferior ( p 1 t &gt; 0 . 1 ) to that of the full feature set. No-Nbest shows the largest num-ber N for which a model without N best fea-tures is not significantly inferior to the full model. N= { 50 , 100 , 150 ,..., 1000 } ; for DP and BL-I, ad-ditionally N= { 1050 , 1100 ,..., 1500 } ; for PBA, ad-ditionally N= { 10 , 20 , 30 , 40 } .

We observe that it is generally sufficient to use a small percentage of the available words to ob-tain the same classification accuracy as with the full feature set, even in high-accuracy cases such as PBA and BL. The effectiveness of a small subset of features is consistent with the observa-tion in the discourse analysis studies that rivals in long-lasting controversies tend to consolidate their vocabulary and signal their perspective with certain stigma words and banner words , that is, specific keywords used by a discourse commu-nity to implicate adversaries and to create sym-pathy with own perspective, respectively (Teubert, 2001). Thus, in abortion debates, using infanti-cide as a synonym for abortion is a pro-life stigma. Note that this does not mean the rest of the fea-tures are not informative for classification, only that they are redundant with respect to a small per-centage of top weight features.

When N best features are eliminated, perfor-mance goes down significantly with even smaller N for PBA and BL datasets. Thus, top features are not only effective, they are also crucial for ac-curate classification, as their discrimination capa-city is not replicated by any of the other vocabu-lary words. This finding is consistent with Lin and Hauptmann (2006) study of perspective vs topic classification: While topical differences be-tween two corpora are manifested in difference in distributions of great many words, they observed little perspective-based variation in distributions of most words, apart from certain words that are preferentially used by adherents of one or the other perspective on the given topic.

For DP and BL-I datasets, the results seem to suggest perspectives with more diffused key-word distribution (No-NBest figures are higher). We note, however, that feature redundancy exper-iments are confounded in these cases by either a low power of the paired t-test with only 4 pairs (DP) or by a high variance in performance among the 10 folds (BL-I), both of which lead to nume-rically large discrepancy in performance that is not deemed significant, making it easy to  X  X atch X  the full set performance with small-N best features as well as without large-N best features. Better com-parisons are needed in order to verify the hypo-thesis of low consolidation.

In future work, we plan to experiment with ad-ditional features. For example, Greene and Resnik (2009) reported higher classification accuracies for the DP-4 data using syntactic frames in which a selected group of words appeared, rather than mere presence/absence of the words. Another di-rection is exploring words as members of seman-tic fields  X  while word use might be insufficiently consistent within a perspective, selection of a se-mantic domain might show better consistency.
