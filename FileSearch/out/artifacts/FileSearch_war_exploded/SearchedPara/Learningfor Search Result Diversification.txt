 Search result diversification has gained attention as a way to tackle the ambiguous or multi-faceted information needs of users. Most existing methods on this problem utilize a heuristic predefined ranking function, where limited features can be incorporated and extensive tuning is required for dif-ferent settings. In this paper, we address search result di-versification as a learning problem, and introduce a novel relational learning-to-rank approach to formulate the task. However, the definitions of ranking function and loss func-tion for the diversification problem are challenging. In our work, we firstly show that diverse ranking is in general a sequential selection process from both empirical and theo-retical aspects. On this basis, we define ranking function as the combination of relevance score and diversity score be-tween the current document and those previously selected, and loss function as the likelihood loss of ground truth based on Plackett-Luce model, which can naturally model the se-quential generation of a diverse ranking list. Stochastic gra-dient descent is then employed to conduct the unconstrained optimization, and the prediction of a diverse ranking list is provided by a sequential selection process based on the learned ranking function. The experimental results on the public TREC datasets demonstrate the effectiveness and ro-bustness of our approach.
 H.3.3 [ Information Search and Retrieval ]: Information Search and Retrieval  X  Retrieval Models Algorithms, Experimentation, Performance, Theory Diversity, Relational Learning-to-Rank, Sequential Selection, Plackett-Luce Model
Most users leverage Web search engine as a predominant tool to fulfill their information needs. Users X  information needs, typically described by keyword based queries, are of-ten ambiguous or multi-faceted. On the one hand, for some ambiguous queries, there are multiple interpretations of the underlying needs (e.g., query  X  X and X  may refer to the rock band, frequency band or rubber band). On the other hand, queries even with clear definition might still be multi-faceted (e.g.,  X  X ritney spears X ), in the sense that there are many as-pects of the information needs (e.g., news, videos, photos of britney spears). Therefore, search result diversification has attracted considerable attention as a means to tackle the above problem [1]. The key idea is to provide a diversified result list, in the hope that different users will find some results that can cover their information needs.

Different methods on search result diversification have been proposed in literature, which are mainly non-learning methods, and can be divided into two categories: implicit methods and explicit methods. Implicit methods [3] assume that similar documents cover similar aspects, and rely on inter-document similarity for selecting diverse documents. While explicit methods [29] directly model the aspects of user queries and select documents that cover different as-pects for diversification. However, most existing methods utilize a heuristic predefined utility function, and thus lim-ited features can be incorporated and extensive tuning is required for different retrieval settings.

In this paper, we address search result diversification as a learning problem where a ranking function is learned for diverse ranking. Different from traditional relevance rank-ing based on the assumption of independent document rele-vance [17], diverse ranking typically considers the relevance of a document in light of the other retrieved documents [29]. Therefore, we introduce a novel Relational Learning-to-Rank framework (R-LTR for short) to formulate the task of search result diversification. R-LTR considers the inter-relationships between documents in the ranking process, be-sides the content information of individual documents used in traditional learning-to-rank framework. However, the def-initions of ranking function and loss function for the diver-sification problem are challenging.

From the top-down user browsing behavior and the ubiq-uitous greedy approximation for diverse ranking, we find that search result diversification is in general a sequential ranking process. Therefore, we propose to define the rank-ing function and loss function in a sequential way: (1) The ranking function is defined as the combination of relevance sc ore and diversity score, where the relevance score only de-pends on the content of the document, and the diversity score depends on the relationship between the current doc-ument and those previously selected. We describe different ways to represent the diversity score. (2) The loss function is defined as the likelihood loss of ground truth based on Plackett-Luce model [18], which can naturally model the se-quential generation of a diverse ranking list. On this basis, stochastic gradient descent is employed to conduct the un-constrained optimization, and the prediction of diverse rank-ing list is provided by a sequential selection process based on the learned ranking function.

To evaluate the effectiveness of the proposed approach, we conduct extensive experiments on the public TREC datasets. The experimental results show that our methods can sig-nificantly outperform the state-of-the-art diversification ap-proaches, with Official Diversity Metrics (ODM for short) of TREC diversity task including ERR -IA [1, 6],  X  -N DCG [11] and N RBP [12]. Furthermore, our methods also achieve best in the evaluations of traditional intent-aware measures such as Precision-IA [1] and Subtopic Recall [37]. In addition, we give some discussions on the robustness of our methods and the importance of the proposed diversity features. Finally, we also study the efficiency of our approach based on the analysis of running time.

The main contributions of this paper lie in: 1. the proposal of a novel R-LTR framework to formu-2. the new definitions of ranking function and loss func-3. an empirical verification of the effectiveness of the pro-
The rest of the paper is organized as follows. We first review some related work in Section 2. We then introduce the R-LTR framework in Section 3, and describe the specific definitions of ranking function and loss function, learning and prediction procedures in Section 4. Section 5 presents the experimental results. Section 6 concludes the paper.
Most existing diversification methods are non-learning meth-ods, which can be mainly divided into two categories: im-plicit approaches and explicit approaches.

The implicit methods assume that similar documents cover similar aspects and model inter-document dependencies. For example, Maximal Marginal Relevance (MMR) method [3] proposes to iteratively select a candidate document with the highest similarity to the user query and the lowest similar-ity to the already selected documents, in order to promote novelty. In fact, most of the existing approaches are some-how inspired by the MMR method. Zhai et al. [37] select documents with high divergence from one language model to another based on the risk minimization consideration.
The explicit methods explicitly model aspects of a query and then select documents that cover different aspects. The aspects of a user query can be achieved with a taxonomy [1, 32], top retrieved documents [5], query reformulations [24, 29], or multiple external resources [15]. Overall, the explicit methods have shown better experimental performances com-paring with implicit methods.

There are also some other methods which attempt to bor-row theories from economical or political domains. The work in [26, 33] applies economical portfolio theory for search re-sult ranking, which views search diversification as a means of risk minimization. The approach in [13] treats the problem of finding a diverse search result as finding a proportional representation for the document ranking, which is like a crit-ical part of most electoral processes.

The authors of [2, 27] try to construct a dynamic ranked-retrieval model, while our paper focuses on the common static ranking scenario. There are also some on-line learn-ing methods that try to learn retrieval models by exploiting users X  online feedback [25, 31, 35, 30, 28]. These research work can tackle diversity problem to some extent, but they focus on an  X  X n-line X  or  X  X oactive X  scenario, which is different from our work (i.e. offline supervised learning scenario).
Recently, some researchers have proposed to utilize ma-chine learning techniques to solve the diversification prob-lem. Yue et al. [36] propose to optimize subtopic coverage as the loss function, and formulate a discriminant function based on maximizing word coverage. However, their work only focuses on diversity, and discards the requirements of relevance. They claim that modeling both relevance and di-versity simultaneously is a more challenging problem, which is exactly what we try to tackle in this paper. In this pa-per, we propose a novel R-LTR approach to conduct search result diversification, which is different from traditional ap-proaches and shows promising experimental performance.
Traditional relevance ranking has been well formulated as a learning-to-rank (LTR for short) problem [17], where a ranking function is defined on the content of each individual document and learned toward some loss functions. However, in diverse ranking scenario, the overall relevance of a doc-ument ranking for a given query, should depend not only on the individual ranked documents, but also on how they related to each other [29]. Therefore, in this paper, we in-troduce a novel R-LTR framework to formulate the diverse ranking problem. The difference between LTR and R-LTR is that the latter considers both contents of individual doc-ument and relations between documents. In the following paper, we use superscript to denote the id of a query and subscript to denote the id of a document.

Formally, let X = { x 1 ,  X  X  X  , x n } , where x i denotes the d dimensional feature vector of a candidate document x i for query q ; Let R  X  X  n  X  n  X  l denote a 3-way tensor representing relationships between the n documents, where R ijk stands for the k -th feature of relation between documents x i and x . Let y be a ground-truth of the query q , in the form of a vector of ranking scores or a ranking list. Supposing that f ( X, R ) is a ranking function, and the goal of R-LTR is to output the best ranking function from a function space F .
In training procedure, given the labeled data with N queries A loss function L is defined, and the learning process is con-ducted by minimizing the total loss with respect to the given tra ining data.
In prediction, given X ( t ) and R ( t ) of n t documents for query q t , we output  X  y ( t ) based on the learned ranking func-
In fact, the proposed R-LTR framework is very general, in the sense that many traditional ranking problems are its special cases. (1) It is obvious to see that the conventional LTR frame-work is a special case of R-LTR. Specifically, if we ignore the relation tensor R , then we get the same function as that in traditional LTR, i.e. f ( X, R ) = f ( X ). (2) The  X  X earning to rank relational objects X  framework [22, 23] is also a special case of R-LTR. Specifically, if we restrict the relation tensor R to be a matrix , with R ij resenting the relation between document x i and x j , then we get the same function as that in the problem of learning to rank relational objects.

The above framework gives a formulation of ranking prob-lems involving relationship. When solving the specific prob-lem, one needs to define the corresponding ranking function and loss function according to the task.
As mentioned in the previous section, it is natural to for-mulate search result diversification under R-LTR framework. In this paper, we mainly focus on the diverse ranking sce-nario. To apply the above framework to this specific task, the most challenging problem is the definition of ranking function and loss function.
In order to properly define the ranking function and loss function, we first look into the diverse ranking problem. (1) Empirically, users usually browse the Web search re-sults in a top-down manner, and perceive diverse informa-tion from each individual document based on what he/she have obtained in the preceding results [8]. (2) Theoretically, diverse ranking can be naturally stated as a bi-criterion optimization problem, and it is NP-hard [1, 4]. Therefore, in practice, most previous approaches on search result diversification are based on greedy approxima-tion, which sequentially select a  X  X ocal-best X  document from the remanent candidate set [29].

From both empirical and theoretical analysis above, we can see that it is better to view diverse ranking as a sequen-tial selection process, in the sense that the ranking list is generated in a sequential order, with each individual docu-ment ranked according to its relevance to the query and the relation between all the documents ranked before it.
As discussed above, diverse ranking is in general a se-quential selection process, where each individual document is ranked according to its relevance to the query and the relation between all the documents ranked before it. The intuitive idea is illustrated in Figure 1, when ranking doc-uments in X \ S given the already ranked results S , both content-based relevance and diversity relation between this Fi gure 1: An illustration of the sequential way to de-fine ranking function. All the rectangles represent candidate documents of a user query, and different colors represent different subtopics. The solid rect-angle is relevant to the query, and the hollow rectan-gle is irrelevant to the query, and larger size means more relevance. X denotes all the candidate docu-ment collection. S denotes previously selected doc-uments, and X \ S denotes the remanent documents. document and the previously selected documents in S should be considered. Noting that larger size of the rectangle means the document is more relevant to the query, and different col-ors represent different subtopics. Therefore, the document 8 may be more preferred than document 4 given S , since it is relevant to the query, and also provides different aspects additionally comparing with the selected set S .

Based on this ranking process, here we give the precise definition of ranking function. Given a query q , we assume that a set of documents have been selected, denoted as S , the scoring function on the candidate document in X \ S , is then defined as the combination of the relevance score and the diversity score between the current document and those previously selected, shown as follows. where x i denotes the relevance feature vector of the can-didate document x i , R i stands for the matrix of relation-ships between document x i and other selected documents, with each R ij stands for the relationship vector between document x i and x j , represented by the feature vector of ( R ij 1 ,  X  X  X  , R ijl ), x j  X  S , and R ijk stands for the k -th rela-tion feature between documents x i and x j . h S ( R i ) stands for the relational function on R i ,  X  T r and  X  T d stands for the corresponding relevance and diversity weight vector. When S =  X  , f S ( x i , R i ) is directly represented as  X  T r ranking function can be represented as the set of scoring function: where S i , denotes the previously selected document collec-tion with i documents. From the above definition, we can see that if we do not consider diversity relation, our ranking fu nction reduce to f ( X ) = ( f ( x 1 ) ,  X  X  X  , f ( x n traditional ranking function in learning-to-rank.
Please note that the relational function h S ( R i ) denotes the way of representing the diversity relationship between the current document x i and the previously selected docu-ments in S . If we treat diversity relation as distance, h can be viewed as the distance of x i to the set S . According to different definitions of the distance between an item and a set of items, h S ( R i ) can be defined as the following three ways.

Minimal Distance. The distance between a document x and a set S is defined as the minimal distance of all the document pairs ( x i , x j ) , x j  X  S .
Average Distance. The distance between a document x and a set S is defined as the average distance of all the document pairs ( x i , x j ) , x j  X  S .
Maximal Distance. The distance between a document x and a set S is defined as the maximal distance of all the document pairs ( x i , x j ) , x j  X  S .
How to define discriminative features that can well cap-ture diversity relation is critical for the success of R-LTR. In this work, we provides several representative features for the learning process, including semantic diversity features (i.e. subtopic diversity, text diversity, title diversity, anchor text diversity and ODP-based diversity) and structural di-versity features (i.e. link-based diversity and url-based di-versity).

Subtopic Diversity. Different documents may associate with different aspects of the given topic. We use Proba-bilistic Latent Semantic Analysis (PLSA) [16] to model im-plicit subtopics distribution of candidate objects, which is important for the diversification task as mentioned before. Therefore, we define the diversity feature based on implicit subtopics as follows.

T ext Diversity. Text dissimilarity is also meaningful for diversity. We propose to represent it as the cosine dissim-ilarity based on weighted term vector representations, and define the feature as follows.
 wh ere d i , d j are the weighted document vectors based on tf idf , and tf denotes the term frequencies, idf denotes inverse document frequencies. There also exists other computing ways such as the work in [14], which is based on sketching algorithm and Jaccard similarity.

Title Diversity. The way of computing title diversity feature is similar as that for text diversity feature, which is denoted as R ij 3 .

Anchor Text Diversity. The anchor text can accurately describe the content of corresponding page and is important. This type of feature is computed similarly as text and title diversity features, denoted as R ij 4 .

ODP-Based Diversity. The existing ODP taxonomy 1 offers a succinct encoding of distances between documents. Usually, the distance between documents on similar topics in the taxonomy is likely to be small. For two categories u and v , we define the categorical distance between them as following: wh ere l ( u, v ) is the length of their longest common prefix. | u | and | v | is the length of category u and v . Then given two documents x i and x j and their category information sets C and C j respectively, we define the ODP-based diversity feature as: where |C i | and |C j | are the number of categories in corre-sponding category sets.

Link-Based Diversity. By constructing a web link graph, we can calculate the link similarity of any document pair based on direct inlink or outlink information. The link-based diversity feature is then defined as follows.

URL-Based Diversity. Given the url information of two documents, we can judge whether they belong to the same domain or the same site. The url-based diversity fea-ture is then defined as follows.

Based on these diversity features, we can obtain the di-versity feature vector R ij = ( R ij 1 , R ij 2 ,  X  X  X  , R feature values are normalized to the range of [0,1]. Please note that there might be some other useful resources for the definition of diversity features, e.g., clickthrough logs, which will be further considered in our future work.
Motivated by the analysis that the process for diverse ranking is in general a sequential selection process, we pro-pose to model the generation of a diverse ranking list in a sequential way, and define the loss function as the likelihood loss of the generation probability .

Intuitively, the generation probability of a ranking list can be viewed as a process to iteratively select the top ranked h ttp://www.dmoz.org/ d ocuments from the remaining documents. The precise def-inition is given as follows.
 where y ( i ) stands for the index of document which is ranked in position i in the ranking list y , X denotes all the candidate selected document collection with i documents, P ( x y (1) stands for the probability that x y (1) is ranked first among the documents in X , and P ( x y ( j ) | X \ S j  X  1 ) stands for the probability that document x y ( j ) is ranked first among the documents in X \ S j  X  1 .
The above sequential definition approach can be well cap-tured by the Plackett-Luce Model [18]. Therefore, we pro-pose to define P ( x y (1) | X ) and P ( x y ( j ) | X \ S way, shown as follows, j  X  2.
 Incorporating Eq.(5) and Eq.(6) into Eq.(4), the generation probability of a diverse ranking list is formulated as follows. where S 0 =  X  , f  X  ( x, R ) =  X  T r x .
Incorporating Eq.(7) into the definition of the loss func-tion Eq.(3), we can obtain the precise definition of the loss function as follows.

L ( f ( X,R ) , y )=  X  We can see that our loss function is similar to that in ListMLE [34], which is formulated as follows.
 wh ere f ( x ) is the score function in traditional learning-to-rank, i.e. f ( x ) =  X  T x .

Therefore, if we do not consider diversity relation in our framework, our loss function will reduce to the same form of that in ListMLE. That is to say, ListMLE is a special case of our loss function.
Based on the definitions of ranking function and loss func-tion, we present the learning and prediction process in this section. Specifically, we first describe how to construct the training data, and then introduce the optimization proce-dure. Finally, we show how to make predictions based on the learned ranking function.
 Al gorithm 1 Construction of Approximate Ideal Ranking List
Inp ut: 1: Initialize S 0  X  X  X  , y ( i ) = (1 ,  X  X  X  , n i ) 2: for k = 1 , ..., n i do 4: S k  X  S k  X  1  X  bestDoc 5: y ( i ) ( k ) = the index of bestDoc 6: end for Al gorithm 2 Optimization Algorithm
Output: model vector:  X  r ,  X  d 1: Initialize parameter value  X  r ,  X  d 2: repeat 3: Shuffle the training data 4: for i = 1 , ..., N do 5: Compute gradient  X   X  r ( i ) and  X   X  d ( i ) 6: Update model:  X  r =  X  r  X   X   X   X   X  r ( i ) , 7: end for 8: Calculate likelihood loss on the training set 9: until the change of likelihood loss is below  X  The labeled data in search result diversification such as TREC diversity task are usually provided in the form of ( q candidate document set of query q i , T i is the subtopics of query q i , t is a specific subtopic in T i , and P ( x ( i ) the relevance of document x ( i ) j to subtopic t . We can see that the above form of labeled data deviates the formulation of y ( i ) in our R-LTR framework, which requires a ranking list of candidate documents. In order to apply R-LTR, we need to construct y ( i ) from the provided form of labeled data.
We propose to construct an approximate ideal ranking list by maximizing the ODM measures (e.g., ERR -IA ), and use the approximate ideal ranking list as the training ground-truth y ( i ) for query q i , as described in Algorithm 1.
According to the results in [20], if a submodular func-tion is monotonic (i.e., f ( S )  X  f ( T ), whenever S  X  T ) and normalized (i.e., f (  X  ) = 0), greedily constructing gives an (1  X  1 /e )-approximation to the optimal. Since any member of ODM is a submodular function, we can easily prove that Algorithm 1 is (1  X  1 /e )-approximation to the optimal (We omit the proof here). And the quality of training ground-truth can be guaranteed.
Given the training data { ( X ( i ) , R ( i ) , y ( i ) ) loss is represented as follows.  X  A lgorithm 3 Ranking Prediction via Sequential Se-lection 1: Initialize S 0  X  X  X  , y ( t ) = (1 ,  X  X  X  , n t ) 2: for k = 1 , ..., n t do 3: bestDoc  X  argmax x  X  X 4: S k  X  S k  X  1  X  bestDoc 5: y ( t ) ( k )  X  the index of bestDoc 6: end for
F or such a unconstrained optimization problem, we em-ploy Stochastic Gradient Descent (SGD) to conduct opti-mization as shown in Algorithm 2. According to Eq.(9), the gradient at training sample X ( i ) is computed as follows.
As the ranking function is defined sequentially, traditional prediction approach (i.e., calculating the ranking score of each independent document simultaneously and sorting them in descending order to obtain a ranking list) fails in our framework. According to the sequential selection essence of diverse ranking, we propose a sequential prediction process, as described in Algorithm 3. Specifically, in the first step, the most relevant document with maximal relevance score will be selected and ranked first. If the top k items have been selected, then the document in position k + 1 should be with maximum f S k . At last, all the documents are ranked accordingly, and we obtain the final ranking list.
Assuming that the size of output ranking is K , the size of candidate set is n , then this type of sequential selection algorithm 3 will have time complexity of O ( n  X  K ). Usually, the original value of n is large, therefore, an initial retrieval can be applied to provide a filtered candidate set with rela-tively small size (e.g., top 1000 or 3000 retrieved documents). With a small K , the prediction time is linear.
In this section, we evaluate the effectiveness of our ap-proach empirically. We first introduce the experimental setup. We then compare our approach with baseline meth-ods under different diversity evaluation measures. Further-more, we analyze the performance robustness of different diversity methods and the importance of our proposed di-versity features. Finally, we study the efficiency of our ap-proach based on the analysis of running time.
Here we give some introductions on the experimental setup, including data collections, evaluation metrics, baseline mod-els and detailed implementation.
Our evaluation was conducted in the context of the diver-sity tasks of the TREC2009 Web Track (WT2009), TREC2010 Web Track (WT2010), and TREC2011 Web Track (WT2011), which contain 50, 48 and 50 test queries (or topics), respec-tively. Each topic includes several subtopics identified by TREC assessors, with binary relevance judgements provided at the subtopic level 2 . All the experiments were carried out on the ClueWeb09 Category B data collection 3 , which com-prises a total of 50 million English Web documents.
The current official evaluation metrics of the diversity task include ERR -IA [6],  X  -N DCG [11] and N RBP [12]. They measure the diversity of a result list by explicitly rewarding novelty and penalizing redundancy observed at every rank. We also use traditional diversity measures for evaluation: Precision-IA [1] and Subtopic Recall [37].They measure the precision across all subtopics of the query and the ratio of the subtopics covered in the results, respectively. All the measures are computed over the top-k search results ( k = 20). Moreover, the associated parameters  X  and  X  are all set to be 0.5, which is consistent with the default settings in official TREC evaluation program.
To evaluate the performance of our approach, we compare our approach with the state-of-the-art approaches, which are introduced as follows.

QL . The standard Query-likelihood language model is used for the initial retrieval, which provides the top 1000 retrieved documents as a candidate set for all the diversifi-cation approaches. It is also used as a basic baseline method in our experiment.

MMR . MMR is a classical implicit diversity method in the diversity research. It employs a linear combination of relevance and diversity as the metric called  X  X arginal rele-vance X  [3]. MMR will iteratively select document with the largest  X  X arginal relevance X . xQuAD . The explicit diversification approaches are pop-ular in current research field, in which xQuAD is the most representative and used as a baseline model in our experi-ments [29].

PM-2 . PM-2 is also a explicit method that proposes to optimize proportionality for search result diversification [13]. It has been proved to achieve promising performance in their work, and is also chosen as baseline in our experiment. F or WT2011 task, assessors made graded judgements. While in the official TREC evaluation program, it mapped these graded judgements to binary judgements by treating values &gt; 0 as relevant and values  X  0 as not relevant. http://boston.lti.cs.cmu.edu/Data/clueweb09/ T able 1: Relevance Features for learning on ClueWeb09-B collection [21, 19].

Lis tMLE . ListMLE is a plain learning-to-rank approach without diversification considerations, and is a representa-tive listwise relevance approach in LTR field [17].
SVMDIV . SVMDIV is a representative supervised ap-proach for search result diversification [36]. It proposes to optimize subtopic coverage by maximizing word cover-age. It formulates the learning problem and derives a train-ing method based on structural SVMs. However, SVMDIV only models diversity and discards the requirement of rele-vance. For fair performance comparison, we will firstly apply ListMLE to do the initial ranking to capture relevance, and then use SVMDIV to re-rank top-K retrieved documents to capture diversity.
 The above three diversity baselines: MMR, xQuAD and PM-2, all require a prior relevance function to implement their diversification steps. In our experiment, we choose ListMLE as the relevance function to implement them, and denote them as: MMR list , xQuAD list and PM-2 list , respec-tively.

According to the different ways in defining the relational function h S ( R i ) in section 4.2.1, our R-LTR diversification approach has three variants, denoted as R-LTR min , R-LTR and R-LTR max , respectively.
In our experiments, we use Indri toolkit (version 5.2) 4 as the retrieval platform. For the test query set on each dataset, we use a 5-fold cross validation with a ratio of 3:1:1, for training, validation and testing. The final test perfor-mance is reported as the average over all the folds.
For data preprocessing, we apply porter stemmer and stopwords removing for both indexing and query process-ing. We then extract features for each dataset as follows. For relevance, we use several standard features in LTR re-search [21], such as typical weighting models (e.g., TF-IDF, BM25, LM), and term dependency model [19, 38], as sum-marized in Table 1, where Q -D means that the feature is dependent on both query and document, and D means that the feature only depends on the document. For all the Q -D features, they are applied in five fields: body, anchor, ti-tle, URL and the whole document, resulting in 5 features in total, respectively. Additionally, the MRF feature has two types of values: ordered phrase and unordered phrase [19], so the total feature number is 10.

For three baseline models: MMR, xQuAD and PM-2, they all have a single parameter  X  to tune. We perform a 5-fold cross validation to train  X  through optimizing ERR -IA . Additionally, for xQuAD and PM-2, the official subtopics are used as a representation of taxonomy classes to simu-h ttp://lemurproject.org/indri late their best-case scenarios, and uniform probability for all subtopics is assumed as [29, 13].

For ListMLE and SVMDIV, we utilize the same training data generated by Algorithm 1 to train their model, and also conduct 5-fold cross validation. ListMLE adopts the relevance features summarized in Table 1. SVMDIV adopts the representative word level features with different impor-tance criterion, as listed in their paper and released code [36]. As described in above subsection, SVMDIV will re-rank top-K retrieved documents returned by ListMLE. We test K  X  X  30 , 50 , 100 } , and find it performs best at K = 30. Therefore, the following results of SVMDIV are achieved with K = 30.

For our approach, the learning rate  X  parameter in Algo-rithm 2 is chosen from 10  X  7 to 10  X  1 , and the best learning rate is obtained based on the performance of validation set.
We now compare our approaches to the baseline methods on search result diversification. The results of performance comparison are shown in Table 2, 3, and 4. We also present the performance of top performing systems on Category-B reported by TREC [7, 10, 9], which are just taken as indicative references. The number in the parentheses are the relative improvements compared with the baseline method QL. Boldface indicates the highest scores among all runs.
From the results we an see that, our R-LTR outperform the plain LTR approach without diversification considera-tion, i.e. ListMLE, which can be viewed as a special case of our approach. Specifically, the relative improvement of R-LTR min over ListMLE is up to 41.87%, 49.71%, 29.17%, in terms of ERR -IA on WT2009, WT2010, and WT2011, respectively. It indicates that our approach can tackle multi-criteria ranking problem effectively, with the consideration of both content-based information and diversity relationship among candidate objects.

Regarding the comparison among representative implicit and explicit diversification approaches, explicit methods (i.e. xQuAD and PM-2) show better performance than the im-plicit method (i.e. MMR) in terms of all the evaluation measures. MMR is the least effective due to its simple predefined  X  X arginal relevance X . The two explicit methods achieve comparable performance: PM-2 list wins on WT2010 and WT2011, while xQuAD list wins on WT2009, but their overall performance differences are small.

Furthermore, our approach outperforms the state-of-the-art explicit methods in terms of all the evaluation measures. For example, with the evaluation of ERR -IA , the relative improvement of R-LTR min over the xQuAD list is up to 17.18%, 11.26%, 13.38%, on WT2009, WT2010, WT2011, respectively, and the relative improvement of R-LTR min over the PM-2 list is up to 18.31%, 10.65%, 10.59% on WT2009, WT2010, WT2011, respectively. Although xQuAD list and PM-2 list all utilize the official subtopics as explicit query aspects to simulate their best-case scenarios, their perfor-mances are still much lower than our learning-based ap-proaches, which indicates that there might be certain gap between their heuristic predefined utility functions and the final evaluation measures.

Comparing with the learning-based diversification base-line method, our R-LTR approach also show better per-fo rmance than the SVMDIV approach. The relative im-provement of R-LTR min over the SVMDIV is up to 12.71%, 9.49%, 10.02%, in terms of ERR -IA on WT2009, WT2010, WT2011, respectively. SVMDIV simply uses weighted word coverage as a proxy for explicitly covering subtopics, while our R-LTR approach directly models the generation proba-bility of the diverse ranking based on the sequential ranking formulation. Therefore, our R-LTR approach shows deeper understanding and better formulation of diverse ranking, and leads to better performance. We further conduct sta-tistical tests on the results, which indicates that all these improvements are statistically significant ( p -value &lt; 0 . 01).
Among the R-LTR approaches, R-LTR min obtains bet-ter performance than the other two variants especially on WT2010 and WT2011 data collection, although their per-formance difference is small. It indicates that when defining the diversity relation between a document and a set of doc-uments, the minimal distance would be a better choice.
Additionally, we also evaluate all the methods under tra-ditional diversity measures, i.e. Precision-IA and Subtopics Recall. The experimental results are shown in Figure 2 and 3. We can see that our approaches outperform all the base-line models on all the data collections in terms of both met-rics, which is consistent with the evaluation results in Table 2, 3, and 4. It can further demonstrate the effectiveness of our approach on search result diversification from different aspects. When comparing the three variants of our R-LTR approaches, they all show similar performance and none ob-tains consistent better performance than the others under these two measures.
In this section we analyze the robustness of these diversifi-cation methods, i.e., whether the performance improvement is consistent as compared with the basic relevance baseline Table 5: The robustness of the performance of all diversity methods in Win/Loss ratio xQuAD list 28 /11 31/12 31/12 90/35 SVMDIV 30 /12 32/11 32/11 94/34 R-LTR min 34/ 9 35/10 35/9 104/28 R-LTR avg 33 /9 34/11 34/10 101/30 R-LTR max 33 /10 35/10 34/10 102/30 QL. Specifically, we define the robustness as the Win/Loss ratio [36, 13] -the ratio of queries whose performance im-proves or hurts as compared with the original results from QL in terms of of ERR -IA .

From results in Table 5, we find that our R-LTR meth-ods achieve best as compared with all the baseline meth-ods, with the total Win/Loss ratio around 3.49. Among the three variants of R-LTR methods, R-LTR min performs better than the others, with the Win/Loss ratio as 3.71.
Based on the robustness results, we can see that the per-formance of our R-LTR approach is more stable than all the baseline methods. It demonstrates that the overall per-formance gains of our approach not only come from some small subset of queries. In other words, the result diversi-fication for different queries could be well addressed under our approach.
In this subsection, we analyze the relative importance of the proposed diversity features. Table 6 shows an ordered list of diversity features used in our R-LTR min model accord-ing to the learned weights (average on three datasets). From the results, we can see that the subtopic diversity R ij 1 is with the maximal weight, which is in accordance with Table 6: Order list of diversity features with corre-sponding weight value. o ur intuition that diversity mainly lies in the rich semantic information . Meanwhile, the title and anchor text diver-sity R ij 3 (title) and R ij 4 (anchor) also work well, since these fields typically provide a precise summary of the content of the document. Finally, The Link and URL based diversity R ij 6 (Link) and R ij 7 (URL) seem to be the least important features, which may be due to the sparsity of such types of features in the data.

As a learning-based method, our model is flexible to in-corporate different types of features for capturing both the relevance and diversity. Therefore, it would be interesting to explore other useful features under our R-LTR framework to further improve the performance of diverse ranking. We will investigate this issue in future.
We further study the efficiency of our approach and the baseline models. All of the diversity methods associate with a sequential selection process, which is time-consuming due to the consideration of the dependency relations of document pairs. While as discussed before, this type of algorithms all have time complexity of O ( n  X  K ), With a small K , the prediction time is linear.

All the learning-based methods (i.e. ListMLE, SVMDIV and R-LTR) need additional offline training time due to the supervised learning process. We compare the average train-ing time of different learning-based methods, and the result is shown as following (unit: hour): ListMLE (  X  1 . 5 h )  X  SVMDIV (  X  2 h )  X  R-LTR (  X  3 h )
We can observe that our approach takes longer but com-parable offline training time among different learning-based methods. Besides, in our experiments, we also found that the three variants of our R-LTR approach are with nearly the same training time. We will attempt to optimize our co de to provide much faster training speed via paralleliza-tion technique in the following work.
In this paper, we propose to solve the search result diver-sification problem within a novel R-LTR framework. How-ever, the specific definitions of ranking function and loss function are challenging. Motivated by the top-down user browsing behavior and the ubiquitous greedy approximation for diverse ranking, we firstly define the ranking function as the combination of relevance score and diversity score be-tween the current item and those previously selected. Then the loss function is defined as the likelihood loss of ground truth based on Plackett-Luce model, which can naturally model the sequential generation of a diverse ranking list. On this basis, we utilize stochastic gradient descent to con-duct the unconstrained optimization. The prediction of a diverse ranking list is then provided by iteratively maximiz-ing the learned ranking function. Finally the experimental results on public TREC data collections demonstrate the effectiveness and robustness of our approach.

The proposed R-LTR framework is quite general that can be used in other applications, such as pseudo relevance feed-back and topic distillation. Therefore, it would be interest-ing to apply our R-LTR framework in different applications in our future work. This research work was funded by the 973 Program of China under Grants No.2012CB316303 and No.2013CB329602, 863 program of China under Grants No.2012AA011003, Na-tional Natural Science Foundation of China under Grant No.61232010 and No.61203298, and National Key Technol-ogy R&amp;D Program under Grants No.2012BAH39B02 and No.2012BAH46B04.
