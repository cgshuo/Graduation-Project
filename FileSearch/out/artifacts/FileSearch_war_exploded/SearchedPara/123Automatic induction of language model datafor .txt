 Abstract In this paper, we address the issue of generating in-domain language model training data when little or no real user data are available. The two-stage approach taken begins with a data induction phase whereby linguistic constructs from out-of-domain sentences are harvested and integrated with artificially con-structed in-domain phrases. After some syntactic and semantic filtering, a large corpus of synthetically assembled user utterances is induced. In the second stage, two sampling methods are explored to filter the synthetic corpus to achieve a desired probability distribution of the semantic content, both on the sentence level and on the class level. The first method utilizes user simulation technology, which obtains the probability model via an interplay between a probabilistic user model and the dialogue system. The second method synthesizes novel dialogue interactions from the raw data by modelling after a small set of dialogues produced by the developers during the course of system refinement. Evaluation is conducted on recognition performance in a restaurant information domain. We show that a partial match to usage-appropriate semantic content distribution can be achieved via user simula-tions. Furthermore, word error rate can be reduced when limited amounts of in-domain training data are augmented with synthetic data derived by our methods. Chao Wang  X  Grace Chung  X  Stephanie Seneff  X  Keywords Language model  X  Spoken dialogue systems  X  User simulation  X  Example-based generation 1 Introduction A mounting challenge in the building of any new spoken dialogue application is the collection of user data. Real user utterances are important for ensuring adequate coverage and countering sparse data problems, especially in the language modelling and natural language understanding components. To obtain an initial corpus, it is customary to conduct a Wizard-of-Oz data collection and/or solicit plausible inputs from potential users. This is usually followed by successive data collections, in parallel with iterative refinements on each dialogue system component. Such an approach tends to be costly, and more automated methods for obtaining data are critical for lowering barriers to deployment.

This paper presents a methodology for synthesizing language model training data tailored to a spoken dialogue query-based application. In our approach, we seek to build a corpus of training sentences which would realistically reflect those of user interactions with the dialogue system. Thus, the data must be similar in style to conversational speech encompassing repairs and disfluencies, while they should also maximize on diversity and coverage in terms of syntactic constructions. Moreover, at the sentence level (e.g., different types of queries), and at the class level (e.g., within-class statistics), frequency distributions should closely approximate those of real user dialogues. We explore several realistic scenarios applicable at various phases of dialogue system development. During the initial stage of system development, there is typically no data from real users in the new domain. Our strategy is to use formal rules to generate user utterances, similar to Jurafsky et al. ( 1994 ), Popovici and Baggia ( 1997 ) in which context-free grammars were used to generate user sentences. However, we emphasize that it is important to achieve appropriate frequency dis-tributions corresponding to sentence and class level semantics, in addition to lin-guistic richness within sentences. We use user simulation technology to guide our sentence generation process (Chung, 2004 ), essentially shaping the distributions of the sentence corpus by database statistics and a probabilistic user model.

A second scenario is to exploit any existing out-of-domain real-user data of similar style, either collected from previously developed systems, or available from other resources such as the Linguistic Data Consortium. To this end, we have developed a two-stage approach in which a data induction stage first harvests a linguistically diverse corpus by transforming out-of-domain data (Chung, Seneff, &amp; Wang, 2005 ), followed by a sampling stage to ensure proper frequency distributions (Wang, Chung, &amp; Seneff, 2005 ). Our approach does not simply identify sentences that are relevant to the new application in the secondary domain, but exploits as much of the out-of-domain corpus as possible. Essentially domain-specific portions of the sentences in the secondary domain are either substituted by artificially gen-erated in-domain phrases, or  X  X  X ranslated X  X  into target domain phrases via formal rules. This process of sentence reconstruction yields a very large variety of patterns harvested from the previous domain, and relies critically on an intermediate step of extensive syntactic and semantic filtering to produce probable user sentences.
The set of transformed sentences aim to cover combinations of variations in both syntactic constructs and semantic content. However, it does not necessarily represent an appropriate distribution in terms of either syntax or semantics. Hence, a second stage samples this over-generated corpus in order to form a final training set that better approximates the distributions of real application specific dialogue interac-tions. Two different techniques have been implemented for sampling, termed user simulation and dialogue resynthesis , which can be applied individually or in tandem. The first technique does not require any in-domain data, and utilizes the same user simulation process as in the first scenario. The difference is that the user sentences were selected from the pool of transformed sentences, instead of generated using formal rules. The second technique is applicable in the scenario where a small set of in-domain development data has been collected after an initial system is in place. In this method, new dialogues similar to the development data are synthesized, again by selecting sentences from the transformed out-of-domain corpus. Hence, we ex-pand the linguistic richness of the development data while maintaining a similar dialogue content distribution. The sentence selection process in both sampling methods relies on an example-based generation (EBG) capability, leveraging pre-vious work in example-based translation (Wang &amp; Seneff, 2004 ).
 The structure of the paper is as follows. Previous related work will be presented in Sect. 2 . Section 3 outlines the overall approach of re-using out-of-domain sentences. The next two sections provide a detailed account of the component technologies. Section 4 covers the data induction phase. Three methods of obtaining in-domain data are introduced, followed by a description of the syntactic and semantic filtering steps. Strategies in modelling meta queries and spontaneous speech phenomena (filled pauses and non-speech events) are also discussed in this section. Section 5 covers the sampling phase. The EBG mechanism is first presented, followed by a description of how it is used in downsampling the over-generated raw corpus through user simulations or re-synthesis of development data (when available). Section 6 details recognition experiments in a restaurant information system, compari ng performances corresponding to the sce-narios discussed previously in this s ection. We end with conclusions in Sect. 7 . 2 Related work A recent trend in dialogue system development is a focus on minimizing the time and cost in developing a new dialogue system, particularly with respect to obtaining 2003; Fosler-Lussier &amp; Kuo, 2001 ). But dialogue systems are better trained on large amounts of user data that properly characterize the user interactions (Bechet, Riccardi, &amp; Hakkani-Tur, 2004 ). Generally, with very little training, researchers have sought to obtain more data by supplementing with alternative text sources such as the Web (Bulyko, Ostendorf, &amp; Stolcke, 2003 ; Feng et al., 2003 ; Zhu &amp; Rosen-feld, 2001 ). Some work has been directed towards selecting from an out-of-domain corpus based on some metric for relevance to the application domain (Bellagarda, 1998; Iyer &amp; Ostendorf, 1999 ; Klakow, 2000 ). Alternatively, others have turned to language model adaptation where the parameters of a smoothed language model trained from generic data are tuned based on in-domain observations (Bacchiani, Rudnicky, 1995 ). Fabbrizio et al. ( 2004 ) address the bootstrapping of out-of-domain data by identifying classes of utterances that are either generic or re-usable in the new application. In the absence of any domain data, one common method is to run a (usually hand-coded) context-free grammar in generative mode (Fosler-Lussier &amp; Kuo, 2001 ; Jurafsky et al., 1994 ; Popovici &amp; Baggia, 1997 ). This is proposed in Galescu, Ringger, and Allen ( 1998 ) to combine with a language model whose back-off model is trained on out-of-domain data.

In contrast, our method assembles entirely new utterances by inserting artificially constructed in-domain phrases into templates from another unrelated domain. Furthermore, we believe that obtaining the appropriate frequency distribution of the semantic content by sampling through simulated dialogue interactions would pro-duce higher quality data. Stochastically generated user simulations are increasingly being adopted to train dialogue systems (Levin, Pieraccini, &amp; Eckert, 2000 ; Scheffler &amp; Young, 2000 ), particularly for selecting and evaluating dialogue strategies (Araki Segura, &amp; Rubio, 2003 ). The method described here uses simulations as one method for pre-selecting training utterances to shape the training corpus statistics.
We use an EBG capability for sentence selection in the sampling phase. This idea is inspired by work done in the field of example-based translation, which typically requires a collection of pre-existing translation pairs and a retrieval mechanism to search the translation memory. Similarity can be based on parse trees (Sato, 1992 ), complete sentences (Veale &amp; Way, 1997 ), or words and phrases (Brown, 1999 ; Levin et al., 2000 ). Our sentences are indexed with lean syntactic and semantic informa-tion, which is obtained automatically by exploiting existing parsing and generation capabilities developed for dialogue systems.

Our method also relates to the instance-based natural language generation work described in Varges and Mellish ( 2001 ). While both are for narrow domain applica-tions, and both take semantic content as input, the approaches taken are very different. In Varges and Mellish ( 2001 ), examples (or instances) are used to re-rank and select candidates produced by a grammar-based generator, using cosine measure as the distance metric. Our method exploits the restrictiveness of the domain and the large candidate corpus. We directly generate the sentence by retrieving it from the example corpus, using formal rule-based generation for backup when the retrieval fails. 3 Approach Figure 1 illustrates the multiple steps proposed in this paper. We begin with generating an initial seed corpus in our target domain; examples are given in a Boston restaurant information system. This domain data (13,000 sentences) was obtained by running the dialogue system in simulated user mode (Chung, 2004 ). The simulations utilized a stochastic user model that, based on the system reply frame, determined a user re-sponse, represented as a string of key-value (KV) pairs. From the KV representation, the system generated user utterances by way of formal generation rules (Baptist &amp; Seneff, 2000 ). The technique of inducing data from first principles using formal gen-eration will be outlined in Sect. 4.1.1.

Following the creation of a seed corpus, phrases extracted from these in-domain utterances, together with a previously collected flight reservation corpus of 31,000 utterances 1 (Seneff, 2002 ), undergo a transformation to yield synthetic sentences in the new domain. Two specific methods for the transformation will be described: an automatic template generation and substitution method (Sect. 4.1.2 ), and a formal transformation method (Sect. 4.1.3 ). The resultant set of over-generated and artifi-cially constructed sentences is successively reduced, first by selecting on legal syn-tactic parses, and then by filtering on semantic relationships. The subsequent steps then address the process of data sampling to refine the data distribution to better match the statistics expected in realistic user dialogue interactions. The resulting sampled data are then further enhanced with generic meta-level queries and speech artefacts modelling. 4 Domain data induction In this section, we first describe three different methods for inducing synthetic corpora for a new domain. The first method involves formal generation from first principles, using a rule-based natural language generation system, and based on a simple semantic representation of the sentence contents. The other two methods involve transforming user queries directly from another  X  X  X ource X  X  domain into queries appropriate for the new  X  X  X arget X  X  domain X  X he first of these methods sub-stitutes phrasal units from the target domain into utterances obtained from the source domain, whereas the second one utilizes formal generation rules to  X  X  X rans-late X  X  queries from one domain to the other. This section also describes how the data are filtered syntactically and semantically to remove ill-formed sentences, and how the data are augmented to cover meta queries and noise events. 4.1 Data generation 4.1.1 Formal generation method The formal generation method works within a user simulation framework to generate in-domain sentences in the absence of any in-domain or out-of-domain data, as illustrated in Fig. 2 . During simulation, the end-to-end dialogue system continuously operates in text-mode with a user simulator, described in Chung ( 2004 ), and the formal rule-based generation system (Baptist &amp; Seneff, 2000 ). The system response, encoded in a frame-based meaning representation, known as the system reply frame, is used by the simulator to generate a next putative user query in the form of a KV string. The formal generation component converts the KV specification into a surface string using trivial generation rules crafted by hand. The generation system can support multiple patterns for a specific template, thus adding some variations to the generated surface strings. Table 1 shows an example simulated dialogue.

A large corpus can be generated by running the simulation process over many thousands of dialogues. The corpus generated by the formal method typically con-tains very well-formed sentences; however, the linguistic richness is limited by the rules created by the developer. Data induced by the formal generation method will be useful as seed data for the transformation methods introduced in the next sections. 4.1.2 Template-based transformation method The template-based transformation method aims to induce in-domain sentences from available out-of-domain corpora. Essentially, the objective of this method is to capitalize on the diverse syntactic constructions and spontaneous speech phenomena found in the out-of-domain data, replacing the domain-specific phrases with alter-natives appropriate in the new application domain. This step will massively over-generate possible sentences that will be refined and pruned using various filtering methods.

In our experiments, we attempt to transform a large corpus of flight reservation sentences into the restaurant information domain. Each step of the transformation method is shown in Fig. 3 . We take advantage of some seed target domain sentences obtained via the formal generation method described in Sect. 4.1.1 . The seed res-taurant sentences are parsed, and all the noun phrases (NPs) and prepositional phrases (PPs), in their various sequential orderings, are gathered under the non-terminal categories in which they occur. Similarly, the flight sentences are parsed, and the locations of NPs and PPs are replaced by non-terminal tags, yielding a set of templates. Some of the non-terminal categories in which NPs and PPs occur are: direct_object, subject , and predicate_adjective . By exhaustively substituting the phrases for each non-terminal category of the target domain into the templates, new artificial sentences incorporating the syntactic constructs of the flight domain and the semantic content of the restaurant domain are synthesized. Figure 4 illustrates the process with an example. We reuse the same parsers developed for dialogue systems in our experiments. It is also possible to use shallow parsing to identify the NPs and PPs (Hammerton, Osborne, Armstrong, &amp; Daelemans, 2002 ).

Our initial seed restaurant domain synthetic data yielded 6800 examples of NPs and PPs, and the flight reservation domain yielded 1,000 unique sentence templates. Because of the vast number of combinations possible, we terminated the sentence generation procedure after randomly creating 450 k unique sentences. Some typical sentences from the original restaurant data and the example transformations are shown in Table 2 . In comparison with the original artificial data, created from a rule-based method, the new synthetic data are richer, embodying more of the charac-teristic features of human X  X omputer interactions found in real data. In particular, this template-based approach is able to harvest domain-independent speech arte-facts that are embedded within the domain-dependent queries. As a result, we found that the newly constructed data compared with the seed data encompass many more novel phrases that constitute repeats, repairs, greetings and corrections. 4.1.3 Formal transformation method Another technique that is feasible for inducing sentences for a new application from a secondary domain is to develop formal generation rules which essentially perform a  X  X  X ranslation X  X  from one domain to another. The method we propose here reuses a machine translation capability for paraphrasing user queries from a second language back into English. The same set of generation rules that translate one language to another is now modified so that they replace certain semantic concepts from the secondary (flight) domain to those of the new target domain (restaurant).

The language generation capability has some sophisticated mechanisms; for in-stance, it is possible to control generation from source and destination such that only one of them maps to  X  X  X n_city X  X  while the other maps to  X  X  X n_street X  X  or  X  X  X n_region. X  X  Thus we prevent an anomalous query that includes two references to cities. Any flight-domain predicates that are difficult to translate can simply be omitted from the generation rules. Some example query transformations through formal generation are shown in Table 3 .

A disadvantage of the formal transformation method is that it requires manual expertise to develop the formal rules. However, this approach can generate novel sentence patterns that are obviously unattainable by the other template method. For example, the template based transformation method ignores verb phrases, so that sentences containing restaurant domain specific verbs (e.g.  X  X  X at X  X ) will be completely missing without the formal transformation method. 4.2 Syntactic and semantic filtering While we have not quantitatively measured the similarity between the flight domain and the restaurant domain data, we do assume that the two applications, being quite different, do not share many common query types. Hence the methods described above are likely to generate many sentences that are not appropriate for the new application. For example, in the template-based method, we only replace NPs and PPs, thereby preserving the verb phrases of the flight domain. Thus extensive fil-tering is necessary to remove irrelevant or improbable sentences.

One obvious approach to filtering is based on syntactic constraints. That is: remove sentences that fail to produce full parse trees under the grammar for the new domain. As for removing unlikely semantic relationships, we have devised a method for filtering based on semantic constraints. The semantics of a sentence are encoded by using a parser to convert it into a hierarchical  X  X  X emantic frame. X  X  Each sentence maps to a clause type captured at the top level of the frame, and subsequent descendent sub-frames capture topic-predicate relationships. An example is shown in Table 4 .

In the semantic filtering phase, the first training step is the compilation of all the topic X  X redicate relationships of the target domain, extracted from the semantic hierarchies. The second filtering step is the parsing and semantic frame construction of the new sentences, and deletion of those containing previously unrecorded topic X  X redicate relationships. The initial training step processes the original seed data using an algorithm that produces a single tree-like hierarchical frame, storing all observed vertical semantic relationships up to a predetermined depth. At three levels deep, all observed parent X  X hild X  X randchild relationships involving clause/ topic/predicate sub-frames are documented in a reference frame. When the new synthetic sentences are parsed, the (parent X  X hild X  X randchild) sub-frame tuples from the semantics are compared with those in the reference frame. If a tuple has not been previously observed, the entire sentence is deleted from the training corpus.
Table 5 displays a portion of a reference frame, derived from the original seed corpus of seven thousand sentences. Although the trained reference frame is quite sparse in semantic relationships, this kind of filtering is a crude way to eliminate sentences with constructs from the flight domain that are not appropriate for the restaurant domain. Generally, novel subject X  X erb X  X bject relationships that tend to be improbable or nonsensical are eliminated, whereas semantic relationships con-sistent with the seed data are preserved. This does presume then that the seed training data has adequate coverage of the basic query types of the domain, although it does not necessitate semantic or syntactic diversity in the seed data.

Examples of filtered or rejected sentences are depicted in Table 6 . Shown are the semantically malformed sentences that have been output from the template-based transformation method but have failed the semantic constraints imposed by the reference frame. To counter sparsity in the seed data, the developer can enrich the filtered data by manually relaxing the semantic constraints. That is, in several iter-ative stages, some legitimate but novel semantic relationships are added to the reference frame so that more synthetic sentences would pass the semantic con-straints. Ultimately, the 450 k synthetic sentences are reduced to 130 k sentences via syntactic and semantic filtering. 4.3 Further enhancements 4.3.1 Meta queries There is a core component of all spoken dialogue systems that involves so-called  X  X  X eta queries, X  X  which include greetings ( X  X  X ello X  X / X  X  X ood-bye X  X ), dialogue navigation quests. There are, for example, a surprising number of different ways to say  X  X  X ood-bye, X  X  for example,  X  X  X hat X  X  it for now, thank you very much! X  X , and one would expect at least one  X  X  X ood-bye X  X  query in each real-user dialogue. Rather than incorporating such activities into the simulated user model, we decided instead to simply harvest a set of 1158 meta-query sentences from our previous data collection efforts in the flight and weather domains, and augment all of our simulated query corpora with these utterances. 4.3.2 Noise models It is typically the case that so-called  X  X  X on-speech events X  X  are prevalent in spoken dialogue user queries. In our definition, these include the filled-pause words, such as  X  X  X m X  X  and  X  X  X r, X  X  as well as laughter, coughs, and other forms of extraneous noise. We have developed a set of acoustic models that are specific to these kinds of sounds, and have painstakingly labelled them in our training corpora for the flight and weather domains. Careful modelling of these events, both acoustically and linguis-tically, can lead to significant improvements in speech recognition accuracy (Hazen, Hetherington, &amp; Park, 2001 ).

Our parser removes such events from the text string before attempting to parse an utterance. As a consequence, they are omitted from the simulated restaurant-domain sentences that are transduced from a flight domain corpus, using either the formal transformation or the template-based method. Of course, they are also missing from the formally generated sentences in our original seed corpus. Hence, we sought a way to reintroduce them with an appropriate distribution over induced corpora.

Our approach was to develop a simple statistical model, described below, by examining a large corpus of manually transcribed flight domain queries we had previously collected from real users. Observing that non-speech events tend to be concentrated at the beginning and end of a sentence, we decided to compute uni-gram statistics for three  X  X  X ositional X  X  specifications:  X  X  X eginning, X  X   X  X  X iddle, X  X  and  X  X  X nd, X  X  where  X  X  X iddle X  X  means simply occurring anywhere in the utterance except the very beginning or the very end. These statistics were measured directly from the flight domain utterances. We also decided to collapse all such events into a single class, termed h nonspeech event i , to simplify the modelling aspects. We then pro-cessed our generated corpus through a procedure which optionally inserted a h nonspeech event i at the beginning, exact middle, or end of an utterance, according to the computed unigram statistics for these three positions. Each inserted h nonspeech event i , in turn, was instantiated as one of the following choices: h um i , There were also a certain number of utterances in the flight corpus that contained only h nonspeech event i , and we added a corresponding small percentage of such utterances to the synthetic corpus. Finally a h nonspeech event i class was included in the class bigram to recapture the appropriate statistics from the enhanced corpus. 5 Data sampling Although the data generated as described above cover many variations in syntactic and semantic constructs, it is expected that the frequency distributions in the pat-terns will not reflect those found in real user data because the data were not gathered in dialogue interaction. For any dialogue system, the proportions of query types, at the sentence level, will depend both on the functionality of the system as well as user behaviour. Intuitively, the first sentence in the following example is more likely than the second one: 1.  X  X  X hat is the telephone number? X  X  2.  X  X  X ell me Chinese restaurants on Massachusetts Avenue near Central Square in
Moreover, the raw data do not encode appropriate within-class statistics, for instance, the lesser prior likelihood of querying for Burmese cuisine versus Chinese cuisine. To gather such statistics, the approach taken here is to reshape the training data by sampling from the raw corpus, utilizing dialogue-level information (Wang et al., 2005 ).

The sampling technology relies heavily on an example-based generation method for selecting semantically related sentences. There are two primary components: a collection of sentences indexed by lean syntactic and semantic information encoded as KV pairs, and a retrieval mechanism to select a candidate from the indexed corpus given a KV specification. Compiled from the raw synthetic data set, the indexed corpus is the pool of synthetic sentences from which we shall sub-sample. During retrieval, the selected candidate sentence can either be used directly, or further processed by substituting the values of certain keys. Two different configu-rations have been invoked for sampling, which we term user simulation and dialogue resynthesis . We will be applying both these methods in our experiments.
In the following, we first describe the EBG component. We then describe the data sampling techniques, utilizing EBG for sentence selection. 5.1 Example-based generation 5.1.1 Generation of indexed corpus The EBG begins with the construction of an indexed sentence corpus. Each can-didate sentence is first parsed to yield a meaning representation called a semantic frame, which encodes the hierarchy of semantic and syntactic structure of the sen-tence. Then, a set of trivial generation rules is created to extract very lean semantic and syntactic information from the semantic frame as KV pairs, which can then be used as an index for that sentence. Figure 5 shows a typical group of such indexed sentences. 5.1.2 Retrieval mechanism The basic function of the retrieval mechanism is to find a candidate sentence whose KV-index matches the input KV specification. To allow certain flexibility in matching the KV pairs, keys are differentiated into several categories, depending on whether they are optional or obligatory, and whether they require matching on the key-only level or the KV level. These are specified in a header file in the indexed corpus, to allow a developer to flexibly modify the matching strategy. Each oblig-atory key in the input KV specification has to be accounted for in the matching process, while optional keys in the input can be ignored to avoid a matching failure (but will be preferred otherwise). If more than one group of sentences is retrieved, the selection pool includes all the groups.

We will illustrate the retrieval process with an example to highlight some of the distinctions in the different key types. Assume we want to retrieve from the indexed corpus a sentence similar to  X  X  X o you know of any inexpensive French restaurants? X  X  The parsing and generation systems will first produce the following KV pairs:
Suppose the corpus contains only the example shown in Fig. 5 , with price_range and cuisine as obligatory keys required to match on the key level, while clause is an optional key required to match on the KV level. If the system is configured to take the values of the retrieved sentence, the output could simply be  X  X  X heap chinese restaurants please, X  X  or  X  X  X es cheap chinese food. X  X  If instead, the system is configured to substitute the values in the input KV, those two outputs would be  X  X  X nexpensive french restaurant please,  X  X  and  X  X  X es inexpensive french food, X  X  respectively. If the clause were specified as an obligatory key matching on the KV level, then the search would fail to generate any output. For an input such as  X  X  X rench restaurants, X  X  ( cui-sine: french clause: clarifier), the search would also fail because of the extra obligatory key, price_range , in the candidates X  KV index. 5.2 Sampling methodology We propose two sampling methods to adapt the distributions of the induced corpus. The first method is designed for the scenario in which there is no  X  X  X eal X  X  in-domain data available for adaptation, which is typically the case before the system has actually been deployed. Our strategy then is to utilize user simulation to filter the raw data, with the goal of achieving a more refined distribution in the semantic content.

The second method assumes that there is a small amount of development data available, which can be hypothesized to represent typical user behaviour. Such utterances can be used as templates to induce other similar utterances, in order to expand the richness of the development corpus in a systematic way. The resulting data are able to extend the linguistic coverage of the development data, while maintaining a similar dialogue-level and sentence-level semantic content distribution. 5.2.1 Sampling via user simulation The first method, depicted in Fig. 6 , is conducted by running the dialogue system in simulation mode through thousands of text dialogues. The raw sentences are first preprocessed into an indexed corpus based on the syntactic and semantic informa-tion in each sentence, encoded as KV pairs. A small portion of such a corpus was illustrated in Fig. 5 . During simulation, given a response from the dialogue system, the user simulator will generate a query, in the form of KV pairs. The KV infor-mation is used to retrieve an appropriate template from the indexed corpus, with classes in the template substituted by values specified in the simulator X  X  KV string. The resulting surface string is sent to the dialogue system to push the dialogue interaction forward. In the case of a retrieval failure, perhaps due to gaps in the raw data coverage, the formal generation method as described in Sect. 4 can be invoked as a backup mechanism to provide a well-formed query.

A large collection of user queries can be harvested from repeated simulation runs, utilizing a probabilistic model of user behaviour. Their semantic content distribution is a result of the complex interactions of different aspects of the user model, as well as the strategies of the dialogue system. Prior probabilities of within class distribu-tions, estimated from frequency counts of database instances, will further influence the semantic content of the final training corpus. 5.2.2 Dialogue resynthesis If some set of development data exists, it becomes appealing to consider using it as a guide in sub-selecting from a large corpus of synthetic data. Figure 7 describes the process of transforming such data into new dialogues via example-based generation. Rather than running a closed-loop dialogue system, we simply drive the selection with the semantics of the development data. This technique enables the develop-ment data to act as a user model to generate similar but novel dialogues from the synthetic data. The expected training corpus would embed more realistic user behaviour, but at the same time, the harvested sentences will contain a richer variety of sentence constructs than those found in the small development set.

In this method, the development data are parsed utterance by utterance and transformed into a KV representation using the same techniques that were used to create the KV-indexed corpus. During retrieval, the keys in the retrieved sentence template can either be substituted with values drawn from the development utter-ance, or left unaltered from their original values in the synthetic corpus. This allows us to experiment with combining probability distributions from different data sources. Specifically, in the first mode, substituting attribute values from the devel-opment set into the synthetic will result in a within-class distribution similar to that of the development data. On the other hand, in the second mode, preserving attri-bute values of the synthetic data will result in a within-class distribution sampled from the input synthetic data.
 6 Experiments and results In this section, we describe the results of several experiments that were conducted on a test set of 520 utterances, obtained through a data collection effort involving 72 telephone conversations between naive users and the system. Users were asked to interact with a Boston-based restaurant information system via telephone. No spe-cific instructions were provided to the subjects other than a brief introduction to the basic system capability. We excluded recordings which did not contain any speech, but the evaluation data includes utterances (5.4%) with out-of-vocabulary words as well as artefacts such as noise, laughter, etc. The data were transcribed manually to provide reference transcripts.

During the course of system development, we have also collected over 3,000 sentences from developers interacting with the system, either via a typed interface or in spoken mode. This set of developer/expert data is probably not representative of real data from naive users. Nevertheless, they are of high quality both in terms of the syntactic constructs and the semantic content of the queries. These data can thus serve both as a benchmark against which to reference our synthetic corpus perfor-mance and as templates from which to guide a sub-selection process.

We conducted a number of recognition experiments, as illustrated in Fig. 8 . These experiments progress through increasingly sophisticated techniques for exploiting the simulation and developer data for language modelling, generally reflected in improvements in recognition results. Systems I and II correspond to the condition when only synthetic sentences are available for language model training. System I is trained on the raw data only, which is equivalent to the  X  X  X iltered synthetic sen-tences X  X  in Fig. 1 . In System II, the data are obtained via the simulation process described in Fig. 6 , either by selecting from the raw data pool, or by using formal rules to generate sentences from first principles.
 System III is a benchmark system based only on the developer data. For System IV, the simulation data are used to generalize utterances drawn from the developer data, in an attempt to broaden its coverage of general language usage, while still maintaining a similar mixture of semantic contents. In other words, we use the developer data as a user model to generate similar but novel dialogues from the synthetic data, following the techniques of Fig. 7 . Two runs were conducted, and the resulting data were combined with the developer data in training the language model. There are two ways to run the example-based generation module: (1) the sentence templates are retrieved from the example corpus, but the class values are inherited from the developer data; or (2) the entire sentence is retrieved without modification. Thus, the resynthesized dialogues have more-or-less inherited the within-class distribution of the developer dat a in the first mode, while the within-class distribution of the simulation data (reflecting database statistics) is sampled in the latter. The second mode is used here since it has been found to achieve a slightly better performance than the alternative (Wang et al., 2005 ).

The recognizer configuration was kept exactly the same for all experiments, ex-cept for the language model training data. We utilize the SUMMIT landmark-based recognition system (Glass, 2003 ). Word class n -gram language models are created using the techniques described in Seneff, Wang, and Hazen ( 2003 ), where the vocabulary and word classes are automatically generated from the natural language grammar used for parsing. In the deployed system, the recognizer utilizes a dynamic class for the restaurant names, which is adjusted based on dialogue context (Chung, Seneff, Wang, &amp; Hetherington, 2004 ). However, for the off-line experiments con-ducted here, the recognizer is configured to uniformly support all the known res-taurant names in the database, under a static RESTAURANT_NAME word class. The vocabulary size is about 2,500 words, with 1100 of these words being unique res-taurant names. The acoustic models are trained on about 120,000 utterances previ-ously collected from telephone conversations in the weather and flight information domains.

In the following two sections, we first discuss a series of experiments intended to assess the quality of a number of different sets of synthetic data, in the absence of any in-domain real user data (I and II). We then describe a set of experiments intended to enhance recognition performance of an initial system trained on developer data, by manipulating and/or augmenting the training utterances with additional similar data induced through our synthesis techniques (III and IV). 6.1 Synthetic data only Table 7 reports word error rates (WERs) for a series of experiments assessing the effectiveness of various sets of synthetic data for speech recognition. These results were compared with a baseline system (F&amp;F) which utilized a set of just 200 made-up sentences that were solicited from  X  X  X riends and family X  X  prior to system develop-ment. Friends were asked to propose queries that they would likely ask a hypo-thetical restaurant system. Such preliminary data collection efforts in the absence of a functioning dialogue system represent one rudimentary method for jump-starting data collection. It realized a rather high WER of 32.1%.

The  X  X  X aw X  X  set is a set of over 130,000 utterances that are induced by the template-based method using 30,000 flight domain sentences. These have been filtered on syntactic and semantic constraints but have not been downsampled via simulation. In spite of its large size, it only improves slightly over the baseline performance.
Systems II(1-4) are all trained on data devoid of any restaurant-specific real-user utterances, but all of them involve user simulation runs. The training utterances for System II(1) were generated from first principles, using only formal generation rules in simulated dialogues. Systems II(2) and II(3) both involve transformations from flight domain utterances. System II(2) uses formal generation for translating flight queries into restaurant-domain queries, coupled with user simulation, whereas System II(3) is trained on data that are sampled on the template-induced  X  X  X aw X  X  data set, as described above. Systems II(1-3) all yield substantial improvements over the results for the  X  X  X aw X  X  simulated data, with the template-based approach being the most effective. In particular, when the template-induced data are sampled, WER is reduced by 28.3%, (30.7 X 22.0%). Evidently, as intended, sampling the raw training data through a strategy of user simulations has yielded higher quality training data, because the probability distributions, as estimated by the simulated user model, are much closer to those of real dialogue interactions.

When all three of these sets are combined into a single large set, (System II(4)), the performance improves further, yielding a recognition error rate of just over 20%. It should be noted that, although the formal method performs relatively poorly by itself, the WER increases to over 21% if data from only Systems II(1) and II(3) are included in the training corpus. The difference in WER is tested for statistical sig-nificance using matched pairs segment word error test (Gillick &amp; Cox, 1989 ), and a significance at a level of 0.03 is established. Hence, the formal transformation method seems to add some novel coverage beyond what the other two sets offer.
We have determined through separate experiments that including meta queries and noise models improves recognition performances. Hence, for all of these experiments except the F&amp;F, the training data were augmented with the meta queries harvested from the flight and weather data, and the synthetic data were manipulated to insert non-speech events. The next section will document the effects of adding meta queries and noise models to development data. 6.2 Augmenting developer data Table 8 summarizes the results of several experiments involving the available corpus of nearly 3500 utterances harvested from developer interactions with the system. By themselves, these utterances (typed plus spoken) yielded a WER that was 1% lower (19.1 versus 20.1) than the best result achieved from data that are entirely artificially generated. A question worth asking, however, is whether these developer data can be improved upon through augmentations with reduplication through dialogue resynthesis to yield variants derived from our corpus of simulated data. As can be seen from the table, each of these augmentations led to further reductions in the WER.

The best performing system (IV), at WER 17.2%, combines the developer data utterances with a synthetic data set. The synthetic data set is obtained by (1) induction via the template-based approach from flight utterances followed by syn-tactic/semantic filtering, (2) downsampling by user simulation, and finally (3) further downsampling guided by the semantic content of the developer utterances (dialogue resynthesis). Two consecutive runs of dialogue resynthesis are conducted, resulting in two additional utterances for each developer utterance. The overall relative improvement achieved by all of these augmentations, compared to the original Dev system, is 9.9%. This WER difference is significant at the level of P = .001. In other experiments we conducted, it was found that combining with synthetic data without applying the dialogue resynthesis technique did not outperform the system using the  X  X  X ugmented X  X  developer data ( Dev + Enhancements ).

These results suggest that real user data, even when derived from developer/ expert users, can be valuable for training a dialogue system. Combining simulated data with developer data has enhanced performance even further. The simulated data clearly add coverage by capturing more novel queries in syntactic constructions and semantic content through the process of induction from flight utterances and simulated dialogue interactions. But this final simulated set also maintains a set of sentence level statistics that directly approximates user interactions of developers. This seems to be better than only using the user model of the simulator.

A further examination of the development set shows that it covers many non-grammatical constructs that are plausible spoken inputs and cause parse failures. Also included are some out-of-domain queries and sentences with unknown words, found in 3.2% of the sentences. These are not modelled by the template-based induction method because the method uses the same parser to derive the meaning representation, and filters out illegal parses such that none of the induced sentences are intended to be out-of-domain or contain unknown words.

In one final experiment, we ascertain a possible lower bound on word error by training the language model on the transcriptions of the test set. This  X  X  X racle X  X  condition achieved a 12.2% WER. We can deduce that further manipulations on the language model training data, whether in terms of quantity or quality, while holding the acoustic model constant, would be unlikely to outperform this system. 7 Summary and future work This paper has described novel methods for inducing language modelling data for a new spoken dialogue system. The methodology we implemented involves a step of generatively inducing a large corpus of artificial sentences assembled via a process of parsing and reconstructing out-of-domain data. This is followed by syntactic and semantic filtering of illegal sentences. A final step is concerned with sampling the corpus based on either simulated dialogues or semantic information extracted from development data.

Our experiments have shown that reasonable performance can be obtained in the absence of real data, simply by using synthetic training data. We have demonstrated a method for assembling linguistically varied sentences for a new application by harvesting the sentence constructs found inside queries of a secondary, essentially unrelated, domain. In addition, we have also shown that a training corpus can be refined by incorporating statistics that estimate user interaction with the system. This can be achieved without user data via a user simulation strategy. On the other hand, collecting even some expert dialogue data, typed or spoken, can be beneficial, and the sentence-level distributions of expert users X  interactions can be exploited to generate even better synthetic data.

While the procedures we have developed here appear complex, most of them are fully automatic once the appropriate scripts and control files are in place. Devel-oping the parsing grammars for the source and target domains is relatively straightforward, since the rules are based on a core grammar capturing syntactic structure. The main task is to populate terminal nodes with appropriate vocabulary for nouns, verbs and adjectives. A set of simple semantic mapping rules are applied to automatically derive a semantic frame from the parse tree. The parent X  child X  X randchild relationships are derived directly from the semantic frames. The templates and filler phrases are created automatically from the parse trees as well. The formal generation method requires both manual effort and expertise. However, we believe that its potential has not yet been fully realized, since we have thus far devoted only a few person-days X  effort to this task. With further rule manipulations, we could conceivably obtain a significantly larger pool of well-formed but novel queries to augment our training corpus.

In future research, we plan to embed our user simulator into deployed spoken dialogue systems, where its role would be to automatically generate several example sentences representative of productive user queries following each system response, which would be displayed in a Graphical User Interface. These can serve as an intuitive help mechanism to guide users through the dialogue. We believe that such a device would greatly reduce the percentage of out-of-domain utterances spoken by real users.
 References
