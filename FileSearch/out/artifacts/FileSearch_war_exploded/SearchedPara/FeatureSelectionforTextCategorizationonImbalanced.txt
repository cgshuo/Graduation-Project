 A num ber of feature selection metrics have been explored in text categorization, among whic h information gain (IG), chi-square (CHI), correlation coefficien t (CC) and odds ra-tios (OR) are considered most effectiv e. CC and OR are one-sided metrics while IG and CHI are two-sided. Feature selection using one-sided metrics selects the features most indicativ e of mem bership only , while feature selection us-ing two-sided metrics implicitly com bines the features most indicativ e of mem bership (e.g. positiv e features) and non-mem bership (e.g. negativ e features) by ignoring the signs of features. The former nev er consider the negativ e features, whic h are quite valuable, while the latter cannot ensure the optimal com bination of the two kinds of features esp ecially on imbalanced data. In this work, we investigate the use-fulness of explicit con trol of that com bination within a pro-posed feature selection framew ork. Using multinomial na  X  X ve Bayes and regularized logistic regression as classifiers, our exp erimen ts sho w both great poten tial and actual merits of explicitly com bining positiv e and negativ e features in a nearly optimal fashion according to the imbalanced data. Feature selection has been applied to text categorization in order to impro ve its scalabilit y, efficiency and accuracy . Since eac h documen t in the collection can belong to multi-ple categories, the classification problem is usually split into multiple binary classification problems with resp ect to eac h category . Accordingly , features are selected locally per cat-egory , e.g. local feature selection.
 A num ber of feature selection metrics have been explored, notable among whic h are Information Gain (IG), Chi-square (CHI), Correlation Coefficien t (CC), and Odds Ratio (OR) [8; 9; 10; 12; 15]. CC and OR are one-sided metrics whic h se-lect the features most indicativ e of mem bership for a cate-gory only , while IG and CHI are two-sided metrics, whic h consider the features most indicativ e of either mem bership (e.g. positiv e features) or non-mem bership (e.g. negativ e features). A feature selection metric is considered as one-sided if its positiv e and negativ e values corresp ond to posi-tive and negativ e features resp ectiv ely. On the other hand, a two-sided metric is non-negativ e with the signs of features ignored.
 One choice in the feature selection policy is whether to rule out all negativ e features. Some argue that classifiers built from positiv e features only may be more transferable to new situations where the bac kground class varies. Others believ e that negativ e features are numerous, given the imbalanced data set, and quite valuable in practical exp erience. Their exp erimen ts sho w that when depriv ed of negativ e features, the performance of all feature selection metrics degrades, whic h indicates negativ e features are essen tial to high qual-ity classification [3]. We think that negativ e features are useful because their presence in a documen t highly indicates its non-relev ance. Therefore, they help to confiden tly reject non-relev ant documen ts.
 The focus in this pap er is to answ er the follo wing three ques-tions with empirical evidence: The first two questions are concerned with the poten tial of optimal com bination of positiv e and negativ e features, and the last with a practical solution.
 Imbalanced datasets are commonly encoun tered in text cat-egorization problems, esp ecially in the binary setting. A two-sided metric implicitly com bines the positiv e and neg-ativ e features by simply ignoring the signs of features and comparing their values. However, the values of positiv e fea-tures are not comparable with those of negativ e features due to the imbalanced data. Furthermore, differen t performance measures attac h differen t weigh ts to positiv e and negativ e features: the optimal size ratio between the two kinds of features should also dep end on the performance measure. Tw o-sided metrics cannot ensure the optimal com bination of the positiv e and negativ e features. However, neither one-sided nor two-sided metrics themselv es allo w con trol of the com bination.
 In order to examine the effect of con trol of the com bina-tion of positiv e and negativ e features, this pap er presen ts a novel feature selection framew ork, in whic h the positiv e and negativ e features are selected separately , and then com bined explicitly afterw ards. Sev eral standard metho ds are unified by this framew ork and a set of new metho ds is prop osed that optimally com bines the positiv e and negativ e features for eac h category according to the data characteristics and performance measure.
 The rest of the pap er is organized as follo ws. Sections 2 and 3 describ e the related work: various feature selection metrics and the imbalanced data problem resp ectiv ely. In Section 4, we presen t the new feature selection framew ork. The exp erimen tal setup is rep orted in Section 5, and results are analyzed in Section 6. The last section concludes. In this section, we presen t six feature selection metrics (four kno wn measures and two prop osed varian ts), whic h are func-tions of the follo wing four dep endency tuples: 1. ( t, c i ): presence of t and mem bership in c i . 2. ( t, c i ): presence of t and non-mem bership in c i . 3. ( t, c i ): absence of t and mem bership in c i . 4. ( t, c i ): absence of t and non-mem bership in c i . where: t and c i represen t a term and a category resp ec-tively. The frequencies of the four tuples in the collection are denoted by A, B, C and D resp ectiv ely. The first and last tuples represen t the positiv e dep endency between t and c , while the other two represen t the negativ e dep endency . Information gain (IG) Information gain [12; 15] mea-Chi-square (CHI) Chi-square measures the lack of in-Correlation coefficien t (CC) Correlation coefficien t of Odds ratio (OR) Odds ratio measures the odds of the According to the definitions, OR considers the first two de-pendency tuples, and IG, CHI, and CC consider all the four tuples. CC and OR are one-sided metrics, whose positiv e and negativ e values corresp ond to the positiv e and negativ e features resp ectiv ely. On the other hand, IG and CHI are two-sided, whose values are non-negativ e. We can easily ob-tain that the sign for a one-sided metric, e.g. CC or OR, is sig n ( AD  X  BC ).
 A one-sided metric could be con verted to its two-sided coun-terpart by ignoring the sign, while a two-sided metric could be con verted to its one-sided coun terpart by reco vering the sign, e.g. CHI vs. CC.
 We prop ose the two-sided coun terpart of OR, namely OR-square, and the one-sided coun terpart of IG, namely signed IG as follo ws.
 OR-square (ORS) and Signed IG (SIG) The overall feature selection pro cedure is to score eac h po-ten tial feature according to a particular feature selection metric, and then tak e the best features. Feature selection using one-sided metrics like SIG, CC, and OR pick out the terms most indicativ e of mem bership only . The basic idea behind this is the features coming from non-relev ant docu-men ts are useless. They will nev er consider negativ e features unless all the positiv e features have already been selected. Feature selection using two-sided metrics like IG, CHI, and ORS, however, do not differen tiate between the positiv e and negativ e features. They implicitly com bine the two. The imbalanced data problem occurs when the training ex-amples are unev enly distributed among differen t classes. In case of binary classification, the num ber of examples in one class is significan tly greater than that of the other. Attempts have been made to deal with this problem in div erse domains suc h as fraud detection [2], in-fligh t helicopter gearb ox fault monitoring [4], and text categorization [6; 1; 9]. When training a binary classifier per category in text cate-gorization, we use all the documen ts in the training corpus that belong to that category as relev ant training data and all the documen ts in the training corpus that belong to all the other categories as non-relev ant training data. It is of-ten the case that there is an overwhelming num ber of non-relev ant training documen ts esp ecially when there is a large collection of categories with eac h assigned to a small num-ber of documen ts, whic h is typically an  X  X m balanced data problem X . This problem presen ts a particular challenge to classification algorithms, whic h can achiev e high accuracy by simply classifying every example as negativ e. To overcome this problem  X  X uery zone X  and  X  X ategory zone X  have been introduced to select a subset of most relevant non-relev ant documen ts as the non-relev ant training data [13]. Essen-tially these techniques try to obtain more balanced relev ant and non-relev ant training data by under-sampling negativ e examples.
 In this work, we consider the imbalanced data problem from a differen t persp ectiv e. As Forman [3] argued, feature selec-tion should be relativ ely more imp ortan t than classification algorithms in highly imbalanced situations. Instead of bal-ancing the training data, our metho ds activ ely-select the most useful features, e.g. com bine positiv e and negativ e features in a nearly optimal fashion, according to the im-balanced data. This pro vides an alternativ e to handle the imbalanced data problem. Exp erimen tal comparison of our metho ds and those sampling strategies will be our future researc h.
 The impact of imbalanced data problem on the standard feature selection can be illustrated as follo ws, whic h primar-ily answ ers the first question of Section 1: First, for the metho ds using one-sided metrics (e.g. SIG, CC, and OR), the non-relev ant documen ts are sub ject to misclassification. It will be even worse for the imbalanced data problem, where non-relev ant documen ts dominate. How to confiden tly reject the non-relev ant documen ts is imp or-tan t in that case.
 Second, given a two-sided metric, the values of positiv e fea-tures are not necessarily comparable with those of negativ e features. Let us use CHI for example. The upp er limit CHI value of a positiv e or negativ e feature is N . For the posi-tive feature, it represen ts the case that the feature app ears in every relev ant documen t, but nev er in any non-relev ant documen t. For the negativ e features, it means that the fea-ture app ears in every non-relev ant documen t, but nev er in any relev ant documen t. Due to the large amoun t and di-versit y of the non-relev ant documen ts in imbalanced data set, it is much more difficult for a negativ e feature to reac h the same maxim um that a positiv e feature does. This ex-treme example shed ligh t on why the CHI values of positiv e features are usually much larger than those of negativ e fea-tures. CHI and CC are very similar when the size of the feature set is small and the data set is highly imbalanced. Third, let T P, F P, F N and T N denote true positiv es, false positiv es, false negativ es and true negativ es resp ectiv ely. Positiv e features have more effect on T P and F N , while negativ e features have more effect on T N and F P . Tw o-sided metrics attac h the same weigh ts to the two kinds of features. Therefore, feature selection using a two-sided met-ric com bines the positiv e and negativ e features so as to op-F 1 has been widely used in information retriev al, whic h is: 2  X  T P + F P + F N [11]. In case of imbalanced dataset where T N is much larger than T P , the two measures are quite differen t. Tw o-sided metrics cannot ensure the optimal com bination of positiv e and negativ e features according to F 1. Finally , some performance measures themselv es, e.g. F  X  tively. Therefore, positiv e and negativ e features are consid-ered of differen t imp ortance. Tw o sided metrics cannot en-sure the optimal com bination according to those measures, no matter whether the dataset is balanced or not. Since implicit com bination of positiv e and negativ e features using two-sided metrics is not necessarily optimal, explicit com bination is the choice. In order to examine the effect of con trol of the com bination, we prop ose separate handling of the two kinds of features in the follo wing framew ork. For eac h category c i : where: l is the size of feature set, whic h is usually prede-fined. 0 &lt; l 1 /l  X  1 is the key parameter of the framew ork to be set. The function = (  X  , c i ) should be defined so that the larger = ( t, c i ) is, the more likely the term t belongs to the category c i . Obviously , one-sided metrics like SIG, CC, and OR can serv e as suc h functions, while two-sided metrics like IG, CHI, and ORS cannot.
 In the first step, we intend to pick out those terms most indicativ e of mem bership of c i , while those terms most in-dicativ e of non-mem bership are selected as well in the second step. The final feature set will be the union of the two. Based on their definition, we can easily obtain: Accordingly , the second step can be rewritten as: Therefore, the framew ork com bines the l 1 terms with largest = (  X  , c i ) and the l  X  l 1 terms with smallest = (  X  , c The standard feature selection metho ds generally fall into one of the follo wing two groups: 1. select the positiv e features only using one-sided met-2. implicitly com bine the positiv e and negativ e features The two groups are two special cases of our feature selec-tion framew ork. The standard feature selection using CC corresp onds to the case where = = CC, l 1 /l = 1. The standard metho d using CHI corresp onds to the case where = = CC , and l 1 /l is implicitly set as follo ws. Considering CHI, we have F i = M ax [  X  2 (  X  , c i ) , l ]. The positiv e subset of F is F 0 i = { t  X  F i | CC ( t, c i ) &gt; 0 } . The feature set F equiv alen tly obtained as a com bination of the terms most indicativ e of mem bership and non-mem bership: features with highest and smallest = values resp ectiv ely. Note that F 0 i  X  F + i . It illustrates the standard feature se-lection using CHI is a special case of the framew ork, where l = | F 0 i | is internally decided by the size of feature set l given the data set. The feature selection framew ork facilitates the con trol on explicit com bination of the positiv e and negativ e features through the parameter l 1 /l .
 As we can see, the com bination of positiv e and negativ e features is solely decided by the size ratio l 1 /l given the pre-defined feature size and metric. One-sided and two-sided metrics actually corresp ond to two particular values of that ratio, whic h are not optimal. How to optimize the size ra-tio is the key. We design the follo wing two scenarios for optimization answ ering the second and third questions of Section 1 resp ectiv ely.
 Ideal scenario is designed to explore the poten tial of ex-Practical scenario Certainly , in practice the optimal size Accordingly , a set of new metho ds corresp onding to = = SIG, CC and OR are prop osed for eac h of the two scenar-ios, where l 1 /l  X  (0 , 1] is empirically chosen per category according to the scenario. The first set of metho ds are re-ferred to as impro ved SIG, CC and OR in ideal scenario while the other set are impro ved SIG, CC and OR in prac-tical scenario.
 The efficien t implemen tation of optimization within the frame-work is as follo ws: Therefore, during the optimization of size ratio l 1 /l for eac h category , we did not conduct feature selection for eac h pos-sible l 1 /l , but once only . In order to determine the usefulness of the new feature selec-tion metho ds, we conduct exp erimen ts on standard text cat-egorization data using two popular classifiers: na  X  X ve Bayes and logistic regression. Reuters-21578 (Mo dApte split) is used as our data collec-tion, whic h is a standard data set for text categorization [14; 15; 16]. This dataset con tains 90 categories, with 7769 train-ing documen ts and 3019 test documen ts. After filtering out all num bers, stop-w ords and words occurring less than 3 times, we have 9484 indexing words in the vocabulary . Words in documen t titles are treated same as in documen t body. On the training algorithms, We used na  X  X ve Bayes (NB for short) and regularized logistic regression (LR for short). The multinomial mixture mo del of NB ( tf represen tation) and multiv ariate mo del of LR (binary represen tation) are used [7; 17].
 A NB score between a documen t d and the category c i can be calculated as: where: f j is the feature app earing in the documen t d , P ( c and P ( c i ) represen t prior probabilities of relev ant and non-relev ant resp ectiv ely, and P ( f j | c i ) and P ( f j tional probabilities estimated with Laplacian smo othing. Logistic regression tries to mo del the conditional probabilit y as: The optimization problem for LR is to minimize: w  X  = argmin w { 1 where d i is the i th training example, w is the weigh t vec-tor, y i  X  { X  1 , 1 } is the lab el asso ciated with d i , and  X  is an appropriately chosen regularization parameter, set to be 0.0001 as suggested in [17]. Column relaxation with Gauss-Seidel is used for solving this optimization [17; 16]. To measure the performance, we use both precision ( p ) and recall ( r ) in their com bined from F 1 : 2 pr p + r [11]. To re-main compatible with other results, the F 1 value at Break Even Point (BEP) [17] is rep orted throughout this pap er, whic h avoids the tuning of thresholds and purely evaluates the direction of the decision hyperplane learned by the linear metho d itself. BEP is defined to be the point where precision equals recall. It corresp onds to the minim um of | F P  X  F N | in practice. To measure the global performance of differen t metho ds, We rep ort both micro and macro-a veraged BEP F1.
 The micro-a veraged F1 is largely dep enden t on the most common categories while the rare categories influence macro-averaged F1. The two most common categories: earn and acq con tain man y more positiv e documen ts than the re-maining categories and both of them have very good perfor-mance on differen t classifiers (around .95 BEP F1 with both NB and LR). The micro-a veraged F1 is dominated by the F1 values of the two categories. Besides, feature selection on imbalanced data is our focus in this study . The top two cat-egories are somewhat balanced. On the other side, for those extremely rare categories con taining only a few documen ts, their F1 values are unstable, whic h will affect the reliabilit y of macro-a veraged F1 [16]. Therefore, we decided to exclude the two most common categories and those categories con-taining less than ten training documen ts from our collection, whic h results in 58 categories from the third to the sixtieth, rank ed in decreasing order according to the num ber of pos-itiv e examples. Figure 1 sho w the percen tages of positiv e Figure 1: The percen tage of # positiv e examples in the training set (58 categories:3rd-60th) examples in the training set for the 58 categories. The per-cen tage ranges from a minim um of 0.1% to maxim um of 7% in the training set, whic h indicate high class imbalances of the dataset.
 We also list the maxim um IG, CHI and ORS values of pos-itiv e and negativ e features for eac h of the 58 categories in table 7 as attac hed, whic h verifies the second illustration of Section 3: the values of positiv e features are not compara-ble with those of negativ e features, given a two-sided metric. During the optimization of the size ratio per category , we (21 possible ratios) and select the ratio having best BEP F1 on test and training sets corresp onding to the two optimiza-tion scenarios resp ectiv ely. In order to compare differen t feature selection metho ds, we apply them to text categorization using NB and LR, and compare their performance in terms of micro and macro av-eraged F1 (BEP). We rep ort the performance of NB and LR with the impro ved feature selection metho ds in both optimization scenarios as compared to the standard feature selection metho ds using one-sided and two-sided metrics. In order to answ er the second question of Section 1, we con-sider the follo wing three groups of feature selection metho ds: The metho ds are compared to eac h other within the same group at differen t sizes of features. Typical size of a lo-cal feature set is between 10 and 50 [10]. In this pap er, the performance are rep orted at a much wider range: 10  X  3000. Tables 1 and 2 list the micro and macro averaged BEP F1 values for na  X  X ve Bayes classifiers with the nine differen t fea-ture selection metho ds (as listed in the first row) at differ-ent sizes of feature set ranging from 10 to 3000 (as listed in the first column); Tables 3 and 4 list the micro and macro averaged BEP F1 values for regularized logistic regression classifiers. The best micro averaged F1 across differen t sizes of feature set is highligh ted for eac h metho d. We can see the impro ved metho ds in ideal scenario significan tly outp erform the corresp onding one-sided and two-sided metho ds, whic h indicates the great poten tial of optimal com bination of pos-itiv e and negativ e features.
 From tables 1 and 2, we can see it is very useful to con-duct feature selection for NB (The micro-a veraged F1 for NB without feature selection is .641). On the other side, tables 3 and 4 sho w that standard feature selection, e.g. IG, SIG, CHI, CC, ORS and OR, will not impro ve LR X  X  per-formance (The micro-a veraged F1 for LR without feature selection is .766), whic h confirms the general conseus that standard feature selection will not help the regularized lin-ear metho ds, e.g. SVMs, LR and ridge regression. However, the best performance of the impro ved metho ds in ideal sce-nario (iSIG: .81, iCC: .812, and iOR: .816) sho ws that the impro ved feature selection metho ds can be very helpful to LR.
 Figure 2 sho w the size ratios implicitly decided by two-sided metrics: IG, CHI and ORS resp ectiv ely (feature size = 50), whic h confirms that feature selection using a two-sided met-ric is similar to its one-sided coun terpart(size ratio = 1) when the feature size is small. When using CHI, only pos-itiv e features are considered for 56 categories(equiv alen t to CC) and only one negativ e feature is included for eac h of the remaining two categories(3rd and 7th).
 Figure 3 visualizes the optimization for the first two ( money-fx and grain ) and last two categories ( dmk and lumb er ) us-ing NB, where = = CC , feature size = 50. Figures 4 and 5 sho w the optimal size ratios for the 58 categories using NB and LR resp ectiv ely. Both are quite differen t from figure 2, whic h confirms that implicit com bination using two-sided metrics are not optimal. The 58 categories are ordered by the num bers of their training documen ts. Intuitiv ely, given the fixed size of feature set, 50 in this case, the optimal size ratio should decrease with category id increases. We can see in figures 4 and 5 that the optimal size ratios vibrate be-tween 0 and 1 from category to category irregularly though the general trend confirms the intuition. Therefore, besides num ber of positiv e examples, category(domain) character-istics also have effects on optimal feature selection. Our results also sho w the optimal size ratios learned by NB with iSIG, iCC and iOR (figure 4) are significan tly differen t from those learned by LR (figure 5) resp ectiv ely. Both results confirm Mladeni  X c X  X  observ ation [9]: a good feature scoring measure for text should consider domain and classification algorithm characteristics. We consider the impro ved CC in the practical optimization scenario also. The performance of the impro ved CC in prac-tical optimization scenario with NB and LR is rep orted in table 5. The best performance with NB is .74 (feature size = 50), whic h is 3% lower than the ideal scenario (.77), but sig-nifican tly (5.5%) better than CHI and CC (both are .685). The adv antage of the impro ved CC in practical scenario over standard CHI and CC is also observ ed with LR. Wilco xon signed rank tests sho w that impro ved CC in practical sce-nario significan tly outp erforms CHI and CC with both NB and LR at the 0.05 significance level. This verifies that the impro ved metho ds have not only great poten tial but practi-cal merits also. Since LR is a highly effectiv e classifier com-pared to non-regularized metho ds suc h as NB, the room for impro vemen t is not that huge. This explains why the perfor-mance gain of LR using our new feature selection metho ds is not as much as that of NB. In this scenario, the size ra-Figure 2: Size ratios implicitly decided by using two-sided metrics: IG, CHI and ORS resp ectiv ely (58 categories:3rd-60th, feature size = 50) tios were optimized over the whole training set, whic h migh t cause overfitting. We exp ect more performance gain by n -fold cross validation with the training set.
 The micro-a veraged F1 values for NB with iCC in ideal and practical scenarios are .77 and .74 resp ectiv ely (fea-ture size = 50), both approac h LR without feature selec-Figure 3: BEP F1 for test over the first two and last two categories (out of 58 categories) at differen t l 1 /l values (NB, = = CC , feature size = 50). The optimal size ratios for money-fx, grain, dmk and lumb er are 0.95, 0.95, 0.1 and 0.2 resp ectiv ely Figure 4: Optimal size ratios of iSIG, iCC and iOR (NB, 58 categories:3rd-60th, feature size = 50) Table 5: Micro and macro-a veraged F1(BEP) values for NB and LR with the impro ved CC in practical scenario at dif-feren t sizes of features over the 58 categories Table 6: BEP F1 values of NB and LR for the two most common categories: 1st and 2nd. iCC and iCC X  represen t ideal and practical scenarios resp ectiv ely, feature size = 50 earn .957 .914 .958 .956 .971 .959 .974 .97 acq .871 .819 .917 .907 .897 .858 .926 .926 tion (.766). This indicates that with the impro ved feature selection metho ds suc h as iCC, NB is comp etitiv e with state-of-the-art classification metho ds suc h as LR.
 Obviously , the efficiency of the impro ved metho ds mainly dep ends on the classification metho d used to learn the op-timal size ratios. For those fast algorithms suc h as NB, lin-ear regression, etc, the optimization is reasonably efficien t. Since the optimization of size ratio for one category is in-dep enden t with other categories, parallel computing can be performed for those time-consuming classifiers, e.g. k NN, neural net works, SVMs, LR, etc, with man y features (fea-ture size &gt; 1000). Possible further work includes an analytic solution to finding optimal size ratios based on the category characteristics. In order to investigate the effect of our new metho ds on balanced data, we rep ort in table 6 the performance val-ues for the two most common categories: earn and acq separately . For earn , no performance gain is observ ed by applying our new metho ds. It X  X  due to the well-balanced na-ture of this category: around 2 5 of its training examples are positiv e. However, the performance of acq was significan tly impro ved. This can be partially explained by the fact that this category is more imbalanced than earn : only around 1 of its training examples are positiv e.
 In order to compare this work with others, we also sho w in figures 6 and 7 the micro-a veraged BEP F1 values for the second group of feature selection metho ds on all 90 cate-Figure 6: Micro-a veraged BEP F1 values for CC, CHI, iCC in practical scenario, and iCC in ideal scenario (NB, all 90 categories) gories. We can see in figure 6 the micro-a verged F1 of all 90 categories for NB using CHI (with 2000 features) is 80.4%. The micro averaged F1(BEP) of LR without feature selec-tion is .854 as sho wn in figure 7. Both are consisten t with previous published results [14; 16; 17]. The micro averaged F1(BEP) score of LR is sligh tly lower than [17] due to a dif-ference of treating documen t titles. [17; 16] treated words in documen t titles as differen t words in documen t bodies while we consider them equiv alen tly.
 As we exp ect, the impro ved metho ds consisten tly outp er-form standard CHI and CC. However, since the micro-a veraged F1 over all 90 categories is dominated by the most common category , whic h happ en to be well-balanced, the impro ve-men t is not as impressiv e. We can also see in figures 6 and 7 that CHI is alw ays better than CC even when the feature size is small. For the two most common categories, CHI and CC will choose features quite differen tly since the CHI values of positiv e and negativ e features are comparable on balanced data. In that case, CHI will select useful negativ e features as well even when the feature size is small. Another interesting point in figure 6 is that with the num ber of fea-tures increases, the performance of CC first increases, then decreases, and finally increases again. The first increase is due to the inclusion of more useful positiv e features. The afterw ard decrease is due to the inclusion of noisy or non-indicativ e (1) positiv e and (2) negativ e features. The final increase is because of the inclusion of useful negativ e fea-tures. In con trast with figure 6, figure 7 does not see notice-able performance drop for CHI and CC, whic h is because that due to the regularization factor of LR, the inclusion of noisy features has much less impact on LR than NB. In order to investigate the usefulness of exp erimen ting with the com bination of positiv e and negativ e features, a novel feature selection framew ork was presen ted, in whic h the pos-itiv e and negativ e features are separately selected and ex-plicitly com bined. We explored three special cases of the framew ork: 1. consider the positiv e features only by using one-sided 2. implicitly com bine the positiv e and negativ e features 3. com bine the two kinds of features explicitly and choose The first two cases are kno wn and standard, and the last one is new. The main conclusions are: Furthermore, we observ e that multinomial na  X  X ve Bayes with our prop osed feature selection is comp etitiv e with the state-of-the-art algorithms suc h as regularized logistic regression. We would like to thank Dr. David Pierce, Dr. Miguel Ruiz, Mr. Wei Dai, and the anon ymous review ers for valuable commen ts and suggestions. Thanks to Mr. Jian Zhang and Dr. Tong Zhang for discussions about the implemen tation of logistic regression and evaluation issues. [1] S. Dumais, J. Platt, D. Hec kerman, and M. Sahami. [2] T. Fawcett and F. Pro vost. Adaptiv e fraud detection. [3] G. Forman. An extensiv e empirical study of feature se-[4] N. Japk owicz and S. Stephen. The class imbalance prob-[5] R. Koha vi and G. H. John. Wrapp ers for feature subset [6] D. Lewis and M. Ringuette. A comparison of two learn-[7] A. McCallum and K. Nigam. A comparison of event [8] D. Mladeni. Machine Learning on non-homo gene ous, [9] D. Mladeni and G. Mark o. Feture selection for unbal-[10] H. Ng, W. Goh, and K. Low. Feature selection, percep-[11] V. Rijsb ergen. Information Retrieval . Butterw orths, [12] F. Sebastiani. Mac hine learning in automated text cate-[13] A. Singhal, M. Mitra, and C. Buc kley . Learning rout-[14] Y. Yang. An evaluation of statistical approac hes to [15] Y. Yang and J. Pedersen. A comparativ e study on fea-[16] J. Zhang and Y. Yang. Robustness of regularized lin-[17] T. Zhang and F. Oles. Text categorization based on Table 7: The maxim um IG, CHI and ORS values of posi-tive (+) and negativ e (-) features for eac h of 58 categories.
