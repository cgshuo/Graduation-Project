 Features selection is used for the purpose of dimensionality reduction by selecting significant terms from text. It can be performed basically in two ways: local and global feature selection. Furthermore, feature set can be seen in another perspective: local dictionary and universal dictionary. In this paper, we intend to answer the fol-lowing questions with empirical evidence on the 7 feature selections: Feature selection in text categorization has enjoyed rich literatures in the past 2 dec-ades, especially on local and global feature selection, plus small number of works on local and universal dictionary. However, there is no work reporting the correspon-dence effect of local and global feature selection on the different dictionary. Feature selection basically can be performed in two distinct ways: local and global them is tagged with category label, C={c 1 , c 2 , ..., c m } . (positive class) of which the specific classifier will operate. Feature set extracted from c thus will be differed from feature set derived from category other than c 1 . The study tions when performing local feature selection: local feature size cannot be fixed at a pre-defined quantity. Rather, it can be seen as threshold (cut-off point), it will there-fore never choose more features than there are unique terms in training documents of that category. This is usually happened to populated categories. The second condition proposes if there are training documents where their distinct features is smaller than threshold; for example, the distinct feature size will make the threshold, no matter how high the threshold is. For smaller training data with smaller unique term, it tends to has smaller feature set. Local feature selection is operated under the category of interest where it capable of capturing more descriptive feature and give more informa-tion to classifier. Hence, it can be smarter [2][4]. Somehow, for scarcely populated category, it turned out to be less robust [1]. Global Feature Selection  X  selects terms from the documents under all categories. In order to specifically weight the term globally, at class independent sense, a globaliza-tion technique is applied to obtain a global score relatively to each category. The most common technique likes sum, averaged or selecting the maximum value of the cate-gory-specific value can be seen in [2][3][5]. Global feature selection by definition has no idea on what classification task is going to be required of the features. They must preserve and obtain if possible every category-specific significant feature that may be important to classification task and can only safely remove features that will not be relevant to classification task. At its best, a feature set f 2 , extracted from all the classes information to work with, especially working on small size training documents, where it is very likely to over-generalize feature set. Beside, it is tough, as to what extend the number of feature is needed to characterize each category in order to capture the char-acteristics of all categories in order to produce the best classification result. After looking at  X  X ow X  features can be selected, we shift our attention to  X  X hat X  kind of features can be obtained. Research on text categorization suggests two possible ways in which the term features can be harvested: document within the positive class (local dictionary), or combination of both the positive and negative classes (universal dictionary). We can treat that the whole feature space is divided into 2 parts: relevant idea of closed-world assumption is applied. Features can be chosen between class c 1 size of c 1  X  is 1  X  c 1  X   X  m . Local Dictionary  X  choses only significance terms from the interested class to train the classifier. Various criteria can be used to measure the amount of good it does to include or exclude the term from consideration in examining whether a document formance. In particular, results from our study show that local dictionary gives con-siderably better performance [6]. For local dictionary, the term X  X  weight for each category is computed based on the relevant document in the category only. Local selection, only features from the interested category are taken into account. In global feature selection, features from each category of interest are collected and normalized by gradually summing or averaging the weights of a feature among the categories. Universal Dictionary  X  is made up of both relevant and irrelevant documents. If a feature set is being derived for a class c 1 classifier, it is considered positive class when working with it, and the training documents for the rest of the classes are labeled as c 1  X . possible that no terms from c 1 could be chosen but only terms from c 1  X  (negative catego-ries) are chosen where this will be the best set of terms to use in order to make such a decision. Note that all of these methods can keep terms that occur only in c 1  X  if they are very telling of the category c 1  X . However, there are special cases where a term can occur up having 2 feature sets: positive, F+ and negative, F-; it is always true that {F+}&lt;{F-}, as F-constitutes from categories other than c 1 . We intend to pick up high weight F+ and low F+ to be the good indicators of the positive and negative category . The larger (smaller) the value is, the more likely the term to positive (negative) category, universal dictionary will be the union of the two. Universal dictionary for global feature selection is build by combining universal dictionary from all categories into one single global feature set. Global score is computed by sum, averaged or selecting the maximum value of the category feature set. There are 7 feature selection measures being examined in our study: Correlation Co-efficient (CC) [7][8], Chi-Square (CC)[4][5][7 ][8], Categorical Term Descriptor (CTD)[9], Gain Ratio (GR)[7], Information Gain (IG)[4][5][8], and Mutual Informa-tion (MI)[5]. For the sake of brevity, we have omitted their definition and justifica-tion. Their detailed can be obtained in the respective literatures. We employed Reuters-21578 as benchmarking dataset in our experiments. In our study, it was di-vided into subsets of the 10 most populous categories and original 115 categories, which we refer them as Reuters(10) and Reuters(115) respectively. The reason of dividing Reuters-21578 into 2 subsets is to examine the feature selection methods X  effectiveness in choosing feature terms from uniform and scattered distributed catego-ries. Feature selection on Reuters(10) is assumed to be easier when compared to Reuter(115). On the other hand, Reuters(115) is seen to be more difficult because it contains scattered document in high number of categories. Theoretically, it tends to generate higher misclassification rate. We employed Multinomial Na X ve Bayes [10] here as the control classifiers to compare various feature selection measures. To evaluate the performance we use FMeasure [3][4]. As we run the experiments with Reuters(10) and Reuters(115), it is crucial that the readings do not to be affected by the large gap of category and document size between them. For this reson, we report only mirco-averaged FMeasure. Overall, we could notice in Table 1 that local feature selection always outperforms global feature selection in both local and universal dictionary. Our finding is aligned with [2][4]. This has suggested that local feature set is informative and better at de-scribing categories. At the same time, we also discovered that categories with high number of document tend to generate more descriptive feature set. On the other hand, the performance of global feature selections is rather pessimistic, especially on the scarcely distributed Reuters(115). There are many repeated features among the feature set; an evidence of over-generalization. The poor performance of global feature selec-tion is very likely caused by improper normalization, such as averaging and selecting of maximum score. We believe that averaging the score of a feature could reduce significantly the high relevancy of the term to the specific category as the score might be dispersed to other category. This is rather true if we are dealing with dataset with high number of category. Moreover, selecting maximum score may literally favor of rare term which is not significant; it biases toward the category which gives more weight to the term, tends to perform unfair selection when dealing with imbalanced data where it ends up high proportion of the feature set are nominated from the highly populated categories. From our study, the performance of averaged-global feature selection is identical to maxed-global feature selection. 
Table 1 also indicates that the universal dictionary in either local or global feature selection does not perform as well as the local dictionary. Local dictionary is the bet-ter descriptor for the category; incorporating negative feature does not optimize the categorization results as claim in [3][7]. Our partial finding has suggested that the negative features can be the noise in disguise. It occurs when the negative features dominate the universal dictionary, increasing misclassification rate. 
Our intuition also suggested that the imbalanced division of dataset into positive and negative category is another factor of poor performance of universal dictionary Our preliminary study suggested that if a dictionary does not take into account of negative features, the chances of them appear to be the noise to classifier is thus re-duced. However, [7] in their study has contrary finding in which incorporating of negative feature can remarkably improve categorization performance. In addition, balanced dataset such as Reuters(10), always has better reading than Reuters(115). 
We find that some feature selection metrics tend to incorporate high number of negative feature (see Table 2). This is especially true to MI and CTD. The negative feature in MI dominates the global feature set, average 70%. Looking at its categori-zation efficiency, it has recorded merely 0.47 and is the worst performer among the 7 feature selection we have studied. On the other hand, IG and CHI contain no and least negative feature in average, both are the top performers. However, it would be unfair arguing that negative feature is harmful for classifier. In the table, we notice to that CC, GSS, and CTD consist of high number negative feature but produce decent result whereas OR has worse result though having small number negative feature. The negative feature accrues in the universal dictionary able to negate opposite document; however, it is bound to the nature of feature selection. 
Table 3 implies that with global feature selection, some categories only capable of contributing small portion of positive features in the 2000 features. By investigating the term distribution pattern, we found high ratio of duplicated term in the categories. In other words, terms appear in local feature set can appear in other local feature set. In the worst case, we even came across with a number of terms which appear in all of the 10 categories. Global feature selection leads to a situation where the feature set made up by irrelevance terms in respective categories. There is high number of over-lapped terms among categories. The inconsistent feature size from each category is because the limited number of unique term in each category can result high rate of misclassification, which is very likely to be overrun by the negative features . In this paper, we discussed local and global feature selection with two different dic-tionaries: local and universal dictionary, with Reuters-21578 variations as dataset. We concluded that:  X  Local feature selection with local dictionary is the best performer among all.  X  Global feature selection tends to leave out some significant terms at the same  X  Universal dictionary does not work well with most of the local and global feature In addition, we also reported that CTD[9], which derived from term weighting scheme can perform equally well compared state-of-the-art feature selections like CHI and IG. It is a mild performer in local and global feature selection, works well with local dictionary. 
