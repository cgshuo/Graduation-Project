 Modeling probability mass functions (pmfs) as random is useful in solving many real-world prob-lems. A common random model for pmfs is the Dirichlet distribution [1]. The Dirichlet is conjugate to the multinomial and hence mathematically convenient for Bayesian inference, and the number of parameters is conveniently linear in the size of the sample space. However, the Dirichlet is a distri-bution over the entire probability simplex, and for many problems this is simply the wrong domain if there is application-specific prior knowledge that the pmfs come from a restricted subset of the simplex.
 For example, in natural language modeling, it is common to regularize a pmf over n-grams by some generic language model distribution q 0 , that is, the pmf to be modeled is assumed to have the form  X  =  X q + (1  X   X  ) q 0 for some q in the simplex,  X   X  (0 , 1) and a fixed generic model q 0 [2]. But once q 0 and  X  are fixed, the pmf  X  can only come from a subset of the simplex. Another natural language processing example is modeling the probability of keywords in a dictionary where some words are related, such as espresso and latte , and evidence for the one is to some extent evidence for the other. This relationship can be captured with a bounded variation model that would constrain the modeled probability of espresso to be within some of the modeled probability of latte . We show that such bounds on the variation between pmf components also restrict the domain of the pmf to a subset of the simplex. As a third example of restricting the domain, the similarity discriminant analysis classifier estimates class-conditional pmfs that are constrained to be monotonically increasing over an ordered sample space of discrete similarity values [3]. In this paper we propose a simple variant of the Dirichlet whose support is a subset of the simplex, explore its properties, and show how to learn the model from data. We first discuss the alternative solution of renormalizing the Dirichlet over the desired subset of the simplex, and other related work. Then we propose the shadow Dirichlet distribution; explain how to construct a shadow Dirichlet for three types of restricted domains: the regularized pmf case, bounded variation between pmf components, and monotonic pmfs; and discuss the most general case. We show how to use the expectation-maximization (EM) algorithm to estimate the shadow Dirichlet parameter  X  , and present simulation results for the estimation. One solution to modeling pmfs on only a subset of the simplex is to simply restrict the support of the Dirichlet to the desired support  X  S , and renormalize the Dirichlet over  X  S (see Fig. 1 for an ex-ample). This renormalized Dirichlet has the advantage that it is still a conjugate distribution for the multinomial. Nallapati et al.considered the renormalized Dirichlet for language modeling, but found it difficult to use because the density requires numerical integration to compute the normalizer [4] . In addition, there is no closed form solution for the mean, covariance, or peak of the renormal-ized Dirichlet, making it difficult to work with. Table 1 summarizes these properties. Additionally, generating samples from the renormalized Dirichlet is inefficient: one draws samples from the stan-dard Dirichlet, then rejects realizations that are outside  X  S . For high-dimensional sample spaces, this could greatly increase the time to generate samples.
 Although the Dirichlet is a classic and popular distribution on the simplex, Aitchison warns it  X  X s to-tally inadequate for the description of the variability of compositional data, X  because of its  X  X mplied ing compositions whose components have even weak forms of dependence X  [5]. Aitchison instead championed a logistic normal distribution with more parameters to control covariance between com-ponents.
 A number of variants of the Dirichlet that can capture more dependence have been proposed and analyzed. For example, the scaled Dirichlet enables a more flexible shape for the distribution [5], but does not change the support. The original Dirichlet(  X  1 , X  2 ,... X  d ) can be derived as Y j / P j Y j where Y j  X   X (  X  j , X  ) , whereas the scaled Dirichlet is derived from Y j  X   X (  X  j , X  j ) , resulting in density p (  X  ) =  X  Q j  X  Another variant is the generalized Dirichlet [6] which also has parameters  X , X   X  R d + , and allows greater control of the covariance structure, again without changing the support. As perhaps first noted by Karl Pearson [7] and expounded upon by Aitchison [5], correlations of proportional data can be very misleading. Many Dirichlet variants have been generalizations of the Connor-Mossiman variant, Dirichlet process variants, other compound Dirichlet models, and hierarchical Dirichlet models. Ongaro et al. [8] propose the flexible Dirichlet distribution by forming a re-parameterized mixture of Dirichlet distributions. Rayens and Srinivasan [9] considered the dependence structure for the general Dirichlet family called the generalized Liouville distributions. In contrast to prior efforts, the shadow Dirichlet manipulates the support to achieve various kinds of dependence that arise frequently in machine learning problems. We introduce a new distribution that we call the shadow Dirichlet distribution. Let S be the prob-ability ( d  X  1) -simplex, and let  X   X   X  S be a random pmf drawn from a Dirichlet distribution with density p D and unnormalized parameter  X   X  R d + . Then we say the random pmf  X   X  X  is distributed according to a shadow Dirichlet distribution if  X  = M  X   X  for some fixed d  X  d left-stochastic (that is, each column of M sums to 1) full-rank (and hence invertible) matrix M , and we call  X   X  the gen-erating Dirichlet of  X  , or  X   X  X  Dirichlet shadow . Because M is a left-stochastic linear map between finite-dimensional spaces, it is a continuous map from the convex and compact S to a convex and compact subset of S that we denote S M .
 The shadow Dirichlet has two parameters: the generating Dirichlet X  X  parameter  X   X  R d + , and the d  X  d matrix M . Both  X  and M can be estimated from data. However, as we show in the following subsections, the matrix M can be profitably used as a design parameter that is chosen based on application-specific knowledge or side-information to specify the restricted domain S M , and in that way impose dependency between the components of the random pmfs.
 The shadow Dirichlet density p (  X  ) is the normalized pushforward of the Dirichlet density, that is, it is the composition of the Dirichlet density and M  X  1 with the Jacobian: Dirichlet precision factor. Table 1 summarizes the basic properties of the shadow Dirichlet. Fig. 1 shows an example shadow Dirichlet distribution.
 Generating samples from the shadow Dirichlet is trivial: generate samples from its generating Dirichlet (for example, using stick-breaking or urn-drawing) and multiply each sample by M to create the corresponding shadow Dirichlet sample.
 Table 1: Table compares and summarizes the Dirichlet, renormalized Dirichlet, and shadow Dirich-let distributions.
 3.1 Example: Regularized Pmfs The shadow Dirichlet can be designed to specify a distribution over a set of regularized pmfs S M = the following d  X  d matrix M will change the support to the desired subset S M by mapping the extreme points of S to the extreme points of S M : where I is the d  X  d identity matrix. In Section 4 we show that the M given in (2) is optimal in a maximum entropy sense. 3.2 Example: Bounded Variation Pmfs We describe how to use the shadow Dirichlet to model a random pmf that has bounded variation on the variation, we first analyze the variation for a given M . For any d  X  d left stochastic matrix Thus, to obtain a distribution over pmfs with bounded |  X  k  X   X  ` | X  k,l for any k,` components, it is sufficient to choose components of the matrix M such that | M kj  X  M lj | X  k,l for all j = 1 ,...,d because  X   X  in (3) sums to 1.
 One way to create such an M is using the regularization strategy described in Section 3.1. For this case, the j th component of  X  is  X  j = M  X   X  i th and j th component of any pmf in S M is: Thus by choosing an appropriate  X  and regularizing pmf  X   X  , one can impose the bounded variation given by (4). For example, set  X   X  to be the uniform pmf, and choose any  X   X  (0 , 1) , then the matrix M given by (2) will guarantee that the difference between any two entries of any pmf drawn from the shadow Dirichlet ( M, X  ) will be less than or equal to  X  . 3.3 Example: Monotonic Pmfs For pmfs over ordered components, it may be desirable to restrict the support of the random pmf distribution to only monotonically increasing pmfs (or to only monotonically decreasing pmfs). A d  X  d left-stochastic matrix M that will result in a shadow Dirichlet that generates only mono-tonically increasing d  X  1 pmfs has k th column [0 ... 0 1 / ( d  X  k + 1) ... 1 / ( d  X  k + 1)] T , we call this the monotonic M . It is easy to see that with this M only monotonic  X   X  X  can be produced, that the monotonic M is optimal in a maximum entropy sense.
 Note that to provide support over both monotonically increasing and decreasing pmfs with one distribution is not achievable with a shadow Dirichlet, but could be achieved by a mixture of two shadow Dirichlets. 3.4 What Restricted Subsets are Possible? Above we have described solutions to construct M for three kinds of dependence that arise in machine learning applications. Here we consider the more general question: What subsets of the simplex can be the support of the shadow Dirichlet, and how to design a shadow Dirichlet for a par-ticular support? For any matrix M , by the Krein-Milman theorem [10], S M = M S is the convex hull of its extreme points. If M is injective, the extreme points of S M are easy to specify, as a d  X  d matrix M will have d extreme points that occur for the d choices of  X  that have only one nonzero component, as the rest of the  X  will create a non-trivial convex combination of the columns of M , and therefore cannot result in extreme points of S M by definition. That is, the extreme points of S M are the d columns of M , and one can design any S M with d extreme points by setting the columns of M to be those extreme pmfs.
 However, if one wants the new support to be a polytope in the probability ( d  X  1) -simplex with m &gt; d extreme points, then one must use a fat M with d  X  m entries. Let S m denote the probability ( m  X  1) -simplex, then the domain of the shadow Dirichlet will be M S m , which is the convex hull of the m columns of M and forms a convex polytope in S with at most m vertices. In this case M cannot be injective, and hence it is not bijective between S m and M S m . However, a density on M S m can be defined as: On the other hand, if one wants the support to be a low-dimensional polytope subset of a higher-dimensional probability simplex, then a thin d  X  m matrix M , where m &lt; d , can be used to implement this. If M is injective, then it has a left inverse M  X  that is a matrix of dimension m  X  d , and the normalized pushforward of the original density can be used as a density on the image M S m : If M is not injective then one way to determine a density is to use (5). In this section we note two information-theoretic properties of the shadow Dirichlet. Let  X  be drawn from shadow Dirichlet density p M , and let its generating Dirichlet  X   X  be drawn from p D . Then the the differential entropy of its generating Dirichlet. In fact, the shadow Dirichlet always has less entropy than its Dirichlet shadow because log | det( M ) |  X  0 , which can be shown as a corollary to the following lemma (proof not included due to lack of space): Lemma 4.1. Let { x 1 ,...,x n } and { y 1 ,...,y n } be column vectors in R n . If each y j is a convex | det[ y 1 ,...,y n ] | X | det[ x 1 ,...,x n ] | .
 It follows from Lemma 4.1 that the constructive solutions for M given in (2) and the monotonic M are optimal in the sense of maximizing entropy: Corollary 4.1. Let M reg be the set of left-stochastic matrices M that parameterize shadow Dirichlet Then the M given in (2) results in the shadow Dirichlet with maximum entropy, that is, (2) solves Corollary 4.2. Let M mono be the set of left-stochastic matrices M that parameterize shadow Dirichlet distributions that generate only monotonic pmfs. Then the monotonic M given in Sec-tion 3.3 results in the shadow Dirichlet with maximum entropy, that is, the monotonic M solves In this section, we discuss the estimation of  X  for the shadow Dirichlet and compound shadow Dirichlet, and the estimation of M . 5.1 Estimating  X  for the Shadow Dirichlet Let matrix M be specified (for example, as described in the subsections of Section 3), and let q be a component of the i th sample pmf for j = 1 ,...,d . Then finding the maximum likelihood estimate of  X  for the shadow Dirichlet is straightforward: arg max where  X  q = M  X  1 q . Note (6) is the maximum likelihood estimation problem for the Dirichlet dis-tribution given the matrix  X  q , and can be solved using the standard methods for that problem (see e.g. [11, 12]). 5.2 Estimating  X  for the Compound Shadow Dirichlet For many machine learning applications the given data are modeled as samples from realizations of a random pmf, and given these samples one must estimate the random pmf model X  X  parameters. We refer to this case as the compound shadow Dirichlet, analogous to the compound Dirichlet (also called the multivariate P  X  olya distribution). Assuming one has already specified M , we first discuss method of moments estimation, and then describe an expectation-maximization (EM) method for computing the maximum likelihood estimate  X   X  .
 One can form an estimate of  X  by the method of moments. For the standard compound Dirichlet, one treats the samples of the realizations as normalized empirical histograms, sets the normalized  X  parameter equal to the empirical mean of the normalized histograms, and uses the empirical variances to determine the precision  X  0 . By definition, this estimate will be less likely than the maximum likelihood estimate, but may be a practical short-cut in some cases. For the compound shadow Dirichlet, we believe the method of moments estimator will be a poorer estimate in general. The problem is that if one draws samples from a pmf  X  from a restricted subset S M of the simplex, then the normalized empirical histogram  X   X  of those samples may not be in S M . For example given a monotonic pmf, the histogram of five samples drawn from it may not be monotonic. Then the empirical mean of such normalized empirical histograms may not be in S M , and so setting the shadow Dirichlet mean M X  equal to the empirical mean may lead to an infeasible estimate (one that is outside S M ). A heuristic solution is to project the empirical mean into S M first, for example, by finding the nearest pmf in S M in squared error or relative entropy. As with the compound Dirichlet, this may still be a useful approach in practice for some problems.
 Next we state an EM method to find the maximum likelihood estimate  X   X  . Let s be a d  X  N matrix of sample histograms from different experiments, such that the i th column s i is the i th histogram for i = 1 ,...,N , and ( s i ) j is the number of times we have observed the j th event from the i th pmf v i . Then the maximum log-likelihood estimate of  X  solves arg max log p ( s |  X  ) for  X   X  R k + . If the random pmfs are drawn from a Dirichlet distribution, then finding this maximum likelihood estimate requires an iterative procedure, and can be done in several ways including a gradient descent (ascent) approach. However, if the random pmfs are drawn from a shadow Dirichlet distribution, then a direct gradient descent approach is highly inconvenient as it requires taking derivatives of numerical integrals. However, it is practical to apply the expectation-maximization (EM) algorithm [13][14], as we describe in the rest of this section. Code to perform the EM estimation of  X  can be downloaded from idl.ee.washington.edu/publications.php.
 and hence arg max  X   X  R k To apply the EM method, we consider the complete data to be the sample histograms s and the pmfs that generated them ( s,v 1 ,v 2 ,...,v N ) , whose expected log-likelihood will be maximized. repeatedly maximize the Q-function such that the estimate of  X  at the ( m + 1) th iteration is: Like the compound Dirichlet likelihood, the compound shadow Dirichlet likelihood is not neces-sarily concave. However, note that the Q-function given in (7) is concave, because log p ( v i |  X  ) =  X  log | det( M ) | + log p D, X  M  X  1 v i , where p D, X  is the Dirichlet distribution with parameter  X  , and by a theorem of Ronning [11], log p D, X  is a concave function, and adding a constant does not change the concavity. The Q-function is a finite integration of such concave functions and hence also concave [15].
 We simplify (7) without destroying the concavity to yield the equivalent problem  X  ( m +1) =  X  where  X  i is the normalization constant for the multinomial with histogram s i .
 We apply the Newton method [16] to maximize g (  X  ) , where the gradient  X  g (  X  ) has k th component  X  (  X  0 )  X   X  0 (  X  1 ) +  X  1 , where  X  0 denotes the digamma function. Let  X  1 denote the trigamma Note that because H has a very simple structure, the inversion of H required by the Newton step is greatly simplified by using the Woodbury identity [17]: H  X  1 =  X  diag(  X  1 ,..., X  d )  X  5.3 Estimating M for the Shadow Dirichlet Thus far we have discussed how to construct M to achieve certain desired properties and how to interpret a given M  X  X  effect on the support. In some cases it may be useful to estimate M directly from data, for example, finding the maximum likelihood M . In general, this is a non-convex problem because the set of rank d  X  1 matrices is not convex. However, we offer two approximations. First, note that as in estimating the support of a uniform distribution, the maximum likelihood M will correspond to a support that is no larger than needed to contain the convex hull of sample pmfs. Second, the mean of the empirical pmfs will be in the support, and thus a heuristic is to set the k th column of M (which corresponds to the k th vertex of the support) to be a convex combination of the k th vertex of the standard probability simplex and the empirical mean pmf. We provide code that finds the d optimal such convex combinations such that a specificed percentage of the sample pmfs are within the support, which reduces the non-convex problem of finding the maximum likelihood d  X  d matrix M to a d -dimensional convex relaxation. It is reasonable to believe that if the shadow Dirichlet better matches the problem X  X  statistics, it will perform better in practice, but an open question is how much better? To motivate the reader to investigate this question further in applications, we provide two small demonstrations. 6.1 Verifying the EM Estimation We used a broad suite of simulations to test and verify the EM estimation. Here we include a simple visual confirmation that the EM estimation works: we drew 100 i.i.d. pmfs from a shadow Dirichlet with monotonic M for d = 3 and  X  = [3 . 94 2 . 25 2 . 81] (used in [18]). From each of the 100 pmfs, we drew 100 i.i.d. samples. Then we applied the EM algorithm to find the  X  for both the standard compound Dirichlet, and the compound shadow Dirichlet with the correct M . Fig. 2 shows the true distribution and the two estimated distributions. Figure 2: Samples were drawn from the true distribution and the given EM method was applied to form the estimated distributions. 6.2 Estimating Proportions from Sales Manufacturers often have constrained manufacturing resources, such as equipment, inventory of raw materials, and employee time, with which to produce multiple products. The manufacturer must decide how to proportionally allocate such constrained resources across their product line based on their estimate of proportional sales. Manufacturer Artifact Puzzles gave us their past retail sales data for the 20 puzzles they sold during July 2009 through Dec 2009, which we used to predict the proportion of sales expected for each puzzle. These estimates were then tested on the next five months of sales data, for January 2010 through April 2010. The company also provided a similarity between puzzles S , where S ( A,B ) is the proportion of times an order during the six training months included both puzzle A and B if it included puzzle A. We compared treating each of the six training months of sales data as a sample from a compound Dirichlet versus or a compound shadow Dirichlet. For the shadow Dirichlet, we normalized each column of the similarity matrix S to sum to one so that it was left-stochastic, and used that as the M matrix; this forces puzzles that are often bought together to have closer estimated proportions. We estimated each  X  parameter by EM to maximize the likelihood of the past sales data, and then estimated the future sales proportions to be the mean six months of sales data as coming from one multinomial which we estimated as the maximum likelihood multinomial, and to taking the mean of the six empirical pmfs.
 In this paper we have proposed a variant of the Dirichlet distribution that naturally captures some of the dependent structure that arises often in machine learning applications. We have discussed some of its theoretical properties, and shown how to specify the distribution for regularized pmfs, bounded variation pmfs, monotonic pmfs, and for any desired convex polytopal domain. We have derived the EM method and made available code to estimate both the shadow Dirichlet and compound shadow Dirichlet from data. Experimental results demonstrate that the EM method can estimate the shadow Dirichlet effectively, and that the shadow Dirichlet may provide worthwhile advantages in practice. [1] B. Frigyik, A. Kapila, and M. R. Gupta,  X  X ntroduction to the Dirichlet distribution and related [2] C. Zhai and J. Lafferty,  X  X  study of smoothing methods for language models applied to infor-[3] Y. Chen, E. K. Garcia, M. R. Gupta, A. Rahimi, and L. Cazzanti,  X  X imilarity-based classifica-[4] R. Nallapati, T. Minka, and S. Robertson,  X  X he smoothed-Dirichlet distribution: a building [5] Aitchison, Statistical Analysis of Compositional Data , Chapman Hall, New York, 1986. [6] R. J. Connor and J. E. Mosiman,  X  X oncepts of independence for proportions with a general-[7] K. Pearson,  X  X athematical contributions to the theory of evolution X  X n a form of spurious [8] A. Ongaro, S. Migliorati, and G. S. Monti,  X  X  new distribution on the simplex containing the [9] W. S. Rayens and C. Srinivasan,  X  X ependence properties of generalized Liouville distributions [10] Walter Rudin, Functional Analysis , McGraw-Hill, New York, 1991. [11] G. Ronning,  X  X aximum likelihood estimation of Dirichlet distributions, X  Journal of Statistical [12] T. Minka,  X  X stimating a Dirichlet distribution, X  Tech. Rep., Microsoft Research, Cambridge, [13] A. P. Dempster, N. M. Laird, and D. B. Rubin,  X  X aximum likelihood from incomplete data [14] M. R. Gupta and Y. Chen, Theory and Use of the EM Method , Foundations and Trends in [15] R. T. Rockafellar, Convex Analysis , Princeton University Press, Princeton, NJ, 1970. [16] S. Boyd and L. Vandenberghe, Convex Optimization , Cambridge University Press, Cambridge, [17] K. B. Petersen and M. S. Pedersen, Matrix Cookbook , 2009, Available at matrixcookbook.com. [18] R. E. Madsen, D. Kauchak, and C. Elkan,  X  X odeling word burstiness using the Dirichlet
