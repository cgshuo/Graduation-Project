 sent sensory information, subject to limitations of noise a nd resources (e.g., number of neurons, metabolic cost, wiring length). It is difficult to test this c oncept because optimization of any formu-substantial literature has considered population models i n which each neuron X  X  mean response to a scalar variable is characterized by a tuning curve [e.g., 1 X  6]. For these simplified models, several papers have examined the optimization of Fisher informatio n, as a bound on mean squared error [7 X 10]. In these results, the distribution of sensory varia bles is assumed to be uniform and the pop-ulations are assumed to be homogeneous with regard to tuning curve shape, spacing, and amplitude. The distribution of sensory variables encountered in the en vironment is often non-uniform, and it is thus of interest to understand how variations in probabilit y affect the design of optimal populations. It would seem natural that a neural system should devote more resources to regions of sensory space that occur with higher probability, analogous to results in coding theory [11]. At the single neuron level, several publications describe solutions in which mo notonic neural response functions allocate greater dynamic range to higher probability stimuli [12 X 15 ]. At the population level, non-uniform allocations of neurons with identical tuning curves have be en shown to be optimal for non-uniform stimulus distributions [16, 17].
 Here, we examine the influence of a sensory prior on the optima l allocation of neurons and spikes in a population, and the implications of this optimal alloca tion for subsequent perception. Given a prior distribution over a scalar stimulus parameter, and a resource budget of N neurons with an average of R spikes/sec for the entire population, we seek the optimal sh apes, positions, and am-plitudes of tuning curves. We assume a population with indep endent Poisson spiking, and consider a family of objective functions based on Fisher information . We then approximate the Fisher in-formation in terms of two continuous resource variables, th e density and gain of the tuning curves. This approximation allows us to obtain a closed form solutio n for the optimal population. For all objective functions, we find that the optimal tuning curve pr operties (cell density, tuning width, and gain) are power-law functions of the stimulus prior, with ex ponents dependent on the specific choice of objective function. Through the Fisher information, we a lso derive a bound on perceptual dis-criminability, again in the form a power-law of the stimulus prior. Thus, our framework provides direct and experimentally testable links between sensory p riors, properties of the neural representa-tion, and perceptual discriminability. We provide prelimi nary evidence that these relationships are supported by experimental data. We assume a conventional model for a population of N neurons responding to a single scalar vari-able, s [1 X 6]. The number of spikes emitted (per unit time) by the n th neuron is a sample from an independent Poisson process, with mean rate determined b y its tuning function, h probability density of the population response can be writt en as We also assume the total expected spike rate, R , of the population is fixed, which places a constraint on the tuning curves: prior, in anticipation of its future use in Bayesian decodin g of the population response. We now ask: what is the best way to represent values drawn from p ( s ) given the limited resources of
N neurons and R total spikes? To formulate a family of objective functions w hich depend on both p ( s ) , and the tuning curves, we first rely on Fisher information, I a function of the tuning curves [1, 18]: The Fisher information can be used to express lower bounds on mutual information [16], the vari-ance of an unbiased estimator [18], and perceptual discrimi nability [19]. Specifically, the mutual information, I ( ~r ; s ) , is bounded by: where H ( s ) is the entropy, or amount of information inherent in p ( s ) , which is independent of the neural population. The Cramer-Rao inequality allows us to e xpress the minimum expected squared The constant  X  determines the performance level at threshold in a discrimi nation task. We formulate a generalized objective function that include s the Fisher bounds on information and discriminability as special cases: where f (  X  ) is either the natural logarithm, or a power function. When f ( x ) = log( x ) , optimizing Eq. (4) is equivalent to maximizing the lower bound on mutual information given in Eq. (2). We refer to this as the infomax objective function. Otherwise, we assume f ( x ) = x  X  , for some exponent  X  . Optimizing Eq. (4) with  X  =  X  1 is equivalent to minimizing the squared discriminability b ound expressed in Eq. (3). We refer to this as the discrimax objective function. The objective function expressed in Eq. (4) is difficult to op timize because it is non-convex. To make the problem tractable, we first introduce a parametriza tion of the population in terms of cell density and gain. The cell density controls both the spacing and width of the tuning curves, and the gain controls their maximum average firing rates. Second, we show that Fisher information can be closely approximated as a continuous function of density an d gain. Finally, re-writing the objective function and constraints in these terms allows us to obtain c losed-form solutions for the optimal tuning curves. 4.1 Density and gain for a homogeneous population If p ( s ) is uniform, then by symmetry, the Fisher information for an o ptimal neural population should also be uniform. We assume a convolutional population of tun ing curves, evenly spaced on the unit lattice, such that they approximately  X  X ile X  the space: We also assume that this population has an approximately con stant Fisher information: That is, we assume that the Fisher information curves for the individual neurons,  X  ( s  X  n ) , also tile the stimulus space. The value of the constant, I curve shape, h ( s ) , which we leave unspecified. As an example, Fig. 1(a-b) shows that the Fisher information for a convolutional population of Gaussian tun ing curves, with appropriate width, is approximately constant.
 Now we introduce two scalar values, a gain ( g ), and a density ( d ), that affect the convolutional population as follows: The gain modulates the maximum average firing rate of each neu ron in the population. The density controls both the spacing and width of the tuning curves: as t he density increases, the tuning curves become narrower, and are spaced closer together so as to main tain their tiling of stimulus space. The effect of these two parameters on Fisher information is: The second line follows from the assumption of Eq. (5), that t he Fisher information of the convolu-tional population is approximately constant with respect t o s .
 The total resources, N and R , naturally constrain d and g , respectively. If the original (unit-spacing) convolutional population is supported on the interval (0 , Q ) of the stimulus space, then the number of neurons in the modulated population must be N ( d ) = Qd to cover the same interval. Under the assumption that the tuning curves tile the stimulus spac e, Eq. (1) implies that R = g for the modulated population. 4.2 Density and gain for a heterogeneous population Intuitively, if p ( s ) is non-uniform, the optimal Fisher information should also be non-uniform. This can be achieved through inhomogeneities in either the tunin g curve density or gain. We thus gener-the convolutional population:
Density (Tuning width)  X  1 d ( s ) Np ( s ) Np 1 2 ( s ) Np  X   X  1 3  X 
Fisher information I
Discriminability bound  X  Here, D ( s ) = R s curve. The value s curve (Fig. 1(b-d)). Note that the warped population retain s the tiling properties of the original convolutional population. As in the uniform case, the densi ty controls both the spacing and width of the tuning curves. This can be seen by rewriting Eq. (7) as a first-order Taylor expansion of D ( s ) around s which is a generalization of Eq. (6).
 We can now write the Fisher information of the heterogeneous population of neurons in Eq. (7) as In addition to assuming that the Fisher information is appro ximately constant (Eq. (5)), we have approximate g ( s Fisher information in terms of the continuous parameteriza tion of cell density and gain. As earlier, the constant I As in the homogeneous case, the global resource values N and R will place constraints on d ( s ) fact that the warped tuning curves sum to unity (before multi plication by the gain function) and use Eq. (1) to obtain the constraint R p ( s ) g ( s ) d s = R . 4.3 Objective function and solution for a heterogeneous pop ulation Approximating Fisher information as proportional to squar ed density and gain allows us to re-write the objective function and resource constraints of Eq. (4) a s A closed-form optimum of this objective function is easily d etermined by taking the gradient of the Lagrangian, setting to zero, and solving the resulting syst em of equations. Solutions are provided in Table 1 for the infomax, discrimax, and general power cases.
 In all cases, the solution specifies a power-law relationshi p between the prior, and the density and gain of the tuning curves. In general, all solutions allocat e more neurons, with correspondingly narrower tuning curves, to higher-probability stimuli. In particular, the infomax solution allocates an approximately equal amount of probability mass to each ne uron. The shape of the optimal gain function depends on the objective function: for  X  &lt; 0 , neurons with lower firing rates are used to represent stimuli with higher probabilities, and for  X  &gt; 0 , neurons with higher firing rates are predicted relationships on experimental data. In addition to power-law relationships between tuning properties and sensory priors, our formulation offers a dir ect relationship between the sensory prior and perceptual discriminability. This can be obtained by su bstituting the optimal solutions for d ( s )  X  ( s )  X   X  min ( s )  X   X  / p I f ( s ) [19]. The resulting expressions are provided in Table 1. Our framework predicts a quantitative link between the sens ory prior, physiological parameters (the density, tuning widths, and gain of cells), and psychophysi cally measured discrimination thresholds. We obtained subsets of these quantities for two visual stimu lus variables, orientation and spatial frequency, both of believed to be encoded by cells in primary visual cortex (area V1). For each variable, we use the infomax and discrimax solutions to conv ert the physiological and perceptual measurements, using the appropriate exponents from Table 1 , into predictions of the stimulus prior  X  p ( s ) . We then compare these predictions to the empirically measu red prior p ( s ) . 5.1 Orientation We estimated the prior distribution of orientations in the e nvironment by averaging orientation statis-tics across three natural image databases. Two databases co nsist entirely of natural scenes [20, 21], and the third contains natural and manmade scenes [22]. Orie ntation statistics depend on scale, so we measured statistics at a scale matching the psychophysic al experiment from which we obtained perceptual data. The average distribution of orientations exhibits higher probability at the cardinal orientations (vertical and horizontal) than at the oblique orientations (Fig. 2(a)). Measurements of cell density for a population of 79 orientation-tuned V1 cells in Macaque [23] show more cells t uned to the cardinal orientations than the oblique orientations (Fig. 2(b)). Finally, perceptual discrimi-nation thresholds, averaged across four human subjects [24 ] show a similar bias (Fig. 2(c)), with humans better able to discriminate orientations near the ca rdinal directions.
 All of the orientation data exhibit similar biases, but our t heory makes precise and testable predic-tions about these relationships. If a neural population is d esigned to maximize information, then the cell density and inverse discrimination thresholds should match the stimulus prior, as expressed in infomax column of Table 1. We normalize these predictions to integrate to one (since the theory provides only the shapes of the functions, up to unknown valu es of the resource variables N and R ), and plot them against the measured prior (Fig. 2(d)). We se e that the predictions arising from cell density and discrimination thresholds are consistent with one another, and both are consistent with the stimulus prior. This is especially remarkable give n that the measurements come from very different domains (in the case of the perceptual and physiol ogical data, different species). For the discrimax objective function, the exponents in the power-l aw relationships (expressed in Table 1) are too small, resulting in poor qualitative agreement betw een the stimulus prior and predictions from the physiology and perception (Fig. 2(e)). For example , predicting the prior from perceptual data, under the discrimax objective function, requires exp onentiating discrimination thresholds to the fourth power, resulting in an over exaggeration of the ca rdinal bias. 5.2 Spatial frequency We obtained a prior distribution over spatial frequencies a veraged across two natural image databases [20, 21]. For each image, we computed the magnitud e spectrum, and averaged over ori-entation. We averaged these across images, and fit the result with a power law of exponent  X  1 . 3 (Fig. 3(a)). We also obtained spatial frequency tuning prop erties for a population of 317 V1 cells [25]. On average, we see there are more cells, with correspon dingly narrower tuning widths, tuned to low spatial frequencies (Fig. 3(b)). These data support t he model assumption that tuning width is inversely proportional to cell density. We also obtained average discrimination thresholds for si-nusoidal gratings of different spatial frequencies from tw o studies (Fig. 3(c)). The gratings were shown at 10% contrast to 3 human subjects for one study [26], and 25% contrast for 7  X  13 human subjects for the other [27]. The thresholds show that, on ave rage, humans are better at discriminating low spatial frequencies.
 We again test the infomax and discrimax solutions by compari ng predicted distributions obtained from the physiological and perceptual data, to the measured prior. We normalize each prediction to integrate to the corresponding area under the prior. The i nfomax case shows striking agreement between the measured stimulus prior, and predictions based on the physiological and perceptual measurements (Fig. 3(d)). However, as in the orientation ca se, discrimax predictions are poor (Fig. 3(e)), suggesting that information maximization provides a better optimality principle for explaining the neural and perceptual encoding of spatial frequency tha n discrimination maximization. We have examined the influence sensory priors on the optimal a llocation of neural resources, as well as the influence of these optimized resources on subsequ ent perception. For a family of ob-jective functions, we obtain closed-form solutions specif ying power law relationships between the probability distribution of a sensory variable encountere d in the environment, the tuning properties of a population that encodes that variable, and the minimum p erceptual discrimination thresholds achievable for that variable. We X  X e shown preliminary supp ortive evidence for these relationships for two different perceptual attributes.
 Our analysis requires several approximations and assumpti ons in order to arrive at an analytical solution. We first rely on lower bounds on mutual information and discriminability based on Fisher information. Fisher information is known to provide a poor b ound on mutual information when there are a small number of neurons, a short decoding time, or non-smooth tuning curves [16, 29]. It also provides a poor bound on supra-threshold discrimina bility [30, 31]. But note that we do not require the bounds on either information or discriminabili ty to be tight, but rather that their optima be close to that of their corresponding true objective funct ions. We also made several assumptions in g ( s ) , varies slowly and smoothly over the width of  X  ( D ( s )  X  n ) . These assumptions allow us to approximate Fisher information in terms of cell density and gain (Fig. 1(e)), to express the resource constraints in simple form, and to obtain a closed-form solu tion to the optimization problem. Our framework offers an important generalization of the pop ulation coding literature, allowing for non-uniformity of sensory priors, and corresponding heter ogeneity in tuning and gain properties. Nevertheless, it suffers from many of the same simplificatio ns found in previous literature. First, neural spike trains are not Poisson, and they are (at least in some cases) correlated [32]. Second, tuning curve encoding models only specify neural responses to single stimulus values. The model should be generalized to handle arbitrary combinations of s timuli. And third, the response model should be generalized to handle multi-dimensional sensory inputs. Each of these limitations offers an important opportunity for future work.
 Finally, our encoding model has direct implications for Bay esian decoding, a problem that has re-ceived much attention in recent literature [e.g., 5, 6, 33 X 3 5]. A Bayesian decoder must have knowl-edge of prior probabilities, but it is unclear how such knowl edge is obtained or represented in the brain [34]. Previous studies assume that prior probabiliti es are either uniform [6], represented in the spiking activity of a separate population of neurons [5] , or represented (in sample form) in the spontaneous activity [35]. Our encoding formulation provi des a mechanism whereby the prior is im-plicitly encoded in the density and gains of tuning curves, w hich presumably arise from the strength of synaptic connections. We are currently exploring the req uirements for a decoder that can correctly utilize this form of embedded prior information to obtain Ba yesian estimates of stimulus variables. References
