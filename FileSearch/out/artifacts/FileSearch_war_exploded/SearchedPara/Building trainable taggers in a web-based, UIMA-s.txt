 The applications of automatic recognition of cate-gories, or tagging, in natural language processing (NLP), range from part of speech tagging to chunk-ing to named entity recognition and complex scien-tific discourse analyses. Currently, there is a variety of tools capable of performing these tasks. A com-monly used approach involves the use of machine learning to first build a statistical model based on a manually or semi-automatically tagged sample data and then to tag new data using this model. Since the machine learning algorithms for building mod-els are well established, the challenge shifted to fea-ture engineering , i.e., developing task-specific fea-tures that form the basis of these statistical models. This task is usually accomplished programmatically which pose an obstacle to a non-technically inclined audience. We alleviate this problem by demonstrat-ing Argo 1 , a web-based platform that allows the user to build NLP and other text analysis workflows via a graphical user interface (GUI) available in a web browser. The system is equipped with an ever grow-ing library of text processing components ranging from low-level syntactic analysers to semantic an-notators. It also allows for including user-interactive components, such as an annotation editor, into oth-erwise fully automatic workflows. The interoper-ability of processing components is ensured in Argo by adopting Unstructured Information Management Architecture (UIMA) (Ferrucci and Lally, 2004) as the system X  X  framework. In this work we explore the capabilities of this framework to support machine Figure 1: Screen capture of Argo X  X  web-based inter-face. Argo X  X  main user interface consists of three panels as shown in Figure 1. The left-hand panel includes user-owned or shared storable objects; the middle panel is a drawing space for constructing workflows and the right-hand panel displays context-dependent information. The storable objects are categorised into workflows , represented as block diagrams of interconnected processing components, documents that represent the user X  X  space intended for upload-ing resources and saving processing results, and ex-ecutions that provide past and live workflow exe-cution details and access points to user-interactive components should such be present in a workflow. Component interoperability in Argo is ensured by UIMA which defines common structures and inter-faces. A typical UIMA processing pipeline consists of a collection reader , a set of analysis engines and a consumer . The role of a collection reader is to fetch a resource (e.g., a text document) and deposit it in a common annotation structure , or CAS , as the sub-ject of annotation. Analysis engines then process the subject of annotation stored in the CAS and populate the CAS with their respective annotations. The con-sumer X  X  role is to transform some or all of the an-notations and/or the subject of annotation from the CAS and serialise it into some storable format.
Readers, analysers and consumers are represented graphically in Argo as blocks with incoming only, incoming and outgoing, and outgoing only ports, re-spectively, visible in the middle of Figure 1. sented as the Feature Generator, CRF++ Trainer and CRF++ Tagger blocks. Figure 2a shows a pro-cess of building a statistical model supported by a document reader, common, well-established pre-processing components (in this case, to establish boundaries of sentences and tokens), and the previ-ously mentioned editor for manually creating anno-tations 6 . The manual annotations serve to generate tags/labels which are used in the training process to-gether with the features produced by Feature Gener-ator. The trained model is then used in the workflow shown in Figure 2b to tag new resources. Although the tagging workflow automatically recognises the labels of interest (based on the model supplied in CRF++ Tagger), in practice, the labels need further correction, hence the use of Annotation Editor after the tagger. 4.1 Training and tagging At present, our implementation of the training and tagging components is based on the conditional ran-dom fields (CRF) (Lafferty et al., 2001). Our choice is dictated by the fact that CRF models are currently one of the best models for tagging and efficient algo-rithms to compute marginal probabilities and n -best sequences are freely available.

We used the CRF++ implementation 7 and wrapped it into two UIMA-compatible components, CRF++ Trainer and CRF++ Tagger. The trainer deals with the optimisation of feature parameters, whereas word observations are produced by Feature Generator, as described in the following section. 4.2 From annotations to features The Feature Generator component is an intermedi-ary between annotations stored in CASes and the training component. This component is customis-able via the component X  X  settings panel, parts of which are shown in Figure 3. The panel allows the user to 1) identify the stream of tokens 8 (Figure 3a), 2) identify the stream of token sequences (usually
Figure 4: UML diagram of transformation types resentation of the token field X  X  value ultimately be-comes the value of the generated feature. If the user declares one or more transformations then these are applied on the token field X  X  value in sequence, i.e., an outcome of the preceding transformation be-comes an input of the following one. Figure 4 shows the various transformations currently available in the system.

Context windows allow for enriching the current token X  X  feature set by introducing observations from surrounding tokens as n-grams. For example, the selected feature definition in Figure 3b,  X  X urface has symbols X , declares the covered text as the feature X  X  basis and defines two transformations and two con-text windows. The two transformations will first transform the covered text to a collapsed shape (e.g.,  X  X F-kappa X  will become  X  X #a X ) and then produce  X  X  X  or  X  X  X  depending on whether the collapsed shape matches the simple regular expression  X # X  (e.g.,  X  X #a X  will become  X  X  X ). The two context win-dows define six unigrams and four bigrams, which will ultimately result in this single feature defini-tion X  X  producing ten observations for training. We show the performance of taggers trained with two distinct sets of features, basic and extended. The basic set of features uses token fields such as the covered text and the part of speech without any transformations or context n-grams. The extended set makes the full use of Feature Generator X  X  settings and enriches the basic set with various transforma-tions and context n-grams. The transformations in-no other information supporting the tokens in the BioNLP/NLPBA dataset. To compensate for it we automatically generated part of speech and chunk la-bels for each token.

The chosen datasets/tasks are by no means an exhaustive set of representative comparative-setup datasets available. Our goal is not to claim the su-periority of our approach over the solutions reported in the respective shared tasks. Instead, we aim to show that our generic setup is comparable to those task-tuned solutions.
 We further explore the options of both Feature Generator and CRF++ Trainer by manipulating la-belling formats (IOB vs IOBES (Kudo and Mat-sumoto, 2001)) for the former and parameter esti-mation algorithms (L 2 -vs L 1 -norm regularisation) for the latter. Ultimately, there are 32 setups as the result of the combinations of the two feature sets, the two datasets, the two labelling formats and the two estimation algorithms. 5.1 Results Table 1 shows the precision, recall and f-scores of our extended-feature setups against each other as well as with reference to the best and baseline solu-tions as reported in the respective shared tasks. The gap to the best performing solution for the chunking task is about 1.3% points in F-score, ahead of the baseline by 15.7% points. Respectively for the NER task, our best setup stands behind the best reported solution by about 7% points, ahead of the baseline by about 18% points. In both instances our solution would be placed in the middle of the reported rank-ings, which is a promising result, especially that our setups are based solely on the tokens X  surface form, part of speech, and (in the case of the NER task) chunk. In contrast, the best solutions for the NER task involve the use of dictionaries and advanced analyses such as acronym resolution.

The tested combinations of the labelling formats and parameter estimation algorithms showed to be inconclusive, with a difference between the best and worst setups of only 0.35% points for both tasks.
The advantage of using the extended set of fea-tures over the basic set is clearly illustrated in Table 2. The performance of the basic set on the chunking dataset is only at the level of the baseline, whereas for the NER task it falls nearly 6% points behind the SRC BB/G53025X/1 From Text to Pathways) and Korea Institute of Science and Technology Informa-tion (KISTI Text Mining and Pathways).

