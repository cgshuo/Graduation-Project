
It is a tough problem in the field of education when grading short answer because of the subjectivity of graders and the complexity of assessment standard [ 1 , 32 ] . Particu-larly, with the increase of computer -based testing , automatic grading has arou sed more critical discussions. It has provided us with more good conditions to apply automatic grading for reducing the efforts of teachers. For example, in Chinese college entrance examination , short answer appeared continually in the reading part. It is obviously that hiring a mass of graders is not a cost -efficient way. However, the research in automatic Chinese short answer grading is incomplete. Therefore, it is very essential to develop more automatic grading method s for Chinese short answer. 
In the past decade, the automatic grading has been widely studied [ 1 , 32 ] . However, the traditional automatic short answer grading methods are limited. Most of these mod-els h ad neglected the context meaning and represented the words in high -dimensional and high -sparse vectors [ 1 , 11 ] . T hese met hods could be divided into feature engineer-ing and automatic grading machine. We need to pre -process student answers and ex-tracted answer features so that the answers could be represented to the form which could be computed. E specially in automatic Chinese short answer grading algorithms, due to most of the traditional automatic sho rt answer grading methods ignored the words order information, they may lose much  X  X  -gram X  information.

The goal of answer represen ta tion is to change the pre -processed data in to the cal-culable data which is a significant part may affect the quality of the automatic grading methods [ 11 ] . The traditional methods including the bag -of -words model and the vector space model have that neglected the contextual information in student answers [ 13 , 14 ] . In these models, every word is independent and the model could not represent semantic information. There are some sentences and words representation methods based on se-mantic as well. These methods were regarded as the deep representation of words [ 15 ] . In our approach, we used words sequence tag s to help get the deep semantic which could be applied on deep sequence model.

Zhang et al. discovered that deep learning is a go od choice for automatic grading [ 11 ]. Therefore, we tried to apply another deep learning model(LSTM) [ 16 ] to auto-matic short answer grading and take the order of words in answers into consideration. For some short answer grading, such as the language translation problem s , it is signif-icant for automatic grading method to consider the order of words student a nswer s . In these translation problem, most of student answers may be highly similar in the usage of words while the grammatical structure represented by words order may be an im-portant grading standard for graders. Therefore, the LSTM can grade these probl ems easily and output the score of each answer.

In this paper, we a pplied LSTM [ 16 ] on automatic Chinese short answer grading after Chinese words segmentation and serialized these Chinese phrases . We validated our model on two datasets for the sake of assessing the robust and the generalization ability of our model. One is BNU Chinese short answer dataset including 533 Chinese answers which generated by us and the other one is well -known Kaggle short answer dataset [ 22 ] , and our approach performed better than the bag -of -words baseline in Chi-nese dataset. The performances of our approach in ten Kaggle short answer sub -dataset s has showed that this method can also be used for English an swers and different grading level s. The se results demonstrated that our method is not specific to a single language or a special sort of short answer, and could be used for building grading models.
In the end, after analyzing the results in two datasets , w e wanted to talk about some way may improve the performances of deep sequence models. A variety of deep se-quence models could be applied to automatic short answer grading and the combination among models may help to improve the grading results of deep sequ ence models . Fur-thermore , new methods to represent student answers would play a significant role in optimizing short answer automatic grading methods even decide the quality of grading .
A utomated grading short natural language answer or studen t response has been ex-tensively studied for a long time. Burrows et al. summed up that short answer question has the following five specific criteria [1] :  X  require students to recall external knowledge rather than recognize from within  X  the answer must be given in natural language;  X  the answer length ranges between one phrase to one paragraph;  X  the grading of student response should focus on con tent as opposed to writing  X  close -ended. 
Obviously, automatic short answer grading (ASAG) require the computer can un-derstand of the natural language. Therefore, ASAG is always considered much more difficult than multiple -choice question and fill -t he -gap question which has a fixed cor-rect response to each question.

Natural Language Processing (NLP) and Machine learning techniques are frequently employed by ASAG. NLP techniques are used to extract various features from model answer and student answer . T hese features are applied to measure the similarity be-tween model and student answer s. F or example, Content Assessment Module (CAM ) types of overlap include word unigrams and trigrams, noun -phrase chunks, text simi-larity thresholds etc. Madnani et. al in [3] used eight features based on the rubric (BLEU, ROUGE, length, coherence etc.) for summary assessment. Dzikovska et. al extract four features, i.e., the raw number of overlapping words, F1, Lesk and cosine score [4] . 
After feature extraction processing, th ese extracted features are used to train various classification, regression or clustering model for grading new student answers automat-ically. Different machine learning models are utilized in the ASAG task. Such as, Lo-gistic regression [3], Decision Tree [5,6], k -Nearest Neighbor [2,7], Na X ve Bayes [8], Support Vector Machine ( SVM ) [9], LDA clustering [10]. Specially, Zhang et al. in-troduced Deep Belief Network (DBN) into the task, which is the first study to apply deep neural network to ASAG as far as we know [11]. The results of this work shows that DBN is the best comparing with other five machine learning classifier s . 
The student answers are in E nglish in the these work . Hence automatic Chinese an-swer grading is another challenging problem .  X  Firstly, different methods for Chinese text segmentation may lead to different  X  Secondly, the grammatical difference between Chinese and English could give 
There are some scholars had take n noticed of this problem and go t ten down to auto-matic Chinese short answer grading. Wang et al. in [12] compare three methods for grading earth science questions in Chinese, which are based on concept m apping, ma-chine learning, or both.

In consideration of the superiority of deep learning, especially LSTM [ 16 ] , in text classification as well as the few research results in automatic Chinese short answer grading, we tried to apply LSTM to automatic Chinese short answer grading witho ut extracting features manually and validated our approached in Chinese answers to ana-lyze more possibility in automatic short answer grading by deep sequence models. And the methods for Chinese answers may be more integrated. 3.1 Model Overview
Due to the importance of considering word order, we app lied LSTM on short answer automatic grading. We used jieba [ 18 ] for Chinese answer segmentation and encoded the student answer to a series of sequence by representing Chinese words with different simple numbers . Therefore, the model can learn the order information for each answer. In this way, the information of each answer could be represented by a number sequence and could be trained by long -short term memory neural network . 3.2 Chinese Answer Segmentation and Serialization As we can s ee in the Fig.1 , Chinese text segmentation is the first step of automatic Chinese short answer grading. T here would be lots of excellent students using advanced idioms and other import phrases. What X  X  more, the teacher may take the usage of ad-vanced phrase s as the measurement for the ability to language using. Hence it is im-portant for the automatic Chinese short answer grading method to take a good Chinese text segmentation method. For Chinese short answer question, we had to cut words in each answer for every sentence by the tool of Chinese Words Segmentation . I n the meantime, a vocabulary was built and it would be used for encoding. In this w ay, we could represent the word by a number and changed the student answer to a sequence .
After re present ed each word in to a number, we could translate the student answer into a list of number sequence. In the previous step of data pre -processing, we had e n-coded each word into a number. Therefore, we could map each word in each sentence to a proper number. What X  X  mor e, the data pre -processing has calculated the maximum length of all student answers and the serialization of each student answer should have the same length. The header of an answer will be encoded to zero if the total number of phrases in a Chinese studen t answer is smaller than the maximum length. We can take a simple old Chinese translation question for example, as we can see in the Fig.2 , the student answer was encoded by a series of numbers and the sequence was formatted to the same length so that the LSTM can be applied.
 3.3 Learning LSTM for Automatic Grading R NN and LSTM
Some Chinese answer assessment standards rely on grammatical structure. Using a network can deal with sequences and discover long -term dependencies [ 17 ] . With the ability of explicitly modeling tim e -series data, RNNs are being increasingly applied to sentence modeling. At every timestep an input is fed to the RNN together with the previous internal memory [ 16 , 17 ] . The RNN then perform some computation that re-which grade level the student answer matched. Long short -term memory net-work(LSTM) was proposed by Hochreiter and Schmidhu ber [ 16 ] . The LSTM contains special units called memory blocks in the recurrent hidden layer. The memory blocks contain memory cells with self -connections storing the temporal state of the network in addition to special multiplicative units called gates to control the information flow. Every block in the architecture contained an input grate and an output gate. The input gate controls the flow of input activations i nto memory cell. The output gate controls the output flow of cell activations into the rest of the network [ 16 ] .
 Embedding .

After serializing each Chinese student answer into a number sequence, we used em-bedding layer [ 19 ] as the first layer of our model to turn integers into dense vectors of fixed size so that LSTM can learn on it.
 LSTM for Grading .

In this part , we used single directional LSTM [ 16 ] to learn the seque nce inf ormation of each answer vector , for a Chinese answer sequence from left to right, LSTM l earned the previous answers and make it a n input to the next unit so that the sequence infor-mation can be passed from left to right. A nd the output of LSTM was send to the full connected layers. For regularization, we applied dropout to prevent co -adaptation [ 20 ] . At last, we used the softmax activation function to grade a score [ 21 ] . 4.1 Data sets W e analyzed o ur model on two datasets to validate our model. The first one is a Chinese short answer dataset which generated by us in Beijing Normal University. This dataset including 533 Chinese short answers and the maximum length of answer is 78. We compared our approach with five baseline methods which are all bag -of -words models. And the results had showed that our approach was better than the bag -of -words models in automatic Chinese short answer grading. The other one is a popular dataset in short answer automatic grading provided by Kaggle competition [ 22 ] . This dataset contained 10 subsets which could test the robust of model in different question. W e tested our model on the public leaderboard data provided by this competition. We cal-culated Q uadratic Kappa in ten subsets which are 3 -4 grading levels , the r esults have showed that our approach can also be applied on automatic English short answer g r ad-ing. 4.2 Baseline Methods
W e compare our approach with a variety of baselines which used the feature ex-tracted through bag -of -words:  X  Logistic Regression(LR) [ 23 ] , one of the most widely used automatic grading  X  Support Vector Machine(SVM) [ 24 , 30 ] , an excellent automatic grading methods .  X  Na X ve Bayes(Bayes) [ 25 , 31 ], a traditional automatic grading machine and text  X  Random Forest(RF) [ 26 ], an optimize d bagging algorithm in ensemble methods.  X  Gradient Boosting Decision Tree(GBDT) [ 27 ], another high -performance ensem-4.3 BNU Short Answer Dataset
For validati ng our approach on Chinese short answer, we generated a dataset con-tained 533 student answers. In this dataset, we used 5 -folds -cross -validation to measure the effect of models. And we made a comparison between bag -of -words model and our approach .

We calculated accuracy and not weighted Kappa in 6 models including two ensem-ble methods and our approach. Our model(LSTM) had the highest Kappa in this dataset, nearly attained 0.75. Howeve r, our model weaker than logistic regression, SVM and gradient boosting tree in accuracy. Nevertheless, our model had higher accuracy value than Na X ve Bayes and Random Forest. This result indicated that our approach can per-form better than bag -of -words mod els in automatic Chinese short answer grading. 4.4 Kaggle Short Answer Dataset tools that would help grade answers comparable to humans [ 22 ] . We want to validate our approach on this dataset to know whether our approach could work on English short answer datasets or not. The public leaderboard dataset including training set and testing automatic grading tools. In this case, we c ompared our model with baseline methods in these ten subsets included in  X  X rain_rel_2.tsv X  and tested it on  X  X ublic_leader-board_rel2.tsv X  . Then we calculated the QW Kappa betwe en model results and the solution in  X  X ublic_leaderboard_solution.csv X  . A s we can see in Fig.4 t he quality of dataset may affect the performance of model s . As we can see in the graph, our model was the best one in dataset 1,2 and 9. In addition, the results in other datasets are c on-tiguous with baseline methods. Therefore, our model can also be applied t o automatic English short answer grading.

To draw a conclusion, after the validation on two datasets, our model performed better than bag -of -words model s in automatic Chinese short answer grading. Without extracting answer feature manually, our approach grading Chinese student answers well. Not on ly can reduce the efforts of teachers but also can speed up the process of constructing short answer automatic grading systems without manual feature. What X  X  more, the application of LSTM in Chinese automatic short answer grading has showed that the validity of deep learning in Chinese ASAG . More importantly, our method can also work well in English answers and even was the best one in some datasets.
However, there are still some ways to improve our approaches:  X  We only used a single LSTM in our approach, ensemble more LSTMs or the  X  In our method, we just encoded each word into a simple number and trained these  X  For Chinese ASAG, the better Chinese segmentation tools are used, the better 
