 HIDEKI MIMA School of Engineering, University of Tokyo SOPHIA ANANIADOU School of Informatics, University of Manchester and KATSUMORI MATSUSHIMA School of Engineering, University of Tokyo ________________________________________________________________________ ________________________________________________________________________ these discoveries, and are created to share new knowledge with other scientists. However, such a large volume of published documents 1 makes it difficult for a person to efficiently single document. The growing number of electronically available knowledge sources (KSs) emphasizes the importance of developing flexible and efficient tools for automatic knowledge acquisition and structuring in order to integrate knowledge. knowledge discovery from large textual collections. The primary goal of text mining is to retrieve knowledge that is  X  X uried X  in text and to present the distilled knowledge to users in a concise form. As compared to  X  X anual X  knowledge discovery, the advantage of this technique is the assumption that automati c methods will enable the processing of enormous amounts of text. It is impossible for any researcher to process such huge amounts of information, particularly when the knowledge spans domains. Text-mining enables scientists to efficiently and systematically collect, maintain, interpret, curate, and discover knowledge for research or education. dynamic nature. Even when confined to a single domain, the KSs are developed autonomously and maintained by independent organizations for different purposes, resulting in a heterogeneous set of KSs. One of the main challenges when text-mining in the information stored in KSs. The terms and their associations will convey knowledge across scientific domains. Terms (e.g., gene names, proteins, gene products, organisms, drugs, chemical compounds, etc.) enable scientific communication. Additionally, they are terms. New terms are introduced in the domain vocabulary on a daily basis, and given the number of new names introduced around the world, it is practically impossible to maintain up-to-date terminologies that are manually produced, maintained, and standardized. For example, various curatorial teams had to identify terminologies in order to integrate them into special databases (such as Swiss-Prot, 2 SGD, 3 FlyBase, 4 and UniProt 5 ). Curatorial teams maintain terminolog ical resources; however, the integration terminology from literature. In addition, since some terms appear frequently and some of terms). system, which combines automatic term recognition, term clustering, information retrieval, and visualization. The main objec tive of this system is to facilitate the acquisition of knowledge from documents and discover new knowledge by calculating the similarities based on terminology and by the visualization (graphically drawn knowledge maps) of automatically structured knowledge. This system also supports the integration of different types of databases (textual and non-textual) and the simultaneous retrieval of different types of knowledge. In order to acceler ate knowledge discovery, we propose a visualization method for generating similarity-based knowledge maps. This method is based on real-time terminology-based knowledge clustering and categorization, and it allows users to graphically observe knowledge maps generated in real time. Lastly, we discuss experiments that were conducted using the GENIA corpus [GENIA Project 2002] in order to assess the practicality of the system. 2.1 Terminology Management The knowledge encoded in textual documents is organized around sets of specialized terms . Hence, knowledge acquisition (KA) relies heavily on the recognition of terms. A prerequisite for knowledge mining is terminology management, which includes automatic term extraction, clusteri ng, and classification. 
Recently, several approaches for automati c term recognition (ATR) applicable to biomedicine have been introduced. Rule-based approaches primarily rely on linguistic information, namely, morpho-syntactic f eatures of terms. For example, LaSIE [Gaizauskas et al. 2000], an adaptive news wire name recognizer, uses a case sensitive terminology lexicon of component terms, a set of morphological cues (biochemical suffixes), and hand-constructed grammar rules in order to recognize terms belonging to PROPER [Fukuda et al. 1998], which relies on simple lexical patterns and orthographic features for protein name recognition. PROPER (PROtein Proper noun phrase Extracting Rules) 6 uses  X  X ore X  and  X  X eature X  terms to identif y strings that correspond to proteins. 
Core terms are domain-characteristic words that reflect the core meaning (containing, e.g., capitals, numerals); feature terms are keywords that describe the terms X  function and precision of 94.7% at a recall of 98.8%. 
A variety of machine learning and statisti cal techniques are used for ATR. Machine-learning systems rely on the existence of training data to learn features that can be used for term recognition. However, the main problem is that there are not many reliable and widely used training resources. An exception is the GENIA corpus that is one of the few terminologically-tagged corpora and is now widely used by the bio-text mining community. 
Hatzivassiloglou et al. [2001] present a st atistically-based unsupervised technique to used HMMs and specific orthographic features (e.g.,  X  X onsisting of letters and digits, X   X  X aving the initial letter in upper case, X  etc.) to discover terms (belonging to a set of ten classes). 
The use of hybrid approaches that combine li nguistic and statistical knowledge is also increasing [Mima et al. 2001 a; Mima and Ananiadou 2001b]. In order to assess the relevance of extracted candidate terms, these methods calculate the weights (i.e., term-term-extraction in biomedicine, we refer the reader to Ananiadou and Nenadic [2006], Ananiadou et al. [2004], and Kr authammer and Nenadic [2004]. 
However, ATR is not the ultimate goal. The large number of new terms necessitates a systematic method for accessing and retrievi ng the knowledge that they represent. Accordingly, the extracted terms must be pl aced in an appropriat e knowledge framework by identifying relationships between the term s and by establishing links between them and different factual databases. 
Several ontologies (e.g., MeSH terms, gene ontology, GENIA ontology) have been developed to support knowledge structuring. An ontology is not concerned with lexical and structured view over a concept sp ace. However, a terminology necessarily incorporates an ontology. Ontologies implement a predefined classifi cation system for concepts and their inter-relationships, as well as inference rules that are used to derive knowledge represented by the concepts. UMLS (unified medical language system) [UMLS 2004], GO (gene ontology), and GENIA are some of the existing biomedical ontologies. However, ontology construction and maintenance are time-consuming activities, since concepts are usually manually integrated into an ontology. This is one reason why ontologies typically contain only a subset of the existing terminology occurring in texts. In addition, solutions for the well-known difficulties of ontology development, ontology conflicts, mismatches [Visser et al. 1997], and a method to map ontological concepts to term-forms in text remain to be found. So techniques for (semi)-automated ontology management [Gamper et al. 1999; Spasic et al. 2005] are urgen tly required for efficient and consistent KA. 2.2 Integration of Knowledge Sources suggested. For example, Semantic Web [Berners-Lee 1998] strives to link relevant XML-based resources in a bottom-up manner using the Resource Description Framework (RDF) and ontological information. XML facilitates the introduction of new domain and/or application-specific tags, while RDF [Brickle and Guha 2000] is employed to define their  X  X eanings X  and inter-relationships. Corresponding ontologies are used to combine and derive additional information (e.g., synonyms, hyponyms, etc.). In this sense, ontologies are used as key domain knowledge repositories. The Semantic Web is efficient in semantically retrieving the content of resources; however, manual description is still required when defining RDF descriptions and ontologies. However, if we endeavor to process large collections of new documents (including new knowledge), we require systems that do not rely solely on manual descriptions. 
In this article, we present our approach toward terminology management and the mining of knowledge sources in a knowledge structuring system [KSS]. 3. KNOWLEDGE STRUCTURING SYSTEM ARCHITECTURE The KSS was developed to address the problems of ontology-driven literature mining and KA. This system, similar to the Semantic Web, deals with documents and ontology-based inference. However, it also facilitates KA tasks by using manually-defined resource descriptions and by exploiting natural-language processing techniques like automatic term recognition (ATR) and automatic term clus tering (ATC). Both techniques are used for the (semi)-automatic management of the underlying ontology. The KSS system also integrates an information-retrieval engine and a similarity-calculation engine; these tools allow users to locate relevant knowledge sources based on keywords and the relationship between them. based on textual data obtained from its various components, which allow it to deal with documents that are generally available in di fferent formats (e.g., pdf, Word, HTML, and XML) and databases (e.g., SQL server, Oracle, and POSTGRES). Typically, as shown in documents is linguistically processed (seg mentation, part-of-speech (POS) tagging, shallow parsing.) and the resulting texts are indexed for subsequent retrieval; second, the automatically recognized and structured (class ified or incorporated into an ontology). The indexing and the ontology development processes described above are performed offline; the index data and ontological information are stored in the corresponding database; third, and last, relevant information is retrieved and structured by using the ontological knowledge structuring to the user. Details regarding the structuring of knowledge and its visualization are explained in Section 5. 3): Data Reader (DR)  X  It extracts textual data from target KSs.
 Ontology Development Engine(s) (ODE) and Ontology Data Manager (ODM)  X  They Text Data Manager (TDM)  X  It stores the index of KSs and ontological information Information Retriever (IR)  X  It retrieves the KSs from the TDM and calculates the Similarity Calculation Engine(s) (SCE) and Similarity Manager (SM)  X  They calculate Graph Visualizer  X  visualizes knowledge structures that are based on simple undirected POS tagging 7 , i.e., the assignment of basic parts of speech (e.g., nouns, verbs, etc.) to other words, ATR is the process of distinguishing terms that belong to a particular subject terms into the corresponding concepts. Howeve r, the lack of clear naming standards in some domains such as biomedicine often makes ATR a non-trivial problem [Fukuda et al. 1998]. Additionally, it typically gives rise to many-to-many relationships between terms and concepts. In practice, two problems arise due to this X  X  X  particular term may represent a number of concepts, while a particular concept may be denoted by more than one term. In other words, some terms may have multiple meanings ( term ambiguity ), while a group negative effects on IE precision while term variation decreases the IE recall. These problems point out the disadvantages of using simple keyword-based IE techniques. Evidently, more sophisticated techniques are re quired. Such techniques should be able to users could benefit from techniques employing ATR/ATC and term variation management methods that perform efficiently and consistently. These methods are also important for organizing domain-specific knowledge as terms should not be treated in isolation from other terms. Rather, relevant relationships between the terms existing among the corresponding concepts should be formed in order to be at least partly reflected in a terminology. 
In our system, term processing is based on the C/NC-value automatic term-recognition method [Mima and Ananiadou 2001b], while ATC is carried out using average mutual information (Figure 4). Its primary purpose is to help domain experts to gather and manage domain-specific terminology. ATC al so automatically recognizes and clusters terms offline and transfers the results to the database. 4.1 Recognizing Biomedical Terms in Text ATR is faced with many challenges, particularly in biomedicine. One of the main challenges is to recognize ad-hoc names (e.g., names of genes, names like bride of multiword units (85%  X  90%). Thus, term boundaries (e.g., whether the word possible is part of the term possible T and natural killer cells ) and nested terms (e.g., terms cell line recognized  X  but these are ty pically non-trivial tasks. And, due to their complexity, variations in terms pose another challenge. In addition, biological names are very complex; the literature contains huge numbers of synonyms and variant term-forms [BioCreAtIvE 2004]. Most terms are used along with synonyms and other variants such as acronyms, morphological and derivational variations, and so on (e.g., TIF2, TIF-2, transcription intermediary factor-2, transcriptional intermediate factor 2 ). Thus, term variations are an integral part of automatic term recognition. Further, many biological terms (systematic ambiguities that have to be resolved with ontological considerations). 
To extract biomedical terms we developed and tuned the C/NC-value method [Mima et al. 2001a; Ananiadou et al. 2004] that recognizes primarily multiword terms by combining linguistic knowledge and statistical analysis. The C/NC-value method is an ATR approach that is independent of domain and language. This method enhances the commonly used baseline approach (frequency of occurrence) by making term extraction cause most of the problems. In addition, we incorporated term variations in order to enhance the performance of the C/NC-val ue method [Nenadic et al. 2004]. 
The C/NC-value method is implemented as a two-step procedure. In the first step, candidate terms are extracted by using a set of linguistic filters and implemented using an LFG-based GLR parser that describes general patterns of term formation. In the second measure. The measure combines four numer ical corpus-based characteristics of a candidate term, namely, frequency of occurr ence, frequency of occurrence as a substring of other candidate terms, number of candidate terms containing the given candidate term as a substring, and the number of words contained in the candidate term. 
The NC-value method improves the C-value results further by taking the context of the candidate terms into account. The relevant context words are extracted and assigned weights based on how frequently they appear with top-ranked candidate terms that are extracted by the C-value method. Subsequently , context factors are assigned to candidate terms according to their co-occurrence with to p-ranked context words. Finally, new term-linear combination of the C-values and the context factors. Evaluation of the C/NC-method (see Section 6) shows that contextual information improves term distribution in the extracted list by placing real terms closer to the top of the list; term variation further enhances the performance of the system. 4.2 Managing Term Variation Term variation and ambiguity cause problems not only for ATR but also for human experts. Several methods for managing term variation have been developed, e.g., the BLAST system [Krauthammer et al. 2000] employs approximate text string-matching techniques and dictionaries to recognize sp elling variations in the names of genes and proteins. FASTR [Jacquemin 2001] handles morphological and syntactic variations by employing meta-rules to describe term normalization, while semantic variants are dealt with by WordNet. All trans retionic acid, all-trans-retinoic ac ids, ATRA, at-RA All trans retionic acid Nuclear receptor, nuclear recept ors, NR, NRs Nuclear receptor 9-c-RA, 9cRA, 9-cis-retinoic acid, 9-cis retinoic acid 9-cis-retinoic acid RAR alpha, RAR-alpha, RA recep tor alph, retinoic acid receptor alpha Retinoic acid receptor a DNA, DNAs, deoxyribonucleic acid Deoxyribonucleic acid NF-KB, NF-kb, nuclear factor kappa B, NF-kappaB Nuclear factor kappa B 2001a; Nenadic et al. 2004]. We consider various sources from which problems regarding term variation originate. In particular, we deal with orthographical, morphological, syntactic, lexico-semantic, and pragmatic phenomena. Our approach to managing term variations considers term normalization as an integral part of the ATR process. Term variants (i.e., synonymous terms) are dealt with in the initial phase of ATR, when candidate terms are separated, as opposed to other approaches (e.g., FASTR handles variants by subsequently applying transformation rules to the extracted terms). In order to conflate equivalent surface expressions, linguistic normalization of individual candidate terms (examples in Table I) is carried out. Firstly, each candidate term is mapped to a specific canonical representative (CR) by semantically isomorphic transformations. Thereafter, we establish an equivalence re lationship wherein two candidate terms are related iff they share the same CR. The partitions of this relationship are denoted as synterms . A synterm comprises surface term representations sharing the same CR. Our aim is to form synterms before the syntactic estimation of term-hoods for candidate terms [Nenadic et al. 2004] .
 4.3 Term Clustering In the literature, term clustering is an indispensable component of the mining process, in addition to term recognition. Since term opacity and polysemy are extremely common in molecular biology and biomedicine, term clustering is essential in order to integrate semantic terms and to construct domain ontologies and semantic tagging. 
In our system the ATC is performed using a hierarchical clustering method that merges clusters based on average mutual informa tion that measures how strongly terms are related to each other [Ushioda 1996]. Th e input consists of terms and their co-occurrences, recognized automatically by the NC-value method; a dendrogram of terms is The calculated term cluster information is encoded and used for calculating semantic similarities in the similarity calculation engine (SCE). 5. VISUALIZATION TO GENERATE KNOWLEDGE MAPS As compared to IE/KA, knowledge mining can be regarded as the broader approach The IE and KA in our system are implemented through the integration of terminology-based ontology development and calculation of semantic similarities. Graph-based visualization for the automatic generation of knowledge maps is also provided to help in retrieving knowledge and KA from documents. The system also supports combining the different types of databases (papers, patents, technologies, and innovations) and retrieves different types of knowledge simultaneously across documents. This feature can accelerate the discovery of knowledge by combining existing types of knowledge. The basic idea behind the discovery of new knowledge by using ontological information follows: If we then C  X  by syllogism; whereas new knowledge cannot be discovered if the relationship knowledge about industrial innovation by structuring the knowledge of up-to-date collections of scientific papers and reports on past industrial innovation. Figure 6 shows an example of the visualizatio n of knowledge structures from paper abstracts relevant to the term  X  X eceptor X  in the GENIA corpus [GEN IA 2002]. In order to structure knowledge, the system constructs a graph in which the nodes indicate relevant KSs for the keywords specified by the user. Links among the KSs indicate semantic similarities that are calculated using ontology information developed by our ATR/ATC components. Semantic similarity is based on comparing ontological informati on extracted from each KS, whereas conventional similarity calculati on is generally based on nouns extracted from each KS. Additionally, th e locations of each node are calculated and optimized when drawing the graph. The distance between nodes depends on how close they are in meaning. The complete algorithm of this knowledge-structuring method follows: end Visualize graph based on every {w(i,j)|i=Q or i  X  R, j  X  R, i  X  j } end. terminology-based categorization. Cluster rec ognition is carried out by detecting groups of nodes in which every combination of included nodes is strongly linked (i.e., their and an SVM-based categorizer. Figure 7 shows a knowledge map that was generated from news articles. The target informat ion was extracted from online articles in Yomiuri and Mainichi (newspapers in both English and Japanese), where the keywords specified for IR are  X  X raq X  and  X  X allujah. X  As shown in Figure 7, seven clusters are recognized and category names assigned: (1) Bin Laden, (2) Secretary of Stat e Powell, (3) Dispatch of the Japanese self-defense forces, (4) Presidential election, (5) Samawah, (6 ) Prime Minister Koizumi, and (7) Prime Minister Allawi. The basic method includes categorizing and mapping concepts in order to help to understand the information. Furthermore, this method can also be used to disambiguate semantically specified keywords. For example, the keyword  X  X pple X  includes at least two meanings, namely,  X  X ru it X  and  X  X omputer company. X  However, by using clustered and categorized IR results, we can find the information we want more easily. 6. EXPERIMENTS AND EVALUATION In this section we report on the experiments conducted using an AI domain corpus for ATR, and the GENIA corpus for terminology-based categorization, to demonstrate how ontology development and knowledge map generation perform in practice. The experiments with term-variation management in ATR were conducted on the GENIA corpus containing 100,000 nouns (2082 abstracts) from the M EDLINE database [MEDLINE 2002]. Figure 8 shows the recall agains t precision graph for the C/NC-value method compared to the frequency of occurren ce. It can also be seen that the NC-value method is slightly more precise than the C-value method, and substantially more precise than conventional pure frequency-based methods. 
The GENIA corpus [GENIA Project 2002] was also used for the categorization experiment. The test set contained approximately 10,000 terms across the three major GENIA classes ( nucleic acid , amino acid, and source ) and approximately 100,000 nouns. The terms and nouns were used for term -based and noun-based categorizations, respectively. It was found that low-frequency terms (nouns) play an important role in the categorization of texts. However, learning with low-frequency terms produces an hand, our method compresses f eatures that reduce calculation costs by using terminology information --resulting in an efficient text -categorization techniqu e. This experiment allowed us to categorize GENIA abstracts into three major categories, namely (1) Immunol, (2) Mol Cell, and (3) Blood. We used TinySVM [2004] to learn the categorization model, and 50 abstracts to learn and create the three categories. shown in the figure, although the sample size was not large enough, term-based categorization precision was better than noun-based categorization. So we expect the method to be practical and efficient enough to generate knowledge maps to make new knowledge discoveries. 7. CONCLUSION This article presents an integrated knowledge-mining system for the biomedicine domain, which integrates automatic term recognition, term clustering, information retrieval, and visualization. Its main objective is to f acilitate knowledge acquisition from documents and the discovery of new knowledge by calculating terminology-based similarities and visualizing automatically-structured knowl edge. Additionally, to accelerate knowledge discovery, we proposed a visualization method for generating similarity-based knowledge maps. This method is based on real-time terminology-based knowledge clustering and categorization, and allows users to observe knowledge maps being generated graphically in real time. Experiments on the GENIA corp us shows that this method is practical enough to use for enhancing new knowledge discovery from existing knowledge sources. intend to investigate the possibility of using a system for classifying terms as an alternative structuring model for knowledge deduction and inference, instead of an ontology. ACKNOWLEDGMENTS Hideki Mima is grateful to the Artificial Intelligence Research Promotion Foundation for promoting this study, in part under the AI research grant scheme. Hideki Mima and Sophia Ananiadou thank the Daiwa Foundation for enabling their collaboration with the support of the Daiwa Adrian Prize scheme. REFERENCES 
