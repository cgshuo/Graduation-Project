 In traditional text clustering me thods, documents are represented as  X  X ags of words X  without cons idering the semantic information of each document. For instance, if two documents use different collections of core words to represent the same topic, they may be falsely assigned to different clus ters due to the lack of shared core words, although the core words they use are probably synonyms or semantically associat ed in other forms. The most common way to solve this problem is to enrich document representation with the background knowledge in an ontology. There are two major issues for this approach: (1) the coverage of the ontology is limited, even for WordNet or Mesh, (2) using ontology terms as replacement or additional features may cause information loss, or introduce noise . In this paper, we present a novel text clustering method to address these two issues by enriching document representation with Wikipedia concept and category information. We develop two approaches, exact match and relatedness-match , to map text documents to Wikipedia concepts, and further to Wikipedia categories. Then the text documents are clustered based on a similarity metric which combines document content information, concept information as well as category information. The experimental results using the proposed clustering framework on three datasets (20-newsgroup, TDT2, and LA Times) show that clustering performance improves significantly by enrichi ng document representation with Wikipedia concepts and categories. I.5.3 [ Pattern recognition ]: Clustering  X  algorithms, similarity measures. Algorithms, Experimentation Text Clustering, Wikipedia, Document Representation Traditional clustering algorithms are usually based on the BOW (Bag of Words) approach. A notorious disadvantage of the BOW model is that it ignores the se mantic relationship among words. As a result, if two documents us e different collections of core words to represent the same topic, they can be assigned to different clusters, although the core words they use are probably synonyms or semantically associat ed in other forms. One way to resolve this problem is to enrich document representation with the background knowledge represented by an ontology. An ontology usually includes at l east three components: concepts, attributes, and the relationships among concepts. A ll of them can be used for document representa tion and clustering. The most common way of applying ontologies for clustering is to match ontology concepts to the topical terms appearing in the documents. Then the matched ontol ogy concepts are either used as replacement or introduced as add itional features to the original text. Further, the attributes of and relationships among the ontology terms can be exploited for clustering. However, a major problem of this approach is that it is usually difficult to find a comprehensive ontology which can cover all the concepts mentioned in a collection, especially when the documents to be clustered are from general domain. Previous research has adopted WordNet [4, 5] and Mesh [12, 13] as the external ontology for text enrichment. However, they all have limited coverage. Another problem is that using ontology terms either as replacement or additiona l features has its disadvantages. While replacing original content with ontology terms may cause information loss, especially when the coverage of the ontology is limited, adding ontology terms to the original document vector can bring data noise into the dataset. Therefore, in order to enhance text clustering by leve raging ontology semantics, two issues need to be addressed: an ontology which can cover the topical domain of individual docum ent collections as completely as possible; and a proper matchi ng method which can enrich the document representation by fully leveraging ontology terms and relations but without introducing more noise. This paper aims to address both issues. In terms of ontology, we rely on Wikipedia concepts and categories for document enrichment. Wikipedia has become the largest electronic knowledge repository on the web with millions of articles contributed collaboratively by vol unteers. Unlike other standard ontologies, such as WordNet and Mesh, Wikipedia itself is not a structured thesaurus. However, it is much more comprehensive and up to date. Moreover, it is well-formed. In Wikipedia, each succinct phrase that resembles an ontology term. Equivalent concepts are grouped together by redirected links. Meanwhile, it contains a hierarchical categorization system, in which each article belongs to at least one cat egory. All these features make Wikipedia a potential ontology which can be exploited for enriching text representation and enhancing text clustering. As for how to integrate ontology concepts into the document representation and clustering proce ss, in this paper, we propose two approaches for mapping ontology concepts to the documents. The first approach, called exact-match, is a dictionary-based approach. It maps the topical terms present in the documents directly to Wikipedia concepts. It is especially useful when Wikipedia concepts can cover most of the topic terms in a collection. The second mapping a pproach is called relatedness-match. Instead of mapping Wikipedia concepts to each document directly, this approach builds the connection between Wikipedia concepts and each document based on the contents of Wikipedia articles. This approach is more useful when Wikipedia concepts cannot fully cover the topical domain of a collection. After the mapping process, each document is associated with a set of concepts. Then based on the hierarchical structure of Wikipedia, each document is further mapped to a set of Wikipedia categories. Finally, the text documents are clustered based on a similarity metric which combines document content information, concept information as well as category information. The proposed Wikipedia-based clus tering framework is evaluated on three datasets: 20-newsgroups, TDT2, and LA Times. We use both agglomerative and partitional clustering for experiments and the traditional BOW model as the ba seline. The results show that, in agglomerative clustering method, enriching document representation with Wikipedia c oncepts and categories by both exact-match and relatedness match can significantly improve the clustering performance. However, the results of partitional clustering vary among different datasets and depend on the matching scheme adopted. The rest of the paper is organi zed as follows: Section 2 reviews related work. Section 3 introduces the proposed method of utilizing Wikipedia concepts and categories to improve document clustering. In Section 4, we pr esent and discuss experimental results. Finally, we conclude the paper in section 5. Recently, there is a growing am ount of research on how to utilizing Wikipedia to enhance text mining tasks, such as information retrieval [7, 8], text classification [2, 11] and clustering [1, 6]. Milne et al. [8] develop a search engine that works on the basis of the thesauri derived from Wikipe dia. The hyperlinks, redirect links and hierarchical relations within Wikipedia are exploited to build the thesauri that are specific to individual collections. Based on the thesauri, the search engine can expand queries automatically and guide users to improve their queries during the search process. Li et al. [7] use Wikipedia category information to improve weak ad-hoc queries. After the initial search with a weak query, the returned articles are re-ranked based on the linear combination of their original ranking score and Wikipedia category score. Then a certain number of terms are selected from top-ranked articles to expand the search query. Phan et al. [9] present a framework for discovering hidden topics from large-scale data collections to resolve the data sparsity problem in short text classifi cation. Instead of using human category information in Wikipedi a, they use Gibbs sampling and LDA to sample topics from both la rge-scale data collection and a sparse testing dataset. Each tes ting document is classified based on a vector combining both content and topic information. Although the approach provides a different perspective on using large-scale text collection, it does not fully utilize useful information embedded in Wikipedia such as the category, link information, etc. Moreover, th e sampling process can be very time consuming and the sampled topics are time sensitive to Wikipedia snapshots. Gabrilovich and Markovitch [2, 3] propose a method to improve text classification performance by enriching document representation with Wikipedia concepts. The mapping between each document and Wikipedia concepts is achieved through a feature generator which acts like a retrieval engine. It receives a text fragment, which can be word s, sentence, paragraph, or the whole document, and outputs the mo st relevant Wikipedia articles to the text fragment. The titles of the retrieved Wikipedia articles are further filtered and those with high discriminative capacity are used as additional features to enrich the representation of the corresponding documents. Empirical evaluation shows that their method can greatly improve classification performance. However, the multi-resolution feature generation procedure they apply for mapping Wikipedia con cepts requires high processing efforts, because each document needs to be scanned multiple times. And it produces too many Wikipedia concepts for each document. Especially, when the te xt fragments used for retrieving Wikipedia articles are generic words or sentences, this procedure only introduces noise. Although the authors apply a filtering step processing efforts and time. In our method of relatedness-match, we only use document words to retrieve Wikipedia articles. However, each document word is weighted based on their tfidf value. Thereby, Wikipedia concep ts retrieved by important words with high tfidf value are ranked higher than those retrieved through unimportant words. Wikipedia has also been applied fo r text clustering. Banerjee et al. [1] use a method similar to that a pplied in [2] for clustering short texts. But different from [2], th ey use query strings created from document texts to retrieve releva nt Wikipedia articles. The titles of top-ranked Wikipedia articles se rve as additional features for clustering Google news. The method in both [1] and [2] only augment document representation with Wikipedia c oncepts without considering the hierarchical relationship embedded in Wikipedia. In our method, we also integrate Wikipedia category information into document representation based on the hierarch ical structure of Wikipedia. We believe that integrating hi gh-level category information can further improve clustering performance by introducing more background knowledge into the clustering process. The Wikipedia category information has also been utilized in [6] and [11] for text clustering a nd classification respectively. Besides, they also extend the Wikipedia concept vector for each document with synonyms and associative concepts based on the redirect links and hyperlinks in Wikipedia. Their methods to a great extent leverage the abundant structural information within Wikipedia. However, they all rely on an exact phrase matching strategy for mapping text documents to Wikipedia concepts. This strategy is limited by the terms appearing in the documents and the coverage of Wikipedia concepts or article titles. For instance, if the topical terms used in a document are not exactly the same as any Wikipedia concept but synonymous to some of them, then the Wikipedia concepts which have the same meaning with the topical terms would not be mapped to the documents. In our paper, to solve this problem, we adopt another mapping strategy called relatedness-match, which doe s not merely using Wikipedia article titles for matching but also considering the content of the whole Wikipedia articles duri ng the matching process. The framework of our method fo r leveraging Wikipedia concept and category information to im prove document clustering is presented in Figure 1. We first define two concept ma pping schemes: exact-match and relatedness-match. Then, based on the two mapping schemes, we construct concept feature vector and category feature vector for each document. The document content vector W n , concept vector C and category vector Cat n are linearly combined to measure document similarity. Finally, with the new similarity metric, the documents are clustered using agglomerative approach and partitional approach respectively. The mapping process includes three steps: (1) build the connection between Wikipedia con cepts and categories; (2) map each document into a vector of Wikipedia concepts; (3) match each document to a set of Wikipedia categories. Each step generates a matrix (see Figure 2). The concept-category matrix is created intuitively based on the connection between concepts and categories which is explicit in Wikipedia. The document-concept matrix is built through two matching schemes: exact-match and relatedness-match. Finally, the document-category matrix is created on the basis of concept-category matrix and document-concept matrix. A proper matching method is cruc ial for ontology-based text clustering. In our research, we adopt two different match schemes (exact-match and relatedness-ma tch) for mapping documents to Wikipedia concepts. The details of each mapping scheme are described below. By exact-match scheme, each document is scanned to find Wikipedia concepts, which ar e mostly short phrases. The searched Wikipedia concepts are used to comprise the concept vector of the corresponding document. An issue of exact-match is how to map synonymous phrases to the same concept. We address this problem by using the redirect links in Wikipedia. In Wikipedia, each topic is described by only one article. A preferred phrase is chosen as th e title of the article. All other phrases representing the same topic are redirected to the same article. Based on this feature, we construct a dictionary. Each entry in the dictionary corresponds to a topic covered by Wikipedia. Each entry includes not only the preferred Wikipedia concept which is used as the title of the article, but also all redirected concepts representing the same topic. Based on this dictionary, both preferred concepts and redirected concepts are retrieved from documents. However, only preferred concepts are used to build the concept vector for each document. The weight of each preferred concepts equals to the frequency of itself plus the frequencies of all the concepts redir ected to it. In this way, we get a document-concept matrix, whose values are the frequencies of each concept appearing in a document. Based on this frequency matrix, we further calculate the document-concept TFIDF matrix, which is used to measure the si milarity between two documents X  
Figure 2. Mapping documents to Wikipedia concepts and concept vectors. Compared to other matching technique, exact-match is very efficient. Howeve r, it always has low recall. It produces good result only when Wi kipedia has a good coverage of the phrases appearing in a dataset. By exact-match, only the concepts which explicitly appear in a document are extracted and used to construct the concept vector of the document. In other words, when the topical terms used in a document can not exactly match the Wikipedia concepts denoting the same topic, they cannot be extracted. In order to resolve this problem, we adopt another ma tch scheme called relatedness-match. Relatedness-match consists of tw o steps. First, we create a Wikipedia term-concept matrix (See Figure 3) from Wikipedia article collection. Thus each word token is represented by a concept vector. The values of th e vector are TFIDF scores, which denote the relatedness between the term and each Wikipedia concept. A word may appear in a huge number of Wikipedia articles. In order to discard insignificant concepts and improve processing efficiency, for each word, we only choose top k concepts with highest TFIDF scores. In this study, we set k to 5. Second, the word-concept matrix is used as a bridge to associate documents with Wikipedia c oncepts. The relatedness of a Wikipedia concept to a given document is calculated using equation (1). where D d j  X  (a document collection) and C c Wikipedia preferred concepts). The procedure of calculating the relatedness of concept k c to document j d is as follows. For each word such as i w in document j d , we calculate its TFIDF scores in both j d and k c . The two scores specify the importance of word use the product of two TFIDF values as the relatedness score of concept the relatedness score of concept word in document j d , we get the final relatedness score j concept k c to document j d . Then, we select top M concepts with highest relatedness score for each document. In this experiment, M is set to 200. Finally, the concept relatedness score vector for each document is normalized. Compared to exact-match, this me thod is more time consuming, it help identify relevant Wikipedia concepts which are not explicitly present in a document. It is especially useful when Wikipedia concepts have less coverage for a dataset. After concept mapping, a document-concept matrix is generated for each collection. Based on the document-concept matrix and the hierarchical relation between Wikipedia concept and category, we derive the document-category matrix (see Figure 2). If the document-concept matrix is created through exact-match, a document-category frequency matrix is first derived from the document-concept frequency matrix by replacing each concept with its corresponding categories. The frequency of a category is the frequency of the concept belonging to it. If a category is mapped to a document through more than one concept, the sum of the frequencies of these concepts is the category X  X  frequency. Based on the generated document-category frequency matrix, we further derive the document-category TFIDF matrix, which is used to measure the similarity between any two documents X  category vectors. If the document-concept matrix is developed through relatedness-match, to get the document-category matrix, we replace each concept with its corresponding cate gories and all these categories share the same normalized relatedness score as the concept. If a category is mapped to a document through more than one concept, its relatedness score to the document is the sum of the scores of all these concepts . The derived document-category matrix denotes the relatedness of each category to each document. Agglomerative clustering appro aches initially consider each document as a cluster and repeatedly merge pairs of clusters with shortest distance until only one cluster is formed covering all the documents. The distance measure between two clusters can be implemented in many ways including single linkage , complete linkage , and average linkage . In our experiment, when using standard vector cosine similarity as document similarity measure, both single linkage and average li nkage suffer a severe chaining problem on all three testing datase ts. Therefore, we use complete linkage as cluster distance meas ure. With complete linkage criterion, the distance of two clus ters is defined as the maximum distance between one document in th e first cluster and the other in the second cluster. In our method, besides word vector, a document is also represented by concept vector or category vector, or both of them. When calculating similarity between two documents, we combine the simila rity values calculated using these vectors (see equation (2)). , where coefficient  X  and  X  indicates the importance of concept vector and category vector in m easuring the similarity between two documents. Partitional clustering approaches iteratively calculate the cluster centroids and reassign each documen t to the closest cluster until no document can be reassigned. Sphe rical k-means is one of these algorithms and most widely used for text clustering. Therefore, we apply spherical k-means for partitional approach. In our method, the distance from a document to a cluster centroid is calculated based on the content similarity as well as concept similarity or category sim ilarity, or both of them. , where  X  and  X  quantifies the influence of the concept and category information on document clustering. Since the clustering result of k-means is influenced by the initial selection of cluster centroids. For each evaluation based on K-means, we run ten times with random initialization and take the average as the final clustering result. In comparative experiment, each run has the same initialization. Wikipedia release its database dumps periodically, which can be downloaded from http://dow nload.wikipedia.org . The Wikepedia dump we use contains 911, 028 articles and about 29000 categories after pre-processing and filtering. We perform clustering experiment s on three datasets: TDT2, LA Times (from TREC), and 20-news groups (20NG). We selected 7,094 documents in TDT2 that ha ve a unique class label, 18,547 documents from top ten sections of LA Times, and all 19,997 documents in 20-newsgroups. The ten classes selected from TDT2 are 20001, 20015, 20002, 20013, 20070, 20044, 20076, 20071, 20012, and 20023 . The ten sections selected from LA Times are Entertainment, Financial, Fo reign, Late Final, Letters, Metro, National, Sports, Calendar, and View . All 20 classes of 20NG are used for testing. For efficiency, we adopt a special evaluation approach. For each dataset, we create five small da tasets. Each small dataset is created by randomly picking 100 documents from each selected class of a given dataset and then merge them into a big pool. The five small datasets are clustered separately, and the average of their results is viewed as th e clustering result for the whole dataset. Cluster quality is evaluated by three metrics, purity [14], F-score [10], and normalized mutual information (NMI) [15]. Purity assumes that all samples of a cluster are predicted to be members of the actual dominant class for that cluster. F-score combines the information of precision and recall which is extensively applied in information retrieval. NMI is an increasingly popular measure of clustering quality. It is defined as the mutual information between the cluster assignments a nd a pre-existing labeling of the dataset normalized by the arithmetic mean of the maximum possible entropies of the empirical marginals, i.e. , where X is a random variable for cluster assignments, Y is a random variable for the pre-existi ng labels on the same data, k is the number of clusters, and c is the number of pre-existing classes. A merit of NMI is that it does not necessarily increase when the number of clusters increases. All the three metrics range from 0 to 1, and the higher their value, the better the clustering quality is. In both agglomerative and partitional clustering approaches, we use the clustering approach based on word-only vectors as the baseline. Other approaches based on different linear combinations of word vector, concept vector and category vector are listed in Table 2. The parameter  X  and  X  in equation (2) and (3) are set in the following way:  X  For clustering based on Word_Concept scheme,  X  is set to  X  For clustering based on Word_Category scheme,  X  is set to  X  For Word_Concept_Category scheme,  X  is set to the value Table 3 shows the results of a gglomerative clustering using two different match schemes: Exact -match (EM) and relatedness-match (RM). The bold values in table 3 are im proved results compared to the baseline. The  X * X  indicates the improvement is significant according to the paired-sample T-test at the level of p&lt;0.05. These symbols are applied in all following experimental result tables. 
Table 3. Agglomerative clustering results on three datasets From Table 3, we can see that the scheme Word_Category and Word_Concept_Category always get the best results across all three datasets. In most cases, they can significantly improve the performance of clustering. Howeve r, out of our expectation, Word_Concept_Category does not perform better than Word_Category. Moreover, although in most cases Word_Concept scheme can also improve clustering results, the improvement are not significant. Sometimes it even performs worse than the baseline. This indi cates that integrating Wikipedia concept information into cluste ring process does not necessarily improve clustering performance. This conclusion can be further confirmed by examining the cl ustering results using Concept vector alone. In most cases, clustering only based on concept information performs worse than th e baseline. On the other hand, Wikipedia category information is much more valuable for improve clustering performance. In general, combining word vector and Wikipedia category vector can significantly improve clustering results. For instance, according to NMI, for 20 Newsgroup, Word_Category achieves 15.3% and 18.8% increase in performance with exact-match and relatedness-match respectively; for TDT2, Word_Category improves the performance by 29.4% and 22.9% with exact-match and relatedness-match respectively. Be sides, clustering solely based on category vector most times performs better than clustering solely using concept vector, and have better or close performance to the baseline. This observation is especially true for the dataset TDT2. We also tested clusteri ng based on category and cluster vector together (Concept_Category). For LATimes and TDT2, it performs better than using either category information or concept information alone. However, fo r 20 Newsgroup, its performance is quite unstable. In summary, our experimental resu lts of agglomerative clustering show that category information is more useful than concept information for improving clusteri ng results. We think the reason is that the Wikipedia concept collection we applied for experiment still contains too much noise. By integrating concept information into document presen tation, we also introduce noise to the clustering process. Another reason is that we do not disambiguate concept senses dur ing the concept mapping process. This may further decrease the discriminative capacity of the concept vectors created for the doc uments. Compared to concept, category information is much le ss suffered from noise, and more accurate and informative. It is not apparent which match sc heme is better. Their effect on clustering results always depends on the datasets and clustering schemes. For instance, accordi ng to NMI, for 20 Newsgroup, relatedness-match based cluste ring outperforms exact-match Word_Concept_Category. For the ot her two datasets, exact-match performs better than relatedness-match in most cases. Table 4 lists the results of par titional clustering based on different vector schemes and using two di fferent match sc hemes. We can see that the effect of category information and cluster information on clustering results is not as significant as in agglomerative clustering. We think this is because, in K-means, category vector and concept vector are not used to measure the similarity between two documents, but used to calculate the distance between a document and a cluster centroid. Accordingly, category information and concept information are not utilized in full scale. Even so, we can still see the contribution of category information to clustering results. For 20 Ne wsgroup, Word_Category scheme still significantly improves the clustering result. The F-Score and Purity of Word_Concept_Category based clustering are also significantly improved. For TDT2, Word_Concept_Category produces the best clustering results. It is also notable that for da taset 20 Newsgroup, relatedness-match always produces better results than exact-match. But for the other two datasets, LATimes and TDT2, exact-match always outperforms relatedness-match. In this paper, we present a general framework for leveraging Wikipedia concept and category information to improve text clustering performance. Base d on two different mapping techniques, exact-match and relatedness-match, we are able to create a Wikipedia concept vector and a Wikipedia category vector for each document in a collection. The concept vector and category vector provide background knowledge about a document. They are linearly combin ed with text word vector to measure document similarity. The propose framework is tested with two clustering approaches (agglomerative and partitional clustering) on three datasets: 20NG, LATimes and TDT2. In orde r to comprehensively evaluate the effect of Wikipedia concept and category information on clustering performance, we experi ment seven different clustering schemes X  X oncept, Category, Wo rd_Concept, Word_Category, Concept_Category, and Word_Concept_Category. Based on the empirical results, we can draw the following conclusions: (1) Category information is most useful for improving clustering results. In both agglomerative clustering and partitional clustering, combining categor y information with document content information generates the best results in most cases. Compared to the baseline scheme, it can significantly improve clustering performance for all three datasets when using agglomerative clustering appro ach and for dataset 20 NewsGoup when using partitional clustering. (2) Clustering based on all three document vectors (word vector, c oncept vector, category vector) also gets significantly better results than the baseline. However, it does not outperform clustering ba sed only on word vector and category vector. (3) Concept info rmation is not as useful as category information for improvi ng clustering performance due to the noise information it contains and sense ambiguity problem. (4)The effect of category and concept information on k-means clustering is not as significant as it on agglomerative clustering. But, in most cases, Word_Category based clustering still achieves best performance among all clusteri ng schemes. (5) The effect of the two mapping schemes depends on the dataset, quality metric and clustering approach. Based on the results of partitional clustering, exact-match is more effective than relatedness-match for dataset LATimes and TDT2, but on the contrary for 20 Newsgroup. We believe that our findings can be extended to other applications based on document similarity meas urement, such as information retrieval and text classification. For future work, we will further improve our concept mapping tec hniques, such as introducing sense disambiguation functions in to the concept mapping process. Moreover, we will explore how to utilize the link structure among Wikipedia concepts for document clustering. This work is supported in part by NSF Career Grant IIS 0448023, NSF CCF 0514679, PA Dept of Health Tobacco Settlement Formula Grant (No. 240205 and No. 240196), and PA Dept of Health Grant (No. 239667). [1] Banerjee, S., Ramanathan, K. and Gupta, A. 2007. [2] Gabrilovich, E. and Markovitch, S. 2006. Overcoming the [3] Gabrilovich, E. and Markovitch, S. 2007. Computing [4] Hotho, A., Staab, S.and Stumme, G. 2003. Wordnet [5] Hotho, A., Maedche, A. and St aab, S. Text Clustering Based [6] Hu, J., Fang, L., Cao, Y., et al. Enhancing Text Clustering by [7] Li, Y., Luk, W.P.R, Ho, K. S.E., and Chung, R.L.K. 2007. [8] Milne, D. 2007. Computing Semantic Relatedness using [9] Phan, X., Nguyen, L. and Horiguchi, S. 2008. Learning to [10] Steinbach, M., Karypis, G. and Kumar, V. 2000. A [11] Wang, P. and Domeniconi, C. 2008. Building Semantic [12] Yoo, I., Hu, X. and Song, I.-Y. 2006. Integration of [13] Zhang, X., Jing, L., Hu, X., et al. A Comparative Study of [14] Zhao, Y. and Karypis, G. 2001. Criterion functions for [15] Zhong, S. and Ghosh, J. 2005. Generative model-based 
