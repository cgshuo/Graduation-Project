 Humans understand the world by classifying objects into an appropriate level of categories. This process is often auto-matic and subconscious. Psychologists and linguists call it as Basic-level Categorization (BLC) . BLC can benefit lots of applications such as knowledge panel, advertising and rec-ommendation. However, how to quantify basic-level concepts is still an open problem. Recently, much work focuses on con-structing knowledge bases or semantic networks from web scale text corpora, which makes it possible for the first time to analyze computational approaches for deriving BLC. In this paper, we introduce a method based on typicality and PMI for BLC. We compare it with a few existing measures such as NPMI and commute time to understand its essence, and conduct extensive experiments to show the effectiveness of our approach. We also give a real application example to show how BLC can help sponsored search.
 I.2.4 [ Artificial Intelligence ]: Knowledge Representation Formalisms and Methods X  Representations (procedural and rule-based), Semantic networks Theory Semantic Network; Conceptualization; Basic Level of Cate-gorization
Humans understand the world by classifying objects into concepts. The concepts an object belongs to form a set of hierarchically organized categories, ranging from extremely general to extremely specific [23]. Furthermore, different lev-els of categorization reflect different levels of abstraction, which associate with different properties.

A human being usually maps an object to an appropri-ate level of category, and regard the object as equivalent to other objects in the category. For example, one would say I have a house, a car, and a dog instead of I have asset, vehicle, and mammal. Similarly, when some-one sees an iPhone 6 , he most likely thinks of high end smartphone or Apple X  X  product , instead of item or popular cellular wireless network phone . For humans, this process of categorization is often automatic and subconscious, and psy-chologists call the process Basic-level Categorization (a.k.a. Basic-level Conceptualization, or BLC for short) .
BLC is important because it provides rich information with little cognitive efforts [23]. When a person obtains the basic-level category of an unfamiliar object, he will asso-ciate the object with the known properties of the basic cate-gory, and he suddenly understands almost everything about the object. The fact that he can do this with little cogni-tive efforts 1 has intrigued and pushed researchers to better understand BLC. One of the most important properties of BLC was obtained by Rosch et al. [24]. We summarize their findings in Table 1, which shows that, at the basic level, per-ceived similarity among category members is maximized and perceived similarities across contrasting categories is mini-mized . As a result, the basic-level category of an object is usually in the middle of the object X  X  taxonomic hierarchies.
As an example, consider the term Microsoft , which can be categorized into a large number of concepts, for example, company , large company , Redmond IT giant , etc. Let us take a closer look at the following three concepts: 1. company 2. software company 3. largest OS vendor
Both 1 and 3 are highly related to Microsoft in the sense that when we think of Microsoft , we think of it as a
I t is shown that three-year-olds already master basic-level categorization perfectly [16]. c ompany , and when we think of largest OS vendor , we think of Microsoft . However, neither of them is an appropri-ate basic-level concept for Microsoft . To see this, assume we want to find objects that are similar to Microsoft . If we go through company , we may find objects such as McDonald X  X  and ExxonMobil , which have not much sim-ilarity to Microsoft . If we go through largest OS vendor , we may not be able to find any reasonable object other than Microsoft . On the other hand, if we go through software company , we may find Oracle , Adobe , IBM , which are a lot more similar to Microsoft . Thus, software company is a more appropriate basic-level concept for Microsoft , or in other words, properties associated with software company are more readily applied to Microsoft , which is also the reason why through software company we can find many objects that are similar to Microsoft .

Unfortunately, although much work has been done on this topic, we still do not have a clean formula to infer the basic-level category for any given object. In other words, we do not know how to pick software company from thousands of concepts that Microsoft belongs to. Psychologists have used word association tests on human subjects to infer con-cepts that may correspond to the basic-level categories [24]. But such approaches do not scale. On the other hand, an increasing number of applications in the field of information retrieval, natural language understanding, and artificial in-telligence require a computational approach for BLC. Short text conceptualization [25, 15, 14, 30] in one of the funda-mental techniques. It maps a short text to the concept space, and can benefit web table understanding [28], query intent detection [29], query recommendation [27], etc. BLC is the basic unit of conceptualization, and is crucial for short text conceptualization.
For many applications, including query understanding and ads matching, finding an object X  X  basic-level category is im-portant. Here, we give two real life applications that may serve to demonstrate the need of BLC.

Figure 1: Examples of the Application for BLC
The rise of applications such as those mentioned above is pushing for computational approaches for BLC. It has become clear that a better understanding of the human cog-nitive process helps us build machines that understand the human world. In recent years, a large variety of knowledge bases [3, 26, 9, 32, 5] have been constructed for text under-standing and a variety of other tasks. Some of these knowl-edge bases define a rich concept space, and provides a map-ping from a term to the concept space. However, there is no mechanism to determine the basic-level concept for the term, which makes it difficult for machines to  X  X nderstand X  the term, which in turn hampers the impact of such knowl-edge bases in natural language understanding and other ap-plications.

In this paper, we focus on computational approaches for deriving BLC. Based on previous studies [24], an object X  X  b asic-level concept is considered to be in the middle of the taxonomic hierarchies. In other words, basic-level concepts are a trade-off between general concepts and specific con-cepts. It is also a trade-off between the accuracy of clas-sification and the power of prediction. Based on this ob-servation, we introduce a method based on typicality and PMI for BLC. We compare our approach with a few exist-ing measures including graph commute time to understand its essence. We also conduct extensive experiments to show the effectiveness of our approach.
The rest of the paper is organized as follows. Section 2 in-troduces background knowledge. Section 3 focuses on com-putational approaches of BLC. We discuss typicality, PMI, and introduce our own approach. Section 4 compares our approach with existing approaches. We conclude in Section 5.
A large variety of knowledge bases, including lexcial knowl-edge bases and encyclopedic knowledge bases, have been constructed for various applications. Some of them, such as WordNet [10], Wikipedia [31], Cyc [18], and Freebase [3], are created by human experts or community efforts. Oth-ers, such as KnowItAll [9], NELL [5], and Probase [32], are created by data-driven approaches. Because information in data-driven knowledge bases is usage based, it is particularly useful for natural language understanding. More specifically, data-driven-based knowledge bases are special in the follow-ing aspects: 1. Data-driven knowledge bases contain more fine-grained 2. Information in data-driven knowledge bases is not black
In this paper, we are concerned with basic-level catego-rization. Many basic-level concepts, such as high-end smart-phone , software company , and theoretical physicist , are fine-grained concepts that are not present in manual-crafted knowl-edge bases. Furthermore, inferring basic-level concepts re-quires the statistical information associated with the knowl-edge. Therefore, we choose data-driven knowledge bases for inferring basic-level concepts.

We use Probase 2 [32] to provide us fine-grained concepts and their statistics, but our techniques can be applied to other knowledge bases which meet above requirements, such as KnowItAll and NELL. Probase is acquired from 1.68 billion web pages. It extracts isa relations from sentences matching Hearst patterns [12]. For example, from the sen-tence ... presidents such as Obama ... , it extracts evidence for the claim that Obama is an instance of the con-cept president . The core version of Probase contains 3,024,814 unique concepts, 6,768,623 unique instances, and 29,625,920 isA relations.
Pro base data is publicly available at http://probase.msra.cn/dataset.aspx In this section, we describe computational approaches for BLC. We start with two popular measures, namely typicality and PMI, and we show why they are insufficient to derive the basic-level category. Based on the insights from these measures, we introduce our approach, and also the intuition why our approach is a better mechanism for BLC.
Scoring is very useful for machines when they leverage knowledge bases. It can make machines do reasoning like hu-man beings. Without scores, knowledge facts in the knowl-edge base or semantic network are not easy to use. E.g. for the term apple , it may belongs to lots of categories, such as fruit, company, book, movie, and music track . For human be-ings, probably they will think of fruit or company when they see apple . For machines, if they do not have scores, they will treat music track as important as fruit or company . This makes machines cannot reason over the knowledge base as human beings. What is worse, according to our observations, all kinds of terms or phrases may be names of book , movie , or music track , etc. Without scores, machines will think all texts are related, because they all belong to these concepts. This leads to many errors when machines understand texts.
Typicality is an important measure for understanding the relationship between an object and its concept. Each cate-gory (concept) may contain one or more typical items, which are ideal or average examples that people extract from see-ing real examples [23]. For example, given a concept bird , people are more likely to think of robin than penguin , although penguin is also a bird . Similarly, when someone mentions Barack Obama without a context, we are more likely to think of the concept of president than author , al-though Barack Obama is actually a best-selling author of several books.

More specifically, we use P ( e j c ) to denote the typical grade of an instance e in concept c , and we use P ( c j e ) to denote the typical grade of a concept c for instance e . We call P ( e and P ( c j e ) typicality. For our previous examples, we have P ( robin j bird ) &gt; P ( penguin j bird ) and P ( president &gt; P ( author j Obama ).

How to compute typicality is an interesting and challeng-ing problem. Mervis et al. [21] found that frequency on its own does not predict typicality. For example, chicken is a bird , and it appears frequently in day-to-day conversations or in texts. But it is much less typical than some much less frequently encountered or discussed birds, such as robin . On the other hand, how often an item is thought of as being a member of a category can be used to measure its typi-cality [1]. For example, by leveraging Hearst patterns [12], we find chicken is used as an example of bird 130 times in a corpus, while robin is used 279 times. This result is consistent with the fact that robin is more typical than chicken as a bird . Based on the above observation, in our work, we derive typicality from co-occurrences of concept and instance pairs: where n ( c; e ) is the co-occurrence of concept c and instance e in Hearst patterns X  sentences from Web documents. On the first look, typicality can be used directly for BLC: For a given instance e , how likely is the concept c that max-imizes P ( c j e ) be the basic-level concept for e ? Or, how likely is the concept c that maximizes P ( e j c )?
In this section, we argue that neither is a good candidate for the basic-level concept. Let us go back to the Microsoft example we mentioned in Section 1. It is easy to see that company is a very typical concept for Microsoft , that is, when we think of Microsoft , we think of it as a com-pany . On the other hand, Microsoft is a very typical in-stance of the concept largest OS vendor , that is, when we talk about largest OS vendor , we are likely talking about Microsoft . In other words, c 1 = company has very large typicality P ( c 1 j e ) and c 3 = largest OS vendor has very large typicality P ( e j c 3 ), where e = Microsoft . But neither is an appropriate basic-level concept for Microsoft , for the rea-sons we described in Section 1. In fact, the two concepts represent two extremes: company is a very general concept for Microsoft , while largest OS vendor is a very specific con-cept. These two extremes have the following characteristics:
In other words, general concepts may be correct answers to a given instance, but they cannot distinguish different kinds of instances. On the other hand, specific concepts preserve more useful information about instances, but their coverage is limited. What we look for is a compromise of the two.
The typicality score we discussed above tends to give higher score to  X  X xtreme X  concepts, that is, concepts either very general or very specific. The reason is obvious: (1) when e is given, P ( c j e ) is proportional to the co-occurrence of c and e , so it tends to map e to some general concepts; (2) when e is given, P ( e j c ) tends to return those specific concepts that only contain e .

Usually, to address the problem of  X  X xtreme X  values, we can use the technique of smoothing. We define smoothed typ-icality as follows: where N instance is the total number of instances, and " is a small constant which assumes every (concept, instance) pair has a very small co-occurrence probability in the real world, no matter whether we observe it or not.

Smoothed typicality reduces extreme values. For example, assume originally we have P ( Microsoft j largest OS vendor ) = 1. In other words, Microsoft is the only instance in the category of largest OS vendor . Assume " = 0.001 and N instance = 1 M , we derive smoothed typicality according to Eq 2 as P ( e j c ) = (1 + 0 : 001) = (1 + 0 : 001 10 6 which is much smaller than the original value. This trans-lates to boosting the typicality of other concepts such as so ftware company , which makes it possible for us to choose a more appropriate basic-level category.

However, the smoothing approach has two problems. First, the results are very sensitive to " , which is difficult to tune. Second and more importantly, smoothing helps avoid ex-treme values using a rationale totally inconsistent with ours. For example, it assumes that there are many largest OS ven-dors for which we do not observe, and as a result it discounts P ( Microsoft j largest OS vendor ) to 0 : 001. But the fact is, Microsoft is indeed one of the very few largest OS vendor , and reducing its typicality to 0 : 001 does not make sense. Thus, smoothing does not address the fundamental chal-lenges in finding the basic-level category.
Pointwise Mutual Information ( P M I ) [20] is a common measure of the strength of association between two terms. We may consider using the P M I between concept c and in-stance e to find the basic-level concepts, that is, we consider c = arg max c P M I ( e; c ) as e  X  X  basic-level concept, where P M I ( e; c ) is defined as This leads to
P M I ( e; c ) = log P ( e For a given e , log P ( e ) is a constant. Then, ranking concepts by P M I is equivalent to ranking by typicality P ( e j c ). In other words, PMI is equivalent to typicality. But as we have already shown in Section 3.3, using typicality to find basic-level concept does not work.

In order to make P M I less sensitive to occurrence fre-quency and at the same time more easily interpretable, Bouma et al [4] proposes a normalized pointwise mutual information ( N P M I ). The N P M I between concept c and instance e is defined as: As we can see from Eq 5, similar to P M I and typicality, N P M I also tends to produce concepts either too general or too specific. More specifically, Neither typicality nor P M I produces basic-level concepts. Based on previous studies [24], we know basic-level concepts are concepts neither too general nor too specific. To this end, we make an intuitive compromise and define a scoring function Rep as follows: For an instance e , we use the above score to find its basic-level concept: Intuitively, our measure tries to boost such a concept c for a given instance e : i) concept c is e  X  X  typical concept, and ii) instance e is also c  X  X  typical instance.

Moreover, if we take the logarithm of our scoring function in Eq 6, we get This in fact corresponds to P M I 2 , which is a normalized form of P M I in the P M I k family [7]. P M I 2 is proposed in an attempt to investigate how to improve upon P M I by introducing the P ( e; c ) inside the logarithm. It normalizes the upper bound of P M I [4]. Therefore, P M I 2 can reduce the extreme values, and help to boost concepts in the middle of the taxonomy.

One important property of an instance X  X  basic-level cate-gory is that the category is more likely to contain its similar instances. In our Microsoft example in Section 1, we men-tioned that going through too general concepts (such as com-pany ) or too specific concepts (such as largest OS vendor ) we cannot find instances similar to Microsoft . If instead we go through a likely basic-level category (such as software company ) we find many similar instances. This inspires a graph traversal approach to finding basic-level categories. In the rest of this section, we show that a graph traversal approach is equivalent to our approach of maximizing the scoring function Rep in Eq 7.

As we mentioned above, given the instance e , the basic-level concept c should be one of e  X  X  typical concepts, in other words, c should have  X  X hortest distance X  with e . Similarly, given this basic-level concept c , e should have  X  X hortest dis-tance X  with c . Therefore, for an instance e , we consider the process of finding its basic-level concepts as a process of find-ing concepts that have shortest expected distance to e . Intu-itively, traversing from node e , it is very likely we arrive at the concept, and traversing from the concept, it is very likely we come back to e . This corresponds to a random walk problem of finding nearest nodes reached by a given node, as shown in Fig. 3. We use commute time [19] as a measure of the distance between two nodes in a graph. Commute time is defined as the expected number of steps that a random w alk starting at node i , going through node j once, and re-turning to i again. The commute time between an instance e and a concept c is:
T ime ( e; c ) = where P k ( e; c ) is the probability of starting from e to c and back to e in 2 k steps.

As we are only interested in concepts within a short com-mute, we may just ignore concepts with commute time larger a threshold of T steps. Let us constrain the random walk within 4 steps, then we have:
T ime  X  ( e; c ) = 2 P ( c j e ) P ( e j c ) + 4 (1 P ( c
This shows that the commute time T ime  X  ( e; c ) has an in-verse relationship with the scoring function Rep we defined in Eq 6. Thus, our simple, easy-to-compute scoring method is equivalent to a graph traversal approach of finding the basic-level category under this random walk assumption. But for the complete commute time, our experiments show that the representativeness score is better than it.
To evaluate the effectiveness of scores proposed in this paper, we conduct our experiments on the Probase [32] se-mantic network. The dataset we use is called  X  X ore X  version. It contains 3,024,814 unique concepts, 6,768,623 unique in-stances, and 29,625,920 edges among them.
Conceptualization is one of the essential processes for text understanding. It maps instances in the text to the concept space. In this evaluation, we select top queries from Bing search logs between January 1st, 2015 and January 31st, 2015, then we use the typicality P ( c j e ), P ( e j c ), and our ap-proach Rep ( e; c ) to rank concepts corresponding to these in-stances. Finally, we generate 1,683 (instance, concept) pairs as labeling candidates.
 We also compare our approaches with several baselines: M I , N P M I , P M I 3 . Since P M I is reduced to typicality P ( e j c ) in our scenario, we do not treat it as another baseline separately. The formulas of these baselines are as follows: Mutual Information (MI) : M I ( e; c ) =
Normal ized Pointwise Mutual Information (NPMI) : please refer to Equation 5.

We now discuss how we evaluate the results of different approaches. As there is not ground-truth ranking for concep-tualization results. We submitted a labeling task to UHRS (Universal Human Rating System), which is a crowdsourcing and human annotation platform. Each (instance, concept) pair is labeled by three workers. Therefore we got 5,049 la-beled records. We took the majority of the labels as the final label. If all three annotators did not agree with each other, this pair was sent to the fourth annotator to label, and so on. In this labeling task, the workers did not our labeling goal. They just strictly follow the guideline shown in Table 3. Lab el Me aning Examp les
Exc ellent Go od matched con-
Go od A little general or
F air T oo general or spe-Ba d No n-sense concepts (b luetooth, issue) T able 3: Labeling Guideline for Conceptualization
Then we employ precision @ K and nDCG to evaluate the results of different approaches. The precision @ K is used for evaluating the correctness of concepts, and nDCG is used to measure the ranking of concepts.
 For precision @ K , we treat Good/Excellent as score 1, and Bad/Fair as 0. We calculate the precision of top-K concepts as follows: where rel i is the score we define above.
 For nDCG , we treat Bad as 0, Fair as 1, Good as 2, and Excellent as 3. Then we calculate nDCG as follows: where rel i is the relevance score of the result at rank i , and ideal r el i is the relevance score at rank i of an ideal list, obtained by sorting all relevant concepts in decreasing order of the relevance score.
We show the comparison from two aspects: without smooth-ing, and with smoothing. Fig. 4 shows the top-20 results for different scoring approaches.

Without smoothing, as Fig. 4(a) shows, the score Rep ( e; c ) is much better in precision than others. Rep ( e; c ) is the best for top-2, top-5, and top-10. P M I 3 is also good for top-1, but it is worse for other cases. So it is not stable. For the ranking of top concepts, as Fig. 4(c) shows, Rep ( e; c ) outperforms all other methods in all cases.

With smoothing, we try different values of " , from 1e-3 to 1e-7 . We observe that the smoothing technique has a signif-icant effect on the typicality P ( e j c ), as shown in Fig. 5. For P ( e j c ), its optimized " is 1e-4 . With this smoothing setting, as shown in Fig. 4(b) and Fig. 4(d), the typicality P ( e outperforms all other methods in both precision and nDCG wh en K is 2, 3, 5, 10, and 15. However, as we mentioned in Section 3.4, the rationale of smoothed typicality is totally in-consistent with our problem. Its top concepts may be good, but not for the rest of concepts. Besides, it is quite sensitive to " , which is hard to tune. On the other hand, Rep ( e; c ) is still quite good, and it wins the precision @1 when its smoothing " = 1e-4 .

Based on this evaluation, we have the following conclu-sions:
Some examples are shown in Table 4. We find top concepts ranked by P ( c j e ) tend to be general. E.g. the top 1 con-cept of battery is item . Obviously, item is not a good con-cept to predict the instance X  X  features. On the other hand, concepts ranked by P ( e j c ) (without smoothing) tends to be specific. Though these concepts have greater power on pre-diction, they have limited coverage. This makes them use-less in the computation. E.g. when we compare two related instances with extremely specific concepts, they may have little overlapping information. In Table 4, concepts ranked by Rep ( e; c ) and P ( e j c ) with smoothing( " = 1e-4 ) are the best. They are the trade-off between general concepts and specific concepts.

However, as we mentioned in Section 3.4, " is difficult to tune, and the rationale of smoothed typicality is inconsistent with the problem we want to resolve in this paper. For ex-ample, largest OS vendor should not be assumed to contain many other instances for which we do not observe. There-fore, smoothed typicality may only be good for top concepts, instead of all concepts.
With the scores proposed in this paper, the knowledge base becomes usable for machines. In this subsection, we showcase one application leveraging the basic-level concepts. Definitely, more applications can be easily developed for dif-ferent scenarios.

Sponsored search is one of the most successful business models for search engine companies. It matches user queries to relevant ads. In reality, each ad is associated with a list of keywords. Advertisers bid for keywords, and also specify matching options for these keywords. One option is exact match where an ad is displayed only when a user query is identical to one of the bid keywords associated with the ad. Another option is smart match which is based on semantic relevance. Exact match targets exact traffic, but it is lim-ited since queries are various. Currently, smart match is the default option provided by mainstream search engines.
In smart match, search engines map the query to bid ad keywords. Since both are short texts, traditional bag-of-words approaches do not work well in this scenario. There-fore, we can leverage the knowledge base for this task in smart match.

For each short text, we first identify instances from the short text (query, or bid keyword), and map it to basic-level concepts with the score Rep ( e; c ). Then we simply merge the concept vectors of different instances in the short text, and get a single concept vector as the representation of this short text. Finally, we can calculate the semantic similar-ity between queries and bid keywords by comparing their concept vectors (E.g. using cosine similarity function).
We conduct our experiments on real ads click log of Bing (from January 1st, 2015 to January 31st, 2015). The exper-iment process is as follows: first, we calculate the semantic similarity score of (query, bid keyword) pairs for each record in the log; Second, we divide all scores into 10 buckets; Third, we aggregate the click number and impression number in the same bucket.

The overall results are illustrated in Fig. 6(a). X-axis rep-resents the bucket number (E.g. bucket 1 means the sim-ilarity score is between 0 and 0.1, and so on). Y-axis is the general click-through rate (CTR) of this bucket, where (1) once our semantic similarity score is low, the CTR is low; (2) once our score is high, the CTR is high; (3) the high-est CTR is about 3 times of the lowest CTR. This means our semantic similarity score can be a strong feature for the query and ads matching.

We further analyze our results by query frequencies. We separate all queries to 10 deciles in the order of frequency, and each decile has the same total volume of traffic. Gen-erally speaking, decile 1 to 3 are head queries, decile 4 to 6 are torso queries, and decile 7 to 10 are tail queries. Usu-ally, head queries can be covered by exact match. Therefore we mostly focus on torso queries and tail queries. Results are shown in Fig. 6(b) and Fig. 6(c). For both torso and tail queries, the correlation between our semantic similarity score and CTR is preserved. This is quite good because long queries usually are lack of click signals, and the semantics can fill this gap.
The goal of BLC is for text understanding. Currently, there are two major efforts on semantic analysis and text representation. One is implicit knowledge mining, the other is explicit knowledge mining.

In terms of implicit knowledge mining, some efforts focus on implicit semantic analysis and topic modeling, such as traditional latent semantic analysis (LSA) [8], probabilistic latent semantic indexing (PLSI) [13], and latent Dirichlet allocation (LDA) [2]. They can be also used for basic-level concept reasoning. However, the semantics mined by these approaches cannot be interpreted by human being. Besides, their computation is quite heavy, which makes them cannot be easily applied for large scale data. Other efforts try to get the term embedding by using deep learning techniques [6, 22]. This embedding can be treated as the representation for a given term in high-dimensional semantic space, and benefit lots of applications. But ususally the embedding is not good for those low frequency terms, and it is not easy to tune once its overall quality is bad.

In terms of explicit knowledge mining, explicit semantic analysis (ESA) [11] and conceptualization [25, 15] are the representative work. The former represents the meaning of texts by Wikipedia X  X  article titles. But it is still not the nat-ural way of human thinking, and the  X  X oncepts X  of a given instance are related articles instead of its categories. The latter is the scope of this paper. But previous work [25, 15] pays attention to short-text level conceptualization, while our paper focuses on a more fundamental part: instance-level conceptualization. A good instance-level conceptualization mechanism can further improve short-text level conceptual-ization.

For scores discussed in this paper, typicality and basic-level conceptualization are actively studied in cognitive sci-ence and psychology at first. E.g. psychologist Gregory Mur-phy X  X  highly acclaimed book [23] discusses the typicality and the basic level of concepts from the perspective of psychol-ogists, which is the basis of these two scores proposed in this paper. Other work [17, 32] proposes typicality scores for some special scenarios. Their scoring functions cannot be easily extended to semantic networks. E.g. Lee et al. [17] leverage instances as intermedia for calculating typicality P ( a j c ) between an attribute a and a concept c . Instead, our typicality is more generic and easy to be adopted by more applications. Besides, we also discuss a smoothing ap-proach and Rep score, which have been proved that they are much better than the standard typicality in instance-level conceptualization. Furthermore, we compare our BLC score function with PMI and commute time theoretically, which reveals the essence of BLC score. In this paper, we discuss computational approaches for Basic-level conceptualization (BLC) . We propose our method based on typicality and PMI . We make deep analysis and compare it with some popular measures to understand its essence. We conduct extensive experiments and show the ef-fectiveness of our approach. We also use a real example to demonstrate how our score helps current ads system. This work was partially supported by the National Key Basic Research Program (973 Program) of China under grant No. 2014CB340403 &amp; No.2015CB358800, the Natural Sci-ence Foundation of China(No.61472085), and the Funda-mental Research Funds for the Central Universities &amp; the Research Funds of Renmin University of China. [1] L. W. Barsalou. Ideals, central tendency, and [2] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [3] K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and [4] G. Bouma. Normalized (pointwise) mutual [5] A. Carlson, J. Betteridge, B. Kisiel, B. Settles, [6] R. Collobert, J. Weston, L. Bottou, M. Karlen, [7] B. Daille. Approche mixte pour l X  X xtraction de [8] S. C. Deerwester, S. T. Dumais, T. K. Landauer, [9] O. Etzioni, M. Cafarella, and D. Downey. Webscale [10] C. Fellbaum, editor. WordNet: an electronic lexical [11] E. Gabrilovich and S. Markovitch. Computing [12] M. A. Hearst. Automatic acquisition of hyponyms [13] T. Hofmann. Probabilistic latent semantic indexing. In [14] W. Hua, Z. Wang, H. Wang, K. Zheng, and X. Zhou. [15] D. Kim, H. Wang, and A. Oh. Context-dependent [16] G. Lakoff. Women, fire, and dangerous things: What [17] T. Lee, Z. Wang, H. Wang, and S.-w. Hwang.
 [18] D. B. Lenat and R. V. Guha. Building Large [19] L. Lov  X asz. Random walks on graphs: A survey. [20] C. D. Manning and H. Sch  X  utze. Foundations of [21] C. B. Mervis, J. Catlin, and E. Rosch. Relationships [22] T. Mikolov, K. Chen, G. Corrado, and J. Dean. [23] G. L. Murphy. The big book of concepts . MIT Press, [24] E. Rosch, C. B. Mervis, W. D. Gray, D. M. Johnson, [25] Y. Song, H. Wang, Z. Wang, H. Li, and W. Chen. [26] F. M. Suchanek, G. Kasneci, and G. Weikum. Yago: a [27] F. Wang, Z. Wang, Z. Li, and J.-R. Wen.
 [28] J. Wang, H. Wang, Z. Wang, and K. Zhu.
 [29] Z. Wang, H. Wang, and Z. Hu. Head, modifier, and [30] Z. Wang, K. Zhao, H. Wang, X. Meng, and J.-R. Wen. [31] Wikipedia. Plagiarism  X  Wikipedia, the free [32] W. Wu, H. Li, H. Wang, and K. Q. Zhu. Probase: A
