 Ontologies are being successfully used to overcome semantic het-erogeneity, and are becoming fundamental elements of the Seman-tic Web. Recently, it has also been shown that ontologies can be used to build more accurate and more personalized recommenda-tion systems by inferencing missing user X  X  preferences. However, these systems assume the existence of ontologies, without consid-ering their construction. With product catalogs changing continu-ously, new techniques are required in order to build these ontologies in real time, and autonomously from any expert intervention.
This paper focuses on this problem and show that it is possible to learn ontologies autonomously by using clustering algorithms. Re-sults on the MovieLens and Jester data sets show that recommender system with learnt ontologies significantly outperform the classical recommendation approach.
 H.1.2 [ Information Systems ]: User/Machine systems Algorithms, Experimentation, Performance.
 Ontology, Recommendation System, Clustering.
With the growing importance of the internet, people are becom-ing overwhelmed by information. Recommender systems have been designed to help people in this situation by finding the most rele-vant items based on the preferences of the person and others.
Today, the most widely used technique is item-based collabora-tive filtering , ([10], CF). Given a user, the goal of CF is to recom-mend items based on the experience of the user as well as other similar users. Collaborative filtering captures the user X  X  experience by asking him to rate the items he has interacted with, and stores all these historical data in a user-item matrix R .
 Copyright 2007 ACM 978-1-59593-609-7/07/0008 ... $ 5.00.

Unfortunately, collaborative filtering suffers from scalability[10] and cold-start problems[11]. The former problem is due to the neighborhood of items that must be constructed in order to extract the experience of similar users, while the latter is a consequence of the unconstrained search space that requires many items to be rated in order to find the right neighbors.

To constrain the search space, a novel technique called ontology filtering has been proposed([12], OF). This approach infers pref-erence ratings of items based on the ratings of known items, and their relative position in an ontology. The ontology is defined as a multi-inheritance graph structure, where an edge represents one or more features, and an item is an instance of at least one con-cept. Given this ontology, OF associates with each concept c an a-priori score , APS(c), which captured the information contained by a concept. To predict the user X  X  rating of an unrated concept y , the system uses knowledge of the normalized rating S ( x ) of the closest concept x , and the lowest common ancestor to concepts x and y in the ontology. Formally, the inference is as follows: where lca is the lowest common ancestor between concepts x and y . Informally, Equation 1 infers the score of concept y by first looking at the closest concept x that has a score. Then, it looks at the commonalities between the concept x and lca , which measures how much information is preserved going up the tree. Finally, we add the information gained by adding the extra features between concepts lca and y . Thus, the recommended items are the ones achieving the best score. Note that contrary to CF, OF only uses the user X  X  preferences for the inference process.

To illustrate these two approaches, we are going to build a sim-ple recommendation system for helping a user choose a means of transportation. Figure 1(a) shows the matrix R that contains the preferences of three users: David, Paolo, and Alex. Such a ma-trix is the input of traditional recommendation system. The ratings range from 1 to 5, and the symbol x means that no preferences were stated by the users. Let X  X  now imagine that Alex is planning to visit one of his friends who lives far away. Further imagine that to get there, Alex can only use a car or the train.
 Figure 1: (a) matrix R , while (b) is matrix S computed from (a)
To use item-based collaborative filtering, the system starts by constructing the item-item similarity matrix S (Figure 1(b)). This matrix is constructed from R using Equation 3 (see section 2.1), where S i,j is the similarity between items i and j . Given Alex, the predicted rating of an item i is computed using a weighted av-erage of Alex X  X  ratings by the similarity of closest neighbors to i . Formally, the predicted rating of item i ,  X  R Alex,i , is computed as follows[8]: where K is the set of the k -closest neighbors to item i . If we set | K | to 2, the predicted ratings of items train and car become 5 (i.e.:  X  R Alex,train =  X  R Alex,car = 5 ). As both items have identical predicted ratings, CF would recommend both to Alex.
 Figure 2: (a) An ontology modeling the Transport domain with the a-priori score of each concept in (b)
On the other hand, ontology filtering needs an ontology to in-fer missing user preferences. Figure 2(a) shows a possible hier-archical ontology that was designed by engineers and fed as in-put to our recommendation system. Given this ontology, the in-ferred score of the concept Train is equal to S ( Train | Plane ) = S ( Plane )  X  ( APS ( Public ) /APS ( Plane ) + ( APS ( Train )  X  APS ( Public )) = 7 / 12 , while the one of the concept Car is equal to S ( Car | Bike ) = S ( Bike )  X  ( APS ( Transport ) /APS ( Bike )+ ( APS ( Car )  X  APS ( Transport )) = 23 / 54 . As S ( Train | Plane ) &gt; S ( Car | Bike ) , the recommended item will be the train, .
Given Alex X  X  preferences, it is quite easy to see that the predic-tion made by ontology filtering is more coherent than the one from CF; as the majority of users who like to travel by public transport would also prefer to travel by train rather than by car. This behavior is represented by the fact that both concepts Plane and Train share the same ancestor, which is different from the ancestor of the con-cept Car. Moreover, if Alex liked to travel by car, then he would have stated it; as users tend to state the preferences they really care about. Note also that ontology filtering uses less data than collab-orative filtering as the structure of the ontology limits the search space to the closest concept with a rating.

In E-commerce applications, many items with new features are being constantly added to E-catalogs. Inversely, old items are re-moved from the E-catalogs as they become obsolete. In such sit-uations, it is unrealistic to imagine that such an ontology can be maintained by a group of experts. Furthermore, the ontology does not exploit the similarities as expressed by the matrix S . Finally, we think that using the same ontology for every user is suboptimal as each user perceives reality differently from others.

This paper focuses on the problem of learning these ontologies using unsupervised learning algorithms. Given this set of ontolo-gies and a particular user, we propose another algorithm to select which ontology to use based on the user X  X  preferences. We show that hierarchical clustering algorithms can be used to learn the on-tologies, and that ontology filtering with these learnt ontologies still outperforms item-based collaborative filtering. We also de-fine a new algorithm that is capable of building multi-hierarchical ontologies, and show that this more complex structure brings some robustness to the prediction accuracy. Finally, we provide fresh experimental results concerning the behavior of hierarchical clus-tering algorithms and ontology filtering recommendation system on two data sets that contain real users data (MovieLens and Jester).
The rest of this paper is organized as follows. First, we intro-duce all the necessary background that is used throughout the pa-per, while our algorithms used to learn and select which ontology to use are presented in Section 3. In the experiment section, we give detailed results of the behavior of learnt ontologies and show that these learnt ontologies actually outperform item-based collab-orative filtering on the Jester and MovieLens data sets. Finally, we conclude this paper in Section 5.
In both collaborative filtering and ontology filtering, users state their preferences by rating a set of items, which are then stored in the user-item matrix R . Formally, this matrix contains all the users X  profiles, where the rows represent the users U = { u 1 , , u columns correspond to the set of items I = { i 1 , , i n } , and R the rating assigned to item i by the user u . It is common practice to denote the average rating of user u by  X  R u , and the average rating of the item i by  X  R i .
Item-based collaborative filtering works by finding similar items to the ones bought by the user, and then combines those similar items into a recommendation list. The fundamental assumption be-hind CF is that similar users like similar items. Formally, the rec-ommendation process is as follows. First, CF computes the pair-wise similarities between all the items in the matrix R . These sim-ilarities are computed using the adjusted cosine similarity metric, which is defined as follows[10]:
Once the item-item similarity matrix S has been computed, the predicted rating of an item i is computed using the k most similar items to i , which is commonly known as i X  X  neighborhood . Thus, the predicted rating is computed using a weighted average of the user X  X  ratings by the similarity of item i  X  X  neighborhood (Equation 2). Once all the possible items to be recommended have been rated, we simply select the N items with the best predicted rating. This selection process is called the top-N recommendation strategy.
By working on the items rather than on the users, CF is able to reduce the search space, which makes it scalable to E-commerce environments. For example, Amazon.com with its 29 million cus-tomers uses item-based collaborative filtering to personalize its rec-ommendations([7]). However, with thousands of items in current E-commerce catalogs, the search space remains huge and uncon-strained. Thus, CF requires the user to rate many items in order to find highly correlated neighbors. Moreover, the recommendation accuracy is greatly influenced by the size of the item X  X  neighbor-hood.
In ontology filtering, two input structures are required: the users X  historical data R and an ontology modeling the domain. The on-tology is defined as a directed acyclic graph, where an edge is a bi-nary isa relation that models some features. Note that the features are usually not made explicit when defining the ontology. Take for example a bottle of red and a bottle of white wine. Both bottles would be children of the wine concept, and the color of the wine would be the feature that differentiates the red wine sub-concept from the white wine one. However, the taste would also be a fea-ture that differentiates these two subconcepts, but this feature will remain implicit.

Ontology filtering starts by extracting the information contained in the ontology. This is achieved by computing an expected normal-ized score of each concept c , known as the a-priori score , APS ( c ) . The APS is defined as a lower bound of the actual score an instance of the concept might have. We define the APS of a concept c as: where n c is the number of descendants of the concept c [12].
As for collaborative filtering, the user expresses his preferences by rating a set of items. For each rated item, the algorithm extracts its features and computes the score of each concept based on the specified rating. Unfortunately, it is very unlikely that the user was able to give enough ratings to compute the score of all the concepts in the ontology. To overcome this problem, for all the concepts y without any score, OF will infer it by transferring the score of the closest concept x (Equation 1).

In Equation 1, the coefficient  X  ( x, lca ) = APS ( lca ) /APS ( x ) represents the ratio of features which are both liked in the concept x and lca , while  X  ( y, lca ) = APS ( y )  X  APS ( lca ) measures how much the new features added to the lowest common ancestor will be liked[12]. The former coefficient represents the score being preserved when traveling upwards to the concept lca , while the latter contains the score gained by traveling downwards from the lca . Note that the inference assumes that the relationships between real scores are the same as those between a-priori scores.
To minimize the error during the inference, OF needs to find the closest concept x to any given concept y in the ontology. To perform this crucial task, Schickel and Faltings[12] derived a new distance function from Equation 1 called OSS . In [13], OSS was re-fined and it was showed that it actually outperforms state of the art metrics on WordNet and the GeneOntology. Formally, OSS defines the distance between concepts x and y in an ontology as follows: where  X  and  X  are the same coefficients as in Equation 1.
There are three fundamental differences between ontology filter-ing and collaborative filtering. First, rather than using the item-item similarity matrix S , the item similarity in ontology filtering is re-stricted to a hierarchical ontology. This reasoning topology allows to constrain the search space, which limits the number of ratings the system has to elicit from the user. Experimental results have shown that ontology filtering leads to significant improvements in the pre-diction accuracy over CF, especially when little data about the user is known [12]. Second, ontology filtering infers the missing score from the closest concept with preferences rather than from a neigh-borhood. As less data is required, it makes this approach much more scalable than collaborative filtering. Finally, missing user preferences are inferred from the user X  X  past experience instead of using other people X  X  preferences, which increases the personaliza-tion of the recommendations list[12].

The main assumption in ontology filtering is that an ontology modeling the items X  features exists. Unfortunately, this is an un-reasonable assumption when we consider the fact that hundreds of items get added or removed from E-catalogs each day. Further-more, the ontology should capture the item similarities as expressed by the user population. Thus, new techniques are required for building these ontologies automatically, and without any user in-tervention. Moreover, collaborative filtering proved that using col-laborative data increased the prediction accuracy. Thus, we show that it is possible to construct these ontologies from the matrix R using clustering algorithms, which allows the ontologies to capture the knowledge of a user population.
Users are being continuously solicited to express ratings on items they have either seen or bought. As a consequence, rated items are becoming widely available. For example, buyers on eBay.com are invited to evaluate a seller each time they have bought an item, while on YouTube.com, users can rate videos they have seen. Mi-crosoft Office also invites users to rate the help tips they have re-ceived. Given these ratings, our first contribution is to show that hierarchical clustering algorithms can successfully be used to learn a set of hierarchical trees that will later be used as ontology in the ontology filtering approach.
Over the years, researchers in the data mining community have proposed many clustering algorithms in order to perform unsuper-vised learning. These algorithms can be classified into at least six categories[6]: fuzzy clustering, nearest-neighbor clustering, hierar-chical clustering, artificial neural networks for clustering, statistical clustering algorithms, and density-based clustering. In this paper, we are interested in building hierarchical ontologies. Thus, we will focus our research on hierarchical clustering algorithms, which can build such a structure.

Hierarchical algorithms can in fact be categorized into two sub-categories: distance-based clustering and conceptual-based clus-tering . Both approaches construct hierarchical trees, but they use very different data representation. With distance-based clustering, objects are represented in a well defined space, like a vector in a 2D cartesian space. Thus, two or more objects will be assigned to the same cluster if they are close according to a given distance func-tion. However, with concept-based clustering, objects are defined by a set of concepts, where a concept is usually an attribute-value pair. Given this representation, two or more objects belong to the same cluster if they share common concepts.
When considering distance-based clustering, there are in fact two distinct ways to build a hierarchical tree: the bottom-up or top-down approach. These approaches are respectively known as the agglomerative clustering and partitional clustering .

Partitional algorithms start by assigning all the items to be clus-tered into a unique cluster. Then, one cluster C j is chosen and is then further bisected into C i , where i  X  [1 , 2] clusters. For each item in C j , we assign them to the cluster C i that optimizes a dis-tance function. This process continues until either all the items are found on the leaf of the tree, or the number of clusters has met a given threshold  X  .

Inversely, agglomerative clusterings assign each item to its own cluster C i ,where i  X  [1 , n ] . Then, the two closest clusters are merged into a unique cluster. As for partitional clustering, the pro-cess reiterates until the entire tree is created, or the number of clus-ters has met a given threshold  X  .

The main advantage of partitional algorithms is their low com-plexities, which allow them to cluster millions of elements. How-ever, partitional algorithms can suffer from local minima, and are dependent on the input order of the items. On the other hand, in general, agglomerative algorithms give better clustering solutions than partitional algorithms. However in [15], Zhao et Karypis have shown that for clustering document data sets, partitional clusterings always led to better clustering solutions than agglomerative ones. The main disadvantage of agglomerative clusterings is their com-plexities. In the first step of the algorithm, all pairwise similarities must be computed, leading to a complexity of O ( n 2 ) .
The most famous (incremental) conceptual clustering algorithm is COBWEB , which was introduced by Fisher[3] in 1987. Contrary to the first two clustering algorithms, items need to be represented by a set of attribute-value pairs. For example, a mammal could be modeled by the attributes body cover, heart chamber and body temperature, which have the following respective values: hair, four, and regulated. Given this representation, a node (class) in the tree represents the probability of the occurrence of each attribute-value pair for all the instances of that node. Thus, the root node repre-sents all the possible attribute-value pairs defined in the system, and nodes become more specific as we go down the tree.

For each item to classify, COBWEB will incrementally incor-porate it to the classification tree by descending the tree along an appropriate path, updating counts along the way, and performing one of the four operators at each level. These operators are as fol-lows.
As item are added incrementally, the ordering of the initial input can lead to different classification results. To reduce this problem, [3] uses the operators merge and split . Furthermore, node merging and splitting are roughly inverse operators, and allow COBWEB to move bidirectionally through a space of possible hierarchies.
The choice of the operator will by be guided by a category utility function that rewards traditional virtues held in clustering, i.e.: sim-ilar objects should appear in the same class, while dissimilar ones should be in different classes. Thus, the category utility function is in fact a trade off between intra-class similarity and inter-class dis-similarity of items. A detailed explanation of category utility can be found in [3].

One of the main advantages of COBWEB over the partitional and agglomerative clustering algorithms is that it allows new items to be added incrementally to the classification tree, without recom-puting the entire tree. Note also that the process is bidirectional, which means that a node that has been created can be merged again later on. Second, conceptual clustering uses probabilistic descrip-tions of concepts, rather than distances. Furthermore, the opera-tors merge and split guarantee homogeneity of the content of the class. However, COBWEB has some known problems. First, the classification tree is not height-balanced which leads to space and time complexity to degrade dramatically. The overall complexity of COBWEB is exponential to the number of attributes, as the cate-gory utility function requires analyzing all the attribute-value pairs.
Despite these problems, and because most authors assume that the number of attributes is small, COBWEB is becoming increas-ingly popular in the Semantic Web. For example in [1], Clerkin et al. proposed to use COBWEB directly on the user-item matrix R to build meaningful hierarchical ontologies of songs. To do this, they consider each item as an attribute, and the rating assigned to each item is transformed in the feature value good if it had a rating su-perior or equal to 4, or bad otherwise. In [14], Sea and Ozden have used COBWEB to generate the ontology from files X  description in order to perform ontology-based file naming.
In the collaborative filtering research community, it is an estab-lished fact that users can be categorized in different communities, and that a community of users behaves in a similar fashion. Follow-ing these observations, we believe that using just one ontology for all the users is not appropriate, and that it is better to select which ontology to use based on the user X  X  preferences. Following this, we will use many clustering algorithms to generate a whole set of ontologies  X  , and then select the best ontology based on the user X  X  preferences.

In order to generate these ontologies, we will use different dis-tance functions or criteria functions ([15]). Table 1 shows the crite-ria functions that we use in our algorithms, where k is the number of clusters to consider, n r denotes the number of elements in clus-ter r , C r is the r th cluster, C t r is the centroid of the r sim ( i, j ) is the similarity between items i and j .

Table 1: Criteria functions used for distance-based clustering 1 I 1 maximize 2 I 2 maximize 3 E 1 minimize 4 G 1 minimize 5 H 1 maximize I 1 E 6 H 2 maximize I 2 E
During partitional clustering, a distance function is required in order to assign each item in C j to the cluster C i the distance function. Thus, our algorithm will use the criteria I I , E 1 , H 1 , and H 2 as distance functions. I 1 and I 2 nal criteria functions that focus on producing a clustering solution that optimizes the function over the items of each cluster individu-ally. The essential difference between I 1 and I 2 is that the former maximizes the average pairwise similarities between all the items assigned to each cluster, while the latter represents each cluster by a center of gravity, known as centroid , and then looks at the simi-larity between the items and this centroid. Notice that I equivalent to the very popular K-Means algorithm, with K set to 2. However, E 1 is known as an external criterion function as it looks on how the various clusters are different from each other, and tries to minimize the cosine between the centroid of each cluster. G graph based criterion function that models the items as a graph (the node corresponds to an item, while an edge between a pair of nodes measures the similarity between each of these nodes), and uses a variant clustering quality measures (the min-max cut,[2]) as the cri-terion function. Finally, H 1 and H 2 are hybrid criteria functions that simultaneously optimize multiple individual criteria functions. Many techniques have been proposed for selecting which clusters to choose next for bisection, ranging from random policy, to size analysis. In order to obtain a more natural hierarchical solution, [15] proposed to choose the cluster among the k possible choices as the one that leads to the k +1 clustering solutions that optimized one of the above criteria functions.

The key criterion in agglomerative clustering is the function used to merge a pair of clusters. Table 1 shows three criteria functions commonly used today. The single-link , slink, and complete-link , clink, criteria functions both compute the similarities by consider-ing a pair of items in the different clusters. The main difference lies in the fact that the single-link considers the closest elements of the two clusters, while the complete-link looks at the furthest pair. However, these criteria functions do not perform well in practice because they only use limited information. The UPGMA criterion function overcomes these problems by measuring the similarity of two clusters as the average of the pairwise similarity of the items in each cluster. Besides these three functions, the first six functions in Table 1 can also be used for selecting which clusters to merge[15].
Formally, Algorithm 1 generates a set of hierarchical ontologies from the user-item matrix R as follows. First, we initialize a set  X  that will contain all the learnt ontologies. In step 2, we compute the item-item similarity matrix S from the matrix R using Equation 3. Using S i,j as sim ( i, j ) , and a threshold  X  as the number of leaf clusters, we generate 15 distinct hierarchical trees using the partitional (step 4) and agglomerative clustering (step 6) algorithms introduced in subsection 3.1.1.
 Algorithm 1 Learning the ontologies  X  with  X  leaf clusters Input : The user-item matrix R , and  X  leaf clusters.
 Output : A set  X  containing 15 ontologies 1.  X  =  X  2. Compute matrix S from user-item matrix R using equation (3) 3. For criteria function 1 to 6 do 4.  X   X   X   X  tree generated by partitional clustering. 5. For each criteria function 1 to 9 do 6.  X   X   X   X  tree generated by agglomerative clustering. 7. return  X 
If the threshold parameter  X  is set to any value less than the to-tal number of items to cluster, then some leaf clusters will contain more than one item. As users can express a rating on any item, it may well occur that a concept has more than one item with different scores assigned to them. In the model defined by [12], each con-cept is represented by a unique feature, rather than a set of them. Furthermore, the score is defined as a lower bound on the expected likelihood of the user liking the concept. This means that for a user to like a concept, he must also like its subconcepts. However, how the score of a concept is computed has not yet been prop-erly defined. Following our ontology construction, a concept will represent a set of items, each having a possible rating assigned by the user. In the literature [9], these concepts can be referred to as messy concepts . These families of concepts have three character-istics: they have gray areas of interpretation (open-textured), they change (non-stationary), and they have exceptions (non-convex). Formally, exceptions in concept can occur if a negative example resides in the concept X  X  interior. To solve this problem, we propose to compute the score of a concept as an average of the user X  X  rat-ings assigned to the concept X  X  instances. Formally, the score of a concept c for a user u is computed as follows: where LS is the set of items that have been rated by the user u , LSC is a subset of LS and contains the rated items that are instance of concept c , and e R u,i is the normalized rating of item i made by the user u . Notice that the average computation will compensate the fact that some items (exceptions) have been incorrectly classified.
The inference process defined in Equation 1 needs to find the closest concept with a score to the one we want to find the score in order to minimize the error. Furthermore, as the inference is done from only one concept, the selection of the closest concept is fundamental. As users tend to like similar items, this implies that theses items will be represented by concepts in the ontology which are close to each other. Thus, if the concepts that represents the items liked by the user are too distant from the disliked ones, then the inference will introduce a bias towards the liked concepts. As a consequence, we must select the ontology that minimizes the distance between the liked concepts and the disliked ones. Algo-selects a subset of ontologies that we will perform the best, and then select the ontology that minimizes the distance between liked and disliked concepts for the selected ontologies. The first stage of the algorithm simply limits the search of the right ontology as the computation of distances is computationally expensive.
 Algorithm 2 Selecting the best ontology from  X  for user u Input : The set of ontologies  X  , and u  X  X  learning set LS Output : The best ontology  X  i for user u . 1. Split u  X  X  learning set LS into LSL and LST . 2. For each  X  i in  X  do 3. Predict the score of items in LST based on items in LSL . 4. prec i (  X  i )  X  precision of the items in LST 4.  X  2  X  ontologies with best prec i (  X  i ) 5. For each  X  i in  X  2 do 6. compute Dist (  X  i ) = 7. return  X  i | arg  X  i  X   X  2 min ( Dist (  X  i ))
Algorithm 2 starts by splitting the user X  X  preferences LS into two sets: LSL and LST . LSL will be used for learning the scores, and will contain 90 % of the data in LS . For each ontology we have learnt using Algorithm 1, we infer the score of the items in LST based on the score of the items in LSL . Then, we compute the pre-cision of the inferred scores based on the real score found in LST , where the precision is defined as the ratio of correct items found by the size of LS T . In step 4, we select the ontology that achieves the best precision. This selection process is as follows. If we have at least two ontologies with the highest precision value, then we return these ontologies. Otherwise, we select the best ontologies and also the ontologies with the second best precision value. This selection process ensures that we always have at least two ontolo-gies for the rest of the algorithm. For each selected ontologies  X  and using Equation 5, step 6 computes the distance between the concepts liked by the user and the disliked ones in the user X  X  learn-ing set LS , where n and m are concepts respectively from the set of concepts Liked that are liked by the user, and from the set of concepts Disliked that he disliked. Note that LS is a partition composed of the sets Liked and Disliked . Finally in step 7, we return the ontology that minimizes the distance.

To summarize, Algorithm 1 uses distance-based clustering algo-rithms to learn a set of ontologies that capture the pairwise simi-larities between the items. Given these ontologies, Algorithm 2 se-lects the personalized ontology that represents best the user X  X  pref-erences. Finally, ontology filtering will use the selected ontology to infer the missing preferences, and recommend a set of N items to that particular user. Consequently, we will call this new approach Personalized Ontology Filtering , pOF.
We believe that the recommendation accuracy can be further im-proved if we slightly increase the search space. When using clas-sical hierarchical clustering algorithms to generate the ontologies, all the implicit features between a concept and its sub-concept will be stored in a single edge. This could potentially limit the concept representation, and thus limit OF X  X  inference process. Moreover, hierarchical clustering algorithms used in Algorithm 1 always se-lect the best cluster to merge/split based on the optimization of one of the criterion function in Table 1. Thus, it ignores other possible suboptimal candidates.

In the next section, experimental results show that, on average, it is the agglomerative clustering with the complete-link criterion function that achieves the best result. Algorithm 3 extends this al-gorithm in order to build multi-hierarchical ontologies.
 Algorithm 3 Learning a Multi-Hierarchical Ontology with a win-dow size  X  and threshold cluster  X  Input : A set of items I , and the similarity matrix S ,  X  , and  X  . Output : a clustering tree modeling a multi-hierarchical ontology. 1. C  X   X  2. Assign each item i to its own cluster C i and C = C  X  C 3. while | C | &gt;  X  do 4. X  X   X  5. For each C i , C j in C do 8. C k  X  merge( C i , C j ) 9. C m  X  X  C i , C j } 10. C  X  C/C m 11. window  X  x i,j  X   X x i,j ; X  X   X  12. For each C p in C m , C q in C do 17. C  X  C  X  C k
The first 10 steps and the step 17 of Algorithm 3 are in fact the classical agglomerative clustering with the complete-link criterion function, where the complete-link criterion function is being used in step 6. Given a coefficient  X   X  [0 .. 1] as input parameter, step 11 computes a window size of acceptable clusters based on the value of the criterion value and  X  . Given this window size, we look at all the possible pairs of clusters that can be made with one of the merged cluster C i or C j , and see if its criterion value is within the window size. For each of these pairs, we merge them into a unique cluster and add it to the list of open clusters C . Notice that x p ,C q cannot be bigger than x C i ,C j as the pair C i C the criterion function in step 6. As cluster C p has a already got a parent (i.e.: C k ), step 15 makes cluster C r another parent of C
To evaluate our algorithms, we performed two identical experi-ments on two famous data sets: MovieLens 1 and Jester 2 . Movie-Lens is the most famous data set in the recommendation system community; it contains the ratings of 943 real users on at least 20 movies. There are 1682 movies in total, which can be described by 19 themes: drama, action, and so forth. Jester is another famous data set that contains the users X  ratings on jokes collected over a pe-riod of 4 years. The data set contains over 4.1 Million ratings and is actually split into three zip files: jester-data-1.zip, jester-data-2.zip, jester-data-3.zip. In this paper, we only used the first data set; it contains already 24,983 users on all the 100 jokes.

These sets were used for three reasons. First and most impor-tantly, they are the most widely used data sets, which makes it easy to reproduce the experiments. Second, both of those data sets con-tain (real) rated items which are necessary for filling in the user-item matrix R . Finally, these sets are very different in content and sparsity, where the sparsity is defined as the fraction of entries in the matrix R without values [10] over the total number of possible entries. For example, MovieLens contains movies that can easily be defined with some features such as the theme, duration, and so forth. However, Jester is made up of jokes, which is much harder to describe and thus is an ideal candidate to test our ontology learn-ing algorithm. Notice also that the sparsity of MovieLens(0.937) is much greater than the one of Jester(0.447).

The experiment set up was as follows. First, we selected all the users who rated at least 65 items, and used the remaining one to populate the user-rating matrix R . After removing users with less than 65 ratings, we are left with 407 users in MovieLens and 6199 users for Jester. For each remaining user, we randomly separated its ratings into two non-overlapping sets IS and TS . The intermediate set IS contains exactly 50 ratings, while the set TS contains the remaining ratings.

The intermediate set IS is used to study the behavior of the rec-ommendation algorithms with different amounts of rating data to learn the model. In the first case, we extracted 5 ratings from IS that we put into a learning set LS , in order to simulate the case when few preferences about the user are known. In the other situa-tion, we used all 50 ratings in IS to see how the algorithms behave when a lot of ratings are available for learning the model. The test-ing set TS is used for computing the prediction using the top-N recommendation policy, with N set to 5.

For these experiments, we used the toolkit CLUTO 3 version 2.1.1 that implements all of the partitional and agglomerative clustering algorithms used by Algorithm 1. For the COBWEB algorithm, we used the java package WEKA 4 version 3.4.
Many metrics have been proposed for evaluating the accuracy of the predictions made by recommendation systems. The most famous metric used to be the Mean Absolute Error, (MAE, [10]), which evaluates the accuracy by measuring the mean average devi-ation between the expected rating and the true rating. Later on in [5], it was argued that this metric was in fact not very accurate when considering rated items for users, as a user is usually interested to know if he X  X  going to like the item or not. As a consequence, [5] proposed to use the classical information retrieval metrics: preci-sion and recall . Given a recommendation set RS , precision is de-fined as the ratio of relevant items N ok by the total number of items shown in RS , while recall is defined by the ratio of relevant items to the total number of relevant items available in the database, N http://www.cs.umn.edu/Research/GroupLens/data http://www.ieor.berkeley.edu/  X  goldberg/jester-data/ http://glaros.dtc.umn.edu/gkhome/cluto/cluto/download http://www.cs.waikato.ac.nz/  X  ml/weka/
There are two challenges when using precision and recall for evaluating the accuracy of recommendation systems. First, preci-sion and recall need to be considered as a whole to fully evaluate the performance. Second, it has been observed in many applica-tions that precision and recall are in fact inversely related. Thus, we need to use a metric that is able to combine both precision and recall. As a consequence, and following the results in [5], we use the F1 metric to evaluate the accuracy of a recommendation sys-tem. This metric combines the precision and recall into a harmonic mean ranging from 0 to 1, where 0 occurs when both precision and recall are null. Inversely, the value 1 only happen if both precision and recall are both equal to 1. Formally, the F1 metric is defined as follows:
The main objective of our learnt ontologies is to help recom-mender systems to correctly recommend items. Thus, we will also use the F1 measure on the recommended items to evaluate the qual-ity of the learnt ontologies.
In this experiment, we studied the behavior of the ontologies generated by Algorithm 1, and their efficiency as ontologies in the personalized ontology filtering approach. Following Algorithm 1, we analyzed all the possible combinations made by the criteria functions given in Table 1. Thus, we obtained 15 different ontolo-gies: 6 using the partitional approach, and 9 with the agglomerative approach. Given these ontologies, we implemented pOF with Al-gorithm 2 to see whether or not the ontology personalization helps to increase the prediction accuracy. Note that the COBWEB algo-rithm was use to benchmark the learnt ontologies. Table 2 summa-rizes the various algorithms used in this experiment.

Each learnt ontology was then fed as input ontology to the ontol-ogy filtering algorithm, and prediction on all the users were made. To measure the quality of the ontologies, we used the prediction accuracy defined in Equation 8 (i.e.: the F1 metric). The under-lying assumption is that a good ontology should generate good recommendations, and thus increase the prediction accuracy. The results obtained by the different criteria functions significantly di-verge within the same family of algorithms (i.e.: partitional or ag-glomerative). As a consequence, we thought that using a simple average measure over all the criteria would induce too much bias. Especially when we consider the fact that there are more agglom-erative algorithms than partitional. To simplify the understanding, we transformed the F1 results into relative F1 value , rF 1 . This is done by dividing the F1 results by the maximum possible value be-tween the 15 possible ontologies. Notice that the relative F1 value is very similar to the relative Fscore introduced in [15]. To have a better understanding of the behavior of each ontology, we tested it with different numbers of leaf clusters, ranging from 5 to the num-ber of items available. Notice also that such a detail study was not performed by [15]. Figure 4 and 5 display the relative F1 values obtained on the Jester data set. First, we can see that on average, pOF using Algo-rithm 2 to select the best learnt ontology performs better than any of the simple clustering algorithms ( rF 1 = 1 . 0355 and rF 1 = 1 . 0469 ), whatever the size of the learning set. This tends to confirm our intuition that not one ontology is strictly better than the others, but rather that some users will reach a better accuracy with a given ontology. Notice that having a relative score superior to 1 for pOF is not a mistake. It is due to the fact that the maximum is computed on the 15 simple clustering ontologies only. Another important re-sult is that the ontology learnt by COBWEB is the worst performing ontology. The main reason for this lies in the definition of COB-WEB: it is a conceptual clustering algorithm. As a consequence, items need to be defined by a set of attribute-value pairs. In our E-commerce context, this data is unavailable as we only have the user-matrix R . This is also an indication that the technique [1] of transforming the item-to-item similarity matrix into attribute-value pairs is not suitable in this domain. When 5 items are used for learning the user X  X  score, the partitional clustering with the i2 crite-ria function (P-i2) has the best average relative score. Then, parti-tional clustering with the h1 function (P-h1) becomes the best algo-rithm when 50 items are used for learning the data set. This tends to go in the direction of Zhao X  X  conclusions [15] that partitional clus-tering algorithms perform better than agglomerative ones. Notice the evolution of the clustering P-g1 with the number of clusters. If only very few clusters are created, then P-g1 performs badly. How-ever, when we have many clusters, P-g1 performs really well which tends to show that the graph criterion requires many leaf clusters to generate a good ontology. When the number of clusters are set to either 40, 60, 80, and LS to 5, then interestingly agglomerative al-gorithms do perform better than partitional clustering ones. This would indicate (and the results with MovieLens confirm this) that agglomerative clustering needs enough leaf clusters to perform cor-rectly, while partitional algorithm seems better with less clusters. It makes sense when we consider the fact that partitional algorithms are top down approaches that recursively split clusters. Thus, too many splits may degrade the ontology, as it generally increases the variance. Inversely, agglomerative clusterings are bottom up ap-proaches that recursively merge clusters.
 When looking at the MovieLens data set (Figure 7 and 8), we can see that the results are very similar than the one obtained with Jester. For example, personalized ontology filtering performs better on average than any of the clustering algorithms taken separately, whatever the size of the learning set LS . As a mater of fact, the improvement is even more significant than with Jester. Second, the ontology produced by COBWEB is again giving a very poor prediction accuracy. For the first time, the agglomerative clustering with the clink criteria function is the best performing clustering algorithm when 5 items are used for learning the user X  X  model. Note however, that P-i1 is nearly as good as A-clink, and thus no clear conclusion can be drawn from this result. Figure 7: Relative F1 values for MovieLens, 5 ratings in LS
When 50 ratings are used in LS , the best performing ontology is again the one learnt using the partitional clustering with the H function. This tends to suggest that, on average and when sufficient data about the user is known, partitional clustering with the H teria function performs the best. A detailed analysis of the ontology size reveals in fact that the agglomerative clustering with the clink function can outperform P-i1 if the number of clusters are set to either 100 or 500. Fortunately, our pOF approach with Algorithm 2 is capable of predicting which ontology to use as its relative F1 score is always bigger than 1, and whenever the number of clusters are less or equal to 1000.

When considering which clustering algorithm to use, the accu-racy of the prediction should not be the only criterion. The exe-cution time required to build the tree should also be an important aspect. In Figure 6, we indicated the execution time (in seconds) required for the clustering algorithm to generate its tree. As ex-pected, the execution time does vary a lot from one algorithm to another. The worst performing algorithm is COBWEB, which re-quires nearly 9 hours to come up with a clustering tree on Movie-Lens! We expected such a result as the complexity is exponential to the number of attribute-value pairs, which in the case is equal to 1682 different items (attribute) and two values (good or bad). As a consequence, COBWEB clearly does not scale well to big problems. However, it is good to point out that this computation can be broken into smaller parts, as COBWEB is an incremental algorithm. Very surprisingly, agglomerative clusterings took less time to compute than partitional clusterings, which is obviously not what is expected when reading the general literature. This is not a mistake, and there are two explanations for this. First, the most ex-pensive step in agglomerative clustering is the pairwise similarity computation of all the pairs of items. In our situation, the matrix S containing this data has already been computed, and thus remove this O ( n 2 ) step. Second, as highlighted by Zhao[15], the pair-wise similarities or the improvements in the value of the criterion func-tion achieved by merging a pair of clusters i and j do not change during the different agglomerative step, as long as i and j is not selected to be merged.

Finally, using the criteria functions H 1 and H 2 significantly in-creases the execution time of the clustering algorithm. These re-sults are coherent with the theory as H 1 and H 2 are both hybrid functions that respectively combine criteria I 1 with E 1 E .
In this experiment, we wanted to see whether the multi-hierarchical ontology generated by Algorithm 3 does increase the recommen-dation accuracy or not. To test this aspect (and due to the limited amount of space) we only reproduced the Jester experiment, and compared it with the behavior of the agglomerative clustering us-ing the clink criterion function.

First, we looked at the number of extra clusters generated by the step 11 to 16 of Algorithm 3. As expected, the dotted line in Figure 9 shows that by increasing the coefficient  X  , many ex-tra clusters are being created. Notice that following algorithm 3, one extra cluster implies that 2 clusters will have at least 2 parents. An interesting aspect is to consider whether keeping increasing the size of the window will always improve the accuracy of the recom-mendation. The plain line in Figure 9 shows the average accuracy of the recommendation made by using Algorithm 3 for generating the ontology. The average accuracy was obtained by averaging the F1 values when respectively using 5, 10, 20, 40, 60, 80, and 100 Figure 9: Number of extra clusters generated by Algorithm 3 leaf clusters in the ontology. Note also that the F1 metric values were scaled up to fit in the interval 0 to 90 by using the formula y = 3000  X  F 1 value  X  900 . The solid line clearly indicates that in-creasing the window size leads to better prediction accuracy. How-ever, if the window becomes to big (i.e.  X  &gt; 0 . 4 ), then the struc-ture becomes overloaded by inheritance edges, which significantly increases the search space. Finally, notice that increasing the coef-ficient  X  will also increase the computational resources required to build the ontology. Thus, a tradeoff will need to be done between prediction accuracy and ontology quality.

In Figure 10, we plotted the prediction accuracy obtained using the ontology filtering approach with different learning sets sizes. The plain line is ontology filtering using Algorithm 3 to build the ontology, while the doted line is OF that uses the classical agglom-erative clustering with clink for the ontology construction. This is respectively represented by the Algorithm 3 * and A clink * and lines, where  X  is the size of the learning set for learning the scores. Figure 10: Accuracy for the Jester data set, with  X  set to 0.4
These results tend to go in favor of our hypothesis, which states that a multi-hierarchical structure leads to a better ontology than a simple hierarchical one. As one can see, the ontology gener-ated with the multiple inheritance algorithm reaches nearly always the best prediction accuracy, with the best improvements when the ontology contains at least 80 leaf clusters. When using agglom-erative clustering with the clink criterion function, the prediction accuracy fluctuates up and down depending on the parameter num-ber of leaf clusters, and significantly decreases when the ontology contains 100 leaves. This unwanted behavior is not observed with the multiple inheritance algorithm, which seems more robust to the number of leaf clusters. This can be explained by the fact that Al-gorithm 3 builds many extra inheritance edges, which means that the inference process has a higher probability of finding a shorter path between two given concepts. This shorter path will allow more score to be transferred between these concepts, which leads to a better inference process.
The first experiment showed that our personalized ontology fil-tering led to the best results compared to classical clustering algo-rithms. Following these results, we focus on whether our person-alized ontology filtering approach is in fact better or worse than collaborative filtering.

For the experiments, we analyzed the item-based collaborative filtering (CF) using the adjusted cosine similarity for the similar-ity between pairs of items, and the personalized ontology filtering (pOF) defined in section 3.2. CF was used as benchmark as it is the most widely used recommendation system. For the ontology filter-ing, we used Algorithm 1 to generate all the ontologies, and used Algorithm 2 to select the best ontology to be used by each user.
The performance of CF greatly depends on the number of neigh-bors used to compute the prediction (Equation (2)). At the same time, the ontologies generated by Algorithm 1 will be different de-pending on the number of leaf clusters (i.e.:threshold  X  ) that were specified. To test these aspects, we ran the algorithms using various values of leaf clusters and neighbors.

Figure 11 shows the accuracy of the pOF and CF recommen-dation systems on the Jester data set. The dashed lines represent collaborative filtering, while the plain lines are the personalized ontology filtering. The notation *-5LS and *-50LS means that 5 and respectively 50 items were used to learn the model. The x-axis shows the number of neighbors that were used for CF, which is also the same parameter used for the number of leaf clusters in Algorithm 1. The y-axis measures the accuracy of the recommen-dation using the F1 metric.
First, and most important, we can see that ontology filtering us-ing the learnt ontologies performs much better than CF. The result is even more emphasized when very few ratings (only 5, OF-5LS ) are used to learn the model. As a matter of fact, the improvement of our personalized OF is always significant (p-value &lt;&lt; 0.01). Furthermore, OF with just 5 ratings performs better than CF with 10 times more data.

Notice that the accuracy of CF actually increases with the num-ber of neighbors, and then decreases again. This is a well known result [4] and is due to two reasons. First, CF needs to have enough neighbors in order to correctly predict items. However, if too many low correlated neighbors are included in the computation, then the accuracy will decrease. This phenomenon seems amplified when 50 items are present in LS . Finally, we can see that pOF is more robust to the number of clusters in the ontology than CF is to the number of neighbors. This is because OF inference is done using the closest element, while CF uses a neighborhood of items.
Figure 12 shows the accuracy of the same experiment, but per-formed on the MovieLens data set. Notice that for Jester, the max-imum number of neighbors was set to 100 as Jester contained 100 items. However, MovieLens contains 1682 different movies, which could theoretically lead to a hierarchical tree with 1682 leaves. However, due to the sparsity of the user-item matrix, the similar-ities between some items could not be computed, which lead to only 1668 clusters. Again, pOF performs much better than CF, and shows significant (with p-value &lt; 0.05) improvement when 5 rat-ings are used to learn the model.
When looking at the behavior of CF, the results are similar to the one observed with Jester. However, the accuracy seems inde-pendent of the number of neighbors when it is lees than 100. This can be explained by the fact that the MovieLens data set is much sparser than Jester. Thus, many more ratings are necessary to build a correct model of the users.

Overall, we can see that pOF is performing extremely well and leads to significant improvement over CF. Furthermore, pOF with just 5 ratings in LS has a prediction accuracy that is nearly as good, sometime better, than the one of CF with ten times more training data. This tends to suggest that the learnt ontologies are of good quality. One important fact that needs to be taken into account is that CF X  X  accuracy tends to be proportional to the number of neigh-bors. When the size of the user X  X  preference set increases, this leads to significant scalability problems. Ontology filtering is less criti-cal to this problem as the inference is carried out from the closest concept, not on a neighborhood.
These experiments bring more insight into the use of clustering algorithms to build ontologies. From these, three main conclusions can be drawn. First, when considering partitional and agglomer-ative clusterings for building ontologies, there is no clear winner even though on average, partitional ones seem slightly better. The accuracy will greatly be influenced by the number of leaf clusters set as parameter. Second, our intuition that using the same ontol-ogy for ever user will be not be the best seems correct, and explains why Algorithm 2 leads to significant increase in prediction accu-racy. Finally, we showed that adding more inheritance edges be-tween concepts leads to better prediction accuracy, which suggest that multi-hierarchical ontologies are more robust for E-commerce applications.
In this paper, we have introduced three algorithms. The first one learns a set of ontologies based on some historical data, while the second is capable of selecting which one to use based on the user X  X  preferences. Our third algorithm extends the famous complete-link agglomerative clustering by building a multi-hierarchical ontology based on a predefined window size. Experimental results on the famous MovieLens and Jester data sets showed that our algorithms produce good quality ontologies and significantly increase the pre-diction accuracy. Furthermore, the learnt ontologies can even out-perform traditional item-based collaborative filtering.

This paper has also given some more insight into the behavior of hierarchical clustering algorithms. This has lead to four con-clusions. First, COBWEB is unsuitable for generating ontologies from historical data as we do not have proper attribute-value pairs. Second, partitional clustering algorithms produce ontologies that, in general, achieve higher prediction accuracy than if they were obtained by agglomerative approaches. Third, the quality of the ontology will also be dependant on the number of leaf clusters be-ing used, and this threshold will deeply influence which cluster-ing algorithm will give the best ontology. Finally, we showed that ontologies can be made more robust by adding more inheritance edges, but this will require more computational resources. [1] P. Clerkin, P. Cunningham, and C. Hayes. Ontology [2] C. Ding, X. He, H. Zha, M. Gu, and H. Simon. Spectral [3] D. H. Fisher. Knowledge acquisition via incremental [4] J. Herlocker, A. Borchers, J. Konstan, and J. Riedl. An [5] J. Herlocker, J. Konstan, L. Terven, and J. Riedl. Evaluating [6] C.-R. Lin and M.-S. Chen. Combining partitional and [7] G. Linden, B. Smith, and J. York. Amazon.com [8] M. McLaughlin and J. Herlocker. A collaborative filtering [9] E. Rissland. Ai and similarity. IEEE Intelligent Systems , [10] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl. Item-based [11] A. Schein, A. Popesucl, L. Ungar, and D. Pennock. Methods [12] V. Schickel and B. Faltings. Inferring user X  X  preferences [13] V. Schickel and B. Faltings. Oss: A semantic similarity [14] C. Seo and B. Ozden. Ontology-based File Naming Through [15] Y. Zhao and G. Karypis. Hierarchical clustering algorithms
