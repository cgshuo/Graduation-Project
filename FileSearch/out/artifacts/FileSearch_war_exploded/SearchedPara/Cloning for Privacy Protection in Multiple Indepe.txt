 Data anonymization has become a major technique in privacy pre-serving data publishing. Many methods have been proposed to anonymize one dataset and a series of datasets of a data owner. However, no method has been proposed for the anonymization of data of multiple independent data publications. A data owner pub-lishes a dataset, which contains overlapping population with other datasets published by other independent data owners. In this pa-per we analyze the privacy risk in the such scenario and vulnera-bility of partitioned based anonymization methods. We show that no partitioned based anonymization methods can protect privacy in arbitrary data distributions, and identify a case that the privacy can be protected in the scenario. We propose a new generalization principle -cloning to protect privacy for multiple independent data publications. We also develop an effective algorithm to achieve the -cloning. We experimentally show that the proposed algorithm anonymizes data to satisfy the privacy requirement and preserves good data utility.
 H.2.0 [ Database Management ]: General Security, Integrity, Protection
Private individual-specific information such as customer data, employee data etc. are maintained and shared for various purposes. The advantages of such sharing are well documented but in the re-cent past several instances of data privacy breaches [2], due to data sharing, have resulted in financial and reputational losses for en-terprises. Partition-based privacy preserving data publishing tech-niques address this problem by anonymizing data such that individ-ual privacy is preserved when data is shared or released. The ba-sic idea behind these techniques is one-in-crowd which guarantees th at an individual cannot be distinguished from a minimum num-ber of other people. Partition-based anonymization techniques are widely discussed in literature and well known schemes include k -anonymity [25], ( , k )-anonymity [28],  X  -diversity [20], t -closeness [19] and anatomy [32]. Existing partition-based techniques focus on one-time publication [25, 20], multiple views of the same data [35, 34]; and the series of datasets by single data owner [29, 10, 31]. Multiple data publications are restricted to single owner, and does not support privacy preserving data publications of overlap-ping records by multiple publishers. Ganta et al. [11] firstly iden-tified the privacy breach of overlapping population within multiple published datasets, and called this privacy breach composition at-tack . However, the solution of [11] supports only interactive setting (where only data statistics and/or query results are released), and is inapplicable for non-interactive setting (where the data is pub-lished after anonymization). Privacy preserving data anonymiza-tion against known and unknown overlapping population remains an open problem. To the best of our knowledge no solution exists for these scenarios.

Before we illustrate the problem, firstly let us consider patient overlapping scenario of three hospitals in Figure 1. There are three overlapping patients i.e. Nancy, David and Eliza. Nancy has vis-ited Hospitals 1 and 2 , Eliza, has visited the Hospitals 2 and 3 and David has visited the Hospitals 1 and 3 . All hospitals indepen-dently anonymize their datasets and are unaware of their overlap-ping records with other hospitals.

Table 1(a) is the original data of Hospital-1, which has three type of attributes. Identifier attributes can directly identify indi-viduals, such as Name, SSN etc. They should be removed in a published dataset. Quasi identifier (QI) attributes could indirectly lead to the identification of individuals in a dataset, such as Age, Zipcode and Sex etc. They are normally generalized so that no individuals are identifiable in a generalized dataset. The Sensitive attribute contains the private information about the individuals that needs to be protected such as Disease, Income etc. There is an-other type of attribute called non-sensitive attribute , which do not fall under aforementioned types but is useful for some data analy-sis. In our case, we do not consider them since they are unrelated to
Age Sex Disease  X  50 *  X  50 * data anonymization. A g eneralized table is considered privacy pre-served, if it satisfies a privacy requirement, such as k -anonymity [25], ( , k )-anonymity [28],  X  -diversity [20], t -closeness [19] etc. Hospital-1 releases Table 1(b) as a 4 -anonymous and 2 -diverse ver-sion of the original Table 1(a). 4 -anonymity means that values in the quasi-identifier have at least 4 identical copies. So one could not be distinguished from other 3 records. Such a group is called an equivalence class (formal definition in Section 4). 2 -diversity means that each of such a group has at least 2 distinct values in the sensitive attribute. So the sensitive value of each individual could not be inferred with a high confidence.
All existing partition-based privacy preserving data publishing techniques, such as [20, 32, 33, 19, 22, 6, 16, 26, 14, 30] fo-cus on forming equivalence classes (also called partitions or QI groups) based upon some generalization principles. To illustrate the problem, let us assume that Hospital-2 releases Table 2(b) as a 6 -anonymous and 3 -diverse version of the original Table 2(a) and Hospital-3 releases Table 3(b) as 4 -anonymous and 3 -diverse version of the original Table 3(a). The Nancy X  X  equivalence class in Table 1(b) (Hospital-1 X  X  anonymous release) and in Table 2(b) (Hospital-2 X  X  anonymous release) has only 1 common sensitive value i.e. { D }. So an adversary knowing the QIs of Nancy ( 50 years old female) and the fact she has visited two hospitals can derive her sensitive value from both tables. Such utilization of more than one individually anonymized datasets to infer the privacy of over-lapping individual(s) is called a  X  X omposition attack X  [11]. Using the composition attack adversary can get the diseases of David and Eliza in Tables 1(b), 3(b) and Tables 2(b), 3(b) respectively.
Individually all three anonymous datasets, Table 1(b), Table 2(b) and Table 3(b) pose low privacy risk but collectively compromise the privacy of overlapping patients due to the composition attack [11, 3]. In other words independently anonymized datasets do not retain the privacy properties under the composition attack. Note that there are other possible generalized tables of original tables too. However, they suffer the same type of privacy disclosures. Our method proposed in this paper leads to the publication of Table 1(d), 2(d) and 3(c). Specifically, each equivalence class of these tables contains all the sensitive values of its respective origi-nal dataset. Moreover, unlike a definite QI range (as done in previ-ous releases of these tables), each equivalence class of Table 1(d), 2(d) and 3(c) has micro-statistics values (to be discussed in detail in Section 6). The purpose of releasing such statistics is for better privacy preserving and retaining statistical properties.

Let adversary now attempt to infer the diseases of Nancy, David and Eliza from Table 1(d), 2(d) and 3(c). The adversary can locate that the equivalence classes of Nancy has 3 , and David/Eliza have 2 sensitive values common i.e. { D,G,J } and { G,J } respectively. Therefore adversary cannot get a specific disease that Nancy, David or Eliza has contracted.

One straightforward approach that can tackle the composition attack is trivial sanitizer [4] which simply suppresses all quasi-identifiers (QIs) or all sensitive attributes and publish all non-sensitive attributes intact. In our case, such sanitization makes data less use-ful since all information of QIs and sensitive value is lost. Our mo-tivation is to achieve privacy against composition attack and also to maintain, as much as possible, the high data utility of anonymous releases.
T he problem of overlapping data publications is not resolvable by the methods of sequential data publishing, such as [29, 31, 10] and multiple views of the same dataset [34, 35]. Both, sequential data and multiple view, deals with two overlapping data publica-tions of the same data owner. The data owner knows the previously released data/view(s) of the dataset and uses these data/view(s) during the anonymization process of the next publication. In our scenario of multiple publications, a publisher is unaware of other datasets that have overlapping records with it. Therefore, methods of sequential and multiple view data publication are inapplicable to multiple independent data publications. The sequential/multiple views inference is between the two different releases of same loca-tion; whereas the composition attack works between the indepen-dent releases of mutually unknown locations.

Firstly, In this paper we analyze the privacy risk in multiple inde-pendent data publications situations and vulnerability of partition based anonymization methods in such an environment. We show that no partition based anonymization methods can protect privacy in arbitrary data distributions, and identify a case that the privacy can be protected in.

Secondly, this paper proposes a multiple data publications model to protect a dataset from the composition attack when different lo-cations independently release the anonymous data of overlapping population. The core of our solution is the integration of two novel concepts: -cloning and micro-statistics based anonymization. The former is a new model, whose satisfaction ensures the association of a tuple with all sensitive values of data; hence providing the max-imum privacy protection. The latter is an anonymization technique that facilitates the implementation of -cloning.
Let P be a dataset maintained by a publisher. There are n other publishers which have overlapping subset with P . We assume that all n publishers and their published datasets are unknown to the independently anonymized from its original dataset Q i .

We classify the columns of P and Q i into three types (already explained in Section 1): (i) an identifier attribute A id primary key of P and Q i , (ii) the d quasi-identifier (QI) attributes A 1 , A can be either numerical or categorical. For each tuple t  X  P , t [ A ] denotes its value on attribute A .
 Definition 1 (Generalized QI group / Equivalence class) For anony-mous dataset P  X  , an equivalence class ( E ) is set of the tuples in P  X  with the same values in QI attributes. Each equivalence class is assigned an ID A g .
 For a tuple t  X  P  X  ; we refer t.E as the hosting equivalence class of the tuple t in P  X  . When the publisher P releases P  X  sary can use P  X  and any Q  X  i to intrude the privacy of overlapping subset by the composition attack. To formalize the attack, we first introduce a notation O ( P  X  , Q  X  i ) .
 Definition 2 (Overlapping set) Let publisher P releases P the overlapping set O ( P  X  , Q  X  i ) contains all those tuples in P Q i such that: two corresponding tuples t i  X  Q  X  i and t  X  P  X  ; which satisfy the following requirements: 1. t [ S ] = t i [ S ] ; both tuples have the same sensitive value For two value intervals (or values), the intersection returns the over-lapping range of two intervals. For example, for age QI intervals (15 X 25)  X  (20 X 30) = (20 X 25). For categorical QI values in gener-alization taxonomies, the intersection returns the most specific QI value of two QI values if the one is a generalization of another oth-erwise returns  X  . For example, for sex QI values (  X   X  male = male ), (  X  X  X  f emale = f emale ) and ( f emale  X  male =  X  );  X   X   X  corresponds to the most generalized QI value in any generalization hierarchy. In sex QI generalization hierarchy,  X   X   X  presents both male and f emale .

Assume that a tuple t occurs in two generalized tables P  X  Q . As per Definition 2 , tuple t  X  O ( P  X  , Q  X  i ) and it has two corre-sponding tuples; each in P  X  and Q  X  i . The probability for inferring the sensitive value of overlapping tuple t f rom tables P prob( t [ S ]  X  ( P  X  , Q  X  i )) : Equation ( 1 ) reveals the reason of the failure of partition based generalization schemes like k -anonymity [25] ,  X  -diversity [20], t -closeness [19] etc. for the composition attack . After proper anonymization, the corresponding generalized tuples of overlap-ping tuple t are protected in each of P  X  and Q  X  i . However, compo-sition attack inference has nothing to do with their respective indi-vidual probabilities in P  X  and Q  X  i . Composition attack inference is only determined by their overlapping set O ( P  X  , Q  X  i ) . An overlap-ping set O ( P  X  , Q  X  i ) may not satisfy either generalization scheme of individual anonymized datasets. In the worst case: The worst case ( 2 ) occurs when only one record in O ( P matches overlapping tuple t or all tuples have the same sensitive value.
 Example 1. Let t = &lt; Nancy, 50, F, D &gt; and Q  X  1 and P 1(b) and 2(b) respectively. The overlapping set O ( P  X  , Q tains the tuple &lt; 15 X 50, F, D &gt; . So the probability for inferring the true sensitive value of Nancy from tables Q  X  1 and P  X  is prob t [ S ]  X  O ( P  X  , Q  X  1 ) = 1 . All matching records in O ( P  X  same sensitive value.
In the previous section, we have shown that the privacy of an individual in the overlapping dataset can be inferred with 100% accuracy. Now we discuss when we can prevent such inference and how to do it.

We start with the inference of the probabilities of sensitive values shared by two different datasets with overlapping QI values. Let S be a set of all possible sensitive values of two datasets D Let f D 1 ( s ) be the frequency of sensitive values in dataset D f 2 ( s ) for D 2 . Note when a sensitive value, say s j , is not present in a dataset, for example D 1 , f D 1 ( s j ) = 0 .

The probability for inferring s i  X  C from both datasets based on their sensitive values distribution is given as the following. Note respectively. Let us assume that  X  D 1  X   X   X  D 2  X  (we can swap the datasets if otherwise) and 0 &lt;  X  1 . prob( s i  X  ( D 1 , D 2 )) = prob( s i  X  O ( D 1 , D 2 )) The probability for inferring an overlapping sensitive value from two overlapping datasets relies on 1) the number of overlapping sensitive values, 2) their frequencies, especially in the smaller dataset of two datasets. In other words, it depends on what have been re-mained in the intersection. Next, we show how to use equation (3) in the following example.
 Example 2. Let D 1 be Table 1(c) and D 2 be Table 2(c). S = { A, C, D, G, J } . We infer Nancy X  X  sensitive value knowing her record is in both datasets. The sensitive values in O = { D (3) , G (2) , J (2) } where the numbers in parentheses are counts. prob( D  X  O ( D 1 , D 2 )) = 3 / (3 + 2 + 2) = 3 / 7 .

If we use the distributional information to infer the probability, we use the following table. We obtain p rob( s = D  X  ( D 1 , D 2 )) :
Now we present the objective of privacy preservation in multiple independent publications in the following.  X  i : prob( s i  X  ( D 1 , D 2 ))  X  max(prob( s i  X  D 1 ) , prob( s prob( s i  X  D 1 ) and prob( s i  X  D 2 ) are the confidences of the adver-sary inferring the sensitive value s i individually from two published datasets D 1 and D 2 respectively. is a small positive number. Let us recall what an adversary knows. Based on the background knowledge and the quasi-identifier values, an adversary knows that the victim X  X  record is in both datasets D 1 and D 2 . max(prob( s  X  D 1 ) , prob( s i  X  D 2 )) means the maximum probability that the adversary would know the sensitive value from the two published datasets individually. The objective (4) is to bound the improve-ment of the confidence of the adversary (caused by the composi-tion attack) by . If = 0 , then the adversary gets no advantage by using two datasets collectively. prob( s i  X  D 1 ) and prob( s can be estimated as f D 1 ( s i ) and f D 2 ( s i ) if two datasets have been anonymized properly.

Now, we discuss when objective (4) can be achieved. We firstly study how difference in sensitive value distributions in D affect the inference probability. Let us fix = 1 . Let = 1 sitive values in both D 1 and D 2 are similar,  X  1 . When the distributions are significantly different, is a large number. Example 3. Let S = { s 1 , s 2 , s 3 , s 4 , s 5 } , datasets D contain some of the sensitive values of S . We give some distribu-tions of sensitive values in D 1 and D 2 and their corresponding values as the following. In an ideal situation when = 1 , the overlapping dataset does not reveal more privacy of an overlapping individual than each individ-ual dataset does. Therefore, the privacy of overlapping individuals in multiple datasets is preserved.
In the worst situation, when is a large number, distributions of two datasets are almost distinct and have very little overlap. In such situation, prob( s i  X  ( D 1 , D 2 ))  X  1 and an adversary can obtain the probability of an overlapping sensitive value with the nearly certain confidence using just the distribution information.

From the above examples, we see that distributions dominate the inference probability in the composition attack. Thus, we have the following observation.
 Observation 1 There is not a general solution for the composition attack for any distribution of two datasets.

P ROOF . Given a dataset D 1 with a distribution. We can always find a distribution of another dataset D 2 so that is a large number. As a result, prob( s i  X  ( D 1 , D 2 ))  X  1 , and the objective (4) will be dissatisfied for some i  X  X .
 For example, in the worst case, the privacy will be revealed just b ased on distributional information of two datasets, such as two datasets have only a single sensitive value in common. Such a dis-closure does not even need QI information.
 In the following, we consider a solution in a restricted form. We firstly define the closed community . In a closed community, the dataset of each data owner (i.e hospital in our case) is drawn from the same population and their data distributions are similar. In our words,  X  1 . For example, different hospitals take different sets of patients, but in most cases, the disease distributions of the hos-pitals are similar. There are some exceptions that the distributions of some specialized hospitals are different from those of general hospitals. However, in such exceptions, the knowledge of a pa-tient visiting a specialized hospital gives an adversary large chance for guessing the disease anyway. Let us assume that a number of general hospitals plan to publish their datasets independently in anonymized form (for research purposes etc.) and want to en-sure privacy against the composition attack. So, a solution in the assumption of the closed community, where hospitals have similar distributions, is still useful.

In the following discussions, we assume that  X  1 for a solution in a closed community. When  X  1 , does not make much difference because both distributions are similar. An ideal solution can be achieved if sensitive values in every equivalence class of P and Q  X  i has the same distribution as of P and Q i respectively. To formalize such best solution scenario, we introduce the following concept.
 Definition 3 (Clone equivalence class) Given the distribution of sensitive values of dataset P as f ( S ) . An equivalence class E  X  P  X  is called a clone equivalence class if it has all the sensitive val-ues S and the distribution of sensitive values in E is the same as f ( S ) .
 Example 4. A clone equivalence class of Table 2(a) is Table 2(c). Apparently, the utility of this clone equivalence class is low. Ta-ble 2(d) has two clone equivalence classes generalized by our micro-statistics method (to be discussed later).
 Observation 2 (Property of clone equivalence classes) Let all equiv-alence classes of anonymized datasets P  X  and Q  X  i be clone equiva-lence classes. The objective (4) can be achieved when  X  max min(prob( s j  X  P  X  ) , prob( s j  X  Q  X  i ))  X  max(prob( s  X  Q  X  i )) for all j  X  1 , 2 , 3 , . . . ,  X  S  X  .
 P ROOF . If all the equivalence classes of anonymized datasets P  X  and Q  X  i are clone equivalence classes (Definition 3), the proba-bility for inferencing a sensitive value in an equivalence class is the same as that for inferencing the sensitive value in whole dataset. Therefore, we can replace D 1 and D 2 with P  X  and Q  X  i in equa-tions (3) and (4) and the lower bound of is obtained.
 Since m ax(prob( s i  X  P  X  ) , prob( s i  X  Q  X  i ))  X  min(prob( s ( s  X  Q  X  i )) and  X  1 , the can be very small.

Clone equivalence classes are ideal but inappropriate for real world situations. We may lose a lot of utility if we require datasets to be anonymized to such an ideal situation (as we see in Tables 1(c) and 2(c)). We need a practical goal. We relax the requirement of clone equivalence classes as the following -clone equivalence classes.
 Definition 4 ( -clone equivalence class) Given the distribution of sensitive values of dataset P as f ( S ) . An equivalence class E  X  P  X  is called -clone if it has all the sensitive values S and for every sensitive value in S , the difference of probabilities of the sensitive value in P and in E is bound by a small number . More specifi-cally,  X  prob( s i  X  P )  X  prob( s i  X  E )  X  X  X  .

The magnitude of is determined by the data distribution of a dataset. We will analyze the upper bound of the in Section 6.2 after the algorithm is presented. Finally, we present the problem of creating -clone equivalence classes.
 Definition 5 ( -clone publication) The objective of privacy pre-serving -clone publication is to compute such anonymous P each equivalence class E in P  X  is -clone equivalence class and retains as much information in anonymous P  X  as possible. The requirement of -clone equivalence class looks like t -closeness [19]. However, they are different in the following way. The t -closeness bounds the total difference of the distribution between an equivalence class and the dataset, but -cloning bounds the fre-quency difference of each sensitive value of an equivalence class and the dataset. When the overall difference is bounded in two distributions (like in t -closeness), the frequency differences of a sensitive value between dataset and an equivalence class can still be large. Therefore, t -closeness [19] is also vulnerable to the com-position attack as shown in [11].
There is a practical problem for anonymizing a table to comply with -clone publication. The size of an -clone equivalence class can be big when the distribution of sensitive values is skewed. The generalization of such a large equivalence class makes a dataset useless, as we saw in Table 1(c) and 2(c) where most QI values have been suppressed. In this section, we will introduce a micro-statistic technique to make such a super equivalence class more useful than the generalization.
We aim at achieving two intuitive goals. First, each equivalence class has all the sensitive values of the anonymized dataset. Sec-ond, we attempt to reduce, as much as possible, the frequency dif-ference for each sensitive value between original dataset and the equivalence class i.e. . We do not pre-specify but adaptively find it out in the process of anonymization. The detail discussion on is given in Section 6.2.
T his phase divides all the tuples of P into  X  S  X  (  X  S  X  is the num-ber of distinct sensitive values in P ) sub-datasets i.e. W W  X  S  X  , such that each sub-dataset contains only the tuples with same sensitive value. In the end of this phase, we calculate the frequency of each sub-dataset f ( W j ) in original dataset P . This phase is dif-ferent from the bucket partition phase of [5] because [5] starts out with buckets of more than one closely related distinct sensitive val-ues in one bucket but we strictly allocate only one sensitive value to each sub-dataset.
 As an example, we consider Table 3(a) as P , u = 4 because P (Table 3(a)) has 4 distinct sensitive values i.e, { J , B , G , L }. We create 4 sub-datasets i.e. W 1 ( J ) , W 2 ( B ) , W 3 ( G ) , W calculate the ratio of each sub-dataset to the whole dataset; for ex-ample the sensitive value J has 5 tuples and its ratio in Table 2(a) is 0 . 26 .
First, we create b empty equivalence classes ( E 1 , E 2 , E E ) ; where b = min(  X  W j  X  ) , i.e. the sub-dataset with the minimum tuples. We call such sub-dataset as the key sub-dataset , denoted as W . Next, we decide how many tuples to be assigned to each empty equivalence class E r ( r = 1 , 2 , 3 , . . . , b ) .
The assignment algorithm takes the  X  S  X  sub-datasets, already cre-ated in the division phase, and calculates a separate j value for each sub-dataset; j is the number of tuples to be assigned to each equivalence class E r from each sub-dataset W j . To calculate the , we first make a temporary calculation of j =  X  W  X   X  b . Where b is the number of equivalence classes created earlier in this phase i.e. b =  X  W  X  . Now we calculate j for j -t X  sub-dataset as: Where round is a function that returns the nearest integer from For example, round (2 . 3) = 2 , round (2 . 8) = 3 and round (2) = 2 . In an ideal situation, the j = j . Such ideal situation exists for all sub-datasets of Table 2(a). If j is not an integer then there is a deficiency of some tuples in W j ; there are not enough tuples in W j that can equally be allocated to all b equivalence classes. In such case, we need to handle this deficiency by suppressing/adding ~ tuples from/to sub-dataset W j , where: We add ~ j counterfeited tuples to sub-dataset W j if deficiency (  X   X  j  X  ) &gt; 0 . 5 because in such case suppression will cause more utility lost. The counterfeit tuples only has sensitive value s dataset W j and all QIs have  X   X   X  values. In other case, we suppress the ~ tuples from sub-dataset W j if deficiency ( j  X  X  X  j  X  ) &lt; 0 . 5 because in such case counterfeit tuples will cause more utility lost. If ( j  X  X  X  j  X  ) = 0 . 50 then suppress and counterfeit equal number of sub-datasets.

Once j information is ready for all sub-datasets, the assignment algorithm selects a sub-dataset W j and calculates the Distortion (a distance metric) [18] of first ( j  X   X  X  X  W j  X  ) tuples with equiv-alence class E r . Next, out of tuples it assigns j closest (defined in next paragraph) tuples to equivalence class E r . This process is repeated for all equivalence classes. The is a parameter, called distortion control , to improve the performance of the assignment algorithm and it restricts the calculation of Distortion to the first tuples of the sub-dataset, instead of calculating distortion for all tuples of the sub-dataset.

Now we explain the closest tuples in detail. Alongside comply-ing with the -clone publication objective (Definition 5 ), we also Algorithm 1 M icro-Statistic based Anonymization Input: P  X  Input dataset to be anonymized k  X  Input parameter for k -anonymity Output: P  X   X  Anonymized dataset 1: Create w ( w = P  X  S  X  ) e mpty sub-datasets W [ ]  X  Division phase 2: Populate sub-datasets W [ ] , each sub-dataset has tuples with 3: b = min(  X  W [ j ]  X  )( j  X  1 , 2 , 3 , . . . ,  X  W [ ]  X  )  X  Get number of 4: Create b empty equivalence classes E [ ]  X  Assignment phase 5: [ j ] = round (  X  W [ j ]  X  b )  X  g et for each sub-dataset 6: for j  X  1 ,  X  W  X  do  X  Access all sub-datasets one-by-one 7: for r  X  1 ,  X  E  X  do  X  Access all equivalence classes 8: Calculate Distortion Distance for first tuples of W [ j ] 9: E [ r ] = [ j ] of in W [ j ]  X  out of tuples in W [ j ] , 10: end for 11: end for 12: Merge/divide equivalence classes in E [ ] by requirement k 13: Generate  X  X icro-statistics X  for each equivalence class in E [ ] 14: Combine all E [ ] to form P  X   X  Anonymized dataset created need to make sure that anonymous dataset P  X  m aintains a reason-able utility and it has been done through putting closest tuples of each sub-dataset W j into an equivalence class E r . We use Dis-tortion [18] to measure the closeness between a tuple t and equiv-alence class E r . The distortion is the sum of the generalization steps in the generalization hierarchies if t is assigned to E the problem of forming equivalence classes with the minimum dis-tortions is similar to other optimal k -anonymity problems [1, 23]. So, we randomly pick tuples from each sub-dataset W j and cal-culate the Distortion for each of tuples with an equivalence class E . We assign that tuple t out of tuples to an equivalence class E r that has the minimum distortion with it. Due to the space con-straint, we omit the calculation details of distortion and readers are referred to the original paper [18] for further details.

In the running example where we assume Table 3(a) as P , the b =  X  W 3 ( G )  X  i.e. 3 and we create 3 empty equivalence classes. We have already created 4 sub-datasets in division phase. For all four sub-datasets, the 1 ( J ) = 2 , 2 ( B ) = 2 , 3 ( G ) = 1 , deficiency values ( j  X  X  X  j  X  ) are 0 . 66 , 0 . 33 , 0 , 0 . 33 respectively. We need to add 1 counterfeit tuple in W 1 ( J ) and need to suppress 1 tuple from sub-datasets W 2 ( B ) and W 4 ( L ) . In the assignment phase, we do not suppress any tuple and once the ( j  X  b ) clos-est tuples are assigned to b equivalence classes; we suppress the remaining (  X  W j  X  X  X  ( j  X  b )) tuples in the sub-dataset W
Assignment operation accesses each sub-dataset and assigns  X  X los-est X  j tuples of each sub-dataset W j to each of three equivalence classes.
In the beginning of this phase, we suppress any remaining tuples in all sub-datasets and divide/merge or leave intact each equiva-lence class as per the input parameter k f rom k -anonymity [25]. We divide the QIs of any equivalence class into sub-equivalence classes if  X  E r  X  &gt; k or merge an equivalence class with some another one if  X  E r  X  &lt; k . We only divide the QIs into sub-equivalence classes and leave the sensitive values intact because we have to maintain the association of each tuple of sub-equivalence class with all the sen-sitive values of the equivalence class. We merge those equivalence classes that have minimum distortion with each other.

Next, we calculate the micro-statistics for all QIs within each sub/equivalence class. We have two types of QIs i.e. categorical and numerical. For categorical QI, we include the number of dis-tinct QI values in each sub-equivalence class. For numerical QI, first we calculate the  X  X verage X  of QI values within sub/equivalence class and next select an indicative range that covers QI values of the same sub/equivalence class; where  X  k and is an input param-eter. Algorithm 1 formally presents the anonymization operation.
In our running example, we have two remaining tuples t &lt; 45, M, Separated &gt; in W 2 ( B ) and t 18 = &lt; 22, M, Separated &gt; in W 4 ( L ) ; we suppress these tuples first. Now, we consider k = 3 and = 1 and divide each equivalence class into two sub-equivalence classes, each of 3 tuples. As last step, we generate the micro-statistics for all the sub-equivalence classes. We have 3 QIs, the Age is numerical whereas Sex and M arital Status are categorical. We replace the Age QI values in all sub-equivalence classes with the average Age of each sub-equivalence class and also include an indicative range that covers 1 ( = 1 ) Age value nearest to the average value in each sub-equivalence class. For Sex and M arital Status QIs, we put the number of all distinct QI values in all sub-equivalence classes. The result is Table 3(c).
The Table 2(a) is case of equivalence class merger. Ideally, there should be 3 equivalence classes (having 4 tuples each) but we as-sume k = 6 for Table 2(a); so one equivalence class is split and merged into others two.
The -clone equivalence classes formed by our algorithm makes use of micro-statistics instead of local recoding [17] and global re-coding [25] generalization schemes often adopted in literature. We employ the micro-statistics scheme because it provides high degree of privacy and utility in anonymous dataset (shown in experiments of Section 7). The concept of -cloning can also be implemented by local/global-recoding generalization. Although it will result in same privacy guarantee but with lower utility (shown in Tables 1(c), 2(c)).

Now, we discuss the effectiveness of our scheme (micro-statistics based anonymization) in achieving the -clone publication. The major concern of the -clone publication is the magnitude of . We aim at the maximum possible value of in our scheme. A large means high distribution difference between the original and the anonymized dataset.

We now compute the maximum value of , for all distinct sen-distinct sensitive values so there will be u sensitive sub-datasets as W 1 , W 2 , . . . , W u . We also assume the ratio of these u sub-datasets as: f ( W 1 )  X  f ( W 2 )  X  . . .  X  f ( W u ) . As the ratio f ( W sitive value s 1 is the smallest, the sub-dataset W 1 will be the  X  X ey sub-dataset X  and the number of initial empty equivalence classes b =  X  W 1  X  (i.e. the number of tuples in key sub-dataset). In an ideal situation, if ~ j = 0 ( j = 2 , 3 , . . . , n ) for all n sub-datasets (it occurs when j = j in (6)) then j = 0 for all sub-datasets. In other case, where ~ j &gt; 0 (i.e. j  X  = j ), we need to distort the sub-dataset(s) by suppressing and/or adding counterfeiting ~ At t ri bu te Age Sex Ed u cati on Birth Pla ce Domain tuples. In such case, we need to find the upper bound of the denote such maximum value of j as max j .

We consider the worst case and assume that, excluding the key sub-dataset W 1 , for all other sub-datasets j  X  = j and ( = 0 . 50 . Now we need to compute the ~ j suppress and counterfeit tuples for equal number of sub-datasets. The values of ~ j lated as per (7) and in any case the maximum number of distorted (suppressed and/or counterfeit) tuples, denoted as ~ max j The W is the key sub-dataset ( W 1 in our case). So as per (8) , the maximum number of distorted (suppress and/or counterfeit) tuples in any sub-dataset are not more than the half number of the tuples in the key sub-dataset. Now if the property (8) holds for all sub-datasets then following property will also hold for any sub-dataset W Now for any dataset P having at least two distinct sensitive values, the frequency of key sub-dataset to get the worst case scenario (  X   X   X  ) = 0 . 50 for the non-key sub-dataset is f ( W ) = 0 . 40 . We have to suppress/counterfeit (0 . 40 / 2  X  X  X  P  X  ) tuples from/to the non-key sub-dataset. Even in such extreme case, the max j &lt; 0 . 2 . If we have 3 sensitive values the max j = 0 . 125 . In real life datasets, we have moderate number of distinct sensitive values; causing even lower max j . For Table 3(c) the max j is 0 . 07 . If we have 10 sensitive values, max j = 0 . 03 . 15]. The minimality attack [27] exploits the minimum generaliza-tion required for anonymization and it cannot be successful on our scheme due to incorporation of micro-statistics instead of (local or global) generalization. Adversary cannot get the exact QIs range for any equivalence class from its micro-statistics and adversary X  X  confidence remains split about the inclusion of a tuple in an equiva-lence class or not. The deFinett attack [15] can be successful when the frequency difference of a sensitive value between original and anonymous dataset is  X  X arge X ; which is not in the case of -clone equivalence classes because each -clone equivalence class has all the sensitive values of the original dataset with a small threshold. So, adversary gets very little information from -cloned dataset af-ter deFinett attack [15]. Experiments are performed on a machine running a 2.4 Ghz CPU with 3 Giga-byte memory. We deploy two real world datasets OCC and SAL from United States census data downloadable from http://ipums.org. Both contain 600k tuples and each tuple con-tains the information of an American adult. OCC includes four QI attributes, age , sex , education , birt X  place , and a sensitive attribute occupation . SAL contains the same set of QI attributes, but different sensitive attribute income . All columns, except age , are discrete and the sizes of their domains are given in Table 4.
We create five disjoint sub-datasets Q occ i ( Q sal i ) ( i  X  1 , . . . , 5 ) from OCC ( SAL ). It suffices to clarify the generation and gener-alization of Q occ i , since the same method is applied for Q assign 1 00 k tuples to each sub-dataset Q occ i . The remaining 100 k tuples initiates an overlap pool. All sub-datasets and overlapping pool have same set of sensitive values.

We randomly select specific overlapping tuples from overlap pool and insert them to each Q occ i . The overlapping tuples control the overlap rate. We insert overlapping tuples of 20 k, 40 k, 60 k, 80 k, 100 k to each of five sub-datasets to create 5 groups of sub-datasets each with the size of 120 k, 140 k, 160 k, 180 k and 200 k. In each group, five sub-datasets have the same size and the same overlapping rate. We refer to the five sub-datasets in each group Q i ( Q sal i ) as independent publishers. We do experiments on five separate publishers for OCC and SAL , and repeat the experiments for five different overlapping rates and sizes.
In the first set of experiments, we show that the existing gen-eralization methods lead to severe privacy disclosure in indepen-dent data publications. The findings were also observed previously in [11, 3]. We increase the overlapping tuples from 20 k to 100 k and adopt the algorithm in [16] to compute  X  -diverse [20] versions Q 1 , Q occ  X  2 , . . . , Q occ  X  n . Then, we identify all the overlapping tuples that appear in any Q occ  X  i and whose sensitive values will definitely be revealed (called privacy risk tuples) using any another sub-dataset. These privacy risk tuples are extracted using Defini-tion 2; where we use Q occ  X  1 as P  X  and all other 4 sub-datasets are treated as Q  X  i . In Fig. 2(a), we plot the maximum number of pri-vacy risk tuples in any sub-dataset as a function of  X  . We repeat this process by increasing  X  from 2 to 10 . The  X  -diversity [20] fails to support independent publications of overlapping data, because it results in a large number of privacy risk tuples. Although fewer pri-vacy risk tuples exist as  X  increases, privacy risk tuples still cannot be completely prevented with larger  X  .

As all sub-datasets Q x  X  i ( x = occ or sal ) and overlap pool have the same set of sensitive values (Table 4) so -cloning preserves the privacy because an -cloning the overlapping set contains most sensitive values and privacy compromise does not occur. We repeat these experiments on sub-datasets Q sal i and the results are illus-trated in Fig. 2(b), confirming the same observations.
In following experiments, we examine the effectiveness of -cloning. We invoke the micro-statistics based anonymization on Q , Q x 2 , . . . , Q x 5 ( x = occ or sal ) to compute generalized versions Q ples and distortion control parameter ( from Section 6.1.2). We use 20% of dataset size as , unless otherwise mentioned.

We cannot benchmark our experiments with existing partition-based generalization schemes i.e. k -anonymity [25],  X  -diversity [20], t -closeness [19] etc. because all of these schemes are prone to composition attack [11, 3].
Our algorithm needs to suppress and/or counterfeit some tuples in the Assignment phase. We demonstrate that only a small number of distorted tuples (both suppressed and counterfeited) are needed to enforce -cloning. In Fig. 3(a), we vary the dataset size from 120 k to 200 k and measure the number of distorted tuples in OCC ( SAL ). The number of distorted tuples increases along with dataset size because higher dataset size requires more tuples in each equiv-alence class for assignment; resulting possibility of more distor-tion. The maximum number of distorted tuples are only 579(607) for dataset size of 200k in OCC ( SAL ).

In Fig. 3(b), we show the percentage of distorted tuples in OCC Figure 2: Successful composition attacks with various overlapping tuples and  X  -diversitity 300 600 900 (a) Distorted tuples vs. d ataset size Figure 3: Average and percentage of distorted (suppressed and counterfeited) tuples in OCC and SAL with various dataset sizes and overlapping rates ( SAL ) versus the dataset size. The percentage decreases as dataset size increases. As a result, that micro-statistics based anonymiza-tion can utilize more tuples. As dataset size increases, the total number of distorted tuples also increases (Fig 3(a)) but overall the percentage of distorted tuples with dataset size decreases (Fig 3(b)). In any case the percentage of distorted tuples is less than 1 % of dataset size.
In the following set of experiments, we will use Q x  X  i ( x = occ or sal ) to answer queries about the original sub-dataset Q use aggregate queries, since they are the basic operation for numer-ous data mining tasks (e.g., decision tree learning, association rule mining, etc.). Specifically, each query has the form:
SELECT COUNT (*) FROM Q x  X  i WHERE pred ( t [ A qi 1 ] AND . . . AND t [ A qi 4 ] AND t [ S ])
The Q x  X  i is the sub-dataset generalized using -cloning, ( t [ A attribute occupation ( income ). For each attribute A , the condition pred ( A ) has the form A  X  . Here is a query parameter called selection range , and has length  X  A  X  X  X  , where  X  A  X  is the domain size of attribute A (see Table 4). A larger result is returned with higher . Our workload consists of 10000 queries with same sub-dataset Q i and sensitive value t [ S ] . qu ery err or (a) Median relative error vs. d ataset size, ( = 25%) Figure 4: Query error with fixed query selection range ( ) and in-creasing dataset size and increasing distance control parameter ( ) qu ery error (a) Workload error vs. ( d ataset size = 120 k ) Figure 5: Average workload error with increasing query selection range ( ) and increasing dataset size
Given a query, we obtain its actual result R act from the original sub-dataset Q x i , and compute an estimated answer R est clone generalized version Q x  X  i . The relative error of a query equals  X  R act  X  R est  X  /R act . We measure the workload error as the median relative error of all the queries of all sub-datasets.
 Fig. 4(a), plots the workload error as a function of dataset size for OCC and SAL respectively. In all experiments, the median error is at most 2.5 % . In the experiments of Fig. 4(b), we set dataset size to 120 k and measure the workload error as the function of distor-tion control parameter . The error decreases (accuracy improves) with because higher results the more search in finding the clos-est tuple during the assignment phase. Our experiments show that error does not vary significantly with dataset size, and is not very sensitive to the distortion control parameter .

In the experiments of Fig. 5, we set dataset size to 120 k and 200 k (i.e. minimum and maximum values of dataset in Fig. 3) and study workload error in Q x  X  i ( x = occ or sal ) as the function of query selection range . The accuracy improves i.e. workload error decreases with increase in . This is expected because higher leads to larger query results, whereas, in general, aggregate analysis is effective for sizable queries.
The last experiment evaluates the efficiency of our micro-statistics based generalization algorithm. In Fig. 6(a), we measure the aver-age time of computing a generalized sub-dataset Q occ  X  i function of dataset size. The cost is more expensive when dataset size is higher, because the algorithm needs to process more tuples in a dataset. In Fig. 6(b), we fix dataset size to 120 k, and get the computation time as a function of distortion control parameter . The overhead increases as increases, since a larger results the involvement of more tuples in finding the closest tuple during the assignment phase. In comparison Figure 6(b) and Figure 4(b), we see that controls a trade off between the efficiency and utility. time ( sec. ) (a) Computation time vs. d ataset size Figure 6: Computation overhead with increasing dataset size and increasing distance control parameter ( )
It is important to point out that the partition-based schemes in the literature were not designed to be used in contexts where indepen-dent releases of overlapping population are available from different locations. Thus, essentially the composition attack is not a flaw in these schemes, but rather it directs the community X  X  attention to an important direction of research.

There has already been made substantial progress in privacy pre-serving data publishing. One line has focused on taking into ac-count other, known releases, such as previous publications by the same organization (called sequential, serial or incremental releases) [29, 31, 10] and multiple views of the same dataset [34, 35]. An-other line has considered incorporating knowledge from partitioned views to group individuals [34]. In our case, each publisher is in-dependent and unaware from the dataset of other publisher(s).
Some other works have sought to model unknown background knowledge of adversary [22, 6]. Martin et al. [22] and Chen et al. [6] provide complexity measures for an adversary X  X  side informa-tion (roughly, they measure the size of the smallest formula within a CNF-like class that can encode the side information). Both works design schemes that provably resist attacks based on side informa-tion whose complexity is below a given threshold. A hypothetical discussion of the same problem is in [11], driving concepts from differential privacy [7, 9]. Differential privacy guarantees that the attacker should not be able to distinguish between two possibilities, i.e. a specific person X  X  record is in or not in a statistical database, thus preserving the privacy. Most relevant to this paper are works that elaborate the privacy risk due to the anonymous data release of overlapping population by multiple locations [21, 13, 12]. All of these incorporate the co-ordinated model ; where all locations com-municate with each other to calculate the privacy risk of overlap-ping population and subsequently release dataset that is k-linkable i.e. each overlapping record is minimum linked to k records in each release.

Independent and asynchronous release by multiple locations (and hence composition attacks) fall outside the models proposed by these works. The sequential release models do not fit because they assume the multiple synchronous releases from the single location. In this paper, we deal with the case when there are multiple in-dependent publishers. The complexity-based measures do not fit because independent releases appear to have complexity that is lin-ear in the size of the datasets. The differential privacy is ideally suitable for interactive-setting (where there is no public release of anonymous data) and solution purposed in [11] lacks the actual im-plementation and test results. Moreover recent test results show that no differently private algorithm can have meaningful utility un-less the privacy requirement is very low [8, 24]. The co-ordinated model also does not satisfy our requirement because we are deal-ing with a non co-ordinated scenario where each location indepen-d ently anonymizes its data without having any communication with other location(s).
Existing data publishing and serial data publishing methods do not support multiple independent data publication by different pub-lishers where there are overlapping individual records. This paper has developed -cloning model to prevent an adversary from using independent data releases of multiple data owners to infer sensi-tive information of overlapping individuals. We have provided an efficient algorithm for computing anonymized dataset to achieve -cloning. We experimentally showed that the anonymized data ade-quately protects privacy and yet supports effective data analysis.
The authors would like to thank the anonymous reviewers for their constructive suggestions. This research has been supported by ARC Discovery grants DP0774450 and DP110103142. [1] A GGARWAL , G., F EDER , T., K ENTHAPADI , K., [2] A GRAWAL , R., K IERNAN , J., S RIKANT , R., AND X U [3] B AIG , M. M., AND J IUYONG L I , J IXUE L IU , H. W. [4] B RICKELL , J., AND S HMATIKOV , V. The cost of privacy: [5] C AO , J., K ARRAS , P., K ALNIS , P., AND T AN , K.-L. Sabre: [6] C HEN , B.-C., L E F EVRE , K., AND R AMAKRISHNAN , R. [7] D WORK , C. Differential privacy. In ICALP (2006), pp. 1 X 12. [8] D WORK , C., M CSHERRY , F., N ISSIM , K., AND S MITH [9] F RIEDMAN , A., AND S CHUSTER , A. Data mining with [10] F UNG , B. C. M., W ANG , K., F U , A. W.-C., AND P [11] G ANTA , S. R., K ASIVISWANATHAN , S. P., AND S MITH [12] J IANG , W., AND C LIFTON , C. Privacy-preserving [13] J IANG , W., AND C LIFTON , C. A secure distributed [14] J IN , X., Z HANG , M., Z HANG , N., AND D AS , G. Versatile [15] K IFER , D. Attacks on privacy and definetti X  X  theorem. [16] L E F EVRE , K., D E W ITT , D. J., AND R AMAKRISHNAN [17] L E F EVRE , K., D E W ITT , D. J., AND R AMAKRISHNAN [18] L I , J., W ONG , R. C.-W., F U , A. W.-C., AND P EI [19] L I , N., L I , T., AND V ENKATASUBRAMANIAN , S. [21] M ALIN , B. k -unlinkability: A privacy protection model for [22] M ARTIN , D., K IFER , D., M ACHANAVAJJHALA , A., [23] M EYERSON , A., AND W ILLIAMS , R. On the complexity of [24] M URALIDHAR , K., AND S ARATHY , R. Does differential [25] S WEENEY , L. k -anonymity: a model for protecting privacy. [26] T AO , Y., X IAO , X., L I , J., AND Z HANG , D. On [27] W ONG , R. C.-W., F U , A. W.-C., W ANG , K., AND P [28] W ONG , R. C.-W., L I , J., F U , A. W.-C., AND W ANG [29] W ONG , R.-W., F U , A.-C., L IU , J., W ANG , K., AND [30] W ONG , W. K., M AMOULIS , N., AND C HEUNG , D. W. L. [31] X IAO , X., AND T AO , Y. m -invariance: towards privacy [32] X IAO , X., AND T AO , Y. Anatomy: simple and effective [33] X IAO , X., AND T AO , Y. Personalized privacy preservation. [34] Y ANG , B., N AKAGAWA , H., S ATO , I., AND S AKUMA [35] Y AO , C., W ANG , X. S., AND J AJODIA , S. Checking for
