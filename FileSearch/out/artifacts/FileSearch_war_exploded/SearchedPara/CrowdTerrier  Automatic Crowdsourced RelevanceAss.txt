 Categories and Subject Descriptors: H.3.3 [Informa-tion Storage &amp; Retrieval]: Information Search &amp; Retrieval General Terms: Experimentation, Performance Keywords: Crowdsourcing, Relevance Assessment, Terrier Information retrieval (IR) systems rely on document rele-vance assessments for queries to gauge their effectiveness for a variety of tasks, e.g. Web result ranking. Evaluation forums such as TREC and CLEF provide relevance assess-ments for common tasks. However, it is not possible for such venues to cover all of the collections and tasks currently in-vestigated in IR. Hence, it falls to the individual researchers to generate the relevance assessments for new tasks and/or collections. Moreover, relevance assessment generation can be a time-consuming, difficult and potentially costly pro-cess. Recently, crowdsourcing has been shown to be a fast and cheap method to generate relevance assessments in a semi-automatic manner [1]. In this case, the relevance as-sessment task is outsourced to a large group of non-expert workers, where workers are rewarded via micro-payments.
In this demo, we present CrowdTerrier , an infrastructure extension to the open source Terrier IR platform [2] 1 that enables the semi-automatic generation of relevance assess-ments for a variety of document ranking tasks using crowd-sourcing. The aim of CrowdTerrier is to reduce the time and expertise required to effectively Crowdsource relevance assessments by abstracting away from the complexities of the crowdsourcing process. It achieves this by automating the assessment process as much as possible, via a close inte-gration of the IR system that ranks the documents (Terrier) and the crowdsourcing marketplace that is used to assess those documents (Amazon X  X  Mechanical Turk (MTurk)).
As illustrated in Figure 1, CrowdTerrier is comprised of three components. CrowdControl handles the conversion of ranked results from Terrier as well as the administration of the MTurk tasks. The JSP Interface is responsible for the presentation of the pages to be assessed and the assessment interface. Finally the Validator performs quality assurance on the assessments produced, possibly with human input. Each of these components is fully customisable to facilitate tackling different tasks and collections. http://terrier.org
