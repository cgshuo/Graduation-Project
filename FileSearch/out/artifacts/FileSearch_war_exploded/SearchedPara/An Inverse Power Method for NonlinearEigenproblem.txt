 Eigenvalue problems associated to a symmetric and positive semi-definite matrix are quite abundant in machine learning and statistics. However, considering the eigenproblem from a variational point functions leads quite naturally to a nonlinear eigenvalue problem, associated to a certain nonlin-problem are preserved and efficient algorithms for the computation of nonlinear eigenvectors are available. In this paper we present an efficient generalization of the inverse power method (IPM) to nonlinear eigenvalue problems and study the relation to the standard problem. While our IPM is a general purpose method, we show for two unsupervised learning problems that it can be easily adapted to a particular application.
 compared to [5] both in terms of runtime and the achieved Cheeger cuts. However, opposed to the suggested method in [19] our IPM is guaranteed to converge to an eigenvector of the 1 -Laplacian. The second application is sparse Principal Component Analysis (PCA). The motivation for sparse nonzero components but which still explain most of the variance. This kind of trade-off has been natural formulation as a nonlinear eigenvalue problem and can be efficiently solved with the IPM. The standard eigenproblem for a symmetric matric A  X  R n  X  n is of the form A , the eigenvectors of A can be characterized as critical points of the functional The eigenvectors of A can be computed using the Courant-Fischer Min-Max principle. While the functionals F of the form where with R + = { x  X  R | x  X  0 } we assume R : R n  X  R + , S : R n  X  R + to be convex, Lipschitz continuous, even and positively p -homogeneous 1 with p  X  1 . Moreover, we assume that for every critical point f  X  of F , we see that every critical point f  X  of F satisfies the nonlinear eigenproblem are both quadratic, r and s are linear operators and one gets back the standard eigenproblem (1). Before we proceed to the general nondifferentiable case, we have to introduce some important con-cepts from nonsmooth analysis. Note that F is in general nonconvex and nondifferentiable. In the following we denote by  X  X  ( f ) the generalized gradient of F at f according to Clarke [9], points of nonsmooth functionals is as follows.
 Definition 2.1 ([7]) A point f  X  R n is a critical point of F , if 0  X   X  X  .
 critical point and in some cases even sufficient. A useful tool is the generalized Euler identity. Theorem 2.1 ([21]) Let R : R n  X  R be a positively p -homogeneous and convex continuous func-tion. Then, for each x  X  R n and r  X   X   X  X  ( x ) it holds that  X  x,r  X   X  = pR ( x ) . Theorem 2.2 Suppose that R,S fulfill the stated conditions. Then a necessary condition for f  X  being a critical point of F is If S is continuously differentiable at f  X  , then this is also sufficient.
 r and s can be set-valued. However, as we assume R and S to be Lipschitz, the set where R and S are nondifferentiable has measure zero and thus r and s are single-valued almost everywhere. A is the inverse power method [12]. Its main building block is the fact that the iterative scheme converges to the smallest eigenvector of A . Transforming (6) into the optimization problem is the motivation for the general IPM. The direct generalization tries to solve Algorithm 1 as the objective in the optimization problem (8) is otherwise unbounded from below. (Note that the 2-norm is only chosen for algorithmic convenience). Moreover, the introduction of  X  k in Algorithm 1 is necessary to guarantee descent whereas in Algorithm 2 it would just yield a rescaled solution of the problem in the inner loop (called inner problem in the following). For both methods we show convergence to a solution of (4), which by Theorem 2.2 is a neces-naturally formulated as 1 -homogeneous problems so that we use in both cases Algorithm 1. Never-theless, we state the second algorithm for completeness. Note that we cannot guarantee convergence to the smallest eigenvector even though our experiments suggest that we often do so. However, as eigenvector with smallest eigenvalue.
 Algorithm 1 Computing a nonlinear eigenvector for convex positively p -homogeneous functions R and S with p = 1 1: Initialization: f 0 = random with f 0 = 1 ,  X  0 = F ( f 0 ) 2: repeat 3: f k +1 = arg min 5: until 6: Output: eigenvalue  X  k +1 and eigenvector f k +1 .
 clustering and sparse PCA the inner problem can be solved very efficiently, for sparse PCA it has even a closed form solution. While we do not yet have results about convergence speed, empirical observation shows that one usually converges quite quickly to an eigenvector. Algorithm 2 Computing a nonlinear eigenvector for convex positively p -homogeneous functions R and S with p &gt; 1 1: Initialization: f 0 = random,  X  0 = F ( f 0 ) 2: repeat 3: g k +1 = arg min 6: until 7: Output: eigenvalue  X  k +1 and eigenvector f k +1 .
 To our best knowledge both suggested methods have not been considered before. In [4] they propose can be seen as a special case of Algorithm 2. In [15] a generalized power method has been proposed which will be discussed in Section 5. Finally, both methods can be easily adapted to compute the largest nonlinear eigenvalue, which however we have to omit due to space constraints. the sequences terminate.
 Theorem 3.1 The sequences f k produced by Algorithms 1 and 2 converge to an eigenvector f  X  continuously differentiable at f  X  , then F has a critical point at f  X  .
 Practical implementation: By the proof of Lemma 3.1, descent in F is not only guaranteed for  X  f k ( u ) &lt; 0 =  X  f k ( f k ) for Alg. 1 and  X  f k ( u ) &lt;  X  f k ( F ( f k ) effort to solve the inner problem accurately. Second, if the inner problem is solved by a descent and F ( f k ) 1 1  X  p f k in the case of Alg. 2 as descent in F is guaranteed after one step. Spectral clustering is a graph-based clustering method (see [20] for an overview) based on a re-laxation of the NP-hard problem of finding the optimal balanced cut of an undirected graph. The Cheeger cut, see [5]. Given a weighted undirected graph with vertex set V and weight matrix W , the ratio Cheeger cut ( RCC ) of a partition ( C, C ) , where C  X  V and C = V \ C , is defined as where we assume in the following that the graph is connected. Due to limited space the normalized relation between the optimal Cheeger cut h RCC = min C  X  V RCC( C, C ) and the Cheeger cut h  X  RCC obtained by optimal thresholding the second eigenvector of the p -Laplacian, see [5, 8], considered directly the variational characterization of the ratio Cheeger cut, see also [8], In [19] they proposed a minimization scheme based on the Split Bregman method [11]. Their method However, they could not provide any convergence guarantee about their method.
 In this paper we consider the functional associated to the 1 -Laplacian  X  1 , where ( X  1 f ) i = n and study its associated nonlinear eigenproblem 0  X   X  1 f  X   X  sign( f ) .
 Proposition 4.1 Any non-constant eigenvector f  X  of the 1 -Laplacian has median zero. Moreover, For the computation of the second eigenvector we have to modify the IPM which is discussed in the next section. 4.1 Modification of the IPM for computing the second eigenvector of the 1 -Laplacian The direct minimization of (10) would be compatible with the IPM, but the global minimizer is the case p = 2 , we cannot simply project on the space orthogonal to the constant eigenvector, since mutual orthogonality of the eigenvectors does not hold in the nonlinear case.
 Algorithm 3 is a modification of Algorithm 1 which computes a nonconstant eigenvector of the 1-zero elements, respectively. Note that Algorithm 1 requires in each step the computation of some This condition ensures that the inner objective is invariant under addition of a constant and thus second eigenvector. Thus we recommend to use multiple random initializations and use the result which achieves the best ratio Cheeger cut.
 Theorem 4.1 The sequence f k produced by Algorithm 3 converges to an eigenvector f  X  of the 1 -or the sequence terminates. 4.2 Quality guarantee for 1 -spectral clustering Even though we cannot guarantee that we obtain the optimal ratio Cheeger cut, we can guarantee denotes the vector which is 1 on C and 0 else.
 Lemma 4.1 Let C, C be a partitioning of the vertex set V , and assume that | C |  X  C . Then for any vector f  X  R n of the form f =  X  1 C , where  X   X  R , it holds that F 1 ( f ) = RCC( C, C ) . Algorithm 3 Computing a nonconstant 1 -eigenvector of the graph 1 -Laplacian 1: Input: weight matrix W 2: Initialization: nonconstant f 0 with median( f 0 ) = 0 and f 0 1 = 1 , accuracy 3: repeat 4: g k +1 = arg min 8: until f  X  = 1 C satisfies F 1 ( f )  X  F 1 ( f  X  ) .
 Theorem 4.2 Let u denote the second eigenvector of the standard graph Laplacian, and f denote Then RCC( C  X  u , C  X  u )  X  RCC( C  X  f , C  X  f ) . 4.3 Solution of the inner problem The inner problem is convex, thus a solution can be computed by any standard method for solving convex nonsmooth programs, e.g. subgradient methods [3]. However, in this particular case we can exploit the structure of the problem and use the equivalent dual formulation of the inner problem. Lemma 4.3 Let E  X  V  X  V denote the set of edges and A : R E  X  R V be defined as ( A X  ) i = P The Lipschitz constant of the gradient of  X  is upper bounded by 2 max r P n s =1 w 2 rs . Compared to the primal problem, the objective of the dual problem is smooth. Moreover, it can be efficiently solved using FISTA ([2]), a two-step subgradient method with guaranteed convergence guarantees descent in functional (9) and thus makes the modified IPM very fast. The implementation can be found in the supplementary material. Principal Component Analysis (PCA) is a standard technique for dimensionality reduction and data given a data matrix X  X  R n  X  p where each column has mean 0 , in PCA one computes where the maximizer f  X  is the largest eigenvector of the covariance matrix  X  = X T X  X  R p  X  p . sparse PCA one wants to get a small number of features which still capture most of the variance. sparsity of the PCA component, which yields a trade-off between explained variance and sparsity. While standard PCA leads to an eigenproblem, adding a constraint on the cardinality, i.e. the num-ber of nonzero coefficients, makes the problem NP-hard. The first approaches performed simple thresholding of the principal components which was shown to be misleading [6]. Since then several methods have been proposed, mainly based on penalizing the L 1 norm of the principal components, including SCoTLASS [14] and SPCA [22]. D X  X spremont et al.[10] focused on the L 0 -constrained formulation and proposed a greedy algorithm to compute a full set of good candidate solutions up Moghaddam et al. [16] used branch and bound to compute optimal solutions for small problem instances. Other approaches include D.C. [18] and EM-based methods [17]. Recently, Journee et al. [15] proposed two single unit (computation of one component only) and two block (simultaneous computation of multiple components) methods based on L 0 -penalization and L 1 -penalization. Problem (11) is equivalent to L 2 norm in the enumerator, which yields the functional that the formulation (12) fits in our general framework, as both enumerator and denominator are 1 -homogeneous functions. The inner problem of the IPM becomes This problem has a closed form solution. In the following we use the notation x + = max { 0 ,x } . Lemma 5.1 The convex optimization problem (13) has the analytical solution sparse principal components shown in Algorithm 4. While the derivation is quite different from thresholding parameter of the inner problem depends on the current eigenvalue estimate whereas it Algorithm 4 Sparse PCA 1: Input: data matrix X , sparsity controlling parameter  X  , accuracy 2: Initialization: f 0 = random with S ( f k ) = 1 ,  X  0 = F ( f k ) 3: repeat 8: until 1-Spectral Clustering: We compare our IPM with the total variation (TV) based algorithm by thresholding the second eigenvector of the graph Laplacian ( p = 2 ). The graph and the two-moons the second eigenvector of the unnormalized graph Laplacian. For [19] we initialize once with the second eigenvector of the normalized graph Laplacian as proposed in [19] and 10 times randomly. Figure 1: Left and middle: Second eigenvector of the 1-Laplacian and 2-Laplacian, respectively. Right: Relative Variance (relative to maximal possible variance) versus number of non-zero compo-nents for the three datasets Lung2, GCM and Prostate1.
 Next we perform unnormalized 1 -spectral clustering on the full USPS and MNIST-datasets ( 9298 resp. 70000 points). As clustering criterion we use the multicut version of RCut , given as ment, we perform one run initialized with the thresholded second eigenvector of the unnormalized graph Laplacian in the case of the IPM and with the second eigenvector of the normalized graph table shows the obtained RCut and errors.
 method achieves the best RCut . However, if one wants to do only a single run, by Theorem 4.2 initializes with the thresholded 2nd eigenvector of the 2-Laplacian.
 Sparse PCA: We evaluate our IPM for sparse PCA on gene expression datasets obtained from [1]. We compare with two recent algorithms: the L 1 based single-unit power algorithm of [15] as well as the EM-based algorithm in [17]. For all considered datasets, the three methods achieve very similar performance in terms of the tradeoff between explained variance and sparsity of the algorithms produce the same trade-off curve if one uses the same initialization strategy. Acknowledgments: This work has been supported by the Excellence Cluster on Multimodal Com-puting and Interaction at Saarland University.
