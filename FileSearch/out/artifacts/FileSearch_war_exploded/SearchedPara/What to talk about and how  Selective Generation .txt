 We consider the important task of producing a natu-ral language description of a rich world state rep-resented as an over-determined database of event records. This task, which we refer to as selective generation, is often formulated as two subproblems: content selection , which involves choosing a sub-set of relevant records to talk about from the ex-haustive database, and surface realization , which is concerned with generating natural language descrip-tions for this subset. Learning to perform these tasks jointly is challenging due to the uncertainty in decid-ing which records are relevant, the complex depen-dencies between selected records, and the multiple ways in which these records can be described.
Previous work has made significant progress on this task (Chen and Mooney, 2008; Angeli et al., 2010; Kim and Mooney, 2010; Konstas and Lap-ata, 2012). However, most approaches solve the two content selection and surface realization sub-tasks separately, use manual domain-dependent re-sources (e.g., semantic parsers) and features, or em-ploy template-based generation. This limits do-main adaptability and reduces coherence. We take an alternative, neural encoder-aligner-decoder ap-proach to free-form selective generation that jointly performs content selection and surface realization, without using any specialized features, resources, or generation templates. This enables our approach to generalize to new domains. Further, our memory-based model captures the long-range contextual de-pendencies among records and descriptions, which are integral to this task (Angeli et al., 2010).
We formulate our model as an encoder-aligner-decoder framework that uses recurrent neural net-works with long short-term memory units (LSTM-RNNs) (Hochreiter and Schmidhuber, 1997) to-gether with a coarse-to-fine aligner to select and  X  X ranslate X  the rich world state into a natural lan-guage description. Our model first encodes the bidirectional LSTM-RNN. A novel coarse-to-fine aligner then reasons over multiple abstractions of the input to decide which of the records to discuss. The model next employs an LSTM decoder to gen-erate natural language descriptions of the selected records.

The use of LSTMs, which have proven effective for similar long-range generation tasks (Sutskever et al., 2014; Vinyals et al., 2015b; Karpathy and Fei-Fei, 2015), allows our model to capture the long-range contextual dependencies that exist in selec-tive generation. Further, the introduction of our pro-posed variation on alignment-based LSTMs (Bah-danau et al., 2014; Xu et al., 2015) enables our model to learn to perform content selection and sur-face realization jointly, by aligning each generated word to an event record during decoding. Our novel coarse-to-fine aligner avoids searching over the full set of over-determined records by employing two stages of increasing complexity: a pre-selector and a refiner acting on multiple abstractions (low-and high-level) of the record input. The end-to-end na-ture of our framework has the advantage that it can be trained directly on corpora of record sets paired with natural language descriptions, without the need for ground-truth content selection.

We evaluate our model on a benchmark weather forecasting dataset (W EATHER G OV ) and achieve the best results reported to-date on content selection ( 12% relative improvement in F-1) and language generation ( 59% relative improvement in BLEU), despite using no domain-specific resources. We also perform a series of ablations and visualiza-tions to elucidate the contributions of the primary model components, and also show improvements with a simple, k -nearest neighbor beam filter ap-proach. Finally, we demonstrate the generalizability of our model by directly applying it to a benchmark sportscasting dataset (R OBO C UP ), where we get re-sults competitive with or better than state-of-the-art, despite being extremely data-starved. Selective generation is a task where a natural lan-guage description is produced for a salient subset of a rich world state represented as an over-determined database of event records. A good deal of atten-tion in this area has been paid to the individual content selection and selective realization subprob-lems. With regards to the former, Barzilay and Lee (2004) model the content structure from unanno-tated documents and apply it to the application of text summarization. Barzilay and Lapata (2005) treat content selection as a collective classification problem and simultaneously optimize the local label assignment and their pairwise relations. Liang et al. (2009) address the related task of aligning a set of records to given textual description clauses. They propose a generative semi-Markov alignment model that jointly segments text sequences into utterances and associates each to the corresponding record.
Surface realization is often treated as a problem of producing text according to a given representation (Reiter et al., 2000). Walker et al. (2001) and Stent et al. (2004) design trainable sentence planners to generate sentences (and their combinations) for con-text planning and dialog, relying upon various lin-guistics features. Soricut and Marcu (2006) propose a language generation system that uses the WIDL-representation, a formalism used to compactly rep-resent probability distributions over finite sets of strings. Wong and Mooney (2007) and Lu and Ng (2011) use synchronous context-free grammars to generate natural language sentences from formal meaning representations. Similarly, Belz (2008) em-ploys probabilistic context-free grammars to per-form surface realization. Other effective approaches include the use of tree conditional random fields (Lu et al., 2009) and template extraction within a log-linear framework (Angeli et al., 2010).

Recent work seeks to solve the full selective generation problem through a single framework. Chen and Mooney (2008) and Chen et al. (2010) learn alignments between comments and their cor-responding event records using a translation model for parsing and generation. Kim and Mooney (2010) implement a two-stage framework that decides what to discuss using a combination of the methods of Lu et al. (2008) and Liang et al. (2009), and then produces the text based on the generation system of Wong and Mooney (2007).

Angeli et al. (2010) propose a unified concept-to-text model that treats joint content selection and surface realization as a sequence of local decisions represented by a log-linear model. Similar to other work, they train their model using external align-ments from Liang et al. (2009). Generation then fol-lows as inference over this model, where they first choose an event record, then the record X  X  fields (i.e., attributes), and finally a set of templates that they then fill in with words for the selected fields. Their ability to model long-range dependencies relies on their choice of features for the log-linear model, while the template-based generation further employs some domain-specific features for fluent output.
Konstas and Lapata (2012) propose an alternative method that simultaneously optimizes the content selection and surface realization problems. They employ a probabilistic context-free grammar that specifies the structure of the event records, and then treat generation as finding the best derivation tree according to this grammar. However, their method still selects and orders records in a local fashion via a Markovized chaining of records. Konstas and La-pata (2013) improve upon this approach with global document representations. However, this approach also requires alignment during training, which they estimate using the method of Liang et al. (2009).
We treat the problem of selective generation as end-to-end learning via a recurrent neural network encoder-aligner-decoder model, which enables us to jointly learn content selection and surface re-alization directly from database-text pairs, without the need for an external aligner or ground-truth se-lection labels. The use of LSTM-RNNs enables our model to capture the long-range dependencies that exist among the records and natural language output. Additionally, the model does not rely on any manually-selected or domain-dependent fea-tures, templates, or parsers, and is thereby general-izable. The alignment-RNN approach has recently proven successful for generation-style tasks, e.g., machine translation (Bahdanau et al., 2014) and im-age captioning (Xu et al., 2015). Since selective generation requires identifying the small number of salient records among an over-determined database, we avoid performing exhaustive search over the full record set, and instead propose a novel coarse-to-fine aligner that divides the search complexity into pre-selection and refinement stages. We consider the problem of generating a natural language description for a rich world state speci-fied in terms of an over-determined set of records (database). This problem requires deciding which of the records to discuss (content selection) and r x r x how to discuss them (surface realization). Train-i = 1 , 2 ,...,n , where r ( i ) is the complete set of (Fig. 1). At test time, only the records are given. We evaluate our model in the context of two publicly-available benchmark selective generation datasets. W
EATHER G OV The weather forecasting dataset (see Fig. 1(a)) of Liang et al. (2009) consists of 29528 scenarios, each with 36 weather records (e.g., temperature, sky cover, etc.) paired with a natural language forecast ( 28 . 7 avg. word length). R
OBO C UP We evaluate our model X  X  generaliz-ability on the sportscasting dataset of Chen and Mooney (2008), which consists of only 1539 pairs of temporally ordered robot soccer events (e.g., pass, score) and commentary drawn from the four-game 2001 X 2004 RoboCup finals (see Fig. 1(b)). Each scenario contains an average of 2 . 4 event records and a 5 . 7 word natural language commentary. We formulate selective generation as inference over a probabilistic model P ( x 1: T | r 1: N ) , where r 1: N = ( r 1 ,r 2 ,...,r N ) is the input set of over-is the generated description with x t being the word at time t and x 0 being a special start token:
The goal of inference is to generate a natural lan-guage description for a given set of records. An effective means of learning to perform this gen-eration is to use an encoder-aligner-decoder archi-tecture with a recurrent neural network, which has proven effective for related problems in machine translation (Bahdanau et al., 2014) and image cap-tioning (Xu et al., 2015). We propose a variation on this general model with novel components that are well-suited to the selective generation problem.
Our model (Fig. 2) first encodes each input record r j into a hidden state h j with j  X  X  1 ,...,N } us-ing a bidirectional recurrent neural network (RNN). Our novel coarse-to-fine aligner then acts on a con-catenation m j of each record and its hidden state as multi-level representation of the input to compute the selection decision z t at each decoding step t . The model then employs an RNN decoder to arrive at the word likelihood P ( x t | x 0: t  X  1 ,r 1: N ) as a function of the multi-level input and the hidden state of the de-coder s t  X  1 at time step t  X  1 . In order to model the long-range dependencies among the records and de-scriptions (which is integral to effectively perform-ing selective generation (Angeli et al., 2010; Kon-stas and Lapata, 2012; Konstas and Lapata, 2013)), our model employs LSTM units as the nonlinear en-coder and decoder functions.
 Encoder Our LSTM-RNN encoder (Fig. 2) takes as input the set of event records rep-resented as a sequence r 1: N = ( r 1 ,r 2 ,...,r N and returns a sequence of hidden annotations h 1: N = ( h 1 ,h 2 ,...,h N ) , where the annotation h j summarizes the record r j . This results in a represen-tation that models the dependencies that exist among the records in the database.We adopt an encoder ar-chitecture similar to that of Graves et al. (2013) where T e is an affine transformation,  X  is the logis-tic sigmoid that restricts its input to [0 , 1] , i e and o e j are the input, forget, and output gates of the LSTM, respectively, and c e j is the memory cell acti-vation vector. The memory cell c e j summarizes the LSTM X  X  previous memory c e j  X  1 and the current in-put, which are modulated by the forget and input gates, respectively. Our encoder operates bidirec-tionally, encoding the records in both the forward and backward directions, which provides a better summary of the input records. In this way, the hid-den annotations h j = ( ward mined using Equation (2c).
 Coarse-to-Fine Aligner Having encoded the in-put records r 1: N to arrive at the hidden annotations h 1: N , the model then seeks to select the content at each time step t that will be used for generation. Our model performs content selection using an extension of the alignment mechanism proposed by Bahdanau et al. (2014), which allows for selection and genera-tion that is independent of the ordering of the input.
In selective generation, the given set of event records is over-determined with only a small subset of salient records being relevant to the output natu-ral language description. Standard alignment mech-anisms limit the accuracy of selection and genera-tion by scanning the entire range of over-determined records. In order to better address the selective generation task, we propose a coarse-to-fine aligner that prevents the model from being distracted by multiple abstractions of the input: both the origi-nal input record as well as the hidden annotations m been shown to yield better results than aligning based only on the hidden state (Mei et al., 2015).
Our coarse-to-fine aligner avoids searching over the full set of over-determined records by using two stages of increasing complexity: a pre-selector and refiner (Fig. 2). The pre-selector first assigns to each record a probability p j of being selected, while the standard aligner computes the alignment likelihood w tj over all the records at each time step t during decoding. Next, the refiner produces the final se-lection decision by re-weighting the aligner weights w tj with the pre-selector probabilities p j : where P , q , U , W , v are learned parameters. Ideally, the selection decision would be based on the highest-value alignment z t = m k where k = arg max j  X  tj . However, we use the weighted average (Eqn. 3e) as its soft approximation to maintain differentiability of the entire architecture.
The pre-selector assigns large values ( p j &gt; 0 . 5 ) to a small subset of salient records and small val-ues ( p j &lt; 0 . 5 ) to the rest. This modulates the stan-dard aligner, which then has to assign a large weight w tj in order to select the j -th record at time t . In this way, the learned prior p j makes it difficult for the alignment (attention) to be distracted by non-salient records. Further, we can relate the output of the pre-selector to the number of records that are selected. Specifically, the output p j expresses the extent to which the j -th record should be selected. The summation a real-valued approximation to the total number of pre-selected records (denoted as  X  ), which we regu-larize towards, based on validation (see Eqn. 5). Decoder Our architecture uses an LSTM decoder that takes as input the current context vector z t the last word x t  X  1 , and the LSTM X  X  previous hid-den state s t  X  1 . The decoder outputs the conditional probability distribution P x,t = P ( x t | x 0: t  X  1 ,r over the next word, represented as a deep output layer (Pascanu et al., 2014), where E (an embedding matrix), L 0 , L s , and L z are parameters to be learned.
 Training and Inference We train the model us-ing the database-record pairs ( r 1: N ,x 1: T ) from the training corpora so as to maximize the likelihood of Additionally, we introduce a regularization term ( P the pre-selector weights based on the aforemen-tioned relationship between the output of the pre-selector and the number of selected records. More-over, we also introduce the term (1 . 0  X  max( p j )) , which accounts for the fact that at least one record should be pre-selected. Note that when  X  is equal to N , the pre-selector is forced to select all the records ( p j = 1 . 0 for all j ), and the coarse-to-fine alignment reverts to the standard alignment introduced by Bah-danau et al. (2014). Together with the negative log-loss function becomes
Having trained the model, we generate the natu-ral language description by finding the maximum a posteriori words under the learned model (Eqn. 1). with the first word x 1 . Beam search offers a way to perform approximate joint inference  X  however, we empirically found that beam search does not perform any better than greedy search on the datasets that we consider, an observation that is shared with previous work (Angeli et al., 2010). We later discuss an al-ternative k -nearest neighbor-based beam filter (see Sec 6.2). Datasets We analyze our model on the benchmark W EATHER G OV dataset, and use the data-starved R
OBO C UP dataset to demonstrate the model X  X  gen-eralizability. Following Angeli et al. (2010), we use W EATHER G OV training, development, and test splits of size 25000 , 1000 , and 3528 , respectively. For R OBO C UP , we follow the evaluation method-ology of previous work (Chen and Mooney, 2008), performing three-fold cross-validation whereby we train on three games (approximately 1000 scenarios) and test on the fourth. Within each split, we hold out 10% of the training data as the development set to tune the early-stopping criterion and  X  . We then report the standard average performance (weighted by the number of scenarios) over these four splits. Dataset Processing In this section, we present the implementation details regarding our data prepro-cessing. We use W EATHER G OV as an example here, since it is our primary dataset, and the same recipe is followed for R OBO C UP .

For tokenization of the textual descriptions, we simply treat as token each string unit delimited by a space, which includes regular words ( X  X unny X ), punctuation ( X , X ), and numerical values ( X 20 X ). A special token is added to represent the beginning and end of the entire textual description. This operation results in a vocabulary of size 338 , and we did not filter out any rare tokens. Moreover, in this setup, numerical values are also generated as any other to-ken during decoding period.

For event record representation, we represent each event as a fixed-length vector, concatenated by mul-tiple  X  X ttribute (field) vectors X . Each attribute vec-tor represents either a 1) record type (e.g.,  X  X ain-Chance X ) with a one-hot vector, 2) record time slot (e.g.,  X 06:00 X 21:00 X ) with a one-hot vector, 3) record mode (e.g.,  X  X SE X ) with a one-hot vector, or, 4) record value (e.g.,  X 20 X ) with a 0-1 vector. The 0-1 vector for record value is simply the signed binary representation of this number. We choose the usage of 0-1 binary representation vectors for numbers be-cause it allows us to share binning-style information between nearby numbers (whereas a one-hot vector is sparse).
 Training Details On W EATHER G OV , we lightly tune the number of hidden units and  X  on the de-velopment set according to the generation metric (BLEU), and choose 500 units from { 250 , 500 , 750 } R
OBO C UP , we only tune  X  on the development set and choose  X  = 5 . 0 from the set { 1 . 0 , 2 . 0 ,..., 6 . 0 } . However, we do not retune the number of hidden units on R OBO C UP . For each iteration, we ran-domly sample a mini-batch of 100 scenarios during back-propagation and use Adam (Kingma and Ba, 2015) for optimization. Training typically converges within 30 epochs. We select the model according to Evaluation Metrics We consider two metrics as a means of evaluating the effectiveness of our model on the two selective generation subproblems. For content selection, we use the F-1 score of the set of selected records as defined by the harmonic mean of precision and recall with respect to the ground-truth selection record set. We define the set of selected records as consisting of the record with the largest selection weight  X  ti computed by our aligner at each decoding step t .

We evaluate the quality of surface realization us-ing the BLEU score 6 (a 4 -gram matching-based pre-cision) (Papineni et al., 2001) of the generated de-scription with respect to the human-created refer-ence. To be comparable to previous results on W
EATHER G OV , we also consider a modified BLEU score (cBLEU) that does not penalize numerical de-viations of at most five (Angeli et al., 2010) (i.e., to not penalize  X  X ow around 58 X  compared to a ref-erence  X  X ow around 60 X ). On R OBO C UP , we also evaluate the BLEU score in the case that ground-truth content selection is known (sBLEU G ), to be comparable to previous work. We analyze the effectiveness of our model on the benchmark W EATHER G OV (as primary) and R
OBO C UP (as generalization) datasets. We also present several ablations to illustrate the contribu-tions of the primary model components. 6.1 Primary Results (W EATHER G OV ) We report the performance of content selection and surface realization using F-1 and two BLEU scores (standard sBLEU and the customized cBLEU of An-geli et al. (2010)), respectively (Sec. 5). Table 1 compares our test results against previous meth-ods that include KL12 (Konstas and Lapata, 2012), KL13 (Konstas and Lapata, 2013), and ALK10 (An-geli et al., 2010). Our method achieves the best results reported to-date on all three metrics, with relative improvements of 11 . 94% (F-1), 58 . 88% (sBLEU), and 36 . 68% (cBLEU) over the previous state-of-the-art. 6.2 Beam Filter with k -Nearest Neighbors We perform greedy search as an approximation to full inference over the set of decision variables (Eqn. 1). We considered beam search as an alterna-tive, but as with previous work on this dataset (An-geli et al., 2010), we found that greedy search still yields better BLEU performance (Table 2).

As an alternative, we consider a beam filter based on a k -nearest neighborhood. First, we generate the M -best description candidates (i.e., a beam width of M ) for a given input record set (database) us-ing standard beam search. Next, we find the K nearest neighbor database-description pairs from the training data, based on the cosine similarity of each neighbor database with the given input record. We then compute the BLEU score for each of the M de-scription candidates relative to the K nearest neigh-bor descriptions (as references) and select the candi-date with the highest BLEU score. We tune K and M on the development set and report the results in Table 3. Table 4 presents the test results with this tuned setting ( M = 2 , K = 1 ), where we achieve BLEU scores better than our primary greedy results. 6.3 Ablation Analysis (W EATHER G OV ) Next, we present several ablations to analyze the Aligner Ablation First, we evaluate the contribu-tion of our proposed coarse-to-fine aligner by com-paring our model with the basic encoder-aligner-decoder model introduced by Bahdanau et al. (2014) (which we originally started with). Table 5 reports the results demonstrating that our two-level aligner yields superior F-1 and BLEU scores relative to a Encoder Ablation Next, we consider the effec-tiveness of the encoder. Table 6 compares the results with and without the encoder on the development set, and demonstrates that there is a significant gain from encoding the event records using the LSTM-RNN. We attribute this improvement to the LSTM-RNN X  X  ability to capture the relationships that exist among the records, which is known to be essential to selective generation (Barzilay and Lapata, 2005; Angeli et al., 2010).
 6.4 Qualitative Analysis (W EATHER G OV ) Output Examples Fig. 3 shows an example record set with its output description and record-word alignment heat map. As shown, our model learns to align records with their corresponding words (e.g., windDir and  X  X outheast, X  temperature and  X 71, X  windSpeed and  X  X ind 10, X  and gust and  X  X inds could gust as high as 30 mph X ). It also learns the subset of salient records to talk about (matching the ground-truth description perfectly for this ex-ample, i.e., a standard BLEU of 100 . 00 ). We also see some word-level mismatch, e.g.,  X  X loudy X  mis-aligns to id-0 temp and id-10 precipChance, which we attribute to the high correlation between these types of records ( X  X arbage collection X  in Liang et al. (2009)).
 Word Embeddings (Trained &amp; Pretrained) Training our decoder has the effect of learning em-beddings for the words in the training set (via the embedding matrix E in Eqn. 4). Here, we ex-plore the extent to which these learned embeddings capture semantic relationships among the training words. Table 7 presents nearest neighbor words for some of the common words from the W EATHER -G
OV dataset (according to cosine similarity in the embedding space).

We also consider different ways of using pre-trained word embeddings (Mikolov et al., 2013) to bootstrap the quality of our learned embeddings. One approach initializes our embedding matrix with the pre-trained vectors and then refines the embed-ding based on our training corpus. The second con-catenates our learned embedding matrix with the pre-trained vectors in an effort to simultaneously ex-ploit general similarities as well as those learned for the domain. As shown previously for other tasks (Vinyals et al., 2014; Vinyals et al., 2015b), we find that the use of pre-trained embeddings results in negligible improvements (on the development set). 6.5 Out-of-Domain Results (R OBO C UP ) We use the R OBO C UP dataset to evaluate the domain-independence of our model. The dataset is severely data-starved with only 1000 (approx.) training pairs, which is much smaller than is typi-cally necessary to train RNNs. This results in higher variance in the trained model distributions, and we thus adopt the standard denoising method of ensem-bles (Sutskever et al., 2014; Vinyals et al., 2015b;
Following previous work, we perform two exper-iments on the R OBO C UP dataset (Table 8), the first considering full selective generation and the second assuming ground-truth content selection at test time. On the former, we obtain a standard BLEU score (sBLEU) of 25 . 28 , which exceeds the best score of 24 . 88 (Konstas and Lapata, 2012). Additionally, we achieve an selection F-1 score of 81 . 58 , which is also the best result reported to-date. In the case of assumed (known) ground-truth content selection, our model attains an sBLEU G score of 29 . 40 , which We presented an encoder-aligner-decoder model for selective generation that does not use any spe-cialized features, linguistic resources, or genera-tion templates. Our model employs a bidirec-tional LSTM-RNN model with a novel coarse-to-fine aligner that jointly learns content selection and surface realization. We evaluate our model on the benchmark W EATHER G OV dataset and achieve state-of-the-art selection and generation results. We achieve further improvements via a k -nearest neigh-bor beam filter. We also present several model ab-lations and visualizations to elucidate the effects of the primary components of our model. Moreover, our model generalizes to a different, data-starved do-main (R OBO C UP ), where it achieves results compet-itive with or better than the state-of-the-art. We thank Gabor Angeli, David Chen, Ioannis Kon-stas, and the reviewers for their helpful comments.
