 learning [2] aims at impro ving the generalization performance by utilizing both the labeled and unlabeled data. The label dependencies among patterns are captured by exploiting the intrinsic geometric structure of the data. The underlying smoothness assumption is that two nearby patterns common to approximate this manifold by a weighted graph, leading to graph-based semi-supervised be easily extended to out-of-sample patterns.
 Recently , attention is dra wn to the development of inducti ve methods, such as harmonic mixtures work proposed in [1]. By dening a data-dependent reproducing kernel Hilbert space (RKHS), smooth on the manifold. Kernel methods, which have been highly successful in supervised learn-ing, can then be inte grated with this RKHS. The resultant Laplacian SVM (LapSVM) demonstrates state-of-the-art semi-supervised learning performance [10 ]. Ho we ver, a decienc y of the LapSVM is that its solution, unlik e that of the SVM, is not sparse and so is much slo wer on testing. of unlabeled data available, existing algorithms are only capable of handling a small to moderate amount of unlabeled data. Recently , attempts have been made to scale up these methods. Sindhw ani not be exible enough for complicated tar get functions. Garck e and Griebel [5] proposed to use supervised learning methods can only handle 100  X  10,000 unlabeled examples. More recently , the y work ed with involv e 75,888 labeled and unlabeled examples. Thus, no one has ever been experimented on massi ve data sets with, say , one million unlabeled examples.
 On the other hand, the Core Vector Machine (CVM) is recently proposed for scaling up kernel meth-ball (MEB) problem in computational geometry , and then use an (1 + ) -approximation algorithm to obtain a close-to-optimal solution efciently . Given m samples, the CVM has an asymptotic time . Experimental results on real world data sets with millions of patterns demonstrated that the CVM is much faster than existing SVM implementations and can handle much lar ger data sets. In this paper , we extend the CVM to semi-supervised learning. To restore sparsity of the LapSVM In Section 2, we rst give a brief revie w on manifold regularization. Section 3 then describes the proposed algorithm for semi-supervised classication and regression. Experimental results on very lar ge data sets are presented in Section 4, and the last section gives some concluding remarks. Given a training set f ( x i ; y i ) g m Given a kernel k and its RKHS H k , we minimize the regularized risk over function f in H k : Here, k k H theorem, the minimizer f admits the representation f ( x ) = P m Therefore, the problem is reduced to the optimization over the nite-dimensional space of i 's. In semi-supervised learning, we have both labeled examples f ( x i ; y i ) g m f x f P P X of X [1].
 manifold, and tak e k f k 2 V and E being the sets of vertices and edges respecti vely . Denote the weight function w and degree is used, and s ( u ) = 1 with the unnormalized one. As sho wn in [1], the minimizer of (2) becomes f ( x ) = P m + n i =1 i k ( x i ; x ) , which depends on both labeled and unlabeled examples. 2.1 Laplacian SVM SVM (LapSVM) [1]. Its training involv es two steps. First, solv e the quadratic program (QP) max 0 1 1 2 0 Q : 0 y = 0 ; 0 1 K graph Laplacian, and J 3.1 Sparsied Manif old Regularizer izer (3) by the -insensiti ve loss function 2 , as minimization and, using the standard representer theorem, the minimizer f then admits the form f ( x ) = P m + n i =1 i k ( x i ; x ) , same as that of the original manifold regularization. Moreo ver, putting f ( x ) = w 0 ' ( x ) + b into (4), we obtain k f k 2 LapSVM can then be formulated as: Here, jEj is the number of edges in the graph, i is the slack variable for the error , e ; variables for edge e , and C; ; are user -dened parameters. As in pre vious CVM formulations [12 , 13], the bias b is penalized and the two-norm errors ( 2 constraints i ; ij ; LapSVM (using two-norm errors). When is also zero, it becomes the Lagrangian SVM.
 The dual can be easily obtained as the follo wing QP: where = [ 1 ; : : : ; m ] 0 ; = [ 1 ; : : : ; jEj ] 0 ; = [ 2 4 K ` is the kernel matrix dened using kernel k on the m labeled examples, U jEjjEj = [ O (1) kernel evaluations. This is particularly favorable to decomposition methods such as SMO as most of the CPU computations are typically dominated by kernel evaluations.
 Moreo ver, it can be sho wn that is a parameter that controls the size of " , analogous to the parameter in -SVR. Hence, only , but not " , appears in (8). Moreo ver, the primal vari-ables can be easily reco vered from the dual variables by the KKT conditions. In particular , ned on both the labeled and unlabeled examples, as in standard manifold regularization. 3.2 Transf orming to a MEB Pr oblem for some 0 2 R m and 2 R . From the variables in (8), dene ~ = 0 0 0 0 and as max ~ 0 ( diag ( ~ K ) + 1 ) ~ 0 ~ K ~ : ~ 0 ; ~ 0 1 = 1 , which is of the form in (9). QP to the form in (9) is also straightforw ard. 3.3 Sparsity similar effect as the ` 1 penalty in LASSO [11 ], which is kno wn to produce sparse approximation. 3.3.1 KKT Perspecti ve from the denition of KKT conditions associated with (7), the corresponding e 's and combination of the k ( x i ; x ) 's weighted by i and e e (Section 3.1), f is thus sparse. 3.3.2 LASSO Perspecti ve Our exposition will be along the line pioneered by Girosi [7], who established a connection between the -insensiti ve loss in SVR and sparse approximation. Given a predictor f ( x ) = P L 2 norm for the error , Girosi argued that the norm of the RKHS H k is a better measure of smooth-ness. Ho we ver, the RKHS norm operates on functions, while here we have vectors f and y w.r.t. x ; : : : ; x m . Hence, we will use the kernel PCA map with k y f k 2 K ( y f ) K 1 ( y f ) . k k 2 2 . As in LASSO, we also add a ` 1 penalty on . The optimization problem is formulated as: where C and are constants. As in [7], we decompose as , where ; 0 and i i = 0 .
 Then, (10) can be rewritten as: using the -insensiti ve loss: min k w k 2 + loss in SVR achie ves a similar effect as using the error k y f k 2 details are skipped. As abo ve, the key steps are on replacing the ` 2 norm by the kernel PCA map, (based on the -insensiti ve loss) can again be reco vered by using the LASSO penalty . 3.4 Complexities and unlabeled examples for a x ed . In contrary , LapSVM involv es an expensi ve matrix inversion for K ( m + n ) ( m + n ) and requires O (( m + n ) 3 ) time and O (( m + n ) 2 ) space. 3.5 Remarks is to extend it for the LapSVM. This  X reduced LapSVM X  solv es a smaller optimization problem that and O (( m + n ) r ) space. Experimental comparisons based on this will be made in Section 4. used in lar ge-scale linear or inte ger programs. Both start with only a small number of nonzero variables, and the restricted master problem in column generation corresponds to the inner QP that is solv ed at each CVM iteration. Moreo ver, both can be regarded as primal methods that maintain howe ver, is that CVM exploits the  X approximateness X  as in other approximation algorithms. Instead On the other hand, we are not aware of any similar results for column generation.
 By regarding the CVM as the approximation algorithm counterpart of column generation, this sug-gests that the CVM can also be used in the same way as column generation in speeding up other optimization problems. For example, the CVM can also be used for SVM training with other loss is that its con vergence bound and comple xity results in Section 3.4 may no longer be available. In this section, we perform experiments on some massi ve data sets 6 (Table 1). The graph (for the in (3) is dened as exp( k x u x ed at 1, and the other parameters are tuned by a small validation set. Unless otherwise specied, we use the Gaussian kernel exp ( k x z k 2 = ) , with = 1 we also run the LapSVM 7 and another LapSVM implementation based on the reduced SVM [8] (Section 3.5). All the experiments are performed on a 3.2GHz Pentium X 4 PC with 1GB RAM. 4.1 Two-Moons Data Set We rst perform experiments on the popular two-moons data set, and use one labeled example for the Gaussian kernel is set to = 0 : 25 . For the reduced LapSVM implementation, we x r = 200 . Figure 1: Results on the two-moons data set (some abscissas and ordinates are in log scale). The two labeled examples are labeled in red in Figure 1(a).
 Results are sho wn in Figure 1. Both the LapSVM and SLapCVM always attain 100% accurac y on the test set, even with only two labeled examples (Figure 1(b)). Ho we ver, SLapCVM is faster than LapSVM (Figure 1(c)). Moreo ver, as mentioned in Section 2.1, the LapSVM solution is non-sparse hand, SLapCVM uses only a small fraction of the examples. As can be seen from Figures 1(c) and 1(d), both the time and space required by the SLapCVM are almost constant, even when the unlabeled data set gets very lar ge. The reduced LapSVM, though also fast, is slightly inferior to both the SLapCVM and LapSVM. Moreo ver, note that both the standard and reduced LapSVMs cannot be run on the full data set on our PC because of their lar ge memory requirements. 4.2 Extended USPS Data Set The second experiment is performed on the USPS data from [12]. One labeled example is randomly sampled from each class for training. To achie ve comparable accurac y, we use r = 2 ; 000 for the reduced LapSVM. For comparison, we also train a standard SVM with the two labeled examples. Results are sho wn in Figure 2. As can be seen, the SLapCVM is again faster (Figures 2(a)) and produces a sparser solution than LapSVM (Figure 2(b)). For the SLapCVM, both the time required and number of kernel expansions involv ed gro w only sublinearly with the number of unlabeled examples. Figure 2(c) demonstrates that semi-supervised learning (using either the LapSVMs or SLapCVM) can have much better generalization performance than supervised learning using the labeled examples only . Note that although the use of the 2-norm error in SLapCVM could in theory be less rob ust than the use of the 1-norm error in LapSVM, the SLapCVM solution is indeed always more accurate than that of LapSVM. On the other hand, the reduced LapSVM has comparable speed with the SLapCVM, but its performance is inferior and cannot handle lar ge data sets.
Figure 2: Results on the extended USPS data set (some abscissas and ordinates are in log scale). 4.3 Extended MIT Face Data Set example are randomly sampled from each class and used in training. Because of the imbalanced here. Instead, we will use the area under the ROC curv e (AUC) and the balanced loss 1 ( TP + comparison, we also train two SVMs: one uses the 10 labeled examples only while the other uses all the labeled examples (a total of 889,986) in the original training set of [12]. Figure 3 sho ws the results. Ag ain, the SLapCVM is faster and produces a sparser solution than LapSVM. Note that the SLapCVM, using only 10 labeled examples, can attain comparable AUC and unlabeled data can be utilized. On the other hand, note that the LapSVM again cannot be run with more than 3,000 unlabeled examples on our PC because of its high space requirement. The reduced LapSVM performs very poorly here, possibly because this data set is highly imbalanced.
Figure 3: Results on the extended MIT face data (some abscissas and ordinates are in log scale). rst issue, we introduce a sparsied manifold regularizer based on the -insensiti ve loss. For the second issue, we inte grate manifold regularization with the CVM. The resultant algorithm has low time and space comple xities. Moreo ver, by avoiding the underlying matrix inversion in the origi-nal LapSVM, a sparse solution can also be reco vered. Experiments on a number of massi ve data sets sho w that the SLapCVM is much faster than the LapSVM. Moreo ver, while the LapSVM can only handle several thousand unlabeled examples, the SLapCVM can handle one million unlabeled examples on the same machine. On one data set, this produces comparable or even better perfor -mance than the (supervised) CVM trained on 900K labeled examples. This clearly demonstrates the usefulness of semi-supervised learning when a lar ge amount of unlabeled data can be utilized.
