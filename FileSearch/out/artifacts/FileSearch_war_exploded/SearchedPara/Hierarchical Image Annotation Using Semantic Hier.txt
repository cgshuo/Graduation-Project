 Semantic hierarchies have been introduced recently to im-prove image annotation. They was used as a framework for hierarchical image classification, and thus to improve classi-fiers accuracy and reduce the complexity of managing large scale data. In this paper, we investigate the contribution of semantic hierarchies for hierarchical image classification. We propose first a new method based on the hierarchy struc-ture to train efficiently hierarchical classifiers. Our method, named One-Versus-Opposite-Nodes, allows decomposing the problem in several independent tasks and therefore scales well with large database. We also propose two methods for computing a hierarchical decision function that serves to annotate new image samples. The former is performed by a top-down classifiers voting, while the second is based on a bottom-up score fusion. The experiments on Pascal VOC X 2010 dataset showed that our methods improve well the image annotation results.
 Image annotation, hierarchical classification.
 H.3 [ Information Systems ]: Content Analysis and Index-ing
Automatic image annotation is a challenging problem deal-ing with the textual description of images, i.e. associating tags or even better descriptive text to images. A wide num-ber of approaches have been proposed to address this prob-lem and to narrow the well-known semantic gap issue. Most approaches rely on machine learning techniques to provide a mapping function that allows classifying images in seman-tic classes using their visual features [3]. However, these approaches face the scalability problem when dealing with broad content image databases, i.e. their performances de-crease significantly when the concept number is high. This variability may be explained by the huge intra-concept vari-ability and a wide inter-concept similarity on their visual properties that often lead to incoherent annotations. Fur-thermore, multimedia retrieval systems require an increas-ing concept classes for annotating images in order to meet user needs. Accordingly, current techniques are struggling to scale up. Therefore, the only use of machine learning seems to be insufficient to solve the image annotation problem. Se-mantic structures, such as semantic hierarchies, appear to be a good alternative to reduce this problem complexity [1].
Semantic hierarchies have shown to be very useful to nar-row the semantic gap. They identify the dependency rela-tionships between concepts and provide valuable informa-tion for many problems. Semantic hierarchies can improve image annotation by supplying a hierarchical framework for image classification, and allow for efficiencies in both learn-ing and representation. Three types of hierarchies for image annotation have been recently explored: 1) language-based hierarchies: based on textual information [11, 5], 2) visual hierarchies: based on low-level image features [8], 3) seman-tic hierarchies: based on both textual and visual features [9, 6, 2]. Semantic hierarchies provide a meaningful semantic structure that helps simplifying the complexity of the classi-fication problem. Thus, this paper proposes a new approach to effectively use semantic hierarchies as a framework for hierarchical image classification.
Image annotation has been considered in the last decade as a multi-class classification problem. To deal with a large number of concept categories, many approaches proposed to combine hierarchical structure with Support Vector Ma-chines(SVM)classifiers[11,8,9,6,4]. Theseapproaches can be qualified as top-down methods, i.e. the class hier-archy is built by recursive partitioning of the set of classes [8, 4, 7], or as bottom-up methods, i.e. the class hierarchy is built by agglomerative clustering of the classes [11, 9, 6, 2]. Two directions have been explored for hierarchical im-age classification: using Decision Directed Acyclic Graphs (DDAGs) [12, 11, 7], and using Binary Hierarchical Deci-sion Trees (BHDTs) [8, 4]. Given C = c 1 ,c 2 ,  X  X  X  ,c N the annotation vocabulary of the database, the DDAG based approaches train N ( N  X  1) / 2 binary classifiers and use a DAG to decide about the belonging of an image i toaclass c  X  C . These methods allow at each node in a distance d from the rooted DAG to eliminate d candidate classes from C , resulting in a N  X  1 decision nodes to be evaluated for labeling a test sample. On the other side, BHDT based ap-Figure 1: Built semantic hierarchy on VOC X 2010 dataset. Double octagon nodes are original concepts.
 proaches build and use hierarchies as binary trees, i.e. data are divided hierarchically into two subsets until each subset consists of only one class. Data partition is often achieved using a clustering algorithm. Thus, one SVM is trained for each node of the tree, resulting in a log 2 N SVM runs to label a test sample. BHDT approaches target to optimize the efficiency of SVM classifiers by reducing the unnecessary comparisons while maintaining a high classification accuracy [4]. However, BHDT and DDAG approaches focus on the classification optimization and do not model in anyway im-age semantics. Although these approaches allow increasing classification accuracy, they constrain hierarchies to binary structures resulting in a significant deadlock when the con-cepts number is large. For instance, the method of [11] is in a deadlock when the concept number exceeds 30, since in-termediate concepts are extracted from WordNet using hy-pernymy relationships which depth is limited to 15 levels.
Alternative approaches have emerged recently and pro-pose the use of semantic relationships between concepts for the building of hierarchies. Fan et al. [6] proposed to in-corporate concept ontology and a multi-task learning algo-rithm for hierarchical concept learning. The labeling of a new sample is obtained by a voting procedure at all levels of the hierarchy, i.e. | C + C | SVM runs are necessary for la-beling new images ( C is intermediate concept nodes in the hierarchy). In [5], a  X  X ree-max classifier X  based on ImageNet hierarchy is proposed. A classifier at each node of the Im-ageNet tree is learned. The decision function is computed according to a target class and all its child nodes.
In this paper we propose a new method for learning hi-erarchical classifiers. Our method relies on the structure of semantic hierarchies to train more accurate classifiers for image classification. Subsequently, we propose two methods for computing the decision function in order to hierarchical image classification. The first one is a bottom-up approach for hierarchical image classification by score fusion. Fusion is performed by the spreading of scores starting from leaf nodes until reaching the root node. The second is a top-down ap-proach and is performed by classifiers voting. Starting from the root node and according to classifier votes, the hierarchy is traversed until reaching leaf nodes.

For the building of the semantic hierarchy we rely on our previous work [2], where we proposed to compute a seman-tic similarity between concepts in order to produce a hi-erarchy faithful to image semantics. This measure, named Semantico-Visual Relatedness of Concepts, integrates: 1) a visual similarity which represents the visual correspondence between concepts, 2) a conceptual similarity which defines a relatedness measure between target concepts based on their definitions in WordNet, and 3) a contextual similarity which measures the distributional similarity between each pair of concepts. The building of the hierarchy is based on a set of rules to link together concepts that are semantically related. Figure 1 illustrates the obtained hierarchy, which is a N-ary tree like structure where leaf nodes are initial concepts. Mo-tivation for this method is that the semantics of images and data is much more complex than binary items.
Based on the hierarchy structure, we propose in the fol-lowing to train several classifiers that represent the same concept at different levels of abstraction. These classifiers are consistent with each other since they are linked by the subsumption relationship, and will represent the same in-formation with different levels of details. Therefore, these classifiers results can be merged in order to achieve rele-vant decision on the membership of an image to a class. Concretely, given a class hierarchy, a classifier for each con-cept node is trained by performing a One-Versus-Opposite-Nodes (OVON) SVM. Indeed, in order to propose a method that scales well with large image databases, a good strategy would be to decompose the problem in several independent tasks based on the hierarchy structure. Thus, instead of considering all database images for training classifiers, we will consider only images of children nodes of a given target concept node. This is similar to cut a target node of a tree from its upper part and treats it independently. Therefore, for training a classifier of a target node, we took as positive samples all images associated with its children leaf nodes. So, if an image is annotated by  X  X ow X  it will also serves to train the classifiers for  X  X ovid X ,  X  X ertebrate X , etc. Negative samples are all images of sibling nodes -cf. Figure 2.
Starting from leaf concept nodes and following the sub-sumption relationships, we compute the average confidence scores of all paths in the hierarchy. The decision function is then computed according to the sign of this average score. A practical standpoint is that the classification results of these SVMs are independent. Therefore, it is also possible to run all SVM classifiers to compute the membership degree of an image to all classes. Subsequently, according to the hierar-chy structure the decision function can be computed easily for all leaf concepts. Thus, the complexity for labeling a given image is  X  (2 N  X  1). Let x v i be any visual represen-tation of an image i , a classifier is trained for each concept class c j in the hierarchy. N = |C| + |C | binary SVM OVON are then used with a decision function G ( x v ): where K ( x v i ,x v ) is the value of a kernel function for the training sample x v i and the test sample x v , y i  X  X  1 , class label of x v i ,  X  i the learned weight of the training sample x ,and b is a learned threshold parameter. RBF kernel is used for SVMs training K ( x, y )= exp x  X  y 2  X  2 .
The final decision function to compute the membership degree of an image z to a concept class c j is: where S is the set of subsumer of c j . G k ( z v ) is the decision function of the classifier associated with concept k.
From a statistical standpoint, the final decision function f ( z ) is computed by achieving n measures of the same event ( n = |S| is the hierarchy depth). Thus, the uncer-tainty about f x ( z ) can be computed as the standard devi-ation  X  f x ( z ) =  X   X  n . Therefore, the final decision function is  X  n times more accurate than the one obtained from a single classifier.
TDCV aims at decomposing the image classification prob-lem into several complementary sub-tasks. It consists in building several classifiers that are able to discriminate one class from the others under a given parent node. Thus, to reach the final decision about the class membership it is es-sential to descend the hierarchy according to the classifier decisions (votes).TDCV is efficient in terms of complexity since it requires to train less than 2 N  X  1 classifiers for hier-archical classification, and to evaluate less than log 2 N deci-sion nodes for labeling a test image -cf. Table 1. However, TDCV is sensitive to the initial classification since classifiers at the subsequent levels cannot recover from the misclassi-fication of a test image that may occur in a higher con-cept level. Thus, this misclassification can be propagated to the terminal node. Nevertheless, the average precision is strongly high for the nodes in highest levels of the hierarchy, and therefore errors propagation is small -cf. Figure 3.
Image classification is performed top-down as illustrated in Algorithm 1. Starting from the root node, the decision functions of subsequent level nodes are evaluated. The nodes with positive confidence value are recursively explored until reaching leaf nodes. Several paths in the hierarchy can be explored, and thus a test image can be associated to many classes. If a path is explored, but all the leaf classifiers have responded negatively, we keep the concept with higher con-fidence value.
We used the Bag-of-Features (BoF) representation to de-scribe image features, which is a widely known method. The BoF model has shown excellent performances and became one of the most widely used techniques for image classifica-tion and object recognition. In our approach, image features are described as follows: Lowe X  X  DoG Detector [10] is used for detecting a set of salient image regions. A signature of these regions is then computed using SIFT descriptor [10]. Afterwards, given the collection of detected region from the training set of all categories, we generate a codebook of size K = 1000 by performing the k-means algorithm. Thus, each detected region in an image is mapped to the most similar Algorithm 1: Top-Down Classifiers Voting Input : A test image, the semantic hierarchy
Result : Image annotation begin Figure 4: Comparison of the OVON and the OVA hierarchical classifiers on VOC X 2010 dataset. visual word in the codebook through a KD-Tree. Each im-age is then represented by a histogram of K visual words, where each bin in the histogram corresponds to the occur-rence number of a visual word in that image.
 Experiments are performed on Pascal VOC X 2010 dataset. We used 50% of the dataset images for training concept clas-sifiers and the other images for evaluating the proposed ap-proaches. To perform a fair comparison, we used the same visual representation of images for all of these methods, i.e. Bag-of-Features representation. The flat classification is per-formed by | C | SVM OVA, where the inputs are the BoF rep-resentation of images and the outputs are the desired SVM responses for each image (1 or -1). We used cross-validation to overcome the unbalanced data problem, taking at each fold as many positive as negative images. Hierarchical clas-sification with OVA classifiers is performed by training a set of ( | C + C | ) hierarchical classifiers consistent with the struc-ture of the hierarchy in Figure 1. The baseline method is built by taking the average submission results to VOC X 2010 challenge. In the following, evaluations are performed using the recall/precision curves and Average Precision score(AP).
In Figure 4, we compared our method (OVON) for train-ing hierarchical classifiers to the One-Versus-All one. OVON performs a better result than the OVA classifiers, with an AP of 63 . 25% while 56 . 42% for the OVA hierarchical classifi-cation. In Figure 5, we compare our methods for hierarchical image classification to the other ones. Our methods achieve a higher AP than the flat classification with a gain of +26.8% forBUSFmethodandagainof+16 . 04% for the TDCV method. Compared to the baseline method our approaches are slightly better. This can be explained by the efficient im-age features used in the submission of VOC challenge, and
