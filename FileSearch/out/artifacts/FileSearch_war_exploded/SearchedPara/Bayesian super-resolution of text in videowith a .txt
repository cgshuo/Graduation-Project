 Katherine Donaldson, Gregory K. Myers Abstract. To increase the range of sizes of video scene text recognizable by optical character recognition (OCR), we developed a Bayesian super-resolution algo-rithm that uses a text-specific bimodal prior. We eval-uated the effectiveness of the bimodal prior, compared and in conjunction with a piecewise smoothness prior, visually and by measuring the accuracy of the OCR results on the variously super-resolved images. The bi-modal prior improved the readability of 4-to 7-pixel-high scene text significantly better than bicubic interpolation and increased the accuracy of OCR results better than the piecewise smoothness prior.
 Keywords: Super-resolution  X  OCR  X  Video 1 Introduction A capability to automatically identify and extract the contents of video imagery would enable videos to be in-dexed in a convenient and meaningful way for later ref-erence and would enable actions (such as automatic no-tification and dissemination) to be triggered in real time by the contents of streaming video. Video text recogni-tion, or video OCR, is a useful tool for characterizing the contents of video containing overlay text (text captions superimposed over the video imagery, such as in broad-cast news programs) and scene text (text that appears in the real scenes of the video, such as text on street signs, nameplates, and billboards). This paper focuses on the recognition of scene text, which is often too small or of too poor quality to be recognized by an OCR process. In particular, text fonts with capital letters that are less than eight pixels high do not have sufficient resolution to be reliably recognized by commercial OCR products. Even custom-developed character recognizers that have been specially designed for recognizing small overlay text do not perform well on text fonts with capital letters smaller than seven pixels [1, 2].
 than the duration of a single video frame, multiple image samples representing the same text can be captured and made available for processing. Therefore, we can consider applying super-resolution techniques to produce an en-hanced representation of the text image with the goal of extending the range of text sizes and qualities of scene text that can be recognized in video imagery. Super-resolution is a process in which a single high-resolution image is constructed from a set of lower-resolution im-ages, which could be degraded and aliased. If there is some relative motion between the camera and the scene (e.g., the text is on a moving object or the pose of the camera is changing), each of the low-resolution im-ages represents a different sampling of the scene. Super-resolution works by combining the complementary in-formation about the scene contained in each of the sam-plings.
 super-resolution for general imagery. Chaudhuri [3] and Park et al. [4] provide good reviews of various super-resolution approaches. However, in our application we can restrict the super-resolution processing to specific regions of an image that are likely to contain text. Pro-cesses that extract and read text from imagery typically apply a text detection process first, so that subsequent OCR processing can be focused only on the regions of the imagery containing text. The performance of scene text detectors is becoming increasingly reliable [5 X 10]. There-fore, we are interested in investigating super-resolution approaches that use the knowledge that the region to be enhanced contains text.
 but in most cases the algorithms were not tailored specif-ically for text. Li and Doermann [11] used the method of projection onto convex sets (POCS), which was based on work by Patti et al. [12], to deblur scene text. Capel and Zisserman [13] tested four estimators for the super-resolution enhancement of text: a maximum likelihood (ML) estimator, the iterative back-projection method of Irani and Peleg [14, 15], a maximum a posteriori or (MAP) estimator that incorporated a piecewise smooth-ness prior with a Huber penalty function (originally de-veloped by Schultz and Stevenson [16]), and an estimator regularized using the Total Variation norm [17]. The lat-ter two estimators generated superior results compared to those of the former two estimators. In later work, Capel and Zisserman [18] compared the performance of a ML estimator with hard constraints on individual pixel values to that of a MAP estimator with smoothness con-straints; the former yielded sharper results because of the lack of any imposed spatial correlation.
 example-based priors. These algorithms are trained on a specific class of images, such as text; the correspondence between low-resolution and high-resolution imagery is learned from samples in a training set, and it is assumed that the relationship is the same as or similar to that in the test set. Baker and Kanade [19] achieved good results by applying this algorithm to text images with the same fonts as those in the training images.
 proach, an edge-based super-resolution technique that attempts to locate the edges to subpixel accuracy in a sequence of images taken of a scene with text and then fuses the conglomerated edge information into the first image. By focusing on derivative information rather than zeroth-order pixel values, they avoid any potential il-lumination problems. In another approach, Chiang and Boult [21] used a noniterative algorithm that resamples and warps the images to create a set of aligned, upsam-pled images, which are then straightforwardly fused; the fused result is then deblurred. This approach was applied to video imagery of text, and performance was measured in terms of character recognition rate.
 is generally bimodal. By its very nature, text characters must have some contrast with the background to make them human-readable. Therefore, for most text in real scenes, the intensities of the text pixels tend to clus-ter around one value, and the intensities of the back-ground pixels tend to cluster around another (of course, there are exceptions, due to large illumination variations within the text region and highly stylized graphic and color design). Therefore, the algorithm described in this paper applies a bimodal prior within a MAP Bayesian super-resolution framework. The MAP Bayesian frame-work allows this a priori knowledge about the imagery to be introduced explicitly as constraints. This approach is similar to that of Cheeseman et al. [22], Schultz and Stevenson [16], Hardie et al. [23], and Capel and Zisser-man [13, 18, 24], except that a bimodal prior is applied instead of a smoothness prior. A bimodal constraint has been successfully applied to the resolution enhancement of text in single images by Thouin and Chang [25], who used a nonlinear optimization technique to maximize the bimodal-smoothness-average score of the expanded im-age.
 compared to that of a piecewise smoothness constraint as a prior in the computation, and no prior (ML). Because our goal is better OCR performance and not merely a better-looking image, the results are quantitatively eval-uated by running an OCR engine on the super-resolved result in addition to visually showing the results of super-resolution on text images. To assess performance under realistic imaging conditions, the algorithms are evalu-ated with video imagery taken with a common consumer-grade camera. 2 Algorithm description Our super-resolution algorithm is based on the basic Bayesian framework. Assuming a set of N low-resolution observation images L = { L k } , the algorithm finds the high-resolution image H such that the conditional prob-ability of H , given the observed L ,P[ H |L ], is maximized. This is difficult to calculate directly, but using our cam-era model we can state each L k in terms of H , which al-lows us to calculate P[ L| H ]. Using Bayes X  law, we obtain P[ H |L ]=P[ L| H ]  X  P[ H ] / P[ L ]. The denominator, P[ does not affect the maximization. Therefore, to find the most probable high-resolution image, given the observa-tion images, we need to find the high-resolution image H that maximizes P[ L| H ]  X  P[ H ], which is the MAP es-timator.
 resolution images, all high-resolution images are equally likely, we could further simplify this algorithm to find-ing H that maximizes P[ L| H ], which is the ML estima-tor. But we have assumed that the set of high-resolution images contains only images of binary text scenes, so we derive priors for bimodality, P B [ H ], and piecewise smoothness, P S [ H ].
 age, H , that maximizes P[ H |L ] is iteratively calculated by stepping down the gradient of the negative log likeli-hood of P[ L| H ]  X  P[ H ] until a minimum is reached or a maximum number of iterations is executed.
 threshold the image so that the binary result can be fed to the OCR engine. The algorithm chooses the dy-namic threshold by fitting Gaussians to the foreground (black) and background (white) pixel distributions using the expectation maximization (EM) algorithm [26, 27] and calculating the value midway between their means. 2.1 Modeling and image formation As we focus on images of text, we assume that the text is on a plane. We are currently using a simplified 2D ver-sion of our camera model to test the effectiveness of our priors. This simplified model assumes that the scene text plane is perpendicular to the camera and that all motion is translational. The model can be directly extended to a 3D projective transform at the cost of additional process-ing time and added complexity due to increased degrees of freedom in the registration and projection steps; how-ever, we are focusing on the relative advantages of our priors, so we use the simplified camera model. the camera with only translational motion, the image formation process can be represented as a translation of H , the high-resolution image, with respect to the cam-era, then a convolution with a pillbox blur kernel repre-senting the camera aperture, followed by summing over the active areas of the camera CCD pixels to account for subsampling and fill factors. We handle illumination variations with an optional preprocessing step to remove x and y linear illumination gradients by using a least-squares fit on the image values. The image formation process can thus be written where L k is the k th observed low-resolution image,  X  is the subsampling lattice,  X  is the pillbox blur kernel, ( x k ,y k ) is the translation vector for L k , and H is the high-resolution image.
 icographic ordering, the transformation between them can be represented as the sparse correspondence matrix A k , and we can write ces A k is the most costly step of the algorithm, although once calculated they are used repeatedly to determine the error of the estimated H in the gradient descent it-erations. We have chosen to compute A k columnwise, where every column of A k is the mapping of one pixel of H into L k . To simplify the computation, we calcu-late the projection by using a numerical integration over a fine grid on the low-resolution camera plane. Our de-fault integration grid resolution is a factor of 5 greater than the resolution increase from L k to H . Assuming a resolution increase of 2  X  to 5  X  , typical integration grid resolutions range from 10 X 25 cells across each low-resolution camera pixel. To fill in one column of the matrix A k , we need to calculate what fraction of the corresponding high-resolution pixel maps to each low-resolution pixel. Thus, for each high-resolution pixel in H we calculate its projection onto the camera plane grid; with our simplified camera model this projected pixel is a translated square. To fill in the projected square on the integration grid, we set each grid cell to the fraction by which it overlaps with the projected high-resolution pixel. We then blur the projection by convolving it with the pillbox blur kernel  X  , which we have precomputed on the same fine-resolution integration grid. Typically, the camera blur is on the order of one low-resolution pixel, which makes its gridded representation 10 X 25 cells wide. The integration of this fine grid over the active area of each low-resolution pixel yields the values for a column of A k . As an optimization, since we are using a transla-tional camera model, we can save the projection of the first pixel of H onto the integration grid, translate every other pixel in H the appropriate amount, and integrate it over the active area of the low-resolution pixels. 2.2 Registration Taking advantage of the simplified camera model, we use pairwise correlation with quadratic interpolation and least-squares fit to determine the subpixel translation vector ( x k ,y k ) for each observed low-resolution image. Each of our N low-resolution images, L k, in the set L ,is correlated against every other image in the set L , yield-ing ( N 2  X  N ) / 2 correlation matrices. A quadratic inter-polation around the low-resolution pixel peak is then used to find the subpixel peak of each correlation ma-trix, yielding a set of observed subpixel translation vec-tors { m i,j } , where m i,j is the translation from L i to L correlation peak to be biased toward integer pixel trans-lation. To reduce this bias in the registration and other error sources such as the mismatched edges of an im-age, we combine the information from all of the pairwise correlations, using a least-squares fit. Since translational motion is additive, the translation ( x ik , y ik ) between im-ages L i and L k is equal to the translation from L i to L ( x ij ,y ij ), plus the translation from L j to L k ,( x jk ,y Thus, using the independently measured translations { m i,j } we can construct a set of ( N 2  X  N ) / 2 linear equa-where m j,i is the measured correlation translation be-tween L j and L i ,  X  j,i is the error in the estimation, and ( x 0 ,i ,y 0 ,i ) are the N unknowns. Given these equations we can use a standard least-squares error minimization to estimate the values for ( x 0 ,i ,y 0 ,i ) that minimize the error values  X  j,i . The registration accuracy of this method was measured at roughly  X  0.05 to  X  0.1 pixel on simulated data. 2.3 ML estimator Making the assumption that all H , prior to observation of
L , are equally likely, the ML estimator calculates the super-resolved image solely from the likelihood of the observed low-resolution images, given an estimate of the high-resolution image, P[ L| H ]. Assuming that the im-age noise is Gaussian with a mean of zero and variance  X  , the probability of observing the low-resolution image L , given an estimate of the high-resolution image H , is P[ L k | H ]= variance  X  2 n , with the deviation from the mean being the estimate error ( L i,j,k  X  L i,j,k ), over image coordinates ( i, j ). L k is an estimate of L k , calculated by projecting the current estimate of the high-resolution image H , using the previously defined correspondence matrix A k from Eq. 2.
 by minimizing the negative log likelihood over all ob-served images L : H 2.4 MAP estimators To form a MAP estimator, we need to state our a pri-ori expectations of H as a prior probability distribution. From the two features of binary text images, bimodality and piecewise smoothness, we derive two prior proba-bility distributions on the high-resolution image, P B [ H ] and P S [ H ]. 2.4.1 Bimodality prior. For our bimodality prior we use an exponentiated fourth-order polynomial with maxima at the calculated centers of the black and white pixel distributions. We use the following exponential-based bi-modal distribution, seen in Fig. 1 and Eq. 5, so that we can easily simplify it when we take the log of P B [ H i,j where  X  0 and  X  1 are current estimates of the positions of the two peaks of the bimodal distribution,  X  b deter-mines the width of the black and white distributions, and c b (  X  b , X  0 , X  1 ) is a normalizing constant for the dis-tribution dependent on  X  b ,  X  0 , and  X  1 . We automati-cally determine  X  0 and  X  1 by using an expectation max-imization algorithm [26, 27] to fit two Gaussians to the histogram of the current super-resolution estimate. An estimate of  X  0 and  X  1 is computed between each gradi-ent descent step. This prior function only somewhat ap-proximates the actual distribution of pixel values in an image region containing scene text. In particular, it ex-actly models the distribution only when the proportions of black and white pixels are equal and their variances are also equal. However, this functional form makes it quite easy to compute the log likelihood function, which we deemed a reasonable tradeoff. 2.4.2 Smoothness prior. Since most text images are lo-cally smooth with step discontinuities, for a smooth-ness prior we use a Gibbs prior with a Huber gradient penalty function, similar to that described by Schultz and Stevenson [16] and Hardie et al. [23]. The Gibbs prior represents piecewise smooth data with the proba-bility density defined as where c s is a normalizing constant,  X  ( x ) is the Huber edge penalty function, and H i,j  X  H i,j is a local measure of image smoothness (which is small where the image is smooth and large where it is discontinuous). H i,j is the average of the four nearest neighbors of H i,j : The likelihood of discontinuities in the data is controlled by the Huber edge penalty function  X  ( x ): where  X  is the threshold between the quadratic and linear regions. This makes larger discontinuities much more likely than they are with a strictly quadratic edge penalty. In the linear region the derivative of this penalty function is constant, which preserves the steep edges in the image when the algorithm steps along the gradient of the log likelihood. 2.4.3 Three MAP priors. Replacing the uniform P[ H ]of the ML estimator with our bimodality and smoothness priors leads to three different maximum a posteriori es-timators. The first uses the bimodal prior P B [ H i,j ] only: The second uses the smoothness prior P S [ H i,j ] only: H The third uses both the smoothness prior and the bi-modal prior: H The maximum a posteriori estimates, H MAP  X  B , H ing the negative log likelihood of H over all observed images, as we did with the ML estimator. Here we show only H MAP  X  SB because it is a superset of the other two MAP estimators. Expanding H MAP  X  SB and taking the negative log, we get H where  X  n ,  X  b , and  X  s have effectively become the weights between the error term and the two priors. 3 Results 3.1 Experimental p rocedure All test images were taken with Sony miniDV cameras with image stabilization disabled. These cameras use the compressed DV25 format to record video data. Digital video (DV) compression uses discrete cosine transform (DCT) to compress pixel data by a factor of 5:1 (in the DV25 format) on an intraframe basis. DV compression is relatively light, but it is not lossless; it produces ring-ing artifacts around high-contrast edges, such as those in text images, and other artifacts. To be useful in practi-cal applications, however, our super-resolution algorithm must tolerate these artifacts gracefully since DV25 is the native format of almost all consumer-grade digital video cameras. We also show some results on data that were further compressed by MPEG-1 software.
 resolution results was the MTX engine within Scansoft X  X  DevKit 2000. The algorithm parameters were experi-mentally determined. The fill factor for the video cam-era X  X  CCD model was set at a reasonable default of 60%  X  60% for all tests. The blur diameter of the camera model and the weighting of the bimodal and smoothness priors were set by hand for each video sequence. With nominally chosen values for  X  n ,  X  b , and  X  s , the algorithm was run with blur diameters from 0.5 to 1.5 camera pixels in steps of 0.25 pixels, and the most visually acceptable blur was chosen. Once the blur was chosen, the code was run over a range of  X  n ,  X  b , and  X  s values, and the im-age with the best subjective noise vs. detail tradeoff was chosen.
 timal solution and would not be an option if the entire process were to be completely automated. The blur pa-rameter could be estimated using a blind image decon-volution algorithm (such as one in a review paper by Kundur [29]), preferably an algorithm that takes into account the underlying binary nature of our scenes. An-other possibility would be to incorporate the estimation into the super-resolution process [13, 30], although this would require the reestimation of the projection matrices A k (which is very time consuming). With the  X  parame-ters our eventual goal is to use a diverse set of images to experimentally determine the relationship between  X  n ,  X  , and  X  s on the one hand and the OCR accuracy on the other. Possibly the optimal settings for  X  n ,  X  b , and  X  s will depend on the initial text height, the resolution increase factor between L and H , and/or the estimated camera blur.
 an interlaced video format. Accordingly, we separate the fields and consider them to be two independent images. To compensate for these double images we double the resolution increase factor in the y dimension and de-crease the camera CCD fill factor to 60%  X  30%. This is not necessary with the MPEG-1 compressed video, which has already been deinterlaced and subsampled as part of the compression process. 3.2 Pixel height vs. readability using MAP-B Figure 3 shows a full frame extracted from a video show-ing text of various sizes from Lincoln X  X  Gettysburg Ad-dress. Table 1 shows the results of a test in which we subjected up to 32 input images of various-size text to our super-resolution process. Figure 4 shows OCR ac-curacy (in terms of the fraction of characters correctly recognized) as a function of text height for various num-bers of low-resolution frames.
 benefits of our MAP estimator with the bimodal prior become apparent when the pixel heights of the capital letters of the observed text are between 4.5 and 6.75 pix-els. The benefits disappear at letter heights of about 9 pixels, since at that height the OCR system is able to recognize the text perfectly, using only bicubic interpo-lation and binarization as preprocessing.
 pressed by an MPEG-1 scheme, the resolution is halved to 320  X  240. The 5.25-pixel-high text is thus reduced to a height of 2.6 pixels. Figure 5 demonstrates the effec-tiveness of the algorithm on the compressed video data. The character recognition rate improved significantly for text approximately 3 to 7 pixels high.
 3.3 Comparison of priors X  effect on reco gnition 3.3.1 Gettysburg text image. Tables 2 and 3 show the recognition results of the two smallest texts in the Get-tysburg Address video frame (3.75 and 4.5 pixels) for the different priors. The recognition improves somewhat when we add the smoothness prior to the ML estima-tor and improves more when the bimodal prior is added. In these cases it can be seen that the advantage of the smoothness prior is subsumed in the advantage of the bimodal prior; if the bimodal prior is being used, adding the smoothness prior does not yield any additional ac-curacy in the OCR. 3.3.2 Text on license plates. The video used in this test is closer than those discussed above to a typical sequence in a library of surveillance videos. It consists of a panning shot across a parking lot, showing text on license plates. Figure 6 shows one frame from this video sequence, and Fig. 7 shows the results for the various priors. 3.3.3 Street sign. Figure 8 shows another test case with MPEG-1 compression. The text was filmed through the window of a moving train. As the street sign moves across the frame of the camera it is distorted and scaled by a 3.75 pixels 4.5 pixels 5.25 pixels 6.75 pixels small amount. Both transformations violate the assump-tions of our 2D model of translational camera move-ment. Figure 9 shows that even under these conditions the MAP-B estimator significantly improves the OCR results over those of the ML and MAP-S estimators. 3.3.4 Discussion. In general, the largest improvements in OCR performance seem to result from increases in character separation rather than from the reduction of general noise. In some cases the super-resolved image can be improved cosmetically, its edges sharpened, and the speckle noise reduced without improving the OCR performance. As the bimodal prior has a larger effect on character separation than the piecewise smoothness prior, it is more effective at improving the OCR perfor-mance.
 false speckle details in the results and halting the gra-dient descent before it reaches the regime of noise am-plification instead of detail amplification (unlike the ML estimator only), but it does not appear to be especially good at separating closely spaced characters. Due to the method the piecewise smoothness prior uses to preserve sharp transitions, it does not move the location of these transitions and therefore has little effect on the thresh-olded images. In contrast, when the bimodal prior raises the values of some pixels near the gray edges of transi-tions toward black, it causes the error constraint to push ML: @n Camillo Rtall MAP-B: Z Camino Real ] MAP-S: D camitko Rtall MAP-BS: Z Camino Real ] other nearby pixels toward white in order to balance the average to the observed low-resolution gray value. In this respect, the estimators that use the bimodality prior (MAP-B and MAP-BS) seem to have a greater im-pact on performance. 3.4 Computational complexity The central step of our super-resolution algorithm, the gradient descent over P[ L| H ]  X  P[ H ], is the fastest part of the algorithm; the computation of the projection matrices A k is the slowest, dominating our total com-putation time; and the computation time for registra-tion is somewhere in between. The registration time is dominated by the pairwise correlation, which, since it uses an FFT-based correlation, is O( N log N ), where N is the number of pixels. Thus, the registration step time is O( number of pixels  X  log( number of pixels )  X  number of images 2 ). The time taken to calculate the projection matrix A k for each image is based on many factors. It is linearly proportional to the number of pix-els in the target high-resolution image and increases with the square of the blur kernel diameter measured in nu-merical integration cells. The time spent on each gradient descent iteration is linearly proportional to the number of input images and their size, and the number of itera-tions is highly variable.
 MAP-BS algorithm running on a 2.8-GHz Pentium 4 computer with 1 GB RAM, with 8 or 32 input images, two image sizes, and various resolution increase factors. The image size of 140  X  40 pixels corresponds to the image displayed in Table 1 with a text height of 6.75 pixels. The blur diameter was set to one low-resolution pixel, with (5  X  resolution increase factor) 2 numerical in-tegration cells per low-resolution pixel. 4 Summary To increase the range of sizes of video scene text processed by OCR, we developed a Bayesian super-resolution algorithm that uses a text-specific bimodal prior. Because our goal was better OCR performance and not merely a better-looking image, we quantita-tively evaluated the results by running an OCR engine on the super-resolved result. We tested two prior dis-tributions on high-resolution imagery, a bimodal prior, and a smoothness prior. The MAP estimator using the bimodality prior significantly increases the accuracy of the OCR results (compared with bicubic interpolation) on scene text 4 to 7 pixels high, when processing DV compressed data, and on text between 3 to 7 pixels high, when processing MPEG-1 compressed data. To our knowledge, the recognition of such small text has not been previously reported. Our tests also show that the bimodality prior is more effective in increasing the ac-curacy of OCR results than the piecewise smoothness prior.
 References
