 Pattern classification is one of the most important issues in machine learning and data mining that encompasses a wide domain of problems. Support vector machine (SVM) and fuzzy neural network (FNN) are considered as important tools for pattern Vapnik in 1992 [1]. Although SVM due to reliance on the principle of structural risk means that using common kernel functions for SVM, due to lack of compliance with 3]. On the other hand, FNN has human-like reasoning benefit for handling uncertainty classifier called ASVMFC is presented that uses capabilities of SVM and FNN together and does not have the mentioned disadvantages. fuzzy clustering method and its parameters are adjusted by training a SVM equipped with an adaptive kernel. ASVMFC tries to achieve generalization power of SVM and human-like reasoning benefit of FNN together. The clustering method, which is used creating ASVMFC. In addition, using an adaptive kernel for training SVM in ASVMFC has been caused ASVMFC achieves good classification performance for paper. This method can identify samples, which are effective in adjusting ASVMFC, samples drastically. Empirical results show that the ASVMFC classification accuracy is more than classification accuracy of the three types of SVM which use RBF kernel, polynomial kernel, and sigmoid kernel respectively. details. The results of experiments are expressed in section 4. Finally, the conclusions are summarized in section 5. such that optimization problem: number of training errors. In fact, (2) is a quadratic programing problem and we can write (2) as (3) by using the Lagrange technique. Values of Lagrange multipliers are obtained by solving (3) and we can use them to compute values of W and b [5]. To extend the above linear SVM to a nonlinear classifier we can use the kernel trick into a higher dimensional feature space". To this end, we apply a kernel function with symmetric and positive-definite and satisfies the Mercer conditions [6]. ASVMFC is a classifier in a form of FNN, which uses a new fuzzy clustering method phase. In this section, these phases will be described in details. 3.1 Clustering Phase either classes of problem by a set of clusters with Gaussian distribution. To this end, rules of ASVMFC. each sample and calculate sum of influences of these distributions on each sample and sample is calculated as follows, 
Clusters Clustering (Samples, InitCovariance, Thershold) { FOR i  X  1 TO number of the samples FOR j  X  1 TO number of the samples IF i = j THEN ELSE ENDIF ENDFOR ENDFOR Influences  X  sum of the rows of InfluenceMatrix Influences  X  Influences/max(Influences) WHILE Influences is not empty I  X  argmax(Influences); IF Influences[I] != -infinite THEN L  X  InfluenceMatrix[I, all ]; WHILE I_th is not empty ENDWHILE IF I_inf has more than one entity THEN ENDIF ENDIF ENDWHILE } where N is the number of the samples of the class which the desired sample is belong distributions. We select the sample, which has maximum influence value, as the initial member of a new cluster. We calculate effect of small distribution of selected samples on the rest samples and select the samples that the acquired value for them are greater than a special threshold (  X _th ) and add these samples to members of the new cluster. samples on the rest samples until no sample is exist that its calculated influence value compute its parameters by using its elements. 
In above algorithm, CalculateInfluence() function takes two samples and sample. Each cluster obtained in this phase, forms a hyper-ellipsoid in n dimensional Gaussian distribution. So, each of these clusters are determined by a center vector and covariance matrix of the new cluster and add it to list of clusters.  X  can be calculated by the algorithm automatically. We can use (6) to determine value of  X   X  X  X  X  automatically, (7), where  X   X  is mean of  X   X  s, and S  X  is standard deviation of  X   X  s. 3.2 SVM Training Phase problems, these kernel functions cannot have reasonable performance in all problems. domain. Because of this, in the learning al gorithm of ASVMFC we try to realize the function which has been used in ASVMFC, related to class B. 
In order that the kernel function defined by (9) is suitable for application in SVM, Mercer's work [6, 7], we know that if K is the symmetrical and continuous kernel of an integral operator O  X  : L  X   X L  X  , such that is positive, i.e., then K can be expanded into a uniformly convergent series with  X   X   X 0 . In this case, the mapping from input space to feature space produced by the kernel is expressed as such that K acts as the given dot product, i.e., kernel. standard kernel. Lemma 2. The product of Mercer kernels is also a Mercer kernel. function of ASVMFC are product of two functions, it is sufficient to prove that each of these functions satisfies Mercer conditions. Lemma 3. For any function  X  X  X  X  that can be expanded as uniformly convergent power define the kernel function, then it is a Mercer kernel. compositions of some exponential functions with positive coefficients, satisfy Mercer conditions according to Lemma1 and Lemma2. Therefore, we can claim that the kernel function of ASVMFC is a standard kernel. use its Lagrange multipliers for adjusting ASVMFC parameters in the next phase. 3.3 Creating and Adjusting ASVMFC Phase In this phase a FNN is created using the results of previous phases. This FNN, which is shown in Fig. 1, is final classifier system of ASVMFC and consists three layers. In the first layer called input layer no computation is done. Each node in the input layer fuzzy rules of this FNN are divided into two groups and each of these groups consist the rules which are related to one of two classes. A and B indices show class label of are computed by (17), w set respectively. 
In Fig. 1 p represents number of data samples of class A and q represents number second layer outputs and two additional inputs. Also, W and b are computed by (18). 3.4 Sample Reduction One of the main problems of SVM is high time consumption of its training phase in affordable for cases with very large training set. A solution to overcome this problem plane. In other word, if we can identify the samples which probably will be chosen as support vectors, then we can use only these samples for training SVM. 
The samples, which are lied in the area between two classes, have more potential to be selected as support vectors; so the problem of sample reduction can be converted to find samples in the area between two classes. This seems as a good solution but it mapped space. However, because in ASVMFC the feature space is scaled form of the original space, we can apply that solution here. area between two classes is done easily based on fuzzy belonging degrees of samples to each of two classes and this process does not require much processing. We can say between two classes. Also, it is better that we add some samples from outer boundary of each class to training samples to determ ine class areas for preventing deviation of hyper plane. To this end, we apply (19) on the all samples and select some of smallest ones for training. not selected for training and separator hyper plane will not be deviated by them. We used the six Benchmark datasets from UCI Repository [11] and LIBSVM software [12] to assess performance of ASVMFC. Table 1 describes these datasets in generalized accuracy of ASVMFC, RBF-kernel-based SVM, polynomial-kernel-based SVM and sigmoid-kernel-based SVM For each data set. In each round of cross each dataset and average all the results for it. The value of cost parameter of SVM is (SV) count of (the numbers of SVs have been rounded). As shown in Table 2, ASVMFC has the best classification accuracy for all cases. The accuracies of polynomial-kernel-based SVM on Heart data set and RBF-Kernel-SVM has unpromising accuracy for majority cases. Better performance of ASVMFC functions for SVM, due to lack of compliance with the conditions of the problem, is caused SVM does not has good performance in any type of problems. Moreover corresponding to results shown in Table 1, numbers of fuzzy rules that ASVMFC has 3 respectively. Also, as shown in Table 2 the number of generated support vectors in and so find the separator hyper plane with lower support vectors. 
Fig. 2 shows how the proposed sample reduction method reduces the number of near 96%. These results actually confirm the good performance of the sample reduction method. FNN and adjusts the parameters of this FNN by using a new fuzzy clustering method and training a SVM equipped with an adaptive kernel. ASVMFC actually takes advantages of the SVM generalization and human-like reasoning of FNN. The fuzzy rules of ASVMFC are generated automatically by a new clustering method according to data distribution of problem. This enhances the accuracy of ASVMFC against the common standard kernel functions. Moreov er, because ASVMFC uses an adaptive using belonging degrees of samples to both classes. 
