
The method of stable random projections is an efficient tool for computing the l  X  distances using low memory, where 0 &lt; X   X  2 may be viewed as a tuning parameter. This method boils down to a statistical estimation task and various estimators have been proposed, based on the geo-metric mean, harmonic mean, and fractional power etc.
This study proposes the optimal quantile estimator, whose main operation is selecting , which is considerably less expensive than taking fractional power, the main opera-tion in previous estimators. Our experiments report that this estimator is nearly one order of magnitude more computa-tionally efficient than previous estimators. For large-scale tasks in which storing and computing pairwise distances is a serious bottleneck, this es timator should be desirable.
In addition to its computational advantage, the optimal quantile estimator exhibits nice theoretical properties. It is more accurate than previous estimators when  X &gt; 1 .We derive its theoretical error bound and establish the explicit (i.e., no hidden constants) sample complexity bound.
The method of stable random projections [36, 16, 21, 30], as an efficient tool for computing pairwise distances in massive, high-dimensional, and possibly dynamic data, provides a powerful mechanis m to tackle some of the chal-lenges in modern data mining and machine learning. In this paper, we provide an easy-to-implement algorithm for sta-ble random projections . Our algorithm is both statistically accurate and computationally efficient.
We denote a data matrix by A  X  R n  X  D , i.e., n data points in D dimensions. Data sets in modern applications exhibit important characteristics which impose tremendous challenges in data mining and machine learning [5]:  X  Modern data sets with n =10 5 or even n =10 6 points  X  Modern data sets are often of ultra high dimensions  X  Modern data sets are sometimes collected in a dynamic  X  Large-scale data are often heavy-tailed, e.g., image,  X  X caling up for high dimensional data and high speed data streams X  has been identified to be among the  X  X en chal-lenging problems in data mining research X  X 37]. The method of stable random projections is often regarded as the stan-dard algorithm for stream computations, provided that the data are generated from the following Turnstile model[32].
The input stream s t =( i t ,I t ) , i t  X  [1 ,D ] arriving se-quentially describes the underlying signal S t , meaning The increment I t can be either positive (insertion) or nega-tive (deletion). For example, in an online bookstore, S t may represent the number of books that the user i has or-dereduptotime t  X  1 and I t is the additional orders (or cancels of orders) at the time t . If a user is identified by his/her IP address, then D =2 64 is possible.

This study mainly concerns computing pairwise dis-tances. We can view the data matrix A  X  R n  X  D as n data streams, whose entries are subject to updating. In reality, the data may not be stored (even on disks)[32]. Thus, a one-pass algorithm is needed to compute and update dis-tances for training. Learning with dynamic (or incremental) data has become an active topic of research, e.g., [11, 2].
Many mining and learning algorithms require a similar-ity matrix computed from pairwise distances of the data matrix A  X  R n  X  D . Examples include clustering, near-est neighbors, multidimensional scaling, and kernel SVM (support vector machines). The similarity matrix requires O ( n 2 ) storage space and O ( n 2 D ) computing time.
This study focuses on the l  X  distance ( 0 &lt; X   X  2 ). Con-sider two vectors u 1 , u 2  X  R D (e.g., the leading two rows in A ), the l  X  distance between u 1 and u 2 is Note that, strictly speaking, the l  X  distance should be de-the same for all pairs, it often makes no difference whether we use d 1 / X  (  X  ) or just d (  X  ) ; and hence we focus on d
The radial basis kernel (e.g., for SVM) is constructed from d (  X  ) [7, 35], i.e., for 0 &lt; X   X  2 , When  X  =2 , this is the Gaussian radial basis kernel. Here  X  can be viewed as a tuning parameter. For example, in their histogram-based image classi fication project using SVM, [7] reported that  X  =0 and  X  =0 . 5 achieved good per-formance. For heavy-tailed data, tuning  X  has the similar effect as term-weighting the or iginal data, often a critical step in a lot of machine learning applications [19, 34]. For popular kernel SVM solvers including the Sequential Minimal Optimization (SMO ) algorithm[33], storing and computing kernels is the major bottleneck. Three compu-tational challenges were summarized in [5, page 12]:  X  Computing kernels is expensive .  X  Computing full kernel matrix is wasteful .
  X  Kernel matrix does not fit in memory.

A popular strategy in large-scale learning is to evaluate distances on the fly [5]. That is, instead of loading the sim-ilarity matrix in memory at the cost O ( n 2 ) , one can load the original data matrix at the cost O ( nD ) and recompute pairwise distances on-demand. Apparently this strategy is problematic when D is not too small. For high-dimensional data, either loading the data matrix in memory is unrealistic or computing distances on-demand becomes too expensive.
Those challenges are general issues in distanced-based algorithms, not unique to kernel SVM. The method of stable random projections provides a promising scheme to reduce the dimension D to a small k (e.g., k  X  100 ), facilitating compact data storage and efficient distance computations.
The basic procedure of stable random projections is to multiply A  X  R n  X  D by a random matrix R  X  R D  X  k ( k is small), which is generated by sampling each entry r ij i.i.d. from a symmetric stable distribution S (  X , 1) . The resultant matrix B = A  X  R  X  R n  X  k is much smaller than A and hence it may fit in memory.

In general, a stable random variable x  X  S (  X , d ) ,where d is the scale parameter, does not have a closed-form den-sity. However, its characteristic function (Fourier transform of the density function) has a closed-form:
E exp which does not have a closed-form inverse (i.e., density) ex-cept for  X  =2 (normal) or  X  =1 (Cauchy). Note that when  X  =2 , d corresponds to  X   X  2  X  (not  X   X   X ) in a normal. The fact that stable distributions in general do not have closed-form density makes the estimation task more difficult. Corresponding to the leading two rows in A , u 1 , u 2  X  R
D , the leading two rows in B are v 1 = R T u 1 , v 2 = R T u The entries of the difference, for j =1 to k , are i.i.d. samples of a stable distribution whose scale pa-rameter is the l  X  distance d (  X  ) , due to properties of Fourier transforms. For example, a weighted sum of i.i.d. standard normals (  X  =2 ) is also normal with the scale parameter (i.e., variance) being the sum of squares of all weights.
After obtaining the stable samples, one can discard the original matrix A and the remaining task is to estimate d Sampling from stable distributions is based on the Chambers-Mallows-Stuck method[6]. Recently, [20] sug-gested a much simpler (but approximate) procedure.  X  Computing all pairwise l  X  Estimating l  X  Learning with (Turnstile) dynamic streaming data  X  Estimating entropy There is a recent trend in entropy
Recall that the method of stable random projections boils down to estimating the scale parameter d (  X  ) from k i.i.d. samples x j  X  S (  X , d (  X  ) ) , j =1 to k . We consider a good estimator  X  d (  X  ) should have the following properties:  X  (Asymptotically) unbiased and small variance.  X  Computationally efficient.  X  Exponential decrease of e rror (tail) probabilities.
The arithmetic mean estimator 1 k k j =1 | x j | 2 is good for  X  =2 .When  X &lt; 2 , the task is less straightforward be-cause (1) no explicit density of x j exists unless  X  =1 or 0+ ;and(2)E ( | x j | t ) &lt;  X  only when  X  1 &lt;t&lt; X  . [21] proposed the geometric mean estimator where  X ( . ) is the Gamma function, and the harmonic mean estimator More recently, [28] proposed the fractional power estimator where
All three estimators are unbiased or asymptotically (as  X  X  X  ) unbiased. Figure 1 compares their asymptotic vari-ances in terms of the Cram  X  er-Rao efficiency, which is the ratio of the smallest possible asymptotic variance over the asymptotic variance of the estimator, as k  X  X  X  .
Figure 1. The Cram  X  er-Rao efficiencies (the higher the better, max = 1 . 00 ) of various es-timators, including the optimal quantile estima-tor proposed in this study.

The geometric mean estimator  X  d (  X  ) ,gm exhibits exponen-tial tail bounds, i.e., the errors decrease exponentially fast: where the constant G gm was explicitly provided in [21].
The harmonic mean estimator,  X  d (  X  ) ,hm , works well for small  X  , and has exponential tail bounds when  X  =0+ .
The fractional power estimator,  X  d (  X  ) ,f p , has smaller asymptotic variance than both the geometric mean and har-monic mean estimators. However, it does not have expo-nential tail bounds, due to the restriction  X  1 &lt; X   X   X &lt; X  in its definition. As shown in [28], it only has finite mo-ments slightly higher than the 2 nd order, when  X   X  2 (be-cause  X   X   X  0 . 5 ), meaning that large errors may have a good chance to occur. We will demonstrate this by simulations. quire evaluating fractional powers, e.g., | x j |  X /k . This op-eration is expensive, especially if we need to conduct this tens of billions of times (e.g., n 2 =10 10 ). For example, [7, 17] reported that, although the radial basis kernel (3) with  X  =0 . 5 achieved good performance, it was not pre-ferred because evaluating the square root was expensive.
We propose the optimal quantile estimator, by selecting the ( q  X   X  k )th smallest | x j | (i.e., 0  X  q  X   X  1 ): where q  X  = q  X  (  X  ) is chosen to minimize the asymptotic variance. This estimator is co mputationally attractive be-cause selecting should be much less expensive than evalu-ating fractional powers. If we are interested in d 1 / X  ( then we do not even need to evaluate any fractional powers.
As mentioned previously, in many cases using either d (  X  ) or d 1 / X  (  X  ) makes no difference. The radial basis kernel (3) requires d (  X  ) and hence this study focuses on d (  X  ) .Onthe other hand, if applications only need d 1 / X  (  X  ) , we can simply use (5) without the  X  th power.

In addition to the computational advantages, this estima-tor also has good theoretical properties, in terms of both the variances and tail probabilities: 1. Figure 1 illustrates that, compared with the geometric 2. The optimal quantile estimator exhibits tail bounds in
The next section will be devoted to analyzing the optimal quantile estimator.
Recall the goal is to estimate d (  X  ) from { x j } k j =1 j  X  S (  X , d (  X  ) ) , i.i.d. Since the distribution belongs to the scale family, one can estimate the scale parameter d (  X  ) quantiles. Due to symmetry, it is natural to consider the absolute values: which can be understood by the fact that if x  X  S (  X , 1) ,then / X  x  X  S (  X , d ) , or more obviously, if x  X  N (0 , 1) ,then 2 1 / 2 x  X  N 0 , X  2 . By properties of order statistics[10],  X  d (  X  ) ,q provides an asymptotically unbiased estimator. Lemma 1 provides the asymptotic variance of  X  d (  X  ) ,q . probability density function and the cumulative density function of X  X  S (  X , d (  X  ) ) , respectively.
 The asymptotic variance of  X  d (  X  ) ,q defined in (6) is
Proof: See Appendix A. . 3.1 The Optimal Quantile q  X  (  X  )
We choose q = q  X  (  X  ) so that the asymptotic variance (7) is minimized, i.e.,
Figure 2. (a) The optimal values for q  X  (  X  ) , which minimizes asymptotic variance of d (  X  ) ,q , i.e., the solution to (8). (b) The constant W  X  ( q  X  )= { q  X  -quantile {| S (  X , 1) |}}  X  . The convexity of g ( q ;  X  ) ensures a unique minimum. Graphically, g ( q ;  X  ) is a convex function of q . An algebraic proof, however, is difficult. Nevertheless, we can obtain an-alytical solutions when  X  =1 and  X  =0+ .
 Lemma 2 When  X  =1 or  X  =0+ , the function g ( q ;  X  ) defined in (8) is a convex function of q .When  X  =1 ,the optimal q  X  (1) = 0 . 5 .When  X  =0+ , q  X  (0+) = 0 . 203 is the solution to  X  log q  X  +2 q  X   X  2=0 .

Proof: See Appendix B. . It is also easy to show that when  X  =2 , q  X  (2) = 0 . 862 .
We denote the optimal quantile estimator by  X  d (  X  ) ,oq which is same as  X  d (  X  ) ,q  X  . For general  X  , we resort to nu-merical solutions, as presented in Figure 2.  X  ) unbiased, it is seriously biased for small k . Thus, it is practically important to remove the bias. The unbiased version of the optimal quantile estimator is  X  =1 , 0+ ,or 2 , we can evaluate the expectations (i.e., inte-grals) analytically or by numerical integrations. For general  X  , because the probability density is not available, the task is difficult and prone to numerical instability. On the other hand, since the Monte-Carlo simulation is a popular alter-native for evaluating difficult integrals, a practical solution is to simulate the expectations, as presented in Figure 3.
Figure 3. The bias correction factor B  X ,k in (9), obtained from 10 8 simulations for every combination of  X  (spaced at 0.05) and k .Note
Figure 3 illustrates that B  X ,k &gt; 1 , meaning that this correction also reduces vari ance while removing bias (be-cause Var ( x/c )= Var ( x ) /c 2 ). For example, when  X  =0 . 1 and k =10 , B  X ,k  X  1 . 24 , which is significant, because 1 . 24 2 =1 . 54 implies a 54% difference in terms of variance, and even more considerable in terms of the mean square er-rors MSE = variance + bias 2 .

B  X ,k can be tabulated for small k , and absorbed into other coefficients, i.e., it does not increase the computa-tional cost. We fix B  X ,k as reported in Figure 3. The simu-lations in Section 4 directly used those fixed B  X ,k values.
Figure 4 compares the computational costs of the geo-metric mean ,the fractional power ,andthe optimal quantile estimators. The harmonic mean estimator was not included as it costs very similarly to the fractional power estimator.
We used the build-in function pow in gcc for evaluat-ing the fractional powers. We im plemented a  X  X uick select X  algorithm, which is similar to quick sort and requires on av-erage linear time. For simplicity, our implementation used recursions and the middle element as pivot. Also, to ensure fairness, for all estimators, coefficients which are functions of  X  and/or k were pre-computed.
Figure 4. Relative computational cost (  X  d (  X  ) ,gm simulations at each combination of  X  and k .

The left panel averages over all k and the right panel averages over all  X  . Note that the cost of  X  d (  X  ) ,oq,c includes evaluating the  X  th fractional power once.

Normalized by the computing time of  X  d (  X  ) ,gm ,we observe that relative computational efficiency does not strongly depend on  X  . We do observe that the ratio of com-tently with increasing k . This is because, in the definition of  X  ate the fractional power once, which contributes to the total computing time more significantly at smaller k .

Figure 4 illustrates that, (A) the geometric mean estima-tor and the fractional power estimator are similar in terms of computational efficiency; (B) the optimal quantile estima-tor is nearly one order of magnitude more computationally efficient than the geometric mean and fractional power es-timators. Because we implemented a na  X   X ve  X  X uick select X  using recursions and simple pivoting, the actual improve-ment may be more significant. Also, if applications require only d 1 / X  (  X  ) , then no fractional power operations are needed and hence the improvement will be even more considerable.
Error (tail) bounds are crucial for determining k ;the variance in general is not sufficient for this purpose. If an estimator of d ,say  X  d , is normally distributed,  X  d N d, 1 k V , then the variance factor V suffices for choos-ing k because its error (tail) probability Pr |  X  d  X  d | X  2exp  X  k 2 2 V is determined by V . Usually, a reasonable estimator will be asymptotically normal, for small enough and large enough k . For a finite k and a fixed ,however, the normal approximation may be (very) poor.

Lemma 3 provides the erro r (tail) probability bounds of  X  d (  X  ) ,q for any q , not just for the optimal quantile q Lemma 3 Denote X  X  S (  X , d (  X  ) ) and its probab ility den-sity function by f X ( x ;  X , d (  X  ) ) and cumulative function by Using  X  d (  X  ) ,q in (6), then
As  X  0+ Proof: See Appendix C.

The limit in (14) as  X  0 is precisely twice the asymp-mality approximation mentioned previously. This explains why we express the constants as 2 /G . (14) also indicates that the tail bounds achieve the  X  X ptimal rate X  for this esti-mator, in the language of large deviation theory. The Bonferroni bound can determine the sample size k pairwise l  X  distance among n points can be approximated within a 1  X  factor with probability  X  1  X   X  . It suffices Lemma 3.

The Bonferroni bound can be too conservative. It is often reasonable to replace  X / ( n 2 / 2) by  X /T , meaning that except for a 1 /T fraction of pairs, any distance can be approxi-mated within a 1  X  factor with probability 1  X   X  .
Figure 5 plots the error bound constants for &lt; 1 ,for both the recommended optimal quantile estimator  X  d (  X  ) ,oq and the baseline sample median estimator  X  d (  X  ) ,q =0 . though we choose  X  d (  X  ) ,oq based on the asymptotic variance,
Figure 5. Tail bound constants for quan-tile estimators; the lower the better. Upper panels: optimal quantile estimators  X  d (  X  ) ,q  X  . Lower panels: median estimators  X  d (  X  ) ,q =0 . 5 . placing n 2 / 2 by T ), with  X  =0 . 05 , =0 . 5 ,and T =10 . Because G R,q  X   X  5  X  9 around =0 . 5 , we obtain k  X  120  X  215 .

It is possible k = 120  X  215 might be still conservative, for three reasons: (A) the tail bounds, although  X  X harp, X  are still upper bounds; (B) using G =max { G R,q  X  ,G L,q  X  } conservative because G L,q  X  is usually much smaller than G
R,q  X  ; (C) this type of tail bounds is based on relative error, which may be stringent for small (  X  0 ) distances.
In fact, some earlier studies on normal random projec-tions (i.e.,  X  =2 ) [4, 13] empirically demonstrated that k  X  50 appeared sufficient.
One advantage of stable random projections is that we know the (manually generated) distributions and the only source of errors is from random number generations. After stable projections, the projected data follow exactly the sta-ble distribution, regardless of the original real data distribu-tion. Therefore, for the purpose of evaluating the proposed estimator, it suffices to simply rely on simulations. Without loss of generality, we simulate samples from S (  X , 1) and estimate the scale parameter (i.e., 1) from the samples. Repeating the procedure 10 7 times, we can evalu-ate the mean square errors (MSE) and tail probabilities.
As illustrated in Figure 6, in terms of the MSE, the op-timal quantile estimator  X  d (  X  ) ,oq,c outperforms both the ge-ometric mean and fractional power estimators when  X &gt; 1 and k  X  20 .The fractional power estimator does not ap-pear to be very suitable for  X &gt; 1 , especially for  X  close to 2, even when k is not too small (e.g., k =50 ). For  X &lt; 1 , however, the fractional power estimator has good perfor-mance in terms of MSE, even for small k .
Figure 6. Empirical mean square errors (MSE, the lower the better), from 10 7 simulations at every combination of  X  and k . The values are multiplied by k so that four plots can be at about the same scale. The MSE for the ge-ometric mean (gm) estimator is computed ex-actly since its closed-form expression exists.

The lower dashed curves are the asymptotic variances of the optimal quantile (oq) estimator. Figure 7 presents the simulated right tail probabilities, the optimal quantile estimator consistently outperforms the fractional power and the geometric mean estimators. In fact, when  X &gt; 1 ,the fractional power estimator exhibits very bad tail behaviors. However, for  X &lt; 1 ,the fractional power estimator demonstrates good performance at least in the simulated probability range.
Figure 7. The right tail probabilities (the lower the better), from 10 7 simulations at each com-bination of  X  and k .
For  X  =2 , there have been many studies of normal random projections in machine learning, for dimension re-duction in the l 2 norm, e.g., [36, 13], highlighted by the Johnson-Lindenstrauss (JL) Lemma [18], which says k = O log n/ 2 suffices when using normal (or normal-like, e.g., [1, 29]) projection methods.

This paper studies 0 &lt; X   X  2 , not just  X  =2 . The tail bounds and sample complexity bounds are provided for all 0 &lt; X   X  2 . We should mention that our bounds at  X  =2 do not precisely recover the (optimal) bounds for normal random projections , because the optimal quantile estimator is not statistically optimal at  X  =2 , as shown in Figure 1.
Quantile-based estimators for stable distributions were studied in statistics literature[12, 31]. [12] focused on 1  X   X  2 and recommended using q =0 . 44 quantiles (mainly for the sake of smaller bias). [31] focused on 0 . 6  X   X   X  and recommended q =0 . 5 quantiles.

This study considers all 0 &lt; X   X  2 and recommends q based on the minimum asymptotic variance. Because the bias can be easily removed (at least in the practical sense), it appears not necessary to use other quantiles only for the sake of smaller bias. Tail bounds, which are useful for choosing q and k , were not provided in [12, 31].
For  X  =1 , the classical work[16] suggested the median (i.e., q =0 . 5 quantile) estimator for  X  =1 and argued that the sample complexity bound should be O 1 / 2 ( n = 1 in their study), although their bound did not specify the constant and required an  X  small enough X  argument.
For  X  =1 , [9] used a linear combination of quantiles with carefully chosen coeffic ients to obtain an asymptoti-cally optimal estimator of the s cale parameter. While it is possible to extend their result to general 0 &lt; X &lt; 2 (requir-ing some non-trivial work), whether it will be practically better than the optimal quantile estimator is unclear because the extreme quantiles severely affect the performance. Dis-carding (truncating) extreme quantiles reduces the sample size. Also, exponential tail bounds of the linear combina-tion of quantiles for stable distributions may not exist or may not be feasible to derive. In addition, the optimal quan-tile estimator is computationally more efficient.
This paper and all previous work on stable random pro-jections used symmetric stable distributions , i.e., the dis-tribution specified by the Four ier transform (4). Recently, [23] proposed Compressed Counting (CC) , which dramat-ically improves the performance of (symmetric) stable ran-dom projections , especially as  X   X  1 . One application of CC is for estimating entropy of data stream[15, 14, 22].
CC used skewed stable random projections and is only applicable to the strict Turnstile model, which restricts S [ i ]  X  0 in the Turnstile model (1) (but the increment I can be either negative or positive). In most data stream com-putations, the strict Turnstile model suffices. For example, one can only cancel an order if he/she did place the order.
A limitation of CC is that it is not applicable to estimat-ing pairwise distances (e.g ., comparing two streams).
One competitor of stable random projections is the tech-nique called Conditional Random Sampling (CRS) [25, 26, 27]. CRS only works well in sparse data such as text and histogram-based image data. A distinct feature of CRS is One-Sketch-for-All , meaning that the same set of sketches (samples) can be utilized for approximating many different types of distances including the l  X  distance and  X  2 distance.
Many data mining and machine learning algorithms op-erate on the training data only through pairwise distances. Computing, storing, updating and retrieving the  X  X atrix X  of pairwise distances is challenging in applications involving massive, high-dimensional, and possibly streaming, data. For example, the pairwise distance matrix can not fit in memory when the number of observations exceeds 10 6 .
The method of stable random projections provides an ef-ficient mechanism for computing pairwise distances using low memory, by transforming the original high-dimensional data into sketches , i.e., a small number of samples from  X  -stable distributions, which are much easier to store and re-trieve. This method provides a uniform scheme for comput-ing the l  X  pairwise distances for all 0 &lt; X   X  2 .
To recover the original dist ances, we face an estimation task. Compared with previous estimators based on the geo-metric mean , harmonic mean ,or fractional power , the pro-posed optimal quantile estimator exhibits two advantages. Firstly, the optimal quantile estimator is nearly one order of magnitude more efficient (e.g., reducing the training time from one week to one day). Secondly, the optimal quan-tile estimator is consider ably more accurate when  X &gt; 1 ,in terms of both the variances and error (tail) probabilities.
One theoretical contribution is the explicit tail bounds for general quantile estimators and consequently the sam-ple complexity bound k = O log n/ 2 , which may guide practitioners in choosing k , the number of projections.
The author wishes to thank the anonymous reviewers for their constructive comments. The author thanks John Lin-derman from AT&amp;T Labs Research for some interesting dis-cussions about sorting algorithms. This work is supported by NSF grant DMS-0808864 and a gift from Google.

Denote f X x ;  X , d (  X  ) and F X x ;  X , d (  X  ) the proba-bility density function and the cumulative density func-tion of X  X  S (  X , d (  X  ) ) , respectively. Similarly we use f
Z z ;  X , d (  X  ) and F Z z ;  X , d (  X  ) for Z = symmetry, the following relations hold ing known statistical results, e.g., [10, Theorem 9.2], the asymptotic variance of  X  d 1 / X   X ,q should be
First, consider  X  =1 . In this case, It suffices to study L ( q )=log g ( q ;1) .

Because sin( x )  X  x for x  X  0 , it is easy to see that Thus, L  X  0 ,i.e., L ( q ) is convex and so is g ( q ;1)= e L Since L (1 / 2) = 0 , we know q  X  (1) = 0 . 5 .

Next we consider  X  =0+ , using the fact [21] that as  X   X  0+ , | S (  X , 1) |  X  converges to 1 /E 1 ,where E 1 stands for an exponential distribution with mean 1.

Denote h = d (0+) and z j  X  h/E 1 . The sample quantile estimator becomes
In this case,
It is straightforward to show that 1  X  q q log 2 q is a convex func-tion of q and the minimum is attained by solving  X  log q  X  2 q  X   X  2=0 , i.e., q  X  =0 . 203 .

Given k i.i.d. samples, x j  X  S (  X , d (  X  ) ) , j =1 to k .Let z = | x j | , j =1 to k . Denote by F Z ( t ;  X , d (  X  ) ) the cumu-cumulative density of z j , j =1 to k .

The basic result of order statistics says kF
Z,k ( t ;  X , d (  X  ) ) follows a binomial distribution[10], i.e., kF we replace F Z ( t ;  X , d (  X  ) ) by F ( t, d ) , F Z,k ( t ;  X , d F ( t, d ) ,and d (  X  ) by d , only in this proof.

Using the original binomial Chernoff bounds [8], we ob-tain, for &gt; 0 , and for 0 &lt; &lt; 1 ,
Consider the general quantile estimator  X  d (  X  ) ,q defined in (6). For &gt; 0 , (again, denote W = q -quantile {| S (  X , 1) where t =(1+ ) 1 / X  W and q =(1  X  ) F ( t ;1) . Thus where For 0 &lt;&lt; 1 , where t =(1  X  ) 1 / X  W and q =(1+ ) F ( t ;1) . Thus, where
Denote f ( t ; d )= F ( t ; d ) . Using L X  X ospital X  X  rule
Similarly To complete the proof, apply the relations about Z = | X |
