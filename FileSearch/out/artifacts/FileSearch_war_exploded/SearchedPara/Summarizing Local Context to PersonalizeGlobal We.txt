 The PC Desktop is a very rich repository of personal infor-mation, efficiently capturing user X  X  interests. In this paper we propose a new approach towards an automatic personal-ization of web search in which the user specific information is extracted from such local desktops, thus allowing for an increased quality of user profiling, while sharing less pri-vate information with the search engine. More specifically, we investigate the opportunities to select personalized query expansion terms for web search using three different desk-top oriented approaches: summarizing the entire desktop data, summarizing only the desktop documents relevant to each user query, and applying natural language processing techniques to extract dispersive lexical compounds from rel-evant desktop resources. Our experiments with the Google API showed at least the latter two techniques to produce a very strong improvement over current web search. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval; H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing X  Linguisting processing ;H.3.5[ Information Storage and Retrieval ]: Online Information Services X  Web-based services Algorithms, Experimentation Personalized Web Search, Desktop Summarization, Rele-vance Feedback, User Profile
Keyword queries are inherently ambiguous. Take for ex-ample the query  X  X anon book X , which covers several dif-ferent areas of interest: religion, digital photography, and Copyright 2006 ACM 1-59593-433-2/06/0011 ... $ 5.00. music. Clearly, since the URLs most likely to be visited are those returned at the very top of the result list [16], search engine output should be filtered to better align the results with user X  X  interests. Thus, the photographer should receive pages about digital cameras, the clergyman should get religious books, and the performing artist should obtain documents on music theory. In fact, a recent study pre-sented by SearchEngineWatch [35] indicates that more than 80% of the users would prefer to receive such personalized search results.

One of the early Information Retrieval techniques used to enhance search quality is Relevance Feedback [31]. Even though it does not deal with user specific personalization per se, it has been shown to be quite effective in improving retrieval performance. It is based on collecting relevance in-formation from a set of carefully selected documents, which is then used to modify the search query and perform an additional retrieval step. However, when selecting these relevant documents automatically (e.g., by considering all Top-K search engine output documents as relevant), several terms unrelated to the user query might still be added as ex-pansion keywords if they are present in these automatically selected documents and have the suitable distribution in the document collection [25]. In this paper we address this prob-lem by choosing query expansion terms from a different data source, the personal desktop, in which all documents are at least to a certain extent related to user X  X  interests. By  X  X er-sonal desktop X  or  X  X C desktop X  we denote the set of per-sonal documents residing on each user X  X  personal computer. Thus, the personalization dimension is also automatically included in the search algorithm.

Several advantages arise when moving web search person-alizationdowntothedesktoplevel. Firstcomesofcourse the quality of personalization: The local desktop is a very rich repository of information, accurately describing most, if not all interests of the user. More, as all the  X  X rofile X  information is stored and exploited locally, on the personal desktop, another very important benefit can be drawn: Pri-vacy. Search engines should not be able to know about a person X  X  interests, i.e., they should not be able to connect some person with the queries she issued, or worse, with the output URLs she clicked within the search interface 1 (see
Generally, search engines can map queries at least to IP ad-dresses, for example by using cookies and mining the query logs. However, moving the user profile entirely down to the desktop level would at least ensure such information is not explicitly associated to a user ID and stored on the search engine side. More, if desired, the desktop level personalized Volokh [38] for a discussion on the privacy issues related to personalized web search). Almost all previous algorithms for personalizing web search need such input information though.

In this paper we propose a novel approach to selecting query expansion terms for web search by adapting sum-marization and natural language processing techniques to extract these supplementary keywords from locally stored desktop documents. After having discussed related work in Section 2, we investigate three broad methods towards achieving this goal. First, in Section 3.1 we propose to sum-marize the entire desktop using term clustering techniques and then to choose suitable expansion terms from these clus-ters, exploring both context independent and query biased approaches. Afterwards, we move our focus towards single document summarization techniques. In Section 3.2 we pro-pose to issue the original web user query on the desktop and to extract expansion keywords from the most significant sen-tences within the Top-K (desktop) hits. Similarly, in Section 3.3 we investigate the possibilities to select query expansion keywords from the most dispersive lexical compounds within the Top-K (desktop) hits returned to user X  X  initial web query. Our experiments performed with the Google API 2 (Section 4) show an improvement in Mean Average Precision (MAP) [2] of up to 81.57% for single word queries and up to 21.11% for multi-word queries when comparing to regular Google web search.
Within this work we find ourselves at the confluence of personalized search and summarization algorithms. There are only very few previous publications combining these ar-eas and even fewer address both the PC Desktop and the World Wide Web environments. From the existing articles, a very important one is the work of Lam and Jones [19]. They improve pseudo relevance feedback by selecting query expansion terms only from the most significant sentences (as computed using query based summarization techniques) of the Top-K search engine output. We used their technique as a starting point for one of our algorithms, but we moved the set of relevant documents from the web down to the desk-top, and we adapted the methods for selecting meaningful sentences appropriately.

The following two sections will now discuss some of the most important works in either one of our two main research areas, personalizing search and summarization.
We distinguish two broad approaches to personalizing web search, based on the way user profiles are exploited to achieve personalization: (1) integrating the personaliza-tion aspect directly into PageRank [27], and (2) using the personalized search algorithm as an additional search engine measure of importance (together with PageRank, TFxIDF, etc.). Letusnowinspecteachofthemindetail.

PageRank-based Methods. The most efficient person-alized search algorithm will probably be the one having the personalization aspect already included in the initial rank-ings. Unfortunately, this seems very difficult to accomplish. searchapplicationcouldbedevelopedinsuchawayasto conceal the user identity and search history from the search engine. http://api.google.com Initial steps in this direction have been already described by Page and Brin [27], who proposed a slight modification of the PageRank algorithm to redirect the random surfer 3 towards some preferred pages. However, it is clearly impos-sible to compute one PageRank vector for each user profile, i.e., for each set of pages  X  X rivileged X  by the random surfer.
Haveliwala [14] proposed to alleviate this problem by building a topic-oriented PageRank, in which a set of 16 PageRank vectors biased on each of the 16 main topics of the Open Directory is initially computed off-line, and then these vectors are combined at run-time based on the sim-ilarity between the user query and each of the 16 topics. This approach is clearly feasible, but also limited to the 16 pre-defined topics.

Finally, Jeh and Widom [15] identified the possibility to express each Personalized PageRank Vector (PPV) as a lin-ear combination of a set of  X  X pecial X  vectors, called basis vectors. At query time, an approximation of the PPV is constructed from the precomputed basis vectors using dy-namic programming. Nevertheless, the input data (a set of preferred URLs) can only be selected from within a small pre-defined group of pages 4 (common to all users) and the computation time is relatively high for large scale graphs.
Hybrid Ranking Methods. As search engines rely on various indicators when ordering query output (i.e., textual content, link structure, etc.), current research has focused more on a different approach: Building an independent and simpler personalized ranking algorithm, whose output is combined with that of PageRank, TFxIDF, and other met-rics.

Sugiyama et al. [34] analyze user X  X  surfing behavior and generate user profiles as features (terms) of the pages they visited. Then, upon issuing a new query, the results are ranked based on the similarity between each URL and the user profile. In a similar work, Gauch et al. [12] build pro-files by exploiting the same surfing information (i.e., page content and length, time spent on each URL, etc.), as well as by spidering the URLs saved in the personal web cache and classifying them into topics of interest. Both solutions are orthogonal to ours, as they only inspect previous web browsing behavior, whereas we explore a much richer infor-mation repository, the entire PC Desktop.

Liu et al. [22] restrict searches to a set of categories defined in the Open Directory. Their main contribution consists in investigating various techniques to exploit users X  browsing behavior for learning profiles as bags of words as-sociated to each topical category. Similarly, Chirita et al. [6] proposed to use already existing large scale taxonomies to personalize search by reordering search results based on several graph distances between the topic associated to each output URL and the topics defined in the user profile. Even though both approaches showed good results when com-pared to non-personalized web search, their performance is very much dependent on the URLs classified within such taxonomies.
PageRank can be viewed as the stationary distribution of an infinite random walk over the web graph, following an outgoing link from the current page with probability (1  X  (usually c =0 . 15) and getting bored and selecting a random page with probability c .
Some work has been done in the direction of improving the quality of this set of pages [7], but users are still restricted to select their preference set from a rather restricted corpus.
More recent, the work of Teevan et al. [36] is the only one also exploiting desktop data for web search. They modified the query term weights from the BM25 weighting scheme [17] to incorporate user interests as captured by her desktop index. However, they selected the query expansion terms from the Top-K documents returned by the web search en-gine, whereas we seek for these terms within the more rel-evant, personal PC desktop data, thus avoiding choosing unrelated expansion keywords which could be present in the Top-K web search output [25].
Automated summarization usually deals with concatenat-ing text-span excerpts (i.e., sentences, paragraphs, etc.) into a human understandable document summary and it dates back to the 1950 X  X  [23]. More, with the advent of the world wide web and large scale search engines, an increased at-tention has been focused towards this research area and quite several new approaches have been proposed. For ex-ample, the diversity of concepts covered by a document has been first explored by Carbonell and Goldstein [4] in 1998. They proposed to use Maximal Marginal Relevance (MMR), which selects summary sentences that are both relevant to the user query and least similar to the previously chosen ones. Later, Nomoto and Matsumoto [26] developed this into a generic single-document summarizer that first identi-fies the topics within the input text, and then outputs the most important sentence of each topic area.

Another quite different new approach was to generate the summary as the set of top ranked sentences from the origi-nal document according to their salience or likelihood of be-ing part of a summary [13, 11]. Consequently, more search specific applications of summarization have been proposed. Zeng et al. [41] for example used extraction and ranking of salient phrases when clustering web search results. Oth-ers have used hierarchies to improve user access to search output by summarizing and categorizing the retrieved doc-uments [20], or to organize the topic words extracted from textual documents [21, 33]. Finally, this work is in fact an application of summarization techniques into a new research area, that of personalizing web search.

Along with the fast growth of information amounts, a re-cent need for multi-document summarization has also been exerted and thus a few algorithms have been proposed. A very popular one is MEAD [28], which first uses an imple-mentation of the  X  X ile metaphor X  [32] to create groups of similar documents, and then selects the most representative sentences from these clusters according to several salience measures. We used it as basis for one of our desktop sum-marization approaches.

There exists also a large amount of document clustering research and we believe this could be exploited to develop better means of summarizing personal information reposito-ries (see Willett [39] for a relatively old, but very comprehen-sive review of the fundamental aspects related to clustering). However, it is outside the scope of this paper to review these techniques here, and thus we only mention some of the rele-vant ones for our scenario, namely Scatter/Gather [9, 8] for clustering based on the term vectors describing each docu-ment, Grouper [40] for clustering based on phrases rather than terms, and the work of Zeng et al. [41] for combining multiple evidences in document clustering.
 Finally, this work is to some extent connected to the Just-In-Time Information Retrieval paradigm, in which each user X  X  currently active desktop document is automatically analyzed to extract its keywords (i.e., using algorithms sim-ilar to the ones we present in this paper), and then to rec-ommend other related documents which could be useful in performing the on-going activity. Several approaches exist, either aimed at finding such relevant documents among the personal (desktop) repository [29, 10], or within the World Wide Web [3, 5].
Summarizing desktop data is itself a challenge, since most of the current summarization research has not tackled such complex data sets. For example, it is quite common nowa-days to have about 100,000 indexable desktop items (i.e., containing some amount of textual information), these doc-uments being either HTML pages, Word documents, small textual notes and chat conversations, or even smaller meta-data for mp3 files, etc. We therefore investigated sev-eral summarization paths towards choosing the right web query expansion keywords: (1) a multi-document summa-rizer which outputs centroids as bags of words with weights associated to them, (2) a single-document summarizer which ranks sentences according to their representativeness for the user query and for the document itself, and (3) a lexical com-pounds generator for the top ranked documents returned when issuing the user query on the desktop. We think these three paths cover most of the important approaches to se-lecting both query specific and query independent expansion terms from the desktop document collection.

Following the work of Lam et al. [19], we chose to index only documents with at least 7 indexable terms (i.e., not stopwords). Moreover, we defined several heuristics to ex-clude from the index some very common automatically gen-erated file categories such as Java documentation, as their large granularity tended to negatively influence the desk-top summaries. Finally, when choosing the terms to expand user X  X  original query (e.g., after the centroids have been out-put by the multi-document summarizer), we decided to only use TF, rather than TFxIDF, as one very frequent local term (e.g., PageRank) might in fact be rather rare in the web 5 A large stopword list was used to initially remove any pos-sible misleading terms. Also, summarization was achieved employing a logged version of TF in order to avoid having some too frequent terms mislead the results. The variants of TF and IDF we used were as follows: where TF t k ,D j is the actual frequency of term t k in docu-ment D j , N is the total number of documents in the collec-tion and DF t k is the document frequency for term t k .
Let us now present the details of each of our above men-tioned three approaches to extract suitable web search query expansion terms from personal information repositories.
Note that frequent local terms have a low TFxIDF score on the desktop. However, they might have a high TFxIDF score on the Web, thus being very discriminative when expanding user X  X  query. The centroid based summarization was first proposed by Radev et al. in [28], who applied the  X  X ile metaphor X  docu-ment clustering approach of Rose et al. [32] for summariza-tion purposes. Its main underlying principle is to represent all documents in a collection as in traditional IR, using term vectors, and then to group these vectors into representa-tive clusters. Thus, a scan is performed over all documents within the collection; for every document, if its similarity with at least one cluster centroid is above a certain thresh-old, it will be associated to its most similar cluster. Other-wise, a new cluster is created having the current document term vector as centroid.

As the algorithm of Radev et al. was intended for much smaller data sets such as news articles, we had to incor-porate in it several desktop specific aspects. First, we or-dered terms within cluster centroids only by their TF values, rather than TFxIDF. The document similarity formula was the sole place where both TF and IDF had been consid-ered. This is reasonable, since two documents both contain-ing many infrequent words are most probably related within the desktop environment as well. Second, due to the large amount of data residing in personal information repositories, we had to limit the centroid cluster size to its top  X  = 500 terms, as otherwise the computation time would have grown too much. Third, we attempted to cluster either all desk-top indexable documents, or only the documents manually accessed within the last three months a system call hook wasemployedheretologeveryuserresourceopen/cre-ate access for this period.. As the former approach covered much more documents, the optimal similarity threshold was  X  =0 . 01, whereas for the latter one we found  X  =0 . 1to perform best. Other 14 possible values were investigated, ranging from 0 . 001 to 0 . 1. Finally, we defined  X  X C Desk-top X  as the collection of all emails, web cache documents, and indexable files of a user. For the latter ones, we did not index the entire hard disks, but only the list of paths con-taining personal documents, as specified by each person 6 The complete form of the algorithm is also depicted in Al-gorithm 3.1.
Although this definition was targeted at single-user PCs, one could easily extend it to multiple-user ones. Algorithm 3.1 . Centroid Based Desktop Summarization. Similarity (Document D i , Centroid C j ): where CF t k ,C j is the weight of term t k in centroid C 1 : For each new document D i 2 :MaxSim= Max  X  j Similarity ( D i ,C j ) 3 :MaxCen= { j | Similarity ( D i ,C j )== MaxSim } 4 : If ( MaxSim  X   X  or Centroids) then 5 : Create new centroid with the top  X D i terms 6 : Else 7 : Let d be the nb. of docs. covered by C M axCen . 8 : For each t k  X  D i  X  C M axCen 10 : Reduce C M axCen to its top  X  terms
Manual inspection showed these clusters to be quite rep-resentative for the data they represented. Yet how can they be used for query expansion? We have investigated several options: 1. Select the Top-C biggest clusters (with respect to the 2. Use W i to choose C expansion terms proportionally to 3. Select the clusters that contain all keywords from the
In all cases we experimented with C =5and C = 7, but due to the space limitations we will only report the former minimally better parameter setting. Also, as in all three cases the selected query expansion terms may not always be related to the actual user query (since they represent a part of the entire desktop), significantly more importance (i.e., weight) should be given to user X  X  initial query keywords. As Google API does not allow specification of term weights, we experimented with this feature by using the above men-tioned BM25 model to re-rank the Top-50 URLs output to the user query. This way, the entire document candidate set was surely related to the original query.
There exist quite several approaches to sentence based summarization. However, we chose to start from [19], as it had a similar end goal with us, i.e., to select terms for query expansion. Thus, for each user query, we first issue it on the PC desktop and retrieve the Top-30 documents using the Lucene 7 scoring function. Then, from each of these docu-mentsweextractedthemostsalientsentenceswithrespect to the user query by evaluating the following formula:
The first term is based on Luhn X  X  cluster measure [23] and is the ratio between the square amount of significant words within the sentence and the total number of words therein. A word is significant in a document if its real frequency (i.e., not logged) is above a threshold as follows:
TF &gt; ms = with NS being the total number of sentences in the docu-ment.
 The second term comes from the work of Tombros and Sanderson [37] and is computed using the ratio between the square number of query terms present in the sentence and the total number of terms from the query. It is based on the belief that the more query terms contained in a sentence, the more likely will that sentence convey information highly related to the query.

Lam et al. [19] also investigated the use of several other sentence salience metrics, such as the document title and the location of each rated sentence within a document. We argue that such metrics are not suitable for PC desktop resources, first because many of them have no title, and second because unlike for news articles, where the first sentences are usually quite representative for the entire document, here there is no clear correlation between the location of sentences and their importance for a document.

Once these sentence scores were computed, we sought for (five) query expansion terms using two approaches: (1) us-ing W i (see BM25 in the previous section) over the top 9 sentences, as reported in [19], and (2) using W i over the top 2% sentences. The new latter approach is motivated by pre-vious findings that longer documents tend to contain more content words [18] and does indeed slightly improve over the former one (see Section 4 for more details).

As these query expansion terms had been selected from documents relevant to the user query, we experimented here with two different techniques: (1) simply expanding the http://lucene.apache.org query with the selected keywords having the same weight as the original ones (since the Google API did not allow us to specify different weights), and (2) re-ranking Google X  X  Top-50 output URLs according to the modified BM25 scheme, as with the previous algorithm. The amount of five query ex-pansion keywords has been selected based on two premises: First, previous Okapi TREC submissions [30] also employed only a small number of expansion terms; second, at experi-mentation time, the Google API allowed at most ten terms per query, and thus using more than five expansion key-words would have incorrectly limited too much the maxi-mum query length available to our testers. Even so, both query expansion approaches showed a highly significant im-provement over the original Google results.
The final algorithm is based on the natural language pro-cessing solutions proposed by Anick and Tipirneni [1]. They defined the lexical dispersion hypothesis , according to which an expression X  X  lexical dispersion (the number of different compounds it appears in within a document set) can be used to automatically identify key concepts of that docu-ment set. As the local desktop resources are implicitly rel-evant for user X  X  interests, we thus sought for such concepts within desktop files that are also relevant for the user query.
As with our previous algorithm, we start by issuing the user query down on the desktop search engine and selecting the Top-30 results. Then, we inspect these output docu-ments for all their lexical compounds of the following form, as defined in [1]: Although we performed this step at run-time (using Word-Net [24]), it could be easily run off-line, at indexing time, as the compound generation process is not influenced by the user query (i.e., all compounds from the selected documents are generated). The only query dependent aspect is the selection of the documents whose compounds are included in the dispersion calculation. Thus, once these lexical con-structions have been identified, they are sorted depending on their dispersion within these Top-30 desktop documents and the terms of the most frequent three compounds are used as query expansion keywords.

The output of the lexical algorithm so far showed itself to be indeed very significant for the original query, and thus we chose to experiment both the regular query expansion approach and that of re-ranking Google X  X  Top-50 original output URLs (see the end of the previous section for more details on each of these solutions).
We started our analysis by manually inspecting the output of each desktop summarization algorithm. In all cases, we found it to be quite representative for the original document or set of documents. However, as in other similar works (e.g., [19]), our main objective measure of summary quality was its overall effect on web search performance, and thus we will focus our presentation only towards this aspect.
To evaluate the precision of our personalization algorithms we interviewed 15 subjects (either architects or researchers in different computer science areas and education). In the first phase of the evaluation they installed our activity log-ging tool and used it continuously for about three months. Then, they installed our desktop indexer and chose six queries related to their everyday activities in a similar man-ner to the work of Chirita et al. [6], as follows:
For each query, the Top-10 results generated by 15 ver-sions of the algorithms we presented in Section 3 were shuf-fled into one set containing usually between 50 and 80 URLs. Thus, each subject had to assess about 400 URLs for all six queries, being neither aware of the algorithm, nor of the ranking of each assessed URL. Overall, 90 queries were is-sued and about 6,000 URLs were evaluated during the ex-periment. For each of these URLs, the testers had to give a rating ranging from 0 to 2, thus dividing the relevant results in two categories, (1) relevant and (2) highly relevant. Also, the output quality was evaluated in terms of Mean Average Precision (MAP) over the first 10 results, precision at the first 5 positions of the resulted ranking (P@5), as well as precision at the top 10 output rankings (P@10). Finally, all our results were tested for statistical significance using T-tests (i.e., we tested whether the improvement over the Google API output 9 is statistically significant).
In all the forthcoming tables, we will label the algorithms we evaluated as follows:
Of course, that did not necessarily mean that the query had no other meaning.
Whenever necessary, we also tested for significance the dif-ference between pairs of the algorithms we proposed. Table1: Precisionatthefirst5results,atthefirst10 results, and Mean Average Precision considering all the relevance judgments for single word ambiguous queries.
Ambiguous queries. Our results for the various sce-narios using ambiguous queries are presented in Tables 1, 2, 3, and 4. All our approaches performed very well on single-word ambiguous queries, the improvement for this particular case reaching even 163.44% for SentQEF (Table 1). On the other hand, with multi-word queries the re-ranking methods performed all relatively poor. More, this phenomenon oc-curred relatively often within all multi-word scenarios, espe-cially for the clustering methods. This is most probably be-cause the query expansion terms were too specific for user X  X  interests to help accurately re-order only the Google Top-50 results. However, inspecting more than the first 50 URLs is very time consuming without having a real search en-gine available. Finally, the fact that the high quality results were usually residing below the 50 th place is also probed by the very good results obtained with LexQE, SentQEF and SentQEP (always a significant improvement over Google), which simply expand the query and issue it again to the search engine.

Semi-ambiguous queries. Theoutcomewasverysim-ilar for the semi-ambiguous queries (Tables 5, 6, 7, and 8) with the only difference that our improvements were slightly smaller, i.e., up to 44.73% for single-word queries with Sen-tQEP (Table 7) and up to 35.67% for multi-word queries with LexQE (Table 6). This is correct, since Google has been shown to perform better as the query specificity in-creases [6]. We should also note that in all query type sce-narios, restricting the analysis to only highly relevant results had a very small impact on the findings, minimally modi-fying them in both directions. For example, the improve-ment generated by LexQE for single-word semi-ambiguous queries was somewhat significant considering all relevance judgments and not significant considering only the highly relevant ones, but on the other hand the improvement of Table 2: Precision at the first 5 results, at the first 10 results, and Mean Average Precision considering all the relevance judgments for multi-word ambiguous queries.
 Table 3: Precision at the first 5 results, at the first 10 results, and Mean Average Precision consider-ing only the highly relevant answers selected by our testers for single word ambiguous queries.
 SentQEF was significant only when exclusively considering the highly relevant results.

Clear queries. As expected, it was more difficult to overcome Google output for such queries, the best perfor-mances obtained being a 64.50% enhancement for single-word queries (LexQE, Table 9) and 9.30% for multi-word ones (SentQEP, Table 12), while the re-ranking approaches had only an average performance with small improvements for single-word queries and even worse results in the multi-word setting. The complete results are depicted in Tables 9, 10, 11, and 12.

Conclusions. Three broad conclusions could be drawn from our results. First, query expansion terms inferred from desktop data are quite specific, and thus useful only when re-ranking a large number of the top search engine results, in order to reach pages that are relevant, while also containing the expansion terms with a large frequency.

Second, when looking at smaller experiment sets (e.g., Ta-bles 1-12), sometimes another phenomenon occurred: Small significance levels for quite high MAP differences, pointing out that although for most users our query expansion algo-rithms are very efficient, for another small number of sub-jects it performed relatively similar to Google. This was because their desktops contained almost no textual content (e.g., for junior students). Further research is needed to cope with such special cases, for example by indexing auto-matically generated metadata about local multimedia files, games, etc.
 Table 4: Precision at the first 5 results, at the first 10 results, and Mean Average Precision consider-ing only the highly relevant answers selected by our testers for multi-word ambiguous queries.
 Table 5: Precision at the first 5 results, at the first 10 results, and Mean Average Precision considering all the relevance judgments for single word semi-ambiguous queries.

Finally, when these expansion terms were used for regu-lar query expansion over the entire web, they did provide overall strong improvements compared to the initial search. For 13 of our subjects, including the junior students, the best algorithm overall was LexQE. The other two persons (both computer science researchers) ranked LexQE second, after SentQEP and Google respectively. Also, over the en-tire experiment, LexQE improved over Google with 37.90% (significant with p&lt; 10  X  9 ), SentQEP improved with 29.00% ( p&lt; 10  X  6 ), and SentQEF with 26.67% ( p  X  10  X  4 ). More-over, LexQE was also significantly better than SentQEP and SentQEF ( p =0 . 04), thus being our best approach by far. All results are depicted graphically in Figure 1.
Practical Issues. The response time is quite impor-tant for current search engines, and thus only those algo-rithms which can yield a quick valuable output are suitable for large scale topic independent applications. Therefore, even though the Sentence Selection approach did yield very good results when used with query expansion techniques, as it is delayed by the computation of query specific sentence scores, it only makes a good candidate for domain specific search engines (e.g., medical), where some additional time can be traded for a better output.
 At the other end, both the Clustering methods and the Lexical Compounds ones provide very quick results, as their computation demanding step can be implemented off-line at indexing time, thus making them (especially the latter one) very suitable candidates for real world search applications. Table 6: Precision at the first 5 results, at the first 10 results, and Mean Average Precision consider-ing all the relevance judgments for multi-word semi-ambiguous queries.
 Table 7: Precision at the first 5 results, at the first 10 results, and Mean Average Precision consider-ing only the highly relevant answers selected by our testers for single word semi-ambiguous queries.
 Figure 1: Relative MAP gain (in %) for each algo-rithm overall, as well as separated per query length category.
In this paper we proposed to select query expansion terms for web search by adapting summarization and natural lan-guage processing techniques to extract these supplementary keywords from locally stored desktop documents. We inves-tigated three possible approaches, based on (1) summarizing the entire desktop data, (2) summarizing only the desktop Table 8: Precision at the first 5 results, at the first 10 results, and Mean Average Precision consider-ing only the highly relevant answers selected by our testers for multi-word semi-ambiguous queries.
 Table 9: Precision at the first 5 results, at the first 10 results, and Mean Average Precision consider-ing all the relevance judgments for single word clear queries. documents relevant to the current user query, and (3) ex-tracting dispersive lexical compounds from relevant desk-top resources. Our experiments showed an improvement in Mean Average Precision of up to 81.57% for single word queries and up to 21.11% for multi-word queries when com-pared to regular Google web search.

While some of our approaches did perform very well al-ready, in future work we intend to investigate their perfor-mance using a study targeted towards persons with very limited textual data, as well as to analyze the possibilities to automatically generate and exploit additional metadata which would further increase web search performance not only for such users, but also for those already possessing vast textual resources.
This work was supported by the Nepomuk project funded by the European Commission under the 6th Framework Pro-gramme (IST Contract No. 027705). [1] P. G. Anick and S. Tipirneni. The paraphrase search [2] R. Baeza-Yates and B. Ribeiro-Neto. Modern Table 10: Precision at the first 5 results, at the first 10 results, and Mean Average Precision consider-ing all the relevance judgments for multi-word clear queries.
 Table 11: Precision at the first 5 results, at the first 10 results, and Mean Average Precision consider-ing only the highly relevant answers selected by our testers for single word clear queries.
 [3] J. Budzik and K. Hammond. Watson: Anticipating [4] J. Carbonell and J. Goldstein. The use of mmr, [5] P. A. Chirita, C. S. Firan, and W. Nejdl. Pushing [6] P.-A. Chirita, W. Nejdl, R. Paiu, and [7] P.-A. Chirita, D. Olmedilla, and W. Nejdl. Pros: A [8] D.R.Cutting,D.R.Karger,andJ.O.Pedersen.
 Table 12: Precision at the first 5 results, at the first 10 results, and Mean Average Precision consider-ing only the highly relevant answers selected by our testers for multi-word clear queries. [9] D.R.Cutting,J.O.Pedersen,D.R.Karger,and [10] S. Dumais, E. Cutrell, R. Sarin, and E. Horvitz. [11] G. Erkan and D. R. Radev. Lexrank: Graph-based [12] S. Gauch, J. Chaffee, and A. Pretschner.
 [13] J. Goldstein, M. Kantrowitz, V. Mittal, and [14] T. Haveliwala. Topic-sensitive pagerank. In In [15] G. Jeh and J. Widom. Scaling personalized web [16] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and [17] K. S. Jones, S. Walker, and S. Robertson.
 [18] S. Katz. Distribution of content words and phrases in [19] A. M. Lam-Adesina and G. J. F. Jones. Applying [20] D. Lawrie and W. Croft. Generating hierarchical [21] D.Lawrie,W.B.Croft,andA.L.Rosenberg.
 [22] F. Liu, C. Yu, and W. Meng. Personalized web search [23] H. Luhn. Automatic creation of literature abstracts. [24] G. Miller. Wordnet: An electronic lexical database. [25] M. Mitra, A. Singhal, and C. Buckley. Improving [26] T. Nomoto and Y. Matsumoto. A new approach to [27] L. Page, S. Brin, R. Motwani, and T. Winograd. The [28] D. R. Radev, H. Jing, M. Stys, and D. Tam.
 [29] B. J. Rhodes and P. Maes. Just-in-time information [30] S. E. Robertson and S. Walker. Okapi/keenbow at [31] J. Rocchio. Relevance feedback in information [32] D.Rose,R.Mander,T.Oren,D.Ponceleon, [33] M. Sanderson and W. B. Croft. Deriving concept [34] K. Sugiyama, K. Hatano, and M. Yoshikawa.
 [35] D. Sullivan. The older you are, the more you want [36] J. Teevan, S. Dumais, and E. Horvitz. Personalizing [37] A. Tombros and M. Sanderson. Advantages of query [38] E. Volokh. Personalization and privacy. Commun. [39] P. Willett. Recent trends in hierarchic document [40] O. Zamir and O. Etzioni. Grouper: a dynamic [41] H.-J. Zeng, Q.-C. He, Z. Chen, W.-Y. Ma, and J. Ma.
