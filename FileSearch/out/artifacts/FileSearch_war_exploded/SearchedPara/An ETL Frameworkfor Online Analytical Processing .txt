 Linked Open Data (LOD) [2] is an increasingly popular method that allows organiza-tions to publish their data in machine-readable format, in particular using the Resource Description Framework (RDF) [3]. According to the State of the LOD Cloud 1 , over 31.6 billion triples in 295 datasets were published as of September 2011, covering life science fields, remote sensors, and governments, among others.

Most LOD datasets comprise not only text data, but also numerical data. For exam-ple, [9] is to publish statistical data about Eurostat 2 and Linked Sensor Data [13] is to publish sensor readings such as temperature, humidity, etc. For this reason, to make the best use of this type of data, it is vital to be able to retrieve necessary informa-tion from massive LOD datasets and apply pot entially complex analytical processing to them [6, 10].

Online analytical processing (OLAP) [4 , 8] has been used in a wide spectrum of applications as a powerful means of performing online analysis and reporting over databases containing numerical attributes. For this reason, OLAP is one of the easi-est and natural ways to analytically process LOD datasets. Several works have applied OLAP to LOD [6, 7, 10]. One approach involves transforming LOD datasets to rela-tional tables, and performing OLAP analysis using existing OLAP systems [10]. Alter-natively, one may directly perform OLAP-like analysis over LOD datasets [6]. Since there are many o ff -the-shelf systems that support OLAP analysis, the former approach has certain advantages.

When applying Extract, Transform, and Load (ETL) [8] processes to LOD datasets for subsequent OLAP analysis, there are three technical challenges: 1) designing the target star (or snowflake) schema consisting of fact and dimension tables, 2) extract-ing concept hierarchies that are associated w ith the dimension tables, and 3) determin-ing how best to deal with external LOD datasets that can be accessed through URL references.

In this paper we propose a general ETL framework for LOD datasets. The basic features are: 1) it can deal with any LOD datasets regardless of the usage of dedicated OLAP cube vocabularies, while other methods require such vocabularies like RDF Data Cube Vocabulary (QB) [5]; and 2) it can exploit inherent semantic hierarchies in the dataset, as well as those extracted from external LOD datasets such as GeoNames [14] and DBpedia [1]. In the case study using real LOD datasets, we show that the proposed framework can successfully extract multidimensional data model for OLAP dataset. 2.1 Storing RDF Data Given an LOD data, they are first stored in intermediate storage. The storage schema is designed in such a way that it is independent of the schemas used in the OLAP system. To this end, we basically use the Property Table (PT) approach [15, 16], where for each distinct type (rdf:type), a table is created with a number of attributes corresponding to RDF properties. Specifically, we take the following steps: Type-partitioned Triple Store (TPTS). After obtaining the RDF data represented as a collection of triples, we partition them according to the type (rdf:type) of the subject, resulting in a TPTS . More precisely, to maintain the di stinction between resource and literal, we annotate the data type informa tion in the object column in the TPTS, such as xsd:integer , xsd:dateTime , etc. Figure 1 shows an example of RDF data. By using TPTS, it can be represented using several tables as shown in Table 1.
 Property Table Generation. Having extracted a TPTS, we convert it to a set of Property Tables (PTs). Basically, we create a PT for each partition in the TPTS, because the subjects in the same partition are of the same type. The primary key of the table is the subject URI. For other attributes, we first enumerate the set of properties appearing in the partition by scanning the table, and use this information to create the set of attributes in the PT being generated. We carry out the same process for each partition, yielding the PTs in Table 2.
 Cardinality Estimation of Foreign Keys. In order to generate a star schema it is impor-tant to maintain the cardinality information for a foreign key, but this information is not explicitly provided. For this reason, we estimate the cardinality from the instance (in other words, from each record). For each fo reign key, we check th e occurrences in the source and referenced tables, and estimate the cardinality (1:1, 1:N, N:1, or M:N). This information is also maintained in the framework (Table 2(d)).
 2.2 Dimension Induction After storing the RDF data in PTs, the system shows the PTs and the respective at-tributes to the user. The user then selects one numerical attribute in a table, which is used as the measure in the fact table in the subsequent process.

From the specified fact table, the system identifies the possible dimensions (except for the measure) that can be found in the attributes of the fact table or those in tables referenced by foreign keys. In the fo llowing, three typical cases are discussed. Using Literals. In this case, an attribute in the fact table, or one in another table that can be reached by following a 1:1 or an N:1 reference, can be directly associated to the concept hierarchy. Timestamp and location are the most popular examples, i.e., the former can be associated with a hierarc hy containing day, week, month, and year, while the latter can be associated with one containing city, state, and country. Exploiting Inherent Hierarchy. In many cases, the RDF data being processed repre-sent hierarchical structures. In such cases, t he hierarchical structure is represented as a triple, such as rdfs:subClassOf , rdfs:subPropertyOf , etc. In our framework, such a structure is represented as a set of self r eferences in a PT. By following them, we can generate a dimension table corresponding to the concept hierarchy.
 Exploiting External Information. A piece of LOD data is likely to contain references to other LOD datasets. Some of the existing LOD datasets are quite popular and tend to collect many references. Examples include GeoNames [14] for location informa-tion and DBpedia [1] for encyclopedic information. In addition, there are various well-known ontologies, such as  X  X ime Ontology X  3 ,  X  X imeline Ontology X  4 ,and X  X G84 Geo Position Ontology X  5 . Such LOD datasets or ontologies are popular and are fre-quently referred to. For these, it is useful to e xtract the concept hierarchies beforehand and maintain them as the known dimension tables. 2.3 Schema Generation Now we are ready to generate the relational schema for the OLAP system. Basically, the system shows the list of possible dimension tables to the user, and he choose some of them to be used in the OLAP analysis. Having specified the list of dimension tables, we generate the concrete schema definition for the fact table and the selected dimen-sion tables. More precisely, we generate pro jected tables by omitting unused attributes, and merge such tables that are associated by 1:1 relationships for the sake of schema simplicity.

Regarding data loading, there are several possibilities. If the database storing the PTs is shared with the OLAP system, it is possibl e to create the fact and dimension tables as the view of PTs. If the database is not shared with the OLAP system, the data records must be dumped and subsequently loaded into the OLAP system. To show the applicability of the proposed sc heme, we have applied the proposed frame-work to two real LOD datasets.
 Datasets. The first dataset is the  X  X ational Radioactivity Stat as Linked Data X  6 .This dataset was created experimentally by MEXT (Ministry of Education, Culture, Sports, Science and Technology) of Japan by converting a non-LOD dataset 7 to LOD. It pro-vides the radioactivity monitoring results from March 16, 2011 to March 15, 2012 at 47 prefectures in Japan.

Another dataset is Linked Sensor Data [13], which is a dataset containing metadata from 9,757 weather observatories in the U.S. These metadata cover ID, coordinates described using WG84 Geo Position Ontology, location information consisting of links to GeoNames, and so on. Additionally, Linked Observation Data contains a dataset of hurricane-related observations linking to th e Linked Sensor Data. Each observation in the Linked Observation Data includes a numerical measurement, the type of observation (such as rainfall, wind speed, humidity, and so on), observation time, and observation site described using the Linked Sensor Data.
 Results. Figures 2(a) and 2(b) show the resulting schemas. By specifying the fact in each schema, we can successfully extract dime nsions, and generated the schema for OLAP system. Kampgen et al. [10] proposed an ETL process for Linked Data that used RDF Data Cube Vocabulary (QB) [5]. Other approaches apply analytical processing to RDF re-sources using RDF / OWL [11] ontologies. Niemi et al. [12] defined the OLAP Core ontology and proposed a method combining RDF, which is made up of distributed and heterogeneous sources, with OLAP. Using a di ff erent approach, Etcheverry et al. [6] de-fined OLAP operations for an RDF data model, and proposed an OpenCube Vocabulary in order to apply OLAP-like operations such as roll-up and slice with the aggregation function of SPARQL. In this paper, we proposed an ETL framework for the analytical processing of LOD. To do this we mapped resources and the relationships among them to relational tables and their reference relations. Once thi s type of mapping is done, we are able to extract concept hierarchies from the dataset, exploiting either literals, relations, or external information. We also demonstrated the prototype of our framework and presented two case studies using real LOD datasets to prove the framework X  X  utility. In the future, we plan to extend the proposed framework to deal with more general LOD data, and apply it to various datasets.
 Acknowledgements. This work was supported by JSPS Grant-in-Aid for Young Sci-entists (B) (#23700102).

