 Community Question and Answering services (cQAs) have become valuable resources for users to pose questions of their interests and share their knowledge by providing answers to questions. They perform much better than the traditional frequently asked questions (FAQ) systems (Jijkoun and Rijke , 2005; Riezler et al., 2007) which are just based on natural language processing and information re-trieving technologies due to the need for human in-telligence in user generated contents(Gyongyi et al., 2007). In cQAs such as Yahoo! Answers, a resolved question often gets more than one answers and a  X  X est answer X  will be chosen by the asker or voted by other community participants. This { question, best answer } pair is then stored and indexed for fur-ther uses such as question retrieval. It performs very well in simple factoid QA settings, where the an-swers to factoid questions often relate to a single named entity like a person, time or location. How-ever, when it comes to the more sophisticated multi-sentence questions, it would suffer from the problem of  X  X ncomplete answer X . That is, such question often comprises several sub questions in specific contexts and the asker wishes to get elaborated answers for as many aspects of the question as possible. In which case, the single best answer that covers just one or few aspects may not be a good choice (Liu et al., 2008; Takechi et al., 2007). Since  X  X veryone knows something X  (Adamic et al., 2008), the use of a single best answer often misses valuable human generated information contained in other answers.

In an early literature, Liu et al.(2008) reported that no more than 48% of the 400 best answers were in-deed the unique best answers in 4 most popular Ya-hoo! Answers categories. Table 1 shows an example of the  X  X ncomplete answer X  problem from Yahoo! Answers 1 . The asker wishes to know why his teeth bloods and how to prevent it. However, the best an-swer only gives information on the reason of teeth blooding. It is clear that some valuable information about the reasons of gums blooding and some solu-tions are presented in other answers.

In general, as noted in (Jurafsky and Martin , 2009), most interesting questions are not factoid questions. User X  X  needs require longer, more infor-mative answers than a single phrase. In fact, it is often the case, that a complex multi-sentence ques-tion could be answered from multiple aspects by dif-ferent people focusing on different sub questions. Therefore we address the incomplete answer prob-lem by developing a novel summarization technique taking different sub questions and contexts into con-sideration. Specifically we want to learn a concise summary from a set of corresponding answers as supplement or replacement to the  X  X est answer X .
We tackle the answer summary task as a sequen-tial labeling process under the general Conditional Random Fields (CRF) framework: every answer sentence in the question thread is labeled as a sum-mary sentence or non-summary sentence, and we concatenate the sentences with summary label to form the final summarized answer. The contribution of this paper is two-fold: First, we present a general CRF based framework and incorporate four different contextual factors based on question segmentation to model the local and non-local semantic sentence interactions to ad-dress the problem of redundancy and information novelty. Various textual and non-textual question answering features are exploited in the work. Second, we propose a group L 1 -regularization ap-proach in the CRF model for automatic optimal fea-ture learning to unleash the potential of the features and enhance the performance of answer summariza-tion.

We conduct experiments on a Yahoo! Answers dataset. The experimental results show that the proposed model improve the performance signifi-cantly(in terms of precision, recall and F1 measures) as well as the ROUGE-1, ROUGE-2 and ROUGE-L measures as compared to the state-of-the-art meth-ods, such as Support Vector Machines (SVM), Lo-gistic Regression (LR) and Linear CRF (LCRF) (Shen et al., 2007).

The rest of the paper is arranged as follows: Sec-tion 2 presents some definitions and a brief review of related research. In Section 3, we propose the summarization framework and then in Section 4 and 5 we detail the experimental setups and results re-spectively. We conclude the paper in Section 6. 2.1 Definitions In this subsection we define some concepts that would be helpful to clarify our problems. First we define a complex multi-sentence question as a ques-tion with the following properties:
Definition: A complex multi-sentence question is one that contains multiple sub-questions.
In the cQAs scenario a question often consists of one or more main question sentences accompany by some context sentences described by askers. We treat the original question and context as a whole single complex multi-sentence question and obtain the sub questions by question segmentation. We then define the incomplete answer problem as:
Definition: The incomplete answer problem is one where the best answer of a complex multi-sentence question is voted to be below certain star ratings or the average similarity between the best an-swer and all the sub questions is below some thresh-olds.

We study the issues of similarity threshold and the minimal number of stars empirically in the experi-mental section and show that they are useful in iden-tifying questions with the incomplete answer prob-lem. 2.2 Related Work There exist several attempts to alleviate the answer completeness problem in cQA. One of them is to segment the multi-sentence question into a set of sub-questions along with their contexts, then se-quentially retrieve the sub questions one by one, and return similar questions and their best answers (Wang et al., 2010). This strategy works well in gen-eral, however, as the automatic question segmenta-tion is imperfect and the matched similar questions are likely to be generated in different contextual sit-uations, this strategy often could not combine multi-ple independent best answers of sub questions seam-lessly and may introduce redundancy in final answer.
On general problem of cQA answer summariza-tion, Liu et al.(2008) manually classified both ques-tions and answers into different taxonomies and ap-plied clustering algorithms for answer summariza-tion.They utilized textual features for open and opin-ion type questions. Through exploiting metadata, Tomasoni and Huang(2010) introduced four char-acteristics (constraints) of summarized answer and combined them in an additional model as well as a multiplicative model. In order to leverage con-text, Yang et al.(2011) employed a dual wing fac-tor graph to mutually enhance the performance of social document summarization with user generated content like tweets. Wang et al. (2011) learned on-line discussion structures such as the replying rela-tionship by using the general CRFs and presented a detailed description of their feature designs for sites and edges embedded in discussion thread structures. However there is no previous work that explores the complex multi-sentence question segmentation and its contextual modeling for community answer sum-marization.

Some other works examined the evaluation of the quality of features for answers extracted from cQA services (Jeon et al., 2006; Hong and Davison , 2009; Shah et al., 2010). In the work of Shah et al.(2010), a large number of features extracted for predicting asker-rated quality of answers was evalu-ated by using a logistic regression model. However, to the best of our knowledge, there is no work in evaluating the quality of features for community an-swer summarization. In our work we model the fea-ture learning and evaluation problem as a group L 1 regularization problem (Schmidt , 2010) on different feature groups. 3.1 Conditional Random Fields We utilize the probabilistic graphical model to solve the answer summarization task, Figure 1 gives some illustrations, in which the sites correspond to the sentences and the edges are utilized to model the interactions between sentences. Specifically, let x be the sentence sequence to all answers within a question thread, and y be the corresponding label se-quence. Every component y i of y has a binary value, with +1 for the summary sentence and -1 otherwise. Then under CRF (Lafferty et al., 2001), the condi-tional probability of y given x obeys the following distribution: where Z ( x ) is the normalization constant called partition function, g l denotes the cQA feature func-tion of site l , f k denotes the function of edge k ( mod-eling the interactions between sentences),  X  and  X  are respectively the weights of function of sites and edges, and y | t denotes the components of y related to site (edge) t . 3.2 cQA Features and Contextual Modeling In this section, we give a detailed description of the different sentence-level cQA features and the contextual modeling between sentences used in our model for answer summarization.
 Sentence-level Features
Different from the conventional multi-document summarization in which only the textual features are utilized, we also explore a number of non-textual author related features (Shah et al., 2010) in cQAs. The textual features used are : The non-textual features used include :
The previous literature (Shah et al., 2010) hinted that some cQA features, such as Sentence Length , Has Link and Best Answer Star , may be more im-portant than others. We also expect that some fea-ture may be redundant when their most related fea-tures are given, e.g., the Author Level feature is pos-itively related with the Total Points received by an-swerers, and Stopwords Rate is of little help when both Sentence Length (not including stop words) and U ppercase Rate are given. Therefore, to explore the optimal combination of these features, we propose a group L 1 regularization term in the general CRF model (Section 3.3) for feature learning.

All features presented here can be extracted au-tomatically from the Yahoo! Answers website. We normalize all these feature values to real numbers between 0 and 1 by dividing them by the corre-sponding maximal value of these features. These sentence-level features can be easily utilized in the CRF framework. For instance, if the rate of upper-case words is prominent or the position is close to the beginning or end of the answer, then the proba-bility of the label +1 (summary sentence) should be boosted by assigning it with a large value. Contextual Modeling Under Question Segmentation
For cQAs summarization, the semantic interac-tions between different sentence sites are crucial, that is, some context co-occurrences should be en-couraged and others should be penalized for require-ments of information novelty and non-redundancy in the generated summary. Here we consider both local (sentences from the same answer) and global (sentences from different answers) settings. This give rise to four contextual factors that we will ex-plore for modeling the pairwise semantic interac-tions based on question segmentation. In this paper, we utilize a simple but effective lightweight ques-tion segmentation method (Ding et al., 2008; Wang et al., 2010). It mainly involves the following two steps:
Step 1 . Question sentence detection: every sen-tence in the original multi-sentence question is clas-sified into question sentence and non-question (con-text) sentence . The question mark and 5W1H fea-tures are applied.

Step 2 . Context assignment: every context sen-tence is assigned to the most relevant question sen-tence. We compute the semantic similarity(Simpson and Crowe, 2005) between sentences or sub ques-tions as: sim ( x, y ) = 2  X  where M ( x, y ) denotes synset pairs matched in sen-tences x and y ; and the similarity between the two synsets w 1 and w 2 is computed to be inversely pro-portional to the length of the path in Wordnet.
One answer sentence may related to more than one sub questions to some extent. Thus, we de-fine the replied question Qr i as the sub question with the maximal similarity to sentence x i : Qr i = argmax Q summary sentences aim at answering different sub questions. Therefore, we design the following two contextual factors based on the similarity of replied questions.
 Dissimilar Replied Question Factor : Given two answer sentences x i , x j and their corresponding replied questions Qr i , Qr j . If the similarity 2 of Qr and Qr j is below some threshold  X  lq , it means that x and x j will present different viewpoints to answer different sub questions. In this case, it is likely that x i and x j are both summary sentences; we ensure this by setting the contextual factor cf 1 with a large value of exp  X  , where  X  is a positive real constant often assigned to value 1; otherwise we set cf 1 to exp  X   X  for penalization. Similar Replied Question Factor : Given two an-swer sentences x i , x j and their corresponding replied questions Qr i , Qr j . If the similarity of Qr i and Qr j is above some upper threshold  X  uq , this means that x i and x j are very similar and likely to provide similar viewpoint to answer similar ques-tions. In this case, we want to select either x i or x j as answer. This is done by setting the contextual factor cf 2 such that x i and x j have opposite labels,
Assuming that sentence x i is selected as a sum-mary sentence, and its next local neighborhood sen-tence x i +1 by the same author is dissimilar to it but it is relevant to the original multi-sentence question, then it is reasonable to also pick x i +1 as a summary sentence because it may offer new viewpoints by the author. Meanwhile, other local and non-local sentences which are similar to it at above the up-per threshold will probably not be selected as sum-mary sentences as they offer similar viewpoint as discussed above. Therefore, we propose the follow-ing two kinds of contextual factors for selecting the answer sentences in the CRF model.
 Local Novelty Factor : If the similarity of answer sentence x i and x i +1 given by the same author is below a lower threshold  X  ls , but their respective sim-ilarities to the sub questions both exceed an upper threshold  X  us , then we will boost the probability of selecting both as summary sentences by setting: Redundance Factor : If the similarity of answer sentence x i and x j is greater than the upper thresh-old  X  us , then they are likely to be redundant and hence should be given opposite labels. This is done by setting:
Figure 1 gives an illustration of these four con-textual factors in our proposed general CRF based model. The parameter estimation and model infer-ence are discussed in the following subsection. 3.3 Group L 1 Regularization for Feature In the context of cQA summarization task, some fea-tures are intuitively to be more important than oth-ers. As a result, we group the parameters in our CRF model with their related features 3 and introduce a group L 1 -regularization term for selecting the most useful features from the least important ones, where the regularization term becomes, where C controls the penalty magnitude of the pa-rameters, G is the number of feature groups and denotes the parameters corresponding to the partic-ular group g . Notice that this penalty term is indeed a L (1 , 2) regularization because in every particu-lar group we normalize the parameters in L 2 norm while the weight of a whole group is summed in L 1 form.

Given a set of training data D = ( x ( i ) , y ( i ) ) , i = 1 , ..., N , the parameters  X  = (  X  l ,  X  k ) of the general CRF with the group L 1 -regularization are estimated in using a maximum log likelihood function L as: where N denotes the total number of training sam-ples. we compute the log-likelihood gradient com-ponent of  X  in the first term of Equation 4 as in usual CRFs. However, the second term of Equation 4 is non-differentiable when some special  X  comes exactly zero. To tackle this problem, an ad-ditional variable is added for each group (Schmidt , 2010); that is, by replacing each norm  X  the variable  X  g , subject to the constraint  X  g  X   X   X  X  X   X  This formulation transforms the non-differentiable regularizer to a simple linear function and maximiz-ing Equation 5 will lead to a solution to Equation 4 because it is a lower bound of the latter. Then, we add a sufficient small positive constant  X  when com-puting the L 2 norm (Lee et al., 2006), i.e., |  X  features in group g . To obtain the optimal value of parameter  X  from the training data, we use an effi-cient L-BFGS solver to solve the problem, and the first derivative of every feature j in group g is, where C gj ( y, x ) denotes the count of feature j in group g of observation-label pair ( x, y ) . The first two terms of Equation 6 measure the difference be-tween the empirical and the model expected values of feature j in group g , while the third term is the derivative of group L 1 priors.

For inference, the labeling sequence can be ob-tained by maximizing the probability of y condi-tioned on x , We use a modification of the Viterbi algorithm to perform inference of the CRF with non-local edges previously used in (Galley , 2006). That is , we replace the edge connection z t = ( y t  X  2 , y t  X  1 , y of order-2 Markov model by z t = ( y N t , y t  X  1 , y t ) , where y N t represents the label at the source of the non-local edge. Although it is an approximation of the exact inference, we will see that it works well for our answer summarization task in the experiments. 4.1 Dataset To evaluate the performance of our CRF based an-swer summarization model, we conduct experiments on the Yahoo! Answers archives dataset. The Ya-hoo! W ebscope T M Program 4 opens up a number of Yahoo! Answers datasets for interested academics in different categories. Our original dataset con-tains 1,300,559 questions and 2,770,896 answers in ten taxonomies from Yahoo! Answers. After fil-tering the questions which have less than 5 answers and some trivial factoid questions using the features by (Tomasoni and Huang, 2010) , we reduce the dataset to 55,132 questions. From this sub-set, we next select the questions with incomplete answers as defined in Section 2.1. Specifically, we select the questions where the average similarity between the best answer and all sub questions is less than 0.6 or when the star rating of the best answer is less than 4. We obtain 7,784 questions after this step. To eval-uate the effectiveness of this method, we randomly choose 400 questions in the filtered dataset and in-vite 10 graduate candidate students (not in NLP re-search field) to verify whether a question suffers from the incomplete answer problem. We divide the students into five groups of two each. We consider the questions as the  X  X ncomplete answer questions X  only when they are judged by both members in a group to be the case. As a result, we find that 360 (90%) of these questions indeed suffer from the in-complete answer problem, which indicates that our automatic detection method is efficient. This ran-domly selected 400 questions along with their 2559 answers are then further manually summarized for evaluation of automatically generated answer sum-maries by our model in experiments. 4.2 Evaluation Measures When taking the summarization as a sequential bi-classification problem, we can make use of the usual precision, recall and F1 measures (Shen et al., 2007) for classification accuracy evaluation.

In our experiments, we also compare the preci-sion, recall and F1 score in the ROUGE-1, ROUGE-2 and ROUGE-L measures (Lin , 2004) for answer summarization performance. 5.1 Summarization Results We adapt the Support Vector Machine (SVM) and Logistic Regression (LR) which have been reported to be effective for classification and the Linear CRF (LCRF) which is used to summarize ordinary text documents in (Shen et al., 2007) as baselines for comparison. To better illustrate the effectiveness of question segmentation based contextual factors and the group L 1 regularization term, we carry the tests in the following sequence: (a) we use only the con-textual factors cf 3 and cf 4 with default L 2 regular-ization (gCRF); (b) we add the reply question based factors cf 1 and cf 2 to the model (gCRF-QS); and (c) we replace default L 2 regularization with our pro-posed group L 1 regularization term (gCRF-QS-l1). For linear CRF system, we use all our textual and non-textual features as well as the local (exact pre-vious and next) neighborhood contextual factors in-stead of the features of (Shen et al., 2007) for fair-ness. For the thresholds used in the contextual fac-tors, we enforce  X  lq to be equal to  X  ls and  X  uq equal to  X  us for the purpose of simplifying the parameters setting (  X  lq =  X  ls = 0 . 4 ,  X  uq =  X  us = 0 . 8 in our ex-periments). We randomly divide the dataset into ten subsets (every subset with 40 questions and the as-sociated answers), and conduct a ten-fold cross val-idation and for each round where the nine subsets are used to train the model and the remaining one for testing. The precision, recall and F1 measures of these models are presented in Table 2.

Table 2 shows that our general CRF model based on question segmentation with group L 1 regulariza-tion out-performs the baselines significantly in all three measures (gCRF-QS-l1 is 13.99% better than SVM in precision, 9.77% better in recall and 11.72% better in F1 score). We note that both SVM and LR, gCRF-QS-l1 79.92%  X  71.73%-75.60%  X  which just utilize the independent sentence-level features, behave not vary well here, and there is no statistically significant performance difference be-tween them. We also find that LCRF which utilizes the local context information between sentences per-form better than the LR method in precision and F1 with statistical significance. While we consider the general local and non-local contextual factor cf 3 and cf 4 for novelty and non-redundancy constraints, the gCRF performs much better than LCRF in all three measures; and we obtain further performance im-provement by adding the contextual factors based on QS, especially in the recall measurement. This is mainly because we have divided the question into several sub questions, and the system is able to se-lect more novel sentences than just treating the origi-nal multi-sentence as a whole. In addition, when we replace the default L 2 regularization by the group L 1 regularization for more efficient feature weight learning, we obtain a much better performance in precision while not sacrificing the recall measure-ment statistically.

We also compute the Precision, Recall and F1 in ROUGE-1, ROUGE-2 and ROUGE-L measure-ments, which are widely used to measure the quality of automatic text summarization. The experimental results are listed in Table 3. All results in the Ta-ble are the average of the ten-fold cross validation experiments on our dataset.

It is observed that our gCRF-QS-l1 model im-proves the performance in terms of precision, recall and F1 score on all three measurements of ROUGE-1, ROUGE-2 and ROUGE-L by a significant mar-gin compared to other baselines due to the use of local and non-local contextual factors and factors based on QS with group L 1 regularization. Since the ROUGE measures care more about the recall and precision of N-grams as well as common substrings to the reference summary rather than the whole sen-tence, they offer a better measurement in modeling the user X  X  information needs. Therefore, the im-provements in these measures are more encouraging than those of the average classification accuracy for answer summarization.

From the viewpoint of ROUGE measures we ob-serve that our question segmentation method can en-hance the recall of the summaries significantly due to the more fine-grained modeling of sub questions. We also find that the precision of the group L 1 reg-ularization is much better than that of the default L 2 regularization while not hurting the recall signifi-cantly. In general, the experimental results show that our proposed method is more effective than other baselines in answer summarization for addressing the incomplete answer problem in cQAs. 5.2 Evaluation of Feature Learning For group L 1 regularization term, we set the  X  = 10  X  4 in Equation 6. To see how much the dif-ferent textual and non-textual features contribute to community answer summarization, the accumulated weight of each group of sentence-level features 5 is presented in Figure 2. It shows that the textual fea-tures such as 1 ( Sentence Length ), 2 ( Position ) 3 ( An-swer Length ), 6 ( Has Link ) and non-textual features such as 8 ( Best Answer Star ) , 12 ( Total Answer Number ) as well as 13 ( Total Points ) have larger weights, which play a significant role in the sum-marization task as we intuitively considered; fea-tures 4 ( Stopwords Rate ), 5 ( Uppercase Rate ) and 9 ( Thumbs Up ) have medium weights relatively; and the other features like 7 ( Similarity to Question ), 10 ( Author Level ) and 11 ( Best Answer Rate ) have the smallest accumulated weights. The main reasons that the feature 7 (Similarity to Question) has low contribution is that we have utilized the similarity to question in the contextual factors, and this simi-larity feature in the single site becomes redundant. Similarly, the features Author Level and Best An-swer Number are likely to be redundant when other non-textual features( Total Answer Number and To-tal Points ) are presented together. The experimental results demonstrate that with the use of group L 1 -regularization we have learnt better combination of these features. 5.3 An Example of Summarized Answer To demonstrate the effectiveness of our proposed method, Table 4 shows the generated summary of the example question which is previously illustrated in Table 1 in the introduction section. The best an-swer available in the system and the summarized an-swer generated by our model are compared in Table 4. It is found that the summarized answer contains more valuable information about the original multi-sentence question, as it better answers the reason of teeth blooding and offers some solution for it. Stor-ing and indexing this summarized answer in ques-tion archives should provide a better choice for an-swer reuse in question retrieval of cQAs.
 We proposed a general CRF based community an-swer summarization method to deal with the in-complete answer problem for deep understanding of complex multi-sentence questions. Our main con-tributions are that we proposed a systematic way for modeling semantic contextual interactions be-tween the answer sentences based on question seg-mentation and we explored both the textual and non-textual answer features learned via a group L 1 reg-ularization. We showed that our method is able to achieve significant improvements in performance of answer summarization compared to other baselines and previous methods on Yahoo! Answers dataset. We planed to extend our proposed model with more advanced feature learning as well as enriching our summarized answer with more available Web re-sources.
 This work was supported by the NSFC under Grant No.61073002 and No.60773077.

