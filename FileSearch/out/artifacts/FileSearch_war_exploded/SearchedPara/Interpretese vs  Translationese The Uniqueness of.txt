 Although simultaneous interpretation has a key role explored within machine translation ( MT ). One key challenge is to achieve a good quality/speed trade-off: deciding when, what, and how to translate. In this study, we take a data-driven, comparative ap-proach and examine: (i) What distinguishes simul-translated text (Translationese)? (ii) What strategies do human interpreters use?
Most previous work focuses on qualitative analy-sis (Bendazzoli and Sandrelli, 2005; Camayd-Freixas, 2011; Shimizu et al., 2014) or pattern counting (To-hyama and Matsubara, 2006; Sridhar et al., 2013). In contrast, we use a more systematic approach based on feature selection and statistical tests. In addition, most work ignores translated text, making it hard to isolate strategies applied by interpreters as opposed to general strategies needed for any translation. Shimizu et al. (2014) are the first to take a comparative ap-proach; however, they directly train MT systems on the interpretation corpus without explicitly examin-ing interpretation tactics. While some techniques can be learned implicitly, the model may also learn unde-sirable behavior such as omission and simplification: byproducts of limited human working memory (Sec-tion 4).
 Prior work studies simultaneous interpretation of Japanese  X  English (Tohyama and Matsubara, 2006; Shimizu et al., 2014) and Spanish  X  English (Sridhar et al., 2013). We focus on Japanese  X  English inter-pretation. Since information required by the target En-glish sentence often comes late in the source Japanese sentence (e.g., the verb, the noun being modified), tributions are three-fold. First, we collect new human translations for an existing simultaneous interpreta-tion corpus, which can benefit future comparative selection methods to examine linguistic characteris-tics comparatively. Third, we categorize human inter-pretation strategies, including word reordering tactics and summarization tactics. Our results help linguists understand simultaneous interpretation and help com-puter scientists build better automatic interpretation systems. In this section, we discuss strategies used in Inter-pretese, which we detect automatically in the next section. Our hypothesis is that tactics used by inter-preters roughly fall in two non-exclusive categories: (i) delay minimization , to enable prompt translation by arranging target words in an order similar to the source; (ii) memory footprint minimization , to avoid overloading working memory by reducing communi-cated information.
 Segmentation Interpreters often break source sen-tences into multiple smaller sentences (Camayd-Freixas, 2011; Shimizu et al., 2013), a process we call segmentation. This is different from what is com-monly used in speech translation systems (Fujita et al., 2013; Oda et al., 2014), where translations of segments are directly concatenated. Instead, humans try to incorporate new information into the precedent partial translation, e.g., using  X  X hich is X  to put it in a clause (Table 1, Example 3), or creating a new sen-tence joined by conjunctions (Table 1, Example 5). Passivization Passivization is useful for inter-preting from head-final languages (e.g., Japanese, German) to head-initial languages (e.g., English, French) (He et al., 2015). Because the verb is needed early in the target sentence but only appears at the end of the source sentence, an obvious strategy is to wait for the final verb. However, if the interpreter uses passive voice, they can start translating immediately and append the verb at the end (Table 1, Examples 4 X  5). During passivization, the subject is often omitted when obvious from context.
 Generalization Camayd-Freixas (2011) and Al-Khanji et al. (2000) observe that interpreters focus on delivering the gist of a sentence rather than du-plicating the nuanced meaning of each word. More frequent words are chosen as their retrieval time is faster (Dell and O X  X eaghdha, 1992; Cuetos et al., 2006) (e.g.,  X  X onorific X  versus  X  X olite X  in Table 1, Example 1). Although Volansky et al. (2013) show that generalization happens in translation too, it is likely more frequent in Interpretese given the severe time constraints.
 Summarization Faced with overwhelming infor-mation, interpreters need efficient ways to encode meaning. Less important words, or even a whole sen-tence can drop, especially when the interpreter falls behind the speaker. In Table 1, Example 2, the lit-eral translation  X  X s much as possible X  is reduced to  X  X ery X , and the adjective  X  X apanese X  is omitted.
Before we study these characteristics quantita-tively in the next section, we visualize Interpretese and Translationese by a word cloud in Figure 1. The size of each word is proportional to the dif-ference between its frequencies in Interpretese and Translationese (Section 3). The word color indicates whether it is more frequent in Interpretese (black) or Translationese (gold).  X  X he X  is over-represented in Interpretese, a phenomenon also occurs in Transla-tionese vs. the original text (Eetemadi and Toutanova, 2014). More conjunction words (e.g.,  X  X nd X ,  X  X o X ,  X  X r X ,  X  X hen X ) are used in Interpretese, likely for segmentation, whereas  X  X hat X  is more frequent in Translationese X  X  sign of clauses. In addition, the pronoun  X  X  X  occurs more often in Translationese while  X  X e X  and  X  X s X  occur more often in Interpretese, which is consistent with our passivization hypothe-sis. We investigate the difference between Translationese and Interpretese by creating a text classifier to dis-tinguish between them and then examining the most useful features. We train our classifier on a bilin-gual Japanese-English corpus of spoken monologues and their simultaneous interpretations (Matsubara et al., 2002). To obtain a three-way parallel corpus of aligned translation, interpretation, and their shared source text, we first align the interpreted sentences to source sentences by dynamic programming fol-of text chunks, with 33 tokens per chunk on average. each source text chunk (one translator per mono-logue). The original corpus has four interpretors per monologue. We use all available interpretation by copying the translation of a text chunk for its addi-tional interpretation. 3.1 Discriminative Features We use logistic regression as our classifier. Its job is to tell, given a chunk of English text, which translation produced it. We add ` 1 regularization to select the non-zero features that best distinguish Interpretese from Translationese. We experiment with three dif-ferent sets of features: (1) POS: n -gram features of POS tags (up to trigram); 7 (2) LEX: word unigrams; (3) LING: features reflecting linguistic hypothese (Section 2), most of which are counts of indicator functions normalized by length of the chunk (Ap-pendix A).

The top linguistic features listed in Table 3 are consistent with our hypotheses. The most promi-nent ones X  X lso revealed by POS and LEX  X  X re the segmentation features, including counts of conjunc-tion words ( CC ), content words (nouns, verbs, ad-jectives, and adverbs) that appear more than once ( repeated ), demonstratives ( demo ) such as this, that, these, those , segmented sentences ( sent ), and proper nouns ( NNP ). More conjunction words and more sentences in a text chunk are signs of segmenta-tion. Repeated words and the frequent use of demon-stratives come from transforming clauses to indepen-dent sentences. Next are the passivization features, in-dicating more passivized verbs ( passive ) and fewer pronouns ( pronoun ) in Interpretese. The lack of pro-nouns may be results of either subject omission dur-ing passivization or general omission. The last group are the vocabulary features, showing fewer numbers of stem types, token types, and content words in Inter-pretese, evidence of word generalization. In addition, a smaller number of content words suggests that inter-preters may use more function words to manipulate the sentence structure. 3.2 Classification Results Recall that our goal is to understand Interpretese, not to classify Interpretese and Translationese; how-ever, the ten-fold cross validation accuracy of LING , POS , LEX are 0.66, 0.85, and 0.94. LEX and POS yield high accuracy as some features are overfitting, e.g., in this dataset, most interpreters used  X  X arsing X  for  X   X   X   X   X   X  while the translator used  X  X yntactic analysis X . Therefore, they do not reveal much about the characteristics of Interpretese except for frequent use of  X  X nd X  and CC , which indicates segmentation. Similarly, Volansky et al. (2013) and Eetemadi and Toutanova (2014) also find lexical features very effec-tive but not generalizable for detecting Translationese and exclude them from analysis. One reason for the relatively low accuracy of LING may be inconsistent use of strategies among humans (Section 4). To better understand under what situations these tac-tics are used, we apply two-sample t -tests to com-pare the following quantities between Interpretese and Translationese: (1) number of inversions (non-monotonic translations) on all source tokens (inv-all), verbs (inv-verb) and nouns (inv-noun); (2) number of segmented sentences; (3) number of natural passiviza-tion (pass-st), meaning copying a passive construc-tion in the source sentence into the target sentence, and intentional passivization (pass-t), meaning intro-ducing passivization into the target sentence when the source sentence is in active voice; (4) number of omitted words on the source side and inserted words by Microsoft Web n -gram X  X igher means more com-mon. 9 For all pairs of samples, the null hypothesis H 0 is that the means on Interpretese and Translationese are equal; the alternative hypotheses and results are in Table 2.

As expected, segmentation and intentional pas-sivization happen more often during interpretation. Interpretese has fewer inversions, especially for verbs; reducing word order difference is important for delay minimization. Since there are two to four different interpretations for each lecture, we further analyze how consistent humans are on these deci-sions. All interpreters agree on segmentation 73.7% of the time, while the agreement on passivization is only 57.1% X  X assivization is an acquired skill; not all interpreters use it when it can speed interpretation.
The tests also confirm our hypotheses on gener-alization and omission. However, these tactics are not inherent to the task of simultaneous interpreta-tion. Instead, they are a byproduct of humans X  limited working memory. Computers can load much larger resources into memory and weigh quality of different translations in an instant, thus potentially rendering the speaker X  X  message more accurately. Therefore, directly learning from corpus of human interpreta-tion may lead to suboptimal results (Shimizu et al., 2014). While we describe how Translationese and Inter-pretese are different and characterize how they differ, the contribution of our work is not just examining an interesting, important dialect. Our work provides op-portunities to improve conventional simultaneous MT systems by exploiting and modeling human tactics. He et al. (2015) use hand-crafted rules to decrease latency; our data-driven approach could yield addi-tional strategies for improving MT systems. Another strategy X  X iven the scarcity and artifacts of interpre-tation corpus X  X s to select references that present delay-minimizing features of Interpretese from trans-lation corpus (Axelrod et al., 2011). Another future direction is to investigate cognitive inference (Cher-nov, 2004), which is useful for semantic/syntactic prediction during interpretation (Grissom II et al., 2014; Oda et al., 2015).
 We use the Berkeley aligner (Liang et al., 2006) for word alignment, the Stanford POS tagger (Toutanova et al., 2003) to tag English sentences, and Kuro-tences. Below we describe the features in detail. Inversion: Let { A i } be the set of indexes of tar-get words to which each source word w i is aligned. We count A i and A j ( i &lt; j ) as an inverted pair if max( A i ) &gt; min( A j ) . This means that we have to wait until the j th word to translate the i th word. Segmentation: We use the punkt sentence seg-menter (Kiss and Strunk, 2006) from NLTK to detect sentences in a text chunk.
 Passivization: We compute the number of passive verbs normalized by the total number of verbs. We detect passive voice in English by matching the fol-lowing regular expression: a be verb (be, are, is, was, were etc.) followed by zero to four non-verb words and one verb in its past participle form. We detect pas-sive voice in Japanese by checking that the dictionary form of a verb has the suffix  X   X  X  X   X .
 Vocabulary To measure variety, we use V t /N and V /N , where V t and V s are counts of distinct tokens and stems, and N is the total number of tokens. To measure complexity, we use word length, number of syllables per word, approximated by vowel se-quences; and unigram and bigram frequency from Microsoft Web N -gram.
 Summarization We use the sentence compression ra-tio, sentence length, number of omitted source words, approximated by counts of unaligned words, and number of content words.
 We thank CIAIR (Nagoya University, Japan) for pro-viding the interpretation data which formed the foun-dation of this research. We also thank Alvin Gris-som II, Naho Orita and the reviewers for their insight-ful comments. This work was supported by NSF grant IIS -1320538. Boyd-Graber is also partially supported by NSF grants CCF -1409287 and NCSE -1422492. Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the view of the sponsor.
