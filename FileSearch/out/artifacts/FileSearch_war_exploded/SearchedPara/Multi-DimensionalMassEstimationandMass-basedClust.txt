
Mass estimation [12] was introduced recently as a base modelling mechanism in data mining. It is as fundamental as density estimation which has been the bedrock for most data modelling methods for a wide range of tasks such as classification, clustering, and anomaly detection. Mass estimation possesses the follo wing properties [12]:  X (i) a mass distrib ution stipulates an ordering from core points to fringe points in a data cloud; (ii) this ordering accentuates the fringe points with a conca ve function X  X he essential property that is easily exploited by existing algorithms to impro ve their task-specific performance; and (iii) it is a constant-time-and-space-comple xities estimation method.  X 
Ting et al [12] sho w that models using mass estimation perform at least as good as and often better than a total of eight state-of-the-art methods in terms of task-specific performance measures in three tasks: information retrie val, regression and anomaly detection.

Notwithstanding its successful applications, the major limitation of existing mass estimation is that it is designed for one-dimensional problems only .

Our first contrib ution in this paper is to extend the abo ve-mentioned work to a new version of mass estimation which allo ws it to  X  remo ve the current limitation of one-dimensional mass estimation to enable multi-dimensional mass estimation;  X  reduce the time comple xity to O (  X  X  ) from O (  X  h ) , making it feasible for very high level mass estimation, where  X  is the sampling size, and h is the level.
These enhancements boost its applicability to a full range of generic problems, unconstrained by one-dimensional ap-plications and low level mass estimation.

While maintaining the three properties mentioned abo ve, the new proposed mass estimation is capable of modelling arbitrary shapes in multi-dimensional data X  X he key weak-ness of one-dimensional mass estimation.

In our second contrib ution, we sho w that the mass model produced for mass estimation can be emplo yed to perform of-the-art methods. The first mass-based clustering method we introduce is distinguished from the existing density-based or distance-based clustering methods by  X  Making no density or distance calculation which is the major expense in any density-based or distance-based methods;  X  Exploiting the structure pro vided by the mass model to identify clusters without expensi ve evaluations. This significantly speeds up the clustering process, leading to a sublinear time comple xity algorithm in average case.
In the next section, we reiterate the original one-dimensional mass estimation, as presented by [12]. Section III introduces our proposed multi-dimensional mass esti-mation. Section IV describes a special tree structure de-signed for the mass estimation, and Section V demonstrates the modelling capability of the proposed mass estimation method. We introduce the first mass-based clustering method in Section VI and present the evaluation result in the next section. The last two sections give further discussion, conclusions and possible future work. The key symbols and notations used are given in Table I.

Ting et al [12] introduce one-dimensional mass estimation mass ( x; h ) , where the level h  X  1 . h = 1 mass estimation guarantees conca vity , independent of the underlying density distrib ution; and h &gt; 1 is used to model multi-modal mass distrib ution. It is defined w.r.t. D = f x where x mass ( x; h ) =
The mass base function m split at s L and R denote the left and right sides of the split, respecti vely; and p ( s the probability of the binary split.

A high level mass estimation is computed recursi vely by using the mass estimations obtained at lower levels. A binary split at s level-( h -1) mass estimations: (a) mass L estimation for x  X  s and (b) mass R which is defined using f x
Further , mass ( x; h ) can be approximated using sample subsets. The approximate mass estimation mass ( x; h ) for a point x 2 R , is defined w.r.t. D = f x is a random subset of D , and  X   X j D j , h &lt;  X  .
Ting et al [12] sho w one way that this one-dimensional mass estimation can be applied to multi-dimensional prob-lems by conducting an one-dimensional mapping of the original R d space to new mass space R t , where d  X  t ; x = [ x 1 ; : : : ; x d ] , x 2 D ; z = [ z 1 ; : : : ; z t ] ; is done by randomly selecting a subset D  X  D and a mass ( x q ; h jD ) . This is repeated t times; and the t models can then be used to map every x 2 D to z 2 D 0 in the new mass space.

Ting et al [12] sho w that four existing algorithms perform better in the mass space than in the original space in terms of task-specific measures in two tasks: regression and information retrie val.

Despite this successful application, it is recognised by [12] that a multi-dimensional mass mapping, rather than the one-dimensional mass mapping, can further widen its applica-tions to a full range of generic problems. We introduce one way to achie ve this aim in the next three sections.
Here we propose a way to generalise the one-dimensional mass estimation that eliminates the need to compute the probability of binary split, p ( s randomised version of the abo ve equations. It requires two functions. First, a function that generates dif ferent (random) subspaces covering each point in the feature space. This generalises the binary split into half-space splits or 2 h splits when h levels of half-space splits are used. Second, a generalised version of mass base function is used to define mass in a subspace. The formal definition follo ws.
Let x be an instance in R d . Let T ( x ) be one of the two half-spaces in which x falls into; and m the number of training instances in the half-space.

Generalised mass base function: m ( T ( x )) is defined as
In one-dimensional problems, let T half-spaces in which x falls into; and T h -spaces in which x falls into.

Equations (1) and (2) can now be approximated as fol-lows:
Here every T probability . Note that p ( s assumption.

The new mass estimation for multi-dimensional problems is the same as Equation (5) by simply replacing x with x : Lik e its one-dimensional counterpart, the multi-dimensional mass estimation (i) stipulates an ordering from core points Figure 1: (a) An example of the new mass estimation for h = 1 ; 2 ; 3 in comparison with Gaussian kernel density estimation. (b) The result of two equal-size splits T h two splits start with dif ferent working spaces X  X he two outer rectangles of solid and dotted box es. The shaded cells are the subspaces in which x falls into in T h (ha ving high mass) to fringe points (ha ving low mass) in a data cloud, regardless of its density distrib ution, including uniform density distrib ution; (ii) this ordering accentuates the fringe points with a conca ve function. An example of the new mass estimation is sho wn in Figure 1(a) for a uniform density distrib ution and a Gaussian density distrib ution.
T h ( x jD ) can be implemented using a tree structure, where a working space is partitioned into 2 ` equal-size subspaces at the lea ves of the tree with height ` = h  X  d where d is the number of dimensions. Let m j be the mass of subspace j ; and there is a total of 2 ` subspaces which have a total mass: jDj = P 2 ` and x is in subspace j of T h .

We implement T h ( x jD ) as an h : d -Tree , where each path from the root to a leaf has h  X  d nodes within which each of the d attrib utes appears exactly h times. For examples: (a) h : d = 1:2 generates a tree of height = 2 in a two-dimensional domain where each attrib ute is used exactly once on every path from the root to a leaf. (b) h : d = 3:4 generates a tree of height = 12 in a four -dimensional domain where each attrib ute is used three times on every path from the root to a leaf.

The procedure to generate an ensemble of h : d -Trees is given in Algorithm 1. Each h : d -Tree , an implementation of
T h ( x jD ) , is generated after the follo wing two randomi-sation processes: (i) Each D of size  X  is a random sample of D . The sampling is conducted  X  X trictly X  without replacement, i.e., x 6 = y ; where x 2D i ; y 2D j ; 8 i; j . This is done in line# 4 of Algorithm 1. The sampling process is restarted with D when the data run out. (ii) Each working space , which covers D , is initialised from a random perturbation as follo ws. For each attrib ute q , a split value ( v Algorithm 1 : BuildT rees ( D; t;  X ; h ) Inputs : D -input data, t -number of trees,  X  -sub-sampling size, h -number of times an attrib ute is emplo yed in a path. Output : F -a set of t h : d -Trees 1: M axH eightLimit  X  h  X  d 2: Initialize F 3: for i = 1 to t do 4: D  X  sampl e ( D;  X  ) f strictly without replacement g 5: ( min , max )  X  InitialiseW orkingSpace( D ) 6: F  X  F [ SingleT ree ( D ; min; max; 0) 7: end for [ min q ( D ) , max q ( D ) ], i.e., the minimum and maximum val-ues of q in D . Then, attrib ute q of the working space is defined having the range [ min where r = max ( v all d dimensions define the working space used to generate an h : d -Tree . This is done in line# 5 of Algorithm 1.
Examples of the splits T h h : d = 2:2, created from two dif ferent working spaces, are sho wn in Figure 1(b). The mass for each x , estimated using Equation (6), is deri ved from t subspaces in which x falls into (see shaded cells in Figure 1(b).) Note that m ( T h ( x jD )) = 0 if x falls outside the working space.
The tree building process is deterministic, sho wn in Algo-an attrib ute in a round-robin manner from the attrib ute set the node is constructed by splitting the working space into two equal-v olume half-spaces (line# 5-7). If the two half-spaces are non-empty , the process is then repeated recur -sively on each half-space (line# 17-20). The tree gro wing process stops when the height level limit is met (line# 2). If one of the two half-spaces is empty (therefore resulting a single-branch node), the ranges in the node defined in min and max is constrained accordingly without actually creating the single-branch node (line# 8-15.) This is to avoid creating unnecessary single-branch nodes X  X o reduce memory requirement.

Mass estimation using h : d -Trees . Once an ensemble of h : d -Trees is built, it is ready for mass estimation. For each instance x , the number of instances of subspace j (of an h : d -Tree ) in which x falls into is returned as mass:
The multi-dimensional mass estimation can now be ex-pressed as
Parameter discussion . We set t = 1000 to get a lar ge ensemble. Then, only two other parameters need to be set: h and  X  . A guideline is given belo w.
 Algorithm 2 : SingleT ree ( D ; min; max; ` ) Inputs : D -input data, min &amp; max -arrays of minimum and maximum values for each attrib ute in A that defines a Output : an h : d -Tree 1: while (true) do 2: if ( ` &lt; M axH eightLimit ) then 3: f Retrie ve an attrib ute from A based on height level. 4: q  X  nextAttr ibute ( A; ` ) 5: p  X  ( max q + min q ) = 2 6: D l  X  f ilter ( D ; q &lt; p ) 7: D r  X  f ilter ( D ; q  X  p ) 8: if ( jD l j = 0 ) or ( jD r j = 0 ) then 9: f Constrain range for single-branch node. g 10: if ( jD l j &gt; 0 ) then max q  X  p 11: else min q  X  p 12: end if 13: `  X  ` + 1 14: continue at the start of while loop 15: end if 16: f Build two nodes: Lef t and Right as a result of a 17: temp  X  max q ; max q  X  p 18: Lef t  X  SingleT ree ( D l ; min; max; ` + 1) 19: max q  X  temp ; min q  X  p 20: Right  X  SingleT ree ( D r ; min; max; ` + 1) 21: end if 22: terminate while loop 23: end while 24: return Node ( Lef t; Right; SplitAtt  X  q;
Setting h : The level setting influences the size of the subspaces in which mass is calculated. To dif ferentiate one cluster from another , the zero-mass subspaces must be small enough to fit into the region separating dif ferent clusters X  X f the subspaces are too lar ge, dif ferent clusters are joined.
Let  X  be the minimum separation between any two clusters (in all dimensions); and r be the range of the working space in the same dimension  X  is measured. In order to have zero-mass subspaces separating the two clusters,  X  &gt; r Note that this setting is with reference to the ratio r than the absolute separation  X  . In other words,  X  may be small, but if all the data is concentrated in a small area, then is small too; thus a low h can be used to separate these clusters with small  X  . An example of separation required between two clusters is sho wn in Figure 2(a), where  X  = 0 : 0077 and r = 0 : 97 which yields h &gt; 6 : 97 .
Figure 2(b) sho ws how h can be set in practice (when used for clustering, to be described in Section VI): h is increased Figure 2: (a) An example of  X  in the horizontal dimension in the Ring-Curv e data set (the full vie w of the data set is sho wn in the last diagram of Figure 4(b)). (b) As h increases from 1 to 8, the number of clusters identified by gmass ( : ) also increases as follo ws: 1,3,5,5,5,7,7,196 (in a 48-dimensional data set.) setting before this lar ge number shall be used, i.e., h = 7
Setting  X  : The comple xity of the data influences the setting of  X  . While a lar ge  X  is preferred, a small  X  often be used. Simple clusters require small  X  ; whereas comple x clusters requires lar ge  X  . Note that h also affects the choice of  X  . A high h should accompan y a high  X  to avoid producing man y isolated small islands in a cluster (when a low  X  is used.) A. Time and Space Comple xities Using h : d -Tree reduces the time comple xity to O ( min ( hd; log X  )  X  X  ) from O (  X  h t ) (using Equation (2)), making it feasible for very high level-h mass estimation. The space comple xity of h : d -Tree is O ( min (  X ; hd ) t ) has almost the same order to O (  X  X  ) when one-dimensional mass estimation using Equation (2) is stored using a lookup table [12].
 B. Relation to k-d Tree
At the algorithmic level, a k-d Tree [6], based on me-dian split, may appear on the surf ace to be similar to the h : d -Tree . For example, constructing a node of a k-d Tree starts on one dimension, and cycles through the dimensions to build subsequent nodes in the tree. h : d -Tree does a similar dimension cycling. Ho we ver, there are important dif ferences. First, the purposes dif fer: k-d Tree is designed to speed up search, e.g., in a near neighbour search; whereas h : d -Tree is specifically designed for mass estimation. Second, k-d Tree emplo ys the median as the split point; in contrast, the split point for a node of h : d -Tree is simply the mid-point of a dimension in the working space, indepen-dent of the distrib ution of the data X  X o search is required to find the split point. Third, a k-d Tree cannot be used to estimate mass because the median-split produces a balanced tree with all external nodes having the same mass X  X seless for our purpose! Fourth, a k-d Tree is constructed using all available data; whereas each h : d -Tree requires a small training sample only . As a result, although both are linear time-comple xity algorithms, k-d Tree is linear with respect respect to  X   X  n .
 C. Mass X  s relation to density
Density is defined as mass per volume; whereas mass is defined independent of volume. Density is equi valent to mass only if the volume is the same for every subspace in a single tree. Because multiple trees are emplo yed, the mass estimated by gmass ( : ) is not equi valent to density since the volume of each subspace varies from one tree to the next (see subspaces created by the two example trees in Figure 1(b).)
The only reason for using equal-size-subspace in each h : d -Tree is for fast tree construction X  X ach division can be done without looking at the data, once the working space is defined. In fact, there is already a tree implementation that produces varying-size grid [9] which is designed specifically for anomaly detection only .

Further distinctions between mass estimation and density estimation have been pro vided by [12]. Their effects on clustering algorithms are given in Section VIII.
Here we compare the one-dimensional mass estima-tion with the multi-dimensional mass estimation using h : d -Trees , in terms of their runtime and ability to model data distrib ution.
 Runtime . Figure 3 sho ws the factor of increase in runtime when h increases from 1 to 5, when both mass estimations use t = 1000 and  X  = 8 in the Ring-Curv e data set. The one-dimensional mass estimation using Equation (2) increases the runtime by a factor of 264 when increasing h from 1 to 5; whereas h : d -Trees  X  X  runtime increases by a factor of 1.07 only . Setting  X  = 256 and h = 5 , the one-dimensional mass estimation fails to complete the run in less than one day , whereas h : d -Trees completes it in 130 seconds.
Because the one-dimensional mass estimation using Equa-tion (2) is so slo w and h : d -Trees can be easily modified to produce one-dimensional mass estimation (by randomly selecting an attrib ute to form A in Algorithm 2), we sho w the result of one-dimensional h : d -Trees instead in the fol-lowing experiments. The one-dimensional mapping applied to multi-dimensional problems is the same as that described in Section II or [12].
 Modelling the underlying data distrib ution for multi-dimensional problems. After h : d -Trees are trained using the given data, the modelling results are presented as con-tour maps, produced from the mass values estimated from h : d -Trees for a lattice of equal-spaced points in the entire feature space. Figure 3: Runtime comparison: one-dimensional mass esti-mation versus multi-dimensional mass estimation.
Figure 4 sho ws the contour maps of the two mass estima-tions using a two-dimensional data set: Ring-Curv e. The y sho w the modelling progression as h increases from 2 to 6. The one-dimensional mass estimation, in Figure 4(a), fails to model the two clusters. In contrast, the multi-dimensional mass estimation successfully models the two clusters when h = 6 (see the last diagram in Figure 4(b).)
Figure 5 sho ws the key weaknesses of one-dimensional mass estimation using three additional data sets: one-Gaussian, three-Gaussian, and Ring. Although it models the uni-modal data reasonably well, it does poorly in the multi-modal data X  X t creates phantom modes (e.g., in the three-Gaussian and Ring data sets) because it assumes data symmetry . Multi-dimensional mass estimation does not have this kind of weakness and models the underlying data distrib ution well, when an appropriate h is used.
Because h : d -Trees has done the hard work to model the underlying data distrib ution, to identify clusters within the data is simply to extract the connecting structures from h : d -Trees . We will describe how this can be done in the next section.

In this section, we sho w how to emplo y mass to perform clustering. It is unique among existing clustering methods because it does not emplo y any distance or density measure. Mass-based clustering has the follo wing characteristics:  X  It emplo ys the same mass model to identify arbitrary-shape clusters and to filter noise;  X  It does not assume any data distrib ution;  X  It scales up well to huge data size;  X  It performs clustering without the need to invoke distance or density calculations.

The local neighbourhood of any instance is readily avail-ing based on mass gets the local neighbourhood information without additional cost.

We pro vide the definitions and the algorithm for mass-based clustering in the next two sections, and the time and space comple xities in the third section. Figure 5: Contour maps from mass estimation using h : d -Trees ( A. Definitions
To simplify notation, we use T to denote a subspace defined by T ( x ) in which an instance x falls into.
Definition 1: An instance ^ x 2 T a cluster , iff m ( T
Recall that T D i  X  D
Definition 2: An instance x 2 D is T -connected by a seed ^ x 2 T (a) x 2 T (b) x 2 T m ( T I I
Definition 3: A seed-based cluster is a subset C  X  D , where 8 x 2 C : x is T -connected by ^ x .

Definition 4: A cluster -pair [ g; h ] consists of two seed-based clusters with ^ x g and ^ x h , where 9 x 2 D : x is connected jointly by ^ x g and ^ x h .

Definition 5: An arbitrary-shape cluster , for the set of seeds S and the set of cluster -pairs P , is a subset C  X  D iff (a) 8 x 2 C; 9 ^ x 2 S : x is T -connected by ^ x , and (b) 8 ^ x g ; ^ x h 2 S , either [ g; h ] 2 P or 9 ^ x F and {  X  1 .

Definition 6: Noise instances are instances in C where j C j &lt;  X  , where  X  is a constant defined by users.
We call the process to find arbitrary-shape clusters from a given data set and h : d -Trees , a T -connection process .
It is interesting to note that one may invoke a search to find a seed which is either the centre of the cluster (i.e., the instance having the highest mass, max ^ x 2 T i having the highest mass, max clustering result obtained is exactly the same as that obtained by any seed satisfying Definition 1. This is because the T -connection process will eventually encompass the entire cluster , no matter which instance in the cluster is chosen as the seed. We emplo y the cheapest, no-search option in formulating the mass-based clustering algorithm called MassTER in the next section.
 B. The MassTER Algorithm
The idea is to extract the connecting structures from dif ferent h : d -Trees  X  X hich have already modelled the underlying data distrib ution X  X n order to identify arbitrary-shape clusters that are free of noise instances, using the definitions pro vided in the last section. Recall that each h : d -Tree is a realisation of T i . The algorithm aims at uniquely assigning every instance to one cluster .
The mass-based clustering procedure, MassTER , is given in Algorithm 3. The first step is to build an ensemble of h : d -Trees in order to obtain the connecting subspaces T i . The second step is to assign a cluster to each instance if has pre viously been labelled with a cluster ID (Definition 2.) If T i is labelled, this step also checks whether the instance is T -connected to another seed-based cluster (Definition 4). If it is, a cluster -pair is formed. If T is designated as the seed of a new cluster (if it satisfies Definition 1) and T ID. The third step is to mer ge all cluster -pairs, that satisfy Definition 5(b), into a single arbitrary-shape cluster . Algorithm 3 : MassTER ( D; t;  X ; h;  X  ) Inputs : D -input data, t -number of trees,  X  -sub-sam-pling size, h -number of times an attrib ute is emplo yed in a path,  X  -minimum number of instances in a cluster . 1: f T i : i = 1 ; : : : ; t g X  BuildT rees ( D; t;  X ; h ) 2: Assign a seed-based cluster to each instance x 2 D 3: Mer ge cluster -pairs which satisfy Definition 5(b). 4: E  X  clusters having number of instances less than  X  . 5: retur n K arbitrary-shape clusters, C | ; | = 1 ; : : : ; K An example of the second step is sho wn in Figure 6(a). Assume ^ x g ; ^ x h ; x follo wing sequence, given T ^ no T T , T (Definition 1.) ^ x h : Since T 5 is unlabelled at this point in time, ^ x h designated as the seed, and T new cluster ID: h (Definition 1.) x 1 : T 5 has a label now, x 1 is assigned with the cluster ID: h . T x 2 : Since both T 3 and T 4 have been labelled with dif ferent cluster IDs, a cluster -pair is formed: [ g; h ] (Definition 4.)
Note that the second step produces dif ferent results for dif ferent orderings, e.g., the reverse ordering of the abo ve example will assign all four instances to a single cluster ID. Ho we ver, the mer ging process will yield the same clustering result, independent of the ordering of the instances.
At the end of the mer ging process, all clusters having the number of instances less than  X  are considered as noise instances; the y are filtered out in the fourth step of the algorithm.  X  is the only additional parameter in MassTER , and it is set to 10 in our experiments X  X  cluster of less than 10 instances is too small to be considered as a proper cluster . instances are at the periphery of each cluster . C. Time and space comple xities
The time comple xity analysis is as follo ws. The construc-tion of h : d -Trees costs O ( t X  X d ) (assume hd &gt; log (  X  ) The most expensi ve part of the clustering procedure is to assign a seed cluster to each instance which costs O ( tnhd ) The last two steps cost O ( n ) in the worse case. Thus, the overall time cost for the MassTER algorithm is O ( tnhd )  X   X  n . The main space requirement is to store h : d -Trees and input D ; the space requirements in other steps are substantially lower . Thus, MassTER has space comple xity O ( thd + n ) during training. The training set is discarded after training, yielding O ( thd ) .

We use DBSCAN [5] and DENCLUE [3] as the bench-marks because the y both claim to be fast running density-based clustering algorithms. MassTER is implemented in JAVA, and we use DBSCAN in WEKA [13] and a version of DENCLUE implemented in R (www .r-project.or g) in our empirical evaluation. The experiments are run on a machine having 2  X  Xeon X5550 Quad-core 2.66 GHz processors and 48GB memory (www .vpac.or g).

The clustering result is reported in terms of CPU run-time (in seconds), number of clusters identified, number of unassigned instances, and F-measure which is calculated based on assigned instances only . F-measure = 1 when all assigned instances are in the correct clusters, i.e., perfect clustering; and F-measure = 0 if all instances are assigned to wrong clusters. We tune the parameters of each algorithm and report the best result.

We use five data sets that have the characteristics in which clustering methods can be evaluated: clusters of dif ferent shapes, sizes and densities; for example, one data set has clusters embedded in high dimensional space; one has significant amount of noise; and one has overlapping clusters. We describe the experimental result with each data set in the follo wing subsections.
 Table II: Clustering results in the Ring-Curv e-W ave-T ri-Gaussian data sets for MassTER ( h = 7 and  X  = 256 ) and DBSCAN (  X  = 0 : 01 and minP ts = 6 ).
Ring-Cur ve-W ave-T ri-Gaussian . It has three two-dimensional synthetic data embedded in either a 3-dimensional data set or a 48-dimensional data set (where 42 dimensions are irrele vant with a constant value.) The three two-dimensional data are Ring-Curv e, Wave and Triangular -Gaussian sho wn in Figure 6(b),(c),(d), which have a total of seven clusters. Each cluster has 10000 instances with a total of 70000 instances.

The clustering results from MassTER and DBSCAN are sho wn in Table II. In both the 3-dimensional and 48-dimensional data sets, MassTER performs better than DBSCAN in three out of the four performance measures: MassTER run faster than DBSCAN by a factor of 23 and 47, respecti vely in the two data sets; MassTER identifies the correct 7 (versus 8) clusters and the perfect F-measure of 1 (versus 0.9999). MassTER has increased its number of unassigned instances from 321 to 692 when the number of dimensions increases from 3 to 48; whereas DBSCAN has the same 332 unassigned instances in both cases. Note that MassTER can reduce the number of unassigned instances by increasing  X  , e.g., setting  X  from 256 to 2560 reduces the number of unassigned instances from 692 to 317; and this only increases the runtime from 256 to 646 seconds.
The unassigned instances by MassTER are all at the periphery of each cluster sho wn in Figure 6(b),(c),(d).
In order to examine how well the algorithms scale up, we use the 48-dimensional data set and increase the data size from 7000 to 70000, 525000, and 1050000. Figure 7 plots runtime ratio versus data size ratio (1, 10, 75 and 150) by using 7000 as the base. The result sho ws that MassTER has Figure 7: Runtime scale up comparison: MassTER vs DB-SCAN in the 48-dimensional Ring-Curv e-W ave-T riGaussian data set. Note that DBSCAN completes the task of the one-million data set (at data size ratio=150) in 36 days versus MassTER  X  X  1.3 hours. a sublinear increase in runtime: The runtime ratio increases from 1 to 112 when the data size ratio increases from 1 to 150. In contrast, DBSCAN X  s runtime ratio increases from 1 to more than 18000 with the same increase in data size ratio. MassTER is faster than DBSCAN by a factor of 655 when the one-million data set is used.

VaryingDensity . This data set has 2 dimensions and 2 clusters of dif ferent densities, and each cluster has 250 instances. The two clusters are two Gaussian distrib utions with density ratio dr =  X  2 cluster 1 has fix ed  X  the boundary of sparse cluster 2. The result sho wn in Table III reveals that MassTER produces better clustering than DBSCAN in terms of F-measure and number of clusters. This result sho ws that MassTER is more tolerant to varying densities than DBSCAN. In this small data set of 500, DBSCAN runs faster than MassTER because MassTER has some fix ed overhead, independent of the data size. Table III: Clustering results in VaryingDensity data sets for MassTER ( h = 5 ) and DBSCAN ( minP ts = 6 ). OneBig . This data set was pre viously emplo yed by [11]. It has 20 attrib utes and 9 clusters. The biggest cluster has 50,011 instances, and each of the other eight clusters has approximately 1000 instances. In addition, there are 10,000 noise instances randomly distrib uted in the feature space. This data set has a total of 68,000 instances.
 The result in Table IV sho ws that MassTER and DB-SCAN have same clustering result in terms of F-measure and number of clusters; but MassTER runs significantly faster than DBSCAN is this lar ge data set. Note that both MassTER and DBSCAN have correctly identified the 10000 noise instances in this data set.
 Table IV: Clustering results in the OneBig data set for MassTER ( h = 3 and  X  = 256 ) and DBSCAN (  X  = 0 : 1 and minP ts = 6 ).

Iris and Yeast . Iris has 150 instances, 4 dimensions and 3 clusters; Yeast has 1484 instances, 8 dimensions and 10 clusters. The clustering results are presented in Table V. In Iris, MassTER performs the best in terms of #cluster and F-measure though slo wer than DBSCAN (for this small data set). It is suspected that Yeast has significant overlapping clusters; this is reflected in low F-measure in all methods. It appears that all three methods have problems with overlapping clusters.
 Table V: Clustering results in Iris and Yeast. The settings used for Iris are: MassTER ( h = 4 ;  X  = 64 ), DBSCAN (  X  = 0 : 1 ; minP ts = 5 ), DENCLUE ( tol = 0 : 0001 ; ctol = 1 ; b = 0 : 3 ). Yeast: MassTER ( h = 3 ;  X  = 16 ), DBSCAN (  X  = 0 : 07 ; minP ts = 5 ), DENCLUE ( tol = 0 : 1 ; ctol = 70000 ; b = 0 : 06 )
Note that DENCLUE is very sensiti ve to parameter set-tings; Han and Kamber [7] have also reported the same observ ation. Because DENCLUE is significantly slo wer than either MassTER or DBSCAN, it cannot complete the exe-cution in reasonable time for the lar ge data sets we have presented earlier .

For the purpose of clustering, the use of mass estimation avoids the key weakness of using density estimation: It is computationally expensi ve to get accurate density estima-tion. This is wh y DENCLUE [3] has to use grid instead for practical applications.
 Mass has the follo wing adv antages over density . First, MassTER can use any instance of a cluster as the seed to form a cluster; DENCLUE relies on an density-attractor to form a cluster  X  X his requires a search for a local maxima in the density estimation function. Though a hill-climbing search is emplo yed (and a further impro ved search is de-scribed in [4]), the search is still a considerable compu-tational expense that MassTER does not need. Second, constructing the grid tak es O ( nlogn ) for DENCLUE when storing the grid in a tree-structure. In contrast, MassTER tak es only O (1) to construct the trees because all the parameters in O ( min ( hd; log X  )  X  X  ) are constant.
Each hyper -sphere centred at x in DBSCAN corresponds with a subspace T in MassTER  X  X hat is the key dif ference between the two algorithms. The subsequent steps to find clusters depend on this first step. Thus, the dif ference boils down to density versus mass X  X BSCAN tak es O ( n 2 ) to compute density; MassTER tak es O ( n ) to compute mass.
Most subspace clustering methods [8] are either bottom-up or top-do wn algorithms. Bottom-up algorithms (e.g., [10]) first examine one dimensional projections and then increasingly higher dimensions. Top-do wn algorithms (e.g., [2]) examine all dimensions and then assess the local neigh-bourhood to determine the best subspaces to identify clus-ters. MassTER is a top-do wn algorithm but it avoids the key pitf all of existing top-do wn algorithms: high computational cost with worse case time comple xity O ( d 2 ) because of distance calculations. MassTER has used the T -connection process to assess local neighbourhood encapsulated in the mass model, without the need to compute distance.
It is interesting to note that MassTER completes a cluster -ing task without computing mass using Equation (7), except disco vered is required, mass distrib ution can be computed with a minimum cost from h : d -Trees for each cluster  X  this pro vides useful information about the structure of the distrib ution of the data over the entire cluster , sho wn as contour maps in Figures 4 and 5.

This paper adv ances the first work on mass estimation in two significant ways. First, without the version we intro-duce here, existing mass estimation can only apply to one-dimensional problems and is limited to low level-h mass estimation because of its high time-comple xity O (  X  h ) sho w that the new mass estimation can model arbitrary-shape distrib utions in multi-dimensional problems, and has time-comple xity O (  X  X  ) only .

Second, we realise one potential of mass, as a base mod-elling mechanism, to solv e dif ferent kinds of data mining problems. The mass-based clustering method we introduced demonstrates that mass can be emplo yed as an alternati ve to distance or density X  X he commonly used measures to design data mining methods. This is an example of applying mass directly to solv e problems that yields a net gain in efficac y and efficienc y. The proposed method identifies each cluster through a T -connection process by examining the structure of the mass model, without expensi ve evaluations. The result is an efficient noise-tolerant clustering method which can identify arbitrary-shape clusters. It has average case sublinear time comple xity and linear space comple xity w.r.t. input size; and it is sho wn to run significantly faster than existing density-based methods DBSCAN and DENCLUE.
In the near future, we will investig ate how to apply the multi-dimensional mass estimation to a host of tasks such as classification and regression. In clustering, we will explore the ability of the mass-based clustering method in dealing with high dimensional problems and overlapping clusters.
This work is partially supported by the Air Force Re-search Laboratory , under agreement# FA2386-10-1-4052. The U.S. Go vernment is authorized to reproduce and dis-trib ute reprints for Go vernmental purposes notwithstanding any cop yright notation thereon. Hiroshi Motoda, Zhouyu Fu, Peter Tischer , Ray Smith and the anon ymous revie wers have pro vided man y helpful comments.

