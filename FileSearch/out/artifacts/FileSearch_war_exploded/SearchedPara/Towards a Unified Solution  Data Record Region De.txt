 Although the task of data record extraction from Web pages has been studied extensively, yet it fails to handle many pages due to their complexity in format or layout. In this paper, we propose a unified method to tackle this task by addressing several key issues in a uniform manner. A new search structure, named as Record Segmentation Tree (RST), is designed, and several efficient search pruning strategies on the RST structure are proposed to identify the records in a given Web page. Another characteristic of our method which is significantly different from previous works is that it can effectively handle complicated and challenging data record regions. It is achieved by generating subtree groups dynam-ically from the RST structure during the search process. Furthermore, instead of using string edit distance or tree edit distance, we propose a token-based edit distance which takes each DOM node as a basic unit in the cost calcula-tion. Extensive experiments are conducted on four data sets, including flat, nested, and intertwine records. The experi-mental results demonstrate that our method achieves higher accuracy compared with three state-of-the-art methods. H.3.m [ Information Storage and Retrieval ]: Miscella-neous Algorithms, Performance, Experimentation  X  Web Data Record Extraction, RST Structure, Web Infor-mation Integration
The World Wide Web has been extensively developed since its first appearance two decades ago. The explosive growth and spread of the Web have resulted in a huge in-formation repository, but yet it is still under-utilized due to the difficulty in automated information extraction (IE) caused by data heterogeneity. The task of Web IE differs largely from the traditional IE task which aims at extract-ing data from basically unstructured free text. In contrast, WebIEprocessesWebdocuments(orWebpages)whichare semi-structured and coded with HTML tags. Furthermore, Web sites commonly employ pre-designed templates to for-mat and present information units, known as data records , which have similar attributes or fields. For example, re-tailing Web sites usually run a server-side program to fill products X  information, retrieved from back-end databases, in a pre-defined template to generate Web pages, which are referred to as deep or dynamic Web pages. Organizations also prefer to display its information in a structured way to facilitate easy browsing, such as list of chief staff, list of breaking events, etc. These pages are known as surface or static Web pages. Therefore, the structured information on the Web is tremendously popular and very valuable for var-ious applications such as online market intelligence [5], data mashups [1], etc.

Some existing works adopt a supervised manner to per-form the Web IE task [15, 28]. They need some pre-labeled pages as input to learn some extraction rules or wrappers. Some others adopt an unsupervised manner [3, 8, 17, 20, 22], which can extract desirable data without manually prepared training data. Considering the availability of single or multi-ple input pages from a particular Web site, Web IE systems can also be classified into single-page based methods and multiple-page based methods. Multiple-page based meth-ods [3, 8, 15, 28] take several pages coming from the same Web site as input, and extract the underlying template or schema automatically by analyzing the differences and sim-ilarities of these pages. Single-page based methods [17, 20, 22] do not need several similar pages from the same Web site. Therefore, they are able to handle Web sites where multiple similar pages are not available. The work introduced in this paper is a single-page based and unsupervised framework, and it can detect the record region ( record section )andseg-ment the records simultaneously under a unified model. One example of such kind of page is given in Figure 1(a). In some existing works, further post-processings are performed such as attribute alignment [26, 30] and labeling [24, 33]. These post-processings make the extracted data suitable for down-stream applications.

Two basic observations on data record and record region are as follows: 1) A group of data records describing a set of similar objects are typically presented in a contiguous re-gion, known as record region, of a page and are formatted using similar HTML tags; 2) A set of similar data records are formed by some child subtrees of the same parent node. Based on these observations, a natural way to detect record region and record boundary is to examine the similarity (or distance) between subtrees. This strategy is adopted by MDR, ViPER, and DEPTA. MDR [17] utilizes string edit distance to assess whether two adjacent subtree groups, named as generalized nodes , are a repetition of the same data type. ViPER [22] is another work which also calculates string edit distance of the subtree pairs to detect record re-gion. Then it involves some visual information to segment a detected region into records. DEPTA [27] calculates tree edit distance between generalized nodes. In summary, MDR and DEPTA calculate similarity for the pairs of neighbor-ing generalized nodes, while ViPER calculates similarity for every pair of subtrees.

The advantage of the above similarity-based approaches is that they are robust against optional fields or tags in a record, and can tackle the problem of approximate matching to identify repeating objects. However, one disadvantage is that the method of determining the granularity involved in the similarity calculation is not flexible. In a record region, records may contain different number of subtrees, as shown in Figure 1. Thus, the number of possible subtree com-binations to compose records is exponential with respect to the number of subtrees. To tackle this problem, MDR and DEPTA introduce a concept known as generalized node, which contains several subtrees. The limitation of the MDR family methods [17, 26, 27] is the greedy search for the best generalized node combination, which is time consuming as noticed by other works [20, 22]. To speed up the greedy search, one inflexible constraint is introduced in their def-inition of record region. This constraint specifies that all generalized nodes must have the same length, i.e. the num-ber of subtrees, in the same record region. This constraint will fail to handle some cases in which the records contain different number of subtrees. For the record region in Fig-ure 1, MDR treats S 2  X  X  X S 7 as one record region, and misses the record R 3 . Noticing this limitation, ViPER only calcu-lates the similarity for single subtree pairs and constructs a similarity matrix. Then with this matrix, some heuris-tic rules are employed to detect the record region. Another limitation of these existing methods is that they decompose record extraction into two separate steps, namely, record region detection, and record segmentation. The reason is that the calculated similarity or distance in region detection, namely, the edit distance of generalized nodes in MDR, and the similarity of pairwise subtrees in ViPER, cannot be uti-lized to decide record boundary directly, since the calculated information is not necessarily boundary related.

In this paper, we propose a unified method to tackle the record extraction task. A new search structure, named as R ecord S egmentation T ree (RST), is designed. Based on the RST structure, several key issues in record extraction are handled in a uniform manner. For example, whether the given region contains a record region, if so, from which subtree it starts, where it ends, and how to segment it into records. To obtain solutions for these issues, we develop sev-eral efficient search pruning strategies on the RST structure of a given region to identify the correct record segmenta-tion. When a correct segmentation is successfully detected, it implies that the record region is also successfully obtained. Based on such approach, our method is able to combine the two steps in the existing works in a unified framework. An intuitive way to understand the advantage of our method is that if the region can be segmented into records having high pairwise record similarity, then we are confident that this region is a record region. At the same time, we also have high confidence on the record segmentation.

Another characteristic of our method which is significantly different from previous works is that it can effectively han-dle complicated and challenging data record regions such as embedded region and nested region. It is achieved by gen-erating subtree groups dynamically from the RST structure during the search process. These groups may contain differ-ent number of subtrees which is determined by taking into account the records X  features in the current region, whereas MDR and DEPTA pre-define a subtree grouping schema, i.e. generalized node, for all record regions. Furthermore, instead of using string edit distance or tree edit distance, we propose a token-based edit distance. Different tag names have different string length, thus using edit distance directly on tag name string is not suitable. In our token-based edit distance, each tag is counted as a basic unit in the edit cost calculation.
Structured data extraction from Web pages has been stud-ied extensively. Early works on manually constructed wrap-pers [4, 18] were found difficult to maintain and be applied to different Web sites, because they are very labor intensive. Semi-automatic methods [2, 13, 14, 15, 16, 21, 28, 31, 32], known as wrapper induction, were proposed to tackle this problem. These methods need some labeled pages in the tar-get domain as input to perform the induction. Thus, they still have limitation for large-scale applications. To over-come the above drawbacks, fully automatic methods have been developed. Methods such as MDR [17], DeLa [24], ViPER [22], and DEPTA [27] were designed to tackle record-level extraction task from a single input page. Our frame-work falls into this category, precisely, it is an automatic, record-level, and single-page based approach.

Techniques that address record extraction from a single page can be categorized into five categories: early methods based on heuristics [6, 10], repetitive pattern based meth-ods [7, 24], similarity-based extraction methods [17, 22, 26], tag path based methods [20], and visual feature based meth-ods [11, 19, 29]. Methods based on heuristic rules cannot be generalized well. Repetitive pattern based methods such as IEPAD [7] and DeLa [24] show some potential in solving this problem because similar templates are used in formatting the records, which make it feasible to mine some repetitive patterns as clues for locating records in the page. One lim-itation of such pattern mining approaches is that it is not robust against optional data and tags inserted into records.
The similarity-based approach tackles this limitation with approximate matching to identify repeating objects. MDR [17] and DEPTA [26] are such techniques, which utilize string or tree edit distance to assess whether two adjacent subtree groups are a repetition of the same data type. Our method also calculates the similarity between two groups of subtrees, but the designed RST structure makes the subtree grouping more flexible and better aligned with the record boundary. ViPER [22] is another work which computes the similarity of each pair of single subtrees to detect record region, then involves some visual perception to segment the detected re-gions into records.

Miao et al. investigate the tag paths in a Web page to perform record extraction [20]. They transform a DOM tree into pieces of tag paths, and cluster the paths according to the defined similarity measure to detect record regions. The limitation of this method is that it cannot take into account the record boundary information during region detection, and needs a separate step to segment records after region de-tection. Therefore, tag paths in the same records and paths among different records cannot be distinguished and treated differently. Inevitably, the accuracy of both steps will be affected. Furthermore, this method clusters the tag paths across the entire page, and does not consider the proximity relations of the paths. However, the same tag path may be used in different blocks of the page, even these blocks are far away from each other.

Although ViPER [22] and work by Miao et al. [20] uti-lize some visual information from rendered Web page to assist record segmentation, they depend on tag structure to detect record regions. In contrast, ViNTs [29] utilizes the visual information first to identify content regularity, and then combines it with tag structure regularity to gener-ate wrappers. ViNTs cannot separate horizontally arranged records, e.g., nested records in a table, and identify multiple regions. Pure visual feature based methods include VEN-Tex [11] and ViDE [19], and they are effective to extract records from pages with well organized visual features. Some other researchers employ pre-defined domain ontology [9] or automatically generated domain ontology [23] to assist the record extraction task.
In the Web page fragment given in Figure 1(a), the com-pany information is organized in a  X  X able X , and each com-pany corresponds to one data record. The table X  X  DOM tree is given in Figure 1(b). We can see that the records share some common fields such as company description, in-vestor, and established year. One can also notice that some records do not contain certain fields. For example, the third record does not have URL information. Furthermore, the records are formatted with similar HTML templates, and each record is composed of several rows in the table. We use T to denote the DOM tree given in Figure 1(b), and T  X  X  subtree sequence is denoted by S , and an element in S is referred to as S i . In the DOM tree in Figure 1(b), T  X  X able X , and S includes S 1 , S 2 ,etc. T and S i are also used to refer to the root nodes of the corresponding DOM trees. A fragment of the sequence S is denoted by S i..j or S i  X  X  X S where 1  X  i  X  j  X | S | .

Considering the characteristics of data record and record region, previous works rely on two basic observations which are reviewed as follows:
Observation 1. A group of data records describing a set of similar objects are typically presented in a particular re-gion of a page and are formatted using similar HTML tags.
Observation 2. A group of similar data records being placed in a specific region is reflected in the tag tree by the fact that they are under one parent node, although we do not know which parent. It is very unlikely that a data record starts from an inner node of a child subtree and ends at an inner node of another child subtree of the parent node. Based on these two observations, existing works separate the record extraction task into two sub-tasks, namely, record region detection and record segmentation. In our frame-work, we propose a unified method which can tackle these two sub-tasks simultaneously. Returning to the example in Figure 1, given the subtree sequence S of the table T , data record extraction aims at identifying the sequence S ( i&lt;j ) and a set of separating indexes b in which each b k  X  b s.t. i &lt; b k  X  j . The identified sequence S i..j record region , and it is separated into data records by the indexes in b . In the above example, the record region is S 2 .. | S | , and separating indexes set { 4 , 7 , 9  X  X  X } indicates the boundary of records in this region. Thus, we know that the that a record region may not start from the first subtree of T , and the length (number of subtrees) of different records may be different. In addition, some DOM trees may contain 0 or more than one regions. If there is no record region, no subtree sequence should be identified. If there are several regions, several subsequences of S should be identified.
Given a subtree sequence S , we design a new search struc-ture, named as Record Segmentation Tree (RST), to detect possible records in this sequence. Based on this search struc-ture, the proposed algorithm can search and identify data records in the sequence. If some data records are identified, they naturally compose record regions. Therefore, region detection and record segmentation are performed simulta-neously. For the convenience of interpretation, we use the subsequence starting from S 1 to illustrate RST construc-tion, as well as the subsequent algorithms. Our framework can detect the region starting from any subtrees in S .We assume that one record at most contains K subtrees in S .
Definition 1. Record segmentation tree is a search tree with the following properties: From the above definition, it can been seen that each node is recursively defined with its parent node, and contains one more record. The record extraction task in S is transformed into a problem that searches a node in the RST structure which best matches with the true records in S .
 Thesegmentationtreewith K =4isgiveninFigure2.
 In this example, each node is labeled by its record set. For instance, the node R = {S 1 .. 4 , S 5 .. 7 } indicates that there are two records, namely, R 1 = S 1 .. 4 ,and R 2 = S 5 .. 7 .Thecov-ered prefix is S 1 .. 7 , and the separating indexes set is | R i | denotes the length of R i , and is equal to the number of subtrees it contains. For instance, | R 1 | =4, | R 2 We use | R | to denote the number of records in R . L R de-notes the average length of the records in R ,calculatedas
In the same record region, it is almost impossible that the lengths of different records are significantly different, say some records contain only 1 subtree, while some others contain 5 subtrees. Based on this observation, we design a slimmed segmentation tree, in which the length of previous records is used to predict that of the next record. Suppose for a node R with L R =2,wemaythinkthatitisimpossi-ble that a child of R has a new record with length 4 or more. Therefore, we only consider the children of R in which the new records X  length is smaller than 4. Another observation is that when L R is large, it is more probable that different records have a larger absolute length difference. For exam-ple, in a page in our experiment data set, the average length of the records is 7. Some records contain 4 subtrees, while some others contain 9 subtrees. Taking the above two obser-vations into consideration, we introduce a slimmed version of segmentation tree.

Definition 2. Slimmed segmentation tree is a sub-graph structure of the basic record segmentation tree. It keeps the first two layers of basic RST. Each child of a non-root node R with prefix sequence S 1 ..n has prefix sequence S 1 ..m ,where n +( L R  X  L R / 2 )  X  m  X  min { n +min { L R L R / 2 ,K } , | S |} .
 An example is given in Figure 3, in which K is 5. It can be seen that in the third layer, we do not construct all possible children of a node R in the second layer. If L R is smaller, we construct fewer number of children for it. Otherwise, we construct more.
Each node in RST is one possible segmentation of the record region starting from S 1 in the sequence S . Accord-ing to the observation that the records in the same region are formatted using similar tags, the correct segmentation should be the node that achieves higher average pairwise record similarity. If we cannot find a node with pairwise record similarity greater than a pre-defined threshold, we may conclude that no record region exists starting from S Precisely, given a DOM tree T and its subtree sequence S , record extraction with the RST structure of S aims at finding a node R  X  such that: where  X  is a pre-defined threshold. Q (  X  ) is the quality func-tion of an RST node R , which is defined as the average pairwise record similarity of records in R : where sim is a similarity function between two records. Let R  X  denote { R | Q ( R )  X   X  } , each element in R  X  is a node whose quality is not less than  X  .Amongthesenodesin R  X  , the one with the maximum number of records is determined as the correct record region R  X  starting from S 1 in S .If more than one nodes have the maximum number of records, we select the one with the best quality as R  X  .If R  X  =  X  means that no record region exists starting from S 1 .
For un-slimmed RST, when we expand it to layer | S | /K , each inner node has K children. Thus, the total number of nodes in layer | S | /K is K | S | /K . In the slimmed version, sup-pose each inner node has  X  K children on average, this number is  X  K | S | /K . Usually, in a possible record region, | S | larger than K . Constructing and searching such a segmenta-tion tree is extremely time and space consuming because of its exponential size. Therefore, some search pruning strate-gies have been developed to allow efficient utilization of the RST structure.
In this subsection, we introduce a threshold-based top k search that can prune the RST significantly, and reduce the complexity to O ( | S | 2 ) without considering pairwise similar-ity computation of subtrees. Furthermore, instead of cal-culating the similarity of all record pairs in Equation 2, we may only check the similarity of a record and its nearest pre-vious neighbors. Accordingly, the time complexity is further reduced to O ( | S | ).
In the beginning of RST construction, we have no idea about the record length in the given subtree sequence. The only information on hand is that each record has at most K subtrees. To attain a better starting, we fully expand the RST in the first 2 layers as shown in Figure 2. Each non-leaf node has K childrenintheselayers. Thenwehave K 2 initial candidate nodes, denoted by  X  R , for future expansion and search. Note that K is very small, and the full expan-sion in the beginning will not cause much increasing of time complexity.

Before proceeding to the next layer of RST, we prune the candidates in  X  R according to their quality, which is defined in Equation 2. For a particular node R in  X  R , it will be pruned if Q ( R ) &lt; X  .Themeaningof  X  will be clear later. After the nodes with lower quality are pruned, if the number of retained nodes is more than a pre-defined threshold k , only the top k nodes with the best quality are retained, and denoted by R . For example in Figure 2, the node {S 1 , S is very likely to be pruned. If R =  X  , then no region starting from S 1 exists in S .
Suppose R is a node in R , the last record in R is S i..n We construct some children of R ,namely, R  X  X S n +1 } , R {S tween newly generated records and the existing ones in R are calculated. Precisely, for each new record S n +1 ..m ( n m  X  n + K )andeach R  X  R , sim ( S n +1 ..m ,R )iscalculated. Then, the quality of S n +1 ..m is defined as: In the same way, each R in R is expanded, and the quality of each new record is calculated.

After the RST structure is expanded one more layer, a new pruning strategy different from that for  X  R is adopted. Q ( R  X  X S n +1 ..m } )  X   X  , R  X  X S n +1 ..m } is retained, otherwise it is pruned, where  X  is smaller than  X  . The rationale behind this strategy is that some record, say S n +1 ..m ,mayhavea larger difference compared with the previous records. But we assume that the difference should not be large. Precisely, Q (
S n +1 ..m ) should not be less than a looser threshold  X  this condition is met, we continue to check the quality of R {S n +1 ..m } . In this way, the search procedure can overcome the problems caused by some outlier records, meanwhile, the quality of the outliers is also bounded by  X  .Recallthat we use  X  instead of  X  in the pruning of the initial candidate nodes  X  R , it is because each element of  X  R contains two records. For a particular R ,ifno R  X  X S n +1 ..m } is retained after pruning, R is put into R  X  . Again, if the number of retained nodes is more than k ,wekeepthetop k nodes with the best quality, and construct a new R .

The above procedure is repeated on the new R until it is empty or the end of S is reached. Then we finish building an RST structure for S starting from S 1 , at the same time, R  X  is also built. Note that the RST structure needs not reach S | S | , which indicates that some subtrees in the end of S should not be included in the record region.
In the above expanding and pruning process, when ex-amining the quality of a new record, all previous records are considered, as shown in Equation 3. This thorough retrospect is not only time consuming, but also unneces-sary. Especially in the region with many records, the cost of such exhaustive comparison outweighs the benefit gained. We adopt a short-term memory retrospect strategy to re-duce the computation workload. When a node is expanded one more layer, we only calculate the similarity between a new record and the nearest r previous records. Thus, in Equation 3, the number of pairwise similarity calculations is reduced from | R | to r . And in Equation 2, the cost of evaluating one node is reduced from | R | ( | R | X  1) / 2to
The computation workload of pairwise record similarity calculation is related to the number of subtrees in the in-volved records. Hence, we use a finer unit to analyze the complexity of utilizing RST in record extraction.
In Section 5.1, when evaluating the quality of new records with Equation 3, we need to calculate the similarity between each existing record in R and each new record. The compu-tation workload is: where L R is the average length of records in R , S R is the subtree sequence covered by R ,and L R l indicates the com-putation needed for calculating the similarity between the records having L R and l subtrees respectively. Thus, the computation workload of segmenting entire S is:
In the top k strategy, the overall computation needed is kK ( K +1)( | S | /L R  X  1) | S | / 4. In our framework, the method of calculating pairwise record similarity is a variant of edit distance. Due to the dynamic programming employed in edit distance, when calculating the distance matrix between two strings, the existing matrix between their prefix strings can be reused. In Equation 3, when calculating the simi-larity between S n +1 ..m and an existing record R , the dis-tance matrix between S n +1 ..m  X  1 and R can be reused. Con-sequently, the above workload can be further reduced to k (
K +1)( | S | /L R  X  1) | S | / 4. The value of k is usually smaller than K ,andboth k and K are small and can be treated as constants. Thus, the overall time complexity is O ( | S | 2
Under the strategy of short-term memory retrospect, the workload in Equation 5 becomes: where r is the number of records retrospected from current layer. Similarly, the overall complexity can be further re-duced to r ( K +1) | S | / 2, which can be regarded as O ( For each sequence S , MDR [17] has time complexity of O ( | S | ) to calculate the similarity among the same length generalized node. In MDR, only the adjacent generalized node pairs are considered in similarity calculation, which is similar to our retrospect strategy with r = 1. ViPER [22] has time complexity of O ( | S | 2 ) to construct the upper tri-angular similarity matrix of all subtree pairs. Note that the above time complexity is only the part needed by MDR or ViPER for record region detection, and they need another step to segment the detected region into records.
Suppose each subtree in S is one record, all RST nodes quite high quality. The reason is that the combination of records is similar to that of the other i records. In some other cases, this kind of combination may even achieve higher quality than the correct record segmentation. For example, there are four records in a region R = {S 1 S 2 , S 3 , S where the primed subtrees are matched ones in different records. The node R c = {S 1 S 2 S 3 , S 4 S 5 S 6 } has a higher quality than R .Wecall R c a composite node of R .Com-posite nodes will increase the computation workload and involve noise during RST search. To tackle this problem, after a period of search, say when all nodes in R exceed S in S , we check the relation between the nodes in R .Ifone node is a composite node of any other node, it is pruned. Note that for the node involving subtrees after S n ,weonly consider its records before S n in the detection of composite node.
In this subsection, we discuss how several kinds of more challenging record regions can be handled with the proposed RST structure and search strategy.
Record region in S may not start from S 1 because of the existence of header information, one example and its DOM tree are given in Figures 4(a) and 4(d). In the same way, the region also may not end at S | S | . We name this kind of region as embedded region . The way to detect embedded regions in our framework is straightforward. In the beginning, we start from S 1 , and generate an initial set  X  R S 1 . If all elements in  X  R S 1 are pruned, we move to S 2 , and repeat the above procedure. In this way, the header subtrees in the beginning are excluded. If R =  X  before coming to S | S | ,itmeansthat there is some footer information which should not be treated as part of any record.

In some other cases, there may be more than one regions in S . For example, in an on-line shopping Web page, S may contain two subsequences introducing new arrival prod-ucts and featured products respectively. And they are sep-arated by some other information. We name this kind of region as non-continuous region . The above procedure for dealing with embedded regions can also be applied to non-continuous region easily. After finishing one region detec-tion, the detection algorithm will move on to the sequence of the remaining subtrees if there are some, and perform the search procedure again to detect the second region.
In some Web pages, the records are formatted in a nested manner. For example, the region formatted with &lt; table each record is packed with one &lt; td &gt; ,each &lt; tr &lt; td &gt; s. The region with &lt; table &gt; as root node is know as nested region . In our framework, with a top-down manner to scan the DOM tree, &lt; table &gt; is detected as a record re-gion with &lt; tr &gt; s as records. Then, each &lt; tr &gt; as a region with &lt; td &gt; sasrecords. Afterthe &lt; tr detected as regions, along with the fact that these &lt; tr have been detected as records of the same root node &lt; ta-ble &gt; in the preceding detection, we may postulate that the &lt; table &gt; is a nested region. To inspect our postulation, we compare the records, i.e., &lt; td &gt; s, from different check whether they are similar. If so, we conclude the node &lt; table &gt; is a nested region. There are several other issues such as the orphan record detection in the last  X  X ow X  will not be discussed here due to the limited space. Note that the above  X  X able X  is used to demonstrate the detecting process, our detection method does not rely on any particular tag.
Intertwine record ,alsoknownas non-continuous record in DEPTA [27], refers to the record whose attributes inter-twine together with other records X  attributes. One example and its DOM tree are given in Figures 4(b) and 4(e). Each record has 3 attributes, namely, image, title, and price, and these attributes scattered in 3 different subtrees ( &lt; our framework, the subtree sequence of &lt; tr &gt; s in the table is first passed to the record detection algorithm, and each successive non-overlapping 3-subtree segment is detected as one record. After that, each &lt; tr &gt; is passed to the detec-tion algorithm, and detected as a region with each &lt; td as one record. Up to now, the above procedure resembles that of nested region detection. We continue to check the similarity between records in neighboring &lt; tr &gt; s, and they are found dissimilar to each other. Thus, we conclude that a intertwine region is detected, and the correct records can be generated by reassembling the detected regions. In this example, three regions (image 1, image 2), (title 1, title 2) and (price 1, price 2) are reassembled to generate records (image 1, title 1, price 1) and (image 2, title 2, price 2).
In Figures 4(c) and 4(f), a publication list is separated into different sections according to the published year. It can be regarded as a non-continuous region. Theoretically, with proper setting of thresholds, we can expect that pub-lications in 2011 will be detected as the first region, and publications in 2010 will be detected as the second region. However, it is also possible that the year row 2011 fuses into record 228, and the year row 2010 fuses into record 225. If a record contains such noise subtrees, the similarity between this record and its context records becomes lower. Meanwhile, the number of subtrees in this record is more than that in others. Using these observations as clues, it is not difficult to exclude noise subtrees from records. Due to space limitation, we only outline the idea and omit the details. Basically, if some subtrees in the first record are detected as noise and excluded, we need to reexamine this record segmentation. The reason is that the polluted first record may lead to wrong segmentation of the region.
In this section, we discuss the method used for measur-ing the similarity between two records (sequences of sub-trees), in sequence S . There are mainly two existing ap-proaches. One is string edit distance based [17, 22], the other is tree edit distance based [27]. In string edit distance based method, each subtree is encoded with a string which is obtained by traversing the subtree in pre-order, and ap-pending the name of visited tag node to the string. Due to the fact that names of different tags may have different length, using the string of tag name directly is not suitable. In spite of this limitation, string edit distance can tackle repetitive fields properly by tandem repeats detection [22], as well as optional fields. In [27], top-down distance, which is a restricted version of tree edit distance, is employed to measure the distance between two subtrees. Its problem is that any crossing layers X  operation is not permitted, making it unable to handle optional tags. On the other hand, the tree edit distance can overcome the limitation in string edit distance since it considers each DOM node as an inseparable unit. To adopt their good points and avoid the shortcom-ings, we propose a token-based edit distance method.
Aiming at fixing the ill-formatted Web pages, some ex-isting works [26, 27] introduce visual information from ren-dered Web page into DOM tree building. To avoid this time consuming rendering operation, we employ an HTML cleansing package, namely, HtmlCleaner 1 ,tocleantheWeb pages. Then the DOM structure of the cleaned page is built.
In the DOM tree, we have two kinds of nodes, namely, tag node and text node. Each tag node has a name such as  X  X r X ,  X  X iv X , etc. A piece of visible text between a pair of  X  and  X  &lt;  X  is regarded as a basic text unit, and normalized to a text node. Table 1 shows the types of text nodes defined in our framework. The priority indicates the order in which different types are attempted when normalizing a piece of text. Each subtree in the DOM is encoded with the sequence of node names in it, which is obtained by traversing the tree in pre-order. Each node name in the sequence is called one token , and it is an inseparable unit in similarity calculation.
A typical characteristic of data records is that they vary in optional or repetitive fields. For example, a book record may have discounted price or not, and one author or sev-eral. An obvious disadvantage of edit distance computation is that repetitive and optional fields increase the edit cost. http://htmlcleaner.sourceforge.net/ As noticed by ViPER [22], similarity threshold is suitable to solve the problem caused by optional fields, but not suitable for repetitive fields. ViPER identifies tandem repeats in two strings before distance calculation, and allows zero cost for deletion and insertion inside additional repetitions.
In our token-based edit distance scenario, the above idea is also applicable. We implement the algorithm proposed by Gusfield et al. [12] to detect the tandem repeats. This algorithm utilizes suffix tree structure to identify the tan-dem repeats which are not longer than z in a sequence of length n in O ( n + z ) time. The difference is that the basic unit in our sequences is token (node name), not single char-acter. Thus we need to perform token comparison instead of character comparison in the detection of tandem repeats. Furthermore, if the token sequence is originated from sev-eral subtrees, it is constrained that tandem repeats are only detected in the token sequence of each single subtree. We refine the tandem repeats based edit distance proposed in ViPER [22] and make it suitable for the token scenario in our framework. For the details of tandem repeat detection and its application in edit distance, we would like to direct the readers to [12] and [22]. Then the calculated distance is normalized into the interval [ 0, 1 ] , and the negative value of the normalized distance is added by 1. Thus, we obtain the record similarity measure used in our framework.
For evaluating the performance of our method and con-ducting comparison with existing methods, we attempt to use existing data sets and available implementation of ex-isting methods. MDR implementation is available 2 ,which is able to deal with flat, nested, and intertwine records. Therefore, it was employed as the comparison baseline for all experiments. We used two existing data sets for testing the performance of our method on flat data record extrac-tion. Besides MDR, we also compare with two other existing methods, namely, TPC [20], and ViNTs [29], whose experi-mental results are available on these data sets. Because no existing data set for nested or intertwine records is avail-able, we collected one data set for each of them, and only compare our method with MDR. With respect to evaluation metrics, we employ the commonly used precision and recall. To avoid the evaluation bias brought in by pages with large number of records, both micro average and macro average are reported.

Given a Web page, our algorithm adopts a top down man-ner to scan its DOM tree for detecting records. For some pages, several record regions are detected. We first filter out the regions whose record size is smaller than 3, where the size of a record is defined as the number of leaf DOM nodes it has. After that, we employ the single-linkage agglomerative hierarchical clustering algorithm to cluster the remaining re-gions into different clusters. The algorithm starts with each region as an initial cluster which contains its records as the elements in it. The stopping similarity threshold is set to be  X  . After clustering, the cluster with the largest number of records is selected as the final result. We used a number of training pages collected separately to tune the parameter values in our framework. Finally,  X  is set to 0.75,  X  is set to 0.65, K is set to 10, both k and r are set to 5. http://www.cs.uic.edu/~liub/WebDataExtraction/
Our first data set is the testbed collected by Yamada et al. [25] which is available at http://daisen.cc.kyushu-u.ac.jp/TBDW/ . This testbed was also used by other works such as ViPER and TPC [20]. The testbed data has 253 Web pages from 51 Web sites randomly drawn from 114,540 Web pages with search forms. The data records in this testbed are manually labeled by the collectors, and results are also available online together with the data set. In our experiment, three sites are excluded because of nested records, garbled code, or ambigu-ous record annotation. Thus, we use the remaining 48 sites including 238 pages in total. TPC also conducted experi-ment on this testbed data set. The authors kindly provided us the Web site IDs in the subset they used, which contains 43 sites out of 48 sites we use. Therefore, without imple-menting their method, we can still conduct a fair compari-son. Our second data set is the data set 3 used in ViNTs [29] which is available at http://www.data.binghamton.edu:8080/vints/ along with the details of ViNTs X  performance on each page. Thus, we can conduct a comparison with ViNTs in more detail. This data set is originated from Omini [6] testbed collected by Buttler et al., which consists of more than 2,000 Web pages collected 50 Web sites. ViNTs took one random page per Web site to construct its data set 3. In our ex-periment, we exclude 2 pages because of the ambiguity on record annotation. Thus, our second data set contains 48 pages. The above two data sets are referred to as TB1 and TB2 respectively. We run MDR on both data sets with the default similarity threshold 60%, and extract the records re-ported. MDR could not produce output for 2 Web sites in TB1 and 5 pages in TB2 because the MDR program termi-nated abnormally. We report both results with and without these pages.
 The experimental results on TB1 and TB2 are given in Tables 2 and 3 respectively.  X  X LL X  refers to the entire page set used,  X  X LL/wrong X  refers to the subset without the pages on which MDR program terminated abnormally, and  X  X PC used X  refers to the subset used by the TPC method.  X  X round X  X enotes the number of ground truth records,  X  X P X  denotes the number of true positive given by a method, and  X  X P X  denotes the number of false positive.  X  X -mi X ,  X  X -ma X ,  X  X -mi X  and  X  X -ma X  are the micro-averaged and macro-averaged precision and recall values respectively.  X  X A X  X eans no corresponding result reported in TPC [20]. In P-ma cal-culation, if both TP and FP are 0 for a particular page, its precision is set to be 0. On TB1, our method outperforms both MDR and TPC.
 The recall of our method is significantly better than that of MDR. In addition the precision of our method outperforms TPC about 7%. On TB2, the performance of our method is also much better than that of MDR. Compared with ViNTs, our method can extract more correct results, and achieve 2% improvement in R-mi. We can see that for MDR P-ma is much smaller than P-mi. It is because for some pages, although the MDR program terminated normally, it cannot give any output, thus both TP and FP are 0.

The details of the extracted records on TB1 and TB2 are given in Figure 5 and Figure 6 respectively. On TB1, our method outputs 10 false positives for site 20. After checking the pages manually, we find that each page in this site con-tains 10 recommended books on the right side bar. But the annotated ground truth only includes the books formatted with &lt; table &gt; in the center of the page. For this site, MDR only reports the books annotated. For site 35, we find that each field of a record is packed in a single subtree in S ,such as id, title, URL, each of digest sentences, etc. Because dif-ferent records have different number of subtrees, MDR fails to output any correct results. For site 48, each page con-tains more than one record regions, MDR only reports the largest one, and misses others. In Figure 6, we can see that for pages 17, 18 and 33, ViNTs misses some records. After checking these pages manually, we find that each of them has 3 or 4 regions. As reported by Zhao et al. [29], ViNTs is designed to extract records just from the major record region, it misses other smaller regions.
To collect data sets with nested and intertwine records, we investigated the online shopping Web sites one by one in an online shopping yellow page http://www.toponlineshopping.com/ . There are 22 categories such as  X  X rt &amp; Collectibles X  and  X  X eauty &amp; Fragrances X  in this page, and each category has about 6 sub-categories on average. Under each sub-category, we randomly selected 2 recommended Web sites. Instead of submitting queries to retrieve record pages, we directly clicked the navigation links in the home page, and obtained record pages. In this way, we successfully obtained record Table 4: Experimental results on nested pages.
 Table 5: Experimental results on intertwine pages. pages from 110 sites, and there are 50 sites adopting nested manner to present products, and 5 sites adopting intertwine manner. We downloaded 2 pages per nested site to con-struct the nested data set, thus it contains 100 Web pages. MDR terminated abnormally for one page in this data set. To construct the intertwine page set, we downloaded 3 pages per intertwine site, thus there are 15 pages in the intertwine data set.

The experimental results for nested and intertwine page sets are given in Tables 4 and 5. Our method achieves nearly perfect results, while MDR misses many records in nested record extraction, and gives more false positives in both nested and intertwine record extraction. After checking the pages for which MDR misses all records manually, we found that these pages format the records in a very complicated manner. In 3 pages, each &lt; td &gt; contains another &lt; to encapsulate the information of one record. In another page, each &lt; td &gt; even contains two layers of nested ble &gt; s. Our method can tackle these pages correctly since it does not rely on any particular tag. Our final record selec-tion process effectively excludes noise records, while MDR outputs more false positives.
In this paper, we present a novel approach to extract data records in a Web page. The proposed RST structure can be utilized to address several key issues in the record extraction task. The two essential sub-tasks, namely, record region de-tection, and record segmentation, are handled in a unified manner with the proposed search pruning techniques on the RST structure. Different from the existing similarity-based methods, our method examines the similarity between the dynamically generated subtree groups taking into account the characteristics of the current record region. Owing to the pruning strategies, our method has a comparative com-plexity compared with the existing methods. Furthermore, we propose a new similarity measure in which each DOM node is regarded as an inseparable unit. Together with the detected tandem repeats, our similarity measure can tackle optional and repetitive fields in records properly. Exten-sive experiments are conducted to evaluate the performance of our method. The results demonstrate that our method can achieve very superior results for the test data sets, and different kinds of records can be tackled effectively. [1] http://www.programmableweb.com/ . [2] B. Adelberg. Nodose -a tool for semi-automatically [3] A. Arasu and H. Garcia-Molina. Extracting structured [4] G.O.ArocenaandA.O.Mendelzon.Weboql: [5] R. Baumgartner, G. Gottlob, and M. Herzog. Scalable [6] D.Buttler,L.Liu,andC.Pu.Afullyautomated [7] C.-H. Chang and S.-C. Lui. Iepad: information [8] V. Crescenzi, G. Mecca, and P. Merialdo. Roadrunner: [9] D. W. Embley, D. M. Campbell, Y. S. Jiang, S. W. [10] D. W. Embley, Y. Jiang, and Y.-K. Ng.
 [11] W. Gatterbauer, P. Bohunsky, M. Herzog, B. Kr  X  upl, [12] D. Gusfield and J. Stoye. Linear time algorithms for [13] A. Hogue and D. Karger. Thresher: automating the [14] C.-N. Hsu and M.-T. Dung. Generating finite-state [15] N. Kushmerick. Wrapper induction: efficiency and [16] A. H. F. Laender, B. Ribeiro-Neto, and A. S. da Silva. [17] B. Liu, R. Grossman, and Y. Zhai. Mining data [18] L. Liu, C. Pu, and W. Han. Xwrap: An xml-enabled [19] W. Liu, X. Meng, and W. Meng. Vide: A vision-based [20] G. Miao, J. Tatemura, W.-P. Hsiung, A. Sawires, and [21] I. Muslea, S. Minton, and C. A. Knoblock.
 [22] K. Simon and G. Lausen. Viper: augmenting [23] W. Su, J. Wang, and F. H. Lochovsky. Ode: [24] J. Wang and F. H. Lochovsky. Data extraction and [25] Y. Yamada, N. Craswell, T. Nakatoh, and [26] Y. Zhai and B. Liu. Web data extraction based on [27] Y. Zhai and B. Liu. Structured data extraction from [28] Y. Zhai and B. Liu. Extracting web data using [29] H. Zhao, W. Meng, Z. Wu, V. Raghavan, and C. Yu. [30] H. Zhao, W. Meng, and C. Yu. Mining templates from [31] S. Zheng, R. Song, J.-R. Wen, and C. L. Giles. [32] S. Zheng, R. Song, J.-R. Wen, and D. Wu. Joint [33] J. Zhu, Z. Nie, J.-R. Wen, B. Zhang, and W.-Y. Ma.
