 Frame-semantic parsing is the task of automati-cally finding semantically salient targets in text, disambiguating the targets by assigning a sense (frame) to them, identifying their arguments, and labeling these arguments with appropriate roles. repository of semantic frames and roles, which we use in the experiments below.

Several learning and parsing algorithms have been developed for frame-semantic analysis (Jo-hansson and Nugues, 2007; Das et al., 2014; T X ck-str X m et al., 2015), and frame semantics has been successfully applied to question-answering (Shen and Lapata, 2007), information extraction (Sur-deanu et al., 2003) and knowledge extraction (S X -gaard et al., 2015b).
In contrast to Propbank-style semantic-role la-beling (Titov and Klementiev, 2012), only very limited frame-semantic resources exist for lan-guages other than English. We therefore fo-cus on multilingual or cross-language frame-semantic parsing, leveraging resources for English and other major languages to build any -language parsers. We stress that we learn frame-semantic parsing models that can be applied to any lan-guage, rather than cross-lingual transfer models for specific target languages. Our approach re-lies on inter-lingual word embeddings (S X gaard et al., 2015a), which are built from topic-aligned documents. Word embeddings have previously been used for monolingual frame-semantic pars-ing by Hermann et al. (2014).
 Contributions This paper makes the following three contributions. We present a new multi-lingual frame-annotated corpus covering five top-ics, two domains (Wikipedia and Twitter), and nine languages. We implement a simplified ver-sion of the frame-semantic parser introduced in Das et al. (2014). Finally, we show how to modify this parser to learn any-language frame-semantic parsing models using inter-lingual word embed-dings (S X gaard et al., 2015a). Figure 1 depicts a F RAME N ET 1.5 frame-semantic analysis of a German sentence from Wikipedia. The annotator marked two words, Idee and kam , as targets. In frame-semantic parsing, target iden-tification is the task of deciding which words (i.e. targets) trigger F RAME N ET frames. Frame iden-tification is the problem of disambiguating targets by labeling them with frames, e.g., C OGITATION or C OMING _ UP _ WITH . Argument identification is the problem of identifying the arguments of frames, e.g., Idee for C OMING _ UP _ WITH .
We had linguistically trained students anno-tate about 200 sentences from Wikipedia and 200 tweets each in their native language. The data was pre-annotated by obtaining all English trans-lation equivalents of the source language words in the F RAME N ET 1.5 training data. We pre-sented annotators with all frames that could be triggered by any of the target word X  X  transla-tions. Both data from Wikipedia and Twit-ter cover the same five topics: Google, An-gelina Jolie, Harry Potter, Women X  X  Rights, and Christiano Ronaldo. The topics were chosen to guarantee coverage for all nine languages, both in Wikipedia and Twitter. Our corpus, which covers nine languages, is publicly avail-able at https://github.com/andersjo/ any-language-frames The languages we cover are Bulgarian ( B G ), Danish ( D A ), German (D E ), Greek ( E L ), English ( E N ), Spanish ( E S ), French ( F R ), Italian ( I T ) and Swedish ( S V ). En-glish is included as a sanity check of our cross-lingual annotation setup.

The English, Danish, and Spanish datasets were doubly-annotated in order to compute inter-annotator agreement (IAA). The overall target identification IAA was 82.4% F 1 for English, 81.6% for Danish, and 80.0% for Spanish. This is lower than a similar monolingual annotation experiment recently reporting target identification IAA at 95.3% (S X gaard et al., 2015b). The frame identification IAA scores were also higher in that study, at 84.5% and 78.1% F 1 . The drop in agreement seems mostly due to pre-tagging er-rors caused by erroneous or irrelevant word-to-word translations. The Spanish data has the lowest agreement score.

We compute test-retest reliability of our anno-tations as the correlation coefficient (Pearson X  X   X  ) between the two annotations. In Cronbach X  X   X  in-ternal consistency table, the cut-off for acceptable reliability is 0.7. While there is certainly noise in our annotations, these are still consistently above
Table 1: Inter-annotator agreement (F 1 in %) the Cronbach cut-off. Also, we evaluate our mod-els across 18 datasets, covering nine different lan-guages with two domains each; although for read-ability, we combine the Wiktionary and Twitter datasets for each language below.

The relatively low reliability compared to pre-vious annotation efforts is due to the cross-lingual pre-annotation step, which was necessary to make annotation feasible. All languages, including En-glish, have been pre-annotated using B ABEL N ET . We expect annotators to only assign frames when meaningful frames can be assigned, so the main source of error is that the pre-annotation may ex-clude valid frames. Hence, we will not only re-port F 1 -scores in our evaluations, but also preci-sion, since recall may be misleading, penalizing for frames that could not be chosen by the annota-tors. 3.1 Target identification Following Das et al. (2014), we use part-of-speech heuristics to identify the words that evoke frames (target words). Frame-evoking words typically be-long to a narrow range of part of speech. There-fore, we only consider words as target candidates when tagged with one of the top k part-of-speech tags most commonly seen as targets in the train-ing set. The k parameter is optimized to maxi-mize F 1 on our development language, Spanish, where we found k = 7 . 3 Surviving candidates are then translated into English by mapping the words into multi-lingual B ABELNET synsets, which rep-resent sets of words with similar meaning across languages. All English words in the B ABEL -NET synsets are considered possible translations. If any of the translations are potential targets in F
RAME N ET 1.5, the current word is identified as a frame-evoking word. 3.2 Frame identification A target word is, on average, ambiguous be-tween three frames. We use a multinomial log-cide which of the possible frames evoked by the target word that fits the context best. Our feature representation replicates that of Das et al. (2014) as far as possible, considering the multilingual set-ting where lexical features cannot be directly used. To compensate for the lack of lexical features, we introduce two groups of language-independent features that rely on multilingual word embed-dings. One feature group uses the embedding of the target word directly, while the other is based on distance measures between the target word and the set of English words used as targets for a possible frame. We measure the minimum and mean dis-tance (in embedding space) from the target word to the set of English target words, as well as the distances to each word individually.

Several of the features in the original repre-sentation are built on top of automatic POS an-notation and syntactic parses. We use the Uni-versal Dependencies v1.1 treebanks for the lan-guages in our data to train part-of-speech taggers (T contrast to Das et al. (2014), we use dependency subtrees instead of spans. 3.3 Argument identification A frame contains a number of named arguments that may or may not be expressed in a given sen-tence. Argument identification is concerned with assigning frame arguments to spans of words in the sentence. While this task can benefit from in-formation on the joint assignment of arguments, Das et al. (2014) report only an improvement of less than 1% in F 1 using beam search to approxi-mate a global optimal configuration for argument identification. To simplify our system, we take all argument-identification decisions independently. We use a single classifier for argument identifica-tion, computing the most probable argument for each frame element. Each word index is associ-ated with a span by the transitive closure of its syn-tactic dependencies (i.e. subtree). Our greedy ap-proach to argument identification thus amounts to scoring the n + 1 possible realisations of an argu-ment for an n -length sentence (i.e. subtrees plus the empty argument), selecting the highest scor-ing subtree for each argument type allowed by the frame.

As the training data contains very few examples of each frame or role (e.g., Buyer in the frame C
OMMERCE _ SCENARIO ), we enable sharing of features for frame arguments that have the same name. The assumption is that arguments with identical names have similar semantic properties across frames; that is the argument Perpetrator , for example, is similar for the frames A RSON and T
The scores are the confidences of a binary clas-sifier trained on &lt; frame , argument , subtree &gt; tu-ples. Positive examples are the observed argu-ments. We use the remaining n incorrect subtrees for a given &lt; frame , argument &gt; pair to generate negative training examples . A single binary clas-sification model is trained for the whole data set.
As with frame identification, our features are similar to those of Das et al. (2014), with a few exceptions and additions. We use dependency sub-trees instead of spans and replace all lexical fea-tures (which do not transfer cross-lingually) with features based on the interlingual word embed-dings from S X gaard et al. (2015a). We use the embeddings to find the 20 most similar words in the training data and use these words to generate lexical features that matched the source-language training data. Each feature is weighted by its co-sine similarity with the target-language word. E E E Baseline Our approach to multi-lingual frame semantics parsing extends Das et al. (2014) to cross-lingual learning using the interlingual em-beddings from S X gaard et al. (2015a). Our base-line is a more direct application of the S EMAFOR guage text to English using word-to-word transla-tions and projecting annotation back. For word-to-word translation we use Wiktionary bilingual dictionaries ( X cs, 2014), and we use frequency multiple translations, preferring the most common one. The baseline and our system both use the training data supplied with F RAME N ET for learn-ing. Consider first the target identification results in Table 2. We observe that using B ABEL N ET and our re-implementation of Das et al. (2014) per-forms considerably better than running S EMAFOR on Wiktionary word-by-word translations.

Our frame identification results are also pre-sented in Table 2. Our system is better in six out of nine cases, whereas the most frequent sense base-line is best in two. It is unsurprising that English fares best in this setup, because it does not undergo the word-to-word translation of the other data sets.
Argument identification is a harder task, and scores are generally lower; see the lower part of Table 2. Also, note that errors percolate: If we do not identify a target, or mislabel a frame, we can no longer retrieve the correct arguments. Never-theless, we observe that we are better than running S
EMAFOR on word-by-word translations in eight out of nine languages X  X ll, except English.

Generally, we obtain error reductions over our baseline of 46% for target identification, 37% for frame identification, and 14% for argument iden-tification. For English, we are only 2% (absolute) below IAA for target identification, but about 40% below IAA for frame and argument identification. For Danish, the gap is smaller.
 If we compare performance on Wikipedia and Twitter datasets, we see that target identifica-tion and frame identification scores are gener-ally higher for Wikipedia, while argument iden-tification scores are higher for Twitter. While Wikipedia is generally more similar to the newswire/balanced corpus in F RAME N ET 1.5, the sentence length is shorter in tweets, making it eas-ier to identify the correct arguments. We presented a multi-lingual frame-annotated cor-pus covering nine languages in two domains. With this corpus we performed experiments to predict target, frame and argument identification, outperforming a word-to-word translated baseline running on SEMAFOR. Our approach is a de-lexicalized version of Das et al. (2014) with a sim-pler decoding strategy and, crucially, using multi-lingual word embeddings to achieve any-language frame-semantic parsing. Over a baseline of using S
EMAFOR with word-to-word translations, we ob-tain error reductions of 46% for target identifica-tion, 37% for frame identification, and 14% for ar-gument identification.

