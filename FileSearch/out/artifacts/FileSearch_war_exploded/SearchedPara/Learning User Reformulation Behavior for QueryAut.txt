
It is crucial for query auto-completion to accurately pre-dict what a user is typing. Given a query prefix and its context (e.g., previous queries), conventional context-aware approaches often produce relevant queries to the context. The purpose of this paper is to investigate the feasibility of exploiting the context to learn user reformulation behavior for boosting prediction performance. We first conduct an in-depth analysis of how the users reformulate their queries. Based on the analysis, we propose a supervised approach to query auto-completion, where three kinds of reformulation-related features are considered, including term-level, query-level and session-level features. These features carefully cap-ture how the users change preceding queries along the query sessions. Extensive experiments have been conducted on the large-scale query log of a commercial search engine. The ex-perimental results demonstrate a significant improvement over 4 competitive baselines.

H.3.3 [ Information Storage and Retrieval ]: Informa-tion Search and Retrieval.
 Algorithms, Experimentation, Performance.

Query auto-completion; Query reformulation.
Most of the modern search engines provide the query auto-completion service to help users formulate their queries while they are typing in search boxes. Such service is especially helpful for mobile phone users because entering keywords on small touch screens is time-consuming. Prevalent methods on query auto-completion 1 utilize search logs to generate suggested completions [2, 29, 30]. For an input prefix, the methods extract those queries with matched leading charac-ters from the logs as candidates . Since the given prefix could be short and ambiguous, the number of the candidates might be extremely large. Existing methods mostly rank the can-didates by their popularity in the logs. Consequently, less popular queries are harder to predict.

A possible solution to the ranking problem is to exploit query context such as previous queries and click-through data in the same session 2 . The idea is reasonable since dis-ambiguating the user X  X  search intent by context has been studied extensively in query suggestion [5,9,16,25,26]. Query suggestion is closely related to query completion. The ma-jor difference is that it has got no partial input, i.e., the prefix. In general, if the outputs coming from query sug-gestion match the prefix, they can also be regarded as the suggested completions. For a given prefix and its context, conventional context-aware methods focus mainly on rank-ing the candidates according to the similarity or the depen-dency between the candidate and the given context. For example, some works rank the candidates based on their similarity to the recent queries [2] or their occurrence with previous queries [17]. Some works [16] model query depen-dencies along the sessions, by which the candidates whose contexts are more similar to the given context will be ranked higher. The work [25] further clusters relevant queries with common click-through data into concepts. Query depen-dency is then transformed to concept dependency. Despite such methods have shown their effectiveness in producing relevant queries to the given context, query completion is required to accurately predict what the user is typing.
Take a query session obtained from a commercial search engine log as an example: stomach sounds  X  irritable bowel syndrome  X  cramps stomach . Suppose the last query is the intended query. The prefix is c . Its preceding two queries serve as the context. Many relevant queries are generated from conventional context-aware methods such as colon can-cer symptoms (from [2]), celiac disease (from [16]), and colon cancer (from [25]) in our experiments. Although the sug-gested queries are conceptually relevant to the context, the problem becomes more challenging when the goal is to pre-dict the actual query cramps stomach . Some works take into account other factors for pursuing better approximation in
Query auto-completion is hereinafter referred to as query completion for simplicity.
A query session is a sequence of queries with the same information need. prediction such as temporal dynamics [30, 32] and demo-graphics [29,33]. Different from ours, they do not utilize the context information. One possible way to boost the predic-tion performance by context is carefully investigating how the users refine their queries to get more satisfactory results. If the system gets to know the how, it would be capable of predicting the potential query the user might submit.
In addition to query dependency, context information also demonstrates how the users reformulate their queries re-peatedly throughout the search process. The way people reformulate their queries, or user reformulation behavior, has been studied as query reformulation strategy in previ-ous work [6,18]. There are two types of query reformulation strategies: semantic and syntactic relations. Semantic rela-tions such as generalization (e.g., lion  X  animal ) and spec-ification (e.g., computer  X  mac ) originate from linguistics. Syntactic relations refer to the statistical information about consecutive queries along the sessions such as term-adding , term-reuse and the change of query lengths . As the way to do reformulation expresses how one query is changed to an-other, to some extent it might be helpful in prediction for query completion. In the previous example, the user reuses stomach (appearing in the preceding query) to form the last query.

Understanding user search behavior benefits various ap-plications, including query suggestion [10], query classifi-cation [8] and Web search-result ranking [37]. The work [31] mines user reformulation activities for query suggestion but it does not consider the context information, which has been shown effective in understanding the user X  X  information need. Though lots of work has studied query reformulation strategies, none of them model user reformulation behavior for query completion. As query reformulation strategies re-flect how the users refine their queries and query completion requires accurate prediction of what a user is typing, these motivate us to investigate the feasibility of exploiting the context to learn user reformulation behavior for boosting prediction performance of query completion.

In this paper, we focus primarily on learning user reformu-lation behavior for query completion. We first conduct an in-depth analysis of how the users reformulate their queries. It is expected that if we know how one query is changed to another, we are able to predict the user X  X  next intended query more accurately. Given a prefix and its context to-gether with a set of candidate queries that start with the prefix 3 , the goal of this paper is learning to rank the candi-date queries so that the top-ranked queries are more likely to be the query the user is typing. In other words, we want to predict the intended query that has been partially entered by the user. The context includes previous queries and their chick-through data in the same session. As syntactic rela-tions can be easily identified by observing the queries them-selves, our ranking model comprehensively considers various factors that are essential in query prediction, including term-level, query-level and session-level syntactic features. There are totally 43 reformulation-related features. Extensive ex-periments have been conducted on the large-scale query log collected from a commercial search engine. The experimen-tal results reveal that the prediction performance can be sig-nificantly improved based on our model, compared to 4 com-
In this paper, we focus on the ranking problem only. petitive baselines. Such improvement is consistent across different datasets with different session lengths.
In the rest of this paper, we make a brief review on related work in Section 2. Based on a statistical analysis on user reformulation behavior in Section 3, we describe our prob-lem and approach to query completion in Section 4. The experimental results are presented in Section 5. Finally, in Section 6, we give our discussions and conclusions.
Query Auto-completion. The query completion task can be divided into two steps: filtering and ranking . The former is to generate candidate queries or phrases that start from the given prefix. The latter is to sort the candidates so that the top-ranked ones are more likely what the user intends to input. Most works on query completion extract candidates from query logs. On the contrary, Bhatia et al. [4] utilized a document corpus to extract a set of candidate phrases and proposed a probabilistic model to select those highly correlated to the prefix as the suggested completions. The first step (filtering) needs to address two issues, includ-ing speed and error-tolerance. Chaudhuri et al. [11] captured input typing errors by calculating edit distance and then proposed algorithms for auto-completion based on n-gram and trie traversal techniques. Bast et al. [3] designed an in-dexing structure for speeding up the query process and sav-ing space, compared to other compressed inverted indices. Once the candidates are generated, they can be ranked by manifold approaches for various applications such as Web search [2] and product search [11].

The second step emphasizes how to accurately select what the user is typing from the pool of the candidates. An in-tuitive way is to rank the candidates by their popularity in the logs; however, less popular queries will become harder to predict. White et al. [35] proposed a real-time query ex-pansion model to produce new expansion terms and update following terms to reflect potential completions. To make a better approximation in prediction, Bar-Yossef et al. [2] regarded the user X  X  recent queries as the context. Due to the sparseness problem of the query logs, they applied the query expansion technique to augment the candidate and the context. The candidates were then ranked according to their similarity to the context. Some works utilize tem-poral dynamics to estimate query frequency more precisely. Shokouhi et al. [30] presented a time-sensitive approach to rank the candidates according to their expected popular-ity or frequencies. Strizhevskaya et al. [32] also predicted the frequency of the candidate by modeling their frequency trend in the past. Shokouhi et al. [29] took personalization into account. They learned personalized rankers based on user age, gender, location, and search history.

Our solution differs from these works in that we try to model user reformulation behavior based on the given con-text. Although the work [31] also mines user reformulation activities for query suggestion, it does not consider context information.

Query Suggestion. Query suggestion, which draws much attention in IR, is closely related to query prediction, but the goal is much more different. The query suggestion methods focus on finding relevant queries to preceding queries, in-stead of finding the query that will be actually submitted by the user. Since previous work on context-aware query suggestion might suggest the intended query that the user is typing, it could be regarded as our related work.
With the query session and click-through data as the con-text, there are many ways to generate suggested queries such as association rules [14], measuring the similarity among queries [15,36] and query clustering [10,25]. Huang et al. [17] and Fonseca et al. [13] sought for the queries having high frequency of co-occurrence with the current query 4 in the sessions. Jones et al. [22] extracted the queries often adja-cent to the current query. Boldi et al. [5] built a query-flow graph in which edges connected the query pairs submitted together. Based on the click-through data, those queries that were analogous to the user X  X  preceding queries were treated as the suggested queries. Cao et al. [10] and Liao et al. [25] made use of click-through data to build a bipartite graph and clustered queries according to the graph. Mei et al. [26] applied the idea of random walk to the click-through bipartite graph for finding relevant queries. Considering the whole query sessions as the contexts, He et al. [16] and Cao et al. [9] adopted the variable order Markov model and the hidden Markov model. They modeled the transition proba-bilities between queries.

Some context-aware query suggestion methods further cope with the problems of data sparseness and new queries when using search logs. For sparse data, He et al. [16] allowed the context information to be partially matched, making the suggestion method more flexible and robust. Liao et al. [25] grouped queries into various concepts. Ozertem et al. [27] and Santos et al. [28] built a machine learning framework and applied ranking models to rate queries which were pro-jected into a feature space. The features considered included query co-occurrences and some simple query reformulation strategies [27]. We also handle the two problems. Our solu-tion differs from these works in that we focus on query com-pletion and abundant reformulation-based features, which are potentially beneficial to query prediction.

Query Reformulation. Query reformulation is the pro-cess of refining preceding queries to get better results. Most of previous work focused mainly on determining the types of the reformulation strategies [18], learning to predict their types [12], and understanding how the users reformulate their queries [18]. They analyzed query reformulation strate-gies in two major aspects, including syntactic (format) and semantic (content) analyses. The semantic analysis cate-gorizes reformulation types into generalization and special-ization [1]. The syntactic analysis focuses on exploring the syntactic change between two queries such as adding words, removing words, and acronym expansion. Boldi et al. [6] defined four types of query transition. Jansen et al. [19] and Huang et al. [18] classified query reformulation into 6 and 15 types, respectively. Some works use query reformu-lation strategies to predict query-term performance. Jones et al. [21] and Lee et al. [23] determined the effectiveness of a query term so that the retrieval performance could be improved by filtering out ineffective terms or including ef-fective terms. The reformulation behavior and context in-formation are also usually used to personalize the search re-sults. White et al. [34] predicted users X  interests with search context. Jiang et al. [20] captured user X  X  preference through mining click-through data and query reformulation. In our
The current query is the last query a user submitted.
Average Term Number Figure 1: Average number of query terms in each positio n of the sessions from length 2 to 9. work, we enumerate copious reformulation behavior as ro-bust features and adopt them for query completion.
In this section, we analyze how users reformulate their queries, by which we further explain why modeling such re-formulation behavior can benefit query completion. This analysis is conducted on the one-week search engine log of a commercial search engine (from 1 May, 2013 to 7 May, 2013). The queries in the log are first segmented into sessions with a 30-minute threshold. Two consecutive queries belong to the same session if they are issued within the threshold. Af-ter removing rare queries and single-query sessions, there are finally 63,661,691 sessions and 5,197,821 unique queries in total. Note that the setting is consistent with the exper-iments presented in Section 5.
Since a query session consists of a sequence of queries h q , q 2 ,  X   X   X  , q T i submitted by a user, each query submission q corresponds to a position i in the session. That is, q 1 is the first query submitted or the first position in the ses-sion. For the sessions with the same length 5 , we calculate the average number of query terms in each position. Figure 1 presents its changes along the sessions from length 2 to 9. It can be found (1) queries in longer sessions tend to contain more terms. One possible explanation is that queries with more terms may carry more complex search intent; therefore users need to reformulate their queries more times; (2) the average number of query terms increases along the session. It X  X  visible from the first position to the second one. The curve drops near the end of the session.

We further correlate this change with the semantic re-lations, including specialization and generalization . Intu-itively, specialization aims to narrow down the search re-sults, thus extra terms, i.e., more constraints, are expected to be added into the original query. On the contrary, gen-eralization may lead to a decrease of term number to relax restrictions. We randomly sample 1136 sessions from the dataset, partition them into 2283 consecutive query pairs,
Ses sion length is the number of queries submitted in the session. Figure 2: The percentage of sessions whose lengths are longer or equal to t over different m  X  X  (maximum term repeat). and manually label them with specialization , generalization , or other. Table 1 gives the statistics of our labels, which show that specialization (27.7%) is about twice as many as generalization (12.2%). It explains that most of time queries are reformulated to be longer ones, as shown in Fig. 1. The number of terms increases for most of specialization (84.2%), while the number decreases for most of general-ization (82.5%). The average and median of positions for specification are both smaller than those for generalization . That is why the average term number increases along the sessions but drops near the end, as shown in Fig. 1. The observation implies the combination of query length and po-sition number as features are important. To some extent, they might be helpful in detecting the semantic relations of specification and generalization .
This section discusses if people use some terms more fre-quently in previous queries, they will be also more likely to use that term again.

For a query q i whose i &gt; 1, i.e., the query not submitted in the beginning, we examine each term in this query and count how many times it has been used to form previous queries h q 1 ,  X   X   X  , q i  X  1 i in the same session. Then we take the maximum value m as  X  X aximum term repeat X  for query q i if there is at least one term in q i used m times in previous queries. Given a threshold t , for any session whose length is longer or equal to t , we extract the first t queries as a sequence and calculate the  X  X aximum term repeat X  value for the t -th query. We calculate the frequency of  X  X aximum term repeat X  from 0 to t  X  1 and divide them by the number of extracted sequences. Figure 3: The percentage of queries containing some iden tical term to the last query over different posi-tions.

From Fig.2, we can observe that most reformulated queries do not share any identical term with their previous queries. Take t = 2 as an instance. After submitting the first queries, 60 . 54% of the users do not repeat any term. Furthermore, if we compare different lengths (i.e., t ) of the sessions whose  X  X aximum term repeat X  X s zero (i.e., m = 0), it can be found that when the length of a session increases, the percentage of repeating previously-used terms also increases. It means that a reformulated query is more likely to contain the terms used before when it appears in the latter steps of a session. We also notice that the percentage decreases as  X  X aximum term repeat X  increases. However, the proportion of refor-mulated queries containing terms used in all previous t  X  1 queries increases, compared to that of t  X  2 as shown in Fig. 2. For example, for the curve of t = 5, its percentage value at m = 4 ( t  X  1) is larger than that at m = 3 ( t  X  2). It indicates that some terms are repeatedly used in all queries throughout the session.

At last, for a reformulated query which contains a used term, we would like to know  X  in which positions  X  that term would appear. We again extract the first t  X  1 queries from the sessions whose lengths are equal to t . Then we count the number of the queries in each position that contain some identical terms to the t -th queries. Figure 3 shows the per-centage of queries containing identical terms in each position within sequences from length 2 to 9. It shows that users tend to repeat terms used in the nearest query, and may not stick to the terms in the first query for query reformulation.
This section discusses if some search results of a query have been clicked, a user may think the query is  X  X ffective X 
Ratio Figure 4: The ratio of two conditional probabilities th at if the previous query causes clicks or not, then the latter query would contain some identical terms, over different steps of reformulation. and reuse some terms of it later. We first segment sessions into consecutive query pairs, and then calculate the proba-bility of a pair containing some identical terms, given that the previous query causes clicks or not, respectively. That is, for a consecutive query pair ( q i , q i +1 ), we calculate and here S ( q ) and c i represent the term set of query q and the number of clicks in position i , respectively.

We find that if the previous query causes clicks, the prob-ability of the latter query containing some identical terms with the previous one is 36 . 06%. If the previous query does not lead to any clicks, the probability of the latter query containing the same terms is 50 . 54%. The result shows that if a query does not cause any clicks, its term would be reused probably later. It can be explained: if a query causes clicks, then the user X  X  search intent may somewhat be satisfied, and some other terms will be considered in the next query; on the contrary, if a query does not cause any clicks, the user may stick to some terms while replacing some with others. Then we calculate the ratio of these two values for each position transition to examine its change through-out the session. From Fig. 4, we find that ratio is relatively low at the first step of reformulation, i.e., repeating terms at the first step of reformulation is more dependent to the clicked result of the first query than that at other steps of reformulation.

Next, we calculate the probability of previous query caus-ing clicks, given that a query pair contains common terms or not, respectively. We find that if the query pairs con-tain some identical terms, the probability that the previous query causes clicks is 43 . 4%. If the query pairs do not con-tain any term in common, the probability that the previous query causes clicks is 57 . 7%. We also calculate the ratio of the two probabilities for each position transition throughout the whole sessions. The results indicate that the dependence between repeating terms and clicks on the previous query decreases when the number of reformulations increases.
Previous statistical analyses show that there are several hidden reformulation patterns users might potentially fol-low when interacting with a search engine. Such findings provide useful clues for query completion methods to more precisely predict what the user is typing. For example, Fig. 1 shows that the number of query terms varies according to how many reformulations the user has performed. Thus, query length may change for different position numbers. In general, a longer query would be recommended in the be-ginning of the session. Figures 2 and 3 explain how the users choose the terms for a new query. Their decisions to term-adding, term-keeping and term-removing are often re-lated to recent queries, instead of the original query, i.e., the first query. To some extent, such decisions also determine the similarity between consecutive queries. Figure 4 further tells us that whether a query causes clicks or not affects if its terms will be reused later. In general, the terms appear-ing in the recent queries that do not cause any clicks would have a better chance to be included in the new query. Over-all, these reformulation patterns carefully capture how the terms are used and how the term-usage is related to session length, query length, number of reformulations and click-through data, which show their good potential for boosting the prediction performance in query completion.
Formally, a search session is defined as a sequence of queries h q , q 2 ,  X   X   X  , q T i issued by a single user within a time inter-val. A query is composed of a set of terms S ( q i ). Each query q i has corresponding timestamp t i (the time issued) and click information c i (the number of clicks for q i ). Sup-pose a user intends to enter q T but only inputs x in the search box, where x is a prefix of q T . h q 1 , q 2 ,  X   X   X  , q regarded as the query context. We want to predict q T for query completion. Assume there is a set of candidates (or candidate queries) Q T = q  X  j that are probably issued after q T  X  1 . Such candidate set can be collected in different ways. For example, q  X  j can be the query following q T  X  1 or the query following q i ( i  X  { 1 ,  X   X   X  , T  X  1 } ) in previous search sessions (i.e., the query logs). Note that x should be a prefix of q  X  Given a search session with T  X  1 queries and its candi-date set of queries Q T = q  X  j , our goal is to give a ranking to each query q  X  j  X  Q T so that the suggested queries with higher rankings may be more likely to be q T . In other words, we would like to estimate a probabilistic-like score positively correlated to the probability P ( q T = q  X  j | h q 1 , q
We propose a supervised approach here. Three kinds of reformulation-related features that possibly affect the pre-diction accuracy are taken into account. These features capture how the user changes preceding queries along the session. Considering different levels of query reformulation, we divide them into three categories: term-level features, query-level features, and session-level features, as shown in Table 2. There are 43 features in total. With the train-ing data, collected from the real search engine log, almost any learning-to-rank algorithm can be applied to obtain our ranking model. We choose LambdaMART [7] because Lamb-Category Feature Class Description Formulas i =1 S ( q i ) | , | S ( q T  X  1 )  X  S ( q T ) | i =1 S ( q i ) | , | S ( q T  X  1 )  X  S ( q T ) | , sgn( | S ( q used ( q T ) | / | S ( q T ) | , 1  X  | S used ( q T ) | / | S ( q sim sim eff ( q T ) | /T , | C eff ( q T ) | / | S ( q T ) | , | C daMART is a boosted version of the L ambdaRank algorithm improving overall ranking performance [7].
A query q i in a session consists of a set of terms S ( q Users might add or remove terms between two consecutive queries. The term-level features try to measure the effective-ness of a term used in a query. The effectiveness of a term can be explained in several ways such as the probability to add the term, delete the term or re-use the term, and the probability to cause clicks on the search results.
Term Combination : For a query q i and its preceding query q i  X  1 in a session, there are four possible term-usage ways: (1) adding terms only; (2) removing terms only; (3) adding terms and removing terms concurrently while some terms are kept; and (4) adding terms and removing terms concurrently while no term is kept. We encode types of term-usage as categorical features (i.e., four binary features). In Table 2, sgn( x ) is a function calculating the binary features. If x &gt; 0, then sgn( x ) = 1. If x = 0, then sgn( x ) = 0.
Set operations among term sets S ( q i ) may measure some statistical information. The union of terms learns how many different terms are used throughout the session, which may imply how much information conveyed by user information need. The intersection of terms learns how many terms kept by the user, reflecting how much unchanged information dur-ing the session. Besides, it is expected that a term is likely to be used again if it has been used recently in the session. We capture such feature by measuring how often the user reuses previous terms when reformulating a new one. In Ta-ble 2, we remove those terms not appearing in previous T  X  1 queries from S ( q T ) to form S used ( q T ). For every term in q , function Rep ( q T ) counts its times of occurrence in pre-vious t  X  1 queries. These counts are summed up and then normalized by the length of the session or the number of terms in q T .

Note that from the analyses in Section 3.2,  X  X aximum term repeat X  is highly related to the term-level features. Larger numbers of  X  X aximum term repeat X  could increase the size of intersection set. Smaller numbers of  X  X aximum term repeat X  could increase the size of union set.
These features are about how users reformulate queries without the consideration of what terms are used.
Query Similarity : As each reformulated query may be syntactically similar to its preceding query under the same information need, the similarity between queries could be helpful in determining whether a query is a suitable reformu-lation of another. In Table 2, term-based cosine similarity (denoted as Sim cos ( q i , q j )) and Levenshtein distance (de-noted as Sim Lev ( q i , q j )) [24] are used as similarity metrics, which measure similarity based on term-and character-level representations, respectively. First we compute the Leven-shtein distance between q T and its preceding query q T  X  1 and then we compute the average similarity of the query pairs in the whole session in two ways: (1) the average sim-ilarity between q T and each of the previous T  X  1 queries; (2) the average similarity of consecutive T  X  1 query pairs. At last we calculate the ratio of the similarity between the reformulated query and its preceding one to the average sim-ilarity mentioned earlier.

Query Length : Under the same search intent, we assume that the number of terms used in a query throughout a ses-sion does not change rapidly. We compute average number of terms in a query for a session. The ratio of term number in q T to that in previous T  X  1 queries is calculated in order to show the degree of change in the number of terms, that is, the trend of term numbers. The larger ratio value means that the last query is much longer than average length of previous queries.

Query Frequency : Search logs can provide useful infor-mation such as the co-occurrence of queries that benefits query suggestion. Here we take the relative frequency of ( q
T  X  1 , q T ) to all the queries immediately following q T  X  1 preceding q T , respectively, as features.
The session-level features capture how users perform re-formulations along the sessions without considering what queries or terms they exactly submit.

Position Number : Since a session is a sequence of queries, the position of a query in the session determines how many times a user has reformulated the queries. Our analyses given in Section 3.1 show that users X  reformulation strate-gies may change over different positions. The number of position is, therefore, adopted as a feature in our work.
Click-through Data : From Fig. 4, we know that click information in a session is related to term-usage. If a new term is added to a query and the new query does cause user clicks on the search results, then the term is more likely to be effective. With the click-through data, we can calculate how many search results of a query are clicked by the user. In Table 2, c i is the number of clicks in position i . For each term in q T , if it has been used in some of the previous queries, we count the number of clicks on the search results of that query and then sum up these counts by C eff ( q T is further normalized by the session length T , the number of terms in q T , and the number of terms in q T that have appeared previously S used ( q T ), respectively.
Time Duration : The duration of time users stay on the search results for a query might affect how they reformulate next query. The features include the average time difference of T  X  1 pairs of consecutive queries, and the ratio of the time difference between q T  X  1 and q T to the average time difference of the previous T  X  2 pairs.
In this section, we conduct extensive experiments on a real, large-scale dataset to verify the performance of our ranking model in predicting the user X  X  intended query.
Our experimental data comprises all of the queries sub-mitted to a commercial search engine from 1 May, 2013 to 7 May, 2013. The query log is first segmented into sessions with a 30-minute threshold as the session boundary. We drop those queries that are misspelled or appear less than 10 times in the whole log. We then discard the sessions that contain only one query since context-aware methods need at least one preceding query as the context. After removing rare queries and single-query sessions, there are 63,661,591 sessions and 5,197,821 unique queries in total. For each ses-sion with T queries, we treat the last query q T as the ground truth, that is, the intended query we want to predict. The preceding queries h q 1 , q 2 ,  X   X   X  q T  X  1 i and their click-through information serve as the context.

The cleaned data is further partitioned into two sets. The first 4-day data is used for training. The remaining 3-day data is for testing. Finally, we collect 35,926,476 sessions submitted before 5 May, 2013 as our training set. We use query frequency to generate candidate set. After filtering out those not starting from the prefix, the top-10 queries ranked by frequency form our candidate queries. For exam-ple, if the prefix is a , the candidates are the top 10 high-frequency queries starting with a in the training set. To get the training set, we remove the sessions whose ground truths (or answers) are not included in the corresponding candidate sets; that is, we guarantee the candidate set con-tains the answer. After filtering, there are 4,900,363 sessions in the testing set. The prefix is set to be the first character of q T . Note that most of our settings are consistent with previous work, including the removal of rare queries [29], the way to generate candidates [29], how to determine the prefix [2], and the dropping of the cases with no answers in the candidate set [29].

To evaluate performance on sessions with different lengths, the testing sessions are divided into three datasets, includ-ing  X  X hort Sessions X  (2 queries),  X  X edium Sessions X  (3 to 4 queries) and  X  X ong Sessions X  (5 or more queries). Besides, we tune our LambdaMART model with parameters of 1,000 decision trees across all experiments.
To compare our approach with others, the following con-ventional methods are adopted as the baselines:
Wi th the ground truth q T , we can evaluate the quality of query prediction by two metrics, including mean reciprocal rank (MRR) and success rate at top-k (SR @ k ). MRR is the multiplicative inverse of the rank of the actual query q in the ranking list. Given the testing set S , the MRR [2] for an algorithm A is defined as follows: whe re C represents the context of a session (i.e., the preced-ing queries and click-through information); q T is the ground truth. The function hitrank ( A, C, q T ) computes the posi-tion of the ground truth in the ordered list ranked by the algorithm A . The success rate at top-k (SR @ k ) denotes the average percentage of the actual queries that can be found in the top-k queries over the testing data. Both of them are widely used for the task whose ground truth is only one instance such as query completion. Table 3 shows the experimental results in MRR and SR@ k . Comparing the baseline methods, we find that all of previ-ous methods perform better than frequency-based MPC in most cases. The difference in MRR and SR@ k between the Hyb.C method and the MPC method is small for short ses-sions. The reason might be because the shorter sessions cannot provide sufficient context information so that the Hyb.C method fails to match the query to the context. On the other hand, while the length of a session increases, the Hyb.C method performs better accordingly. The QVMM method outperforms the Hyb.C method since QVMM mod-els query transitions, rather than the similarity between two consecutive queries. It is not necessary for two semantically-relevant queries to have strong query dependency in search. Similarly, although CACB can deal with the problems of sparseness and new queries by grouping relevant queries into a cluster, its performance still approximates to that of the QVMM method. It is inferred that if a query is conceptually relevant to the context, it is not necessary to be what the user intends to type. Hence, the solutions to query sugges-tion cannot be fully applied to query completion.
Our approach achieves the best performance (compared with all of the 4 comparative baselines) and significantly out-performs MPC with progress by 9.84% to 36.27% of SR@ k . It is worth noticing that our approach consistently performs better when the lengths of query sessions increase. More context is, therefore, preferable in query prediction. We have performed significant tests for the improvements of our ap-proach against MPC using a paired t-test with a significant level of 95%.
To verify the effectiveness of the features, we inspect cor-relation between the features and P ( q T | h q 1  X   X   X  q standard measurement, Kendall X  X  tau, is adopted.

Figure 5 shows the absolute values of correlation for all of the 43 features. It can be found that all of the three types of features, namely term-level, query-level and session-level features, are quite useful. Obviously, the query-frequency feature is the most significant one. The feature measures the dependence between consecutive queries. It explains why the conventional query prediction (or suggestion) methods based [16] on query dependence generally have reasonable performance. Besides, the feature of query length also has strong connection to query prediction.

Many term-level features are helpful in determining how a user reformulates a query in term level such as the union of the terms in a session, if a term stays in the reformulated query (term-keeping), if a term has ever appeared or not in preceding queries (number of used terms and number of repeat times), and the cosine similarity or the Levenshtein Kendall distance between two queries (especially between the latest two queries).

In the session-level features, the position of a query in a session is found to be highly related. This observation im-plies that how users reformulate their queries depends on how many queries they have submitted before. The click-through data like the number of clicks for a query has moder-ate correlation with reformulated queries. The information about time duration is quite important. This is because the duration of time a user stay on the search result is often related to other features such as the number of clicks, the session length, and the complexity of reformulating a query.
In this paper, we propose a supervised approach for query completion aiming to predict the user X  X  intended query with a partial input. Compared to conventional context-aware methods that directly model the term similarities or the query dependencies, our approach tries to learn how users change preceding queries, i.e., user reformulation behavior, along query sessions. The results of extensive experiments show that our methods can significantly improve the per-formance of the existing context-aware query completion and suggestion methods. Such improvement is consistent across different datasets with different session lengths. This is because (1) The reformulation-based model requires less training data. The number of reformulation features is only dozens or hundreds; (2) The reformulation-based model con-siders different user behavior for query reformulation. To capture such reformulation behavior, several kinds of fea-tures are taken into account such as session-related, query-related, and term-related features. The results of both the user behavior analysis in Section 3 and the feature analy-sis in Section 5.5 support that all of three-type features are useful and important. Based on the analyses and experi-mental results, we give an insight into the problems of how users reformulate their queries and why the reformulation-based features are helpful in query completion. Our future work includes the incorporation of semantic features into the model.
We would like to thank Dr. Chin-Yew Lin for many valuable suggestions, and the anonymous reviewers for their helpful comments. This work was partially sponsored by the Ministry of Science and Technology, Taiwan, under Grant 102-2221-E-002-156. [1] J. Akahani, K. Hiramatsu, and K. Kogure.
 [2] Z. Bar-Yossef and N. Kraus. Context-sensitive query [3] H. Bast and I. Weber. Type less, find more: fast [4] S. Bhatia, D. Majumdar, and P. Mitra. Query [5] P. Boldi, F. Bonchi, C. Castillo, D. Donato, and [6] P. Boldi, F. Bonchi, C. Castillo, and S. Vigna. From [7] C. J. Burges, K. M. Svore, P. N. Bennett, [8] H. Cao, D. Hu, D. Shen, D. Jiang, J. Sun, E. Chen, [9] H. Cao, D. Jiang, J. Pei, E. Chen, and H. Li. Towards [10] H. Cao, D. Jiang, J. Pei, Q. He, Z. Liao, E. Chen, and [11] S. Chaudhuri and R. Kaushik. Extending [12] V. Dang and B. Croft. Query reformulation using [13] B. Fonseca, P. Golgher, B. P X ossas, B. Ribeiro-Neto, [14] B. M. Fonseca, P. B. Golgher, E. S. de Moura, [15] J. Guo, X. Cheng, G. Xu, and X. Zhu. Intent-aware [16] Q. He, D. Jiang, Z. Liao, S. C. H. Hoi, K. Chang, [17] C.-K. Huang, L.-F. Chien, and Y.-J. Oyang. Relevant [18] J. Huang and E. N. Efthimiadis. Analyzing and [19] B. J. Jansen, D. L. Booth, and A. Spink. Patterns of [20] D. Jiang, K. W.-T. Leung, and W. Ng. Context-aware [21] R. Jones and D. C. Fain. Query word deletion [22] R. Jones, B. Rey, O. Madani, and W. Greiner. [23] C.-J. Lee, R.-C. Chen, S.-H. Kao, and P.-J. Cheng. A [24] V. I. Levenshtein. Binary codes capable of correcting [25] Z. Liao, D. Jiang, E. Chen, J. Pei, H. Cao, and H. Li. [26] Q. Mei, D. Zhou, and K. Church. Query suggestion [27] U. Ozertem, O. Chapelle, P. Donmez, and [28] R. L. Santos, C. Macdonald, and I. Ounis. Learning to [29] M. Shokouhi. Learning to personalize query [30] M. Shokouhi and K. Radinsky. Time-sensitive query [31] Y. Song, D. Zhou, and L. He. Query suggestion by [32] A. Strizhevskaya, A. Baytin, I. Galinskaya, and [33] I. Weber and C. Castillo. The demographics of web [34] R. W. White, P. N. Bennett, and S. T. Dumais. [35] R. W. White and G. Marchionini. Examining the [36] W. Wu, H. Li, and J. Xu. Learning query and [37] B. Xiang, D. Jiang, J. Pei, X. Sun, E. Chen, and
