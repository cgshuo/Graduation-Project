 Sunita Sarawagi sunita@iitb.ac.in Rahul Gupta grahul@cse.iitb.ac.in IIT Bombay, India The max-margin framework for training structured prediction models generalizes the benefits of support vector machines (SVMs) to predicting complex ob-jects. A popular member of this framework is the margin scaling method (Tsochantaridis et al., 2005; Taskar, 2004; LeCun et al., 2006; Crammer &amp; Singer, 2003) that tries to ensure that the score of the cor-rect prediction is separated from the score of an in-correct prediction by a margin equal to the error of the prediction. This method has been used exten-sively in many applications, including sequence label-ing (Taskar, 2004; Tsochantaridis et al., 2005), im-age segmentation (Taskar, 2004; Ratliff et al., 2007), grammar parsing (Taskar et al., 2004), dependency parsing (McDonald et al., 2005b), bipartite match-ing (Taskar, 2004) and text segmentation (McDonald et al., 2005a). A reason for its wide-spread use is that it can exploit the decomposability of the error function to find the most violating constraint using the maxi-mum a-posteriori (MAP) inference algorithm used for prediction.
 An alternative formulation (Tsochantaridis et al., 2005) is to ensure that all labelings are separated by a fixed margin of one but penalize violations in propor-tion to their errors. This method, called slack scaling, generally provides higher accuracy than margin scal-ing which gives too much importance to labelings with large errors even after they are well-separated, some-times at the expense of instances that are not even separated. Another shortcoming of margin scaling is that it requires an error function that is linearly com-parable with the feature values, whereas slack scaling is invariant to scaling of the error function. In spite of the advantages, slack scaling is not popular because it requires inferring the labeling which maximizes a non-decomposable metric  X  difference of score and error inverse.
 In this paper we make two contributions in max-margin training of structured models.
 First, we address the computational challenge of infer-ing the labelings required when training via slack scal-ing. We propose a variational approximation of the slack loss so that the most violating labeling is found using the same loss augmented MAP inference as in margin scaling. We demonstrate that accuracy-wise our slack approximation is much better than margin scaling and close to the more expensive slack scaling. Second, we propose a new max-margin framework for training models with decomposable error functions that, like the slack scaling method, is scale invari-ant and discounts labelings well-separated from the margin. The inference step it requires is much sim-pler than required by slack scaling. In particular, for Markov models we show that the inference of the most violating labelings is only a factor of two more expen-sive than in margin scaling. The basic idea of the new learner, that we call PosLearn, is to associate a different slack variable for each error position of a de-composable error function. We show that this leads to a better characterization of the loss than both the slack and margin scaling methods that define loss in terms of a single most violating labeling. Empirically, PosLearn reduces error by up to 25% over margin scal-ing and 10% over slack scaling in various tasks. We consider structured prediction problems that as-sociate a score s( x , y ) for each output y  X  Y of an input x , and predict the output y  X  with maximum score. The scoring function s( x , y ) is a dot product of a feature vector f ( x , y ) defined jointly over the in-put x and output y , and the corresponding param-eter vector w . The space of possible outputs Y can be exponentially large. Thus, efficient solutions for y composability of the feature vector f over components of y . During training the goal is to find a w using a set of labeled input-output pairs ( x i , y i ) : i = 1 . . . N so as to minimize prediction error. The error of pre-dicting y for an instance x i whose correct label is y i is user-provided. We denote it by L i ( y ). In max-margin methods, the training goal is translated to finding a w that minimizes the sum of the loss on the labeled data while imposing a regularization penalty for overfitting. The loss is a computationally convenient combination of the user-provided error function and feature-derived scores so as to both minimize training error and max-imize the margin between correct and incorrect out-puts. There are two popular loss functions for struc-tured learning tasks: margin scaler and slack scaler. We review them briefly. 2.1. Margin Scaling In margin scaling, the goal is to find w such that the difference in score w T  X  f i ( y ) = w T f ( x i , y w
T f ( x i , y ) of the correct output y i from an incorrect labeling y is at least L i ( y ). This is formulated as: Two category of methods have been proposed to opti-mize the above QP. The first category is based on the cutting plane algorithm to avoid generating the expo-nentially many constraints. This involves incremen-tally finding the output y M = argmax L ( y )) which most violates the constraint. y M can be found using the same inference algorithm as MAP y variable subsets no larger than the subsets in the de-composition of f ( x i , y ). This category includes exact gradient ascent methods (Tsochantaridis et al., 2005), stochastic gradient methods (Bordes et al., 2007) and online sub-gradient methods (Ratliff et al., 2007). The online structured learning methods of (Crammer &amp; Singer, 2003) follow a perceptron based framework but their constraints are identical to the margin scaling method described here. The second category of meth-ods (Taskar, 2004; Taskar et al., 2006) exploit the de-composability of the error function to create a com-bined program for the inference and parameter learn-ing task. 2.2. Slack Scaling Slack scaling demands a margin of one but scales the slacks of violating outputs in proportion to their errors. The corresponding optimization problem is: The optimization of the above QP via the cutting plane algorithm requires the inference of the labeling y gin scaling, even with decomposable loss and scoring functions, it is not easy to find y S efficiently. For this reason, the slack scaling approach is not popular. However, the slack loss is in many ways better behaved than margin loss (Tsochantaridis et al., 2005). Margin scaling gives too much importance to instances which are already well-separated from the margin. This hurts because the loss  X  i is determined by a single most vio-lating labeling. If a labeling imposes a difficult margin requirement because of its large error, the optimizer will appropriately increase  X  i . After that, there is no incentive to improving separability of any other label-ing of that instance. In contrast, the slack scaling loss will ignore instances that are separated by a margin of 1, and  X  i is determined by labelings that matter be-cause of their being close to the margin. Empirically, we found slack scaling to give better accuracy than margin scaling (Section 5). Slack scaling also makes it convenient for an end-user to define an error func-tion and a feature vector and tune C because the error function can be arbitrarily scaled vis-a-vis the feature vector. We present a variational approximation to the slack in-ference problem that is applicable for any structured model for which we can only solve for the MAP effi-ciently. The slack inference problem is to find where s i ( y ) = w T f ( x i , y ), Y = { y : y 6 = y i , s We approximate y S with another labeling y A . Our ap-proximation is based on the observation that s i ( y )  X  ( y ) is concave in L i ( y ) and its variational ap-proximation can be written as a linear function of L ( y ) (Jordan et al., 1999). Here on, we drop the subscript i wherever possible.
 Claim 3.1. s ( y )  X   X  L ( Proof. Any concave function f ( z ) can be expressed as the conjugate function of f ( z ). The result follows from the fact that the conjugate function of  X   X  z is 2 Let F 0 ( y ;  X  ) , s( y ) +  X L ( y )  X  2 We now approximate the exact slack MAP objective with an upper bound as follows: For a fixed  X  , we can compute F (  X  ) using the loss aug-mented MAP algorithm employed in margin scaling to first find y  X  = argmax ting F (  X  ) = F 0 ( y  X  ;  X  ). The constraint y 6 = y i be met by asking the loss augmented MAP algorithm to return top two MAPs. The algorithmic extension to return top two MAPs is straight forward in many structured tasks.
 We search for the  X  for which the upper bound F (  X  ) is minimized by exploiting the fact that F (  X  ) is convex in  X  .
 Claim 3.2. F (  X  ) is convex in  X  .
 Proof. It can be seen that F 0 ( y ;  X  ) is convex in  X  . Since F (  X  ) is a max of finitely many convex functions, and max is also convex, F (  X  ) is convex.
 We can compute min  X   X  0 F (  X  ) using efficient line search algorithms such as Golden Search. During the search phase, for each  X  that we encounter, we evaluate F (  X  ) and thus get one labeling. Of all these labelings, we return the one with the highest s( y )  X   X  L ( We show in the next section the range [  X  l ,  X  u ] within which it is sufficient to perform the line search. 3.1. Upper and Lower Bounds for  X  Since  X   X  0, we can use  X  l = 0 as the lower limit. However, with  X  = 0, F 0 ( y ;  X  ) is not able to distin-guish between high and loss labelings with same scores commonly seen in early training iterations. It can be shown that with  X  l = L mum possible loss, F (  X  ) for any  X  &lt;  X  l will not return any violating labeling with slack score more than of the score of a labeling returned with  X   X   X  l . By set-ting to the tolerance of the cutting-plane algorithm, we get a provably correct lower bound.
 For the upper bound, it is sufficient to pick a  X  u such that for any  X   X   X  u , either F (  X  ) gets the same vio-lator as F (  X  u ) or a non-violator that we are not in-terested in. It is sufficient to pick a  X  u such that argmax among all violators in Y . Hence we need: Let y 1 , argmax ference between two distinct loss values (e.g. L  X  = 1 for Hamming loss). Then the right side can be atmost s( y 1 )+  X  u ( L ( y 0 )  X  L  X  ). So we require  X  u  X  s( Now, y 0  X  Y  X  s( y 0 )  X  s( y i )  X  1 +  X  L ( s( y i )  X  1 +  X  L 3.2. Limitation of Approximate Slack In the worst case, it is possible that the exact slack MAP y S violates the inequality but y A does not, as we show next.
 Claim 3.3. s ( y A )  X   X  L ( Proof. We prove the claim with a counter example. Let y j , j = 1 , 2 , 3 be three labelings with scores s j  X  2 ,  X  s 2 &gt; s 3 and L 1 &lt; L 2 &lt; L 3 . Let the score of the true labeling be s = 0, the slack be  X  = 19 36 , and let  X  0. By computing sgn( s j  X   X  L that labelings y 1 and y 3 are not violators but y 2 is. In order to return y 2 as the worst violator, there must exist  X  such that s 2 +  X L 2  X  s j +  X L j , j = 1 , 3. This translates to the constraints  X  &gt; 2 9 and  X  &lt; 1 9 , which are infeasible.
 The above counter example showed that it is impossi-ble to approximate the slack scaled constraint by any method that depends on finding MAP with varying weights on error. This limitation though seemingly restrictive, only slightly hampers the performance in practice, as evident in our experimental results. We next propose a new formulation for max-margin training that directly exposes the decomposability of the error function so as to require solving a consid-erably simpler inference problem. We show that this new formulation not only addresses the computational problem of slack scaling inference, but also provides a more accurate characterization of the loss of scoring functions.
 The basic premise of the new learner, which we call PosLearn, is that when error is additive over a set of positions, the loss should also additively reflect margin violations at each possible error position. This is in contrast to both the margin and slack scaling where loss is in terms of a single most violating labeling. Let L i ( y ) = P c  X  C L i,c ( y c ) denote a decomposition of the error function. Our goal during training is to en-sure that at each possible error position c , the correct labeling has a margin over all labelings where c is in-correctly labeled. If not, we add a hinge loss on the difference in score between the correct labeling y i and the best labeling argmax at c . This yields the following constrained optimiza-tion In the above program, the number of slack variables is equal to the total number of error positions over all instances. Otherwise, the form of the QP is the same as in Section 2.2 and therefore can be solved via similar cutting plane algorithms. For a given position c of an instance i , the most violating constraint is the labeling This inference problem can be solved efficiently by any structured learning task in which MAP can be found efficiently since max where the outer max is over a small number of values as the size of c is typically small and the inner max is MAP inference with label of c constrained to y c . The MAPs y P : c will typically be evaluated simultaneously for each c . In many structured learning tasks, all these MAPs can be found in just twice the amount of time it takes to compute a single unrestricted MAP, as we show in Section 4.3.1.
 In addition to these computational advantages, PosLearn also provides better loss characterization than slack scaling. 4.1. Comparison with Slack Scaling First, we claim that the PosLearn loss is an upper bound of the slack loss.
 Claim 4.1. The slack loss P i max y L i ( y )[1  X  w T  X  f i ( y )] + is upper bounded by the PosLearn loss P Proof. Let y S = argmax
L i ( y S )[1  X  w T  X  f i ( y S )] + Next, we show that slack scaling by defining the total loss in terms of a single most violating labeling, can-not discriminate amongst scoring functions as well as the PosLearn loss that involves different labelings at different error positions.
 Consider one example where w is such that three la-belings y 0 = [0 0 0 0] , y 1 = [1 1 0 0] , y 2 = [0 0 1 0], all have the same score of 1. Let y 0 be the correct labeling, then L ( y 1 ) = 2 , L ( y 2 ) = 1, assuming Ham-ming error. Let the score of all remaining labelings be 0. The total slack loss in this case is 2 whereas the PosLearn loss is 3. Now consider the case where y 2 has score 0. The slack loss remains unchanged whereas PosLearn loss reduces to 2.
 An important consequence of the reduced error cover-age is that, when the cutting plane algorithm termi-nates in slack scaling, PosLearn could continue to find violating constraints. The reverse is not true. 4.2. Comparison with M 3 N Training The PosLearn program appears similar to the M 3 N program of (Taskar, 2004) because both decompose the slack variable over multiple positions. However, the similarity is only superficial. The training objec-tive of M 3 N is Margin scaling and the position spe-cific slack variables are for integrating training with inference for loss augmented MAP. In PosLearn the position specific slacks lead to a very different training objective. 4.3. Common Decomposable Error Functions We show examples of decomposable error functions in several structured learning tasks and show how to effi-ciently find the most violating constraints over all error positions simultaneously. 4.3.1. Markov Models Many structured prediction tasks can be modeled as Markov models. Popular examples are sequence label-ing for information extraction (Lafferty et al., 2001), and grid models for image segmentation (Taskar, 2004; Boykov et al., 2001). A natural error function here is Hamming loss that decomposes over the nodes of the Markov network. Typical MAP inference algorithms based on belief propagation also give max-marginals at each node. The max-marginals gives us at each (node c , label y ) pair, the best labeling y c : y with node c la-beled y . We can now find the most violating labeling at each position c via where L i,c ( y ) = 1 when y 6 = y i,c for Hamming loss. In general L i,c ( y ) can be any arbitrary real-value, for example a mis-classification matrix M ( y 0 , y ) could give the cost of misclassifying a y 0 node as y . 4.3.2. Segmentation The output space Y consists of all possible labeled seg-mentations of an input sequence x . A segmentation y consists of a sequence of segments s 1 . . . s p where each s j = ( t j , u j , y j ) with t j = segment start position, u = segment end position, and y j = segment label. Seg-mentation models have been proposed as alternative models for information extraction that allows for more effective use of entity-level features (McDonald et al., 2005a; Sarawagi &amp; Cohen, 2004).
 The feature vector decomposes over segments and is a function of the segment and the label of the pre-vious segment. Thus f ( x , y ) = P p j =1 f ( x , s j , y The error function also decomposes over segments as L ( y ) = P s  X  L (( t, u, y )) is defined as L (( t, u, y )) = where p y is the precision penalty of labeling a segment as y and r y 0 is the recall penalty of missing a true segment of label y 0 and M ( y 0 , y ) is the misclassification cost matrix applicable when the same span appears in both segmentations.
 The number of slack variables is the number of possible segment spans ( t, u ), which is O ( nm ) for a sequence of length n and maximum segment size m .
 The MAP segmentation can be found using an ex-tension of the Viterbi algorithm (Sarawagi &amp; Cohen, 2004). Viterbi also gives the highest scoring segmenta-tion of the sequence from 1 to i with the last segment ending at i with label y for all possible i and y . Call this  X  ( i, y ). Similarly, we can use a backward Viterbi pass to get  X  ( i, y ) the highest scoring segmentation from i + 1 to n with label y on the segment ending at i . These can be combined to find the most vio-lating constraint for a slack variable corresponding to segment ( t, u ) as: 4.3.3. Unlabeled Dependency Parsing In unlabeled dependency parsing, the goal is to assign each token to its  X  X ead X  token (or to a dummy token), such that the head links form a directed spanning tree. The feature vector for a tree y over a sentence x is de-composable over the edges (McDonald et al., 2005b): f ( x , y ) = P t f ( y t , x , t ) where t is a token and y its head. A natural error function for a dependency parse tree is then the number of words that are as-signed an incorrect head word. In this case, the error and features decompose in exactly the same way, over individual words. The only coupling amongst the pre-dictions of different words is that they need to form a tree.
 We use the combinatorial non-projective parsing al-gorithm of (McDonald et al., 2005b), which cannot be easily extended to simultaneously return MAP for each position. For PosLearn we return the worst vio-lator for each position by first finding the unrestricted MAP y  X  . Then, for each position where y  X  is cor-rect, we re-invoke MAP with the correct assignment disabled. In the worse case, this will lead to n MAP invocations. We present experimental results on three tasks  X  se-quence labeling, text segmentation and dependency parsing, performed on the following datasets and set-tings: CoNLL X 03: We use the English benchmark from the CoNLL X 03 shared task on named entity recognition. The corpus consists of train, development and test sets of  X  14000, 3200 and 3400 sentences respectively. We used exactly the same features as in the trained model from Stanford X  X  Named Entity Recognizer 1 .
 Cora: This is a database of  X  500 citations (McCal-lum et al., 2000), containing entities such as Author, Journal, Title, Year and Volume. We used standard extraction features defined over the neighborhoods of each token and the label of the previous token (Peng &amp; McCallum, 2004). For the segmentation task on this dataset, we also used the segment length feature. Address: This is a collection of  X  400 non-US postal addresses. Unlike US addresses, these addresses are highly irregular and relatively difficult to segment. The features for sequence labeling and segmentation tasks are as defined in (Sarawagi &amp; Cohen, 2004). CoNLL-X: We use the freely available treebanks for Swedish, Dutch and Danish from the CoNLL X Shared Task for unlabeled dependency parsing. The training sets contain  X  11000, 13350, and 5200 sentences re-spectively. We use the first-order features, the on-line MIRA trainer (Crammer &amp; Singer, 2003), and the non-projective parsing algorithm provided in the MSTParser package 2 . 5.1. Results Table 1 shows test errors (as defined in Section 4.3) and Span F1 (where ever applicable) of all four training approaches on all the tasks. For Cora and Address results are averaged over ten splits of 25% train  X  75% test, the rest are with the standard training and test files as available in the benchmark. For sequence and segmentation tasks, we are able to solve the Slack inference problem exactly using a quadratic algorithm that finds the MAP for each possible error value. For dependency parsing, it was not easy to find MAP with a pre-specified error. Hence, numbers for Slack scaling methods are missing for this task.
 Sequence labeling We note that the errors go down in the order Margin &gt; Slack &gt; ApproxSlack &gt; PosLearn, and PosLearn achieves  X  20% error reduction over Margin and 5-10% over Slack. The difference between PosLearn and Margin is statistically significant (p-value from paired t-test is &lt; 0.001), while that between ApproxSlack and Slack is not. This confirms that the approximations done in ApproxSlack are empirically good.
 We also reports the entity span F1 values in Table 1 (numbers after the  X / X ). PosLearn provides signifi-cant improvements over Margin for Cora and Address, going from 75 to 83 and 71 to 78 respectively. This shows that optimizing for the error directly translates to significantly better span F1 scores. For CoNLL X 03 the gains are modest both for error and Span F1 for reasons we will highlight in Section 5.2. Figure 1 in-vestigates the effect of increasing training size on the sequence labeling errors of all the approaches on Cora. PosLearn remains the best approach for all training sizes, with a 25% error reduction over Margin even for 75% training data. ApproxSlack and Slack are almost identical for all training sizes.
 Figure 2 compares the training time of the four ap-proaches on Cora over various training sizes. PosLearn and ApproxSlack turn out to be the cheapest of all the approaches. Two key observations here are (a) PosLearn is up to five times faster than Margin in spite of generating many more constraints, and (b) The training time of Margin reduces with an increase in data. These can be attributed to two reasons. First, PosLearn quickly generates a lot of relevant constraints and terminates in much fewer iterations, whereas Mar-gin spends too much time in separating high loss la-belings which are already far enough. Second, when data is scarce, Margin is not able to find good support vectors early on and takes many more iterations. This provides another empirical support for the recent ob-servations in (Bottou &amp; Bousquet, 2008) on the inverse dependence of training time on data sizes.
 Segmentation The results for segmentation are similar to sequence labeling. Again, PosLearn provides 7-10% decrease in error over Margin and Slack. ApproxSlack again turns out to be a close approximation to Slack.
 Unlabeled dependency parsing The difference between PosLearn and Margin turns out to be very insignificant in this case. We cannot evaluate Slack as its MAP inference algorithm is not feasible in this setting. Our discussion in the next section shows that we do not expect ApproxSlack to score over Margin either.
 Note our baseline numbers are competitive with the state of the art for these tasks. For Swedish and Dan-ish, the errors for Margin scaling are significantly lower than the average errors of the CoNLL X Shared Task participants  X  15.8% and 15.5% respectively . For Dutch, Margin scaling model is better than the best model in the Shared Task (error 16.4%). 5.2. Discussion We observed that Margin scaling was significantly worse than other loss functions for tasks like sequence labeling on the Address and Cora datasets, while being the highest performing on tasks like dependency pars-ing. We explain the reasons behind the varying gains of Margin relative to other loss functions, in particu-lar PosLearn, based on the decomposition of the error function compared to the feature function.
 We argue that margin scaling is a bad loss func-tion only when the model comprises of features that strongly couple larger subset of variables than the er-ror function. Consider the case when the feature func-tion decomposes over each position of y , exactly as in the error function. This is true for dependency pars-ing, and for sequence labeling models with no edge features. In such cases, a structured formulation adds little value, and a multi-class SVM with independent constraints over the local features and loss at each posi-tion, is just as adequate. The constraints of structured margin scaling turn out to be a summation of the con-straints of multi-class SVM and the two solve equiv-alent objectives as shown in (Joachims, 2006). Inter-estingly, (McDonald et al., 2005b) indeed finds that such a model (which they call the factored model) is very close to the structured model using margin scal-ing. We verify that for sequence labeling, if we disable all edge features, then for the Address dataset, span F1 drops from 71 to 62 and for Cora from 75 to 44 with Margin scaling. This indicates the strong impor-tance of structured features for these datasets. In con-trast, for CoNLL X 03 where Margin is competitive with PosLearn, removal of edge features causes only a small drop in Span F1, from 84.7 to 81. Without edge fea-tures, PosLearn shows little or negative improvement over Margin scaling for all three datasets.
 This indicates that in domains where the feature func-tion does not induce strong coupling amongst vari-ables, there is no reward in going beyond simple mar-gin scaling, and possibly even multiclass SVMs. In truly structured problems where features strongly cou-ple multiple variables, margin scaling gets adversely af-fected by the unnecessary margin requirements of high error labelings due to shared slack variables. PosLearn ignores labelings separated from the margin, and by defining per-position slacks instead of a single shared slack, handles such structured cases better. We presented an efficient variational approximation to the slack scaling approach, which only requires a slightly modified loss augmented MAP algorithm, in-stead of the inefficient slack scaling inference algo-rithm. We demonstrated that in practice it performs much better than margin scaling and closely approxi-mates slack scaling.
 Next, we argued that all existing approaches that de-fine loss in terms of a single most violating label-ing achieve inadequate separation from the correct la-beling. We proposed a new trainer, PosLearn that involves multiple labelings in trying to ensure max-margin separation at each possible error position in the structured output. The PosLearn constraints can be generated using only the MAP algorithm, and for many structured models the time required is no more than twice the time taken to find MAP. Empirically, this leads to significant error reduction over Margin scaling on structured models that induce strong cou-pling amongst output variables.
 A compelling future direction is theoretically analyz-ing the generalizability of PosLearn vis-a-vis other loss scaling methods.

