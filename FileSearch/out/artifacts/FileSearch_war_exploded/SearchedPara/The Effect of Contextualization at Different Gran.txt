 In the hierarchical XML structure, the ancestors form the context of an XML element. The process of taking element X  X  contextualization. The aim of this paper is to separate different granularity levels and test the effect of contextualization on these levels. H.3.3 [ Information Storage and Retrieval ]: Information search and retrieval  X  Retrieval models, Search process, Selection process Measurement, Performance, Experimentation Contextualization, Evaluation, Granularity level, XML Passage and XML retrieval allo w systems to provide fine grained access to documents, and thus only the most relevant parts of a document can be retrieved. In this kind of content-oriented XML the exact paths and names of the elements are not of interest. Similar to full document retrieval, in content-oriented XML retrieval the elements are typically organized according to their relevance ranking and provided to the user. This study investigates the effect of contextualization [1] (i.e. utilizing the elements ancestors ) in content-oriented XML retrieval. In particular, this st udy isolates granularities in an XML collection and explores how the contextualization affects the retrieval performance on different granularity levels. Context is an ambiguous term. In this paper the context of an element refers basically to the ancestor elements. As an illustration, an accustomed text document, such as a book, a newspaper article etc. has a basic hierarchical division, for example an article-section-subsec tion-paragraph division. This established division gives natural contexts of different size. For example, a paragraph can be viewed in the context of an article, a section, and possible subsections, i.e. the ancestors. In a collection with documents having such a hierarchy, the full documents of a collection belong to the same granularity level with each other. In the same way the paragraphs or sections belong (more or less) to the granularity level of their own. The context defines part of the semantics of the content of an element, and thus it may carry beneficial information for retrieval. Contextualization means mixing the evidence from an element and its context in matching. In ot her words, contextualization is a general re-scoring method, where the initial scores of matching elements are combined with their ancestors X  scores.[1] Accordingly, we introduce a general contextualization function. In the function we assume a basic scoring function Score ( q , e ). The argument q is the underlying query, e is the contextualized element, S is the set of the elements whose score is combined with the score of e , and w is a weight in terms of which the power of contextualization can be tuned. Generally, contextualization has been discovered successful in element matching [1,5,6]. However, there is a lack of more detailed research on how contextu alization affects different kind of elements. Hence, we present the following research questions: Does the effectiveness of contextualization vary at different granularity levels? Is th e effectiveness of the different contextualization scenarios dependent on the granularity level? As the core retrieval system, TRIX [1,3], is used to test the effect of contextualization. It was tested against the IEEE collection and a set of 29 content-only topics of the year 2005 [2]. As special cases in this study we investigate four contextualization scenarios, namely parent, root, 2xroot and tower contextualizations [1]. The scenarios for an element e are defined by parameterizing the ba sic scoring function with the following arguments. For the parent contextualization we set w= 1, and S ={ parent ( e )}, parent ( e ) yield the root and parent elements respectively, whereas ancestors ( e ) yields the set of all ancestor elements of the element e . The various XML retrieval eval uation metrics reward not only the matching, but also the selec tion of appropriate granularity level [4]. Here, instead, we have extracted three granularity levels from the collection in order to measure the effect of these contextualization scenarios for elements of different granularity, ACM 978-1-59593-991-3/08/10. and also to eliminate the effect of selecting elements of appropriate granulation in the results. The first level covers biggish elements, i.e. major s ections. The second level covers  X  X edium size X  elements, i.e. minor sections. The smallest level is the content element [3] level consisting of smallest referable units such as paragraphs, headings, list items etc. None of the granularity levels contains structurally overlapping elements. In addition, for completeness, the selection of elements has been completed so that each level covers all text content of the XML collection. Consequently, for the major section the average text length of an element is 4243 characters, for the minor section 2420 and for the content element 121 characters. The queries have been executed against each of the three granularity levels with TRIX. In order to evaluate these results, the INEX recall base has been filtered so that relevant elements belonging to each corresponding granularity level has been accepted. Thus, there exist totally three recall bases. For the recall bases we have applied bina ry relevance criteria. This all enables the usage of traditional IR evaluation metrics. Such an evaluation setting is novel in the field of the XML retrieval research. Test results in MAP (Mean Average Precision) values are given in Table 1. The results show th at contextualization is most effective when retrieval is focussed on content elements; the effects of contextualization diminish towards the major contextualization methods vary according to the granularity level. The tower and root contextualization methods are the most effective; tower especially for content elements, and root for minor and major elements. Table 1. MAP values for different c ontextualization methods at different granularity levels. (The statistical significance of the differences between the baseline and the contextualization methods was tested withthe non-pa rametric Friedman test.) *=p&lt;0.05 Comparison between the contextu alization methods shows some statistically significant differences although the differences in MAP are not striking. The best performing method (tower/root) outperforms parent and double root contextualization methods for content and major elements. For minor elements there are no significant differences between the methods. The results verify the fact, that contextualization benefits on using all levels of context on any element. Referring to the former studies [1,5,6] it is not su rprising that the results for large elements improve the most with the root level contextualization. However, the difference between various contextualization methods is small. This is most likely due to the lower number of context levels for major sections. Similarly, the experiments show that contextualization in general improves the effectiven ess most on deep and small elements. This is understandable while it is known that they possess scant textual evidence. Interestingly, utilizing the near context delivers better results than the root for the small elements. It seems like the root is not an enough focussed context for small elements. Instead context levels in between are more informative in this sense. This is rather intuitive, because the content of, for example, the pa ragraphs and headings is more related to the section rath er than the whole article Consequently, our CScore function gives a better base on finer grained investigation of the near context than former approaches based on sole ancestor elements [5,6]. This is because it is based on a general set of elements, which may include a deliberate selection of following and preceding siblings as a context, instead of atomic ancestors. Te sting this is a matter of an ongoing study. This study was funded by the Academy of Finland under grant number 115480. [1] Arvola, P., Junkkari, M., and Kek X l X inen, J. 2005. [2] INEX 2005 homepage, INEX 2005. Available at: [3] Kek X l X inen, J., Junkkari, M., Arvola, P., and Aalto, T. [4] Lalmas, M. and Tombros, A. 2007. Evaluating XML [5] Mass, Y., and Mandelbrod, M. 2005. Component ranking [6] Sigurbj X rnsson, B., Kamps, J ., and de Rijke, M. 2004. An 
