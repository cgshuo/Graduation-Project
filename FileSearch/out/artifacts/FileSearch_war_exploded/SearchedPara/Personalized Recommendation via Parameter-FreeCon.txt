 Personalized recommendation services have gained increas-ing popularity and attention in recent years as most useful information can be accessed online in real-time. Most on-line recommender systems try to address the information needs of users by virtue of both user and content informa-tion. Despite extensive recent advances, the problem of per-sonalized recommendation remains challenging for at least two reasons. First, the user and item repositories undergo frequent changes, which makes traditional recommendation algorithms ineffective. Second, the so-called cold-start prob-lem is difficult to address, as the information for learning a recommendation model is limited for new items or new users. Both challenges are formed by the dilemma of exploration and exploitation.

In this paper, we formulate personalized recommenda-tion as a contextual bandit problem to solve the explo-ration/exploitation dilemma. Specifically in our work, we propose a parameter-free bandit strategy, which employs a principled resampling approach called online bootstrap, to derive the distribution of estimated models in an on-line manner. Under the paradigm of probability matching, the proposed algorithm randomly samples a model from the derived distribution for every recommendation. Extensive empirical experiments on two real-world collections of web data (including online advertising and news recommenda-tion) demonstrate the effectiveness of the proposed algo-rithm in terms of the click-through rate. The experimental results also show that this proposed algorithm is robust in the cold-start situation, in which there is no sufficient data or knowledge to tune the parameters.
 Categories and Subject Descriptors: H.3.5[Information Systems]: On-line Information Services; I.2.6[Computing Methodologies]: Learning; H.2.8[Database Applications]: Data Mining Keywords: Recommender Systems;Personalization;Contextual Bandit;Probability Matching;Bootstrapping  X 
Personalized recommender systems promptly identify pop-ular items and tailor the content according to users X  interest. A user X  X  interest often evolves over time. The uncertain-ties of information need can only be captured via collecting users X  feedbacks in real time and adapting recommendation models to the interest changes. Further, a significant num-ber of users/items might be completely new to the system, that is, they may have no consumption history at all, which is known as the cold-start problem [28]. Such a setting ren-ders traditional recommendation approaches ineffective in providing reasonable recommendation results, as it is diffi-cult to learn the match between user preferences and items in a cold-start situation.

The aforementioned issues are often recognized as an ex-ploration/exploitation problem, in which we have to find a tradeoff between two competing goals: maximizing users X  satisfaction in a long run, while exploring uncertainties of user interests [3]. For instance, a news recommender should prompt breaking news to users while maintaining user pref-erences based on aging news stories. In practice, such a dilemma is often formulated as a contextual bandit prob-lem [35]. The problem setting consists of a series of trials. Each trial provides a context. An algorithm selects an arm to pull, and after pulling, it receives a reward. The reward is drawn from some unknown distribution determined by the selected arm with the context. The goal is to maximize the total received reward. In personalized recommendation, each trial is seen as a user visit. Every arm is an item (e.g., a news article or advertisement). Pulling an arm is recom-mending that item. A context is a set of user features. The reward is the user response (e.g., a click). Therefore, per-sonalized recommendation can be seen as an instance of the contextual bandit problem.

Typical solutions of the contextual bandit problem involve unguided exploration (e.g., -greedy [34], epoch-greedy [18]) and guided exploration (e.g., LinUCB [19], EXP4 [4]). Most of the existing algorithms require an input parameter to con-trol the importance of exploration, such as in greedy-based algorithms and  X  in LinUCB. In practice, however, it is of-ten difficult to determine the optimal value for the input parameter. The EXP4 algorithm adopts the exponential weighting technique, but it is computationally expensive es-pecially when the context is high-dimensional.

Another family of algorithms is probability matching [32], which randomly allocates the pulling opportunities accord-ing to the probability that an arm gives the largest expected reward 1 . Compared with other methods, the benefit of prob-ability matching is that the tradeoff between exploration and exploitation evolves with the learning process, rather than being arbitrarily set [8]. The pulling allocation is usually implemented by random sampling from the posterior distri-bution of Bayesian learning models [29, 21]. This strategy is also referred to as Thompson sampling or Bayesian bandits. It provides promising performance in many empirical stud-ies [13, 14, 29, 36]. However, an improper prior for Bayesian learning models can lead to imbalanced exploration and ex-ploitation and jeopardize the overall performance. More-over, in the cold-start situation, there is no enough data for tuning the prior or parameters.

This paper proposes a non-Bayesian implementation of the probability matching strategy. The key idea is to ap-ply the online bootstrap to maintain a collection of boot-strap replications for learning model coefficients. To make each recommendation decision, the model coefficient vec-tor is randomly drawn from these bootstrap replications, rather than the posterior distribution. One advantage of this method is that it does not require a prior or predefined parameters that can affect the tradeoff of exploration and exploitation. In summary, the contribution of our work is three-fold:
The rest of this paper is organized as follows. In Sec-tion 2, the preliminaries of our work are introduced, and the detailed algorithmic description is presented in Section 3. Extensive empirical evaluation results are reported in Sec-tion 4. Section 5 presents a brief summary of prior work relevant to bandit problems, probability matching and boot-strapping. Finally, Section 6 concludes the paper.
In this section, we briefly describe the online learning paradigm for the contextual bandit problem in the setting of personalized recommendation, and then discuss the frame-work of probability matching in solving the contextual ban-dit problem. Table 1 lists the important notations through-out the paper.
The traditional probability matching is based on the prob-ability of an arm having the largest reward, not the largest expected reward.

Personalized recommender systems recommend items (e.g., movies, news articles) to users based on the predicted users X  interests on these items. The user X  X  response helps the sys-tem improve their future interest prediction [1]. However, the response to particular items can only be available after these items are recommended. If the items are never shown to the users, the recommender systems cannot collect the response on these items. This problem can be naturally modeled as a contextual bandit problem [35].

Notation Description a ( i ) the i -th arm.

A the set of arms, A = { a (1) ,...,a ( k ) } . x t the context of the t -th trial, and repre-r t,a t the reward of pulling the arm a t in the t -th  X  r t,a t the expected reward of pulling the arm a t y t the observation received in the t -th trial,
D t the set of received observations from the
D ( i ) t the set of observations in D t that are re- X  n ( i ) t the number of observations in D ( i ) t . f ( x ,  X  ) the reward prediction function using the  X  a ( i ) the coefficient vector of the reward predic- X   X  L (  X  ; y ) the likelihood of y given  X  .

Formally, given a set of independent arms A , a contex-tual bandit algorithm makes a decision for each trial t = 1 , 2 ,...,n . For the t -th trial, the context is a feature vector x . The algorithm selects an arm a t  X  X  to pull. By pulling the arm a t , the algorithm receives a reward r t,a t , which is drawn from some unknown distribution determined by the arm a t with the context x t . The goal of the algorithm is to maximize the total received reward R = P n t =1 r t,a t . Con-textual bandit algorithms can make use of the past t trial data D t = { ( x 1 ,a 1 ,r 1 ,a 1 ), ..., ( x t ,a t ,r t,a decision for future trials [18], but the past data may not be sufficient for learning. Typically, in the t -th trial the algo-rithm first predicts the expected reward  X  r t,a for each arm a before making the decision. The expected reward is where  X  a is a vector of the unknown coefficients with respect to the arm a and f is a predefined prediction function. For gression model. By learning from the past observations D the unknown coefficients  X  a can be estimated.
Several recommendation algorithms consider both user and item information simultaneously, and represent the data as a feature-based user-item matrix. The recommendation can be achieved by utilizing feature-based matrix factoriza-tion techniques [9], and the goal is to fill the missing values in the matrix. Our problem setting is orthogonal to theirs as we expect that the total reward is maximized by running a series of trials.
Probability matching is a widely used decision strategy in k -armed bandit algorithms [8, 29]. In this strategy, the probability of pulling the arm a,a  X  A equals to the prob-ability that a has the largest expected reward. In the t -th trial, the probability of arm a ( i ) having the largest expected reward is where i = 1 ,...,k . The selected arm is a random sample drawn from Q ( i ). But Q ( i ) does not need to compute ex-plicitly. The algorithms usually draw a vector  X   X  a ( i ) probability distribution of  X  a ( i ) , i = 1 ,...,k , and select the In Thompson sampling [32], the probability distribution of  X  a ( i ) in the t -th trial is the posterior distribution, denoted as in D t  X  1 that are only obtained by pulling the arm a ( i ) It assumes that the unknown coefficient  X  a ( i ) follows a pre-defined probability model, e.g., a Gaussian model, where  X  and  X  are unknown parameters. Estimating Pr(  X  a ( i ) ) is to find  X  t ,  X  t such that N (  X  t ,  X  t ) and Pr(  X  a ( i ) close to each other. Sampling from the posterior distribution is similar to sampling from N (  X  t ,  X  t ).

Based on Thompson sampling, in the ( t + 1)-th trial, the sampling area of  X  a ( i ) is determined by the variance of Pr(  X  a ( i ) |D ( i ) t ). When t is not sufficiently large, Pr(  X  mainly depends on the prior Pr(  X  0 ), denoted by N (  X  0 In other words, if  X  0 is large, the algorithm performs more exploration; otherwise, it does less. Therefore  X  0 controls the tradeoff between exploration and exploitation in early trials. Moreover,  X   X  1 0 is the regularization weight for estimating  X  t . If  X  0 is too small, the sampling will only focus on a small area around  X  0 , but  X  0 may not be ac-curate. For instance, let f ( x t ,  X  a ) = 1 / (1 + exp(  X  x then the prediction model is a logistic regression model. Let D t = { ( x proximation [33], where  X  0 is the start point of  X  since we do not know the values from the data D ( i ) t in ad-vance, it is not easy to come up with  X   X  1 0 that is balanced with  X   X   X  1 .

Therefore, the given Pr(  X  0 ) dominates the balance of ex-ploration and exploitation in early trials. An improper esti-mation in earlier trials also affects later trials.  X  X ood X  arms might be underrated in earlier trials, and then would have few chances to be pulled later and be corrected. As a re-sult, the algorithm would take a long time to converge to the optimal estimation.
In this section, we present a non-Bayesian algorithm to implement the probability matching strategy. The basic idea is using the sampling distribution obtained by the bootstrap instead of the posterior distribution to sample the prediction model coefficients for each item. We first discuss an offline bootstrap method for solving the contextual bandit problem. Then, we present an online implementation of the bootstrap method along with an online optimization algorithm.
The bootstrap is a method to derive the distribution of an estimator by data resampling [10]. Instead of specifying a generative model for data generating process, it only uses the information from the observed data.

In the ( t + 1)-th trial, we have the previous t pulling ob-servations, D t . Let D ( i ) t denote the observations only from pulling the arm a ( i ) , i  X  X  1 ,...,k } . k is the number of arms. D than 30 observations [15]), i = 1 ,...,k , we randomly select an arm. When all D ( i ) t are sufficient large, for the ( t + 1)-th trial, given the context x t +1 , the offline bootstrap based contextual bandit algorithm has the following steps: 1 and step 2 are repeated for many times, we can have a collection of bootstrap replications of  X   X  a ( i ) , which approxi-mately represents the sampling distribution of  X   X  a ( i ) fore,  X   X  a ( i ) can be seen as a random sample drawn from the
Let L (  X  ; y ) denote the likelihood of an observation y by given  X  , where  X  is the unknown coefficient vector. Assum-ing the observations are i.i.d and the arms are independent, we have the following lemma.

Lemma 1. The offline bootstrap based contextual bandit algorithm implements the probability matching strategy.
Proof. In the ( t + 1)-th trial, given a context x t +1 , let a = arg max model coefficient vectors estimations of the k arms. Since these estimations vary with different observations,  X   X  a (1) are random variables. Then, a is a random variable and randomized by the joint random variable (  X   X  a (1) Based on probability matching , the selected arm should be a sample randomly drawn from the distribution Pr( a ).
Let  X  a ( i ) denote some unknown distribution for generating the observations by pulling arm a ( i ) , i = 1 ,...,k . Y are independent random variables following  X  a ( i ) . Let D D so in the t -th trial, where d is the dimensionality of the context.  X   X  a ( i ) domized by D ( i ) t . Since every observation in  X  D ( i ) from  X  a ( i ) and every random variable in D ( i ) t follows  X   X  D t is also a random sample of D bootstrap method,  X   X  a ( i ) = arg max
To sum up all k arms,  X   X  a (1) , ... ,  X   X  a ( k ) are random sam-be seen as a sample randomly drawn from Pr( a ).
In real recommender systems, each recommendation deci-sion must be made in real time. The algorithm cannot go through all previous observations to generate a bootstrap sample  X  D ( i ) t . On the other hand, the learning algorithm cannot estimate  X   X  a ( i ) utilizing the entire  X  D ( i ) our problem setting, we apply the online bootstrap method to solve the contextual bandit problem [23].
The basic idea of online bootstrap is to generate a random variable P j , which is the proportion of times the j -th obser-vation is picked to a bootstrap sample. Then in the online setting, when we receive the j -th observation, we know how many times this observation should appear in a bootstrap sample. After processing the j -th observation, we do not pick it again. Let D ( i ) t be the set of received observations by of elements. The elements in  X  D ( i ) t are randomly resampled from D ( i ) t with replacement. For each resampling, each el-ement in D ( i ) t has an identical probability of 1 /n picked. There are n ( i ) t independent chances of resampling. Therefore, P j  X  Binom( n ( i ) t , 1 /n ( i ) t ). When n binomial distribution is approximated to a Poisson distribu-tion Pois( n ( i ) t  X  1 /n ( i ) t ) = Pois(1) [23]. Then, P j does not depend on t .
The online learning algorithm processes every observation in a streaming manner. When a new observation is received, by online bootstrap, this observation should appear P j times in the bootstrap sample, where P j  X  Poisson(1). We in-voke the online learning algorithm to learn this observation P j times. Then, the learned model is approximated to the model that learns observations in a bootstrap sample offline.
In our proposed algorithm, we apply this idea with the stochastic gradient ascent algorithm for updating the esti-mation of each bootstrap replication. Let  X   X  a ( i ) be a current bootstrap replication of  X  a ( i ) .  X  z is the current learning rate for this bootstrap replication in the stochastic gradient as-cent algorithm, where z is the number of updated times. Usually,  X  z = 1 / received new observation. Based on online bootstrap, we draw a random integer p t from Poisson(1). The new boot-
If we only maintain one bootstrap sample for each arm, the decisions made for different trials may not be fully inde-pendent. Our solution is to maintain a collection of indepen-dent bootstrap samples for each arm at the same time. In every trial, we randomly select one of them to make the re-ward prediction. Let B be the number of bootstrap samples for each arm. There are B independent bootstrap repli-cations of  X   X  a ( i ) maintained for each a ( i )  X  A , denoted by B randomly select a bootstrap replication from B a ( i ) every bootstrap replication has an equal probability to be selected. Let  X   X  a ( i ) be the selected replication. Pr( represented by B a ( i ) . When B and n ( i ) t are sufficiently large, B a ( i ) provides an approximation of the sampling distribution of  X   X  a ( i ) [23, 26]. The details of the online bootstrap algo-rithm for contextual bandits are stated in Algorithm 1. Algorithm 1 OnlineBootstrapBandit 1: Receive the context x t . 2: for i = 1 ,...,k do 4: end for 6: Receive the reward r t . 8: for j = 1 ,...,B do 9: Draw p from Pois(1) 10: for z = 1 ,...,p do 14: end for 15: end for
In general, the time complexity of calculating a log-likelihood is O ( d ), where d is the dimensionality of context feature vec-Generating a Poisson random variable is O ( p t ), where p the generated value [17]. Eq.(1) has p t iterations, and hence updating one bootstrap estimation requires O ( p t  X  d + p O ( p t  X  d ). Based on the cumulative distribution function (CDF) of Poisson(1), Pr( p t  X  p ) = e  X  1 P p z =0 1 z ! ple, the probability of p t  X  3 is 0.981. Therefore, the time cost of updating one bootstrap estimation is O ( d ) with a high probability. There are k arms and B bootstrap replica-tions for each arm. As a result, T ( t ) = O ( Bkd ) with a high probability.

In practice, the larger the B , the better the sampling dis-tribution approximation. However, the memory cost and time cost will become significantly large. Thus, the choice of B depends on the actual computational power of the sys-tem. As the B bootstrap replications are independent, they can be easily implemented in a parallel system, where each computing node handles a few replications independently.
We verify the proposed algorithm on two real-world data sets, including news recommendation data (i.e., Yahoo! To-day News) and online advertising data (i.e., KDD Cup 2012, Track 2). We start with an introduction to these two data sets, and then describe the implementation of the baseline algorithms. Finally, we present experimental results of the proposed algorithm with comparison to the baselines.
Personalized news recommendation aims to display suit-able news articles on the web page for different users based on the prediction of their individual interests. The predic-tion model is usually built upon user feedbacks on displayed news. However, the feedbacks are only available when the news articles are displayed to the users. Therefore, the prob-lem of personalized news recommendation can be regarded as an instance of the contextual bandit problem.

The experimental data set is collected by Yahoo! Today module and published by Yahoo! research lab 2 . The news were randomly displayed on the Yahoo! Front Page from October 2nd, 2011 to October 16th, 2011. The data set contains 28,041,015 user visit events to the Today Module on Yahoo! Front Page. Each visit event is associated with the user X  X  information, e.g., age, gender, behavior targeting features, etc., represented by a binary feature vector of di-mension 136. This data set has been used for evaluating contextual bandit algorithms in other literatures [19, 8, 20]. 2 million user visit events are used in this evaluation.
Online advertising systems deliver relevant advertisements (ads) to individual users to maximize the click-though rate (CTR) of the displayed ads. Sponsored search is one typical instance of online advertising. Given a user profile and a set of search keywords, the search engine selects an ad (adver-tisement) to display in the search result page. In practice, a huge amount of new ads will be continually imported into the ad pool. The system has to display these new ads to users and then collects the feedbacks to improve the CTR prediction. Hence, the ad selection problem is an instance of the contextual bandit problem, where an arm is an ad, a http://webscope.sandbox.yahoo.com/catalog.php. trial is an ad impression for a search activity, the context is the user profile with the search keywords, and the reward is the click count of the user.

The experimental data set is collected by a search engine and published by KDD Cup 2012 3 . In this data set, each instance is an ad impression, which consists of the user pro-file, search keywords, displayed ad information and the click count. The user profile contains the user X  X  gender and age. In our work, the context is represented as a binary feature vector, each entry of which denotes whether a query token is contained in the search query or not. The user X  X  profile information is also appended to the context vector using the binary format. The dimension of the context feature for this data set is 1,070,866. 1 million user visit events are used in the experiments.
For evaluation purpose, we use the averaged reward as the metric, which is the total reward divided by the total number of trials, i.e., 1 n P n t =1 r t , where n is the number of trials. In the aforementioned data sets, the averaged reward is the overall CTR (click-through rate) of the corresponding items (news articles or ads). The higher the CTR, the better the performance. In the experiments, to avoid the leakage of business-sensitive information, we report the relative CTR, which is the overall CTR of an algorithm divided by the overall CTR of random selection.

To demonstrate the efficacy of our proposed approach, we implement the following algorithms as baselines: http://www.kddcup2012.org/c/kddcup2012-track2.
In the following experiments, the reward in a single rec-ommendation activity is the user click, which is a binary value. Therefore, logistic regression is applied as the learn-ing model. Since the contextual bandit algorithms are online algorithms, stochastic gradient ascent is used as the learning algorithm [7]. Notice that the algorithms digest the data in an online manner, and hence all the user visits in the data sets are used for the testing purpose.

Thompson sampling with logistic regression is described in [8]. The prediction function f ( x ,  X  ) = (1+exp(  X   X  The posterior distribution is obtained by Laplace approx-imation and the unknown coefficient vector  X  is assumed to be normally distributed [7]. Let N (  X  t ,  X  t ) denote the posterior distribution after receiving t observations. The estimated  X  t is the maximum a posteriori (MAP) estima-tion , which is learned by the stochastic gradient ascent method. The inverse of the estimated covariance  X   X  1  X  0 + P covariance, y t = f ( x t ,  X  t ), and x 1 ,..., x t are the context feature vectors [8].

As we mentioned in Section 2, to decrease the exploration in Thompson sampling, we can give a small variance to the prior. But when the variance of the prior is small, the weight for the regularization in the stochastic gradient ascent will be high. As a result, the learned  X  t would focus on a small area around the mean of the prior. To solve this conflict, we propose a variation of Thompson sampling, TSNR . In stochas-tic gradient ascent, it ignores the regularization by the prior in the learning step. The prior is only used to compute the variance of the posterior.
The experiments on the Yahoo! Today news is evaluated by the replayer method [20], which provides an unbiased of-fline evaluation by utilizing the historical logs. It shows that, for a testing algorithm, the CTR estimated by this replayer approaches the real CTR of the deployed online system if the items in historical user visits are random uniformly rec-ommended. Therefore, we apply the replayer to evaluate the performance of various algorithms on the Yahoo! To-day News data. The basic idea of replayer is to replay each user visit to the testing algorithm. If the recommended item is equal to displayed news in the log, this visit is regarded as a matched visit. The estimated CTR is the sum of the user clicks in the matched visits over the total number of matched visits.
 However, the replayer only works for a small item pool. When the number of items is large, the number of matched visits for each item would be very small. As a result, the CTR estimation based on a small number of matched visits is not reliable and would have a large variance [8]. For the Yahoo! Today news, the number of recommending articles is less than 50. But for online advertising data, the number of ads is usually over 16,000. We evaluate the KDD Cup 2012 online ads data using a simulation method, which is used in [8] for the same purpose. To this end, we select 100 ads from the entire ads pool. The context data of these ads are real and given in the data, but the rewards are simulated using a weight vector w for each ad. Given a context x , the click of an ad is generated with a probability (1 + exp(  X  w T x )) For each user visit and each arm, the weight vector w is drawn from a fixed normal distribution that is randomly generated before the testing.
We consider the performances of algorithms in two situ-ations: cold start and warm start. In cold start, there is no training data at the beginning for every algorithm. The algorithm can only learn the model by exploration. In warm start, we have 10,000 records of user activities for training. We train the logistic regression model using these data first. Tables 3 and 2 report the results of Yahoo! News data and KDD Cup 2012 online ads data, respectively. For each algo-rithm, we enumerate different parameter values. Except for LinUCB and Exploit , all other algorithms are randomized al-gorithms. For each trial, we also randomly shuffle the pool of recommending items. Thus, the performance of LinUCB and Exploit may vary in different runs. We run each al-gorithm with each parameter value 10 time, and keep track of the mean, standard deviation, minimum and maximum of the overall CTR. The best mean is highlighted in bold . The worst mean is marked with an asterisk (  X  ).

As depicted in the tables, the baseline algorithms of -greedy and LinUCB , which take into account both explo-ration and exploitation, exhibit a common trend: when the controlling parameter is small, i.e., with more exploitation, the algorithms can achieve better performance in terms of the CTR, whereas the deviation is high; Comparatively, when the parameter is set to be larger, i.e., with more explo-ration, the CTR shrinks, but the deviation decreases. Hence, the problem of personalized recommendation requires a trade-off between exploration and exploitation. Further, the per-formance of -greedy , LinUCB highly depends on the pa-rameter setting. The parameters of both algorithms explic-itly or implicitly control the balance of the exploration and exploitation. If the parameter setting is perfect, the perfor-mance approaches the optimal. If the parameter setting is improper, the performance is poor. This conclusion is also mentioned in the empirical studies of [5, 31].

TS and TSNR are two types of Thompson sampling. TS is the straightforward implementation. It makes use of the given priori distribution and iteratively maximizes the pos-terior mean, where the inverse of the priori variance is the regularization weight. If the prior variance is large, e.g., TS(0.001) , the sampling area of TS will be too large, which may significantly sacrifice the exploitation part. If the prior variance is small, e.g., TS(10) , the regularization weight is large and the learned  X   X  will be close to the given prior 0 . Obviously, 0 is not a good guess of  X  . Thus, for all param-eter settings, the performance of TS is not satisfactory in terms of the CTR.

TSNR only maximizes the likelihood using stochastic gra-dient ascent and ignores the regularization from the prior. Therefore, the priori distribution only affects the sampling area of  X   X  in the exploration part but not learning steps. As shown in Table 3 and 2, when the priori variance is appro-priate, e.g., TSNR(1000.0) , it can achieve good performance. In TSNR(1000.0) , the variance is q  X  1 0 I = 1 / 1000 . 0 I , which is quite small, meaning that we do not have to explore the ar-eas that are far away from the estimated  X   X  . However, when q is arbitrarily large, the variance approaches 0 and there will be no exploration at all. Consequently, TSNR becomes Exploit . As reported in the tables, the performance of Ex-ploit is relatively poor compared with Bootstrap , -greedy and LinUCB in terms of the mean and deviation of the CTR.
The performance of Bootstrap is comparable with the ones of -greedy and TSNR in terms of the CTR, as it takes a non-Bayesian strategy based on bootstrapping without con-sidering the prior and posterior distributions. The tradeoff between exploration and exploitation is handled in an evolv-ing manner as the data size increases. In addition, when we use different numbers of bootstrap samples, the averaged re-ward varies very slightly. The reason here is straightforward: the number of bootstrap samples dominates the accuracy of the approximation towards the sampling distribution, rather than the one controlling the balance of the exploration and exploitation.
The sample size of bootstrap determines the approxima-tion accuracy of the bootstrap method. The sample size is larger, the approximation accuracy is higher. Therefore, it is not a parameter for the recommendation model. We argue that once the sample size is large enough, it would not af-fect the performance much, as it is not a dominant factor to control the exploration/exploitation or the recommendation model. To verify this claim, we set different bootstrap sam-ple sizes from 1 to 500, and report the result in Figure 1. As depicted in this figure, the performance is still relative stable comparing to other algorithms. It is interesting to see that even if B = 1, in which the sampled bootstrap replications have some dependence for the same arm, the performance of Figure 1: Relative CTR on different bootstrap sam-ple sizes.
 Bootstrap is not poor. Hence, Bootstrap provides a better and safe choice for personalized recommendation when we do not have any prior information about the data, and in this sense, we argue that our proposed method is parameter-free in terms of the exploration/exploitation tradeoff.

We also investigate the scalability of Bootstrap (see Fig-ure 2). We focus on the time cost for each model update with online estimation on different bootstrap sample sizes. As presented in Figure 2, the number of bootstrap samples increases from 50 to 500 (10 times), whereas the millisec-onds per update increases about 3 times. Hence, a real-Figure 2: Time cost on different bootstrap sample sizes. world production server cluster can easily handle more than 1000 bootstrap samples for personalized recommendation services, e.g., online advertising or news recommendation.
Besides the overall CTR of each algorithm, we also eval-uate the CTR on individual time bucket. The CTR on each bucket is only calculated by the clicks collected in that bucket. The entire testing data is split into 20 time buckets. For Yahoo! Today news data, each bucket has 100,000 user visits. For KDD Cup 2012 online ad data, each bucket has Figure 3: Relative CTR on different time buckets for Yahoo! Today News. 50,000 user visits. All the user visit events are order by the time. Figures 3 and 4 show the relative CTR on individual buckets. As shown in Figure 3, Bootstrap (5), TSNR (100) and -greedy are superior to other baselines starting from the 8-th bucket. Also, on different buckets the lift of each algorithm with respect to random is different. The main reason is that the user interests on these news articles may change over time. It is worthy to note that the large lifts of CTR are only in the middle buckets. In the last a few buckets, the lifts become smaller, although the prediction models have more feedback to learn. In other words, when Figure 4: Relative CTR on different time buckets for KDD Cup 2012 Online Ads. a news article becomes aging, there is no much space for a recommender system to improve its CTR. Therefore, the recommender system should be able to promptly learn the predictive model in an online manner. If it takes a long time to cumulate user feedbacks, user preferences may have changed even if the prediction model can be trained well.
The reward of the KDD Cup 2012 online ads is simulated by the logistic regression function and a fixed weight vector with some random noises. For each ad, the relation between the context and reward is much simpler than the Yahoo! To-day news data. Thus, the data is easy to learn by a logistic regression model. The CTR curves in Figure 4 are very sta-ble. Bootstrap (5), TSNR (100) and -greedy converge very quickly staring from the second bucket. The performance of TS (0.001) increases slowly. The prior variance of TS (0.001) is (1 / 0 . 001) I = 1000 I , which is very large. The sampled coefficients are usually far away from the posterior mean in the early stage. But when it learns more data, the poste-rior variance becomes smaller and the performance increases gradually.

To summarize, our findings from the experiments are three-fold: (1) For solving the contextual bandit problem, the al-gorithms of -greedy and LinUCB can achieve the optimal performance, but the input parameters that control the ex-ploration need to be tuned carefully; (2) The probability matching strategies, e.g., Thompson sampling, can have the optimal result, but it highly depends on the selection of the prior; and (3) Our proposed approach, Bootstrap , is a safe choice of building predictive models for contextual bandit problems under the scenario of cold-start .
Our work is primarily relevant to three active areas of re-search, namely (1) modeling multi-armed bandit problems, (2) probability matching for contextual bandit problems, and (3) bootstrapping for statistics estimation.

Multi-armed bandit problem : The primary challenge in multi-armed bandit problems is to balance the tradeoff be-tween exploration and exploitation. In practice, for solving a cold-start problem, neither a pure exploitation or a pure exploration works best, as there are always uncertainties of user preferences to explore. A wide range of application-oriented problems have been modeled as the context-free multi-armed bandit problem, such as online advertising [25, 30], web search and content optimization [2, 27], network optimization [11], game playing [12], routing [6], etc. A list of computational strategies have been proposed in the past decades, including -greedy, EXP3 [4], upper confidence bound (UCB) [5, 19], etc. The basic paradigm for solving the context-free bandit problem is to run multiple trials to estimate the reward distribution. However, these solutions are either sub-optimal or require careful settings of the bal-ancing parameters. In our work, we present a parameter-free algorithm to avoid the process of parameter tuning, i.e., without the input parameter to allocate the importance of exploration.

Probability matching for contextual bandit : A more general version of the bandit problem is called contextual bandit, which has not been well studied. Here the contextual information is related to the specific environment of pulling the arms with a learning problem; for example, in online ad-vertising systems, the context might involve a user X  X  query, or the web page on which an ad is placed. Several interest-ing approaches have been reported by following the -greedy or UCB [4, 19] paradigms.

Another family of algorithms for solving contextual ban-dit problems is probability matching [32], which pulls differ-ent arms according to the probability that the correspond-ing arm has the largest expected reward. Instead of re-quiring a controlling parameter for the balance of explo-ration/exploitation, the strategy of probability matching en-ables the learning process to automatically adjust the trade-off [8]. Representative work along this stream involves [8, 14, 22], which follow the Bayesian paradigm to estimate the uncertainties. However in many practical scenarios, an inap-propriate prior for Bayesian learning models may lead to im-balanced exploration/exploitation in the early stage of learn-ing, and consequently jeopardize the overall performance. Comparatively in our work, we propose a non-Bayesian al-gorithm based on the framework of probability matching, which utilizes bootstrapping for coefficient estimation, and does not require the input of priors.

Bootstrapping for statistics estimation : Bootstrap-ping, in statistics, is a useful technique for estimating some statistical properties of an estimator [10]. This technique resamples the observed data with replacement, and then utilizes the resampled data sets to infer the properties of the estimator. In machine learning, it is often used in the bootstrap aggregation (bagging) to ensemble different learn-ing algorithms. However, the traditional bootstrap method operates in an offline way in which the observations are all already given. In some situations, the observations arrive from a data stream and the bootstrapping must be made in an online manner. To handle the online setting, [23] presents an online paradigm of data resampling using a Poisson dis-tribution. [26] makes use of this method to improve the robustness of the learning model for large data sets. In our work, we employ the idea of online bootstrapping to the sce-nario of estimating cofficients of contextual bandit models.
In personalized recommender systems, the dilemma of ex-ploration/exploitation in the cold-start situation remains a challenging issue due to the uncertainty of user preferences. In this paper, we formulate the problem of personalized rec-ommendation as a contextual bandit problem to balance the tradeoff between these two competing goals. We propose a parameter-free strategy for bandit problems, which employs a principled resampling approach called online bootstrap, to derive the sampling distributions of learning model esti-mators. The proposed algorithm is essentially an ensemble method to achieve optimal rewards without specifying ex-ploration parameters. Extensive empirical experiments on two real-world data sets, i.e., online advertising and news recommendation, demonstrate the efficacy of our proposed approach in terms of averaged rewards.

As for the future work, the recommend items, e.g., ad-vertisements or news articles, may have some underlying relations with each other. For example, two advertisements may belong to the same categories, or come from business competitors, or have other same features. In the future, we plan to consider the potential correlations among dif-ferent items, or say, arms [24]. It is interesting to model these correlations as constraints, and incorporate them into the contextual bandit modeling process. A second direction to extend our proposed contextual model is to consider the temporal information of user preferences, as the interests of users may often evolve over time.
 The work is partially supported by NSF grants CNS-1126619, IIS-1213026, and CNS-1461926, DHS grant 2010-ST-062-000039, Purdue VACCINE/DHS 4112-35822, and an FIU Dissertation Year Fellowship.
