 REGULAR PAPER Cane Wing-ki Leung  X  Stephen Chi-fai Chan  X  Fu-lai Chung Abstract The rapid development of Internet technologies in recent decades has imposed a heavy information burden on users. This has led to the popularity of recommender systems, which provide advice to users about items they may like to examine. Collaborative Filtering (CF) is the most promising technique in rec-ommender systems, providing personalized recommendations to users based on their previously expressed preferences and those of other similar users. This paper introduces a CF framework based on Fuzzy Association Rules and Multiple-level Similarity (FARAMS). FARAMS extended existing techniques by using fuzzy as-sociation rule mining, and takes advantage of product similarities in taxonomies to address data sparseness and nontransitive associations. Experimental results show that FARAMS improves prediction quality, as compared to similar approaches. Keywords Collaborative filtering  X  Recommender systems  X  Fuzzy association rule mining  X  Similarity 1 Introduction The rapid development of Internet technologies in recent decades has imposed a heavy information burden on users. This has led to the popular use of recom-mender systems, which receive information from users about items that users are interested in, and recommend items that may fit users X  needs.
 knowledge-based and social-or collaborative-filtering-(CF-) based [ 5 ]. Content-based recommender systems establish users X  interest profiles by analyzing the fea-tures of their preferred items. They compare the features of recommendable items to those of the preferred items of the user. They then recommend the more rele-vant items, measured by features similarity. Knowledge-based systems make use of knowledge about users and products to generate recommendations. They use a reasoning process to determine what products meet a user X  X  requirements. Social-filtering or CF-based recommender systems provide personalized recommenda-tions according to user preferences. They maintain data about target (or active) users X  purchasing habits or interests and use this data to identify groups of similar users. They then recommend items liked by other, similar users.
 mender systems [ 9 , 26 , 30 , 32 ]. First, they do not take into account content in-formation, and second, they are simpler and easier to implement. Because CF systems do not take into account content information, they can filter items that are not computer-parsable. Further, ignoring content information allows CF systems to generate recommendations based on user tastes rather than the objective prop-erties of domain items themselves. This means that the system can recommend items very different (contentwise) from those that the user had previously shown a preference for. This overcomes a major limitation of content-based recommender systems [ 32 ]. As noted, CF techniques are also much simpler and easier to imple-ment than knowledge-based recommender systems. In order to build a knowledge base of items, knowledge-based systems require a domain knowledge engineering process, whereas CF systems can be fully automated. Consequently, it is easy to apply CF to domains where a database of user preferences is available. To date, CF has already been successfully applied in various domains to recommend, for jokes [ 10 ], books [ 25 ] and web pages [ 33 ].
 Memory-based algorithms, such as neighborhood-based methods, operate over the entire user database and recommend products using statistical methods. Model-based algorithms construct compact models from a user database and then recom-mend products using probabilistic methods. Since such models can be constructed offline, the online performance of model-based algorithms is usually better. This, along with the recognition of the scalability problem of memory-based algorithms, has meant that most recent algorithms have been model-based. Some well-known algorithms include classification, clustering, hybrid algorithms that combine CF and content-based techniques, as well as the focus of this paper, association rule mining (ARM). There has been considerable research into CF, yet the issues of scalability, data sparseness [ 31 ] and the nontransitive association problems [ 18 ] remain open challenges.
 introduces a CF framework based on fuzzy association rules and multiple-level similarity (FARAMS). The use of ARM-based techniques in CF has a number of desirable outcomes. First, because they allow product hierarchies to be inte-grated into the association rule mining process, the fact that relationships among items are already implicit in these hierarchies reduces nontransitive association problems [ 23 ]. Second, ARM techniques apply higher-level association rules to address data sparseness, resulting in improved algorithm recall rate [ 19 ]. Third, the use of integrated product hierarchies in ARM techniques increases the num-ber of items that can be recommended to users for whom only limited known preference data is available. Finally, ARM techniques provide the flexibility to, if necessary, easily mine associations among content-related attributes and item ratings. For example, a new user using a travel recommender system may specify trip-related attributes which are then used to find destination or tourist spots that are strongly related with those attributes.
 proaches to CF that are related to the proposed CF framework, FARAMS. We analyze the underlying technologies of these popular approaches and comment on their strengths and weaknesses. Sect. 3 details related ARM techniques including their problem definitions, and describes a variation of ARM known as fuzzy asso-ciation rule mining (FAR). Sect. 4 discusses the various steps in FARAMS. Sect. 5 presents and discusses results of an evaluation of FARAMS. Sect. 6 concludes the paper. 2 Related work proaches include the traditional neighborhood-based methods, classification, clus-tering, association rule mining (ARM), as well as hybrid algorithms that combine CF and content-based techniques. This section describes three of these approaches that are related to the proposed framework: k-nearest neighbor, classification, and ARM techniques. 2.1 K-nearest neighbor (k-nn) K-nearest neighbor (k-nn) was commonly used in early CF-based systems. It con-sists of three major steps, namely user similarity weighting, neighbor selection, and prediction computation [ 4 , 15 , 30 ].
 according to their similarity with the active user. Similarities are reflected in the ratings that users have given items. In order for two particular users to be compa-rable, only items that both users have rated are counted. The neighbor selection step requires that a number of k-nearest neighbors of the active user be selected as item predictors. These selected users have the highest similarity weights and it is based on their interests and some partial information of the active user that an item X  X  prediction score is computed.
 as the active user rates more items [ 4 , 15 ]. Further, ratings-based and content-independent predictions allows K-nn to be used even in domains where textual categorized by any attribute.
 problems, one of the most crucial being that of data sparseness. Data sparseness arises because users cannot rate unobserved items in the system. In reality, for example, where a purchase database is used, the number of items unobserved by each user is certainly very large. This has an adverse effect on prediction quality [ 6 ] as the known preferences of users may be insufficient for generating recom-mendations.
 the nontransitive association problem, which manifests in two forms. In user but-not-identical items, their correlation is lost. This is known as the user-based nontransitive association problem, which we may attempt to avoid by using item similarity instead of user similarity but even then, if two similar items have never been experienced or rated by the same user, we face the item-based nontransitive association problem.
 This is because K-nn requires that each prediction be made from real-time com-putations performed over the entire database. This leads to severe performance bottlenecks in the similarity weighting step. As the number of users and items in the system grows, the performance of the algorithms degrades. The proposed method deals with data sparseness and nontransitive associations by taking advan-tage of item similarities that are implicit in their taxonomies. Scalability is ensured by identifying item similarities offline. 2.2 Classification ings are first discretized into classes such as  X  X ike X  and  X  X islike. X  Classification approaches, such as Bayesian network [ 4 ] and Simple Bayesian classifier [ 28 ], are constructed offline, and as a consequence only related models have to be in-spected in real-time, classification approaches improve scalability. Furthermore, because their class labels use linguistic terms, classification approaches produce more human-understandable results.
 their predictions, a problem arising from their probabilistic nature, and second, the sharp boundary problem, which is caused by boolean discretization of quantitative ratings data [ 12 , 22 , 34 ]. While prediction quality has been one of the major goals if any.
 instance, ratings above a certain value are transformed into  X  X ike X , or  X  X islike X  otherwise. Given the Jester dataset [ 10 ], in which ratings were recorded as real numbers ranging from  X  10 to + 10, and the like threshold 3, the rating 3 is trans-formed into  X  X ike X  while the rating 2.9 is transformed into  X  X islike. X  In other words, a 0.1 difference between the two values cause them to fall into two totally different classes. Furthermore, using this method, both of the ratings 10 and 3 will be classified as  X  X ike. X  Clearly, this is problematic as we intuitively know that the preference implied by a rating of 10 is much higher.
 to this problem is the use of fuzzy logic, which has been widely-adopted in related studies, such as data mining on quantitative data [ 12 , 21 , 22 ]. FARAMS shall also adopt this approach, which is further described in Sect. 3.3 . 2.3 Association rule mining (ARM) Association rule mining (ARM) is a well-studied data mining technique, and the purpose of which is to discover interesting relationships between items by finding items that have frequently appeared together [ 1 , 2 , 14 ]. When applied to CF, ARM can be used to generate recommendations after interesting rules are mined [ 8 , 17 , 19 , 24 ]. For example, if the association rule  X  A  X  B  X  is interesting in a certain system, and the active user has liked item A previously, the user will probably like item B also.
 (ASARM) algorithm for CF. ASARM mines rules for one target item at a time, and automatically adjusts the minimum support value to mine a user-specified number of rules. Numeric ratings are discretized into two classes,  X  X ike X  and  X  X islike X , based on some chosen threshold value.
 between categories in multiple-level product taxonomies to address data sparse-ness. MAR can be applied on items that are organized in a hierarchical category structure, such as the classification of goods in a department store (the  X  X s-a X  hi-erarchy). Taking advantage of such a product hierarchy increases the number of recommendable items for those users whose known preference data are otherwise so limited that it is not possible to produce recommendations. The rating a user has given an item is classified as  X  X ike, X  if it is above the average rating given on all items by that user in the MAR approach.
 proposed framework attempts to address such problem by incorporating fuzziness in quantitative ratings. 3 Association rule mining This section first describes the problem definitions of ARM, including its classical definition and the adaptive-support ARM tailored for CF, followed by the fuzzy association rule mining technique (FAR) that is used to address the sharp boundary problem in FARAMS. 3.1 The ARM problem The following formal problem definition of ARM is provided in Agrawal et al. certain syntactic and support constraints. Syntactic constraints involve restrictions on items that can appear in a rule, while support constraints define the number of transactions in T that support a rule. or the body of the rule, is a set of items in I ,andwhere I j , known as the conse-quent or the head of the rule, is a single item in I that is not present in X .The rule X  X  statistical significance and strength are measured by the two values known as support and confidence. The support of a rule is defined as the percentage of transactions in T that contain both X and I j , while confidence is defined as the percentage of transactions containing X that also contain I j . For example, the rule  X  A  X  B  X  [20%, 90%] means that 90% of users who purchased item A also pur-chased item B , and that 20% of all users purchased both A and B . Note that a rule is considered interesting only if its support and confidence values are higher than the user-specified minimum.
 1. Find all combinations of items, known as large itemsets or frequent itemsets, 3.2 Adaptive-support ARM definition is inefficient for collaborative recommendation. First, ARM algorithms mine rules for all items in the database. Many rules mined will not be relevant for a given user. Second, the minimum support and confidence values have to be specified in advance. Due to variations in user tastes and the popularity of various items, this might lead to either too many or too few rules as rules involving less popular items may be difficult to discover. In view of these problems, Lin et al. [ 24 ] proposed a new ARM problem definition for CF:
Find : A set of association rules S having the target item in the heads such that the tiple passes over the data, this is done offline and therefore does not affect the response time of the recommendation process. The proposed method adopts the above ARM problem definition, and therefore, with some extensions and modifi-cations, the ASARM algorithm.
 3.3 Fuzzy association rule mining Fuzzy association rule mining (FAR) addresses the sharp boundary problem by extending the classical problem definition of ARM [ 12 , 21 , 22 ]. FAR is in the form of  X  AisX  X  BisY  X , where X and Y are fuzzy sets that characterize attributes A and B . The fuzzy set concept provides a smooth transition between members and nonmembers of a set. An attribute can be a member of a fuzzy set to a certain degree in [0, 1]. This value is assigned by the membership function (MF) associated with each fuzzy set. Three examples are shown in Fig. 1 ,inwhich numeric ratings of items are fuzzified into three classes, Like , Neutral and Dislike , respectively represented by L , N and D .
 transformed ratings of the two movies M 1and M 2. Each attribute, which was a movie name, is expanded into a Movie ID, Fuzzy Set pair, and each value, which was the user rating of that movie, is transformed into its membership degree with respect to the specified fuzzy set.
 porting an itemset but also their degree of support. The FS of an itemset A , X is defined as follows [ 13 ]: the value of a j in the i th record in the transactional database T , and the value the product of the degree of membership of each a j that it contains. The votes of wards the support count of itemsets [ 12 ]. An example is the movie M 2inthefirst transaction with UserID 100 ( t 100). The support counts of the itemsets M 2 , L ,
M 2 , N and M 2 , D are 0.8, 0.5 and 0.2, respectively. In other words, t 100 will be counted 1.5 times (0.8 + 0.5 + 0.2) for the movie M 2. To avoid some movies contributing more than others, ratings are normalized using Eq. ( 2 ). This ensures that every movie has a total contribution of 1 if rated [ 12 ]. for a j . According to this equation, the fuzzified ratings in Table 2 are normalized to 0.8/1.5, 0.5/1.5 and 0.2/1.5 (i.e., 0.53, 0.33, and 0.14) as shown in Table 3 .
M 1 , L , B , Y = M 2 , D and therefore C , Z = { M 1 , M 2 } , { L , D } .Given the ratings in Table 3 ,FS A , X = ( 0 + 0 + 1 + 1 )/ 4 = 0 . 5, FS B , Y = ( 0 . 14 + 0 + 0 . 53 + 1 )/ 4 = 0 . 4175, and FS C , Z = ( 0  X  0 . 14 ) + ( 0  X  0 ) + ( 1  X  0 . 53 ) + ( 1  X  1 )/ 4 = 0 . 3825.
 FS A , X ,where A  X  C , B = C  X  A , X  X  Z and Y = Z  X  X .
 tween the body and the head of a fuzzy association rule. CORR between A , X and B , Y , denoted by CORR A , X , B , Y is defined as follows [ 13 ]: where and similar for B , Y .
 ance and covariance in statistics [ 13 ]. The value of fuzzy correlation ranges from  X  1 to 1. Only positive value tells that the body and the head of a rule are pos-0 . ( 0 ( 0 . 3251  X  0 . 1743 ) = 0 . 1508, and therefore CORR A , X , B , Y = 0 . 17375  X  0 . 895. 4 The FARAMS framework The FARAMS framework adopted some existing ARM techniques, including Lin et al. [ 24 ] and Kim and Kim [ 19 ], to generate collaborative recommendations. niques for handling quantitative ratings data. FAR mining, which is a variation of the classical ARM techniques, is used to address the resulting sharp boundary problem in existing techniques [ 12 ].
 tasks. The second step mines interesting associations among domain items and categories from user preferences. Rules mined in this step are stored in the sys-tem, and are used when users request recommendations. The third step is predic-tion computation, which determines relevant rules for a user and assigns predicted preferences to items recommended by those rules. The fourth step produces rec-ommendations. If the number of recommendable items is smaller than the prede-fined number, multiple-level similarity among items is used to predict user pref-erences for items that are not covered in the product-level association rules. The following sections provide further details about the tasks involved in each step. 4.1 Data preprocessing This section describes the various procedures required in the data preprocessing both items and categories, fuzzifying user preferences for FAR mining and trans-forming transactions to allow efficient support counting. 4.1.1 Mapping user-item matrixes to transactions CF ratings data are usually represented as preference matrixes. They are trans-formed into transactional databases for ARM tasks. As shown in Table 4 (a), each transaction consists of a transaction identifier (TID), which is the User ID of the user to which the transaction belongs, and of the IDs and ratings of the items that have been rated by that user. 4.1.2 Computing user preferences for higher-level categories When the known preferences of a user is so limited such that recommendations cannot be produced, FARAMS makes use of multiple-level similarity among items (hereafter referred to as MS). Users X  preferences for higher-level categories are items rather than their categories. These have to be computed from the original transactions containing users X  item preferences, so that rules involving categories can be mined.
 the same category can be different. In order to compute a user X  X  preference for a category, ratings given to each category are averaged. In the sample transactions showninTable 4 (b), a category ID is enclosed in square brackets, followed by the average rating the user has given it. 4.1.3 Fuzzifying ratings for both higher-level categories and product-level items. First, the fuzzy sets and membership functions for ratings are determined. Second, items in transactions are expanded into Item, Fuzzy Set or Category, Fuzzy Set pairs. Third, the de-gree of membership of each rating is determined with respect to each fuzzy set. Finally, the fuzzified ratings are normalized so that each transaction makes the same contribution, which is 1.
 the fuzzy sets and membership functions in Fig. 1 a. For simplicity, the table does not show Item, Fuzzy Set pairs with membership degrees of 0. 4.1.4 Transforming transactions for efficient support counting Like the ASARM algorithm [ 24 ], FARAMS obtains the support counts of item-sets by making multiple passes over the data. One major optimization made in transformed into a vertical tid-list format [ 35 ], with each Item, Fuzzy Set pair associated with a list of transactions that contained the pair and the corresponding membership degrees it obtained. As shown in Table 6 , the same transformation procedure takes place for both items and categories. This transformation enables efficient support counting of itemsets, as described in Sect. 4.2.3 . 4.2 Mining user preferences The overall structure and flow of our algorithms for adjusting the minimum sup-port value and mining user preferences are similar to those described in Lin et al. [ 24 ]. This section therefore focuses on the adaptations and extensions made in the association rule miner of FARAMS. Several issues that were considered when de-signing the association rule miner are first described, followed by the description of the mining algorithm of FARAMS. 4.2.1 Association mode used Existing CF frameworks use two major association modes, user association and and recommends items preferred by users similar to the active user. Mining user associations produces rules that are in the form of  X  User 1 , Like  X  Active User , by him/her, this rule fires if it has been liked by User 1[ 24 ]. It will therefore be considered for recommendation.
 form of  X  Article A, Like  X  Target Article , Like  X . This rule fires if the ac-tive user has liked Article A but has not rated Target Article previously. The Target Article in the head of the rule will be considered for recommendation. This can be applied to categories by treating a category as an article. To facilitate the use of product taxonomies in the recommendation process, article association is adopted in FARAMS. 4.2.2 Defining candidate 1-itemsets As FARAMS mines rules for one target item ( targetItem ) at a time, association rules are generated using only itemsets containing the targetItem . Candidate 1-itemsets can therefore be limited to the  X  X elated items X , defined as the union of all items that appeared in transactions containing the targetItem . FARAMS mines rules in an apriori-like fashion, which iteratively generates k -itemsets by joining two ( k  X  1)-itemsets. Once the related items are determined in the first iteration, their associated tid -lists serve as a reduced database for support counting in the subsequent iterations.
 results of the rule mining process. This is because the downward closure property of support values means that all subsets of a frequent itemset must be frequent [ 1 ]. If an item did not appear with the targetItem , and as rules are generated using only itemsets containing the targetItem ,any k -itemset, where k &gt; 1, containing that item and the targetItem must be infrequent, and can therefore be excluded from consideration in the first place. 4.2.3 Computing fuzzy support values In traditional methods, the fuzzy support count of an itemset is obtained by scan-ning an entire database. The transformed transactions used in FARAMS scan more efficiently because they inspect only k records, where k is equal to the size of the itemset. As shown in the following examples, fuzzy support counting can be car-ried out using simple calculations and joining operations [ 35 ].
 be found efficiently by inspecting only one record, and the result is (1 + 1) = 2. necessary to inspect only two records (Table 7 (b)). Before the fuzzy support count first determined by a simple join operation. As shown in Table 7 (b), 300 and 400 appeared in both records. The fuzzy support count of the 2-itemset can then be determined, and is found to be equal to (1  X  0.5) + (1  X  1) = 1.5. 4.2.4 The mining algorithm Algorithms 4.1 and 4.2 describe our mining algorithm and one of its subroutines, ( targetItem ) at a time. Given the targetItem and a transactional database ( T ), the algorithm first determines the list of related items that are frequent ( F 1 )andtheir associated tid-lists ( TID ) using Algorithm 4.2. It then proceeds to find frequent rules ( R k ) are generated from the frequent k -itemsets, and at most maxNumRules association rules having the highest support values ( R t ) are returned. If the total number of rules mined in the k iterations is larger than maxNumRules ,raise the aboveMaxNumRulesFlag , which will then cause the rule mining process to terminate. After all the iterations, if the total number of rules mined is smaller than minNumRules ,raisethe belowMinRulenumFlag .
 Algorithm 4.1. The mining algorithm.
 Input: Transactional database ( T ), targetItem , minimum support ( minSup-port ), minimum confidence ( minConfidence ), minNumRules , maxNumRules , maximum number of items in a rule X  X  body ( maxRuleLength ). Output: Set of association rules ( R t ), so that each rule in R t : (1) has targetItem in its head, (2) with no more than maxRuleLength items in its body, and (3) satisfies the minSupport and minConfidence constraints. The number of rules in R t is at most maxNumRules . If the number of rules in R t is above maxNumRules (resp. below minNumRules ), raise the aboveMaxNumRulesFlag (resp. belowMinNumRulesFlag ). 1( F 1 , TID )= find frequent 1 itemsets ( T , targetItem ); 2 for ( k =2; ( k  X  maxRuleLength +1)and( F k  X  1 =  X  ) and (not 3 C k = gen candidate ( F k  X  1 ); 4 for each candidate c  X  C k do { 5 c . fuzzySupport = compute fuzzy support ( c , TID ); 6 if ( c . fuzzySupport  X  minSupport ) then 7add c to F k ; 8 } 9 R k = gen rules ( F k , targetItem , minConfidence ); 10 if ( | R t | + | R k | &gt; maxNumRules ) then 11 set R t . aboveMaxNumRulesFlag ; 12 R t = maxNumRules rules with highest support from R t . rules  X  13 } 14 if ( | R t | &lt; minNumRules ) then 15 set R t . belowMinNumRulesFlag ; 16 return ( R t ); Algorithm 4.2. The find frequent 1 itemsets subroutine.
 Input: T , targetItem .
 Output: Set of frequent 1-itemsets ( F 1 ) that are related to targetItem ,and their associated tid-lists ( TID ). 1 U t = { users who had rated targetItem } ; 2 for each u  X  U t do { 3 I u = { items rated by u } ; 4 C 1 = C 1  X  I u ; //C1 denotes the candidate 1-itemsets that are related 5 } 6 for each c  X  C 1 do { 7 c . fuzzySupport = compute fuzzy support ( c , T ); 8if( c . fuzzySupport  X  minSupport ) then { 9add c to F 1 ; 10 add tid -list of c to TID ; //tid-list of c is contained in T 11 } 12 } 13 return ( F 1 , TID ); 1. find frequent 1 itemsets ( T , targetItem ): This subroutine, as described in 3. compute fuzzy support ( c , TID ): This subroutine determines the fuzzy sup-4. gen rules ( F k , targetItem ): This subroutine produces association rules having 1. compute fuzzy support ( c , T ): This subroutine determines the fuzzy support level items and categories. The association rules mined in this step are stored in the system. They will be used when users request for recommendations. 4.3 Predicting scores of recommendable items When the active user ( activeUser ) requests recommendations, the system finds the list of items the activeUser has previously rated, based on which the relevant rules, which are rules that fire for him or her, are determined. The system will assign predicted scores to the relevant rules, and the items in the heads of the rules may be recommended to the activeUser [ 24 ]. An item may appear in the heads of more than one rule. After all relevant rules are determined, the scores of the rules that recommend the same item are summed up to obtain the item score predictions. fidence values, are used as predicted rule scores. In Lin et al. [ 24 ], for example, a rule X  X  score is defined as the product of its support and confidence, while Kim and Kim [ 19 ] takes into account only its confidence. The fuzzy correlation value (CORR), which measures the correlation between the body and the head of a rule, has not been used in any CF algorithm. We have therefore evaluated it empirically. The results are presented in Sect. 5.3.2 . 4.4 Generating recommendations After predicted scores are assigned to recommendable items, the recommendation process proceeds to determine which of the items are actually recommended to the activeUser . This section describes the how FARAMS generates recommenda-tions, as well as when and how multiple-level similarity (MS) is utilized. Then, it describes the recommendation algorithm used in FARAMS. 4.4.1 Recommendation strategy There exist two main strategies for generating recommendations. The first strategy recommends items with predicted scores above a score threshold. The threshold applied in Lin et al. [ 24 ], for example, is a linear function of the number of rules, but the reason for choosing such a value is unclear. The second strategy recom-mends a fixed number of items and is known as Top-N recommendations. This strategy is used in Kim and Kim [ 19 ]. Both strategies rank items in descending order of their predicted scores, so that items with higher scores are recommended first. There are two reasons why FARAMS uses the Top-N approach to recom-mend items. First, the number of recommended items is fixed and therefore con-trollable. Secondly, Top-N helps determine when MS should be used.
 Therefore, to predict the item preference, we assign a weight w l to a rule X  X  score, product-level). 4.4.2 Recommendations using multiple-level similarity (MS) MS is utilized to produce recommendations when the known preferences of the ac-tiveUser are very limited and, consequently, the number of recommendable items may be very small. This is done as follows: 1. Find the preferred categories C a of the active user. 2. Determine association rules that fire for C a (rules containing C a in their bod-3. Find rules containing items in those associated categories and assign predicted 4.4.3 The recommendation algorithm Algorithm 4.3 describes the recommendation algorithm, or recommender, of FARAMS. Given the activeUser and his or her previously rated items ( P a ), assigned predicted scores if they are not in P a . If the number of recom-mendable items is smaller than the predefined Top-N value ( topN ), more recommended items are generated by using MS. Finally, the recommendable TopN items having the highest scores ( I ) are recommended to the activeUser . Algorithm 4.3. The recommender of FARAMS.
 Input: Set of association rules ( R ) in the system, active user ( activeUser ), set of items rated by the activeUser ( P a ), maximum number of recommendation ( TopN ).
 Output: Set of recommended items ( I )forthe activeUser . The number of items in I is at most topN . 1 Items = recommend ( R , activeUser , P a ); 2 if ( | Items | &lt; TopN ) then 3 Items = recommend by MS ( R , activeUser , P a , Items ); 4 sort ( Items ); // in descending order of their scores ; 5 I = topN items in Items ; 6 return( I ); 1. recommend ( R , activeUser , P a ): This subroutine returns a list of recommend-2. recommend by MS ( R , activeUser , P a , Items ): This subroutine determines 3. sort ( Items ): This subroutine ranks the recommendable items in descending 5 Experimental results We carried out several experiments to evaluate the performance of FARAMS, and to compare it with some related work. In this section, we first describe the datasets used and the settings of the experiments and then provide the experimental results. 5.1 Datasets We used a number of CF datasets, or their subsets, as test-beds, depending on the purposes of various experiments. These included: 1. MovieLens: The MovieLens dataset contains 100,000 ratings of 1,682 movies 2. Jester: The Jester dataset contains ratings of 100 jokes from 73,421 users. Rat-3. EachMovie: The EachMovie data set contains 2,811,983 ratings of 1,628 ments. There exist methods to learn fuzzy sets and membership functions from datasets [ 7 , 16 ], but since the focus of this work is not on the modeling of user preferences, we used the simple fuzzy sets and membership functions shown in Fig. 1 , as described later in the individual results and discussions. In fact, we per-formed a set of experiments using different membership functions and found little sensitivity of results, especially for the MovieLens and EachMovie datasets. This is probably due to the small rating-scales of the datasets. 5.2 Experimental settings 5.2.1 Parameters This section describes some predefined parameters that were used in the experi-ments. As they have already been evaluated in related work, these parameters are adopted in our experiments without repeating the related evaluation procedures. [ minNumrules, maxNumrules ]. As stated in Lin et al. [ 24 ], the range [10, 100] is desirable and is therefore adopted in our experiments. For categories, the range [1, n  X  1] is used in the preliminary experiments, where n equals to the number of categories of the dataset in use.
 In FARAMS, it is possible to obtain Like , Neutral and Dislike associations: for example,  X  M 1 , L AND M 2 , N  X  M 3 , D  X . As stated in Lin et al. [ 24 ], em-ploying both Like and Dislike associations does not outperform employing Like associations alone. Our experiments therefore adopt the same strategy. the MovieLens and EachMovie datasets are organized into similar 2-level product taxonomies. The weights assigned to level-0 and level-1 association rules are 0.9 and 0.1, respectively, as suggested in Kim and Kim [ 19 ]. 5.2.2 Performance evaluation There are several commonly used evaluation metrics for evaluating CF algorithms. They include accuracy measures such as Mean Absolute Error (MAE), precision and recall, and the percentage of recommendable items in the system known as coverage. The choice of these performance metrics is dependent upon the recom-mendation strategies used.
 validation, and the results of all trials were averaged to obtain the final result. In each trial, one preference data of each user in the test set was hidden. For each active user, a list of Top-N recommendations was generated based on the user X  X  known preferences. If the hidden item was recommended, it is called a hit. The recall rate of an algorithm is defined as the number of hits over the number of hidden items in the test set. 5.3 Results and discussions In this section we present and discuss our results, which demonstrate the effects of various factors on the recommendation quality as measured by the algorithms X  recall rates. 5.3.1 Rule length Rule length refers to the number of items contained in the body of a rule. In Lin et al. [ 24 ], a rule X  X  body can contain multiple items, while in Kim and Kim [ 19 ], it can only contain a single item. Figure 2 a shows the recall rates achieved using both approaches in FARAMS as applied to the MovieLens dataset. SGL represents the recall rates of the algorithm having a maximum rule length of 1, while MUL-8 represents that of the algorithm having a maximum rule length of 8 [ 24 ]. for all values of N . This indicates that rules containing a single body item pro-duce better recommendations. One reason for this may be that rules are easier to fire when they are shorter, so that a larger number of applicable rules as well as recommendable items can be found for the active user, resulted in a lower demand for the use of MS to produce recommendations. As stated in Sect. 4.4.1 , product-level association rules are usually more relevant to user preferences, and therefore produce better predictions. Given these results, in the remaining experiments we use 1 as the maximum rule length. 5.3.2 Scores of recommendable items Section 4.3 describes the various measurements that can be used to predict the scores of recommendable items. These measurements include the product of sup-port and confidence (SC), confidence (C), and correlation (CORR). Figure 2 b shows the results of an empirical comparison between these measurements. confidence (C). C outperforms CORR by around 369.3, 160.5, 91.7, 70.2 and 51.3%, respectively for all values of N . This shows that the correlation of items is not as important as the strength and interestingness of rules when producing collaborative recommendations.
 for all values of N . While support indicates a rule X  X  statistical significance in the entire database, confidence indicates the interestingness of the rule with respect to its rule body. For an applicable association rule, since we already know that the active user has liked the items in its body, it is reasonable to choose to use confidence to measure the probability that the user will like the head. We adopt C to compute the scores of recommendable items in the subsequent experiments. 5.3.3 Effects of multiple-level similarity Figure 2 c compares the performance of FAR (that is, without using MS) and FARAMS. When N is equal to 10 and 50, FAR and FARAMS produce very sim-ilar recall rates. When N is equal to 20, 30 and 40, FARAMS outperforms FAR slightly, by around 5.5, 3.2 and 3.1%, respectively. It can be concluded that the use of MAR helps to alleviate data sparseness in CF dataset by increasing the number of recommendable items to users. In the MovieLens dataset, however, the improvement in prediction quality is small. This demonstrates that category-level association rules are less relevant to users X  preferences. 5.3.4 Effects of fuzzy association rules Two sets of experiments were performed to evaluate the effects of fuzzy associa-tion rules on recommendation quality using the MovieLens and the Jester datasets. As the MovieLens dataset contains discrete ratings in a small rating scale, it is expected that the effect of the sharp boundary problem would be small and, as a result, using fuzzified ratings may not improve recommendation results. The Jester dataset, in contrast, contains continuous ratings in a larger rating scale. The sharp boundary problem is believed to be more significant than in the MovieLens dataset. As a result, using fuzzified ratings would improve recommendation qual-ity.
 (Fig. 1 a). In Fig. 2 d, MS indicates the results of the experiments using MS only, while FARAMS indicates the performance of our framework using fuzzified rat-ings. As shown in the figure, FARAMS outperforms MS by 7.5 and 2.0% when N is equal to 10 and 20, respectively. This shows that hits appeared in higher ranks in our approach. As recommendable items are ranked according to the confidence values of their related rules, the results reveal that fuzzy confidence provides a bet-ter indicator of a rule X  X  interestingness than does the classical confidence measure. For larger values of N , however, MS outperforms FARAMS by 1.3, 1.8 and 0.1%, respectively. This confirmed our expectation that the sharp boundary problem is small in the discrete ratings with a small rating scale, which therefore do not have much fuzziness.
 tions in MF(B) (Fig. 1 b). Figure 2 e shows the results. As items in this dataset do not have any hierarchical category structure, recommendations were generated using only product-level association rules, denoted by AR in the figure. The per-formance of AR recorded for different Top-N values is compared to that of FAR, which uses fuzzy association rules without MS.
 provements in recall rates achieved are approximately 2.0, 1.8, 2.4, 2.5 and 2.1%, respectively for the Top-N values. This proves that the sharp boundary problem does adversely affect recommendation quality, and that the use of FAR does ef-fectively address this problem. 5.3.5 Comparisons with related work Lin et al. [ 24 ] tested their ASARM algorithm on the EachMovie dataset. As col-laborative users (the training set), they used the first 1,000 users who have rated more than 100 movies. As target users (the test set), they used the first 100 users whose user IDs are greater than 70,000 and who have rated more than 100 movies. Although this experimental setup is small-scale and neglected the adverse effect of data sparseness on prediction quality, to ensure the comparability of their work and ours, we nonetheless tested our algorithm under similar conditions. Ratings in the dataset were fuzzified using the membership functions in MF(C) (Fig. 1 c). Ta b l e 8 shows the results.
 but with a lower minimum confidence value. This is because when computing the support and confidence values (in percentages) of rules, our approach counts all transactions. The ASARM algorithm, however, counts only transactions con-taining the target item. This approach underestimates the support values of some items other than the target item, which in turns over-emphasizes the confidence values of rules, especially of less-popular items in the dataset. In other words, the confidence value assigned to the same association rule may be much higher in ASARM than in our approach. This difference in the number of transactions used to determine support and confidence values of rules is why our approach uses a lower minimum confidence.
 Lens dataset as well as on the KDD dataset. As the KDD dataset does not contain ratings data, the comparison uses only the MovieLens dataset. Figure 2 fshows the experimental results for the five Top-N values, with FARAMS outperforming MAR by 61.0, 46.3, 33.4, 25.7 and 23.0%, respectively. This shows that our ap-proach is effective. Furthermore, the significant improvement achieved in Top-10 recommendation proves that fuzzy confidence produces better rankings of recom-mended items. 6 Conclusions problem of information overload. It provides personalized recommendations to describes the FARAMS framework for CF and its application of fuzzy associa-tion rule mining to address the sharp boundary problem in existing techniques. It also approached the data sparseness and the nontransitive association problems by taking advantage of multiple-level similarities that are implicit in taxonomies of items. Further, this paper presented and discussed the results of an evaluation of FARAMS, finding that FAR is effective on datasets containing continuous rat-ings and that FARAMS outperforms existing techniques in similar experimental settings.
 count only categorical item information. Like most recommendation algorithms, it treats domain items as single, independent units. In complex recommender sys-tems, however, interitem relationships, such as component hierarchies, may exist [ 23 ]. The integration of these hierarchies into the recommendation process as a way of developing more sophisticated and intelligent recommender systems re-mains as a topic for future research.
 References
