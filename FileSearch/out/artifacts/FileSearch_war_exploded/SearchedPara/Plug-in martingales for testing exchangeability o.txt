 Valentina Fedorova valentina@cs.rhul.ac.uk Alex Gammerman alex@cs.rhul.ac.uk Ilia Nouretdinov ilia@cs.rhul.ac.uk Vladimir Vovk v.vovk@rhul.ac.uk Many machine learning algorithms have been devel-oped to deal with real-life high dimensional data. In order to state and prove properties of such algorithms it is standard to assume that the data satisfy the ex-changeability assumption (although some algorithms make different assumptions or, in the case of prediction with expert advice, do not make any statistical assump-tions at all). These properties can be violated if the assumption is not satisfied, which makes it important to test the data for satisfying it.
 Note that the popular assumption that the data is i.i.d. (independent and identically distributed) has the same meaning for testing as the exchangeability as-sumption. A joint distribution of an infinite sequence of examples is exchangeable if it is invariant w.r. to any permutation of examples. Hence if the data is i.i.d., its distribution is exchangeable. On the other hand, by de Finetti X  X  theorem (see, e.g., Schervish, 1995, p. 28) any exchangeable distribution on the data (a potentially infinite sequence of examples) is a mixture of distribu-tions under which the data is i.i.d. Therefore, testing for exchangeability is equivalent to testing for being i.i.d.
 Traditional statistical approaches to testing are inap-propriate for high dimensional data (see, e.g., Vapnik, 1998, pp. 6 X 7). To address this challenge a previous study (Vovk et al., 2003) suggested a way of on-line testing by employing the theory of conformal prediction and calculating exchangeability martingales. Basically testing proceeds in two steps. The first step is im-plemented by a conformal predictor that outputs a sequence of p-values. The sequence is generated in the on-line mode: examples are presented one by one and for each new example a p-value is calculated from this and all the previous examples. For the second step the authors introduced exchangeability martingales that are functions of the p-values and track the deviation from the assumption. Once the martingale grows up to a large value (20 and 100 are convenient rules of thumb) the exchangeability assumption can be rejected for the data.
 This paper proposes a new way of constructing mar-tingales in the second step of testing. To construct an exchangeability martingale based on the sequence of p-values we need a betting function, which determines the contribution of a p-value to the value of the martin-gale. In contrast to the previous studies that use a fixed betting function the new martingale tunes its betting function to the sequence to detect any deviation from the assumption. We show that this martingale, which we call a plug-in martingale, is competitive with all the martingales covered by the previous studies; namely, asymptotically the former grows faster than the latter. 1.1. Related work The first procedure of testing exchangeability on-line is described in Vovk et al. (2003). The core testing mechanism is an exchangeability martingale. Exchange-ability martingales are constructed using a sequence of p-values. The algorithm for generating p-values assigns small p-values to unusual examples. It implies the idea of designing martingales that would have a large value if too many small p-values were generated, and suggests corresponding power martingales. Other martingales (simple mixture and sleepy jumper) implement more complicated strategies, but follow the same idea of scoring on small p-values.
 Ho (2005) applies power martingales to the problem of change detection in time-varying data streams. The author shows that small p-values inflate the martingale values and suggests to use the martingale difference as another test for the problem. 1.2. This paper To the best of our knowledge, no study has aimed to find any other ways of translating p-values into a martingale value. In this paper we propose a new more flexible method of constructing exchangeability martingales for a given sequence of p-values.
 The rest of the paper is organised as follows. Section 2 gives the definition of exchangeability martingales. Section 3 presents the construction of plug-in exchange-ability martingales, explains the rationale behind them, and compares them to the power martingales, which have been used previously. Section 4 shows experimen-tal results of testing two real-life datasets for exchange-ability; for one of these datasets power martingales work satisfactorily and for the other one the greater flexibility of plug-in martingales becomes essential. Sec-tion 5 summarises the paper. This section outlines necessary definitions and results of the previous studies. 2.1. Exchangeability Consider a sequence of random variables Z 1 ,Z 2 ,... ) that all take values in the same example space. Then the joint probability distribution P ( Z 1 ,...,Z N ) of a finite number of the random variables is exchangeable if it is invariant under any permutation of the random variables. The joint distribution of infinite number of random variables Z 1 ,Z 2 ,... ) is exchangeable if the marginal distribution P ( Z 1 ,...,Z N ) is exchangeable for every N . 2.2. Martingales for testing As in Vovk et al. (2003), the main tool for testing ex-changeability on-line is a martingale. The value of the martingale reflects the strength of evidence against the exchangeability assumption. An exchangeability mar-tingale is a sequence of non-negative random variables S ,S 1 ,... that keep the conditional expectation: where E refers to the expected value with respect to any exchangeable distribution on examples. We also assume S 0 = 1. Note that we will obtain an equivalent definition if we replace  X  X ny exchangeable distribution on examples X  by  X  X ny distribution under which the examples are i.i.d. X  (remember the discussion of de Finetti X  X  theorem in Section 1).
 To understand the idea behind martingale testing we can imagine a game where a player starts from the capital of 1, places bets on the outcomes of a sequence of events, and never risks bankruptcy. Then a martin-gale corresponds to a strategy of the player, and its value reflects the acquired capital. According to Ville X  X  inequality (see Ville, 1939, p. 100), it is unlikely for any S n to have a large value. For the problem of testing exchangeability, if the final value of a martingale is large then the exchangeability assumption for the data can be rejected with the corresponding probability. 2.3. On-line calculation of p-values Let ( z 1 ,z 2 ,... ) denote a sequence of examples. Each example z i is the vector representing a set of attributes x i and a label y i : z i = ( x i ,y i ). In this paper we use conformal predictors to generate a sequence of p-values that corresponds to the given examples. The general idea of conformal prediction is to test how well a new example fits to the previously observed examples. For this purpose a  X  X onconformity measure X  is defined. This is a function that estimates the strangeness of one example with respect to others: Algorithm 1 Generating p-values on-line Input: ( z 1 ,z 2 ,... ) data for testing
Output: ( p 1 ,p 2 ,... ) sequence of p-values for i = 1 , 2 ,... do end for where in general { ... } stands for a multiset (the same element may be repeated more than once) rather than a set. Typically, each example is assigned a  X  X oncon-formity score X   X  i based on some prediction method. In this paper we deal with the classification problem and the 1-Nearest Neighbor (1-NN) algorithm is used as the underling method to compute the nonconformity scores. The algorithm is simple but it works well enough in many cases (see, e.g., Hastie et al., 2001, pp. 422 X 427). A natural way to define the nonconformity score of an example is by comparing its distance to the examples with the same label to its distance to the examples with a different label: where d ( x i ,x j ) is the Euclidean distance. According to the chosen nonconformity measure,  X  i is high if the example is close to another example with a different label and far from any examples with the same label. Using the calculated nonconformity scores of all ob-served examples, the p-value p n that corresponds to an example z n is calculated as where  X  n is a random number from [0 , 1] and the symbol # means the cardinality of a set. Algorithm 1 sum-marises the process of on-line calculation of p-values (it is clear that it can also be applied to a finite dataset ( z ,...,z n ) producing a finite sequence ( p 1 ,...,p n ) of p-values).
 The following is a standard result in the theory of conformal prediction (see, e.g., Vovk et al. 2003, Theo-rem 1).
 Theorem 1. If examples ( z 1 ,z 2 ,... ) (resp. ( z 1 ,z 2 z ) ) satisfy the exchangeability assumption, Algorithm 1 produces p-values ( p 1 ,p 2 ,... ) (resp. ( p 1 ,p 2 ,...,p that are independent and uniformly distributed in [0 , 1] . The property that the examples generated by an ex-changeable distribution provide uniformly and indepen-dently distributed p-values allows us to test exchange-ability by calculating martingales as functions of the p-values. This section focuses on the second part of testing: given the sequence of p-values a martingale is calculated as a function of the p-values.
 ( p 1 ,p 2 ,... ) be the sequence of p-values generated by Algorithm 1. We consider martingales S n of the form function f i ( p ) a betting function .
 To be sure that (2) is indeed a martingale we need the following constraint on the betting functions f i : Then we can check: E ( S n +1 | S 0 ,...,S n ) = Using representation (2) we can update the martin-gale on-line: having calculated a p-value p i for a new example in Algorithm 1 the current martingale value becomes S i = S i  X  1  X  f i ( p i ). To define the martingales completely we need to describe the betting functions f . 3.1. Previous results: power and simple Previous studies (Vovk et al., 2003) have proposed to use a fixed betting function where  X   X  [0 , 1]. Several martingales were constructed using the function. The power martingale for some  X  , denoted as M  X  n , is defined as The simple mixture martingale, denoted as M n , is the mixture of power martingales over different  X   X  [0 , 1]: Such a martingale will grow only if there are many small p-values in the sequence. This follows from the shape of the betting functions: see Figure 1. If the generated p-values concentrate in any other part of the unit interval, we cannot expect the martingale to grow. So it might be difficult to reject the assumption of exchangeability for such sequences. 3.2. New plug-in approach 3.2.1. Plug-in martingale Let us use an estimated probability density function as the betting function f i ( p ). At each step the probability density function is estimated using the accumulated p-values: where bility density function using the p-values p 1 ,...,p i  X  1 output by Algorithm 1.
 Substituting these betting functions into (2) we get a new martingale that we call a plug-in martingale. The martingale avoids betting if the p-values are distributed uniformly, but if there is any peak it will be used for betting.
 Estimating a probability density function. In our experiments we have used the statistical environ-ment and language R. The density function in its Stats package implements kernel density estimation with different parameters. But since p-values always lie in the unit interval, the standard methods of kernel den-sity estimation lead to poor results for the points that are near the boundary. To get better results for the boundary points the sequence of p-values is reflected to the left from zero and to the right from one. Then the kernel density estimate is calculated using the extended sample  X  n i =1  X  p i ,p i , 2  X  p i . The estimated density function is set to zero outside the unit interval and then normalised to integrate to one. For the results presented in this paper the parameters used are the Gaussian kernel and Silverman X  X   X  X ule of thumb X  for bandwidth selection. Other settings have been tried as well, but the results are comparable and lead to the same conclusions.
 The values S n of the plug-in martingale can be updated recursively. Suppose computing the nonconformity scores (  X  1 ,..., X  n ) from ( z 1 ,...,z n ) takes time g ( n ) and evaluating (3) takes time h ( n ). Then updating S n  X  1 to S n takes time O ( g ( n ) + n + h ( n )): indeed, it is easy to see that calculating the rank of  X  n in the multiset {  X  1 ,..., X  n } takes time  X ( n ).
 The performance of the plug-in martingale on real-life datasets will be presented in Section 4. The rest of the current section proves that the plug-in martingale provides asymptotically a better growth rate than any martingale with a fixed betting function. To prove this asymptotical property of the plug-in martingale we need the following assumptions. 3.2.2. Assumptions Consider an infinite sequence of p-values ( p 1 ,p 2 ,... ). (This is simply a deterministic sequence.) For its finite prefix ( p 1 ,...,p n ) define the corresponding empirical probability measure P n : for a Borel set A in R , We say that the sequence ( p 1 ,p 2 ,... ) is stable if there exists a probability measure P on R such that: 1. P n weak  X  X  X  X  X  X  X  2. there exists a positive continuous density function Intuitively, the stability means that asymptotically the sequence of p-values can be described well by a probability distribution.
 tions. (This is simply a deterministic sequence of func-tions f i : [0 , 1]  X  [0 ,  X  ), although we are particularly interested in the functions f i ( p ) =  X  i ( p ), as defined in (3).) We say that this sequence is consistent for ( p 1 ,p 2 ,... ) if Intuitively, consistency is an assumption about the algorithm that we use to estimate the function  X  ( p ); in the limit we want a good approximation. 3.2.3. Growth rate of plug-in martingale The following result says that, under our assumptions, the logarithmic growth rate of the plug-in martingale is better than that of any martingale with a fixed betting function (remember that by a betting function we mean any function mapping [0 , 1] to [0 ,  X  )).
 Theorem 2. If a sequence ( p 1 ,p 2 ,... )  X  [0 , 1]  X  is sta-is consistent for it then, for any positive continuous betting function f , lim inf First we explain the meaning of Theorem 2 and then prove it. According to representation (2) after n steps the martingale grows to Note that if for any p-value p  X  [0 , 1] we have f i ( p ) = 0 then the martingale can become zero and will never change after that. Therefore, it is reasonable to consider positive f i ( p ). Then we can rewrite product (4) as sum of logarithms, which gives us the logarithmic growth of the martingale: We assume that the sequence of p-values is stable and the sequence of estimated probability density functions that is used to construct the plug-in martingale is consistent. Then the limit inequality from Theorem 2 states that the logarithmic growth rate of the plug-in martingale is asymptotically at least as high as that of any martingale with a fixed betting function (which have been suggested in previous studies).
 To prove Theorem 2 we will use the following lemma. Lemma 1. For any probability density functions  X  and f (so that R 1 0  X  ( p ) dp = 1 and R 1 0 f ( p ) dp = 1 ), Proof of Lemma 1. It is well known (Kullback, 1959, p. 14) that the Kullback X  X eibler divergence is always non-negative: This is equivalent to the inequality asserted by Lemma 1.
 Proof of Theorem 2. Suppose that, contrary to the statement of Theorem 2, there exists  X  &gt; 0 such that lim inf Then choose an  X  satisfying 0 &lt;  X  &lt;  X / 4. Substituting the definition of  X  ( p ) into Lemma 1 we obtain From the stability of ( p 1 ,p 2 ,... ) it follows that there exists a number N 1 = N 1 (  X  ) such that, for all n &gt; N and Then inequality (6) implies that, for all n  X  N 1 , By the definition of the probability measure P n , the last inequality is the same thing as By the consistency of f 1 ( p ) ,f 2 ( p ) ,... there exists a number N 2 = N 2 (  X  ) such that, for all i &gt; N 2 and all p  X  [0 , 1], Let us define the number From (8) and (9) we have Denote N 3 = max ( N 1 ,N 2 ). Then, using (10) and (7), we obtain, for all n &gt; N 3 , 1 n Denoting N 4 = max ( N 3 , MN 3  X  ), we can rewrite the last inequality as for all n &gt; N 4 , This contradicts (5) and therefore completes the proof of Theorem 2. In this section we investigate the performance of our plug-in martingale and compare it with that of the simple mixture martingale. Two real-life datasets have been tested for exchangeability: the USPS dataset and the Statlog Satellite dataset. 4.1. USPS dataset Data The US Postal Service (USPS) dataset consists of 7291 training examples and 2007 test examples of handwritten digits, from 0 to 9. The data were collected from real-life zip codes. Each example is described by the 256 attributes representing the pixels for displaying a digit on the 16  X  16 gray-scaled image and its label. It is well known that the examples in this dataset are not perfectly exchangeable (Vovk et al., 2003), and any reasonable test should reject exchangeability there. In our experiments we merge the training and test sets and perform testing for the full dataset of 9298 examples.
 Figure 2 shows the typical performance of the martin-gales when the exchangeability assumption is satisfied for sure: all examples have been randomly shuffled before the testing.
 Figure 4 shows the performance of the martingales when the examples arrive in the original order: first 7291 of the training set and then 2007 of the test set. The p-values are generated on-line by Algorithm 1 and the two martingales are calculated from the same sequence of p-values. The final value for the simple mixture martingale is 2 . 0  X  10 10 , and the final value for the plug-in martingale is 3 . 9  X  10 8 .
 Figure 6 shows the betting functions that correspond to the plug-in martingale and the  X  X est X  power mar-tingale. For the plug-in martingale, the function is the estimated probability density function calculated using the whole sequence of p-values. The betting func-tion for the family of power martingale corresponds to the parameter  X   X  that provides the largest final value among all power martingales. It gives a clue why we could not see advantages of the new approach for this dataset: both martingales grew up to approximately the same level. There is not much difference between the best betting functions for the old and new meth-ods, and the new method suffers because of its greater flexibility. 4.2. Statlog Satellite dataset Data The Statlog Satellite dataset (Frank &amp; Asun-cion, 2010) consists of 6435 satellite images (divided into 4435 training examples and 2000 test examples). The examples are 3  X  3 pixel sub-areas of the satellite picture, where each pixel is described by four spectral values in different spectral bands. Each example is represented by 36 attributes and a label indicating the classification of the central pixel. Labels are num-bers from 1 to 7, excluding 6. The testing results are described below.
 Figure 3 shows the performance of the martingales for randomly shuffled examples of the dataset. As expected, the martingales do not reject the exchange-ability assumption there.
 Figure 5 presents the performance of the martingales when the examples arrive in the original order. The final value for the simple mixture martingale is 5 . 6  X  10 and the final value for the plug-in martingale is 1 . 8  X  10 17 . Again, the corresponding betting functions for the plug-in martingale and the  X  X est X  power martingale are presented in Figure 7. For this dataset the generated p-values have a tricky distribution. The family of power betting functions  X p  X   X  1 cannot provide a good approximation. The power martingales lose on p-values close to the second peak of the p-values distribution. But the plug-in martingale is more flexible and ends up with a much higher final value.
 It can be argued that both methods, old and new, work for the Statlog Satellite dataset in the sense of rejecting the exchangeability assumption at any of the commonly used thresholds (such as 20 or 100). However, the situation would have been different had the dataset consisted of only the first 1000 examples: the final value of the simple mixture martingale would have been 0 . 013 whereas the final value of the plug-in martingale would have been 3 . 74  X  10 15 . In this paper we have introduced a new way of con-structing martingales for testing exchangeability on-line. We have shown that for stable sequences of p-values the new more adaptive martingale provides asymptotically the best result compared with any other martingale with a fixed betting function. The experi-ments of testing two real-life datasets have been pre-sented. Using the same sequence of p-values the plug-in martingale extracts approximately the same amount or more information about the data-generating distribu-tion as compared to the previously introduced power martingales.
 Remark. The previous studies were based on the nat-ural idea that lack of exchangeability leads to new examples looking strange as compared to the old ones and therefore to small p-values (for example, if the data-generating mechanism changes its regime and starts producing a different kind of examples). This is, how-ever, a situation where lack of exchangeability makes the p-values cluster around 1: we observe examples that are ideal shapes of several kinds distorted by ran-dom noise, and the amount of noise decreases with time. Predicting the kind of a new example using the nonconformity measure (1) will then tend to produce large p-values.
 Our goal has been to find an exchangeability martin-gale that does not need any assumptions about the p-values generated by the method of conformal predic-tion. Our proposed martingale adapts to the unknown distribution of the p-values by estimating a good bet-ting function from the past data. This is an example of the plug-in approach. It is generally believed that the Bayesian approach is more efficient than the plug-in approach (see, e.g., Bernardo &amp; Smith, 2000, p. 483). In our present context, the Bayesian approach would involve choosing a prior distribution on the betting functions and integrating the exchangeability martin-gales corresponding to these betting functions over the prior distribution. It is not clear yet whether this can be done efficiently and, if yes, whether this can improve the performance of exchangeability martingales.
