 In this paper, we study  X  X etworked bandits X , a new bandit problem where a set of interrelated arms varies over time and, given the contextual information that selects one arm, invokes other correlated arms. This problem remains under-investigated, in spite of its applicability to many practical problems. For instance, in social networks, an arm can ob-tain payo ff s from both the selected user and its relations since they often share the content through the network. We examine whether it is possible to obtain multiple payo ff s from several correlated arms based on the relationships. In particular, we formalize the networked bandit problem and propose an algorithm that considers not only the selected arm, but also the relationships between arms. Our algorithm is  X  X ptimism in face of uncertainty X  style, in that it decides an arm depending on integrated confidence sets constructed from historical data. We analyze the performance in sim-ulation experiments and on two real-world o ffl ine datasets. The experimental results demonstrate our algorithm X  X  e ff ec-tiveness in the networked bandit setting.
 H.3.5 [ Information Systems ]: Social and Information Net-works; 1.2.6 [ Computing Methodologies ]: [Learning] Algorithm, Theory Networked bandits; social network; exploration/exploitation dilemma
Amulti-armedbanditproblem(orbanditproblem)isa sequential decision problem defined by a set of actions (or arms). The term  X  X andit X  originates from the colloquial term for a casino slot machine ( X  X  one-armed bandit X ), in which a player (or a forecaster) faces a finite number of slot machines (or arms). The player sequentially allocates coins (one at atime)todi ff erentmachinesandearnsmoney(orpayo ff ) depending on the machine selected. The goal is to earn as high a payo ff as possible.

Robbins formalized this problem in 1952 [20]; in the multi-armed bandit problem, K arms exist that are associated with unknown payo ff distributions, and a forecaster can se-lect an arm sequentially. In each round of play, a forecaster selects one arm and then receives the payo ff from the se-lected arm. The forecaster X  X  aim is to maximize the total cumulative payo ff , i.e., the sum of the payo ff s of the chosen arms in total. Since the forecaster does not know the process generating the payo ff s but has historical payo ff information, the bandit problem highlights the fundamental di ffi cultly of decision making in the face of uncertainty: balancing the de-cision of whether to exploit past choices or make new choices with the hope of discovering a better one.

The bandit problem has been studied for many years, with works primarily focusing on the theory and designing dif-ferent algorithms based on di ff erent settings, such as the stochastic setting, adversarial setting, and contextual set-ting [9]. In real-world applications, the multi-armed bandit problem is an e ff ective way of solving situations where one encounters an exploration-exploitation dilemma. It has his-torically been used to decide which clinical trial is better when multiple treatments are available for a given disease and there is a need to decide which treatment to use on the next patient.

Modern technologies have created many opportunities for use of the bandit problem, and it has a wide range of applica-tions including advertising, recommendation systems, online systems, and games. For example, an advertising task may be the choice of which advertisement to display to the next visitor to a web page, where the payo ff is associated with the visitor X  X  actions. More recently, the bandit algorithm has been used in personalized recommendation tasks [17], where a user visits a website and the system collects the user X  X  information. The system selectively provides content from a content pool through the user X  X  current and past be-haviors analyzing to best satisfy the user X  X  needs, and the payo ff is based on user-click feedback.

All the above bandit problems have the major underly-ing assumption that all the arms are independent, which is inappropriate for web-based social network applications. In a network, including social networks, the users are con-nected by relationships [2, 22]. Contextual information can new payo ff information. be obtained from other users and can be spread via these relationships. Content promoted to one user provides feed-back, not only from that user, but also from his/her rela-tions. For example, a user of Twitter or Facebook can read a tweet/message and can re-post someone else X  X  tweet/message, allowing the user to quickly share it with his/her follow-ers. Impact can be assessed by counting the number of  X  X a-vorites/likes X  from di ff erent users X  pages. Therefore, careful selection of a user for tweet/message posting can maximize the number of  X  X avorites/likes X .

Our study is motivated by the observation that even when auserisrandomlyselectedforpromotion,otherusersclose to the selected user in the network will be influenced [18, 22]. Specifically, as shown in Figure 1, in a social network, if we promote a content to a user, the user may share it with others and the payo ff s can be collected from the user and its relations. The goal is to gain higher payo ff s. The process is similar to share-then-like, which occurs daily in social net-works and needs to be considered for optimized recommen-dation and advertising tasks, the important point being that the context is extended from the selected user to all other invoked users. There are several challenges to realizing this problem. First, only partial information is available about the chosen users when content is posted, and the information of other users is unknown. Therefore, there is a dilemma of whether the system should select a user with the best payo ff history or a new user in order to explore more possibilities. Second, the content may frequently change and few overlap-ping historical records may exist. Furthermore, relationships exist between users and these relationships may change over time. These challenges inspire us to formalize the networked bandit problem.

The above problem can be considered a balance of the trade-o ff between exploration (discovering a new user) and exploitation (using the current best user) when network topol-ogy is known.

We formalize a well-defined but simple setting for the net-worked bandit problem, in which there exist K arms con-nected by network topology G .Weproposeanapproach in which a learning algorithm optimally selects an arm at each round based on contextual information and the net-work topology information of arms. The networked bandit problem can be considered an extension of the contextual multi-armed bandit problem, the di ff erence being that in our problem the arm can be connected to other arms and the payo ff s come from the multiple arms.

Our contribution is three-fold: first, we formalize a new networked bandit problem motivated by real network ap-plications; second, we provide an algorithm based on con-fidence sets to solve it along with theoretical analysis; and third, we design a set of experiments to test and evaluate the algorithm. To the best of our knowledge, we define and solve this problem for the first time and answer the fundamental question of how to define regret when payo ff s come from in-terrelated multiple arms. We design an e ff ective strategy to select arms in order to increase payo ff s over time, known as NetBandits, which provides a solution to this problem. Our approach is an  X  X ptimism in the face of uncertainty X -style algorithm that considers the integrated confidence sets and we prove a regret bound for it. Finally, we analyze empir-ically the performance, which shows that our algorithm is e ff ective in the networked bandit setting.
The traditional multi-armed bandit problem does not as-sume that side information is observed. The forecaster X  X  goal is to maximize the sum of payo ff s over time based on the historical payo ff information. There are two basic set-tings. In the first, the stochastic setting, the payo ff s are i.i.d. drawn from an unknown distribution. The upper con-fidence bound (UCB) strategy has been used to explore the exploration-exploitation trade-o ff [4, 6, 16], in which an up-per bound estimate is constructed on the mean of each arm at a fixed confidence level, and then the arm with the best estimate is selected. In the second, the adversarial setting, the i.i.d. assumption does not exist. Auer et al. [7] proposed the EXP3 algorithm for the adversarial setting, which was later improved by Bubeck and Audibert [3].

The contextual multi-armed bandit problem is a natural extension of the original bandit problem. Our setting ad-dresses bandit problem with contextual information. Com-pared to the traditional K -armed bandit problem, the fore-caster may use action features to infer the payo ff in the con-textual setting. This problem largely considers the linear model assumption about payo ff of action [1, 5, 12, 14, 21]. Auer [5] proposed the LinRel algorithm, a UCB-style algo-rithm that has a regret of  X  O ( ied the LinRel and provided an  X  O ( d proved this upper bound is tight. Chu et al. [12] provided the LinUCB and SupLinUCB algorithms, and proved an O ( ! that holds with probability 1  X   X  .Abbasi-Yadkorietal.[1] proposed an algorithm that modified the UCB-style algo-rithm based on the confidence sets, and showed a regret of O ( d log(1 /  X  ) /  X  ).

Recently, the bandit problem has been used in real-life problems, such as recommendation systems and advertising. Li et al. [17] first introduced the bandit problem to recom-mendation systems by considering a personalized recommen-dation as a feature-based exploration-exploitation problem. This problem was formalized as a contextual bandit problem with disjoint linear payo ff s and by focusing on the article-selection strategy based on user-click feedback, maximizing the total number of clicks. The features of the users and articles were defined as contextual information, and the ex-pected payo ff of an arm was assumed to be a linear function of its contextual features, including the user and article in-formation. Finally, the LinUCB algorithm was proposed to solve this problem and attained a good empirical regret. They further extended the algorithm as SupLinUCB and provided the theoretical analysis [12].

There are limited studies that consider the networked ban-dit problem or that combine bandit problem and network. Buccapatnam et al. [10] considered the bandit problem in social networks, and assumed that the forecaster can take advantage of side observations of neighbors, except for the selected user (arm). The side observations were used to up-date the sample mean of other related users and the payo ff of the selected arm was collected each time, the goal once again being to maximize the total cumulative payo ff of se-lected arms. Bnaya et al. [8] considered a bandit view for network exploration and proposed VUCB1 to handle the dynamic changes in arms when crawling the network. More recently, Cesa-Bianchi et al. [11] considered the recommen-dation problem by taking advantage of the relationships be-tween users in the network. They proposed GOB.Lin, which models the similarity between users and used this similarity to help predict the behavior of other users.

Our work belongs to the contextual bandit setting. How-ever, in contrast to these previous studies we assume that the arms (actions) are correlated in the network. The se-lected arm can invoke other related arms and the forecaster obtains multiple payo ff s from these arms. It is a more gen-eral setting in networked bandit problem.
We consider a network G .Let V indicate the nodes in the network and E indicate the edges of the network. We can then use G =( V , E )torepresentthenetworkedbandits where v  X  V can be considered as an arm and e  X  E indicates the relationship between arms; nodes here are correlated. Thus, given the network G and a node v ,itispossibleto obtain the information for the node v and its relations N ( v ).
In our formulation, we consider a sequential decision prob-lem with contextual information. At round t , except for con-textual information x t ,wehaveanetworktopologyofarms denoted by G t . Given v we let N t ( v )beitsrelationsand N ( v )maychangeovertime.If v is selected then N t ( v )will also be invoked. We define this setting as networked ban-dits. Formally, a networked bandit algorithm A proceeds as follows: at each round t ,thealgorithmobservesasetofarms K t = { 1 , 2 ,  X  X  X  ,k } t ,contextualinformation x t ,andthenet-work topology G t of arms associated with the relationships of arms. The set of relations of arm a is denoted by N t ( a ). If we also consider the information of arms, we can redefine formation. When the algorithm selects an arm a t ,the a t invoke other related arms N t ( a t ). N t ( a t )canbeobserved based on the network topology of arms. Before the decision algorithm selects the arm, it observes G t , C t ,and K t on historical payo ff records, the algorithm selects an arm a and receives a set of payo ff s { y a t }  X  { y a | a  X  N algorithm will improve the selection strategy after collect-ing new payo ff information. It then proceeds to the next round t +1. Notethattraditionalcontextualbanditprob-lems usually assume that the arms are independent. How-ever, in our problem, we assume that the correlation exists between the chosen arm and its relations. After a total T rounds the cumulative payo ff is defined as g a .Forsimplicity,weuse N t ( a )toindicateboth a and its relations. We rewrite g a t ,t as g a t ,t =
For this networked bandit problem, the algorithm A se-lects an arm a t at each round t =1 , 2 ,  X  X  X  and receives the associate payo ff g a t ,t .After n selections a 1 ,a 2 ,  X  X  X  ,a define the regret as follows: The regret can now be used to compare the best decision with the algorithm A .Inthisproblem, R n is a random variable; therefore, the goal is to calculate the expectation of R n with high probability, and it is not easy to obtain expectation directly since its search space is large. Normally we try to bound the pseudo-regret, i.e., where the pseudo-regret competes against the optimal action in the expectation.

There are two important issues in the networked bandit problem: arms and their network topology. In the context of a social network, the users in the pool may be viewed as arms, the provided message or article as context, and the user X  X  information as additional contextual information. The new context vector then summarizes information of both user and context. A payo ff of 1 is incurred when aprovidedmessageis X  X avorited X  X r X  X iked X ;otherwise,the payo ff is 0. The network topology of a social network nat-urally constructs the relationships between users. When a message is posted to a user, the message can be seen by relations (followers). The payo ff can be collected from the user X  X  page (selected arm). Furthermore, any  X  X ike X ,  X  X hare X , or  X  X omment X  action by a follower will allow the message to be reposted on the follower X  X  page and to be seen by the follower X  X  friends. The payo ff can then be collected from the followers X  pages (invoked arms). In the special case that the follower does not repost the message, the payo ff can be con-sidered as 0 or the arm is not invoked. For simplicity, we only consider the selected arm and its relations. With these definitions of payo ff , arm, and invoked arms, the collected payo ff after selecting an arm involves the selected user and his/her relations. Thus, the payo ff at round t is defined as g
It is assumed that algorithm A can observe the network topology prior to make a decision. This is intuitive, since network structure information between users can easily be collected or the network structure information can be ob-tained in advance. In practice, given an arm, we only need concern itself with the invoked arms, and therefore knowl-edge of full network topology is unnecessary. The invoked arms depend on how we define N t ( a ). The worst case sce-nario is that the whole network needs to be searched to find the invoked arms and feedback; however, we do not concern how to constrict N t ( a )usingsuchanetworkpropagation model since, as stated above, we only focus on the selection strategy and we simplify the problem by only observing the invoked arms.
In this work, we propose an algorithm to solve the net-worked bandit problem and show that an integrated confi-dence bound can e ffi ciently be computed in a closed form when the payo ff model of an arm is linear. As with previous contextual bandit work [17], we assume that the expected payo ff of an arm a is linear in context x t and coe ffi cient w At round t ,forarm a given context x a,t ,weassumethatthe expected payo ff of the arm a is a linear function: where di ff erent arms have di ff erent w a and  X  a is conditionally R -sub-Gaussian when R  X  0isafixedconstant. Formally, this means that &amp;  X  and we have where x a, 1: t denotes the sequence x a, 1 ,x a, 2 ,  X  X  X  ,x similarly  X  a, 1: t  X  1 denotes the sequence  X  a, 1 ,  X  X  X  ,  X  arms therefore have disjoint linear payo ff s. The decision of the algorithm lies on w with distribution  X  .BasedonourR-sub-Gaussian assumption of the noise, we can obtain mean-ingful upper bound on the regret. According to this sub-Gaussian condition, we know that E [  X  a,t | x a, 1: t ,  X  show that  X  a,t is bounded by a zero-mean noise lying in an interval of length of at most 2 R .

As the networked bandit problem, the algorithm faces a set of uncertainties of arms which involve N t ( a ). We design anewalgorithmwhichisthe X  X ptimisminthefaceofuncer-tainty X  principle, by maintaining confidence of parameter w for each arm. The basic idea is to construct the confidence sets for parameters of each disjoint payo ff function and then provide an integrated upper bound.

We use technology from the  X  X elf-normalized bound for vector-valued martingales X  [19] and confidence sets [1]. For each arm  X  w a is defined as the L 2 -regularized least-squares estimate of w  X  a with regularization parameter  X  &gt; 0: where X a is the matrix whose rows are x 1 ,  X  X  X  ,x n a ( t ) sponding to historical contexts of an arm a and Y a  X  R n is the corresponding historical payo ff vector. For a positive definite self-adjoint operator V ,wedefine  X  x  X  V = as the weighted norm of vector x .Itcanbeprovedthat X  w lies with high probability in an ellipsoid centered at w  X  follows:
Theorem 1. [1, 19] According to the  X  X elf-normalized bound for vector-valued martingales X , let V =  X  I,  X  &gt; 0 ,and V V + ing the covariates. Define y t = x  X  t w  X  +  X  t and assume that  X  w  X   X  2  X  S .Then,forany 0 &lt;  X  &lt; 1 , with probability at least 1  X   X  ,forall t  X  1 we can bound w  X  in such a confidence set:
C In addition, if  X  x t  X  X  X  L then with probability at least 1  X   X  , for all t  X  1 ,wecanbound w  X  in a new confidence set:
The above bound provides the confidence region at time t .Itshowsthatwithgoodchoiceoftherightpartsofthe equation, w  X  always remains inside this ellipsoid for all times t with probability 1  X   X  .Next,weshowtheboundofthe arm with a single linear payo ff .

Theorem 2. Let ( x 1 ,y 1 ) ,  X  X  X  , ( x t  X  1 ,y t  X  1 ) , x R satisfy the linear model assumption. Furthermore, we have the same assumption as Theorem 1. Then, for any 0 &lt;  X  &lt; 1 ,withprobabilityatleast 1  X   X  ,forall t  X  1 we can have:  X  x  X   X  w  X  x  X  w  X   X  X  X  In addition, if  X  x t  X  X  X  L then for all t  X  1 , with probability 1  X   X  we can have:
Proof. According to (6), with probability at least 1  X   X  ,forall t  X  1, we have:  X  x  X   X  w  X  x  X  w  X   X  X  X  According to (7), with probability at least 1  X   X  ,forall t  X  1 we have:
Lemma 1. Given an arm a  X  K t with the context feature history records of arm a before t and x a  X  X a and y a  X  Y and let  X  w a =( X  X  a X a +  X  I )  X  1 X  X  a Y a .Wehave As shown in (10), we have a possible upper bound of x  X  w which has two parts. The first term can be deemed as em-pirical expected estimation of payo ff of the arm, and the second term can be considered as a penalty. This penalty is typically a high probability upper confidence bound on the payo ff of the arm.

Thus, given an arm a and its relations N t ( a ), we face the exploitation-exploration problem. We use the integrated confidence bound on the payo ff s of these invoked arms.
Lemma 2. In the networked bandits, given an arm a  X  K t and the network relationship N t ( a ) ,weobtain: # #
We believe that the confidence bound can be successfully applied to this situation with the exploitation-exploration trade-o ff . We use the confidence bound generated by the confidence sets of parameters, defined by:  X   X  ( t )isthelasttermof(11)andindicatesthepenaltyofthe estimation. Figure 2 shows the upper bound of arms from our illustrative example. Each arm has the empirical payo ff and a potential value. Thus, in each round, our algorithm selects an arm based on the estimation from the confidence bound, such that the predicted payo ff is maximized. Our algorithm is shown in Algorithm 1.
We next provide a bound on the regret of our algorithm when run through the confidence sets constructed in Theo-rem 1. We assume that the expected estimation of payo ff is bounded. We can view this as a bound on parameters and the bound on the arms set. We state a bound on the regret of the algorithm as follows: Figure 2: An example of the upper bound B in 10-arm networked bandits when t =120. Bardenotesthepayo ff estimation and vertical line denotes the penalty of the esti-mation.
 Algorithm 1 NetBandits Input: K t , G t , C t , t =1 ,  X  X  X  ,T 1: for round t =1 , 2 ,  X  X  X  ,T do 2: For each arm we can observe the features x a,t ,a  X  K 3: for each a  X  K t do 4: Compute  X  w a according to (5) 5: Compute the quality 6: end for 7: Choose arm a t =argmax a  X  K 8: Observe the multiple payo ff s { y a,t | a  X  N t ( a 9: for each node a  X  N t ( a t ) do 10: Update X a ,Y a 11: end for 12: end for
Theorem 3. On the networked bandits, assume that each arm X  X  payo ff function satisfies the linear model, and assume that the contextual vector is x a,t for each arm a  X  K t , |K K and t =1 ,  X  X  X  ,T .Then,forany 0 &lt;  X  &lt; 1 ,withproba-bility at least 1  X   X  ,thecumulativeregretsatisfies where
Proof. Considering the instantaneous regret at round t , we select an optimal arm according to our algorithm. Thus, we have optimistic ( a t ,  X  w a )and  X  w a for the N ( a t ,werelyon[1]tohave: For each arm a  X  N t ( a ), we define and we have We then rewrite the instantaneous regret (14) as Regarding to the fact that r a,t  X  2, we have Given arms N t ( a ), we define a  X  =argmax a  X  x a,t  X  2 we have Thus, with probability at least 1  X   X  ,forany T  X  1, According to log (1 + z )  X  z ,wehave: Then according to z  X  2log(1+ z ) ,z  X  [0 , 1], we have We choose V =  X  I ,thenwerewrite R T as
Lemma 3. Assume that x  X  R d and V =  X  I .Then,for any 0 &lt;  X  &lt; 1 ,withprobabilityatleast 1  X   X  ,theboundis  X  / We are mainly interested in the interrelated arms. Our re-gret bound depends on the number of invoked arms | N t ( a ) | or loose K .Figure3showsexperimentationofourbound applied to the networked bandit problem. Our algorithm keeps the regret as low as possible, and can reach R t /t  X  0 with high probability when t is large enough.
 Figure 3: An example of the regret value in 10-arm net-worked bandits. The experiments are repeated 100 times and the average regrets are shown. y = x is provided for comparison.
In real-world applications, according to di ff erent assump-tions about the network topology of arms, we can consider special cases of the networked bandit problem. We focus on N ( a ). In our algorithm, we make the very loose assumption that N t ( a )variesovertime.However,thenetworktopology is sometimes stable over a fixed duration. For example, a school social network is stable for the duration of a semester. This means that N t ( a )= N 0 ( a ), which is a special case of network bandits. In other cases, for example when inquiring users in the same company, we only need to consider their colleagues or a group of interest.
In the networked bandit problem the network topology of arms is usually dynamic over time, which means for each round t we have di ff erent N t ( a ). Although we assume that N ( a )willbeactiveaftertheforecasterselects a ,weomit how to generate N t ( a )andhowarm a invokes N t ( a ), which are not our primary concerns. Instead, we simplify our prob-lem using the simple setting of selecting an arm and then receiving payo ff s from invoked arms. We assume that we can observe invoked arms N t ( a ). In practice, we can di-rectly obtain N t ( a )bypredefiningthearms,forexampleas neighbors or groups, or by observing feedback and collect-ing arms which provide feedback. We focus on how to select arms in order to maximize the total payo ff , and therefore we concern the arms of N t ( a )andtheforecastercanobtainthe invoked bandits over the course of collecting the payo ff s from the network. In particular, at each round the algorithm ob-serves the network topology of arms; it then decides which arm to select using the knowledge of the network topology and historical payo ff information.

In Algorithm 2, we provide a pseudo-code for the selection at each round in a dynamic network.
 Algorithm 2 Selection at round t in dynamic network 1: For each arm we have  X  w a and observer the context x a,t 2: For each arm we collect N t ( a ) 3: for each a  X  K t do 4: Compute B a,t 5: end for 6: Select arm a t =argmax a  X  K 7: Observe the payo ff s { y a,t | a  X  N t ( a t ) } from the network 8: For each arm a  X  N t ( a t )update X a , Y a and  X  w a
We make the simple assumption that the network topol-ogy is fixed. In other words, the relationships between arms do not change. For all t ,wehave G t = G 0 , K t = K 0 and N ( a )= N 0 ( a ). For example, DBLP, Last.FM, and many o ffl ine social network datasets are of fixed duration. This is adegenerateversionofourproblemandcanbesolvedusing our algorithm.

In Algorithm 3, we provide a pseudo-code for the selection at each round in a static network.
 Algorithm 3 Selection at round t in static network 1: For each arm we have  X  w a and N 0 ( a ), and observer the 2: for a  X  K 0 do 3: Compute B a,t 4: end for 5: Select arm a t =argmax a  X  K 6: Observe the payo ff s { y a,t | a  X  N 0 ( a t ) } from the network 7: For each arm a  X  N 0 ( a t )update X a , Y a and  X  w a
In networks, especially in social networks, a simple yet common assumption is that the node largely influences its neighborhoods or community [13, 15]. Moreover, some ap-plications only focus on people who have the same interest or are in a group. This makes it possible to assume that the selected arm only invokes its neighbors or a group; that is, N ( a )= Neig t ( a ), where Neig t ( a )indicatestheneighbors of a .Wecanonlycollectthepayo ff sofneighborsofanarm, and therefore N t ( a )isappropriate.Althoughtherearealso two cases in this situation -static and dynamic -here we focus only on the neighborhood.

In Algorithm 4, we provide a pseudo-code for the selection at each round with specific N t ( a ).
 Algorithm 4 Selection at round t with neighborhood 1: For each arm we have  X  w a and observer the context x a,t 2: For each arm we collect Neig t ( a ) 3: for a  X  K t do 4: Compute B a,t 5: end for 6: Select arm a t =argmax a  X  K 7: Observe the payo ff s { y a,t | a  X  Neig t ( a t ) } from the net-8: For each arm a  X  Neig t ( a t )update X a , Y a and  X  w
We first illustrate our model by a synthetic example (Fig-ure 4), which contains 10 arms (A0-A9) randomly connected at each round. At di ff erent rounds, the networks are di ff er-ent. At rounds t =11and t =20,theupperbound B (the second row, blue) is large; however, the expected estimation is small (the third row, red) because the variance is large. Our algorithm selects the arm with maximal upper bound. We also show the real payo ff s of all the arms, which are not known to the algorithm. At an early stage, the selection is poor compared to the real payo ff (the fourth row, green). At round t =20,ouralgorithmchoosesA0however,thebest is A1, illustrating that the expected estimation is small and the algorithm can try another arm that has potential, but with uncertainty. Later, selection becomes e ffi cient and at t =120thealgorithmchoosesA1sincemoreinformation has been learned and the upper bound becomes more stable with lower penalty. This is close to the real situation and provides a good estimation.
In this section we evaluate the proposed NetBandits strat-egy on four synthetic datasets and two public real-world datasets. We perform two types of experiments: simulation experiments and o ffl ine evaluation of two real applications.
We compare our proposed method against two baselines: astate-of-artalgorithmforthecontextualbanditproblem, referred to as TraBandits, and the random strategy. Since there is no existing method for the networked bandit prob-lem, these methods are little altered for networked bandits. The details are follows:
We use two methods to assess the performance of our al-gorithm. We first analyze the average payo ff at each round, and then we analyze the cumulative payo ff at each round, which ignores the performance of the algorithm at each fixed round but gives an overall view of the lifetime performance of the algorithm. We test our algorithm on a series of synthetic datasets. In contrast to previous work, we need to construct the net-work topology, which can be either static or dynamic. Static network is a special case of dynamic situation and static net-work is generated in advance and remains unchanged. We therefore construct the networked bandits based on the dy-namic network as follows: we first construct a fixed number of nodes k ,whichareconsideredasarms.Wethenrandomly create edges between them, which are used to generate the relations for each arm. Neighborhood is considered as rela-tionship. For every node, we assign di ff erent norm random vector u i ,u i  X  R 10 and we use the following stochastic model to generate its payo ff s: y a ( x )= x  X  u a +  X  a ,where  X  formly distributed in a bounded interval centered around zero and u a and  X  a are not known to the decision algorithm. For contextual information, at each round t we randomly create a set of context vectors { x 1 ,t ,  X  X  X  ,x k,t } ,x The network topology does not have strict assumptions and is created simply: in the dynamic situation, we generate the network topology at each round (relationships between the nodes change at each round). We randomly create k 2 / 3 edges between the nodes, and therefore for most nodes the relations will be no greater than k/ 3.

We present the results from k =10, 100, 1000, and 10000 arms with dynamic network topology. In Figure 5 and Fig-ure 6 we present the results of average payo ff and cumula-tive payo ff ; our NetBandits outperforms the other baselines. TraBandits does not work well, indicating that best single arm does not always have the best payo ff in a network but also depends on its relations. As per the network construc-tion, the average payo ff for each node is around k/ 3ifthe node and its relations provide feedback, and the average pay-o ff of TraBandits and Random is around k/ 3. For example, as shown in Figure 5, when k =10thepayo ff rangesfrom3.2 to 3.6; when k =100thepayo ff rangesfrom34to35;when k =1000thepayo ff rangesfrom340to350;when k =10000 the payo ff ranges from 3450 to 3550. However, NetBandits usually performs better except the earliest time points, and its value is greater than 4.2 when k =10,40when k =100, 400 when k =1000,and4500when k =10000. Thisisbe-cause NetBandits performs more exploration than exploita-tion to begin with. Figure 6 shows that our algorithm ob-tains the best cumulative payo ff over all rounds. As average payo ff improves, NetBandits also exhibits higher cumulative payo ff . This indicates that more early exploration improves later selections, leading to a fairer assessment of the perfor-mance of the di ff erent algorithms.

The running time of NetBandits according to di ff erent numbers of arms and network topology is also shown in Ta-ble 1, and demonstrates the running time increases rapidly as the number of the scale of the networks increase. For example k =100isslowerthan k =10bymorethan k 2 but Arms 10 100 1,000 10,000 Avg of invoked arms 3 33 333 3333 Total round 100 1,000 10,000 100,000 Time (second) 0.1 23.4 2034.2 173,628.3 Table 1: Running time results of NetBandits on four syn-thetic datasets. less than k 3 .Thetimetakendependsonthesizeofthenet-work, including the number of nodes and edges. The time complexity of NetBandits is O ( TKN  X  ), where T is the to-tal number of rounds, K is the number of arms, N indicates the average number of invoked arms, and  X  indicates the time taken to compute the parameters; it is no more than O ( TK 2  X  ) where N = K .Itcanbeimprovedbycalculating each arm in parallel for a large number of arms.
We also test our algorithm on two publicly available real-world datasets 1 : Delicious Bookmarks, a dynamic dataset, denoted by Del; and Last.FM, a static dataset, denoted by LFM.

Delicious Bookmarks is a social network for storing, shar-ing, and discovering web bookmarks. The Del dataset con-tains 1,861 nodes and 7,668 edges and 69,226 URLs de-scribed by 53,388 tags. Payo ff s are created using the in-http://grouplens.org/node/462 formation about the bookmarked URLs for each user: the payo ff is 1 if the user bookmarked the URL, otherwise the payo ff is 0. Pre-processing is performed by breaking the tags down into smaller fragile items made up of single words, ig-noring the underscores, hyphens, and dashes. Each word is represented using the TF-IDF context vector based on the words of all tags, i.e., these feature vectors are the context vectors. PCA was performed on the dataset and the first 16 principle components selected as context vectors building a linear function based on payo ff records for each user. This linear function generates a payo ff when given a new context. At each round t , we provide x k,t  X  R 16 for all users k .
The Last.FM dataset is a music website that builds a de-tailed profile of each user X  X  musical taste by recording details of the tracks that the user listened to from a range of dig-ital devices. LFM contains 1,892 nodes and 12,717 edges and has 17,632 artists described by 11,946 tags. We use the listened-to artists information to construct payo ff s: if the user listened to an artist at least once the payo ff is 1, oth-erwise the payo ff is 0. Similar pre-processing is performed as Delicious Bookmarks. Compound tags are broken down into several corresponding single words resulting in 6,036 words. We represent context features using the TF-IDF fea-tures, and after PCA the first 16 principle components are selected as context vectors. For each user we then build a linear function based on payo ff records. This linear function can generate a payo ff when given a new context. At each round, we provide x k,t  X  R 16 for all users k .

We construct the network topology according to the social network of the users. Neighborhood is considered as rela-tionship. The linear payo ff function for each user is learned in advance and unknown to the algorithm, which decides its next selection according to previous feedback.

For the Del dataset, there exists the timestamp informa-tion that records when contact relationships were created, and we can therefore construct a dynamic network according to the timestamps. Timestamps are from 1146752335000 to 1288104100000; we therefore divide them into 14 groups ac-cording to the first three numbers (114, 115, . . . , 128). We set the total rounds T =14000andupdatethenetworkev-ery 1000 rounds. For the Last.FM dataset, there is no time information, thus we construct a static network.

The results of average payo ff and cumulative payo ff are shown in Figure 7. Our algorithm outperforms the other baselines. Although the two networks have a similar number of users, LFM has more relationships and the average and cumulative payo ff results are higher than Del. For the aver-age payo ff of Del, there exist three low intervals marked by (red) rectangles in Figure 7(a). These occurred at the begin-ning and close to round t =1000and t =10000.Sincemany new nodes and edges are added at these rounds and NetBan-dits performs more exploration than exploitation. The pay-o ff s improve after exploration. For the average payo ff results of LFM, there is a low interval at the start, denoted by the (red) rectangle in Figure 7(c), because NetBandits is trying to select possible better arms and perform exploration; then later the performance improves. Figure 7(b)(d) show that the cumulative payo ff results of NetBandits increase faster, and are much greater than the other algorithms, and demon-strate that exploration does not hurt the total performance.
In this paper we formalize a new bandit problem, termed networked bandits. We presented the novel problem of how to select the arm with multiple payo ff s in networked bandits by considering a multi-armed bandit of interconnected arms, one of which can invoke other related arms at each round. After selecting an arm, we can obtain payo ff s from this arm and its relations.

We consider this approach in the contextual bandit setting and assume disjoint linear payo ff s for arms. We propose a new networked bandit algorithm NetBandits that considers the uncertainty of the payo ff s using integrated confidence sets. We also provide a regret bound for our solution. Our experiments show that it is better to consider both the net-work topology and the payo ff s of arms, and we observe that our approach performs well in this setting.

The networked bandit problem requires further work. Some interesting problems still remain, such as how to model N In our work we do not make any assumption about the struc-ture of the network topology; for example the hub may have higher priority, and it is possible to find a more e ffi cient method for some fixed structures.

Another problem is arm complexity. We assume that one arm invokes other arms, which in turn can invoke other arms sequentially, with processing occurring at the same time. However in some real applications, the structure is possible to be much more complex and evolve over time, which is likely to delay the payo ff s. This work is supported by Australian Research Council Projects FT-130101457 and DP-140102164.
