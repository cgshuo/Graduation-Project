 Min Xu minx@cs.cmu.edu John Lafferty lafferty@galton.uchicago.edu Sparse coding, also called dictionary learning, is an approach to approximating a collection of signals by sparse linear combinations of a codewords chosen from a shared, learned dictionary. The method was pro-posed by Olshausen &amp; Field (1996) for encoding nat-ural images, with the motivation of developing a sim-ple computational model of neural coding in the visual cortex. Through the use of sparsity and a large learned dictionary of codewords, sparse coding is able to effi-ciently capture a rich collection of features that are common to a population of signals. Variants of sparse coding have enjoyed considerable success in computer vision (Elad &amp; Aharon, 2006; Lee et al., 2007; Mairal et al., 2009; Yang et al., 2009; Zhou et al., 2010; Bengio et al., 2009).
 In this paper we apply the intuition behind sparse coding to design a new procedure for multivariate re-gression with data that fall into possibly overlapping groups or tasks. In traditional multivariate regres-sion, the data consist of a set of response vectors Y  X  R q , and for each Y , a corresponding covariate vector X  X  R p . In a vector autoregressive time se-ries model, for instance, Y = Z t is a vector at time t , and X = Z t  X  1 is the vector at the previous step. In predicting brain activation patterns in neuroscience, Y might be the neural activations in different regions of the brain with X a vector of external stimuli. Under a linear model, Y = BX +  X  , where B  X  R q  X  p is a matrix of parameters and  X   X  R q is a random, mean zero error vector.
 In many applications, the data naturally occur in groups or tasks, and assuming the same model Y = BX +  X  for each group may be unjustified. For in-stance, in a non-stationary time series, the distribu-tion of Y = Z t varies over time. In the neuroscience example, different people may have different neuronal activation patterns. In both cases it may be natural to place the data into possibly overlapping groups. More generally, the groups could be determined by any fac-tor in the data or experimental design.
 In settings where the input and output dimensions p and q are high, the number of parameters in B may be be too large to estimate accurately from limited data. One approach to estimating reduced complexity models is to perform a least squares regression with a rank constraint on the coefficient matrix B . The nuclear norm serves as a convex surrogate for low rank constraints, and has be recently studied in the context of multivariate regression (Yuan et al., 2007; Negahban &amp; Wainwright, 2011). For grouped data, a different model could be estimated for each group using this approach; however, carrying out separate regressions ignores commonality between the groups, and worsens the problem of limited data.
 Our approach is to estimate the parameter matrices as where each dictionary entry D k is a low rank matrix, { D k } and {  X  ( g ) } are learned from data. The coef-ficients  X  ( g ) k are estimated for each group g , but the  X  X odewords X  or  X  X ictionary elements X  D k are shared across groups. This exploits the same intuition behind sparse coding for image analysis. Sparsity allows the dictionary entries D k to specialize and capture predic-tive aspects of the data shared by many groups, while the coefficients  X  ( g ) tailor the model to the specific group g . Allowing the size K of the dictionary to be large enables a rich class of parameter matrices to be modeled, while a low rank condition on the individual codeword matrices D k allows them to be estimated from limited data.
 We perform both a  X  X essimistic X  and  X  X ptimistic X  analysis of our method. In the pessimistic analysis, the model may not be correct; that is, we do not assume any underlying common structure among the groups. In this case the model cannot achieve lower risk than the alternative of separate low rank regres-sions within each group. However, our analysis shows that the method suffers little excess risk relative to separate regressions. In the optimistic analysis, when the learned dictionary has captured common structure between the groups, the method produces an accurate estimator with much lower sample complexity than re-quired by low rank regression. In both analyses, we measure statistical accuracy through non-asymptotic bounds on the excess risk R ( D,  X  ( g ) )  X  R ( B  X  ). We show that the new procedure is effective and practical with experiments on simulated data and brain imaging data, reported in Section 6. Mairal et al. (2010) have studied a different way of using dictionary learning for supervised tasks; in this approach one first encodes data X and then uses the encoding to perform classification or regression. Our work is more related to multi-task learning (Caruana, 1997; Evgeniou &amp; Pontil, 2004) and is in particular a generalization of a model by Argyriou et al. (2006). They require that all  X  ( g ) have the same sparsity pat-tern, so that all groups use the same small subset of dictionary elements. By allowing different groups to use different subsets of the dictionary, our model is much more flexible, though at the cost of requiring a non-convex optimization. Kang et al. (2011) used mixed integer programming to generalize the model of Argyriou et al. (2006) although our formulation is still more flexible and our optimization simpler. The ap-proach of Liu et al. (2010) could be adapted to our set-ting, although their notion of task-relatedness is very different from ours.
 Existing approaches to theoretical analysis of multi-task learning differ significantly from our analysis by focusing on PAC-learnability with respect to a more abstract notion of task-relatedness (Maurer, 2006; Ben-David &amp; Schuller, 2003). Theoretical analy-sis of sparse coding is rather limited. Some work studies the generalization error of dictionary learning (Vainsencher et al., 2010; Maurer &amp; Pontil, 2010) and the local correctness of the non-convex objective for dictionary learning (Geng et al., 2011). Jeong &amp; Kim (2009) consider sparse approximability and prove an information theoretic lower bound on sparse approx-imability of general p -dimensional vectors. They fur-ther show, non-constructively, that the lower bound can be achieved via an optimally constructed dictio-nary. We instead consider sparse approximability of a variety of structured spaces with respect to a dic-tionary that could plausibly be learned by a practical procedure. In this work we focus on problems where the data are naturally grouped. Suppose we have G groups, in-dexed by g = 1 , . . . , G . Let X ( g ) i  X  R p , Y ( g ) denote the explanatory and response variables for the i th sample in group g . For each group, we let B matrix where we define For convenience, we will assume the sample size n is the same for all groups, noting that more generally it will vary with g . Let X ( g ) = ( X ( g ) 1 , . . . , X R samples of group g arranged as matrix columns. Our goal is to estimate B  X  ( g ) . We consider estimates of the form b B ( g ) = a low rank matrix, and b  X  ( g ) = ( b  X  ( g ) 1 , . . . , b  X  estimated sparse vector. The codewords, or dictionary entries, D k are themselves estimated from data using nuclear norm regularization from data pooled across groups, as described in Section 4. The basic idea underlying conditional sparse coding is to learn a collection of low rank matrices { D 1 , ..., D (a dictionary) and estimate b B ( g ) as a sparse linear combination of the dictionary entries. We optimize the overall objective function f (  X , D ) defined by f (  X , D ) = G where the optimization min  X  min D  X  X  carried out over the set
C D (  X  ) = The  X  1 norm penalty induces sparsity on the  X  vectors and the nuclear-norm restriction forces the matrices D k to be low rank. The spectral norm constraint en-sures no particular dictionary entry can be too large, and serves as an identifiability constraint; a similar constraint in sparse coding requires that all dictionary vectors must have norm no larger than one.
 The objective function is biconvex but not jointly con-vex in  X  and D . Thus, we follow the standard sparse coding approach and alternately optimize over {  X  ( g ) } with fixed { D k } , and optimize over { D k } with fixed {  X  ( g ) } . We refer to the algorithm as conditional sparse coding (CSC) since it is a coding procedure for the re-sponse vectors Y conditioned on the covariate vectors X .
 Algorithm 1 Conditional Sparse Coding (CSC) 1. Initialize dictionary { D 1 , ..., D K } as random rank 2. Alternate between the following steps until con-The encoding step is equivalent to an independent  X  1 -constrained least squares fit, or lasso optimization, for each group g : min A variety of algorithms are available to solve the lasso efficiently, notably iterative soft thresholding, a form of coordinate descent (Friedman et al., 2007). For optimizing the dictionary entries, we designed both a projected gradient descent algorithm and a fast iterative shrinkage and thresholding algorithm (FISTA) following Beck &amp; Teboulle (2009). A com-plication is that since the constraint set C D (  X  ) is an intersection of nuclear norm and spectral norm balls, the projection needs to be done with care. We leave details of the optimization algorithms and the projec-tion procedure to the appendix.
 Remarks on implementation details Although learning the dictionary is computationally intensive, fitting the coefficients to the dictionary is very fast due to efficient lasso optimization algorithms. Thus, an easy way to speed up CSC is to learn the dic-tionary with a smaller number of groups. The CSC op-timization, being non-convex, is sensitive to initializa-tion. We suggest random initialization both because our theoretical guarantees assume random initializa-tion and because it works well in practice.
 In sparse coding, one never picks a dictionary size K equal to or greater than number of vectors to encode to avoid the trivial solution of letting each vector be a dictionary element itself. In CSC however, one can choose K &gt; G because of the nuclear-norm constraint on the dictionary entries. Based both on theory and experimental results, we recommend that  X  is held to a constant between 1 and 0 . 5, and that  X  is then chosen with cross-validation. To get a more complete understanding of CSC, we perform both a pessimistic analysis and an optimistic analysis. In the pessimistic analysis, we do not as-sume that our model is correct, and we do not as-sume any underlying common structure among the the groups. It is obvious that, under the general pes-simistic setting, we cannot achieve higher statistical accuracy with CSC than with the alternative of es-timating separate low-rank matrices for each group. Our pessimistic analysis provides a simple rule for de-termining, in the worst case, how much worse CSC is than the alternative.
 In the optimistic analysis, we focus on a very specific setting where we only have to fit the coefficients to a pre-existing set of learned dictionary entries. We assume that the learned dictionary has thus captured common structure that exists among the groups. We show that in this setting CSC can produce an accurate estimator with fewer samples than the alternative of estimating separate matrices.
 In all of our analyses, we measure statistical accu-racy through non-asymptotic bounds on the excess risk R ( D,  X  ( g ) )  X  R ( B  X  ). For clarity of presentation, we will use same symbols c and C to represent possibly different, generic constants in the theorem statements. Before beginning the analysis, we enumerate and jus-tify the underlying assumptions.
 A1. For all groups g , X ( g ) and Y ( g ) are zero A2. For all groups g , k B  X  ( g ) k  X   X  L and B  X  ( g ) is of A3. The sample size satisfies n  X  ( p + q ). We make assumption A1 only to leverage results on concentration of measure; we do not use any other properties of the Gaussian distribution. Our analy-sis will thus easily extend to subgaussian random vec-tors. Assumption A2 is merely notation, allowing us to state our bounds in terms of L and r . Assumption A3 is made so that many of the results in our pes-simistic analysis can be stated more compactly; we do not make this assumption in our optimistic analysis. It should be emphasized that since we are carrying out an excess risk analysis, we do not require incoherence ten assumed in high-dimensional statistical analysis of sparsity.
 Because we will repeatedly compare the excess risk rate of CSC against estimating separate matrices, we first prove an excess risk bound for using nuclear-norm regularization in each group.
 Theorem 5.1. Suppose that assumptions A1, A2, A3 hold. Let Then with probability at least 1  X  exp(  X  cp ) , we have that max where c, C are constants depending only on k  X  k 2 as defined in A1.
 We provide proof sketches of all theorems in Sec-tion 5.3. 5.1. Pessimistic Analysis output by Conditional Sparse Coding. The results of this section establish bounds on the excess risk not assume D learn ,  X  learn( g )  X  is the global minimizer of the non-convex CSC objective f (  X , D ). We use only the fact that the learned dictionary and coefficients achieve a lower objective than the random initial dic-tionary.
 Before we state our main theorem, it is instructive to first consider the excess risk bound we would obtain if using only the random initial dictionary entries with oracle coefficients, with no additional dictionary learn-ing.
 Proposition 5.1. Suppose that assumptions A1, A2, A3 hold. For a given sparsity level s , define Let K  X  max( n, r ( p + q )) , and  X   X  s  X  r ( p + q ) . Then with probability at least 1  X  1 K , where C is a constant depending only on k  X  k 2 as de-fined in A1.
 Setting s = r ( p + q ) 2 , we observe that a large enough dictionary of random rank one matrices with the (non-sparse) oracle coefficients yields an excess risk bound that, up to multiplicative constants, matches the bound in Theorem 5.1 X  X he best we can hope for. But because the oracle coefficients  X  init( g ) oracle sparse, the learned coefficients  X  init( g )  X  will be a poor estimate of the oracle coefficients, and the resulting excess risk may be significantly larger.
 Proposition 5.1 and the preceding discussion motivate the need for learning the dictionary X  X e may improve statistical accuracy if we can customize the dictionary, allowing reconstruction of B  X  ( g ) from the dictionary using sparse coefficients. Our main theorem in this subsection formalizes this intuition.
 Theorem 5.2. Suppose assumptions A1, A2, A3 hold. Suppose K  X  max( n, r ( p + q )) ,  X   X  and  X   X  1 . Then with probability at least 1  X  1 K , This result implies that if the learned coefficients are of conditional sparse coding is, up to a multiplicative constant factor, no greater that the excess risk for esti-mating separate low-rank matrices within each group. Of course, the excess risk can be worse if k  X  learn( g ) increases with ( p + q ) or n ; we cannot rule out this possibility because the dictionary learning optimiza-tion is nonconvex and does not admit a direct anal-ysis. We note in our experimental section, however, that  X  learn( g )  X  is very sparse in our simulations. We note also that our proof uses critically the fact that our algorithm places a nuclear-norm constraint on the dictionary entries, thus showing that the constraint is necessary to reduce overfitting when learning the dic-tionary.
 Theorem 5.2 and Proposition 5.1 suggest a rule of thumb in applying conditional sparse coding. If the sparsity levels of the coefficients do not decrease with the iterations of dictionary learning, then the resulting statistical accuracy may be poor. 5.2. Optimistic Analysis For our optimistic analysis, we consider the specific setting where the dictionary is already learned and we analyze the excess risk incurred when we fit the coeffi-cients from data that were not used in the dictionary learning process.
 A4. The learned dictionary { D learn 1 , ..., D learn K } is in-With the dictionary fixed, we let be the sparse coefficients that minimize the true risk. We can then interpret the oracle excess risk tent to which the oracle regression matrices B  X  ( g ) share structure, and the learned dictionary has captured this structure.
 Theorem 5.3. Suppose assumptions A1, A2, A4 hold. Suppose  X   X  least 1  X  1 n , where C is some constant depending only on k  X  k 2 as defined in A1.
 Under the optimistic assumption that the excess risk the dictionary has effectively learned the common in-formation among the groups, then we require on the order of the same excess risk as in Theorem 5.2. If we further assume that k  X  learn( g )  X  k 1 does not increase with p and q , meaning that the oracle coefficients are sparse, then the excess risk in the optimistic setting is also lower than the bound in Theorem 5.1. 5.3. Proof Sketches Proof sketch of Theorem 5.1. The crux of our ar-gument is the following uniform generalization error bound.
 Lemma 5.1. With probability at least 1  X  exp(  X  cp ) , for all matrices B ( g ) such that k B ( g ) k  X   X  L , R ( B b constants depending only on k  X  k 2 as defined in A1, and R u is a term that does not depend on B ( g ) . We prove Lemma 5.1 by combining the technique of Greenshtein &amp; Ritov (2004) with a concentration re-sult from random matrix theory which states that for independent subgaussian random vectors Z 1 , ..., Z n , k least 1  X  exp  X  cp for some constants c, C . Theorem 5.1 then follows from a standard argument.
 Proof sketch of Proposition 5.1. The proof is construc-tive. It uses a theoretical procedure, similar to orthog-onal matching pursuit, but infeasible to implement, to produce a  X  ( g ) with sparsity level s for the random rank 1 dictionary entries so that the reconstruction excess risk would be sufficiently low. Since  X  init oracle the optimal set of s -sparse coefficients, we can upper bound its risk with the risk of our constructed coef-ficients. We do not prove that our bound is tight, but analysis by Jeong &amp; Kim (2009) suggests that our bound cannot be significantly improved. We discuss this point further in the appendix.
 Proof sketch of Theorem 5.2. We first rewrite the ex-cess risk as where  X  init( g ) oracle is as defined in Proposition 5.1 with s We then bound (5.1) using Lemma 5.1. To control (5.2), we observe that although the dictionary learn-ing procedure is nonconvex, it is guaranteed to im-prove the objective. Thus, we have immediately that from Proposition 5.1. Term (5.3) requires the follow-ing lemma concerning uniform generalization error of learning coefficients for a fixed dictionary. Lemma 5.2. Let D 1 , ..., D K be a fixed set of dictionary entries with k D k k  X   X  1 . We have that with probability at least 1  X  1 n , for all co-efficients  X  ( g ) , max g R ( D,  X  ( g ) )  X  b R ( D,  X  ( g ) pending only on k  X  k 2 as defined in A1 and R u is a term that does not depend on  X  ( g ) Proof sketch of Theorem 5.3. The proof is straightfor-ward by combining Assumption A4, Theorem 5.3, and Lemma 5.2. The main purpose of our experiments is to compare conditional sparse coding against reduced-rank regres-sion. The experiments also illustrate that the coeffi-cients estimated by CSC are indeed sparse and that the dictionary entries are low rank. 6.1. Simulation Data We generate data using a linear model Y ( g ) = B is a p  X  p square matrix. We build a random design matrix X ( g ) by drawing each sample X ( g ) i  X  N (0 , I We consider three different settings: 1. In the structured case, we construct each B  X  ( g ) 2. In the unstructured case, we construct each 3. The structured same design case is the same as We measure performance of the algorithms in terms of both estimation error 1 G prediction error b R test ( b B ( g ) ), which is computed from a large test set of ( X ( g ) , Y ( g ) ) pairs. We compare CSC against performing separate reduced rank regressions for each group using nuclear norm-regularization. It can be seen from Figure 6.1 that when the parameter matrices { B  X  ( g ) } have significant common structure, CSC easily outperforms separate regressions with ei-ther different or the same design for each group. CSC performs worse in the unstructured case, as expected, but is still competitive with separate regressions. In Figure 6.1, we show the sparsity of the coefficients together with the ranks of the learned dictionary en-tries, as a function of iterations of alternation in the algorithm. It is seen that (1) CSC does not require many iterations to converge, (2) the coefficients be-come increasingly sparse, and (3) although the ranks of the dictionary entries may increase, the learned dic-tionary entries are still relatively low rank. Sparsity of coefficients Rank of dictionary entries We note that in Section 8 of the Appendix, we also perform simulations with overlapping groups. 6.2. fMRI Data The dataset, gathered by Mitchell et al. (2008), com-prises the brain activity patterns of 9 human sub-jects when presented with a single concrete English noun. We down-sample the original neural signal by retaining only one measurement in every 4  X  3  X  4 voxel region of the brain. More precisely, we have X as a design matrix of neural signals with dimension ( p = 434)  X  ( n = 60) and Y as the response matrix with dimension ( q = 192)  X  ( n = 60) of semantic fea-tures of the 60 nouns being shown to the subjects. We let each subject be a group and hence we have that G = 9.
 The goal is to predict the semantic features of the noun being shown to the subject, based only on the neural signal of the subject X  X  brain. The predicted semantic features can then be used to guess which word the sub-ject was viewing and thus  X  X ead the subject X  X  mind. X  Following Mitchell et al. (2008), we use hold-two-out cross-validation for evaluation. In each run of the ex-periment, we hold out two words, using the remaining 58 words for training, and then compute three eval-uation metrics: 2 vs. 2 classification, 1 vs. 2 clas-sification, and squared error. Let y 1 , y 2 be the se-mantic feature vectors of the heldout words. Let b y , b y 2 be the predicted semantic feature vectors. We say that we correctly made a 2 vs. 2 classification if d ( y 1 , b y 1 ) + d ( y 2 , b y 2 ) &lt; d ( y 1 , b y say that we correctly made a 1 vs. 2 classification if both d ( y 1 , b y 1 ) &lt; d ( y 1 , b y 2 ) and d ( y 2 If we make random predictions, then the expected 1 vs. 2 classification accuracy is 0 . 25 and the expected 2 vs. 2 classification accuracy is 0 . 5. Our parameters are tuned by separate cross-validation trials. We used K = 20 dictionary entries.
 In Figure 3, we compare the performance of CSC to separate trace-norm-regularized regressions for each subject. CSC often shows significant improvement in both 2 vs. 2 and 1 vs. 2 classification tasks, with very few cases of significant degradation. In terms of squared error, CSC shows improvement for most sub-jects, although on average, the improvement is statis-tically insignificant.
 Although there is indeed sharing of dictionary entries across the various groups (subjects), it is important to mention that the pattern of sharing is unstable from trial to trial. Figure 6.2 shows two patterns of group-dictionary utilization derived from the  X  ( g ) co-efficients. We see that in the first trial, subject 3 shares significantly with subject 7, while subject 1 shares with no other subjects; in the second trial, subject 3 shares with subject 5 and subject 1 shares with subjects 6 and 9. The instability is possibly due to the low sam-ple size. As a result of this instability, we cannot de-duce subject-subject similarities from the dictionary utilization patterns.
 Research supported in part by NSF grant IIS-1116730 and AFOSR contract FA9550-09-1-0373.

