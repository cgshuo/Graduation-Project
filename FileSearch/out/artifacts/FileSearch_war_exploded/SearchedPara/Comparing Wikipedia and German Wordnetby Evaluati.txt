 Semantic similarity ( SS ) is typically defined via the lexical relations of synonymy ( automobile  X  car ) and hypernymy ( vehicle  X  car ), while semantic re-latedness ( SR ) is defined to cover any kind of lexi-cal or functional association that may exist between two words. Many NLP applications, like sense tag-ging or spelling correction, require knowledge about semantic relatedness rather than just similarity (Bu-danitsky and Hirst, 2006). For these tasks, it is not necessary to know the exact type of semantic rela-tion between two words, but rather if they are closely semantically related or not. This is also true for the work presented herein, which is part of a project on electronic career guidance. In this domain, it is important to conclude that the words  X  X aker X  and  X  X agel X  are closely related, while the exact type of a semantic relation does not need to be determined.
As we work on German documents, we evalu-ate a number of SR measures on different German datasets. We show that the performance of mea-sures strongly depends on the underlying knowledge source. While WordNet (Fellbaum, 1998) mod-els SR, wordnets for other languages, such as the German wordnet GermaNet (Kunze, 2004), contain only few links expressing SR. Thus, they are not well suited for estimating SR.

Therefore, we apply the Wikipedia category graph as a knowledge source for SR measures. We show that Wikipedia based SR measures yield better cor-relation with human judgments on SR datasets than GermaNet measures. However, using Wikipedia also leads to a performance drop on SS datasets, as knowledge about classical taxonomic relations is not explicitly modeled. Therefore, we combine GermaNet with Wikipedia, and yield substantial im-provements over measures operating on a single knowledge source. Several German datasets for evaluation of SS or SR have been created so far (see Table 1). Gurevych (2005) conducted experiments with a German trans-lation of an English dataset (Rubenstein and Goode-nough, 1965), but argued that the dataset ( Gur65 ) is too small (it contains only 65 noun pairs), and does not model SR. Thus, she created a German dataset containing 350 word pairs ( Gur350 ) con-taining nouns, verbs and adjectives that are con-nected by classical and non-classical relations (Mor-ris and Hirst, 2004). However, the dataset is bi-ased towards strong classical relations, as word pairs were manually selected. Thus, Zesch and Gurevych (2006) semi-automatically created word pairs from domain-specific corpora. The resulting ZG222 dataset contains 222 word pairs that are con-nected by all kinds of lexical semantic relations. Hence, it is particularly suited for analyzing the ca-pability of a measure to estimate SR. Semantic wordnet based measures Lesk (1986) introduced a measure ( Les ) based on the number of word overlaps in the textual definitions (or glosses) of two terms, where higher overlap means higher similarity. As GermaNet does not contain glosses, this measure cannot be employed. Gurevych (2005) proposed an alternative algorithm ( PG ) generating surrogate glosses by using a concept X  X  relations within the hierarchy. Following the description in Budanitsky and Hirst (2006), we further define sev-eral measures using the taxonomy structure. sim P L = l ( c 1 , c 2 ) sim LC =  X  log sim Res = IC ( c i ) =  X  log( p ( lcs ( c 1 , c 2 ))) dist JC = IC ( c 1 ) + IC ( c 2 )  X  2 IC ( lcs ( c 1 , c 2 sim Lin = 2  X  PL is the taxonomic path length l ( c 1 , c 2 ) between two concepts c 1 and c 2 . LC normalizes the path length with the depth of the taxonomy. Res com-putes SS as the information content (IC) of the low-est common subsumer ( lcs ) of two concepts, while JC combines path based and IC features. 1 Lin is derived from information theory.
 Wikipedia based measures For computing the SR of two words w 1 and w 2 using Wikipedia, we first retrieve the articles or disambiguation pages with titles that equal w 1 and w 2 (see Figure 1). If we hit a redirect page, we retrieve the correspond-ing article or disambiguation page instead. In case of an article, we insert it into the candidate article set ( A 1 for w 1 , A 2 for w 2 ). In case of a disam-biguation page, the page contains links to all en-coded word senses, but it may also contain other Figure 1: Steps for computing SR using Wikipedia. links. Therefore, we only consider links conforming to the pattern  X  Title_(DisambiguationText)  X  2 (e.g.  X  X rain_(roller coaster) X ). Following all such links gives the candidate article set. If no disambiguation links are found, we take the first link on the page, as most important links tend to come first. We add the corresponding articles to the candidate set. We form pairs from each candidate article a i  X  A 1 and each article a j  X  A 2 . We then compute SR ( a i , a j ) for each pair. The output of the algorithm is the maxi-mum SR value max a
As most SR measures have been developed for taxonomic wordnets, porting them to Wikipedia re-quires some modifications (see Figure 2). Text over-lap measures can be computed based on the article text, while path based measures operate on the cate-gory graph. We compute the overlap between article texts based on (i) the first paragraph, as it usually contains a short gloss, and (ii) the full article text. As Wikipedia articles do not form a taxonomy, path based measures have to be adapted to the Wikipedia category graph (see the right part of Figure 2). We define C 1 and C 2 as the set of categories assigned to article a i and a j , respectively. We compute the SR value for each category pair ( c k , c l ) with c k  X  C 1 and c l  X  C 2 . We use two different strategies to com-bine the resulting SR values: First, we choose the best value among all pairs ( c k , c l ) , i.e., the minimum for path based, and the maximum for information content based measures. As a second strategy, we average over all category pairs. Table 2 gives an overview of our experimental re-sults on three German datasets. Best values for each dataset and knowledge source are in bold. We use the P G measure in optimal configuration as reported by Gurevych (2005). For the Les measure, we give the results for considering: (i) only the first para-graph (+First) and (ii) the full text (+Full). For the path length based measures, we give the values for averaging over all category pairs (+Avg), or tak-ing the best SR value computed among the pairs (+Best). For each dataset, we report Pearson X  X  cor-relation r with human judgments on pairs that are found in both resources ( B OTH ). Otherwise, the re-sults would not be comparable. We additionally use a subset containing only noun-noun pairs ( B OTH NN ). This comparison is fairer, because article titles in Wikipedia are usually nouns. Table 2 also gives the inter annotator agreement for each subset. It con-stitutes an upper bound of a measure X  X  performance.
Our results on Gur65 using GermaNet are very close to those published by Gurevych (2005), rang-ing from 0.69 X 0.75. For Gur350, the performance drops to 0.38 X 0.50, due to the lower upper bound, and because GermaNet does not model SR well. These findings are endorsed by an even more sig-nificant performance drop on ZG222. The measures based on Wikipedia behave less uniformly. Les yields acceptable results on Gur350, but is generally not among the best performing measures. LC +Avg yields the best performance on Gur65, but is outper-formed on the other datasets by P L +Best, which performs equally good for all datasets.

If we compare GermaNet based and Wikipedia based measures, we find that the knowledge source has a major influence on performance. When evalu-ated on Gur65, that contains pairs connected by SS, GermaNet based measures perform near the upper bound and outperform Wikipedia based measures by a wide margin. On Gur350 containing a mix of SS and SR pairs, most measures perform comparably. Finally, on ZG222, that contains pairs connected by SR, the best Wikipedia based measure outperforms all GermaNet based measures.
 The impressive performance of P L on the SR datasets cannot be explained with the struc-tural properties of the category graph (Zesch and Gurevych, 2007). Semantically related terms, that would not be closely related in a taxonomic word-net structure, are very likely to be categorized under the same Wikipedia category, resulting in short path lengths leading to high SR. These findings are con-trary to that of (Strube and Ponzetto, 2006), where LC outperformed path length. They limited the search depth using a manually defined threshold, and did not compute SR between all candidate ar-ticle pairs.

Our results show that judgments on the perfor-mance of a measure must always be made with re-spect to the task at hand: computing SS or SR. De-pending on this decision, we can choose the best un-derlying knowledge source. GermaNet is better for estimating SS, while Wikipedia should be used to estimate SR. Therefore, a measure based on a single knowledge source is unlikely to perform well in all settings. We computed a linear combination of the best measure from GermaNet and from Wikipedia. Results for this experiment are labeled Linear in Ta-ble 2. POS is an alternative combination strategy, where Wikipedia is only used for noun-noun pairs. GermaNet is used for all other part-of-speech (POS) combinations. For most datasets, we find a combi-nation strategy that outperforms all single measures. We have shown that in deciding for a specific mea-sure and knowledge source it is important to con-sider (i) whether the task at hand requires SS or SR, and (ii) which POS are involved. We pointed out that the underlying knowledge source has a ma-jor influence on these points. GermaNet is better used for SS, and contains nouns, verbs, and adjec-tives, while Wikipedia is better used for SR between nouns. Thus, GermaNet and Wikipedia can be re-garded as complementary. We have shown that com-bining them significantly improves the performance of SR measures up to the level of human perfor-mance.

Future research should focus on improving the strategies for combining complementary knowledge sources. We also need to evaluate a wider range of measures to validate our findings. As the simple P L measure performs remarkably well, we should also consider computing SR based on the Wikipedia arti-cle graph instead of the category graph.

