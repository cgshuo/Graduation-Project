 Most modern web search engines employ a two-phase rank-ing strategy: a candidate list of documents is generated us-ing a  X  X heap X  but low-quality scoring function, which is then reranked by an  X  X xpensive X  but high-quality method (usu-ally machine-learned). This paper focuses on the problem of candidate generation for conjunctive query processing in this context. We describe and evaluate a fast, approximate postings list intersection algorithms based on Bloom filters. Due to the power of modern learning-to-rank techniques and emphasis on early precision, significant speedups can be achieved without loss of end-to-end retrieval effectiveness. Explorations reveal a rich design space where effectiveness and efficiency can be balanced in response to specific hard-ware configurations and application scenarios.
 Categories and Subject Descriptors : H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval General Terms: Algorithms, Experimentation Keywords: scalability, efficiency, conjunctive queries
There is general consensus that the challenge of docu-ment ranking is best tackled using machine learning tech-niques [11, 9]. This  X  X earning to rank X  approach generally assumes that a candidate list of potentially-relevant docu-ments has already been gathered by other means. Thus, learning to rank is actually a reranking problem, using, for example, boosted regression trees [5, 9]. Hence, modern web search can be viewed as a two-phase process: candidate gen-eration followed by reranking.

This paper focuses on the problem of candidate gener-ation for learning to rank algorithms. Following previous work, we operationalize this as postings list intersection [4, 17]. Our intuition is that modern rankers are relatively in-sensitive to the quality of the candidate list. As long as this intermediate product is of  X  X easonable X  quality, the end re-sult will remain good X  X articularly in the web context due to its emphasis on early precision. Thus, our goal is to gen-erate candidate documents as quickly as possible, even at the cost of introducing approximations.

We see this work as having two main contributions: First, we propose a novel algorithm for postings list intersection that takes advantage of Bloom filters. Evaluations on real world data show that it is fast, yet does not sacrifice end-to-end retrieval effectiveness. Second, we empirically character-ize the tradeoff space between effectiveness (result quality), time (retrieval speed), and space (index size), illustrating how these three aspects can be traded off for each other.
For modern web retrieval, query evaluation is often bro-ken into two phases [4, 17]. In the first phase, a fast,  X  X heap X  algorithm generates a candidate list of potentially-relevant documents (e.g., using interpolated BM25 and static prior). In most cases, queries are processed conjunctively, i.e., only documents that contain all the query terms are considered. For web-scale collections, this leads to higher early precision and faster query evaluation [4]. The candidate documents from the first phase are then reranked by a slower,  X  X xpen-sive X  but better (machine learned) algorithm (e.g., boosted regression trees [5, 9]). Within this general setup, our work focuses on the candidate generation process.

Conjunctive query processing requires solving the prob-lem of postings list intersection, which has been studied in detail [8, 1, 18]. In particular, we use the eliminator-based  X  X mall adaptive X  algorithm proposed by Demaine et al. [8] as the baseline. Although Bloom filters have been used in P2P retrieval systems [12] and retrieval based on bit signa-tures [16], to our knowledge this application is novel.
The starting point of this work is that modern learning-to-rank approaches for web search are sufficiently powerful to return high-quality results as long as they are presented with a  X  X easonable X  candidate list of documents X  X specially based on metrics that focus on early precision. As a result, we can leverage probabilistic data structures to speed up candidate generation (postings list intersection). Given a query Q , the task is to retrieve the top n documents, based on a query-independent score, that contain all the query terms. As is the case with commercial web search engines, we assume that all index structures are held in memory.
In most collections, documents are arbitrarily numbered, so document-sorted indexes do not provide any scoring or ranking. To address this issue, we renumber the collec-tion so that document ids are assigned based on a query-independent score (e.g., PageRank or spam scores). In our case, the smallest document id is assigned to the document estimated to have the highest quality, and documents are successively numbered in order of decreasing page quality, with ties broken arbitrarily. With this  X  X enumbering trick, X  postings now implicitly capture ranking information, which provides early termination: if we wish to generate top n doc-uments (in terms of the query-independent score), we tra-verse postings in order and stop when we X  X e gathered enough documents. Finally, since document ids are guaranteed to be in ascending order, we can continue to use efficient gap-based techniques to compress postings. Following standard practice, we use PForDelta [19]. Note that since we focus on postings list intersection, there is no need to store tf  X  X  and positional information in the index.

The core contribution of this work is a novel algorithm for postings list intersection using Bloom filters. In our ap-proach, each postings list is stored both as a compressed sequence of integers and as a Bloom filter [2]. A Bloom filter is a fast, compact data structure that supports O (1) approximate set membership tests; that is, the Bloom filter representation of a postings list allows us to quickly answer the question,  X  X s this document id contained in the postings list? X  The relevant parameters for a Bloom filter are r (bits per element) and k (number of hash functions), which we fix for all Bloom filters in the index. Given a parameter setting, we can analytically model the false positive rate, as provided by Bose et al. [3]. In the interest of space, we do not repeat the bounds or the derivation here.

The postings list intersection algorithm proceeds as fol-lows: for a query Q with | Q | terms, we find the term q with the smallest document frequency (i.e., least frequent query term) and look up its standard postings list. We refer to this as the base postings list. This postings list is then traversed: walking down the list of document ids, the algorithm probes the Bloom filter representation of postings lists correspond-ing to the other query terms to compute the set intersection. A document is added to the candidate list if all member-ship tests pass. Since the base postings list is sorted by the query-independent score, to generate a list of n candidate documents we only need to traverse the base postings list until the n th matching document is decoded, thereby allow-ing early termination. In our experiments, we set n to be 10000, but this parameter can be tuned to the application scenario. The approximation aspect of our algorithm lies in the fact that Bloom filters can produce false positives X  X hat is, a filter can assert that an element is contained within it, even when in reality the element was never inserted.
When constructing Bloom filter representations for a col-lection of N documents, it is clear that by fixing r (bits per element), the size of the Bloom filters for postings lists that have more than  X  I = N r elements exceeds N bits. Thus, for very frequent terms, we replace the Bloom filter with a bit-array index. Not only does this reduce the false positive rate to zero, but it also obviates the need to compute hash values, thereby increasing the speed of the probes.
For postings lists with n &lt;  X  I elements, we build a Bloom filter with k hash functions and r  X  n bit positions. We use the Jenkins integer hash function, with the form h ( x,S ), where S is a seed. For Bloom filter setting k = 1, we simply use a large prime number as the seed and compute h ( x,S ) mod ( n  X  r ) as the hash value, for a particular setting of r on a postings list with n postings. For k &gt; 1, the n hash value is computed by seeding the ( n  X  1) th hash value to h ( x,S ) mod ( n  X  r ). Recognizing the accuracy, time, and space tradeoffs discussed above, we experimented with a range of settings to empirically quantify the effects.
We performed experiments on the first English segment of the ClueWeb09 collection. For our query-independent score, we used the Waterloo spam scores [7]. After document id reassignment (see Section 3), we compressed postings lists (document ids only) using PForDelta with a block size of 128. The compressed postings lists total 13.88 GB in size. We used two different sets of queries for evaluation. The 50 queries from the TREC 2009 web track were used for effectiveness experiments. For efficiency, we used the AOL query log [15], which contains around 10 million queries. Implementations of the algorithms in this paper are in Java, built on top of the open-source Ivory toolkit [13]. We used LinkedIn X  X  PForDelta implementation in the open-source Kamikaze package. 1 Following standard practice, we used gap-compressed PForDelta [19] (block size of 128) for the baseline implementation (small adaptive). The choice of Java puts us at a disadvantage compared to, say, implemen-tations in C/C++. Since our task is an intermediate step in a retrieval pipeline, ease of component integration is im-portant. For this, Java holds a number of advantages in to-day X  X  software ecosystem X  X or example, Twitter X  X  real-time search engine, which serves over 2 billion queries per day, is implemented in Java [6]. Regardless, since all implemen-tations are in the same language, the comparison remains fair X  X ere we to reimplement everything in C/C++, the relative results should remain the same. We have put in a best faith effort to optimize the small adaptive algorithm in our implementation. Thus, we are confident that observed differences are not caused by neglect or an underperform-ing baseline. Finally, we note that all implementations are presently single-threaded.

Although the focus of this work is on fast postings list in-tersection, to illustrate end-to-end retrieval effectiveness, we implemented a simple learning-to-rank algorithm to rerank the candidate documents. We used the simple greedy fea-ture selection algorithm proposed by Metzler [14] with a standard set of features, which include basic information re-trieval scores (e.g., BM25 and language modeling scores), term proximity features (e.g., exact phrase, ordered and unordered windows), and query-independent features (e.g., spam score). There are a total of 43 features. We performed two-fold cross-validation optimizing NDCG.
 Experiments were performed on a server running Red Hat Linux, with dual Intel Xeon  X  X estmere X  quad-core proces-sors (E5620 2.4GHz) and 128GB RAM. This particular ar-chitecture has a 64KB L1 cache per core, split between data and instructions; a 256KB L2 cache per core; and a 12MB L3 cache shared by all cores (of a single processor). As stated previously, we assume all index structures are completely held in memory. This is not an unreasonable assumption given the capabilities of commodity servers today X  X nd as we shall see, the memory requirements are modest.

There are three important considerations in the design of search engines: effectiveness (result quality), time (query evaluation speed), and space (index size). The last two are straightforward to measure. Query evaluation speed is mea-sured in terms of latency, the per-query time required for Table 1: Relative recall for the TREC 2009 queries com-performing postings list intersection. Index size can be eas-ily computed. A setting of r yields Bloom filters of a par-ticular size (unaffected by k ). We apply the optimization described in Section 3, where very long postings lists are re-placed with bit arrays, such that the maximum size required for any term is the size of the document collection in bits.
Finally, effectiveness: we report both component-level and end-to-end metrics. At the component level, relative recall with respect to exact postings list intersection (i.e., small adaptive) best captures output quality. That is, of all rele-vant documents retrieved by the exact algorithm, what frac-tion is returned by our approximate algorithm (factoring in errors introduced by the Bloom filters). More precisely, rel-ative recall is computed via micro-averaging (i.e., computed per topic, then averaged across topics); this has the effect of disproportionately weighting topics that have fewer relevant documents (which is desirable, in our case).

The other aspect of effectiveness is end-to-end effective-ness. Given the emphasis on early precision in the context of web search, we measured NDCG at cutoff n [10], a well established metric for these types of search tasks. For the second stage reranker we used the linear model described above, whose quality on our limited feature set is on par with state-of-the-art tree-based methods.
The relative recall of our approximate postings list inter-section algorithm with various settings of r (bits per post-ing) and k (number of hash functions) is shown in Table 1 on the TREC 2009 web track queries (for a candidate list containing the top 10000 hits sorted by spam score).
These results are exactly what we would expect: Increas-ing the number of hash functions k reduces the false posi-tive rate and hence improves recall. Increasing the number of bits per element r reduces hash collisions, thereby reduc-ing the false positive rate and increasing recall. There is, of course, no free lunch: larger values of r increase memory requirements and larger values of k reduce speed.

For the end-to-end evaluation, output of the candidate generation phase (by our approximate postings list intersec-tion algorithm and the exact baseline) were reranked by the linear machine-learned model described in Section 3. For each value of r and k in Table 1, we compared the two out-puts in terms of NDCG@ { 1 , 3 , 5 , 10 , 20 } . We omit the re-sults here in the interest of space, but Wilcoxon tests (with p = 0 . 05) showed no significant difference in NDCG values.
These results empirically validate the assumption that un-derlies our work: that modern machine-learned ranking func-tions are sufficiently powerful to deliver high quality results as long as they are presented with a  X  X easonable X  set of candi-date documents. This is particularly true with the emphasis on early precision in web search X  X or example, to obtain a perfect NDCG@1 score, the ranker simply needs to iden-tify one relevant document (of the highest relevance grade). Therefore, the fact that the relative recall of our approx-Table 2: Average query latency (ms) for the AOL imate postings list intersection algorithm isn X  X  perfect has no statistically significant impact on end-to-end NDCG.
Average query latency of our approximate postings list intersection algorithm on the AOL queries is shown in Ta-ble 2, for different values of r (bits per posting) and k (number of hash functions). Reported values represent the average across 10 trials for each parameter setting, along with the 95% confidence interval. Our best configuration is r = 24 ,k = 1, with an average query latency of 4.4ms; this achieves 93.96% relative recall compared to the baseline (see Table 1). If a higher relative recall is desired, r = 24 ,k = 2 is a good option, at 5.09ms per query and a relative recall of 98.04%. However, since in all our effectiveness experiments, end-to-end NDCG was statistically indistinguishable from the exact baseline, there does not appear to be a downside to simply selecting the fastest configuration.

As we would expect, increasing k increases average query latency, since it requires computing more hash values and probing additional bit positions. Increasing r , however, calls into play two counteracting factors. On the one hand, in-creasing r leads to larger Bloom filters and hence less lo-cality, which translates into more cache misses and longer memory latencies. However, on the other hand, as the size of the Bloom filter increases the false positive rate drops, and therefore fewer hash computations are needed to reject a non-existent document id. Empirically, we see that the sec-ond effect is stronger for the range of r values we explored: r = 24 yields the fastest speed for all values of k .
Figure 1 show average query latency for the AOL queries, broken down by query length (i.e., number of query terms after tokenization, stopword removal, etc.). As expected, query latency increases for longer queries, due to the need for more Bloom filter membership tests. However, beyond a certain point, latency levels off and even drops due to the appearance of rare terms in longer queries. With conjunctive query processing, in the worst case we need to only traverse the postings list of the least frequent term and can remove a document from consideration as soon as the membership test for a query term fails.

A final interesting observation: as r becomes larger, run-ning time becomes less sensitive to k . In other words, in-creasing k increases query time less with larger values of r than with smaller values of r . This is because greater r re-duces the probability of hash collisions, and our Bloom filter probing method early exits as soon as it finds an unset bit.
As a reference, the average query latency for exact post-ings list intersection using the small adaptive algorithm was 87.3ms on the AOL queries (for 10000 hits). Not only is small adaptive much slower overall, but query latency in-creases with longer queries. As discussed earlier, we put in a best faith effort to optimize our implementation, so we are confident that this represents a fair comparison.

We believe that one contributing reason for the slow speed of small adaptive is the amount of PForDelta decoding that it must perform. In our Bloom filter algorithm, we only need to decode postings corresponding to the least frequent query term; and we only need to decode as many blocks as is necessary to accumulate 10000 candidate documents. The small adaptive algorithm, on the other hand, needs to de-code postings for all query terms X  X nd because PForDelta is blocked-based, probing an element (i.e., in binary search) re-quires reconstructing document ids for the entire block. As a result, small adaptive is highly dependent on the raw decod-ing speed of the PForDelta implementation, and this may be where language choice makes a significant difference. In all our algorithms we use the Kamikaze package for PForDelta compression, which was released by LinkedIn and represents an  X  X ndustrial-strength X  implementation. While it is perhaps true that a C/C++ reimplementation of our techniques may be faster, we are confident that the relative performance dif-ferences will hold, since Bloom filters will also benefit from a more efficient implementation.

Furthermore, we note that the small adaptive algorithm has aspects that are not cache friendly. Although there is definitely a dominant memory access pattern as postings are consumed, the binary search for locating the elimina-tor suffers from poor locality: subsequent probes move in unpredictable directions (thus, difficult to pre-fetch), not to mention further slowdowns by branch mispredicts.

Our third dimension of evaluation is index size, which is the amount of space required by the Bloom filters. As a ref-erence, for the first segment of ClueWeb09 (around 50 mil-lion pages), the base postings lists (document ids only) total 13.88 GB in size. For our postings list intersection algorithm to work, we need additional space for the Bloom filters: with r = 8, an additional 11.67 GB; r = 16, 20.24 GB; r = 24, 27.40 GB. In terms of modern server configurations, these re-quirements are quite reasonable X  X n a mid-range commodity server today, one might expect 64GB RAM, which is more then sufficient to serve document partitions of the size we consider here.
This work tackles the problem of candidate generation in a two-phase retrieval architecture. Since powerful machine-learned rerankers are typically applied in the second phase, end-to-end retrieval effectiveness can be maintained as long as the candidate generation phase returns  X  X easonable X  re-sults. Thus, we can  X  X ut corners X  with approximate al-gorithms to speed up candidate generation X  X e propose a novel postings list intersection algorithm based on Bloom filters to accomplish this. Experiments showed that the al-gorithm is much faster than the small adaptive baseline, and yields end-to-end NDCG measures that are indistinguishable from those generated with the exact algorithm. This work has been supported in part by NSF under awards IIS-0916043 and IIS-1144034. Any opinions, findings, con-clusions, or recommendations expressed are the authors X  and do not necessarily reflect those of the sponsors. The first au-thor X  X  deepest gratitude goes to Katherine, for her invaluable encouragement and wholehearted support. The second au-thor is grateful to Esther and Kiri for their loving support and dedicates this work to Joshua and Jacob.
