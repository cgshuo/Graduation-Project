 Identifying user tasks from query logs has garnered consid-erable interest from the research community lately. Several approaches have been proposed to extract tasks from search sessions. Current approaches segment a user session into disjoint tasks using features extracted from query, session or clicked document text. However, user tasks most often than not are entity centric and text based features will not exploit entities directly for task extraction. In this work, we explore entity specific task extraction from search logs. We evaluate the quality of extracted tasks with Session track data. Empirical evaluation shows that terms associated with entity oriented tasks can not only be used to predict terms in user sessions but also improve retrieval when used for query expansion.
 H.3.3 [ Information Search and Retrieval ]: Information Search and Retrieval search tasks, query log analysis, task discovery
Users constantly interact with search engines to accom-plish some tasks such as  X  buy a car  X ,  X  plan a wedding  X  etc. Such broad requirements prompts the use of multiple queries, sometimes spanning multiple sessions. Approximately 75% of user search sessions involve multi-tasking [5], which makes, task identification an important step towards understanding user goals. Recent approaches [3, 4, 5, 6] use either search query or clicked documents to identify tasks. Most of these approaches cluster queries from current or neighboring ses-sions into tasks based on lexical or semantic similarity.
Often, tasks are associated with some entity. Extracting and mining entity information from queries across users can provide insight into different tasks that can be accomplished with a search engine. However, current approaches do not directly leverage entities for task extraction. They use en-tity or entity type information as features. They semanti-cally represent a query using features extracted, either from Wikipedia [5] or from some knowledge base [3]. These rep-resentations do not concretely capture entity specific intents in user queries. For instance, two queries that share same or similar type of entities, may still have diverse concept rep-resentation. Such queries will not be classified as part of the same task. For example, above approaches may not consider  X  buy wedding flowers  X  and  X  book wedding destinations  X  to be part of the same task due to the topic drift that  X  destina-tion  X  and  X  flower  X  induce in concept vectors, even though both queries represent the same task - X  wedding planning  X . Whereas the entities in these two queries - X  flower  X ,  X  destina-tion  X  and  X  wedding  X  shall have high co-occurrence in search logs. Queries, such as these, can be easily mapped to the same task by leveraging their entities. By considering the entities and their associations one can extract better seman-tics from user queries. Existing work extracts tasks from in-dependent sessions, thus providing information only about a single user. Such tasks have limited applications as they do not give a complete picture about tasks that exist globally. However, entity oriented tasks can be extracted from search sessions accross several users. Such a global set of tasks can benefit related search applications too. For instance, it can be used to find similar users by mining their task histories or for query suggestions.

With this motivation, in this work we explore entity based task extraction from search logs. Our system finds entity oriented tasks for each category by populating words that co-occur with entities from that category. We evaluate the quality of extracted tasks in two ways: 1) Query term pre-diction and 2) Query expansion. Given a session, we use task terms related to entities in the first query to predict terms of subsequent queries in the session. We further show that the proposed method can improve retrieval performance when used for query expansion. Experiments on Session track 1 data indicate that terms associated with entity ori-ented tasks can not only predict query terms in a session but can also improve retrieval when used for query expansion.
Recent work mainly explores task extraction from search sessions. The objective in [3, 4, 5] is to segment a search session into disjoint sets of queries where each set represents http://ir.cis.udel.edu/sessions/
Figure 1: Task dictionary construction example a different task. These approaches do not aggregate tasks across users, thus cannot combine or differentiate between tasks extracted from different sessions. Although, Lucchese et al . [6] attempt to cluster tasks across users to create a global representation, their approach uses only category in-formation in Wikipedia to calculate semantic features. How-ever, their approach cannot distinguish between tasks associ-ated with different types of entities. Entity recognition with a knowledge base is used in [3], again, only to enrich concept vectors. Ji et al . [4] find global task representations using a similar approach, but they manually tag phrases with tasks. Our work differs from previous work as we identify tasks not by calculating semantic similarity, but by aggregating queries with its entities and their category information.
To find entities in text one can use entity linking tech-niques such as [2, 7]. These systems can link short texts such as queries to a knowledge base such as DBPedia 2 . An-other work [8], closely related to ours, finds query reformu-lations using entity linking and Wikipedia. They extract expansion terms from Wikipedia and score them using an-chor text information. However, we extract expansion terms from search logs and use only category information of an en-tity to score terms.
Our goal is to use search queries to create a comprehen-sive list of tasks associated with an entity. Several factors have to be considered before building such a list. First con-sideration is the representation of task itself. What shall be used to represent a task? In earlier works, it was a se-quence of queries. For simplicity, we use a dictionary of terms to represent a list of tasks. Alternatively, one can use a list of phrases, queries or more complicated representa-tions. We chose terms as they can succinctly capture the task (e.g. buy, sell, design) associated with an entity (e.g. ticket) while providing the flexibility to build more complex representations (vectors, networks etc.) of tasks.
The second factor is the granularity of tasks associated with an entity. How specific or general will this list be? It depends on the source of entities. For instance, Freebase has more entities and entity types than DBPedia. Thus, http://www.dbpedia.org Table 2: Summary of Category Task Dictionaries tasks for more categories will be extracted from Freebase than DBPedia. However, since DBPedia is extracted from Wikipedia it is less prone to noise. In this work, we use DBPedia entities and categories to extract tasks.
 Final consideration is the method of creating this task list. One can use several approaches like K-Means clustering or algorithms like Random Walk to build task list for an entity. We choose to group terms based on the entities and queries they appear with to build a list of task terms.
Naturally, a system relying on entities needs a method to link query text with entities. We use Dexter [1] to link queries with entities. Dexter, in turn relies on DBPedia for entities and their type information. An entity may belong to different categories. For each query, Dexter returns the phrases that map to an entity (entity mentions), the entity and its categories.
We represent tasks as a collection of diverse but concep-tually related terms. We refer to these lists as task dictio-naries. Constructing a dictionary for every entity will yield too many entities with only handful of words. The Zip-fian distribution of queries will yield a skewed list of terms, since popular entities will contain more words than rare en-tities. However, aggregating these terms under entity cat-egory will yield a comprehensive list as terms from similar entities (popular or rare) will get grouped together. The process of creating these task dictionaries is as follows.
An example of aggregating and filtering query terms for the task dictionary of category  X  wood work  X  is depicted in 0.034 0.045 0.055 0.059 0.062 0.096 * 0.062 0.069 0.090 * 0.092 * 0.113 * 0.070 0.075 0.102 * 0.092 * Figure 1. During Step 1, the system aggregates terms on a category node and in Step 2 it cleans this term set.
Manual evaluation of dictionaries constructed above is in-feasible. Since there is no labeled dataset for tasks evalua-tion, in this work, we evaluate them indirectly with query term prediction and query expansion. Query reformulations in a session are users X  indication of possible terms that can be added or removed from the query to accomplish a cer-tain task. The tasks associated with entities in current query can be used to predict terms of future queries. Similarly, the terms from these tasks can be used to enrich this query to improve retrieval. This is the underlying intuition of using both query term prediction and query expansion to evaluate the quality of generated task dictionaries.

Since a limited number of quality terms are required for evaluation, we need to score task dictionary terms with re-spect to the query. For instance, the query  X  lake murray resort  X  has one entity  X  lake murray  X  whose category is  X  Reser-voirs in South Carolina  X . Task dictionary of this category contains over 50 terms, and since all its terms may not be equally relevant (either for query term prediction or query expansion), its necessary to rank them with respect to the query. Thus, we propose the following scoring mechanism to find most suitable terms for a query given its entities and their types respectively.
We use the above method to rank terms both for term prediction and query expansion. To summarize, we adopt the following mechanism for evaluation:
Query Term Prediction : For each session, we use the first query to predict terms in subsequent queries of the ses-sion. We remove the overlapping terms between the base query and subsequent queries in the session to avoid scor-ing query terms twice during prediction. We also remove stop words from this list. We ignore those sessions where subsequent queries do not contain additional terms.
Query Expansion : For this work, we use the category task dictionaries to expand a user query. An input query is first tagged using Dexter, each entity is then mapped to a single category. The system ranks terms from these cat-egories and uses the top scored terms for expansion. We choose top K terms to expand the query. Terms are ranked using the approach above.

We refer to our approach as ENT in the tables. To con-struct a dictionary of task phrases we use publicly available 2006 AOL query logs which consists of 20 mil search queries issued by over 657000 users within 3 months. We empirically determined the frequency thresholds to filter terms for tasks in each category. For each category, we retain terms with tf-Idf greater than 9. This was selected by manually sam-pling and inspecting term quality of some categories. Table 2 summarizes the resulting task phrase dictionaries. In the statistically significant values are marked with * and high-est values are in bold following section, we briefly explain the task extraction base-lines used to compare our approach.
As baselines, we use task identification approaches pro-posed in [5] to find tasks in a session. They refer to a set of (consecutive or otherwise) queries with same intent in a session as a single task. Lucchese et al . [5] explore several clustering methods to identify these tasks. We use QCC-wcc (QCC) , query clustering based on weighted connected components, a graph based algorithm to identify tasks. It builds a graph G = ( V, E ), whose nodes V are queries in a session and edges E are weighted by the similarity of the corresponding nodes. The aim is to drop edges with low sim-ilarity, and to build clusters on the basis of the strong edges which identify the related query pairs. We also compare our approach with QCC-htc (HTC) , which clusters queries based on head-tail components, a variation of the connected components based algorithm, which does not need to com-pute the full similarity graph. We use the same similarity functions proposed in the paper for both the algorithms. We use similar parameter settings from [5] for both methods. We use the following setup to compare our tasks with [5]. We begin by building a task index using tasks extracted by QCC and HTC on a large query log. This is done to improve real time task identification for a query. For an input query, we retrieve top K tasks. The system then extracts and ranks terms from these tasks based on their frequency in this set.
The results of both query term prediction and query ex-pansion are shown in Table 1 and Table 3 respectively.
Query Term Prediction : We use Session track 2011 and 2012 dataset with 145 sessions, with total of 456 terms, i.e. an average of 3 terms per session. The table shows precision values for term prediction at various cutoffs. For a given query, we retrieve top 50 and 100 tasks using QCC and HTC to extract terms. We follow the method in Section 4 to score terms from entity task dictionaries. Entity based task dictionaries (ENT) perform significantly better for 2012 sessions but do not outperform the baselines QCC and HTC for 2011 sessions.

Query expansion : We compare the both QCC and HTC with 50 and 100 tasks each to retrieve top terms. We also report results with no expansion-No Exp . We varied the value of K and found K=25 to be ideal as addition of more terms did not affect the retrieval performance. The re-sults indicate that task based query expansion is effective in improving performance. The results, however, indicate a mixed performance of our approach on 2011 and 2012 ses-Table 4: Evaluation on 2012 Exploratory Queries 3 sion dataset. While, we outperform the baselines on 2011 queries, for 2012, retrieval performance does not improve with expansion.

On manually inspecting expansion terms, we observed that task based query expansion is effective for queries ex-ploratory in nature. Exploratory queries are more ambigu-ous in nature, thus adding terms from different tasks re-lated to the query would improve performance. On the other hand, for specific queries, adding terms from different tasks will only harm retrieval. Hence, task specific expansion does not do well on specific queries. In 2012 dataset, there are 19 exploratory queries, for which our method outperform the baseline. The results are shown in Table 4. We shall perform further experiments to confirm this hypothesis.
In this work, we explored entity specific task extraction from search logs. We evaluated the quality of extracted tasks with Session track data. Empirical evaluation indi-cates that terms associated with entity oriented tasks can improve query expansion, especially for queries that are ex-ploratory in nature. It can also predict subsequent query terms in a session.
