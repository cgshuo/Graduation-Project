 Information retrieval research ers and engineers use human computation as a mechanism to produce labeled data sets for product development, research a nd experimentation. To gather useful results, a successful labeli ng task relies on many different elements: clear instructions, user interface design, representative high-quality datasets, appropriate inter-rater agreement metrics, work quality checks, and channels for worker feedback. Furthermore, designing and impl ementing tasks that produce and use several thousands or millions of labels is different than conducting small scale research inve stigations. In this paper we present a perspective for collec ting high quality labels with an emphasis on practical problems and scalability. We focus on three main topics: programming crowds, debugging tasks with low agreement, and algorithms for quality control. We show examples from an industrial setting. H.0 [Information Systems] : General Design, Experimenta tion, Human Factors Labeling; crowdsourcing; inter-rater agreement; debugging; Captchas; worker reliability; experimental design Researchers and engineers use human computation via platforms like CrowdFlower or Amazon Mechanical Turk as a mechanism to produce labeled data sets. Several areas of information management such as informati on retrieval, machine learning, recommender systems, and natu ral language processing rely heavily on such labels for product research and development. Crowdsourcing has become increasingly important for curating, labeling, and processing Web-scale datasets. Many members of the information retrieval community have interest in crowdsourcing and would like to inco rporate it into their research and practice. We share practical recommendations for practitioners who are already working with crowdsourcing platforms, and would like to step up their game. Labeled corpora may be used to train and evaluate ranking algorithms or may be used as re ference datasets for implementing and evaluating information retrie val techniques. Assigning a label is usually considered a judgmen t task, performed by a human (e.g., worker, expert, judge, annotator, rater, etc.), where the judgments may be more or le ss subjective. A judgment on the objective end of the spectrum may suggest a single right answer, so we can rely on a single worker or a small number of workers to accurately label an item. If the judgment is more subjective, we probably need to ask more workers the question, and use an aggregation method to determin e the best candidate label. In general, our approach for collecting labels not only looks at designing work from the standpoint of results, but also explicitly acknowledges the human aspects of human computation, and identifies other issues that include worker motivation, fatigue, subjective judgment, and making the most of worker characteristics and diversity. In this paper, we assume a supe rvised or semi-supervised learning setting. That is, there is a hum an involved in the production of labels to bootstrap a learning mechanism or for evaluation purposes. We also add two constr aints from real-world systems: scalability and continuity. By scalability, we mean gathering several thousands or millions of labels depending on the application. Rather than a one-off process for acquiring labels in a single step, we see this as a c ontinuous process that runs at specific intervals (e.g., daily, weekly, etc.) as part of a bigger workflow for training, modeling, or evaluation. We argue that gathering high quality labels at scale is not a straightforward activity. While it is desirable to use crowdsourcing techniques, the process should not be outsourced. Quite the contrary, teams that depend on these data need to have tight control on the many factors that can influence the outcome including specific crowdsourced data management techniques [6]. We now describe three areas that impact the overall quality of labels: wetware programing (or how to ask humans to perform a task), debugging human computati on tasks, and algorithms for assessing work quality and worker performance. We use the phrase  X  X etware programming X  to describe a computation that is designed by a human and performed by another human in a crowdsourci ng-like platform. The idea behind human computation is to use humans as processors in a distributed system for performing tasks that machines cannot do well yet. Human computation is usually performed via form-based tasks or games (e.g., GWAP). When comparing to a machine, the instruction set for humans is somewhat unknown, making it more difficult for an engineer to design and implement solutions based on human computation. We outline a number of issues that engineers and developers face when writing software that uses human computation. We need to be able to trust the data collected so far. If two or more workers agree on a judgment, the highe r the chances that the label is correct. A good practice is to use in ter-rater agreement statistics for measuring the agreemen t among workers. An inter-rater statistic produces a value between -1 and 1, where 1 is perfect agreement, 0 is due to chance, and -1 means total lack of agreement. The most common statistics used in practice are Cohen X  X  kappa (2 raters), Fleiss X  kappa (n raters), and Kr ippendorff X  X  alpha (n raters with missing values). It is strongl y recommended to always use a good inter-rater static and avoid things like percentage agreement. Finally, it is very likely that each task or microtask will be part of a larger workflow so, overall, there is an upfront cost when instrumenting human computation as part of large systems. This step may look overwhelming at first but there is great benefit if implemented correctly. Data quality and experimental designs are preconditions to make sure we get th e right kind of labels before we move further down the modeling pipe line. These labels will be used for rankers, machine learning models, evaluations, etc. Survey designers and researchers that produce annotated corpora have a good process in place to achieve good results. Engineers use a development process (e.g., agile, waterfall, etc.) to produce software. However, when dealing w ith a human instruction set, the process is a bit unclear. A number of issues with the adoption of human computation and crowdsourcing in organizations are due to a lack of development process and unrealistic expectations. Crowdsourcing works but requires attention and care. In other words, the work is outsourced to the crowd but not the process. It is good practice to have a development process in place that allows fast iteration and experiment ation. We do not claim that ours is unique but it has been tested in production with good results. It consists on the following three phases. Relevance evaluation is one of the most widely used applications of crowdsourcing. The task is very simple: given a query and a web page, assess its relevance in a binary or graded relevance scale. The labels collected are used to measure the performance of a particular algorithm and for building test collections. In practice, however, not all evaluation tasks are so simple. A common mistake is trying to do too much on a single task. That is, asking several questions while mi nimizing payment instead of a more practical alternative of decomposing a task into smaller ones. At the end, we want a single label but it does not mean that such label needs to be produced in a single task in one pass. In the context of machine translation, a very nice overview of task design and data acquisition is presented in [4]. In a different example, for evaluating the performance of a near-duplicate algorithm, the task was partitioned in two: one for detecting if a document is a news article and a second one to comp are if two news documents are duplicates or near-duplicates. Each phase was conducted in two different platforms (MTurk and Mi crosoft X  X  UHRS) with different payment schemes [3]. Figure 1 summarizes the workflow. Figure 1. Two-phase evaluation for near-duplicate detection. 
Phase 1 uses Mechanical Turk while phase 2 uses UHRS. Both In practice, a common approach is to start from the feature engineering or modeling phase and re verse the type of labels that are needed. For example, say that a given classifier model needs three types of labels ( X  X es X ,  X  X o X ,  X  X ot sure X ). One solution is to create a task so humans can provide those exact three labels for a class. A problem with this strategy is that the kind of labels that are needed for the machine (the model) do not necessary represent the same labels for humans. In other words, we should use labels that humans can perform best at and then map them into what input is required for the machine. This may include, for example, using a binary scale for a machine but a 5-point grade for humans. There are cases where a task w ill produce results with low inter-rater agreement in many runs. The typical reaction by the engineer or developer is usually adversar ial: identify the workers that are spammers or low performers and replace them. While this may be we know if the task is working? Debugging a task that uses a human instruction set has a number of problems and it is usually a difficult thing in practice. We now describe a framework for debugging subjective judgment tasks and for improving label quality before the crowdsourcing task is run at scale. The framework alternately varies characteristics of the work, assesses the reliability of the workers, and aims to improve task design by disaggrega ting the labels into components that may be less subjective to the workers, thereby potentially improving inter-rater agreement [2]. The framework presented in Figure 2 consists of a  X  X ata-worker-task X  pattern where the aim is to dive into each of the three contingent factors with the end goal of debugging (and fixing) each potential problem. Figure 3 shows an example of a production task using HIDDENs to label tweets in a classification task. We can think of Q1 as the algorithmic HIDDEN whereas Q2 is more semantic. Q3 is the question that we use to collect labels. By maintaining high inter-rater agreement on the HIDDENs we can iterate and debug a potentially problematic ques tion, in our case Q3. 
Figure 3. HIDDEN structure. The first question is completely objective and computable; the second is partially objective; and While there is quite of bit of research on quality control from an adversarial perspective (i.e., workers are spammers), in practice quality has to be applied to both requesters and workers. Workers may be spamming or not performing, but, at the same time, our own design may be so bad that is im possible for honest and experience workers to produce anything useful. How do we measure work quality? If we can produce honey pots (known answers for questions), we can remove workers who are not passing them. Other options are to compare the performance of a worker to other workers in the same task or to use a tiered approach, where certain workers are more experts. An example of a tiered approach is the  X  X ind-fix-verify X  pattern: one set of workers find and fix a problem and a separate set of workers verify that the correction is good. But more importantly, when and how should we enforce the quality control? Quality control is an on-going activity, in particular in industrial environments that require tasks to run continuously. Quality control check points should be introduced as follows: A new proposed approach for enhancing worker performance is by providing micro breaks [5]. As a lot of tasks are repetitive and, in certain cases, probably boring, providing workers with a break can potentially improve overall work quality. As noted before, a good technique is to compare a label to a predefined answer. This is very cheap to implement but assumes that such gold set or ground truth exists or that is somewhat affordable to produce down the road. A second problem is to produce good honey pots that are not so easy to identify by workers. Qualification tests have also some disadvantages. There is an extra cost involved for designing and main taining such tests. This may also turn off workers and hurt completion time. Most of the algorithms for managing work quality are based on majority vote, EM-based and maxi mum likelihood. Majority vote is straightforward to implement and tends to produce good results. However, there are cases when it is not possible to get majority so easily. The algorithm  X  X et another la bel X  decides how many labels to use on a given task based on the distribution of all previously encountered tasks [7]. Vox Populi uses the aggregate label as an approximate ground truth and eliminates the workers that provide incorrect answers [8]. Very recently there are new directions that use adaptive approaches for managing crowds [1]. In explore-exploit techniques there is a quality-cost tradeoff. That is, maxi mizing the quality of the labels gathered while minimizing the overall cost per task. The idea of exploration is to try out alternatives to gather information. Exploitation is about choosing alte rnatives that perform well based on information already collected. How many workers to ask in a given task is also a fair question to pose. For example, for the query {facebook} and the web page {www.facebook.com}, one worker should be enough to produce a good judgment. However for the query {mars expedition} and the web page {www.mars-one.com} is not clear how many workers are needed. Developing and implementing stopping rules that know when to stop asking a worker for a new label is very important. Often perceived as boring and tim e consuming process, gathering high quality labels requires full attention so it is key to own the entire process end-to-end. These labe ls will be used for training set creation, modeling, and evaluation. There is no need to rush things. Repeatable label quality at scale works but requires a solid framework. The end goal is to get the labels right first and then move to modeling. In this paper we presented the areas that need attention that should be useful for practitioners who are considering crowdsourcing at scale but are a bit unsure about how to implement and focus resources. We described a devel opment process that is currently used in-house to test human comput ation tasks. We also introduced a reliability framework that shows the three aspects that need attention: workers, data and task design. We also outline a number of algorithmic solutions that can be used for managing quality. implementing successful tasks that can produce high quality labels. For example, social and behavioral science, human factors, algorithms, economics, distributed systems, and statistics. It is important to know our own limitati ons and be ready to collaborate with other teams. [1] I. Abraham, O. Alonso, V. Kandyla s, R. Patel, S. Shelford, and [2] O. Alonso, C., Marshall, and M. Najork.  X  Debugging a [3] O. Alonso, D. Fetterly, and M. Manasse.  X  X uplicate News [4] C. Callison-Burch.  X  X ast, Cheap, and Creative: Evaluating [5] P. Dai, J. Rzeszotarski, P. Paritosh, and E. Chi.  X  X nd Now for [6] A. Marcus and A. Parameswaran. Crowdsourced Data [7] V. Sheng, F. Provost, P. Ip eirotis.  X  X et Another Label? [8] O. Dekel and O. Shamir.  X  X ox Populi: Collecting High-
