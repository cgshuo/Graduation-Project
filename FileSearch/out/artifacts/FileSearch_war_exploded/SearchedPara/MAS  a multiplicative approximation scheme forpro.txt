 Probabilistic graphical models gained popularity in the recent decades due to their intuitive rep-resentation and because they enable the user to query about the value distribution of variables of interest [19]. Although very appealing, these models suffer from the problem that performing infer-ence in the model (e.g. computing marginal probabilities or its likelihood) is NP-hard [6]. As a result, a variety of approximate inference methods have been developed. Among these meth-ods are loopy message propagation algorithms [24], variational methods [16, 12], mini buckets [10], edge deletion [8], and a variety of Monte Carlo sampling techniques [13, 19, 21, 4, 25]. Approxima-tion algorithms that have useful error bounds and speedup while maintaining high accuracy, include the work of Dechter and colleagues [2, 3, 10, 17], which provide both upper and lower bounds on probabilities, upper bounds suggested by Wainwright et.al. [23], and variational lower bounds [16]. In this paper we present an approximation scheme called the Multiplicative Approximation Scheme (MAS), that provides error bounds for the computation of likelihood of evidence, marginal probabil-ities, and the Maximum Probability Explanation (MPE) in discrete directed and undirected graphical models. The approximation is based on a local operation called an -decomposition, that decom-poses functions used in the inference procedure into functions over smaller subsets of variables, with a guarantee on the error introduced. The main difference from existing approximations is the ability to translate the error introduced in the local decompositions performed during execution of the algo-rithm into bounds on the accuracy of the entire inference procedure. We note that this approximation can be also applied to the more general class of multiplicative models introduced in [27]. We explore optimization of -decompositions and provide a fast optimal closed form solution for the L 2 norm. We also show that for the Kullback-Leiber divergence the optimization problem can be solved using variational algorithms on local factors. MAS can be applied to various inference algorithms. As an example we show how to apply MAS to the Variable Elimination (VE) algo-rithm [9, 20], and present an algorithm called DynaDecomp, which dynamically decomposes func-tions in the VE algorithm. In the results section we compare the performance of DynaDecomp with that of Mini-buckets [10], GMF [28] and variational methods [26] for various types of models. We find that our method achieves orders of magnitude better accuracy on all datasets. We propose an approximation scheme, called the Multiplicative Approximation Scheme (MAS) for inference problems in graphical models. The basic operations of the scheme are local approxima-tions called -decompositions that decouple the dependency of variables. Every such local decom-position has an associated error that our scheme combines into an error bound on the result. Consider a graphical model for n variable X = { X 1 ,...,X n } that encodes a probability distribu-tion P ( X ) = Q j  X  j ( d j ) where D j  X  X are sets determined by the model. Throughout the paper we denote variables and sets of variables with capital letters and denote a value assigned to them with lowercase letters. We denote the observed variables in the model by E = X \ H where E = e .  X  j can be multiplied by a constant z j such that the assumption holds, and the result is obtained after dividing by Q j z j . Thus, here we assume positivity but discuss how this can be relaxed below. In addition to approximating functions  X  by which the original model is defined, we also may wish to approximate other functions such as intermediate functions created in the course of an inference algorithm. We can write the result of marginalizing out a set of hidden variables as a factor of functions f i . The log of the probability distribution the model encodes after such marginalization can then be written as where A  X  H . When A = H we can choose sets U i = D i and functions f i ( U i ) =  X  i ( D i ) . Definition 1 ( -decomposition) Given a set of variables W , and a function  X  ( W ) that assigns real values to every instantiation W = w , a set of m functions  X   X  l ( W l ) , l = 1 ...m , where W l  X  W is an -decomposition if S l W l = W , and for some  X  0 , where w l is the projection of w on W l .
 Note that an -decomposition is not well defined for functions  X  that equal zero or are infinite for some instantiations. These functions can still be -decomposed for certain choices of subsets W l by defining 0 0 = 1 and  X   X  = 1 . We direct the interested reader to the paper of Geiger et.al. [12] for a discussion on choosing such subsets. We also note that when approximating models in which some assignments have zero probability, the theoretical error bounds can be arbitrarily bad, yet, in practice the approximation can sometimes yield good results.
 The following theorems show that using -decompositions the log-likelihood, log P ( e ) , log of marginal probabilities, the log of the Most Probable Explanation (MPE) and the log of the Max-imum Aposteriori Probability (MAP) can all be approximated within a multiplicative factor using a set of -decompositions.
 Lemma 1 Let A  X  H , and let P ( A,E ) factor according to Eq. 1, then the log of the joint prob-ability P ( a,e ) can be approximated within a multiplicative factor of 1 + max using a set of i -decompositions, where max = max i { i } .
 Proof: Theorem 1 For a set A 0  X  A the expression log P a 0 P ( a,e ) can be approximated within a multi-plicative factor of 1 + max using a set of i -decompositions.
 Proof: Recall that P j ( c j ) r  X  P j c j using Lemma 1 summing out any set of variables A 0  X  A does not increase the error: log X Similarly for the upper bound approximation we use the fact that P j ( c j ) r  X  P j c j of numbers c j  X  0 and 0 &lt; r  X  1 .
 approximated within a multiplicative factor of 1 + max . In addition, for any E  X  X by setting A 0 = A the log-likelihood log P ( e ) can be approximated with the same factor.
 A similar analysis can also be applied with minor modifications to the computation of related prob-lems like the MPE and MAP. We adopt the simplification of the problems suggested in [10], reduc-ing the problem of the Most Probable Explanation (MPE) to computing P ( h  X  ,e ) = max h P ( h,e ) and the problem of the Maximum Aposteriori Probability (MAP) to computing P ( a  X  ,e ) = Denote the operator  X  as either a sum or a max operator. Then, similar to Eq. 1, for a set H 0  X  H we can write Theorem 2 Given a set A  X  H , the log of the MAP probability log max a P H \ A = h  X  P ( h,e ) can be approximated within a multiplicative factor of 1 + max using a set of i -decompositions. ( max j c j ) r for any set of real numbers c j  X  0 and r  X  0 .
 An immediate conclusion from Theorem 2 is that the MPE probability can also be approximated with the same error bounds, by choosing A = H . 2.1 Compounded Approximation The results on using -decompositions assume that we decompose functions f i as in Eqs. 1 and 3. Here we consider decompositions of any function created during the inference procedure, and in particular compounded decompositions of functions that were already decomposed. Suppose that a function  X   X  ( W ) , that already incurs an error 1 compared to a function  X  ( W ) , can be decomposed error of P l  X   X  l ( W l ) is (1 + 1 )  X  (1 + 2 ) wrt  X  ( W ) .
 To understand what is the guaranteed error for an entire inference procedure consider a directed graph where the nodes represent functions of the inference procedure, and each node v has an asso-in the model and are associated with zero error ( r v = 1 ). Every multiplication operation is denoted by edges directed from the nodes S , representing the multiplied functions, to a node t representing the resulting function, the error of which is r t = max s  X  S r s . An -decomposition on the other hand has a single source node s with an associated error r s , representing the decomposed function, and several target nodes T , with an error r t = (1 + ) r s for every t  X  T . The guaranteed error for the entire inference procedure is then the error associated with the sink function in the graph. In Figure 1 we illustrate such a graph for an inference procedure that starts with four functions ( f a ,f b ,f c and f ) and decomposes three functions, f a ,f g and f j , with errors 1 , 2 and 3 respectively. In this example we assume that 1 &gt; 2 and that 1 + 1 &lt; (1 + 2 )(1 + 3 ) . 2.2 -decomposition Optimization -decompositions can be utilized in inference algorithms to reduce the computational cost by par-simoniously approximating factors that occur during the course of computation. As we discuss in Section 3, both the selection of the form of the -decomposition (i.e., the sets W i ) and which factors to approximate impact the overall accuracy and runtime of the algorithm. Here we consider the problem of optimizing the approximating functions  X   X  i given a selected factorization W i . order to minimize the error f introduced in the decomposition. The objective function is therefore This problem can be formalized as a convex problem using the following notations.
 problem as
This type of problems can be solved with geometric programming techniques, and in particular using interior-point methods [18]. Unfortunately, in the general case the complexity of solving this problem requires O ( m 3 | W | 3 ) time, and hence can be too expensive for functions over a large do-main. On the other hand, many times functions defined over a small domain can not be decomposed amount of time is needed for such optimization. To reduce the computational cost of the optimiza-tion we resort to minimizing similar measures, in the hope that they will lead to a small error f . Note that by deviating from Eq. 4 to choose the functions  X   X  i we may increase the worst case penalty error but not necessarily the actual error achieved by the approximation. In addition, even when using different measures for the optimization we can still compute f exactly. 2.2.1 Minimizing the L 2 Norm An alternative minimization measure, the L 2 norm, is closely related to that in Eq. 4 and given as: We give a closed form analytic solution for this minimization problem when the sets W i are disjoint, but first we can remove the square root from the optimization formula due to the monotonicity of the square root for positive values. Hence we are left with the task of minimizing:
Figure 1: A schematic description of an inference proce-We use the notation w  X  w k to denote an instantiation W = w that is consistent with the instan- X   X  ( w k ) and set to zero. Choosing the constraint P w  X   X  i ( w i ) = constrained set of linear equations we get As the last term is independent of the index i we finally obtain The second term of Eq. 8 is computed once for a decomposition operation. Denoting | W | = N this term can be computed in O ( N ) time. Computing the first term of Eq. 8 also takes O ( N ) time but it needs to be computed for every resulting function  X   X  k , hence taking an overall time of O ( Nm ) . 2.2.2 Minimizing the KL Divergence The Kulback-liebert (KL) divergence is another common alternative measure used for optimization: Although no closed form solution is known for this minimization problem, iterative algorithms were devised for variational approximation, which start with arbitrary functions  X   X  i ( W i ) and converge to a local minimum [16, 12]. Despite the drawbacks of unbounded convergence time and lack of guarantee to converge to the global optimum, these methods have proven quite successful. In our context this approach has the benefit of allowing overlapping sets W i . Our multiplicative approximation scheme offers a way to reduce the computational cost of inference by decoupling variables via -decompositions. The fact that many existing inference algorithms compute and utilize multiplicative factors during the course of computation means that the scheme can be applied widely. The approach does require a mechanism to select functions to decompose, however, the flexibility of the scheme allows a variety of alternative mechanisms. One simple cost-focused strategy is to decompose a function whenever its size exceeds some threshold. An alternative quality-focused strategy is to choose an and search for -decompositions W i . Below we consider the application of our approximation scheme to variable elimination with yet another selection strat-egy. We note that heuristics for choosing approximate factorizations exist for the selection of disjoint sets [28] and for overlapping sets [5] and could be utilized. The ideal application of our scheme is likely to depend both on the specific inference algorithm and the application of interest. 3.1 Dynamic Decompositions One family of decomposition strategies which are of particular interest, are those which allow for dy-namic decompositions during the inference procedure. In this dynamic framework, MAS can be in-corporated into known exact inference algorithms for graphical models, provided that local functions can be bounded according to Eq. 2. A dynamic decomposition strategy applies -decompositions to functions in which the original model is defined and to intermediate functions created in the course of the inference algorithm, according to Eq. 1 or Eq. 3, based on the current state of the algorithm, and the accuracy introduced by the possible decompositions.
 Unlike other approximation methods, such as the variational approach [16] or the edge deletion ap-proach [8], dynamic decompositions has the capability of decoupling two variables in some contexts while maintaining their dependence in others. If we wish to restrict ourselves to functions over three or less variables when performing inference on a 4  X  4 Ising model, the model in Figure 2 is an inevitable minor, and from this point of the elimination, approximation is mandatory. In the vari-ational framework, an edge in the graph should be removed, disconnecting the direct dependence between two or more variables (e.g. removing the edge A-C would result in breaking the set ABC into the sets AB and BC and breaking the set ACD into AD and CD). The same is true for the edge deletion method, with the difference in the new potentials associated with the new sets. Dynamic decompositions allow for a more refined decoupling, where the dependence is removed only in some of the functions. In our example breaking the set ABC into AB and BC while keeping the set ACD intact is possible and is also sufficient for reducing the complexity of inference to functions of no more than three variables (the elimination order would be: A,B,F,H,C,E,D,G). Moreover, if decom-posing the set ABC can be done with an error ABC , as defined in Eq. 2, then we are guaranteed not to exceed this error for the entire approximate inference procedure. An extreme example will be the functions for the sets ABC and ACD as appear in the tables of Figure 2. It is possible to decompose the function over the set ABC into two functions over the sets AB and BC with an arbitrarily small error, while the same is not possible for the function over the set ACD. Hence, in this example the result of our method will be nearly equal to the solution of exact inference on the model, and the theoretical error bounds will be arbitrarily small, while other approaches, such as the variational method, can yield arbitrarily bad approximations.
 We discuss how to incorporate MAS into the Variable Elimination (VE) algorithm for computing the likelihood of a graphical model [9, 20]. In this algorithm variables V  X  H are summed out iteratively after multiplying all existing functions that include V , yielding intermediate functions f ( W  X  X ) where V /  X  W . MAS can be incorporated into the VE algorithm by identifying -decompositions for some of the intermediate functions f . This results in the elimination of f from are not necessarily disjoint and can have common variables. Using -decompositions reduces the computational complexity, as some variables are decoupled in specific points during execution of the algorithm. Throughout the algorithm the maximal error max introduced by the decompositions
Table 1: Accuracy and speedup for grid-like can be easily computed by associating functions with errors, as explained in Section 2.1. In our experiments we restrict attention to non-compounded decompositions. Our algorithm decomposes a function only if it is over a given size M , and if it introduces no more than  X  error. The ap-proximating functions in this algorithm are strictly disjoint, of size no more than the variables assigned randomly to the functions. We call this algorithm DynaDecomp (DD) and provide a pseudo-code in Algorithm 1. There we use the notation  X  ( T ) to denote multiplication of the functions f  X  T , and ( f ) to denote decomposition of function f . The outcome of ( f ) is a pair ( ,  X  F ) where the functions  X  f i  X   X  F are over a disjoint set of variables. probabilistic models which are widely used, thus gaining similar benefits as those algorithms. For example, applying MAS to the junction tree algorithm [14] a decomposition can decouple vari-ables in messages sent from one node in the junction tree to another, and approximate all marginal distributions of single variables in the model in a single run, with similar guarantees on the error. This extension is analogous to how the mini-clusters algorithm [17] extends the mini-bucket algo-rithm [10]. We demonstrate the power of MAS by reporting the accuracy and theoretical bounds for our Dy-naDecomp algorithm for a variety of models. Our empirical study focuses on approximating the likelihood of evidence, except when comparing to the results of Xing et. al. [28] on grid mod-els. The quality of approximation is measured in terms of accuracy and speedup. The accuracy is reported as max { log L achieved by DynaDecomp. We also report the theoretical accuracy which is the maximum error introduced by decomposition operations. The speedup is reported as a ratio of run-times for obtain-ing the approximated and exact solutions, in addition to the absolute time of approximation. In all experiments a random partition was used to decompose the functions, and the L 2 norm optimization introduced in Section 2.2.1 was applied to minimize the error. The parameter M was set to 10 , 000 and the guaranteed accuracy  X  was set to 1% , however, as is evident from the results, the algorithm usually achieves better accuracy.
 We compared the performance of DynaDecomp with the any-time Mini-buckets (MB) algo-rithm [10]. The parameters i and m , which are the maximal number of variables and functions in a mini-bucket, were initially set to 3 and 1 respectively. The parameter was set to zero, not con-straining the possible accuracy. Generally we allowed MB to run the same time it took DynaDecomp to approximate the model, but not less than one iteration (with the initial parameters). We used two types of grid-like models. The first is an Ising model with random attractive or repulsive pair-wise potentials, as was used in [28]. When computing likelihood in these models we randomly assigned values to 10% of the variables in the model. The other kind of grids were Bayesian net-as parents in the model. In addition, every variable X ij has a corresponding observed variable Y ij connected to it. Probabilities in these models were uniformly distributed between zero and one. In-ference on these models, often used in computer vision [11], is usually harder than on Ising models, due to reduced factorization. We used models where the variables had either two, five or ten values. The results are shown in Table 1. In addition, we applied DynaDecomp to two 100  X  100 Ising grid models with binary variables. Inference in these models is intractable. We estimate the time for exact computation using VE on current hardware to be 3  X  10 15 seconds. This is longer than the time since the disappearance of the dinosaurs. Setting  X  to 2% , DynaDecomp computated the approximated likelihood in 7.09 seconds for the attractive model and 8.14 seconds for the repulsive one.
 Comparing our results with those obtained by the MB algorithm with an equivalent amount of com-putations, we find that on the average the accuracy of MB across all models in Tables 1 is 0.198 while the average accuracy of DynaDecomp is 9 . 8 e  X  4 , more than 200 times better than that of MB. In addition the theoretical guarantees are more than 30% for MB and 0 . 96% for DynaDecomp, a 30-fold improvement. As a side note, the MB algorithm performed significantly better on attrac-tive Ising models than on repulsive ones. To compare our results with those reported in [28] we computed all the marginal probabilities (without evidence) and calculated the L 1 -based measure P 1 . 86 e  X  5 compared to 0 . 003 of generalized belief propagation (GBP) and 0 . 366 of generalized mean field (GMF). Although the run times are not directly comparable due to differences in hardware, DynaDecomp average run-time was less than 0 . 1 seconds, while the run-time of GBP and GMF was previously reported [28] to be 140 and 1 . 6 seconds respectively, on 8  X  8 grids.
 We applied our method to probabilistic phylogenetic models. Inference on these large models, which can contain tens of thousands of variables, is used for model selection purposes. Previous works [15, 26] have obtained upper and lower bounds on the likelihood of evidence in the models suggested in [22] using variational methods, reporting an error of 1% . Using the data as in [26], we achieved less than 0 . 01% error on average within a few seconds, which improves over previous results by two orders of magnitude both in terms of accuracy and speedup.
 In addition, we applied DynaDecomp to 24 models from the UAI X 06 evaluation of probabilistic inference repository [1] with  X  = 1% . Only models that did not have zeros and that our exact infer-ence algorithm could solve in less than an hour were used. The average accuracy of DynaDecomp on these models was 0.0038 with an average speedup of 368.8 and average run-time of 0.79 seconds. We also applied our algorithm to two models from the CPCS benchmark (cpcs360b and cpcs422b). DynaDecomp obtained an average accuracy of 0 . 008 versus 0 . 056 obtained by MB. We note that the results obtained by MB are consistent with those reported in [10] for the MPE problem.
