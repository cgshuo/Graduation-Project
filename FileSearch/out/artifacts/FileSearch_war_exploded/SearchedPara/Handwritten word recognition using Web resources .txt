 ORIGINAL PAPER Cristina Oprean 1  X  Laurence Likforman-Sulem 1  X  Adrian Popescu Chafic Mokbel 3 Abstract Handwriting recognition systems usually rely on static dictionaries and language models. Full coverage of these dictionaries is generally not achieved when dealing with unrestricted document corpora due to the presence of Out-Of-Vocabulary (OOV) words. We propose an approach which uses the World Wide Web as a corpus to improve dictionary coverage. We exploit the very large and freely available Wikipedia corpus in order to obtain dynamic dic-tionaries on the fly. We rely on recurrent neural network (RNN) recognizers, with and without linguistic resources, to detect words that are non-reliably recognized within a word sequence. Such words are labeled as non-anchor words (NAWs) and include OOVs and In-Vocabulary words recognized with low confidence. To recognize a non-anchor word, a dynamic dictionary is built by selecting words from the Web resource based on their string similarity with the NAW image, and their linguistic relevance in the NAW con-text. Similarity is evaluated by computing the edit distance between the sequence of characters generated by the RNN recognizer exploited as a filler model, and the Wikipedia words. Linguistic relevance is based on an N -gram language model estimated from the Wikipedia corpus. Experiments conducted on a word-segmented version of the publicly avail-able RIMES database show that the proposed approach can improve recognition accuracy compared to systems based on static dictionaries only. The proposed approach shows even better behavior as the proportion of OOVs increases, in terms of both accuracy and dictionary coverage.
 Keywords Handwritten word recognition  X  Out-Of-Vocabulary word  X  Web resources  X  Dynamic dictionary  X  Recurrent neural networks While largely solved in controlled conditions, handwriting recognition is still a field of research in a general case. Recognition systems have to cope with variability of both character shapes and content. Character shapes vary with styles and individuals. Variability can be partly solved by increasing the size of training sets and using robust recogni-tion systems such as hidden Markov models (HMMs)-and recurrent neural networks (RNNs)-based systems. Hidden Markov modeling can cope with nonlinear distortions, while RNNs can learn distant dependencies between observations.
Recognition systems, HMMs or RNNs, also rely on lin-guistic resources including (static) dictionaries and language models in order to direct the recognition process. Thus, per-formanceof recognitionsystems is tributarytoanappropriate choice of the size of their static dictionary. Large dictionaries ensure good coverage, but accuracy drops due to higher con-fusion between words that look similar. Conversely, small size dictionaries achieve high accuracy, but fail to cover a large proportion of the words to recognize. Thus, when docu-ments include a limited number of topics, a dictionary limited to these topics ensures better performance. But when consid-ering less focused collections, such as unconstrained mails (see Fig. 1 ) or historical archives, the vocabulary tends to be much larger, increasing the size of static dictionaries.
Since any limited-size dictionary fails to be exhaustive, there are always words that are not included. These words are called Out-of-Vocabulary (OOV) words and are usually the least frequent words that were removed when creating static dictionaries. In addition to the non-frequent words, OOVs include named entities (e.g., first or last names, geo-graphic locations, phone numbers, dates, company names, ages, bank account numbers) that were not encountered in training resources, words associated with new topics which appear over time, words from other languages embedded in texts, grammatical forms of verbs or nouns which were not present in training resources (e.g.,  X  X ignale X  but not  X  X ig-nal X s X ).

OOV recognition can be handled using different approaches. One way to deal with OOVs is to build open-vocabulary systems such as filler models. In such models, any character sequence can be output, however, a character N -gram model guides the recognition in order to improve performance [ 1 , 2 ]. However, when no static dictionary is used, the recognition performance drastically drops. More recently, vocabulary enrichment was achieved by decom-posing the lexicon based on a morphological analysis [ 3 ]. The new vocabulary is a combination of words and sub-words (roots, prefixes and suffixes) obtained as a result of the decomposition. Although theoretically interesting, this method is complex and improves results only by a small margin. OOV recognition is also important in speech recog-nition. In this domain, recent works exploit Web resources to recover OOVs [ 4 , 5 ]. The local context of a word detected as OOV is used to query the Web with a search engine.
We propose a new word recognition approach that com-bines the advantages of static dictionaries with the use of external resources. We consider the Web as a corpus and assume that OOV words are likely to appear in this corpus. We rely on Wikipedia, a large and publicly available Web resource which is constantly updated but other Web resources could be used instead. However, words which appear in the external resource are equally valuable. Inspired by works in NLP (natural language processing, see Sect. 2 ), we pro-pose to build dynamic dictionaries from this Web resource for non-anchor words (NAWs). In this work, non-anchor words designate both the OOVs and the IV (In-Vocabulary) words not reliably recognized. To recognize a non-anchor word, a dynamic dictionary is built by selecting words from the Web resource based on their string similarity with the NAW image and their linguistic relevance in the NAW con-text. Similarity is evaluated by computing the edit distance between the character string generated by an RNN recog-nizer and the Wikipedia words. Linguistic relevance is based on an N -gram language model estimated from the Wikipedia corpus. To our knowledge, this is the first study that lever-ages large-scale Web resources, such as Wikipedia, for large or open-vocabulary handwriting recognition. Experimental results show that the use of dynamic dictionaries improves recognition accuracy compared to the sole use of a static dic-tionary. We also show that our approach behaves better as the size of the static dictionary decreases and can also work even if starting without any static dictionary.

The paper is organized as follows: Sect. 2 presents the related work concerning dictionary reduction techniques and NLP approaches using Web as an external resource. Section 3 describes the general overview of the proposed approach. Section 4 presents the baseline RNN recogni-tion system. Wikipedia resources at unigram and bigram levels are described in Sect. 5 . Based on an anchor/non-anchor (AW/NAW) word classification, described in Sect. 6.1 , dynamic dictionaries are built for the words classified as NAW. Dynamic dictionary construction is presented in Sects. 6.2 and 6.3 . The experimental results are provided in Sect. 7 . Finally, Sect. 8 is dedicated to summarize the major findings and to propose future work. Handling large lexicons and coping with unknown words are major issues for recognition tasks such as speech and hand-writing recognition. Several approaches have been proposed. The first approach consists in starting with a large static dic-tionary and filtering it to better fit with the domain of interest. Since dealing with large lexicons increases both computa-tional complexity and confusions with similar words, lexicon reduction approaches have been proposed to filter an initial dictionary.

A simple solution to filter out words from a dictionary is to consider word length as a decision criterion. In [ 6 ], an estimation of the minimal and maximal lengths of a word is performed based on the sequence of feature vectors. These values delimit an interval, and only words whose lengths fall within that interval are selected. In [ 7 , 8 ], the word length is estimated by counting the strokes in the area between upper and lower baselines.

Another approach for lexicon reduction is to consider the shape of the word as a filtering criterion. In [ 9 , 10 ], the stroke types for each character are identified (e.g., ascender, descen-der, medium). A word is considered to be a concatenation of these symbols. In the recognition phase, the sequence of symbols obtained for a word to recognize is compared to the representation of each word from the dictionary and only those words that are similar are kept. Both approaches for dictionary reduction (by using length and shape) are based on features extracted from word images and are therefore not very robust to noise and variations in writing styles.
Other approaches for dictionary reduction consist in auto-matically identifying the topics of the processed document, and to use the lexicon of the identified topic for further recog-nition [ 11 , 12 ]. The topics may be identified from recognition outputs, considering top-N hypotheses [ 13 , 14 ]. Reducing the dictionary to related concepts improves recognition. Lexi-con reduction approaches do not cope with OOV words, but rather assume that the word to recognize belongs to the initial dictionary. In contrast, our approach extends the initial sta-tic lexicon in order to cope with unknown words. However, our approach also includes a selection step in which the best word candidates from the external resource are kept.
The World Wide Web is an unlimited resource that can be successfully used in a variety of NLP tasks. It is constantly updated, and words that do not belong to general dictio-naries are frequently included. It has been recently used as linguistic resources in complement or instead of closed and handcrafted corpuses [ 15 ]. The Web can be successfully used for building language resources in different fields, includ-ing computational linguistics [ 16 , 17 ], statistical machine translation [ 18 , 19 ], speech recognition [ 4 , 20 ] and spelling corrections. For instance, unknown and sparse N -grams can be estimated through Wikipedia corpus [ 21 ]. Part-of-Speech (POS) tagging of unknown words can be achieved through Web search by including in the query the known context of the unknown word [ 22 , 23 ]. In [ 15 ], the authors propose a method that uses the Web as a source of misspellings, to automaticallybuilderror models. Morerecentlyin[ 24 ], OCR errors are postprocessed using Google suggestions. In [ 25 ], the unigrams provided by the WEB-IT corpus are used to rank hypotheses derived from character recognition outputs. From the results obtained for spelling corrections, it is impor-tant to notice that 80% of misspelling errors can be found at edit distance of one. In handwriting recognition systems, the words to recover are at a larger edit distance and therefore the problem is far more complex [ 26 ].

The objectives of the works presented above are similar to ours: dealing with unknown words, absent from an ini-tial corpus or dictionary. These works show that large Web repositories are efficient for coping with unknown words, and we propose to use such repository within a handwriting recognizer. We propose a handwriting recognition approach which com-bines high performance, due to reasonably sized static dic-tionaries, and flexibility provided by external Web resources. Dynamic dictionaries are built from such resources, enlarg-ing the coverage of the initial static dictionary. We process a sequence of word images as shown in Fig. 2 . We consider that this sequence includes anchor words and non-anchor words. Anchors words (AWs) are words reliably recognized, while non-anchor words (NAWs) are the remaining words (OOVs words or non-reliable IV words). In order to differ-entiate between anchor and non-anchor words, word images are input to a RNN recognizer, exploited in two configura-tions: without a dictionary as a filler model and with a static dictionary. We use a special kind of RNN, namely a Bidirec-tional Long Short-Term Memory (BLSTM) that takes into account dependencies between distant observations.
For each NAW, a dynamic dictionary is built by selecting words from the Web resource based on their string similar-ity with the NAW image, and their linguistic relevance in the NAW context. Similarity is evaluated by computing the edit distance between the sequence of characters generated by the RNN recognizer exploited as a filler model, and the Wikipedia words. Linguistic relevance is based on an N -gram language model estimated from the Wikipedia corpus.
The NAW word image is then re-decoded with the dynamic dictionary. Once an NAW image has been recog-nized, it becomes an anchor word (AW). The process is iterated till there are no more NAWs. The recognition system is based on the sliding window approach, thus avoiding word segmentation into characters. A sliding window of width w = 9 pixels is shifted from left to right on the word image in order to extract a sequence of feature vectors. However, word images are first preprocessed, deskewed and deslanted by the approach described in [ 27 ].
Each sliding window is divided into 20 fixed cells, and 37 features are extracted. These statistical, geometrical and directional features are described in [ 28 ]. Two consecutive sliding windows have a shift of  X  = 3pixels(seeFig. 3 ). These parameters were optimized in [ 29 ].
Recurrent neural networks are a class of artificial neural networks where the connections between hidden units allow dynamic temporal behavior and information storing. Bidi-rectional RNNs [ 30 ] process the data forward and backward by using two separate recurrent layers. Thus, bidirectional RNNs take advantage of the past and future context of the sequence given as input. The forward pass processes the sequence from left to right, while the backward pass takes the input sequence in the opposite direction. Both of them are connected at the same input and output layers (see Fig. 3 ).

BLSTMs are a type of bi-directional RNN architecture where the summation units in the hidden layer are replaced with memory blocks and they were introduced to solve the vanishing gradient problem [ 31 ]. The network input layer is composed of the features extracted from the sliding window at each time t . The output layer at time t includes as much cells as the number of symbols and letters used in the lexicon, e.g., 79 symbols corresponding to all 79 French characters (a X  X , A X  X , 0 X 9,  X  /  X ,  X   X   X  ,  X   X ,  X  X  X  , blank symbol and accentuated characters). Following the work developed in [ 32 ], each BLSTM hidden layer has 100 memory blocks. For training the network, the  X  X ack-Propagation Through Time X  method [ 33 , 34 ] is used for each utterance. The weights are updated using the gradient descent with momentum method.
For each frame, the posterior probability corresponding to each character class is computed by the BLSTM. These posterior probabilities are given as input to a CTC layer (Con-nectionistTemporalClassification)[ 35 ]whichoutputs,along with a score, a sequence of characters when no constraint is imposed (case without a dictionary) or a word from a dic-tionary when a dictionary is used. The CTC implementation is the one introduced in [ 32 , 36 ] which relies on a forward X  backward algorithm. Wikipedia is a comprehensive encyclopedia that describes a large number of concepts and is thus fitted for creating dictionaries or language models with a good language cover-age. Compared to other Web corpora, the choice of Wikipedia offers two important advantages. First, the encyclopedia cov-ers a wide range of domains and, therefore, can be effectively used to process handwriting corpora covering a large num-ber of domains. Second, the resource is freely available and constantly updated. A dump of French Wikipedia from Sep-tember 2012 is used in this work. A total of 410,482 articles that contain each at least 100 distinct words were selected.
Language models vary from one domain to another and their effectiveness is determined by the relatedness between the background collection and the domain repre-sentation. Consequently, we first focus on the selection of a Wikipedia subset which is most relevant to the target domain. Wikipedia articles are modeled using a classical TF X  X DF (Term Frequency X  X nverse Document Frequency) represen-tation [ 37 ]. TF X  X DF measures how important a word is for a document through TF, but also accounts for its distribution within the Wikipedia collection through IDF. Simply put, the importance of a term for a document is directly proportional with its number of occurrences in the document and inversely related to the number of different documents of a collection in which it appears.

The cosine similarity measure [ 38 ] between the TF X  X DF representation of the training collection used in experiments (i.e., the RIMES database [ 39 ] in our experiments), consid-ered as a single document, and that of each Wikipedia article is then computed. As a result, Wikipedia articles are ranked according to their proximity with the training collection. A domain-adapted dictionary is obtained by parsing the first 20,000 most similar articles, as illustrated in Fig. 4 .Only wordsthatoccuratleast12timesinthesearticlesareretained. This constraint is useful to discard erroneous words such as typos and non-word strings present in the Wikipedia corpus. In addition, it can be noted that rare Wikipedia words are also rare in the evaluation dataset and, more generally, in written texts. As a consequence, changing the value of this parame-ter in a range between 1 and 20 results in small performance variation and the best result was obtained with a value of 12 on a validation dataset. The domain-adapted Wikipedia dictionary thus includes around 76,000 unigrams, together with their document frequency (i.e., the number of unique documents in which terms appear).

The 20,000 most similar Wikipedia articles are also used as a corpus for providing word bigrams. For each word in the Wikipedia corpus, we count the number of occurrences of words which appear in the first position to its left and right. The left and right contexts obtained for the word  X  X accroche X  are illustrated in Fig. 5 . We will refer to the two lists of bigrams as left bigrams and right bigrams, respectively. Dynamic dictionaries are built for non-anchor words. Thus, the proposed approach starts with detecting such non-anchor words by classifying each word in the sequence as anchor word (AW) or non-anchor word (NAW). The classification is describedinSect. 6.1 .WordsfromtheexternalWebresources are selected to build these dictionaries as indicated in Sects. 6.2 and 6.3 . The dynamic dictionary built for each NAW is used to re-decode it with the BLSTM recognizer (Sect. 6.4 ). 6.1 Anchor/non-anchor word classification Anchor words are reliable In-Vocabulary (IV) words, while non-anchor words are the remaining words, i.e., OOVs and unreliable IVs. Anchor words are thus words from the static dictionary recognized with high confidence. Confidence is measured by the probability provided by the recognizer and by the fact that a vocabulary-independent recognizer (filler model) and a vocabulary-dependent recognizer would yield two similar character strings. Such match/mismatch between phoneme and word-based recognizers has been studied for speech recognition for detecting OOV regions [ 40  X  42 ].
The BLSTM recognizer provides for each word image the best word w from the static dictionary along with its recog-nition score L p (w) . Similarly, the BLSTM filler provides the vocabulary-independent best character string c associ-ated with w . Both outputs w and c are useful for classifying words as anchors or non-anchors. In order to label a recog-nized word w as an AW, its recognition score must be greater than a specific threshold and its lexical distance to the corre-sponding sequence c must be smaller than another threshold.
To compute the distance between a word w and its corre-sponding character string c , we use the following measure, dist Le v , which is a normalized Levenshtein distance calcu-lated as: dist Le v( c ,w) = where s , d and i are the minimum numbers of single character edits such as substitutions, deletions and insertions, respec-tively, to transform c into w .

Preliminary experiments showed that the optimal thresh-olds vary with the text to recognize. Therefore, the thresholds must be related to local statistics of the text. We propose to derive local statistics such as the average recognition score a v glog Pr oba , as well as the average Levenshtein distance a v gdist Le v between the words and their corresponding character strings, from a subset of words recognized with enough confidence (IV words). This subset is defined by constraining the recognition scores L p to be greater than a threshold thre , found on a validation set.

The local statistics computed from this subset are used within the decision rule which makes the final classification for a given word w , associated with its string c . w is an anchor word if it satisfies the following equations: dist Le v( c ,w)  X  a v gdist Le v + 0 . 3(2) L Bias values 0 . 3 and 0 . 01 are empirically determined on a validation dataset and are used throughout the experiments. The remaining words are non-anchors words (NAWs): They do not satisfy either Eq. ( 2 )orEq.( 3 ). Dynamic dictionaries are built for NAWs only, while AWs do not need any further processing.

We set the threshold thre on the Rimes word validation set (see Sect. 7.1 ). We assume that most IV words should belong to the subset of words recognized with enough confi-dence. Thus, we use the IV/OOV ground truth of the Rimes validation database. Recognition scores for each class, i.e., IV and OOV classes, are collected (see Fig. 6 ), and thre is set as the average of the recognition scores of the OOV class. 6.2 Exploiting non-anchor word linguistic context We consider a sequence of n words w 1 ,w 2 ,...,w n .The AWs are denoted by  X  w i . All the other words are denoted by  X  w j . It is supposed, without loss of generality, that the number of AWs is m and their indexes belong to the set I = i ,..., i probability of an NAW  X  w j , given its context, can be written as: P (  X  w N -grams can be used to estimate these probabilities. It is well known that the estimation of N -grams requires huge amount of data and reliable estimates could only be obtained if the context N is limited. In this case, the conditional probability becomes: P (  X  w = P (  X  w where j  X  N + 1  X  i k  X   X  X  X   X  i k + l  X  j + N  X  1.
In the case of N = 2, bigrams are being used. This approx-imation introduces an issue related to the fact that the left or right neighbors of a target NAW might not be AWs. In this case, two approaches can be considered:  X  An iterative approach, in which at each iteration the  X  Maintain a bigram approximation using probabilities not
In the present work, the iterative approach is adopted. In order to illustrate it, Fig. 7 provides examples of possible scenarios related to the positions of AWs and NAWs. The probability P (  X  w j | w 1 ,...,w n ) (Eq. 5 ) has to be computed for each  X  w j from the text. Depending on the configuration of the NAWs and AWs, this probability is estimated differently. The configurations are the following:  X  C a s e AW X  NAW X  AW this is the case for words w  X  C a s e AW X  NAW X  NAW X  AW this case is represented in Fig.  X  C a s e AW X  NAW X  NAW X  NAW X  AW represents the configu-It is worth noting that for larger contexts AW X  NAW X  ...  X  NAW-AW , several iterations as the latest one may be used considering knowledge from the exterior to the interior. 6.3 Dynamic dictionary construction from Web resource The best case consists in building the dynamic dictionary based on the bigrams of adjacent AW words. However, it is sometimes not possible to build the dynamic dictionary from bigrams since they may not be available. Two cases illustrate this scenario: (i) The first case corresponds to an application where we start recognition with no static dic-tionary. Thus there is no AW, inhibiting the possibility of building dynamic dictionaries based on bigrams, (ii) the sec-ond case corresponds to NAWs whose contextual words have a too short bigram list, inhibiting the possibility to rely on bigrams only. In such cases, unigrams are used as a backup solution. In the following, the dynamic dictionary creation using unigrams or bigrams is described. 6.3.1 Collecting words from unigrams Words from the domain-adapted Wikipedia dictionary (see Sect. 5 ) can be selected to integrate the dynamic dictio-nary, based on their string similarity to an NAW word. For this selection, the NAW decoded character string and the unigrams of the Wikipedia words are supposed to be avail-able. The Levenshtein distance [ 43 ] is used to compare the Wikipedia words to the character string. It computes the number of edits necessary for one sequence to turn into the other: deletions, substitutions and insertions. The most similar Wikipedia words to the entry character string c are grouped based on their Levenshtein distances. At equal dis-tance, Wikipedia words are sorted using their document frequency. The Wikipedia words for which the difference between their length and that of the decoded sequence c is at most l , are retained in the dynamic dictionary while tak-ing care not to have the size of this dictionary exceeding k . Note that the k retained words might also include words selected using bigrams. The values k and l are empirically determined on a validation database and are set to 500 and 5, respectively. Varying the size k of the dynamic dictionary with values ranging between 100 and 1000 has limited influ-ence on global performance. This behavior is determined by the fact that the average Levenshtein distance between NAW sequences obtained with the BLSTM filler and ground truth words is 2.8. With such a mismatch, the ground truth word is often among the nearest neighbors with respect to the Lev-enshtein distance, and it is not necessary to retain a lot of candidates. For comparison, the authors of [ 44 ] report that in spelling correction, 80% of the misspellings have an edit distance of one. These results show that the problem tackled here is more difficult.

The dynamic dictionary creation from unigrams is illus-trated in Fig. 8 . The word image  X  X ignalais X  is initially decoded as  X  X innxhsas X  when a filler model is used. By com-puting the Levenshtein distance against all words included in the Wikipedia dictionary, we obtain three groups of words (Levenshtein distances equal to 4, 5 and 6). Note that the ground truth word  X  X ignalais X  is found at a Levenshtein dis-tance equal to five and has a low document frequency. 6.3.2 Collecting words from bigrams As described above, when bigrams are available in the iter-ative process, they are used to select the words from the domain-adapted Wikipedia. As previously, the most likely words according to the bigrams are grouped based on their Levenshtein distance to the character string decoded by the filler model. The words to include in the dynamic dictio-nary are selected first following the increasing Levenshtein distance and second by a decreasing bigram. If the dictio-nary obtained from neighboring AWs is too rich, only the k most frequent words are retained. If this dictionary is not rich enough, it is complemented with Wikipedia words selected from unigrams (Sect. 6.3.1 ).

Higher-order N -grams ( N &gt; 2) could be introduced in the process by including in the dynamic dictionary words collected first from N -grams, then from N  X  1-grams ...till unigrams. There should thus be enough AWs after the first recognition step, and accurate N -grams estimates should be available. The preference given here for bigrams is motivated by the fact that we have good bigram estimates which encode accurate linguistic relations between words. 6.4 Word recognition In order to develop the iterative solution for recovering NAWs, presented in Sect. 6.2 , the word sequence is first tra-versed from top to bottom, collecting for each NAW whose neighbor on its left is an AW, the list of words correspond-ing to the right bigrams of this AW. Then, we traverse the + word sequence from bottom to top in order to collect the list of words corresponding to the left bigrams of NAWs whose neighbor on their right is an AW.
 An example of the processing algorithm for the sequence AW X  X AW X  X W  X  X e signalais l X  X ccueil X  is shown in Fig. 8 . The character sequence c , output by the filler model for the NAW word, is  X  X innxhsas X  . Running the word sequence from top to bottom, the right bigrams for word  X  X e X  are retrieved. Running the sequence from bottom to top, the left bigrams for word  X  X  X  X ccueil X  are identified. For instance for the AW word  X  X e X , the most frequent words that have this AW on the left are:  X  X e X  ,  X  X uis X  ,  X  X e X  ,  X  X ous X  ,...,  X  X on-naissais X  ,...,  X  X ignalais X  . For the AW word  X  X  X  X ccueil X  ,the most frequent words that have this AW on the right are:  X  X e X  ,  X  X t X  ,...,  X  X ermettant X  ,...,  X  X ignifiait X  . We select only the most similar words with the NAW character sequence. In this case, the number of most similar words to the NAW charac-ter sequence, obtained from neighboring AWs is not enough. Therefore, the lists are expanded with the most similar uni-grams. From these collected words, a dynamic dictionary is created and a second decoding is run with this adapted dic-tionary. Even though the ground truth word does not have a high document frequency in Wikipedia and its distance with the character sequence c is high, it can still be recovered. The NAW is replaced with the result of this decoding, in this case  X  X ignalais X . For further iterations, this word will be con-sidered as an AW. The algorithm is run until all NAWs are replaced by AWs.

An example of the use of the dynamic dictionary is given in Fig. 9 . From the input word sequence, the result of the AW/NAW classification is provided: AWs are in black, NAWs in red. NAWs are still represented by the character sequence provided by the filler model. The final output is obtainedbyreplacingtheNAWsafterdecodingwithdynamic dictionary. For this word sequence, the algorithm was iterated once, since all NAWs are surrounded by AWs. To assess the effectiveness of dynamic dictionary creation presented in Sect. 6 , experiments are carried out with the RIMES [ 39 ] database. The metric used throughout all exper-iments is accuracy computed as the number of correctly recognized words divided by the testing set size. In the exper-iments described below, the results are case-insensitive (i.e., a = A), but accent errors are counted (i.e., a =` a). 7.1 RIMES database The RIMES database (Reconnaissance et Indexation de donn X es Manuscrites et de fax-simil X S/Recognition and Indexing of handwritten documents and faxes) [ 45 ] gathers different types of manuscripts written in French: correspon-dence, forms and faxes. It was created by the French Ministry of Defense to assess automatic handwriting recognition sys-tems. RIMES has been used in evaluation campaigns since 2007 [ 39 , 46 ]. RIMES was created by asking volunteers to write letters in their own words related to scenarios such as bank account information, letters of complaint, payment difficulties or reminder letters. It brings together more than 12,500 handwritten documents written by around 1300 vol-unteers. The letters are written on white paper, in black ink, without guide lines, and are sampled at 300 dpi in gray scale (see Fig. 1 ).

For system implementation, we use the RIMES word and text databases used for the ICDAR 2011 French word recog-nition campaign [ 39 ]:  X  The RIMES Word Dataset includes a training set of  X  The RIMES Text Dataset includes 1500 training text-
The original RIMES Text Dataset is not segmented into words and this segmentation is necessary for our experiments, in order to use contextual information, as described in Sect. 6.3 . Therefore, from the text-blocks of the testing dataset, we have created a new dataset which we call the WS-RIMES-text database (word-segmented RIMES text database), which contains 5586 word images. This new set of word images includes the segmenta-tion of the text-blocks into words. An HMM system [ 26 ] has been used for this purpose in a semi-automatic way (forced alignment).

The system is trained with the RIMES word training set usingalexiconofaround5000differentwords,andcalibrated with the RIMES word validation dataset, comprising a lexi-con of around 1600 different words. The word-segmented WS-RIMES-text dataset was used for testing purposes. Around 7% of the testing set words are not present in the training set and are OOVs. 7.2 AW/NAW classification results Figure 10 shows the histogram of NAW occurrences in the testing dataset. NAWs represent 17.1% of the words in this set. Figure 10 also provides recognition accuracy in each bin, when NAWs are recognized using the dynamic dictionary-based approach.

A majority of the NAWs appear only once, and recogni-tion accuracy reaches 40% in these cases. Recognition rates are higher for more frequent NAWs that appear at least two or three times. This result is explained by the fact that the Wikipedia context of very rare NAWs is not robust enough. 7.3 Word recognition results For each NAW word, a dynamic dictionary is created based on the domain-adapted Wikipedia bigrams and unigrams, with priority given to bigrams. In Table 1 , we present results obtained with the Static dictionary and with our method Dynamic dictionary . In addition, we provide results that could be obtained if a perfect AW/NAW classification was available from an oracle, by considering all IVs as AWs and OOVs as NAWs. This is the Dynamic dictionary, ideal AW / NAW case. The ideal separation is used in order to high-light the maximum accuracy gain that could be expected using the proposed approach.

We provide the Wald confidence intervals computed as  X  p recognized words, N the number of testing data, and k is the 100 ( 1  X  with a risk  X  = 5 %. In Table 1 , the improvement brought by the use of a Web resource exceeds 3% in absolute value (77.06 vs. 73.88%) and is significant following the Wald test: The 77.06% recognition rate is outside and over the Static dictionary approach. The difference between real and ideal AW/NAW separation shows that further progress is possible if this classification is improved.

A second set of experiments has been conducted to show the impact of dynamic dictionary creation on accuracy and dictionary coverage when different percentages of OOVs are considered. Starting with a 7.1% rate of OOVs in the test-ing dataset, the least frequent words from the testing set are eliminated from the static dictionary, each time by 10%, until OOVs represent approximately 55% of the testing set. The sizes of the new static dictionaries are 4506, 4204, 4065, 3999 and 3966 words, corresponding to 15, 25, 35, 45 and 55% of OOV words, respectively. Results are shown in Fig. 11 for all percentages of OOVs, when decoding with the corresponding static dictionary or using dynamic dictionar-ies. As expected, the overall recognition accuracy decreases for both cases as the OOV rate increases but it decreases less when using dynamic dictionaries. For instance, when approximately 55% OOVs are present in the testing set, the recognition accuracy is equal to 60.66% and only 43.62% with the static dictionary. The improvement is thus greater than17%inabsolutevalue.Thisimprovementisevengreater when the ideal AW/NAW classification is used: 22.5%.
Figure 12 plots dictionary coverage as a function of OOV proportion in the testing set. When only 7% of OOVs are present in the testing set, the coverages for the static dic-tionary and the dynamic dictionary are very similar due to AW/NAW misclassifications. When dynamic dictionaries are used, the coverage is always greater than when static dictio-naries are used, because missing words are retrieved from the external resource. When the number of OOVs increases, both dictionary coverages decrease, but the coverage of the dynamic dictionary decreases less than that of the static dictionary. In the case the OOV rate is equal to 55%, the coverage improvement brought by the dynamic dictionary is greater than 20% .
Figure 13 provides examples of recognition errors. The first example combines misrecognition of an AW word in the neighborhood of an NAW, with errors within the Wikipedia corpus. The AW word  X  X ie X  has been misrecognized as  X  X e X . Unexpectedly, the right bigrams for word  X  X e X  include word  X  X an X  even if  X  X an X  is not a French word and the transi-tion  X  X e  X  can X  does not exist in the language. However,  X  X an X  belongs to the French Wikipedia corpus. This can be explained by the fact that in the case of Wikipedia, different persons can edit articles and, as such, the online encyclopedia ( a ) (b) (c) ( d )(e) is a representative example of crowd sourcing. The articles may contain though words that come from other languages or misspelled words (misprints, typos or syntax errors). Thus, string  X  X an X  has been introduced in the dynamic dictionary for the input image  X  X ar X  shown in Fig. 13 a and a confu-sion has occurred. The second error example is due to NAW sequences. When two NAWs follow each other, each NAW is recognized in isolation taking into consideration only one context, left other right. The linguistic relevance of the suc-cession of the recognized words is currently not checked. In Fig. 13 b, the NAW word sequence  X  X oordonn X es bancaires X  (i.e., bank account details) has been recognized as  X  X oor-donn X es fonci X res X  since each NAW has been recognized separately. However, this word sequence does not exist in French ( X  X onci X res X  meaning  X  X roperty X ).
 Recognition errors may occur when an input word is an OOV that is not included in the Wikipedia corpus. This happens for family names or words not commonly used. Even when these words are classified as NAWs, the dynamic dictionary cannot include the correct word. Thus the recog-nized words are similar words from Wikipedia. In Fig. 13 c,  X  X unida X  is a family name which has been recognized as  X  X u X de X  (Sweden in French), while word  X  X ffectivit X  X  in Fig. 13 d is an uncommon word in French which has been recognized as  X  X ffinit X  X  (affinity in French).
 The RIMES corpus itself can be a source of error. Since RIMES documents are written by volunteers using their own words, they contain misspelling errors. Typical examples are  X  X ddresser X  (instead of adresser) or  X  X ourier X  (instead of courrier). Around 3% of the words from the static dictio-nary (training lexicon) are misspelled. Thus, if the spelling error is present in the training corpus, it will be introduced in the static dictionary. Therefore, during the decoding step, the erroneously spelled word may be chosen instead of the correct word such as the word  X  X  X cesaire X  in Fig. 13 e.
The relative improvement brought by dynamic dictionar-ies over static dictionaries is higher when OOV proportion increases, or equivalently, when static dictionary coverage decreases. Thus, the experiment presented in the following section consists of using no static dictionary (100% OOVs, 0% coverage) and Web-based dynamic dictionaries only. 7.4 Recognition without any initial static dictionary These experiments are conducted in order to assess recogni-tion accuracy starting with no static dictionary.
In the first experiment, the BLSTM filler is used in iso-lation, and the accuracy is 44.75% (see Table 2 ). In the second experiment, all words are initially included in the NAW class. Thus, dynamic dictionaries are built from Web resources at unigram level only. The recognition accuracy (BLSTM filler + dyn. dict, N-gram, N = 1) at this step is 67.66%. This result already improves the BLSTM filler by more than 23% in absolute value. After this first iteration, some of the words are classified as AWs and bigrams can be included in dynamic dictionaries. The recognition process continues until no NAW is left. Recognition accuracy is fur-ther improved, brought by the use of bigrams (BLSTM filler + dyn. dict, N-gram, N = 1 , 2).

If we now compare this result with the one using a static dictionary (Table 1 , Static dictionary ), the performance drop is less than 5%.

This experiment is interesting on the one hand, because it highlights the discriminative power of RNNs and, on the other hand, because it shows that interesting results are obtained without using a static dictionary. Results from pre-vious works on open vocabularies [ 1 , 47 ] show a larger gap between approaches which exploit only filler models and approaches which build a static dictionary. Here, this gap is much smaller and the combination of filler models and of dynamic dictionaries can be considered as a first step toward building high-performance open-vocabulary systems. 7.5 Recognition with large static dictionaries Another set of experiments was performed in order to show the impact of the static dictionary size on the recognition rate. Instead of enlarging the dictionary dynamically, unigrams from the domain-adapted dictionary are gradually included into the static dictionary, according to their frequency.
When the entire Wikipedia dictionary is used for decod-ing, together with the training dictionary (around 75,000 words), the system X  X  performance drops to 60.84%. This means around 9% less than a system that uses no static dic-tionary at the beginning and dynamic dictionaries in a second phase ( BLSTM filler + dyn. dict, N-grams, N = 1 , 2) (see Sect. 7.4 ). This shows that as the dictionary is growing, con-fusions become prevalent and the overall quality of results decreases. Accuracies and dictionary coverages are shown in Table 3 . 7.6 Discussion Other results are available for the RIMES word testing database, including those of the ICDAR 2011 [ 39 ] French Handwriting Recognition Evaluation Campaign. However, these results are not easily comparable to ours. First, our testing set, WS-Rimes-text, is obtained by the segmentation of the RIMES text-lines into words, while the testing sets used at ICDAR 2011 are either line-based or word-based but without accounting for the lexical order of words. Second, the dictionary used for the word ICDAR 2011 competition includes the words of the training and testing sets, and there-fore it does not contain any OOV word, a situation which is not often encountered in real-life applications. Third, the best results at ICDAR 2011 are obtained with relatively complex classifier combinations. Such combinations would probably have a positive effect on the performance of our proposed approach, but are out of focus here. The processing of OOV words is a central challenge in hand-writing recognition. In our work, we propose an approach for robust handwriting recognition based on the combination of static and dynamic dictionaries. While recognizing a text, the proposed method builds on the fly dynamic dictionar-ies for words labeled as non-anchors, which are words for which we are not confident of the recognition results. There-fore, and after a first recognition pass, recognized words are classified as anchors and non-anchors based on their recog-nition score and on the similarity between two recognition results represented as sequence of letters and obtained when the same recognizer is applied with and without a vocab-ulary. We propose to exploit Wikipedia, a large-scale Web lexical and linguistic resource to build the dynamic dictio-naries associated with each non-anchor word. The proposed approach makes use of two criteria to select the words from Wikipedia to include in the dynamic dictionary. First, the words to be included in the dynamic dictionary need to be similar to the sequence of letters decoded for the non-anchor word when using a filler recognizer (recognition without vocabulary). Second, the words need to be relevant linguistically in the context of the non-anchor word. The pro-posed method performs better than a baseline system that uses a static dictionary. For the testing set of RIMES, we start from an initial static dictionary yielding 7% of OOV words. The accuracy obtained is 73.88 % when the static dictionary is used and reaches 77.06 % when our method is applied creating dynamic dictionaries for non-anchor words. The behavior of dynamic dictionary-based recognition in the presence of a larger number of OOVs is assessed by pro-gressively decreasing the static vocabulary size. Naturally, a performance drop appears for both static and dynamic dictio-naries, but results for dynamic dictionary-based recognition decrease more slowly due to the influence of the context and the exploitation of external resources, showing thereby higher robustness. Equally interesting, we show that dynamic dictionaries can be used to significantly reduce the perfor-mancegapbetweenapproachesthatexploitstaticdictionaries and open-vocabulary systems.

Future work will be focused on the following points:  X  The recognition of special cases of OOV words (codes,  X  Currently, only the immediate neighborhood of candidate  X  The methods introduced here are built on top of word- X  Improving the AW/NAW classification method is impor- X  Finally, the way we are building dynamic dictionaries can
