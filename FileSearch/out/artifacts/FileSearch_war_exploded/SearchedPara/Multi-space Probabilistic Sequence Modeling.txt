 Learning algorithms that embed objects into Euclidean space have become the methods of choice for a wide range of problems, ranging from recommendation and image search to playlist prediction and language modeling. Probabilis-tic embedding methods provide elegant approaches to these problems, but can be expensive to train and store as a large monolithic model. In this paper, we propose a method that trains not one monolithic model, but multiple local embed-dings for a class of pairwise conditional models especially suited for sequence and co-occurrence modeling. We show that computation and memory for training these multi-space models can be e ciently parallelized over many nodes of a cluster. Focusing on sequence modeling for music playlists, we show that the method substantially speeds up training while maintaining high model quality.
 I.2.6 [ Artificial Intelligence ]: Learning; I.5.1 [ Pattern Recognition ]: Models Algorithms, Experimentation, Performance Music Playlists, Recommendation, Embedding, Sequences, Parallel Computing
Learning methods that embed objects into Euclidean space have become the method of choice for a wide range of prob-lems, ranging from recommendation and image search to playlist prediction and language modeling. Not only do they apply to modeling problems where a feature-vector represen-tation of objects is not available (e.g., movies, users, songs), they actually compute a vectorial representation that can be used as the basis for subsequent modeling steps (e.g., semantic and syntactic language modeling).

While small to medium-scale models can be trained by standard methods, training large-scale models may not be feasible on a single machine. This is especially true for em-bedding problems that go beyond the Gaussian model of rating prediction. For example, when the embedding is used to model probability distributions over discrete objects like sequences (e.g., playlists [18], words in a sentence[10], pur-chases [16]) or complex preferences (e.g., as extension to [15]), computation time and memory for storing data and model become a bottleneck.

In this paper, we explore training algorithms for embed-ding models that execute a distributed fashion, especially for logistic embedding models of sequences and co-occurrences. We formulate an extended logistic model that directly ex-ploits the properties of the data  X  namely that many de-pendencies are local even though we are training a global model. By uncovering the locality in the data, we parti-tion the global embedding problem into multiple local em-bedding problems that are connected through narrow inter-faces, which we call portals . We show that training this portal model in a distributed fashion can be decomposed into two steps, each of which performs maximum-likelihood optimization.

By deriving this portal model as an explicit probabilis-tic model for merging multiple local embeddings, the model not only allow more e cient training but also allows e -cient prediction in a distributed fashion. Furthermore, the portal model provides understanding for why and when par-allel training will be e  X  ective. We conduct extensive experi-ments on probabilistic sequence modeling for music playlist prediction, showing that we can train on hundreds of nodes in parallel without substantial reduction in model fidelity, but in orders of magnitude less time.
Embedding methods have been long studied and proved to be e  X  ective in capturing latent semantics of how items (e.g. words in sentences) interact with each other. These meth-ods only have a linear blowup in parameters as the number of items goes up. For the purpose of this paper, this line of works can be categorized into two classes. The first class [17, 19, 20, 6, 21, 8, 16] defines a score to measure the in-tensity of any interaction, which is optimized while learning the embedding. The other class [1, 5, 11, 4, 10] explicitly reasons about the embedding under a probabilistic model of the data. Particularly relevant for this paper are those that normalize via a soft-max function to model distribu-tions over discrete items, and that are trained via maximum likelihood.

There are at least two approaches to training these em-bedding models. First, one can formulate a relaxation of the training problem that can be optimized globally (e.g. a semidefinite programming or singular value decomposition [17, 19, 20]). Second, one can explicitly fix the dimensional-ity and solve the resulting non-convex objective function to a local optimum. Most popular and generally e  X  ective for this second approach are stochastic gradient method [18, 4, 16] that take one interaction at a time to compute and up-date with local gradients. However, for probabilistic models using the soft-max function computing gradients requires summation over all items. This can be troublesome when the number of total items scales up.

One way to address the growing computational needs aris-ing from large datasets is the use of parallel computation. In particular, there are several works that aim to parallelize training via stochastic gradient methods for both shared-memory [14] and distributed-memory settings [22, 2]. The extension to multi-space embedding models we propose is di  X  erent from these works. The embedding problem is di-vided into subproblems in multiple spaces that are only losely coupled, so that they can be solved in an embarrass-ingly parallel fashion. Furthermore, the model we introduce not only distributes computation, but also reduces that over-all amount of computation that is necessary.
We would like to first introduce a general family of models that can benefit from the techniques we propose in this pa-per. Suppose we have n types of items X = all X i being disjoint. Each type X i contains |X i | distinct of directional pairwise observations in the form of ( y | x ), where x, y 2 X .Foreach x 2 X ,weassociateitwith a d -dimensional ( d is predefined) vector X ( x ), so that the conditional probability of the pair ( y | x ) can be modeled as Here { X } is the indicator function and I (  X  ,  X  ) is the inter-action function between two vectors in the d -dimensional space. Common choices include negative Euclidean distance and the inner product. The goal is to learn the collection of all the d -dimensional vectors (or a which can be done by maximizing the likelihood
Several existing works fall into this class of models:
One should note that it is also possible to model joint distributions instead of conditional ones. Following [4], we could let where Pr( x ) is estimated empirically from the training set. If symmetry is important, one could also consider
In the rest of the paper, we mainly focus on sequence mod-eling via LME. However, it is possible to extend the ideas to this more general family, as to be discussed in Section 4.5.
LME [18, 12] was introduced as a probablistic model for learning to generate playlists. Given a collection of songs S = { s 1 ,...,s |S| } , a playlist is a sequence of songs from S . We use p =( p [1] ,...,p [ k p ] ) to denote a playlist p of length k . We use D to represent a collection of playlists. Given a training sample D of playlists, the goal is to learn a d -dimensional vector for each of the songs in S .

Suppose the vector that represents song s is X ( s ). The transition probability given the previous song in a playlist p to the next song is modeled as where Z ( p [ i 1] ) denotes the partition function in the denom-inator, and the distance || X ( s ) X ( s 0 ) || 2 is abbreviated by ( s, s 0 ). Given this local transition probability, the proba-bility of a playlist is modeled as The learning problem is to find the coordinates for each of the songs (they form a |S| by d matrix X ) that maximize the likelihood on a training playlist collection D
Since this model only uses order-one Markov dependency, it is convenient to use a transition matrix T to represent the collection of playlists D . The element at the i th row and j th column T ij is the number of transitions from s i s j (We will denote this transition pair as ( s i ! s j ), with s called from-song and s j called to-song) in the playlist collection. T is usually very sparse. Thus it can be stored e ciently via hashing. Using T , the optimization problem can be rewritten as Following the empirical results in [18], it is beneficial to in-clude a popularity boost term b ( s ) for each of the songs, slightly modifying (5) into where idx( s ) returns the index of a song in the song col-lection (e.g. idx( s j )= j ). Equation (6) -(8) need to be changed accordingly. We call this model boosted-LME and the original model unboosted-LME. For brevity, we use the unboosted-LME for all mathematical derivations in this pa-per, but use the boosted-LME in the experiments.
The LME is typically trained using stochastic gradient, with the gradient for the log likelihood of any transition pair ( s a ! s b ) expressed as @ l ( s a ,s b ) @ X ( s i ) where putation of Z ( s a ) involves summing over |S| terms.
In each iteration of stochastic gradient descent, the algo-rithm sequentially traverse all the transition pairs ( s i in D . To be more specific, it first picks a from-song s i putes and accumulates gradients for the transition pairs that have s i as from-song, then adds it back to update the em-bedding and move on to the next from-song. This grouping by from-song allows that gradients for pairs ( s i ! s j ) and ( s i ! s j 0 ) can share the computation of Z ( s i ). There are |S| from-songs, and for each from-song, the complexity for computing the partition function is O ( |S| ). Thus, the time complexity for each iteration is O ( |S| 2 ). This causes serious scalability issue, and training LME on dataset with around 75K songs and 15K playlists took more than two weeks even for dimensionality d =2.
A first approach to speed up training is to parallelize the algorithm. The most natural approach is to perform the gradient computation in parallel. In each iteration, we as-sign each thread/process an approximately equal number of from-songs, and let it be responsible for the gradients that are associated with those from-songs. When it comes to the implementation of this method, there is a distinction be-tween shared-memory setting and distributed-memory set-ting. Figure 1: Time for computing the embedding on yes big with respect to number of cores used for shared-memory (left) and distributed-memory (right) parallelization. The di  X  erence in runtime for 1 core result from di  X  erent hardware used in the two experiments.

In the shared-memory parallelization, all threads share the same copy of the parameters (in our case, the embedding matrix X itself) to learn in main memory. A read-write lock ensures consistent access to X . We implemented the shared-memory parallelization with pthread, and tested it on an eight-core machine. A typical time-against-number-of-cores curve is shown in Fig. 1 (left), showing a close to linear speedup. However, the cost of a multi-core CPU goes up superlinearly with respect to the number of cores, and locking X will become a bottleneck as the number of cores increases.

In a distributed-memory architecture, where multiple ma-chines connected via a network are used, it is easier to get a large number of processors. However, the communication overhead is typically larger. We implemented the algorithm using the Message Passing Interface (MPI), again letting each process be in charge of a subset of from-songs. Since each process now has its own copy of the embedding matrix X , we need to introduce checkpoints for communication over network to sync the matrices. At each checkpoint, one mas-ter process collects all the accumulated gradient from all the processes, adds them together to update the embedding matrix, and then redistributes the updated embedding ma-trix to each process. Each checkpoint for communication involved one MPI Reduce and one MPI Bcast call. A curve from one run is plotted in Fig. 1 right. The speedup is much less than for the shared-memory implementation, and it is not monotone, let alone linear. In general, network commu-nication creates large overhead and large variability in the runtime.

We conclude that naive parallelization provides only lim-ited benefits and has its own scalability limits. In particular, the naive parallelization does not reduce the total compu-tation needed for training, meaning that even under perfect scaling through parallelization we need to grow the number of processors quadratically for this O ( |S| 2 ) algorithm. In the next section, we therefore explore a new probabilistic embedding model that not only enables parallelization, but also addresses the O ( |S| 2 ) scaling. The key insight motivating the Multi-space Logistic Markov Embedding (Multi-LME) proposed in the following lies in the locality of the data. Generally, songs only have transition connecting with a small subset of (similar) songs Figure 2: Illustration of transitions under Multi-LME. Gray, blue and orange circles represent real songs, entry portals and exit portals respectively. Left: intra-cluster transition from s a to s b in the same cluster C u . Right: inter-cluster transition from s used in the process. (e.g. songs of the same genre). While we would still like to learn a global transition model, we only need to model these local transitions in detail, while a coarser model su ces for songs that are far away. The Multi-LME exploits this lo-cality and abstracts far-away song songs behind a narrow interface that allows them to be processed largely indepen-dently on di  X  erent compute nodes.

The Multi-LME model itself is a probabilistic embedding model that is trained via maximum likelihood in two steps. First, we formulate the problem of coarsely partitioning the songs as a maximum likelihood problem. Second, models for local sets of songs and their interfaces, called portals ,to remote songs can be solved as largely independent maximum likelihood problems. Suppose, for now, we already have a partition of all songs S into c clusters { C 1 ,C 2 ,...,C c } . How to get this parti-tion will be explained later. Naively, we could train a sepa-rate LME on each cluster in fully parallel fashion to embed its songs in an individual space. This o  X  ers a probabilis-tic model for the intra-cluster transitions. However, we still need to account for the inter-cluster transitions, since there is typically still a substantial portion of inter-cluster transi-tions.

We model these inter-cluster transitions via what we call portals 1 . Portals can be thought of as virtual songs added to each cluster to connect them. Each cluster has 2( c 1) portals, half of which are entry portals from the other c 1 clusters and the other half are exit portals to the other c 1 in cluster C u from/to cluster C v . We also use O u to denote the set of portals in C u .

With the help of portals, it it now possible to model inter-cluster transition in a two-step fashion: suppose we want to do the transition ( s a ! s b ), with s a 2 C u and s b 2 C shown in Fig. 2 (right). The Markov chain first transitions from s a to the exit portal o exit u,v to cluster v in cluster u (colored orange). It then transitions with probability 1 to the entry portal o entry v,u from cluster u in cluster v (colored blue). From there, the chain takes a second step to go to s
The name portal is inspired by the video game Portal by Valve Corporation. A illuminating video that explains the idea can be found on their website http://store. steampowered.com/video/400 .
 Figure 3: Illustration of the e  X  ect of portal trick on the transition probability matrix for the case of three clusters. We assume the songs within the same cluster are grouped together. The intra-cluster transitions (diagonal blocks) are decided by the local LME. The inter-cluster transitions (o  X  -diagonal blocks) are rank-one approximated by the outer product (denoted by  X  ) of an exit vector and an entry vector.
 This means that Pr( p [ i ] | p [ i 1] ) is modeled as the product of its embedding in its own space. More specifically, we have
Adding portals does not change the representations of the intra-cluster transition by much. Intra-cluster transition still takes only one-step. As shown in Fig. 2 (left), we go directly from s a to s b in cluster u . A slight di  X  erence is that, when it comes to the partition function, we also need to consider the contribution of the portals. Formally, we have
Adding popularity terms to equation (11) and (12) for both songs and portals is straightforward.

Figure 3 summarizes and further illustrates the structure of the portal model. Denote with P exit u,v the length-| C vector that contains Pr( o exit u,v | s ) , 8 s 2 C u , and with P length-| C v | entry vector that contains Pr( s | o entry Then the |S|  X  |S| transition probability matrix that con-tains all the Pr( s j | s i ) is structured as illustrated in Fig. 3: diagonal blocks that govern the intra-cluster transitions are represented by their local LMEs; any o  X  -diagonal block that stands for inter-cluster transitions can be seen as a rank-1 approximation by the outer product of an exit vector P exit and an entry vector P entry .

Finally, to verify that the Multi-LME is a valid proba-bilistic model, the sum of transition probabilities to all the songs in the collection must always be upper-bounded by 1.
X = = =  X  =  X  =1 . (13) The fact that it is not equal to 1 is because our model assigns some probability to transitions from real songs to entry por-tals and transitions that make more than one pass through portals. We allowed them for the convenience of implemen-tation. Note that this approximation is conservative, since any probability or likelihood we compute is actually a lower bound of the true value. We argue that the impact is mini-mal as long as the number of portals/clusters is small com-pared to the number of songs in each cluster. Also, when it comes to playlist generation, we can always renormalize the transition probabilities to make them sum to 1.
The key advantage of the Multi-LME model is that train-ing can be completely (with respect to both objective func-tions and parameters) decomposed for each cluster. To see it, we can write the likelihood on the training sample as D u is the local subset of the training sample D restricted to the songs in cluster C u , as computed by Alg. 1. Note that each L ( D u | X u ) depends only on the parameters X u , which is a | C u | +2( c 1) by d matrix representing the coordinates of the songs and portals in the space of C u . To maximize the entire likelihood L ( D | X ), we can optimize each L ( D independently.

In practice, one can solve all local LMEs in parallel, with each single LME running on one node without communicat-ing with others over the network. If the number of local LMEs exceeds the number of processors, we find the fol-lowing scheduling algorithm to be e  X  ective [9, p.600-606]. For each of the c LMEs, we associate it with the number of the nonzero elements in the transition matrix as a load factor. We then sort these LMEs in descending order of the Algorithm 1 Build training set for a cluster Input: Training set D , partition { C 1 ,C 2 ,...,C c } Output: Training set D u for C u
Initialize D u as an empty set. for ( s i ! s j ) 2 D do end for load factors. Starting from the LME with biggest load, we sequentially assign them to the process with least total load.
If we sequentially solve all local LMEs on a single proces-sor and assuming a fixed number of iterations, the overall complexity is O ( c (max i | C i | +2( c 1)) 2 ) for an appropri-ate value of c , which is much better than the O ( |S| 2 ) of the monolithic LME.
Finally, we need to resolve how to partition the collection of songs S into c disjoint clusters. To a first approximation, we want to create a partition of songs that gives us as many one-step intra-cluster transitions as possible on one hand. On the other hand, we would like to have the size of clusters to be as balanced as possible, so that the whole process would not be bottlenecked by one big local cluster. Overall, this preclustering step needs to be e cient and scale well, since it will be executed on a single machine.

As shown in [18], the embedding produced by LME forms meaningful clusters, with songs that have high transition frequencies being close to each other. This suggests that LME itself could be used for preclustering.

Consider a small subset of songs S int from S , which we call  X  X nternal songs X . All other songs are called  X  X xternal songs X , denoted by S ex = S\S int . We propose a modified LME objective that models transitions between internal songs as usual, and aggregates transitions to external songs behind special objects called X  X edleys X  M = { m 1 ,m 2 ,...,m c } .We consider c medleys, one for each cluster, which are points in embedding space like portals. Suppose we have a mapping f (  X  ) that maps a external song to a medley. Then our entire training playlist dataset or training transition pairs D can be rewritten into a new one D 0 by replacing any external songs s with its medley f ( s ) and only keeping transitions that involve at least one internal songs.

We then train an LME on this new dataset with |S int | real songs plus c medleys. Once the embedding is trained, we can reassign each external songs to a medley that further maximizes the training likelihood. For an external song s 2 S Algorithm 2 LME with medleys
Input: Training set D , internal and external song collec-tion S int and S ex .

Initialize f (  X  ) as random mapping from external songs to medleys. while have not converged do end while with transition probabilities given by the LME. This can be done by simply traversing all the medleys.

Training the LME and reassigning external songs to med-leys can be alternated to greedily maximize likelihood. This algorithms is summarized in Algs. 2 and 3. Once the al-gorithm converges, we do the following to assign each song into a cluster:
There are a few tweaks we used in our implementation that help improve the performances. First, we select the songs with the largest number of appearances as internal songs. Second, we stop the two-step algorithm when less than 0 . 5% of the external songs that have transitions with internal songs change its medley compared to the last itera-tion. Third, we choose to train an unboosted LME as empir-ically it gives more balanced clustering results, which is good for parallelization to be discussed in later sections. Fourth, we fix the dimensionality to be 2 to make the precluster-ing phase as fast as possible. Fifth, each LME run except the first one is seeded with the embedding produced by the previous iteration.
 The number of points that need to be embedded in each LME run is |S int | + c , so the complexity of each LME it-eration is O (( |S int | + c ) 2 ). As we control |S int than 10% of S , this is acceptable. Also, because of seed-ing, later LME runs take much less time than earlier runs, as most of the vectors are already near its optimal position at initialization. Usually it takes less than 20 LME runs to converge.

There are other algorithms/packages that could be used for preclustering, and we explore the following two in the following experiments: 1. Spectral Clustering [13] can be used to cluster an undi-Algorithm 3 Build transition pairs with medleys
Input: Training set D , internal and external song collec-tion S int and S ex , external song to medley mapping f (  X  ). Output: New training set with medleys D 0 .

Initialize D 0 as an empty set. for ( s i ! s j ) 2 D do end for Table 1: Statistics and baselines of the playlists datasets. 2. METIS [7] is a popular software package that does
We have implemented two versions of Multi-LME. One is a single-process version, which is written in C and se-quentially solves all local LMEs in the second phase. The other one is an MPI version, which is implemented in C with Open MPI [3]. It dispatches the LMEs in the sec-ond phase to di  X  erent processes. MPI allows the program to be run on both shared-memory (a multi-core machine) and distributed-memory (cluster of machines) setting, and it handles communication transparently. For both versions, we use the LME-based algorithm as the default precluster-ing method. We also o  X  er the option to take partitioning results produced by other programs as an input file. The source code is available at http://lme.joachims.org .
The portal trick for parallelization can also be applied more generally to the family of models defined in Section 3. The key modification is to introduce portals (potentially di  X  erent portals for di  X  erent types of items), and rewrite the conditional probability Pr( y | x ) as Pr( y | o entry )  X  Pr( o Then the embedding problem breaks up into several inde-pendent and much smaller problems in separate spaces. For the preclustering phase, one can come up with a problem-specific algorithm, or just use the general purpose clustering methods discussed in the Section 4.3.
The following experiments analyze training e ciency and prediction performances of the Multi-LME on datasets of Figure 4: A plot of a multi-spaced embedding pro-duced by Multi-LME with d =2 and c =9 for yes big . Gray points represent songs. Blue and orange num-bers represent entry and exit portals respectively, with the number itself denoting the cluster it is linked with. di  X  erent sizes. The monolithic LME will serve as the key baseline. We also explore the e  X  ect of di  X  erent precluster-ing methods, and how robustly the model behaves under di  X  erent parameter choices.
 We evaluate on the playlists datasets collected from Yes.com that is described in [18]. Yes.com is a website that provides radio playlists of hundreds of stations in the United States. Its web-based API 2 allows user to retrieve the playlists played in last 7 days at any station in the database. Playlists were collected without taking any preferences on genres. Preprocessing that keeps songs whose number of appearance is above certain threshold o  X  ered three datasets with di  X  erent number of songs, namely yes small (thresh-olded by 20), yes big (thresholded by 5) and yes complete (thresholded by 0, everything is kept). Then each of the dataset was divided them into a training set and a testing set, with the training set containing as few playlists as possi-ble to have all songs appear at least once. The key statistics about the datasets are given in Table 1, and the datasets are available at http://lme.joachims.org .

Unless specified otherwise, our experiments use the fol-lowing setup. Any model is first trained on the training set and then tested on the testing set. The test performance is evaluated by using the average log-likelihood. It is de-fined as log(Pr( D test )) /N test , where N test is the number of transitions in testing set.

Following [18], our baselines include uniform, unigram and bigram (with Witten-Bell smoothing) models. We al-together list the baselines on three datasets in Table 1. A detailed comparison against these baseline is not of great in-terest in this paper, since the test log-likelihood of all mod-els is substantially above these baselines. The baselines were largely added to provide a meaningful scale for performance di  X  erences.

The experiments were run on a cluster of computers, with each single one having a dual-core Intel Xeon CPU 3.60GHz and 8Gb RAM.
To get an idea of how the songs and portals distribute in the multiple spaces, we first provide the following qualitative http://api.yes.com Figure 5: Test log-likelihood (left) and run time (right) for various settings of c and d on yes small . Figure 6: Test log-likelihood (left) and run time (right) for various settings of c and d on yes big . Figure 7: Test log-likelihood (left) and run time (right) for various settings of c and d on yes complete . analysis. We took the yes big dataset, set dimensionality of embedding d = 2 and number of clusters c = 9, and then plotted all the embeddings in their own 2D plane. The plots can be seen in Fig. 4.
 There are several interesting things worth pointing out. First, di  X  erent spaces have di  X  erent scales, as can be seen from the di  X  erent granularities of x and y axes. This is the result of independent training, and it shows that by intro-ducing portals, we add links between clusters without en-forcing any constraints to coordinate them. Second, some clusters (e.g. Cluster 1 and 7) exhibit inner structure with subclusters, which suggest that it is possible to further par-tition into more clusters without hurting model fidelity. Fi-nally, it can be observed that most of the portals are dis-tributed in the peripheral areas of the mass of songs. This makes sense, as the portals represent songs that are outside the current cluster.
As the first question in our quantitative analysis, we want to explore whether the decoupled training in multiple space Figure 8: Test log-likelihood against the ratio of internal songs in preclustering phase. Tested on yes big ,with d =5 . substantially degrades model fidelity. We trained the Multi-LME with various choice of d and c ( c = 1 means the original LME with no partitioning), on both yes small and yes big . We used the LME-based preclustering method, with 8% of songs chosen as internal songs. The test log likelihoods are reported in Figs. 5 and 6 left. As can be seen, although the curves tend to go down as we increase the number of clusters, they are still high above the three baselines even for the worst case. Note that for yes small the use of c =200 clusters is excessive, and we even have more portals than real songs in some clusters. The small loss in model fidelity is well acceptable.
 How does the runtime scale with the number of clusters? To avoid any outside influence on the runtime, each experi-ment was run sequentially on a single machine and process. The time reported here are the time spent on preclustering phase plus the average time for the local embeddings accross all clusters. This gives us an idea of how fast training can be done given enough machines so that all individual embed-dings can be run at the same time. Figs. 5 and 6 right, show that a substantial speedup until a sweet spot at around 100 clusters is reached. After that runtime increases again, as preclustering and the number of added portals slows down training. The bumps are largely due to some runs taking a larger number of LME iterations than other due to nu-merical issues with the stopping criterion. As expected, the speedup is bigger the larger the dataset and the larger the dimensionality.

The Multi-LME can also handle the yes complete dataset with 75K songs, which is largely intractable using conven-tional LME training. Here, we fix the ratio of internal songs for preclustering to be 0 . 03. Fig. 7 shows the results. Note that we are missing results for c =1and d&gt; 2  X  even for d = 2, training original LME with a single process already took us more than two weeks. For the test log-likelihood on the left, we can see that Multi-LME is even slightly bet-ter than the brute-force training. We conjecture that the added modeling flexibility of not having to fit all points into a single metric space can improve model fit. In terms of runtime, the Multi-LME improves over the LME from more than two weeks to just a few hours. There is a standalone single red point, which stands for the naive parallelization in the distributed memory setting on 50 cores for d =2. The Multi-LME is substantially faster when using the same number of processors. Figure 9: Test log-likelihood on yes big ,with d =5 and the preclustering method varied. The ratio of internal songs is set to 0 . 03 for LME-based method.
We explore how many songs are needed in the precluster-ing stage. For a range of di  X  erent numbers of clusters we vary the ratio of internal songs in the preclustering phase. The resulting curves are shown in Fig. 8. As expected, using more internal songs produces Multi-LME embeddings with better test log-likelihood. The models with bigger c tends to need to more internal songs. The curves gradually flatten once the ratio is above certain threshold, which should be considered the best ratio.

In practice, the best ratio needs to be tuned for di  X  erent datasets. As in our experiments, 0 . 08 tends to work well for yes small and yes big , but for yes complete 0 . 03 is enough. This may be due to the fact that a small set of popular songs are responsible for most of the plays in the playlists dataset.
As mentioned in Section 4.3, the preclustering phase can be replaced by general graph partitioning methods. Here we investigate how di  X  erent methods a  X  ect the final test log-likelihood. For di  X  erent methods, we vary the number of clusters to draw the curves in Fig. 9. Spectral clustering tends to perform slightly better than the LME precluster-ing. METIS is the worst, but is still high above any of the three baselines. The di  X  erences between three methods fairly small.

The comparison does not tell much in some sense, since for all of the three methods, there are quite a few parameters that need tuning. The LME preclustering takes the ratio of internal songs; spectral clustering needs to choose the type of Laplacian and number of rounds kmeans needs to run; METIS has its balancing factor and type of algorithms to use. For the experiments in Fig. 9, these parameters were left at their default settings.

It is also di cult to quantitatively compare run time, as the three method are implemented quite di  X  erently. LME preclustering is implemented in C without use of any non-standard libraries; Spectral clustering is written in MAT-LAB, making use of the e cient eigs function to solve the eigenvalue decomposition problem; METIS is written in highly optimized C code. Also, run time varies when dif-ferent parameters are applied. Qualitatively, we observed that METIS is almost always the fastest. Depending on the size of the dataset, the advantage our method and spectral clustering may shift.
This paper proposes a probabilistic embedding model that exploits locality in the data to reduce training complexity and to permit parallelization. The key idea is to model highly connected regions in detail, but connect remote re-gion only through a narrow interface that largely decouples the training problems. The formulation as a single proba-bilistic model and its associated maximum likelihood train-ing objective guides not only how each local model should be fit and connected to other local models, but also how the training problem should be split across multiple computer nodes. Empirical results show orders of magnitude reduced runtime with at worst slightly reduced, but sometimes even improved model fidelity. We thank Johannes Gehrke, Wenlei Xie and Guozhang Wang for their valuable suggestions and advice. This work was supported in part by NSF Awards IIS-1217686 and IIS-0905467. [1] Y. Bengio, H. Schwenk, J.-S. Sen  X ecal, F. Morin, and [2] O. Dekel, R. Gilad-Bachrach, O. Shamir, and L. Xiao. [3] E. Gabriel, G. E. Fagg, G. Bosilca, T. Angskun, J. J. [4] A. Globerson, G. Chechik, F. Pereira, et al. Euclidean [5] G. Hinton and S. Roweis. Stochastic neighbor [6] E. H. Huang, R. Socher, C. D. Manning, and A. Y. [7] G. Karypis and V. Kumar. A fast and high quality [8] M. Khoshneshin and W. N. Street. Collaborative [9] J. Kleinberg and E. Tardos. Algorithm design . Pearson [10] Y. Maron, M. Lamar, and E. Bienenstock. Sphere [11] A. Mnih and G. Hinton. Three new graphical models [12] J. Moore, Shuo Chen, T. Joachims, and D. Turnbull. [13] A. Ng, M. Jordan, Y. Weiss, et al. On spectral [14] F. Niu, B. Recht, C. R  X e, and S. J. Wright. Hogwild!: A [15] S. Rendle, C. Freudenthaler, Z. Gantner, and [16] S. Rendle, C. Freudenthaler, and L. Schmidt-Thieme. [17] S. T. Roweis and L. K. Saul. Nonlinear dimensionality [18] Shuo Chen, J. Moore, D. Turnbull, and T. Joachims. [19] J. B. Tenenbaum, V. De Silva, and J. C. Langford. A [20] K. Q. Weinberger, B. D. Packer, and L. K. Saul. [21] D. Zhou, S. Zhu, K. Yu, X. Song, B. L. Tseng, H. Zha, [22] M. Zinkevich, M. Weimer, A. Smola, and L. Li.
