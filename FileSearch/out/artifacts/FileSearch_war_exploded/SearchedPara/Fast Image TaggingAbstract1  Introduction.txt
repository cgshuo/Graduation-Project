 Minmin Chen minmchen@amazon.com Amazon.com, Seattle, WA 98109 Alice Zheng alicez@microsoft.com Microsoft Research, Redmond, WA 98052 Kilian Q. Weinberger kilian@wustl.edu Washington University in St. Louis, St. Louis, MO 63130 Image tag annotations are an important component of searchable image databases such as Flickr TM Picassa TM or Facebook TM . However, a large fraction (over 50% in Flickr) of images have no tags at all and are hence never retrieved for text queries. Automatic image annotation is an essential tool towards surfac-ing this  X  X ark content X . A working image annotation engine can suggest tags to users (Weinberger et al., 2008) and thus increase the number of tagged images, or generate relevant tags for image retrieval directly. Automatic image annotation is a difficult machine learning task. Different type of objects require dif-ferent image descriptors, e.g. rainbows can be iden-tified through color histograms (Hafner et al., 1995), whereas insects can be best identified through local image descriptors (Lowe, 1999). Similar objects can look very different across images and may only be partially visible, thus necessitating large training data sets. Training labels are typically obtained through crowdsourcing and are noisy and notoriously incom-plete. The ESP game (Von Ahn &amp; Dabbish, 2004) proposes a solution to improve label quality by incen-tivizing pairs of labelers to match their answers. This results in tag sets with high precision but with no guar-antees for high recall: each image may be tagged with only a small subset of tags that describe the most ob-vious visual features.
 Recently, Makadia et al. (2008); Guillaumin et al. (2009) proposed new algorithms for automatic image annotation based on nearest neighbor methods. Guil-laumin et al. (2009) carefully learn embeddings into metric spaces that combine a diverse set of image de-scriptors and assign tag-specific weights to overcome label sparsity. The resulting algorithm significantly improves over the prior state-of-the-art in both preci-sion and recall. Although these approaches yield im-pressive results, they are impractical for large image databases with n 0 images. Their training proce-dures scale on the order of O ( n 2 ). Moreover, the task of tagging a single test image is O ( n ), linear with the training set size.
 In real world applications, the number of images can be very large. Millions of images are added every day ( e.g. 300 million images are uploaded to Facebook per day, with a total of 100 billion images 1 ), rendering these methods impractical even to index the daily uploads. In this paper, we present a novel learning algorithm for image tag annotation that achieves comparable ac-curacy to Guillaumin et al. (2009), but can be trained in O ( n ) time and applied during testing in constant time w.r.t. the training set size. Our proposed algo-rithm, FastTag , can naturally incorporate many image descriptors and address the difficulties of label sparsity with a novel approach. It interprets its training data (images with partial tags) as unlabeled multi-view data and learns two classifiers to predict tag annotations: one attempts to reconstruct the (unknown) complete tag set from the few tags available during training; the other learns a mapping from image features to this reconstructed tag set. We propose a joint con-vex loss function that combines both classifiers via co-regularization and coerces them into agreement. Our loss function can be trained efficiently through alter-nating optimization with simple closed-form updates. We demonstrate on real world data sets that Fast-Tag matches the highly competitive state-of-the-art in terms of precision and recall, but is several orders of magnitude faster during training and almost instanta-neous during testing. In this section, we review some of the popular meth-ods for automatic image annotation. The first group of methods are based on parametric topic models. Monay &amp; Gatica-Perez (2004) extend the probabilistic latent semantic analysis model, and Barnard et al. (2003) extend the latent dirichlet allocation model to multi-modal data. Each annotated image is modeled as a mixture of topics over visual and text features. The mixture proportions are shared between feature modes, but the topic distributions are distinct. The second group of methods (Jeon et al., 2003; Lavrenko et al., 2003; Feng et al., 2004) models the joint distri-bution of the image features and the tags with mixture models. The third group of methods trains discrimi-native models, such as SVM (Cusano et al., 2003), ranking SVM (Grangier &amp; Bengio, 2008) and boost-ing (Hertz et al., 2004), to predict tags from image features.
 While these methods achieve promising annotation re-sults, their complex training processes limit the num-ber of descriptors that can be incorporated. Recently proposed models such as the Joint Equal Contribu-tion model of (Makadia et al., 2008) and the TagProp model of (Guillaumin et al., 2009) rely on local near-est neighborhoods and work surprisingly well despite their simplicity. TagProp is the current state-of-the-art method for image annotation. Its success can be attributed to three elements: 1. it incorporates a large number of different visual descriptors; 2. it can be trained effectively on images with incomplete tag sets; 3. it treats rare tags special.
 Although Tagprop achieves superior performance on several benchmark datasets, the O ( n 2 ) training and O ( n ) test complexity hinder its applicability to large scale datasets (where n is the number of examples in the training set). In this work, we introduce a new model that incorporates these three elements for suc-cessful annotation much more cheaply.
 Most existing models assume that a complete list of relevant tags for each image is available at training time. However, in practice, this is either impractical or impossible for a large training set. It is much eas-ier to tag an image with a few of the most prominent visual features than to obtain the complete list from a tag dictionary. To alleviate the need for complete la-beling, several existing approaches (Fergus et al., 2009; Schroff et al., 2007; Socher &amp; Fei-Fei, 2010) resort to semi-supervised approaches to leverage unlabeled or weakly labeled data from the web. We adopt the same assumption of sparse training tags and incorpo-rate partial supervision in our work. We assume, as it is the case in real world applications, that only a few relevant tags are provided for each image during training. Given the training images an-notated with incomplete tags, our goal is to learn a model that can infer the full list of tags from image features at test time. Our proposed algorithm is fast in training and almost instant prediction during test-ing (only a linear transformation is required). Thus we refer to our algorithm as FastTag .
 Notation. Let T = {  X  1 ,  X  X  X  , X  T } denote the dictio-nary of T possible annotation tags. Let the training data be denoted by D = { ( x 1 , y 1 ) ,  X  X  X  , ( x n , y R d  X { 0 , 1 } T , where each vector x i  X  X  d represents the features extracted from the i-th image (for details see section 3.3 and section 4) and each y i is a small partial subset of tags that are appropriate for the i-th image. Our goal is to learn a linear function W : R d  X  X  , which maps a test image x i to its complete tag set. 3.1. Duo Classifier Formulation In this section we introduce a new model for automatic image annotation from incomplete user tags. It jointly learns two classifiers on two sources, i.e. , image and text, to agree upon the list of tags predicted for each image. It leads to an optimization problem which is jointly convex and has closed form solutions in each iteration of the optimization.
 Co-regularized learning. As we are only provided with an incomplete set of tags, we create an additional auxiliary problem and obtain two sub-tasks: 1) train-ing an image classifier x i  X  Wx i that predicts the complete tag set from image features, and 2) training a mapping y i  X  By i to enrich the existing sparse tag vector y i by estimating which tags are likely to co-occur with those already in y i . We train both clas-sifiers simultaneously and force their output to agree by minimizing Here, By i is the enriched tag set for the i-th training image, and each row of W contains the weights of a linear classifier that tries to predict the corresponding (enriched) tag based on image features.
 The loss function as currently written has a trivial so-lution at B = 0 = W , suggesting that the current for-mulation is underconstrained. We next describe ad-ditional regularizations on B that guides the solution toward something more useful.
 Marginalized blank-out regularization. We take inspiration from the idea of marginalized stacked de-noising autoencoders (Chen et al., 2012) and related works ( ? ) in formulating the tag enrichment mapping B : { 0 , 1 } T  X  X  T . Our intention is to enrich the incom-plete user tags by turning on relevant keywords that should have been tagged but were not. Imagine that the observed tags y are randomly sampled from the complete set of tags: it is a  X  X orrupted X  version of the original set. We leverage this insight and train the en-richment mapping B to reverse the corruption process. To this end, we construct a further corrupted version of the observed tags  X  y and train B to reconstruct y from  X  y . If this secondary corruption mechanism matches the original corruption mechanism, then re-applying B to y would recover the likely original pristine tag set.
 For simplicity, we use uniform corruption as the sec-ondary corruption mechanism. In practice, human la-belers may select tags with bias, not uniform proba-bility. We can approximate the unknown corrupting distribution with piecewise uniform corruption in the learning step (see section 3.2). If prior knowledge on the original corruption mechanism is available, it can also easily be incorporated into our model.
 More formally, for each y , a corrupted version  X  y is created by randomly removing ( i.e. , setting to zero) each entry in y with some probability p  X  0 and there-fore, for each user tag vector y and dimensions t , p (  X  y t = 0) = p and p (  X  y t = y t ) = 1  X  p . We train B to optimize Here, each row of B is an ordinary least squares re-gressor that predicts the presence of a tag given all existing tags in  X  y . To reduce variance in B , we take repeated samples of  X  y . In the limit (with infinitely many corrupted versions of y ), the expected recon-struction error under the corrupting distribution can be expressed as Let us denote as Y  X  [ y 1 ,  X  X  X  , y n ] the matrix contain-ing the partial labels for each image in each column. Define P  X  then we can rewrite the loss in (2) as We use Eq. (3) to regularize B . For the uniform  X  X lank-out X  noise introduced above, we have the ex-pected value of the corruptions E [  X  y ] p (  X  y | y ) Here  X  (  X  ) stands for an operation that sets all the en-tries except the diagonal to zero (as we corrupt each tag independently, the variance matrix has non-zeros entries only on the diagonal). We can then compute the two matrices in Eq. (3) as Joint loss function. Combining the squared loss in Eq. (1) with the marginalized blank-out regularization term r ( B ) in Eq. (3) and the standard ridge regression l 2 regularizer for W , the joint loss function can be written as ` ( B , W ; x , y ) = The first term enforces that the tags enriched through co-occurrence with existing labels agree with the tags predicted by the content of the image. A regularizer on W is included to reduce complexity and avoid over-fitting. The last term ensures that the enrichment mapping B reliably predicts tags if they were to be removed from the training label set.
 Test time. At test time, given an image x , the final mapping W  X  is used to score the dictionary of tags. 3.2. Optimization and Extensions The loss in Eq. (5) can be efficiently optimized using block-coordinate descent. When B is fixed, the map-ping W reduces to standard ridge-regression and can be solved for in closed form: where X and Y respectively contain the training image features and labels in columns.
 Similarly, when W is fixed, the solution to Eq. (5) can be expressed as the well-known closed-form solution for ordinary least squares (Chen et al., 2012): where P and Q can be computed analytically follow-ing eq. (4). In other words, we can derive the optimal mapping B under closed form without explicitly creat-ing any corruptions. The conclusion holds for any cor-rupting models of which the expected value and vari-ance can be computed analytically. The loss is jointly convex with respect to B and W and consequently coordinate descent converges to the global minimum. Fig. 1 contains a depiction of this algorithm. Tag bootstrapping. The enrichment mapping B is trained to predict missing tags based on pairwise co-occurrence patterns. We would like to also reconstruct tags that do not co-occur together but tend to ap-pear within similar contexts. As an example, the tag  X  X ond X  might rarely co-occur with  X  X ake X , as both de-scribe similar things and annotators tend to use one or the other. However, it would be good to give the predictor W the flexibility to predict both from sim-ilar image features. We can achieve this via stacking: starting with the enriched vector By i as the tag repre-sentation for the i-th image 2 , we optimize another layer of ` ( B 0 , W 0 ; x , By ) to obtain new mappings B 0 We can have an arbitrary number of layers, each re-sulting in a new linear mapping W t from image fea-tures to tags. To find the right trade-off between two much bootstrapping and too little, we perform model selection on a hold-out set, adding layers until it no longer improves the F1 measure.
 Rare tags and Non-Uniform Corruption. Eq. (5) solves for the linear predictors W for all T tags simul-taneously. This is computationally efficient in that it requires only one matrix inversion per iteration. How-ever, it has the disadvantage that the prediction loss for each tag is weighed equally, which leads to the over-all loss to be dominated by contributions from more frequent tags, sacrificing the prediction accuracy of rare tags. This is a known problem in tag prediction. Other approaches also find that dealing with rare tags is the key to improving tagging performance (Guillau-min et al., 2009). We introduce several re-optimization stages, where at each stage we solve a sub-problem of Eq. (5). That is, we identify a subset of tags with a recall below a certain threshold (in our experiments we set it to the average recall). We re-optimize (5) restricted to only the rows of B and W corresponding to such tags. We iterate until we no longer improve the F1 measure on a hold-out set.
 This stage-wise re-optimization also allows us to ap-proximate the unknown true corrupting distribution with piecewise estimates: each stage of re-optimization may set a different corruption probability p based on validation results on a hold-out set, keeping the cor-ruption probability of remaining tags fixed at their pre-vious values.
 In addition, we weigh each example in a tf-idf-like fash-ion so that losses from rare tags are given more weight during training. Specifically, each tag  X  is assigned a cost c  X  = 1 n appears in the training set. Thus, rarer tags are given a higher cost than the more frequent ones. We then assign each example a weight by simply summing over the costs of its active tags, so that examples with rarer tags contribute more to the loss in eq. (5). Let  X  de-note an n  X  n diagonal matrix containing the weight for each training example, we can then solve for the optimal mapping as W = BY  X  X T ( X  X  X T + n X I )  X  1 . The tag enrichment mapping B can be generalized to the weighted version in the same fashion. 3.3. Homogeneous feature mapping Local kNN methods (Guillaumin et al., 2009; Makadia et al., 2008) enjoy the advantage of naturally identi-fying non-linear decision boundaries based on multi-ple feature spaces from different image features. In our work, we adopt linear image feature classifiers for their simplicity and speed, and instead incorpo-rate non-linearity into the feature space as a pre-processing step. To this end, we adopt the homoge-neous feature mapping method of Vedaldi and Zis-serman (Vedaldi &amp; Zisserman, 2012). For each vi-put image, it uses an explicit feature mapping  X  m R d m  X  R d m (2 r +1) to project it to a slightly higher-dimensional feature space, in which the inner product approximates the kernel distance well. In other words,  X   X  m ( f m ( x ) ,  X  m ( f m ( x 0 ))  X   X  K m ( f m ( x ) , f the family of additive kernels, such as the l 1 -distance and  X  2 -distance used in our experiments, the mapping  X (  X  ) can be computed analytically and approximates the kernel well even with small r (in our experiment, we set r = 1). After projecting each visual descrip-tor independently, we further apply random projec-tion (Vempala, 2005) to reduce the dimensionality 3 . We evaluate FastTag 4 on three standard image anno-tation benchmark datasets. All data sets (with pre-extracted features) were obtained from http://lear. inrialpes.fr/people/guillaumin/data.php . 4.1. Experimental Setup We begin with a detailed description of the data sets, the visual descriptors and the evaluation metrics. Corel5K. The dataset (Duygulu et al., 2006) contains 5,000 images collected from the larger Corel CD set. Each image is manually annotated with keywords from a dictionary of 260 distinct terms. On average, each image was annotated with 3.5 tags.
 ESP game. The dataset consists of 20,770 imagesof a wide variety, such as logos, drawings, and personal photos, collected for the ESP collaborative image la-beling task (Von Ahn &amp; Dabbish, 2004). The images are annotated with a total of 268 tags. Each image is associated with a maximum of 15 and 4.6 tags on average.
 IAPRTC-12. 5 . The dataset consists of 19,627 images of sports, actions, people, animals, cities, landscapes and many other aspects of contemporary life (Grub-inger et al., 2006). Tags are extracted from the free-flowing text captions accompanying each image. Over-all, 291 tags are used.
 For all these datasets, we follow the training/test split used in previous work (Guillaumin et al., 2009; Maka-dia et al., 2008). Please refer to Guillaumin et al. (2009) for more detailed statistics on the datasets. Feature extraction. We use the 15 different visual descriptors, extracted by Guillaumin et al. (2009) for each dataset. These include one Gist descriptor (Oliva &amp; Torralba, 2001), six global color histograms, and eight local bag-of-visual-words features. As described in section 3.3, we adopt the explicit feature mapping of Vedaldi &amp; Zisserman (2012) to obtain a non-linear feature transformation. Here we use the l 1 approxi-mation ( i.e. the Euclidean distance after the mapping approximates the l 1 distance) for the global color de-scriptors, and the approximated  X  2 distance for the local bag-of-visual-words features. Finally, we apply random projection after each feature mapping to re-duce the dimensionality.
 Evaluation metric. For full comparability, we adopt the same evaluation metrics as in Guillaumin et al. (2009). First, all image are annotated with the five most relevant tags ( i.e. tags that have the high-est prediction value). Second, precision (P) and recall (R) are computed for each tag. The reported measure-ments are averaged across all tags. For easier com-parability, both factors are combined in the F1-score ( F 1 = 2 P  X  R P + R ), which is reported separately. We also report the number of keywords with non-zero recall value (N+). In all metrics a higher value indicates better performance.
 Baselines. We compare against leastSquares , a ridge regression model which uses the partial subset of tags y ,..., y n as labels to learn W , i.e., FastTag without tag enrichment. We also compare against the Tag-Prop algorithm (Guillaumin et al., 2009), a local kNN method combining different distance metrics through metric learning. It is the current best performer on these benchmark sets. Most existing work do not pro-vide publicly available implementations. As a result, we include their previously reported results for ref-erence (Lavrenko et al., 2003; Metzler &amp; Manmatha, 2004; Yavlinsky et al., 2005; Carneiro et al., 2007; Feng et al., 2004; Liu et al., 2009; Makadia et al., 2008) . 4.2. Comparison with related work Table 1 shows a detailed comparison of FastTag to the leastSquares baseline and eight published results on the Corel5K dataset. We can make three obser-vations: 1. The performance of FastTag aligns with that of TagProp (so far the best algorithm in terms of accuracy on this dataset), and significantly outper-forms the other methods; 2. The leastSquares base-line, which corresponds to FastTag without the tag enricher, performs surprisingly well compared to exist-ing approaches, which suggests the advantage of a sim-ple model that can extend to a large number of visual descriptor, as opposed to a complex model that can af-ford fewer descriptors. One may instead more cheaply glean the benefits of a complex model via non-linear transformation of the features. 3. The duo classifier formulation of FastTag, which adds the tag enricher, alleviates the intrinsic label sparsity problem of image annotation. It leads to a 10% improvement on preci-sion, 28% on recall, and an overall 20% improvement on F1 score over the leastSquares baseline. We also increase the number of tags with positive recall by 34. Table 2 compares the performance of FastTag over leastSquares and three existing methods on the ESP game and IAPRTC-12 datasets. Similar trends are observed. First, FastTag significantly outperforms the baseline, MBRM (a generative mixture model) of Feng et al. (2004), and JEC (a local NN method) of Maka-dia et al. (2008) on both datasets. FastTag performs slightly worse than TagProp. However, as we demon-strate next, FastTag achieves enormous speedup over TagProp in both training and testing.
 Computational time. All experiments were con-ducted on a desktop with dual 6-core Intel i7 cpus with 2.66Ghz.
 Figure 3 shows the F1 score vs. the training time re-quired for different methods on these three datasets. The time is plotted in log scale. We can make three ob-servations: 1. TagProp outperforms all other related work in terms of F1 measure, but is also the slowest to train. It takes close to one hour to train on the rela-tively small Corel5K dataset, which has around 4,500 training examples. For the larger datasets (ESPgame and IAPRTC-12) with close to 17,000 examples, the training time blows up to 16 hours. 2. The JEC method of (Makadia et al., 2008) falls into the same category of local NN method as TagProp, with the difference that it uses the simple average of the 15 distance metrics to define neighbors. JEC does not require training. However, we can see that it cannot compete in terms of accuracy performance. Note that, it still has O ( n ) test-time complexity, where n is the number of training examples, because each query ex-ample requires a neighbor-lookup during testing. 3. The training time of FastTag is over 50x faster than that of TagProp. Note the time reported in the fig-ure for FastTag also includes the feature preprocess-ing time, i.e. , performing homogeneous feature map-ping and random projection, which takes up the ma-jority of the computation time. For a total of 16,748 training examples (dimensionality d = 15 , 000) and 268 tags, FastTag takes on average 34 seconds to train for one bootstrap iteration. The optimal number of bootstrap iterations ranges from 1 to 8 in different re-optimization iterations (The number of iterations is usually very small at the beginning, but gradually in-creases in the later re-optimization stages as it needs bootstrapping to recover rare tags.). The algorithm converges within a few re-optimization stages. 4.3. Sample annotations Figure 2 shows example images from the ESP game data set and their tag annotations obtained with Fast-Tag. The figure shows three rows of results. The top row consists of images with high F 1 score, i.e. these are images on which FastTag reliably retrieves relevant tags. The middle row shows images that are picked uniformly at random. Although not perfect, the vast majority of tags are relevant to the particular image. The bottom images have low F 1 score, and represent examples where FastTag fails to retrieve relevant tags. 4.4. Further analysis While these benchmark data sets are appropriate for algorithm comparisons, they may not be representa-tive of the quality of training image tags found in the wild. In practice, most of the images are annotated with far fewer tags. We run the algorithms on images with down-sampled sparse tags in order to gauge their performance in this more realistic setting. Figure 4 de-picts the comparison of FastTag and TagProp at differ-ent levels of training set tag sparsity. We  X  X tage X  the training data into successively larger tag sets, starting by giving each image only one tag (down sampled from the full set if more tags are available), then up to two tags, and so on. We can see that FastTag out-performs TagProp when the maximum number of provided tags is small. In general, FastTag performs comparably to Tagprop across different tag sparsity levels. In other words, the tag enrichment mapping of FastTag indeed helps to alleviate the intrinsic tag sparsity problem. We present an image tagging method, FastTag, that performs on-par with current state-of-the-art algo-rithms, at a fraction of the computation cost. We re-cast a supervised multi-label classification problem as unlabeled multi-view learning. We define two classi-fiers, one for each view of the data, and coerce them into agreement via co-regularization in a joint loss function. We trade off complexity in the classifiers with non-linear mapping of the features and demon-strate that such a choice pays off. FastTag is computa-tionally efficient during training and testing yet main-tains tagging accuracy. It can effectively deal with sparsely tagged training data and rare tags that are often obstacles in such large-scale learning problems. We thank David Grangier and Larry Zitnick of Mi-crosoft research for helpful discussions. KQW was sup-ported by NSF grants 1149882 and 1137211. Part of this work was done while MC was an intern at Mi-crosoft Research, Redmond.

