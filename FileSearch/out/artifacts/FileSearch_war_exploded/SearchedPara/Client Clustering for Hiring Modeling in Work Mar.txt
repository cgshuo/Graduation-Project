 An important problem that online work marketplaces face is grouping clients into clusters, so that in each cluster clients are similar with respect to their hiring criteria. Such a sepa-ration allows the marketplace to  X  X earn X  more accurately the hiring criteria in each cluster and recommend the right con-tractor to each client, for a successful collaboration. We pro-pose a Maximum Likelihood definition of the  X  X ptimal X  client clustering along with an efficient Expectation-Maximization clustering algorithm that can be applied in large market-places. Our results on the job hirings at oDesk over a seven-month period show that our client-clustering approach yields significant gains compared to  X  X earning X  the same hiring cri-teria for all clients. In addition, we analyze the clustering results to find interesting differences between the hiring cri-teria in the different groups of clients.
Online work marketplaces such as oDesk.com, Elance.com and Freelancer.com help  X  X lients X  and  X  X ontractors X  across the globe to connect with each other and work for more than $1 billion in annual contractor earnings just in 2014. Typically, in such marketplaces, contractors apply to a job posted by a client and clients hire the applicant(s) that seems to be the best fit for the job posted. As these platforms grow, a fundamental problem they have to solve is the under-standing of successful client hiring practices so that they can help clients make the right hiring decisions. Without such help, clients will have to deal with the friction of screening tens to hundreds of contractors to determine the ideal candi-dates for their jobs. The screening process is not only time-consuming, but it is also error-prone, since clients often lack the necessary knowledge to assess the qualifications of con-tractors (e.g., education and work experience from schools and companies that are unknown to a client).

Understanding and modeling the hiring behavior of clients is challenging not only due to the variety of jobs that are Work done while authors were at Elance-oDesk c  X  Figure 1: Left: The hiring decisions of all of the mar-ketplace clients in a two-dimensional feature space. Middle: The hiring decisions of quality optimizers. Right: The hiring decisions of cost optimizers. posted and the diversity of contractors, but also due to the heterogeneity of client hiring criteria. For example, two dif-ferent clients that have posted two seemingly similar jobs looking for  X  X hp developers X  may be looking for totally dif-ferent people. Say that the first client is a quality optimizer that is willing to pay a high hourly rate to get the most qualified contractor while the second client is a cost opti-mizer who is willing to take the risk of working with an inexperienced contractor to reduce his costs. To make rec-ommendations that satisfy both of these clients we would ideally develop a dedicated model for each client. However, in practice, developing a model for each client is not an op-tion, since the marketplace rarely has sufficient data points for a single client to make training possible.

Although all clients are not the same, there are usually sufficiently large groups of clients with similar hiring crite-ria that can provide us with data for model training. To illustrate our hypothesis, we show in Figure 1 the hiring de-cisions in a marketplace that is composed of quality and cost optimizers. In the first plot of the figure we illustrate the hiring decisions of all clients in a two-dimensional feature space. Each  X + X  point looks at an application that ended up in a hire while a  X   X   X  point looks at an application that got rejected. The point positions on the X-axis indicate the bid prices asked by the contractors and the positions on the Y-axis indicate the years of contractor experience. A linear model trained on all of the client decisions would  X  X earn X  a linear separator w 0 to distinguish hired from rejected appli-cations. Such a separator would misclassify many rejected applications as hires ( X - X  points to the left of w 0 ) and many hires as rejected applications ( X + X  points to the right of w However, if we could split the clients into quality (middle plot) and cost optimizers (right plot) we could then learn a different model for each client group. The derived separa-tors w 1 and w 2 would then almost perfectly separate the hires in each of the two groups. Figure 2: The Hiring Criteria Clustering component in the overall oDesk recommendations workflow.

Clustering clients based on their hiring criteria is a crit-ical component in our oDesk recommendations workflow. Figure 2 depicts the overall workflow and shows how the clustering component fits in the workflow. The clustering component generates groups of clients along with one hiring model per group. Our first objective is to predict more ac-curately who is the right contractor for a client X  X  task, based on the hiring practices of that client. Nevertheless, predict-ing accurately the contractor that a client will hire is not our only objective. We observed that very often unsuccess-ful collaborations take place because of clients relying on the  X  X rong X  hiring practices. Therefore, we have to monitor the collaborations in each group of clients and  X  X ntervene X  when we detect that a group X  X  hiring practices often lead to unsuccessful collaborations. We  X  X ntervene X  by adjusting the group X  X  hiring model based on the  X  X roblematic X  crite-ria that we detected. For example, we may adjust a hiring model when we detect that clients are biased against work-ing with contractors from specific countries, while we  X  X now X  that those countries provide a large pool of experts for the tasks posted by those clients.

The monitoring and re-adjustment components typically require both algorithmic models and manual effort. Hence, it is important to use human-interpretable hiring models in our workflow (see also paper [18]). Furthermore, our work-flow also involves a component that assigns new clients with-out any hiring history to the right group/model. (Note that a positive experience for new clients is particularly impor-tant for the marketplace.)
In this paper, we choose to focus on the hiring-criteria clustering component, which forms the basis for the overall workflow and is the most interesting component both from an applied and a research perspective. Below we discuss two classic approaches for client clustering and point out why they are not suitable for our context.
 Traditional Clustering Methods: One straightforward approach for client clustering is to apply an algorithm like k -means using client attributes like age or occupation. How-ever, as we observed in the oDesk platform, clients that have the exact same characteristics (age, occupation, etc.), often have very different criteria on hiring contractors. Clearly, a clustering method suitable for our problem must be based on the clients X  decisions and not just attributes like age and occupation. Of course, we can also include client attributes related to their past decisions. However, we decided not to rely on a  X  X ustom X  approach, where we would have to come up with past decisions X  attributes and a distance metric to represent the distance between clients. (Instead, we rely on a bayesian approach, as we discuss later in the introduction.) Collaborative Filtering (CF) Methods: CF techniques, like the ones presented in papers [2, 5, 7, 10, 11, 19, 24, 31], fit well in domains like electronic commerce or stream-ing media recommendations, but have an inherent limitation for online job marketplaces: each contractor cannot be con-sidered as a  X  X ixed X  item as with movies or products. For instance, a contractor may be much more appropriate for a task involving Fortran debugging than Python debugging if she is a Fortran expert but a Python novice. That is, clients do not really  X  X ote X  for a contractor when they hire her but  X  X ote X  for her application on a specific task.

Since the quality of the client clustering drastically af-fects the percentage of successful collaborations, we had to develop a new approach tailored to our context. The ap-proach we developed is driven by our findings in papers [4] and [18]. In summary, in those two papers we studied the problem of learning how to rank the applicants on a specific task posted by a client. Our main finding was that a simple sparse logistic regression model, using the accepted applica-tions as positive examples and the rejected ones as negatives examples, proves to be very effective for applicant ranking. Hence, the approach we propose in this paper for the hiring-criteria clustering problem, builds on top of a logistic regres-sion model: we develop a Finite Mixture Logit model and a simple, yet effective and scalable algorithm based on Ex-pectation Maximization with hard assignments (hard EM). In simple words, the algorithm starts with a random assign-ment of clients into clusters and for each cluster it applies sparse logistic regression to learn the hyperplane expressing the client criteria in this cluster. It then re-assigns a client to the hyperplane/cluster that best  X  X xplains X  her choices. These two steps are repeated until convergence, i.e., until no re-assignments are needed. After the last step, the algo-rithm has computed a partition of clients into clusters along with the criteria of clients in each cluster; given by sparse logistic regression.

Although our model bears some resemblance to models proposed in the past (e.g., mixture models proposed for clusterwise regression [8, 27], mixtures of Support Vector Machines [3, 9, 32] or models found in the marketing and economics literature [1, 15, 20, 23, 30]), there are some im-portant differences that do not allow previous models to be applied in the context discussed here: 1. Most previous models (e.g., [3, 6, 8, 9, 27, 32]) apply 2. As previous models were designed to solve different
In our experiments, we use the 865 , 000 accept/reject de-cisions made by oDesk clients in a seven-month period. As our results show, the predictive hiring models trained on the clusters yielded by our algorithm can improve the per-formance of a global model (i.e., a single hiring model for all clients) by 43%. Furthermore, our analysis on the clus-ters produced by our method reveals some very interesting differences on the hiring practices of different client groups. To the best of our knowledge, our study is the first one on the detection and analysis of the differences between hiring practices in online work marketplaces.

In summary, our contributions are the following:
We will use a running example throughout the definition of our model and algorithm in Sections 2 and 3.

For simplicity, let us assume that there are only three fea-tures, experience, score, bidding , affecting a client X  X  decision and that all information for the contractors X  applications are organized in a single table: Apps(clientID, experience, score, bidding, decision)
The semantics of the 5 columns are the following: 1. clientID : The client that opened the task to which the 2. experience : The total number of hours the contrac-3. score : The aggregated rating of the contractor based 4. bidding : The amount asked by the contractor for per-5. decision : The decision of the client ( X  X PPROVED X /
In practice, the three features, experience, score, bidding , are normalized so that their value range is [0 . 0 , 1 . 0]. For example, the bidding can be normalized by the maximum amount a contractor may ask for a task; a restriction that could be enforced by the platform X  X  provided functionality.
In our example, the dataset consists of only four clients, each having ten contractor applications approved and ten contractor applications rejected. In addition, we want to form two clusters, i.e., we want to split the four clients into two groups such that the clients in each group have  X  X ery similar X  criteria regarding the experience, score, and bidding of an application. Our model in the next section quantifies the notion of  X  X imilar X  by defining the optimal clustering.
In this section, we formally define the problem of find-ing the optimal client partition based on the clients X  hir-ing criteria. Intuitively, our definition requires that the reject/accept applications in each cluster of clients are as  X  X ell-separated X  as possible. For example, the reject/accept applications ( X   X   X  X / X + X  X ) in the middle and right side of Fig-ure 1 are  X  X ell-separated X ; a different partition of clients into two clusters could result in having  X + X  X  diffuse over the  X   X   X  X  area, and the opposite. We use a logit model to quantify how  X  X ell-separated X  the applications in one cluster are. Based on the cost defined by the logit model, the optimal partition of clients is the one minimizing the cost across all clusters.
We start by describing the dataset notation and the cost for a single cluster and then define the clustering optimiza-tion problem (equations (11) to (13)). All the past applications are stored in a single table: Apps(clientID, a1, a2, ... , aF, decision)
The features describing each application are denoted by a1, a2, ... , aF and they are normalized so that their value range is [0 . 0 , 1 . 0]. In our running example, we use only three features: a1  X  experience, a2  X  score, a3  X  bidding .
We denote by x i the row i in table Apps ( Apps[ i ] ), pro-jected over columns a1, a2, ... , aF . Thus, in our running example, each x i is a 3-dimensional vector, e.g., if an ap-plication involves an experience of 0 . 2, a score of 0 . 9, and a bidding of 0 . 8, then x i = (0 . 2 , 0 . 9 , 0 . 8) T .
In addition to x i , we use u i to express the clientID of row i ( Apps[ i ].clientID ), in a 1-of-K scheme. In our running example, where we have K = 4 clients, if the second client approved/rejected application i , then u i = (0 , 1 , 0 , 0)
To simplify notation we split past applications into two subsets {P , N} , such that:
In the running example, |P| = 40 since each of the four clients has approved ten contractor applications, and |N| = 40 since each of the four clients has rejected ten applications.
Moreover, we denote with:
The single-cluster cost is based on the logistic regression model. Here, we give a brief overview of logistic regression in the context of our running example. Note that, in this section, we focus only on the applications {P , N} of the clients that belong to a single cluster.

We denote by w the vector expressing the criteria of clients for approving/rejecting the applications. Note that all the clients of a cluster share the same w . In the running exam-ple, a w = (1 . 0 , 0 . 0 , 0 . 0) T expresses that clients prefer con-tractors with a lot of experience and do not care about the score and the bidding in an application. (In practice, w in-volves an additional coefficient for the general bias. That is, in our running example, an application x i = (0 . 2 , 0 . 9 , 0 . 8) would be extended with a constant term on a fourth di-mension: x i would become (0 . 2 , 0 . 9 , 0 . 8 , 1 . 0) become a 4-dimensional vector.)
In logistic regression, the probability of an application i being approved is given by the logistic function: That is,
Therefore, as the value of the dot product w T x i approaches +  X  , P ( x i approved | w ) approaches 1 . 0, while when w approaches  X  X  X  , P ( x i rejected | w ) approaches 1 . 0.
The objective in logistic regression is finding the criteria w , maximizing the likelihood:
Taking into account regularization, the cost of a single cluster is the negative log-likelihood plus a regularization term involving a hyperparameter  X  and the 1-norm of the criteria vector w : Cost ( w ) :  X  k w k 1  X  X In this section, we generalize the model to many clusters. Thus, our dataset {P , N} refers to the applications from all clients.

We use the matrix M = [ m 1 ,..., m C ]  X  X  0 , 1 } K  X  C to ex-press the clients X  membership, i.e., how the K clients are partitioned into C clusters. Column j , m j , gives the clients that belong to cluster j , while we denote by m 0 k k of M , which gives the cluster where client k belongs. In our running example, suppose that the first cluster contains only the third client while the second cluster contains the other three clients. In that case, and
Therefore, the dot product u T i m j is 1 if the client that ap-proved/rejected application i belongs to cluster j and 0 oth-erwise. For instance, if the second client approved/rejected application i and we have the same clustering as in the example above, u T i m 1 = (0 , 1 , 0 , 0)(0 , 0 , 1 , 0) u i m 2 = (0 , 1 , 0 , 0)(1 , 1 , 0 , 1) T = 1.

The criteria vector for cluster j is given by w j . We use the matrix W = [ w 1 ,..., w C ]  X  IR F  X  C to refer to the union of vectors for all clusters.
 The likelihood of the evidence P ( {P , N}| W , M ) becomes:
Note that the term for a cluster j involves only the appli-cations of clients that belong to that cluster. (For all of these applications the exponent u T i m j is 1, while for all other ap-plications that do not belong to the cluster j the exponent u i m j is 0.) The log-likelihood, ` ( W , M ), becomes: X Algorithm 1 1: M := randomly assign the clients into C clusters 4: W := solve problem of (11)-(13), with M fixed (M step) 5: M := solve problem of (11)-(13), with W fixed (E step) 6: end while
Therefore, the cost of a client partition defined by mem-bership and criteria matrices M and W , is:
Note that the cost involves the sum of the regularization terms for each cluster.

Our objective is to find the membership and criteria ma-trices that solve the following optimization problem:
The constraint (12) expresses that each client must be part of exactly one cluster, i.e., we do not allow overlapping clusters.
The exhaustive approach for solving the optimization prob-lem in equations (11)-(13) involves a O ( C K ) time complex-ity; for C clusters and K clients. Therefore, we propose a scalable algorithm based on Expectation Maximization with hard assignments. In each iteration two steps are involved:
Our algorithm is given by Algorithm 1. The input is the set of all clients X  applications, {P , N} , along with the num-ber of clusters C . Note that in practice there are many ways to compute the number of clusters to use as input. The sim-plest approach is to try several different C values and keep the one maximizing a metric like Mean Average Precision or Discounted Cumulative Gain on a testing set.

In each E step, the value of the objective function in (11) decreases or remains the same; in the worst case there are no changes in the client memberships that would decrease the value of the objective function. Likewise, in each M step the value of the objective function always decreases; or at least remains the same. Hence, the algorithm eventually converges to a minimum; when the client memberships re-main the same for two consecutive iterations. Nevertheless, the minimum may be a local minimum and not a global one, since the problem of (11)-(13) is not convex. In practice, we run Algorithm 1 more than once, using different initial as-signments of clients to clusters, and keep the solution that gives the lowest value for the objective function. In our tech-nical report [29], we examine how the number of runs affects the convergence of our algorithm to the global minimum us-ing synthetic data; so that the true global optimum is known to us in advance.

One of the main advantages of our algorithm is its scala-bility. In the E step, a single pass over the clients is needed. (For each client we find the cluster that  X  X xplains X  better her decisions on her applications, while keeping the crite-ria for each cluster fixed.) In the M step, we just need to solve C sparse logistic regression problems, i.e., one problem per cluster. (Solving sparse logistic regression problems is a process that has been highly optimized for large datasets, e.g., [14], [17].) In Section 4.1.4, we discuss our scalability experiments and results.
In the E step, we keep criteria matrix W fixed and we find for each client the cluster that  X  X xplains X  better her decisions. That is, if U a is the set of applications that were approved/rejected by a client a , we compute for each cluster j the log-likelihood ` ( W ; j,U a ):
Then, we assign client a to the cluster that gives the high-est ` ( W ; j,U a ) (or, equivalently, the lowest negative log-likelihood). In our running example, if the second cluster gives the highest ` ( W ; j,U 3 ) for the third client, the third row of M , i.e., m 0 3 , becomes (0 , 1).

At the end of E step, the assignment changes for each client will be reflected on the membership matrix M .
In the M step, we keep M fixed and we find for each clus-ter j the criteria vector w j that best  X  X xplains X  the clients X  decisions in that cluster. That is, for a cluster c , if U is the set of applications that were approved/rejected by the clients in c , we solve the following sparse logistic regression problem: min
The optimal solutions to the logistic regression problems form the W for the next E step.
Our experiments are organized in three parts: Figure 3: Angle distance to ground truth, as we increase the number of samples(applications) from 8 to 128 applications per client, for 2 and 4 clusters.
In this section, we briefly discuss our experiments with synthetic data. A detailed description of the settings and the synthetic data generation process can be found in our technical report [29].
We first try to quantify how our algorithm is affected by the number of samples available. Figure 3 depicts the be-havior of our algorithm as we increase the number of appli-cations (X-axis) from 8000 to 128000, for a fixed number of K = 1000 clients. The number of dimensions (i.e., the fea-tures that affect a client X  X  decision) is fixed to 8. The Y-axis gives the average angle distance between the vectors (hiring criteria) computed by our algorithm and the ground truth vectors. We run experiments for C = 2 and 4 clusters and we plot one curve for each C value.

For C = 2 we see a huge increase in accuracy (the distance to the ground truth drops from 50 to 12 degrees) when we go from 8 to 16 applications per client. As we keep increasing the number of applications, the accuracy further increases and approaches a zero-degree distance to the ground truth (less than 3 degrees for 128 applications per client).
For C = 4 the steep decrease in the distance to the ground truth, happens from 16 to 32 applications per client. In fact, if we look closely at the two curves we observe that for 4 clusters we need, roughly, twice as many applications as we need for 2 clusters, to reach the same distance to the ground truth. This can be explained by the fact that we have twice as many clusters and, hence, each cluster is assigned half the applications.
In Figure 4(a), we plot the number of applications needed (Y-axis) to reach within a 10-degree distance to the ground truth, as we increase the number of dimensions, from 4 to 24 (X-axis). The number of clusters, C , is 4 and the number of clients, K , is 1000
As we increase the number of dimensions from 4 to 12, the number of applications needed for the 10-degree dis-tance, increases slowly. For more than 12 dimensions, there is a steep increase for the number of applications needed, reaching almost 500 per client for 24 dimensions.

We also tried a second experiment showing the behavior of our approach when the number of dimensions increase, Figure 4: In the non-sparse case, we plot the num-ber of applications needed to get a 10 -degree angle distance to ground truth, as we increase the num-ber of dimensions from 4 to 24 , for 4 clusters. In the sparse case, we increase the number of dimensions from 64 to 256 , for 4 , 8 , and 16 clusters. (a) Distance to ground truth Figure 5: Distance to ground truth and number of iterations needed for convergence, when we increase the number of clients while we keep the number of total samples(applications) fixed. however, this time we used sparse ground-truth criteria vec-tors. That is, out of all dimensions/criteria only a few of them play an important role in the clients X  decisions (for details see our technical report [29]).

As in Figure 4(a), Figure 4(b) depicts the number of ap-plications needed (Y-axis) to reach a 10-degree distance to the ground truth, as we increase the number of dimensions (X-axis), for the sparse case. For 4 clusters, although the number of dimensions is much greater than the number of dimensions in the non-sparse case of Figure 4(a), the num-ber of applications needed for the 10 degree distance, is two orders of magnitude lower; requiring around 20 applications per client for 256 dimensions.

For a larger number of clusters, i.e., C = 8 and C = 16 clusters, we need considerably more applications to reach the 10-degree distance, compared to C = 4 clusters. Still, even for C = 16 and 256 dimensions, we need less than 60 applications per client, which is an order of magnitude lower than the 500 applications per client for C = 4 and 24 dimensions, in the non-sparse case of Figure 4(a).
The experiments we discussed until now had a fixed num-ber of 1000 clients. The next direction we explore is to keep the total number of applications fixed, and vary the number of clients, K , that decide upon those applications. We use 64000 applications for 8 dimensions and C = 4 clusters.
Figure 5(a) depicts the distance to the ground truth, as K increases from 312 to 5000. For K up to 2500, the distance to the ground truth stays below 10 degrees. However, if we increase K further, to 5000 clients, the distance steeply increases from 10 degrees to 35 degrees. Moreover, there is a significant increase in the variance for K = 5000. Figure 6: Running time and number of iterations needed for convergence, when we increase the num-ber of samples(applications) and dimensions.

The steep increase from K = 2500 to K = 5000 can be ex-plained by the decrease in the number of samples per client; 12 . 8 per client on average for 5000 clients. Note that when having a small number of samples per client, it is  X  X ifficult X  to accurately predict in which cluster a client should be as-signed, in the E step. Therefore, while the total number of samples remains constant, the convergence of the algorithm to the ground truth vectors becomes more  X  X ifficult X , as we keep decreasing the number of samples per client.

The above fact is confirmed by the number of iterations, until converging to an optimum, for the experiments of Fig-ure 5(a). As we see in Figure 5(b), the number of iterations steadily increases as K increases from 312 to 5000.
In this section, we study the scalability of our algorithm as we increase the number of samples and dimensions. In Figure 6(a), we plot the running time of our algorithm (Y-axis) on a 1.8 GHz Intel Core i5 processor with 4 GB of RAM. The algorithm is implemented in Python using scikit-learn, and the local optima are explored in parallel. We use the same non-sparse setting with the one used in Figure 4(a).
The computational overhead for solving the sparse logis-tic regression problems increases as the number of samples increases (X-axis). Surprisingly, though, the overall time of our algorithm is not monotonically increasing: up to a point it is constant or decreasing and then slowly increases. More-over, that point is different for each number of dimensions.
The outcome of Figure 6(a) is due to the trade-off between the computational overhead for solving the logistic regres-sion problems and the number of iterations needed to con-verge to an optimum. As the number of samples increases, the computational overhead increases but the number of it-erations decreases, as Figure 6(b) shows. This causes the overall running time to decrease or remain constant. In ad-dition, the point where the number of iterations essentially stops decreasing is different for 8, 12, and 16 dimensions: the more the dimensions, the more the samples needed for the number of iterations to stop decreasing. This explains why the overall time starts to slowly increase in different points for each number of dimensions, in Figure 6(a).
We start with a brief overview of the datasets, the fea-tures, and the metric, and then we discuss our findings.
Training and Testing Datasets: We collected all the applications of contractors to tasks opened by clients in the oDesk platform, from September 1st of 2012 to December 15 of 2012. In addition, we filtered this set of applications in order to keep only the applications rejected/approved by clients who had a  X  X ufficient X  number of approved applica-tions within this period. For example, consider a threshold of 10 approved applications, a client A that had 9 approved applications within that period, and a client B with 11 ap-proved applications: all the applications for tasks opened by A are excluded from the training set, while all applications for tasks opened by B are included in the training set.
In particular, we generated 5 training sets using 5 differ-ent thresholds: 10, 20, 30, 40, and 50 approved applications. Note that each of these sets is a subset of the previous one. Table 2 summarizes the details for the 5 training sets: col-umn approved refers to the threshold of approved applica-tions, column clients refers to the number of clients satisfy-ing the respective threshold constraint, and column training refers to the number of rejected/approved applications by those clients, i.e., the actual samples in the training set.
Furthermore, we generated the 5 testing sets correspond-ing to the 5 training sets: for each training set we kept the clients that satisfy the respective threshold constraint, and we collected the rejected/approved applications for their tasks from 01/15/2013 to 03/01/2013 ( testing in Table 2).
Features: We ran experiments with 4 different tiers of features: tier 1 includes 5 features, tier 2 includes the 5 fea-tures of tier 1 plus 5 more features and so on. Table 1 gives the names and descriptions of the features, along with the tiers that include each feature. Note that each application in our training/testing sets is represented by the values for those features the moment the application is made. We nor-malize all the feature values to [0 . 0 , 1 . 0], taking into account what is the maximum value the feature may have. For ex-ample, for the contractor X  X  score that has a maximum of 5 . 0 stars, the score in each application is divided by 5 . 0.
Metric: The metric we use is the area under the ROC curve (AUC). Moreover, when there are more than one clus-ters, we compute a separate AUC for each cluster. Let us describe how we quantify the improvement over the single-cluster approach through an example. Consider running Al-gorithm 1 with 2 clusters as input. For each cluster i Al-gorithm 1 computes, we use the criteria w i learned for this cluster, and we compute the AUC: assume an AUC 1 = 0 . 8 for cluster 1 and an AUC 2 = 0 . 9 for cluster 2. Note that in order to compute AUC i , we use the testing-set applica-tions of the clients that were placed in cluster i . In addition, we run Algorithm 1 using a single cluster as input (i.e., we run the classic logistic regression model) and we compute the criteria w s for a single cluster. For each of the 2 clus-ters Algorithm 1 initially produced, we use w s to compute the AUC for the testing-set applications that belong to that cluster: assume an AUC 0 1 = 0 . 8 and an AUC 0 2 = 0 . 5. Then, we compute the ratio AUC AUC 0 for each cluster and the weighted average of all ratios: assume that in the testing set of cluster 1 there are 200 approved applications and in the testing set of cluster 2 there are 800 approved applications. The com-an improvement of 64% over the single-cluster approach.
It is important to note that the AUC cannot be greater than 1 . 0 and will not be less than 0 . 5; assuming a predic-tion accuracy not worse than random. Hence, the combined AUC ratio will always be less than 2 . 0, or in other words, the improvement over the single-cluster approach cannot be more than 100%.
Figure 7 depicts the ratio between the AUC for the multi-cluster approach and the AUC for the single-cluster ap-proach (baseline) on the Y-axis, as we increase the number of clusters (X-axis). Each curve refers to a different feature tier (see Table 1): a) 5 features in tier 1, b) 10 features in tier 2, c) 15 features in tier 3, and d) 20 features in tier 4. In Figure 7(a), we used the training/testing set for an approved threshold of 10, in Figure 7(b), we used the training/testing set for an approved threshold of 20, and so on.

There are three main factors that can explain the results in Figure 7. First, as we increase the number of dimensions (features), the improvement over the baseline (same criteria for all clients) drops: note that in most cases the curves for fewer features stay above the curves for more features. This factor is aligned with our observations in Section 4.1.2, i.e., as the number of features increase, our algorithm needs more samples and at some point the number of samples in the respective training set is not sufficient to improve over the prediction accuracy of the baseline approach.

Second, if we increase the number of clusters beyond a certain point, there is no improvement over the baseline (in fact, the baseline gives a higher AUC): in Figure 7(c) for 16 clusters, the curve for 10 features falls below 1 . 0, in Fig-ure 7(d) for 16 clusters, both curves for 10 and 15 features fall below 1 . 0, and in Figure 7(e) for 16 clusters, the curves for 10, 15, and 20 features fall below 1 . 0. This observation is aligned with the discussions for Figures 3 and 4(b).
Third, if we reduce the number of samples (applications) beyond a certain point, the improvement over the baseline diminishes: the curve for 5 features drops from a 30%-40% improvement over the baseline in Figures 7(a)-7(c) to a 20% improvement in Figures 7(d)-7(e), while the curve for 10 fea-tures does not show any improvement over the baseline (or even drops below 1 . 0) in Figures 7(c)-7(e). This observation is aligned with the discussion in Section 4.1.1.

Nevertheless, the three factors discussed above, fail to ex-plain why in some cases where the number of samples de-creases, the improvement over the baseline increases. For example, from Figure 7(a) to Figure 7(b), there is a large increase in the curves for 5 and 10 features, for 2 and 4 clusters. (Or, from Figure 7(b) to Figure 7(c), the improve-ment for 5 features increases from 30% to almost 40%, for 16 clusters.) This behavior is the outcome of an important trade-off: by increasing the approved threshold, the number of samples in the training set drops from 760000 to 50000, from Figure 7(a) to Figure 7(e), however, the number of ap-plications per client increases. Hence, we have more samples per client in order to accurately  X  X earn X  her criteria and place Figure 9: Weights of the criteria vector when a sin-gle cluster is used (traditional logistic regression). her in the  X  X ight X  cluster. (The positive effect of having more samples per client was also discussed in Section 4.1.3.)
Let us now focus on one of the most interesting outcomes of our algorithm: The 4 clusters produced for 10 features and an approved threshold of 20, an outcome achieving a sub-stantial 40% improvement over the baseline, in Figure 7(b). (As we discussed in the description of the metric, the im-provement cannot be greater than 100%.)
Figures 8(a) to 8(d) depict the values of the criteria vec-tors, w 1 to w 4 , for the 4 clusters. Specifically, the figures show the 4, out of 10, criteria for which at least one of the vectors had a non-zero value. As these figures indicate, there are important differences in the criteria of clients for: a) clusters 1 and 3 (Figures 8(a) and 8(c)), b) cluster 2 (Fig-ure 8(b)), and c) cluster 4 (Figure 8(d)).

Clients in cluster 2 have a strong tendency in selecting contractors whose skills match well with the task they post (large positive weight for the  X  X atched Skills X  feature), clients in cluster 4 tend to reject contractors that have completed many hours working in the platform (large negative weight for  X  X ontractor X  X  Total Hours X ), and clients in clusters 1 and 3 decide based on the contractor X  X  score, skills, and agency independence (see Table 1 for a description of the features).
Before discussing further the differences between client cri-teria in different clusters, let us compare these criteria with those learnt when using a single cluster, i.e., when learn-ing the same criteria for all clients. Figure 9 depicts the non-zero values of the criteria vector when a single cluster is used, i.e., when we run the traditional logistic regression. There are two main inconsistencies in those values: 1) they do not capture the fact that only a specific group of clients (cluster 4) has a negative bias towards contractors with a lot of hours completed in the platform, and 2) they miss the strong positive bias of a group of clients (cluster 2) towards contractors with the appropriate skills for a task. These two inconsistencies explain why our algorithm gives this sub-stantial 40% improvement (Figure 7(b)) over the baseline approach of learning the same criteria for all clients.
Some of the differences in the clients X  criteria can be ex-plained by observing the IT/KPO and FP/HR percentages of the applications in the different clusters. IT stands for In-formation Technology and refers to applications for all tasks related to software development and administration, e.g., web development, DB administration, or software for sci-entific experiments. Applications for all other tasks fall un-der the Knowledge Processing Outsourcing (KPO) category, e.g., translation, marketing, or logo design. A second catego-rization for tasks is the Fixed Price(FP)/Hourly Rate(HR) categorization that refers to whether a fixed amount will be given to the contractor upon the completion of the task or the client and the contractor will get into a contract with (c) criteria vector w 3 (g) IT vs KPO cluster 3 (k) FP vs HR cluster 3 predefined hourly-rate earnings. Figures 8(e) to 8(h) depict the IT/KPO percentages, while Figures 8(i) to 8(l) depict FP/HR percentages, in the 4 clusters.

In cluster 2 there is a large percentage of IT tasks (57.9%), as we see in Figure 8(f). This large percentage provides an explanation for the criteria in cluster 2. Most IT contractors receive good reviews by clients; although clients are not al-ways fully satisfied with the contractor X  X  work (e.g., software consistent with the specifications that does not do exactly what the client had in mind). As a result, IT contractors tend to have a very high score and, hence, the contractor X  X  score stops being a powerful signal for a client to base her decision. On the other hand, the technologies where the con-tractor is an expert, in many cases indicate if a contractor is a good fit for an IT task. Therefore, the clients in cluster 2 have a good reason to base their decisions on the  X  X atched Skills X  (Figure 8(b)).

Although the criteria in clusters 1 and 3 (Figures 8(a) and 8(c)) seem similar, there is an interesting difference: clients in cluster 1 give a large weight to the  X  X atched Skills X  while the value for this feature in cluster 3 is almost zero. As Figures 8(i) and 8(k) point out, there is an important differ-ence in the percentage of the HR tasks in the two clusters: in cluster 1 only 18 . 9% of the tasks are HR, while in cluster 3 50 . 5% of the tasks are HR. The  X  X atched Skills X  signal is usually more useful for FP tasks that require expertise in a very specific piece of work that needs to be completed, while HR tasks mostly require a good collaboration between the client and the contractor.

As for the large negative weight on the  X  X ontractor X  X  To-tal Hours X  for cluster 4 (Figure 8(d)), it appears that the IT/KPO and HR/FP percentages cannot provide any plau-sible explanation. Cluster 4 simply consists of clients that prefer to work with contractors that are  X  X ew X  to the plat-form. In fact, there is a good reason for selecting such contractors: in many cases  X  X ew X  contractors are eager to produce a high-quality outcome in order to build a good reputation in the system.
A problem similar to the one we study is addressed by collaborative filtering (CF) approaches (see [28] for a recent survey). Traditional memory-based CF approaches estimate the preference of a user U to an item I using the ratings of U for items  X  X imilar X  to I or the ratings of users  X  X imilar X  to U for I [11, 26]. Other CF approaches are based on latent factor models such as matrix factorization [24, 31], latent Dirichlet allocation [2], Boltzmann machines [25], La-tent Semantic Analysis [12], or user/item co-clustering [7, 10], while others [19] have proposed models that combine the memory and model based approaches.
 The hetereogeneity of user preferences is also studied by Lenk et al [21]: user preferences depend on users X  spatial region, in that study. User partitioning can also be based on whether a user is an  X  X nnovator X  or  X  X mitator X  [13]: inno-vators make decisions based on their own preferences, while imitators decide based on a product X  X  stage of maturity.
The main limitation for directly applying CF methods to the problem discussed in this paper, is the fact that contrac-tor applications to a task posted by a client are  X  X nique X . That is, while most CF methods assume that users rate, ex-plicitly or implicitly, the same products, in our case, clients do not actually rate the same contractors but their applica-tions to a specific task.

The effect of clients having different preferences when se-lecting among alternatives, is studied thoroughly in the eco-nomics/marketing literature [15, 23, 20, 1]. In particular, most studies focus on how a market is partitioned in seg-ments, with each segment having clients with same prefer-ences when selecting among brands. Most related to our work, are the studies using Finite Mixture Logit models [22, 15, 23]. An important difference of those studies with our work is that we don X  X  focus on  X  X earning X  the distribution of market segments. On the contrary, we focus on partition-ing the existing clients, with each client belonging to exactly one segment; instead of belonging to any segment with some probability (see [16] for an analysis of the tradeoffs between  X  X ard X  and  X  X oft X  assignments). Moreover, as noted earlier in the discussion for CF, the preferences of clients refer to  X  X nique X  applications in our model, as opposed to a fixed set of alternatives, e.g., brands. Our model is simpler than the Finite Mixture Logit models used in economics and market-ing studies, and aims for a scalable solution to a problem met in most of the online work marketplaces.
Identifying the groups of clients with similar hiring criteria is of great importance in online work marketplaces. In this paper, we presented our model for hiring-criteria clustering and we developed a clustering algorithm that can be applied effectively on large datasets. When applied on oDesk job applications, our approach significantly improves the pre-diction accuracy for future hirings of clients.

Furthermore, the analysis of the clusters generated by our algorithm reveals some interesting facts about the way differ-ent groups of clients choose contractors for their tasks: some clients are positively biased to contractors that are  X  X ew X  to a marketplace (probably because many new contractors are eager to build a competitive profile), while other clients ig-nore the contractor X  X  reputation and focus on how well the contractor X  X  skills match to the task requirements. Our ap-proach discovers such differences in client hiring criteria and can drastically improve the matching between clients and contractors in work marketplaces.
