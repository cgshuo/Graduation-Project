 Computers, since their creation , have exceed ed human beings in (speed and accuracy of) mathe-matic al calculation. However, it is still a big chal-lenge nowadays to d esign algorithms to automat-ically solve even primary -school -level math word problems (i.e., math problems described in natural language ) .

E fforts to automatically solv e math word prob-lems date back to the 1960s ( Bobrow, 1964a, b ) . Previous work on this topic falls into two catego-ries: symbolic approaches and statistical learning methods . In symbolic approaches ( Bobrow, 1964a, b ; Charniak , 1968; Bakman, 2007 ; Liguda &amp; Pfeiffer , 2012 ) , math problem sentences are transformed to certain structure s by pattern matching or verb categorization . E quation s are then derived from the structure s . Statistical learn-ing methods are employed in two recent papers ( Kushman et al., 2014; Hosseini et al., 2014).
Most (if not all) previous symbolic approaches suffer from two major shortcomings. First, natural language (NL) sentences are processed by simply applying pattern matching and/or transformation rules in an ad -hoc manner (refer to the related work section f or more details) . Second, s urpris-ingly , they seldom repo rt evaluation results about the effectiveness of the methods (except for some examples for demonstration purpose s ) . For the small percentage of work with evaluation results available, it is unclear whether the patterns and rules are specially designed for specific sentences in a test set .
 Figure 1: Nu mber word problem examples
In this paper, we present a computer system called SigmaDolphin which automatically solv es math word problems by semantic parsing and rea-soning. We design a meaning representation lan-guage called DOL (abbreviation of d o lphin l an-guage) as the structured semantic representation of NL text. A semantic parser is implemented to transform math problem text into DOL trees. A reasoning module is included to derive math ex-pression s from DOL trees and to calculate final answers. Our appr oach falls into the symbolic cat-egory , but makes improvements over previous symbolic methods in the following ways , 1) We introduce a systematic way of parsing NL text , based on context -free grammar (CFG) . 2 ) E valuation is enhanced in terms of both data set construction and evaluation mechanisms . We split the problem set into a development set ( called dev set) and a test set. Only the dev set is accessi ble during our algorithm design (especially in designing CFG rules an d in implementing the parsing algorithm) , which avoids over -tuning to-wards the test set. T hree metrics ( precision, recall, and F 1 ) are employed to measure system perfor-mance from multiple perspectives , in contrast to all previous work (including the statistical ones) which only measures accuracy .

We target, in experiments, a sub type of word problems: number word problems (i.e., verbally expressed number problems , as shown in Figur e 1). We hope to exten d our techniques to handl e general math word problems in the future .

We build a test set of over 1 ,5 00 problems and make a quantitative comparison with state -of -the -art statistical methods. Evaluation results show that our approach significantly outperforms base-line methods on our test set. Our system yields an extremely high precision of 9 5.4 % and a reasona-ble recall of 60 . 2 %, which shows promising appli-cation of our system in precision -critical situa-tions . 2.1 Math w ord pr oblem s olving Most previous work on automatic word problem solving is symbolic . STUDENT ( Bobrow, 1964a, b ) handles algebraic problems by first transform-ing NL sentences into kernel sentences using a small set of transformation patterns. The kernel sentence s are then transformed to math expres-sions by recursive use of pattern matching. CARPS ( Charniak, 1968 , 1969) uses a similar ap-proach to solve English rate problems. The major difference is the introduction of a tree structure as the internal representation of the information gathered for one object . Liguda &amp; Pfeiffer ( 2012) propose model ing math word problems with aug-mented semantic networks . Addition/subtraction problems are studied most in early research ( Bri-ars &amp; Larkin, 1984 ; Fletcher, 198 5 ; Dellarosa, 1986 ; Bakman, 2007 ; Ma et al., 2010) . Please re-fer to Mukherjee &amp; Garain (2008) for a review of symbolic approaches before 2008.
No empirical evaluation results are reported in most of the above work. Almost all of the se ap-proaches parse NL text by simply ap plying pattern matching rules in an ad -hoc manner. For example, as mentioned in Bobrow (1964b), due to the pat-tern  X ($, AND $) X , the system would incorrectly divide  X  X om has 2 apples, 3 bananas, and 4 pears. X  into two  X  X entences X :  X  X om has 2 apples, 3 bananas. X  and  X 4 pears. X  tomatically solving elementary math word prob-lems , with technique details unknown to the gen-eral public . Other examples on the web site demonstrate a large coverage of short phra se que-ries on math and other domains. By randomly se-lecting problems from our dataset and manually testing on their web site, we find that i t fails to handle most problem s in our problem collection .
Statistical learning methods have been pro-posed recently in two papers: Hosseini et al. (2014) solve single step or multi -step homoge-nous addition and subtraction problems by learn-ing verb categories from the training data. Kush-man et al. (2014) can solve a wide range of word problems, given that the equation sy stems and so-lutions are attached to problems in the training set. The method of the latter paper (referred to as KAZB henceforth) is used as one of our baselines. 2.2 Semantic parsing There has been much work on analyzing the se-mantic structure of NL strings . In s emantic role labeling and frame -semantic parsing (Gildea &amp; Jurafsky, 2002; Carreras &amp; Marquez, 2004; Marquez et al., 2008; Baker et al., 2007; Das et al., 2014) , predicate -argument structures are dis-covered from text as their shallow semantic repre-sent ation. In math problem solving, we need a deeper and richer semantic representation from which to facilitate the deriving of math expres-sion s.

Another type of semantic parsing work ( Zelle &amp; Mooney, 1996; Zettlemoyer &amp; Collins, 2005; Zettlemoyer &amp; Collins , 2007; Wong &amp; Mooney, 2007; Cai &amp; Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013 ; Berant &amp; Liang, 2014 ) map s NL text into logical forms by supervised or semi -supervised learning . Some of them are based on or related to c ombinatory c ategorial g ra mmar ( CCG ) ( Steedman , 2000). Abstract Meaning Rep-resentation (AMR ) ( Banarescu et al., 2013) keeps richer semantic information than CCG and logical forms . In Section 3.1.4, w e discuss the differences between DOL , AMR , and CCG, and explain why we choose DOL as the meaning representation language for math problem solving. Consider the first problem in Figure 1 ( written be-low for convenience),
To automatically solve th is problem, the com-puter system need s to figure out , somehow , that 1) two numbers x, y are demanded, and 2) the y sat-isfy the equation s below,
To achieve this, reasoning must be performed based on common sense knowledge and the infor-mation provided by the source problem. Given the difficulty of performing reasoning directly on un-structured and ambiguous natural language text, it is reasonable to transform the source text into a structured , less ambiguous representation .
O ur approach contains three modules: 1) A meaning representation language called 2) A semantic parser which transforms natu-3) A reasoning module to derive math expres-Figure 2: DOL example 3.1 DOL: Meaning representation language Every meaningful piece of NL text is represented in DOL as a semantic tree of various node types. Figure 2 shows the DOL representation of the sec-ond problem of Figure 1. It contains two semantic trees, corresponding to the two sentences. 3.1.1 Node types Node types of a DOL tree include constants , clas-ses , and functions . Each interim node of a tree is always a function; and each leaf node can be a constant, a class, or a zero -argument function.
Constants in DOL refer to specific objects in the world. A constant can be a number (e.g., 3.57), a lexical string (like  X  X ew York X ) , or an entity .
Classes : An entity class refers to a category of entities sharing common semantic properties. For example , all cities are represented by the class lo-c ation .city ; and math.number is a class for all numbers . It is clear that,
A class C 1 is a sub -class (denoted by  X  ) of an-other class C 2 if and only if every instance of C 1 are in C 2 . The foll owing holds according to com-mon sense knowledge,
Template classe s are classe s with one or more parameters, just like template classes in C++ . The most important template class in DOL is where c is a class ; m and n are integers. Each in-stance of this class is a list containing at least m and at most n elements of type c . For example, each instance of t .list&lt;math.number,2,+  X  &gt; is a list containing at least 2 numbers .

Functions are used in DOL as the major way to form larger language units from smaller ones. A function is comprised of a name , a list of core arguments , and a return type . DOL enables func-tion overloading ( again borrowing ideas from pro-gramming languages) . That is, one funct ion name can have multiple core -argument specifications. Below are two specifications for fn.math.sum ( which appears in the example of Figure 2 ).
Here  X $1: math.expression X  means the first ar-gument has type math.expression.

DOL supports three kinds of functions: noun functions, verb functions, and modifier functions.
Noun functions map entities to their properties or to other entities having specific relations with the argument(s). For example, n f .math.sum maps math expressions to their sum. Noun functions are used to represent noun phrases in natural language text . More noun functions are shown in Table 1.
Among all noun functions, n f .list has a special important position due to its high frequency in DOL trees . The function is specified below,
For example nf.l ist(math.number , 5) returns a list containing 5 elements of type math.number . It is the semantic representation of  X  X ive numbers X .
P ronoun functions are special zero -argument noun func tions. Examples are nf.it (representing an already -mentioned entity or event) and nf. what ( denoting an unknown entity or entity list ) .
Verb functions act as sentences or sub -sen-tences in DOL. As an example, v f .be.equ (in Fig-ure 2) is a verb function that has two arguments of the quantity type .
In addition to core arguments ($1, $2, etc.) , many functions can take additional extended ar-guments as their modifiers . Our last function type called m odifier functions often take the role of e x-tended arguments , to modify noun functions, verb functions , or other modifier functions. Modifier functions are used in DOL as the semantic repre-sentat ion of adjectives, adverb phrases (including conjunctive adverb phrases), and prepositional phrases in natural languages. In the example of Figure 2, the function mf.number.even modifies the noun function nf.list as its extended argument . 3.1.2 Entity variables Variables are assigned to DOL sub -trees for indi-cating the co -reference of sub -trees to entities and for facilitating the construction of logical forms and math expressions from DOL. In Figure 2, the same variable v 1 (meaning a variable with ID 1) is assig ned to two sub -trees in the first sentence and one sub -tree in the second sentence. Thus the three sub -trees refer to the same entity.
 Table 1: Example DOL functions 3.1.3 Key features of DOL DOL has some nice characteristics t h at are critical to building a high -precision math problem solving system. That is why we invent DOL as our mean-ing representation language instead of employing an existing one.

First, DOL is a strongly typed language. Every function has clearly defined argument types and a return type . A valid DOL tree must satisfy the type -compatibility property:
For example, in Figure 2, the return type of nf.math.power is math.expression, which matches the second argument of vf.be.equ. However , the following two trees ( yielded from the correspond-ing pieces of text) are invalid because they do not satisfy type -compatibility .
Second, we maintain in DOL a n open -domain typ e system. The typ e system contains over 1 000 manually verified classes and more automatically generated ones (refer to Section 3.2. 1 for more de-tails). Such a comprehensive typ e system make s it possible to define various kinds of functions and to perform type -compatibility checking. In con-trast, most previous semantic langua ges ha ve at most 100+ types at the grammar level . In addition, by introducing template classes, we avoid main-taining a lot of potentially duplicate type s and re-duce the typ e system management efforts . To the best of our knowledge, template classes are not available in other semantic representation lan-guages.

Third , DOL has b uilt -in data structures like t.list and nf.list which greatly facilitate both func-tion declaration and text representation ( espe-cially math text representation). For example, the two variants of nf.math.sum (refer to Section 3.1.1 for their specifications) are enough to represent the following English phrases:
Without t.list or nf.list, we would have to define a lot of overloaded functions for nf.math.sum to deal with different numbers of addend s . 3.1.4 Comparing with other languages Among all meaning representation languages, AMR (Banarescu et al., 2013) is most similar to DOL . Their major differences are: First, they use very different mechanisms to represent noun phrases. In AMR, a sentence (e.g.,  X  X he boy de-stroyed the room X ) and a noun phrase (e.g. ,  X  X  he boy X  X  destruction of the room  X ) can have the same representation. While in DOL, a sentence is al-ways represented by a verb function; and a noun phrase is always a noun function or a constant. Second, DOL has a larger type system and is stricter in t ype compatibility checking. Third, DOL has template classes and built -in data struc-tures like t.list and nf.list to facilitate the represen-tation of math concepts .

CCG (Steedman, 2000) provides a transparent interface between syntax and semantics. In CCG, s emantic information is defined on words (e.g.,  X   X x.odd(x)  X  for  X  X dd X  and  X   X x. number (x)  X  for  X  X umber X  ). In contrast, DOL explicitly connects NL text patterns to semantic elements . For exam-ple, as shown in Table 2 (Section 3.2.1) , one CFG grammar rule conne cts pattern  X  {$1} raised to the power of {$2}  X  to function nf.math.power .
Logical forms are another way of meaning rep-resentation. We choose not to transform NL text directly to logical forms for two reasons: On one hand, s tate -of -the -art methods for map ping NL text into logical forms typically target short, one -sentence queries in restricted domains. However, many math word problems are long and contain multiple sentences . On the other hand, variable -id assignment is a big issue in direct logical form co nstruction for many math problems. Let X  X  use the following problem ( i.e., the first problem of Figure 1 ) to illustrate,
For this problem , it is difficult to determine whether  X  X he smaller number X  refers to  X  X ne num-ber X  or  X  an other X  in directly constructing logical forms . I t is therefore a challenge to construct a correct logical form for such kind s of problems .
Our solution to the above challenge is assign ing a new variable ID (which is different from the IDs of  X  X ne number X  and  X  X nother X ) and to delay the final variable -ID assignment to the reasoning stage. To enable this mechanism, the meaning representation language should support a lazy var-iable ID assignment and keep as much infor-mation (e.g., determiners, plurals, modifiers) from the noun phrases as possible. DOL is a language tha t always keep s the structure information of phrases, whether or not it has been assigned a var-iable ID.
 In summary, compared with other languages , DOL has some unique features which make it more suitable for our math problem solving sce-nario . 3.2 Semantic Pars ing O ur parsing algorithm is based on context -free grammar (CFG) (Chomsky, 1956; Backus, 1959; Jurafsky &amp; Martin , 2000) , a commonly used mathematical system for modeling constituent structure in natural languages . 3.2.1 CFG for connecting DOL and NL
The core part of a CFG is the set of grammar rules. Example English grammar rules for build-ing syntactic parsers include  X  S  X  NP VP  X ,  X  NP  X  CD | DT NN | NP PP X , etc . Table 2 shows some example CFG rules in our system for mapping DOL nodes to natural langua ge word sequences. The left side of each rule is a DOL element (a function, class, or constant); and the right side is a sequence of words and arguments. The grammar r ules are consumed by our parser for building DOL trees from NL text.

So far there are 9 , 600 grammar rules in our sys-tem. For every DOL node type , the lexicon and grammar rules are constructed together in a semi -automatic way. Math -related classes, functions, and constants and their grammar rules are manu-ally built by referring to text books a nd online tu-torials. About 35 classes and 2 00 functions are ob-tained in this way . Additional instances of each element type are constructed in the ways below.
Classes : Additional c lasses and grammar rules types , and automatically extracted lexical seman-tic data . By treating Freebase types as DOL clas-ses and the mapping from types to lexical names as grammar rules , we get the first version of gram-mar for classes . To improve coverage, w e run a term peer similarity and hypernym extraction al-gorithm (Hearst, 1992; Shi et al., 2010; Zhang et al., 2011) on a web snapshot of 3 billion pages, and get a peer -similarity graph and a collection of is -a pairs. An is -a pair e xample is ( M egan F ox , actress) , where  X  M egan F ox  X  and  X  X ctress X  are in-stance and type names respectively . In our peer similarity graph,  X  X egan Fox X  and  X  X ritney Spears X  have a high similarity score . The peer similarity graph is used to clean the is -a data col-lection (with the idea that p eer terms often share some common type names) . Given the cleaned is -a data, w e sort the type names by weight and man-ually create classes for top -10 00 t ype names. For example, create a class person.actress and add a grammar rule  X  X erson.actress  X  actress  X . For the other 2000 type names in the top 3000, we create classes and rules automatically, in the form of  X  X lass.TN  X  TN X , where TN is a type name. For example, create rule  X  class. succulent  X  succu-lent  X  for name  X  succulent  X . Table 2: Example grammar for connecting DOL and NL
F unctions : Additional noun functions are auto-matically created from Freebase properties and at-tribute extraction results ( Pasca et al., 2006; Durme et al., 2008), using a similar procedure with creating classes from Freebase types and is -a extraction results. W e have over 50 manually defined math -related verb functions . Our future plan is automatically generat ing verb functions from databases like PropBank ( Kingsbury &amp; Palmer, 2002 ) , FrameNet ( Fillmore et al., 2003 ) , and VerbNet 4 ( Schuler , 2005) . Additional modi-fier f unctions are automatically created from an English adjective and adverb list, in the form of  X  X f.adj.TN  X  TN X  and  X  X f.adv.TN  X  TN X  where TN is the name of an adjective or adverb . Figure 3: The DOL semantic parse tree for  X  X ine plus an integer is equal to 314 X  Figure 4: A syntactic parse tree 3.2.2 Parsing Parsing for CFG is a well -studied topic with lots of algorithms invented ( Kasami , 1965 ; Earley , 1970). The core idea behind almost all the algo-rithms is exploiting dynamic programming to achieve efficient search through the space of pos-sible parse trees. For syntactic parsing, a well -known serious problem is ambiguity: the appear-ance of many syntac tically correct but semanti-cally unreasonable parse trees. Modern syntactic parsers reply on statistical information to reduce ambiguity. They are often based on probabilistic CFGs (PCFGs) or probabilistic lexicalized CFGs trained on hand -labeled TreeBanks .

With the new set of DOL -NL grammar rules (examples in Table 2 ) and the type -compatibility property (Section 3.1.3), ambiguity can hopefully be greatly reduced , because semantic ally unrea-sonable pars ing often result s in invalid DOL trees . We implement a t op -down parser for our new CFG of Section 3.2.1, following the Earley algo-rithm (Earley, 1970). No probabilistic information is attached in the grammar rules because no Tree-banks are available for learning statistical proba-bilities for the new CFG. Figure 3 shows the parse tree returned by our parser when processing a sim-ple sentence. The DOL tree can be obtained by re-moving the dotted lines (corresponding to the non -argument part in the right side of the grammar rules). A traditional syntactic parse tree i s shown in Figure 4 for reference.
 During parsing, a score is calculated for each DOL node. The score of a tree T is the weighted averag e of the scores of its sub -trees , where  X   X  is a sub -tree, and  X  (  X   X  ) is the number of words to which the sub -tree corresponds in the original text . I f the type -compatibility property for T is satisfied ,  X  (  X  ) =1 ; otherwise  X  (  X  ) =0.
All leaf nodes are assigned a score of 1.0, ex-cept for pure lexical string nodes (which are used as named entity names). The score of a l exical string node is set to 1 /( 1 +  X  n ), where n is t he num-ber of words in the node , and  X  (=0.2 in experi-ments) is a parameter whose va lue does not have much impact on parsing results . Such a score function encourage s interpreting a word sequence with our grammar than treating it as an entity name.

Among all candidate DOL trees yielded during parsing , we return the one with the highest score as the final parsing result . A null tree is r eturned if the highest score is zero. 3.3 Reasoning The reasoning module is responsible for deriv ing math expression s from DOL trees and calculating problem answers by solving equation system s. Math expressions have different definitions in dif-ferent contexts. In some definitions, equations and inequations are excluded from math expressions . In this paper, equations and inequations (like  X  a = b  X  and  X  ax + b &gt;0 X ) are called s -expression s be-cause they represent mathematical sentences , while other math expressions (like  X  x +5 X ) are named n -expressions since they are essentially noun phrases . Our definition of  X  math expres-sions  X  therefore includes both n -expressions and s -expressions .

Different types of no des may generate different types of math expressions. In most cases, s -ex-pressions are derived from verb function nodes and modifier function nodes, while n -expressions are generated from constants and noun function nodes . For example, the s -expression  X 9+ x =314 X  can be derived from the DOL tree of Figure 3, if variable x represents the integer. In the same Fig-ure, The n -expression  X 9+x X  is derived from the left sub -tree.

The pseudo -codes of our math expression deri-vation algorithm are shown in Figure 5. The algo-rithm generates the math expression for a DOL tree T by first calling the expression derivation procedure of sub -trees, and then applying the se-mantic interpretation of T. All the s -expressions derived so far are stored in an expression list named XL .
 Figure 5: Math expression derivation algorithm Table 3: Example semantic interpretations
The semantic interpretation of DOL node s plays a critical role in the algorithm. Table 3 shows some example interpretations of some rep-resentative DOL functions. In the table, $1, $2 etc. are function arguments, and $  X  for a modifier node denotes the node which the modifier modi-fies . So far the semantic interpretations are built manually. Please n ote that it is not necessary to make semantic interpretations for every DOL node in solving number word problems. For ex-ample, most class nodes and many adverb nodes can have null interpretations at the moment. 4.1 Experimental setup Datasets : Our problem collection 5 contains 1 , 878 math number word problems, collected from two we b sites: a lgebra.com 6 (a web site for users to post math problems and get help from tutors) and answers.yahoo.com 7 . Problems on both site s are organized into categories. For algebra.com, prob-lems are randomly sampled from the number word problems category ; for answers.yahoo.com , we first randomly sample an initial set of prob-lems from the math category and then ask human annotators to manually choose number word problems from them. M ath equations 8 and an-swers to the problem s are manually added by hu-man annotators .

We randomly split the dataset into a dev set (for algorithm design and debugging) and a test set . More subsets are extracted to meet the require-ments of the baseline methods (see below) . Table 4 shows the statistics of the datasets.
 Baseline me thods : We compare our approach with two baselines : KAZB ( Kushman et al., 2014) and Basic Sim .

KAZB is a learning -based statistical method which solve s a problem by mapping it to one of the equation templates determined by the anno-tated equations in the training data . We run the ALLEQ version of their algorithm since it per-forms much better than the other two (i.e., 5EQ and 5EQ+ANS). Their codes support on ly linear equations and require that there are at least two problems for each equation template (otherwise an exception will be thrown). By choosing prob-lems from the collection that meet these require-ments, we build a sub -data set called LinearT2 . In the dataset of KAZB, each equation template cor-responds to at least 6 prob lems. So we form an-other sub -dataset called LinearT6 by removing from the test set the problems for which the asso-ciated equation template appears less than 6 times.
BasicSim is a simple statistical method which works by computing the similarities between a testi ng problem and those in the training set , and then applying the equations of the most similar problem . This method has similar performance with KAZB on the ir dataset, but does not have the two limitations mentioned above. Therefore we adopt it as the second baseline .

For both baselines , experiments are conducted using 5 -fold cross -validation with the dev set al-ways included in the training data. In other words, we always use the dev set and 4/5 of the test set as training data for each fold .
 Evaluation metrics : Evaluation is performed in the setting that a system can choose NOT to an-swer all problems in the test set. In other words, one has the flexibility of generating answers only when she knows how to solve it or she is confident about her answer. In this setting, the following three metrics are adopted in reporting evaluation results (assuming, in a test set of size n , a system generates answers for m problems, where k of them are correct): Precision: k / m Recall (or coverage): k / n F1: 2PR/( P+R) = 2 k /( m + n ) All dev 374 1.79 20.3 test 1,504 1.75 22.5 Linear dev 247 1.78 19.6 test 986 1.72 19.0 LinearT2 dev 172 1.85 18.8 test 669 1.71 17.4 LinearT6 dev 71 1.96 16.8 test 348 1.80 16.1 Table 4: Dataset statistics (Linear: problems with linear equations; T2: problems corresponding to template size  X  2) 4.2 Experimental r esults The Overall evaluation results are summarized in Table 5 , where  X  X o lphin X  represents our ap-proach. The results show that our approach signif-icantly out performs (with p&lt;&lt;0.01 according to two -tailed t -test) the two baselines on every test set , in terms of precision, recall, and F -measure . O ur approach achieves a particularly high preci-sion of 95 % . That means once a n answer is pro-vided by our approach , it has a very high proba-bility of being correct.

Please note that our grammar rules and parsing algorithm are NOT tuned for the evaluation data. Only the dev set is referred to in system building. Since the baselines generate results for all prob-lems, the precision, recall, and F1 are all the same for each dataset.
 Dataset Method Precision (% ) Recall (%) F1 (% ) LinearT6 LinearT2 Linear BasicSim 32.3 32.3 32.3 Dolphin 95.7 63.6 76.4
Test set all Table 5: Evaluation results
The reason for such a high precision is that, by transforming NL text to DOL trees, the system  X  understands  X  the problem (or has structu red and accurate information about quantity relations) . Therefore it is more likely to generate correct re-sults than statistical methods who simply  X  X  uess  X  according to features. By examining the problems in the dev set that we cannot generate answers, we find that most of them are due to empty parsing results.

On the other hand, statistical approaches have the advantage of generating answers without un-derstanding the semantic meaning of problems (as long as there are similar problems in the training data). So they are able to handle (with probably low precision) problems that are complex in terms of language and logic .

Please pay attention that our experimental re-sults reported here are on number word problems . General math word problems are much harder to our approach because the entit y types , properties, relations, and actions contained in general word problems are much larger in quantity and more complex in quality . We are working on extending our approach to general math word problems. Now our DOL language and CFG grammar al-ready have a good coverage on common entity types, but the coverage on properties, relations , and actions is quite limited. As a result, our parser fails to parse many sentences in general math word problems because they contain properties, relations or actions that are unknown to our sys-tem . We also observe that sometimes we are able to parse a problem successfully, but cannot derive math expressions in the reasoning stage. This is often because some rela tions or actions in the problem are not modeled appropriately . As future work , we plan to extend our DOL lexicon and gr ammar to improve the cove rage of properties, relations, and actions. W e also plan to study the mechanism of model ing relations and actions. We proposed a semantic parsing and reasoning approach to automatically solv e math number word problems. We have designed a new meaning representation language DOL to bridge NL text and math expression s. A CFG parser is imple-mented to parse NL text to DOL trees. A reason-ing module is implemented to derive math expres-sions from DOL trees, by applying the semantic interpretation of DOL nodes . We achieve a h igh precision and a reasonable recall on our test set of over 1 ,5 00 problems. We hope to extend our tech-niques to handling general math word problems and to other domains (like physics and chemistry) in the future .
 We would like to thank the annotators for their ef-forts in assigning math equations and answers to the problems in our dataset . Thanks to the anony-mous reviewers for their helpful comments and suggestions.

