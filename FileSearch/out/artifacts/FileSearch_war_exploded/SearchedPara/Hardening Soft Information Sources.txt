 The web contains a large quantity of unstructured infor-mation. In many cases, it is p ossible to heuristical ly ex-tract structured information, but the resulting databases are \soft": they contain inconsistenci es and duplication , and lack unique, consistently-used ob ject identi ers. Examples include large bibliograp hi c databases harvested from raw sci-enti c pap ers or databases constructed by merging hetero-geneous \hard" databases. Here we formally mo del a soft database as a noisy version of some unknown hard database. W e then consider the hardening problem, i.e., the problem of inferring the most likely underlying hard database giv en a particular soft database. A key feature of our approach is that hardening is global | man y sources of evidence for a given hard fact are tak en in to accoun t. W e formulate hardening as an optimization problem and giv eanon trivial nearly linear time algorithm for nding a lo cal optimum. H.4.m [ Information Systems ]: Miscellaneous data integration The web contains a large quantity of unstructured infor-mation. In many cases, it is p ossible to heuristical ly ex-tract structured information, but the resulting databases are \soft": they contain duplicati on and lack unique consistently-used ob ject identi ers.
 As an example of a soft database, consider the large bib-liographic databases harvested from ra wscien ti c pap ers Current address: Whizbang Labs-Researc h, East, 4616 Henry St, Pittsburgh, PA 15213. y Current address: Dept. of Computer Science, Univ. of W ashington, Seattle W A.
 A \soft" datab ase S : author(\Bart Selman", \Critic al b ehavior in satis ability"). aliation(\Ba rt Selman", \Cornel l University"). author(\B. Selman", \Critic al b ehavior for satis ability"). author(\B. Selman", \BLACKBO Xthe orem proving").
 A\har d" datab ase H : author(b art selman, critical). aliation(bart selman, c ornel l). author(b art selman, blackb ox).
 Figure 1: A \soft" bibliographic database and the underlying \hard" data by systems such as ResearchIndex [11] or Cora [4]. These databases contain information ab out thousands of technical pap ers. How ev er, it is often hard to determine if two cita-tions refer to the same pap er, or if two names (e.g. \William Cohen" and \W. E. Cohen") refer to the same p erson. This mak es it dicult to p erform basic op erations such as count-ing the n um b er of citations to a particular pap er or p erson. Here w e formalize a soft database as one in which distinct iden ti ers may refer to the same entity .W e formalize hard-ening as the problem of determining the (most likely) co-references b etween the soft identi ers|th at is, determining which pairs of soft identi ers refer to the same real-world ob ject. The result of this is a hard (conventional) database. An example is shown in Figure 1. Notice that more con-clusions can b e drawn from the hard database H than the soft database S . In particular, H implies that an author of the pap er \BLA CKBO X theorem proving" is aliated with Cornell, where as S do es not.
 Although we will use bibliogra phi c data as a running ex-ample, we wish to emphasize that soft data arises in many di eren tcon texts. For instance, soft data might b e cre-ated by automatically extracting facts from classi ed ads [2], or newsgroup p ostings [5]. A soft database might also arise from merging the con ten ts of several heterogeneous, autonomously-created \hard" databases.
 Formally ,w e will assume soft facts are given along with some measure of ho wlik ely two identi ers are to b e co-referen t. (For example, the strings \W. E. Cohen" and \Willi am Co-
Permission to make digital or hard copies of part or all of this work or permission and/or a fee.

KDD 2000, Boston, MA USA  X  ACM 2000 1 -58113 -233 -6/00/0 8 ...$5.00 hen" are a-priori plausibly co-referen t, whereas the strings \Willia m Cohen" and \Bart Selman" are not.) W e then for-malize the problem of nding the b est \hard" mo del of a set of soft facts probabili stica ll y .W e de ne a prior probabilit y distribution o v er p ossible hard databases and a prior prob-abilit y distributio n o v er name co-references. Giv en these distribution s, the task of constructing the most lik ely hard database b ecomes a w ell-de ned optimization problem. This optimization problem in v olv es minimizin g the sum of the n um b er of tuples in the hard database, plus a cost asso ci-ated with the set of co-reference assumptions required. W e will assume a set of \references", eac h of whic h is some string referring to some real-w orld ob ject. A tuple of refer-ences is written R ( r 1 ; ::: ; r n ) where R ranges o v er some xed set of relations and r 1 , ::: , r n are references. Unlik e a con v en tional hard database, w e will allo w distinct refer-ences to refer to the same real-w orld en tit y .W e de ne a soft datab ase to b e a set of tuples of the form R ( r 1 ;:::;r where R ranges o v er some xed set of relations and r 1 , n are references.
 In the bibliograp hi c domain the same string migh t denote di eren ten tities when used in di eren tcon texts, e.g., the string \Mic hael Jordan" migh t denote a di eren t p erson in a news story than in a pap er on graphical mo dels. T o cir-cum v en t this problem w e can construct references b ycon-catenating a string with some indication of the con text from whic h that string w as extracted; for instance, the author name \Mic hael Jordan" extracted from a p ostscript pap er p w ould b e represen ted as the reference \Mic hael Jordan" and w ould b e distinct from the reference \Mic hael Jordan" extracted from a news story b .
 T o return to the example of Figure 1, supp ose one has pro-cessed t w o p ostscript pap ers p 1and p 2. The follo wing facts ha v e b een extracted from the title page of p 1: author(\Bart Selman " p 1 , \Critic al b ehavior in satis abilit y" al(\Bart Selman " p 1 , \Cornel l University" p 1 ). and supp ose the follo wing facts ha v e b een extracted from the bibliogra ph y section of p 2: author(\B. Selman " p 2 , \Critic al b ehavior for satis abil ity" author(\B. Selman " p 2 ,\BLA CKBO X: Applying the or em T ogether these facts form a soft database S .
 Hardening determines the co-reference relationships b et w een references in the soft database. The co-reference relation is p erhaps most naturally represen ted b y an equiv alence rela-tion on the soft references. Ho w ev er,hereitistec hnically more con v enien ttow ork with an in terpretation function | a function mapping eac h soft reference to a hard in terpre-tation. In order to incremen tally construct in terpretation functions w e form ulate an in terpretation function as a set of interpr etation ar cs eac h of whic h is an arc of the form ! r 2 where r 1 and r 2 are distinct references, and w is a non-negativ erealn um ber w eigh t (or cost) discussed in more detail b elo w. An interpr etation I is an acyclic set of in ter-pretation arcs, suc h that for all references r 1 app earing in I there is at most one arc of the form r 1 w ! r 2 2 I .This de nition allo ws in terpretation arcs to b e c hained, i.e., it is p ossible that r 1 w ! r 2 2 I and r 2 w 0 ! r 3 2 I . In this case is indirectly in terpreted as r 3 .F or an y giv en in terpreta-tion I and an y reference r ,w e de ne I ( r ) to b e the ultimate in terpretation of r : Note that an in terpretation I de nes a hard database I ( S ) deriv ed b y replacing eac h reference in S b y its ultimate in-terpretation under I .
 Of course not all in terpretations are p ossible | w ew ould not w an ttoin terpret \B. Selman" as \H. Kautz". The w eigh t w in the arc r 1 w ! r 2 is the cost (or unlik eli ness) of in terpreting r 1 as r 2 .F ormally w e assume the costs are pro vided in the form of a p otential interpr etation set , i.e., a set I pot of (w eigh ted) p oten tial in terpretation arcs. Giv en a soft database S and a set I pot of p oten tial in terpretations our goal will b e to nd an in terpretation I I pot minimizing the cost function c ( I ) de ned as follo ws where 1 and 2 are parameters of the cost function; j I ( S ) j is the n um ber of distinct tuples in the hard database I ( S ); and j I j is the n um ber of in terpretation arcs in I . Note that as more arcs are added to I w eha v ethat w ( I )+ j I j increases while 1 j I ( S ) j decreases. The cost function hence represen ts a tradeo b et w een the unlik eli ness of the in terpretation and the compactness of the resulting hard database. This ob jectiv e function is appro ximately deriv ed from a probabilisti c mo del in the follo wing section. In this section w egiv eanappro ximate probabilis tic deriv a-tion of the cost function (1). Let Pr ( H; I ; S ) a distribution on hard databases H ,in terpretations I , and soft databases S . An optimal hardening of S is a pair H; I that maximizes ( H; I j S ). Notice that for a xed S the optimal hardening can b e found b y simply maximizing the join t probabilit yof H; I and S .
 T ode ne Pr ( H; I ; S )w e will assume that there is a nite set U of p oten tial real-w orld ob jects and a nite set of relations, eac h of xed arit y . These assumptions imply that there is a nite set of p ossible tuples whic h could app ear in H . W elet N be the n um b er of p ossible hard tuples. In the probabilis tic mo del w e assume that b oth the hard data base H and the soft database S can con tain duplicate tuples. W elet j H j and j S j be the n um b er of tuples in H and S resp ectiv ely where distinct o ccurrences of the same tuple are treated as separate elemen ts. W e also let j I j b e the n um ber of arcs in an in terpretation I | note that duplication of arcs in I is ruled out b y the requiremen tthatev ery reference has at most one outgoing arc. W eno w assume that Pr ( U; I ; S ) can b e decomp osed as follo ws where H , I ,and S are real n um b er parameters in the in terv al (0 ; 1) and the n um ber Z is selected so that Pr ( I ) sums to 1.
 In (5) w eha v ethat I 1 ( H ) is the set of soft tuples suc h that I () 2 H and j I 1 ( H ) j is the cardinalit y of this set. The ab o v e equations corresp ond to a certain pro cess for gen-erating the triple H; I ; S . First H is generated b y rep eatedly selecting one of the N p ossible hard tuples at random. A t eac h iteration the selection pro cess stops with probabilit y
H and con tin ues with probabili t y1 H . A similar pro-cess is used to generate I b y selecting arcs from I pot where the probabilit y of the arc r 1 w ! r 2 is prop ortional to e w and where, on eac h selection iteration, the selection pro cess terminates with probabilit y I . In the case of generating I , ho w ev er, after the arcs ha v e b een selected w ec hec k that I is w ell formed, i.e., that it is acyclic and that eac h reference has at most one outgoing edge. If I is not w ell formed then w e start o v er. If the w eigh ts in I pot are normalized so that the probabili t y of selecting r 1 w ! r 2 is exactly e w ,w eha v e that the constan t Z equals the probabilit y that a I is w ell formed when the selection phase terminates. Finally ,giv en H and I w e generate S using a pro cess analogous to that for generating H except that w e select individua l tuples from the set I 1 ( H ).
 Giv en a soft data base S our ob jectiv e is nd the v alues I and H maximizing P r ( I; H j S ). This is equiv alen t to nd-ing I and H so as to maximize Pr ( H; I ; S ). F or an ygiv en v alue of S and I the optimal v alue of H is simply I ( S ). So, giv en S , it suces to nd the v alue of I maximizing ( I ( S ) ;I;S ). This is in turn equiv alen t to minimizing the \complexit y" of the triple H , I ,and S , i.e., minimizing the quan tit y ln Pr ( I; H; S ). F or this minimization problem w e can ignore terms that do not dep end on the c hoice of I .So the problem b ecomes that of selecting I so as to minimize the cost c 0 ( I ) de ned as follo ws. T o deriv e (1) w e assume that the last term in (6) can b e appro ximated b y a linear function of the form + j I j . Here w esho w that the optimization problem de ned b y (1) is NP-hard. The problem remains NP-hard ev en under rather sev ere restrictions. In particular, w e can assume that all w eigh ts in I pot are zero and that 2 in (1) is also zero so that w e are simply minimizi ng j I ( S ) j .F urthermore, w ecan assume that I pot has a rather restricted form. De ne a \hard reference" to mean a reference h suc hthat I pot con tains an arc of the form r w ! h and de ne a \soft reference" to b e a reference r suc hthat I pot con tains an arc of the form r The graph I pot is bipartite if and only if the hard references are disjoin t from the soft references.

Theorem 4.1. Unless NP = P ,ther eisno p olynomial time algorithm for c omputing an interpr etation I I p ot minimizing j I ( S ) j even under the assumption that I p ot bip artite and every soft r efer enc eo c curs at most onc ein S . Pr oof. Theproofisb y reduction of v ertex co v er [7]. Let G =( V; E ) b e a graph. The v ertex co v er problem is that of determining if there exists V 0 V with j V 0 j &lt;k suc h that ev ery edge in E con tains a v ertex in V 0 .Giv en the graph let S b e the soft data base suc h that for eac h edge f x; y g2 E w eha v ethat S con tains the tuple R ( r f x;y g ). Note that no reference o ccurs more than once in S .Let I pot consist of pot is bipartite. F or an yin terpretation I pro viding a hard in terpretation for ev ery reference w eha v e that I ( S ) de nes a subset of the v ertices that co v ers the edges. W eno wha v e that there exists an I with j I ( S ) j &lt;k if and only if there exists a v ertex co v er of the giv en graph smaller than k . While nding an optimal hardening is in tractable, it is clearly p ossible to heuristicall y searc h for a go o d hardening. A nat-ural approac h is to use a lo cal searc h algorithm that b egins with an empt yin terpretation I and iterativ ely extends it so as to impro v e the cost function (1). W e will consider a simple algorithm that adds one arc at a time | at eac h iter-ation I is replaced with I [f r 1 w ! r 2 g .Eac h arc is selected greedily so as to induce the largest reduction in c ( I ). T o constructing an ecien t greedy algorithm it is con v enien t to in tro duce some formal notation. W e abbreviate I [f r g as I + r 1 w ! r 2 .W ede ne candidates ( I ) to b e the set of arcs r 1 w ! r 2 2 I pot suc hthat I + r 1 w ! r 2 is a w ell formed in terpretation, i.e., suc h that I ( r 2 ) 6 = I ( r 1 ) and I ( r Assuming that I and S are clear from con text w e de ne ( r 1 w ! r 2 ) to b e the reduction in cost ac hiev ed b y adding ! r 2 , i.e., the quan tit y c ( I ) c ( I + r 1 w ! r 2 ). The greedy algorithm can b e view ed as rep eatedly merging equiv alence classes. A teac h p oin tintimethein terpretation I de nes an equiv alence relation on the set of references used in S , i.e., r 1 and r 2 are equiv alen tunder I if I ( r Eac h new arc r 1 w ! r 2 added to I merges t w o equiv alences classes.
 Eac h stepofthe form I := I + r 1 w ! r 2 can also b e view ed as a kind of heuristic inference. F or example, if S con tains a tuple stating that \B. Selman" p 1 is an author of pap er x and another tuple stating that the (di eren t) reference \Bart Selman" p 2 is also an author of x then merging these t w o references reduces the n um b er of tuples in I ( S ). If t w o references ha v e sev eral o v erlapping facts, e.g., they are b oth authors of the same two pap ers, then w eha v eev en stronger evidence that they are the same. The ab o v e algorithm it-erativ ely nds the \safest" heuristic inference, i.e., the one for whic h the p ositiv e evidence in the form of shared tuples most strongly out w eighs the cost of the iden ti catio n. Note that as more inferences are made additional evidence can b e generated for further inferences. F or example, orig-inally w ema yha v e that r 1 authors pap ers x and y ; r thors pap ers y and z ;and r 3 authors pap ers x and z .The evidence for an y single merger is originall y one shared pa-p er, e.g., r 1 and r 2 share pap er x .Oncew eha v e merged r and r 2 ,ho w ev er, the evidence for merging r 3 with the class f r 1 ;r 2 g b ecomes stronger | it is no w based on t w o shared pap ers.
 The ab o v e commen ts imply that ev en for a xed arc r 1 the v alue of ( r 1 w ! r 2 ), whic h can b e view ed as the curren t w eigh t of evidence in fa v or of r 1 w ! r 2 , can c hange during the execution of the algorithm. So it is not immediately clear that nding the next greedy arc (the safest next inference) can b e done without searc hing though all the p oten tial arcs. Assuming that I pot is roughly prop ortional to j S j , the naiv e implemen tation w ould run in time quadratic in the size of S .F or soft data bases of sizes t ypical for w eb applications quadratic b eha vior is prohibitive. Our main result is that the greedy algorithm can b e implemen ted in a w a y that runs to completion in nearly linear time.
 W eno w de ne the am biguit y of a reference r 1 to b e the n um b er of arcs of the form r 1 w ! r 2 in I pot .Theam biguit yof a p oten tial in terpretation set I pot is the maximal am biguit y of an y reference r 1 app earing in I pot .Let d b e the am biguit y of I . Let k b e the maxim um n um b er of argumen ts of an y relation sym b ol. W eno wha v e the follo wing.

Theorem 5.1. It is p ossible to run the gr e e dy se ar ch al-gorithm to c ompletion in time O ( j S j k 3 d log ( j S j kd )) . The ecien t implemen tation k eeps the elemen ts of candidates ( I ) in a priorit y queue where the priorit yof 1 w ! r 2 is ( r 1 w ! r 2 ). The algorithm incremen tally up-dates priorities on eac h iteration. The ecien tv ersion can b e written as follo ws where Q is a priorit y queue con taining arcs in I pot . Eac h time an arc is remo v ed from Q it is ei-ther determined to no longer b e a candidate or is added to I . The algorithm main tains priorities so the priorit yofan arc r 1 w ! r 2 in candidates ( I ) is alw a ys ( r 1 w ! algorithm also uses a union-nd data structure to main tain the curren t equiv alence relation on references. The op era-tion union ( r 1 ;r 2 ) declares t w o references to b e equiv alen t, and t w o references r 1 and r 2 ha v e b een made equiv alen tif and only if find ( r 1 )isthesameas find ( r 2 ). The ecien t implemen tation is summarized b elo w.
 Ecien t Implemen tation I := ; ; Q := a priorit y queue con taining all arcs in I pot while Q is not empt y and the largest priorit y is p ositiv e The remainder of this section describ es ho w the priorit y up-dates on eac h iteration can b e done in sublinear time (amor-tized o v er all iterations). W e rst reduce the problem of main taining priorities to the conceptually simpler problem of main taining the e ect of eac h arc on the quan tit y j I ( S ) j . Note that the equiv alence relation main tained in the union-nd data structure is the same as that de ned b y I , i.e., w e ha v e that I ( r 1 )= I ( r 2 ) if and only if find ( r 1 F or an yarc r 1 w ! r 2 2 I pot de ne H ( r 1 w ! r 2 ) to b e the c hange in j I ( S ) j .W ethen ha v e the follo wing where is the result of replacing eac h soft reference r in S b y find ( r ) and find + union ( r 1 ;r 2 ) denotes the nd map that results from p erforming the union op eration on r 1 and r 2 . Since ( r 1 w ! r 2 ) can b e written as 1 H ( r 1 w ! r 2 it no w suces to incremen tally main tain H ( r 1 w ! r 2 T o ecien tly up date H ( r 1 w ! r 2 )w emain tain a set E of e ect assertions eac hofwhic h is a triple h ;r 1 w ! r 2 where 2 find ( S ), r 1 w ! r 2 2 I pot with find ( r 1 ) o ccur-ing in , and where is the result of replacing find ( r 1 ) b y find ( r 2 )in. Notethatw e do not require the arc to be in candidat es ( I ). The assertion h ;r 1 w ! r 2 ; i can b e view ed as a kind of meta-assertion ab out the e ect of adding the arc r 1 w ! r 2 to I . The assertion h ;r 1 w ! r 2 ; i states that if w e union r 1 and r 2 and c hange the nd map so that the nd of elemen ts in the equiv alence class of r 1 are set to the nd of r 2 , then will b e con v erted to . The algorithm main tains the in v arian tthat E is the set of all e ect asser-tions (for the curren tv alues of I and find ). The initial set 0 is the set of triples h ;r 1 w ! r 2 ; i suc h that 2 S , r o ccurs in and is the result of replacing r 1 b y r 2 in . W eno w describ e the relationshi p b et w een the set E of e ect assertions and the quan tities H ( r 1 w ! r 2 ). De ne, for an y arc x , the set range ( x )tobethesetof suc h that there ex-ists a 2 find ( S )suc h that E con tains a triple of the form h ;x; i .F or an y in range ( x )w e de ne count ( x; ) to be the n um b er of triples in E of the form h ;x; i plus one if find ( S )con tains . It is p ossible to sho w the follo wing. By main tainin g appropriate indices, it is p ossible to e-cien tly incremen tally main tain the set E , the set find ( S ), and the v alues of count ( x; ) and H ( x ). One k ey detail is that in merging equiv alence classes for references r 1 2 w ec hange the v alue of the nd map on the smaller of the t w o classes. This ensures that eac h time the nd of a reference c hanges the size of its equiv alence class at least doubles. So the n um b er of nd c hanges for a giv en refer-ence can b e at most the log of the n um b er of references, i.e., at most log ( j S j k ). Another k ey observ ation is that in an y triple h ;x; i2 E w eha v e thatand x determine .
 This implies that are at most j S j kd triples in E . Details are giv en in the full pap er. Previous w ork has considered reasoning directly with soft databases (e.g. [3, 6, 1]). One disadv an tage of this approac h is that queries to a soft, probabilis tic database are generally more exp ensiv etoansw er than queries to a con v en tional \hard" database. There is also close connection b et w een the w ork describ ed here and w ell-studied problem of r e c or d linkage (e.g. [10, 8, 9]). In record link age the goal to deter-mine whic hen tit y descriptions are co-referen t (that is, refer to the same real-w orld ob ject). It is generally assumed that eac hen tit y is represen ted b y a \record"|a v ector of atomic v alues|and the similarit y or dissimil arit yoft w o records is measured b y comparing these v ectors. Ho w ev er, there are man y situations where the similarit yoft w oen tit y names dep ends on prop erties that cannot b e easily represen ted in a single record. F or instance, \B. Selman" and \Bart Sel-man" are clearly more lik ely tobev arian ts of the same name if \B. Selman" and \Bart Selman" ha v e authored similarly -titled tec hnical pap ers; ho w ev er, a publicati on history is not easily stored as a record attribute. This heuristic could b e easily incorp orated in to our approac hb y extending a soft bibliograp hi c database with facts ab out co-authorship. In this pap er w eha v e considered the follo wing more general problem: giv en a soft database S , and a structure I pot dicating whic h name co-reference relationship s are p ossible, nd the hard database H that is most lik ely giv en S .W e sho w that a natural formalization of this optimization prob-lem is NP-complete but that optimally greedy hardening can b e done in time nearly linear in j S j . [1] D. Barbara, H. Garcia-Molina , and D. P orter. The [2] M. E. Cali and R. Mo oney . Relational learning of [3] W. W. Cohen. In tegration of heterogeneous databases [4] Cora: Computer science researc h pap er searc h engine. [5] D. F reitag. Multistrategy learning for information [6] N. F uhr. Probabilisti c Datalog|a logic for p o w erful [7] M. R. Gary and D. S. Johnson. Computers and [8] M. Hernandez and S. Stolfo. The merge/purge [9] A. Monge and C. Elk an. An ecien t [10] H. B. New com b e, J. M. Kennedy , S. J. Axford, and [11] Researc hindex: The NECI scien ti c literature digital
