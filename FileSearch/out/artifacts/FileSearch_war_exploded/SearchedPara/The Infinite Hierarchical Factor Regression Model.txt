 in overfitting.
 we do not assume a known number of factors; (2) we do not assume factors are independent; (3) ysis framework itself, instead of having to model it separat ely.
 is Kingman X  X  coalescent [6], a popular distribution over in finite binary trees. problem, in which case our genes might correspond to movies, our samples might correspond to related (romance to comedy versus to action); many movies ma y be spurious. relationships. We further use Kingman X  X  coalescent to mode l latent pathway hierarchies. 2.1 Indian Buffet Process m k / ( i  X  1) value Z distribution over infinite binary matrices.
 p (
Z |  X  ) = Q K tomers. Taking K  X   X  yields the IBP. The IBP has several nice properties, the most important parameter  X  controls the sharability of dishes. 2.2 Kingman X  X  Coalescent the limit n  X  X  X  . In our case, the individuals are factors .
 n denote by t t the time between events (note  X  indepentently with exponential rate 1 ; so  X  the limit as n  X  X  X  , called the coalescent.
 the parent, and variance  X  matrix consisting of N samples [ x size P  X  K and F = [ f of idiosyncratic variations. K , the number of factors, is known.
 model and our mechanism for modifying the factor analysis problem to factor regression . 3.1 Nonparametric Gene-Factor Model allows our model to also have an unbounded number of factors.
 fore, we instead use the Hadamard (element-wise) product of a binary matrix Z and a matrix V becomes: x the prior over V would simply be a Gaussian: V  X  N or (0 ,  X  2  X  e = N or (0 ,  X  ) models the idiosyncratic variations of genes where  X  is a P  X  P diagonal matrix ( diag ( X  1 , ...,  X  P ) ). Each entry  X  P has an inverse-gamma prior on it. 3.2 Feature Selection Prior zero, there is no penalty.
 p is associated with Bernoulli random variable T T vector is given a parameter  X  , which, in turn, is given a Beta prior with parameters a, b . 3.3 Hierarchical Factor Model [8].
 Figure 1: The graphical model for nonparametric 3.4 Full Model and Extension to Factor Regression prior over Z , the sparse binary vector T , and the Coalescent prior over V . linear regression problem would involve estimating a K -dimensional parameter vector  X  with re-nonparametric factor analysis framework itself. We do so by prepending the responses y expression vector x outcomes from real-valued responses. We use Gibbs sampling with a few M-H steps. The Gibbs distribu tions are summarized here. sampling existing dishes, an entry in Z is set as 1 according to p ( Z an acceptance probability (following [9]) given by a = min { 1 , p ( rest |  X   X  ) Coalescent) but, for faster mixing, we propose F new from its posterior. We then use importance sampling to select an insertion time f or the new node y  X  between t t Here, y passed down through the tree (compare to Eq (1)).
 Sampling the sparse IBP vector T: In the sparse IBP prior , recall that we have an additional P -many variables T any dishes. T given a B et ( a, b ) prior. For inference, we collapse  X  and  X  and get Gibbs posterior over T V ) F , g/h, g )) and p ( T p = 0 | . )  X  ( b + P  X  P q 6 = p T q ) S tu ( x p | 0 , g/h, g ) , where S tu is the non-standard Student X  X  t-distribution. g, h are hyperparam-eters of the inverse-gamma prior on the entries of  X  .
 Sampling the real valued matrix V: For the case when V has a Gaus-sian prior on it, we sample V from its posterior p ( V N or ( V  X  P  X  v corresponds to the column of V being sampled.
 where  X  = A T ( AA T +  X  )  X  1 X and  X  = I  X  ( AA T +  X  )  X  1 A , where A = Z  X  V Sampling the idiosyncratic noise term: We place an inverse-gamma prior on the diagonal entries of
 X  and the posterior too is inverse-gamma: p ( X  p | . )  X  IG ( g + N 2 , h 1+ h X  X  ( Z  X  V ) F .
 G am ( K P i =1 1 / (  X  + i  X  1) Sampling the Factor Tree: Use the Greedy-Rate1 algorithm [8]. To get around this, one can perform model selection via Rever sible Jump MCMC [10] or evolu-assumes a fixed number of components.
 Bayesian Factor Regression Model (BFRM) of [1]. BFRM assume s a sparsity inducing mixture prior on the factor loading matrix A . Specifically, A where  X  gives: A written as A to see that, for BFRM where  X  recovers our model in the limiting case when K  X  X  X  . extension to BFRM.
 synthetic dataset having 100 samples of 50 genes and 8 underl ying factors. Since we knew the underlying factors (we know this from domain knowledge). 6.1 Nonparametric Gene-Factor Modeling and Variable Select ion to be off from the actual loadings (even modulo column permut ations). we also introduced spurious genes by adding 50 random featur es in each sample. We observe the relevant genes in such a case. 6.2 Hierarchical Factor Modeling squared error sense) and the log-likelihood, when compared to the case with Gaussian prior on V be visited as they are constrained by the hierarchy. 6.3 Factor Regression low error variances suggest that our method is fairly robust w.r.t. initializations. mance and more understandable outputs. We applied Kingman X  X  coalesc ent as a hierarhical model is whether the IBP can, itself, be modeled hierarchically.

