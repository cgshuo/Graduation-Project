 As data becomes dynamic, large, and distributed, there is in-creasing demand for what have become known as distributed stream algorithms . Since continuously collecting the data to a central server and processing it there incurs very high com-munication and computation complexities, it is advantageous to define local conditions at the nodes, such that  X  as long as they are maintained  X  some desirable global condition holds.
A generic algorithm which proved very useful for reducing communication in distributed streaming environments is geometric monitoring (GM). Alas, applying GM to many important tasks is computationally very demanding, as it requires solving a notoriously difficult problem  X  computing the distance between a point and a surface, which is often very time-consuming even in low dimensions. Thus, while useful for reducing communication, GM often suffers from exceedingly heavy computational burden at the nodes, which renders it very problematic to apply, especially for  X  X hin X , battery-operated sensors, which are prevalent in numerous applications, including the  X  X nternet of Things X  paradigm. Here we propose a very different approach, designated CB (for Convex/Concave Bounds). CB is based on directly bounding the monitored function by suitably chosen convex and concave functions, that naturally enable monitoring dis-tributed streams. These functions can be checked on the fly, yielding far simpler local conditions than those applied by GM. CB X  X  superiority over GM is demonstrated in reducing computational complexity, by several orders of magnitude in some cases. As an added bonus, CB also reduced communi-cation overhead in all application scenarios we tested. Disributed Streams; Distributed Monitoring; Resource Lim-ited Devices
The following is a canonical problem in distributed sys-tems and databases: given are distributed nodes, and a function which depends on the data at all of them. How can its value be computed, or approximated, with minimal communication? Typically, the trivial solution (collecting all data to a central node and computing the function there) is impractical.

With the advent of data stream systems and their increasing importance in quickly evolving fields such as social networks, a more difficult variant of this problem began attracting con-siderable interest: assume that the data at the nodes is also dynamic . Continuously and exactly computing the function X  X  value is typically infeasible, as real-world data consists of many nodes, each holding a large, rapidly changing data stream. This led to the introduction of the distributed moni-toring problem (also referred to as the functional monitoring problem , [29, 7, 34]; see also the survey in [9]), which can be broadly defined as follows: Definition 1. Given is a distributed system, with nodes N ...N k , with N i holding a dynamic data vector v i ( t ) ( t will be suppressed hereafter to reduce equation clutter). Also given is a function f , which depends on all the v i  X  X , and a threshold T . The goal is to define local conditions at the nodes, such that:
Case study. As a motivating real-life example [26], which applies the Pearson Correlation Coefficient function (treated in this paper), consider a distributed sensor network used to monitor air quality. Often, not only the information on the individual pollutants is important, but the correlations between them as well. For example, if an event i is defined as pollutant i crossing a certain threshold, one may wish to know whether there exists a correlation between events i,j for two different pollutants. A commonly used such measure, the Pearson Correlation Coefficient (PCC), quantifies such a correlation by the value z  X  xy  X  the probabilities of event i , event j , and both events simulta-neously. For a distributed system, the global probabilities are averaged over the nodes. It is easy, however, to see that the PCC value of the global probabilities can be above a given threshold T , while the local value at some of the nodes is below T , and vice-versa (for example, in a system with two nodes and local values x 1 = 0 . 8 ,y 1 = 0 . 2 ,z 1 = 0 . 17 and x = 0 . 2 ,y 1 = 0 . 7 ,z 1 = 0 . 15 , the local PCC values are 0.062 and 0.054, and the global value is  X  0 . 26 ). This is because, for arbitrary functions, there is generally no correlation between the average of the values and the value at the average .
For general functions, defined over a distributed system, it is typically impossible to determine the position of their global values vis-a-vis T , when given just the local values. The distributed monitoring problem is to impose local con-ditions which will guarantee that the global value did not cross T .

This problem is known to be rather difficult (NP-complete even in very simple scenarios; see [21]). Nonetheless, consid-erable progress has been made for real-life problems (Section 2).

Geometric monitoring (GM), introduced in [32], deals with the case in which the monitored function can be expressed as the application of an arbitrary function to the aggregated to be rich enough to be applicable to a wide range of problems (see Section 2.1). To the best of our knowledge, GM is the only completely generic method for monitoring arbitrary functions over the aggregated data; in this paper, the most recent version of GM [22] is the baseline for comparison. While a more complete description of GM is deferred to Section 2.1, we note that in order to apply it, the following problem should be repeatedly solved: let S be the hyper-surface (or threshold surface ) defined by S = { u | f ( u ) = T } . Then, it is required that each node, at every time-step, calculate the distance of a certain point (unique to that node) from S . This problem can be exceed-ingly difficult even for surfaces in low-dimensional Euclidean space, and it can render GM unsuitable for monitoring even relatively simple functions, such as the cosine similarity be-tween two vectors (more on this in Section 2.2).
We propose here a very different, simpler, and more direct method to solve the distributed monitoring problem. It relies on the simple observation that, if f is a convex function, then, if f ( v i )  X  T holds at every node, it also holds that f ( function (from above) is trivial  X  just monitor its value at every node.

To handle a general f , we propose to search for a convex function c such that c ( u )  X  f ( u ) for all vectors u , and monitor the condition c  X  T . This yields a far simpler monitoring condition, whose correctness implies the correctness of the desired condition f  X  T . Naturally, the following conditions should hold: We refer to the proposed method as convex bound (CB). Clearly, monitoring f  X  T can be similarly achieved by finding a concave lower bound.
We offer the following contributions:
Much of the early work on monitoring distributed streams dealt with the simpler cases of linear functions [20, 19]. Dis-tributed sensor networks were studied in [28, 27]. Other work included top-k monitoring [4]; distributed monitoring of the value of a single-variable polynomial [30], and perturbative analysis of eigenvalues, which was applied to determine local conditions on traffic volume data at the nodes of a distributed system, in order to monitor system health [17]. In [33], the monitoring problem is studied in a probabilistic setting, and in addition to the function X  X  score, a probability threshold is applied; see also [25]. Monitoring entropy was studied in [3]. Ratio queries are handled in [15]. In [35] the norm of the average vector is monitored.

While some problems in monitoring over distributed sys-tems were treated in the past, we are not aware of any general method (capable of handling arbitrary non-linear, non-monotonic, non-convex functions) except for GM and its derivatives, which are surveyed next.
In [31, 32] a general approach, geometric monitoring (GM), was proposed for tracking the value of a general function over distributed streams. GM rests on a geometric result, the so-called bounding lemma (details follow in this subsection), which makes it possible to  X  X reak up X  a global threshold query into conditions that can be checked locally at each site. Followup work [22, 13] proposed various extensions to the basic method. Recent work on GM includes efficient outlier detection in sensor networks [8] and sketch-based monitoring of norm, range-aggregate, and join-aggregate queries over distributed streams [12].

While GM achieved state-of-the-art results in reducing communication overhead for a nice range of central problems, its application is typically hampered by high computational overhead at the nodes. It is this problem which the proposed CB approach aims to alleviate.
Since GM is our baseline for comparison, and it also shares some basic terminology with CB, we next briefly describe it. Proofs and further details can be found in [22] (which is the version that was implemented).

A brief view of GM. Recall that the distributed mon-itoring problem considers whether f ( v 1 + .. + v k k )  X  T , where { v i } denote the local dynamic data vectors at the nodes. GM rests on the following geometric interpretation of this question: define the admissible region , A , by A , { u | f ( u )  X  T } . Then, the question is whether the con-dition v 1 + .. + v k k  X  A holds. The first step in answering this question is the following:
Lemma 1. [32] Let v i (0) denote the initial value of the data vector at the i -th node, and let the so-called reference point , p 0 , be equal to the average of these initial values: p chronization, a coordinator node broadcasts p 0 to all nodes. Denote the change in the data vector at the i -th node, i.e. v  X  v i (0) , by d i (it will be referred to as the i -th drift vector ). Then, the following holds: Now, the i -th node can independently compute p 0 + d i ; and since the global vector v is equal to the average of p 0 + d i = 1 ..k , it obviously lies in their convex hull. This can be used to impose local conditions on p 0 + d i , which will guarantee that v  X  A . This is achieved by the bounding lemma :
Theorem 1. [32] Let B i denote the (solid) sphere with center p 0 + d i / 2 and radius || d i || / 2 . Then the union contains the convex hull of the vectors p 0 ,p 0 + d 1 ...p hence it contains v .
 As a result of the bounding lemma, the local condition used in GM is the following: node i remains silent as long as its sphere B i is contained in A (Fig. 1). If this condition is violated, the system enters a violation recovery phase [32, 12].
 Figure 1: Applying local conditions in GM. The drift vector d i causes a violation, since the sphere it de-fines with p 0 intersects the inadmissible region  X  A ; however, d j does not cause a violation.
To apply GM, it must be repeatedly checked whether a certain sphere intersects with  X  A  X  that is, whether its radius is smaller than the distance from its center to A  X  X  boundary, which is defined by the threshold surface { u | f ( u ) = T } . Computing the distance from a point to the threshold surface of a general function is a notoriously difficult problem, and a closed-form solution very rarely exists. Worse, there exist no algorithms which guarantee that the distance will be recovered. Even for the case of a polynomial f , the solution may require an inordinate amount of time; closed-form solutions are often impossible to derive, and iterative schemes are slow and not guaranteed to converge. Known upper bounds on the complexity are extremely high (doubly exponential in the number of variables [2]).

In [24], GM was extended by the convex decomposition (CD) approach, which works by decomposing  X  A into convex subsets. However, the resulting algorithm has to be specifi-cally tailored to each monitored function, and it suffers from the need to solve the same type of problem as GM (finding the closest point on a surface). For the inner product func-tion, the solution was quite complicated, and we could not apply CD to the PCC or to cosine similarity, which are easily treated by CB.

Clearly, run-times as those often incurred by GM and its derivatives are unacceptable for many distributed stream-ing systems. We now introduce CB, and demonstrate its advantage for monitoring four popular functions.
As noted in the Introduction, it is easy to define local conditions for the monitoring problem (Def. 1) when f is convex: every node i must only check the condition f ( p d )  X  T (correctness follows immediately from Lemma 1). We propose to extend this simple observation to monitor arbitrary functions, using an approach which works directly in the realm of functions, as opposed to seeking a geometric solution. The proposed solution works by  X  X elaxing X  f to a convex function c that bounds f from above, and monitoring the condition c  X  T . This condition both implies f  X  T , and is also easy to monitor. We shall refer to c as a convex bound for f . Fig. 2 schematically demonstrates the idea behind CB.
 Figure 2: x 2 + 10 (blue curve) is a convex bound for x + 10 sin( x ) (dark curve).

The next theorem states that every solution which GM provides is also realizable as a solution provided by CB. The proof is omitted due to lack of space.

Theorem 2. For every monitoring problem, there is a solution obtained with CB which is exactly identical to the solution obtained with GM  X  that is, it imposes exactly the same local conditions.
There are, of course, an infinite number of convex bounds for f , and the question is which of them to choose. To this end, we first propose the following definition.

Definition 2. Let f be the monitored function. A tight family of convex bounds for f , denoted CB ( f ) , is a set of convex functions satisfying the following requirements: Clearly, if g 1 ,g 2 are both convex bounds for f , and g it is better to use g 2 when monitoring f (since the condition g ( v )  X  T is weaker than g 1 ( v )  X  T ). Therefore we have the following:
Lemma 2. When applying CB to monitor f , the convex bound should belong to some family of tight bounds of f . In the following case, it is possible to define CB ( f ):
Lemma 3. Let f be a concave function. Then the family of all tangent planes to f defines a family of tight bounds.
Proof. Every tangent plane is linear, hence convex. Fur-ther, it is known that a concave function lies under any of its tangent planes. Now, let g be convex and g f . Denote all points below f 0 s graph. Then both U ( g ) ,B ( f ) are convex, and the minimal distance between them is therefore obtained at points on their boundaries. The tangent plane at the point on f  X  X  boundary is the desired element of CB ( f ) . The idea of the proof is outlined in Fig. 3.
 Figure 3: A convex function g and concave function f such that g f . S is the segment connecting the two closest points on the graphs. The tangent at the closest point on f  X  X  graph, L , satisfies g L f , proving that the set of f  X  X  tangent planes is a tight family of convex bounds.
Replacing the monitored condition T  X  f by T  X  g f , for a convex g , enables efficient monitoring  X  alas, it might also result in potential false alarms (i.e. vectors u for which T  X  f ( u ) but T &lt; g ( u )). We refer to this problem as the convexity gap , or simply the gap (referring to the gap between
We deal here with differentiable functions, which in-clude many functions of practical interest. Further, non-differentiable functions can be arbitrarily approximated by differentiable functions on any bounded domain. f and g ). Figuratively speaking, the  X  X ystem price X , one must pay in order to allow distributed monitoring, is reflected in the  X  X onvexity price X , which is the gap between the monitored function and its convex bound. To minimize the number of false alarms, the gap should be minimized. However, as the following simple example demonstrates, it is often impossible to choose a single optimal g to achieve this goal. As depicted Figure 4: The impossibility of choosing a single best convex bound for the function f (dark curve). g 1 (resp. g 2 ) is better in the vicinity of p 1 (resp. p 2 ). in Fig. 4, it is evident that, loosely speaking, different bounds are better at different regions of the data space, and there is typically no hope of finding a unique bound that is always better than all the others. We formalize this observation with the following definition, which is both realizable and appropriate for the monitoring problem:
Definition 3. A convex bound g 1 is better than g 2 at a point p iff there exists a neighborhood of p in which g 2 Thus, in Fig. 4, g i is better at p i for i = 1 , 2.
Def. 3 is appropriate for the following reason. Recall that the drift vector d i is equal to zero; assuming that the data at the nodes behaves continuously, or can be approximated by a random walk ([18, 14, 21, 22]), it follows that the local vector p 0 + d i can be modeled by a continuous process which starts at p 0 and gradually wanders away from it. Therefore, a bound is sought which is optimal (i.e. smaller than all other bounds) in a certain neighborhood of p 0 . For the case of a concave f , such a bound is provided by the following result:
Lemma 4. If f is concave, the tangent plane at a point p is the best convex bound at p .
 The proof is trivial, since the tangent plane X  X  value at p is equal to f ( p ), but all other tangent planes lie above f . Thus the deviation of the tangent plane at p from the point ( p,f ( p )) is quadratic; hence, locally, it is smaller than that of the tangent planes at other points, which is linear. Con-sequently, when bounding a concave function from above (or, equivalently, a convex function from below), we will replace it with its tangent plane at p 0 . This is next used to trans-form a threshold condition on general functions to a convex condition.
If the monitored f is itself convex, the choice of a convex bound c is trivial  X  choose c = f . If f is concave, then, following Lemma 4, the tangent plane at p 0 is the optimal candidate for c . We next handle a more general case.
Definition 4. : Assume that f = c 1  X  c 2 , where both c ,c 2 are convex. The convexization of the condition f  X  T is defined by c = c 1  X  L c 2 ( p 0 )  X  T , where L c 2 tangent plane of c 2 at p 0 .
 Note that the c defined in Def. 4 is convex, bounds f from above, and that its definition is motivated by the special cases where f is convex or concave. The lower bound case is similarly handled: the inequality f  X  T is replaced by the condition L c 1 ( p 0 )  X  c 2  X  T (note that L c 1 ( p 0 )  X  c and bounds f from below).

We next prove that, for a very wide class of real problems, it is always possible to express f as the difference of two convex functions. First we recall a definition from calculus that comes in handy for testing convexity: Definition 5. Let f be a function of x 1 ...x n . Its Hessian H f is the n  X  n matrix H f ( i,j ) =  X  It is well known that a function is convex in a given domain D iff its Hessian is positive semidefinite (PSD) at every point in D 2 .

Lemma 5. If f possesses bounded second derivatives in a domain D , it can be expressed as the difference of two convex functions.

Proof. Since the elements of H f are bounded over D , there is an upper bound,  X , on the absolute values of H f negative eigenvalues. Define c 1 ( u ) = f ( u ) +  X  2 || u || || u || 2 . Clearly f = c 1  X  c 2 and c 2 is positive definite. Also, H 1 = H f + H c 2 = H f +  X  I (where I is the identity matrix). Hence all the eigenvalues of H c 1 are  X  0 and c 1 is convex. All the functions we deal with in this paper either have bounded second derivatives, or their derivatives are continu-ous and the domain of interest is bounded; hence, Lemma 5 is applicable. We will apply it for monitoring cosine similarity (Section 4.3).

The process outlined in Lemma 5 can be extended to provide a convex bound which is optimal to second order Taylor expansion. First, let us formalize the concept of  X  X nnihilating X  negative eigenvalues:
Definition 6. Given a symmetric matrix A with spectral decomposition [6] A = P i  X  i e i e t i (where  X  i are A  X  X  eigen-values and e i its eigenvectors), define its positive part by P ( A ) = P i max {  X  i , 0 } e i e t i .

Theorem 3 (whose proof is omitted due to lack of space) enables to define a convex bound which is optimal to second order.

Theorem 3. For a function f and reference point p 0 , define a convex bound by c opt ( p ) = f ( p 0 ) +  X  X  X  f ( p u convex bound g of f which satisfies g ( p 0 ) = f ( p 0 ) , it holds that H g ( p 0 )  X  H c opt ( p 0 ) (where  X  holds for both the operator and Frobenius norms of the respective Hessians). That is  X  up to second order, c opt ( p ) is an optimal convex bound at the vicinity of p 0 .
A matrix B is PSD iff uBu t  X  0 for every vector u . A symmetric matrix is PSD iff all its eigenvalues are  X  0.
Since c 1  X  c 2  X  T iff c 1  X  c 2 + T , c 3 , we can assume that the monitored condition is given as an inequality between two convex functions, c 1  X  c 3 . This condition is especially amenable to convexization: we replace it with c 1  X  L c 3 where, as before, L c 3 ( p 0 ) is c 3  X  X  tangent plane at p use this form of convexization for the inner product, cosine similarity, and PCA-Score functions (Section 4).
We now apply CB to monitor four popular functions: Pear-son correlation coefficient, inner product, cosine similarity, and PCA-Score ( X  X ffective dimension X ). In Section 5 we com-pare the run-time and communication overhead of CB and GM in a variety of real scenarios.

To apply CB, we follow the method described in Section 3.3.1. If the monitored function cannot be directly written as the difference of two convex functions (as in the case of cosine similarity), we apply Lemma 5.
Let x,y denote the frequency of appearances of two items in elements of a certain set, and z the frequency of their joint appearances. A very typical example is when x,y denote the ratio of documents in which certain terms appear, and z the same for appearances of both terms simultaneously. The range over which PCC is defined is therefore 0  X  x,y  X  1 and z  X  x,y . The function measures the strength of correlation between the appearances of x and y , and is defined by We will assume T &gt; 0; the case T  X  0 is treated similarly.
The condition P ( x,y,z )  X  T can be written as z  X  xy + T First, note that xy is neither convex nor concave; it is simple to verify that the Hessian X  X  eigenvalues for xy are always 1 and  X  1 (i.e. every point on the function X  X  surface is a saddle point ). We therefore use the identity xy = ( x + y ) 2 4 convex). We also need the following:
Lemma 6. The function The proof is omitted due to lack of space.

The condition P ( x,y,z )  X  T can therefore be written as and, since this last expression is the difference of two convex functions 3 , we can proceed by applying the paradigm de-scribed in Def. 4. The lower bound case is similarly handled. It remains to calculate the tangent planes of Q ,Q 2 , tivariate calculus. The bounds are depicted, for some typical values, in Fig. 5.
As explained in Section 2.2, to apply GM we must be able to solve the closest point problem for the surface defined by z = xy + T software [16], which first reduces the surface X  X  equation to
Since T Figure 5: Left: a convex upper bound (blue) for PCC (green). The reference point (in red) is x 0 = 0 . 3 ,y 0 = 0 . 6 , and T = 0 . 4 . Right: a concave lower bound. an algebraic one, and then solves for the closest point. This incurred a run-time far higher than the simple CB solution (by more than three orders of magnitude), and also resulted in higher communication overhead; results are provided in Section 5.2.1.
The inner product function is also extensively applied in data mining and monitoring tasks as a measure of similarity. We assume that the monitored function f is over vectors of length 2 n , and is equal to the inner product of the first and second halves of the vector; denoting the concatenation of vectors x,y by [ x,y ], we have f ([ x,y ]) =  X  x,y  X  . To express f as the difference of two convex functions, note that 4  X  x,y  X  = || x + y || 2  X  X | x  X  y || 2 . Since the norm squared function is convex, the condition  X  x,y  X  X  X  T is convexized by where the reference point p 0 = [ x 0 ,y 0 ], and the gradient multivariate function f , the tangent plane at a point u 0 given by f ( u 0 ) +  X  X  X  f ( u 0 ) ,u  X  u 0  X  ).
In order to apply GM, one must be able to solve the closest point problem for the threshold surface,  X  x,y  X  = T . If the point outside the surface is denoted [ x 0 ,y 0 ], the problem can be formulated as Minimize ( || x  X  x 0 || 2 + || y  X  y 0 || 2 ) such that  X  x,y  X  = T. This problem can be solved with Lagrange multipliers. Defin-These equations can be manipulated to obtain a quartic equation in  X  :
T X   X  (2 T +  X  x 0 ,y 0  X  )  X  2 + || x 0 || 2 + || y 0 || 2  X   X  X  x After solving for  X  , it is easy to solve for x,y .
While the inner product case is the only one addressed here for which a relatively simple solution for GM could be found, it still incurs the overhead of computing the quartic X  X  coefficients, solving it, and checking the solutions to see which one yields the closest point on the surface. GM X  X  overall run-time was about 20 times higher than CB X  X .
Another very popular measure of similarity is cosine simi-larity (referred to as Csim hereafter), which resembles the inner product function, but normalizes it by the length of the vectors. For example, if we have two histograms of word frequencies, derived from two document corpora, Csim will  X  X eutralize X  the effect of the corpus size when measuring the histogram similarity; the inner product function, however, is biased towards larger corpora.

As in the inner product case, the data vector p is [ x,y ], the concatenation of two n -dimensional vectors x,y , and the reference point will be denoted p 0 = [ x 0 ,y 0 ]. Then, Csim bound, i.e. Csim ( p )  X  T (we assume T &gt; 0; the case of negative T is similarly treated), we need to monitor the con-dition  X  x,y  X  X  X  T || x |||| y || . This problem is more complicated than the inner product case, since there is no obvious way to decompose it into an inequality between two convex func-tions; this is due to the fact that, while representing  X  x,y  X  as the difference of two convex functions is relatively easy, it is more difficult to derive such a representation for || x |||| y || . We therefore resort to using the method outlined in Lemma 5. We must first determine the smallest eigenvalue of the Hessian of || x |||| y || . It follows from the following lemma that it equals  X  1:
Lemma 7. At a point x,y , the eigenvalues of H ( || x |||| y || ) (each with multiplicity n  X  1 ).
 The proof is omitted due to lack of space. Now we can pro-ceed to convexize the problem. First, we write the inequality  X  x,y  X  X  X  T || x |||| y || as || x + y || 2  X || x  X  y || 2 to make both sides convex, we add 2 T ( || x || 2 + || y || to obtain Lastly, the inequality is convexized by replacing the RHS with its tangent plane at p 0 . This step is straightforward, requiring only computation of the gradient, and is omitted for brevity.
The problem of calculating the distance of a point to the Csim surface { [ x,y ] | X  x,y  X  = T || x |||| y ||} is exceedingly difficult. No closed-form solution exists, and three different software packages we applied took about three minutes to complete the task for a single point.
PCA (Principal Component Analysis) is a fundamental dimension reduction technique with numerous applications. Given a set of vectors in Euclidean space, PCA seeks a low-dimensional subspace which, on the average, well-approximates the vectors in the set. Formally:
Definition 7. Given 1 &gt; T &gt; 0 (typically T  X  0 . 9 ) and a finite set of vectors S  X  X  m , the effective dimension of S is defined as the smallest dimension of a sub-space V  X  X  m such that X projection of u on V 4 .
 It is well-known that the effective dimension, denoted k hereafter, can be computed as follows: 1. Construct the m  X  m scatter matrix M = X 2. Compute M  X  X  eigenvalues,  X  1  X   X  2  X  ...  X   X  m . 3. Determining the smallest k such that X In [23], PCA was applied to measure the health of a system consisting of distributed nodes. This proceeds as follows: at each timestep, a vector of various system parameters is associated with each node (typically, the vectors X  components are various traffic volume indicators). System-wide anomalies (i.e. DDOS attacks) are highly correlated with an increase in the effective dimension of the union of the parameter vectors over all nodes, in a sliding window of pre-determined length.
Hence, the condition to monitor is that the PCA-Score , de-fined by ( X T . As for the previous functions we handled, the difficulty lies in that  X  i are the eigenvalues of a global matrix which is equal to the sum of the local matrices, hence its exact com-putation at every timestep will incur a huge communication overhead. In order to apply CB for distributed monitoring, we must express the PCA-Score as a function of the average matrix, as opposed to the sum; however, since (i) eigenvalues scale linearly when the matrix is multiplied by a scalar, and (ii) the PCA-Score is defined as the ratio of sums of squares of eigenvalues, its values on the average and sum matrices are equal.
 What remains is to  X  X onvexize X  the inequality We rely on the following two lemmas:
Lemma 8. For an m  X  m scatter matrix S , X Tr 2 ( S ) , and is a convex function of S . The proof follows immediately from the fact that every scatter matrix is sym-metric.

Lemma 9. For a symmetric S , X
Proof. This follows from the famous Fan identities , specif-ically Theorem 2 in [11].
 Since both sides of Eq. 5 are convex, we can proceed as in Section 3.3.1, by replacing the LHS with the tangent plane at the reference scatter matrix S 0 . All that is required is to compute the gradient of the LHS; for that, we use the following result from linear algebra.
We assume that S is centralized , i.e. its average is zero. The general case proceeds along the same lines and is omitted due to lack of space.

Lemma 10. The derivative of  X  i with respect to S is equal to e i e t i , where e i is the eigenvector of S corresponding to  X  Hence, the monitored condition in Eq. 5 is convexized by
X where S 0 is the reference matrix, and S the local matrix.
In order to apply GM, we must be able to compute the minimal PCA-Score over all matrices in a sphere in the m 2 dimensional space of m  X  m matrices. This can be done using perturbative bounds that were applied in [17], which also addressed monitoring the health of a distributed system. We also tested a simpler method, analogous to the ones used in [17], in which the safe-zone is defined by the maximal sphere around the reference matrix which is contained in the admissible region. Both methods require bounding the change in the eigenvalues, given the magnitude of change in the matrix. Two such perturbative bounds can be applied, which relate the change in the eigenvalues to the Frobenius norm or the spectral norm of the change in the matrix. We refer to the algorithms which use the Frobenius norm resp. spectral norm as FN resp. SN (a detailed description is impossible due to lack of space). We note, however, that all these methods (GM, FN, SN) require solving the difficult problem of finding the closest point on the surface of matrices whose PCA-Score equals T ; this renders them slower than CB. Further, CB was better in reducing communication overhead. Details are provided in Section 5.2.4.
In the experiments, CB was applied to the tasks of monitor-ing the functions discussed in Section 4, over a few datasets and for different threshold values. For the PCC, Csim and inner product functions we compared CB to GM. To the best of our knowledge, GM represents the state-of-the-art in mon-itoring threshold queries over distributed streams. We are not aware of any other work on monitoring cosine similarity and the PCC, and while there is other work on monitoring the inner product [10], GM improved on it [12]. For the PCA-score function we compared CB to GM as well as to the Frobenius norm (FN) and spectral norm (SN) perturbative bounds described in [17].

We examined the sliding window scenario, in which the data of interest are the last m records for some pre-defined m (or the last records received within a certain period); for example, one may wish to continuously monitor only the last 1000 tweets in a tweet stream. The sliding window case corresponds to the turnstile model , in which the data vector X  X  entries can both increase and decrease, and is more general than the cash register model, in which the entries can only increase.

In all the experiments, CB outperformed the other methods in both communication reduction and run-time, with the improvement factor in run-time being orders of magnitude for PCC, cosine similarity, and PCA-Score. We used three data sets: the Reuters Corpus (RCV1-v2, REU), a Twitter crawl (Dataset-UDI-TwitterCrawl-Aug2012, TWIT), and the 10 percent sample supplied as part of KDD Cup 1999 Data (KC). The overall sizes of these data sets were: REU 374MB, TWIT 691MB, KC 46MB.

REU consists of 804,414 news stories, produced by Reuters between August 20, 1996, and August 19, 1997. Each story was categorized according to its content. A total of 47,236 features were extracted from the documents and indexed. Each document is represented as a vector of the features it contains.

TWIT is a subset of Twitter, containing 284 million fol-lower relationships, 3 million user profiles, and 50 million tweets. We filtered the dataset to obtain only hashtagged tweets, which left us with 9 million tweets from 140,000 users. For each tweet, the dataset contains information about the tweet content, ID, creation time, re-tweet count, favorites, hashtags and URLs.

KC was used in the  X  X hird International Knowledge Dis-covery and Data Mining Tools Competition X . It contains information about TCP connections. Each connection is described by 41 features, such as duration, protocol, bytes sent, bytes received etc.

For all data sets, in order to simulate multiple streams, we distributed the data between the nodes in round-robin fashion. Results are presented for 10 streams, and in Section 5.3 we present some results for communication reduction for up to 1,000 streams (the reduction in computational overhead does not depend on the number of streams).
Next we summarize the main results of this paper  X  the reduction in running-time for monitoring the four functions discussed in Section 4. Then we briefly summarize the com-munication reduction results.
 In Fig. 6 we present a summary of the running times for GM and CB, on the various functions and data-sets; details are provided in Sections 5.2.1 to 5.2.4. Figure 6: Running times for a local condition check.  X  X N X  and  X  X N X  stand for previous methods to moni-tor PCA-Score (see Section 5.2.4). Note logarithmic scale.
We evaluated PCC on REU, where every document may be labeled as belonging to several categories. The most frequent category is  X  X CAT X  (the  X  X ORPORATE/INDUSTRIAL X  category). In the experiments our goal was to select features that are most relevant to this category, i.e. whose PCC with the category is above a given T . Each node holds a sliding window containing the last 6,700 documents it received (this is roughly the number of documents received in a month). We monitored the correlation of  X  X CAT X  with the features  X  X osnia X  and  X  X ebru X .
 Run-time evaluation. The majority of GM X  X  run-time is spent on testing for sphere intersection with the PCC surface. To solve this problem we used the Gloptipoly global opti-mization package [16]. In CB, the local conditions for PCC monitoring are very simple, and only require computing the functions composing the PCC and their derivatives (Section 4.1).

The experiments demonstrated that the run-time of check-ing the local condition a single time, for the CB method, is almost four orders of magnitude lower than for GM (see Table 1). Note  X  to reduce space, the tables also include results for inner product and Csim 5 .
 Table 1: Run-time for checking the local condition CB vs. GM.
We monitored the inner product on REU and TWIT. As in [12], we calculated the inner product of feature vectors from two streams (created by splitting the records). For REU, we used the top 2050 features left after removing features which appear in less than 1% of the documents. We used the NLTK [5] package to tokenize and stem the tweets in TWIT, and then selected the top 1250 features, ignoring features appearing in less than 0.1% of the tweets.

In the REU experiment, each node held a sliding window of the last 6,700 documents, while in TWIT each node held a sliding window containing the last 1000 tweets. We used threshold values between 7000 and 17000 for TWIT, and between 4.9E7 to 5.5E7 for REU.

Run-time evaluation. Although GM requires no optimization to find the closest point on the surface, but only to solve a quartic equation, CB checks local conditions about 20 times faster than GM (see Table 1); this is due to the time required to construct and solve the equation, and then check the distinct solutions to see which one yields the closest point. Checking the local conditions requires more time for the REU, since the feature vectors are longer (2050 vs. 1250).
To evaluate the computational overhead for the cosine similarity function, we monitored both REU and TWIT. Data was the same as for the inner product experiments (see Section 5.2.2 for more details).

Run-time evaluation. The run-time of checking a local condition a single time in GM is almost 3 minutes, while for CB it is less than 0.2 milliseconds (See Table 1). In the PCA-Score experiments (Section 5.2.4) we compared CB to three different methods, hence the results are provided separately; see Table 2. For monitoring the PCA-Score function, we compared CB with GM as well as methods based on the Frobenius norm (FN) and spectral norm (SN) perturbative bounds, described in [17] (see also Section 4.4). All methods except CB require solving complex optimization problems, which were implemented using Matlab and the CVXOPT package [1].

We monitored the PCA-Score over KC using 10 nodes, each holding a sliding window of the last 100 feature vectors. We ran experiments with threshold values T ranging between 0.8 and 0.95, and effective dimension values ranging from 3 to 6.

The experiments show that the three methods which were compared with CB  X  SN, FN and GM  X  offer a trade-off between communication cost and run-time.

GM achieves the best communication cost of the three but is also the slowest method. FN is faster than GM but its communication cost is slightly higher. SN is the faster of the three by far, but it achieves relatively poor communication reduction. CB improves on all three methods, achieving better communication cost than GM and better run-time than SN.

Run-time evaluation. Run-time results for monitoring the PCA-Score are displayed in Table 2. The table shows average run-time of a single round of each method the as well as the speedup factor achieved by CB.

CB is about 3 times faster than SN, two orders of magni-tude faster than FN, and three orders of magnitude faster than GM. Note that while SN X  X  runtime results are better than GM X  X , it achieves a rather poor reduction in communi-cation (Fig. 7) CB speedup 1 3.20 232.81 1105.95 Table 2: Run-times (seconds) for monitoring the PCA-Score over KC.
While the work presented here focuses on reducing com-putational overhead, we also briefly provide results on its performance in reducing overall communication. To eval-uate the communication cost, we measured the number of messages sent. The naive method, in which every message is sent to the coordinator, is used as a common baseline. At the opposite extreme, we compared to a hypothetical algorithm, which alerts only when the threshold condition is locally violated, i.e. when f ( v i )  X  T for some local vector v . Clearly, every monitoring algorithm will have to alert in such a case. However, to maintain correctness, the local conditions have to adhere to the more restrictive constraint f ( algorithm are more restrictive, it will issue more alerts, lead-ing to a higher communication cost (unless, of course, f is convex). We refer to this super-optimal bound  X  the number of local violations  X  as RLV (real local violations); if the ratio between the number of messages sent by a certain algorithm and the number RLV sent is close to 1, then this algorithm can hardly be improved.

Figure 7 shows a summary of the communication required by CB, GM, and RLV for the functions we studied as well as SN and FN for the PCA-Score function. Each bar represents the results across multiple thresholds and datasets. CB is always better than GM. In most cases CB is close to the super-optimal lower bound RLV, meaning it can be hardly be improved. Note that while FN and SN achieved better runtimes than GM (Table 2) they have higher communication costs. CB did better than the competing methods (GM, FN, SN) in both runtime and communication costs. Figure 7: Communication reduction summary (Lower is better). y -axis is ratio to naive. Each bar represents the results across multiple thresholds and datasets. The SN bar is cropped (actual ratio is 1.4)
We also tested the effect of the window size on the com-munication cost (Fig. 8). The results can be explained by the slower change in the function X  X  value when the window size increases, thus making the monitoring problem easier.
Rat io to naive Figure 8: Communication cost as a function of win-dow size for Inner-Prod on TWIT (lower is better).

To test the scalability of CB, we ran experiments with up to 1,000 nodes. Fig. 9 shows the results. The advantage of both CB and GM (and RLV) over the naive grows with the number of nodes, while CB maintains its superiority over GM.
Ra tio to naive Figure 9: Relative communication cost for up to 1000 nodes (lower is better).
We presented a new method, CB, to monitor threshold functions over distributed streams. The novelty lies in that the monitoring takes place directly over functions, as opposed to previous methods which require solving very difficult opti-mization problems.

We presented a general paradigm for implementing CB and demonstrated its superiority over previously known methods for four important functions, achieving one to six orders of magnitude run-time improvement, while also reducing the communication cost.

With the move towards the Internet of things, smart-home, smart-cities etc., the deployment of resource-constrained de-vices is expected to exponentially increase. Systems com-posed of these devices will have to perform complex monitor-ing tasks in real-time; hence, the need for computationally efficient solutions, such as the one presented here, is expected to increase.

Future work will concentrate on further applications, as well as on more theoretical directions, e.g. studying alterna-tive methods to  X  X onvexize X  monitoring problems.
The research leading to these results has received funding from the [European Union X  X ] Seventh Framework Programme [FP7-ICT-2013-11] under grant agreement N  X  619491 and N  X  619435 and from the European Commission Horizon 2020-the Framework Programme for Research and Innovation (2014-2020) under grant agreement N  X  688380.
 The authors are very grateful to four anonymous reviewers. All their remarks will be incorporated in the planned journal submission.
