 1. Introduction
The Web information age has brought a dramatic increase in the sheer amount of information, the access to this infor-mation, as well as the intricate complexities governing the relationships within this information. Nowadays, users tend to prefer personalized information that is easily delivered, without much hassle, to them. The aforementioned facts, probe for new interconnection architectures and transferring of data. Following this path, several interconnection protocols have risen in the last few years: XML/XSL [6], SAX [18] and DOM [5].

Based on the fact that many information retrieval (IR) tasks, such as classification, summarization or clustering rely heav-ily on keyword  X  oriented information originating from the source texts, it is easy to conclude that keyword extraction is a key step for them. In essence, keyword extraction, aims to select the appropriate keywords out of a text, accompanying them with a suitable score that depicts their importance. By appropriate, we mean the most representative words, as far as the text X  X  overall meaning is concerned. Following the keyword extraction procedure, text summarization and categorization techniques come. Both of them operate on a keyword basis, working in a bipartite interactive way [3]. Moreover, user pro-filing serves another significant role in our system allowing a continuous feedback from the users, enhancing in a secondary stage the resulting information. In our research, a unified, yet autonomous system is developed, PeRSSonal [16], in which keyword extraction, summarization categorization and personalization are the core procedures. The system is stable enough for everyday use through its portal website [16] .

We are focusing on news articles served by the numerous portals from around the internet. This overwhelming amount of data deprives the users from easy access to unbiased, high quality information and this is the terrain of the proposed mech-anism. We are thus researching the usage of multiple data mining and retrieval techniques which are incorporated to the proposed system. This is exactly the key feature that distinguishes the to-be presented approach from similar ones like [19,8,14] . The contribution of the current article is twofold: the evaluation of the effect that part of speech (POS) tagging has on text summarization, and the presentation of the applied personalization algorithm with regard to the content delivery to the user X  X  desktop.

In Section 3 we give a brief description of the developed system. In Section 4 the architecture of the complete system is pre-sented. Section 5 outlines the algorithm procedures of noun retrieval and personalization that are followed by the system. In
Section 6 we present the evaluation results of the developed mechanism. Section 6 states the overall conclusions of this arti-cle, while Section 7 presents some thoughts for future additions to the system. 2. Related work
Automatic part of speech tagging is a well-known problem that has been addressed by several researchers during the last 20 years. It is a firm belief that when it comes to keyword extraction, the nouns of the text carry most of the sen-tence meaning. In a sense, extracted nouns should lead to better semantic representation of the text, and hence, im-proved IR results. Noun extraction, a subtask of Part of Speech (POS) tagging, is the process of identifying every noun (either proper or common) in an article or a document. In many languages, nouns are used as the most important terms (features) that express a document X  X  meaning applied by natural language processing (NLP) techniques used widely in information retrieval, document categorization, text summarization, information extraction, etc. Various methodologies have been proposed making use of linguistic [12], statistical [4], symbolic learning knowledge [17] or support vector ma-chines [9] and can be categorized to: morphological analysis, or POS tagging based. The former methods try to generate all possible interpretations of a given phrase by implementing a morphological analyzer or a simpler method using lex-ical dictionaries. It may over-generate or extract inaccurate nouns due to lexical ambiguity and shows a low precision rate. On the other hand, the POS tagging based methods choose the most probable analysis among the results produced by the morphological analyzer. Due to the resolution of the ambiguities, they can obtain relatively accurate results. How-ever, they also suffer from errors not only produced by the POS tagger, but also triggered by the preceding morpholog-ical analyzer.
 move to personalization is no longer an option, but a necessity. Several challenges however come into place for the person-grading multiple sources of data, as well as privacy issues are only some of the aspects that our system faces up. From an tering, and collaborative filtering systems.

Personalizing news feeds is an interesting subtask of news filtering and personalization that has emerged in the few last in [1], the authors make use of a machine learning classification framework to filter news following the user X  X  choices. An-other technique for adaptive news, which is based on user modeling, is presented in [19], where the system maintains sep-arate user models for each topic of news. However these systems lack serving of news summaries and use fairly trivial keyword extraction techniques. Also, they expect from the users to explicitly modify their profiles which is often a tedious task. Another interesting approach is presented in [8] where the notion of information novelty is utilized. Despite the fact that the implemented algorithms perform well, they also lack automatic recording and prediction of user preferences which change frequently. An alternative approach is researched in [14] where the system produces multi-document summaries that could be combined with the resulting summaries, as in our work.
 Presenting to the user summaries matching their needs is a very crucial procedure that can assist information filtering.
Even though automatic text summarization dates back to Luhn X  X  work in the 1950s [13], several researchers continued investigating various approaches to the summarization problem up to nowadays. A summary [20] usually helps readers identify interesting articles or even understand the overall story about an event. Most of the time, the summarization ap-proaches are based on a  X  X  X ext-span level X  [10], with sentences being the most common type of text-span having each of them lem to a simpler one: ranking sentences according to their salience or likelihood of being part of a summary, concatenating them at a second stage. Some techniques [7] try to identify special words and phrases in the text, while in [11] the authors compare patterns of relationships between the sentences.

Several text classification (categorization) approaches have been researched over the years: Naive Bayesian (NB), K-Near-est Neighbor (KNN), and Centroid-based (CB) techniques are some examples. Linear Least Squares (LLSF) [21], a multivariate regression model that is automatically learned from a training set of documents and their categories, gives good results and is utilized in our work.

In this article we present the incorporation of noun retrieval techniques in PeRSSonal, using the support vector machine (SVM) method for POS tagging, as part of its keyword extraction algorithms, and we explore, through experimentation, the possible improvements this change has on the mechanism X  X  IR procedures: summarization and categorization. Furthermore, we are dealing with the effective and adequate presentation of personalized news summaries from articles that derive from the WWW to the user X  X  desktop. We present the personalization algorithm that is used for presenting the pre-categorized and summarized articles to the user X  X  desktop application which is capable of exchanging information with our already ma-ture categorization and summarization system. Our personalization approach is mainly content-based with some collabora-tive filtering features by enhancing the algorithm with the ability to automatically adopt over time to the continuously changing user choices. Furthermore, we base our summarization procedure (enhanced by the personalization module) on the TF X  X DF term-weighting model. 3. System description: PeRSSonal
PeRSSonal [16], the automatic summarization, text categorization, personalized syndication system, applies several data mining techniques through a layered infrastructure that achieves filtering of information and adaptability to the user. We focus on news articles that are gathered from numerous news portals from around the internet and we are targeting to the alleviation of the end-user from the cumbersome task of searching through this overwhelming plethora of information.
PeRSSonal initiates a new sense of news exchange over the WWW,  X  X  X eta-portals X , which aims to consolidate and index news content from a multitude of sources. The proposed architecture however does not merely index and serve static con-tent, but it incorporates surplus value by categorizing, summarizing and personalizing its content. Moreover with the use of the client-side module, the user is not anymore overrun by unnecessary content, keeping thus the network traffic to a minimum.

The user participates in the procedure by explicitly defining his/her preferences or by allowing the system to automat-ically adapt to his/her dynamically changing profile. This generates the notion of the  X  X  X asily modified X  user profile which is used extensively by the proposed approach. 4. Architecture
PeRSSonal follows a classic n -tier architectural approach. The system consists of four layers which work autonomously and collaborate through a centralized database. The procedure that is followed, as depicted in Fig. 1 , starts with the inter-connection between the mechanism and the web sources and it is where the primary tasks take place. These include: the content fetching procedure, the analysis of the downloaded content and finally the extraction of the useful information from the web content. In order to capture web pages, a simple focused web crawler is used. The crawler takes as input the ad-dresses that are extracted from existing RSS feeds, deriving from several major news portals. These RSS feeds point directly (e.g. pre-categorized feed). The crawling procedure is distributed across multiple systems which synchronize thought the centralized database.

The various layers of the PeRSSonal server-side mechanism work autonomously and collaborate through the centralized database. Crawled HTML pages are analyzed and are stored without any other unnecessary page element (images, css, java-only the useful text, as well as some other page meta-data, such as URL and insertion date, the database is populated with news articles that are ready for the text preprocessing step.

The second layer of the system, which is the focus of this article, operates on the article X  X  title and body applying several preprocessing techniques. Text preprocessing is probably the most important preceding task of any text-based
IR technique and hence our approach pays much attention to the various algorithms that are used. In particular, after the article X  X  language is recognized either directly through language identifying procedures, or indirectly using the pre-determined language of the origin-feed. Following is a sentence separation and punctuation removal step. Afterwards, the noun identification step takes place which, by utilizing the POS SVM-based tagger [9], is able to determine with high precision the article X  X  nouns. Some common text extraction techniques follow: stopwords removal and stemming. Noun extraction should precede these procedures if it is to succeed with high probability. It is important to note that the noun identification, stopwords removal and stemming procedures are language dependant, meaning that specific language rules, stopword lists and stemming rules respectively, have to be applied for different languages. The above set the foun-dations for multi-language support by our mechanism, even though only the English language has been incorporated so far. The results of the procedures described in this layer are stemmed keywords either marked as nouns or not, their location in the text and their frequency of appearance in it. These are represented through term frequency  X  inverse document frequency (TF X  X DF) vector statistics that are stored in the database and are utilized by the procedures of the third analysis level. The aforementioned keyword extraction subtasks are to their majority, language dependant, meaning for example that different stopwords lists, stemming rules and noun retrieval algorithms are used for every different language.

The information retrieval tasks of our mechanism are located in the third analysis level, where the summariza-tion and categorization algorithms are applied. The main scope of the categorization module is to assist the sum-marization procedure by pre-labeling the article with a category and has proven in [3] to be providing better results, as far as summarization is concerned. Following the IR tasks of the system, personalization algorithms take place. The personalization module that is also described in this article is easily adaptable to the user meaning that, small changes to the user X  X  preferences as expressed by his/her browsing behavior are detected, adjusting thus the profile. Our personalization algorithm uses a variety of user-related information in order to filter the results pre-sented to the user.

Finally, the content is delivered to the user using XML formatting for data content and XSD schemas for the transmitted data. The personalization procedure on the server-side, assisted by the feedback information originating from the user X  X  behavior, as well as the delivery of the summarized content to the user X  X  desktop through the client-side application are extensively covered in the current work. 5. Algorithmic aspects
Our analysis consists of the following different algorithmic steps: extraction of keywords and identification of nouns, cat-egorization and personalized summarization procedures and finally, delivery and presentation of the information to the user X  X  application. 5.1. Keyword extraction and noun identification procedure The input to the keyword extraction module is plain text that defines the article X  X  body and title as well as its language.
Apart from the previous, some parameters have to be tuned in order for the mechanism to be the most efficient: (a) mini-mum word length (all words with length smaller than the minimum are removed) and (b) the language dependant stopword articles written in English are concerned.

Noun identification involves an off-line learning step for the POS tagger using language specific rules. Previous to the tag-ging, SVM models (weight vectors and biases) are learned from a training corpus using the learning component. A modified ing is complete, the article X  X  body is forwarded to the tagger and the text X  X  nouns are marked. Stopwords removal takes place and stemming rules are applied, resulting in the TF X  X DF vector for all the texts and their terms. 5.2. Categorization procedure The categorization subsystem is based on the cosine similarity measure, dot products and term weighing calculations.
The system is initialized with a training set of 1500 pre-categorized articles, belonging to seven different basic catego-ries. Those categories are the basic ones that are served by most of the news portals online. The amount of articles is enough to give an adequate knowledge base on which text classification can proceed. The categorization module receives as input the extract of the preprocessing mechanism, which is: (a) stemmed keywords, (b) noun-related information, (c) absolute and relative frequency of the keywords appearance in the article and (d) the article X  X  title and body. After the initialization of the training set, the categorization module creates lists of keywords-nouns that are representative of a unique category, consisting of nouns with high frequency at a specific category, and small or zero frequency for the others.

The categorization attempt of a recently fetched article resembles the LLSF method and proceeds as follows: firstly we generate the list of the representative (stemmed) keywords of the text together with the frequencies evaluated by the pre-processing mechanism as depicted in Table 1 . Following this we produce identical keyword-frequency lists for all the cate-gories that reside in the database and which consist of the same keywords followed by their frequency into the category (Table 2 ).

In order to determine the text X  X  category, we examine the cosine similarity of the text and the categories based on the aforementioned lists.

An article is most of the time related with a similarity measure to more than one category. However, for a categorization result to be accepted we define two thresholds: (a) the cosine similarity between the text and the category should be over
T exceed T hr2 . Experimentation, gave us the best suited thresholds for T tively. If T hr1 or T hr2 is not met, the article is forwarded to the summarization module and the resulting generic summary is used as input to a second categorization attempt for the article. Should (with the second categorization attempt) the above thresholds now be met, the labeling of the summary is kept, while at a different case, when neither the initial nor the sec-ondary attempt satisfy the thresholds, the initial labeling of the article is kept. 5.3. Summarization procedure
During the summarization procedure, we utilize three factors: (a) the existence of a keyword in the title (b) the frequency of a keyword and (c) the noun tagging information of a keyword. We call these factors k with very high frequency in the text is considered to be representative of it and thus, any sentence that includes it can be one, so the sentences that include it are more representative. Moreover, when a keyword is tagged as a noun, we consider it significant thus boosting it with some extra weight. Parameters k experimental procedure we have concluded values for k 1 and k equation: where z  X  0 if the keyword is not a noun and z  X  1 if it is. L conveys the desired extra weight that a noun existing in a sen-tence should have. Experimentation with various L values revealed that L should be no more than 1.5 or else sentences with few keywords-nouns receive low scores, compared to sentences with many nouns, and are substantially excluded from the summary. Typical values for L range from 0 to 1 with the former depicting that the summarization algorithm is not taking into consideration the noun relevant information.

Based on these heuristics, we create a summary which consists of the most representative sentences of the text. In order to determine these, we deploy a score for each sentence according to the factors k sentences where i  X  X  1 :: s and f keywords where k  X  X  1 :: f , each sentence i is assigned a score according to the following equation: where rel ( fr ( kw k,i )) is the relative frequency of the keyword k in sentence i .

After creating a generic summary, we retry to achieve a categorization, as the summarized text is more refined and con-sists only of important sentences rather than the whole text, which may include sentences with keywords that are distract-ing the categorization procedure.

The procedure that is followed in order to summarize a text after a successful categorization differs from the aforemen-tioned steps due to the fact that another factor is included in the scoring.

As the summarization procedure of our module is based on the selection of the most representative sentences which are selected by weighting them appropriately, the categorization outcomes can be helpful in adjusting more effectively the weighting of the sentences. Common sense implies that a keyword that has very high frequency for a specific category, should give more weight to the sentence in which it appears, while a keyword that has small or zero frequency for a category could add less to the weight of a sentence. This factor, namely k which the document belongs. More specifically, k 3 can express the positive or negative effect that a keyword X  X  category has on the summarization procedure and is automatically tunable by the core system processes. As long as the text is catego-rized, we can utilize this factor in order to create a more efficient summary. With the use of k tion is depicted in Eq. (3).
 5.4. Personalization using user feedback
The developed mechanism is based on the continuous feedback from the user. The steps that are followed by the personalization procedure are presented in Algorithm 1 . When a new user is registering to the PeRSSonal service, she states the keywords of his/her preference as well as the scores that describe this preference initializing thus his/her profile. This procedure is trivial and can be avoided altogether since the personalization subsystem keeps track of the user X  X  choices and browsing history, and so the user X  X  preferences are updated on each visit. The user X  X  profile consists of two keyword lists: a positive one, where the user-preferred keywords are placed, and a negative one where uninteresting keywords for the user are kept. By using these lists, the summarization procedure can personalize the summaries with exceptional results. Note that a given keyword can only affect a user X  X  profile in a positive or negative manner, but not both ways. We are thus weighting the overall influence of each keyword with a positive or negative outcome.
 Algorithm 1. Personalization algorithm that utilizes user feedback.
 Update _ profile (a, b, c) { }
Get _ article(lists) { //Recovers from the database browsed articles //and the amount of time spent reading the full article or // its summary (a,b) Recovers also the negative articles (c) }
Update _ list(list, keywords) { }
The profile update procedure, running constantly at every user X  X  visit, takes note of the following aspects: (a) the browsed articles (the ones that the user selected to view), (b) the amount of time a user spends viewing the summary or the full text from the simple logical assumptions that follow. A user will most likely spend an amount of time above a certain threshold,
R an upper bound, R ar_thr2 and R sum_thr2 , should be used for these metrics since we donot want the mechanism to mistake for-gotten browsed articles for the really interesting ones. The thresholds that are used for R respectively, defining thus which article X  X  keywords should be added (or have their weight increased) in the user X  X  positive keywords list. Note that even though an article X  X  length may be varying, we observed that it is quite uncommon for a user to spend over 3 mins reading the same full text article. Consequently we chose the R The summary viewing thresholds are calculated in the following way: Where S ratio expresses the summarization  X  X  X ompression ratio X : the article X  X  title or summary. Lastly, a user will probably avoid visiting articles that she finds uninteresting and thus the keywords that represent those articles should be receiving a lessened or negated weight ( factor c ).

From the above factors, the personalization algorithm keeps track of the keywords that the user has expressed preference to and thus, the articles (containing these keywords) that she is likely willing to read in the future. The parameter that de-picts the user X  X  preference for a keyword according to the aforementioned factors (a X  X ) is U frequency that the keyword has on the list, a frequency that is constantly modified by the user X  X  choices. U word of sentence i , derives from the following equation: thermore, we expect that when the user profile reaches its steady state, the mean times of the keywords preferences will be correct, hence depicting the overall user preferences.

The overall personalization factor for each keyword i , named k to the negative keyword list, then B &lt; 1. Of course the norm of the B parameter can take any large or short value that we desire, thus increasing or decreasing at will the effect that personalization and dynamic profile generation have on the sen-tence weighting procedure. From the previous, j B j &gt; 1 and thus k where no information about the user X  X  preference of the specific keyword is provided.

We could depict the overall user profile as a vector in a multi-dimensional space (sized as per the total keywords) con-sisting of all the k 4 parameters for each keyword. Each user profile has a point, direction and magnitude in the keywords-space. The direction pinpoints the keywords that do actually have an effect on the user X  X  profile, while the magnitude ex-presses the measure of this effect. This vector is constantly changing as keywords X  profile is modified by the user choices. Fig. 2 gives a graphical representation of two user profiles as expressed by their preference for three keywords.
From the previous, the overall weighting formula (3) (with the personalization factor) of the k th keyword of sentence i , becomes as follows: 5.5. Client-side application and content delivery
According to the personalization features of the mechanism that were described earlier, the server transmits the re-sponses to the client using the XML communication protocol derived from the existing XSD schemas. Following the delivery of the content, a number of features are available to the user. Users can browse through the four basic information  X  X  X hannel X  The summaries of the selected articles are loaded and sent to the application screen. Concurrently, information about the a modular manner, since the GUI is dynamically created using the existing XSD schema. For example, should a future version of PeRSSonal include a new channel, the client application will recognize the addition offering the user the choice to place the new module according to his/her desire.

It is also important to note that information is loaded in a multithreaded manner by the client. Having available informa-tion about the user X  X  profile and browsing habits, we are able to pre-fetch articles, summaries and relativity information be-
PeRSSonal, since it makes it more responsive and usable. For instance, consider the following scenario: a user X  X  profile con-tains many keywords from the politics category with high preference (as explained in Section 5.4) and from the fetched  X  X  X atest X  channel X  X  articles, some of them are related with high similarity to this category  X  more precisely, the similarity check is done upon the keywords, so an article belongs with high similarity to a category when the majority of its keywords belong to this category, see [3] (categorization subsystem) for more details  X  the article X  X  summary and relativity informa-tion are automatically loaded and stored for future use.
 6. Experimental procedure and results
The current section presents the experimental evaluation of PeRSSonal with the application of noun retrieval techniques as well as the personalization algorithm that was presented earlier. Moreover, we evaluate the desktop application (presen-tation subsystem) in order to detect its acceptance level by the system X  X  users. For our experimentations we blocked any unrelated access to the system X  X  web interface so as to be sure that the results observed are accurate. 6.1. Noun retrieval effects in summarization
In order to evaluate the summarization performance of PeRSSonal, with the appliance of noun retrieval techniques, we conducted two sets of experiments. Firstly, we tried to determine the best possible value for the L parameter of Eq. (1). Fur-thermore, we tried to evaluate the effect of the appliance of the noun retrieval algorithm explained earlier, to the overall system performance using classic IR measures. For conducting the experiments we used a corpus of 3000 news articles that where obtained from major news articles portals from around the Web (namely: CNN, BBC, Reuters, MSNBC, ABCNews,
Washington Post and Guardian). The articles belonged with high relevance to one of the seven major categories of the sys-tem, and this information was used as explained in Section 5 (pre-categorized articles) in order for the summarization pro-cedure to produce the best possible summary. The categories that the system served for our experimentation were: business, entertainment, health, politics science, sports and education. We used a same article amount for each of the system X  X  cate-gories so as not to bias any specific one.

As reported earlier, the parameter L is deployed for controlling the effect that noun retrieval has on the article summa-rization procedure. We conducted experiments tuning L in order to decide on its best value as far as news articles, which is the case of PeRSSonal, are concerned. The various results are presented in Fig. 3 .
 attenuate both the precision and the recall of the summarization procedure compared to the L  X  0 case, i.e. when noun re-trieval information is not used. This intuitively means that, when sentences that contain mostly nouns are kept at the sum-marization procedure (i.e. large L values), excluding the rest of the sentences, the effectiveness of the summarization procedure slightly deteriorates. However, finding a golden section for the L parameter, which is dependable on the target texts, can enhance the summarization efficiency significantly. The value of L that we observed to give high precision/recall as to why some category should benefit more that someone else when weighting its nouns with a higher or lower L param-mails or published papers) would give similar results. The same applies with the case of more article texts since the observed result was with an experimental training set of 3000 texts; however we believe that the observed L values can definitely give a good estimation even for a larger article set.

The aforementioned improvement to the summarization results is also obvious at the following graph, where precision and recall results are depicted (using an L value of 0.6) when summarization proceeds with and without the noun retrieval information.

From Fig. 4 it is concluded that the noun retrieval information can give a notable precision boost to the resulting sum-maries compared to the case where noun retrieval information is not utilized; in other words, the resulting summaries are more precise. As far as recall is concerned, the improvement is small, yet significant, taking into account the fact that a text X  X  summary represents a layer of abstraction, notably a low recall representation of the original text X  X  information; expecting thus high recall improvements would not be wise. 6.2. Evaluation of the personalization and presentation subsystem
In order to evaluate the possible enhancement of the proposed personalization mechanism on the system X  X  procedures, and more specifically, on the summarization subsystem, we conducted a set of experiments. We also experimented in order to determine whether the client-side desktop application is actually a nice equivalent to the web interface.
For our summarization evaluation approach, we used classic precision X  X ecall metrics and 15 university students. We first asked our test users to register to the system and use it (through its web interface and the desktop application) for one month X  X  period so that the personalization algorithm fully adapts their profile to their preferences. Afterwards, we provided them 50 full text articles that were matching their created profiles and we asked them to rate some sentences of these arti-cles for a suitable summary of the article. We also produced the personalized as well as the  X  X  X eneric X  summaries of these articles (without utilizing the personalization features explained in this article) and compared them with the users X  choices factor which are based on the generic summaries that the system generates for the given articles. Extracted summaries are then compared with the sentences selected by the users and precision as well as recall metrics are evaluated.
From Fig. 5 it is deducted that the appliance of our new personalization scheme has provided a significant benefit to the overall summarization performance of the mechanism as far as precision and recall metrics are concerned. We mea-sured this increase to be around 17% for precision and 14% for recall. It is important however to note that the statistics are based on the user choices and are subjectively biased by nature. On the other hand though, there are no real objective criteria for the extraction of a summary from a text and it is this  X  X  X ias X  that the proposed personalization mechanism is trying to estimate.
 As already explained, the system features two ways for presenting information to the user: the Web interface and the
Desktop application. Figs. 6 and 7 give a depiction of both. Note that the presented amount of actual news-related informa-tion is equal in both of the cases.

Moving to the next set of evaluation, we asked our users to utilize both the web interface and a beta version of the client-side application of PeRSSonal for a period of 30 days. We then asked them to rate both of the presentation systems in terms of: (a) usability (is the system serving enough and good summaries? Is everything reachable within a few mouse-clicks? Is the displayed information easily understood?) and user-friendliness, (b) performance and interactivity (are response times good?) and (c) efficiency and briefing in content representation. The rating was done with a scale from one to ten, with ten being the best.
 for the presentation subsystem of PeRSSonal. It is clear though that some users thought low of the desktop application in is concerned. This is however expected since (a) to our knowledge this is the first desktop application that focuses on such information retrieval and personalization tasks, (b) taking into consideration its compact representation of quite a big amount of information.
 Furthermore, as far as performance and interactivity are concerned, the desktop application outruns the web interface.
This originates from the caching and pre-fetching techniques that the desktop application makes use of. Lastly, efficiency and briefing in content representation, a key target of the PeRSSonal system is an aggregation of the previous factors and shows that despite the fact that the client-side application is still under development, the novel features that it provides are considered overall useful by the users. Even though the above results depend on highly subjective user choices, they give that we should look into. 7. Conclusions and future work
In this article we explored the effects that noun retrieval techniques, based on POS tagging, can have on information re-trieval mechanisms and summarization in specific. We have also outlined the personalization algorithm that our system uti-lizes for presenting pre-categorized and summarized articles to the user. Furthermore, we presented the communication channel that PeRSSonal uses for delivering content to the end users, as well as the desktop application, which is capable of exchanging information with our mature system. Through the proposed framework that is utilized in an existing system,
PeRSSonal, we are able to improve the summarization procedure by simple modifications to our keyword extraction algo-rithm. Our personalization approach is mainly content-based with some collaborative filtering features adopting over time to the continuously changing user profile.

We conducted experimental procedure in order to evaluate the overall improvement of PeRSSonal X  X  summarization capa-bilities with the appliance of the new personalization algorithm, as well as the new keyword extraction capabilities. We used real system users and even though the evaluation of a summarization system is a difficult and subjective task, we discovered a significant amendment. Moreover, we evaluated the developing client-side desktop application and explored the signifi-cance of developing such a communication infrastructure from the point of the end user. The results are encouraging and express the users X  need for efficiency and interactivity from an application that is targeted not as a replacement, but as a complement of the Web 2.0 technologies that are utilized by the system. The efficiency improvements concerning the ap-plied noun identification technique are small, yet significant, considering the fact that summarization is a difficult, mostly subjective procedure and that objective criteria of efficiency are difficult to appoint.

Having incorporated noun retrieval techniques, as far as the core procedures of the system are concerned, we intend to incorporate multilingual support, as well as support for multimedia and improved caching features. The addition of such features to PeRSSonal will require a basic redesign of the main parts that constitute the system. Furthermore, we are focusing on a stable version of our desktop application and on a wider evaluation of PeRSSonal. Also, we consider a wider evaluation of the improvements that the applied noun retrieval technique has on both the summarization and the cate-gorization procedure, as well as evaluation on how the system behaves with regard to different paces of user preference changes.
References
