 1. Introduction
Many information retrieval (IR) applications such as information extraction, question answering and dialog systems re-quire user queries to be congruent with the documentary databases we are exploiting. In this sense, although formal IR mod-of accuracy in typing; and errors resulting from noisy generation tasks, usually deriving from texts written and published presence can substantially hinder the performance of IR applications.  X  spelled queries arising from the phenomenon of globalization, led by increased access to information and the widespread the appearance of specific proposals both with regard to language ( Hagiwara &amp; Suzuki, 2009; Magdy &amp; Darwish, 2008; foresee the inclusion of mechanisms for managing misspelled queries of this nature during the design stage of IR tools ( Konchady, 2008 ).

From a practical point of view, most significant experimental examination seems to be limited to texts written in English that while baseline IR can remain relatively unaffected by misspellings, relevance feedback via query expansion becomes misspelled queries.

The operational basis for the treatment of misspelled queries consists of replacing the original string matching algorithm inputting the initial query.
 a dictionary. We can here distinguish two forms of spelling correction problems: fail to detect real-word errors , i.e. errors that produce another word that is also valid. An example would be the query  X  X  X ord swimming championships  X , which contains a misspelling of  X  X  X orld  X ; this would not be detected because each indi-vidual term in the sentence is correctly spelled in isolation.
 error case and the correction of non-word errors that have more than one potential correction.
 The second strategy is to consider a technique based on the use of character n -grams ( McNamee &amp; Mayfield, 2004a; the need for dictionaries.

In order to study the validity of these strategies and make the relevant comparisons, a testing framework has been formally designed. To the best of our knowledge, no relevant in-depth work of this kind has been previously documented.
This testing framework allows us to study the influence, if any, of whether or not linguistic information is taken into account. We consider three incremental levels: the total exclusion of linguistic information, the use of dictionaries alone and the additional integration of contextual information. This cline is paralleled in the sphere of computational complexity, thus enabling us to also evaluate the real impact of each strategy in terms of its cost. The consideration of Spanish as a case in point will allow us to estimate the validity of these strategies outside standard working frames for English.
 ing sections the experiments we have performed for testing the proposed strategies. First, Section 5 states the research objectives pursued, while Section 6 describes the methodology we have used for designing our experiments. Next, the results conclusions and proposals for future work. 2. The state-of-the-art
As previously stated, the state-of-the-art distinguishes two generic approaches ( Manning et al., 2008 ), commonly docu-these takes complete dictionary entries as the matching unit between the query and the database for the retrieval task, whilst the second one considers subwords instead. 2.1. The spelling correction approach Focusing first on entire dictionary entries, spelling correction is a well-known subject matter in NLP ( Mitton, 2009;
Reynaert, 2004; Savary, 2001; Vilares et al., 2004 ), often based on the notion of edit distance dealing with misspelled queries, the aim is to replace the erroneous term or terms in the query with those considered to be quality and computational complexity according to the strategy adopted ( Mihov &amp; Schulz, 2004 ).
Given that applications of this kind in IR should require fully automatic correction ( Agirre et al., 1998; Kukich, 1992 ), these methods can be extended to eliminate, as far as possible, any intermediate decision to be made by the user. One of the first attempts in this sense was to consider phonetic information when applying correction, assuming that misspellings phonetic hash for each term, in such a way that similar-sounding terms hash to the same value. These methods, known as reason for ruling out their use.
 linguistic criteria. So, term weighting functions may be introduced to assign importance to the individual words of a docu-guage model for correction can minimize the need for morphologically sensitive error repair.
 Other works interpret spelling correction as a statistical question, also known as the noisy channel model ( Kernighan, advantage, however, is the possibility of generating contextual information, which adds linguistically-motivated features ference in average precision in misspelled texts can be reduced to a few percentage points in comparison with properly-spelled ones ( Ruch, 2002 ). More appropriate for dealing with real-word errors, its success depends as much on the wealth a method incorporating a small number of simple learned correction rules can achieve comparable performance, although querying IR applications correspond to real-word ones ( Kukich, 1992 ), which would appear to suggest the need to have strategies of this kind available.

There are also some general considerations that should be taken into account when attempting to apply algorithms of this which any dictionary-based solution would appear to be hard to implement given the huge amount of terms and spheres of knowledgeto whichreferencewouldhave to be made ( Kukich,1992 ). This is the reason forthe introduction,withthe intention spelling correction strategies. Unfortunately, these methodologies lack effectiveness when dealing with rarely-used terms, uncommonmisspellings and out-of-vocabulary (OOV) words, 3 improveexistingqueryspellingcorrectionmodelsbased solelyon query-logsbyleveragingthe informationonthe webrelated to the query and its top-ranked candidate. However, although this technique seems to achieve some promising results, it should only be considered as a simple complement to more general and robust baseline correction models. 2.2. The n-gram based approach
We can consider two bases for the characterisation and manipulation of text ( Robertson &amp; Willett, 1998 ): on the one hand, the individual characters that form the basis for the byte-level operations available to computers, and on the other, the individual words that are used by people  X  in this work represented by the spelling correction approaches previously discussed. These basic units can then be assembled into larger text segments such as sentences, paragraphs, etc. n -Grams, however, provide an intermediate level that has advantages in terms of efficiency and effectiveness over the conventional character-based or word-based approaches to text processing.
 can split the word "potato" into four overlapping character 3-grams:
Character n -grams have been successfully used for a long time in a wide variety of text processing problems and domains, Beljanski, 2009; Cheng &amp; Carbonell, 2007, Tomovic  X  , Janic  X  ic  X  , &amp; Ke X elj, 2006 ).
In this way, n -gram based processing has become a standard state-of-art text processing approach, whose success comes from its positive features ( Tomovic  X  et al., 2006 ): Simplicity : no linguistic knowledge or resources are required.
 Efficiency : one pass processing.
 Robustness : relatively insensitive to spelling variations and errors.
 Completeness : token alphabet known in advance.
 Domain independence : language and topic independent.
 Such advantageous features have not been ignored by the IR research community either ( McNamee &amp; Mayfield, 2004a; on the use of compression and dictionary-reduction techniques in order to reduce the demand of the at-the-time expensive grams as index terms for IR applications is widely extended because of the advantages they provide, advantages directly de-rived from their very nature.

These systems typically utilize language-specific resources such as stopword lists, phrase lists, stemmers, decompounders, ument contents or language, it being a knowledge-light approach which does not rely on language-specific processing approach can be easily incorporated into traditional IR systems independently, for example, of the retrieval model being 2004a; Dolamic &amp; Savoy, 2008 ).

The second major benefit of using n -gram based index terms, and the one directly involved in the present work, is the robustness of this approach. This robustness comes from the redundancy derived from the tokenization process. Since every for working in noisy environments, since it is able to cope not only with spelling errors, but also with out-of-vocabulary
Mustafa &amp; Al-Radaideh, 2004 ), in contrast with classical conflation techniques based on stemming, lemmatization or morphological analysis, which are negatively affected by these phenomena. This feature is extremely valuable, not only matching 4 ( McNamee &amp; Mayfield, 2004a ).

The third major factor for the success of n -grams in IR applications comes from their inherent language-independent nat-ure. As explained above, they need no prior information about grammars for stemming, stopwords, or even tokenization. So, based matching itself provides a surrogate means of normalizing word forms and allowing languages of very different nat-count, particularly in the case of multilingual environments or when linguistic resources are scarce or unavailable.
However, the use of n -gram based indexing, as with any other technique, is not totally free of drawbacks, the main one being the need for higher response times and storage space requirements due to the larger indexing representations they corpora, the number of unique n -grams will be larger than unique words in the same text corpus for n &gt; 3. However, the main reason for such an increase is not the size of the dictionary, but the number of postings. During the indexing of the uments, since most of the unique n -grams will have already appeared. Nevertheless, this is not the case of the number of postings, which grows linearly in the number of documents throughout the complete collection, consuming most of the stor-age space ( Miller et al., 2000 ).

The logical choice for minimizing this problem would be to reduce the index by using some kind of direct or indirect pruning technique. In the first case, McNamee and Mayfield (2004a) propose as a possible solution the use of static index pruning methods ( Carmel et al., 2001 ). In the second case, an n -gram based stemming approach is proposed of each word are selected for indexing, attaining a similar index size to that of classical stemming-based systems. This so-called  X  X  X seudo-stem X  would be those n -grams of highest inverse document frecuency (IDF), i.e. the least frequent and most discriminatory. On the other hand, Savoy and Rasolofo (2002) propose just the contrary, the use of a stop-n -gram list for eliminating those most frequent and least discriminative n -grams. However, their list was not automatically generated, but obtained from n -grams created from a previously existing stopword list. This means that the system would become language-dependent, in this case for Arabic. Foo and Li (2004) used a similar manually created list for Chinese.

Nevertheless, the advantages of using n -grams as index terms seem to compensate for the drawbacks, since n -gram based retrieval has been successfully applied to a wide range of languages of very different natures and widely differing morpho-logical complexity. It has been used, for example, with most European languages ( McNamee et al., 2009; McNamee &amp; and others like Greek, Hungarian and Finnish; it being particularly accurate for compounding and highly inflectional languages. Moreover, although n -grams have been successfully applied to many other languages such as Farsi (Persian) and Japanese are characterized by being unsegmented languages where word boundaries are not clearly indicated by delimiters such as spaces, thus sentences are written as continuous strings of characters or ideographs. Thus, traditional
IR word-based approaches cannot be directly applied. In the case of Korean, however, the problem comes from its aggluti-native nature, where word stems are often compound words, resulting in a serious decrease of retrieval effectiveness when applying classical word-based indexing. In both cases the solution comes from using NLP techniques for segmenting the text the application of these techniques has several drawbacks. Firstly, they require large dictionaries and complex linguistic knowledge, not always available, which also require constant maintenance. Secondly, they are sensitive to spelling errors, spelling variants, out-of-vocabulary words and tokenization ambiguities. n -Gram based indexing solves these problems, attaining similar performance with a much simpler approach.

In conclusion, we can say that, over time, n -gram indexing has passed from being considered as a mere alternative index-other approaches should be measured against  X .

Other IR-related, but more complex, applications of n -grams are the use of skipgrams, and the use of subword translation for CLIR applications.

However, McNamee (2008) showed that skipgrams are dramatically more costly than traditional n -grams and, while per-forming reasonably well, they are not demonstrably more effective. Moreover, their application is much more complex than for regular n -grams, since they require considerable modiffications in the IR system. For these reasons their use here has been discarded.
 Finally, subword translation ( Vilares, Oakes, &amp; Vilares, 2009, chap. Character n -grams as text alignment unit: n -gram-level alignment of parallel corpora in different languages for query translation in that the Spanish source 4-gram -lech-corresponds to the English 4-gram based approaches to both the query translation process and the matching process.
 2.3. Formulation and discussion
The nature of the corpus under consideration conditions the way in which misspelled queries are dealt with. Its subject ficult to extrapolate results. This makes it advisable to study the problem of misspelled queries in languages with a more complex lexical structure.

With regard to the algorithms involved, if we exclude those auxiliary techniques whose fundamental interest lies in refin-and OOV words would appear to justify the use of context-dependent correction methods as well as n -gram based ones. This given their value as a point of reference since non-word errors account for the majority of misspelling queries.
The above justifies even further the choices we have made when designing the experiments for this work. Firstly, to com-tifies the use of Spanish, a much more complex language than English from a morphological point of view, whose morphological features will be discussed later in Section 4 . 3. Spelling correction techniques
We introduce and justify the spelling correction approach we consider in our testing frame. We will take as our starting word that are within a given edit distance threshold. 3.1. Isolated-word error correction: a global approach
Savary X  X  proposal forms part of a set of strategies known as global correction , and is based on a simple operational prin-hypothesis, 5 although the user may choose to associate an alternative specific weight. These operations must be applied recursively until a correct spelling is reached.
 be entries containing the prefix  X  X  X ra  X , e.g.  X  X  X rado  X ( obtain the word  X  X  X rado  X ( "field" ), and in the latter, because we could delete  X  X  X   X  and obtain the word  X  X  X ato  X ( transpose the  X  X  X   X  and the  X  X  X   X  to obtain  X  X  X arto  X ( "childbirth" by  X  X  X   X  and the  X  X  X   X  X y  X  X  X   X  to give us  X  X  X lata  X ( "silver"
By rendering error detection and location tasks unnecessary, global correction strategies ensure that no correction option is omitted, giving a robust performance in the event of multiple errors and/or those precipitated by a previous wrong cor-rection. This makes it easy to determine, on the basis of their edit distance from the misspelled word, which are the best candidates that from a morphological standpoint have a similar quality, i.e. when there are several words sharing the same closest edit distance from the original misspelled word. So, assuming discrete costs, not only  X  X  X ato  X ( previous corrections.

The price one has to pay for using this protocol is the excessive computing cost of the construction, whether total or par-a given moment. This is the case of the possible correction  X  X  X lata  X (  X  X  X   X  to obtain  X  X  X lata  X ( "silver" ) will never occur because the cost of each of the alternative corrections  X  X  X rado  X ( lem without having to perform any kind of edit operation on the letter  X  X  X   X  X n  X  X  X rato  X .
 In this context, Savary X  X  proposal maintains the essence of global correction techniques, introducing finite automata ( operational kernel. For completeness, we introduce a brief description of how this algorithm works. We first assume an
Q R into 2 Q defining the transitions of the automaton, q Motwani, &amp; Ullman, 2006, chap. 2 ).

The procedure starts like a standard recognizer, attempting to proceed from the initial state to a final one through tran-the algorithm tries to continue from state p and skips the next two characters in the input.

These operations are applied recursively until a correct configuration is achieved, from both the state where the error is detected and all previous configurations of the FA .

Savary X  X  main contribution lies in giving only the nearest-neighbors, i.e. the valid corrected words with the minimal edit are taken into account, which should not only reduce the practical complexity but also the possibility of choosing a wrong correction. 3.2. Contextual-word error correction: a global approach
However, it is possible to go beyond Savary X  X  proposal by taking advantage of the contextual linguistic information embedded in a tagging process in order to rank the final corrections proposed by the base isolated-word algorithm ( Otero on a dynamic extension of the Viterbi algorithm ( Viterbi, 1967 ) over second order Hidden Markov Models ( Gra X a, Alonso, f X cil trabajar bajo presi X n  X ( "It is not easy to work under pressure" contains the words of the sentence to be tagged and their possible tags appear in columns below them, the goal being to compute the most probable sequence of tags for the input sentence.
 ple adaptation of the Viterbi equations, the probability of each possible path can be computed.
 ings. Let us now assume that our spelling corrector provides both  X  X  X  X cil  X /Adj-singular ( ( "easy" ) as possible corrections for  X  X  X  X cile  X . Let us also assume that the words  X  X  X ajo  X /Adj ( under" ),  X  X  X ajo  X /Verb ( "I bring down" ) and  X  X  X a X o  X /Noun ( dynamic Viterbi algorithm over this lattice then provides us both with the tags of the words and also the most probable on the basis of the computed probability for each path in the lattice. 4. Spanish as a case in point
Our approach has initially been tested for Spanish. This language can be considered a representative example since it 2004 ). The most outstanding features are to be found in verbs, with a highly complex conjugation paradigm, including nine simple tenses and nine compound tenses, all of which have six different persons. If we add the present imperative with two pronouns producing changes in the stem due to the presence of accents:  X  X  X a  X ( go" )or  X  X  X er  X ( "to be" ); and others include gaps in which some forms are missing or simply not used. For instance, mete-cate past participles, like  X  X  X mpreso  X  and  X  X  X mprimido  X (
This complexity extends to gender inflection, with words considering only one gender, such as  X  X  X ombre  X (  X  X  X ujer  X ( "woman" ), and words with the same form for both genders, such as  X  X  X zul  X ( arate forms for masculine and feminine, we have a lot of models such as:  X  X  X utor/autora  X ( ( "boss" )or  X  X  X ctor/actriz  X ( "actor/actress" ). We have considered 20 variation groups for gender.
We can also refer to number inflection, with words presenting only the singular form, such as  X  X  X str X s  X ( others where only the plural form is correct, such as  X  X  X atem X ticas  X ( ( "red" )or  X  X  X uz/luces  X ( "light(s)" ), for example. We have considered 10 variation groups for number. 5. Experiments: research objective ing) in order to reduce such performance loss.
 mation into account when dealing with misspellings. Each of the approaches proposed in this work corresponds to a differ-ent incremental level of linguistic knowledge integration: excluding its use (in the case of n -gram based indexing), tionally integrating contextual information (in the case of contextual spelling correction).
 Moreover, since the language we will use in our tests is Spanish, which has a much more complex lexical structure than English, the results obtained will be easier to extrapolate to other languages.

Finally, it must be noted that we have tried to make this study as complete as possible by using a wide range of config-urations in our test runs.
 Next, we will describe the set-up of our experiments.
 6. Experiments: methodology 6.1. The evaluation framework Our testing information retrieval system employes the open-source ras, 2007 ) as its core retrieval engine, using an InL2 6 ument collection used in the evaluation process, we have used the Spanish corpus of the CLEF 2006 robust task established for that task: C050 X  X 059, C070 X  X 079, C100 X  X 109, C120 X  X 129, C150 X 159 and C180 X 189. As shown in Fig. 3 , topics evance assessment criteria. 6.2. Error rate
The evaluation has been performed by introducing misspellings in the topic set and analyzing their impact on the results obtained. In order to study the behavior of our proposals in the presence of different error densities, we have tested them for a wide range of error rates: ronments such as those where input is obtained from mobile devices, those based on handwriting (e.g. tablet computing, digital pens, PDAs), or even speech-based interfaces.

However, it must be noted that in the case of using human errors , as will be explained below in Section 6.3.2 , the max-imum feasible error rate we could obtain was T = 60%. 6.3. Error type
Two different approaches have been considered for introducing spelling errors into the topics: artificial errors and human errors . 6.3.1. Artificial errors
In this first approach for error generation, the misspellings have been randomly introduced by an automatic error-gen-rors whenever and wherever necessary.

Firstly, for each topic word with a length of more than three characters, that a human writer or an OCR device could make. At the same time, a random value between 0 and 100 is generated. Such a each word, its corresponding misspelled form, and a probability value.

As can be seen, this methodology we have developed is very simple and makes use of minimal resources, a very interest-the same time, it has a great flexibility since these are generated in a controlled environment, allowing us to create them according to our precise needs. 6.3.2. Human errors
In a second approach, real human errors have been employed instead. In this case the situation is the opposite to before, since these kinds of error are much harder to generate, requiring much more effort and time, and the control over the test new information about the behavior of our system when facing a practical environment, information which is not available when using artificial errors. Moreover, the methodology we have developed for the generation and management of human tests.
 example  X , and not to correct any error they might make when typing. In this way we obtained a basis corpus formed by 27 to be of practical use for a detailed study.
 pus were parallelized, thus gaining access to all the ways a given word in a given position had been typed. Next, the most frequent error for each word in the topics was identified. By these means, the maximum number of errors available could be needed to design a way of progressively introducing such errors in order to obtain increasing error rates. Moreover, as we have done in the case of artificial errors, such errors are required to be accumulative in order to avoid any distortion in the results.

So, in a third phase, test sets for increasing error rates were finally obtained. To do this, all the words which have been badly typed at least once are randomly and uniformly distributed into 66 groups. first T groups.
 At this point some differences between human errors and the previously-mentioned artificial errors must be pointed out. was introduced, but in the case of human errors two or more errors may appear at a time. Moreover, such a number is not homogeneous over the document, some words will contain only one error, others will contain two errors, others may contain rate, imposed by the number of errors introduced by typers. In this way, the maximum rate we can work with is T = 60%, considered for these experiments, since narrative is not available in the copies.

As a general conclusion, we can state that the use of human errors is more appropriate if we intend to study the perfor-mance of the system in an environment closer to real world use. However, because of their much higher costs, human errors the present work, we will use both types of errors, thus making our study more complete and allowing us to analyze possible differences we may find. 6.4. Query length
In order to study the impact on performance of the length and information redundancy of the query, three different rounds of experimental runs have been performed for each test configuration. Following previous CLEF works, the different different ways: text. In this way, in the case of the sample topic of Fig. 3 , the source text for generating the final query would be:  X  X  X irus inform X ticos.  X  Such a short length corresponds to that of web search queries: 2 X 3 terms on average in most studies ( Croft et al., 2009; 1998 ). Moreover, title fields consist mainly of noun phrases, as in our sample, which also agrees with the nature of web the case of short queries such as those used in commercial web search engines. taking again our sample topic of Fig. 3 , the source text for generating the final query is:  X  X  X irus inform X ticos. Encontrar documentos sobre virus inform X ticos.  X  in traditional (non-web) IR systems: 7 X 15 terms on average queries of this kind are interesting for their study. behavior of the system with very long queries. In this case, the resulting source text for our sample topic is: virus inform X tico, y posiblemente el da X o que causa.  X 
These are our longest queries, with 25.72 terms on average. Such queries are very rare in web searches, although they may only studying them in the case of errors generated automatically. 6.5. Indexing-retrieval process
Two strategies have been proposed in this work for dealing with misspelled queries. Firstly, the use of spelling correction techniques in order to remove the misspellings from the query. Two correction techniques have been described (see Section we have used a classical stemming-based approach ( stm ) as our baseline. Next, we will describe the set-up employed during the indexing-retrieval process for each approach. 6.5.1. The baseline
As explained, our baseline ( stm ) consists of a classical stemming-based approach used for conflation during both indexing and retrieval. We have chosen to work with SNOWBALL stemmer, stop-word list used was that provided by the University of Neuch X tel. community. Following Mittendorfer and Winiwarter (2001), Mittendorfer and Winiwarter (2002) , a second list of so-named meta-stop-words has also been used in the case of queries. Such stop-words correspond to meta-level content, i.e. those expressions corresponding to query formulation but not giving any useful information for the search. This is the case, for example, of the phrase:  X  X  X ncuentre aquellos documentos que describan ...  X ( 6.5.2. The Correction-based strategy
The basic configuration for the experiments corresponding to our correction-based approaches ( Sav , cont ) is the same as ually disambiguated training corpus is also required for training the tagger. We have chosen to work with the MULTEXT-JOC
Spanish corpus and its associated lexicon. The MULTEXT-JOC corpus ( V X ronis, 1999 ) is part of the corpus developed within language for English, French, German, Italian and Spanish. Moreover, about 200000 words per language were grammatically tagged and manually checked, with the exception of German. Regarding the lexicon of the Spanish corpus, that used in the experiments, it contains 15548 words which, once compiled, build an automaton of 55579 states connected by 70002 transitions.

In the case of using Savary X  X  approach ( Sav ), the querying process works as follows. The correction module takes as input the misspelled topic, obtaining as output a corrected version where each misspelled word has been replaced by the closest
For example, taking as input the sample sentence previously considered in Section 3 :  X  X  X o es f X cile trabajar baio presi X n  X  the output returned by the algorithm, to be submitted to the system, would be:  X  X  X o es f X cil f X ciles trabajar bajo ba X o presi X n  X .

It must be noted that this implies that at the same time the misspelled word is being corrected (e.g. X  X  baio  X  measuring the noise introduced into the system is through the number of candidate corrections proposed by the algorithm: more than one candidate implies that extra words have been introduced. Table 1 shows the mean number of candidate cor-rections per misspelling retrieved by Savary X  X  algorithm during our experiments.
 able to take the initial output:  X  X  X o es f X cil f X ciles trabajar bajo ba X o presi X n  X  and, by filtering it, to obtain the right correction:  X  X  X o es f X cil trabajar bajo presi X n  X .
 6.5.3. The n-gram based strategy
In the case of our n -gram based strategy ( 4gr ), documents are lowercased, and punctuation marks, but not diacritics, are removed. The resulting text is split and indexed using 4-grams, as a compromise on the n -gram size after studying the pre-vious results of McNamee and Mayfield (2004b) . No stop-word removal is applied in this case. Such a process, which needs no extra resources, is applied both during indexing and retrieval. 7. Experiments: results
As we have previously explained, we have tried to make this study as complete as possible by using a wide range of configurations in our experiments, also gathering as much data as possible. We have also tried to give access to all these data in such a way that the reader can examine them at a glance, avoiding the need to examine several parallel tables at once. This resulted in the tables of results used throughout this paper, where absolute performance, perfor-mance loss, statistical significance and other data can be displayed simultaneously, making their analysis as a whole easier. However, since these tables might initially seem somewhat dense, we will describe how to interpret them before continuing.

Let us take Table 2 , for example, which corresponds to the results obtained with the different approaches proposed when which we will explain later. For each test configuration the performance obtained, in terms of mean average precision ( shown in column MAP , with column %loss also showing the performance loss (in percentage) with respect to the correction-based approach(es) for that given error rate T , this will be indicated by means of superindexes and , respec-( Sav and cont ). The number of queries for which no documents are retrieved is also indicated as subindex row avg . Average loss over T 6 60% is also shown in row avg . human errors. 17 avg . error rate. 19 Furthermore, the non-filled superindex shows us that it also outperforms Sav for that error rate, but such improvement is not statistically significant. 20
Finally, in order to make the analysis of the correction-based strategy more complete, an extra indicator has been calcu-lated in that case. This indicator value, which we refer to as tained are displayed in Table 3 .
 obtained by simply calculating the difference between the performance loss for Sav approach ( %loss = 13.48%, obtained from
Table 2 ) and the performance loss for the stm baseline ( %loss = 17.69%):
Now we have explained how to interpret the tables of results, we can present them. 7.1. Results with artificial errors Our first set of experiments has been performed using misspellings that have been artificially introduced in the topics.
Next, we present the output results obtained for the different approaches proposed in this paper. 7.1.1. Short queries
The first round of experiments with artificial errors was performed using the so-called short queries, those built using
The results obtained are shown in Table 2 . 7.1.1.1. Baseline. The early tests we have studied are those contained in column group stm , which shows those results ob-tained using the misspelled (non-corrected) topics in the case of our baseline, a classical stemming-based approach. 7.1.1.2. Savary X  X  approach. Our second series of experiments tested the behavior of the system when using the first of the correction approaches considered in this work, that is, when submitting the misspelled topics once they have been processed textual correction approach. third series of tests has been performed applying our contextual spelling corrector instead. when the misspelled (non-corrected) topics are submitted to our n -gram based IR system.
 7.1.2. Mid-size queries
As explained before, in order to study the impact of query length and information redundancy in our approaches, a second round of experiments was performed with the so-called mid-size queries, those generated using both title and description topic fields. The results obtained appear in Table 4 . 7.1.3. Long queries long queries, obtained using all the topic fields: title , description and narrative . 7.2. Results with human errors
A second set of experiments was performed using real human errors. As previously explained in Section 6.3.2 , although we can work with is T = 60%, since the maximum error rate available was 65.62%. Moreover, long queries have not been con-sidered for these experiments, since human errors were not available for the narrative topic field. 7.2.1. Short queries field, are shown in Table 6 . 7.2.2. Mid-size queries second round of experiments makes use of the mid-size queries generated using both title and description topic fields. The results obtained appear in Table 7 .
 8. Experiments: discussion of results with artificial errors
Having presented the results obtained in our experiments for the different test configurations available, we will now pro-ceed to discuss them. Because of the high number of configurations available we have opted to distribute such a discussion ficial errors, while the next section discusses the case of human errors. 8.1. Short queries 8.1.1. Baseline
The figures obtained, shown above in column group stm of Table 2 , indicate that stemming is very sensitive to misspell-formance, since MAP decreases by 18%, an impact which increases as the number of errors introduced grows: 25% loss for key importance. As explained in Section 6.4 , these queries have approximately three searchable stems on average. In this way, the loss of a single matching because of a misspelling implies the loss of one third of the information contained in the query. As stated, each single term becomes of key importance. 8.1.2. Savary X  X  approach
On analysis, the results obtained for the first of our correction-based approaches, shown in column group Sav of Table 2 , ing  X  the impact of misspellings, not only for low error rates ( high error rates (from 0.0863 to 0.1352 for T = 70%), thus reducing the average number of queries not retrieving documents has been greatly reduced: from 2 to 1 documents for T = 50% and from 13 to 5 for T = 100%, for example. Data analysis also shows that the effectiveness of the correction, the creases with the error rate, as shown in the column artificial errors ? short ? Sav of Table 3 . 8.1.3. Contextual spelling correction
The results obtained with this approach were shown in column group cont of Table 2 . As expected, results consistently column cont of Table 3 and, logically, it is slightly better than that for Savary X  X  approach.
 8.1.4. Character n-grams
As can be seen in column group 4gr of Table 2 , although stemming performs better than n -grams for the original queries, the opposite is the case in the presence of misspellings. n -Grams not only clearly outperform regular stemming ( stm , our baseline) when no correction is applied, such improvement being significant for T P 40%, but also outperform both correc-superior to that of any of the previous stemming-based approaches. If we take a look at its for T = 100%; i.e. we have no [  X  ] entries. 8.2. Mid-size queries 8.2.1. Baseline
Results in column group stm of Table 4 show that stemming remains sensitive to misspellings, although the performance loss is less than with short queries  X  particularly for low-medium error rates  X , with a 41% trast with the previous 55%, such a loss not becoming significant until T = 30%. Moreover, since the table shows no it means that no queries fail to retrieve documents, even for very noisy environments. The main reason for this improvement is the redundancy of information. As a result of increasing the length of the query, now with approximately 11 searchable that remains in the rest of the query terms now makes it easier to be able to continue retrieving relevant documents. As a result, a higher error rate is needed in order to attain the same decrease in performance as with short queries. 8.2.2. Savary X  X  approach ever, higher error rates once more result in a loss of performance, which is significant for T P 40%, although such perfor-mance losses are much less than in the case of shorter queries, with average case of short queries) to the current 25%. The column artificial errors ? +mid-size ? Sav of Table 3 again shows that loss ter for T 6 60% (see avg. 60 values). 8.2.3. Contextual spelling correction
The relative behavior of our contextual correction approach (column group cont of Table 4 ) with respect to the baseline formance loss from 41% to 23%. Moreover, the integration of contextual information has again allowed us to attain a small with respect to short queries. However, it remains slightly better than that for Savary X  X . 8.2.4. Character n-grams figure is also clearly superior to that obtained for stemming-based approaches. However, because of the greater improvement attained for stemming when enlarging queries, the previously existing advantage of n -grams over stemming in the presence rection proposal ( cont ). However, it must be noted that when no errors are introduced (i.e. for T = 0%) the baseline higher for stemming (0.3427) than for n -grams (0.3075), giving a much wider loss margin for stemming. Nevertheless, even with such an initial disadvantage, n -grams have managed to outperform stemming for high error rates. 8.3. Long queries 8.3.1. Baseline
Column group stm of Table 5 contains the results for non-corrected stemmed queries, showing, as expected, a major per-approximately 26 searchable stems on average (see Section 6.4 ), and the redundancy and greater information availability this implies, result in a smaller average performance loss ( avg. ) than in the case of shorter queries: 34% instead of 41% and 55% in the case of short and mid-size queries, respectively, although such a performance loss becomes significant at a lower rate: T P 10% instead of T P 20%. 8.3.2. Savary X  X  approach
As shown in column group Sav of Table 5 , when applying Savary X  X  algorithm over the misspelled topics the results ob-tained indicate a general improvement, which becomes significant for T P 50%. Although it decreases for high error rates, for mid-size queries, with a resulting reduction of the mean shorter queries. 8.3.3. Contextual spelling correction
In the case of applying contextual correction (column group cont of Table 5 ) the results show once more that its relative behavior with respect to the other stemming-based approaches continues to be similar to that for shorter queries. As in the ings on performance, and when compared with Savary X  X  approach ( Sav ), it again shows a small but consistent improvement: 20% average loss ( avg. ) in the case of contextual correction, with respect to 21% for Savary X  X . Regarding ( avg. ) is not as good as for shorter queries. 8.3.4. Character n-grams
Finally, our n -gram based approach (column group 4gr of Table 5 ) also attains a greater robustness than in the case of Regarding its relative performance with respect to stemming, the and somewhat more than a half of that for correction-based approaches. This allows n -grams to again outperform non-cor-proach, with T = 100%). 9. Experiments: discussion of results with human errors 9.1. Short queries 9.1.1. Baseline
Column group stm of Table 6 contains the MAP figures obtained in the case of stemming the misspelled (non-corrected)
Table 2  X , although somewhat less in the case of the lowest error rates (it does not become significant until T = 20%) but higher for the rest; as a result, the average MAP loss increases from 37% for artificial errors to 39%. some topics fail to retrieve documents when using misspelled topics ( although document loss now starts at T = 20% as opposed to T = 30% for artificial errors. 9.1.2. Savary X  X  approach Results for Savary X  X  correction-based approach are shown in column group Sav of Table 6 . As expected, indicate that correction reduces the impact of misspellings at all rates, resulting in an average 39% to 30%, with all topics retrieving documents. When compared with the results obtained for the same approach using icant until T = 20%), but increases for higher ones, finally resulting in a higher stead of 24% for artificial ones. In the same way, the number of topics with non-retrieved documents ( although the recovery rate is less than before. 9.1.3. Contextual spelling correction
With respect to contextual spelling correction, whose results are shown in column group cont of Table 6 , it consistently improves Savary X  X  approach (column group Sav ), although such improvement remains small, as with artificial errors: an approximately 2% additional loss recovery on average, only significant on two specific occasions (for T = 20% and
T = 40%). When compared with the results obtained with artificial errors for this same approach, shown earlier in column man errors. In the same way, loss recovery is slightly higher than Savary X  X , as shown when comparing Sav and cont columns in Table 3 . 9.1.4. Character n-grams
As in the case of artificial errors (previously shown in Table 2 ), although stemming-based methods outperform n -grams for the original queries (i.e. when T = 0%), the introduction of errors changes this, since n -grams not only outperform non-corrected stemmed topics ( stm ) for T P 20% (becoming significant at T P 50%), but also improve correc-tion-based approaches ( Sav , cont ) for T P 40%, as can be seen column group 4gr of Table 6 . In the same way, n -gram robustness again shows itself to be far superior to that of previous stemming-based approaches, since its 20% average loss nearly halves that for non-corrected stemming and is 50% less than that for correction-based approaches. Finally, when comparing these results with those previously obtained for artificial errors (shown in column group 4gr of Table 2 ), the latter also performed better, as in the case of stemming-based approaches, with average from 11% to 20%.
 errors. However, Savary X  X  correction approach succeeds in reducing the impact of such misspellings while our contextual nally, character n -grams have shown in both cases a greater robustness in the presence of misspellings, being able to out-perform the rest of the approaches when the error rate increases, even when stemming performs better for the original queries (i.e., with no extra misspellings). However, it must be noted that human errors showed a greater impact on results rection-based and n -gram based solutions. 9.2. Mid-size queries 9.2.1. Baseline
The results obtained, displayed in column group stm of Table 7 , continue to show the clearly negative impact of misspell-uments, even for the noisiest environments. At the same time, if we compare these results with those obtained when using artificial errors, previously shown in Table 4 , we find a performance reduction, with average 19% to 29%. 9.2.2. Savary X  X  approach
The results contained in column group Sav of Table 7 show a major difference with respect to all previous tests using Sa-attain a minor non-significant improvement with respect to misspelled stemmed topics ( stm ), merely reducing the average loss ( avg . 60 ) from 29% to 28%. This is caused by the noise introduced by the high number of candidate corrections re-trieved by Savary X  X  algorithm for the same misspelled word. As shown in Table 1 , the mean number of candidate corrections for each misspelled word, more and more extra words are being introduced in the query during the correction process, these not always being related with the original word. If we study the average lengths of the queries submitted to the system for the current configuration ( mid-size queries with human errors), we can see that the mean length has increased from approx-the query. The introduction of so many additional terms distorts the semantics of the original information need, finally ery, as shown when comparing the figures of the column human errors ? mid-size ? Sav of Table 3 with those of the corre-sponding column in the same table for artificial errors: as we can see, average recovery ( avg . artificial errors to 1.17% in the case of human errors. 9.2.3. Contextual spelling correction
However, when looking at the results obtained using contextual correction instead, shown in column group cont of Ta-on performance as before, noticeably reducing the average current 18%, and also notably outperforming both non-corrected ( stm ) and Savary X  X  ( Sav ) being significant for T P 20%. As previously explained, our contextual correction algorithm performs a drastic pruning word according to its context, thus avoiding the introduction of extra words in the query and thereby minimizing the noise introduced during the correction process. Data analysis again reveals that loss recovery increases at the same time human errors with short queries. 9.2.4. Character n-grams ( stm ), showing the greater robustness of n -grams. Moreover, n -gram performance continues to be better than that of
Regarding contextual correction ( cont ), this performs better than n -grams for the range examined, although performance differences progressively diminish as the error rate increases, finally leveling at T = 60%, with both average n -grams showing a somewhat higher performance loss.

The main conclusion we can draw from all our tests using mid-size queries containing human errors, is that on this contextual correction approach has had a clear positive impact on performance, being far superior to Savary X  X . Regarding and character n -grams, although positive, is not as great as in the case of artificial errors. 10. Conclusions and future work
This work introduces a proposal in the design of robust search on IR systems, intended to be used in a generic, non-spe-tinue while avoiding complex implementation, not only from the computational point of view but also from the linguistic one.

For this task two different strategies have been described throughout this work. Firstly, a correction-based strategy has been proposed. This way, the input misspelled query is corrected before being submitted to the IR system, which employs a classical stemming-based approach, i.e. the misspelled words of the query are replaced by their candidate corrections pro-posed by the correction algorithm. Two different correction techniques have been studied. On the one hand, a global correc-employed. This algorithm is an extension of the former which makes use of contextual information obtained through part-of-speech tagging, thus providing a solution for ties and returning a single correction.

The second strategy we have proposed consists of using character n -grams instead of classical stems as the processing unit. This allows us to work directly with the misspelled topics without further processing, since the matching process is no longer performed at word-level, but at subword-level, thus increasing robustness since partial matchings between a word and its misspelled form are now allowed.

Moreover, in this work we also introduce two methodologies for the design of experiments in this field by introducing, topic set in order to analyze their impact on the performance of the system.

These methodologies provide three major contributions. Firstly, their simplicity, both in their use and their understand-distortion in the results.

Once performed, our experiments demonstrate that classic stemming-based approaches are highly sensitive to mis-spelling may not be recovered from the rest of the topic. Such a negative impact can be appreciably reduced by the use of correction mechanisms during querying. Moreover, our contextual correction approach has been proved to outperform clas-sical global correction in a consistent way, particularly in the case of mid-size queries containing human errors (a not contextual correction proved to be far superior by remarkably reducing the impact of misspellings on performance. This is because of the high level of noise introduced by the global corrector in such a context.
On the other hand, our n -gram based strategy has shown a remarkable robustness, with average performance losses appreciably smaller than those for classical stemming. It must be noted that in the presence of no misspellings classical stemming-based approaches obtain a better performance than n -grams. However, in the case of very short queries such as those of practical systems, n -grams have been able to outperform stemming when misspellings are introduced. In the case of longer queries, n -grams are also able to do this, but only for high error rates. Moreover, since such a subword-based approach does not rely on language-specific processing, it can be used with languages of very different natures, even in the face of the lack of linguistic information and resources available; in contrast, previous correction-based ap-proaches needed language-specific resources for their application, such as stemmers, stop-word lists, lexicons, tagged cor-pora, etc.

With regard to future work, in the case of our contextual corrector we plan to extend it for dealing with tokenization er-to both increase the performance of the system and reduce processing and storage resources. Such stop-n-grams should be this approach.
 Acknowledgements
The authors wish to thank Miguel A. Alonso, of Univ. of A Coru X a (Spain), and the reviewers for their helpful comments and suggestions in order to improve this article.
 This research has been partially funded by the Spanish Ministries of Education and Science and FEDER (through Projects HUM2007-66607-C04-02, HUM2007-66607-C04-03, TIN2010-18552-C03-01 and TIN2010-18552-C03-02), and by the
Autonomous Government of Galicia (through Projects INCITE08-PXIB302179PR, PGIDIT07-SIN005206PR, INCITE08-E1R-104022ES, INCITE08-ENA166098ES, INCITE09-E2R104007ES and INCITE09-E1R305070-ES; and through the Galician Network for Corpus Linguistics and the Galician Network for NLP and IR ).
 References
