 Rich Gazan * 1. Introduction
In a social Q&amp;A system designed for solitary exploration of user-generated content, how do people collaborate? This paper defines microcollaborations as brief, informal expressions of mutual interest and mutual effort toward seeking information on a given topic. Most studies of collaborative information seeking in distributed environments have been conducted with users in work-based or task-based communities, who have relatively frequent interactions and use tools specifically de-signed to support collaboration. Social Q&amp;A sites, also called social reference sites or question answering communities, are collections of user-generated questions and answers on an extremely diverse array of formal and informal topics. Their native social interaction and content recommendation functions offer, at best, an implied collaboration with the aggregated wisdom of many anonymous others. However, collaboration in the stronger sense involves both direct contact with, and mu-system studied here demonstrate a preference for collaborators with high social capital within the site, and that they tran-scend the limitations of the site to find them.

The process of how people acknowledge mutual information need and work together to solve problems of mutual interest on this site was observed and analyzed over a 10-month period, as part of an ongoing long-term participant observation.
Content and transaction log analyses of user interactions, workarounds and requests suggest that users regularly bend or seeking in future social Q&amp;A environments. 2. Background
The focus of research in collaborative information seeking must account for the socially embedded nature of collabora-tions, and include empirical observation of interactions as well as more system-centered analyses ( Karamuftuoglu, 1998 ). Hansen and J X rvelin (2005, p. 1105) discuss cooperative activities as a component of collaborative information seeking.
Though their research focuses on work tasks in professional communities, they include a discussion of asynchronous, loosely-coupled activities that are more akin to casual information seeking in a Web environment:  X  X  X n loosely coupled activities , the system will take advantage of recommendations from other people through observations of their information seeking behaviour such as search paths and annotations; recommendations based on usage rates, and explicitly stated recommendations. X  Hansen and J X rvelin (2005, p. 1105) ; emphasis in original)
The authors also found that even in a professional patent searching environment, roughly two-thirds of all collaborative activities were document-related, while one-third were human-related. Human-related collaborative activities included task cooperation, division of labor, sharing search strategies, and locating external and internal domain expertise, which could take the form of either people or information resources.

Hertzum (2008) adds a dimension of information sharing to Hansen and J X rvelin X  X  (2005) definition, and proposes a wider tively construct shared understanding of the information need and seeking process. Based on the work of Clark and Brennan (1991) and Olson and Olson (2000) among others, collaborative grounding attempts to address the social and dynamic nat-ure of collaborative information seeking by focusing on the processes and channels through which information is under-stood, filtered and distributed among the collaborators.

Social Q&amp;A sites are natural environments for collaborative grounding in Hertzum X  X  sense. Built around a Web 2.0 model of user-generated and user-vetted content, these sites allow anyone to ask and answer questions, and through various algo-community can be created as well.

Shachaf and Rosenbaum (2009) call for more research on the social processes underlying social Q&amp;A sites. While analysis of questioners and answerers can reveal a more nuanced understanding of questions such as why people participate and contribute content, how quality content is judged and how community norms evolve, all toward the larger goal of under-standing the process of collaborative question answering.

However, while social Q&amp;A sites aggregate the collected contributions and assessments of other users, most such sites are designed around the traditional  X  X olo seeker X  model. They are not designed to be full-fledged collaborative systems, which support the work of two or more individuals with a shared information need. Social Q&amp;A sites may have elements such as rating and recommender systems to help users judge one another X  X  trustworthiness or expertise, but they are not de-signed as formal expertise networks, where individuals have a way to identify potential collaborators, or people with similar information needs.
 Uncontrolled user-generated content and conversations are one means by which users bypass these filtering systems.
Annotations of content, from marginalia in print works ( Sherman, 2008 ) to social annotations of Web resources ( Gazan, 2008 ), can lead to increased user engagement and a sense of collaborative information seeking across the boundary object the future.

However, finding out who knows what is rarely straightforward, even with enterprise level systems designed to support precisely that activity. Tiwana and Bush (2005) analyzed 122 expertise networks in engineering firms and technical organi-zations, and found that most languished from underuse or were abandoned outright. Successful systems, on the other hand, were social: expertise networks which allowed  X  X  X ystem-mediated relationships with other users of the system increase[d] continuance X  (p. 85).

Kari and Hartel (2007) present compelling evidence for a view of information science that transcends information need as ied domain of information seeking research, and identify a realm of information behavior based on reasons both pleasurable (art, celebration, relaxation) and profound (altruism, emotion, ethics, creativity). Nahl (1998) emphasizes the affective dimensions of information seeking as well; people often engage with information systems for reasons that are entirely out-side the literal queries and results that constitute the traces of their interactions.

Shah, Oh, and Oh (2008) ascribe the success of the Yahoo Answers social Q&amp;A site to its reward structures, such as the levels and ranks achieved through contributing answers voted best by other users. Similarly, they propose that one reason for the failure of Google Answers was its lack of a social component. Adamic, Zhang, Bakshy, and Ackerman (2008) also stud-ied Yahoo Answers, and point out the flaw in using its native  X  X est answer X  tool as a means of analysis, since more than one answer may be equally good, and the standards by which an answer is judged best are idiosyncratic. They found that answer length and the track record of the user were related to whether a given user X  X  answer was chosen best. This echoes the find-ings of Smith (2002) , who studied social accounting metrics on Usenet and found that a mutual awareness of participants X  revealing author histories correlates with trust, and a user X  X  desire to read more content posted by that author.
Naver X  X  Knowledge-iN, an extremely popular South Korean Q&amp;A site, allows multiple answers to be chosen as best. In their study of Knowledge-iN, Nam, Ackerman, and Adamic (2009) found that the best answers are correlated with consistent participation. Somewhat paradoxically, they found that social/affective factors such as altruism and a reward structure based on points motivate participation, but users of the site interviewed by the researchers reported that Knowledge-iN  X  X  X acks a alized in this paper; in essence, it is the difference between wanting to know something and wanting others to know it.
Gazan (2006) found that in the Answerbag social Q&amp;A site, answers from synthesists, those who claim no topic expertise but provide links and supporting evidence, tended to be rated more highly than answers from specialists, who claim exper-tise and provide no supporting evidence in their answers. In their comparative study of five online question answering sites (not including Answerbag), Harper, Raban, Rafaeli, and Konstan (2008) also found evidence to support the notion that an-Q&amp;A sites are particularly fertile ground for synthetic, collaborative approaches to information seeking.
Amershi and Morris (2009) conducted a diary and observational study of co-located collaborative Web searching, when participants are in the same physical location. Among their proposed design implications for systems to support co-located collaboration are: Maintain a queue or history of suggestions Include consensus facilities to agree on a group X  X  focus Provide group awareness mechanisms Provide a shared context ( Kari and Savolainen, 2003 )
Whether intentionally or not, social Q&amp;A sites offer many of these functions, as well as allowing participants to commu-nicate to site administrators and one another about new ways in which the site could better support collaborative informa-that is largely inhospitable to the activity. 3. Setting and method
Answerbag ( http://www.answerbag.com ) is a social Q&amp;A site designed around a one question X  X ultiple answers archi-tecture. Users submit and rate questions and answers, and the highest-rated answers are listed first, providing a collabora-comment threads beneath any answer, create personal profile pages, and  X  X  X riend X  one another to be selectively notified about their friends X  activity on the site ( Gazan, 2007 ). Answerbag became the author X  X  research testbed in 2004 and is now a thriving Website with over 12 million unique visitors per month. Administrator-level access to all site data is avail-able, and research is conducted as a participant observation.

Answerbag is not a collaborative information system. It was designed as a portal to collect frequently-asked questions and allow users to append and rate multiple answers. However, when social functions were added to the site, such as level titles site content. In this environment, question pages and answer comment threads regularly become convenient gathering places for conversation, as well as negotiation and debate about what constitutes appropriate behavior ( Gazan, 2009 ). From the site X  X  inception, users have shaped the site for their own needs, well beyond what designers intended.
While research questions relating to the speed of user responses were initially part of this analysis, the considerable inconsistencies in user notification preferences, page views per session and other participation habits made temporal com-of a high-volume social Q&amp;A site, intermittent participation and long absences are common.
 Similarly problematic was attempting to measure the number of  X  X  X ight answers X  resulting from the collaborative activity.
Unlike Yahoo Answers and Knowledge-iN, Answerbag provides no mechanism by which askers or answerers can determine one or more final  X  X  X est answers X . Answers are sorted in descending order of the rating points they have accumulated, and attempt to classify collaborations as successful or unsuccessful based on whether the participants X  needs were ultimately met, the focus of this study is limited to observed instances and characteristics of collaborative activity.
In the simplest terms, collaborative activity is operationalized in this study as mutual interest and mutual effort; both are required to be present for an interaction to be coded as a microcollaboration. Mutual interest refers to two or more individ-uals who express a shared information need. Mutual effort occurs when the interested parties post strategies or resources mation exchanged between collaborators, and the casual and informal nature of the interactions, many of which can be maintained simultaneously.
 3.1. Mutual interest
Most Answerbag interactions begin simply enough, with a user posting a question. Through mechanisms such as catego-rization, notification lists and new arrivals feeds, the site is designed to invite answers, ratings and comments from subse-quent users. However, an expression of mutual interest as operationalized in this paper must meet a higher threshold of engagement than merely rating or responding to a question. When a user breaks out of the site X  X  preset structure, and re-laborative interaction. When a second person (often the person who posted the original question) acknowledges mutual in the course of regular site observations over a 10-month period, and a content analysis was performed on the initial set to reveal common terms and phrases indicative of mutual interest. Examples include  X  X  X ost if you find out X ,  X  X  X  X  X  having the same problem X ,  X  X  X  want to know that too X , and so on. Searches were then conducted on these and other phrases to produce a total set of 816 expressions of mutual interest for further analysis. This is an extremely small percentage of the entire
Answerbag database, which at this writing contains over 1.4 million questions, over 7 million answers and roughly 20 mil-lion answer comments.

In Fig. 1 , user  X  X  X ipple X  posts an answer to a question which is called incorrect by another user,  X  X  X ncacal X . Ripple X  X  sub-sequent comment indicates mutual interest, a shared information need with the original questioner.
 3.2. Mutual effort
The set of 816 expressions of mutual interest was then tracked for subsequent activity. At first, an instance of microcol-laboration was operationalized as occurring when two people who had stated a common information need also exhibited mutual effort toward addressing it, usually by posting resources or strategies. However, as tracking continued it became evi-mutual interest, and mutual effort consists of several dozen opinions and anecdotes in answers and comment threads. Com-
Redhawk has used the answer edit function to create a de facto microcollaborative workspace to report her experiences and respond to the inquiries of others, and has been heavily uprated by other users for her effort.

Finally, a content analysis of the interactions around instances of mutual interest and mutual effort was conducted, as well as a transaction log analysis to determine the patterns and relationships of the participants in these microcollabora-&amp; Guba, 1985 ), where categories for analysis emerge from the examination of naturalistic data.

In parallel with the content and transaction log analyses, expressions of frustration about any roadblocks to collaborative activity were also collected, whether posted to the page directly or sent to site administrators as upgrade suggestions. 4. Results
The results of the study suggest that users regularly transcend the question X  X nswer X  X omment structure of the site in or-der to collaborate, and that the topics of microcollaborations tend to be relatively complex, a hybrid of fact and opinion.
Users X  contributions and interactions suggest the strong influence of social and affective factors beyond strict information need as an important motivator for collaborative activity.
 4.1. Extent and location of microcollaborations
After content and transaction log analyses of the 816 instances of potential collaboration identified in the initial phase of data collection, 122 were coded as instances of microcollaboration. However, this number should be viewed with caution.
When instances of potential collaboration were initially sorted into location pairs (question X  X nswer, answer X  X ubsequent an-swer, answer X  X omment, comment X  X ubsequent comment) to determine the location of collaborative activity, only 99 were identified. Further analysis revealed that once a shared information need had been established in one comment-based loca-tion, many users then created a new space for the microcollaboration by asking a new question, and referencing the initial conversation with a link. Since comments cannot be rated, but questions and answers can, there is an incentive for users to create new questions. Including linking and new questions increased the number of collaborative instances from 99 to 122 ( Table 1 ).

It is immediately clear from Table 1 that the Q&amp;A format of the site does not limit users who wish to communicate outside the preset structure. There is no way to restrict the placing of questions into answer or comment fields, and this freedom allows the nature of the information need to be debated and refined. Questions, answers and comments are public and per-tribute strategies and solutions.

Many interactions take place between users who are selectively notified about particular content. When users are logged into the site simultaneously, a notification icon appears when a new answer or comment has been appended to content posted by the user, allowing near real time interaction. Users can also receive notifications via email or RSS when content has been added to a question or answer to which they X  X e contributed, or to which they have requested notification. Upon their next login, a list of requested content update notifications awaits them.
 Another caveat is that much communication happens off the site, particularly among users who have interacted before.
Frequently, a comment thread will end with  X  X  X heck your inbox X  or  X  X  X M me X  to indicate that conversation will continue out-side the site. 4.2. Characteristics of microcollaborations
Analysis of the types of content likely to generate expressions of mutual interest and mutual effort in this environment opinion .

The threshold of complexity characteristic was identified after an early phase of content analysis sought to count  X  X orrect X  answers. However, 106 of the 122 instances of microcollaboration centered precisely around questions that did not have straightforward answers; usually these were context-dependent, or otherwise not easily findable through a Web search en-gine. Nuanced topics and situations provided more opportunity for users to interact, and generated many more notifications, views and contributions than the average Answerbag question. Examples included: Identification, e.g. What is this plant I found growing in my garden? Should I get rid of it? Child care, e.g. How can I get my toddler to stop hitting? Buyers X  guides, e.g. What is the point of diminishing returns when spending money on a home audio system? Cookery tips, e.g. What can I do to make my cornbread turn out less dry?
Similarly, 111 of the 122 instances of collaborative information seeking observed in the sample were addressed with a hybrid of fact and opinion. Personal experience and analogy-based speculation were frequently offered alongside more objec-native aggregation of multiple opinions via ratings and recommendations, as well as the human tendency to trust word-of-mouth information based on claimed personal experience, yields a set of collaborative topics that contain both fact and opinion.

Most of the remaining 694 instances of potential collaboration that did not result in a coded collaboration were either purely factual (e.g. How many endangered amphibian species are there?) or purely opinion-based (e.g. Who X  X  the best gui-tarist ever?). While conversational questions tend to generate more responses than factual questions with or without an ini-engaged, mutual effort toward collaborative problem solving. 4.3. Characteristics of collaborators
While collaborators have been operationalized as those who have expressed mutual interest and mutual effort in a given and actions of collaborators warrant more detailed analysis.

Not every question asker on a social Q&amp;A site is interested in interacting with members of the community, let alone engaging in collaborative information seeking. Many take the site X  X  name literally, and are interested in answers, not inter-action X  X ver 70% of registered Answerbag users have an empty friends list. Conversely, high-ranking users who have dozens of friends often resort to posting a message on their user profiles such as this one:  X  X  X f I decline your friend request please don X  X  be offended, it just means I haven X  X  gotten to know you yet. Keep trying! X 
Users of social Q&amp;A sites enter a node of concentrated information exchange, where the rewards of participation and interaction are in constant tension with the overwhelming volume and speed of new information. Even when a user has of content is too much to even scan comprehensively, let alone read, respond and interact. Answerbag provides no simple way for users to highlight or prioritize questions they are most interested in, nor to identify others with similar interests and a similar level of motivation to address a specific information need. In the absence of these tools, users must rely on a combination of chance and persistence, the latter of which is reflected in the points and levels Answerbag users accumulate for providing content judged valuable by other users.

The 122 instances of microcollaboration drew 330 collaborators. Specifically excluded are individuals who contributed answers or comments to ongoing collaborations without demonstrating both a shared information need and effort toward addressing it. In terms of raw numbers, collaborators tended to be either low-ranking or high-ranking individuals in the
Answerbag community, based on their accumulated rating points. Table 2 shows the relative distribution of users at each level, which normalizes the results somewhat, but still suggests that high-ranking users are most likely to engage in collab-orative activity.

The content analysis strongly suggested social/affective factors as an important component of collaboration. Users at lev-els 1 X 19 can bestow up to 3 points when rating other users X  questions or answers. At levels 20 X 39 the maximum award in-
Expert, Professor or Guru, and a badge displayed on each of the user X  X  contributions throughout the site, as well as on their aware of who the highest-ranking users are, and who can bestow the most points.

People tend to collaborate with people they already know on the site; collaborators in the sample appear on one another X  X  tion pages, where friends and followers are automatically invited to participate.

While it must be noted that content, ratings and user level can change frequently over time, the data in Table 3 suggest that if one of the collaborators was a high-ranking user, responses, page views and question/answer ratings increased sub-stantially. Table 3 also shows how collaborations draw responses, page views and ratings well above the site average for users at each level group.

The impact of the social nature of the site is evidenced in the content analysis of collaborative instances as well. While microcollaborations involving the highest-level users drew by far the most page views and content ratings, many of the con-tributions were off-topic social comments, suggesting an affective need for contact rather than an interest in the subject of the collaboration. Similarly, Answerbag does not allow users to see who has rated their content, but to communicate their positive action, many people append a  X  X +4 X  or similar rating indication to a comment when they uprate a high-ranking user, so the person knows who bestowed the points.

Interestingly, it was also observed that when high-ranking users contributed content to a microcollaboration between collaborators of any level, the number of page views increased by as much as a factor of two, even if the high-ranking users high-ranking users through strategies like uprating and leaving social comments, regardless of whether they share the infor-mation need.

The data suggest that the most important factor in successfully transforming a potential collaborative instance into a coded microcollaboration was simply getting someone with a large network of friends and followers to contribute. The up-date notification function would bring a large number of people to the page, including more potential collaborators. The re-ward for the bringer of the useful link or answer is the opportunity to be publicly uprated, thanked and possibly friended, building one X  X  social capital and network on the site. 4.4. Collaboration roadblocks and requests
The content analysis also included user suggestions to support collaboration, whether posted publicly within a collabo-topics in an average 10-month period is roughly 300, though suggestions found within content pages are normally collected as found, not systematically. Suggestions over the 10-month period related to better support for collaboration accounted for a significant percentage of total feedback suggestions received. They were collected and coded as follows:
More control over contacts (48)  X  Send private messages to individuals or groups  X  View which friends are online at any moment  X  Block content/friend requests from particular individuals  X  Address a question to a particular individual or group
Identification of potential collaborators (24)  X  Subdivide friends in list based on expertise  X  Amend system-generated measures of category expertise  X  Automatic matching of users with similar interests
More control over content (50)  X  Create lists of questions they would like answered  X  Subscribe to lists created by others  X  Link answers and comments to subsequent questions which are more relevant  X  Know how many times a question or answer was viewed  X  Limit searches to specific categories  X  Search comment threads 4.4.1. Control of contacts
High-ranking users are selectively approached for friending and contact, and from the perspective of those users, there is a need to filter their contacts to those with whom contact and potential collaboration would be most productive. These users would very likely resist a function that would allow people to see when they are online, and embrace a blocking function, since they get copious and often frivolous attention on the site. Answerbag also provides no way to direct a question to a particular user or set of users. Once effective collaborators are found, the only way they can keep in touch is by becoming one of possibly hundreds of friends on a given user X  X  roster. This limitation may explain the frequency of user references to off-site communication mentioned earlier. 4.4.2. Potential collaborators
A common focus of user frustration centered around the expert identification system implemented on Answerbag. In a prominent position near the top of most category pages, it lists the users who have contributed the most answers, and the most highly-rated answers, in the category through a proprietary algorithm. Often, the outcome of this process is that a user who answers just a few questions about household repair may find himself forever listed as an expert in Toilets, while another user who has answered hundreds of questions in a relationship advice category may not be listed as an expert there. a particular category.

While it is possible to sign up for notifications when new questions and answers have been submitted in a category of interest, several users wanted the ability to know which other users had signed up for the same notifications. One intriguing suggestion imagined a function akin to a dating site, where the contributions and watch lists of users could be compared, and people with similar interests might be able to identify and communicate with one another more easily. 4.4.3. Control of content
The most common request in the domain of content control involved the ability for users to create lists of questions they would like to have answered. While it would be dubious to interpret those requests to have been made solely in pursuit of collaboration, it is another potential pathway by which people might discover others with mutual information needs. Several lected others.

Finally, microcollaborations may be buried in comment threads, where text is not searchable. Though some users migrate their collaborations to searchable questions and answer threads, they wanted the ability to search both within categories and within comment threads to ease retrieval. 5. Discussion
The problem-centric view of information science puts topical information needs at the core of an information seeking episode. The social/affective view prioritizes emotional needs such as contact, praise and altruism over the content of the question or answer. The sheer volume of participation in social Q&amp;A sites presents the opportunity to observe naturalistic interactions and information behavior between both theoretical poles. The results of this study sug-gest that a hybrid view of collaboration is warranted, one that takes into account embedded technical infrastructures and affordances, affective needs, the social dynamics of the community, and the information around which collaboration happens.

The initial operationalization of collaboration in this study focused on mutual interest and mutual effort. However, the results of the content and transaction log analyses suggest that a key factor in maximizing the probability of finding a like-minded collaborator may be social capital, however it is expressed. A high-status person in any environment is likely to draw selective attention, from a Nobel laureate at a conference to a celebrity at a neighborhood bar, and finding a way to unpack and account for social factors in the selection of collaborative partners and the assessment of information quality has implications well beyond information science.
 not just the desire to seek facts. As discussed earlier, Answerbag was built around a solo asker model, and people currently structure and unanswered question filter could be hybridized to create a  X  X  X oin this question X  option, which could create a individual efforts.

With the rise in texting and other forms of microcommunication, future research should attempt to refine the concept of microcollaborations, and leverage research in traditional collaborations to look for ways in which research in the two areas might inform one another. It is natural to expect more and more environments where people interact through devices, and engage both friends and strangers in casual collaborations. Future research could also address the extent to which a success-ful collaboration strategy reflects a balance between getting maximum exposure to potential collaborators and filtering out unpromising contacts. Or do off-topic, purely social interactions also serve a collaborative purpose? 6. Conclusion
Users of information systems usually find ways to work around system limitations to suit their own purposes. This study has described some of the strategies employed by users of the Answerbag social Q&amp;A site to find each other and work together in lightweight interactions termed microcollaborations. Topics around which collaborators interact share a common threshold of complexity and invite combinations of both fact and opinion. The results suggest that the social and affective aspects of collaboration are key factors in interpreting site transaction statistics, and that the reasons peo-ple initiate contact and work with others in a social Q&amp;A environment are not always motivated by a shared information need.
 References
