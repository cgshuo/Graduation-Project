 Query suggestion refers to the process of suggesting related queries to search engine users. Most existing researches have focused on improving the relevance of suggested queries. In this paper, we introduce the concept of diversifying the con-tent of the search results from suggested queries while keep-ing the suggestion relevant. Our framework first retrieves a set of query candidates from search engine logs using random walk and other techniques. We then re-rank the suggested queries by ranking them in the order which maximizes the diversification function that measures the difference between the original search results and the results from suggested queries. The diversification function we proposed includes features like ODP category, URL and domain similarity and so on. One important outcome from our research which contradicts with most existing researches is that, with the increase of suggestion relevance, the similarity between the queries actually decreases. Experiments are conducted on a large set of human-labeled data, which is randomly sampled from a commercial search engine X  X  log. Our experimental re-sults indicate that the post-ranking framework significantly improves the relevance of suggested queries by comparing to existing models.
 H.3.3 [ Information Search and Retrieval ]: Text Mining Algorithms query suggestion, post-ranking re-ordering, learning-to-rank, random walk
Query suggestion technique has been widely used in most commercially web search engines which facilitates the inter-action between users and search engines. By its definition, a set of relevant queries are suggested to a user on the search engine result page (SERP) after the user submitted a query. If the user is not satisfied with the results shown on the page, he/she may choose to click on one of the suggested queries to refine the search. Research works have indicated that query suggestion greatly improves user satisfaction rate, especially for informational queries.

Previous efforts on improving the quality of query sug-gestion have mainly focused on discovering relevant queries from search engine logs. For example, leveraging co-clicked URLs and session information to identify relevant queries [2, 16, 3] have shown significant improvement over other meth-ods. In the meantime, random-walk based models [17, 19] have been extensively studied due to their simplicity and scalability.

While many approaches have exhibited their effectiveness, most of them have failed to address an important issue, which is the diversification of the query suggestions. When a user clicks on a suggested query, he/she is expecting to gain additional information from a SERP with relevant topics to the original query. Therefore, we define the diversification between a query-suggestion pair to be different but relevan-toftheircontents . Essentially, we try to find the optimal balancing point between the similarity and diversification between query-suggestion pairs. If the diversification factor is removed from our model, then the model should exhibit the same behavior as previous similarity-based approaches.
Since it is relatively difficult to measure the diversification directly by only looking at two query strings, we propose a query suggestion framework which leverages post-ranking features from search results. Specifically, post-ranking fea-tures are different from pre-ranking computation where al-l features are required to be computed before the ranked search results are returned to the users. Post-ranking fea-tures generally means the observable elements on the SERP, e.g., the titles, snippets and URLs of the results. Therefore, we define SERP diversification between two queries to be the difference between their top-returned search results. For ex-ample, Figure 1 shows an example where  X  X elta air X  is the user query and  X  X elta Airline X  and  X  X elta Air Lines Jobs X  are suggested queries. In this example, we treat  X  X elta Air-line X  as a bad suggestion since the top-3 results are almost identical to those of  X  X elta air X . On the other hand,  X  X elta Air Lines Jobs X  is recognized as a good suggestion with di-versified SERP results. The detailed diversification function will be discussed in our framework in Section 3.

The rest of the paper is organized as follows: Section 2 presents the related work of query suggestion; Section 3 in-troduces our post-ranking query suggestion framework; in Section 4 we conduct experiment to evaluate our proposal; we conclude this paper with future work in Section 5. ( Query Suggestion ) Improving the performance and qual-ity of query suggestion techniques have been extensively s-tudied in the past decades. As a method mainly for inter-action between search engines and users, query suggestion techniques usually cannot directly improve the relevance of the search results, but rather enhancing the entire us-er search experience within the same search intent. Most of the existing works rely on search engine server logs to suggest relevant queries to user inputs. Among all propos-als, random walk-based methods [20, 17, 19] have exhibit-ed noticeable performance imp rovement when comparing to other models. The basic idea behind random walk models is quite straightforward. Queries and URLs are represent-ed as nodes in a bipartite graph where each edge connects one query with one URL, which indicates a click. A random walk model calculates the stable transition probability from one node to another and uses the probability to estimate the closeness between two nodes.

In [8], the authors used entropy to estimate the weight of edges instead of using the raw count. The model addressed the fact that various user clicks have different importance and therefore should be treated differently. For example, a click on a more specific URL is weighted higher than a click on a general (more common) URL. The authors introduced inverse query frequency (IQF) for edge weight estimation, which exhibited better performance than raw count-based random walk models.

The authors in [19] focused on a more difficult problem: suggest queries for rare queries. As rare queries often have very few or not click information, traditional click-based models are unable to perform well in this scenario. The authors therefore proposed to combine the information from clicked URLs and skipped URLs (URLs that are observed by the user who chose not to click) by constructing two bipar-tite graphs respectively. Two random walks are performed on each graph. Their results are combined optimally by minimizing the error function that calculates the correlation between URLs. Experiments suggested significant improve-ment over all other random walk models for rare queries. (Diversify Query Suggestions) Recently, authors in [15] proposed a framework for diversifying query suggestion results. Their method leveraged Markov random walk model on the query-URL bipartite graph by calculating the query hitting time. The model was capable of encouraging di-versities in the search results while keeping the queries se-mantically related. There are some key differences between our approach and theirs. First of all, we leverage the full SERP information which contains rich signals and features, while their approach only leveraged click information from log. Since it is well known that query-URL bipartite graph is often very sparse, the quality of suggestion may be im-pacted. Secondly, due to the sparseness of the query-URL Figure 2: An illustration of our framework. Step 1 generates suggestion candidates. Step 2 rank the candidates by applying a diversification function. graph, a rare query may not exist in the log and therefore the authors X  method cannot work in this case, which has no impact on our framework. Finally, our framework is quite flexible and responsive. Given a set of suggestions, we are able to rank them in real-time and adjust to the change of SERP accordingly, while their approach worked on a static set of query-URL logs that needs to retrain itself very often. ( Diversify Search Results ) There has been many re-searches that addressed the diversification of search result-s [1, 5, 25]. The basic idea is that search results should not only be relevant to the user query, but also relevant and different from each other. While resembles some of the techniques in this area, the objective of our research in this paper is quite different. Rather than focusing on improving the relevance of documents by re-ranking them, we aim at re-ranking suggested queries which help users refine their in-tent. Besides, previous researches mainly focused on with-in query document re-ranking for a particular query, while our approach handles between-query document list comparison X  arguably a more challenging problem. One concern is that most of the existing works on diversifying search results only focused on ambiguous queries where those queries have more than one user intents, while our approach does not possess such limitation and can be generalized to all queries. In general, these two approaches are orthogonal to each other and can both be leveraged to improve user search experience without affecting each other. ( Post-ranking Relevance Improvement )Tothebest of our knowledge, there exists only a few techniques that leverage post-ranking or SERP features to improve the re-trieval quality in real-time. In [22], the authors collect nega-tive implicit feedbacks from users to improve ranking. Specif-ically, when a user clicks the next page button on SERP, the authors assume that all previous results are irrelevant. Giv-en those top-N negative examples, the authors propose to re-rank the remaining U results in real-time. Consequently, the authors introduce two strategies for negative feedback: 1. modifies query directly given the negative feedback, and 2. combines scores from positive and negative feedbacks op-timally. Improvements are observed on benchmark TREC data sets over traditional retrieval systems.
Our framework is divided into primarily two steps. Dur-ing the first step, we generate query suggestion candidates from different sources based on log information. After that, we apply our diversification function to rank the candidates by leveraging post-ranking features. Figure 2 sketches the process.
Step 1 of our process focuses on generating candidates from search logs. In this step we put emphasis on the recall of the suggestion, therefore a raw set of candidates from different sources are generated, regardless of their relevance, and to be ranked in the next step.

The first source of candidates we collect comes from ran-dom walk models. A basic random walk with restart (RWR) model is applied to the query-click logs due to its simplicity and scalability. For detail implementation of RWR, we refer interested users to [20].

User session is another important source for generating candidates. A user session is defined by search engines which often contains all user activities within a certain period of time. Search engines usually record user queries, clicks, timestamps and other information in the log. If the user becomes inactive for a while (e.g., 30 minutes), that user session ends. There exists much useful information that can be used to extract relevant queries. Specifically, we pay at-tention to user refined queries within the same user session. A user refined query is defined as the query reformulated by the user which followed immediately after a no-click query. For example, a user issued query  X  X uper bowl X  but did not get the desired results. The user then reformulated the query to  X  X uper bowl 2011 X  and clicks on one of the results.
Note that there also exist many other sources that related queries can be discovered, which we do not explore in this paper. Since the focus of this paper is the diversification effect on query suggestion, this part only serves as a prelim-inary source that generates a set of query candidates, which could certainly be improved in the future work.
To achieve the diversification of search results while keep the suggested queries relevant, we introduce the following ranking function R : where Q o denotes the original query and Q s a suggested query.Thefirsttermontherighthandsideofeq.(1)mea-sures the SERP diversification between two queries, while the second term estimates the similarity between them.  X  is a tuning parameter which balances the contributions of these two terms. This equation indicates that queries with high diversification score and high similarity with the orig-inal queries are likely to achieve high ranking scores (for  X &gt; 0). Next, we discuss how the functions D and S are defined.

Since it is difficult to directly measure the diversification between two queries, we instead resort to post-ranking fea-tures, i.e., SERP features returned by search engines. Our assumption is similar to the traditional pseudo-relevance feedback strategy [11, 24] which treats all top results as rel-evant to the user queries. Specifically, the diversification of two queries is defined as the aggregated score over top N returned search results (SR): where SR Q o indicates the ordered search results list for query Q o . Furthermore, a set of features f is defined, where an adoption of a linear function for D that combines the scores from features is assumed.
 with w k measuring the weight of the k th feature. In Table 1, we list several features used in this paper. The first feature f 1 measures the similarity of the ODP categories between two URL sets SR Q o and SR Q s ,which aggregates the similarity over each individual pairs of URL-s. Specifically, the Open Directory Project (ODP) contains a repository of human-labeled URL categories with over 4 million URLs. A tree-based taxonomy is specified in ODP where lower levels indicate more specific categories, e.g., /Sports/Baseball/International/ . To determine the cat-egory of URLs, we leverage a sophisticated content-based hierarchical classifier [4]. Algorithm 1 sketches the method to calculate the similarity between two hierarchical labels. Basically, two labels are more similar if their lowest com-mon ancestor (LCA) is located at lower level. For example, as illustrated in Figure 3, label /Sports/Football/NFL/ has lower similarity with /Sports/Baseball/International/ than /Sports/Baseball/Amateur/ .

Features f 2 and f 3 check the similarity between URL strings and domain names. The indicator function I equals to 1 if the two strings are the same and 0 otherwise. A dis-counted denominator is applied to address the importance of the URL position. Similar to the DCG [10] measurement, if URL i and URL j are the same with very high SERP po-sitions (smaller values of i and j ), they will be penalized a lot by the denominator. Algorithm 1 URL Similarity Calculation 2: Initialize base =1, sim =0, denominator =0 3: for each level i in the tree 5: sim = sim + base 6: end if 7: denominator = denominator + base 8: base = base  X  m 9: end for 11: Output sim
Feature f 4 uses Kendall  X  rank correlation coefficient [12] to compute the correlation between two ordered SERP lists. A pair is said to be concordant if and only if both URLs are identical and ranked at the same position. Otherwise, they are discordant .

On the other hand, the second term S ( Q o ,Q s )ineq.(1) measures the string similarity between the query and a sug-gestion. A standard Levenshtein distance-based algorithm is applied here, with  X  a free parameter controlling its rel-ative weight. Algorithm 2 sketches the process. Note that there are lots of more sophisticated methods to calculate query similarity and arguably ours is an oversimplified one. However, since the focus of this paper is primarily on the diversification part, we leave this part for improvement in the future. Besides, as we shall see in the experiments, the query string similarity in general decreases with the increase of suggestion relevance. Nevertheless, even this oversimpli-fied function leads to significant performance improvement when combined with several diversification features. Combining eq.(2) and eq.(3) into eq.(1), we have
R ( Q o ,Q s )= which contains parameters  X  and w = { w 1 , ..., w k } .Were-name  X  = w 0 and expand w = { w 0 ,w 1 , ..., w k } to be the fea-ture weights to learn. The objective is therefore to optimize these parameters given a set of labeled query-suggestion Algorithm 2 Query Similarity Calculation 3: word distance = LevenshteinDistance(RemoveStopWords( Q 4: query similarity = word distance / max length 5: output query similarity pairs. Similar to the learning-to-rank [14] framework, we use a supervised learning approach in our paper.
We use human assessors to evaluate the relevance between query and suggestions. For each pair of query suggestion-s, the assessors are given the queries, their corresponding search result pages side-by-side, and asked to make a bina-ry decision of whether these two queries are related. Every pair is given to three independent assessors. The final score is the number of assessors who labeled that pair as related. Therefore, the label is always between 0 and 3 with 3 being the most related and 0 unrelated.

It is important to notice that in the judgment guidelines, the assessors are only asked to label the pairs as related or not based on their impressions on the SERP. Because if the assessors are asked to judge the diversification of the page or the similarity between queries, then the judgment labels may not reflect the true relevance of the suggested query. The weight of the diversification function should be learnt from the labels rather than being used as a guideline for judgment, since diversifying the SERP may or may not lead to a higher click through rate (CTR) on the suggested queries, before we have the true relevance labels.
Besides the  X  X elated X  label, we also ask assessors to judge if the two SERPs are totally identical on their top K ( K =3 in our guideline) results. If so, a  X  X uplicate X  label becomes positive which overrides the related label to be zero.
The general ranking function in eq. (4) presents us with opportunity to apply more than one categories of learning algorithms to the training data. Specifically, we learn the ranking function by using classification, linear regression, ordinal regression and learning-to-rank frameworks.
With four classes of labels ranging from 0 to 3, a multi-class Support Vector Machines (SVMs) classifier [21, 7] is applied to classify instances into one of the four classes. The objective of SVMs is to maximize the margin of the sepa-ration hyper plane while subject to minimizing the classi-fication error rate. The output of SVMs is a probability distribution over the classes. Therefore, the final ranking of an instance is calculated as follows:
For example, the instance with probability distribution of { of { 0 . 3 , 0 . 1 , 0 . 1 , 0 . 5 } which has a rank of 1.8, even though both instances are classified into class 3. This approach has shown very effective ranking results in previous models [13].
A linear regression model estimates the label (response) y i as a continuous variable given a set of C independent variables x i = { x i 1 , ...x iC } : with i being the error term. Linear regression optimizes by minimizing the sum-of-squared residuals (SSE). The output is directly used as the final ranking of the queries.
Ordinal regression is essentially an extension of the multi-nomial logistic regression, which is a regression model but generalizes logistic regression to discrete outputs. The short-coming of logistic regression is due to its ignorance of the orders of categories. Instead, ordinal regression considers this situation by using a logit function in the model: The model is usually estimated using maximum likelihood method. Previous results have indicated its superiority over logistic regression [6].
Unlike all aforementioned algorithms, the learning-to-rank framework directly optimizes the ranking loss, i.e., it focuses on the relative order between two list items. Therefore, the objective of learning is to directly minimize the number of misclassified pairs. Out of all loss functions used in learning-to-rank, Normalized Discounted Cumulated Gain (NDCG) is most commonly used. Specifically, NDCG is defined as the DCG score of a ranking result, divided by the ideal DCG, Algorithm 3 The LambdaSMART algorithm 1: Input : N =#ofqueries, M = # of iterations, 2: for i =1to N 4: end for 5: for m =1to M 6: for i =1to N 9: end for 11: for l =0to L 13: end for 14: for i =0to N 16: end for 17: end for where the DCG score is calculated by dividing the relevance score with the position of the ranking, where s ( i ) is the relevance score assigned by human asses-sors. The higher the NDCG score is, the better the ranking result is assumed.

We choose the LambdaSMART [23] algorithm as our frame-work due to its superior ranking performance. The algorith-m leverages the MART (multiple additive regression trees) boosting framework [9] to perform gradient descent at each iteration. Algorithm 3 sketches the framework.
This section presents experimental results on real-world data mined from a commercial Web search engine. We dis-cuss how the diversification function affects the relevance of related queries, as well as the importance of individual features and their combinations.
From search engine server logs, we randomly sampled 13,421 queries between September 2010 and November 2010. These are queries that trigger at least one related search on the search result page. The search engine we use generates re-lated queries from a combination of different resources, in-cluding but not limited to: random walk on the query-click graph, similar queries from the same user session, collabora-tive filtering from similar users and so on. Since generating query candidates is not the focus of this paper, we won X  X  discuss in detail about this part. However, we will show the improvement of our approach by comparing to one of the techniques in later sections.

As mentioned above, the query-suggestion pairs are given to assessors for evaluation and assigned with a score between 0 and 3. Table 2 summarizes the statistics of the data set. In Table 3, we show several query-suggestion pairs and their judgment labels.
We pre-define the following parameters in our experiments. Table 2: Statistics of data used in our experiment. For comparing SERP difference, we choose the top-10 ( N = 10) blue links returned by a major search engine. For the LambdaSMART algorithm, we choose M = 500 iterations and v =0 . 1 the shrinkage parameter. These parameters are fixed since the algorithms are relatively robust to their changes, as shown in previous researches [23]. We show the sensitivity of the remaining parameters in later sections.
Figure 4 plots the boxplots of feature values vs. the hu-man assigned labels. The first plot shows the query simi-larity vs. the labels, where the query similarity is used as a regularization factor in eq.(1). It can be observed that with the increase of relevance (from 0 to 3), the similarities be-tween queries and suggestions generally decrease. This find-ing is contradictory to many existing researches that lever-aged query similarity to find related queries, which strongly supports our introduction of the diversification metric. The second figure shows the ODP category similarity scores with the change of labels. We show that the most related query suggestions have higher category similarity than unrelated suggestions, which indicates that users are still interested in seeing more results from similar topics (categories), rather than switching to other intents.

The third and fourth figures plot the URL and domain similarity scores vs. labels. Both results demonstrate that the increase of relevance demands a decrease in result sim-ilarity. Comparatively, the decrease of URL similarity is more drastic than domain similarity, which makes sense s-ince URLs within the same domain might still exhibit dif-ferent topics which are potentially of interests to the users. The Kendall  X  rank shows similar results as the domain sim-ilarity in Figure 5.

We further use Pearson X  X   X  2 test [18] to test the correlation significance between features and labels. Table 4 depicts the results of  X  2 as well as their corresponding p -values, where in general high  X  2 values with low p -value indicates higher significance (for a fixed degree of freedom). It can be observed that all five features have strong correlation with the labels, with statistically significance given p -values &lt; 0 . 05.
In this section, we present the detailed results of the four learning algorithms. We separate the data into 60% train-ing, 10% validation and 30% testing for tuning the parame-ters. For multi-class SVM, we use the linear kernel since it performed better than sigmoid and polynomial kernels for our data. The best balancing factor c is tuned using cross validation and found to be 0.85. For the LambdaSMART algorithm, we specify L the number of nodes in the leaf to be 2 (stump) and 20, respectively. The weight of a feature from LambdaSMART is calculated according to the number of times that feature is used to split the decision tree. Table 5 lists the feature weights learnt from all models. In gener-al, the URL similarity and ODP category similarity features demonstrate the highest weights. The query similarity fea-ture ranks second to the last, which indicates the importance of diversification features in overall performance.
Next, we compare the ranking performance of these algo-rithms. As illustrated in eq.(8) and eq.(9), NDCG is the most popular metric for retrieval performance which ad-dresses both relevance and the position of the returned re-sults. Since most of the time search engines return no more than six suggested queries, we will measure NDCG@5 and NDCG@1 in our experiments. For regression, the output is used directly to rank the suggestions. For SVM, eq. (5) is used to calculate the ranking based on the classification output. Figure 5 summarizes the results of NDCG@1 and NDCG@5. While linear regression shows the worst perfor-mance, the LambdaSMART algorithm significantly outper-forms all others. In general, LambdaSMART achieves 0.79 and 0.74 for NDCG@1, with 20 and 2 leaf nodes respective-ly. For NDCG@5, it also performs well with 0.63 and 0.59 NDCG scores.

Furthermore, we illustrate the importance of individual features in retrieval performance. Specifically, we calculate the NDCG scores by using only one feature at a time, ver-sus combining the only similarity feature (Query) with each individual diversification features. Since in our previous ex-periment, LambdaSMART demonstrated significantly bet-ter performance than all others, we will only use LambdaS-MART for all our remaining experiments. Table 6 tabu-lates the results. In general, for individual feature compar-ison, ODP and URL similarity yield the highest NDCG s-cores. With the combination of similarity measurement and diversification criteria, we observe that Query+URL yields the best performance among all, whereas Query+ODP al-so shows competitive performance. Both combinations are very close to the optimal performance of LambdaSMART that uses all features, which suggests the usefulness of the similarity and diversification mixtures in real-world applica-tions.
Furthermore, we slice and dice the results to see how the proposed diversification measurement affects different types of queries. Statistically, in our data set, the average length of queries is 2.51, whereas the average length of suggestions sign up for hotmail Create Free Hotmail Account 1 youtube broadcast YouTube Videos 2 Table 3: Examples of query-suggestion pairs with their labels. Dup = 0 in our definition. Table 4: Pearson X  X   X  2 Test result with df =13 , 420 and corresponding p -values. is 3.02. We break down the queries into Long ( length &gt; 4), Medium (2  X  length  X  4) and Short ( length &lt; 2). In to-tal, there are 1350 long queries, 7551 medium queries, and 4283 short queries. On the other hand, we split queries into frequent queries and infrequent queries. Arguably, most fre-quent queries can be treated as navigational queries such as  X  X acebook X , while infrequent queries are mostly information-al queries. Figure 6 shows the log-log plot of query length vs. query frequency. In our experiment, we specify the top 1% most frequent queries to be navigational, and the bottom 50% to be informational 1 . Table 7 summarizes the statistics.
In Figure 7, we report the comparison for different query types in terms of NDCG values, where the overall NDCG of all queries (the top bar) is measured against all query types. It is evident to observe that informational queries gain higher NDCG scores than navigational queries, whereas short queries gain lower NDCG scores than medium and long queries. As shown in Table 7, the average suggestion lengths for short and navigational queries are much higher than the queries themselves. In our opinion, users who issue
Note that our cut-off threshold is not very strict, since the definition of navigational and informational are quite vague and varies from different data sets.
 Navigational 1.62 2.46 Informational 2.26 2.73 Table 7: Statistics of average query-suggestion lengths in several query types. navigational or short queries to search engines often have a quite clear intent in mind, and thus don X  X  click on suggested queries very often. On the other hand, users who issue longer queries are less clear about their intent, and more likely to use related queries to reformulate their intent.
Recall that our framework is essentially a ranking (or re-ranking) algorithm based on an existing set of query candi-dates. As discussed in Section 3.1, random walk techniques have shown noticeable performance on related search area. It is therefore important to see how much NDCG gain our framework can achieve by comparing to the results from random walks.

The underlying algorithm is quite straightforward which iteratively performs random walk on query-click bipartite graph until convergence. For details of using random walk on click graphs, we refer interested users to [20]. After random walk, each query and its related queries are represented in a vector, where each entry is a float point number indicating how close the suggestion is to the query. This number is naturally used to rank related queries. 1) two ordered URL lists LambdaSMART, we try both L =2 and 20 for the number of leaf nodes. NDCG@1 0.6466 0.6997 0.6606 0.5925 0.5641 0.7284 0.7245 0.6933 0.6873 NDCG@5 0.5728 0.6433 0.6082 0.5218 0.5039 0.6489 0.6407 0.6284 0.6177 Average 0.6097 0.6365 0.6694 0.5572 0.5340 0.6686 0.6826 0.6608 0.6525 delta airline sierra trading Sierra Equipment Sierra Trading Post Jobs
