 Understanding users X  search intent expressed through their search queries is crucial to Web search and online adver-tisement. Web query classification (QC) has been widely studied for this purpose. Most previous QC algorithms clas-sify individual queries without considering their context in-formation. However, as exemplified by the well-known ex-ample on query  X  X aguar X , many Web queries are short and ambiguous, whose real meanings are uncertain without the context information. In this paper, we incorporate context information into the problem of query classification by using conditional random field (CRF) models. In our approach, we use neighboring queries and their corresponding clicked URLs (Web pages) in search sessions as the context infor-mation. We perform extensive experiments on real world search logs and validate the effectiveness and efficiency of our approach. We show that we can improve the F 1 score by 52% as compared to other state-of-the-art baselines. H.3.m [ Information Storage and Retrieval ]: Miscella-neous; I.5.2 [ Pattern Recognition ]: Design Methodology-Classifier design and evaluation Algorithms, Experimentation Search context, Query classification
Search engines have become one of the most popular tools for Web users to find their desired information. As a result, understanding the search intent behind the queries issued  X  The work was done when Huanhuan Cao and Derek Hao Hu were interns at Microsoft Research Asia.
 by Web users has become an important research problem. Query classification (or query categorization ), denoted as QC, has been studied for this purpose by classifying user queries into a ranked list of predefined target categories. Such category information can be used to trigger the most appropriate vertical searches corresponding to a query, im-prove Web page ranking [17], and help find the relevant on-line advertisements.

Query classification is dramatically different from tradi-tional text classification because of two issues. First, Web queries are usually very short. As reported in [5], most queries contain only 2-3 terms. Second, many queries are ambiguous [10], and it is common that a query belongs to multiple categories. For example, [26] manually labels 800 randomly sampled queries from the public data set from ACM KDD Cup X 05 1 , and 682 queries have multiple cate-gory labels.

To address the above challenges, a variety of query clas-sification approaches have been proposed in the literature. In general, these approaches can be divided into three cate-gories. The first category tries to augment the queries with extra data, including the search results returned for a cer-tain query, the information from an existing corpus, or an intermediate taxonomy [7, 27]. The second category lever-ages unlabeled data to help improve the accuracy of super-vised learning [5]. Finally, the third category of approaches expands the training data by automatically labeling some queries in some click-through data via a self-training-like approach [20]. Although the existing methods may be suc-cessful in some cases, most of them are not context-aware; that is, they treat each query individually without consider-ing the user behavior history.

A MOTIVATING EXAMPLE . Suppose that a user issues a query  X  Michael Jordan  X . It is not clear whether the user is interested in the famous basketball player or the machine learning researcher at UC Berkeley. Without understanding the user X  X  search intent, many existing methods may classify the query into both categories  X  X ports X  and  X  X omputer Sci-ence X . However, if we find that the user has issued a query  X  NBA  X  right before  X  Michael Jordan  X , it is likely that the user is interested in the category of  X  X ports X . Conversely, if the user issues some queries related to machine learning before the query  X  Michael Jordan  X , it may suggest the user is interested in the topics related to  X  X omputer Science X .
Intuitively, using search context information, such as the adjacent queries in the same session as well as the clicked URLs of these queries, can help better understand users X  search intent and thus improve the classification accuracy. As shown in previous studies (e.g., [8, 9, 12]), adjacent queries raised by the same user are usually semantically re-lated. Moreover, compared with search queries, which are often short and ambiguous, the URLs that are selectively clicked by a user after issuing the queries may better reveal the search intent of the user.

As a first attempt to leverage context information in query classification, in this paper, we intend to answer the fol-lowing questions: 1) How do we model context information effectively and incorporate it into the problem of query clas-sification? 2) How much improvement can we achieve by using context information in query classification? 3) Would incorporating context information add too much computa-tional burden and would it be possible to extend the idea for real world commercial search engines?
To answer these questions, we propose to use the Con-ditional Random Field model ( CRF for short) [18] to help incorporate the search context information. We have several motivations for using this model. First, CRF is a sequential learning model which is particularly suitable for capturing the context information of queries. Second, the CRF model does not need any prior knowledge for the type of condi-tional distribution. Finally, compared with Hidden Markov Models, the CRF model is more flexible to incorporate richer features, such as the knowledge of an external Web directory.
In this paper, we show how CRF can be used for mod-eling the context information for query classification. We conduct extensive experiments on real world search logs to empirically evaluate our proposed model. Our experiments show that the CRF approach improves the F 1 score by as much as 52% as compared to the state-of-the-art baselines. Moreover, after the CRF model is trained offline, the online inference stage is very fast (e.g., less than 0.1 millisecond in our experiments), which makes our approach feasible to use in real world search engine systems.

The rest of this paper is organized as follows. In Section 2, we discuss the related work of QC. In Section 3, we describe the problem of context-aware query classification. We then briefly review the CRF model in Section 4 and present the features in CRF in Section 5. The experimental results are reported in Section 6. Finally, we conclude our paper and point out some future research directions in Section 7.
Given a query and a predefined taxonomy, the objective of query classification (QC) is to classify the query into a ranked list of categories which are leaf nodes of the tax-onomy. Previous studies on QC can be classified into two categories depending on the types of taxonomy.

In the first category, the taxonomy is defined by consid-ering the Web query types. In [6], Broder gave the first taxonomy of Web query types such as Navigational Queries, Informational Queries and Transactional Queries. Rose et al. [23] introduced a more complex taxonomy of Web query types based on the popular taxonomy proposed by Broder. However, both of their works do not deal with automatic classification based on a taxonomy of categories. Lee et al. [19] studied the classification problem and introduced an approach to classify Web queries into either Informa-tional Queries or Navigational Queries. Recently, the prob-lem of detecting commercial intent (OCI) attracts some re-searchers X  interest [11]. This problem is also a QC problem by considering Web queries types.

For the second category, a taxonomy is defined by con-sidering the topics of queries. Early work of query topic classification was done by manually classifying Web queries for query analysis, especially on the query topic distribu-tion [4]. Since it is expensive for manually classifying Web queries, it is an interesting problem to design an automatic approach for classifying Web queries with a taxonomy of topics. Early works on this problem only considered the local information of queries, i.e., the terms of queries [5]. However, as mentioned by [7], queries are usually short and carry very limited information. Recently, some works pro-posed to leverage the external Web knowledge to enrich the queries, such as extracting information from the top related search results of the query from a search engine [7] or taking advantage of a Web directory [27]. Given the fast growth of non-English web, some researchers studied the problem of cross-language query classification [22]. The approach pro-posed in this paper does not consider different languages.
In recent years, some researchers realized the importance of search context. In [15] and [8], two context-aware ap-proaches to query suggestion were proposed. Cao et al. [9] proposed a general context-aware model for query sugges-tion and ranking. These works confirmed that search con-texts are effective for disambiguating Web queries and can help improve the quality of multiple search services. How-ever, to the best of our knowledge, none of existing works on query classification considered the search context. In our approach, click information is considered as an important part of context. Though many existing works such as [8, 29] studied how to use click-information to enrich query X  X  semantic feature, none of them proposed an approach for leveraging click information for QC.

After proposed by Lafferty et al in [18], CRFs have been widely used in various domains such as named-entity recog-nition (NER) [21], identifying protein names in biology ab-stracts [25] and Web query refinement [13]. Some researchers also studied some variants of the basic Linear chain-CRF such as Skip-chain CRF [28]. In this paper, we use the basic Linear-chain CRF to model the query context.
In this section, we introduce several notations and then give a description of context-aware query classification based on the definition of traditional query classification problem in [27].

Definition 1. (Search context and contextual queries) . A user search session o is a series of observations o . . . o T , where each observation o t (1  X  t  X  T ) consists of a query q t and a set of URLs U t clicked by the user for q For any query q k (1 &lt; k  X  T ), the observations o 1 . . . o the search context of q k . In particular, the series of queries q . . . q k  X  1 are called the contextual queries of q k .
Various methods have been proposed for session segmen-tation in the literature [16]. In this paper, we adopt a sim-ple yet effective rule to divide sessions [8]; that is, two user queries are divided into different sessions if the interval be-tween their issued times is longer than 30 minutes. This rule has been widely used in previous works, e.g., [8, 15], and proved effective. Table 1 shows some examples of real user sessions. The symbol  X   X   X  indicates the user clicks a URL. Note that the query  X  gmc  X  is an ambiguous query that refers to the  X  X MC cars X  in session S 1 , but refers to the  X  X eneral Medical Council of Britain X  in session S 2 .
Definition 2. (Taxonomy) . A taxonomy  X  is a tree of categories where each node corresponds to a predefined cat-egory. The semantic meaning of each category is defined by the labels along the path from the root to the corresponding node.
 Figure 1 shows a part of the taxonomy of ACM KDD Cup X 05. Sometimes we need to map the categories of a taxonomy  X  to the categories of another taxonomy b  X  for reusing the category labels of  X . In this paper, we adopt an effective mapping method introduced by Shen et al. [27] as a part of our approach.

Definition 3. (Level-n Category, Ancestor Cate-gory, and Sibling Category) . Given a category c in a taxonomy  X , c is called a level-n category if the node of c is located at n -th level of  X . A category c  X  is a level-m ances-tor category of c , denoted by  X  m c ( m &lt; n ), if c  X  category and an ancestor node of c in  X . A category c # is an level-m sibling category of c , denoted by  X  m c ( m &lt; n ), if c # is at the same level with c and c # shares a common ancestor category  X  m c with c .

For instance, in Figure 1, given a level-2 category c  X  X iving \ Car&amp; Garage X ,  X  1 c is the level-1 ancestor category  X  X iving X  and  X  X iving \ Career&amp; Jobs X  is a level-1 sibling category of c . Problem Statement ( Context-aware query classifi-cation ). Given a target taxonomy  X , a user-specified pa-rameter K , and a user query q T , context-aware query classi-fication incorporates the search context of q T to classify q into a ranked list of K categories c T 1 , c T 2 ,..., c TK N c leaf categories { c 1 , c 2 , ..., c N c } of  X .
The Conditional Random Field (CRF) model is a discrim-inative graphical model, which focuses on modeling the con-ditional distribution of unobserved state sequences given an observation sequence [18]. The strength of processing se-quential data and incorporating rich features makes CRF model particularly suitable for context-aware query classifi-cation. Figure 2: Modeling search context by a Linear-chain CRF.
 As shown in Figure 2, in our problem, a Linear-chain CRF defines the conditional probability of a category la-bel sequence c = c 1 ...c T  X  1 c T given an observation sequence where Z ( o ) = tor and c 0 is an empty category label which is added for sim-plicity of defining the model. Potential functions  X  describe the Linear-chain transitions and the association between la-bels and observations. They are defined as: where f k is a feature function and  X  k is the weight of f Given training data D = { o ( n ) , c ( n ) } N n =1 , the objective of training a Linear-chain CRF is to find a set of parameters  X  = {  X  k } that maximize the conditional log-likelihood:
Once the parameters  X  have been learned using a training data set, we can infer the category label c  X  T for the test query q
T as c  X  T = arg max
When we use CRFs to model search contexts, one of the most important parts is to choose the effective feature func-tions. In this section, we introduce the features used for building a CRF model of the search context for QC. In gen-eral, the features can be divided into two categories. The features that do not rely on the context information are called local features , and those that are dependent on con-text information are called contextual features .
To leverage the local information of individual queries, we consider three types of features that associate queries with the corresponding category label, namely, query terms, pseudo feedback, and implicit feedback.
Given a query q t (1  X  t  X  T ) and its category label c t the elementary features that reflect the association between q and c t are the terms of q t . Suppose q t consists of a set of terms { t q t } , each t q t can be considered as a feature to support the category label c t . The weights of these features can be learned in the training process of the CRF model.
The problem of this type of features is that query terms are usually sparse. Consequently, the available training data are usually with limited size and could not cover a sufficient set of query terms that are useful for reflecting the associa-tion between queries and category labels. Therefore, given a new query whose partial, or all terms do not occur in the training data, this kind of features will not work.
The above problem is difficult to solve because it is hard to label a large number of sessions with a complex taxon-omy for a sufficiently large set of terms for all categories. For this reason, we also consider some other features that repre-sent the association between queries and category labels by leveraging some external Web knowledge.
This type of features exploits the top M results returned by an external Web directory. Given a query q t (1  X  t  X  T ) and its category label c t , we first submit q t to an external Web directory, such as the Google Directory [2] or Yahoo Directory [1], and get the top M search results. In the sec-ond step, for each of the top-M results, we follow the method in [27] and map its category label from a category in the Web directory X  X  taxonomy to a category in the target taxonomy. Finally, we calculate a general label confidence score : where M c t ,q t means the number of returned related search results of q t whose category labels are c t after mapping. In-tuitively, the GConf score reflects the confidence that q labeled as c t gained from pseudo feedback; the larger the score, the higher the confidence.
The third type of local features considers the click infor-mation as the implicit feedback from users. Similar to the type of features from pseudo feedback, we also exploit an external Web directory. However, we use the clicked URLs by users instead of the top-M results returned by the Web directory to enrich queries. To be more specific, given a query q t (1  X  t  X  T  X  1), let the set of clicked URLs of q U t = { u t } , the click-based label confidence score of c q is defined as: where CConf ( c t , u t ) means the confidence that c t is the most appropriate category label of u t .

We calculate CConf ( c t , u t ) in three steps. The first two steps are similar to those in calculating the general label confidence score. That is, we first submit q t to a Web direc-tory and then map the category of each top-M result to a corresponding category in the target taxonomy. After these two steps, we obtain a document collection for each possible category of q t in the target taxonomy, which will be used to calculate CConf ( c t , u t ). In the third step, we build a Vector Space Model (VSM) [24] for each category from its document collection and make the cosine similarity between the term vector of c t and the term vector of u t as CConf ( c t , u snippets of the web pages are used for generating the term vectors.

It is a special case that the top-M search results returned by the Web directory contain the clicked URL u t . In this case, u t is associated with a Web directory label e c t . De-noting the mapped category label of e c t as b c t , we define CConf ( b c t , u t ) = 1 and  X  c 6 = b c t CConf ( c, u t
Note that the CConf score is only applicable when the click information of q t is available. If a user does not click on any URL for q t , or q t is the current query to be classified, this score cannot be calculated.
To use the context information, we consider some features that can reflect the association between adjacent category labels. Occurrence of a pair of adjacent labels  X  c t  X  1 , c t  X  (1 &lt; t  X  T ) is an obvious feature of the association between adjacent labels, where c t  X  1 and c t are leaf categories in the target taxonomy. The higher the weight  X  c t  X  1 , c t  X  , the larger the probability c t  X  1 transits into c t . The weights of these fea-tures are learned from the training data during the training process of the CRF model.
Limited by the size of the training data, some transi-tion between categories may not occur in the training data. Moreover, the number of observed transitions may not re-flect the distribution in real world applications. Conse-quently, the CRF model may not be able to capture the direct association between categories properly.

To reduce the bias of training data, besides considering the feature of direct association between adjacent labels, we also consider the structure of the taxonomy. Intuitively, the association between two sibling categories is stronger than that of two non-sibling categories. For example, the category  X  X omputer \ Software X  is more relevant to  X  X omputer \ Hard-ware X  than to  X  X ive \ Career&amp; Jobs X . Please refer to Defini-tion 3 for the formal definition of sibling categories.
To be more specific, given a pair of adjacent labels  X  c t  X  1 where c t  X  1 and c t are both leaf categories at level n , we con-sider n  X  1 features of taxonomy-based association between c of these features are learned from the training data. This idea is similar to smoothing, where, if there are no train-ing data for the feature when  X  c t  X  1 , c t  X  occurs, there may still be some training data for the features at higher-level transitions  X   X  i c  X  t be the level-i siblings of c t  X  1 and c t , respectively. It is easy to see that  X   X  n c  X   X 
In this section, we validate our proposed methods through a systematic empirical comparisons with two baselines over a real data set.
We use the target taxonomy of ACM KDD Cup X 05 as our target taxonomy, which is widely used in the literature for QC. This taxonomy is a two-level taxonomy and has seven level-1 categories and 67 level-2 categories.

We randomly extract 10,000 sessions from one day X  X  search log of a major commercial search engine under the session segmentation rule mentioned in Section 3. In this paper, all the extracted sessions contain at least two queries so that we can exploit the impact of contextual information for query classification. The proportion of sessions with more than one query is usually not small. As shown in [14], about 55% sessions have more than one query in a search log of the Excite search engine. Our search log also shows that there are more than 45% such sessions among all sessions. It implies that our approach can help in many cases. Moreover, the queries in single query sessions are mostly  X  X asy queries X  that have clear meanings and are easy to be classified. In the extracted sessions, there are 23,091 unique queries and 32,410 unique clicked URLs in total. Figure 3: Distributions of (a) session lengths and (b) query frequencies of the training data.

Figure 3 (a) and Figure 3 (b) show the session length dis-tribution and the query frequency distribution of the data set, respectively. From these two figures we can see that in this data set, both the distribution of session lengths and the distribution of query frequencies roughly follow the power law. This phenomenon is consistent with some previ-ous analysis on large scale search logs [8].

We invited three human labelers to label the queries of each session with the 67 level-2 category labels. For each query, a labeler gives a most appropriate category label by considering not only the query itself, but also the search con-text and the clicked URLs of the query. A query X  X  final label is voted by the three labelers. Since each query is associated with context information (except for the beginning queries of sessions) and real user clicks which can help determine the meaning or intent of the query, the consistency among the labelers is quite high. For more than 90% queries, the three labelers give the same labels. This is very different from the general query classification problem [27].
Figure 4 shows the category distribution of the labeled queries. From this figure, we can see the category labels of the queries in our data set cover all seven level-1 categories.
In this paper, we adopt two baselines to evaluate the per-formance of our approach: Figure 4: Distribution of different category labels in the training data. Given a test session q 1 q 2 ...q T , we take the last query q the test query and take the queries q 1 q 2 ...q T  X  1 and their cor-responding clicked URL sets U 1 U 2 ...U T  X  1 as the search con-text. In order to evaluate the performance of our approach and the two baselines on the task of query classification with search context, we use three metrics, namely, overall preci-sion, overall recall and overall F 1 score. For a test query q
T with the true category label c T , given the classification results C T,K where C T,K is a set of the top K predicted cat-egory labels from a tested approach, the precision( P ) for q tion of indicating whether  X  is true(=1) or false(=0). The recall( R ) for q T is represented as  X  ( c T  X  C T,K ) and the F is calculated as test cases and P n means the precision for the n th test query. The overall recall and overall F 1 score are both calculated in similar ways.

To reduce the uncertainty of splitting the data into train-ing data and test data, we adopt a ten-fold cross validation as follow: 1) Firstly we randomly partition the labeled ses-sions into ten folds; 2) Then we take each of the ten folds as test data and the remaining nine folds as training data; 3) Finally, we report the average performance of the ten runs.
In order to study the contribution of context information, we compare three CRF models with different features: CRF-B ( CRF with Basic features 2 ), CRF-B-C ( CRF with Basic features + Click-based label confidence ) and CRF-B-C-T ( CRF with Basic features + Click-based label confidence + Taxonomy-based association ), respectively. In our exper-iments, we choose Google Directory as our external Web directory for calculating general label confidence and click-based label confidence. We set M , i.e., the number of used search results of a Web directory, to be 10, which equals the number of search results in a search page.

In this section, we evaluate the overall precision, overall recall and overall F 1 score with different K for each tested approach. We set the maximum K to be 5. Figure 5: The average overall precision of CRF-B, CRF-B-C, CRF-B-C-T and two baselines with dif-ferent K .
 Figure 5 compares the average overall precision of CRF-B, CRF-B-C, CRF-B-C-T to the two baselines with different K values. From this figure we can see that all tested ap-proaches X  average overall precision numbers drop when we increase K . Compared with the non-context-aware baseline BC, the average overall precision of CRF-B, CRF-B-C and CRF-B-C-T is improved across different K by 50%, 52% and 57% , respectively. Compared with the naive context-aware baseline CC, average overall precision of CRF-B, CRF-B-C and CRF-B-C-T is also improved by 2%, 3% and 7%, re-spectively.
 Similarly, Figure 6 compares the average overall recall of CRF-B, CRF-B-C, CRF-B-C-T and the two baselines with different K . From this figure we can see that all tested approaches X  average overall recall values increase when we increase K . It is reasonable because the probability that the ground truth label is covered by the predicted results will in-crease with more predicted category labels. Compared with the non-context-aware baseline BC, the average overall re-call of CRF-B, CRF-B-C and CRF-B-C-T is improved across Figure 6: The average overall recall of CRF-B, CRF-B-C, CRF-B-C-T and two baselines with different K . different K by 33%, 35% and 37% , respectively. Compared with the naive context-aware baseline CC, the average over-all precision of CRF-B, CRF-B-C and CRF-B-C-T is also improved by 2%, 3% and 4%, respectively. Figure 7: The average overall F 1 scores of CRF-B, CRF-B-C, CRF-B-C-T and two baselines with dif-ferent K .
 Figure 7 compares the average overall F 1 scores of CRF-B, CRF-B-C, CRF-B-C-T and the two baselines with different K . From this figure, we can see the CRF-B, CRF-B-C and CRF-B-C-T can improve the average F 1 scores by 46%, 48% and 52%, respectively, when compared to the non-context-aware baseline BC. Compared with the naive context-aware baseline CC, the average overall F 1 scores of CRF-B, CRF-B-C and CRF-B-C-T are also improved by 2%, 3% and 6%, respectively.

We conduct a series of paired T-tests of 0.95 confidence level which show that the improvements of our approaches on overall precision, overall recall and overall F 1 are all sta-tistically significant. We also study the variances of overall precision, overall recall and overall F 1 scores of all tested approaches in the ten-fold cross validation. Table 2 shows the mean deviations of these values of each tested approach in the ten-fold cross validation with K = 1 and K = 2, re-spectively. Notice that when K = 1, the overall precision, overall recall and over all F 1 scores are same for each tested approach. From this table we can see that the variances of all three CRFs X  performance are consistently smaller than the collaborating classifier. It implies that there is indeed a major advantage of using CRFs for extracting context in-formation, as compared to the collaborating classifier based on a naive context-aware strategy.

We also compare the performance of our proposed ap-proaches and the two baseline methods on user-session data with different lengths, where the shortest length is two. From the experiments, we find that the performance of all tested approaches on length-two sessions is a little better than sessions with more queries. This is because it is often the case that the shorter the sessions are, the more likely the queries are common queries that are easy to be classi-Table 2: Mean deviations of overall precision, overall recall and overall F 1 scores of each tested approach in the ten-fold cross validation. fied. Moreover, for sessions with more than two queries, we compare the performance of CRFs by considering different lengths of search context. We find that considering longer search context does not significantly improve the perfor-mance as compared to considering only one previous query and its corresponding clicked URLs.

From the above experiments, we can come to the following conclusions: 1) Firstly, all three CRF models and collabo-rating classifier consistently outperform the bridging classi-fier on the task of query classification given search context, which implies the effectiveness of context information; 2) Secondly, all three CRF models consistently outperform the collaborating classifier, which is a naive context-aware base-line. It implies that it X  X  an effective approach of modeling context information by CRFs; 3) Thirdly, CRF-B-C outper-forms CRF-B, which shows that click information is a good source of context information for query classification; 4) Fi-nally, CRF-B-C-T outperforms CRF-B-C, which indicates that the taxonomy-based association between adjacent la-bels is useful for the query classification problem with search context. In addition to the study on the overall performance of CRF-B, CRF-B-C, CRF-B-C-T and the two baselines, we also study the cases in which our approach outperforms the baselines.
 Table 3: An example of query classification with a search context.

Table 3 shows an example of query classification with a search context. In this example, the test query is  X  santa fe new mexico  X . Without considering the context, this query may have multiple possible search intents. One possible in-tent is that the user wants to know some general information of the city of Santa Fe, such as the area, the population of this city, etc. In this case, the query should be classified into the  X  X nformation \ Local &amp; Regional X  category. Another possible intent is that the user wants to go on a vocation in the city of Santa Fe and need some travel information about this city, such as hotels and tourist attractions. In this case, the query should be classified into the  X  X iving \ Travel &amp; Va-cation X  category. However, given the context with the query  X  travel guide  X  in which the user visits a web site related to travel, the appropriate category of this query should be narrowed down to  X  X iving \ Travel &amp; Vacation X . From Ta-ble 3, we can see that both CRF-B-C-T and the collaborat-ing classifier give the correct category label in the first po-sition because they consider contextual information, while the bridging classifier X  X  first label is not appropriate. This case exemplifies the effectiveness of considering context in-formation.
 Table 4: Another example of query classification with a search context.

Table 4 shows another example of query classification given the search context. In this example, the test query is  X  FIFA news  X . Without considering the context, this query may have two possible meanings: news of the International Fed-eration of Association Football, or news on a soccer video game named  X  FIFA  X . And the corresponding categories are  X  X ports \ Soccer X  and  X  X ntertainment \ Games &amp; Toys X , re-spectively. However, given the context that the user has issued a query  X  FIFA  X  and clicked a URL which is related to the video game  X  X IFA X  , the appropriate category is most likely X  X ntertainment \ Games &amp; Toys X . From Table 4 we can see that CRF-B-C-T gives the correct category label in the first position, while the collaborating classifier and bridging classifier X  X  first labels are not appropriate. This case exem-plifies that CRF-B-C-T leverage search context better than the collaborating classifier. Our approach consists of an offline part and an online part. In the offline part, the time cost of our approach comes from the training cost for the CRF model. Figure 8 (a), (b) and (c) show the convergence curves of CRF-B, CRF-B-C and CRF-B-C-T, respectively. From these figures we can see the objective function value of CRF-B-C converges to a better optima as compared to CRF-B and the objective function value of CRF-B-C-T converges to a better optima point than CRF-B-C. This implies that considering click information and taxonomy-based association between adjacent category labels can help build a stronger CRF model. The training algorithms are implemented on an Intel Core2 2  X  2.0G, 4G main memory machine. Each iteration of these algorithms takes about 300 milliseconds. Therefore, the time cost of training a CRF is acceptable as an off line process. Figure 8: Objective function values per iteration of training CRF-B, CRF-B-C and CRF-B-C-T.

It is well known that Web users often have strict require-ments on the response time of online applications. Thus, the efficiency of an online application is an important prob-lem. In the online part, the time cost of our approach comes from calculating features and inference. In the stage of cal-culating features, the main cost comes from the process of calculating label confidence. This process can be very fast for a commercial search engine since most modern search engines have their own Web directories locally. Moreover, if we calculate these features offline in advance and store them in local servers, the process will be even faster. Besides, the stage of reference is very fast (less than 0.1 millisecond). This is because usually the length of search context is short and the number of possible categories for a query is small as well. For improving the efficiency of inference further, we can consider only one previous query and its correspond-ing clicked URLs as search context, since our experiments show that such context information is effective enough for improving the quality of QC significantly.
Web query classification is an important problem with wide applications. However, although many existing works have studied this problem, none of them considered the search context together with query classification. In this paper, we propose a novel approach for leveraging context information to classify queries by modeling search context though CRFs. Experiments on a real data set extracted from a commercial search engine log clearly show that our ap-proach consistently outperforms a non-context-aware base-line and a naive context-aware baseline.

Our current approach cannot handle the first-query prob-lem well, which is the problem of not being able to find a search context if the query is located at the beginning of a search session. However, if we can capture some events that occurred a little earlier at the beginning of the session, such as events of Web page browsing, we can solve the first query problem well. In our future research, we plan to study this problem in detail. Huanhuan Cao and Enhong Chen thank the support of MSRA Internet Services Theme. Qiang Yang and Derek Hao Hu thank the support of Microsoft Research project MRA07/08.EG01.
