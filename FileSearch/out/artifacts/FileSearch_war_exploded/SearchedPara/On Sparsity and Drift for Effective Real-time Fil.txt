 In this paper, we approach the problem of real-time filter-ing in the Twitter Microblogging platform. We adapt an effective traditional news filtering technique, which uses a text classifier inspired by Rocchio X  X  relevance feedback al-gorithm, to build and dynamically update a profile of the user X  X  interests in real-time. In our adaptation, we tackle two challenges that are particularly prevalent in Twitter: sparsity and drift. In particular, sparsity stems from the brevity of tweets, while drift occurs as events related to the topic develop or the interests of the user change. First, to tackle the acute sparsity problem, we apply query expansion to derive terms or related tweets for a richer initialisation of the user interests within the profile. Second, to deal with drift, we modify the user profile to balance between the im-portance of the short-term interests, i.e. emerging subtopics, and the long-term interests in the overall topic. Moreover, we investigate an event detection method from Twitter and newswire streams to predict times at which drift may hap-pen. Through experiments using the TREC Microblog track 2012, we show that our approach is effective for a number of common filtering metrics such as the user X  X  utility, and that it compares favourably with state-of-the-art news filtering baselines. Our results also uncover the impact of different factors on handling topic drifting.
 [ H.3.3 Information Search and Retrieval ]: Information filtering Real-time filtering; Microblogs; Event detection
Social media have grown as massive networks of infor-mation publishers and consumers. In these networks, con-sumers may have difficulties to keep up with the vast amounts of real-time information and publishers have no way to en-sure that their content can reach their targeted audience. In-formation filtering (IF) [5] can help both publishers and con-sumers by ensuring that only relevant information is deliv-ered to the right audiences. We aim to approach an emerging problem in the area of IF on social media. In particular, we focus on the Twitter microblogging platform, as it is one of the most popular and fastest growing social media plat-forms. Information filtering in Twitter faces unique chal-lenges that do not necessarily exist in traditional domains such as newswire, which have been extensively studied in the literature, e.g. [2, 18]. Indeed, the nature of tweets makes them dissimilar to news articles for several reasons. Unlike news articles, tweets are a form of user-generated text. A tweet is a very short text, as it is bounded with a 140 char-acter limit, and typically consists of few terms (around 11 terms on average [10]). Moreover, Twitter operates in a real-time fashion where users can immediately access tweets posted by tweeters they are subscribed to (their social space) or in fact any public tweet. More interestingly, users in Twit-ter may quickly reflect on news and events happening in the real world [12, 22]. Finally, Twitter has been rapidly grow-ing in terms of active users and volume of activity, which can reach hundreds of millions of tweets in a day [25].
In this paper, we study the problem of real-time filtering in Twitter. We devise a solution by building on an effective news filtering technique that is based on the text classifica-tion approach of Incremental Rocchio [2]. In particular, we introduce novel adaptations to Incremental Rocchio to deal with the unique challenges in Twitter. The first challenge is the shortness of the documents (tweets), a sparsity problem that is less prevalent in the news domain and therefore tra-ditional filtering approaches may not work as effectively on tweets. To deal with the acute sparsity issue, we propose to use a query expansion (QE) approach to enrich the represen-tation of the user X  X  profile (the explicit relevant judgments of the user) during the filtering process. In particular, our QE approach derives additional terms and documents that are relevant and timely during the filtering process. The second challenge is the potential drift over time where the interests of the user swing between different aspects (subtopics) of the more general topic or particular aspects of the topic become more popular than other aspects over time. We hypothesise that events in the real world may be the main reason behind this drift. To give the reader a concrete example, let us say the topic of filtering is a football match. A few days before the match, the user will be interested in tweets about which players are going to play or who are going to miss. Dur-ing the match, the interest drifts towards goals and after the match it drifts towards reactions on the game. To tackle this topic drift issue, we propose to modify the classifier such that it recognises short-term interests (emerging subtopics), and balances between the importance of short-term interests and the long-term interests in the overall topic. However, our modification requires correctly identifying what is con-sidered to be the short-term interests in the topic, which can be a challenging task. For example, while filtering, we can consider the explicit user judgments in an arbitrary in-terval, e.g. the current day, to represent the short-term in-terests, but the drift may occur more than once during the day. Since events may be the main reason for topic drifts, we explore the automatic detection of such events from either the Twitter stream itself or external newswire streams, to identify potential times of drift to other aspects of the topic and to capture the short-term interests automatically. Using the real-time filtering task of the Microblog track of TREC 2012, we thoroughly evaluate our adaptations against the standard Incremental Rocchio and a state-of-the-art news filtering technique based on Regularised Logistic Regres-sion [30]. Our empirical results show that the traditional news filtering techniques are not effective in the context of tweet filtering. They also show the power of the adaptations we propose to improve these filtering techniques and miti-gate the problems of sparsity and topic drifting in Twitter. The remainder of the paper is structured as follows. In Section 2, we present related work. Section 3 provides a for-mal description of the real-time tweet filtering problem and describes the filtering approaches we build on. Section 4 de-scribes the sparsity challenge and our query expansion ap-proach to deal with it, while Section 5 describes the challenge of topic drifting and our solution to it. Section 6 describes our experimental methodology and Section 7 reports and discusses the results of the experiments. Finally, Section 8 summarises our conclusions and future work directions.
IF and information retrieval (IR) are similar in the sense that they both aim to provide users with access to informa-tion and they deal with the same kind of information [5]. However, the fundamental difference is that IF deals with information needs spanning over a long period of time, i.e. a user profile , rather than a one-shot query that vanishes at the end of a search session. The user profile can be maintained over time and can possibly be improved if the user provides explicit feedback to the system. This is the case of adaptive filtering [2], which is the focus of this paper. Adaptive fil-tering has been studied extensively in the news domain and several approaches were proposed to tackle adaptive filter-ing in news, e.g. [6, 18, 30]. Some adaptive news filtering approaches treat the problem as a retrieval task and intro-duce an adaptive threshold on the retrieval score to make a binary decision. The threshold in this case can be defined as a function of the retrieval scores for documents which are explicitly judged relevant by the user. Examples include Bayesian inference on content representation nodes [6] and thresholding on the Okapi probabilistic model [18]. Other approaches are those which treat filtering as a special case of a single-label text classification, where relevance feedback is then used to update the classifier. An example of such approaches is Incremental Rocchio [2], which uses Rocchio X  X  relevance feedback in the vector space model. Incremental Rocchio has shown to be effective in news filtering and it was among the best performing approaches in the TREC Filter-ing track [19]. Text classification with logistic regression has also been popular in adaptive news filtering and is considered as the state-of-the-art news filtering approach. For example, Zhang [30] introduces a news filtering method that employs a regularised logistic regression model using a Bayesian frame-work, which outperformed Incremental Rocchio. Similarly, Yang et al. [27] found that a regularised regression model is more robust than Incremental Rocchio when tuning its pa-rameters across different corpora. As a summary, Incremen-tal Rocchio and Regularised Logistic Regression are effective traditional news filtering methods. We aim, in this paper, to investigate whether these methods are effective when applied on tweet filtering. We also build on Incremental Rocchio to tackle specific challenges in tweet filtering.

Despite of the wealth of research in adaptive filtering of news, there has been little work done on adaptive filtering of tweets until TREC has recently introduced the real-time filtering task in the Microblog track 2012. However, since we build on a text classification approach to perform adaptive tweet filtering, it is worth reviewing work on tweet classi-fication to highlight challenges that can be relevant to our problem. For example, Hu et al. [11] studied tweet clus-tering where they used a bag-of-word approach. They ad-dressed the sparsity problem stemming from the shortness of the tweets. Using Wikipedia as an external knowledge source, they extracted useful related phrases that can en-rich the cluster representation. Similarly, Sriram et al. [23] proposed to use metadata about the author of the tweet to enrich the bag-of-word representation of the tweets. Other work on short user-generated content includes the classifi-cation of SMS, short emails and blog comments. Cormack et al. [7] studied spam classification in this type of content and pointed out that shortness may have an impact on the classifier X  X  performance. They suggested performing lexi-cal expansion to derive a richer word and character n-gram representation, which can account for the shortness of mes-sages. We also aim to address the sparsity issue but instead of using external knowledge sources as in some of the afore-mentioned techniques, we apply query expansion that relies on a pseudo-relevance feedback approach to extract terms that are both relevant and timely.
As discussed in Section 2, adaptive filtering was previ-ously investigated in the TREC Filtering track [19] within the news domain. In adaptive filtering, and unlike a tradi-tional search query, user information needs reflect a long-term interest and they are represented in a user X  X  profile, which is usually a set of documents considered relevant by the user [5]. Moreover, instead of searching a static doc-ument collection, an adaptive filtering system examines a stream of incoming documents over time and decides for each document whether it matches the user X  X  profile such that it is displayed immediately to the user [2]. Generally, the filtering system starts with a user profile and a very small number of positive feedback examples (relevant documents). The profile can then be adapted using the feedback informa-tion provided by the user for the displayed documents [5]. The problem we are tackling in this paper is the real-time adaptive filtering of tweets. In the remainder of the section, we first provide a formal description of the real-time tweet filtering problem. We then describe the state-of-the-art news filtering methods we build on for our solution.
We regard the problem of real-time tweet filtering as an instance of the adaptive filtering problem. Given a user u with an initial information need, i.e. an input query q , at a certain starting time t s and a small set of positive example tweets prior to t s , the filtering system should decide whether subsequent tweets posted after t s are relevant to the user and therefore should be displayed to the user. This should allow the user to stay updated in real-time by browsing relevant tweets for a developing topic. The user can examine these tweets and provide explicit feedback to the filtering system whether they are relevant or not. The filtering system can hence adapt the user X  X  profile, which is defined to be the set of feedback tweets provided by the user. Indeed, Twitter al-ready implements a filtering functionality ( X  X  new tweets X ) in its search page, 1 however it does not allow the user to provide explicit judgments and, to our knowledge, there is no study published on the effectiveness of their filtering tool.
We introduce two effective adaptive filtering methods on news that can be employed on tweets, namely Incremental Rocchio [2], and Regularised Logistic Regression [30].
This method is based on a classifier that uses the pop-ular Rocchio X  X  relevance feedback approach [21] to build a profile of the user X  X  interests, which is then updated online using the explicit judgements provided by the user [2]. More specifically, at each point of time t , the profile of the user is represented in the term vector space model by a vector ~c , which is called the centroid of the user X  X  interests. The centroid is calculated as follows: where R t is the set of tweets judged relevant by the user so far (at time t ), N t is the set of tweets judged non-relevant by the user so far (at time t ),  X  and  X  are co-efficient pa-rameters for positive and negative feedback documents re-spectively. For each incoming tweet ~ d , the cosine similarity is computed between the centroid at the time at which the tweet arrives and the actual tweet Sim ( ~c t , ~ d ). If the cosine value Sim ( ~c t , ~ d ) exceeds a certain threshold  X  R , the tweet is predicted relevant and is therefore displayed to the user, otherwise it is not. When the tweet is judged relevant by the user it will be added to the set of relevant tweets for the next time point R t +1 , otherwise it will be added to N t +1 , and as a result the centroid c t +1 will be updated. Our initial ex-periments have revealed that penalising negative documents does not improve tweet filtering effectiveness and hence we only consider positive feedback documents (  X  = 0), i.e the centroid is reduced to: The terms in the vector space are weighted using any ap-propriate term weighting models such as BM25 [20].
This state-of-the-art filtering method is based on a logistic regression learner with a Gaussian prior [30]. More specifi-cally, logistic regression estimates the posterior probability https://twitter.com/search/ of an unobserved variable (a topic) given an observed vari-able (a tweet) using a log linear function: where ~x is a tweet vector in the term vector space, ~w is the vector of regression coefficients, and y  X  { 0 , 1 } is the boolean topic output variable corresponding to whether the tweet ~x is non-relevant or relevant to the topic. 2 adaptive filtering, at each point of time t , the profile of the user is represented with the training set of labelled tweets D is applied to find the Maximum a Posteriori (MAP) estima-tion of the regression coefficients: ~w MAP = argmin Equation (4) has a unique and numerically stable solution using a stochastic gradient descent (SGD) algorithm [29]. The second term in this objective function is equal to adding a Gaussian prior with the mean ~ X  and variance covariance matrix 1 / 2  X .I , where I is the identity matrix. Following the work in [27], we fix the mean ~ X  of the Gaussian prior to 0, and tune the variance parameter  X  for the the best filtering performance. Once the regression coefficients are estimated, the filtering prediction can be made for each incoming tweet ~x by calculating the posterior probability P ( y | ~x, ~w ). If it exceeds a certain threshold  X  L , then the tweet is considered relevant, otherwise it is not. Note that ~w is constantly up-dated whenever a relevance judgment is available from the explicit feedback provided by the user. As in Incremental Rocchio, the terms in the vector space are weighted using any appropriate term weighting model. The Regularised Lo-gistic Regression has shown to outperform Incremental Roc-chio in adaptive news filtering [27, 30]. In our experiments, we will investigate the effectiveness of both the Incremen-tal Rocchio and the Regularised Logistic Regression news filtering approaches when applied on tweets.

In addition, we also discuss in the following sections how we modify Incremental Rocchio to tackle specific challenges in filtering tweets. We choose to devise these modifications on Incremental Rocchio due to its generative nature, which may well suit adaptive filtering in the highly dynamic envi-ronment of Twitter, in comparison to the more complex dis-criminative regression approach that requires a large number of training examples to have a stable performance.
The acute sparsity issue in filtering tweets is a unique chal-lenge caused by the shortness of tweets. As discussed before, this problem is less prevalent, for instance, in traditional fil-tering tasks, such as adaptive filtering of news streams for which the Incremental Rocchio method we build on was orig-inally designed. Sparsity is particularly problematic when we know little about the user X  X  interests or the topic itself, i.e. when the filtering system knows only a small number of documents (tweets) that the user is interested in. As a result, the centroid of the Rocchio X  X  classifier has a small number of terms, which may not be representative of the topic or the user X  X  interests. ~x is identical to ~ d , but we use the notation ~x instead for uniformity with conventional logistic regression notations.
To tackle the sparsity of the initial centroid representa-tion in incremental Rocchio, our approach is to make use of query expansion (QE), a traditionally successful method for improving the retrieval performance in a number of IR tasks, such as adhoc retrieval [26]. In a static document col-lection, QE automatically derives terms that can be added to a user X  X  original query from a pseudo-relevant set of doc-uments (the initial set of highly ranked documents retrieved for the query). We apply QE as a mechanism to enrich our knowledge of the topic and the user X  X  interests by deriving terms that are both topically relevant and timely. For this, and to initialise our classifier, we apply QE using the user X  X  query and a document collection comprising tweets publicly posted up to the time of issuing the query or triggering an in-terest in a tweet (by providing an explicit positive feedback).
Formally, at a certain time t i and for a user with a query q , we use a set of tweets that were posted before t i . We denote this set of tweets by T s . We limit T s to an arbitrary period of time before t i . We can then apply QE to score terms in the pseudo-relevant set of tweets T q  X  T s . We denote this set of terms by E . In particular, the centroid of the classifier at a certain time t can be computed as follows: where ~e represents a vector that is comprised of all the terms in the expansion set E weighted with their scores provided by the QE weighting model. This vector can be expressed formally as ~e = X basis vector for dimension (term) e i . Note that ~e is particu-larly useful when the set of positive feedback documents R is small, e.g. only one tweet, which is the main purpose of adding this component to the centroid.

Furthermore, we investigate adding the entire set of pseudo-relevant tweets to the centroid. In this case, the vector is calculated as follows:
As for the documents in R t , each document in T q is weighted with an appropriate term weighting model.
One of the challenges of filtering in general is topic drift-ing [2]. This can occur because the interest of the user may change or events related to the considered topic develop. In particular, we aim to address the problem of topic drift in adaptive tweet filtering, where the problem is particularly prevalent due to the highly dynamic environment of Twit-ter [25]. We first discuss this phenomenon with examples from the TREC Microblog track. Then, we propose our methods to deal with drifting and enhance the effectiveness of filtering tweets.
To illustrate how topic drifting occurs and affects the real-time filtering of tweets, we perform the following analysis. Considering the training topics of the real-time filtering task of the Microblog track 2012, we examine the relevant tweets for each topic that a real-time filtering system should trigger as relevant and present to the user, who eventually provides positive feedback indicating they are actually relevant to the topic. The main purpose is to understand how the topic develops over time. To achieve this, we analysed over time the distribution of terms in those relevant tweets. Figure 1 presents three coloured grids representing the distribution of stemmed terms in relevant tweets for each day in the considered filtering period. The intensity of the colour in each cell represents the frequency of the corresponding term in the relevant tweets during the corresponding day. For each topic, we only consider terms of a medium frequency within the relevant tweets. The reason for discarding the most frequent terms is that they do appear in almost all the relevant tweets. They represent the long-term aspects of the topic and would not give insights on how the topic develops. Moreover, terms of low frequency only appears occasionally and may not be representative of certain aspects of the topic.
By inspection of the grids, it can be observed that there are certain terms which peak on different days. This in-dicates that there are different aspects (subtopics) of the original topic that become more popular over time, or there are certain events that occurred and drifted the topics into new aspects. To validate this observation, we manually ex-amine the content of the relevant tweets for each topic in question over time as well as the content of any hyperlinked document in these tweets. For example, for the first topic  X  X BC World Service staff cuts X , on the first day (Jan 24), the terms  X  X nline X  and  X  X udget X  were important. This is the day when the BBC announced that it was  X  X utting its online budget X  and as a result more tweets containing those terms were publicly posted on Twitter. Later, these terms were rarely used. Moreover, on the third day (Jan 26), we can ob-serve that other terms started to emerge. On that day, two different stories were spread about the topic of BBC cuts. The first one is that the BBC is going to  X  X lash 650 jobs X  and the second one is about  X  X utting five BBC language ser-vices X . As a result, the terms  X 650 X ,  X  X anguage X  and  X  X ervice X  have become dominant on that day. A similar discussion applies to the second and the third grids corresponding to the other two topics (21 and 41). For topic 21  X  X manuel res-idency court rulings X , Jan 24 corresponds to the day when the news about  X  X ahm Emanuel being booted off ballot by the Appellate court and becoming ineligible for the mayoral race X  emerged. Later on Jan 26, several tweets were posted saying that  X  X he State high court will hear Rahm X  X  appeal X . For topic 41  X  X bama birth certificate X , Jan 25 is the day where the news spread about  X  X  swear of a Hawaii official that Obama birth certificate does not exist X . Later on Jan 28, several tweets were posted about  X  X  new Hawaii bill that would make it possible to release Obama X  X  birth records X .
As a summary, we can see that certain terms become pop-ular at different times as the topic develops and this may be due to the occurrence of real-world events. To deal with drifting, we propose to make a distinction in the user pro-file between the short-term interest in emerging subtopics and the long-term interest in the overall topic. In the fol-lowing, we describe how the Incremental Rocchio approach, introduced in Section 3, can be modified to balance between short-term and long-term interests in order deal with the drifting phenomenon and aid tweet filtering.
To deal with the topic drifting illustrated in the previous examples, we suggest to modify the representation of the classifier X  X  centroid defined in Equation (2). Our idea is to dynamically change the centroid over time by introducing Figure 1: Distribution of medium frequency stemm-ed terms over time for the relevant tweets of three training topics of the TREC 2012 Microblog track filtering task. The colour intensity in each cell repre-sents the frequency of the corresponding term dur-ing that day. We only consider terms of a medium frequency within the relevant tweets of each topic. a decay factor that balances between short-term and long-term interests. In particular, the profile of the user can be represented by the vector: where 0  X   X   X  1 is a decay factor that determines the balance between short-term and long-term interests, L the set of all the relevant tweets so far representing the long term interests in the overall topic, i.e L t = R t , and S set of the most recent relevant tweets representing the short term interests. Considering the example in the first grid of Figure 1, over time, L t will contain the most frequent terms of the topic e.g.  X  X bc X ,  X  X orld X ,  X  X ervice X  and  X  X uts X . On the other hand, S t will boost over time recent subtopics which are becoming popular. For instance,  X  X nline X  and  X  X udget X  on Jan 24 or  X 650 X  and  X  X anguag X  on Jan 26. However, defining S t to contain representative tweets of the short-term inter-ests can be a challenging problem. We explore two intuitive adhoc methods for defining S t : (A) Arbitrary adjustments : We consider S t to be the most n recent tweets added to R t , where n is a free parameter. (B) Daily adjustments : We consider S t to be the tweets in R t that have been added in the current calendar day, i.e. S = { d i : d i  X  R t  X  time ( d i )  X  today } .

Both methods (A) and (B) may still produce inaccurate representation of the short-term interests. For instance, when using daily adjustments (B) to define S t , our balancing approach may not be capable of handling cases where the topic drifts more than once within the same day. Indeed, in the examples of Figure 1, there are such cases (Jan 26 of topic 1). However, we observe in those examples that events trigger new aspects of the topic. In the next section, we propose to use an approach for event detection from social media and newswire streams to identify points of time for potential topic drift, such that S t is automatically adjusted rather than using arbitrary or daily adjustments. Indeed, we show, in our experiments, that using the event detection method is better than the adhoc methods (A) and (B) for adjusting S t in the terms of the achieved filtering quality.
Moreover, our balancing approach has an important pa-rameter, which is the decay factor  X  that practically adjusts the emphasis on the short-term interests. In our experi-ments, we aim to conduct a sensitivity analysis on how the decay factor  X  affects the filtering quality.

Finally, the balancing approach is independent of the rep-resentation of the centroid. Indeed, it can be combined with the QE approach for tackling sparsity in Section 4, where the centroid is calculated as follows: ~c t = (1  X   X  )  X 
As discussed in the previous section, identifying a repre-sentative tweet set of short-term interests is a challenging problem. Our main objective here is to identify the points of time at which topic drift happens to automatically adjust the set S t such that it contains relevant tweets, which have been posted after the topic has drifted. Our assumption is that real-world events result in a shift in interest towards new aspects of the topic, and detecting those events helps in identifying the potential drift. We apply a method of event detection from social media streams that has been recently proposed [1]. In this section, we describe this method and how we use it to automatically adjust S t . The idea is that social media may reflect real-world events, and hence when an event related to a given topic occurs, it is expected to find (i) topically related social posts about it and (ii) increasing microblogging activity [31], causing peaks of tweeting rates during the event (bursts). In other words, the stream of tweets can be used as a source of evidence to detect events. Similarly, news articles reflect on real-world events and the previous assumptions (i) and (ii) hold when considering news articles in a newswire stream. Therefore, a newswire stream can be an another source of evidence to detect events.
Considering the Twitter stream, it is assumed that a cer-tain point of time t i is characterised by the microblogging activities within a given time frame ( t i  X  t i  X  1 croblogging activities are represented with a set of tweets (documents) within the given time frame ( t i  X  t i  X  1 ), which is denoted by D i . Note that the fixed time frame is defined us-ing an arbitrary sampling rate  X  ;  X  i : t i  X  t i  X  1 =  X  . To decide whether t i represents a potential time for an occurrence of an event related to the topic, the two components of evidence identified above are quantified, namely the topically related tweets and the increased tweeting activity. First, the score S ( q,D i ) is quantified representing how the tweets D i lated to the topic q (the query) that the user is interested in. In particular, S ( q,D i ) is estimated using the Comb-SUM voting technique [13], which estimates the final score of the tweet set D i by aggregating the scores (the votes) of individual tweets within D i . Second, it is necessary to quan-tify the change in the tweeting rate about the topic  X  the volume of tweets over time related to the topic  X  observed within the time frame ( t i  X  t i  X  1 ) when compared to obser-vations over previous time frames, i.e. comparing the score S ( q,D i ) retrospectively to the scores S ( q,D i  X  1 ), S ( q,D .. , S ( q,D i  X  k ). This is achieved by applying Grubb X  X  test [8], which determines if the tweeting rate about the topic at the current time t i is an outlier with respect to the tweeting rates of previous observations in a window ( t i  X  k , t i k . Grubb X  X  test gives a binary decision for each considered point of time. More specifically, the tweeting rate about the topic at the current time r i = S ( q,D i ) is an outlier if: where x i,k is the mean tweeting rate in the window ( t i  X  k t ),  X  g is the standard deviation of the tweeting rates in the window ( t i  X  k , t i ), and z is a fixed threshold.
Of course, we can also apply the event detection approach on an external source of newswire instead of the Twitter stream itself. In this case, each newswire article can be seen as a tweet and each point of time t i is characterised with the news articles posted in the time frame ( t i  X  t i  X  1 ).
Finally, we apply the event detection approach to aid fil-tering as follows. During the filtering process, periodically at certain times t i , corresponding to time frames ( t i of size  X  , we apply the Grubb X  X  test to decide if the current point of time represents an occurrence of an event related to the topic. If this is case, we can reset the recent rele-vant tweet set S t . We continue adding tweets judged rele-vant by the user to S t until we detect another event using the Grubb X  X  test. The assumption we make here is that a new event erases the current short-term interests of the user. However, those are still considered in the overall long-term component L t . The process of adjusting S t with this approach is illustrated in Algorithm 1.
In this section, we describe our experimental methodol-ogy. We first identify the main research questions that the experiments aim to answer in light of the discussion before in Sections 3, 4 and 5. Then we describe our experimental setup. The results of the experiments are further reported and discussed in Section 7.
Our experiments aim to validate the models we have de-scribed in the previous sections to uncover the characteris-tics of the adaptive tweet filtering problems. In particular, we aim to answer the following research questions:
Initialise S t s , L t s ; while filtering tweets do end
Algorithm 1: The filtering process with event detection
We use the real-time filtering task of the TREC 2012 Mi-croblog track as our main testbed. The document collection used in this track is the Tweets2011 corpus, where the num-ber of available tweets may change over time as users can delete their tweets or their accounts [24]. We use a filtered version from a list of tweetIDs provided by TREC for the 2012 tasks, which consists of 10,561,763 tweets in the period between 23 Jan and 8 Feb 2011. 49 topics are used in this task, which are split into 10 training and 39 testing topics.
In our experiments, we use Dirichlet language models [28] for weighting the terms in the vector representation as de-scribed in Section 3. To speed up the experiments, tweets that do not contain at least one query term are not consid-ered for similarity computation and are regarded as irrel-evant (we refer to this heuristic later as h1). We also ex-periment with relaxing this condition by considering tweets that contain at least one term in either the query or the first positive example (we refer to this heuristic later as h2).
Filtering Thresholds : The filtering threshold for Incre-mental Rocchio  X  R we use is fixed, throughout the filtering process for all topics. It has been tuned for both official fil-tering measures of the TREC 2012 Microblog track ( T 11 SU and F 0 . 5), using the ten training topics provided by TREC. The best value was then used on the testing topics. It should be noted that adapting the threshold has been investigated in news filtering [19] and a number of methods were pro-posed. In this paper, we do not look into this issue and we leave for future work the exploration of methods for adapting the threshold over time. As for the threshold of the Regu-larised Logistic Regression approach  X  L , an optimal value can be computed for particular filtering metrics directly as the logistic regression methods produce probabilistic scores. In particular, we use the optimal threshold for the T 11 SU metric. The linear utility measure gives a credit of 2 when-ever a relevant document is retrieved and penalises a system by 1 for a false positive [19], and therefore the value of the threshold used is  X  L =0.33.

QE setup : For applying QE to extract expansion terms and tweets as described in Section 4, we used the Kullback Leibler divergence weighting model to rank terms in the pseudo-relevant set. The size of the pseudo-relevant set of tweet is chosen to be the one that achieved the best perfor-mance in previous experiments on real-time ad-hoc Twitter search [14]. In particular, | T q | = 20, and the number of expansion terms used is | E | = 10.

Event detection : We apply the event detection approach using one of two different streams: the Tweets2011 stream (the tweets we are filtering) and a newswire stream we have collected in the same 16 day period of the Tweets2011 dataset. The newswire stream was crawled from major global news sources: BBC, CNN, Google News, New York Times, Guard-ian, Reuters, The Register and Wired. The crawl resulted in a total of 5,967 news articles (approximately 373 articles per day). The details of applying the event detection de-scribed in Section 5.3 are as follows. We use a sampling rate of  X  = 10 minutes, i.e. every 10 minutes at time t i group the accumulated tweets or news articles in the last 10 minute time frame and consider them as a single candi-date representing t i . To score individual documents (tweets or news articles) within that candidate, for a topic (query), the weighting models employed were as follows: For tweets, we use the DFReeKLIM weighting model of the Divergence from Randomness framework designed particularly to rank short texts  X  DFReeKLIM was one of the most effective tweet ranking approaches submitted to the TREC 2011 Mi-croblog track [4]; For the more conventional text of news articles, we use the BM25 weighting model. Regarding the Grubb X  X  test parameters, we tuned the values of the window size of the Grubb X  X  test k and the threshold z for both fil-tering measures using the training topics. In particular, k is set to 60 (60 * 10 minutes = 10 hours) and z is set to 5 standard deviations.

Finally, to conduct the experiments, we develop a stream processing infrastructure based on the open source Storm 3 framework. Within our infrastructure, we extend the Terrier IR platform [15] with real-time in-memory data structures. Performing the real-time filtering can be described as fol-lows. Tweets are streamed from local disk. Each tweet is then passed to the rest of the Storm infrastructure where it is first preprocessed by removing stopwords and stemming using the English Porter stemmer. Then using the Terrier in-memory extension, the collection statistics are updated. Collections statistics include term frequencies, number of documents (tweets) accumulated, and term document fre-quencies. The filtering procedure is applied for each topic (when it is active) to make a binary decision (relevant or not) and update the profile of the topic if necessary. Note that the topic is active if the first positive example arrives, as this is considered that starting time of the filtering task. https://github.com/nathanmarz/storm With this setup, we ensure that the task is conducted in a truly real-time manner.
In this section, we report and discuss the results of the various experiments we conducted under the general setup we described in Section 6. Our discussion aims to convey answers for the research questions of Section 6.1
The first four rows of Table 1 report the results of two vari-ants of the standard Incremental Rocchio classification (RC) and the Regularised Logistic Regression (LR) approaches described in Section 3.2. For each combination, the sec-ond column specifies precisely how the method is instanti-ated. As discussed in Section 6.2, we introduce heuristics on considering a tweet for classification. The third column specifies the heuristic used. The table reports the set pre-cision (set prec), the set recall (set recl) and the filtering measures (F 0.5, and T11SU). For the LR approaches, we report the best results achieved after tuning  X  on the train-ing topics for both filtering measures ( T 11 SU and F 0 . 5). The best  X  is then used for the testing topics and reported in the second column of Table 1. The key finding observed from the first four rows, which answers our research ques-tion (RQ1), is that the traditional news filtering approaches are not effective when applied to tweets. This is inferred from the low scores of the filtering measures in comparison with those of the TREC 2012 best run according to the of-ficial T 11 SU measure (the last row of Table 1). Moreover, in the news filtering literature, Yang et al. [27] reported a T 11 SU of 0.5715 for the state-of-the-art LR approach when applied on adaptive news filtering in the TREC 2002 Fil-tering track, which is much higher than the performances observed here. The scores are particularly low when relax-ing the heuristic to (h2). However, in the second row, we observe an improvement in recall but that X  X  on the expense of swamping the user with a very large number of false pos-itives (set prec is 0.0093). Finally, the state-of-the-art LR approach, which has been shown to outperform RC in the news filtering literature [27, 30], exhibits a poor performance and it is markedly outperformed by the RC approach. LR X  X  poor performance on tweets can in fact be explained by the discriminative nature of LR in comparison to the generative model of Rocchio. LR needs more data to achieve a good performance. In a high dimensional space and with very few sparse learning examples, the LR model is not capa-ble of correctly identifying relevant tweets. In addition, the highly dynamic nature of Twitter makes it harder for the LR model to converge over time to its optimal linear decision.
The rows (5 to 8) of Table 1 report the results of our adap-tation of the RC approach to tackle sparsity using QE as de-scribed in Section 4. We observe a significant improvement in the overall filtering effectiveness. In fact, the best per-forming variant in terms of the two official filtering measures is the one which uses an enriched representation of the clas-sifier X  X  centroid using QE in row 8 of Table 1. It significantly improves the filtering effectiveness using either filtering mea-sures over the original RC baseline approach of rows 3 and 4 in Table 1 according to a paired t-test ( p &lt; 0 . 05). Moreover, using the entire pseudo-relevant set instead of only expan-sion terms results in a better filtering effectiveness overall. In addition, our adaptation with QE outperforms the best TREC 2012 run using the F 0 . 5 measure. However, the util-ity achieved T 11 SU in row 8 of Table 1 is lower than the best TREC 2012 run that seems to be a conservative approach with a high precision and a low recall unlike ours, where we achieve a better balance. As a summary, and in answer to our research question RQ2, our QE modification tackle the sparsity of the tweets is in fact effective and improves the filtering quality over the standard RC baseline.
We conduct a number of experiments to evaluate our adap-tation of Incremental Rocchio to handle topic drift during filtering that we introduced in Section 5. As a baseline, we use our best performing variant in the previous experiment (the last row in Table 1), which applies QE to tackle sparsity and applies the heuristic (h2). We then experiment with a number of methods to apply the balancing approach of the centroid specified in Equation (7), while using the same heuristic (h2) of the baseline. As discussed in Section 5.2, defining the tweet set S t , the set of the relevant tweets rep-resenting the short-term interests at a certain time t , can be performed by two adhoc intuitive methods: (A) arbitrary adjustments or (B) daily adjustments. Using the 10 train-ing topics, we tune the arbitrary number of recent tweets n of method (A) and the decay factor  X  for both filtering mea-sures ( T 11 SU and F 0 . 5). The best values are then used on the testing topics. For method (A) of defining S t (Arbitrary adjustments), the best performance is achieved for the com-bination (  X  = 0 . 3, n = 1) on the training topics, but never-theless we report other combinations in Table 2 to illustrate the effect of each parameter. Although there was a slight improvement on the training topics for (  X  = 0 . 3, n = 1), the filtering performance using both filtering measures degrades on the testing topics however only marginally. Also, we can observe an increase in the recall over the baseline, but not statistically significant according to a paired t-test ( p&lt; 0.05). In fact, with a higher decay of long-term interests, we can achieve a significant increase in recall but on the cost of the precision and therefore a significantly worse filtering effec-tiveness in both filtering metrics (rows 4 and 5 of Table 2). For method (B) of defining S t (Daily adjustments), we ob-serve negative results in Table 2. The best performance on the training topics was achieved when (  X  = 0.3) and it yielded marginal improvement but this is not the case on the testing topics. Indeed, in this case, even the recall does not improve and the filtering performance degrades marginally when  X  = 0.3 and significantly when  X  = 0.7. This indicates that using daily intervals to adjust short-term interests does not handle topic drift. In answer to our research question RQ3(a), we conclude that the adhoc methods for captur-ing short-term interests may not be useful for our balancing approach for topic drift, as it fails to improve the filtering quality when these methods are employed.
We next experiment with our automatic approach for ad-justing the tweets set S t , as described in Section 5.3, to aid our balancing approach of handling topic drift. As in Sec-tion 2, we tune the decay factor  X  for both filtering measures using the training topics and use the best value on the test-ing topics. However, we will provide a comprehensive sen-sitivity analysis of this parameter in the next section. The results are reported in Table 3. When using the Tweets11 stream for event detection (second row of Table 3), as in Section 2, the recall increased on the expense of precision and both filtering measures. However, the main difference is that we see a statistically significant increase in recall over the baseline but not a statistically significant decrease on ei-ther filtering metrics according to a paired t-test ( p &lt; 0 . 05). This is an interesting finding and it shows the usefulness of our event detection to aid the balancing approach. Shifting the centroid to the short-term interests results in more false positives, which degrades precision. However, event detec-tion helps to better represent the short-term interests over time, thereby reducing the number of false positives, while at the same time identifying more relevant tweets, thereby enhancing the recall. We perform a further analysis where we examine the differences in recall with the baseline (first row of Table 3) across all the testing topics. We plot these differences in Figure 2. We observe that the increase in recall is fairly consistent across all topics. Only on a sin-gle occasion, we see a slight decrease in recall, while for all other topics we see either an increase or no difference at all. When using an external newswire stream for detecting events (last row of Table 3), the same pattern is observed. If we compare this approach to the one that uses the Tweets11 stream (the row above), we observe that the differences are marginal. This may indicate that the events detected with either streams have a large overlap, which has been shown in a recent study [17]. As a summary and in answer to research question RQ3(b), we find that the event detection method, using either the Tweets11 stream itself or the exter-nal newswire stream, helps our balancing approach to tackle drift. In particular, it results in significantly improving recall while only marginally decreasing the filtering performance.
To answer research question RQ3(c), we conduct a sensi-tivity analysis of the decay factor for the approach specified in the second row of Table 3. We vary the decay factor  X  between 0.0 (only long-term interests; a baseline equivalent to the the first row in Table 3) and 1.0 (only short-term in-terests) increasing it by 0.1 at a time. Figure 3 plots the changes in the various evaluation measures. As the decay increases, we first observe a marginal increase on all mea-sures apart from F 0.5. Later, we observe that the recall starts to improve but the precision degrades. At  X  =0.4, the improvement in recall is statistically significant using a paired t-test ( p&lt; 0.05) while the decrease in both filter-ing measures and in precision is statistically insignificant. After this point, recall continues to improve and stabilises between  X  =0.6 and  X  =0.8. However, precision and both fil-tering measures fall sharply in this region. At  X  =1.0, when the classifier puts all the emphasis on the latest tweets only, we observe a sharp decrease in recall and on filtering mea-sures and a slight increase in precision. This is possibly due to the fact that the S t set may be empty at certain times (just after resetting due to a detection of an event), which may reduce the false positives picked up by the L t compo-nent of the centroid when  X &lt; 1.0. This analysis uncovers a typical problem in IF and IR, where there is a trade-off between recall and precision. More importantly, it shows that achieving a good filtering effectiveness is particularly challenging because it has a higher sensitivity to precision than to recall. An effective filtering technique should strive to achieve the best trade-off between the two measures. Fi-nally, as a summary and in answer to the overall research Figure 2: Changes in recall across all the testing topics between the approach in the first row and the one in the second row of Table 3. Figure 3: Sensitivity analysis of the decay factor  X  when using the balancing approach with adjustment aided by event detection (the approach in the second row of Table 3). question RQ3, our approach to handle topic drift works best when aided with event detection to capture potential times of topic drift. It significantly improves recall at the cost of a marginal decrease in the overall filtering performance.
Real-time filtering of tweets is an emerging information filtering task that can still be tackled using traditional fil-tering techniques with appropriate adaptations to address the unique challenges prevalent in Twitter. Our thorough evaluation, using a standard TREC collection, shows that traditional state-of-the-art news filtering techniques are not as effective when applied on tweets. However, the modifica-tions we proposed on a traditional news filtering approach are shown to mitigate the acute sparsity issue that is preva-lent in Twitter. In particular, our query expansion approach to tackle the sparsity of tweets yields a significant improve-ment in filtering effectiveness, by deriving a richer represen-tation of the user profile with relevant and timely terms. Moreover, we introduced another adaptation to tackle topic drifting during filtering, which can be amplified by the highly dynamic nature of Twitter. The results show that by us-ing event-detection to balance between short-term and long-term interests, we can significantly improve the filtering re-call while only marginally harming the filtering utility.
There is plenty of scope for future work. First, to tackle the sparsity issue, we aim to explore the use of dynamic knowledge resources, such as Wikipedia, in a timely manner to complement our query expansion approach. This may allow the classifier, for example, to capture terms that are not picked up by query expansion, and hence obtain a richer representation of the user X  X  information needs. Secondly, the TREC dataset has the limitation that explicit user judg-ments are simulated and may not necessarily reflect realistic Classifier X  X  centroid parameters heuristic set prec set recl F 0.5 T11SU QE using Equation (6) h2 0.4206 0.3370 0.3435 0.3615
QE and handling drift using Equation (8) with event detection to adjust S t feedback. Therefore, we aim to conduct an online evalua-tion, in the form of A/B testing, where we can obtain ex-plicit feedback from real users.
 This work has been carried out in the scope of the EC co-funded project SMART (FP7-287583). [1] M. Albakour, C. Macdonald, I. Ounis. Identifying [2] J. Allan. Incremental relevance feedback for [3] G. Amati. Probability models for IR based on [4] G. Amati, G. Amodeo, M. Bianchi, G. Marcone, [5] N.J. Belkin, and W.B. Croft. Information filtering and [6] J. Callan. Learning while filtering documents. In Proc. [7] G. Cormack, J. M. G. Hidalgo, E. P. S  X anz, Spam [8] F. Grubb. Procedures for detecting outlying [9] L. Horv  X ath. The maximum likelihood method for [10] L. B. Jabeur, L. Tamine, and M. Boughanem.
 [11] X. Hu, N. Sun, C. Zhang, and T.-S. Chua. Exploiting [12] B. Krishnamurthy, P. Gill, and M. Arlitt. A few chirps [13] C. Macdonald and I. Ounis. Voting for candidates: [14] R. M. C. McCreadie, C, Macdonald, R, L. T. Santos [15] I. Ounis, G. Amati, V. Plachouras, B. He, [16] I. Ounis, C. Macdonald, J. Lin, and I. Soboroff. [17] S. Petrovic, M. Osborne, R. McCreadie, [18] S. Robertson. Threshold setting and performance [19] S. E. Robertson and I. Soboroff. The TREC 2002 [20] S. Robertson and H. Zaragoza. The probabilistic [21] J. Rocchio. Relevance feedback in information [22] T. Sakaki, M. Okazaki, and Y. Matsuo. Earthquake [23] B. Sriram, D. Fuhry, E. Demir, H. Ferhatosmanoglu, [24] I. Soboroff, D. McCullough, J. Lin, C. Macdonald, I. [25] T. Wasserman. Twitter says it has 140 million users. [26] J. Xu and W.B. Croft Query expansion using local and [27] Y. Yang, S. Yoo, J. Zhang, B. Kisiel. Robustness of [28] C. Zhai and J. Lafferty. A study of smoothing [29] T. Zhang. Solving large scale linear prediction [30] Y. Zhang. Using bayesian priors to combine classifiers [31] A. Zubiaga, D. Spina, V. Fresno, and R. Mart  X  X nez.
