 A critical step in bridging the knowledge base with the huge corpus of semi-structured Web list data is to link the en-tity mentions that appear in the Web lists with the cor-responding real world entities in the knowledge base, which we call list linking task. This task can facilitate many differ-ent tasks such as knowledge base population, entity search and table annotation. However, the list linking task is chal-lenging because a Web list has almost no textual context, and the only input for this task is a list of entity mentions extracted from the Web pages. In this paper, we propose LIEGE, the first general framework to L ink the entI ties in wE b lists with the knowledG ebasE to the best of our knowl-edge. Our assumption is that entities mentioned in a Web list can be any collection of entities that have the same con-ceptual type that people have in mind. To annotate the list items in a Web list with entities that they likely mention, we leverage the prior probability of an entity being mentioned and the global coherence between the types of entities in the Web list. The interdependence between different entity assignments in a Web list makes the optimization of this list linking problem NP-hard. Accordingly, we propose a prac-tical solution based on the iterative substitution to jointly optimize the identification of the mapping entities for the Web list items. We extensively evaluated the performance of our proposed framework over both manually annotated real Web lists extracted from the Web pages and two public data sets, and the experimental results show that our frame-work significantly outperforms the baseline method in terms of accuracy.
 H.3.3 [ Information Systems ]: Information Storage and Retrieval X  Information Search and Retrieval Algorithms, Experimentation List linking, Knowledge base, Similarity metric, Iterative substitution
A large number of Web pages contain data structured in the form of lists. A Web list on some Web page may enu-merate a list of famous American basketball players, a list of best-selling albums in the United States or a list of films directed by James Cameron. In addition, table which is another major source of structured data on the Web can be regarded as a set of lists/columns. Therefore, lists are considered as a general data structure to express entity ref-erences. Items in Web lists often refer to entities, however, as being free text form, the list items are potentially am-biguous: the same textual name may refer to several differ-ent real world entities. For instance, the entity mention of  X  X ichael Jordan X  can refer to the famous basketball player, the computer science professor who is one of the keynote speakers in KDD X 12, or some other persons.

Recently, the success of Wikipedia and the development of information extraction techniques have facilitated the au-tomated construction of large scale machine-readable knowl-edge base about the world X  X  entities, their semantic classes and their mutual relationships. Such kind of notable en-deavors include DBpedia [2], KnowItAll [9], YAGO [21, 20] and KOG [23, 24]. Bridging these knowledge bases with the semi-structured Web lists is beneficial for the exploitation of this huge corpus of structured data on the Web. Addition-ally, the list linking task enables powerful join and union operations that can integrate information across different lists, tables and pages.

The list linking task is of practical importance and can be used in various applications. For instance, 75% of the tables on the Web typically have a column that is the sub-ject of the table, and the subject column contains the set of entities the table is about [22]. Linking this subject col-umn with the knowledge base is significantly helpful for the task of table annotation [16] and recovering the semantics of tables [22]. As another example, linking the Web lists or table columns with a knowledge base can enrich the existing knowledge base and impulse the trend to advance the tra-ditional keyword-based search to the semantic entity-based search.

In recent years, considerable progresses have been made in linking entities in free text with a knowledge base [3, 6, 7, 13, 15, 19]. When linking entities in free text, the textual context is considered as the key evidence for disambigua-tion. The essential step in these previous approaches is to define a similarity measure between the text around the en-tity mention and the document associated with the entity. While in our setting, lists never have any header text in-formation and textual context. The only input for the list linking task is a list of entity mentions extracted from the Web pages, such as the Web list on the left of Figure 1. On the other hand, most of the methods addressing the problem of linking entities in free text exploit the observation that entities mentioned from a single document are likely to be semantically related [6, 15, 19]. However, the entities in a Web list can be any set of entities that have the same con-ceptual type, and they are semantically similar, rather than semantically related. Accordingly, this observation for link-ing entities in free text clearly cannot be applied to the Web list data. From these two dimensions mentioned above, it can be seen that the list linking task is challenging and dif-ferent from the task of linking entities in free text. Moreover, we are interested in linking these Web list items in a fully automatic manner, and avoid tedious tuning or parameter settings.

Given a knowledge base about the world X  X  entities, their semantic classes and their mutual relationships, for each Web list in the list corpus, our goal is to link each list item in the Web list with the real world mapping entity existing in the knowledge base. In this paper, we propose LIEGE, a general framework to annotate the list items in the Web lists with entities based on the assumption that entities in a list can be any set of entities that have the same conceptual type. Specifically, we propose to measure the linking qual-ity of the candidate mapping entities in a straightforward and comprehensive way. Intuitively, a candidate mapping entity is  X  X ood X  for some list item if it has two key proper-ties: (1) the prior probability of the entity being mentioned is high; (2) the type of the candidate mapping entity is co-herent with the types of the other mapping entities in the same list, in the sense that they represent a list of entities having the same conceptual type. If the type of an entity is coherent with the type of another entity, we say these two entities are semantically similar. To calculate the seman-tic similarity between entities, we leverage two categories of information: (1) type hierarchy based similarity; (2) distri-butional context similarity. To combine these signals to give the linking quality for each candidate mapping entity, we use the max-margin technique to automatically learn the proper weights for different features. The interdependence between different entity assignments in a Web list makes the infer-ence of this problem NP-hard. Thus, we develop a practical and effective solution based on the iterative substitution to jointly optimize the entity assignments for each Web list. Contributions. The main contributions of this paper are summarized as follows.
The remainder of this paper is organized as follows. Sec-tion 2 introduces some preliminaries and notations. We present the framework of LIEGE in Section 3. Section 4 presents our experimental results and Section 5 discusses the related work. Finally, we conclude this paper in Section 6.
In this section, we begin by describing the knowledge base and the task of list linking. Next, we introduce the genera-tion of candidate mapping entities for each list item. Knowledge base. The knowledge base consists of a type hierarchy and entities that are instances of types. The set of types is denoted by T ,andeach t  X  T is a type, such as the type American singers . Types are connected by the subtype relation t 1  X  t 2 ,whichmeans t 1 is a subtype of t . For example, American singers is a subtype of Singers . We use t 1  X   X  t 2 to denote that t 2 is an ancestor node of t in the type hierarchy. The set of entities existing in the knowledge base is denoted by E ,andeach e  X  E is an entity, such as the entity Michael Jackson .Wesayentity e is an instance of one type t , denoted by e  X  t . For instance, the entity Michael Jackson is an instance of the type American singers .Wealsouse e  X   X  t to denote that type t is an an-cestor node of entity e in the hierarchy, thus, we can say, Michael Jackson  X   X  Singers .Let T ( e )= { t | e  X  t } be the set of types of which entity e is the instance. Likewise, let E ( t )= { e | e  X   X  t } be the set of entities having type t as an ancestor node in the hierarchy. In this paper, the knowledge base we adopt is YAGO [21, 20], an open-domain ontology combining Wikipedia and WordNet [10] with high coverage and quality. In YAGO, they use unique canonical strings from Wikipedia as the entity names. Currently, YAGO con-tains about 250,000 types and over one million entities. List linking. The input of our framework LIEGE is a col-lection of Web lists, and let L be a Web list with | L | items. We use 1  X  i  X | L | to index the list item in L , and the list item with index i is denoted by l i . Each list item l i  X  is a token sequence of an entity mention that is potentially linked with an entity in the knowledge base (see Figure 1 for an example). For this list L , it is noted that we do not have a name for it (i.e., the type of entities this list L enumerates). And there are no textual context and header text about this list L . Here, we formally state the list linking task as fol-lows. Given the set of entities E in the knowledge base and the Web list L , the goal is to identify the mapping entity list M , with the same size as L , such that each m i  X  M is the mapping entity for the corresponding list item l i  X  L . In this paper, we assume that the knowledge base contains all the mapping entities for all the list items, i.e., M  X  Candidate mapping entity. For each l i  X  L , the map-ping entity m i should be the entity that may be referred by the token sequence of l i . Therefore, we firstly identify all the entities that may be referred by l i , and denote this set of entities as the candidate mapping entity set R i for the list item l i .Let R = { R i | l i  X  L } be the set of candidate mapping entity sets for all the list items in the Web list L .
To identify R i for each l i  X  L , we need to build a dic-tionary D that contains vast amount of information about various mention forms of the named entities, like name vari-ations, abbreviations, confusable names, spelling variations, nicknames, etc. The structure of the Wikipedia provides a set of useful features for the creation of such a dictionary D . In addition, Wikipedia has high coverage of named entities [25], which is profitable for constructing the dictionary D as well. The dictionary D is a &lt; key, value &gt; mapping, where the column of the key K is a list of mention forms and the column of the mapping value K.value is the set of named entities which are referred by the key K .Weconstructthe dictionary D by leveraging the following four structures of Wikipedia: Entity page: Each entity page in Wikipedia describes a single entity, and generally, the title of each entity page is the most common name for that entity, e.g., the page title  X  X BM X  for that giant American company headquartered in Armonk. Thus, we add the title of the entity page to the key K , and add the entity described in this page to K.value . Redirect page: A redirect page exists for each alternative name which can be used to refer to an existing entity in Wikipedia. For example, the redirect page titled  X  X P X  con-tains a pointer to the entity page titled  X  X ewlett-Packard X . Henceforth, we add the title of the redirect page to the key K , and add the pointed entity to K.value .
 Disambiguation page: When multiple entities in Wikipedia could be given the same name, a disambiguation page is created to separate them and contains a list of references to those entities. For instance, the disambiguation page for the name  X  X ichael Jordan X  lists eight associated entities having the same name of  X  X ichael Jordan X , including the famous NBA player and the Berkeley professor. For each disam-biguation page, the title of this page is added to the key K , and the entities listed in this page are added to K.value . Hyperlink in Wikipedia article: The article in Wikipedia often contains some hyperlinks each of which links to the page of the corresponding entity mentioned in this article. For example, in the entity page titled  X  X ewlett-Packard X , there is a hyperlink pointing to the entity William Redding-ton Hewlett whose anchor text is  X  X ill Hewlett X . Then we add the anchor text of the hyperlink to the key K ,andadd the pointed entity to K.value .

Using the four structures of Wikipedia described above, we construct the dictionary D . A part of the dictionary D is shown in Table 1. For each list item l i  X  L ,welookupthe dictionary D and search for l i in the column of the key K . If a hit is found, i.e., l i  X  K , we add the set of the entities l .value to the candidate mapping entity set R i .Wedenote the size of R i as | R i | , and use 1  X  j  X | R i | to index the candidate entity in R i . The candidate mapping entity with index j in R i is denoted by r i,j .

In most cases, the size of the candidate mapping entity set is larger than one. Figure 1 shows an example. The Web list in Figure 1 enumerates 6 best-selling single-volume books on the left. For each list item, we show its candidate mapping entity set generated from the dictionary D on the right of the figure, and the real mapping entity is underlined. It is noted that in Figure 1 each candidate entity has the name which is the unique canonical string for that entity in Wikipedia, and the size of the candidate mapping entity set for most of the list items (5 out of 6) is larger than 1.
In this section, we introduce how to pick the proper entity from R i as the mapping entity m i for the list item l i ,when |
R i | &gt; 1. We firstly illustrate how to define the linking qual-ity metric for each candidate mapping entity in Section 3.1. Next, we show how to learn the proper weights for different features to calculate the link quality for each candidate map-ping entity in Section 3.2. Finally, we present an effective algorithm, called iterative substitution algorithm ,tojointly optimize the identification of the mapping entities for the list items of each Web list in Section 3.3.
Since each list item can refer to several candidate entities, it is nontrivial to select the mapping entity of the list item from the candidate mapping entity set. In this section, we propose an intuitive and comprehensive metric to measure the linking quality of the candidate mapping entity. Prior probability: The first observation we have is that, each candidate mapping entity r i,j  X  R i having the same mention form l i has different popularity, and some entities are very obscure and rare for the given mention form l i .For the example in Figure 1, for the list item  X  X  Tale of Two Cities X , the candidate entity A Tale of Two Cities (musical) , the stage musical by Jill Santoriello, is much rarer than the candidate entity ATaleofTwoCities , the novel by Charles Dickens, and in most cases when people mention  X  X  Tale of Two Cities X , they mean the novel rather than the stage musical whose name is also  X  X  Tale of Two Cities X . We formalize this observation via taking advantage of the count information from Wikipedia, and define the prior probability P pr ( r i,j ) for each candidate mapping entity r i,j  X  R i respecttothelistitem l i as the proportion of the links with the mention form l i as the anchor text which point to the candidate mapping entity r i,j : where count ( r i,j ) is the number of links which point to entity r i,j andhavethementionform l i as the anchor text.
For the example in Figure 1, with respect to the list item  X  X  Tale of Two Cities X , the prior probability of the entity ATaleofTwoCities , the novel, is 0.6528, while the prior probability of the entity A Tale of Two Cities (musical) ,the Candidate Mapping Entity Set stage musical, is 0.1319. And the prior probabilities of all other candidate entities for the list item  X  X  Tale of Two Cities X , e.g., A Tale of Two Cities (1935 film) , ATaleof Two Cities (Lost) , are much smaller than that for the entity ATaleofTwoCities . Moreover, in Figure 1, the candidate mapping entities for each list item are ranked by their prior probabilities shown on the right of the figure in decreasing order. It can be seen that the notion of prior probability suitably expresses the popularity of the candidate mapping entity being mentioned given a mention form.
 Coherence: A Web list usually enumerates some entities which have the same conceptual type. As an example, the Web list in Figure 1 enumerates some best-selling single-volume books. We observe that the definition of prior prob-ability does not capture the contextual meaning the Web list represents. For the example in Figure 1, for the list item  X  X he Godfather X , the entity The Godfather ,anAmerican epic crime film, which has the highest prior probability ,is not the corresponding mapping entity for this list item in this Web list. The same situation occurs for the list items  X  X one with the Wind X  and  X  X ear of Flying X  in Figure 1. Accordingly, to choose the mapping entity for a list item, we should select the candidate entity whose type is coherent with the types of the other mapping entities in the same Web list. In other words, the mapping entity for some list item should be semantically similar to the other mapping entities in the same Web list. Therefore, we have the intuition that the more semantically similar the candidate mapping entity is to the other mapping entities in the same Web list, the more likely the candidate entity is the correct mapping en-tity for the list item. To capture this intuition, we formally define the notion of coherence Coh ( r i,j ) for each candidate mapping entity r i,j  X  R i as: where m u is the mapping entity for the list item l u  X  L , and Sim ( e 1 ,e 2 ) is the function that measures the semantic similarity between entities e 1 and e 2 , e 1 ,e 2  X  E .
To calculate the semantic similarity between entities, we leverage two categories of information in this paper: (1) type hierarchy based similarity; (2) distributional context similarity.
 Type hierarchy based similarity: To measure the se-mantic similarity between entities based on type hierarchy, we have the assumption that two entities are semantically similar if they are in close places in the type hierarchy. Given two entities e 1 ,e 2  X  E , T ( e 1 )= { t | e 1  X  t } is the set of types of which entity e 1 is the instance in the knowledge base, likewise, T ( e 2 )= { t | e 2  X  t } . To measure the se-mantic similarity Sim hr ( e 1 ,e 2 )between e 1 and e 2 based on type hierarchy, we firstly define how to calculate the seman-tic similarity between the sets of types T ( e 1 )and T ( e Since the sizes of T ( e 1 )and T ( e 2 ), and the elements in T ( e 1 )and T ( e 2 ) are likely to be different, we start by defin-ing the correspondence between the elements of types from one set to another set. For each type t 1  X  T ( e 1 ), we as-sign a target type  X  ( t 1 )inanotherset T ( e 2 ) as follows:  X  ( t is the semantic similarity between two types t 1 and t 2 based on type hierarchy, and  X  ( t 1 )isthetypein T ( e 2 )whichmax-imizes the semantic similarity between these two types.
To compute Sim hr ( t 1 ,t 2 ), we adopt the approach intro-duced in [17] which is an information-theoretic method. As-suming t  X  T isatypeintheknowledgebase,theamountof information contained in the statement X  x  X  t  X  X s  X  log ( P ( t )), where P ( t ) is the probability that a randomly selected en-tity e belongs to E ( t ), which is the set of entities having the type t as an ancestor node in the hierarchy. We also assume that t 0 is the lowest common ancestor node for type nodes t and t 2 in the hierarchy. The following is the definition of the semantic similarity between types t 1 and t 2 based on type hierarchy: Next, we can calculate the semantic similarity from one set of types T ( e 1 )toanothersetoftypes T ( e 2 ): Similarly, we can calculate the semantic similarity from T ( e to that in Formula 4. Based on the definitions mentioned above, we can define the semantic similarity between enti-ties e 1 and e 2 based on type hierarchy as the average of the semantic similarity from T ( e 1 )to T ( e 2 ) and that from T ( e to T ( e 1 ): Sim hr ( e 1 ,e 2 )= Sim hr ( T ( e 1 ) ))
For the example in Figure 1, the semantic similarity be-tween entities ATaleofTwoCities and The Da Vinci Code , a mystery-detective novel, based on the type hierarchy is 0.9527, while the semantic similarity between ATaleofTwo Cities (musical) and The Da Vinci Code is much smaller, i.e., 0.3637.
 Distributional context similarity: To measure the se-mantic similarity between entities based on the contexts where they appear in the external document corpus (e.g. the Wikipedia article corpus), we have the assumption that entities that occur in similar contexts are semantically sim-ilar, which is the extension of the distributional hypothesis [14]. We define the document context  X  e for each entity e  X  E as the set of windows of words (removing all punc-tuation symbols) before and after each occurrence of the entity e in the external document corpus. Assume that an entity e of length | e | words appears in a document at posi-tion p .Thesize-k document context  X  e of entity e should contain these two windows of words, i.e., w p  X  k ,...,w p instance, the entity ATaleofTwoCities occurs in a doc-ument containing such a sentence,  X  X implified versions of A Tale of Two Cities , a novel by Charles Dickens, have been published ... X . When the size k is set to 4, we should add the windows  X  X implified versions of X  and  X  X  novel by Charles X  to the document context of the entity ATaleofTwoCities . To compute the n-gram vector V e for each entity e  X  E ,we firstly add all n-grams in the size-k document context  X  entity e together with their frequencies, and then discard the top W most frequent n-grams which appear in this vector V . In the experiments, we set k =4, W = 50. In order to measure the distributional context similarity between enti-ties e 1 and e 2 , we calculate the cosine similarity of the two n-gram vectors V e 1 and V e 2 for entities e 1 and e 2 respectively as follows: where g is the number of distinct n-grams in the union of the n-gram vectors for entities e 1 and e 2 ,each a i ( b the frequency of the corresponding n-gram which appears in the vector V e 1 ( V e 2 ), and V e 1 = a 1 ,a 2 ,...,a b ,b 2 ,...,b g .

For the example in Figure 1, the distributional context similarity between entities ATaleofTwoCities and The Da Vinci Code is 0.0934, while the distributional context similarity between A Tale of Two Cities (musical) and The Da Vinci Code is much smaller, i.e., 0.0137.

In this paper, we only leverage these two categories of information to calculate the semantic similarity between en-tities, the function Sim in Formula 2 could be either one of these two semantic similarity metrics, i.e., Sim hr and Sim ds , or their weighted sum. In addition, we emphasize that our framework is general and extensible enough that any other semantic similarity methods can be easily plugged in to be used.
 Linking quality: Based on the observation that both prior probability and coherence contribute to the linking quality of candidate mapping entity, we define the linking quality LQ ( r i,j ) for each candidate mapping entity r i,j  X  R i weighted sum of prior probability and coherence as follows: where  X  is a weight factor that balances the importance between prior probability and coherence .

With the definition of linking quality for each candidate mapping entity, we also define the linking quality for the mapping entity list M as the sum of the linking qualities of the mapping entities for all the list items with respect to the Web list L :
LQ ( M )=  X   X  where m s  X  M and Coh ( m s ) can be calculated in the way introduced in Formula 2. Thus, we have LQ ( M )=  X   X  Now, we formally state the list linking task as follows: List linking: Given the set of entities E in the knowledge base, the Web list L and the definition of linking quality in Formula 9, the goal is to identify the mapping entity list M  X  E , with size | L | , such that the objective function LQ ( M ) is maximized.

However, the inference problem of identifying the map-ping entity list M  X  E , with size | L | , which maximizes the objective function LQ ( M ) is NP-hard. The hardness can be shown using a reduction from the maximal clique prob-lem[11]. Details of proof are omitted to save space.
To measure the linking quality LQ ( r i,j ) for each candidate mapping entity r i,j  X  R i , we use the definitions introduced in Formulae 7 and 2. Thus, we have
LQ ( r i,j )=  X   X  P pr ( r i,j )+ 1 where the function Sim could be the weighted sum of the two semantic similarity metrics, i.e., Sim hr and Sim ds ,in-troduced in Formulae 5 and 6 respectively. Thus, LQ ( r i,j )=  X   X  P pr ( r i,j )+  X   X  1 | where  X  +  X  +  X  =1,and  X  ,  X  and  X  are weight factors that give different weights for different feature values in the calculation of the linking quality LQ ( r i,j ). Here, we denote  X  X  X  w as the weight vector, and  X  X  X  w =  X ,  X ,  X  .Inthissection, we introduce how to learn the weight vector training data set.
To learn training data set. Given the ground truth mapping entity m i  X  M for each list item l i  X  L , we assume that the linking quality of the mapping entity LQ ( m i ) is larger than the linking quality of any other candidate entity LQ ( r i,j )with a margin, where r i,j  X  R i and r i,j = m i . This gives us the usual SVM linear constraints for all list items: and we maximize the objective C l i l i  X  l i where C l i constant with respect to  X  l i , to learn the weight vector  X ,  X ,  X  such that  X  +  X  +  X  =1.

If the assumption is violated by some list item l i ,thatisto say, LQ ( m i ) is smaller than LQ ( r i,j ) for all r i,j we have  X  l i &lt; 0. Therefore, for the list item l i which violates the assumption, i.e.,  X  l i &lt; 0, we give some penalty to it and set C l i =  X  ,where  X  is a constant that is larger than 1, called penalty parameter; Otherwise, for the list item l i  X  l i  X  0, we set C l i =1.  X  is the only parameter we have to set in our framework, and in the experiments, we show that thechoiceof  X  does not affect the results of our framework greatly. Accordingly, our framework can run without tedious parameter tuning.
As the inference problem of identifying the mapping en-tity list M  X  E , with size | L | , which maximizes the objec-tive function LQ ( M ) is NP-hard, we propose an effective algorithm, called iterative substitution algorithm ,tojointly optimize the identification of the mapping entity list M for each Web list L in this section.

The iterative substitution algorithm starts with a good guess of the mapping entity list from the candidate mapping entity sets, and then iteratively refines the mapping entity list to improve the linking quality as defined in Formula 9, until the algorithm converges and a local maximum of the linking quality of the mapping entity list is reached. To im-prove the linking quality of the mapping entity list in each iteration, the algorithm replaces the mapping entity of the previous iteration with the candidate mapping entity which maximizes the improvement of the overall linking quality of the mapping entity list. The details of the iterative substi-tution algorithm are described in Algorithm 1.

The iterative substitution algorithm depicted in Algorithm 1 takes the Web list L and the candidate mapping entity sets R as input, and outputs the mapping entity list M for the Web list L . We firstly calculate the prior probabil-ity P pr ( r i,j ) for each candidate entity r i,j in the candidate entity set R i of each list item l i ,asdefinedinFormula1, and pick the candidate entity which has the maximum prior probability as the initial estimate of the mapping entity m for the list item l i (line 1-line 3). Let M (0) be the initial map-ping entity list for the Web list L (line 4). In the subsequent iterations in the while loop, we iteratively identify the new mapping entity list M ( iter ) based on the mapping entity list of previous iteration M ( iter  X  1) , and gradually improve the linking quality of the mapping entity list until the algorithm converges and a local maximum is reached (line 6-line 20). Specifically, in each iteration iter , for each candidate map-ping entity r i,j = m ( iter  X  1) i  X  R i of each list item l where m ( iter  X  1) i is the estimate of the mapping entity for l of previous iteration, we compute the new mapping entity Algorithm 1 Iterative Substitution Algorithm Input: Web list L , candidate mapping entity sets R . Output: mapping entity list M . 1: for each l i  X  L do 3: end for 5: iter =1 6: while true do 7: for each l i  X  L do 11: end for 12: end for 14: if IncreLQ r max i,j &gt; 0 then 16: iter ++ 17: else 18: break 19: end if 20: end while list M ( iter ) r i,j assuming that we substitute r i,j for m choose r i,j as the mapping entity for l i in iteration iter (line 9), and then calculate the improvement of the linking quality of the mapping entity list IncreLQ r i,j if this substitution is operated (line 10). Let r max i,j be the candidate entity which achieves the maximum improvement of the linking quality of the mapping entity list (line 13). If IncreLQ r max i,j &gt; 0, we list M ( iter ) of iteration iter , and continue the iteration(line 14-line 16). Otherwise, if IncreLQ r max i,j  X  0, the algorithm has converged and reached a local maximum, thus, we stop the algorithm (line 18) and return M ( iter  X  1) as the result of the mapping entity list M for L (line 21).
 We state that the computation of the mapping entity list M in Algorithm 1 is guaranteed to converge ,andthe itera-tive substitution algorithm depicted in Algorithm 1 is bound to terminate. Here, we give out the proof of the conver-gence of the iterative substitution algorithm . Wenotethat the linking quality of the mapping entity list LQ ( M ( iter of each iteration iter is greater than the linking quality of the mapping entity list LQ ( M ( iter  X  1) ) of the previous iter-ation ( iter  X  1). Thus, we can say that the linking quality LQ ( M ( iter ) ) is monotonically increasing with the number of iteration iter .Moreover,since M  X  E ,thenumberof different mapping entity lists is finite. The linking quality LQ ( M ) has an upper bound. Accordingly, we know that the iterative substitution algorithm is bound to stop and the computation of M in Algorithm 1 converges. Though we do not know the upper bound on the number of iterations Al-gorithm 1 may run until it terminates, in our experiments, we observe that it converges quickly and typically takes only a few iterations.
To evaluate the effectiveness of LIEGE, we present a thor-ough experimental study in this section. We firstly describe the experimental setting in Section 4.1 and then show the experimental results in Section 4.2.
To the best of our knowledge, there is no publicly available data set for the list linking task. However, we found that the table annotation task introduced in [16] has a subtask, whose goal is to annotate the table cells in Web tables with the mapping entities in the knowledge base. Though this subtask is not the same as our list linking task, we could regard each column of each table in the data sets as a Web list, and used the ground truth annotation in [16] to evaluate the performance of our framework. Specifically, we used two Web table data sets, i.e., Wiki Manual and Web Manual, introduced in [16]. The Wiki Manual data set contains 36 (non-Infobox) tables selected from Wikipedia article text, and the Web Manual data set consists of 371 Web tables similar to the Web tables in Wiki Manual. The main dif-ference between Wiki Manual and Web Manual is that the cells in the latter are more noisy. In both of these two data sets, the table cells whose mapping entities do not exist in the knowledge base have been dropped from the labeling task [16]. A summary of the data sets used in this paper is shown in Table 2. From the summary in Table 2, we can see that the average number of annotated columns for each tableislessthan2inbothWiki Manual and Web Manual data sets.

In addition, to verify the effectiveness of our framework over the real Web list data set, we collected 50 Web lists from the Web and manually annotated the list items with the mapping entities in the knowledge base, which we refer as the Web Lists data set. The Web lists in our Web Lists data set enumerate various kinds of entities, such as a list of American basketball coaches, a list of best-selling single-volume books and a list of best films winning the Empire Award. In our experiments, we evaluate the performance of LIEGE using only the list items whose mapping entities exist in the knowledge base. The summary of this data set is also shown in Table 2.

In the following experiments, we used YAGO(1) 1 of ver-sion 2008-w40-2 as the knowledge base, which is the same version as that used in [16]. We downloaded the May 2011 version of Wikipedia to construct the dictionary D intro-duced in Section 2. Then we used the dictionary D to gen-erate the candidate mapping entity set R i for each list item l  X  L . If the size of the candidate mapping entity set | R of some list item l i equals 0, we considered that the textual form of this list item has some noise. We processed this kind of list items by some methods, such as eliminating ap-positives (either within parentheses or following a comma), removing the sequence number at the beginning of the tex-http://www.mpi-inf.mpg.de/yago-naga/yago/ Table 3: Experimental results over Wiki Manual Table 4: Experimental results over Web Manual tual form, eliminating the redundant white spaces in the tex-tual form and correcting the spelling errors using the query spelling correction supplied by Google. Then we generated the candidate mapping entity sets for these list items using the dictionary D again. To generate the document context for each entity to calculate the distributional context simi-larity, we employed these downloaded 3.5 million Wikipedia pages as the external document corpus, where each entity in the Wikitext has been converted to its canonical representa-tion. To learn the weight vector introduced in Section 3.2, we set  X  = 100. In the next section, we will show that the results of our framework LIEGE is insensitive to the param-eter  X  and the choice of  X  does not affect the results greatly. To evaluate the results of our framework, we calculated the accuracy as the number of correctly linked list items divided by the total number of all list items.
We compared our framework LIEGE with the algorithm proposed in [16], which we refer as TableAnno ,overthe Wiki Manual and Web Manual data sets. To give a fair comparison, we used the Wiki Manual data set to learn the weight vector and Web Manual data sets, which is the same way as that proposed in [16]. The experimental results of the baseline method TableAnno performing collective inference in the full model over the Wiki Manual and Web Manual data sets in-troduced in [16] are shown in Tables 3 and 4, respectively. The experimental results of our framework LIEGE over the Wiki Manual and Web Manual data sets are also presented in Tables 3 and 4 respectively. In these two tables, besides the accuracy, we also show the number of correctly linked list items. For our framework LIEGE, we do not only show the performance of LIEGE leveraging all the features intro-duced in this paper, which we refer as LIEGE full , but also present the performance of LIEGE leveraging a subset of the features. When we calculated the linking quality using the Formula 11, if we set  X  =0and  X  =0,itmeansweonly leveraged the feature of prior probability to address the list linking task, which we refer as LIEGE  X  =0 , X  =0 .Ifweonlyset  X  = 0, it means that we leveraged the features of prior prob-ability and distributional context similarity, which we refer as LIEGE  X  =0 . Likewise, if we only set  X  =0,itmeansthat we leveraged the features of prior probability and type hier-archy based similarity, which we refer as LIEGE  X  =0 .From the experimental results shown in Tables 3 and 4, we obtain the same conclusion that our framework leveraging differ-ent subsets of features achieves significantly higher accuracy than the baseline method TableAnno ,whichperformscol-lective inference using entities, types and relationships. It can be also seen from the results in Tables 3 and 4 that ev-ery feature has a positive impact on the performance of our framework, and with the combination of all features LIEGE can obtain the best results.

Thebaselinemethod TableAnno is proposed to deal with the table annotation task [16], while for the Web list data set, TableAnno cannot perform collective inference using entities, types and relationships. Moreover, the approach LIEGE  X  =0 , X  =0 , only leveraging the feature of prior probabil-ity , significantly outperforms the approach TableAnno over both Wiki Manual and Web Manual data sets. Thus, we did not compare our framework LIEGE with the TableAnno method over the Web Lists data set. To learn the weight vector validation. We show the experimental results of LIEGE leveraging different subsets of features over the Web Lists data set in Table 5. From the results in Table 5, it can be seen that the list linking task over the Web Lists data set is more challenging compared with that over the Wiki Manual and Web Manual data sets, as the accuracy achieved by the approach LIEGE  X  =0 , X  =0 over the Web Lists data set (78.13%) is much smaller than those over the Wiki Manual (86.40%) and Web Manual (85.78%) data sets. However, the approach LIEGE full leveraging all the features obtains very high accuracy over the Web Lists data set, which demon-strates the effectiveness of our framework LIEGE over the Web list data set.

To better understand the performance characteristics of our proposed LIEGE framework, we conducted sensitivity analysis to understand the influence of the parameter  X  to the results of LIEGE. Here, we used our framework leverag-ing all the features LIEGE full as the example. Figure 2 de-picts the performance of LIEGE full with varied parameter  X  over these three data sets, i.e., Wiki Manual, Web Manual and Web Lists, respectively. Recall that the parameter  X  is called the penalty parameter in Section 3.2, which gives some penalty to the list item which violates the assumption. From the trends plotted in Figure 2, it can be seen that when  X  is set to be larger than 40, the accuracies achieved by LIEGE full arestablewithvaried  X  and remain the best over these three data sets, which demonstrates that the results of our framework are insensitive to the parameter  X  ,and thechoiceof  X  does not affect the results greatly. Thus, we set  X  = 100 in all experiments introduced above. While we only report the performance of LIEGE full over these three data sets, similar trends are observed for the approaches LIEGE  X  =0 and LIEGE  X  =0 over all these three data sets. As more and more knowledge bases like DBpedia [2], Know-ItAll [9], YAGO [21, 20] and KOG [23, 24] are available pub-licly, considerable progresses have been made in linking en-tities in free text with a knowledge base [3, 6, 7, 13, 15, 19]. Entity linking is the task to link a textual entity mention in the unstructured free text, with the corresponding real world entity in an existing knowledge base. Bunescu and Pasca [3] firstly tackled this problem by exploiting a set of useful features derived from Wikipedia for entity detection and disambiguation. They leveraged the bag of words model to measure the cosine similarity between the context of the mention and the text of the Wikipedia article, and selected the entity whose Wikipedia article is most similar to the con-text where the mention appears as the mapping entity for this mention. Cucerzan [6] proposed a solution which is the first system to recognize the global document-level topical coherence of the entities. The system addresses the entity linking problem through a process of maximizing the agree-ment between the context of the entity mention and the con-textual information extracted from the Wikipedia, as well as the agreement among the categories associated with the candidate entities. The learning based solution in [7] focuses on the classification framework to resolve entity linking. It develops a comprehensive feature set based on the entity mention, the contextual document and the knowledge base entry, and then uses a SVM ranker to score each candidate entity. Han and Sun [13] proposed a generative probabilis-tic model for the entity linking task. This model incorpo-rates multiple types of heterogenous entity knowledge, i.e., popularity knowledge, name knowledge and context knowl-edge. Our previous work [19] proposed LINDEN to deal with the entity linking task. LINDEN is a novel framework to link named entities in text with a knowledge base unify-ing Wikipedia and WordNet, by leveraging the rich semantic knowledge embedded in the Wikipedia and the taxonomy of the knowledge base. In all these work, the essential step is to define a similarity measure between the text around the entity mention and the document associated with the entity. While in our list linking task, lists never have any header text information and textual context. The only input for the list linking task is a list of entity mentions extracted from the Web pages. Therefore, these methods cannot be applied to our list linking task.

Kulkarni et al. [15] proposed a general collective disam-biguation approach based on the observation that coherent documents refer to entities from one or a few related top-ics or domains, and entities mentioned from a single docu-ment are likely to be semantically related. In that paper, they gave precise formulations for the trade-off between lo-cal compatibility between mention and entity and measures of global coherence between entities. However, this observa-tion cannot be applied to the Web list data, since the entity mentions in a Web list can be any set of entities that have the same conceptual type, that is to say, they are semanti-cally similar, rather than semantically related. Accordingly, it can be seen that the list linking task is challenging and greatly different from the traditional entity linking task.
Another thread of research related to our work is manag-ing structured data on the Web [1, 4, 5, 18, 8, 12, 22, 16]. These solutions were proposed to find, extract and integrate structured data on the Web. The work most related to our list linking task is the model proposed in [16] to address the problem of table annotation. In that paper, the table an-notation task is defined as three subtasks: annotating each column of the table with one or more types, annotating pairs of columns with a binary relation in the knowledge base and mapping table cells with entities that they likely mention in the knowledge base. They proposed a new probabilis-tic graphical model for simultaneously associating entities for cells, types for columns and binary relations for column pairs. Specifically, they modeled the table annotation prob-lem as a joint distribution over variables, and the goal is to find optimal values to the variables that maximize the joint probability. However, for the subtask of annotating table cells with entities, they did not consider the prior probabil-ity of entities being mentioned and the semantic similarity between each pair of entities in the same column or list, which have been shown to be significantly important and effective in our experiments for the list linking task. In this paper, we have studied the problem of list linking. We propose LIEGE, the first framework that can effectively link list items in the Web lists with a knowledge base to the best of our knowledge. LIEGE is based on the observation that the type of the mapping entity should be coherent with the types of the other mapping entities in the same Web list. To evaluate the effectiveness of LIEGE, a thorough experi-mental study was conducted, and the experimental results demonstrate that our framework significantly outperforms the baseline method in terms of accuracy. Moreover, LIEGE can run in a fully automatic manner without tedious param-eter tuning.
This work was supported in part by National Natural Sci-ence Foundati on of China under Grant No . 60833003, Na-tional Basic Research Program of China (973 Program) un-der Grant No. 2011CB302206, and an HP Labs Innovation Research Program award.
