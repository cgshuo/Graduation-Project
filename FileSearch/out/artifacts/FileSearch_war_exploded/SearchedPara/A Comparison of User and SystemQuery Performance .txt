 Query performance prediction methods are usually applied to estimate the retrieval effectiveness of queries, where the evaluation is largely system sided. However, little work has been conducted to understand query performance prediction from the user X  X  perspective. The question we consider is, whether the predictions of query performance that systems make are in line with the predictions that users make. To this aim, we compare the performance ratings users assign to queries with the performance scores estimated by a range of pre-retrieval and post-retrieval query performance predic-tors. Two studies are presented that explore the relationship between user ratings and system predictions on two levels: (i) the topic level , and, (ii) the query suggestions level .Itis shown that when predicting the performance of query sug-gestions, user ratings were mostly uncorrelated with system predictions. At the topic level though, where a single query is judged for each information need, we observed moder-ate correlations between user ratings and a subset of system predictions. As query performance prediction methods are often based on intuitions of how users might rate queries, these findings suggest that such methods are not represen-tative of how users actually rate query suggestions and top-ics. This motivates further research into understanding the rating process engaged by users, and developing models of query performance prediction in order to bridge the divide between systems and users.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Human Factors, Experimentation Query Performance Prediction, Query Suggestions, User Rat-ings
An essential part of the Information Retrieval (IR) pro-cess is the formulation or generation of an effective query, i.e., one that retrieves relevant information. To determine whether a query is likely to be effective, there has been sig-nificant investment into the development of Query Perfor-mance Prediction (QPP) methods [8, 9, 14, 31, 36, 38, 39, 40]. These automated techniques aim to estimate the qual-ity of the search results returned by a retrieval system in response to a query. The idea is that if an ineffective query can be detected apriori , measures can be taken by the sys-tem to improve the query and subsequently the search re-sults. Conversely, if a query is predicted to be effective, its search results may be further improved by automatic query expansion.

The evaluation of the numerous QPP methods proposed in the past is largely based on their correlation with retrieval effectiveness measures such as average precision. However, it has been shown that such system oriented retrieval ef-fectiveness measures can be poor indicators of actual user satisfaction [1, 16, 28, 30]. Perhaps the predictions made by QPP methods are more indicative of user ratings of query quality than retrieval effectiveness. Determining the extent of the relationship between the predictions of query qual-ity 1 made by QPP methods and the quality ratings made by users is extremely important in the context of interactive and adaptive IR. For example, when offering query sugges-tions, the retrieval system needs to determine which sug-gestions the user is most likely to select. And, when users are presented with a set of query suggestions they are es-sentially being asked to assess a set of queries. While many researchers have examined the potential usefulness of term and query suggestions and techniques for automatically gen-erating query suggestions in particular contexts [18, 26, 34, 35], little research has been conducted about how users make decisions about queries and suggestions, in contrast, to the system sided query performance prediction work.

Thus, an open question is whether the predictions about a query X  X  quality by users are similar to the predictions made by the QPP methods employed by retrieval systems. To this aim, we compare the performance ratings users assign to queries with the performance scores estimated by a range of QPP methods. Two studies are presented that explore the relationship between user ratings and system predictions on two levels: (i) the topic level , and, (ii) the query sugges-tions level . At the topic level, the performance of a query is estimated for each topic (i.e. information need), given a set of topics. This setup is the typical system sided QPP evaluation. At the query suggestions level, the performance of each query of a given set of queries is estimated for an information need. This is a setting that is more commonly experienced by users either implicitly (as they may have to think about posing several possible queries), or explicitly (as the retrieval system may recommend a set of suggestions). Thus, the query suggestions level can be considered more user oriented as it focuses on the problem of query selec-tion. The studies performed as part of this research provide two distinct levels on which to consider these query perfor-mance prediction tasks. They allow us to better understand the relationship between users, retrieval systems and per-ceived/predicted query quality.

The paper is organized as follows: we first outline the two areas of related work, namely query performance prediction (Sec. 2.1) as well as term and query suggestions (Sec. 2.2). In Sec. 3 the user studies conducted and the QPP methods used in our experiments are described. Then, the results are presented and analyzed in Sec. 4, before Sec. 5 rounds off the paper with a discussion and the conclusions.
We first present an overview of QPP methods before de-tailing previous work on term and query suggestions. The first part is largely system oriented research, whereas the second part is more user oriented research. Although there is much overlap between the two areas, i.e., both systems and users making predictions about the quality of queries, little work has been conducted to consider both sides to-gether.
Automatically predicting the performance (or retrieval ef-fectiveness), of queries is a very active area of research [8, 9, 14, 36, 38, 39, 40]. This is because it is believed that accurately predicting a query X  X  effectiveness would enable the development of adaptive components in retrieval sys-tems [3, 36]. QPP methods can be classified according to the time of their prediction, either pre-retrieval (before the retrieval stage), or, post-retrieval (after one or more retrieval stages). The key difference between both classes is the amount and type of information that the methods use in the estimation of query quality. The key difference be-tween each specific QPP method is the intuition underlying the method. Interestingly, while most QPP methods have been motivated and developed based on how a user might rate a query, these intuitions have never been empirically validated.

In this paper, we examine eight pre-retrieval and five post-retrieval predictors over a variety of parameter settings to determine whether the intuitions from which they are de-rived are in line with how users rate queries.
Pre-retrieval QPP methods estimate the performance of a query without considering the ranked list of results returned by a retrieval system in response to the query. Such meth-ods generally exploit one of four heuristics [12] when mak-ing their prediction: specificity , ambiguity , term relatedness or ranking sensitivity . Specificity based predictors exploit collections statistics such as the inverse term or document frequencies. The intuition is that if a query has a higher specificity then it is likely to perform better than a query with low specificity [14, 23, 38]. Ambiguity based predic-tors either rely on external semantic sources such as Word-Net [20] or on the clustering of the corpus documents [15] to determine the number of possible senses (clusters) associated with a term. Query terms that always appear in the same or similar senses or contexts across all documents, are consid-ered unambiguous and thus better performing than queries with highly ambiguous terms. Predictors that utilize the re-latedness of terms examine the relationship between query terms; highly related query terms indicate a well formed query which is likely to be successful [12]. Finally, ranking sensitivity based predictors exploit the potential sensitivity of the result ranking by predicting how easy it will be for the retrieval approach to rank the documents containing the query terms [38]. If the distribution of query terms is uni-form across a large set of documents, the retrieval system is assumed to have difficulties ranking those documents and the query is considered to be of low quality.
Post-retrieval predictors are employed after retrieving one or more ranked lists of results. The strategies employed are manifold. For example, a comparison between the ranked list and the corpus [8] yields a homogeneity score: the more homogeneous the top retrieved documents, the better the estimated quality of the query. Perturbing the query terms and subsequently comparing the generated ranked lists of re-sults with respect to their overlap was evaluated in [31, 36, 40]. It was found that the higher the overlap, the less the results were influenced by a change in query term weight-ing and thus the better the estimated quality of the query. A similar strategy is the perturbation of documents in the ranked list of results retrieved in response to the initial query in order to determine the list X  X  stability [31, 39]; the more stable the ranked list the better the quality of the search re-sults (and thus the query) is assumed to be. Relying on the distribution of retrieval scores assigned to the documents in the result list has also been explored as an avenue for pre-dicting query performance [24]. Finally, the reliance on a variety of retrieval approaches to form predictions based on the diversity of the returned documents [9] has also been proposed; the more the different retrieval methods agree on the top retrieved documents, the better the query quality.
The standard methodology of evaluating QPP methods is based on comparing predicted performance scores with ac-tual system performance. First, a QPP method computes a predicted performance score for each query of a set of top-ics. A system effectiveness metric, such as average precision, is also calculated for each query and a chosen retrieval ap-proach. The correlation is then computed between the QPP-based predicted performance scores and the actual retrieval effectiveness scores. It is assumed that a higher correlation coefficient means a better QPP method 2 .
However, there has been related work examining and per-haps challenging the relationship between system-centered retrieval effectiveness measures and actual user performance in traditional IR settings [1, 16, 28, 30]. This research has shown consistently that traditional system-centered per-formance measures do not correlate well with user perfor-mance. Specifically, it has been found that when users use systems that are considered  X  X ood X  according to a system-centered evaluation framework, they perform no differently than when using systems that are considered X  X ad X  X 16]. Re-cently, it was shown that users adapt their search behaviors to compensate for poorly performing systems and, with a little more time, are able to perform just as well as those who use a better system [25]. Overall, the findings from this body of research pose a fundamental question about the transferability of experimental results that are obtained using traditional system-centered evaluation frameworks to user scenarios. Given that QPP methods have mainly been developed within the context of a system-centered evalua-tion framework, it is important to examine QPP methods in a user-centered evaluation as well. Only few attempts have been made in this direction. In previous work they consid-ered how QPP methods relate to human judgments of query quality along two lines: 1. user ratings vs. system performance, and 2. inferred user ratings vs. system performance.
 Of the first line, in an experiment in the late 1990 X  X  [32], a number of IR researchers were asked to classify TREC topics as either easy , medium or hard for a newswire cor-pus they were familiar with. The researchers were given the TREC topic statements, though not the search results. It was found that they were largely unable to predict the top-ics X  quality correctly and, surprisingly, they could not agree among themselves on how to classify the topics. Of the sec-ond line, in [29, 37] initial experiments were performed that compared a user based measure (the median time to find a relevant document) with the post-retrieval QPP method Clarity Score [8] and a range of pre-retrieval QPP meth-ods [37]. In [29], no significant correlation was found for Clarity Score , while in [37], the best pre-retrieval predictor achieved a Kendall X  X  Tau rank correlation of  X  =0 . 2. How-ever, these experiments were conducted in limited contexts, i.e., IR researchers on a small data set [32] or using time as an implicit user rating of query quality [29, 37]. We examine a wider range of QPP methods than previously, we consider explicit (instead of implicit) ratings of query quality by users and furthermore we investigate two different tasks, on the topic level as well as on the query suggestions level.
The work on term and query suggestions is related to in-teractive query expansion, query substitution, and query completion. All of these approaches strive to assist users with query formulation and refinement. One can distinguish between suggestions of terms, phrases or whole queries. In the latter case the advantage is the preserved lexical coher-ence, while the suggestion of single terms might make sense from a statistical corpus-based point of view only.
User studies have shown that users prefer to be in control of query expansion terms, but that in many cases the use of suggested terms does not result in improved performance [19, 21, 22]. Such findings are often contrary to results ob-tained using system-centered evaluations of query expansion techniques which have generally shown that automatic query expansion is effective [22]. For example, even though users in [21] rated interactive query expansion better than auto-matic query expansion, there were no differences between the actual effectiveness of the searches. Users in Belkin et al. X  X  study [6] were also positive about the potential of term suggestions but did not necessarily use, or perform better with, this feature.

Several studies have been conducted to better understand potential reasons for these differences [6, 10, 11, 22]. Great variability was found by Ruthven [22] in users X  abilities to identify good expansion terms, with users identifying 32-73% of the good terms. Ruthven [22] also found that users often identified terms that had a high collection frequency as being good expansion terms, even though from a system X  X  standpoint these terms are unlikely to be useful. Certainly the statistical information is important for the system, but this information is not available to users and even if it was, it might not be useful to them. Given that many QPP methods rely on collection statistics similar to those used to identify term suggestions, it may be that the user X  X  and system X  X  ideas about what constitutes an effective query differ as well. In this paper, we examine whether the QPP methods that rely upon collection statistics (i.e., document frequency and collection frequency) provide a better explanation of user ratings than other QPP methods, if at all.

A number of studies have also been conducted concerning query suggestions [4, 5, 18, 26, 27, 34, 35]. Query sug-gestions address one of the problems identified with term suggestions in that queries are larger semantic units that are lexically more coherent than single terms. This may better assist users in determining the potential usefulness of suggestions. This is a key point since the usefulness of suggestions depends on the user identifying and using the best suggestions among a set. Systems produce many dif-ferent kinds of suggestions with varying quality. Ultimately, the benefit of suggestion techniques lies in the user X  X  ability to make predictions about the usefulness of the suggestions. This prediction, we note, occurs after the system has made its own predictions, and these query suggestions, like terms suggestions, are generated using a number of different meth-ods. Some techniques rely on matching the current query with previously submitted queries, while other techniques attempt to automatically construct queries. Kelly et al. [18] used traditional term suggestion techniques to identify terms which were then combined with user queries in various ways to generate synthetic query suggestions. The authors com-pared these synthetic query suggestions to query suggestions created by other users, as well as to simple term suggestions. Overall, users selected more human-generated query sugges-tions, and many commented about the poor quality of the synthetic suggestions and suggested terms, which again indi-cates that terms predicted as useful by the system may differ from those found useful by users. This suggests that users might agree more between themselves about what makes a good query, than with, and in contrast to, QPP methods.
In summary, there is a shortfall in research examining the link between users and systems with regards to how they rate the quality of queries and predict query performance.
To determine whether predictions about a query X  X  quality by users are similar to the predictions made by QPP meth-ods, we have performed an analysis which consists of ratings and predictions from: (i) a user study which obtained query ratings pre-retrieval, at both the topic level and the query suggestions level , (ii) a secondary analysis of data from a pre-vious user study which obtained query ratings post-retrieval at the query suggestions level , and (iii) corresponding predic-tions by pre-retrieval and post-retrieval QPP methods. The experiments were performed on two different TREC Test Collections. The results from these experiments enabled us to examine the following operational research questions: In the topic level setup, we examined how well users are able to predict the performance of a query given the textual description of its underlying information need (the TREC topic description). That is, for each information need a sin-gle query was presented for rating. Since in our study the human assessors did not have access to the search results of these queries they acted as human pre-retrieval predictors.
In the second setup, at the query suggestions level ,we evaluated the ability of users to judge the quality of query suggestions. For each information need/topic, the users were presented with eight different query suggestions to rate. These ratings were obtained from two different user studies, in one, pre-retrieval ratings by users were obtained based on the query and the information need (the TREC topic de-scription), and in the other post-retrieval ratings by users were obtained based on the query, information need, and interaction with the search results.

For both setups we also obtained corresponding ratings from eight pre-retrieval and five post-retrieval QPP meth-ods. The topic level experiment was conducted on the TREC ClueWeb09 (category B) collection, while the query sug-gestions level experiment was performed using the TREC Aquaint collection. An overview of the experimental setups for each collection and task level is presented in Tab. 1. The remainder of this section details the QPP methods used, and then the two user studies conducted.

Table 1: Overview of experimental conditions.
In our analysis, we relied upon a variety of well-known prediction algorithms there were applied to both the topic level and query suggestions level for both collections. Due to space constraints, we only briefly describe each method used and where appropriate the parameter settings used. The pre-retrieval predictors evaluated were:
AvIDF , MaxIDF , AvSCQ and SumSCQ belong to the class of specificity-based predictors, which rely on term and document frequencies of the query terms in the corpus to de-rive a predicted quality score. AvQL is part of this category as well, but considers only the average number of characters in a query -the assumption being that longer terms are less common in a corpus and thus more specific. In contrast, Av-VA R and SumVAR exploit the distribution of TF.IDF based term weights in the corpus and thus belong to the ranking sensitivity category. Finally, the relatedness between query terms is expressed by AvPMI , where a higher relatedness be-tween query terms indicates a better query quality. Please note, that all of these QPP methods are parameter-free.
The post-retrieval predictors , i.e. those that rely on one or more retrieved result lists, evaluated were as follows:
These methods were chosen due to their state-of-the-art performance and the diversity of approaches they represent. In the Ranking Robustness [39] approach, the top m re-trieved documents of the initial search are perturbed by adding or removing terms. The perturbed documents are then ranked based on the original query and retrieval ap-proach. The higher the correlation between the original and perturbed result list, the higher the predicted query quality. We evaluated this approach for m = { 10 , 50 , 100 , 250 , 500 , 1000 , 2500 , 5000 } .

In the Spatial Autocorrelation [9] method, a document X  X  retrieval score is replaced by the weighted sum of retrieval scores of its k most similar documents in the retrieved result list as determined by TF.IDF. The linear correlation coef-ficient between the original document scores and the per-turbed document scores form the predicted query quality score. The parameter k was varied between 2, 5, 10 and 15.
In contrast to document (score) perturbation, Query Feed-back [40] is based on query perturbations: from the origi-nally retrieved top m ranked documents, a new, perturbed query is generated consisting of the n most discriminative terms. A second ranked list is retrieved based on the per-turbed query and the overlap between the two lists is uti-lized as query quality score. The lower the overlap be-tween the two lists, the lower the predicted query qual-ity. We evaluated all possible parameter combinations with m = { 10 , 20 , 30 , 40 , 50 , 100 } and n = { 10 , 20 , 30
Clarity Score [8] is based on the intuition that the top m ranked documents of an unambiguous query will be topically cohesive and terms particular to the topic will appear with high frequency. Thus, the higher the difference between the term distribution of the top retrieved documents and the term distribution of the corpus, the higher the predicted query performance. We experimented with values of m = { 10 , 50 , 100 , 250 , 500 , 750 , 1000 } . An adaptation of Clarity Score, which ignores terms in the calculation that appear in more than 1 k th of the corpus was introduced in [13]. We evaluated k = { 1 , 10 , 100 } .

Lastly, Query Commitment [24] determines the standard deviation  X  of the retrieval scores of the top m retrieved doc-uments, possibly normalized by a query-dependent corpus statistic. The higher  X  , the higher the difference in retrieval scores in the result list, indicating a few top ranked docu-ments with a high query commitment and thus high quality is predicted. We experimented with m = { 10 , 50 , 100 , 250 , 500 , 750 , 1000 , 2500 , 5000 , 10000 } .
Two user studies provided the human ratings of query quality at the topic level and the query suggestions level, as indicated by Tab. 1. Details of each study are described below.
Following on from the previous experiments [29, 32, 37], we performed a similar experiment at the topic level using the most recent TREC test corpus: ClueWeb09 (category B) [7], a 50 million document crawl of the Web from 2009. We utilized the fifty topics of the TREC 2009 Web ad hoc retrieval task 4 which consist of a query part (to be submit-ted to the IR system) and a description (the information need). E.g., the topic wt09-3 consists of the query  X  X etting organized X  and the description  X  X ind tips, resources, supplies for getting organized and reducing clutter X  .

We provided assessors with the queries and descriptions and instructed them to judge, on a scale from 1 (poor qual-ity query) to 5 (high quality query), the queries according to  X  X hat you expect the search results to be, if you would sub-mit these queries to a Web search engine (keeping in mind the actual information need) X . The presentation order of the queries was randomized across assessors. Note, that the queries were not actually submitted to a search engine.
In a second experiment, this time on the query suggestions level , the same assessors were asked to judge eight query sug-gestions for each of four topics taken from the topics of the TREC Robust track [33] which were based on the Aquaint corpus of news stories. The particular query suggestions used were obtained from user study US2, described in Sec-tion 3.2.2. The assessors were presented the topic descrip-tions and the query suggestions, but not the result list, thus providing sets of pre-retrieval query quality ratings for the query suggestions level.
Eighteen users, recruited via email solicitation from two university research groups (Databases, Human Media In-teraction) participated in the experiments. Eleven partici-pants were male. Lemur 5 was utilized as the underlying re-trieval system. The document indices were Porter stemmed and stopwords were removed. The KL-divergence based retrieval model was relied upon to calculate the system-oriented ground truth with respect to TREC relevance judg-ments (Dirichlet smoothing with  X  = 1000). The retrieval effectiveness was measured in precision at 30 documents (P@30) and average precision (AP) for the Aquaint corpus. In the case of ClueWeb09, AP and P@30 were estimated according to [2], which is the evaluation measure/procedure for this corpus at TREC. To denote the difference between these measures, we refer to the ClueWeb09 measures as es-timated AP and estimated P@30 respectively.
Data for this second study, which was performed on the query suggestions level , was generated from a user study per-formed previously where the use of query suggestions was the focus [17]. For our experiments here, we have performed a secondary analysis of the data. The basic goal of the pre-vious study was to examine subjects X  selection of query sug-gestions.

This study involved subjects engaging in search using an experimental IR search application, where Lemur was used for indexing and retrieval. As part of this experience, the subjects evaluated the quality of query suggestions that were presented to them after they had ended their search session for a search topic.

Four of the eight presented query suggestions were high quality queries and four were low quality queries, a clas-sification that was performed by examining the number of relevant documents retrieved in the top 20 results. An exam-ple is TREC topic 354 from the TREC Robust track whose description is  X  X dentify instances where a journalist has been put at risk (e.g., killed, arrested or taken hostage) in the per-formance of his work. Any document identifying an instance where a journalist or correspondent has been killed, arrested or taken hostage in the performance of his work is rele-vant. X  ; two high quality query suggestions in this case were  X  X ournalist killed X  and  X  X ournalists arrested for work X  wheras two poorly performing suggestions were  X  X eporter killed X  and  X  X ournalist at risk reporting danger X  . The average number of terms per query suggestion was 3 . 19. Note, the query sug-gestions for User Study 1 and User Study 2 were the same. Since the participants of this study rated the query sugges-tions after they completed their search task, they acted as human post-retrieval predictors. The participants of this study were twenty-three university students who responded to a campus-wide email solicitation for research subjects. Most of them were female ( n = 16) and their average age was 21 years. Students X  majors varied across the humanities, social sciences and sciences.
We first report the results on the topic level experiments in Sec. 4.1. Then, in Sec. 4.2 we give an overview of the query suggestions level experiments. In both instances, we analyze: (i) the human assessor ratings of query quality, (ii) the QPP methods X  predictions of query quality, and, (iii) the relationship between assessor ratings and the QPP methods X  predictions.
Recall, that in this setup, the assessors and QPP methods predict the quality of a single query per topic or information need. The assessor ratings were collected in User Study 1 which focused on the pre-retrieval setting.
The ratings of query quality by the 18 study participants varied considerably, leading to a relatively low inter-rater agreement. When comparing all possible pairs of partici-pants, we observed a maximum linearly weighted Cohen X  X  Kappa of  X  =0 . 54, which is a moderate agreement. The median agreement between all pairs reached  X  =0 . 36, while the minimum amounted to  X  =0 . 12, indicating low agree-ment. These findings echo those found in [32], where a low agreement among assessors was also noted.

But, how did the assessors fare in identifying queries that performed well and those that performed poorly according to system effectiveness measures? To investigate this, we split the 49 queries into four partitions with ten queries and one with nine based on system performance. This was done for both estimated AP and P@30, to provide two rankings of query quality according to system performance (Tab. 2, columns 2&amp;3). In both instances, the top ten perform-ing queries were in partition one, the next best performing queries were in partition two, and so on. For each mea-sure and each partition, we then averaged all observed as-sessor ratings for the queries within the partition. Columns 4&amp;5 in Tab. 2 show the average user rating and the stan-dard deviation  X  for each partition and measure. The trend suggests that the assessors rated poorly performing queries lower than the highly performing queries. When we consider the results for estimated AP (P@30 is similar), particularly stark is the contrast between the best and worst partition: while the average rating for queries of the best partition is 3 . 87 (out of a max. of 5 . 0), the worst partition is assigned an average of 2 . 51 (out of a min. of 1 . 0). This suggests that although the assessors do not agree to a high degree with each other on the quality ratings, on average, they are able to distinguish good from bad queries at the topic level.
To evaluate the relationship between assessor ratings and system performance in more detail, we computed the corre-lation between assessor ratings and retrieval effectiveness, following the methodology that is used to evaluate QPP methods. Here, we report the rank correlation coefficient Kendall X  X  Tau, a standard QPP evaluation measure. The worst correlated assessor obtains a correlation of  X  =0 . 17 with estimated AP (  X  =0 . 20 with P@30), the median cor-relation coefficients are  X  =0 . 31 for AP and  X  =0 . 35 for P@30 respectively (both statistically significant at p&lt; 0 . 01). The most highly correlated assessor reaches a correlation of  X  =0 . 47 with estimated AP and  X  =0 . 45 with P@30 (both statistically significant at p&lt; 0 . 01). This result shows that the assessors X  ability to rate the quality of queries cor-rectly varies significantly, despite the fact that the assessors all have a comparable educational and search-engine-usage background. It also indicates that although, on average, assessors could rate the quality of queries given the differ-ent bands of query quality (as shown in Tab. 2), the asses-sors had much more difficulty precisely rating the quality of queries.
As mentioned, QPP methods are usually evaluated by reporting their correlation with retrieval effectiveness mea-sures. For completeness, we report these results in Tab. 3, columns 2&amp;3. In the case of the post-retrieval QPP meth-ods, we report the results of the best performing parameter settings only. Note that at the topic level for this particular corpus, the pre-retrieval QPP methods SumSCQ and Sum-VA R achieve the highest correlation and thus outperform the more complex post-retrieval QPP methods. This is in contrast to previous findings where in older test corpora it is the more complex QPP methods that obtain higher cor-relations. We suspect that in the case of ClueWeb09, which was derived from a recent crawl of the Web, relying on doc-ument content as the post-retrieval QPP methods do, can also be a disadvantage as nowadays Web pages do not only contain informational content, but also a large amount of non-informative content (e.g., navigational elements, adver-tisements, spam, etc.), which may adversely affect the abil-ities of these predictors.
 Table 3: Topic level experiment: Kendall X  X  Tau correlation coefficients between QPP methods and system effectiveness (columns 2&amp;3). The correla-tion between QPP methods and assessor ratings (minimum, median and maximum Kendall X  X  Tau) are listed in columns 4-6. Significant correlations ( p&lt; 0 . 01 ) are marked with a star.
So far we have seen that neither assessor ratings nor QPP methods correlate highly with system effectiveness, at the topic level. Given that there is quite a mismatch with system performance, it may be the case that there is higher agree-ment between the assessors and QPP methods. So now, we turn our attention to the focus of this paper, and determine whether at the topic level the predictions of query perfor-mance by QPP methods fall in line with the quality ratings made by the assessors, and consider the correlation between them. The results are reported in Tab. 3, columns 4-6. Due based on estimated AP and P@30 respectively (columns 4&amp;5). to the, at best, moderate level of agreement between the assessors we resort to reporting the minimum, the median and the maximum correlation between the assessor ratings and the QPP methods. We observe the highest min., med. and max. correlation between assessor ratings and Sum-SCQ [38]. This predictor combines the collection term fre-quency and inverse document frequency. It is summed over all query terms of a query Q = { q 1 , ..., q m } : Here, doccount is the number of documents in the corpus, df is the document frequency and cf is the collection term frequency. Zhao et al. [38] argue that a query, which is sim-ilar to the corpus as a whole is easier to retrieve documents for, since the similarity is an indicator of whether documents answering the information need appear in the corpus. Since the score assigned to a query is proportional to collection term frequency and inverse document frequency of terms, the terms that appear in few documents many times are favored. Those terms are highly specific, as they occur in relatively few documents, while at the same time they occur often enough to be important to the query.

It is also worth mentioning, that SumSCQ and SumVAR achieve significant correlations with most assessors (the me-dian correlation is significant), while the remaining 11 QPP methods only sometimes obtain a significant correlation with few assessors at best. The post-retrieval QPP methods, which are reported with their best parameter settings, apart from Query Feedback all perform poorly, resulting in no sig-nificant correlation with any human assessor.
For the query suggestions level experiments, we relied on the data from the two user studies; in US1 (Sec. 3.2.1) the assessors were asked to rate query suggestions without ac-cess to the search results (i.e., pre-retrieval) while in US2 (Sec. 3.2.2) the assessors actually performed different search tasks and rated the query suggestions after completing each search session (i.e., post-retrieval). Recall, that at the query suggestions level, assessors were given eight query sugges-tions to rate per topic -four high quality suggestions and four low quality suggestions.
In the pre-retrieval setup, when we averaged the user ratings over all high and low quality query suggestions re-spectively, the high quality query suggestions received an average rating of 3 . 74 (  X  =1 . 02) while the low quality sug-gestions were rated on average with 2 . 83 (  X  =1 . 23). The inter-rater agreement between any of the assessors was, at best,  X  =0 . 55, while the median was  X  =0 . 25 and the minimum  X  =  X  0 . 05. When comparing these numbers to the topic level experiments (the same set of assessors), we note that while on average the group can distinguish good from bad query suggestions in terms of system effectiveness, when it comes to the agreement between assessors at the query suggestion level there was less agreement between as-sessors -in the topic level experiments the median agree-ment reached  X  =0 . 36. This may suggest that rating query suggestions is a more difficult task.
 Table 4: Query suggestions level experiment: rating overview of the high and low quality query sugges-tions the study participants were offered as well as those that the participants used/not used.

In the post-retrieval setup, the assessors gave the high quality queries an average rating of 3.00 (  X  =1 . 15), and the low quality queries on average were rated as 2.80 (  X  =1 . 18). While, on average, the subjects from US2 tended to rate the effective query suggestions higher, it was not to the same ex-tent as the assessors in US1. Along with the average ratings for the high and low quality query suggestions, Tab. 4 also reports the total number of times the two groups of query suggestions were issued by the users in US2, out of the pos-sible number of suggestions for each type. The participants issued significantly more high quality query suggestions than low quality suggestions (  X  2(1 , 736) = 5 . 95, p =0 . 015) and rated high quality queries significantly higher than low qual-ity queries ( t (734) = 2 . 38, p =0 . 017). While the difference was significant, it was lower than in the pre-retrieval setup. This suggests that the influence of the interaction with the search results and issuing only a subset of the possible query suggestions may affect the ratings of the suggestions. In or-der to investigate whether a confirmation bias exists in the ratings, that is, whether a subject selects a query sugges-tion (good or bad) and then feels the need to justify her decision and rate the suggestion a bit higher than sugges-tions that she did not select, we investigated the average rating of query suggestions that were used and not used re-spectively by the subjects. The results are also shown in Tab. 4: while the difference in rating is very small for the high quality queries between the used and unused sugges-tions, in the case of poorly performing suggestions, larger differences appear: query suggestions that were issued re-ceive a lower average rating than suggestions that were not used.

The inter-rating agreement of assessors in the post-retrieval setup was lower than in the pre-retrieval setup. The median agreement among these participants was  X  =0 . 01 and the minimum and maximum were  X  =  X  0 . 26 and  X  =0 . 33 re-spectively. This again suggests that the queries each partic-ipant issued, and the different search results that the par-ticipants interacted with, also leads to less agreement. This is presumably because each participant X  X  state of knowledge has changed in different ways. Whereas in the pre-retrieval setting the assessors only have their base knowledge and the topic description, so the information that they have avail-able is common to all assessors, which may be the reason for the higher levels of agreement between them.
The correlations achieved by the QPP methods at the query suggestions level are reported in Tab. 5, columns 3&amp;4. We found the post-retrieval QPP methods to exhibit a higher correlation with system performance than the pre-retrieval predictors. Specifically, Query Feedback , Clarity Score and Query Commitment were all significantly correlated with AP and the former two with P@30 as well. On the other hand, of the pre-retrieval predictors only AvQL was signifi-cantly correlated with P@30. This is an interesting finding because it shows that when comparing queries for the same topic, pre-retrieval predictors are not as indicative of perfor-mance.
We now turn our attention again to the main research question and consider the relationship between the asses-sor ratings and QPP methods X  predictions. At the query suggestions level we can examine this relationship with re-spect to (i) pre-retrieval ratings (with assessors from US1), and (ii) post-retrieval ratings (with the assessors from US2). Columns 5-10 of Tab. 5 report the correlations between the predictions made by QPP methods and the ratings of query quality made by the assessors. As earlier, we report the min-imum, median and maximum correlation across all assessors due to the low inter-rater agreement. Several interesting observations can be made.

In the pre-retrieval setup (US1), we observed that the correlation between assessors and QPP methods tends to be positive overall, however, no QPP methd is significantly cor-related with the majority of assessors, that is, the median correlation is not significant. The majority of QPP meth-ods, both pre-and post-retrieval, were significantly corre-lated with the most correlated assessor. Similarly to the topic level experiments, SumSCQ and SumVAR performed very well with respect to the other QPP methods. This suggests that these predictors are the most indicative of as-sessor ratings, although the relationship appears stronger at the topic level than at the query suggestions level. In contrast to the topic level experiments, the majority of eval-uated post-retrieval QPP methods, specifically Robustness , Spatial Autocorrelation and Query Feedback ,leadtoasig-nificant correlation with the most correlated assessor.
In the post-retrieval setup (US2), the results were markedly different. The correlation between assessors and QPP meth-ods varied considerably, ranging from negative to positive correlations. The median correlation for the post-retrieval assessor with any of the QPP methods was close to, or around, zero, indicating that there was no relationship be-tween them. In fact, some assessors were quite negatively correlated with QPP methods (see minimum correlations) which ranged from  X  0 . 36 to  X  0 . 14. The only significant correlation (  X  =0 . 48) can be reported for Query Feedback . These results suggest that the post-retrieval assessments by participants are very varied, and with respect to QPP meth-ods are not consistently related in one way or another. It appears that the interaction with results and usage of the system has a considerable effect on the subjects ratings of query quality.
We have performed and analyzed two empirical studies comparing the predictions of query quality made by auto-matic QPP methods with the predictions made by human assessors. We have conducted a more comprehensive anal-ysis than previous work by examining the relationship of assessors with thirteen pre-and post-retrieval QPP meth-ods, at both the topic level and the query suggestions level. From this work, our main findings relating to our operational research questions outlined in Sec. 3, indicate that:
While the findings from this work are partially in tune with previous research (e.g., Turpin &amp; Hersh [29] found that Clarity Score , a commonly employed post-retrieval QPP ap-proach, has no correlation with implicit user ratings of query quality), we have teased out more precisely which QPP methods exhibit a relationship with explicit user ratings of query quality. In contrast to [32], we have found that, on average, quality ratings of queries tended to be in line with system performance at both the topic and the query sugges-tions level. However, query quality ratings obtained post-retrieval did not emphasize the difference in quality as well as those ratings obtained pre-retrieval from human assessors. Unfortunately, we are unable to definitively provide reasons for this difference. This is due to the number of experimental parameter settings are shown. variables and the limitations of the studies conducted here. Nonetheless, we have made substantial progress in determin-ing the relationship between automatic QPP methods and user ratings of query performance. And, we have identified a number of factors which appear to influence the ratings of queries. We discuss them in turn below: User Background In US1, the assessors may have had more system knowledge as they were largely computer sci-ence post-graduates, where as in US2 the assessors were un-dergraduates mainly from humanities. These different back-grounds may be the source of the differences observed given the ratings of query suggestions.
 Information Available While it is likely that the back-ground of the assessors may influence their ratings, the in-formation provided to assessors to rate the queries is prob-ably a source of greater variation. Depending on whether the assessor sees the query, or the query and the underlying information need, along with the search results, it is likely to affect the assessor X  X  perception of the query X  X  quality. In par-ticular, the interaction with the system and the engagement with the information is also likely to influence his ratings (see below).
 Interaction The interaction with the search system by the subjects as observed in US2 clearly impacted the ratings of query quality and is very likely to be responsible for the dif-ferences we observed between the groups. We found a con-siderable difference on the query suggestions level between the assessors who predicted the query quality before the re-trieval stage and after completing the search task. Assessors rating the suggestions pre-retrieval, agreed with each other to a higher degree than assessors who rated the suggestions post-retrieval. We posit that during the course of search-ing the cognitive states of the assessors would have changed in various ways depending on their interaction. This diver-gence from the initial state of query and information need is likely to account for the greater variation between user agreement and quality ratings.
 Obtaining Ratings Many difficulties are presented when obtaining ratings for queries. This is especially the case in the post-retrieval setup, where there are a number of po-tential variables which may influence the ratings. The main question is when should the query quality ratings be ob-tained? Should one wait until the end of the search task (as we did in our experiment) when they might have prob-lems distinguishing and remembering the different queries? Or should they be interrupted each time they issue a new query to rate the previous one? Also, at what point does it become a rating of actual performance, as opposed to a rating of predicted performance? Topic or Query Suggestions The difference between eval-uating one query per topic (topic level) or multiple queries per topic (query suggestions level) may also play a role. QPP methods are traditionally evaluated and optimized for the former, possibly a reason why the results for the topic level setup are more in line with actual user ratings. In the query suggestions level setup, the ratings are more comparative in nature, and so it might be that X  X anking X  X uggestions is more appropriate than  X  X ating X  suggestions.
 System Dependence vs. Method Independence Though not specifically considered in this study explicitly, the employed retrieval approach is also a factor when investi-gating the query quality ratings against system effectiveness. Pre-retrieval QPP methods predict a query X  X  performance independent of the retrieval approach. While this is not problematic when working with TREC corpora, where the ranking functions employed are similar, it becomes an issue when moving to the Web where search engines have access to a lot of additional features such as links, click-through data, etc. Note, that since the focus of this work was on whether QPP methods make predictions in line with users this is perhaps not relevant here, but may be applicable to future work.
 User Expectation With the prevalence of highly effective search engines users may be expecting a certain level of per-formance. This apriori expectation may influence the rat-ings of queries, and so the context of the system needs to be considered in the rating process. In summary, we have performed an analysis comparing and relating the predictions of QPP methods to the ratings per-formed by users. While some valuable insights have been gained by this study, substantially more research needs to be conducted in this direction. In particular, there needs to be a concentrated effort on understanding and developing methods for the user-sided query suggestions level; as this task appears to be more important than the standard QPP prediction task in the context of developing adaptive Infor-mation Retrieval systems. In future research we would like to explore the influence of the above factors on query perfor-mance prediction and to develop more sophisticated predic-tive models that include the user, their state of knowledge and the retrieval system within the process.
