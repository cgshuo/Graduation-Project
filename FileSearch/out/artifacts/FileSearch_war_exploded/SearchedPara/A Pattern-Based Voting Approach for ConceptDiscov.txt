 Domain-specific concepts are useful in a variety of traditional applications, in-cluding document summarizing, classification, clustering, indexing and query expansion [16, 17]. With the development of the Web, some new research areas and new applications are brought up, in which concept discovery also plays an important role. For example, in the Semantic Web area, concept discovery is a fundamental task of the knowledge capturing and ontology engineering pro-cesses. Although the potential benefit of concepts is large, the majority of current documents do not have an explicit declaration of the concepts presented. So au-tomatically discovering concepts from text is necessary.
 guage Processing (NLP) to analyze syntax and semantics of the text [1]. Other applications try to avoid expensive NLP techniques and apply machine learning or statistical approaches to find rules or patterns to identify phrases as concepts [2]. Some emerging techniques also attempt to mine topic-specific concepts and definitions on the Web [3]. In [3], the authors first identify the sub-topics and salient concepts of a specified topic using some heuristics, and then find the informative pages containing definitions and descriptions for the users. In this paper, we apply a data mining approach, called frequent sequence mining, to automatically discover patterns from text. Our goal is to use these patterns to identify concepts from sentences.
 two different ways. One is heuristics based, and the other is machine learning technique based. [6] and [7] present some heuristic methods for the extraction of medical terminologies and their definitions from online documents. [8] describes a system called GenEx for keyphrase extraction. GenEx has two components: the Extractor algorithm to process the input text and produce a set of keyphrases, and the Genitor genetic algorithm to tune the twelve parameters of Extractor to maximize its performance on training data. [9] presents an algorithm, Kea, to find important phrases from documents using a machine learning technique. In this study, two attributes are used to discriminate between keyphrases and non-keyphrases, namely, the TF*IDF score of a phrase and the distance into the document of a phrase X  X  first appearance. Then, a Naive Bayes model is generated from a set of training documents where keyphrases are provided. In the concept extraction stage, Kea computes TF*IDF scores and distance values for all phrases in the new document, then the Naive Bayes model is applied to each phrase, computing the estimated probability of it being a keyphrase. the Web. The patterns used in our approach are automatically obtained through Web mining. Through the evaluation, we can see that using automatically mined patterns, the recall of concept discovery increases greatly without decrease in precision. Compared with heuristics based systems, our approach does not need the manual labor to describe predefined patterns; compared with the machine learning based methods, our algorithm is more flexible. Given a sentence, the algorithm proposed in this paper can determine whether this sentence contains a concept and if so, accurately identify it. In contrast, the machine learning based methods must use the whole Web page to determine the attributes of a candidate phrase, such as distance and TF*IDF value which rely on features of the whole page. Another advantage is that our concept discovery algorithm uses sentences as the input unit, while other machine learning based methods treat phrases as input unit, and they need to first generate a lot of candidate phrases by some mechanical heuristics. In order to evaluate the discovery ability of a pure pattern based approach, we do not use TF*IDF or distance to measure the importance of candidate phrases; moreover, we deliberately avoid any heuristic to select more important sentences from a Web page.
 the process of pattern mining. Section 3 discusses the algorithm to refine pat-terns generated in the former step. The approach which automatically discovers concepts is given in Section 4. The evaluation results are presented in Section 5. In Section 6, we summarize our study and discuss some future work. In the pattern mining stage, we aim to discover the frequent sequences that characterize the syntactic patterns of sentences containing concepts. This stage altogether includes two phases: corpora construction and frequent sequence min-ing. In the corpora construction phase, based on several manually constructed concept hierarchies, Web pages relevant to the concept hierarchies are retrieved through a search engine (Google). Web pages are segmented into sentences, and divided into two categories, the concept-relevant corpus and concept-irrelevant corpus. In the first corpus, each sentence contains at least one concept in the concept hierarchies; the second corpus includes the rest of the sentences. Then sentences in both corpora are preprocessed according to part-of-speech and the appearance of concepts. A frequent sequence mining approach is performed on the first corpus to discover frequent sequences that characterize the syntactic patterns of sentences containing one or more concepts. In the following, we will describe these processes in detail. 2.1 Corpora Construction In many concept-based information retrieval models, a conceptual structure for mapping descriptions of information objects to concepts is widely used [10]. The authors of [10] divide such conceptual structures into 5 categories: conceptual taxonomy, formal or domain ontology, semantic linguistic network of concepts, thesaurus and predictive model. We make use of the first type, i.e. concep-tual taxonomy, which is a hierarchical organization of concept descriptions. The main relationships among concepts in a conceptual taxonomy are aggregation (parent-child) and association (sibling). We will use  X  X oncept hierarchy X  to refer to conceptual taxonomy in the rest of this paper.
 ferent knowledge domains, including computer science, mathematics, chemistry, physics, botanical science, etc. However, it is not necessary for a hierarchy to be complete, as long as all aggregation and association relationships involved are accurate. Next, a query corresponding to each concept in a hierarchy is gen-erated. We adopt a typical query expansion technique to produce the queries. Each concept which has a parent concept existing in the hierarchy will generate a query of the concept itself and its parent. The root concept of each hierarchy will generate a query of itself only. Then the queries are submitted to a search engine ( [11]), and the top N retrieved documents are saved to our local disk. Duplicate pages are deleted from the dataset to avoid bias during the mining process. operations are performed to the sentences in order to eliminate noisy words and concentrate on the more contributive ones. We define two types of uniform labels: concept related labels replace the appearance of concepts in a sentence, and part-of-speech labels are used to substitute articles and pronouns. The uniform labels still function as ordinary words, and the mining algorithm will ignore their difference from other words. If a sentence includes two or more appearances of concepts and two of them reflect an aggregation relationship in the concept hierarchy, then the parent concept is substituted by CD : P arentConcept and the child by CD : ChildConcept , and all other concepts in this sentence is replaced by CD : OtherConcept ; if there is no aggregation relationship in the sentence, all concepts are replaced by label CD : Concept . Articles and pronouns are substituted by CD : Article and CD : P ronoun respectively. Adjectives, adverbs, interjections, predeterminers are removed. The following is an example of the result of a processed sentence: Sentences with a concept related label are saved into the concept-relevant corpus, and the rest are saved into the concept-irrelevant one for further mining. 2.2 Frequent Sequence Mining Sequential pattern mining is a data mining task that discovers frequent subse-quences as patterns in a sequence database. The sequential pattern mining prob-lem was first introduced in [12]: Given a set of sequences and a user-specified min support threshold, the problem of sequential pattern mining is to find all the frequent subsequences, i.e., the subsequences whose occurrence frequency in the set of sequences is no less than min support .
 element in a sequence has only one item. The length of a sequence is then the number of elements in the sequence. We use the concept-relevant corpus as the input sequence set, regarding each sentence in the corpus as a sequence, and each word within as an element. So a sequence in our application is denoted as word 1 word 2 ...word l , and its length is the number of words in it. We use PrefixSpan [13] as our pattern mining algorithm.
 min length is widely used to constrain the minimum length of result sequences. In our work, we introduce a new parameter, min coverage , to restrict the re-sulting patterns. Min coverage is used to minimize the influence of a single knowledge domain (corresponding to a concept hierarchy) on the resulting pat-terns. Every knowledge domain has its own frequent vocabulary, hence tends to produce its own high frequency sequences. So we make sure that only the patterns that occur in no less than min coverage domains can be recorded as resulting patterns. After the frequent pattern mining phase, the sequential pat-terns that are longer than min length and occur no less than min support times in at least min coverage domains are recorded in the concept pattern set (CPS, or Concept Pattern Set). A detailed example is provided below.
 2, and min length is 2.
 3 and 4 all contain it, and its coverage is 3 since sequences 1, 3, and 4 belong to domains 1, 2, and 3 separately. Sub-sequence ce is not a resulting sequence since it fails to satisfy the constraint on support even though it satisfies the constraint on coverage. We call a frequent sequence that characterizes the syntactic features of a sentence including concepts a concept representative pattern. Yet not all the patterns in CPS bear such a quality. Some of them are common syntactic patterns that may appear in any natural language sentence. For example,  X  X his is for X  and  X  CD : P ronoun should be X  are both frequent sequences in natural language, but do not help much in finding concepts. So in the pattern refining phase, we first find out the frequent sequences in natural language. 3.1 Frequent Sequences in Natural Language To distinguish concept representative patterns from common frequent patterns in natural language, we adopt the following method. As we have mentioned in Section 2.1, based on the same concept hierarchies, we have constructed another corpus that is composed of sentences not containing concepts, viz. the concept-irrelevant corpus. With the same mining algorithm, we use the concept-irrelevant corpus as input, and get another resulting pattern set (NPS, or Non-concept Pattern Set).
 fewer than that in CPS. And the common part of them is the patterns that are common in natural language, and should be filtered from CPS. The difference of CPS and NPS is called the final pattern set (FPS, or Final Pattern Set). Through experiment we find that the precision of the concept discovery system based on FPS is 11% higher than that based on CPS. 3.2 Pattern Credit Evaluation Another obstacle to our goal is that even the most ideal pattern can sometimes find phrases that are not really concepts. A typical example would be the pat-tern  X  X re examples of CD : ChildConcept  X . It can find actual concepts like mathematical term  X  X roup X  and  X  X omology theory X , but at the same time it distinguishes phrases like  X  X his strategy X  and  X  X he more abstract statements X  as well. We consider this as the drawback of pattern-based methods to identify concepts. To reduce the negative effects, we introduce a pattern evaluation tech-nique to give credit to each pattern, and the credit will be used in the concept discovery process.
 in the pattern can be found in a sentence and the order of the words in it are identical to that of the pattern, the pattern is called a  X  X atching X  pattern of the sentence. The words in the sentence corresponding to the position of the concept related label are regarded as a concept. As we have discussed, almost all patterns make both right and wrong decisions. The proportion of times it makes a right decision to a wrong decision indicates the accuracy rate of the pattern.
 ing set is a set of concepts and a set of sentences containing at least one of the concepts. Two extra concept hierarchies about algebraic structures and group theory are used to generate queries and gather Web pages containing the con-cepts. The training sentence set is comprised of sentences in the retrieved Web pages. For each concept hierarchy, a corresponding glossary is obtained from several online math dictionaries. Training sentences that do not contain any of the entries in the glossaries are deleted. Entries that do not appear in any sen-tence are removed. The current training set includes 48,053 sentences and 306 corresponding concepts.
 decision for eac h-ap ositive decision by identifying a concept or a negative decision for regarding that the sentence contains no concept. For a positive decision, we compare the detected concept with the concept set. If it is found in the concept set, we consider it as a right decision, otherwise a wrong one. For negative decisions, we simply ignore them, because no single pattern can be versatile enough to discover any concept in any sentence. Thus we can get the accuracy rate of the pattern, which we define as the Credit of a pattern: pattern has made a positive decision that has an 80% probability to be right. In our context, this is equivalent to say a candidate concept is a real concept with an 80% probability. Also, if the Credit of a pattern is 0, we can conclude that the candidate concept discovered by the pattern is generally not a real concept. If the Credit is 0.5, it suggests that the pattern makes decisions quite randomly.
 In the concept discovery phase, the following steps are performed to discover the concept: it. The matched patterns will identify some phrases as candidate concepts. Then the probability for each phrase identified as a candidate concept is computed using the Credits obtained in the previous step. If the largest probability is larger than a threshold, the phrase with the largest probability is identified as a concept finally.
 patterns for it. Some of them may identify the same phrase as a candidate concept; others may identify different words. To decide whether the  X  X atching X  patterns have made a right decision and which phrase is the concept, a voting algorithm is designed as described in the following.
 { Consider a sentence S = { word 1 , word 2 , ..., word j , ..., word n } . Through match-ing each pattern with the sentence, some phrases are identified as candidate concepts. For each candidate concept, we can compute the probability that the candidate concept is really a concept according to the Credit of the patterns that identify the candidate concept. If a phrase, PH, is identified as a candidate concept by a set of patterns P X   X  FPS, then the probability that PH is a real concept is: where I(PH,P X ) denotes that PH is identified as a concept by all patterns in P X . Pr ( PH | I ( PH,P )) is the probability that PH is really a concept in condition that it is identified by all patterns in P X . Pr ( PH,I ( PH,P )) is the probability that PH is really a concept and identified by P X , which is to say that all patterns in P X  made a right decision. Since all patterns make decisions independently, Pr ( PH,I ( PH,P )) is equal with PH is identified by P X , meaning either PH is really a concept and all patterns in P X  made the right decision, or PH is not a real concept and all patterns in P X  made a mistake. Thus, Pr ( I ( PH,P )) equals to is a concept is larger than the probability that it is not a concept. For example, assume our pattern set has three patterns, { p a ,p b ,p c } , with the correspond-word 3 } is given. Word 1 is identified by p a and p b , and word 2 is identified by p c , as follows: 0 . 27 / (0 . 27+0 . 07)=0 . 8 all larger than 0.5, so both of them can be regarded as concepts. But compared with word 2 , word 1 has a larger possibility and is chosen as the final detected concept. In this section, we first evaluate a previously proposed pattern-based concept discovery method ( [3, 7, 14, 15]). Then we perform two experiments on our algo-rithm: the first experiment shows how the performance of the algorithm varies with the min coverage parameter of the frequent sequence mining process, and compares the performance of our approach with the previous pattern-based method; the second shows how performance varies with the size of the train-ing data. The test set is obtained in the same way as the training set and independent of the initial set for mining and the training set. It contains 133,980 sentences with 398 concepts corresponding to 2 concept hierarchies about atomic and molecular physics and computer architecture.
 information retrieval systems. Although recall is not a key issue in the Web context because of its richness in information, it is still an important factor in consideration for the efficiency of a system. In particular, the Web pages relevant to the user specified topic are retrieved by a search engine and collected by our system; then every Web page is parsed into a set of sentences. Finally, a set of automatically mined patterns are matched to each sentence to identify concepts within them. So if the patterns can achieve a higher recall, we can collect fewer Web pages with the highest relevance, and subsequently spend less time to parse the Web page set and get fewer sentences. This helps reduce the execution time significantly. tion identification patterns are considered to be suitable for Web pages [3, 7, 14, 15]: to discover concepts from sentences. On our test set, the precision of the method is 48.5%, recall is 32.5% and F1 value is 0.389. We can see that the precision and recall are all low even though the patterns seem to be reasonable. the pattern set used by the algorithm and the training set used to evaluate the patterns. And the former one depends on the parameters in the frequent sequence mining process, including min support , min length and min coverage . From our experiment, we find that min support and min length do not have a fixed relationship with performance. So, in the following, we conduct the experiments that show how the performance of the algorithm varies with the min coverage parameter as well as the size of the training data.
 Figure 1 shows how the performance of the algorithm varies with the coverage of the patterns. It can be seen from Table 2 that with the rising of min coverage , from 2 to 9, the number of patterns dramatically decreases from 112,000 to 5 correspondingly. From Figure 1, we can see that recall also decreases sharply, from 0.7 to 0.13. Contrasting with the sharp decrease in the pattern sum and recall value, the precision varies very little when the coverage is less than 8, but it increases notably after the minimum coverage gets beyond 8. This means that the more domain areas a pattern is suitable for, the higher its precision is. From Table 3, we can see that the optimum performance of our system is achieved when min coverage is set to 4. Compared with the performance of previous pat-tern based methods, the recall and precision are both improved in our approach when the pattern sets with a min coverage less than 8 are used. Precision is higher in all patterns sets, independent of the min coverage parameter. we conduct the experiment on the same test set using the pattern set with min coverage 4. Figure 2 plots the precision, recall and F1 values against the number of sentences for training, from 5 to 50,000. It can be seen that when the number of training sentences exceeds 50, the performance improves stably along with the increase in the training set size. If more than 5,000 sentences are used for training, however, little improvement in both recall and precision is gained by increasing the number of training samples further. The result shows that 5,000 sentences are sufficient to push the performance of our algorithm to its optimum.
 are generally noisier and have fewer salient concepts than journal papers and technical reports from a digital library. Also, the concepts in Web pages are usually presented in special styles such as within the anchor text of a hyperlink instead of a full sentence in natural language. All these factors decrease the performance because our algorithm does not do any preprocessing on Web pages, nor does it deal with any of the peculiar properties of HTML.
 sets and test sets. Patterns will be evaluated with different training sets, and tested with different test sets, and the average performance will be obtained. Such an experiment can reduce the effect of the training and test set on the evaluation results, and then provide a more accurate evaluation. In this paper, we have presented a pattern based approach to concept discovery for any given sentence. In the approach, the patterns are automatically mined from the Web, and a voting algorithm is presented to identify concepts from sentences on the basis of all the patterns X  decisions.
 For example, some other words such as adjective, adverb and a part of the sen-tence may appear in the result even though we have used a length parameter to constrain the length of the result. In our future work, we will conduct some refinement work on the result, aiming to identify the exact concept. And we will combine additional information such as TF*IDF and the importance of a sentence to improve the performance of the current approach. Another inter-esting work is that the pattern mining method can also be used to mine the sequences that characterize the syntactic patterns of sentences containing con-cept relationship descriptions. So the resulting patterns can be used to discover the relationship between concepts as well.
