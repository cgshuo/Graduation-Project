 In many respects, human language processing is the ultimate goldstandard for computational lin-guistics. Humans understand and generate lan-guage with amazing speed and accuracy, they are able to deal with ambiguity and noise effortlessly and can adapt to new speakers, domains, and reg-isters. Most surprisingly, they achieve this compe-tency on the basis of limited training data (Hart and Risley, 1995), using learning algorithms that are largely unsupervised.

Given the impressive performance of humans as language processors, it seems natural to turn to psycholinguistics, the discipline that studies hu-man language processing, as a source of informa-tion about the design of efficient language pro-cessing systems. Indeed, psycholinguists have un-covered an impressive array of relevant facts (re-viewed in Section 2), but computational linguists are often not aware of this literature, and results about human language processing rarely inform the design, implementation, or evaluation of artifi-cial language processing systems.

At the same time, research in psycholinguis-tics is often oblivious of work in computational linguistics (CL). To test their theories, psycholin-guists construct computational models of hu-man language processing, but these models of-ten fall short of the engineering standards that are generally accepted in the CL community (e.g., broad coverage, robustness, efficiency): typ-ical psycholinguistic models only deal with iso-lated phenomena and fail to scale to realistic data sets. A particular issue is evaluation, which is typ-ically anecdotal, performed on a small set of hand-crafted examples (see Sections 3).

In this paper, we propose a challenge that re-quires the combination of research efforts in com-putational linguistics and psycholinguistics: the development of cognitively plausible models of human language processing. This task can be de-composed into a modeling challenge (building models that instantiate known properties of hu-man language processing) and a data and evalu-ation challenge (accounting for experimental find-ings and evaluating against standardized data sets), which we will discuss in turn. 2.1 Key Properties The first part of the challenge is to develop a model that instantiates key properties of human language processing, as established by psycholinguistic ex-perimentation (see Table 1 for an overview and representative references). 1 A striking property of the human language processor is its efficiency and robustness. For the vast majority of sentences, it will effortlessly and rapidly deliver the correct analysis, even in the face of noise and ungrammat-icalities. There is considerable experimental evi-dence that shallow processing strategies are used to achieve this. The processor also achieves broad coverage: it can deal with a wide variety of syntac-tic constructions, and is not restricted by the do-main, register, or modality of the input.

Human language processing is also word-by-word incremental . There is strong evidence that a new word is integrated as soon as it is avail-able into the representation of the sentence thus far. Readers and listeners experience differential processing difficulty during this integration pro-cess, depending on the properties of the new word and its relationship to the preceding context. There is evidence that the processor instantiates a strict form of incrementality by building only fully con-nected trees. Furthermore, the processor is able to make predictions about upcoming material on the basis of sentence prefixes. For instance, listen-ers can predict an upcoming post-verbal element based on the semantics of the preceding verb. Or they can make syntactic predictions, e.g., if they encounter the word either , they predict an upcom-ing or and the type of complement that follows it.
Another key property of human language pro-cessing is the fact that it operates with limited memory, and that structures in memory are subject to decay and interference. In particular, the pro-cessor is known to incur a distance-based memory cost: combining the head of a phrase with its syn-tactic dependents is more difficult the more depen-dents have to be integrated and the further away they are. This integration process is also subject to interference from similar items that have to be held in memory at the same time. 2.2 Current Models The challenge is to develop a computational model that captures the key properties of human language processing outlined in the previous section. A number of relevant models have been developed, mostly based on probabilistic parsing techniques, but none of them instantiates all the key proper-ties discussed above (Table 1 gives an overview of model properties). 2
The earliest approaches were ranking-based models (Rank), which make psycholinguistic pre-dictions based on the ranking of the syntactic analyses produced by a probabilistic parser. Ju-rafsky (1996) assumes that processing difficulty is triggered if the correct analysis falls below a certain probability threshold (i.e., is pruned by the parser). Similarly, Crocker and Brants (2000) assume that processing difficulty ensures if the highest-ranked analysis changes from one word to the next. Both approaches have been shown to suc-cessfully model garden path effects. Being based on probabilistic parsing techniques, ranking-based models generally achieve a broad coverage, but their efficiency and robustness has not been evalu-ated. Also, they are not designed to capture syntac-tic prediction or memory effects (other than search with a narrow beam in Brants and Crocker 2000).
The ranking-based approach has been gener-alized by surprisal models (Surp), which pre-dict processing difficulty based on the change in the probability distribution over possible analy-ses from one word to the next (Hale, 2001; Levy, 2008; Demberg and Keller, 2008a; Ferrara Boston et al., 2008; Roark et al., 2009). These models have been successful in accounting for a range of experimental data, and they achieve broad cover-age. They also instantiate a limited form of predic-tion, viz., they build up expectations about the next word in the input. On the other hand, the efficiency and robustness of these models has largely not been evaluated, and memory costs are not mod-eled (again except for restrictions in beam size).
The prediction model (Pred) explicitly predicts syntactic structure for upcoming words (Demberg and Keller, 2008b, 2009), thus accounting for ex-perimental results on predictive language process-ing. It also implements a strict form of incre-mentality by building fully connected trees. Mem-ory costs are modeled directly as a distance-based penalty that is incurred when a prediction has to be verified later in the sentence. However, the current implementation of the prediction model is neither robust and efficient nor offers broad coverage.
Recently, a stack-based model (Stack) has been proposed that imposes explicit, cognitively mo-tivated memory constraints on the parser, in ef-fect limiting the stack size available to the parser (Schuler et al., 2010). This delivers robustness, ef-ficiency, and broad coverage, but does not model syntactic prediction. Unlike the other models dis-cussed here, no psycholinguistic evaluation has been conducted on the stack-based model, so its cognitive plausibility is preliminary. 2.3 Beyond Parsing There is strong evidence that human language pro-cessing is driven by an interaction of syntactic, se-mantic, and discourse processes (see Table 2 for an overview and references). Considerable exper-imental work has focused on the semantic prop-erties of the verb of the sentence, and verb sense, selectional restrictions, and thematic roles have all been shown to interact with syntactic ambiguity resolution. Another large body of research has elu-cidated the interaction of discourse processing and syntactic processing. The most-well known effect is probably that of referential context: syntactic ambiguities can be resolved if a discourse con-text is provided that makes one of the syntactic alternatives more plausible. For instance, in a con-text that provides two possible antecedents for a noun phrase, the processor will prefer attaching a PP or a relative clause such that it disambiguates between the two antecedents; garden paths are re-duced or disappear. Other results point to the im-portance of discourse coherence for sentence pro-cessing, an example being implicit causality.
The challenge facing researchers in compu-tational and psycholinguistics therefore includes the development of language processing models that combine syntactic processing with semantic and discourse processing. So far, this challenge is largely unmet: there are some examples of models that integrate semantic processes such as thematic role assignment into a parsing model (Narayanan and Jurafsky, 2002; Pad  X  o et al., 2009). However, other semantic factors are not accounted for by these models, and incorporating non-lexical as-pects of semantics into models of sentence pro-cessing is a challenge for ongoing research. Re-cently, Dubey (2010) has proposed an approach that combines a probabilistic parser with a model of co-reference and discourse inference based on probabilistic logic. An alternative approach has been taken by Pynte et al. (2008) and Mitchell et al. (2010), who combine a vector-space model of semantics (Landauer and Dumais, 1997) with a syntactic parser and show that this results in pre-dictions of processing difficulty that can be vali-dated against an eye-tracking corpus. 2.4 Acquisition and Crosslinguistics All models of human language processing dis-cussed so far rely on supervised training data. This raises another aspect of the modeling challenge: the human language processor is the product of an acquisition process that is largely unsupervised and has access to only limited training data: chil-dren aged 12 X 36 months are exposed to between 10 and 35 million words of input (Hart and Ris-ley, 1995). The challenge therefore is to develop a model of language acquisition that works with such small training sets, while also giving rise to a language processor that meets the key criteria in Table 1. The CL community is in a good posi-tion to rise to this challenge, given the significant progress in unsupervised parsing in recent years (starting from Klein and Manning 2002). How-ever, none of the existing unsupervised models has been evaluated against psycholinguistic data sets, and they are not designed to meet even basic psy-cholinguistic criteria such as incrementality.
A related modeling challenge is the develop-ment of processing models for languages other than English. There is a growing body of ex-perimental research investigating human language processing in other languages, but virtually all ex-isting psycholinguistic models only work for En-glish (the only exceptions we are aware of are Dubey et al. X  X  (2008) and Ferrara Boston et al. X  X  (2008) parsing models for German). Again, the CL community has made significant progress in crosslinguistic parsing, especially using depen-dency grammar (Haji  X  c, 2009), and psycholinguis-tic modeling could benefit from this in order to meet the challenge of developing crosslinguisti-cally valid models of human language processing. 3.1 Test Sets The second key challenge that needs to be ad-dressed in order to develop cognitively plausible models of human language processing concerns test data and model evaluation. Here, the state of the art in psycholinguistic modeling lags signif-icantly behind standards in the CL community. Most of the models discussed in Section 2 have not been evaluated rigorously. The authors typically describe their performance on a small set of hand-picked examples; no attempts are made to test on a range of items from the experimental literature and determine model fit directly against behavioral measures (e.g., reading times). This makes it very hard to obtain a realistic estimate of how well the models achieve their aim of capturing human lan-guage processing behavior.

We therefore suggest the development of stan-dard test sets for psycholinguistic modeling, simi-lar to what is commonplace for tasks in computa-tional linguistics: parsers are evaluated against the Penn Treebank, word sense disambiguation sys-tems against the SemEval data sets, co-reference systems against the Tipster or ACE corpora, etc. Two types of test data are required for psycholin-guistic modeling. The first type of test data con-sists of a collection of representative experimental results. This collection should contain the actual experimental materials (sentences or discourse fragments) used in the experiments, together with the behavioral measurements obtained (reading times, eye-movement records, rating judgments, etc.). The experiments included in this test set would be chosen to cover a wide range of ex-perimental phenomena, e.g., garden paths, syntac-tic complexity, memory effects, semantic and dis-course factors. Such a test set will enable the stan-dardized evaluation of psycholinguistic models by comparing the model predictions (rankings, sur-prisal values, memory costs, etc.) against behav-ioral measures on a large set of items. This way both the coverage of a model (how many phenom-ena can it account for) and its accuracy (how well does it fit the behavioral data) can be assessed.
Experimental test sets should be complemented by test sets based on corpus data. In order to as-sess the efficiency, robustness, and broad cover-age of a model, a corpus of unrestricted, naturally occurring text is required. The use of contextual-ized language data makes it possible to assess not only syntactic models, but also models that capture discourse effects. These corpora need to be anno-tated with behavioral measures, e.g., eye-tracking or reading time data. Some relevant corpora have already been constructed, see the overview in Ta-ble 3, and various authors have used them for model evaluation (Demberg and Keller, 2008a; Pynte et al., 2008; Frank, 2009; Ferrara Boston et al., 2008; Patil et al., 2009; Roark et al., 2009; Mitchell et al., 2010).

However, the usefulness of the psycholinguis-tic corpora in Table 3 is restricted by the absence of gold-standard linguistic annotation (though the French part of the Dundee corpus, which is syn-tactically annotated). This makes it difficult to test the accuracy of the linguistic structures computed by a model, and restricts evaluation to behavioral predictions. The challenge is therefore to collect a standardized test set of naturally occurring text or speech enriched not only with behavioral vari-ables, but also with syntactic and semantic anno-tation. Such a data set could for example be con-structed by eye-tracking section 23 of the Penn Treebank (which is also part of Propbank, and thus has both syntactic and thematic role annotation).
In computational linguistics, the development of new data sets is often stimulated by competi-tions in which systems are compared on a stan-dardized task, using a data set specifically de-signed for the competition. Examples include the CoNLL shared task, SemEval, or TREC in com-putational syntax, semantics, and discourse, re-spectively. A similar competition could be devel-oped for computational psycholinguistics  X  maybe along the lines of the model comparison chal-lenges that held at the International Conference on Cognitive Modeling. These challenges provide standardized task descriptions and data sets; par-ticipants can enter their cognitive models, which were then compared using a pre-defined evalua-tion metric. 3 3.2 Behavioral and Neural Data As outlined in the previous section, a number of authors have evaluated psycholinguistic models against eye-tracking or reading time corpora. Part of the data and evaluation challenge is to extend this evaluation to neural data as provided by event-related potential (ERP) or brain imaging studies (e.g., using functional magnetic resonance imag-ing, fMRI). Neural data sets are considerably more complex than behavioral ones, and modeling them is an important new task that the community is only beginning to address. Some recent work has evaluated models of word semantics against ERP (Murphy et al., 2009) or fMRI data (Mitchell et al., 2008). 4 This is a very promising direction, and the challenge is to extend this approach to the sentence and discourse level (see Bachrach 2008). Again, it will again be necessary to develop standardized test sets of both experimental data and corpus data. 3.3 Evaluation Measures We also anticipate that the availability of new test data sets will facilitate the development of new evaluation measures that specifically test the va-lidity of psycholinguistic models. Established CL evaluation measures such as Parseval are of lim-ited use, as they can only test the linguistic, but not the behavioral or neural predictions of a model.
So far, many authors have relied on qualita-tive evaluation: if a model predicts a difference in (for instance) reading time between two types of sentences where such a difference was also found experimentally, then that counts as a suc-cessful test. In most cases, no quantitative evalu-ation is performed, as this would require model-ing the reading times for individual item and in-dividual participants. Suitable procedures for per-forming such tests do not currently exist; linear mixed effects models (Baayen et al., 2008) pro-vide a way of dealing with item and participant variation, but crucially do not enable direct com-parisons between models in terms of goodness of fit.
Further issues arise from the fact that we of-ten want to compare model fit for multiple experi-ments (ideally without reparametrizing the mod-els), and that various mutually dependent mea-sures are used for evaluation, e.g., processing ef-fort at the sentence, word, and character level. An important open challenge is there to develop eval-uation measures and associated statistical proce-dures that can deal with these problems. In this paper, we discussed the modeling and data/evaluation challenges involved in developing cognitively plausible models of human language processing. Developing computational models is of scientific importance in so far as models are im-plemented theories: models of language process-ing allow us to test scientific hypothesis about the cognitive processes that underpin language pro-cessing. This type of precise, formalized hypoth-esis testing is only possible if standardized data sets and uniform evaluation procedures are avail-able, as outlined in the present paper. Ultimately, this approach enables qualitative and quantitative comparisons between theories, and thus enhances our understanding of a key aspect of human cog-nition, language processing.

There is also an applied side to the proposed challenge. Once computational models of human language processing are available, they can be used to predict the difficulty that humans experi-ence when processing text or speech. This is use-ful for a number applications: for instance, nat-ural language generation would benefit from be-ing able to assess whether machine-generated text or speech is easy to process. For text simplifica-tion (e.g., for children or impaired readers), such a model is even more essential. It could also be used to assess the readability of text, which is of interest in educational applications (e.g., essay scoring). In machine translation, evaluating the fluency of sys-tem output is crucial, and a model that predicts processing difficulty could be used for this, or to guide the choice between alternative translations, and maybe even to inform human post-editing. Altmann, Gerry T. M. and Mark J. Steedman. 1988. Interaction with context during human sentence processing. Cognition 30(3):191 X 238. Baayen, R. H., D. J. Davidson, and D. M. Bates. 2008. Mixed-effects modeling with crossed ran-dom effects for subjects and items. Journal of Memory and Language to appear.
 Bachrach, Asaf. 2008. Imaging Neural Correlates of Syntactic Complexity in a Naturalistic Con-text . Ph.D. thesis, Massachusetts Institute of Technology, Cambridge, MA.
 Brants, Thorsten and Matthew W. Crocker. 2000.
Probabilistic parsing and psychological plau-sibility. In Proceedings of the 18th Interna-tional Conference on Computational Linguis-tics . Saarbr  X  ucken/Luxembourg/Nancy, pages 111 X 117.
 Crocker, Matthew W. and Thorsten Brants. 2000.
Wide-coverage probabilistic sentence process-ing. Journal of Psycholinguistic Research 29(6):647 X 669.
 Demberg, Vera and Frank Keller. 2008a. Data from eye-tracking corpora as evidence for theo-ries of syntactic processing complexity. Cogni-tion 101(2):193 X 210.
 Demberg, Vera and Frank Keller. 2008b. A psy-cholinguistically motivated version of TAG. In
Proceedings of the 9th International Workshop on Tree Adjoining Grammars and Related For-malisms . T  X  ubingen, pages 25 X 32.
 Demberg, Vera and Frank Keller. 2009. A com-putational model of prediction in human pars-ing: Unifying locality and surprisal effects. In Niels Taatgen and Hedderik van Rijn, editors,
Proceedings of the 31st Annual Conference of the Cognitive Science Society . Cognitive Sci-ence Society, Amsterdam, pages 1888 X 1893.
 Dubey, Amit. 2010. The influence of discourse on syntax: A psycholinguistic model of sentence processing. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics . Uppsala.
 Dubey, Amit, Frank Keller, and Patrick Sturt. 2008. A probabilistic corpus-based model of syntactic parallelism. Cognition 109(3):326 X  344.
 Ferrara Boston, Marisa, John Hale, Reinhold
Kliegl, Umesh Patil, and Shravan Vasishth. 2008. Parsing costs as predictors of reading dif-ficulty: An evaluation using the Potsdam Sen-tence Corpus. Journal of Eye Movement Re-search 2(1):1 X 12.
 Ferreira, Fernanda, Kiel Christianson, and An-drew Hollingworth. 2001. Misinterpretations of garden-path sentences: Implications for models of sentence processing and reanalysis. Journal of Psycholinguistic Research 30(1):3 X 20.
 Frank, Stefan L. 2009. Surprisal-based compar-ison between a symbolic and a connectionist model of sentence processing. In Niels Taat-gen and Hedderik van Rijn, editors, Proceed-ings of the 31st Annual Conference of the Cog-nitive Science Society . Cognitive Science Soci-ety, Amsterdam, pages 1139 X 1144.
 Garnsey, Susan M., Neal J. Pearlmutter, Elisa-beth M. Myers, and Melanie A. Lotocky. 1997.
The contributions of verb bias and plausibility to the comprehension of temporarily ambiguous sentences. Journal of Memory and Language 37(1):58 X 93.
 Gibson, Edward. 1998. Linguistic complexity: locality of syntactic dependencies. Cognition 68:1 X 76.
 Grodner, Dan and Edward Gibson. 2005. Conse-quences of the serial nature of linguistic input. Cognitive Science 29:261 X 291.
 Haji  X  c, Jan, editor. 2009. Proceedings of the 13th
Conference on Computational Natural Lan-guage Learning: Shared Task . Association for Computational Linguistics, Boulder, CO.
 Hale, John. 2001. A probabilistic Earley parser as a psycholinguistic model. In Proceedings of the 2nd Conference of the North American Chapter of the Association for Computational Linguis-tics . Association for Computational Linguistics, Pittsburgh, PA, volume 2, pages 159 X 166.
 Hart, Betty and Todd R. Risley. 1995. Meaning-ful Differences in the Everyday Experience of Young American Children . Paul H. Brookes, Baltimore, MD.
 Jurafsky, Daniel. 1996. A probabilistic model of lexical and syntactic access and disambigua-tion. Cognitive Science 20(2):137 X 194.
 Kamide, Yuki, Gerry T. M. Altmann, and Sarah L.
Haywood. 2003. The time-course of prediction in incremental sentence processing: Evidence from anticipatory eye movements. Journal of Memory and Language 49:133 X 156.
 Kehler, Andrew, Laura Kertz, Hannah Rohde, and
Jeffrey L. Elman. 2008. Coherence and coref-erence revisited. Journal of Semantics 25(1):1 X  44.
 Kennedy, Alan and Joel Pynte. 2005. Parafoveal-on-foveal effects in normal reading. Vision Re-search 45:153 X 168.
 Klein, Dan and Christopher Manning. 2002. A generative constituent-context model for im-proved grammar induction. In Proceedings of the 40th Annual Meeting of the Association for
Computational Linguistics . Philadelphia, pages 128 X 135.
 Kliegl, Reinhold, Antje Nuthmann, and Ralf Eng-bert. 2006. Tracking the mind during reading:
The influence of past, present, and future words on fixation durations. Journal of Experimental Psychology: General 135(1):12 X 35.
 Landauer, Thomas K. and Susan T. Dumais. 1997.
A solution to Plato X  X  problem: The latent se-mantic analysis theory of acquisition, induction and representation of knowledge. Psychologi-cal Review 104(2):211 X 240.
 Levy, Roger. 2008. Expectation-based syntactic comprehension. Cognition 106(3):1126 X 1177. McRae, Ken, Michael J. Spivey-Knowlton, and
Michael K. Tanenhaus. 1998. Modeling the in-fluence of thematic fit (and other constraints) in on-line sentence comprehension. Journal of Memory and Language 38(3):283 X 312.
 Mitchell, Jeff, Mirella Lapata, Vera Demberg, and
Frank Keller. 2010. Syntactic and semantic fac-tors in processing difficulty: An integrated mea-sure. In Proceedings of the 48th Annual Meet-ing of the Association for Computational Lin-guistics . Uppsala.
 Mitchell, Tom M., Svetlana V. Shinkareva, An-drew Carlson, Kai-Min Chang, Vicente L.
 Malave, Robert A. Mason, and Marcel Adam
Just3. 2008. Predicting human brain activity as-sociated with the meanings of nouns. Science 320(5880):1191 X 1195.
 Murphy, Brian, Marco Baroni, and Massimo Poe-sio. 2009. EEG responds to conceptual stimuli and corpus semantics. In Proceedings of the Conference on Empirical Methods in Natural
Language Processing . Singapore, pages 619 X  627.
 Narayanan, Srini and Daniel Jurafsky. 2002. A
Bayesian model predicts human parse prefer-ence and reading time in sentence processing. In Thomas G. Dietterich, Sue Becker, and Zoubin
Ghahramani, editors, Advances in Neural In-formation Processing Systems 14 . MIT Press, Cambridge, MA, pages 59 X 65.
 Pad  X  o, Ulrike, Matthew W. Crocker, and Frank
Keller. 2009. A probabilistic model of semantic plausibility in sentence processing. Cognitive Science 33(5):794 X 838.
 Patil, Umesh, Shravan Vasishth, and Reinhold
Kliegl. 2009. Compound effect of probabilis-tic disambiguation and memory retrievals on sentence processing: Evidence from an eye-tracking corpus. In A. Howes, D. Peebles, and R. Cooper, editors, Proceedings of 9th In-ternational Conference on Cognitive Modeling . Manchester.
 Pickering, Martin J. and Martin J. Traxler. 1998.
Plausibility and recovery from garden paths: An eye-tracking study. Journal of Experimental
Psychology: Learning Memory and Cognition 24(4):940 X 961.
 Pickering, Martin J., Matthew J. Traxler, and
Matthew W. Crocker. 2000. Ambiguity reso-lution in sentence processing: Evidence against frequency-based accounts. Journal of Memory and Language 43(3):447 X 475.
 Pynte, Joel, Boris New, and Alan Kennedy. 2008.
On-line contextual influences during reading normal text: A multiple-regression analysis. Vi-sion Research 48(21):2172 X 2183.
 Roark, Brian, Asaf Bachrach, Carlos Cardenas, and Christophe Pallier. 2009. Deriving lex-ical and syntactic expectation-based measures for psycholinguistic modeling via incremental top-down parsing. In Proceedings of the Con-ference on Empirical Methods in Natural Lan-guage Processing . Singapore, pages 324 X 333. Roland, Douglas and Daniel Jurafsky. 2002. Verb sense and verb subcategorization probabilities. In Paola Merlo and Suzanne Stevenson, editors,
The Lexical Basis of Sentence Processing: For-mal, Computational, and Experimental Issues , John Bejamins, Amsterdam, pages 325 X 346.
 Sanford, Anthony J. and Patrick Sturt. 2002.
Depth of processing in language comprehen-sion: Not noticing the evidence. Trends in Cog-nitive Sciences 6:382 X 386.
 Schuler, William, Samir AbdelRahman, Tim
Miller, and Lane Schwartz. 2010. Broad-coverage parsing using human-like mem-ory constraints. Computational Linguistics 26(1):1 X 30.
 Staub, Adrian and Charles Clifton. 2006. Syntac-tic prediction in language comprehension: Evi-dence from either . . . or. Journal of Experimen-tal Psychology: Learning, Memory, and Cogni-tion 32:425 X 436.
 Stewart, Andrew J., Martin J. Pickering, and An-thony J. Sanford. 2000. The time course of the influence of implicit causality information: Fo-cusing versus integration accounts. Journal of Memory and Language 42(3):423 X 443.
 Sturt, Patrick and Vincenzo Lombardo. 2005.
Processing coordinated structures: Incremen-tality and connectedness. Cognitive Science 29(2):291 X 305.
 Tanenhaus, Michael K., Michael J. Spivey-Knowlton, Kathleen M. Eberhard, and Julie C.
Sedivy. 1995. Integration of visual and linguis-tic information in spoken language comprehen-sion. Science 268:1632 X 1634.
 Vasishth, Shravan and Richard L. Lewis. 2006.
Argument-head distance and processing com-plexity: Explaining both locality and antilocal-ity effects. Language 82(4):767 X 794.
