 Although distributional models (Turney and Pan-tel, 2010; Clark, 2015) have proved useful for a variety of NLP tasks, the fact that the meaning of a word is represented as a distribution over other words implies that they suffer from the ground-ing problem (Harnad, 1990); i.e. they do not ac-count for the fact that human semantic knowledge is grounded in the perceptual system (Louwerse, 2008). Motivated by human concept acquisition, multi-modal semantics enhances linguistic repre-sentations with extra-linguistic perceptual input. These models outperform language-only models on a range of tasks, including modelling semantic similarity and relatedness, and predicting compo-sitionality (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013; Bruni et al., 2014). Al-though feature norms have also been used, raw image data has become the de-facto perceptual modality in multi-modal models.

However, if the objective is to ground seman-tic representations in perceptual information, why stop at image data? The meaning of violin is surely not only grounded in its visual properties, such as shape, color and texture, but also in its sound, pitch and timbre. To understand how per-ceptual input leads to conceptual representation, we should use as many perceptual modalities as possible. A recent preliminary study by Lopopolo and van Miltenburg (2015) found that it is possible to derive uni-modal semantic representations from sound data. Here, we explore taking multi-modal semantics beyond its current reliance on image data and experiment with grounding semantic rep-resentations in the auditory perceptual modality.
Multi-modal models that rely on raw image data have typically used  X  X ag of visual words X  (BoVW) representations (Sivic and Zisserman, 2003). We follow a similar approach for the auditory modality and construct bag of audio words (BoAW) representations. Following pre-vious work in multi-modal semantics, we evalu-ate these models on measuring conceptual simi-larity and relatedness, and inducing cross-modal mappings between modalities to perform zero-shot learning. In addition, we evaluate on an unsupervised musical instrument clustering task. Our findings indicate that multi-modal representa-tions enriched with auditory information perform well on relatedness and similarity tasks, particu-larly on words that have auditory assocations. To our knowledge, this is the first work to combine linguistic and auditory representations in multi-modal semantics. Information processing in the brain can be roughly described to occur on three levels: perceptual in-put, conceptual representation and symbolic rea-soning (Gazzaniga, 1995). While research in AI has made great progress in understanding the first and last of these, understanding the middle level is still more of an open problem: how is it that per-ceptual input leads to conceptual representations that can be processed and reasoned with?
A key observation is that concepts are, through perception, grounded in physical reality and sen-sorimotor experience (Harnad, 1990; Louwerse, 2008), and there has been a surge of recent work on perceptually grounded semantic models that try to account for this fact. These models learn se-mantic representations from both textual and per-ceptual input, using either feature norms (Silberer and Lapata, 2012; Roller and Schulte im Walde, 2013; Hill and Korhonen, 2014) or raw image data (Feng and Lapata, 2010; Leong and Mihal-cea, 2011; Bruni et al., 2014) as the source of per-ceptual information. A popular approach in the latter case is to collect images associated with a concept, and then lay out each image as a set of keypoints on a dense grid, where each keypoint is represented by a robust local feature descriptor such as SIFT (Lowe, 2004). These local descrip-tors are subsequently clustered into a set of  X  X i-sual words X  using a standard clustering algorithm such as k-means and then quantized into vector representations by comparing the descriptors with the centroids. An alternative to this bag of vi-sual words (BoVW) approach is transferring fea-tures from convolutional neural networks (Kiela and Bottou, 2014).

Various ways of aggregating images into visual representations have been proposed, such as tak-ing the mean or the elementwise maximum. Ide-ally, one would jointly learn multi-modal repre-sentations from parallel multi-modal data, such as text containing images (Silberer and Lapata, 2014) or images described with speech (Synnaeve et al., 2014), but such data is hard to obtain, has limited coverage and can be noisy. Hence, image repre-sentations are often learned independently. Ag-gregated visual representations are subsequently combined with a traditional linguistic space to form a multi-modal model. This mixing can be done in a variety of ways, ranging from simple concatenation to more sophisticated fusion meth-ods (Bruni et al., 2014).

Cross-modal semantics, instead of being con-cerned with improving semantic representations through grounding, focuses on the problem of ref-erence. Using, for instance, mappings between visual and textual space, the objective is to learn which words refer to which objects (Lazaridou et al., 2014). This problem is very much re-Table 1: Examples of pairs in the datasets where auditory is relevant, with the similarity score. lated to the object recognition task in computer vision, but instead of using just visual data and labels, these cross-modal models also utilize tex-tual information (Socher et al., 2014; Frome et al., 2013). This allows for zero-shot learning , where the model can predict how an object relates to other concepts just from seeing an image of the object, but without ever having previously encoun-tered an image of that particular object (Lazaridou et al., 2014). Multi-modal and cross-modal ap-proaches have outperformed state-of-the-art text-based methods on a variety of tasks (Bruni et al., 2014; Silberer and Lapata, 2014). Following previous work in multi-modal seman-tics, we evaluate on two standard similarity and re-latedness datasets: SimLex-999 (Hill et al., 2014) and the MEN test collection (Bruni et al., 2014). These datasets consist of concept pairs together with a human-annotated similarity or relatedness score, where the former dataset focuses on gen-uine similarity (e.g., teacher-instructor ) and the latter focuses more on relatedness (e.g., river-water ). In addition, following previous work in cross-modal semantics, we evaluate on the zero-shot learning task of inducing a cross-modal map-ping to the correct label in the auditory modality from the linguistic one and vice-versa. 3.1 Multi-modal Semantics Evidence suggests that the inclusion of visual rep-resentations only improves performance for cer-tain concepts, and that in some cases the introduc-tion of visual information is detrimental to perfor-mance on similarity and relatedness tasks (Kiela et al., 2014). The same is likely to be true for other perceptual modalities: in the case of com-parisons such as guitar-piano , the auditory modal-Table 2: Number of concept pairs for which repre-sentations are available in each modality. ity is certainly meaningful, whereas in the case of democracy-anarchism it is probably less so. Therefore, we had two graduate students annotate the datasets according to whether auditory percep-tion is relevant to the pairwise comparison. The annotation criterion was as follows: if both con-cepts in a pairwise comparison have a distinctive associated sound, the modality is deemed rele-vant. Inter-annotator agreement was high:  X  = 0 . 93 for MEN and  X  = 0 . 92 for SimLex-999. Some examples of relevant pairs can be found in Table 1. Hence, we now have four evaluation datasets: the MEN test collection MEN and its auditory-relevant subset AMEN ; and the SimLex-999 dataset SLex and its auditory-relevant sub-set ASLex . Due to the nature of the auditory data sources, it is not possible to build auditory representations for all concepts in the test sets. Hence, unless stated otherwise, we report results for the covered subsets (using the same subsets when comparing across modalities, to ensure a fair comparison). Table 2 shows how much of the test 3.2 Cross-modal Semantics In addition to evaluating our models on the MEN and SimLex tasks, we evaluate on the cross-modal task of zero-shot learning. In the case of vision, Lazaridou et al. (2014) studied the possibility of predicting from  X  X e found a cute, hairy wampimuk sleeping behind the tree X  that a  X  X ampimuk X  will probably look like a small furry animal, even though a wampimuk has never been seen before. We evaluate zero-shot learning, using partial least squares regression (PLSR) to obtain cross-modal mappings from the linguistic to audi-representation for e.g. guitar , the task is to map it to the appropriate place in auditory space without ever having heard a guitar; or map it to the appro-priate place in linguistic space without ever having read about a guitar (having only heard it). One reason for using raw image data in multi-modal models is that there is a wide variety of re-sources that contain tagged images, such as Im-ageNet (Deng et al., 2009) and the ESP Game dataset (Von Ahn and Dabbish, 2004). However, such resources do not exist for audio files, and so we follow a similar approach to Fergus et al. (2005) and Bergsma and Goebel (2011), who use Google Images to obtain images. We use the files. Freesound is a collaborative database re-leased under Creative Commons licenses, in the form of snippets, samples and recordings, that is aimed at sound artists. The Freesound API allows users to easily search for audio files that have been tagged using certain keywords.

For each of the concepts in the evaluation datasets, we used the Freesound API to obtain samples encoded in the standard open source OGG numbers of files, with varying duration per indi-vidual file, we restrict the search to a maximum of 50 files and a maximum of 1 minute duration per file. The Freesound API allows for various degrees of keyword matching: we opted for the strictest keyword matching, in that the audio file needs to have been purposely tagged with the given word (the alternative includes searching the text descrip-tion for matching keywords). For example, if we are searching for audio files of cars, we retrieve up to 50 files with a maximum duration of 1 minute per file that have been tagged with the label  X  X ar X . 4.1 Linguistic Representations For the linguistic representations we use the con-tinuous vector representations from the log-linear skip-gram model of Mikolov et al. (2013). Specifi-cally, we trained 300-dimensional vector represen-tations trained on a dump of the English Wikipedia types of representations have been found to yield the highest performance on a variety of semantic similarity tasks (Baroni et al., 2014). 4.2 Auditory Representations A common approach to obtaining acoustic fea-tures of audio files is the Mel-scale Frequency Cepstral Coefficient (MFCC) (O X  X haughnessy, 1987). MFCC features are abundant in a wide variety of applications in audio signal process-ing, ranging from audio information retrieval, to speech and speaker recognition, and music analy-sis (Eronen, 2003). Such features are derived from the mel-frequency cepstrum representation of an audio fragment (Stevens et al., 1937). In MFCC, frequency bands are spaced along the mel scale, which has the advantage that it approximates hu-man auditory perception more closely than e.g. linearly-spaced frequency bands. Hence, MFCC takes human perceptual sensitivity to audio fre-quencies into consideration, which makes it suit-able for e.g. compression and recognition tasks, but also for our current objective of modelling au-ditory perception. We obtain MFCC descriptors for frames of audio files using librosa, a popu-lar library for audio and music analysis written tors, we cluster them using mini-batch k -means (Sculley, 2010) and quantize the descriptors into a  X  X ag of audio words X  (BoAW) (Foote, 1997) rep-resentation by comparing the MFCC descriptors to the cluster centroids. This gives us BoAW rep-resentations for each of the audio files. Auditory representations are obtained by taking the mean of the BoAW representations of the relevant au-dio files, and finally weighting them using positive point-wise mutual information (PPMI), a standard weighting scheme for improving vector represen-tation quality (Bullinaria and Levy, 2007). We set k = 300 , which equals the number of dimensions for the linguistic representations. 4.3 Multi-modal Fusion Strategies Since multi-modal semantics relies on two or more modalities, there are several ways of combining or fusing linguistic and perceptual cues (Bruni et al., 2014). When computing similarity scores, for instance, we can either jointly learn the represen-tations; learn them independently, combine (e.g. concatenate) them and compute similarity scores; or learn them independently, compute similarity scores independently and combine the scores. We call these possibilities early , middle and late fu-sion, respectively, and evaluate multi-modal mod-els in each category. 4.3.1 Early Fusion A good example of early fusion is the recently introduced multi-modal skip-gram model (Lazari-dou et al., 2015). This model behaves like a nor-mal skip-gram, but instead of only having a train-ing objective for the linguistic representation, it in-cludes an additional training objective for the vi-sual context, which consists of an aggregated rep-resentation of images associated with the given target word. The skip-gram training objective for a sequence of training words w 1 ,w 2 ,...,w T and a context size c is: where J  X  is the log-likelihood P obtained via the softmax: where u w and v w are the context and target vec-tor representations for the word w respectively, and W is the vocabulary size. The objective for the multi-modal skip-gram has an additional vi-sual objective J vis (in this case a margin criterion):
Here, we take a similar but more straightfor-ward approach by making the auditory context a part of the initial training objective, which is pos-sible because linguistic and auditory representa-tions have the same dimensionality. That is, we modified word2vec to predict additional auditory contexts that have been set to the corresponding BoAW representation. We jointly learn linguistic and audio representations by taking the aggregated mean  X  a w of the auditory vectors for a given word w , and adding this mean vector to the context:
The intuition is that the induced vector for the target word now has to predict an auditory vec-tor as part of its context, as well as the linguis-tic ones. As an alternative, we also investigate re-tained by uniformly sampling from the set of au-ditory representations for the target word. We re-fer to these two alternatives as MMSG -MEAN and MMSG -SAMPLED , respectively. For this model, auditory BoAW representations are built for the ten thousand most frequent words in our corpus, based on 10 audio files retrieved from FreeSound for each word (or fewer when 10 are not available). 4.3.2 Middle and Late Fusion Whereas early fusion requires a joint training ob-jective that takes into account both modalities, middle fusion allows for individual training ob-jectives and independent training data. Similarity between two multi-modal representations is calcu-lated as follows: where g is some similarity function, u l and v l are linguistic representations, and u a and v a are the auditory representations. A typical formulation in multi-modal semantics for f ( x,y ) is  X x k (1  X   X  ) y , where k is concatenation (see e.g. Bruni et al. (2014) and Kiela and Bottou (2014)).

Late fusion can be seen as the converse of mid-dle fusion, in that the similarity function is com-puted first before the similarity scores are com-bined: where g is some similarity function and h is a way of combining similarities, in our case a weighted average: h ( x,y ) = 1 larity is the normalized dot-product, and the uni-modal representations are themselves normalized, middle and late fusion are equivalent if  X  = 0 . 5 , which we call MM . However, when  X  6 = 0 . 5 , we distinguish between the two models, calling them 5.1 Conceptual Similarity and Relatedness We evaluate performance by calculating the Spear-man  X  s correlation between the ranking of the concept pairs produced by the automatic similar-ity metric (cosine between the derived vectors) and that produced by the gold-standard similarity scores. To ensure a fair comparison, we evaluate Table 3: Spearman  X  s correlation comparison of uni-modal and multi-modal representations. The MMSG models perform early fusion, MM repre-sents middle and late fusion with  X  = 0 . 5 (see Section 4.3.2). on the common subsets for which there are repre-sentations in both modalities (see Table 2).
The results are reported in Table 3. We find that, while performance decreases for linguistic representations on the auditory-relevant subsets of the two datasets, performance increases for the uni-modal auditory representations on those sub-sets. This indicates that our auditory representa-tions are better at judging auditory-relevant com-parisons than they are at non-auditory ones, as we might expect.

For all datasets, the accuracy scores for multi-modal models are at least as high as those for the purely linguistic representations. In the case of the full datasets this difference is only marginal, which is to be expected given how few of the words in the datasets are auditory-relevant. How-ever, the results indicate that adding auditory in-put even for words that are not directly auditory-relevant is not detrimental to overall performance.
In the case of the auditory-relevant subsets, we see a large increase in performance when using multi-modal representations. It is also interesting that this performance increase is found in the sim-ple MM model, compared to the more complicated MMSG models, which seems to indicate that the latter models are still too reliant on linguistic in-formation, which harms their performance when performing auditory-specific comparisons. The model which performs consistently well across the four datasets is MM , the middle-late fusion model with  X  = 0 . 5 . 5.2 Cross-modal Zero-shot Learning We learn a cross-modal mapping between the linguistic and auditory spaces using partial least squares regression, taking out each concept, train-ing on the others, and then projecting from one Table 4: Cross-modal zero-shot learning accuracy. space into the other. Zero-shot performance is evaluated using the average percentage correct at N (P@ N ), which measures how many of the test instances were ranked within the top N highest ranked nearest neighbors. Results are shown in Table 4, with the chance baseline obtained by ran-domly ranking a concept X  X  nearest neighbors. In-sofar as it is possible to make a direct comparison with linguistic-visual zero-shot learning (which uses entirely different data), it appears that the cur-rent task may be more difficult: Lazaridou et al. (2014) report a P@1 of 2 . 4 and P@20 of 33 . 0 for their linguistic-visual model. 5.3 Qualitative Analysis We also performed a small qualitative analysis of the BoAW representations for the words in MEN and SLex. As Table 5 shows, the nearest neighbors are remarkably semantically coherent. For exam-ple, the model groups together sounds produced by machines, or by water. It even finds that dinner, meal, lunch and breakfast are closely related. In contrast, nearest neighbors for the linguistic model tend to be of a more abstract nature: where we find mouth and throat as auditory neighbors for lan-guage , the linguistic model gives us concepts like word and dictionary ; while auditory gossip sounds like maids and is something you might do in the corridor , it is linguistically associated with more abstract concepts like news and newspaper . There are many parameters that were left fixed in the main results that could have been adjusted to improve performance, particularly in the middle and late fusion models. It is useful to investigate some of the parameters that are likely to have an impact on performance: what the effect of the  X  mixing parameter is, whether a different k would have yielded better auditory representations, and whether the number and duration of the audio files from FreeSound has any effect.
 Figure 1: Performance of middle and late multi-modal fusion models compared to linguistic rep-resentations on the four datasets when varying the  X  mixing parameter on the x-axis. 6.1 Mixing with  X  The mixing parameter  X  plays an important role in the middle and late fusion models. We kept it fixed at 0 . 5 for the MM model above, but here we experiment with varying the parameter, yield-ing results for two different models, MM -MIDDLE and MM -LATE . The results are shown in Figure 1, where moving to the right on the x-axis uses more linguistic input and moving to the left uses more auditory input. The late model consistently out-performs the middle fusion model, which is prob-ably because it is less susceptible to any noise in the auditory representation. Optimal performance seems to be around  X  = 0 . 6 for both fusion strate-gies on all four datasets, indicating that it is bet-ter to include a little more linguistic than auditory input. It appears that any 0 . 5  X   X  &lt; 1 (i.e., where we have more linguistic input but still some auditory signal), outperforms the purely linguis-tic representation, substantially in the case of the auditory-relevant subsets. 6.2 Number of Auditory Dimensions We experimented with different values for the number of audio words k (i.e. the number of clus-ters in the k-means clustering that determines the number of  X  X udio words X ). As Figure 2 shows, the quality of the uni-modal auditory representations is highly robust to the number of dimensions. In fact, any choice of k in the range shown provides similar results across the datasets. Figure 2: Performance of uni-modal auditory rep-resentations on the four datasets when varying the k parameter. 6.3 Number and Duration of Audio Files We experimented with the number of audio files by querying FreeSound for up to 100 audio files per search word, while keeping k = 300 . The results are shown in Figure 3. It appears that  X  X he more the better X , although performance does not increase significantly after around 40 audio files.
In order to examine the effect of audio file du-ration, we experimented with specifying the du-ration of audio files when querying the database, either taking very short (up to 5 seconds), medium length (up to 1 minute) or files of any duration. The results can be found in Figure 4, showing that performance generally increases as the files get longer (except on AMEN where a duration of 1 minute provides optimal performance). To strengthen the finding that multi-modal repre-sentations perform well on the auditory-relevant subsets of the datasets, we evaluate on an alto-gether different task, namely that of musical in-Figure 3: Performance of uni-modal auditory rep-resentations on the four datasets when varying the number of audio files per target word. strument classification. We used Wikipedia to col-lect a total of 52 instruments and divided them into 5 classes: brass, percussion, piano-based, string and woodwind instruments. For each of the in-struments, we collected as many audio files from FreeSound as possible, and used the MM -MIDDLE model with parameter settings that yielded good results in the previous experiments ( k = 300 and  X  = 0 . 6 ). We then performed k-means cluster-ing with five cluster centroids and compared re-sults between auditory, linguistic and multi-modal, evaluating the clustering quality using the standard V-measure clustering evaluation metric (Rosen-berg and Hirschberg, 2007).

This is an interesting problem because instru-ment classes are determined somewhat by conven-Figure 4: Performance of uni-modal auditory rep-resentations on the four datasets when varying the maximum duration. tion (is a saxophone a brass or a woodwind in-strument?). What is more, how instruments ac-tually sound is rarely described in detail in text, so corpus-based linguistic representations cannot take this information into account. The results are in Table 6, clearly showing that the multi-modal representation which utilizes both linguistic infor-mation and auditory input performs much better on this task than the uni-modal representations. It is interesting to observe that the linguistic repre-sentations perform better than the auditory ones: a possible explanation for this is that audio files in FreeSound are rarely samples of a single individ-ual instrument, so if a bass is often accompanied by a drum this will affect the overall representa-tion. The table also shows, for the 5 clusters under both models, the nearest instruments to the cluster centroids, qualitatively demonstrating the greater cluster coherence for the multi-modal model. We have studied grounding semantic representa-tions in raw auditory perceptual information, us-ing a bag of audio words model to obtain au-ditory representations, and combining them into multi-modal representations using a variety of fu-sion strategies. Following previous work in multi-modal semantics, we evaluated on conceptual sim-ilarity and relatedness datasets, and on the cross-modal task of zero-shot learning. We presented a short case study showing that multi-modal rep-resentations perform much better than auditory or linguistic representations on a musical instrument clustering task. It may well be the case that the Table 6: V-measure performance for clustering musical instruments, together with instruments closest to cluster centroid for linguistic and multi-modal. auditory modality is better suited for other evalua-tions, but we have chosen to follow standard eval-uations in multi-modal semantics to allow for a di-rect comparison.

In future work, it would be interesting to inves-tigate different sampling strategies for the early fusion joint-learning approach and to investigate more sophisticated mixing strategies for the mid-dle and late fusion models, e.g. using the  X  X udio dispersion X  of a word to determine how much au-ditory input should be included in the multi-modal representation (Kiela et al., 2014). Another in-teresting possibility is to improve auditory repre-sentations by training a neural network classifier on the audio files and subsequently transferring the hidden representations to tasks in semantics. Lastly, now that the perceptual modalities of vi-sion, audio and even olfaction (Kiela et al., 2015) have been investigated in the context of distribu-tional semantics, the logical next step for future work is to explore different fusion strategies for multi-modal models that combine various sources of perceptual input into a single grounded model. DK is supported by EPSRC grant EP/I037512/1. SC is supported by ERC Starting Grant DisCoTex (306920) and EPSRC grant EP/I037512/1. We are grateful to Xavier Serra, Frederic Font Corbera, Alessandro Lopopolo and Emiel van Miltenburg for useful suggestions and thank the anonymous reviewers for their helpful comments.

