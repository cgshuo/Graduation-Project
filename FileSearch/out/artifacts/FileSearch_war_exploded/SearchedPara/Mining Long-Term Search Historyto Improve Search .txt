 Long-term search history contains rich information about a user X  X  search preferences, which can be us ed as search context to improve retrieval performance. In this paper, we study statistical language modeling based methods to mine contextual information from long-term search history and exploit it for a more accurate estimate of the query language model. Experiments on real web search data show that the algorithms are effective in improving search accuracy for both fresh and recurring queries. The best performance is achieved when using clickthrough data of past searches that are related to the current query.
 H.3.3 [ Information Search and Retrieval ]: Retrieval models Algorithms Search history, query expansion, context
Most existing retrieval systems, including the web search en-gines, suffer from the problem of  X  X ne size fits all X : the decision of which documents to return is made based only on the query, with-out consideration of a particular user X  X  preferen ces and search con-text. When a query (e.g.,  X  X ython X ) is ambiguous, the search results are inevitably mixed in content (e.g., containing documents on the snake and on the programming language), which is certainly non-optimal for the user, who is burdened by the need to sift through the mixed results. Therefore, instead of relying solely on the query, which is usually just a few keywords, retrieval systems should ex-ploit the user X  X  search context, which can reveal more about the user X  X  true information need. Indeed, contextual retrieval has been identified as a major challenge in information retrieval research [1].
There are a wide variety of search contexts, from the user X  X  back-ground and interests, personal document collection (e.g., emails Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00. and saved web pages), to what activities the user is doing before submitting the query (e.g., reading an article on wildlife). In this paper, we focus on the user X  X  search history , which is often kept in log format and records what queries the user made in the past and what results he/she chose to view. This is arguably the most important form of search context for the reasons below. First, a user X  X  background and interests can usually be learned from his/her search history by looking at the topics covered by the past queries. For example, if there were many queries such as  X  X ebugging X  and  X  X GI code X , the user is probably interested in programming and  X  X ython X  is likely to mean the programming language. Second, from the user X  X  past (implicit) indication of document relevance we can predict his/her reaction to the current retrieved documents. For example, if the user searched with the same query  X  X ython X  before and clicked on Python language website X  X  link, we have high con-fidence that the user would do it again this time, and it makes good sense to list that webpage in the top. Even when there is no exact occurrence of the current query in history, we may still find similar queries like  X  X ython doc X  helpful (e.g., discovering that the user prefers results from the www.python.org site). Because the rele-vance judgment is usually only inferred from user activities (e.g., clicking on a link, viewing/saving/bookmarking a page), this be-longs to the category of implicit feedback, which has been studied in [3, 4, 9, 5] and shown to effectively improve retrieval perfor-mance. Finally, search history is readily available without extra user efforts. In the web search domain, user search history can be obtained by a proxy from web logs, or by a search engine using HTTP redirects. If privacy is a concern, we can use browser plug-ins and perform result reranking at the client-side [7].
 Search history can be divided into short-term and long-term types. Short-term search history is limited to a single search session ,which contains a (normally consecutive) sequence of searches with a co-herent information need and usually spans a short period of time. Often, a user composes an initial query, views the returned docu-ments, and if unsatisfied, modifies the query and repeats the search process. All these activities, which form the short-term search his-tory, shed light on the current information need and make useful search context. As shown in [5], queries and clickthrough data in the short-term search history provide implicit feedback that can be used to estimate a more accurate query language model and im-prove retrieval performance.

Long-term search history is, in contrast, unlimited in time scope and may include all search activities in the past. Compared with short-term search history, it has several advantages. There is no need to detect session boundaries (determining whether a previous search shares the same information need as the current one), which is often a difficult task. Nor do we need to limit the context to the contiguous chain of searches in a session; any search in the past that is related to the current one should be leveraged. This also means that we may find context for the very first query in a chain, which is impossible if the search history is constrained to be short-term.
Although the extension from short-term to long-term seems nat-ural and promising, the full potential of long-term search history cannot be reached easily. This is because long-term history in-evitably involves a lot of noisy information that is irrelevant (some-times even distractive) to the current search; only those searches that are related to the current one should be considered as useful context. For example, searches like  X  X orld cup X  that are irrelevant to the current query  X  X ython X  would not be helpful, and such noise can overwhelm the signal of related past searches. For this reason, when exploiting short-term search history we need to detect session boundaries first, so that only those searches with the same informa-tion need are used. Unfortunately, most existing studies on long-term search context 1 fail to address this problem, even though they still tend to get positive results; they often use all available context as a whole (or divide it into chunks by time), without distinguish-ing between relevant and irrelevant parts. Such work includes [9], which interpolates the current query with different chunks (time periods) of history (browsed web pages) for personalized search, and [10], which constructs user profiles from indexed desktop doc-uments for search result reranking.

In this paper, we systematically study how to exploit a user X  X  long-term search history to improve retrieval accuracy. We pro-pose mixture models to represent a user X  X  information need and apply statistical language modeling techniques to discover relevant context from the search history, and exploit it to obtain improved estimates of the query model. We then evaluate the methods on a test set of Web search histories collected from some real users. We find that mined search history information, can substantially im-prove retrieval performance for both recurring and fresh queries, and works best when clickthrough data is used with a discrimina-tive weighting scheme for past searches. We also find that although recent history tends to be much more useful than remote history (especially for fresh queries), all of the entire history is helpful for improving the search accuracy of recurring queries.

The rest of the paper is organized as follows. In Section 2, we introduce a context-sensitive information retrieval approach that al-lows us to incorporate contextual information mined from search history. In Section 3, we define the search history mining task and cast it as a query language model estimation problem. In Section 4, we develop several algorithms based on statistical language models to mine long-term search history. We describe how we build a test set by collecting users X  web search history in Section 5 and present our experiment results in Section 6. Section 7 concludes our work.
Traditional retrieval models take the retrieval problem as match-ing a query with a set of documents [8], and thus are inadequate for modeling personalized and contextual search. Previous work [5, 6] has proposed to use statistical language models for context-sensitive search. We follow this approach in this paper. As the background, we briefly describe here how context-sensitive search can be achieved through statistical language modeling.

A language model is a probabilistic model of text. In retrieval, we often use the simplest unigram language models (i.e., word dis-tributions) to model both queries and documents. In the Kullback-
They are often not strictly what we define as search history (queries and viewed results), but being instead all documents (web pages and/or emails) the user has viewed over a long period of time. Leibler (KL) divergence retrieval method [11], the retrieval task is taken as computing a query language model  X  q for the query q and a document language model  X  d for each document d and then scoring the document according to their KL divergence D (  X  q ||  X  is defined as where w is a word in the vocabulary set V .

Clearly, with this method, our main task is to estimate  X   X  . We estimate the document model  X  d using Bayesian smoothing with Dirichlet prior [12]: where c ( w ; d ) is the count of w in d , | d | is the document length, p ( w |  X  C ) is the collection language model and  X  is the Dirichlet prior. In our study, we use the TREC web corpus WT10g to esti-mate the collection language model p ( w |  X  C ) and set  X  to 10, which is optimized for the contextless baseline and suitable for short snip-pet texts.

When no other evidence is available (i.e., being contextless), the query model  X  q can be estimated solely based on the query text using the Maximum Likelihood Estimator (MLE): where c ( w ; q ) is the count of w in q and | q | is the query length. But when there is contextual information mined from search history, we can incorporate it as additional evidence to improve our estimate of the query language model. This natural incorporation of extra information is why language models are particularly suitable for modeling context-sensitive search.

Specifically, given search history H , we can estimate the context-sensitive query model p ( w |  X  q,H ) as where p ( w |  X  q ) is the context-independent language model esti-mated using query text only, p ( w |  X  H ) is a history language model learned from search history, and  X   X  [0 , 1] is the interpolation weight.

In the following sections, we will describe how to compute the context-sensitive query model p ( w |  X  q,H ) based on long-term search history, and how to estimate  X  .
In a typical scenario of information retrieval, after a query is submitted, the retrieval system will return a set of result documents with titles and summaries displayed. The user can then select to view the full texts of some results (usually by clicking on them). Thus the search history generally includes three components: past queries, their search results, and the information on which results were clicked/viewed (also known as clickthrough data).
 Formally, let q i be a query, D i be the set of its result documents, C ( C i  X  D i ) be the set of clicked ones, and t i be q i  X  X  submission time. If q k is the current query, its search history H k all previous queries q 1 ,q 2 ,  X  X  X  ,q k  X  1 ordered by time, and their corresponding D i  X  X  and C i  X  X .

As described previously, we can use the following interpolation formula to compute the context-sensitive query model for the cur-rent query q k : where  X  q k is the weight on the original query, and  X  H k model for q k .

The goal of search history mining is to estimate the best history model  X  H k from q k  X  X  history H k , the one that is most informative of the user X  X  search context and thus can bring greatest increase in retrieval accuracy. There are several challenges in this task: First, a past search can contain different components (query, results and clickthrough). We should find the best way to combine these pieces of contextual information. Second, as we have discussed, not all past queries are equally important. We need to identify queries re-lated to the current one and weight them appropriately. Third, when the search history has hundreds or thousands of entries, efficiency may become a concern. These issues will be addressed in the next section.
In this section, we discuss how to compute the history language model  X  H k , which is regarded as the search context of the current query q k and to be interpolated with  X  q k . Our strategy is to first generate a unit history model for each history query, and then com-bine them to get the overall history model.
For each past query q i  X  H k , we will estimate a language model  X  that captures the user X  X  information need at that particular mo-ment. We call this a unit history model, because it represents a basic unit of search history that can be integrated to produce the overall history model. We use the following formula to compute it: p ( w |  X  i )=  X  q p ( w |  X  q i )+(1  X   X  q )  X  where  X  q is the interpolation weight on the original query model,  X  q i and  X  d j are the query and document language models, and NC i = D i  X  C i is the set of non-clicked results. The fraction in the above formula is essentially a weighted average of result docu-ment models, with  X  C and  X  NC being the weights on clicked and non-clicked results, and | C | and | NC | being the number of clicked and non-clicked results.

Below we discuss three special degenerated cases for setting the parameters, each involving a single component of search history (namely, query, documents and clickthrough):
The general form of (6) combines different components of search history. Typically, we set  X  C &gt; X  NC &gt; 0 , so that clicked results receive more weight than non-clicked ones. On our data set, we find that the setting  X  q =0 , X  C =20 , X  NC =1 achieves good performance.
We use a weighted average of the unit history models of past queries as the overall history model: We discuss two general weighting schemes below.
With equal weighting, unit history models of different past queries are assigned equal weights: If the unit history models only rely on query texts (7), and queries are assumed to be of equal length, the probability of a term in the overall history model is proportional to its global frequency in all queries of the history. Similar things can be said about (8) and (9) for search results and those clicked ones.

This simple weighting scheme suffers from the problem that, as it tries to assign equal weights to every piece of search history, none of them obtains much weight to be influential. It produces a global but weak description of the user X  X  long-term interests.
As we have discussed in Section 1, out of all past searches, only those that are related to the current query are important as its con-text. We should therefore concentrate the weight mass on them, and ignore other random, noisy parts of the search history. We call this approach discriminative weighting, as we are selective about which parts of search history to use according to the current query.
Generally, the more similar to the current query a previous query is, the more weight it should have in the computation of the overall history model. Below we describe several methods for calculating similarity scores between two queries, which can be used as inter-polation weights in (10).
For each query q i , we compute a TF-IDF vector v i that corre-sponds to the concatenation of all its result documents: where v i [ w ] is the element in the TF-IDF vector that corresponds to term w , N is the number of documents in the background cor-pus (WT10g), and DF ( w ) is w  X  X  document frequency. We choose to use concatenation of result documents rather than query text be-cause query text is usually very short, so there may not be enough overlapping between two queries, even if they are related.
The cosine similarity between two vectors is defined as and is always in [0 , 1] .

Since cos ( v i ,v k ) measures how close q i is related to q naturally use it for  X  i in (10).
Here we present a more principled approach, in which  X  i is de-rived from mixture weights in a generative model.

Suppose there is a mixture model  X  mix : p ( w |  X  mix )=  X  C p ( w |  X  C )+  X  q p ( w |  X  q k )+ where  X  C is the background language model estimated from the corpus (WT10g),  X  q k is MLE from the current query q k  X  X  text, and  X  is MLE from the concatenation of result documents of q i ,apast query in H k :  X  C ,  X  q and  X  i are mixture weights and constrained by
Let  X  denote the set of mixture weights (  X  C , X  q , X  i ). We want to choose  X   X  to maximize the log likelihood for the mixture model to generate the result documents of q k :
From (14) and (17), it can be easily seen that, the closer q related to q k , the larger mixture weight (  X  i )  X  mixture model (because it fits D k better). Indeed,  X  i reaches its maximum when D i is identical to D k . Therefore, we can use  X  for  X  i in (10).

To estimate these mixture weights, we use the EM algorithm. Let w j be the j -th word in the concatenation of all result documents in D k . The Q-function is where L = d j  X  D i | d j | is the sum of q k  X  X  result document lengths,  X  n ) is the set of parameters at the n -th iteration, and Z are the hidden variables, indicating the events of w j being gener-ated by  X  C ,  X  q k ,  X  i respectively.

In the E-step, we have
In the M-step, we have
Because we have computed  X  q , the mixture weight on q k may estimate  X  q k in (5) based on it, instead of using a fixed value: This way, the weighting in the final contextual model  X  flexible: when there is a rich amount of relevant search history (re-flected by a large value of k  X  1 i =1  X  i compared to  X  q be significant weight on the history model  X  H k ; on the other hand, when the search history is mostly irrelevant, the MLE model  X  from query text will dominate. Moreover, all the weighting param-eters (i.e.,  X  q k and  X  i  X  X ) will be estimated rather than manually set.
The EM estimation method, although shown to produce more ac-curate weights, runs much slower than the cosine similarity method, due to the fact that the EM algorithm usually needs many iterations to converge, and each iteration is generally more complex than just computing a cosine similarity value. This will be a big concern for longer search history, when there are hundreds or thousands of queries.

We observe on our data set that, with discriminative weighting, only a small number of previous queries are most related to the current query and receive non-insignificant weights (which is ex-actly what we intend to see). Motivated by this, we first run the cosine similarity method, identify the queries with highest similar-ity scores, and keep them in a working set . We then run the EM estimation method only on the queries in this working set, and as-sign zero weights to other queries in the search history. We find this approach yields similar retrieval accuracy as the original EM method, yet runs almost as fast as the cosine method.
To our knowledge, there is no publicly available collection of search logs that contain reasonably long period of users X  search history with implicit feedback information. Therefore, we chose to create our own data set in the web search domain by making a plug-in for the Firefox browser to record a user X  X  long-term search history. Specifically, the plug-in saves to a log file all user search activities that are captured from the browser, including queries is-sued to the Google search engine , search results (with titles, sum-maries and URLs) returned, and the information of which results are clicked on. The plug-in collects search history in the back-ground and is intentionally kept transparent from the user so that it will not interfere with her normal search activities.

Four computer science students kept the plug-ins installed on their personal computers for over a month and then submitted their individual search logs to us (they were free to delete any sensi-tive queries that they do not want to disclose). Next the users were asked to pick at least 15 queries from their own search logs, starting from the back (the most recent history). The queries selected from the search logs would be evaluated to create a test data set. They must satisfy the following conditions, so that there is room for po-tential improvement of retrieval accuracy with long-term context. 1. A selected query should have at least one relevant document. 2. A selected query should either match the person X  X  interests
For each query, we chose the top 20 results retrieved from Google as the collection of documents (with Google X  X  ranking information removed) to be scored by our retrieval methods. We only use their snippet texts (title + summary). To evaluate these result documents, the users were presented with the set of top 20 results retrieved from Google and asked to judge whether each document was relevant or not. If a query was a known-item search, i.e., if the user knew exactly what the needed result would look like, then only that re-sult should be deemed relevant. Otherwise, if the user was explor-ing some topic, then he/she should mark all results matching that topic as relevant. For example, if the user has visited Python lan-guage X  X  website before and is searching for it again, only this result should be considered relevant; if the user does not know Python and searches to get some ideas about it, then a tutorial on Python is also relevant.

We distinguish two types of queries due to their different nature and retrieval performance. If a query has occurred before in the search history (in exact form or with keywords X  order changed) and there are clicks associated with its earlier occurrence(s), it belongs to the category of recurring queries. Otherwise, we call the query fresh . Usually, the purposes of recurring queries are navigational rather than informational or transactional [2]. Recurring queries are also more likely to reflect the user X  X  long-term interests. It tends to be easier to improve the retrieval performance of recurring queries, as the user is very likely to choose exactly those results clicked on in an earlier search.

Table 1 shows some statistics of the collected log data. The large difference in the number of queries is due to some users not using Firefox for all of their web searches.
 avg. # rel. results per query 2.09 4.14 3.58 6.59
In this section, we empirically evaluate the performance of the proposed methods on our data set of personal web search logs. We will also study the influence on retrieval accuracy of individ-ual components and different time cutoffs in search history.
We use the standard TREC mean average precision (MAP) and precision at top 5 documents (Pr@5) as our evaluation metrics, which respectively measure the system X  X  overall retrieval accuracy and its performance for those documents that are most viewed. We pool together the queries and judgments of all four users, so that the evaluation result will be a weighted average over these users, with the number of testing queries of each user as weights. We also report the performance for fresh and recurring queries separately, because they display very different behaviors.
We first study how useful each search history component (i.e., query, documents and clickthrough) are as search context. Table 2 shows the performance of using only certain components with the equal weighting and EM estimation methods. The rows  X  X on-textless X ,  X  X qual X ,  X  X M X  correspond respectively to the baseline method of using only query text, equal weighting and discrimina-tive weighting with EM estimation. The text in parentheses indi-cate which component is used. For equal weighting, we set  X  0.1 for fresh queries and 0.02 for recurring queries, which perform better than other values.
 Table 2: Effects of using different search history components
We find that from the search history, queries alone usually does not help (except in the case of recurring queries with equal weight-ing), probably because query texts are too short to make useful search context. In contrast, result documents are able to improve retrieval performance, especially for recurring queries. Finally, clicked results yield the highest increase in search accuracy, sug-gesting the usefulness of clickthrough as implicit feedback.
The fact that both result documents and clickthrough bring im-provement in retrieval performance prompts us to combine them by setting  X  C =20  X  NC in (6), so that both result documents and clickthrough are used in the computation of the history model. However, we do not observe performance gain over using only clickthrough.
Table 3 shows the retrieval accuracy of different methods. The rows  X  X ontextless X ,  X  X qual X ,  X  X osine X ,  X  X M X ,  X  X ybrid X  correspond, respectively, to the baseline method of using query text only, equal weighting, discriminative weighting with cosine similarity, discrim-inative weighting with EM estimation and the hybrid method. In the methods that use search history, we combine result documents and clickthrough by setting  X  C =20  X  NC as before.

We observe that all contextual methods perform better than the contextless one, indicating that long-term history indeed provides helpful search context. We also find that recurring queries get a lot more improvement from the use of search context than fresh queries, because by nature recurring queries have more relevant search history available as implicit feedback. As we have expected, the discriminative weighting methods outperform the equal weight-ing one, proving the advantage of selective use of search history. Finally, we note that the EM estimation method achieves higher re-trieval accuracy than the cosine similarity method, and the hybrid method has comparable performance.
To find out whether recent search history is most useful and whether remote search history helps , we truncate the search history to different lengths (e.g., one day back from the current query), and plot the change of MAP (of the EM weighting method) with respect to time cutoff in Figure 1. Figure 1: Effects of different search history lengths: within 12 hours (top) &amp; within 70 days (bottom)
We see that for fresh queries, the dominant increase in MAP comes from the most recent history (especially within one hour), while for recurring queries, although recent history is clearly more important, remote history also contributes to the improvement in retrieval accuracy. We believe the difference is because recurring queries are more likely to reflect a user X  X  long-term interests, and thus have more relevant history to leverage.
In this paper, we systematically explored how to exploit long-term search history, which consists of past queries, result docu-ments and clickthrough, as useful search context that can improve retrieval performance. We emphasized the importance of discrimi-native use of search history, by concentrating on the most relevant past queries. We cast the search history mining problem as esti-mating a more accurate query model from evidence in the search history, and developed methods based on statistical language mod-eling for this task. We collected real web search data as our test set and shown in our experiments that the contextual methods can ef-fectively improve search accuracy over the traditional, contextless method for both fresh and recurring queries, with the EM-based discriminative weighting scheme achieving best performance. We also found through our study of different cutoffs in search history that although recent history is more important, remote history is also useful, especially for recurring queries..

The current work can be extended in several ways: First, the mixture model used in this paper is quite simple. For example, we just concatenate the result documents and treat them equally, ig-noring their internal relationship (e.g., they may be clustered). A more appropriate generative model should take these issues into ac-count. Second, in the web search domain the result documents are actually structured (e.g. they have URLs) and it would be interest-ing to explore how these structural elements could be used. Third, we plan to implement our algorithms inside a web browser search plug-in (UCAIR Toolbar [7]) to provide contextual search on the client-side, which would greatly benefit people X  X  daily search.
This work is supported in part by the National Science Foun-dation grants IIS-0347933 and IIS-0428472 and by a Google Re-search Grant. We thank the graduate students who have helped us collect the search history data and make the relevance judgments.
