 An effective approach to detecting anomalous points in a data set is distance-based outlier detection. This paper describes a simple sampling algorithm to efficiently detect distance-based outliers in domains where each and every dis-tance computation is very expensive. Unlike any existing algorithms, the sampling algorithm requires a fixed num-ber of distance computations and can return good results with accuracy guarantees. The most computationally ex-pensive aspect of estimating the accuracy of the result is sorting all of the distances computed by the sampling algo-rithm. The experimental study on two expensive domains as well as ten additional real-life data sets demonstrates both the efficiency and effectiveness of the sampling algorithm in comparison with the state-of-the-art algorithm and the reliability of the accuracy guarantees.
 Categories and Subject Descriptors: H.2.8 [Database Management]: Database Applications  X  Data mining Keywords: Outliers, distance-based, sampling algorithms, statistical modeling, approximate algorithms, nearest neigh-bor
One effective approach for detecting anomalous data points in a set is distance-based (DB) outlier detection [6, 11, 1]. In DB-outlier detection, each data point is represented as a point in a multi-dimensional feature space and a distance function is chosen based on domain-specific requirements. Data points that are significantly far away from all of the others are flagged as outliers.

Though it may seem that DB-outlier detection is a solved problem, it turns out that even using advanced algorithms, the method is computationally prohibitive. This is particu-larly true in domains that require expensive distance func-tions. For example, the edit distance function for strings  X  This paper is based upon work supported by the National Science Fo undation under G rant No. 0347408.
 Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00. [12], the ERP distance function for time series [3], the quadratic-form distance function for color histograms [5] and various scoring matrices for aligning bioinformatics sequences [4] all are computationally expensive. Given two input points that are both of dimension n , the aforementioned distance func-tions require  X ( cn 2 ) time. In these domains, state-of-the-art algorithms may take days to detect DB-outliers, even in data sets of small sizes.

The goal of this paper is to define an algorithm that can provide users with interactive-speed performance over ex-pensive domains. The question we consider is: How can we reduce the required number of distance computations in DB-outlier mining? For certain distance measures and data sets, indexing and pruning techniques can be used to reduce the number of distance computations. Unfortunately, index-ing [2] is not useful in the domains mentioned above due to high data dimensionality, and pruning [1] tends only to re-duce the required number of distance computations to a few hundred or a few thousand per data point; as we will show experimentally, this is still too costly in expensive domains.
In this paper, we consider a simple sampling algorithm for mining the kth nearest neighbor ( kth -NN) DB-outliers [11]. Given integer parameters k and  X  ,the kth -NN outliers are the top  X  points whose distance to its kth -NN is greatest. Sampling has been considered before as an option for detect-ing outliers [7], but never along with a rigorous treatment of the effect on result quality. If the user is able to toler-ate some rigorously-measured inaccuracy, our algorithm can give arbitrarily fast response times.
 Algorithm 1 Sampling Algorithm Input : Adataset G of size n; k , specifying which NN distance Output :  X  points as the kth -NN outliers 1: for each point i  X  G do 2: Draw a random sample of size  X  from G (not including 3: Calculate point i  X  X  kth -NN distance in its sample 4: end for 5: Return the top  X  points whose kth -NN distance in its sample
Our algorithm (Algorithm 1) works as follows. For each data point i , we randomly sample  X  points from the data set. Using the user-specified distance function, we find the kth -NN distance of point i in those  X  samples. After re-peating the above process for each point, the sampling algo-rithm returns the  X  points whose sampled kth -NN distance is greatest. Aside from the algorithm X  X  obvious simplicity, its biggest benefit is that it allows a user to control the total number of distance computations as  X (  X n ), thus bounding the time required to run the algorithm to completion.
To provide accuracy guarantees for Algorithm 1, we for-mally analyze the statistical properties of the algorithm, and describe an effective technique that gives the user a statisti-cal indication of the algorithm X  X  result quality. Specifically, we treat the number of the true top  X kth -NN outliers in Algorithm 1 X  X  return set as a random variable whose char-acteristics can be used to measure the quality of the outliers returned by the algorithm. Thus, if we tell the user that the expected number of true outliers returned is 20 for a given data set, then this implies that if the sampling algorithm were run many times on that data set, on average 20 out of the 30 returned points will be among the true top 30 kth -NN outliers. Finally, extensive experiments were performed in comparison with the state-of-the-art algorithm.
The remainder of the paper is organized as follows. Sec-tion 2 describes the overall process of providing quality guar-antees for the sampling algorithm. In Section 3, a statistical analysis of the sampling algorithm is performed. Section 4 illustrates how the analysis can be applied to a constructed distance database efficiently. Section 5 details the experi-mental study. The paper is concluded in Section 6.
Algorithm 1 (hereafter referred to as the  X  X ampling algo-rithm X ) is so simple that there is little benefit in discussing it in greater detail. However, the issue that clearly does de-serve more attention is the question of how to give the user an understanding of exactly how accurate this algorithm is likely to be in practice; it is this question that we consider in detail in the remainder of the paper.

The paper describes a two-step process for analyzing the quality of the sampling algorithm, so that the estimated result quality can be returned to the user along with the discovered outliers. In the first step, the distances between each point and their samples are used to (logically) construct a distance database D , where the set of pairwise distances in D is a reasonable approximation of the actual distance database D .By distance database , we refer to a matrix that stores the pairwise distance from point i to point j for every i and j . During the construction of D , we wish to avoid additional distance computations beyond those required by the sampling algorithm. Thus, for a data point i ,thesam-pled distances { d 1 ,d 2 , ..., d  X  } between i and the other points in its sample are replicated as needed to serve as a surro-gate for the actual neighbor distances { d 1 ,d 2 , ..., d i . This reconstruction is identical to a type of reconstruc-tion advocated when making use of the without-replacement bootstrap from statistics [10].

In the second step, an exact, statistical analysis of the quality of the sampling algorithm for the database D is performed, and the result is returned to the user as an in-dication of the accuracy of the algorithm when it is applied to D . While it is true that bounds that are returned rely on the supposition that D is in fact a reasonable surrogate for D , such assumptions are generally unavoidable in statisti-cal analysis. An analogous assumption in classical sampling theory is that the sample variance can be used as a rea-sonable surrogate for the population variance. In order to address this concern, Section 5 shows that over twelve real-life data sets, for various sample sizes, the bounds derived by our method do in fact reliably predict the accuracy of the sampling algorithm.
This section describes a formal, probabilistic analysis of the quality of the sampling algorithm X  X  return set.
Our analysis needs to be performed with respect to some measure of the algorithm X  X  quality. Let A be the set of the true top  X kth -NN outliers, and let A be the return set of the sampling algorithm. Then the size of the intersection A  X  A is a reasonable measure of the sampling algorithm X  X  quality. Let N be a random variable denoting the size of this intersection set for a single run of the sampling algo-rithm. To characterize the sampling algorithm X  X  quality, we characterize the expectation and variance of N (denoted by E [ N ]and Var ( N ) respectively). They describe the average number of correct outliers retu rned by the algorithm, as well as how much this number is expected to vary.
We begin with a mathematical expression for N .Let y i evaluate to one if the ith point in the data set G is a true kth -NN outlier, and zero otherwise. We define a series of Bernoulli (zero/one) random variables of the form M i ,where M i is one if the ith point is declared as an outlier by the sampling algorithm, and zero otherwise. Then: Taking the expectation of both sides, we have: Note that for a given input of the sampling algorithm, y i constant. Thus, deriving an expression for E [ N ] reduces to the problem of deriving an expression for E [ M i ]. Since M a Bernoulli random variable, this is equivalent to computing the probability that M i evaluates to one.
M i evaluates to one if point i is reported as an outlier by the sampling algorithm. Obviously, point i is flagged as an outlier only if there are at most  X   X  1pointsin G whose kth -NN distance in its sample is larger than point i  X  X . Let T be a random variable denoting the total number of such points. Then M i is one if and only if T i  X   X   X  1. Noting that T i is asymptotically normally distributed 1 ,wehave: In general, T i is normally distributed due to the Lyapounov Central Limit Theorem. Since T i is the sum of n  X  1 inde-pendent Bernoulli random variables, this Theorem applies; see Lehmann [8], Corollary 2.7.1.
In Equation 3,  X ( x ) is the standard normal cumulative distribution function. In order to make use of  X ( x ), we need to calculate the mean and variance of T i . Beforewedo this, for each point i  X  G , we introduce a random vari-able N i , which denotes the sampled kth -NN distance of point i . In addition, we define a function one ( expr )that evaluates to one if and only if the boolean-type argument expr is true, and zero otherwise. Given these notations, T = to the vector of point i  X  X  neighbor distances ordered from the smallest to the largest, then the following lemma gives the formulas for computing the mean and variance of T i . This lemma provides the base formulas that we will use to compute E [ N ] in Section 4.
 Lemma 1. The mean and variance of T i are given by: Var ( T i )= E [ T i ]  X  E 2 [ T i ]+ sum 1  X  sum 2 In Lemma 1, sum 1and sum 2aregivenby: sum 1=
We will not present the proof due to space limits.
Knowing on expectation how many correct outliers in the return set of the sampling algorithm is not enough. It is crucial to help the user to understand how the num-ber of correct outliers is going to vary around its mean. Thevarianceof N gives such a measure. This quantity is Var ( N )= E [ N 2 ]  X  E 2 [ N ]. Since we have already discussed how to compute E [ N ] in Section 3.2, the remaining task is to calculate the second moment of N : Note that in Equation 6, y i and y j are constants for a given input of the sampling algorithm. Therefore, deriving an expression for Var ( N ) reduces to deriving an expression for E [ M i M j ]. Since M i M j is a Bernoulli random variable, this is equivalent to computing Pr [ M i M j =1].
M i M j evaluates to one if and only if both point i and point j are reported as outliers by the sampling algorithm. Point i and point j are both flagged as outliers only if there are at most  X   X  2pointsin G whose sampled kth -NN dis-tances are larger than the smaller of i  X  X  and j  X  X  . Otherwise, either point i or j (or both) will not be flagged as outliers. In a manner analogous to the way that we use T i to help calculate E [ M i ], we will use U ij to help calculate E [ M Let U ij be a random variable denoting the total number of the points whose sampled kth -NN distance is larger than the smaller of i  X  X  and j  X  X  sampled kth -NN distance. Given this, M i M j is one if and only if U ij  X   X   X  2. Noting that U is asymptotically normally distributed (for reasons identical to T i  X  X  normality), we have: To make use of  X ( x ), we need to calculate the mean and vari-ance of U ij .Toexpress U ij in a mathematical form, we again introduce a series of random variables, where S ij denotes the smaller of point i  X  X  and point j  X  X  kth -NN distances in their samples. Obviously, the domain of S ij is D i  X  D j .Given these notations, we have U ij = Lemma 2 gives the formulas for computing the mean and variance of U ij . It provides the base formulas for computing Var ( N ) in Section 4. Lemma 2. The mean and variance of U ij are given by: E [ U ij ]= Var ( U ij )= E [ U ij ]  X  E 2 [ U ij ]+ sum 3  X  sum 4 In Lemma 2, sum 3and sum 4aregivenby: sum 3= sum 4=
Again, we will not present the proof due to space limits.
Equipped with Section 3 X  X  theoretical foundation, we now follow the two steps discussed in Section 2 to perform the estimation on a constructed distance database.
Since we wish to avoid new distance computations in the construction of D , one strategy is to uniformly replicate the distances computed by the sampling algorithm to cre-ate a full-size distance database that can subsequently be analyzed. We describe such a construction with an exam-ple, illustrated in Figure 1. On the left-hand side of Figure 1, a two-dimensional matrix is used to represent the dis-tance database, where each column of the matrix contains the sorted distances from a given point to all of the other data points. Given this data, our sampling algorithm ran-domly selects two neighbor distances for each point of the input data set. As a result, we obtain the sampled distance matrix shown in the middle of Figure 1. In order to build a complete distance matrix D by making use of the sampled Figure 1: Replicating sample distances to construct D . distance matrix, we replicate each row of the sampled dis-tance matrix twice, which gives us the approximate distance matrix D shown on the right-hand side of Figure 1. Note that no additional distance computations are performed.
In general, the process of constructing D uses n  X  1  X  repli-cations of each distance computed by the original sampling algorithm. As a result, we regard each column D i in D as an approximation of its corresponding column D i in D . Algorithm 2 EN Algorithm Input : the sample distance array SD ; Algorithm 1 X  X  parameters Output : the E [ N ]for D 1: Sort SD ascendingly 2: for j =  X n to 1 /*backward scan SD */ do 4: Follow Lemma 1 to update E [ T i ]andthe sum 1and 5: end if 6: Update the sufficient statistics 7: end for 8: Apply Lemma 1 and Equation 3 to calculate E [ M i ]foreach
In this section, we show efficient algorithms that can ob-tain E [ N ]and Var ( N )for D in  X (  X n log(  X n )) time. To compute E [ N ], the main task is to compute E [ T i ]and Var ( T i ) for each outlier column i in D (those with the  X  largest D i [ k ]valuesin D ).

To calculate E [ T i ], the following two observations of Equa-tion 4 and the E [ T i ] formula in Lemma 1 are critical: 1. Each distance d  X  D is associated with a fixed prob-2. The main task of computing
Furthermore, notice that for a given column D i of D ,the  X  rows resulting from a single sample distance can be rep-resented by one row, if the probability associated with the representative row is the sum of the probabilites associated with its n  X  1  X  replications. With this compression, for each distance d in column i , we must find all of those sample distances that are not from column i and are greater than d . A straightforward way to do this is to sort all of the sample distances and scan the sorted distance array from back to front. During the scan, we maintain sufficent statis-tics so that when we encounter a sample distance d  X  D i , Pr [ N i = d ]  X  We find that the following statistics are sufficient for this purpose: (1) the sum of the probabilities associated with each sample distance that has been checked; (2) how many sample distances from column i have been passed. Then we can update E [ T i ] by following Lemma 1 during the back-ward scan. This requires O ( n ) time. A similar idea applies to the calculation of sum 1and sum 2 in Lemma 1. There-fore, Var ( T i )requires O ( n ) time too, provided the sample distance array is sorted (this setup requires  X (  X n log(  X n )) time).

Algorithm 2 (hereafter referred to as the EN algorithm) presents the pseudo code for computing E [ N ]on D .Since Lemma 1 and Lemma 2 have similar structures, we can cal-culate E [ U ij ]and Var ( U ij ) according to Lemma 2 similarly by maitaining some sufficient statistics during the backward scan of the sorted sample distance array. After obtaining E [ U ij ]and Var ( U ij ), computing Var ( N ) is trivial. We call the algorithm to compute Var ( N )for D as the VarN al-gorithm.
This section details the results of two sets of experiments designed to test our methods.
The first set of experiments has two goals. First, we wish to test whether our algorithms are able to return a high-quality answer set in an acceptably short time in a domain requiring an expensive distance function. Second, we wish to check whether a representative, state-of-the-art algorithm for this purpose (Bay X  X  nested loop algorithm [1]) can also provide acceptable performance.

To accomplish these goals, the first task we consider is detecting outlier nucleotide sequences in human chromo-some 18 downloaded from NCBI ftp site. This data set was created by randomly selecting 4000 non-overlapping subse-quences of length 2000 from human chromosome 18 (Chr18). We employed Needleman-Wunsch algorithm [9] to compute the Edit distance between two subsequences. The second task we consider is detecing outlier images in UCID version 2 [13], which is an image database containing 1338 uncom-pressed color images. We transformed each image to a 576 dimensional color histogram. The quadratic distance be-tween two histograms was used [5].

The experiments were performed on a Linux workstation having an Intel Xeon 2.4GHz Processor with 1GB of RAM. All the algorithms were implemented in C++ and compiled with gcc version 4.02. Since we were interested in testing the scenarios that the distance computations dominate all the other costs, we loaded the entire data set into memory for all the algorithms that we tested. We report the wall clock time here. All experiments were run to return the top 30 5th-NN outliers.

We began our experiments by running Bay X  X  nested loop algorithm over both data sets. We recorded the time re-quired, as well as the average distance computations per data point. The results are reported in Table 1. Next, we ran our implementations of the sampling algorithm and the EN and VarN algorithms over the same data, and then in-Data Set Cont./Feature Size BA Wisconsin cancer 30/31 569 165 Balance scale 4/4 625 625 Florida farm 188/188 811 140 California farm 188/188 1,373 175 FCAT Read 27/27 1,404 275 FCAT Math 28/28 1,405 274 California house 9/9 20,640 654 Baseball pitching 22/22 36,245 454 Baseball batting 17/17 85,979 745 Cover Type 10/55 581,012 4527 Table 3: Description of the 10 real data sets and the average number of distance computations per data point (denoted by BA) by Bay X  X  algorithm. Cont./Feature means the number of continuous fea-tures over the number of total features. tersected the result set of our sampling algorithm with the result set of Bay X  X  algorithm to get the observed N .Mean-while, we recorded the bound calculated by E [ N ]  X   X  ,where E [ N ] is the estimate of the expected value of N ,and  X  (the standard deviation of N ) is the square root of the estimate of the variance of N . For each data set, we ran 10 trials with the sample size  X  set to 10. The performance results are also shown in Table 1.
 Discussion . Even though the two data sets have just a few thousand points, Bay X  X  algorithm had relatively poor performance on both, taking nearly one day for the first one. In contrast, our algorithms did an excellent job in these two domains. Our algorithms provided between one and two orders of magnitude speedup, and still maintained relatively high result quality. The sampling algorithm returned more than half of the total true 5th-NN outliers in both cases (and nearly all of them in the UCID case) using only ten samples per data set point. Furthermore, in seven of the ten trials over the Chr18 data set and in ten of the ten trials over the UCID data set, the actual number of outliers returned either exceeded or was almost equal to E [ N ]  X   X  . This indicates that (at least for these two data sets), the reported E [ N ] constitutes a safe lower bound on qualitative performance.
This subsection describes an additional experimental eval-uation of our algorithms, aimed at evaluating the reliability of our estimation algorithms. We wish to obtain some ex-perimental evidence that the results given by our estimation algorithms are reliable with various sample sizes and differ-ent sizes of data sets.

The test was designed as follows. We selected 10 real data sets with different characteristics summarized in Table 3. The experimental setup was identical to what we de-scribed in the previous subsection. We processed the data by normalizing all continuous variables to the range [0,1] and converting all categorical variables to an integer repre-sentation. Hamming distance was used for categorical fea-tures and Euclidean distance was used for continuous fea-tures. The average number of distance computations per data point required by Bay X  X  algorithm are reported in the last column of Table 3. For each of the 10 real data sets, we systematically tested the paper X  X  algorithms with the sample Sampling algorithm 10 47m:24s 10 14m:7s sampling algorithm and the EN and VarN algorithms. size  X  set to 10, 60 and 110 respectively. For each experi-ment, we performed 10 trials. The average statistics of the 10 trials are reported in Table 2.
 Discussion . We observed high reliability for our estima-tion algorithms, indicating that the constructed distance database D is a reasonable surrogate for the original dis-tance database D . Specifically, for the 300 totals runs that were performed in order to construct Table 2, the actual number of true 5 th -NN outliers returned was larger than E [ N ] 199/300 times, larger than ( E [ N ]  X   X  ) 236/300 times, and larger than ( E [ N ]  X  2  X  ) 253/300 times. The table de-picts in detail the close correlation between the predicted E [ N ] and the observed average N . Again, this shows the general utility of our analysis for predicting the accuracy of the algorithm.
We have considered the problem of how to efficiently de-tect DB-outliers when the distance function is expensive. A simple sampling algorithm requiring a fixed number of distance computations is proposed and the statistical char-acteristics of this sampling algorithm is formally analyzed. Based on the analysis, two estimation algorithms are pro-posed, requiring only sorting time of the sampled distances. As a result, this paper provides a practical tool to explore DB-outliers in expensive domains. [1] S.D.BayandM.Schwabacher.Miningdistance-based [2] M.M.Breunig,H.-P.Kriegel,R.T.Ng,and [3] L. Chen and R. T. Ng. On the marriage of lp-norms [4] M. Dayhoff, R. M. Schwartz, and B. C. Orcutt. A [5] C. Faloutsos, R. Barber, M. Flickner, J. Hafner, [6] E.M.Knorr,R.T.Ng,andV.Tucakov.
 [7] G. Kollios, D. Gunopulos, N. Koudas, and [8] E. Lehmann. Elements of Large-Sample Theory . [9] S. Needleman and C. D. Wunsch. A general method [10] B. Presnell and J. Booth. Resampling methods for [11] S. Ramaswamy, R. Rastogi, and K. Shim. Efficient [12] E. S. Ristad and P. N. Yianilos. Learning string-edit [13] G. Schaefer and M. Stich. Ucid -an uncompressed
