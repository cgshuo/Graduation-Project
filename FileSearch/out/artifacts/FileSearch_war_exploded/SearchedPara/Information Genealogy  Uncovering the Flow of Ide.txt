 We now have incrementally-grown databases of text docu-ments ranging back for over a decade in areas ranging from personal email, to news-articles and conference proceedings. While accessing individual documents is easy, methods for overviewing and understanding these collections as a whole are lacking in number and in scope. In this paper, we ad-dress one such global analysis task, namely the problem of automatically uncovering how ideas spread through the col-lection over time. We refer to this problem as Information Genealogy . In contrast to bibliometric methods that are limited to collections with explicit citation structure, we in-vestigate content-based methods requiring only the text and timestamps of the documents. In particular, we propose a language-modeling approach and a likelihood ratio test to detect influence between documents in a statistically well-founded way. Furthermore, we show how this method can be used to infer citation graphs and to identify the most influential documents in the collection. Experiments on the NIPS conference proceedings and the Physics ArXiv show that our method is more effective than methods based on document similarity.
 H.4 [ Information Systems Applications ]: Miscellaneous Algorithms, Measurement, Performance Information Genealogy, Flow of Ideas, Language Models, Citation Inference, Text Mining, Temporal Data
In many domains, complete electronic records of docu-ments now reach back for over a decade, including computer science research papers, US news articles, and most peo-ple X  X  personal email. These databases incrementally grow Copyright 2007 ACM 978-1-59593-609-7/07/0008 ... $ 5.00. through an  X  X volutionary X  process, where new documents are influenced by the content of already existing documents. For example, scientific documents extend existing ideas, newssto-ries refine and comment on other articles, and emails aggre-gate or respond to other emails.

While keyword-based retrieval systems allow efficient ac-cess to individual documents in such corpora, we yet lack methods to understand the corpus as a whole. To rem-edy this shortcoming, this paper investigates whether it is possible to uncover the temporal dependency structure of a corpus. Which documents influenced each other? How did ideas spread through the corpus over time? Which docu-ments (or authors) were most influential? While many of these question have been addressed for hyperlinked data with explicit citation structure, explicit citations are not available in most domains. We therefore aim to address these questions based on the (textual) content of the docu-ments alone.

The premise for this research is that ideas manifest them-selves in statistical properties of a document (e.g. the dis-tribution of words), and that these properties can act as a signature for an idea which can be traced through the database. Following this premise, we present a probabilistic model of influence between documents and design a content-based significance test to detect whether one document was influenced by an idea first presented in another document. The test takes the form of a Likelihood Ratio Test (LRT) and leads to a convex programming problem that can be solved efficiently. Our goal is to use this test for inferring an influence graph derived from the text of the documents alone. Analogous to detecting inheritance from genes, we refer to this text-mining problem as Information Genealogy .
Using corpora of scientific literature, we show that it is in-deed possible to infer meaningful influence graphs from the text of the documents. Evaluating against the explicit cita-tion graphs for these corpora, we find that the automatically-computed influence graphs are similar to the citation graphs. The ablility to automatically generate an influence graph for a collection enables a range of applications, from browsing, to visualizing and mining the structure of the network. As a simple example, we demonstrate that the in-degree of the influence graph provides an interesting measure of document impact, similar to the in-degree of the citation graph.
In this paper, we investigate and operationalize the no-tion of influence between documents. Influence is an inter-esting relationship between documents in historically grown databases, since such corpora have grown through a self-referential process: documents are influenced by the con-tent of prior documents, but also contribute new ideas which in turn influence later documents. Our goal is to uncover and mine how ideas introduced in some document spread through the corpus over time.

At first glance, one might think that similarity, as cap-tured by information retrieval metrics like TFIDF cosine similarity (see e.g. [32]), provides the full picture of influ-ence. However, this is not the case.
 On the one hand, similarity can occur without influence. First, if a document d (1) introduces an idea that is picked up in documents d (2) and d (3) , then d (2) and d (3) will likely be similar but do not necessarily influence each other. Second, two documents might concurrently propose the same idea. Again, neither document influences the other although the documents likely are similar.

On the other hand, influence can occur with very little similarity. In the scientific literature, for example, a large textbook might devote a section to an idea introduced in an earlier research paper. Clearly, the paper had influence on the textbook. However, the overall similarity between the book and the paper is small, since the book covers many other ideas as well.

As we will briefly review in the following, most prior work on analyzing temporal corpora has focused on identifying relatedness between documents, not influence. We will then develop a probabilistic model and a statistical test for de-tecting influence, and show that it captures influence better than similarity and provides a more complete understanding and model of influence.
Topic Detection and Tracking (TDT) [5, 6] has the goal of grouping documents by topic. Unlike influence, which is a directed relationship, TDT aims to group documents into equivalence classes. While TDT approaches have relied heavily on finding similarity measures that capture closeness in topic, this approach is not necessarily detecting influence, as we have argued above. Methods that model influence not only can detect and track topics and ideas, but also can provide reference points for why a document collection developed as it did. Another minor difference is that the TDT studies were performed in an online setting, while we assume access to the full corpus at any time.

Similar work on detecting and visualizing topic develop-ment includes visualization methods such as Temporal Clus-ter Histograms [34] and ThemeRiver [15], EM-based cor-pus evolution detection [29], temporal clustering methods [7, 37], continuous time clustering models [37], Thread Decom-position [14], Independent Component Analysis [22], topic-intensity tracking [23], and Topical Precedence [27].
Research on Burst Detection [21] and TimeMines [36] aims to identify hidden causes based on changes in the word dis-tribution over time. However, their notion of influence is different from ours. These approaches determine influence from real-world events on topics (e.g., events influencing US State of the Union Addresses). Instead, we model the influ-ence of documents on each other.
In bibliometrics, a document X  X  influence is measured through properties of the citation graph [30, 31, 20, 12]. Our work differs from citation analysis because our method is based on document content, not on citations. We assume that in-fluence is inherently reflected in the statistical properties of documents. In particular, we conjecture that when one doc-ument influences another, the influenced document shows traces of the word distribution of the original document 1 Besides bibliometrics X  consideration of citation analysis on research papers, other methods work on general hyperlink structure. One of the most well-known such methods is PageRank [31], which uses hyperlink structure to find in-fluential Web pages.
There is related work on automatically adding hyperlinks in information retrieval and related fields. Most promi-nently, Link Detection was a key task in the TDT evalu-ations [5]. Several proposals and methods exist for intro-ducing hyperlinks between similar documents or passages of documents [11, 10, 33, 26, 2, 4, 3, 24, 25]. Good surveys are given in [38] and the 1997 special issue of Information Processing and Management [1]. The work we propose is different in several respects. First, our goal is to detect influence between documents, not just their  X  X elatedness. X  This will allow a causal interpretation of the resulting cita-tion graph. Second, we take a statistical testing approach to the problem of identifying influence links, which can be seen as synonymous to citations. This will give a formal semantic to the predictions of the methods, give theoretical guidance on how to apply the methods, and expose under-lying assumptions.
We take a probabilistic language modeling approach in the development of our methods. While we rely on a rather basic language model for the sake of simplicity, more de-tailed language models exist and can possibly be employed as well. Previous work by Steyvers et al. [35] looks at how document text can be generated by a two-step model of gen-erating topics probabilistically from authors, and then words probabilistically from topics. There has also been language modeling work done in the natural language processing and machine learning [28, 16, 8], speech recognition [19], and information retrieval communities [39, 24, 25].
In constructing an influence graph for a database of doc-uments, the core problem is to determine when and where ideas flow from one document to another document. In the following, we propose a probabilistic model of influence in a language-modeling framework, and develop a Likelihood Ratio Test (LRT) [9] for detecting whether one document has significantly influenced another document.
To make the method widely applicable, we have only two basic requirements for our corpus of documents  X  first, the documents contain text and, second, the documents have
Note that our goal is not plagiarism detection, where au-thors would try to disguise their choice of words. timestamps. Formally, the corpus D is a collection of n doc-uments { D (1)  X  X  X  D ( n ) } , where each document D ( i ) an associated timestamp time ( D ( i ) ). There are m differ-ent terms (i.e. words) across the entire corpus, which are denoted by { t 1  X  X  X  t m } .

We assume that the document is a vector-valued random variable D = ( W 1  X  X  X  W | D | ), which describes a document as a sequence of random variables W i , one for each word in the document. A particular observed document is denoted as d = ( w 1  X  X  X  w | d | ). In the following, we assume that each document D ( i )  X  X  was generated by a unigram language model P ( D ( i ) = d ( i ) |  X  ( i ) ) with parameters  X  that document.
 Model 1. (Document Language Model)
A document D ( i )  X  X  is assumed to be generated by independently drawing | D ( i ) | words from a document spe-cific distribution with individual word probabilities  X  ( i ) (  X  Note that we do not explicitly model document length. We chose this basic language model for mathematical and computational convenience. However, our approach can be extended to more complex language models as well (e.g. n-gram models).

Since we wish to detect the flow of ideas and influence between documents, we also need a model of inter-document relationship. We formalize this as a question of how the language model  X  ( new ) of a new document D ( new ) depends on the language models {  X  (1)  X  X  X } of the documents that precede  X  ( new ) in time. In particular, we assume that the language model of a new document can be (approximately) expressed as a mixture distribution over the language models of previous documents.
 Model 2. (Inter-Document Influence Model)
A new document D ( new ) is generated by a mixture distri-bution of the already existing documents D ( i ) with i  X  X  for P = { i : time ( D ( i ) ) &lt; t 0 } , in particular with mixing weights  X  satisfying 0  X   X  i and P
In this dependency model, a new document is composed of parts generated by the word distributions of old docu-ments, where the mixing coefficient  X  p indicates the frac-tion of D ( new ) that is generated from D ( p ) . Clearly, there is direct influence of a document D ( p ) on D ( new ) , if the respec-tive mixing coefficient is non-zero. Note that the resulting language model for D ( new ) is again a unigram model, so that
In actual document collections, documents typically con-tain some original part that does not come from previous documents. To account for the original portion of a docu-ment in our model, we include a distribution  X  ( o ) with weight  X  o in the mixture. It models the distribution of words that is original to the document and that cannot be explained by previous documents. (In practice, we will assume that  X  o fixed, but that we have no knowledge of  X  ( o ) . Model 3. (Inter-Document Influence Model with Original Content)
A new document D ( new ) is generated by a mixture distri-bution of the already existing documents D ( i ) with i  X  X  for P = { i : time ( D ( i ) ) &lt; t 0 } , and a document specific mixture component  X  ( o ) with weight  X  o , in particular with mixing weights  X  s.t. 0  X   X  i ,  X  o and  X  o + P
In the case when the documents have no original content, setting  X  o = 0 in the Inter-Document Influence Model with Original Content results in Model 2. Vice versa, Model 2 also subsumes Model 3 by simply introducing an artificial single-word document for each term in the corpus and con-straining their mixture weights to sum to  X  o . We will there-fore focus our further derivations on Model 2 for the sake of simplicity.

We will now show how this probabilistic setup can be used in a significance test for detecting whether a particular mixing weight  X  p is non-zero in a given document collection.
How can one decide whether a candidate influential doc-ument d ( can ) had a significant influence on d ( new ) given the other documents in the collection? First, d ( can ) can only have had an influence on d ( new ) if it had been pub-lished before d ( new ) (i.e. time ( d ( can ) ) &lt; time ( d that this is already encoded in the Inter-Document Influ-ence Models defined above. Second, influence should be attributed to the first publication that introduced an idea through an original section or portion, not to other docu-ments that later copied an idea. To illustrate this in the context of research papers, this means that influence should be credited to the original article, not a tutorial that repro-duced the original idea.

Under these conditions, the decision of whether document d ( new ) shows significant influence from d ( can ) can be phrased as a Likelihood Ratio Test [9]. In general, a Likelihood Ratio Test decides between two families of densities described by sets of parameters  X  and  X  0 that are nested, i.e.  X  0  X   X . Applied to our case,  X  will be all mixture models of D ( new ) as in Eq. (1) with parameters  X  i for all documents P published prior to t 0 = time ( d ( can ) ) (and therefore prior The subset  X  0 of the mixture models in  X  will be the models where d ( can ) has zero mixture weight (i.e.  X  can = 0). Note that the set of prior documents P = { i : time ( d ( i ) time ( d ( can ) ) } serves as a  X  X ackground model X  of what was already known when d ( can ) was published. Against this background, we can then measure how much the new ideas
The null hypothesis of the Likelihood Ratio test is that the data comes from a model in  X  0 (i.e. document d ( new ) was not influenced by d ( can ) given the documents published before d ( can ) ). To reject this null hypothesis, a likelihood ratio test considers the following test statistic Note that P ( D ( new ) = d ( new ) |  X  ) is convex over  X  and  X  so that the suprema can be computed efficiently. We will elaborate on the computational aspects below. Intuitively, the mixture model better explains the content of d ( new ) just using previously published documents. More formally,  X  d ( new ) |  X  0 ) } of the best mixture model containing d ( can ) the likelihood sup  X   X   X  mixture model that does not use d ( can ) (i.e.  X  can = 0). The test then decides whether there is significant evidence that a non-empty part of d ( new ) was generated from d ( can ) comparison to using a mixture only over the other language models.
 If the null hypothesis is true, then the distribution of the LRT statistic  X  2 log( X  d ( can ) ( d ( new ) )) is asymptotically (in the document length under the unigram model)  X  2 with one degree of freedom. The null hypothesis H 0 should be rejected, if for some c selected dependent on the desired significance level. For a significance level of 95%, c should be 3.84. This captures the intuition that we can reject the null hypothe-sis and conclude that d ( can ) had a significant influence on d ( new ) , if the best model that does not use d ( can ) has a much worse likelihood than the best model that considers d ( can ) . Specifically, if  X  2 log( X  d ( can ) significantly influenced d ( new ) given all other docu-ments published at that time.

To estimate the language models  X  ( i ) of the documents en-tering into the mixture model of d ( new ) , we use the maximum-likelihood estimate. We denote with tf ( i ) the term frequency (TF) vector of document d ( i ) , where each entry tf ( i ) number of times that term t j appears in the document d ( i ) The estimator is which is simply the fraction of times the particular word oc-curs in the observed document d ( i ) . Using a more advanced estimator instead is straightforward, but we will not discuss this for the sake of simplicity.
What does it mean for the LRT to significantly reject the null hypothesis? A good intuition is to think of this method in the context of trying to explain the ideas and content found in d ( new ) . There are two choices. First, ex-plain d ( new ) using only other documents preceding d ( can ) well as some original component. Second, explain d ( new ) with these plus an additional d ( can ) . If the first case already provides a wonderful model for d ( new ) , then adding d ( can ) will not explain d ( new ) any more accurately. Thus, d ( can ) really does not contribute to d ( new ) . On the other hand, if d ( can ) introduced some new ideas and terminology that then flowed to d ( new ) , using d ( can ) will provide a better ex-planation than only using P . Consequently, the likelihood of d ( new ) using d ( can ) will be significantly higher than with-out it, and we can reject the null hypothesis. To summarize, rejecting the null hypothesis means that d ( can ) significantly exerted influence on d ( new ) . two optimization problems.
 Given our model, these problems can be solved efficiently. Note that we can write the log-likelihood L (  X  | d ( new ) the document d ( new ) w.r.t. a fixed  X  as With S we denote the set of documents considered in the model. This gives S = P X  X  can } for  X  and S = P for  X  0 . In this notation, each of the optimization problems in Eq. (4) and (5) takes the form For Model 3 an additional linear constraint is introduced to limit the amount of original content  X  o to not be more than a user-specified parameter  X  . This constraint is necessary, since otherwise the  X  ( o ) mixture component could always perfectly explain d ( new ) .

It is easy to see that these optimization problems are con-vex, which means that they have no local optima and that there are efficient methods for computing the solution. We currently use the separable convex implementation for the general-purpose solver Mosek [18] to solve the optimization problems. However, more specialized code is likely to be substantially more efficient.

While solving each optimization problem is efficient, an-alyzing a collection requires a quadratic number of LRTs, each with on the order of n documents in the background model. In particular, for each document d ( new ) , we need to test all prior documents in the collection, since all of these are candidates for having influenced d ( new ) . For each document d ( can ) in the candidate candidate set C of d ( new ) , we then have a background model Computing all tests exhaustively for a large corpus can be expensive. We therefore use the following approximations.
Both approximations are based on the insight that some similarity is necessary for influence. The potentially influen-tial document d ( can ) must have some similarity with d ( new ) Therefore, we first approximate the candidate set to con-tain the k C nearest neighbors of d ( new ) from C . We use cosine distance between TF and TFIDF vectors for docu-ment similarity. Second, an analogous argument applies to the background models P d ( can ) . We therefore approximate the background model, using only the k P most similar docu-ments from P . Since selecting P combines document vectors by addition, we use cosine distance between document TF vectors to select P . In the experiments we set k C = k P refer to this parameter as k . We will empirically evaluate the effect of these approximations depending on k .
We wish to measure how well these models X  assumptions match real data. First, how does an influence graph inferred by the LRT method compare against a citation graph? Sec-ond, can the influence graph identify top influential papers?
The concept of influence and idea flow between documents corresponds well with the notion of a citation. Consequently, we focus on research papers to provide a quantitative eval-uation of the LRT method by comparing with citations.
The first corpus is the full-text proceedings of the Neu-ral Information Processing Systems (NIPS) conference [17] from 1987-2000, with a timestamp of the publication year. NIPS has 1955 documents, with 74731 terms (features). We manually constructed the graph of 1512 intra-corpus cita-tions, but only compare to citations of previous documents in time. We ignore citations of first-year documents since the LRT requires a background model.

The second corpus is the theoretical high-energy physics (HEPTH) section of the Physics ArXiv [13] from Aug. 1991 to Apr. 2006. We aggregate the full-text papers by year. HEPTH has 39008 documents, 229194 terms, and 557582 citations. SLAC-SPIRES compiled these citations.
This set of experiments analyzes how well the LRT re-covers the influence graph. After an illustrative example, we explore the LRT X  X  sensitivity on synthetic data under controlled experiment conditions, and then evaluate on two real-world datasets.
We first discuss a simple example to illustrate the LRT method X  X  behavior and how it compares to citations. Fig-ure 2 shows those documents that NIPS document 1541 Figure 1: ROC-Area comparing the LRT method against a cosine similarity baseline. The x-axis is  X  can . At a  X  can level, the ROC-Area measures the quality of influence prediction in documents with the specified  X  can as compared against documents with  X  (Schoelkopf et al. on  X  X hrinking the Tube: a New Sup-port Vector Regression Algorithm X ) most significantly in-fluenced according to the LRT statistic. Three of the top five papers actually cite document 1541 (or a document with equivalent content from another venue). Furthermore, the top document could arguably have cited 1541 as well, since it relies on the  X  -parameterization of SVMs that document 1541 introduced to NIPS. In fact, all papers (except  X  X ast Training of Support Vector Classifiers X ) consider this new parameterization. Note that the paper  X   X  -arc: Ensemble Learning in the Presence of Outliers X  is not about SVMs, but uses the  X  -parametrization in the context of boosting.
The LRT appears to accurately focus on the paper X  X  origi-nal contribution, the  X  -parameterization. General SVM pa-pers do not score highly, since they are already modeled by earlier papers, e.g. paper 1217  X  X upport Vector Method for Function Approximation, Regression Estimation, and Sig-nal Processing X  of V. Vapnik et al., which was one of the first SVM papers in NIPS. When considering influencers of  X  X  Support Vector Method for Clustering X  by A. Ben-Hur et al. (using the conventional parameterization), the method correctly recognizes that paper 1541 X  X  influence is low (  X  2 log( X  d (1541) ( d ( new ) )) = 67 . 0) even though the docu-ments are similar. Paper 1217 already  X  X xplains X  the SVM Beyond this qualitative example, how accurately can the LRT discover influence? How much must d ( new ) copy from d ( can ) before the LRT can detect it?
To explore these questions, we constructed artificial docu-ments d ( new ) from the NIPS corpus as follows. A candidate document d ( can ) and a set P of k = 100 previous documents are chosen at random form the NIPS corpus so that the documents in P preceed d ( can ) in time. Then, 101 artificial new documents are generated according to Eq. 1, where each new document has been influenced by d ( can ) at the fractional mixing weights  X  i are selected by generating random num-bers uniformly on the interval [0 , 1], and then normalizing them so that they sum to 1  X   X  can . The LRTs are run on each new document. Additionally, TF document vector co-sine similarity is measured between d ( can ) and each d ( new ) The entire process is repeated for 1000 random selections of
We computed ROC-Area in the following manner. First, we select a particular  X  can  X  X  0 . 01  X  X  X  1 . 00 } . The generated documents at the  X  can level are marked as positive exam-ples. The negative examples are documents with  X  can = 0. Finally, a ranking, either LRT statistic scores or cosine dis-tance similarity, is used to compute ROC-Area.

Figure 1 shows that even if only a small portion (i.e. a few percent) of d ( new ) is drawn from d ( can ) , the LRT accu-rately detects the influence. The similarity baseline needs a much larger signal. This example illustrates that similarity and influence are in fact different, and that the well-founded statistical approach can be more accurate and sensitive than an ad-hoc heuristic.
Moving to real data, we use the LRTs to discover the in-fluence graph for NIPS and HEPTH. For each document d ( new ) , we first compute a set of candidate documents C based on similarity. The elements of C are then ranked ac-cording to the LRT statistic (i.e. whether d ( can ) was signif-icant in explaining d ( new ) ). The higher d ( can ) is ranked, the more likely that it influenced d ( new ) , and we can derive the influence graph by thresholding (discussed below).
We evaluate the influence graph by a graph-based mean-average-precision (G-MAP) metric. For a document d , aver-age the precision of the ranked predicted list of influencers at the positions corresponding to documents that d actu-ally cites. Citations not in the list are averaged as 0, i.e. ranked at infinity. (As an information retrieval analogy, the influence list is the search result page, with citations being relevant results.) G-MAP is the mean of the per-document average precision scores. We exclude documents from the first two years due to edge effects (the LRT cannot predict citations for the first years since C or P are empty).
We compare G-MAP for the LRT method against G-MAP of a similarity-based heuristic, which serves as a baseline. This baseline method ranks the elements of C not by LRT score, but by similarity. We explored several similarity mea-sures. The best similarity measures in our experiments are TF cosine and TFIDF cosine. We report their performance. Note that citations are not necessarily a perfect gold stan-Table 1: G-MAP scores comparing the LRT against the similarity baseline. The similarity measure to select P is the TF cosine and to select/rank C is either the TF cosine or the TFIDF cosine. Results are reported for k = 100 and  X  = 0 . 05 . Figure 3: Precision vs. Recall on NIPS. The three lines are (from top to bottom) the LRT method X  X  precision at a recall level with TFIDF cosine used to select C , the TFIDF distance C similarity baseline, and the TF distance C similarity baseline. dard for influence, since they reflect idiosycracies of how sci-entific communities cite prior work. For example, in Figure 2 authors sometimes cited a journal paper or book instead of the NIPS paper. Therefore, a G-MAP of 1 is not achievable.
Table 1 shows that the LRT achieves higher G-MAP scores than the similarity baselines on both NIPS and HEPTH. Among the two heuristic baselines, TFIDF cosine performs better then TF cosine. TFIDF cosine also appears to select better sets C for the LRT. The HEPTH results are reported for a random sample of 1600 documents.
 Table 2: G-MAP scores comparing the LRT for a range of d ( can ) influence mixing weights  X  against the similarity baseline. The similarity measure to select C is either TF or TFIDF cosine. Results are reported on NIPS for k = 100 .
 Table 3: G-MAP scores comparing the LRT against the similarity baseline for two k -NN approximation levels. The similarity measure for selecting C is ei-ther TF or TFIDF cosine. Results are reported on NIPS and HEPTH for  X  = . 05 .

Table 1 showed that the LRT can find the most influential papers for one particular document. Figure 3 measures how well it can find the strongest edges in the whole influence graph. This precision-recall graph uses the ranking of all LRT statistic scores of all documents, with actual citations marked as positive examples. Figure 3 also shows the scores for using lists of TF and TFIDF cosine similarities. The LRT graph dominates the similarity baselines over the whole range and the difference in performance is larger than in the per-document evaluation. We conclude from this that LRT scores are more comparable between documents than similarity scores. This is to be expected because the LRT values have a clear probabilistic semantic. However, the similarity scores have no such guarantees.

Table 2 shows that the LRT is robust over a large range  X  values. The LRT X  X  G-MAP dominates the similarity base-lines. However,  X  = 0 . 01 seems to perform better than our initial guess of 0 . 05 used above.
 Table 3 shows G-MAP scores at differing levels of the k -NN approximation. Recall from Table 1 that G-MAP scores for HEPTH are substantially lower than for NIPS. We con-jecture that this is due to the size of the corpus in relation to k . With a large corpus, k = 100 is likely to exclude too many relevant documents from consideration. We further analyze the role of k , in its two roles in controlling the sizes of C and P .

First, k controls the size of C . If k is too small, truly influential documents will not be tested by the LRT. E.g., in HEPTH, each document has 14 citations on average. With k = 10, it would be simply impossible to recover the entire citation graph. Therefore we conclude that k must be large enough to include all documents that make contributions to d ( new ) . On HEPTH, k = 100 is better than k = 20 for TF Table 4: How close is the approximation to the op-timal? G-MAP scores are reported for S = . 05 . and TFIDF cosine, and for LRT and similarity baseline. We believe this is because k = 20 is too restrictive. NIPS with TF cosine shows the same behavior.

To better understand how much loss in performance is due to the k -NN approximation of C , the following experiment explores the G-MAP scores of the LRT for a  X  X erfect X  C . In particular, we construct C so that it includes all documents that d ( new ) actually cites, and then fill the remaining places in C with the most similar documents. Table 4 shows that for k = 100 the loss in performance due to an approximate C is fairly small on NIPS. For HEPTH, on the other hand, k = 100 shows a much greater loss, with G-MAP scores only about 60-65% of the optimal. We believe this loss oc-curs because C is too small to accomodate all the influential documents.
What are the influential documents that have the most effect on the document collection X  X  development? Which documents should one read to best grasp this development? We have already shown that LRTs can be used to infer an influence graph that is similar to a citation graph. We now investigate whether this influence graph can be used to iden-tify the documents with the overall largest influence on the collection. In analogy to citation counts (i.e. the in-degree in the citation graph), we propose the in-degree in the influ-ence graph as a measure of impact. If not noted otherwise, we form the influence graph by connecting each document d ( new ) with the l other nodes that receive the highest LRT value. We typically use l = 10, although we also explore this parameter X  X  effect on performance.
For each year in NIPS, Table 5 lists the paper with the highest in-degree in the influence graph computed by the LRT method with k = 100 and l = 10. We expect these to have high citation counts, which we test by showing the paper X  X  citation counts both from within the NIPS corpus (as of 2000) and from Google Scholar (as of 2007). For most documents, the citation count is indeed high when compared to the average NIPS document citation count of 0.7734 other NIPS papers. An interesting example is  X  X up-port Vector Method for Function Approximation, Regres-sion Estimation, and Signal Processing X  from 1996. While this is one of the papers that introduced SVMs to NIPS, it has only 3 citations within NIPS and only 44 citations in Google Scholar. Nevertheless, SVMs had a huge impact on NIPS. In this sense our LRT method is correct and is not influenced by citation habits. In this example, most au-thors cite Vapnik X  X  later book (with 5144 citations) instead of this paper. The LRT method is unaffected and correctly identifies the SVM idea as highly influential on NIPS. citation counts (on Feb. 28, 2007).

We compare the ranking of documents by in-degree in the influence graph to the ranking by citation count. As simi-larity measures, we use Kendall X  X   X  and a ranking version of MAP, which we term R-MAP.

Kendall X  X   X  measures how many pairs two rankings rank in the same order. It ranges between -1 and 1, with higher numbers indicating greater similarity. Formally, R-MAP@ k measures the average precision of a ranking. With the k top-ranked documents as positive examples, av-erage the ranking X  X  precision at the positions of these docu-ments. We calculate R-MAP@3 and R-MAP@12.

There is one caveat with rank-based metrics. Edge effects (e.g., older papers have more citations, papers from the last year have no citations) make it difficult to present one uni-fied ranking of all documents. Therefore, we calculate each metric per-year and average the year-by-year values to get a single score for the entire corpus. Additionally, because of edge effects, the first two and the last two years are not used, since they do not contain meaningful results.
The TF and TFIDF baselines use the most similar docu-ments instead of the LRT predictions.

Table 6 shows that the LRT gives substantially better rankings than the similarity baseline for all metrics on both NIPS and HEPTH with both TF and TFIDF cosine C .

The left plot of Figure 4 explores whether selecting in-fluencers is sensitive to the parameter l . For the influence graph, we considered each document X  X  l predicted influencers with highest LRT scores. Figure 4 shows how varying l af-fects  X  for both LRT and the similarity baseline. Since NIPS documents do not have many citations, we explore l = 1 to 15. The upper line is LRT performance with 95% confi-dence interval error bars. (The confidence interval is com-puted using the multiple  X  values per data point, because each graphed  X  is the average of multiple (here, 10) years of  X  metric scores.) The lower line depicts  X  on the similar-ity baseline. For the TFIDF cosine C , when l is small, the method computes a count over only the few top influential documents selected by the LRTs for d ( new ) . It turns out that small l seem to perform better than our initial guess of l = 10. As l increases, more non-influential documents are counted and  X  correspondingly falls. When l approaches 100 (not shown), the LRT and the baseline are identical as expected by construction.

The right plot of Figure 4 depicts how  X  varies if we do not select a fixed number of l neighbors per document, but instead use a threshold on the LRT statistic. The LRT is set up to reject the null hypothesis and declare that d ( can ) influences d ( new ) if the LRT statistic is sufficiently large. Varying this threshold controls the level of confidence in the LRT, so we use the threshold level as the x-axis and examine how it affects  X  . Thresholding the LRT values actually gives better performance than using the l parameter, since we are not forcing a certain number of influence links for each doc-ument. There are four different regions in this graph. First, TF plot looks similar, except that the baseline is smoother. if the threshold is too low, performance suffers because the null hypothesis is being accepted erroneously. Second, per-formance increases as the threshold approaches reasonable confidence levels. Third, a large range of threshold values (approximately 100-2000) give good and similar  X  scores, showing that the LRT method is robust. Fourth, when the threshold is too high, many influential documents are no longer detected, and performance subsequently falls.
Note that a confidence level of 95% per test (i.e. a thresh-old of 3.84) performs quite poorly. This level means that 5% of the influence links are erroneous. NIPS, with 2000 papers, would have an expected 100,050 false links (and only 1512 real citations). Therefore, we need a much higher confidence level to account for the multiple-testing bias. Using Bonfer-roni adjustment, each test X  X  level is the overall level divided by the number of tests.
One obvious limitation of the current model is the sim-plicity of the language model. The assumption that each document is a sequence of independent words is, in reality, likely violated. This observation motivates more expressive language models such as n -gram language models.

There is also the question of whether these methods can generalize to other domains. LRTs do not use citation data, so many domains should be applicable. However, we have only conducted experiments on research publications.
Finally, there is scalability and efficiency. Much of the computing time is spent solving convex optimization prob-lems. While C and P prune this space, there may be other criteria to provably eliminate certain LRTs without affecting the results. Furthermore, the optimization problems have a special structure, which can probably be exploited by spe-cialized methods to solve the optimization problems.
We presented a probabilistic model of influence between documents for corpora that have grown over time. In this model, we derived a Likelihood Ratio Test to detect influ-ence based on the content of documents and showed how the test can be computed efficiently. We found that the influence graphs derived from the content resemble the structure of ex-plicit citation graphs for corpora of scientific literature. Fur-thermore, we showed that in-degree in the influence graph is an effective indicator of a document X  X  impact. The ability to create influence graphs based on document content alone has the potential to open databases without explicit citation structure to the large repertoire of graph mining algorithms.
We thank Johannes Gehrke and Rich Caruana for the dis-cussions that lead to this work. This work was funded in part by NSF Career Award IIS-0237381, NSF Award OISE-0611783, and the KD-D grant. [1] M. Agosti and J. Allan. Introduction to the special [2] M. Agosti and F. Crestani. A methodology for the [3] M. Agosti, F. Crestani, and M. Melucci. On the use of [4] J. Allan. Automatic Hypertext Construction . PhD [5] J. Allan, J. Carbonell, G. Doddington, J. Yamron, [6] J. Allan, R. Papka, and V. Lavrenko. On-Line New [7] D. Blei and J. Lafferty. Correlated topic models. In [8] D. Blei, A. Ng, and M. Jordan. Latent dirichlet [9] G. Casella and R. L. Berger. Statistical Inference , [10] J. H. Coombs. Hypertext, full text, and automatic [11] R. Furuta, C. Plaisant, and B. Shneiderman. A [12] E. Garfield. The Meaning of the Impact Factor. [13] P. Ginsparg. The physics e-print arxiv. [14] R. Guha, D. Sivakumar, R. Kumar, and R. Sundaram. [15] S. Havre, B. Hetzler, and L. Nowell. ThemeRiver: In [16] T. Hofmann. Probabilistic latent semantic analysis. In [17] http://nips.djvuzone.org/txt.html. NIPS Online: The [18] http://www.mosek.com/index.html. Mosek. [19] F. Jelinek. Statistical Methods for Speech Recognition , [20] J. Kleinberg. Authoritative Sources in a Hyperlinked [21] J. Kleinberg. Bursty and Hierarchical Structure in [22] T. Kolenda, L. K. Hansen, and J. Larsen. Signal [23] A. Krause, J. Leskovec, and C. Guestrin. Data [24] O. Kurland and L. Lee. Corpus structure, language [25] O. Kurland and L. Lee. Respect my authority! hits [26] A. Lelu. Automatic generation of hypertext links in [27] G. Mann, D. Mimno, and A. McCallum. Bibliometric [28] C. D. Manning and H. Schuetze. Foundations of [29] Q. Mei and C. Zhai. Discovering Evolutionary Theme [30] F. Osareh. Bibliometrics, Citation Analysis and [31] L. Page, S. Brin, R. Motwani, and T. Winograd. The [32] G. Salton and C. Buckley. Term weighting approaches [33] G. Salton and C. Buckley. Automatic text structuring [34] B. Shaparenko, R. Caruana, J. Gehrke, and [35] M. Steyvers, P. Smyth, M. Rosen-Zvi, and [36] R. Swan and D. Jensen. TimeMines: Constructing [37] X. Wang and A. McCallum. Topics over time: A [38] R. Wilkinson and A. F. Smeaton. Automatic link [39] C. Zhai. Risk Minimization and Language Modeling in
