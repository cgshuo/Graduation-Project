 Data mining aims to discover implicit, previously unknown, and potentially use-ful knowledge from data (e.g., shopper market, social network data). Common data mining tasks include classification [11], social network mining [10,23,25], and frequent itemset mining [2]. The latter finds those frequently co-occurring items, objects, or events. Over the past two decades, numerous frequent itemset mining algorithms [8] have been proposed. While many of them are designed to mine precise data in which the existence of data items is certainly known, there is also demand for mining uncertain data (e.g., sensor network data, clinic re-ports) [9,14,15,19,30] in many other real-life applications. In these applications, each item is associated with an existential probability expressing the likelihood of the presence of that item. See Table 1 for a sample database consisting of 5 transactions of uncertain data. In comparison, the traditional precise data can be seen as being present with a 100% possibility.

To mine uncertain data, many existing algorithms use expected support based mining [7] together with the  X  X ossible worlds X  semantics. An itemset X is frequent if its expected support meets or exceeds the user-specified minsup threshold; i.e., expSup ( X )  X  minsup . Over the past few years, researchers have extended precise frequent pattern mining algorithms to accommodate uncer-tainty [13,16,17,18,22].

In addition to expected support based mining, some other existing algorithms use probabilistic based mining [3,4,21]. An itemset is probabilistic frequent if the probability of its existence in at least minsup transactions (i.e., frequent-ness probability) meets or exceeds the user-specified minprob threshold; i.e., P ( sup ( X )  X  minsup )  X  minprob .The frequentness probability P ( X ), which can be derived from a support probability distribution ( spd ), computes the prob-ability that X occurs in at least minsup transactions. Existing probabilistic based mining algorithms mostly apply a candidate generate-and-test approach (i.e., Apriori-based). To prune candidates early, Sun et al. [24] used Chernoff bound [29]. However, their algorithm is designed to handle tuple-level uncer-tainty (instead of attribute-level uncertainty). Besides these exact algorithms, others adopt the normal distribution [26,28] or Poisson distribution [6] to ap-proximate the spd for a shorter runtime. However, these approximate algorithms only discover very likely probabilistic frequent itemsets, which means they may miss some probabilistic frequent itemsets and may find some false positives. To avoid generating-and-testing candidates, a tree-based algorithm called ProFP-Growth was proposed [5]. However, it s uffers from another problem X  X amely, large memory consumption. To reduce computation, some algorithms [20,27] find probabilistic closed frequent itemsets.

Tong et al. [26] surveyed and reported that, when mining expected support based frequent itemsets, hyperlinked structure-based algorithms outperformed both Apriori-based and tree-based algorithms. In this paper, we propose two algorithms for probabilistic uncertain hyperlinked structure-based mining to find all and only those probabilistic frequent itemsets. Our key contributions include: 1. a hyperlinked structure-based algorithm called B-PUH, which finds exactly 2. a novel upper bound of frequentness probability to prune infrequent itemsets 3. a variation of basic B-PUH called VI-PUH, which pushes the pruning step The remainder of this paper is organized as follows. The next section provides some background. In Section 3, we present the calculation of frequentness proba-bility and its upper bound. We then describe our proposed B-PUH and VI-PUH algorithms in Section 4. Finally, experimental results and conclusions are given in Sections 5 and 6, respectively. Let (i) I be the set of all possible items and (ii) T represent the uncertain database, where a transaction t j  X  T is a set of uncertain items, i.e., t j  X  I . Unlike the traditional precise database, each item x i  X  t j is associated with an existential probability P ( x i ,t j )  X  (0,1], which denotes the likelihood that x i is truly present in t j . For an itemset X sumption that items in X are independent [1,7,12], the existential probability P ( X  X  t j )= x  X  X P ( x, t j ). The expected support of X is then the sum of existential probability over all transactions expSup ( X )= t The frequentness probability P ( X )[4]of X can be computed by summing P ( X ) for all i  X  minsup : where P i ( X ) records the probability that X occurs in exactly i transactions: If P ( X )  X  minprob ,then X is a probabilistic frequent itemset .

Given (i) an uncertain transactional database T , (ii) a minimum support threshold minsup and (iii) a minimum frequentness probability threshold min-prob , the research problem of probabilistic frequent itemset mining is to find all and only those probabilistic frequent itemsets; i.e., find every X such that P ( X )  X  minprob .

Recall that frequent itemsets can be min ed from uncertain data using Apriori-based, tree-based, and hyperlinked structure-based approaches. As an example of the latter, the UH-Mine algorithm [1] adopts a pattern-growth manner with a hyperlink data structure called UH-struct, which is a two-dimensional array. Every row in the structure represents a tra nsaction. Every element in the row cor-responds to an item, which contains (i) an item ID, (ii) its existential probability and (iii) a pointer linked to the next transaction within the same partition. UH-Mine first scans the database once to construct the UH-struct. Consequently, all the singleton frequent itemsets are discovered, and infrequent ones are re-moved. Then, a header table keeps the information of the items that could be extended in the next step. Every item i in the header table consists of (i) an item ID, (ii) its expected support and (iii) a pointer to the head of the corre-sponding projected list. The support of the itemset P  X  X  i } can be computed by following the links from its header. If the new itemset is frequent (with the expected support  X  minsup ), further extensions can be recursively mined within the projected database of P  X  X  i } . To compute the frequentness probability with the spd, we adopt the approach of a generating function [5] with some modifications as follows. Consider a generat-spd (i.e., the probability that X is present in exactly i transactions in the whole database T ).
 Theorem 1. Given an uncertain transactional database T , the probability that an itemset X is frequent among the first j transactions, denoted by P j ( X ) ,can be recursively computed as follows: where The frequentness probability of X is P | T | ( X ) .
 Example 1. Consider the uncertain database shown in Table 1. Let minsup =2 and minprob =0.5. So, we keep two coefficients: c 1 and c 0 . The singleton item-t , P 1 ( { A } )=0+0  X  0.4=0usingEq.(3).AccordingtoEq.(4), c 1 1 =1  X  0.4 +0  X  0.6 = 0.4 and c 1 0 =1  X  0.6 = 0.6. Next, P 2 ( { A } )=0+0.4  X  1.0 = 0.4, c 1 =0.6 0.4  X  0=0.6and c 3 0 =0  X  0.5 = 0. Finally, the frequentness probability P ( { A } ) = P 4 ( { A } ) = 0.7 + 0.3  X  0.2 = 0.76. Hence, { A } is probabilistic frequent.
Observe from Example 1 that we only need one multiplication and addition to compute P j ( X )(cf. minsup additions for each transaction in many existing c etc., for to all the coefficients c i for i&lt; minsup . This results in a complexity of O( | T | X  minsup ). However, the frequentness probability computed by Eq. (3) is monotonically in creasing. So, once P j ( X )  X  minprob , X is guaranteed to be probabilistic frequent. For instance, in Example 1, we can stop the calculation after t 4 because P 3 ( { A } )=0.7 &gt; minprob . 3.1 Our Enhancement #1: The 1-Upper Bound Here, we introduce our first upper bound to prune infrequent itemsets early without the expensive computation of exact frequentness probability. The upper bound UB ( P j ( X )) to P j ( X ) shown in Eq. (3) can be computed by
To avoid updating the entire coefficient array (from c 0 all the way up to c minsup  X  1 ) in each iteration during the computation of c c the first j transactions. Note that j  X  1 i =0 c j  X  1 i =1and We can recursively compute this upper bound. Otherwise, (1  X  2 P ( X  X  t j )) becomes negative. This leads to the following definition. We call it 1-upper bound (or 1-UB in short) because the sum of c j i equals 1 when j is fixed. Definition 1. The 1-upper bound of c j minsup  X  1 can be computed as Example 2. Revisit Example 1 by substituting Eq. (7) into Eq. (5). Initially, We skip t 3 (from which A is absent). Afterwards, as P ( { A } X  t 4 )=0.5  X  0.5, P ( { A } X  t potentially probabilistic frequent. Again, we can the stop the calculation after t because UB ( P 3 ( { A } ))=0.9 &gt; minprob . 3.2 Our Enhancement #2: The Subset Upper Bound from the problem that the upper bound to c minsup minsup  X  1 could jump to at least P ( X  X  t minsup ). In order to reduce this gap, we propose another definition. We call it subset upper bound (or SUB for short) because of i ts subset-selection nature. See Lemma 1 and Definition 2.
 Definition 2. The subset upper bound of c j minsup  X  1 can be computed as Example 3. Revisit Examples 1 and 2 by substituting Eq. (8) into Eq. (5). Af-1 . 5. Next, UB ( P 2 ( { A } ))=0+0.4  X  1.0 = 0.4 and UB ( c 2 1 ) = 0.4(1  X  1.0 + ample 2, and this difference leads to a tighter upper bound to frequentness prob-(from which A is absent). Afterwards, UB ( P 3 ( { A } ))=0.4+0.6  X  0.5 = 0.7 and UB ( P 4 ( { A } ))=0.7+0.525  X  0.2 = 0.805. Hence, { A } is potentially probabilistic frequent. 3.3 Our Enhancement #3: The Combined Upper Bound Note that, the 1-UB sometimes provides a tighter upper bound to c j  X  1 minsup  X  1 than the SUB, and the SUB sometime provides a tighter bound than the 1-UB. Hence, a logical solution is to incorporate these two bounds by picking their minimum in each iteration. This leads to the combined upper bound (or CUB for short).
 Definition 3. The combined upper bound of c j minsup  X  1 can be computed as UB ( c j minsup  X  1 ) in Eq. ( 9 ) into Eq. ( 5 ) leads to the following:
To compute the CUB, we need to update (i) UB ( P j ( X )) with Eq. (5), would be O( | T | ), which is much better than the complexity O( | T | X  minsup )of computing the spd and exact frequentness probability. In this section, we propose two algorithms for mining p robabilistic frequent item-sets from u ncertain data using the h yperlinked structure-based mining model. The first algorithm X  X alled Basic PUH (B-PUH)  X  X irst finds all the poten-tial probabilistic frequent itemsets using the upper bound proved in Section 3, and then verifies the candidates with a database scan at the end of the min-ing process. The second algorithm X  X alled Verified Immediately PUH (VI-PUH)  X  X erifies the candidate as soon as it is generated. 4.1 Our Basic PUH (B-PUH) Mining Algorithm On the one hand, like UH-Mine described in Section 2, our B-PUH algorithm also uses a hyperlinked structure. On the other hand, unlike UH-Mine, our pro-posed B-PUH algorithm computes the Combined Upper Bound (CUB) of fre-quentness probability UB ( P ( X )) using Definition 3. In addition, we also cap-called the PUH-struct . Fig. 1 shows the global PUH-struct built for the uncer-tain database in Table 1. As we only compute an upper bound (instead of the exact frequentness probability), what we discover from the mining process is a superset of the truly probabilistic frequent itemsets. To remove those false posi-tives, we quickly scan the database once more to compute the exact frequentness probability with the method described in Section 3. 4.2 Our Verified Immediately PUH (VI-PUH) Mining Algorithm Recall from Section 3 that the computation of exact frequentness probability for a single itemset requires O( | T | X  minsup ) runtime. However, we also observed that the use of upper bounds such as CUB (instead of exact frequentness proba-bility) may increase the number of false positives, especially when mining dense datasets. Mining the extensions of these false positives not only prolongs the mining process, it also plagues the verification process due to a larger number of candidates. For instance, UB ( P ( { C } ))=0.542  X  minprob =0.5 for the uncertain database shown in Table 1 and Fig. 1. As this upper bound exceeds the minprob threshold, our B-PUH algorithm recursively mines its extensions. On the other is not probabilistic frequent and thus we do not need to consider all its supersets (due to the downward closure property for probabilistic frequent itemsets: All subset of a probabilistic frequent itemset must also be probabilistic frequent).
In order to avoid redundant mining, we propose the VI-PUH algorithm. After getting all the information in the header table with respect to the prefix item-set P , the VI-PUH algorithm prunes those infrequent extensions according to their small CUB. Then, VI-PUH immediatel y verifies those remaining potentially frequent, we add it to the result, and recu rsively discover its extensions. Oth-erwise, we will not consider it anymore. Compared with B-PUH, this VI-PUH algorithm verifies candidates early and prunes those false positives early, thus reducing the potential overhead of extending and verifying infrequent itemsets. This generate-and-verify-immediately framework is no confined to probabilistic frequent itemset mining; it is also applicable to other itemset mining algorithms that exploit the upper bounds.
 Example 4. Consider the uncertain database in Table 1. When building the tially frequent, but { B } is not and thus removed. Then, we compute the exact frequentness probability for these four candidates immediately, and confirm that { C will not appear in { A }  X  X  header table. Moreover, we also skip C after mining the { A } -projected database. We evaluated the algorithms using both synthetic and real datasets. The (sparse) synthetic dataset is generated by the IBM data generator [2], which contains 1000 items and 1M transactions with an average length of 25. Real datasets include mushroom, retail, and connect4 from the UCI Machine Learning Repos-itory and Frequent Itemset Mining Implementation Repository. To avoid rep-etition, we use the (dense) mushroom dataset as a representative in our ex-perimental results. We assigned a random existential probability to each item in these datasets. The default minsup values are 0.5% and 1% for the synthetic and mushroom datasets, respectively; the default minprob is 0.8 for both datasets. The longest frequent itemsets contain 2 items for synthetic dataset and 6 items for mushroom. All programs were written in C++, compiled with Visual Studio 2010, and run on Windows XP on a 1.8 GHz dual-core AMD processor with 2GB RAM. 5.1 Scalability We first evaluated the scalability of both B-PUH and VI-PUH algorithms with existing algorithms such as Apriori-based PFIM DynamicOpt+P algorithm [4] and tree-based ProFP-Growth [5] on different numbers of transactions to demon-strate their scalability. The database size ranges from 5K up to 1M. Fig. 2 shows that our PUH-mining algorithms (i.e., B-PUH and VI-PUH) scale the best compared with the two existing algorithm s. The reason is tha t, with increasing database size, the computation of exact frequentness probability becomes much more costly. Thus, the benefits of pruning those infrequent itemsets by our upper bounds become more significant.

Between our two PUH-mining algorithms, VI-PUH is observed to take shorter runtime than B-PUH. This observation is expected due to the benefits of pushing the pruning step early. Due to its shorter runtimes, only VI-PUH will be shown in the following experimental results.
 5.2 Runtime We then evaluated the runtime of differen t algorithms on different datasets with respect to minsup and minprob . We compared our VI-PUH algorithm with exist-ing algorithms such as Apriori-based PFIM DynamicOpt+P algorithm [4] and tree-based ProFP-Growth [5]. Figs. 3(a) &amp; 3(b) show the results on the first 10K transactions of the synthetic dataset with respect to a range of minsup and minprob . The Apriori-based algorithm took a significantly longer runtime than the others. Figs. 3(c) &amp; 3(d) show the runtime on the mushroom dataset. Our VI-PUH mining algorithm took much shorter runtimes on the sparse synthetic dataset, while performing consistently well on the dense mushroom dataset. The primary reason is that the usage of upper bounds to prune infrequent itemsets saves a lot of runtime for the expensive spd computation (recall from Section 3 that the complexity is O( | T | X  minsup )), especially for big databases. Another reason lies in the pattern-growth framework, which avoids the candidate gener-ation.

Regarding effects of varying minprob , the performances of all the algorithms generally remain stable for different minprob . Hence, we will only consider the effect of minsup in the following experiments. 5.3 Pruning Effect: The Number of Candidates Since the computation of the spd is the most time-consuming part, the number of candidates (i.e., itemsets whose spd are computed) makes a big difference to the overall performance. Here, we evaluated the pruning effect of our CUB as well as the generate-and-verify-immediately search manner, which helps reduce the number of candidates and the runtime.

Fig. 4 shows the effect of our three upper bounds (1-UB, SUB, and CUB) when compared with the Chernoff Bound [24]. Recall from Section 1 that the Chernoff bound [24] was proposed to prune candidates without computing exact frequentness probability, but it only handles tuple-level uncertainty. All these upper bounds are tested using the VI-PUH algorithm on both the synthetic dataset and mushroom. For the (sparse) synthetic dataset, CUB significantly outperforms the other three bounds when minsup decreases.

Among them, they have their strengths and weaknesses. For instance, on the one hand, the 1-UB gets loose when we just scan minsup transactions. However, it is beneficial in a long run. On the other hand, the SUB alleviates this problem. However, it gradually loses its power when we scan more transactions (due to the increase of accumulative sum). Hence, the use of the CUB takes advantages of both worlds.

As for the Chernoff bound, minsup affects the bound exponentially. This is not the case for our upper bounds. So, our bounds generated fewer false positives than the Chernoff bound, especially for small minsup . 5.4 Effect of the Search Manner Besides the tightness of upper bounds, another contributor in the reduction in runtimes and the number of candidates that need to be checked is the generate-and-verify-immediately search manner. So, we evaluated this aspect of our PUH algorithms by comparing them with the Apriori-based PFIM DynamicOpt+P algorithm [4]. Between the two PUH algorithms, they produce almost the same number of candidates. For readability, we only pick one (say, VI-PUH) in Fig. 5, which shows that VI-PUH provides fewer candidates for the synthetic dataset. In this paper, we proposed two algorithms that mine probabilistic frequent pat-terns from uncertain data using the hyperlinked structure-based mining model. Both Basic PUH (B-PUH) and Verified I mmediately PUH (VI-PUH) efficiently mine all and only probabilistic frequent patterns without candidate generation. Both algorithms adopt novel upper bounds of frequentness probability X  X amely, (i) 1-upper bound (1-UB), (ii) subset upper bound (SUB), and (iii) their combi-nation called the combined upper bound (CUB) X  X o prune infrequent patterns early to reduce the expensive computation of the spd and exact frequentness probability. Experimental results showed the efficiency of our algorithms, es-pecially the VI-PUH that performs consi stently well on both sparse synthetic dataset and dense real datasets in mining probabilistic frequent patterns. Acknowledgement. This project is partially supported by (i) CSC (China), (ii) Mitacs (Canada), (iii) NSERC (Canada), (iv) U. Manitoba &amp; (v) Wuhan U.
