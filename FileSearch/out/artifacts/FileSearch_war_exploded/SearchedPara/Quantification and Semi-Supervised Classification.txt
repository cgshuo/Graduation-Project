 In realistic settings the prevalence of a class may change after a classifier is induced and this will degrade the performance of the classifier. Further complicating this scenario is the fact that l a-beled data is often scarc e and expensive. In this paper we address the problem where the class distribution changes and only unl a-beled examples are available from the new distribution . We d e-sign and evaluate a number of methods for coping with this pro b-lem and compare the performance of these methods. Our quantif i-cation-based methods estimate the class distribution of the unl a-beled data from the changed distribution and adjust the original classifier accordingly, while our semi-supervised methods build a new classifier using the examples from the new (unlabeled) distr i-bution which are supplemented with predicted class values. W e also introduce a hybrid method that utilizes both quantification and semi-supervised learning. All methods are evaluated using accuracy and F-measure on a set of benchmark data sets. Our results demonstrate that our methods yield substantial improv e-ments in accuracy and F-measure. I.2 .6 [ Artificial Intelligence ]: Learning-induction Algorithms, Measurement, Design Semi-supervised learning, quantification, classification, concept drift, class distribution In real-world data mining settings it is often the case the classif i-cation  X  X oncept X  we are trying to learn may change over time and, in particular, may change after a classifier is induced . This pro b-lem is known as concept drift [14] and in this paper we focus on a specific type of concept drift where the class distribution changes over time, yielding a distribution mismatch [7] problem . This problem occurs frequently. For example, epidemiologists often find that although the cause of a disease is stable, the prevalence of the disease changes over time . The same phenomenon has been found in help-desk support applications, where the occurrence of certain support issue s varies over time (e.g., there are more reports of cracked computer screens on July 4, the U.S. Independence day [7]) . This problem of a changing class distribution is further complicated by the fact that labeled examples are often scarce or costly to obtain  X  and it may not even be possible to label newly acquired examples in a timely manner. This paper focus es on two research questions associated with the data mining scenario just described: 1) How can we maximize classification performance when the class distribution changes but is unknown, and 2) How can we utilize unlabeled data from the changed class distribution to accomplish this goal? More formally, the class of problems we study has some original distribution, D orig , from which we are provided a set of labeled examples, ORIG label , with class distribution ORIG CD point the distribution of data changes to D new with a new but unknown class distribution, NEW CD , and from this distribution w e are provided with a set of unlabeled examples, NEW evaluation purposes we are also provided with labeled examples, NEW eval , drawn from D new . Given this terminology we can state our learning problem more precisely. Problem statement: Given: ORIG label drawn from D orig (with ORIG CD ) Do: Construct the classifier C , using ORIG We introduce Figure 1 to illustrate the distribution mismatch problem and to establish some performance goals for our work. Figure 1 shows how two baseline methods , Na X ve and Oracle, perform for a two-class data set when the original class distrib u-tion is balanced (i.e., ORIG CD = 1:1 with a positive class rate of 50%) but then is altered so that the class distribution for the new distribution (NEW CD ) varies between 1% and 99% positive e x-amples, in 1% increments (details of the experiment are provided in Section 3 ). The Na X ve approach ignores the unlabeled data and the fact that the class distribution may change and utilizes the classifier i n-duced from ORIG label to classify the examples in NEW Oracle method provides a potential upper bound for achievable performance by building a classifier using NEW unlabel true class labels  X  X ncovered. X  E valuation is based on NEW The results for Na X ve clearly demonstrate the distribution mi s-match problem since the accuracy of Na X ve degrades with respect to the  X  X esired X  performance of Oracle for most cases where ORIG CD  X  NEW CD (the shape of these curves is discussed in Section 4). We can state our performance goals in terms of these two baseline methods: we will develop methods that perform strictly better than Na X ve and approach the perform ance of Oracle. In thi s paper we utilize two basic techniques for improving cla s-sifier perform ance beyond that of the Na X ve approach : class di s-tribution estimation (CDE) and semi -supervised learning (SSL) . The CDE technique exploits the fact that if we can esti mate the class distribution from which future examples will be drawn, then we can adjust the original classifier to account for the differences in class distribution. In this paper we describe and analyze two CDE -based methods : an iterative method of our own d esign and a quantification -based method based on a quantification technique [7] . The CDE -based methods use NEW unlabel in the learning process, but only to estimate NEW CD . Our semi -supervised lear n-ing methods, on the other hand, use examples from N EW the classifier induction process, where these new examples are assigned predicted class labels. We introduce two main SSL -based meth ods: a simple method that only uses the examples from NEW unla bel to build the cl assifier and a self -training [20] variant that iteratively merges the examples from ORIG label with NEW . Finally, we introduce a hybrid method that i ntegrate s features from the CDE -based and SSL -based methods . We evaluate all methods using accu racy and F-Mea sure . The remainder of this paper is structured as follows . Section 2 describe s our methods in detail. Section 3 present s our experiment methodology and our results are then presented and analyzed in Section 4. Related work is described in Section 5 and Section 6 summarizes our conc lusions and discusses areas for future r e-search . In this section we describe the methods used to handle changes in class distribut ion, with the exception of the Na X ve and Or acle methods, which were introduced earlier. Recall that these methods serve as lower and upper performance bounds, respectively, for our methods. Our cla ss distribution estimation methods require some background before they can be properly understood and this background is provided in Section 2.1. The CDE -based methods are then described in Section 2.2 , the SSL -based methods in Section 2.3 , and the hybrid CDE/SSL method in Section 2.4. In this section we discuss the impact that a changing class distr i-bution has on classification and how we can compensate for this change in class distribution. We begin by introducing some basic terminology. Table 1 shows a standard confusion matrix for a two -class domain , where all pr edictions can be categorized as true positives (TP), false negatives (FN ), false positives (FP ), and true negatives (TN).
 The following terms are defined based on the values in the conf u-sion matrix: positive rate ( pr ), negative rate ( nr ), distribution rate ( fpr ). We use the prime symbol (  X  ) to denote the values ass o-ciated with the new distribution. Now that we have introduced the basic terms we can discuss what happens if the class distribution changes and how we can c o m-pensate for these changes. This topic is described in detail from a theoretical perspective by Elkan [6] and an applied perspective by Weiss and Provost [16] . I n the interest of clarity we discuss the issue from the applied perspective and use an example to motivate the key conce pts and explain the relevant equations.
 In o ur example the data set drawn from the original distribution has 900 positive ex amples and 1 00 negative examples ( p r = 9/10 , nr = 1/10 ) a nd the data drawn from the new distribution has 200 positive and 8 00 negative examples ( p r  X  = 2/10 , nr  X  = 8/10 ). The distribution mismatch ratio d m r indicates the factor by which the ratio of positive to negative examples changes between the orig i-nal and new distribution . For this example dmr = 9 :  X  or , equiv a-lently , 36 : 1 . Thus, based on the ra tio of the positive rate to neg a-tive rate, the positive s are 36 times more prevalent in the original distribution than in the new distribution . Note that if we used the fraction of positive examples rather than the positive to negative ratio , the n dmr wo uld only be 4.5 (i .e., .9/.2), but if fractions are used then equation 1 become s much more com plex and difficult to understand [16] .
 We can adjust for changes in class distribution during the classif i-er induction process by any of these three methods [6] : 1) sam-pling (or reweighting) the training examples so as to alter the class distribution to match the new distribution, 2) altering the probabi l-ity threshold s used to determine the class label , or 3) altering the ratio of misclassification costs between false positive and false negative predictions. We employ the third method because t he learning package that we use, WEKA, supports cost -sensitive learning and thus no changes were required to the learning alg o-rithm. We use equation 1 to deter mine the cost ratio (the ratio of a false positive to false negative prediction) that should be used when building the classifier: Returning to our example, the cost ratio COST FP : COST equal 36:1 . This adjustment can informally be shown to be correct as follows. Without loss of generality, imagine that we build a decision tree classifier and at an arbitrary leaf node there are P positive and N negative examples. Without cost -sensitive lear n-ing, the leaf will be labeled with the majority class. A cost -sensitive learner will classify the leaf to minimize the total cost and in this case t he costs will be perfectly balanced if P= 36N , since a positive label yields a cost of 36  X  COST FN and a negative pred iction yields a cost of 1  X  COST FP . If the positive rate is above this it will be labeled positive and if below this it will be labeled negative. This is the desired behavior since the new distr i-bution will cause the ratio of positive to negative examples , as noted earlier, to decrease by a fac tor of 36 (i.e., a leaf with a positive to negative class ratio of 36:1 using ORIG ponds to a class ratio of 1:1 when using NEW eval and the 1:1 ratio is the normal threshold for labeling a classification  X  X  ule X ). We introduce three class distribution estimation ( CDE ) methods in this section. Since quantification is the task of estimating the class distribution of new data, these CDE methods can also be considered quantification -based methods. The key difference between the quantification task and our task is that for quantifi ca-tion the ultimate goal is to estimate the prevalence of each class, whereas in our case this is only an intermediate step  X  the ul timate goal is to improv e classification performance on data drawn from a new distribution . For all CDE -based methods the fina l classifier is induced from ORIG label with the cost ratio computed with e qu a-tion 1, utilizing the estimate of NEW CD pro duced by the specific CDE method ( note NEW CD determines the pr  X  /nr  X  ratio). Our CDE -Iterate method iteratively generates estimates of NEW CD . It first builds a classifier C 1 using ORIG uses C 1 to classify NEW unlabel . The initial estimate of NEW then calculated from these predictions. However, assuming ORIG CD  X  NEW CD , the original predictions will be biased and will tend to underestimate the change in class distribution (this bias is why we need the  X  X djustment X  in the first place). To co m-pensate for this, the process is repeated but in the second iteration the classifier C 2 is built using the cost ratio calculated using equ a-tion 1 with the estimate of NEW CD from the first iteration. The expectation is that additional iterations will reduce the undesired bias and the subsequent classifiers will be better able to classify NEW unlabel , yielding more accurate estimates of NEW iterations terminate once a pre -specified maximum number of iterations is exceeded (in this paper we only report results for the first 3 iterations). CDE -Iterate -n refers to the classifier produced by the n th iteration of this method.
 To make the al gorithm more concrete, we specify the CDE -Iterate algorithm in Figure 2 using pseudo -code . The algorithm begins by initializing the values for the cost ratio to the default values (line 1) , builds an initial classifier C 1 2) and then calcu lates the pr/pn ratio for ORIG label the Pos( ) and Neg() functions return the number of positive and negative examples for the specified data sets ( ORIG algorithm then iterates in lines 4-10 until the maximum number of iterations (maxIterations) is reached. In this loop this algorithm first uses the previous classifier C i to classify NEW Then i n line s 7-8 the distribution mismatch ratio ( dmr ) is co m-puted and the cost ratio information is updated. In line 9 a new classifier C i+1 is generated using ORIG label with the u pdated cost ratio. Finally, once the loop terminates the last classifier is r e-turned (line 11).
 1. COST FP = COST FN = 1; 2. C 1 = build_ classifier( ORIG label , COST FP , COST 3. Pos 2Neg = P os(ORIG label ) / Neg(ORIG label ); 4. for (i=1; i&lt; maxIterations; i++) 5 . { 6 . NEW label = C lassify(NEW unlabel, C i ); 7 . dmr = P os 2 Neg : ( P os(NEW label ) / N eg(NEW 8 . COST FP = COST F N  X  dmr; 9 . C i +1 = build_ classifier( ORIG label , COST 10 . } 11 . return C i +1 ; The CDE -AC method relies on the Adjusted Count (AC) quant i-fication technique [7] to estimate NEW CD . The method uses equ a-tion 2 to produce its adjusted estimate of the positive rate of the new distribution, pr* . Note that it still requires pr  X  , the unadjusted estimate of NEW CD , which is calculated in the same manner as it was calculated in the first iteration of CDE -Iterate  X  a classifier is built from ORIG label , used to classify NEW unlabel , and pr  X  is calc u-lated fr om the predicted class labels . Furthermore, tpr and fpr , which are associated with the original distribution, are estimated by using 1 0 -fold cross validation on ORIG label . E quation 2 esse n-tially compensates for the fact that p r  X  will underestimate changes i n class distribution due to the undesired bias dis cussed earlier (see Forman [7] for further details and a derivation of equation 2 ). T he estimate of NEW CD can easily be calculated from the adjusted positive rate, pr*. Finally, in order t o evaluate the effectiveness of our two CDE -based methods , we introduce the CDE -Oracle method, which obtains the correct value of NEW CD ( via an oracle) and then uses equation 1 to determ ine the appropriate cost ratio . The classifier is then induced from ORIG label using this cost ratio. One would expect this method to be an upper bound on the performance of all CDE -based methods. In this section we discuss semi -supervised learning (SSL) m e-thods [4] , which induce a classifier using examples from NEW unla bel . Unlike the methods in the previous section, no explicit adjustment is made for differences in class distribut ion. Our first SSL -based method, SSL -Na X ve , is quite straightforward. It builds a classifier C from ORIG label , uses C to label NEW uses the labeled version of NEW unlabel to build a new classifier, C  X  . Note that this method does not directl y use any of the labeled data from the original distribution when building the final classifier. Our next SSL -based method is more sophisticated in that it indu c-es a classifier using examples from both the original and new distribution s. This method, SSL -Self -Train , uses a semi -supervised technique known as self -training [20] . As the case with the SSL -Na X ve method, this method starts by building a model based on ORIG label and uses it to classify NEW unlabel this method then moves the examples from NEW un label the most confident predictions (i.e., above the median confidence level) into ORIG label . In this case confidence is based on how close the class membership probability estimate is to 1.0. The above set of steps is repeat ed until either all of the examples in NEW unlabel have been merged with those in ORIG label mum number of iterations have been executed. In our impleme n-tation a maximum of 4 iterations are executed . We combine the class distribution estimation and semi -supervised self -training methods into a Hybrid method , where the goal is to exploit the power of the CDE technique but also include data from the new distributio n when training the classifier. The hybri d method starts like the CDE -Iterate method: it builds a classifier C from ORIG label , applies C to NEW un label to estimate NEW equation 2 to determine the appropriate cost ratio, and then reg e-nerates the classifier from ORIG label using this cost inf ormation . It then applies the self -training technique  X  it uses the adjusted classifier to relabel NEW unlabel and then effectively  X  X oves X  the examples where the confidence of the predicted label is above the median value to ORIG label . This method then deter mines the a p-propriate cost ratio (equation 2) to account for differences in the class distribution of ORIG label and NEW CD . This calculation takes into account the fact that the class distribution of ORIG changes as new examples are moved into it . This process is then repeated until all examples have been removed from NEW unlabel a maximum of 4 iterations have been executed . D ue to space limitations we do not report the results for each iteration , as was done for CDE -Iterate . In summary, this method i s essen tially the semi -supervised self -training method, but where we compensate for the difference between the class distribution of the training data and NEW CD . This section describes our experiment al setup . It describes the data sets that we use, the specific experiments that we run, the classifier induction algorithm we employ , and the metrics that we use to evaluate classifier performance. The methods that we eva-luate were described in Sec tion 2 and are n ot described here. Table 2 describes the five UCI data sets [3] that we use in this study. Any data sets that had more than two classes were co n-verted into two -class data sets . The Covertype data set was con-verted to two classes by designating  X  2  X  as the positive class and all other values as the nega tive class, while the Letter -Vowel data set was converted by designating the vowels as the positive class and all other letters as the negative class. The positive class is the minority class for all data sets except for the Magic Gamma dat a set , because the documentation for that data set specifi cally states that the majority class is the class of interest. Table 2 shows the original data set size, the percentage of the examples that bel ong to the positive class, and the numbe r of examples in each partition (explained shortly).
 As described earlier, the problem we investigate requires three data sets: ORIG label , NEW unlabel and NEW eval . We generate each of these by splitting the original d ata set into three equal -sized part i-tions. For our experiments the positive rate of ORIG label 50% while the positive rate of NEW unlabel and NEW eval varied between 1% and 99% in 1% in crements . In order to gene r-ate the desired class distributions without duplicating any exa m-ple s, the size of the partitions must be limited . We use the max i-mum possible partition size for each data set and these partition sizes are specified in the last column of Table 2 (the value dis-played for Covertyp e is not the maximum possible value but was reduced due to the size of the data set and time constraints ). Table 3 shows the results of this partitioning process for the Adult data set. Each of the three partition s contain s 4,700 examples, even as the positive rate (Pr) varies . The maximum number of positive or negative examples required is 11,656 (2,350 + 2  X  4,653), which for the positives examples occurs when Pr= 99% and for the negative examples when Pr=1%. Since the original data set contain s 11,67 3 (23.9% of 48,482) positive and 37,168 negative examples, these partitions can be generated without duplicating examples . In this case 17 positive examples are not used because fractional examples cannot be used to generate the appropriate positive rates.
 In our methodology the partition ORIG label is created first and does not vary as the 99 partitions for NEW unlabled and NEW generated. In order to produce more reliable results, the exper i-ments , and thus the partitioning process, were repeated 10 times and the results presented in this pape r are averages over those 10 runs (Covertype experiments were repeated only 4 times due to time constraints).
 All experiments i n this paper utilize the J48 [17] implementation of C4.5 [12] from W EKA release 3.5.8. Our CDE -based methods compensate for changes in class distribution using WEKA X  X  cost -sensitive learning capabilities, which in WEKA are implemented using example reweighting . All of our methods described in Sec tion 2 are implemented as wrapper -based learn ers and could easily be applied to other base learning methods. The classific a-tion performance of our methods is evaluated using both accu racy and F -measure. The F -measure is defined as t he harmonic mean between precision and recall and is defined in equation 3 . We track the performance of F -me asure because we are interested in what happens when a class distribution becomes highly skewed and accuracy is known to be an inappropriate evaluation measure in these cas es [11] . One might expect AUC to be a mor e natural choice than F -measure given its current popularity in the data mining community  X  and in fact we did track AUC for all exper i-ments . However, we do not report these results because ROC curves are, by design, not sensitive to changes in class distribution and hence are an inappropriate evaluation measure for this pro b-lem ( the results were also uninteresting in that most methods performed similarly). In this section we present and analyze our experimental results. We begin by analyzing the detailed results for one representative data set, Adult , and then analyze more highly summarized results for all five data sets. The detailed accuracy results for the Adult data set are shown in Figure 3 and the same data is shown at a slightly less granular level (i.e., only data from 13 of the 99 pos i-tive ra tes are shown) in Table 4.
 Figure 3 shows that three of the methods, Na X ve, SSL -Na X ve, and SSL -Self -Train, perform much worse than the remaining methods that are displayed, including all CDE -based methods and the Hybrid method. Of the three methods that pe rform poorly, Na X ve X  X  performance is in the middle, with SSL -Na X ve performing the worst for low positive rates and best at high positive rates. With the exception of CDE -Iterate -1, the remaining methods all perform about the same for high positive rates bu t vary at low positive rates. Here again CDE -Iterate -1 does the worst, while CDE -AC does the best and Hybrid is in the middle. Because of difficulties visually differentiating between the curves, the CDE -Iterate -2 and CDE -Iterate -3 me thods are not shown, a lthough their performance is better than CDE -Iterate -1 but worse than CDE -AC (their more highly summarized performance is provided in Table 5). The CDE -Oracle and Oracle methods are not shown because in the figure they were indistinguish able from the CDE -AC m e-thod.
 It is worth commenting on the shapes of the curves in Figure 3 . Three of the curves are nearly linear and this includes the Na X ve method, which is the easiest to analyze. Since the Na X ve method ignores the new distribution in the learning phase, we might expect its performance to be linear and parallel to the x -axis (i.e., invariant with respect to positive class rate). However, the o b-served performance is not inconsistent with this, which simply means that the accuracy of the induced classifier is not the same for both classes even though the classes are equally represented in the training data. The performance curves for the other methods, including the Oracle method (not shown) exhibit a  X  X  X  sha pe with a minimum near a positive class rate of 50%. This is simply due to the fact that it is easiest to achieve high accuracy when a data set is highly skewed  X  X nd the strategies that exhibit the  X  X  X  shape can adapt to the skewed distribution.
 The overall performance characteristics of the methods are shown more effectively in Table 4, which includes the results for all 10 methods. Of particular value are the averages for the methods over the different positive rates , shown in the last row (thes e averages are computed over all 99 positive class rates, not just the 13 that are displayed) . The Oracle and Na X ve methods determine the range of expected behavior, while the CDE -Oracle provides what should be an upper bound on the performance of the CDE -based methods. First, note that the CDE -Oracle provides perfo r-mance very close to that of the overall Oracle method. Based on the averages we see that the CDE -Iterate methods get progressiv e-ly better with additional iterations and that the CDE -AC method is the best overall performing CDE -based method . As we saw in Figure 3 the SSL -based methods perform poorly and in fact, based on average performance, perform worse than the Na X ve method. The Hybrid method performs in the middle range of the CDE -Based method s and thus shows promise if it can be improved . The overall results suggest that CDE -AC is a very good method and looking at each individual row, we see that not only does it have the best average performance, but it performs best or nearly best for each p ositive rate. Table 5 will present a more summarized view of the data in Table 4, but for all five data sets.
 The F-measure results for the Adult data set are displayed in Fig ure 4 . Because the curves are even harder to distinguish than for accuracy, the figure was simplified slightly  X  the x -values are shown at 5% increments, curves that were essentially indisti n-guish able were labeled together using one of the curves , some of the 10 methods were omitted, and the positive rate is clipped at 70% because the relative performance of the methods does not vary after that point.
 The results for F -measure are interesting in that they vary greatly from those for accuracy  X  and t he Oracle does not perform best. As Table 6 will show us, this behavior is relatively consistent over all data sets, so it is worth further analysis. First, Figure 4 shows that the same three methods that did poorly for accuracy for both high and low posit ive class rates (Na X ve, SSL -Self -Train, and SSL -Na X ve) exhibit the same behavior for F -measure. All other methods perform similar to one another for high positive class rates. But at low positive class rates CDE -Iterate -2 consistently does the best. The Or acle and CDE -AC methods perform nearly identically to one another, and the Hybrid method sometimes does better and sometimes worse than Oracle and CDE -AC. We defer the discussion of this interesting behavior until after we introduce the F -measure results f or the five data sets in Table 6.
 We now present the more summarized results for all five data sets. Table 5 provides t hese results for accuracy, which is similar to Table 4 but averages the results over all 99 positive rates . The methods are sorted in order of decreasing average accuracy, so the best methods are toward the top. When determining which m e-thod is the best practical method, the Oracle and CDE -Oracle methods are excluded, since in practice oracles are not available. The best performing practical method for each individual data set is underlined and from this we see that CDE -AC is not just the best when averaged over all five data sets, it performs best on each individual data set. The overall performance of the other methods is roughly consistent with what we saw for Adult. Again, we see that the CDE -Iterate method benefits from additional iterations and the Hybrid method shows some promise. We again see that the SSL -based methods perform poorl y  X  and worse than the Na X ve method which essentially ignores the changing distrib u-tion completely.
 Table 6 shows the summary performance of the ten methods over the five data sets for F -measure. The methods are again ordered by decreasing average performance, and since the values in the Adult column are decreasing , as they were in Table 5, the relative efficacy of each method for the Adult data set matches the pattern over all data sets . The key conclusions here are th at the SSL -based methods do poorly and that the Oracle and CDE -AC m e-thods are in the middle of the performance range and the CDE -Iterate methods outperform these two methods. While not all of our results could necessar ily be predicted a priori , most of our results are not surprising and can be explained . For example, for accuracy it makes some sense that the CDE -Iterate method performs better after one iteration because of the undesi r-able bias mentioned in Section 2. CD E-based methods outperform SSL -based methods because, as we shall discuss in Section 5, class distribution estimation (i.e., quantification) is fundamentally an easier task than classification and thus the CDE -based methods introduce less uncertainty than the SSL -based methods, which require us to classify examples from the new distribution before a classi fier can be retrained.
 The F -measure results that have the CDE -Iterate methods outpe r-forming the CDE -Ora cle and Oracle methods are harder to justify, but we feel we have a plausible explanation. Our explanation begins with the Oracle method , which uses the correct (hidden) labels from the new distribution to train a classifier. While this seems like the perfect strategy, most classification methods are opti mized for accuracy maximization and often sacrifice perfo r-mance of the minority class for improved performance of the major ity class  X  which will likely degrade the F -measure perfo r-mance, due to the need to achieve high recall values. Thus we can see that th e Oracle method may produce poor F -measure values when trained on data with low positive class rates. Because the CDE -Iterate method will train a classifier using data with a pos i-tive rate of 50%, it has the opportunity to achieve better F -measure results. The cost ratio used to adjust the classifier can undermine this by placing less emphasis on the positive class when the new distribution has a lower estimated positive rate, but, as discussed in Sec tion 2, the CDE -Iterate method will tend to underestimate any changes in class distribution , especially in the initial iteration. This explains why CDE -Iterate -1 perform s well. This above explanation may explain the observed results, but is there any useful lesson here? There is a lesson, but it not a new one . If one wants to perform better on one class than another, it is appropriate to bias the learner toward that class and if one wants to do well on a measure that balances the performance of both classes, then one should not bias the learner toward one clas s (e.g., by training on data mainly from one class). Thus, we should take this into account when  X  X djusting X  a classifier to compensate for changes in class distribution. This suggests an extension to our work  X  how to adjust for a changing class distributi on when either the classes are not equally important or are equally important. Interestingly enough, there has been work on cost quantification [7] and these methods would be appropriate to counteract changes to the class distribution when we have knowledge about the rel a-tive importance of the different classes . In this section we describe some related work, although much of the most relevant work was already mentioned in Section 2. The problem of improvi ng classifier performance in response to u n-known changes in class distribution has been studied previously [1, 9, 13] and the most relevant and successful appro ach thus far has involved expectation maximization (EM) [9, 13] and this approach has also been adapted to the related problem of classif y-ing non -stationary data sequences [18] . Our CDE -Iterate method is a variant of the basic EM method , but there are some minor differences. Namely, the CDE -Iterate method thresholds the generated probability estimates to assign a class label and from this generates the cl ass distribution estimate, while EM uses the probability estimate s directly, without applying any threshold . In addition, CDE -Iterative  X  X djust s  X  the original model using cost -sensitive learning while the prior work using EM adjusted the original model X  X  posterior probability outputs . Based on our results and the published results in prior work, we believe that both the EM and CDE -Iterate methods are similar and perform similarly (however, as we discuss in Section 6, we view CDE -AC as superior to both methods).
 In some situations one may have no information (e.g., unlabeled examples) about future changes in class distribution but still wants to maximize classifier performance on future data [19] . The a p-proach in this case is not to adapt to changing condition s, but rather to build a  X  robust  X  classifier that performs well under a wide variety of situations [2] . In fact, this desire for classifier s that exhibit robust behavior over wide ranges of class distributions and misclassification costs is the primary reason that ROC analysis [10] has gained such prominence in the data mining community. While this a pproach of generating robust classifiers has its adva n-tages and is applicable to our problem , it clearly is not the best strategy when the class distribution of the new data i s known or can be reliably estimated  X  and as we have seen in this paper, we can re liably estimate the new class distribution when unlabeled data from the new distribution is available.
 While our work focused on improving classifier performance in response to changes in the class distribution of the data, a related, but different, task i s the quantification task. Quantification i n-volves estimating the prevalence (i.e., class distribution) of the classes over time [7] . Quantification is a simpler task than class i-fication because one can often come up with good estimates of a class distribution without producing accurate predictions for individual examples. As an example, auto insurance agencies can accurately predict what fraction of their policy holders will have an accident without being able to acc urately predict which speci f-ic policy holders will have an accident. As we have seen this paper, quantification methods can also help solve our more co m-plex task (i.e., our CDE methods perform quantification then model adjustment). Quantification technique s are discussed in detail by Forman [7, 8] and while we applied some of those m e-thods to our problem (i.e., Adjusted Count) there are other rel e-vant methods that could be analyzed in the future (e.g., Median Sweep, Mixture Model). Our CDE -based methods compensate for changes in the class distribution via cost -sensitive learning and the EM accomplishes the same thing by changing the probability thresholds . A third way of compensating for differin g class distributions is to sample from the original distribution so that its class distribution matches the estimated class distribution of the new data. Many appropriate sampling methods exist [15] , such as oversa mpling and unde r-sampling, as well as more advanced methods which make better use of the data [5] . Since our learning task involve s labeled and unlabeled data, s emi -supervised learning methods [4, 20] are relevant . The results thus far for the SSL -based methods have not been very promising, probably due to the fact that, as just discussed, it is easier to acc u-rately estimate the class distribution of the u nlabeled examples than to classify them. We believe, however, that the CDE -based methods can ultimately benefit from semi -supervised learning and thus we feel that semi -supervised learning warrants further study. In this paper we evaluated several methods for dealing with the situation where the class distribution can change after an initial classifier is built and only un labeled data from the new distrib u-tion is available . Our results clearly indicate that the na X ve a p-proach of not doing anything leads to very poor results but that there are very effective methods for dealing with this problem. The class distribution estimation b ased methods generally pe r-form ed the best and when accuracy must be maximized the CDE -AC method is the best choice based on its performance, comput a-tional requirements (i.e., only a single iteration ) and the fact that there are no parameters, such as the numb er of itera tions, to set. The CDE -based methods perform ed relatively close to the ORACLE method , es pecially in comparison to the Na X ve method , which performed poorly . In general we found that the CDE -Iterate -2 method outperforms the CDE -Iterate -1 method, indicating that our iterative process does lead to improved cla ss distribution estimates. Relative t o the CDE -based methods, the semi -supervised learning methods did poorly. The hybrid method did much better than the SSL -based methods, but consistently underperformed the CDE -based methods. The results for F -measure were quite different and the reasons fo r this were di s-cussed in Section 4. However, the results were still quite clear, with the CDE -Iterate -2 method performing best, and all of the CDE -based methods were effective, significantly outperforming the na X ve strategy.
 The relatively poor performance of the SSL -based methods , for both accuracy and F -measure, is quite notable. This failure was explained earlier by the fact that class distribution estimation is fundamentally an easier task than classification and hence our class distribution estimates a re thus going to be more accurate than the estimates of the class values for the new, unlabeled, data. A second insight, however, is related to the task itself. Semi -supervised learning is typically employed when labeled training data is very scarce but un labeled data abounds. That is not the case in our problem setting or in our experimental setup and was not the motivation for the use of semi -supervised learning. In our setting we assume that there is sufficient labeled data to build a reasonable classifi er. The problem is that the class distribution changes and that is the motivation for the use of semi -supervised learning (i.e., we want to learn from the more representative, but unlabeled, examples). Given this understanding and the fact that CDE is easi er than classification, the disparity in the results is explained . Nonetheless, in theory the use of the new data for training could improve overall classifier performance and we do believe that SSL methods can be of use in this context. We di s-cuss improve ments to the SSL methods in our discussion of future work toward the end of this section.
 This paper provides a number of contributions. First, we evaluate methods that have not previously been used to address the chan g-ing class distribution problem  X  and th ese new methods  X  especially CDE -AC  X  are shown to work well. Furthermore, the CDE -AC method that we recommend is simpler and easier to implement than iterative methods. The failure of the SSL -based methods is also notable and should help guide future research . We also provide a more comprehensive empirical study of the pro b-lem than past work: we analyze a total of ten methods, including several baseline methods for comparison, evaluate our results with respect to F -measure in addition to accuracy, combine mult iple approaches ( i.e., Hybrid ), and evaluate our methods under a large number (99) and range (1% -99%) of class distributions. Finally, the problem we address does occur in many realistic settings and given the current state -of-the -art of data mining tool s, it is feasible for practitioners to implement our solution (i.e., most tools pr o-vide one of the three methods that we discuss for adjusting a classifier). Finally, we also hope that our work will bring deserved attention to the problem of changing class distributions and the fact that good solutions do exist.
 Although our results for the CDE -based methods are quite e n-couraging, there is certainly some room for improvement, since even a 1% difference in accuracy between our method and the ORACLE method is significant. One improvement would be to adapt the CDE -Iterative method to automatically terminate once the class distribution estimate con verges. This might improve overall performance over any specific CDE -Iterate -n method and would eliminate the proble m of identifying the appropriate nu m-ber of iterations. It is possible that such a CDE -converge method would outperform CDE -AC. We have performed some prelim i-nary research in this area and have found that in many cases convergence does occur, although we ne ed to refine our notion of convergence (e.g., to handle small cyclical fluctuations in the estimates) . As discussed, s emi -supervised learning should enable us to improve our CDE results further by making additional data available for classifier induction , although the clear superiority of the CDE -based methods over the SSL -based methods indicate that it may not be easy to develop an effective hybrid approach . One possibility we are investigating is to use a classifier that accepts class membership probabili ties in the training phase , so that the uncertain predictions generated by semi -supervised learning can be factored into the learning process . We also would like to apply our current methods, and some new variants o f these methods, to more complex problem settings. One such problem setting differs only slightly , where NEW NEW eval are replaced by a single data set that serves both purpo s-es. In this new scenario the goal is to exploit new data to build a new classifier and then classify that same new data, rather than our current setting where additional data drawn from the new distribution is classified . This new setting would be an example of transductive learning. A more ambitious setting that we intend to study is where the concept drift is of a much less restrictive form , such that the actual concept can change over time, rather than just its class distribution. The CDE -based methods may still prove useful in this more challenging setting  X  since the class distrib u-tion will most likely change if the concept changes  X  but the CDE -based methods clearly will not be sufficient by themselves . In this more complex setting semi -supervised learning will have more to offer and hybrid methods may then perform best. [1] Alaiz -Rodriguez, R., Guerrero -Curieses, A. and Cid -Sueiro, [2] Alaiz -Rodriguez, R. and Japkowicz, N. Assessing the I m-[3] Asuncion, A. and New man, D. J. UCI Machine Learning [4] Chapelle, O., Scholkopf, B. and Zien, A. Semi -Supervised [5] Chawla, N. V., Bowyer, K. W. and Kegelmeyer, W. P. [6] Elkan, C. The foundations of cost -sensitive learning. In the [7] Forman, G. Quantifying counts and costs via classification. [8] Forman, G. Counting Positives Accurately Despite Inacc u-[9] Latinne, P., Saerens, M. and Decaestecker, C. Adjusting the [10] Provost, F. and Fawcett, T. Robust Cl assification for Impr e-[11] Provost, F., Fawcett, T. and Kohavi, R. The Case against [12] Quinlan, R. J. C4.5: Programs for Machine Learning. [13] Saerens, M., Latinne, P. and Decaestecker, C. Adjusting the [14] Tsymbal, A. The problem of concept drift: Definitions and [15] Weiss, G. M. Mining wit h rarity: a unifying framework. [16] Weiss, G. M. and Provost, F. Learning when training data [17] Witten, I. H. and Frank, E. Data Mining: Practical Machine [18] Yang, C. and Zhou, J. Non -stationary data sequence classif i-[19] Zadr ozny, B. and Elkan, C. Learning and making decisions [20] Zhu, X. Semi -Supervised Learning Literatur e Survey . Co m-
