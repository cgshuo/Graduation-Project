 We report the statistically significant mean impacts of blind feedback, as implemented by 7 participants for the 2003 Re-liable Information Access (RIA) Workshop, on 30 retrieval measures, including several primary recall measures not orig-inally reported. We find that blind feedback was detrimen-tal to measures focused on the first relevant item even when it boosted  X  X arly precision X  measures such as mean Preci-sion@10, implying that the conventional reporting of ad hoc precision needs enhancement.

Blind feedback (BF), also known as pseudo-relevance feed-back or (more ambiguously) query expansion, is a contro-versial retrieval technique in which terms from the top re-trieved items of the original base query are used to expand the query for re-running on the same collection. BF is com-monly used by the participants of the ad hoc 1 retrieval tasks of TREC 2 , CLEF and NTCIR, but typically it is not used for the known-item 3 tasks of these forums, and the technique X  X  robustness has been criticized (see Section 5).

Based on our own recent BF experiments in 8 languages [2, 3, 4], we formed the hypothesis that BF impaired measures focused on  X  X rimary recall X  (retrieval of the first relevant item for a topic) such as mean reciprocal rank (MRR) and Success@10 (S10), while boosting measures with most of their weight on  X  X econdary recall X  (retrieval of additional relevant items) such as mean average precision (MAP) and Precision@10 (P10). In this paper, we test this hypothesis on 7 other groups X  BF systems.
Table 1 shows the impact of BF as implemented by 7 other groups for the 2003 RIA Workshop [1].  X  X ase X  gives  X  X d hoc X  searches seek relevant items for a topic.
Text REtrieval Conference ( http://trec.nist.gov/ ).  X  X nown-item X  searches seek one particular item.
 Table 1: Mean Impact of Blind Feedback on 7 RIA Systems (4 Retrieval Measures) the mean scores (over the 150 topics of TRECs 6-8) of the original base query (RIA code bf.0.0 4 ).  X  X F X  gives the mean scores of the blind feedback run using the groups X  favored parameters (bf.system).  X  X iff X  is the BF score minus the Base score (rounded to 3 decimal places). The difference is in bold if it is statistically significant (at the 5% level by the t -test). Consistent with the hypothesis, the statistically significant differences were positive for MAP and P10 and negative for MRR and GS10 (GS10 is defined below).
Table 2 summarizes the impact of BF for 30 retrieval mea-sures, including the 25 major mean ad hoc measures of the trec eval 8.0 utility 5 and the 4 popular Success@ n mea-sures 6 .  X  X iff7 X  is the average difference over the 7 systems. The 5-tuple gives the number of systems for which the mean difference was negative and statistically significant ( n erwise negative ( n o ), zero ( z ), positive but not statistically from http://ir.nist.gov/ria/experiments/bf base/ from http://trec.nist.gov/trec eval/ (our scores may differ slightly because we do not re-order  X  X ied X  rows)  X  X uccess@ n  X  (for a topic) is 1 if a relevant item is retrieved in the first n rows, 0 otherwise. Table 2: Mean Impact of Blind Feedback on 30 Re-trieval Measures (Across 7 RIA Systems) significant ( p o ), or positive and statistically significant ( p The measures are in ascending order by p s  X  n s and then Diff7. Of the 22 mean measures reported on the RIA web site 7 (marked with an asterisk (*)), 21 favored BF, typical of the lack of coverage of BF X  X  downside.  X  X arly precision X  is usually evaluated as precision after the first 5-30 documents retrieved. In Table 2, the listed Precision@ n measures 8 (P5, P10, P15, P20, P30, etc.) were all higher with BF, including at least one statistically sig-nificant increase for each measure.

But if one eliminates any possible influence of secondary recall either by evaluating precision after just 1 item is re-trieved, i.e. Success@1 (S1), or after just the first retrieved relevant item, i.e. mean reciprocal rank (MRR), one finds that BF impaired early precision (the few statistically sig-nificant differences for S1 and MRR were negative).
Similarly, the earliest of the  X  X nterpolated X  precision mea-sures (I0) declined with BF, but when even 10% recall was required before measuring precision (I10), BF boosted it.
The situation for measuring early precision is unsatisfac-tory. The common choice, Precision@ n , is apparently com-bf base/runs/*/eval files viewed December 15, 2005  X  X recision@ n  X  (for a topic) is x / n where x is the number of relevant items retrieved in the first n rows. promised by secondary recall even for n of 5 or 10. Mean-while, the pure early precision measures, S1 and MRR, are overly sensitive to small changes in the first few ranks, not accurately reflecting user benefit and making statistical sig-nificance harder to achieve.
An alternative to precision is to estimate  X  X eading saved X  to the first relevant item, such as by using the Generalized Success@10 (GS10) measure (introduced as  X  X irst Relevant Score X  in [2]). GS10 is 1 . 08 1  X  r where r is the rank of the first relevant item retrieved (GS10 is 0 if no relevant item is sharply in the early ranks; GS10 is 1.0 at rank 1, 0.93 at rank 2, 0.86 at rank 3, etc. GS10 is considered a generalization of Success@10 because it rounds to 1 for r  X  10 and to 0 for r&gt; 10. GS10 drops by less and less as r increases, reflecting that users are less and less likely to read deeper into the result list.

GS10 produced more negative statistically significant dif-ferences for BF ( n s =5 in Table 2) than any of the precision-based measures.
The robustness of blind feedback has been criticized be-cause of its tendency to  X  X ot help (and frequently hurt) the worst performing topics X  [5]. However, the experimental  X  X eometric MAP X  measure of the TREC Robust Track [6] still favored BF ( p s =3 in Table 2). Primary recall measures are more consistent with the expectations of a robustness measure ( n s  X  1), and GS10 particularly so.

This result suggests that robustness is essentially about getting at least one relevant item near the top of the list for every topic, intuitively a clean fit with the Robust Track goal of avoiding  X  X bject failures X  [5]. Blind feedback is detrimental if seeking just one item. This downside is not reflected by most of the commonly reported early precision measures. A more careful focus on the first relevant item is needed.
Thanks to Donna Harman for suggesting that we test our hypothesis on the RIA archive.
