 We address the problem of how to safely compare rankers for in-formation retrieval. In particular, we consider how to control the risks associated with switching from an existing production ranker to a new candidate ranker . Whereas existing online comparison methods require showing potentially suboptimal result lists to users during the comparison process, which can lead to user frustration and abandonment, our approach only requires user interaction data generated through the natural use of the production ranker. Specif-ically, we propose a Bayesian approach for (1) comparing the pro-duction ranker to candidate rankers and (2) estimating the confi-dence of this comparison. The comparison of rankers is performed using click model-based information retrieval metrics, while the confidence of the comparison is derived from Bayesian estimates of uncertainty in the underlying click model. These confidence es-timates are then used to determine whether a risk-averse decision criterion for switching to the candidate ranker has been satisfied. Experimental results on several learning to rank datasets and on a click log show that the proposed approach outperforms an exist-ing ranker comparison method that does not take uncertainty into account.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Ranker evaluation; Learning to rank; Click models
Comparing rankers is an essential problem in information re-trieval. In an industrial setting, an existing production ranker must often be compared to a new candidate ranker to determine whether to replace the former with the latter. Practical constraints make it difficult to make such decisions well. Because the performance of the candidate ranker is unknown, trying it out to gather data about its performance is often too risky: if it proves to be sub-stantially inferior to the production ranker, the user experience may degrade, leading to abandonment. However, existing historical data (typically collected using the production ranker) may not be suffi-ciently informative about the candidate ranker. Hence, decisions about when to switch to the candidate ranker that are based on such data can be erroneous, also leading to a degraded user experience.
Controlling the risks associated with switching to a candidate ranker requires ranker comparison methods that can (1) estimate the performance of the production and candidate rankers using only historical data, and (2) quantify their uncertainty about such esti-mates. Quantification of uncertainty is essential for controlling risk because it enables system designers to switch to a candidate ranker only when they are highly confident that its performance will not be substantially worse than that of the production ranker.
Existing ranker evaluation methods do not fully meet these re-quirements. Traditional methods rely on explicit relevance labels provided by human assessors to measure metrics such as Normal-ized Discounted Cumulative Gain (NDCG) and Mean Average Pre-cision (MAP) [20]. However, such labels are expensive to ob-tain and may not reflect the preferences of real users. Click-based approaches overcome these limitations by estimating performance from the implicit signals in users X  click behavior, using A/B test-ing or interleaving experiments [5, 6, 12, 13, 15, 16, 18, 19, 21]. Nonetheless, these methods typically require trying out the can-didate ranker to gain data about it, which, as mentioned above, may be too risky. While interleaving methods have been developed that use importance sampling to estimate ranker performance given only historical data [14], they have limited generalization proper-ties and require historical data to be gathered stochastically.
Click models [3, 10], i.e., probabilistic models of user behavior, can also estimate ranker performance given only historical data [7] and do not share these limitations. The uncertainty in the resulting estimates depends on the relationship between the historical data and the rankers to be evaluated. E.g., if a candidate ranker differs from the production ranker only in that it swaps a lowly ranked but relevant document with a highly ranked but irrelevant one, then the click model can confidently conclude that the candidate ranker is better so long as the historical data shows which of the two docu-ments is relevant. It can do so even if the result lists containing such a swap do not appear in the historical data. By contrast, if the can-didate ranker gives a high rank to a document that does not appear in the historical data, then its relevance cannot be estimated with confidence. A key limitation of existing click models is that they do not distinguish between cases such as these. Because they do not quantify their uncertainty about the comparisons they perform, they cannot judge whether the information in the logs is sufficient to compare rankers. Only methods that can measure the confidence of the performed comparisons can be used to safely decide whether to switch from the production ranker to a candidate one.
We present Bayesian Ranker Comparison (BARACO), a click model-based approach to ranker evaluation that compares the per-formance of ranker pairs using only historical data and quantifies the uncertainty in such comparisons. The key novelty lies in main-taining a full posterior distribution over the relevance of documents for queries. This posterior is used to estimate the probability that the candidate ranker X  X  performance is not substantially worse than that of the production ranker. By switching to the candidate ranker only when this probability is sufficiently high, the risks associated with switching can be controlled in a principled way.

BARACO is able to estimate not only the probability of a can-didate ranker being as good as the production ranker, but also the expected difference in performance between two rankers. Measur-ing the expected difference can be useful in practice, e.g., to select a single ranker from a set of candidates, all of which have a high probability of beating the production ranker.

We present the results of an empirical evaluation on several learn-ing to rank datasets and on a real click log published by Yandex that compares BARACO to an existing non-Bayesian click model-based ranker evaluation method that estimates only the expected difference in performance between ranker pairs, without quantify-ing uncertainty. We addresses the following research questions: RQ1 Can BARACO determine whether the production ranker should RQ2 Does BARACO produce better estimates of the difference in Our results show that BARACO has better performance than the baseline method and can identify more good rankers in a set of candidates more reliably. In addition, the full Bayesian approach enables the production of more precise estimates of the expected differences between rankers than the baseline method.
Ranker evaluation has long been a central topic in information retrieval. The classical approach is to perform offline evaluation based on the Cranfield paradigm [8, 20], where a collection of doc-uments is manually annotated by human experts. A representative set of queries together with associated user intents are crafted by a group of experts. Then, for each query, documents in the collection are assigned relevance labels. The documents and their relevance labels can then be used in metrics such as NDCG. This approach is widely used, well understood, and benefits from controlled labo-ratory settings. But it is expensive and assessors X  relevance judge-ments may not adequately reflect real users X  opinions.

Online evaluation is a family of techniques that addresses these difficulties by letting users be the judges. In A/B testing [16], the user population is split into two groups, and pairs of rankers are compared by presenting one group with one ranker and the other group with another ranker. The ranker with the best performance on a selected metric (such as CTR) is typically considered to be the winner [16]. Another approach is interleaved comparison: search engine result pages (SERPs) presented to users are obtained by in-terleaving SERPs of two competing rankers under consideration. The user feedback in the form of clicks is then interpreted as user preference for one ranker over the other [19]. The key problem of online methods is that they require user feedback to evaluate each pair of rankers, which often requires exposing suboptimal SERPs. Consequently, they may be too risky for many real-world settings.
Interleaving methods that exploit importance sampling [14] pro-vide a way to compare rankers using only historical click data. However, importance sampling requires that the source distribution is non-zero everywhere where the target distribution is non-zero (i.e., every SERP that can result from interleaving has a non-zero probability of occurring in the historical data), a requirement that is often not met in practice. Importance sampling guarantees only that the resulting estimator is unbiased, not that it has low variance: the addition of more historical data is not guaranteed to improve estimates, as new data can actually increase the variance [1]. Ad-ditionally, importance sampling has poor generalization properties compared to click-based approaches that infer relevance labels of documents. In particular, the latter can generalize across SERPs that differ in the order of documents, while the former cannot.
In recent years, a number of probabilistic models have been de-veloped to describe, understand, and predict user behavior while interacting with an IR system. In particular, click models such as DBN [3], DCM [10] and UBM [9] infer the relevance of documents for queries and model user clicks on documents in the SERP by analysing search engine log files. These inferred relevance labels can be further used for learning to rank [3] and for ranker compar-isons using click model-based metrics [7]. However, to our knowl-edge, none of these approaches provide a way to quantify the uncer-tainty in the resulting comparison, which is critical for making in-formed decisions about when to switch from a production ranker to a candidate ranker. For metrics such as EBU [23], ERR [4] and the utility and effort based metrics in [2, 7], the effect on the compari-son of a document seen just once and that of one seen a hundred of times is the same, given that the inferred relevance labels have the same value. Furthermore, these metrics do not take into account the number of previously unseen documents that appear in the SERPs produced by the candidate rankers. Hence, these approaches cannot properly control the risks associated with switching to a candidate ranker, as the uncertainty in the comparison is not measured.
We present a new approach to ranker evaluation that is designed to overcome these limitations. It compares rankers using only the log files collected through natural user interactions with the produc-tion ranker. In contrast to importance sampling based interleaving methods [14], our method does not require the data to be obtained stochastically. In contrast to the metrics in [2, 4, 7, 23], our method takes into account the uncertainty associated with the inferred rel-evance labels and the presence of unseen documents and measures the confidence of the resulting comparison. We consider two related problems. The first is the Switching Problem : deciding whether or not to switch from a production ranker R p to a candidate ranker R c . The second is the Difference Estimation Problem : estimating the expected difference in perfor-mance between the production ranker R p and candidate ranker R
The Switching Problem is as follows. System designers must decide whether or not to switch from a production ranker R a candidate ranker R c . They are willing to do so only if they are highly confident that the candidate ranker is at least almost as good as the production ranker, i.e., if where M p and M c are the expected performance of the production and candidate ranker, respectively, according to some metric; is the degree to which the candidate ranker is allowed to be worse than the production ranker; and  X  is the probability with which the can-didate ranker is allowed to fall outside this threshold. The goal of a ranker comparison method in the Switching Problem is to deter-mine whether (1) holds. Note that this requires explicitly reasoning about the uncertainty of comparisons between R p and R c .
By contrast, the goal of a ranker comparison method in the Dif-ference Estimation Problem is to accurately estimate the expected difference of the metric values for the two rankers: Unlike the Switching Problem, the Difference Estimation Problem does not require explicitly reasoning about uncertainty. Nonethe-less, it can be useful for, e.g., selecting a single ranker from a set of candidates, all of which satisfy (1).

For both problems, the ranker comparison method is given only a log L = [ l 1 ,...,l | L | ] of user interaction sessions gathered using R . Each session l j consists of the following: q , the query the user has submitted; [ d 0 ,...,d N ] , the list of N documents returned by R p that make up the SERP; and [ c 0 ,...,c N ] , the clicks produced by the user on those documents.
In this section, we describe the metric used to define M p within BARACO, the Bayesian ranker comparison method we in-troduce in  X 5. While BARACO can work with any click model that provides relevance estimates, our implementation uses DBN [3] be-cause it has shown high performance in predicting user clicks [7].
DBN, the Dynamic Bayesian Network click model [3], repre-sents the relevance of a document for a query as a combination of two variables: attractiveness and satisfactoriness . In DBN, each user interaction session l j begins with a query q from which the SERP, consisting of titles and snippets of the documents [ d d q,N ] , is generated. By assumption, the user starts by examining the first document X  X  title and snippet. If they find the document attractive , they click on and examine the document, or otherwise proceed to examine the following documents X  titles and snippets in order. If a clicked document satisfies the user, they terminate the session. Otherwise, they may or may not continue to examine the SERP. DBN assumes the user will not click a document prior to examining its snippet and cannot be satisfied by a document before clicking on it.

Specifically, for a query q the document at position i in the SERP is modelled with four binary variables: whether it is attractive ( A satisfactory ( S q,i ), examined ( E q,i ), and clicked ( C lowing relationships hold between variables for a given query: In addition, the following stochastic relationships hold: Thus, attractiveness and satisfactoriness are governed by station-ary Bernoulli distributions with unknown parameters a q,i which must be inferred from clicks, the only observed variables. As in [3, 7], we assume  X  is fixed and known. Fig. 1 depicts the relationships between all variables in a given session. Figure 1: Graphical representation of the DBN click model. The grey circles represent the observed variables.

Once a q,i and s q,i have been inferred, they can be used to com-pute a metric such as EBU [23], ERR [4], and the utility and effort-based metrics described in [2, 7]. In this paper, we use the expected effort metric defined in [7]. For a query q that yields a document list with N documents, the metric is defined as: For a given ranker R x , the metric M x required by (1) can then be defined as the expected value of rrMetric ( q ) across queries: Comparing the production ranker to a candidate ranker is compli-cated by the presence of unseen documents in the candidate rank-ings. In order to compute the values of a metric for rankings with unseen documents, the unseen documents can be either assigned some a priori values of attractiveness and satisfactoriness or ex-cluded from the ranking. The latter strategy is taken in [7] and in the baseline method used in our experiments.

Using an approach based on expectation maximisation (EM), a maximum a posteriori estimate of M x can be computed [3]. Hence, the Difference Estimation Problem can be addressed by simply es-timating M c and M p using this EM-based method and then com-puting the difference between them. However, this approach suf-fers from a key limitation. Because it is based only on maximum a posteriori estimates, the difference computed by such an EM-based approach is not the true expected difference of the metric values for the two rankers. Instead, it is only the first mode of the poste-rior distribution of the difference of the metric values, which may not be equal to the expectation if the distribution is multimodal or asymmetric. Because BARACO is a fully Bayesian approach, it can estimate the true expected value of the difference between M and M p , which leads to better quality estimates, as we will see.
Furthermore, the Switching Problem cannot be directly addressed using an EM-based approach because such an approach gives no in-formation about the uncertainty in the resulting estimate. That is, it does not estimate the distribution over the metric values of the two rankers. Consequently, it provides no way to estimate the probabil-ity that M c + is greater than or equal to M p . Instead, addressing the Switching Problem using an EM-based method requires resort-ing to a heuristic approach, e.g., switching only when the estimated difference exceeds some manually tuned threshold. This forms the core of the baseline method we compare against in  X 7.
Below, we present a method that addresses the shortcomings of an EM-based approach and enables better solutions to both the Switching Problem and the Difference Estimation Problem.
In this section, we present BARACO. First,  X 5.1 describes how the posterior distributions over a q,i and s q,i can be inferred from L , the set of user interaction sessions;  X 5.2 describes how to solve the Switching Problem by evaluating (1) when M p and M c are defined according to (4), given posterior distributions over a q,i  X 5.3 describes how to solve the Difference Estimation Problem.
Evaluating (1) and (2) requires knowing the posterior probabili-ties p ( a q,i | L ) and p ( s q,i | L ) for each ranker, query, and document. In this subsection, we describe how to estimate these posteriors.
The algorithm works by iterating over the sessions. To process the first session, the posteriors p ( a q,i | l 1 ) and p ( s puted given uniform priors p ( a q,i ) and p ( s q,i ) . Then, for each sub-sequent session l j , p ( a q,i | L j ) and p ( s q,i | L gorithm terminates when l = | L | , yielding p ( a q,i | L ) and p ( s
We now describe how to compute p ( a q,i | L j ) and p ( s given p ( a q,i | L j  X  1 ) and p ( s q,i | L j  X  1 ) and the next session l cause A q,i and S q,i are Bernoulli variables, we can model the dis-tribution over their parameters a q,i and s q,i using a Beta distribu-tion. Focusing on a q,i , this yields: where  X  is the number of observations of A q,i = 1 and  X  is the number of observations of A q,i = 0 that have occurred up to and including session j , and B (  X , X  ) is a Beta function. Beta (1 , 1) corresponds to a uniform distribution, i.e., no prior knowledge about a q,i . If A q,i is observed in session l j , then p ( a q,i dated using Bayes X  rule: p ( a q,i | L j ) = In this case, the update reduces to simply incrementing  X  or  X  . Since the Beta distribution is the conjugate prior for the Bernoulli variable, the posterior remains a Beta distribution.

In our setting, A q,i is not directly observed. However, when we know E q,i , we can directly infer A q,i from C q,i because the user always clicks on an attractive examined document. Thus, the difficult case is when we do not know E q,i , which occurs whenever i &gt; c , where c is the index of the last clicked document. The remainder of this subsection describes how to address this case.
There are two steps. Since we do not know E q,i when i &gt; c , in the first step we must instead compute a posterior over it: p ( E q,i | L j ) . Then, in the second step, we use p ( E mate p ( a q,i | L j ) and p ( s q,i | L j ) .

To perform the first step, we use the sum-product message pass-ing algorithm [1]. In particular, we extract the subgraph of the graphical model that represents the documents below the last clicked one and remove the nodes representing the satisfactoriness ( S and s q,i ) for documents without clicks. This is because it is not possible to say anything about s q,i for i &gt; c as it is not observed and has no effect on what is observed. Since the resulting graph, shown in Fig. 2, is a polytree, the sum-product algorithm enables us to perform exact inference, which yields p ( E q,i | L Figure 2: Graphical representation of the DBN click model for the part of the SERP below and including the last clicked doc-ument in a session. Grey circles represent observed variables.
For the second step, we compute p ( a q,i | L j ) and p ( s p ( E q,i | L j ) . Focusing now on p ( a q,i | L j ) and starting from Bayes X  rule, we have: p ( a q,i | L j ) = The likelihood term p ( C q,i | a q,i ) can be computed by marginalizing across A and E : Sticking (8) into (7) and ignoring normalization gives: Since we are focusing on the case where i &gt; c , we know that C q,i = 0 , i.e., d q,i was not clicked. Furthermore, we know that p ( C q,i = 0 | E q,i = 0 ,A q,i = A ) = 1 and p ( C q,i = 0 | E 1 ,A q,i = A ) = 1  X  A , yielding: Algorithm 1 ComputePosteriors ( L ) 1: InitializePriors() 2: for l j in L do 3: for i in l j do 4: p ( E q,i )  X  sumProduct ( l j ) 5: if C q,i = 1 then 8: else Now we can substitute p ( A q,i = A | a q,i ) for the values of A : Thus, (10) is the Bayesian update rule for attractiveness. When p ( E q,i = 1) = 1 , e.g., if the document was clicked, the posterior is a Beta distribution because plugging p ( E q,i = 1) = 1 in (10) yields a term conjugate to the parametrization in (5). Otherwise, we use p ( E q,i | L j ) , as computed in the first step via a message-passing algorithm that takes into account the satisfactoriness of the last clicked document and the attractivenesses of the unexamined documents. Instead of a Beta distribution, this yields a more gen-eral polynomial function. The normalization constant can be found by integration: since the function is a polynomial with known co-efficients, it can be integrated analytically and then the definite in-tegral evaluated on the interval [0 , 1] . The cumulative distribution function is therefore also a polynomial and easy to compute. The same holds for the expectation of the distribution.

Analogously, we can derive an update rule for satisfactoriness: Algorithm 1 summarizes the steps involved in computing p ( a and p ( s q,i | L ) . First, the attractiveness and satisfactoriness of all document query pairs are assigned uniform Beta (1 , 1) priors. Then, the log file is processed session by session and the attractiveness and satisfactoriness of all document query pairs in the session are updated using the Bayesian update rules described in this section: (6) and (10) for attractiveness and (11) for satisfactoriness.
To decide whether or not to switch from R p to R c determine whether (1) holds. Determining this from L requires evaluating the following double integral: p ( M c +  X  M p | L ) = Algorithm 2 BARACO-SP ( R p ,R c ,L,, X  ) 1: p ( a | L ) , p ( s | L )  X  computePosteriors ( L ) .  X 5.1 2: for k in 1: K do .  X 5.2 3: M k p  X  sample ( R p ,p ( a | L ) ,p ( s | L )) 4: M k c  X  sample ( R c ,p ( a | L ) ,p ( s | L )) 5: if M k c +  X  M k p then where M  X  is the maximum possible value of the metric defined in (4). Evaluating this integral requires knowing the posterior proba-bilities p ( M p | L ) and p ( M c | L ) , which can be computed from (3) and (4) given p ( a q,i | L ) and p ( s q,i | L ) for each ranker, query, and document.

Computing (12) analytically is hard, but it can be estimated us-ing a Monte-Carlo sampling scheme. Algorithm 2 describes this scheme, which yields a version of BARACO that solves the Switch-ing Problem.

First, we compute the posteriors p ( a | L ) and p ( s | L ) using the approach described in  X 5.1 (line 1). Then, each sampling itera-tion k consists of drawing a sample a q,i and s q,i for each ranker, document, and query from p ( a q,i | L ) and p ( s q,i | L ) . Using these samples, we compute M k c and M k p , the values of the metrics given the sampled probabilities from the k -th iteration (lines 3 X 4). Es-timating (12) then reduces to computing the fraction of sampling iterations for which M k c +  X  M k p (lines 6 X 7). If this fraction is greater than 1  X   X  , we can safely switch to the candidate ranker.
However, sampling a q,i and s q,i from p ( a q,i | L ) and p ( s is itself hard because their cumulative distribution functions are high degree polynomials that are hard to invert. Therefore, we employ the Metropolis-Hastings algorithm [1] with the expected value of the sampled distribution, which can be calculated analyt-ically through integration, as the starting position of the random walk. The proposal distribution is a Gaussian with fixed variance.
If we are interested only in the expected value of a ranker R and not the uncertainty of this estimate, we can compute it as fol-lows: Using the same sampling procedure described above, we can solve the Difference Estimation Problem by estimating this expected value: where K is the number of sampling iterations. We can similarly estimate the expected difference between M c and M p : Algorithm 3 summarizes the resulting version of BARACO that solves the Difference Estimation Problem. It is essentially the same as Algorithm 2 except that it estimates the expected difference be-tween the production and the candidate rankers X  metrics.
In this section, we describe the experimental setup used to eval-uate BARACO. This evaluation is complicated by the fact that the user X  X  information need, and therefore the true relevance labels, are unknown. We present two evaluations which deal with this problem Algorithm 3 BARACO-DEP ( R p ,R c ,L,, X  ) 1: p ( a | L ) , p ( s | L )  X  computePosteriors ( L ) .  X 5.1 2: P  X  3: for k in 1: K do .  X 5.3 4: M k p  X  sample ( R p ,p ( a | L ) ,p ( s | L )) 5: M k c  X  sample ( R c ,p ( a | L ) ,p ( s | L )) 6: P  X  in two different ways. The first evaluation, based on the LETOR datasets [17], uses manual relevance assessments as ground-truth labels and synthetic clicks as feedback to BARACO. The second evaluation, based on the WSDM 2014 Web search personalization challenge, 1 uses dwell time as ground-truth labels and real clicks as feedback to BARACO.

Another possibility would be to evaluate BARACO using ex-plicit relevance labels provided by human assessors to measure metrics such as Normalized Discounted Cumulative Gain (NDCG) but this approach is known to have low agreement with metrics based user behavior such as A/B testing or interleaving [4, 6, 19, 24], so it is natural to expect that it would have a low agreement with BARACO as well. It would also be possible to evaluate BA-RACO using A/B tests or interleavings, but A/B tests have low agreement with interleavings and different metrics collected during A/B tests such as click through rate, clicks@1 and others have low agreement with each other [19]. The only definitive way to evalu-ate BARACO would be in an industrial setting that measures long-term metrics such as engagement. Such results, however, would be difficult to reproduce. Consequently, the LETOR evaluation is the most reliable and reproducible, as it depends on indisputably unbiased ground truth. In addition, it allows us to explore how the discrepancy between the click model and user behavior affects BA-RACO X  X  performance. However, the WSDM evaluation, though less reliable, is nonetheless useful because it gives insight into how BARACO performs in a real-world setting with real users.
We compare BARACO to a baseline that, in lieu of our Bayesian approach, uses the EM-based approach described in [3] to com-pute maximum a posteriori estimates of (4) for M p and M c estimates can then be directly used in the Difference Estimation Problem. Because this approach does not compute full posteriors, it does not quantify uncertainty in the resulting estimates and there-fore cannot be directly used in the Switching Problem. Instead, the baseline method, which we call Manual Thresholding (MT) , resorts to a heuristic approach to determine whether (1) holds. In particu-lar, R c is deemed safe when  X  M c  X   X  M p &gt; m , where are the maximum a posteriori estimates of M c and M p produced by the EM-based method and m is a threshold parameter whose value must be tuned manually. Because we want to switch when-ever M c  X  M p &gt;  X  , the quantity + m acts as a buffer, i.e., an extra gap that R c must exceed to be considered safe. Adding this buffer heuristically accounts for the uncertainty in  X  M c
The need to tune m for MT poses significant difficulties in prac-tice. To understand the effect of m on the behavior of MT would require access to ground truth about a set of candidate rankers, i.e., whether they are in fact no more than worse than the production ranker. While such ground truth could be obtained using off-line or
Personalized Web Search Challenge 2013 https://www.kaggle.com/c/ yandex-personalized-web-search-challenge on-line evaluations, such evaluations pose exactly the difficulties that motivate the need for methods like BARACO: the former is expensive and may not reflect real user preferences. The latter re-quires showing potentially poor rankers to real users. Furthermore, while such ground truth, even if it could be obtained, would shed light on how m affects to which candidate rankers MT switches, it would still not make it possible to select the m that is best for a given  X  supplied by the system designers. Doing so would require a quantification of the uncertainty about each candidate ranker that is inherent to BARACO but absent in MT.

Importantly, BARACO does not require tuning m or any analo-gous parameter. On the contrary,  X  and , which are supplied by the system designers, are simply quantifications of their risk tolerance.
The first evaluation is based on the LETOR datasets [17], which include manual relevance assessments. However, they do not in-clude user clicks. In addition, even if they did, these would likely correlate only poorly with the relevance assessments, since the as-sessors may not interpret the users X  information need correctly. To address this difficulty, we instead axiomatically define the rele-vance assessments to be correct ground-truth labels and use click models to generate the clicks. Since BARACO also relies on click models, we evaluate it in settings where the clicks are generated and interpreted using different click models, to assess BARACO X  X  robustness to errors in its modeling assumptions.
 LETOR is split into six sub-datasets: HP2003, HP2004, NP2003, NP2004, TD2003, and TD2004. For each run of each algorithm, we use the data from one sub-dataset. First we train a ranker R using AdaRank [22] on all the data in this sub-dataset. AdaRank, which performs reasonably on all these datasets [22], trains a linear ranking function, i.e., a linear combination of the ranking features for a given query-document pair. The documents are then sorted based on the values produced by this ranking function.

To ensure that some candidate rankers will be better than the pro-duction ranker, we craft R p by  X  X amaging X  R Ada , i.e., randomly by adding random vectors to it. These vectors were generated by randomly sampling a normal distribution with mean equal to 0 and standard deviation of 0.2. Finally, to generate a population of can-didate rankers R c , we again perturb R p 1000 times using the same sampling method. This ranker generation methodology is moti-vated by the gradient descent algorithm [25]; the standard devia-tion value was chosen so that some of the rankers would be similar enough to the production ranker and some too different from it for the algorithm to be confident about their performance.

The next step is to generate the log file. To this end, we generate user click interaction data using the DBN, sDBN and UBM click models with three user model settings: perfect, navigational and informational. The clicks are then interpreted using the DBN click model. The parameters of the click models are summarized in Ta-ble 1, where p ( C | R ) and p ( C | NR ) denote the probability of a user clicking a relevant document and an irrelevant document, respec-tively, and p ( s | R ) and p ( s | NR ) denote the probability of abandon-ing the SERP after clicking a relevant document and an irrelevant document, respectively. The closer p ( C | R ) is to p ( C | NR ) and p ( s | R ) to p ( s | NR ) , the more noise there is in the feedback and the more difficult inference becomes. The user interaction data is gen-erated for the production ranker by randomly sampling 500 queries and generating the SERPS and clicks.

The perfect user model setting is used to obtain an upper bound in performance: the user clicks all relevant documents and no ir-relevant ones. The navigational and informational user models are based on typical user behavior in web search [11]. The navigational Model p ( C | R ) p ( C | NR ) p ( s | R ) p ( s | NR ) Perfect 1.0 0.0 0.0 0.0 Navigational 0.95 0.05 0.9 0.2
Informational 0.9 0.4 0.5 0.1 user model reflects user behavior while searching for an item they know to exist, such as a company X  X  homepage. Because it is easy for users to distinguish between relevant and irrelevant documents, the noise levels are low. The informational user model reflects user behavior while looking for information about a topic, which can be distributed over several pages. Because this task is more difficult, there is more noise in the feedback.

The clicks are generated and interpreted using either the same or different click models. When they are generated and interpreted using the same click model, our findings are not affected by the ac-curacy of the assumptions in the click model, allowing us to focus on the differences between BARACO and the baseline method. Of course, some assumptions made by DBN, which is used to inter-pret clicks, may not always hold in practice. For example, DBN as-sumes that the document that was clicked and led to abandonment is the document that satisfied the user. This assumption typically holds for navigational queries, but may not be valid for informa-tional queries. Therefore, experiments in which the clicks are gen-erated and interpreted using different click models help measure the robustness of BARACO to settings whether the assumptions underlying DBN do not always hold.

In the LETOR Evaluation setup, we compare the performance of BARACO and MT using area under roc curves (AUC), Pearson correlation, and the square root of the mean squared error (RMSE). The rankers are compared using the metric rrMetric (3).
We additionally evaluate BARACO using an anonymized click log released by Yandex for the WSDM 2014 Web search person-alization challenge. The click log contains sessions with queries submitted by a user. For each query there is a list of search results returned by the engine and the clicks produced by the user. The queries and the clicks have timestamps. However, the units of time are not disclosed. The queries, query terms, documents, and users are represented by IDs in order to protect the privacy of users.
The organizers of the challenge have defined three levels of rel-evance based on the clicks that the documents receive: documents that receive clicks with dwell time less than 50 time units have rel-evance 0, clicks with dwell time between 50 and 150 units have relevance 1, and clicks with dwell time of more than 2 time units as well as clicks that are the last clicks for a given query have rele-vance 2. We use all but the last session for a query for training and use the last session for extracting the relevance labels for query-document pairs.

The candidate rankers are generated by perturbing the result lists observed in the click log in a random way. In order to ensure that some of the candidates are better than the production ranker, the relevant documents have a higher chance to be promoted to top than the irrelevant ones.
 In the WSDM Evaluation setup, we compare the performance of BARACO and MT using the following metrics: AUC and Pearson correlation as before. But we do not use RMSE because the graded relevance and the estimated relevance have different scales from 0 to 2, and from 0 to 1 respectively. The candidates are compared using DCG instead of the metric in (3) because (3) requires a map-ping from relevance labels to attractiveness and satisfactoriness that is not available for the graded relevance in the click log X  X t could be computed using, e.g., DBN but then the evaluation would be less sound because the same click model would be used for training the parameters of the documents and for training the metric. 2
Both BARACO and MT are instantiated with the user persistence parameter  X  = 0 . 9 , as in [3, 7], = 0 . 01 . For the Metropolis-Hastings sampling procedure described in  X 5.2, the variance of the proposal distribution was set to 0.1, which was determined ex-perimentally to provide a rejection ratio of around 0.6. For each query/document pair, N samples = 1000 samples are drawn and shuffled to reduce autocorrelation in order to evaluate (1). All re-sults are averaged over 30 independent runs for each algorithm. In order to check the significance of observed differences between re-sults, we perform Wilcoxon signed-rank tests; in the result tables, denotes a significant difference at p = 0 . 01 , and M at p = 0 . 05 .
In this section, we present our experimental results aimed at an-swering RQ1 and RQ2. In  X 7.1, we analyse the performance of BARACO and MT on the LETOR data; in  X 7.2, we analyse their performance on the WSDM data.
In  X 7.1.1, we compare BARACO and MT on the Switching Prob-lem; in  X 7.1.2, we compare BARACO and the EM-based approach [3] that underlies MT on the Difference Estimation Problem. To address RQ1, we compare the ROC curves of BARACO and MT on the Switching Problem. Such curves show how the true and false positives of both methods change when we fix = 0 . 01 and vary across different values of  X   X  [0 , 1] for BARACO and m  X  [  X  4 , ] for MT. In this context, a true positive occurs when the algorithm recommends switching to R c and M c +  X  M p while a false positive occurs when it recommends switching but M c + &lt; M p . We then compare the AUC of both methods for different datasets and user and click models.

Note that this comparison is fundamentally unfair to BARACO because its parameter,  X  , does not require tuning but instead is in-put by the system designers as a quantification of their risk toler-ance. By contrast, m is a parameter that requires manual tuning and cannot be derived from  X  , which MT ignores. As discussed in  X 6, tuning this parameter is quite difficult in practice. Because the ROC curves show performance across different values of  X  and , they allow MT to  X  X heat X  by optimizing its critical parameter, a step that is unnecessary in BARACO. In other words, these ROC curves answer the following question: for each value of  X  that a system designer could input to BARACO, how well can MT match the quality of BARACO X  X  decisions about when to switch to R an oracle provides MT with the best possible value of m ? Thus, BARACO can be considered a success if it can match the perfor-mance of MT when MT is given this advantage.
 Fig. 3 plots the area under the ROC curves for BARACO and MT for all six data sets, three click models, and three user model settings. Fig. 4 shows the ROC curve for the TD2004 dataset, infor-mational user model with the DBN model used for generation. The other ROC curves, omitted for brevity, are qualitatively similar.
In the case of the LETOR evaluation experiments, this mapping is known and can be read from Table 1.
The results in Fig. 3a show that, even at the best value of BARACO substantially outperforms MT on four of the six datasets (HP2003, HP2004, NP2003, TD2004). On the other two datasets (NP2003 and TD2003), the two methods perform similarly. Anal-ysis of the latter two datasets shows that the production ranker was at a local minimum. Hence, nearly all candidate rankers are better than the production ranker and the best performance is obtained by always switching. As this degenerate policy can be represented just as well by MT as by BARACO, the two perform similarly.

Furthermore, the fact that BARACO performs nearly as well when clicks are generated and interpreted with different models, as shown in Figs. 3b and 3c, shows that BARACO is robust to vi-olations of its modeling assumptions. The baseline substantially outperforms BARACO only for the perfect sDBN user model on the TD2003 and TD2004 datasets. The baseline shows superior performance because there are many more relevant documents in the TD datasets, and many of them are not presented to the user by the production ranker. In the case of the perfect user model, the user only clicks on relevant documents. Therefore, there are many queries for which no clicks are produced because no relevant docu-ments were shown. The baseline essentially removes such queries from consideration through condensing [6], which may be an ef-fective strategy in this case. Overall, these results demonstrate that BARACO can offer a robust and effective means for deciding when to switch rankers: especially in cases where its modeling assump-tions hold, it outperforms MT with a tuned m parameter for nearly all combinations of dataset and user model.

The values of m shown here were chosen because their inflec-tion point lies in the interval  X   X  [0 , 1] . These are all negative values of m because MT has a consistent negative bias: almost all candidate rankers receive a score lower than the production ranker. This bias is a consequence of condensing [6]: almost all candidate rankers have unseen documents that do not contribute to the met-ric. This further highlights the brittleness of MT: as more data is collected, the relative number of unseen documents may decrease, which would reduce the effect of condensing and therefore the amount of bias, necessitating a retuning of m .
To address RQ2, we compare BARACO to the EM-based method [3] that underlies MT, on the Difference Estimation Problem. BA-RACO uses (14) to estimate the expected difference while the EM-based method uses  X  M c  X   X  M p , where  X  M x is the maximum a pos-teriori estimate of M x . First, we consider how the RMSE scores of the two approaches differ. Error is defined here to be the dif-ference between the true and estimated value of M c  X  M p true values are computed from the expert-generated relevance la-bels in the datasets. Table 2 summarizes the MSE of BARACO and MT. These results show that BARACO consistently outperforms the EM-based approach, in many cases by an order of magnitude. The only exception is the TD2004 dataset for clicks generated using UBM. This exception occurs because there are many unseen rele-vant documents in the TD2004 dataset and, when the user model as-sumptions do not hold, the baseline X  X  condensing strategy [6] may be more effective because it does not rely on these assumptions.
However, the fact that BARACO has lower RMSE scores is im-portant only if we are interested in the absolute values of the metric differences. Instead, if we want to be able to rank the candidate rankers by their metric values, we need a different way to compare the methods. To this end, we measure the Pearson X  X  correlation between the ground truth value M c  X  M p and the estimates pro-duced by BARACO or MT. For example, if the correlation with the ground truth was perfect, ordering all the candidate rankers by Figure 3: AUC for all data sets, user and click models. The error bars are standard errors of the means. P -perfect user model setting, I -informational, N -navigational (LETOR eval-uation). their ground truth difference with the production ranker would be the same as ordering them by the estimated difference. Thus, the correlation with the ground truth is more informative than RMSE in cases where we care about preserving the ground-truth rank-ing. This occurs, e.g., when several candidate rankers confidently outperform the production ranker. In such cases, it is desirable to switch to the one that outperforms it by the largest margin, while the exact absolute values of the estimated metrics are not important.
Table 3 summarizes the correlations between the ground truth difference of rankers and the difference of rankers computed by BARACO and the EM-based method. Higher correlations with the ground truth mean that the way the rankers are ranked is closer to the ground truth. These results show that BARACO again out-performs the EM-based method. The negative correlation in the informational setting of NP2003 dataset is due to a heavily skewed distribution of candidate rankers when the production ranker is at a Figure 4: The ROC curves for BARACO and MT, using DBN for generation and interpretation, TD2004 dataset, informa-tional user model (LETOR evaluation).
 Table 2: RMSE between the predicted outcome of the compar-ison and the ground truth, P -perfect user model setting, I -informational, N -navigational (LETOR evaluation).
 Dataset UM BARACO EM BARACO EM BARACO EM HP2003 P 3 . 1 e  X  5 N 3 . 0 e  X  4 3 . 8 e  X  5 N 3 . 1 e  X  4 3 . 5 e  X  5 HP2004 P 2 . 7 e  X  5 N 2 . 3 e  X  4 4 . 3 e  X  5 N 2 . 0 e  X  4 2 . 7 e  X  5 NP2003 P 1 . 3 e  X  5 N 2 . 9 e  X  4 1 . 5 e  X  5 N 2 . 8 e  X  4 1 . 4 e  X  5 NP2004 P 4 . 1 e  X  6 N 6 . 2 e  X  5 6 . 6 e  X  6 N 5 . 8 e  X  5 4 . 3 e  X  6 TD2003 P 7 . 0 e  X  5 N 1 . 1 e  X  4 3 . 7 e  X  4 H 1 . 2 e  X  4 6 . 8 e  X  5 TD2004 P 4 . 0 e  X  4 N 7 . 8 e  X  4 1 . 2 e  X  3 H 5 . 9 e  X  4 5 . 1 e  X  4 local minimum of the ranker space and almost all candidate rankers are better for the queries in which the production ranker has not presented any relevant documents. This situation is unlikely to oc-cur in the real world since production rankers are typically highly engineered and thus more likely to be local maxima than local min-ima. As with our earlier results, we see that the performance on the TD2004 dataset using UBM for generation is qualitatively different from the other conditions for the reasons mentioned earlier.
To answer RQ2, we observe that both the RMSE and correlation results show that BARACO outperforms MT: BARACO achieves better estimates in both absolute and relative terms, except on the TD2004 dataset with the UBM click model for generation, whose special nature has been recognized before. In this section, we compare BARACO and MT on the Switching Problem and Difference Estimation Problem, respectively, using the WSDM experimental setup Table 3: Correlation between the predicted outcome of the comparison and the ground truth, P -perfect user model set-ting, I -informational, N -navigational (LETOR evaluation) Dataset UM BARACO EM BARACO EM BARACO EM HP2003 P 0 . 961 N 0 . 846 0 . 950 N 0 . 858 0 . 963 N HP2004 P 0 . 963 N 0 . 933 0 . 946 0 . 937 0 . 969 N 0 . 940 NP2003 P 0 . 979 N 0 . 890 0 . 982 N 0 . 901 0 . 982 N NP2004 P 0 . 922 N 0 . 841 0 . 915 N 0 . 871 0 . 925 N TD2003 P 0 . 738 M 0 . 678 0 . 398 H 0 . 619 0 . 732 0 . 723 TD2004 P 0 . 846 M 0 . 783 0 . 434 H 0 . 817 0 . 887 N To address RQ1, we compare the ROC curves of BARACO and MT on the Switching Problem. The AUC of these curves for both methods are stated in Table 4. The AUCs illustrate that both meth-ods are able to distinguish between strong and weak candidates. Both methods suffer from a weak, systematic bias X  X hey consis-tently underestimate the quality of the candidates because the rele-vance labels used in the evaluation are biased towards top results. Table 4: AUC and Correlation between the predicted outcome of the comparison and the ground truth (WSDM evaluation).
The real proportion of candidate rankers that are better than the production ranker across different probability levels computed by BARACO is summarized in Table 5. The probabilities output by BARACO are not perfectly calibrated and instead tend to be under-estimates. Thus, not all risk prescribed by  X  and can be utilized, making the system somewhat overly conservative.

Unfortunately, a limitation of these WSDM experiments is that there is no way to ascertain how much of this bias is due to dis-crepancies between the relevance labels and the true information needs of the users who generated the clicks and how much is due to discrepancies between BARACO X  X  click model and those users X  behavior. However, because the bias is consistent, correcting for this bias, e.g., by learning a constant offset, is straightforward.
The correlations between the true and estimated value of M M p computed by BARACO and MT are also stated in Table 4. The estimated difference between rankers is strongly correlated with the ground truth in the WSDM dataset, suggesting that both methods can estimate the difference between rankers well given the logged user interactions with the production ranker. evaluation).

In both experimental setups, i.e., the LETOR and WSDM setup, both BARACO and MT have good performance in most cases and high agreement with the ground truth. In the LETOR setup the per-formance is often superior to that in the WSDM experiments, espe-cially when there is little noise and no discrepancy between the user behavior and the click model. However, when there is much noise and the clicks are generated and interpreted using different click models, the performance drops to levels lower than in the WSDM experiments. Overall, these results show that, given a reasonable click model, BARACO makes it possible to make informed deci-sions whether or not to switch to a candidate ranker given historical user interaction data obtained using the production ranker.
We presented BARACO, a new click model-based method of ranker comparison with two key features: (1) it compares the per-formance of rankers using only historical data, and (2) it quantifies the uncertainty in such comparisons. Using BARACO, it is possi-ble to decide, using only historical data collected with the current production ranker, whether one can confidently replace the produc-tion ranker with a candidate ranker. The algorithm takes as input , the degree to which the candidate ranker is allowed to be worse than the production ranker;  X  , the probability with which the candidate ranker is allowed to fall outside this threshold; and user interaction logs collected with the production system.

Our experiments show that BARACO can correctly and confi-dently identify candidate rankers that are as good as the produc-tion ranker. BARACO outperforms or has performance comparable to that of the MT baseline that requires manual tuning of the thresh-old m through offline assessments or online user experiments.
A natural application of BARACO is within online learning to rank algorithms, many of which require an evaluation subroutine. For example, in Dueling Bandit Gradient Descent (DBGD) [25], the algorithm randomly picks a candidate ranker from the neigh-borhood of the current ranker, compares it to the current ranker, and, if the candidate ranker appears to be better, updates the current ranker so that is is closer to the candidate ranker. The evaluation step is usually done using interleaving, which requires showing po-tentially poor rankings to the user. Using BARACO, DBGD could be performed while restricting the rankings shown to users to those generated by production rankers or candidate rankers in which we have sufficient confidence.

