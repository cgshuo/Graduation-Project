 Instituto de Investigaci X n en Inform X tica de Albacete (i3A) and Departamento de Sistemas Inform X ticos, Universidad de Castilla-La Mancha, Albacete, Spain 1. Introduction
Complex and emergent information systems is an extensive field of knowledge. Generally speaking, a complex system (CS) is a composite object which consists of many heterogeneous (and, in many occa-sions, complex as well) subsystems [26], and incorporates emergent features that arise from interactions within the different levels. Such systems behave in non-trivial ways, originated in composite functional internal flows and structures. As a general rule, researchers face difficulties when trying to model, sim-ulate and control complex systems. Due to these facts, it would be correct to say that one of the crucial issues of modern science is to puzzle out the CS paradigm. In fact, modeling complex systems is not an easy and trivial task. Because of high complexity of CS, traditional approaches fail in developing theories and formalisms for their analysis [31].

Such a study can only be realized by a cross-sectoral approach, which uses knowledge and theoretical backgrounds from various disciplines as well as collaborative efforts of research groups and interested institutions. On the other hand, an effective approach to CS study has to follow the principles of system analysis, when we have to switch over to the abstract view of the system and perform the following flow of tasks:  X  Description of a system. Identification of its main properties and parameters.  X  Study of interconnections amongst parts of the system, which includes informational, physical,  X  Study of external system interactions with the environment and with other systems.  X  System decomposition and partitioning. Decomposition supposes extraction of series of system  X  Study of each subsystem or system part, using optimal corresponding tools (multidisciplinary ap- X  Integration of results received from the previous stage, and obtaining a pooled fused knowledge
In Table 1 some possible approaches and tools are offered, which can be used at each stage and are not limited by the mentioned methods. To view in more details the stages (as provided in the second column of Table 1) and the methods (offered in the third column), the most frequently used are described. The first stage is  X  X ystem description X  that serves as a background for future system analysis. On this stage knowledge, rules and databases are created. A number of various methods are applied here: (1) expert description, essential in this case, can be combined with graphical methods of system representation (data flow diagrams, decision tables and trees, and so on), (2) graphs in form of flow graph represen-tations, potential graph representations, resistance graph representations, line graph representations, (3) Petri nets, (4) taxonomies, vocabularies, and various kinds of hierarchies, (5)  X  X ND-OR X , morphologi-cal trees and their modifications, (6) ontologies which unify necessary expert and technical information about domains of interest and databases.

The second stage is  X  X ystem decomposition X , necessary for studying the system components. Among the possible solutions there are criterion-based methods. The third stage,  X  X tudy of subsystems X , implies a wide usage of problem-solving methods aimed to describe knowledge and reasoning methods used to solve a task. Then, on this stage there is a great variety of data mining techniques to solve the follow-ing tasks: (1) classification, (2) regression, (3) attribute importance, (4) association, and (5) clustering (e.g. [8,23]). Some specific methods used for this list of tasks include statistics, artificial intelligence, decision trees, fuzzy logic, etc. Of course, there are a number of novel methods, modifications and hy-bridization of existing tools for data mining that appear permanently and are successfully used at this stage [11]. At the fourth stage,  X  X ntegration X , sets of methods dedicated to integration and composition are used: (1) methods for data integration, (2) multi-criteria evaluation, (3) decision making, including group and distributed decision maki ng, (4) evolutionary methods, (5) ar tificial intelligence methods, and so on.

A researcher has to accept some assumptions about the system X  X  detailed elaboration level to be able to analyze it in practice. This means that he/she has to chose those entities considered to be the elemental ones for research. As a rule, this task is solved by specialists. Generally, there are various specialists who collaborate on the same case study. Levin [26] enumerates the traditional classes of specialists: (1) novice or beginner, (2) professional or competent, and (3) expert. The author cites the classification of Denning [7] who suggests a seven-level specification: (1) novice or beginner, (2) advanced beginner or and (7) legend. The role played by specialists, especially at higher levels, should not be underestimated, as they provide informational support and determine an ontological basis of the research. The qualitative and quantitative research outcomes have to be evaluated. In case of complex systems different types of criteria are needed, often hybrid and at composite scales, as mono-scaled viewpoints have proved themselves to be wrong and unappropriated for CS solutions [33]. Usually, in such cases they have to be provided as local measurements for each component of the abstract system model, and general evaluation criteria for the system are received as a fused hybrid estimation.

The rest of the paper is organized as described next. Section 2 presents a short state of the art in the sphere of complex systems modeling. Next, Section 3 introduces DeciMaS, our methodological ap-proach to mining and simulating data in complex information systems. Then, in Section 4, the DeciMaS data mining work flow is explained in detail. In order to assess the proposal, an experiment designed to evaluate the possible harm caused by environmental contamination upon public health is introduced in Section 5. Lastly, some conclusions are drawn in Section 6. 2. Related works
Complex adaptive systems are characterized by self-organization, adaptation, heterogeneity across scales, and distributed control. Though these systems are ill-determined, ambiguous, uncertain, they have commonalities, that help working out a general way for their study [41]. Also, complex systems or systems of systems are characterized by high complexity and great number of interacting components. Here difficulties already appear during the creation of an abstract model of a CS, due to a great number of decisions to be made regarding its design. Non-traditional tools from different domains are highly effective in case of CS, providing novel ways for decision generation and solutions finding. CS cover several dimensions, including economical, ecological, social sub-systems, which may have any level (positive or negative) of collaboration and acceptance between them [28,32].
Notice that the aim of this approach is to introduce an integrated framework based on the agent paradigm, which enables intelligent knowledge discovery and support to a specialist during all stages of the decision creation process. Indeed, as the decision making environment has become more com-plex and decentralized, support provided to decision makers by traditional one function or single user focused decision support system (DSS) has evolved from simple predefined reports to complex and in-telligent agent-based analysis, suggestion and j udgment [15,16]. A little kno wledge of non-specialists in data mining leads to erroneous conclusions and dangerous decisions. The best solution to avoid these problems is to apply a  X  X hite-box X  methodology, which suggests that users should understand the al-gorithmic and statistical model structures underlying the software [25]. However, usage of intelligent agents may lead a user through a sequence of steps starting with information retrieval and finishing with the creation of decision alternatives and their evaluation. In this case it is more effective to use  X  X lack-box X  methodologies for data mining applications, where the agents create and simulate the dynamic behavior of a complex system.

Indeed, many decision support and expert systems are based on intelligent agents [22,29]. One tra-ditional field of application of DSS is medicine, and some recent academic reports deal with examples of novel usage of agent-based DSS for home and hospital care, pre-hospital emergency care and health monitoring and surveillance [2]. Some complex systems are sensor-based, such as medical equipment and diagnostic centers [24,30], modern sensor-based aircrafts [42], or software program tools, such as expert systems for socio-environmental simulation [38].

Specialists working with CS often face difficulties in their analysis and decision making. This problem is caused by the existence of numerous approaches that overlap and supplement each other, but do not meet all requirements of a specialist who needs a multi-focal view on a problem at hand and clear methodology applied to it. Existing approaches offer their solutions, however just a few are organized into methodologies [26,33]. However, not all of them permit both to extract general principles as well as to create specific decisions for a given domain. 3. The DeciMaS approach for complex systems study
The purpose of the DeciMaS framework is to provide and to facilitate complex systems analysis, simulation, and their comprehension and management. From this standpoint, principles of the system approach are implemented in this framework. The overall approach used in the DeciMaS framework is straightforward. The system is decomposed into subsystems, and intelligent agents are used to examine them. Then, obtained fragments of knowledge are pooled together and general patterns of the system behavioral tendencies are produced [35,36].

The framework consists of the following three principal phases: 1. Preliminary domain and system analysis. This is the initial and preparatory phase where an analyst, 2. System design and coding. The active  X  X lement X  of this phase is a developer who implements an 3. Simulation and decision making. This is the last phase of the DeciMaS framework. During this
The proposed framework offers a sequence of steps to support a domain expert, who is not a specialist in data mining, during the knowledge discovery process. With this aim, a generalized structure of a decision support system is worked out. The DSS is virtually and logically organized into a three-leveled and pre-processing, the second one discovers knowledge from the data, and the third layer deals with making decisions and generating output information. The DSS has on open architecture and may be filled with additional data mining methods. 4. The DeciMaS data mining work flow
In a standard DSS, information is transformed from an initial  X  X aw X  state to a  X  X nowledge X  state, which suggests organized data sets, models and dependencies, and, finally, to a  X  X ew knowledge X  state that contains recommendations, risk assessment values and forecasts.

The way the information changes, as it passes through the DeciMaS stages, is shown in Fig. 2. In general terms, data mining refers to extracting or  X  X ining X  knowledge from data sources. Data matching is a process of bringing together data from different, and sometimes heterogeneous, data sources and comparing them to find out whether they represent the same real-world object [1,9]. However, data mining may be viewed as a part of a knowledge discovery process which consists of an iterative sequence of the following steps [13]: 1. Data cleaning (to remove noise and inconsistent data). 2. Data integration (where multiple data sources may be combined). 3. Data selection (where data relevant to the analysis task are retrieved from the database). 4. Data transformation (where data are transformed or consolidated to be appropriate for mining by 5. Data mining (an essential process where intelligent methods are applied to extract data patterns). 6. Pattern evaluation (to identify the truly interesting patterns representing knowledge based on some 7. Knowledge presentation (where visualization and knowledge representation techniques are used to
Actually, steps from one to four constitute data preprocessing, which is performed to prepare data for mining. The DeciMaS framework uses various data mining techniques to support knowledge transfor-mation to achieve the following global goals:  X  Information preprocessing. As a rule, primary data are incomplete, noisy and inconsistent. At- X  Data mining. In general, data mining is aimed to solve the following problems: association, classi- X  Support in decision making. DeciMaS supports data mining process, presenting to a user infor-5. Data and results
The experiment designed to evaluate possible harm caused by environmental contamination upon public health was conducted for the Spanish region of Castilla-La Mancha. Retrospective data dated from 1989 until 2007, was used. Resources offered by Instituto Nacional de Estad X stica and by Instituto de Estad X stica de Castilla-La Mancha were used for the research [17]. The factors that described the  X  X nvironmental pollution  X  Human health X  system were used as indicators of human health and as influencing indirect factors of environmental pollution [19]. Morbidity, classified by sex and age, was accepted as an indicator to evaluate human health. The diseases included in the research were chosen in accordance with the International Statistical Classification of Diseases and Related Health Problems (ICD) [18] and are given in Table 2. Pollutants are represented with indirect pollution sources such as number of vehicles in use, wastes (including dangerous wastes), quality of potable water, and others (see Table 2).

Information was retrieved from CSV, DOC and XLS-files and fused together. There were 148 data files that contain information of interest. After extraction, data was placed in data storages agree with the domain ontology. 5.1. Information preprocessing The case of the human health impact assessment appeared to be an optimal domain for application. First, the information is scarce and heterogeneous. The heterogeneity of the initial data is multiple as var-ious parameters (for example, morbidity indicators versus waste subgroups) have different amounts of data available. The periods between registration of parameters were different. For example, one parame-ter was registered on a monthly and the other one on a yearly basis. Some data was measured in different scales, for example, morbidity was measured in persons and in thousands of persons. Second, data sets appeared to be short, and it was decided to apply special data mining methods, such as GMDH-based models, and committee machines.

Last but not least, the domain of the study represents one of the most difficult problem areas. The real interrelations between its components have not been thoroughly studied yet, even by domain ex-perts [12]. That is why application of the DeciMaS methodology can be very effective as it facilitates the discovery of new knowledge as well as gives a new understanding of the nature of the complex system. Data are checked for the presence of outliers. These can be caused by registration errors or misprints. Outliers are eliminated in the previous step, and are marked as missing values. The data sets with more than 30% of missing values were excluded from the analysis. The outcomes are given in Table 3. The bar chart, given in Fig. 3, visualizes the filling gaps procedure for given data sets before (in red) and after (in blue).

Next, data were normalized and smoothed. The reason to apply smoothing is to homogenize data after the treatment of missing values. The exponential smoothing with the coefficient  X  equal to 0.15 was used. Data is normalized using two normalization methods:  X  Z -score standardization X  (for the case extreme values are not established) and the  X  Min-Max  X  normalization (if minimal and maximal values are presented in a data set).

Decomposition of the studied complex system,  X  X nvironmental pollution-Human health X , was carried out by means of correlation analysis. A set of non-correlated independent variables X for each dependent variable Y were created. The independent variables (pollutants) that showed insignificant correlation with the dependent variable (disease), were also included into the set. Owing to this fact that data sets are short, non-parametric correlation coefficients are calculated using rank correlation coefficient and Kendall X  X   X  statistic. 5.2. Data mining
For every class of diseases, plotting morbidity value against pollutant or several pollutants, regression analysis was performed. Simple, power, hyperbolic and exponential regression models were created. Each model was evaluated with Fisher F-value. The models that did not satisfy the F-test were elimi-nated from the list of accepted models. Generally, the number of accepted regression models was low, the predictability of the best performing univariate regression models ranged from 0.48 to 0.82 for the discrimination coefficient. Figures 4 and 5 present examples of regression models and approximation to real data.
 and is equals Y 0 =6 . 42 X 1  X  0 . 068 . The red line represents initial data, and the blue line represents data approximated with the model. The correlation coefficient for this model equals R = 0.48, the determination coefficient equals D = 0.23 and the F-criterion equals F = 4.4. The regression model shows better results in fitting the initial line, as well as proving the statistical criteria: the correlation coefficient R = 0.82, the determination coefficient D = 0.68 and the F-criterion F = 30.09.
In general, univariate regression models for the current case study have been characterized with low values of statistical indicators and have demonstrated that they cannot be used for modeling. Multiple regression models have shown better performance results. For instance, the multiple regression model for the Y 15 is given in Fig. 6.

The model is written as Y 15 =0 . 022 X 14 +0 . 001 X 4 +0 . 012 and its statistical criteria for this model are: the correlation coefficient R = 0.77, the determination coefficient D = 0.59 and the F-criterion F = 20.69. Meaning the explanatory variables, X 4 and X 14 , explain the dependent variable Y 9 in 59% of cases. In other words, in 59% of cases the model would give a correct result, and in 49% of cases the model would give an incorrect result.

Neural network-based models, calculated for the experimental data sets, have demonstrated high per-formance results. Networks trained with resilient propagation and with backpropagation algorithms have similar architectures, and the training and testing procedures were equivalent. Neural networks trained with genetic algorithms were also applied. In this case, the genes of the chromosome represent weights, and with every new offspring population the error function is moved down until it approaches the optimal or acceptable solution, by changing the matrix of internal weights [40].

Networks trained with resilient propagation and with backpropagation algorithms have similar archi-tectures, and the training and testing procedures were equivalent. Feedforward networks trained with the backpropagation algorithm, the values of learning rate and momentum were varied within the interval [0 and the values of momentum within the range [0 . 3 , 0 . 4] . Feedforward neural networks trained with the resilient propagation training algorithm have shown high performance results with the zero tolerance
Neural network trained with genetic algorithms using training sets and with the following parameters for training:  X  Population size, is the size of population, used for training, vary from 30% to 60%,  X  Mutation percent, equal to 20%, is the percent of the population, to which the mutation operator is  X  Percent to mate, equal to 30%, is the part of the population, to which the crossover operator is
Dependence between diseases and pollutants was modeled with GMDH-algorithms which enable identifying both linear and nonlinear polynomial models using the same approach [20]. Figure 10 shows the models Y 27 =0 . 2 X 31  X  0 . 04 X 60 +0 . 24 with R = 0.94, D = 0.88, MAE = 0.077 and Y 89 =4 . 20 X 31  X  0 . 01 X 21  X  1 . 01 with R = 0.91, D = 0.83, MAE = 0.124.

In general, GMDH-based models demonstrate high performance results and efficiency when working with short data sets. The models were received with combinatorial algorithm, where the combination selection of the model was stopped when the regulation criterion started to reduce.

A final model for every dependenceis a committee machine that is written as Y = f ( X 1 ,X 2 ,...,X n ) , where n is a number of pollutants included into a model. As an example of a committee machine, the out-comes of modeling for the variable of interest Y 35 Disease : External causes of death. Age group : all ages are discussed. First, after the decomposition of the number of variables (pollutants) that could X and were then evaluated and the best were selected.

The models which are included into this committee machine are: The final model generated by the committee machine is: where f i is a model, included into the committee machine, and R f i is the correlation coefficient for the i  X  th model, i  X  [0 ,...,n ] ,where n is the number of models.

Table 4 shows statistical criteria for the models included into the committee machine, and for the com-mittee machine. The criteria calculated are: the correlation coefficient R , the determination coefficient D , and the minimal absolute error MAE . Table shows that the committee machine models has been eval-uated with higher results: the correlation coefficient R = 0.91, the determination coefficient D = 0.83 and the mean absolute error MAE = 0.12. The second column indicates pollutants which was included into the model. The committee machine includes all the factors from the models. 5.3. Simulation
For this case study, a decision is made by the specialist; however, information that could help him/her to ground it is offered by the system. First, models in the form of committee machines and predictions were created, and hidden patterns and possible tendencies were discovered. Second, the results of im-pact assessment explain the qualitative and quantitative dependencies between pollutants and diseases. Finally, the possib ility of simulation is supported by the DSS.

The committee machine model for the variable of interest  X  X eoplasms X , Y 20 , given in formula 1, is used. Suppose, there is a need to change the value of a pollutant and observe how a specific morbidity class would response to this change. Suppose that the pollutant is  X  X sage of fuel-oil X , X 8 .Thereare five models that compose a committee machine for the variable Y 20 and one of the models, included into the committee machine for this disease (neural network trained with resilient propagation algorithm) includes X 8 as an input variable. Table 5 shows outcomes of simulation.

The first column contains values predicted by a model, the others contain values of Y 20 calculated under the hypothesis that the variable X 8 is going to vary. With this aim the values of the variable X 8 are increased to 20, 40, and 80 percents. The model is characterized with correlation coefficient R = 0.904 and F = 7.354 ( F&gt;F table ). The determination coefficient, D , shows that the variable X 8 explains approximately 81.8% of the variable Y 20 . The values of the variable Y 20 are given in a normalized scale, and represent relative growth of the process. 6. Conclusions
In the DeciMaS framework we have embodied two solutions that were proposed in the Introduction section. Firstly, we have brought together existing methods for decision support systems creation, and more concretely within an agent-based decision support system architecture. These methods include data preprocessing, data mining and decision generation methods and approaches. Second, we have introduced an interdisciplinary and flexible methodology for complex, systemic domains and policies, which facilitates study and modeling for a wide range of applications.

The case study presented in the article was performed in order to apply the DeciMaS framework for the identification and evaluation of environmental impact upon human health and generation of alternative decisions sets. The computational experiment was carried out by means of an agent-based decision support system that sequentially executed and completed each stage of DeciMaS. The study resulted in several constitutive outcomes and observations, r egarding both subjects an d methods of the study. It supported making predictions of its possible states and creating a pool of alternative decisions for the better understanding and possible correction of the complex system.

Modeling helped to discover non-linear relationships between indicators of human health and pollu-tants, and generate linear and non-linear mathematical models, based on hybrid technique, which in-cluded different types of regressions and artificial neural networks. Our results show that models based on hybrid collective machines and neural networks may be implemented in decision support systems, as they have demonstrated good performance characteristics in approximation and forecasting. This performance gap confirms a non-linear relationship between factors influencing morbidity. Finally, dif-ferent predictions of possible morbidity states under various assumptions were calculated with hybrid and neural network-based models. The predictions were generated with sensitivity analysis for the cases of explanation variables increasing and decreasing.

The general conclusions to the usage of the DeciMaS framework are the following ones. DeciMaS supports the general standard flow of steps for information system X  X  life cycle. It makes the DeciMaS framework useful and general for application across a wide range of complex domains. The possibility of DeciMaS to be easily adapted to any domain of interest is important. The framework is organized in such a way that the change of domain is realized during the first stage of the DeciMaS framework, but all the further procedures of data mining and decision generation are completed in a similar way for various domains. This characteristic adds flexibility to the framework and widens the areas of its application. Moreover, usage of agent teams let to distribute, control, and synchronize the work flows within the system that are supervised and organized by the team leader agents to manage autonomous knowledge discovery.

Additionally, the DeciMaS framework uses known terminology, and integrates tools and methods from various disciplines, making good use of their strong sides. This facilitates the usage of the DeciMaS framework by non scientific users. Acknowledgements
This work was partially supported by Spanish Ministerio de Econom X a y Competitividad/FEDER under project TIN 2010-20845-C03-01, and by Spanish Junta de Com unidades de Castilla-La Man-cha/FEDER under project PII2I09-0069-0994.
 References
