 With the advent of the era of big data, a lot of information stored in the form of structured data has been built in knowledge bases which are multi-relational graph data whose nodes represent entities and edges corresponds to relations [2]. Relations in knowledge bases are always be denote by triples in the form of (entity, relation, entity). Recently, knowledge bases are extremely useful re-source for many nature natural language processing tasks such as coreference resolution, information retrieval, recommendation systems and social networks. However, the entities and relationships that exist in knowledge bases are always incompleteness for users due to various practical reasons. Hence, learning new facts based on the knowledge bases is a necessary way to improve them. Markov Logic Network et al.) has focused on extending existing knowledge bases using pattern or classifiers applied to large text corpora[1]. However, not all knowledge is recorded by text, especially common sense which is obvious to people. For example, given the fact ( t {  X   X   X  !  X   X   X   X  ), a person can infer the new fact ( t { ... /  X   X   X  !  X   X   X   X  ) without needing to find any textual evidence. Therefore, learning new relations(triples) based on knowledge bases has been increasingly popular. Nickle et al.(2011 and 2013)[3, 4] introduced a tensor factorization method for learning on multi-relation data. Mukherjee et al.(2013)[2] used a matrix tri-factorization approach to extracting new facts in knowledge bases. Recently, Socher et al.(2013)[1] applied a neural tensor network to reasoing common sense, which is the base of our work.
 plish common sense reasoning with the facts that exist in Chinese knowledge bases. This network model can accurately predict the additional relationships among entities that are not exist in knowledge bases. We represent every entity as a vector. For sharing statistical strength among the entities that contain sim-ilar substrings, we represent each entity as the average of its word or character vectors whose initial value are pre-trained with a neural network language model based on an unsupervised large scale corpora or are sampled from a zero mean Gaussian distribution. Each relation corresponds to a group of parameters of neural tensor networks. The entities and relationships can interact well through the neural tensor networks.
 new facts based on Chinese bases. The paper is organized as follows. Section 2 and section 3 introduce some related work and the Neural Tensor Network model , repectively. Section 4 discuss how to use the NTN model to reason on Chinese knowledge bases. Section 5 reports parameters X  setting and results of experiments. Section 6 we summarize our contribute and consider the further work directions. This work mainly involves two areas of NLP research: semantic vector spaces and deep learning.
 Semantic vector spaces Neural language models (Bengio et al.2003; Mnih and Hinon,2007; Collobert and Westion,2008; Schwenk and Gauvain,2002; Ema-mi et al.,2003) have been shown to be very powerful at language modeling, a task where models are asked to accurately predict the next word given previ-ously seen words[5]. By using distributed representations of words which model words X  similarity, most of these models addresses the data sparseness problem that n-gram models encounter when large contexts are used. Collobert and We-ston(2008) used a neural language model to compute scores g ( s ) and g ( s w ) of the n-gram s and s w , where s w is s with the last word replaced by word w , and a ranking-loss training object to make g (s)to be larger than g ( s w ) by a margin of 1 for any other word w in the vocabulary. This model have showed increased performance and the word embedding produced by it have been used in many lit-eratures. Huang and Socher(2013) added global information of documents to the neural language model of Collobert &amp; Weston and produced multiple word pro-totypes. Our Chinese word and character vectors come from Huang and Socher X  X  language model.
 Deep learning The neural tensor network is related to several neural network models in deep learning literature. Ranzato and Hinton introduced a factored 3-way Restricted Boltzmann Machine which is also parameterized by a tensor[1]. D.Yu and L.Deng introduce a model with tensor layers for speech recognition[1]. Socher and Chen[1] introduced the neural tensor network for reasoning for knowl-edge bases completion, and they developed a recursive version of this model for sentiment analysis. Bowman[5] trained a recursive neural tensor networks model on a new corpus of constructed examples of logical reasoning in short sentences and the result is promising to capture logical reasoning. Nickle[3] introduced a tensor factorizaton method for multi-relational learning, where a knowledge base was regarded as a three dimensional tensor. This section introduces the neural tensor network to reason relationships among entities by learning vector representations for them. As shown in Fig. 1(part and relationship respectively, corresponds a tensor network whose inputs are e ( i = 1 , 2). The model obtains a high confidence if they are in that relationship and a low one otherwise. The following are three sections: (i)model structure, (ii)training objective, (iii)classify relation triples. 3.1 Model Structure In this model, we introduce the NTN model structure. First, we define a set of parameters indexed by R for each relation X  X  scoring function and let e 1 , e 2 be the vector representations of two entities. Then the Neural Tensor Network (NTN) computes the score of a given triple ( e 1 ,R,e 2 ) through a bilinear tensor layer which relates two entity vectors across multiple dimensions. where f = tanh or f = sigmoid that often be used in neural network models, T h  X  R k , where each entry is computed by one slice i = 1 ,...,k of the tensor: h form of standard neural network. The main advantage of this model is that it can relate the two inputs multiplicatively, which incorporates the interaction of the two inputs efficiently. 3.2 Training Objective higher confidence than a triple whose second entity is replaced with a random entity. Provided that there are N R many different relations which are indexed by R ( i ) ( i = 1 ,...,N R ). Each relation corresponds to a set of neural tensor net parameters. We call the triple with a random entity negative sample and rep- X  = U,T,W,b,L , where U,T,W,b are model parameters and L is word vectors. We minimize the following objective: Where N is the total number of triples in the training set. We score the positive relation triple higher than its negative one up to a margin of 1[1]. In order to enhance the learning ability of the NTN model, for each positive triple we sample C random negative triples. We use the L 2 weight regularization for all neural tensor network parameters whose weighted hyper parameter is  X  . 3.3 Classify relation triples The goal is to predict the likely truth in the form of triples ( e 1 ,R,e 2 ) according to their scores in the testing data. We should find a threshold t R for every relation Hence, we need to create a valid set to decide the thresholds and a testing set to evaluate the model. Before reasoning with Neural Tensor Network, we also need to do word segmen-tation for entities existing in Chinese knowledge bases. However, in our Chinese knowledge bases, entities are always phrases which are difficult to segment cor-rectly. Therefore, we consider two kinds of Chinese word segmentation: word and character. 4.1 Word and Character Representations We represent Chinese word and character as continuous vectors. We explore t-wo settings. In the first setting we simply initialize each word(character) vector x  X  R n by sampling it from a zero mean Gaussian distribution: x  X  N (0 , X  2 ). In the second setting, we pre-train the word (character) vectors with an unsuper-vised neural language model(Huang and Socher,2013). Fig. 2 shows the model. It makes use of local and global context to pre-train the word(character) vectors. These word(character) vectors are then stacked into a word embedding matrix L  X  R | V | X  n , where V represents the size of vocabulary and n is the dimension of vectors. These vectors will be revised to capture certain semantic during learning process.
 to the embedding matrix which we use to retrieve the word X  X  (character X  X ) vector representation. Mathematically, if we want to get the k-th word X  X (character X  X )vector of the vocabulary, we only need to use a binary vector which is zero in all posi-tions except at the k-th index, 4.2 Entity Representation Previous work[6 X 8] assigned a single vector representation to each entity of the knowledge base, which does not allow the sharing of statistical strength between the words describing each entity[1]. We represent each word(character) as a d-dimensional vector and compute an entity vector as the composition of its word(character) vectors. For example, entity t {  X   X  is consist of words t { ,  X  and  X  , or is consist of character t , { ,  X  , and  X  . In this task, we represent the entity vector by averaging its word or character vectors. For example, or As word or character is the smallest unit, we train the model on the word X  X  and character X  X  level. The training err derivatives are also back-propagated to these vectors. Our goal is to predict whether a given triple holds, which can be seen as common sense reasoning or prediction in relationship networks[1]. For example, if t {  X   X  is in  X   X  . , it is also t { ... /  X   X  . In this section, we first introduce the datasets and then show some experiments. We compare the NTN with other models and obtain several conclusions based on analyses of these results, such as whether to use word vectors or character vectors.
 hyperparamters to get the highest accuracy. (i) vector initialization with a zero mean Gaussian distribution; (ii)  X  = 10  X  4 ; (iii) number of training iterations T = 7000; (iv) the dimensionality of word or character d = 50; (v) number of slices of tensor k = 3; (vi) number of neural nodes of hidden layer in single layer model h = 200.
 of all parameters. Theano is a Python library that allows you to define, opti-mize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently[10].It can be used for doing efficient symbolic differentiation, even for function with many inputs[10]. The NTN model is trained by taking derivatives with respect to all the above parameters. We use minibatched CG and L-BFGS algorithm for optimization which converges to a local optimum of our objective function. 5.1 Date Sets datasets named AniPla and Diet sampled from Baidu Encyclopedia infobox. All data stored in the form of ( e 1 ,R,e 2 ). In AniPla, the first entities are animal X  X  and plant X  X  names, the second entities are the qualities of animals and plants, for example,( 9 c 8 o  X  8 ) and (  X   X   X   X   X  ). In total, there are 12881 unique entities in 12 different relations. We use 28110 triples for training and 7030 triples for testing. The valid set has 7026 triples for compute the thresholds t ( R = 1 , 2 ,..., 12) for all relations. As Diet, the first entities are food X  X  names, the second entities are the qualities of food, for example, (  X  ~  X   X   X   X  ~ ) and (  X   X   X   X   X  ). In total, there are 23547 unique entities in 8 different relations. We use 27000 triples for training and 6000 triples for testing. The valid set has 6000 triples for compute the thresholds t R ( R = 1 , 2 ,..., 8) for all relations. We do word segmentation for AniPla and Diet by manual handling and software, respectively. 5.2 Results This section we compare the results of NTN and other models with different initialize methods of word(character) representations. The Tensor Factorization model[3] dosen X  X  need to use word or character vectors. Table.2 and Table.3 show the resulting accuracy of each model.
 vectors than with random vectors. In Table.3, pre-trained word vectors don X  X  have any advantages than random vectors. We can conclude that software can X  X  do word segmentation for entities perfectly.
 reports the accuracy of each model. With unsupervised word vectors X  initializa-tion, the single layer and NTN models can reach high classification accuracy: 84.6% and 91.1% on the dataset respectively. However, with random initializa-tion, they both obtain a lower accuracy. It shows that the models have improved accuracy with initialization from pre-trained word vectors. In Fig. 4, the red bar is longer than the blue bar among most relations that also confirms it. By contrast, pre-trained character vectors don X  X  have significantly advantage than random vectors. This phenomenon shows that Chinese semantics are expressed through words, not characters.
 and Table.3shows that the NTN model is more power than other models. Even with character vector and random initialization, the NTN model can achieve accuracy 89.6%, which shows that when Chinese word segmentation is a hard task (the entities in many Chinese datasets are always phrases which are difficult to segment), the NTN model with character vectors from random initialization is a feasible strategy.
 use the NTN model to evaluation. The accuracy varies among different relations. In AniPla, the accuracy rangs from 38.7% to 96.2% and the two most difficult relations are  X   X   X   X  and  X   X  . In Diet, the accuracy rangs from 61.0% to 87.1% and the two most difficult relations are  X   X  and  X  X . According to Fig. 3 and Fig. 4, we can see the Diet is more difficult to reason than AniPla. We introduce the Neural Tensor Network for common sense reasoning based on existing Chinese knowledge bases. This model constructs a tensor to enhance the interaction between entities in an efficient way and obtains a high prediction accuracy by training word vectors whose initial value come from a unsuper-vised large corpora. The future work is to improve these ideas to achieve higher accuracy and apply the NTN model to complete knowledge bases.

