
Plant Breeding and Genetics, Cornell University, Ithaca, NY, USA USDA-ARS, Cornell University, Ithaca, NY, USA 1. Review of ensemble methods
Ensemble learning [22,25,28] provides solutions to complex statistical prediction problems by simul-common behavior, ensemble modeling approaches provide solutions that as a whole outperform the sin-gle models. Some influential early works in ensemble learning were by Breiman with Bagging (bootstrap aggregating) [2], and Freund and Shapire with AdaBoost [15]. These methods involve  X  X andom X  sam-pling the  X  X pace of models X  to produce an ensemble of base learners and a  X  X ost-processing X  of these to construct a final prediction model [33].

In this article, we review several approaches for ensemble post-processing and propose some new ones. To this end, we summarize the ensemble generation procedure and post-processing procedure regression, weighing trees with their out-of-bag performances and truncation and kernel smoothing in the case of regression; a unsupervised and a semi-supervised clustering algorithm based on rules; rule based unsupervised and semi-supervised kernel matrix learning. Our illustrations indicate that these new models are competitive, if not superior, to some popular approaches. However, a main theme in this article is that the base learners and rules can be used as input variables in any learning problem work can be considered as an extension of the methods in [17] and these are briefly summarized below. algorithm [9], ensemble classification using generalized additive models [8], ensemble selection from libraries of models [6], weighting individual outputs in multiple classifier systems [36], subspace en-sembles using neighborhood accuracy [37]. The ensemble clustering problem was recently introduced ing techniques have been proposed [12,19,29]. Reviews of semi-supervised learning is available in [42] and [20].

In the remainder of this section, we will review the recently proposed importance sampling learning ensembles (ISLE) framework [17] for ensemble model-rule generation and post-processing. In Section 2, we propose some new ensemble post processing methods for supervised learning including partial least squares regression, multivariate kernel smoothing and use of out-of-bag observations. Rule ensemble approach to supervised learning is adapted to the unsupervised and semi-supervised clustering problem in Section 3. Section 4 is reserved for examples and simulations by which we compare the methods proposed here with the existing ones. Some remarks about hyper parameter choice and directions for future research are provided in Section 5. 1.1. ISLE approach
Given a learning task and a relevant data set, we can generate an ensemble of models from a pre-determined model family. Bagging bootstraps the training data set [2] and produces a model for each bootstrap sample. Random forest [5,24] creates models by randomly selecting a few aspects of the data set while generating each model. AdaBoost [15] and ARCing [4] iteratively build models by varying case weights and employ the weighted sum of the estimates of the sequence of models. There have been attempts to unify these ensemble learning methods. One such framework is the ISLE due to Popescu &amp; Friedman [17].

We are to produce a regression model to predict the continuous outcome variable y from p vector of input variables x and a model family F = { f ( x , X  ):  X   X   X  } indexed by the parameter  X  is given to generate M models. The final ensemble models considered by the ISLE framework have an additive form: proceeds with combining the base learners by choosing weights { w j } M j =0 in Eq. (1). The pseudo code to produce M models { f ( x ,  X  j ) } M j =1 under ISLE framework is given below: Algorithm 1: ISLE( M, X , X  )
F 0 ( x )=0 . for j =1 to M return ( { T j ( x ) } M j =1 and F M ( x ) . ) scheme  X , and 0  X  1 is a memory parameter.

The classic ensemble methods of Bagging, Random Forest, AdaBoost, and Gradient Boosting are special cases of ISLE ensemble model generation procedure [33]. In Bagging and Random Forests the weightsin(1aresettopredeterminedvalues,i.e. w 0 =0 and w j = 1 M for j =1 , 2 ,...,M. Boosting and takes F M ( x ) as the final prediction model.

Friedman and Popescu [17] recommend learning the weights { w j } M j =0 using lasso [39]. Let T = ( T ensemble. The weights ( w 0 , w = { w m } M m =0 ) are obtained from prediction model. The final ensemble model is given by 1.2. Rule ensembles
The base learners in the preceding sections of this article can be used with any regression model, however they are often used with regression trees. Each decision tree in the ensemble partitions the A tree with K terminal nodes define a K partition of the input space where the membership to a specific node, say node k, can be determined by applying the conjunctive rule intervals for a continuous variable and a subset of the possible values for a categorical variable. that are estimated from in the final prediction model 2. Post processing ensembles revisited We can use the base learners or rules in an ensemble as input variables in any regression method. Since the number of models in an ensemble can easily exceed the number of individuals in the training sample ( p n ), we prefer regression methods that can handle high dimensional input. A few such ensemble post-processing methods like partial least squares regression, multivariate kernel smoothing and weighting are proposed in this section. We will compare these approaches to the existing standards random forests and rulefit in the Section 4. 2.1. Partial least squares regression
The models in an ensemble are all aligned with the response variable and therefore we should expect regression (PLSR) is a technique which is suitable for high dimensional regression problems where the predictor variables exhibit multicollinearity. An additional advantage in using PLSR is that response variables can be multivariate.
 In PLSR the input matrix X is decomposed into orthogonal scores S and loadings L leads to biased but low variance estimates of the regression coefficients in model 1. PLSR incorporates information on both input and output variables in the loadings.

PLSR behaves as shrinkage method [16] where the amount of shrinkage is controlled by the number of loadings included. An obvious question is to find the number of loadings needed to obtain the best generalization for the prediction of new observations. This is, in general, achieved by evaluating the age parameter in PLSR is easier than setting the continuous sparsity parameter in lasso regression. The illustrations following section demonstrate the good performance of PLSR for post processing trees or rules. PLSR, as opposed to lasso, achieves shrinkage without forcing sparsity on the input variables. the k th tree is evaluated as k th rule is calculated similarly as where s k = from sum of the importances of the trees or rules which contain that variable. The PLSR model is in be used to calculate tree rule or variable importances the same way they were calculated for the lasso post processing approach. A measure of importance for each variable can be obtained as the sum of the importances of rules that involve that variable.
 2.2. Multivariate kernel smoothing
We will concentrate on kernel smoothing using the Nadaraya-Watson estimator. For a detailed presen-
The kernel function K h ( . ) is a symmetric function that integrates to one, h&gt; 0 is the smoothing parameter. In practice, the kernel function and the smoothing parameter are usually selected using the cross validated performances for a range of kernel functions and smoothing parameter values. 2.3. Weighting ensembles using out-of-bag observations
As mentioned earlier, most of the important ensemble methods combine the base models using weights. Both bagging and random forest algorithms use equal weighting. Estimating  X  w by minimizing subject to the constraint w 0 gives the Stacking approach of Wolpert [41] and Breiman [3]. In stacking final prediction model is given by Subspace ensembles (SENA) [37] uses the idea of local neighborhood accuracy to obtain the ensemble model weights for the classification problem. Below we propose a similar weighting scheme based on the generalization performance of individual models.

The ensemble generation algorithms based on bootstrapping the observations builds the base learners the generalization performance of that particular learner. The following weighting scheme will down can use mance each model in the ensemble and using the weights l =1 , 2 ,...,M.
The value of h controls the smoothness of the model. For large values of this parameter the kernel method will assign approximately equal weights to the learners T l ,l =1 , 2 ,...,M and hence it is equivalent to random forest weighting. Smaller values of the parameter assigns higher weights to the it is sometimes beneficial to eliminate the models with lowest weights from the final ensemble.
Note that, a modification of the above model allows us to localize the predictions by weighting models the model in [37]. An estimator of the response at input value x new can be written as where K h 1 and K h 2 are two kernel functions indexed by the width parameters h 1 and h 2 . 3. Rule ensemble clustering
The main difference between the supervised learning and unsupervised learning is the existence of a target variable in the former. The sample partitioning approaches discussed in the previous section ponents which give a good segregation of the target variable. When we have many target variables we can randomly map them to the real line and when we are not provided with target variables, we can con-struct our target variables by randomly mapping the input variables in a similar fashion. Consequently, overall cluster rules are obtained from combining the rules for many target variables into an ensemble distance matrix. By making an analogy to ISLE algorithm reviewed in the previous section, we can boost the search of the space of  X  X oncepts X  by including a memory parameter. The details of this clustering procedure are described below.
 Let Y be the n  X  q matrix of observed target variables for which a good segregation is needed. Let X be the n  X  p matrix of observed input variables to the clustering algorithm. We name the following algorithm semi-supervised importance sampling clustering algorithm (SS-ISCA).
 Algorithm 1: SS-ISCA( X,Y,M,m, X  )
R 1 : A random projection of Y for j =1 to M return ( T ( X )) Here P T ( X ) is the projection matrix on to the space spanned by the columns of T ( X ) calculated as linear projection though this is not necessary. The projection step and a positive  X  value allows the algorithm to cover the target space quicker. However when a nonzero memory parameter is used the computational burden grows quickly as the algorithm advances in its stages. One simple strategy is to apply lasso regression to select rules at each stage j and therefore reducing m. Another strategy sampling proportion. We have used both of these techniques successfully for high dimensional clustering problems. Of course, if the memory parameter  X  is set to zero then we do not have to worry about calculating P T ( X ) . In this case the SS-ISCA algorithm is parallelizable.

An implementation of SS-ISCA can be tried at the Triticeae Toolbox (T3). T3 is the web portal for the for Food and Agriculture (NIFA) of the United States Department of Agriculture (USDA). T3 contains SNP, phenotypic, and pedigree data from wheat and barley germplasm in the Triticeae CAP. by applying a distance based hierarchical clustering algorithm [27] to the rule based distance matrix D ( X ) . The method we have used to combine the clusters from many rules is referred to as the cluster calculate T ( X new ) without the knowledge of the corresponding target variables.

In distance based hierarchical clustering, first, each object is assigned to its own cluster. At each between clusters are recalculated at each stage by a linkage criterion such as single-linkage, complete linkage or average linkage. In our illustrations in the following section, we have uniformly used the average linkage criterion for combining clusters.

A modification of the semi supervised clustering algorithm when the target variable is univariate takes advantage of the rule importances. To obtain M rules for the target variable y use the ISLE algorithm to extract rules and use either PLSR or lasso post-processing to estimate the weights w . From T ( X ) and the estimated weights  X  w a similarity matrix can be obtained as S ( X )= T ( X ) diag ( |  X  w | s (1  X  s )) T ( X ) /r where s =( s 1 ,s 2 ,...,s r ) are the supports of rules 1 to r.
For generation of rules useful for ensemble clustering when there are no target variables, we modify the semi-supervised rule generation algorithm in 1 as follows (importance sampling clustering algo-rithm): Algorithm 2: ISCA( X,M,m, X  )
R 1 : A random projection of X for j =1 to M return ( T ( X )) tering, the number of clusters has to be determined. When the clustering problem is accompanied by can be measured by Rand measure (R) [30], Jaccard index (J) [10], Fowlkes Mallows index (FM) [13], houette) [32], Dunn index (dunn) ( [11], Connectivity (connect) [7] can be used to identify the number Handl et al. [21] provides an excellent overview of cluster validation measures.

Each rule gives a binary partition of the input space. Therefore, an ensemble of rules together can be of clusters provide more insight then only one solution. For instance, when grouping genes with similar function we need a multiple clustering approach since some genes might have multiple functional roles. views of grouping the data. ISCA and SS-ISCA algorithms can benefit from such a view and the author believes that these algorithms are flexible enough to answer such questions. 4. Case studies and experiments
In this section we will deliver several data analysis that uses the supervised, semi-supervised and unsupervised approaches discussed in this paper. The main properties of the data sets that are used in this section are summarized in Table 1.
 The following ensemble models are compared in the context of supervised learning: 1. r(pslr): Partial Least Squares Regression with Rules, 2. t(pslr): Partial Least Squares Regression with Trees, 3. r(lasso): lasso with Rules, 4. t(lasso): lasso with Trees, 5. w(oob): Weighting Using Out-of-Bag performance, 6. wt(oob): Weighting Using Out-of-Bag performance (best 60% of the trees), 7. rf: Random Forest, 8. ksr: Kernel Smoothing with Rules, 9. kst: Kernel Smoothing with Trees.
 sample. Aggressive use of cross-validation can be supported by the theorem in [40]. The models 3, 4, and 7 are the existing methods and models 1, 2, 5, 6, 8, and 9 are the models introduced in this paper.
Our first example involves the Fusarium head blight (FHB) data set that is available from the author upon request. A very detailed explanation of this data set is given in [26].
 Example 1 . (FHB Data, Regression) FHB is a plant disease caused by the fungus Fusarium Gramin-earum and results in tremendous losses by reducing grain yield and quality. In addition to the decrease in grain yield and quality, another damage due to FHB is the contamination of the crop with mycotoxins. Therefore, breeding for improved FHB resistance is an important breeding goal. Our aim is to build a cluded FHB measurements along with 2251 single nucleotide polymorphisms (SNP) on 622 elite North 2007, and 2008. We have used a semi-parametric mixed model (SPMM) to account for the location and obtaining corresponding estimated best linear unbiased predictors for the random effects.
Our SPMM for the n  X  1 response vector y is expressed as (mean, location, year), Z is the n  X  q design matrix for the random effects (lines); the random effects ( g , e ) are assumed to follow a multivariate normal distribution with mean 0 and covariance where K is a q  X  q similarity matrix obtained as K = MM / diag ( MM ) and M is the 622  X  2251 dimensional markers matrix. In the second step the EBLUPS of genetic values of the 622 lines are estimated using models 1:10. The 10 fold cross validated accuracies measured by the correlations of true responses to the predicted values are displayed in Fig. 1.
 Example 2 . (Simulated Data, Regression) In our second example we repeat the following experiment 100 times. Elements of the 150  X  100 input matrix X are independently generated from a uniform (0 , 1) and 85% of these were selected randomly and set to zero. 150 dimensional response vector y was ratio was about 2 to 1. The data was separated as training data and test data in the ratio of 2 to 1. The boxplots in Fig. 2 compare the different approaches to ensemble post processing in terms of the accuracies in the test data set.
 Example 3 . (Friedman Simulated Data, Regression) In this example, we repeat the experiment in Friedman and Popescu [18]. Elements of the 1000  X  100 input matrix are independently gener-ated from unif (0 , 1) distribution. 1000 dimensional response vector y was generated according to { y i =10 performances of the different approaches over 100 replications of the experiment.
 Example 4 . (Boston Housing Data, Regression) In order to compare the performance of prediction models we use the benchmark data set  X  X oston Housing X  [23]. This data set includes n = 506 obser-vations and p = 14 variables. The response variable is the median house value from the rest of the 13 PLSR approach has the best cross validated prediction performance.

We illustrate the clustering approaches in four real data sets. In addition to the SS-ISCA and ISCA approaches, we employ existing methods like model based clustering (Mclust) [14], partitioning around medoids (PAM) [31], divisive analysis clustering (DIANA) [35] and random forest clustering (RF) [5]. The different clusterings are compared using the internal measures (Silhouette, Dunn index, Connec-tivity) or using the external measures (Rand measure, Fowlkes-Mallows index, Wallace index, Jaccard index). In the case where a supervisory target variable was available and there were only two clusters, we also provided the p-values from comparing the means for the target variable in these clusters. displayed in Table 2. The existing data labels are used to calculate the internal validity measures. We have also provided internal measures of cluster quality. ISCA algorithm is uniformly the best according to the internal measures. With respect to the external validation measures ISCA algorithm ranks second after the model based clustering.
 Example 6 . (FHB Data, Clustering) We would like to segregate the 622 barley lines described in Ex-using different approaches are summarized by the boxplots in Fig. 5. SS-ISCA clearly gives the best segregation of the FHB variable among other clusterings which we measure by the p value correspond-ing to the two sample t test for comparing group means. Some internal measures for cluster quality for clusterings by different clustering approaches are provided in Table 3.
 which are affected by the disease include wheat, barley and triticale. We had estimated breeding values ISCA approach. Example 8 . (Mouse Data, Learning the Number of Clusters) We use data from an Afyymetrix microar-ray experiment comparing gene expression of mesenchymal cells from two distinct families, neural crest and mesoderm derived. The dataset consists of 147 genes and expressed sequence tags (EST) which were measures of cluster quality is displayed for 2 to 3 0 groups clustering by ISCA in Table 7. The optimal number of clusters is determined to be two using the connectivity and silhouette width. Although the Dunn Index increases almost uniformly after 3 clusters and attains very high levels after 10 or more clusters, there is indication that 2 groups provides a reasonable clustering.
 Example 9 . (Rule Based Kernel Matrix for SPMM) The role of this example is to demonstrate the use of ISCA or SS-ISCA algorithm for learning a gram matrix. Gram matrix is used in kernel based learning. Some kernel learning methods include reproducing kernel Hilbert space regression, support vector regression, Gaussian processes and SPMM. All of these methods depend on using a gram matrix that task specific kernels might improve these models. An issue off the shelf kernel functions like the will compare SPMM X  X  with similarity matrix S ( X ) obtained from SS-ISCA and ISCA algorithms with SPMM X  X  with linear, polynomial and Gaussian kernels. We use the data from the results of the FHB dataset in Example 1.
 We use the SPMM model in Eq. (6) with linear, polynomial, Gaussian, ISCA, and SS-ISCA kernels for K . The fixed effects design matrix X includes a column of ones and a dummy variable representation of the location variable. The design matrix Z includes the dummy variable representation of the line specific genetic effects. The parameters  X , X  2 g , X  2 e of the SPMM models are estimated by maximizing the likelihood functions. The hyper-parameters of each of the kernels were learned using the profile likelihood functions over a grid of values. Final models are compared using 10 -fold cross validated from this example that using a task specific semi-supervised kernel improves the model accuracies.
The results of the experiments in this section are summarized in Table 4. 5. Conclusions In this article, we have proposed several approaches for post processing a large ensemble of rules. problem.

The results from our simulations and benchmark experiments show that the methods introduced in this article are promising. In most cases, the proposed methods obtained better performances than the ones supervised learning PLSR with rules uniformly produced the models with best prediction performances. The ensembles based on rules extracted from trees, in general, had better performances.
We have also seen that rule ensembles can be used to learn a similarity matrix which, in turn, can be utilized in clustering and similarity based learning. We have shown by many examples that high quality clusterings are obtained based on ISCA or SS-ISCA approaches. The use of SS-ISCA for learning task specific similarity matrix increased model accuracies.
 Deep learning algorithms [1] are based on learning representations which can be shared across tasks. Learning about a large set of interrelated concepts might provide a key to generalizations. In a multi-target variables with the SS-ISCA algorithm can be used to obtain many hierarchical representations of the input variables. Furthermore the similarity matrices obtained for multiple tasks can be combined in several ways to give new similarity matrices which in turn be used for learning a related but new task.
The complexity of trees or rules in the ensemble increases with the increase in number of nodes from the root to the final node (depth). The maximum depth is an important parameter since it controls the degree of interactions between the input variables incorporated by the ensemble model and the its value trees by the ISLE algorithm.
 The ensemble approaches introduced in this paper can also be a remedy for analyzing big datasets. By adjusting the sampling scheme in the ISLE, ISCA and SS-ISCA algorithms we were able to analyze load the whole data into the memory. This is a great advantage to importance sampling algorithms. ensemble is also straightforward with PLSR, kernel smoothing regression and the clustering approaches. Acknowledgement This research was also supported by the USDA-NIFA-AFRI Triticeae Coordinated Agricultural Project, award number 2011-68002-30029.
 References
