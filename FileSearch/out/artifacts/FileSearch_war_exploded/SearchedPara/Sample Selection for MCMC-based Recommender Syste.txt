 Bayesian Inference with Markov Chain Monte Carlo (MCMC) has been shown to provide high prediction quality in recom-mender systems. The advantage over learning methods such as coordinate descent / alternating least-squares (ALS) or (stochastic) gradient descent (SGD) is that MCMC takes uncertainty into account and moreover MCMC can easily integrate priors to learn regularization values. For factor-ization models, MCMC inference can be done with efficient Gibbs samplers.

However, MCMC algorithms are not point estimators, but they generate a chain of models. The whole chain of models is used to calculate predictions. For large scale models like factorization methods with millions or billions of model pa-rameters, saving the whole chain of models is very storage intensive and can even get infeasible in practice.

In this paper, we address this problem and show how a small subset from the chain of models can approximate the predictive distribution well. We use the fact that mod-els from the chain are correlated and propose online selec-tion techniques to store only a small subset of the models. We perform an empirical analysis on the large scale Netflix dataset with several Bayesian factorization models, includ-ing matrix factorization and SVD++. We show that the proposed selection techniques approximate the predictions well with only a small subset of model samples.
 H.3.3 [ Information Systems ]: Information Search and Re-trieval X  Information filtering Algorithms, Experimentation, Measurement, Performance MCMC, Markov Chain Monte Carlo, Recommender System, Factorization Model, Bayesian Inference
Many state-of-the-art recommender systems rely on com-plex machine learning models. The models usually have a large number of model parameters and several regulariza-tion parameters to prevent overfitting. The most common learning algorithms for large scale recommender systems are based on (stochastic) gradient descent (SGD) or alternating least squares (ALS)/ coordinate descent. These algorithms try to find the most probable values for the model parame-ters. The regularization parameters are mostly searched by external methods, e.g. grid search.
 Recently, Bayesian inference with Markov Chain Monte Carlo algorithms (MCMC) has been proposed for learning large scale recommender systems [5]. The advantage of MCMC over SGD and ALS is that MCMC takes the uncer-tainty in the model parameters into account (resulting often in higher prediction quality) and MCMC allows to jointly make inference over model parameters and regularization values.

SGD and ALS are point estimators and their learning al-gorithms result in one state of model parameters, which can be used directly for prediction. In contrast to that, the steps ( X  X terations X ) in an MCMC algorithm generate samples of the model parameters. Each generated sample is a valid state which should be taken into account for prediction. From a practical point of view, the prediction for an MCMC algo-rithm is the average prediction of each step, i.e. the model parameters of each step make a prediction and the predic-tions are averaged. This means for MCMC algorithms, the whole chain of model parameters has to be available when predicting. Whereas the chain can be compressed into a single model state for linear regression models, this is not possible for non-linear models such as factorization mod-els where the full chain has to be considered. Storing the full chain is demanding or even infeasible for models with a large number of model parameters, e.g. factorization models in recommender systems with millions or billions of model parameters.

The contributions of this work are as follows: 1. The important practical problem of handling chains 2. Two online-selection algorithms are proposed that make 3. The properties of sample selection algorithms are stud-
Recommendation tasks such as rating or click-through prediction can be formalized as a supervised prediction task, where for a case/ data point x , a target variable y should be predicted based on historical observations S train = { ( x ( x 2 ,y 2 ) ,... } . To simplify readability, we discuss mostly re-gression (i.e. y  X  R ) but our results also hold for classifi-cation (i.e. y  X  { + ,  X  X  ). As common in machine learning, we assume that there exists a function  X  y that models the unobserved relation between the target y and the predictor variables x . The model equation  X  y typically depends on a set of model parameters  X .

The model parameters  X  of popular recommender mod-els like matrix factorization (MF), SVD++ [1] or factoriza-tion machines (FM) [3] can be learned with several algo-rithms. Stochastic gradient descent (SGD) or alternating least-squares are popular point estimators, that try to max-imize the likelihood argmax  X  p ( S train |  X ) p ( X  |  X  H hyperparameters  X  H (typically the regularization values). Thus, the result of SGD and ALS algorithms is a single state of model parameters  X . For predicting, the unknown target y of a new case x the model equation  X  y ( x |  X ) is computed.
In contrast to point estimators, Bayesian inference uses the whole predictive distribution of the target y given the case x and the whole dataset S train With marginalization over model parameters and hyperpa-rameters ( X  ,  X  H ) and the typical conditional independence assumptions, the predictive distribution can be written as p ( y | x ,S train ) = For complex models, this integral cannot be evaluated ana-lytically and it is approximated by samples  X  1 ,...  X  M drawn from p ( X  | S train ,  X  H ) Given this predictive distribution, one can calculate any statistic of interest. E.g. for a real valued target variable, the prediction (expectation value) of the target y for the case x is 1
The advantages of MCMC over SGD and ALS come to the prize that the chain of models  X  1 ,...  X  M has to be available when a case x is predicted (see eq. 4). Compared to SGD and ALS, the storage and prediction complexity increases linearly in M . Especially if the model has many parameters and the chain is long (i.e. M is large), storing the whole chain can get very expensive or even infeasible. Here, the likelihood is y | x ,  X  i  X  X  ( X  y ( x |  X  i ) , 1 / X  ).
In this paper, we investigate techniques to select a subset of states from the chain to reduce the storage and prediction complexity. We will make use of the fact, that samples from MCMC are not i.i.d. but are correlated. Formally, we de-velop selection methods that select a subset I  X  X  1 ,...,M } from the full chain such that the size of the subset |I| = m is considerably smaller than the full chain: m M . The algorithms will decide during MCMC sampling if a sample  X  i should be kept or discarded. This means in total only m M model states have to be stored and prediction sim-plifies to
For regression with a linear model (e.g. linear regression), one can compress the whole chain into a single state  X  := very important to note that with non-linear models (e.g. MF, SVD++, FMs) this is not possible and the whole chain is needed for prediction.
In the following, we present algorithms to select a subset I of m states from the chain of models  X  1 ,...  X  M . Our online algorithm is integrated in the MCMC sampler and has to decide on the fly if a state is selected without knowing any subsequent state.
 In practice, there is typically a limited storage for models. We assume that at maximum m models can be stored. With MCMC sampling, long chains M m can be created but in our setting even during sampling at maximum m states (models) can be stored. Thus online selection methods have to decide on the fly if a model should be kept or discarded.
Algorithm 1 shows the basic idea of integrating sample selection in MCMC. The sampler starts with an initial state  X  0 and an empty set I of selected states. Then samples are generated iteratively and for each sample an online selection method decides how to refine I . Basically, the online selec-tion algorithm can decide to include or discard the current state  X  i . If the state is included, the selection method has to make sure that the storage is not exceeded ( |I| X  m ) and a previously selected state might have to be discarded  X  i.e. the current state overwrites an existing state.
 Algorithm 1 MCMC Sampling with Online Selection 1:  X  0  X  random 2: I  X  X } index set for subset to store 3: for i = 1 to M do 4: sample  X  i e.g. with Gibbs [5, 3] 5: I  X  onlineSelection( I ,i,m ) 6: if i  X  X  then 7: store  X  i 8: end if 9: discard any  X  j where j 6 X  X  limit storage to I 10: end for 11: return I
Next, we discuss several choices for online selection algo-rithms (i.e. for line 5 in Algorithm 1).
If the samples in the chain would be i.i.d., any subset from { 1 ,...,M } could be chosen. Under this assumption, taking the first m samples would be the easiest choice. An ad-vantage of this strategy is that the sampling process could be stopped already after only m steps without the need to sample all M states. Clearly, the assumption of independent samples does not hold for an MCMC algorithm. Neverthe-less, we use this strategy as a baseline which more sophisti-cated selection methods should outperform.

A basic property of any MCMC algorithm is that each sample is generated from its preceding state  X  e.g.  X  i gen-erated from  X  i  X  1 . Thus correlation of neighbors is sup-posed to be largest. To decrease correlation of selected states, states with maximum distance can be chosen. E.g. if m = 2, we may want to choose the two states at position 1 / 3 M and 2 / 3 M . Or for m = 10, the states at position 1 / 11 M,... 10 / 11 M . Equidistant selection is an unsuper-vised strategy which does not take the states of the par-ticular chain into account. E.g. at some position j there might be more correlated states than in other regions  X  e.g. because the sampler got stuck for a short time.

Next, we propose a supervised online sample selection al-gorithm. The overall goal of sample selection is to find the subset I of m states with the best predictive accuracy. In general, we are interested in a low error (high quality) on an unknown test set S test . As the test set and especially its labels are unknown, a holdout/ validation set S valid can be used to guide the selection algorithm. Any kind of evalua-tion function can be used for measuring the quality of the selected size I on the set S valid , e.g. the root mean square er-ror (RMSE). The selected subset I can be refined online by trying to replace a state j  X  X  with the new sample i if and only if the holdout error decreases for the set ( I\{ j } )  X  X  i } .
In the evaluation, we study the properties of the selec-tion strategies on four models. We compare online selection strategies among each other and also use basic backward and forward selections strategies. Backward and forward are off-line selection methods because they have access to all M states  X  i.e. the full chain. The backward method starts with the full chain and removes the state that gives the low-est RMSE after deletion. The forward method starts with an empty subset and adds the state that gives the lowest RMSE after addition. Both strategies are used only as lower bounds because we want to avoid to store the whole chain. The overall interest is to examine if the selection strategies succeed in selecting a small portion m M of the whole chain while preserving most of the predictive information. The proposed selection strategies are evaluated on the Netflix prize dataset. The dataset consists of about 480 , 000 users and 17 , 770 movies. We use the official prize split with a training set of | S train | = 100 , 480 , 507 observed rat-ings. As a validation set, the private leaderboard set with | S valid | = 1 , 408 , 342 is used. To allow comparing our results to the existing work on the Netflix prize, we report RMSE scores for the public leaderboard set with | S test | = 1 , 408 , 789 ratings. All sets ( S train , S valid , S test ) are disjoint.
Four recommender models are investigated: matrix factor-ization with biases (MF), SVD++ [1], a time-aware matrix factorization and time-aware SVD++ 2 . The smallest model (MF) has already more than 60 , 000 , 000 model parameters (for a single state of the chain) which justifies the need for efficient sample selection. All models use Bayesian inference with the Gibbs sampler described in [3] and for each model, a chain of M = 512 samples is generated. The sampler is ini-tialized with a randomly generated state ( X  0  X  N (0 , 0 . 1)) and all subsequent 512 samples are used. We use factor-ization machines from libFM [3] for all experiments. The number of latent dimensions for the four factorization mod-els is k = 128.
The first experiment investigates the prediction quality of the proposed techniques under a varying size of selected states. The experiment allows to compare the selection strategies among each other and allows to investigate how many samples are necessary for a good prediction quality. Figure 1 shows the RMSE on the test set ( S test ) depending on the size m of the selected subset I . As expected, the error of the selection techniques gets more similar the larger m . For small subsets ( m M ), the differences between the selection techniques become clearly visible.

Comparing the online selection methods, one can see that early stopping is clearly outperformed by equidistant and greedy holdout selection. This result is in line with our ex-pectation because all states selected by early stopping are correlated whereas equidistant and greedy holdout selection favor uncorrelated states. Secondly, greedy holdout selec-tion outperforms equidistant selection. This indicates that the states of the generated chain are not perfectly equidis-tant distributed  X  e.g. because sometimes it takes longer (more states) to move into another region of the posterior distribution. The holdout set seems to be well suited to identify such changes.

The experiments show that a small subset of states can approximate the predictive quality of the whole chain very closely. Note that the vertical axis of the plots is scaled under a limit of 0 . 90 which is known to be a very good result for MF on the Netflix dataset. With 30 states (less than 6% of the 512 states), online holdout selection already achieves a very low error. With 50 states (about 12%), the quality is almost as good as the full chain.

An interest finding is that with backward, forward and holdout selection, the best quality is not achieved with the full chain ( m = M ) but with a subset of models ( m &lt; M ). The reason could be that the validation set is slightly more representative for the test set than the training set. The
The time-aware MF and time-aware SVD++ use a simpler treatment of time than the timeSVD++ model of Koren [2]. See [4] for details about the time-aware MF and time-aware SVD++ used in our experiments. baseline. results indicate that holdout selection can correct this bias by selecting the right subset. Though due to the scale of the vertical axis, the improvements are only marginal.
In this paper, we presented the problem of selecting states for MCMC-based models. This problem is very relevant in practice because storing the whole chain of models is im-practical for large recommender systems with millions or billions of model parameters. We have proposed subset se-lection strategies that can select during MCMC sampling whether or not to store a sample. Both equidistant and holdout selection are online methods that can be integrated easily in existing MCMC samplers. We have shown empiri-cally for four state-of-the-art recommender models that the proposed algorithms generate high quality subsets. With less than 10% of the original chain, our best online strategy was able to produce predictions almost as good as the full chain. Moreover, the prediction quality of online holdout selection clearly outperforms early stopping and is almost as good as the offline selection bounds.
 This work was supported by DFG under grant RE3311/2-1. [1] Y. Koren. Factorization meets the neighborhood: a [2] Y. Koren. Collaborative filtering with temporal [3] S. Rendle. Factorization machines with libFM. ACM [4] S. Rendle. Scaling factorization machines to relational [5] R. Salakhutdinov and A. Mnih. Bayesian probabilistic
