 CHENG-WEI LEE National Tsing-Hua University Taiwan and Academia Sinica and MIN-YUH DAY, CHENG-LUNG SUNG, YI-HSUN LEE, TIAN-JIAN JIANG, CHIA-WEI WU, CHENG-WEI SHIH, YU-REN CHEN and WEN-LIAN HSU Academia Sinica 12: 2  X  C.-W. Lee et al.
 1. INTRODUCTION In recent years, question answering (QA) has become a key research area in several major languages because of the urgent need to deal with the in-formation overload caused by the rapid growth of the Internet. Since 1999, many international question-answering contests have been held at confer-ences and workshops, such as TREC 3 , CLEF 4 , and NTCIR 5 . Several top-performing systems have evolved from these contests, for example, the LCC system [Harabagiu et al. 2005] for English at TREC and the QRISTAL sys-in contests usually employ sophisticated techniques to analyze the passages and infer the answer. For example, QRISTAL and the University of Singa-pore [Cui et al. 2005] use parsers in their systems, and the LCC system uses a logic prover. Both parsers and logic provers are heavy techniques that are state-of-the-art QA systems for them are often difficult to implement.
Questions in QA research can be categorized into several types, such as to a factoid question is a noun or a short phrase, such as a person name, an organization name, a location, a number, time, or an object. For example, person X  X  name,  X  X hat company is South Korea X  X  No. 1 carmaker? X  is asking for an organization X  X  name (a company), and  X  X ow long is a cow X  X  pregnancy? X  involve filtering and ranking answers.

In contrast to most state-of-the-art systems, we focus on lightweight tech-languages have such rich or high-quality resources as English. For example, Chinese parsers do not usually perform as well as English parsers due to the word segmentation problem. Therefore, English QA methods that are heav-ily dependent on parsing may not be effective when applied to Chinese. The situation is worse when we try to deal with regional languages, such as Taiwanese (Minnan) or Cantonese. Lightweight techniques could also be use-ful in resource-limited situations, such as in resource-restricted hand-held because of the limited memory, CPU power, and network bandwidth.
We propose two novel lightweight methods that do not require parsers or method is the Sum of Co-occurrences of Question and Answer Terms (SCO-QAT), which measures the closeness of an answer and the question keywords by calculating some co-occurrence scores for them. The second method is called Alignment-based Surface Patterns (ABSPs), which automatically gen-erate syntax patterns of relations between terms from question-answer pairs.
SCO-QAT utilizes co-occurrence information. It is similar to Magnini X  X  ap-answer validation mechanism. However, SCO-QAT differs from Magnini X  X  ap-Web), SCO-QAT only uses the retrieved passages to calculate a co-occurrence a method needs to query a corpus several times, the system X  X  response time Magnini X  X  approach needs manually created word-ignoring rules to deal with situations when the required statistics are unavailable. The rules may vary depending on the question and need to be adjusted when the domain or lan-guage changes. SCO-QAT resolves the problem by calculating all combinations of the co-occurrence scores for the answer and the question keywords.
Surface patterns are syntactic patterns that connect answers and ques-tion keywords. For example, Ravichandran and Hovy [Ravichandran and  X  &lt; NAME &gt; ( &lt; BIRTHDATE &gt; - X  to answer BIRTHDATE questions (When was For example, in addition to using a DATE question type, we may need more date-related question types, such as BIRTHDATE, BUILTDATE, ... etc. How-ever, that would increase the burden on the Question Classification module. Second, the method cannot deal with questions that have multiple keywords. Third, the method cannot handle cases where the information for validating 12: 4  X  C.-W. Lee et al. the answer is spread over several passages. Fourth, since it requires exact matches, the method cannot be applied when there is a high language varia-tion. Our proposed surface pattern method, ABSP, can resolve the first three problems. ABSPs are generated from question-answer pairs regardless of the question type. In situations involving multiple question keywords and mul-tiple passages, several ABSPs are used together to calculate a score for an answer.

We show that it is possible to use lightweight techniques to boost QA system performance. To achieve our goal, we employ two novel lightweight methods, SCO-QAT and ABSPs, in a Chinese QA system, which have significantly in-creased the RU-Accuracy of the testbed QA system, ASQA, from 0.445 to 0.535 on the NTCIR-5 CLQA dataset. It also achieved the top RU-Accuracy 0.5 on the NTCIR-6 dataset. The result shows that lightweight methods are not only performances. In summary, we improve a Chinese QA system by employing ing or logic provers.

The remainder of the article is organized as follows. We review related works in Section 2, and introduce the host QA system in Section 3. Our pro-posed methods, SCO-QAT and ABSPs, are presented in Section 4. We describe detail the experiments in Section 6. Section 7 contains a discussion. Then, in Section 8, we summarize our conclusions and consider the direction of our future work. 2. RELATED WORKS 2.1 QA with Surface Patterns Surface patterns have been successfully applied in a number of QA systems. For example, Ravichandran and Hovy [Ravichandran and Hovy 2001] proposed a surface text pattern for extracting answers, while Muslea [Muslea 1999] Soubbotin and Soubbotin [Soubbotin and Soubbotin 2001] used richer patterns definition patterns) to answer questions and won the TREC-2001 competition. However, since none of the above patterns include semantic information, they are called  X  X oor-knowledge approaches X  [Saiz-Noeda et al. 2001].
There has been some progress in adding semantic representations to surface patterns to improve the coverage of questions that the surface patterns can be applied to. Saiz-Noeda et al. [Saiz-Noeda et al. 2001] proposed a type of semantic pattern that uses EuroWordNet as a lexical database, but it cannot et al. 2001] proposed another kind of semantic pattern that can be used for communications between semantic Web developers, as well as for mapping and professional use, it is difficult to implement without domain knowledge.
A substantial amount of research has focused on paraphrasing [Barzi-lay and Lee 2003]. Bouma et al. [2005] use syntactic information about paraphrases to solve QA with a dependency parser. Takahashi et al. [2004] developed a system for QAC2, which uses paraphrasing to perform greedy an-swer seeking. However, the approach is not efficient because it is intended for structural matching-based answering, which needs large-scale paraphrase patterns.

As mentioned in the the Introduction, four issues related to Ravichandran an exact match, so it cannot be applied when there is a high language varia-tion. Our proposed surface pattern method, ABSPs, deals with the first three problems to a certain extent. ABSPs do not need to define additional ques-tion types because they are generated from question-answer pairs regardless the information is spread over multiple passages, we use several ABSPs and combine all the information to calculate a score for an answer. 2.2 QA with Co-occurrence Information Clarke et al. [2001] suggested that redundancy could be used as a substitute high-ranking passages. Several systems [Clarke et al. 2002; Cooper and Ruger 2000; Kwok and Deng 2006; Lin et al. 2005; Zhao et al. 2005; Zheng 2005] in-corporate answer frequency, which is redundant answer information, in their answer ranking components. However, using this feature alone would be in-of documents in which the question terms and the answer co-occur is useful occurrence methods to measure the relevance of an answer to the given ques-tion based on Web search results. As the co-occurrence information tends to some word-ignoring rules to reduce the number of question keywords when the number of returned documents is less than a certain threshold.

Magnini et al. X  X  approach is not applicable to some QA scenarios because system with an average number of 40 answers for a question, it will require engines usually do not allow a large number of queries in a short period of time.

To cope with such resource limited situations, we developed a novel method called SCO-QAT, which is based on the same assumption as Magnini et al. X  X  hypothesis. However, instead of querying the Web multiple times, SCO-QAT 12: 6  X  C.-W. Lee et al.
 relies on retrieved passages solely; therefore, it does not need any word-ignoring rules. 3. THE HOST QA SYSTEM: ASQA Experiments in this article were conducted on a host QA system, ASQA ( X  X cad-emia Sinica Question Answering system X  6 ), which we developed to deal with Chinese related QA tasks. The system participated in the CLQA C-C (Chinese-to-Chinese) subtasks at NTCIR-5 and NTCIR-6, and achieved state-of-the-art module to get keywords, named entities (NEs), and the question type. Then, passages to obtain candidate answers, which are then filtered and ranked by the answer filtering module and answer ranking module, respectively. 3.1 Question Processing As Chinese written texts do not contain word delimiters, we incorporate a Chinese segmentation tool to break a question into question segments com-prised of words and parts-of-speech (POS). With these question segments and other information, such as HowNet 7 sense, ASQA can identify six coarse-grained question types (PERSON, LOCATION, ORGANIZATION, ARTIFACT, TIME, and NUMBER) and 62 fine-grained question types. ASQA adopts an integrated knowledge-based and machine learning approach for Chinese ques-tion classification.

We use InfoMap [Hsu et al. 2001] as the knowledge-based approach, which uses syntactic rules to model Chinese questions, and adopt SVM (Support Vec-type or types by InfoMap and the SVM module. Then, the integrated module 3.2 Passage Retrieval ASQA splits documents into sentences and indexes them with Lucene 8 , an open source information retrieval engine. Two indices are used in ASQA: one based on Chinese characters and the other on Chinese words. At run-time, ASQA utilizes the question segments and POS to form Lucene queries. Query terms are weighted according to their POS. Two Lucene queries are con-question, the top 100 passages are chosen for answer extraction. 3.3 Answer Extraction To identify both coarse-grained and fine-grained candidate answers, ASQA uses a coarse-grained Chinese NER (Named Entity Recognition) engine [Wu et al. 2006] combined with a fine-grained taxonomy and rules. The coarse-grained NER engine, which can identify person names, organization names, and locations, ensembles several CRF (Conditional Random Fields) models with character and word features. The taxonomy and rules are compiled man-NEs. 3.4 Answer Filtering In the next step, ASQA applies answer filters to reduce the number of candi-date answers. There are two filters, the EAT (Expected Answer Type) Filter and the ABSP Filter. The EAT Filter screens candidate answers according mation about question types and their corresponding expected answer types. Answers whose types are not found among the expected answer types are re-moved, and the remaining are the answer candidates. More information about expected answer types can be found in Day et al. [2005]. We discuss the ABSP Filter in detail in Section 4.1. 3.5 Answer Ranking An answer ranking score is calculated for each answer by combining several features as a weighted sum: 12: 8  X  C.-W. Lee et al. the weight and score of the i th feature. All the weights are determined by a genetic algorithm with a training dataset. We tested the answer-ranking formula on different feature combinations. Note that the SCO-QAT feature discussed in this article contributed the most to our NTCIR-6 performance. [2005; 2007]. 4. PROPOSED METHODS 4.1 ABSPs X  X lignment-Based Surface Patterns In ASQA, ABSPs are used in an answer filter to confidently identify correct answers. Next, we introduce the alignment algorithm and describe the gener-ation process, which involves the following steps: 1) generate ABSPs by mul-tiple sequence alignment, 2) select ABSPs based on a set of question-answer pairs, 3) apply ABSPs and combine extracted relations, and 4) calculate the scores. 4.1.1 The Alignment Algorithm. Sequence alignment is the process that finds similar sequences in a pair of sentences. Pair-wise sequence alignment (PSA) algorithms that generate templates and match them against new text have been researched extensively. Huang et al. [2004] employ a PSA algo-tions from biomedical texts annotated with part-of-speech (POS) tags. The sequences are padded with gaps so that similar characters can be aligned as niques [Smith and Waternman 1981] to generate surface patterns.
To apply the alignment algorithm, we first perform word segmentation. In the following discussion each unit is a word. Our templates contain named to y j of Y.
 two alphabet letters a and b . The function is defined as where NE(a) denotes the Named Entity (NE) tag of a , and POS(a) denotes prefix, the degree of similarity is subtracted with a penalty.
 be found by back-tracking in F . 4.1.2 ABSP Generation. An ABSP is composed of ordered slots. For exam-ple, there are five slots in the ABSP  X  X RGANIZATION Na -NE h :  X . This case demonstrates that a slot in an ABSP could be a semantic tag (PERSON, ORGANIZATION, LOCATION, TIME, and OCCUPATION), a POS tag, a term or a gap (which indicates that position can be any word). We generate ABSPs the sentences are segmented and tagged with POS by a Chinese segmenta-We use an NER engine to label PERSON, ORGANIZATION, LOCATION, and words without any semantic tag are tagged  X  X  X . Thus, every segment of a sen-tence contains a word, a POS tag and a semantic tag in the format:  X  X ord/POS tag/semantic tag X . For example, the sentence  X 2000 t g K (  X   X  L  X  would be preprocessed into  X 2000 t /Nd/TIME g K /Nb/O ( /P/O  X   X  /Nc/LOCATION L /VC/O X  11 .

Using the proposed alignment algorithm, our ABSP generation algorithm sentences based on their similarity. Closely matched pairs are then aligned ing in common, the algorithm creates a gap ( X   X ) in that position. Table I pattern  X  X  N Na  X  LOCATION Na / PERSON, X  which means a verb fol-12: 10  X  C.-W. Lee et al.
 positions, the aligned pairs have the same common prefix for POS tag  X  X , N; X  eralized gap,  X  ; X  in the fourth and seventh positions, they have the same / ; X  and in the sixth and ninth positions, they have the same semantic tag  X  X OCATION, PERSON. X  The complete ABSP generation algorithm is detailed in Algorithm 1.
 4.1.3 ABSPs Selection. The selection process chooses patterns that can connect question keywords and the answer. It is assumed that useful patterns usually contain important tags . We deem all the NE Tags, the Nb 12 the POS tag, and all the verb POS tags as important tags , because, based on our ob-servations, these tags usually associate with question keywords and answers. We define pattern slots with important tags as important slots and question terms with important tags as important terms . For example, there are three important slots ( X  X  X ,  X  X OCATION X , and  X  X ERSON X ) in the pattern  X  X  N Na  X  LOCATION Na / PERSON, X  and four important terms ( X   X  r  X ,  X   X   X  &gt;  X ,  X 
W  X   X , and  X  = q  X ) in the question  X   X  r /VJ/O  X   X  &gt; /Nb/ORGANIZATION  X  s N /Na/O  X  /DE/O W  X  /Nc/LOCATION = q /Na/OCCUPATION / /SHI/O  X  /Nh/O X .
We apply each generated ABSP to its source passages. When a matched source passage is found, we extract the corresponding terms from the impor-tant slots. If the extracted terms do not contain the answer and any of the important terms of the source question, the ABSP is removed. In our exper-iment, we collected 126 useful ABSPs from the 865 training questions. The detail is described in Algorithm 2.
 4.1.4 Relation Extraction and Score Calculation. We are now ready to ap-ply the selected ABSPs in the QA system. In this work, we applied ABSPs as a filter to choose highly confident answers. We assume words matched by an ABSP have certain relations between them. For example, the pattern  X  &lt; When a pattern matches the words, a relation is identified and we construct a Related-Terms-Set (RTS) which contains the related terms. By using AB-SPs for matching, we are able to find and combine the RTSs of the question keywords and answers in passages. We calculate a score for each candidate answer according to these RTSs.

Given the passages retrieved for a question, all the ABSPs are applied to each passage. If an ABSP matches a passage, we extract an RTS, which is comprised of the matched important terms (i.e., we discard terms that do not one ABSP matches different terms in a passage. If the RTS contains common elements (i.e., the same term is matched by at least two ABSPs,) we check value, the two RTSs are merged, as shown by the example in Table II. The ta-ble contains a question, two passages retrieved from a corpus, and two ABSPs g  X  a /Nb, s M  X  /OCC } from Passage ... " X   X  /PER X ,  X  ^ 7  X  %  X  /ART X ,  X  r /VJ X ,  X  g  X  a /Nb X  and forms RTS 12: 12  X  C.-W. Lee et al.
  X  g  X  a  X  already exists in RTS merge it with RTS 1 to form a new RTS (the Merged RTS).

After all the RTSs for the given question have been constructed, we use the question X  X  important terms ( s  X  , ... " X   X  , r  X  , g  X  a , s M  X  , in this example) to calculate an RTS score. The score is calculated as the ra-matched important terms is three. Therefore, the score of answers belonging candidate answers by the sum of their RTS scores for the sentences in which they appear and retain the top-ranked answer(s).
 4.2 SCO-QAT: Sum of Co-occurrences of Question and Answer Terms The basic assumption of SCO-QAT is that, in good quality passages, the more often an answer co-occurs with the question terms, the higher the confidence that the answer will be correct. We regard a co-occurrence as an indication terms. Passages are chosen instead of documents, because we assume that co-occurrence information provided by passages is more reliable. We formulate our concept as an expected confidence score from which the SCO-QAT formula is deduced.

Let the given answer be A and the given question be Q , where Q consists created from the word segmentation result of Q with some stop words removed (the stop word list is provided in Appendix B.: Stop Word List for SCO-QAT). Based on QT , we define QC as a set of question term combinations, or more occurrence confidence score of an answer A with a question term combination qc i is calculated as follows: where freq(X) is the number of retrieved passages in which all the elements of X co-occur. We assume that all question term combinations have an equal confidence score is defined as have the following formula for SCO-QAT: We rank candidate answers according to their SCO-QAT scores. For example, passages are presented as P1: qt1 qt2 c2 P2: qt1 qt2 qt3 c1 P3: qt1 qt2 c1 P4: qt1 c2 P5: qt2 c2 P6: qt1 qt3 c1.
 12: 14  X  C.-W. Lee et al.
 We use Equation (5) to calculate the candidate answer X  X  SCO-QAT score as follows:
SCO -QAT ( c 1) =
SCO -QAT ( c 2) = Since the SCO-QAT score of c1 is higher than that of c2, c1 is considered a better answer candidate than c2. 4.3 Enhancing SCO-QAT with Distance Information Generally speaking, co-occurrence information for QA tends to be unreliable We encountered some failures caused by these issues. For example, given the question terms { president, United States of America } and a corresponding P1: Taiwan president Chen Shui-bian thought that it is not a big ... in
P2: G. W. Bush , the 43rd President of the United States of America
SCO-QAT cannot determine whether  X  X . W. Bush X  or  X  X hen Shui-bian X  is the correct answer because they have the same SCO-QAT score. However, resolve the dilemma.

We enhance SCO-QAT by incorporating distance information to obtain the term density in passages when the number of question terms is small. Density The following is the extended SCO-QAT formula:
Conf dist ( qc i , A ) =
SCO -QAT w ith Distance ( A ) = where n denotes the number of retrieved passages. If the passage does not QAT function in Equation (7), we only switch to Conf dist when the number of question terms is smaller then a threshold. The avgdist function is the average number of characters between the question term combination qc i and the answer A in passage p j , which is calculated as: The dist function is the character-distance between the question term k and 5. EVALUATION SETUP To increase the confidence of our experiments, we created new datasets and introduced a new metric, called the Expected Answer Accuracy (EAA), to com-pare performances when ranking several top answers that have the same score. 5.1 Datasets We experiment on several new QA datasets, some of which were expanded from NTCIR CLQA, while others were created by us. A QA dataset (the gold standard) is defined as a set of questions, their answers, and the document IDs of supporting documents. For the CLQA Chinese-Chinese (CC) subtask, we use three datasets from NTCIR-5 and NTCIR-6, denoted as NTCIR5-CC-D200, NTCIR5-CC-T200, and NTCIR6-CC-T150 in this article. The last item where T stands for  X  X est X  and D stands for  X  X evelopment X  (Table III).
According to Lin et al. [2005], datasets created by QA evaluation forums ficiently comprehensive. This means we have to manually check all the extra periment results. Since the number of questions in our experiments is quite 12: 16  X  C.-W. Lee et al. ing documents. Therefore, we use RU-accuracy, which is described in Section documents; only answers are checked. The manually examined answers are then fed back to the datasets to form three expanded datasets: NTCIR5-CC-D200e, NTCIR5-CC-T200e, and NTCIR6-CC-T150e. In addition, we created the IASL-CC-Q465 dataset to increase the confidence in our experiments. It was created by three people using a program that randomly selected passages from the CIRB40 corpus. The human creator can use whatever keywords to search for more documents about a randomly selected passage, and create one 465 questions in the IASL-CC-Q465 dataset, and combined with the NTCIR provided datasets, we had a total of 1,015 questions. 5.2 Evaluation Metrics In this sub-section, we describe related evaluation metrics. 5.2.1 R-Accuracy and RU-Accuracy. R-Accuracy and RU-Accuracy are used to measure QA performance in NTCIR CLQA. A QA system returns a list of ranked answer responses for each question, but R-accuracy and RU-accuracy only consider the correctness of the top-1 rank answer response on the list. An answer response is a pair comprised of an answer and its source document. Each answer response is judged as Right, Unsupported, or Wrong, as defined in the NTCIR-6 CLQA overview [Lee et al. 2007]: answered questions divided by the total number of questions. R-accuracy means that only  X  X ight X  judgments are regarded as correct, while RU-accuracy means that both  X  X ight X  and  X  X nsupported X  judgments are counted.
RU -Accuracy = to refer to RU-accuracy when the context is not ambiguous.
 Mean Reciprocal Rank (MRR)
We use MRR to measure the QA performance based on all the highest ranked correct answers, not just the top1 answer. The MRR is calculated as follows:
MRR = Expected Answer Accuracy (EAA)
In addition to using the normal answer accuracy metrics, we propose a new metric called the Expected Answer Accuracy (EAA). There are some cases MRR value. This phenomenon usually occurs when several top answers have the same ranking score. We use the EAA to resolve such problems. The EAA score of a ranking method is defined as follows: 6. EXPERIMENTS We conducted three experiments. In the first two, we compared SCO-QAT, SCO-QAT with distance, and some shallow answer-ranking features. The re-sults show that SCO-QAT is more accurate than the other shallow features, and the distance information further improves SCO-QAT X  X  performance. In the third experiment, we applied an ABSP-based filter to evaluate the effec-tiveness of ABSPs. 6.1 Comparing SCO-QAT with Other Single Ranking Features Answer correctness features are usually combined to achieve the best perfor-mance. However, the features in QA are usually combined by using heuristic methods. Although some systems have used machine learning approaches in other QA work. This may be because QA feature combination methods are not mature enough to deal with the variability of QA systems, or the amount of sume they are more reliable and can be applied to other systems or languages more easily.

As well as SCO-QAT, we tested the following widely used shallow features: quency . The keyword overlap is the ratio of question keywords found in a passage, as used in Cooper and Ruger [2000], Molla and Gardiner [2005], and Zhao et al. [2005]. The IR score [Kwok and Deng 2006; Zheng 2005], which is 12: 18  X  C.-W. Lee et al.
 and the question keywords in a passage. There are several ways to calculate density. In our experiment, we simply adopt Lin X  X  formula [Lin et al. 2005], which performed well in NTCIR-5 CLQA. The mutual information score is cal-culated by the PMI method [Magnini et al. 2001].

The experiment results are listed in Table IV. For C-C datasets, SCO-QAT EAA for the NTCIR5-CC-D200e dataset, 0.515 for the NTCIR5-CC-T200e dataset, 0.546 for the IASL-CC-Q465 dataset, and 0.406 for the NTCIR6-CC-also showed that SCO-QAT is significantly more accurate than all the other shallow ranking features.
 In addition to comparing to single ranking features, we compare the SCO-QAT results with those of other participants in the NTCIR5 CLQA task combined-feature comparison. In the NTCIR5 CLQA [Sasaki et al. 2005], there were thirteen Chinese QA runs, and the accuracy ranged from 0.105 to 0.445, with a mean of 0.315. It is impressive that SCO-QAT achieved 0.515 accu-performing system in the NTCIR5 CLQA C-C subtask).

Although frequency is the simplest of the shallow features, it performs sur-module, or the characteristics of the Chinese news corpus, or the way ques-tions were created, which caused questions with high frequency answers to frequency feature only. Further investigation is therefore needed to explain the phenomenon.

As shown in Table VI, the MI approach does not perform well in our experi-The performance of the density approach, which is popular in QA systems, was zation X  type questions, as its accuracy was only 0.10 to 0.15.

Although SCO-QAT was the best shallow feature in the experiment, a num-there is more than one highly related answer to the given question, SCO-QAT cannot determine which one is better. Take the Chinese question  X   X  O  X  fl  X  =  X   X   X  ?  X  (Who is the president of Microsoft?) in the dataset, for exam-company, SCO-QAT cannot determine which answer is correct. Another rel-atively minor problem is that of improper question terms, such as functional words. This could be solved by removing the improper terms, but it would require some heuristic rules or external knowledge. 6.2 Enhancing SCO-QAT with Distance Information We experimented on the extended version of SCO-QAT, described in Section 4.3 are listed in Table VII. SCO-QAT with distance information achieved 0.568 EAA for the NTCIR5-CC-D200e dataset, 0.538 for the NTCIR5-CC-T200e dataset, 0.565 for the IASL-CC-Q465 dataset, and 0.453 for the NTCIR6-CC-T150 dataset. Compared to the original SCO-QAT, improvements in the EAA distance was significantly more accurate than SCO-QAT at the 0.01 level. 12: 20  X  C.-W. Lee et al.
 6.3 ABSP-Based Answer Filter ABSPs have been incorporated into ASQA as an answer filter. To evaluate the filter X  X  performance, we used 865 training questions from NTCIR5-CC-D200e, NTCIR5-CC-T200e, and IASL-CC-Q465 datasets. For each training question, we applied the generation algorithm to the top 200 most relevant passages retrieved by the passage retrieval module and generated about 500 ing questions. When the ABSP-based answer filter was used in ASQA for the NTCIR-6 dataset, the RU-accuracy increased from 0.453 to 0.5, as shown in Table VIII. To determine whether the improvement was statistically signifi-cant, we applied the McNemar test. The result shows that, at the 0.01 level, the system with the ABSP-based filter is significantly more accurate than the system without the filter. Because the answer filter can only be applied ABSP performance on the questions covered by the filter. For the NTCIR-6 dataset, the question coverage was 37.3% and the accuracy of the questions covered was 0.911. The accuracy rate was much higher than the overall accu-racy rate, which was 0.5. The high accuracy score demonstrates the accuracy of surface pattern-based approaches. With regard to question coverage, al-ABSPs. We believe we can increase the question coverage score if we have larger training dataset. 7. DISCUSSION Co-occurrence methods and surface pattern methods rely on global informa-tion and local information respectively. In other words, co-occurrence-based methods like SCO-QAT are suitable for questions where the answers are pro-vided in several passages. For example, consider the question  X   X  O  X   X   X   X   X  ... i  X   X  W  X  8 f _  X   X   X   X   X  x X   X   X   X  (Which Chinese scientist was accused of violating the Atomic Energy Act because of his purported mis-be answered by SCO-QAT. Surface pattern-based methods like ABSPs are suit-commonly used and easy to extract, for example, surface patterns for answer-ing birth date questions.

Because SCO-QAT relies on passage retrieval, it is highly dependent on the quality of the passages retrieved. SCO-QAT fails when other related terms are the same type as the answer type. For example, the question  X   X   X   X   X  - X  m  X   X   X  (Who was abducted in the Xi X  X n Incident 14 ?) requires a person as its answer, but  X  # -c  X (Chiang Kai-shek) the abducted person and  X  5 x |  X (Zhang Xueliang) the abductor are usually mentioned in sen-tences describing  X   X   X   X   X   X  (the Xi X  X n Incident). The improved version of cannot be solved. For example, in the following sentence,  X  5 x o  X   X   X   X   X  SCO-QAT with distance information to deal with this case.

From the viewpoint of surface patterns, the above cases are easy to handle, it is time-consuming and labor intensive. Surface pattern-based methods can-not be applied to, or cover, some questions if there is no surface pattern.
Next we describe the strategies adopted to solve the coverage problem. 7.1 Generate Patterns for Relations Instead of for Questions Instead of generating surface patterns for question types, we generate sur-tions. Also, we do not define the relation types, but let the ABSP training process choose patterns according to the training data. For example, the pattern  X  (ORGANIZATION) Na Nb (PERSON) h :  X  is useful because it matches passages that describe the relations between the question keywords and the answers. For the same relations, the traditional surface pattern ap-proach may predefine some question types, such as Q PERSON WORKFOR or Q ORGANIZATION POSITION. It then applies question analysis methods manually defines the mapping between extracted question keywords and the proach because we only generate patterns for capturing important relations. Moreover, our approach can be used for any question types that need relation information to verify the correctness of an answer. 12: 22  X  C.-W. Lee et al. 7.2 Simulate Long Patterns with Multiple Short Patterns based on their common terms. This technique partly solves the low coverage face patterns with several small surface patterns. In other words, our ABSP method needs fewer surface patterns. The drawback of this approach is a loss experiment results show that the accuracy is still acceptable. 7.3 Merge Relations from Multiple Passages fore, we merge relations extracted by ABSPs from multiple passages. This mechanism leads to the same accuracy issue as that in strategy 7.2 above. 7.4 More Semantic Tags We tag sentences with more semantic tags to make surface patterns more ab-
Both the SCO-QAT and ABSP methods are affected by the word canonical- X   X  c  X  in traditional form;  X  X hirteen X  could be  X 13 X  in Arabic numerals,  X   X  canonicalization in the ASQA system, SCO-QAT underestimates the score of some correct answers and ABSPs fail to merge some important relations.
Although we have demonstrated the effectiveness of SCO-QAT and ABSPs on Chinese Factoid QA, more experiments are needed before they can be ap-plied to other languages. We believe SCO-QAT can be used directly in other languages, since SCO-QAT is a pure co-occurrence-based method that is not dependent on syntax features. However, for ABSPs to perform well in lan-guages with complex syntactic structures, more syntactic constructs, such as noun phrases or verb phrases, may be needed. For example, in our experience, English sentences are more structured than Chinese sentences and the dis-tances between dependent words are usually longer. This may result in a less than optimal performance, because the patterns generated by ABSPs may not be long enough to capture the relations described in English sentences. 8. CONCLUSION AND FUTURE WORK We propose two lightweight methods, SCO-QAT and ABSPs, for use in a state-of-the-art Chinese factoid QA system. The methods require fewer resources than heavy methods, such as the parsers and logic provers used in state-of-the-art QA systems in other languages.

We show that lightweight methods can operate in resource-limited situa-tions, and that they have great potential to boost QA performance. We im-proved the RU-Accuracy of the ASQA system from 0.445 to 0.535 when it was tested on NTCIR-5 CLQA C-C datasets. The result is significant and impres-sive because only the answer filtering and answer ranking modules are changed. The quality of the answers extracted by the answer extraction mod-ule is unchanged, but the answer selection strategy is improved. The en-hanced system also performed well in NTCIR-6, achieving 0.553 RU-Accuracy (0.5 of the RU-Accuracy was contributed by SCO-QAT and ABSPs) in the C-C subtask.

The ABSP method is a variation of surface pattern methods. It tries to in-crease question coverage and maintain accuracy by targeting surface patterns extracted by multiple surface patterns from multiple passages, and incorpo-coverage and 0.911 RU-Accuracy on the questions covered.

The SCO-QAT method utilizes co-occurrence information in retrieved pas-sages. Since it calculates all the co-occurrence combinations without extra Moreover, SCO-QAT does not require word-ignoring rules to handle missing counts and it can be combined with other answer ranking features. ASQA achieved 0.535 on the NTCIR-5 C-C dataset by only ranking with SCO-QAT enhanced with distance information, which was better than all the combined features used at NTCIR-5. We attribute the success of the ABSP and SCO-QAT methods to the effective use of local syntactic information and global co-occurrence information.

SCO-QAT and ABSPs can be improved in several ways. In both methods, applying rules with taxonomy or ontology resources would solve most canon-icalization problems. For SCO-QAT, it would be helpful if we were to use a better term weighting scheme. Using more syntactic information, such as incorporating surface patterns, would result in more reliable co-occurrence grained, would improve the accuracy while maintaining question coverage. because some errors are caused by tagging, such as wrong word segmentation. APPENDIX A. ABSPs Used in ASQA at NTCIR-6 CLQA (1) ARTIFACT Na PERSON VE (2) ARTIFACT Na -PERSON (3) ARTIFACT Na Na Na PERSON (4) ARTIFACT TIME P LOCATION (5) LOCATION Na PERSON -VC (6) LOCATION Na P PERSON LOCATION Na -Na 12: 24  X  C.-W. Lee et al. (7) LOCATION Na P TIME VH (8) LOCATION Na PA ARTIFACT PA (9) LOCATION Na -NUMBER OCCUPATION -PERSON (10) LOCATION Na OCCUPATION PERSON (11) LOCATION -TIME V LOCATION (12) LOCATION -Na Na PERSON (13) LOCATION LOCATION Na PERSON Na (14) LOCATION LOCATION  X  ORGANIZATION (15) LOCATION NUMBER Na -OCCUPATION PERSON (16) LOCATION OCCUPATION PERSON (17) LOCATION OCCUPATION PERSON VJ TIME -PERSON Na (18) LOCATION ORGANIZATION T ORGANIZATION Na (19) LOCATION ORGANIZATION -Na PERSON (20) LOCATION  X  LOCATION (21) LOCATION  X  Nc LOCATION (22) LOCATION Nc LOCATION (23) LOCATION  X  OCCUPATION PERSON (24) LOCATION  X  ORGANIZATION (25) LOCATION M  X  LOCATION LOCATION (26) OCCUPATION Na -V -PERSON (27) OCCUPATION PERSON PERSON (28) ORGANIZATION Caa ORGANIZATION TIME VE V (29) ORGANIZATION N -PERSON (30) ORGANIZATION N -PERSON PA N (31) ORGANIZATION N -PERSON V N N (32) ORGANIZATION N -NE VE (33) ORGANIZATION N Na PERSON (34) ORGANIZATION N Na PERSON V (35) ORGANIZATION N Na NE P (36) ORGANIZATION N Na NE PA N N N (37) ORGANIZATION OCCUPATION PERSON (38) ORGANIZATION Na PERSON (39) ORGANIZATION Na PERSON -VE CO (40) ORGANIZATION Na PERSON -h : (41) ORGANIZATION Na PERSON V (42) ORGANIZATION Na PERSON VE ORGANIZATION (43) ORGANIZATION Na -PERSON (44) ORGANIZATION Na -PERSON V (45) ORGANIZATION Na -NE VE CO (46) ORGANIZATION Na -NE VE (47) ORGANIZATION Na -NE h : (48) ORGANIZATION Na NE V -Nb (49) ORGANIZATION Na NE VE CO (50) ORGANIZATION Na NE VE CO N (51) ORGANIZATION Na NE VE (52) ORGANIZATION Na NE VE Nb (53) ORGANIZATION Na NE h : Nb (54) ORGANIZATION Na NE  X  CO (55) ORGANIZATION Na Na PERSON VC (56) ORGANIZATION Na Na -NE (57) ORGANIZATION Na Nb PERSON h : (58) ORGANIZATION Nc Na PERSON (59) ORGANIZATION Nc Na PERSON V (60) ORGANIZATION Nd Na ORGANIZATION (61) ORGANIZATION ! w NE (62) ORGANIZATION VCL LOCATION (63) ORGANIZAITON C ORGANIZATION TIME VE V (64) ORGANIZATION -PERSON VC LOCATION (65) ORGANIZATION -PERSON VC ORGANIZATION (66) ORGANIZAITON -OCCUPATION PERSON (67) PERSON V ORGANIZATION (68) PERSON h : ORGANIZATION (69) PERSON Nd VJ Na X Na VJ ORGANIZATION (70) PERSON P ORGANIZATION Nc Nc Na (71) PERSON P ORGANIZATION V (72) PERSON P V LOCATION OCCUPATION (73) PERSON Na VC LOCATION (74) PERSON Na VC ORGANIZATION (75) PERSON Nc -OCCUPATION PERSON (76) PERSON VC -Na ARTIFACT (77) PERSON VC ARTIFACT VC ORGANIZATION (78) PERSON VC LOCATION Na (79) PERSON VC OCCUPATION Na (80) PERSON VC PERSON (81) PERSON VC PA ARTIFACT PA (82) PERSON VC  X  -PA ARTIFACT PA (83) PERSON VG LOCATION OCCUPATION (84) PERSON VG -VG --OCCUPATION (85) PERSON VJ ORGANIZATION -ARTIFACT (86) PERSON / ORGANIZATION -OCCUPATION 12: 26  X  C.-W. Lee et al. (87) PERSON  X  ORGANIZATION (88) PERSON  X  PA ARTIFACT PA (89) TIME -LOCATION LOCATION VJ (90) TIME P LOCATION VE DE ARTIFACT (91) TIME ARTIFACT V (92) TIME LOCATION Na -Na (93) TIME  X  LOCATION Na (94) TIME VC LOCATION Na (95) C ORGANIZATION N Na PERSON (96) C ORGANIZATION Na -PERSON (97) CO ORGANIZATION N N (98) CO PERSON -P VG LOCATION Neu Nf -Na (99) N Na X PERSON VE ORGANIZATION (100) N -ORGANIZATION N N Nb VC (101) NE VE ORGANIZATION N N (102) Na -V ORGANIZATION c  X  w NE (103) Na NE V ORGANIZATION (104) Na Na Nb VE ORGANIZATION (105) Na Nb V X ORGANIZATION N N (106) Na PERSON P PERSON Nd V (107) Na PERSON P PERSON V Na (108) Na OCCUPATION FW PERSON (109) Nc Na PERSON  X  ORGANIZATION N V (110) Nc NE X NE LOCATION (111) P ORGANIZATION Na PERSON (112) P ORGANIZATION Na PERSON X V (113) P PERSON T ORGNIZATION LOCATION (114) P LOCATION ORGANIZATION VH (115) PA ARTIFACT PA OCCUPATION PERSON (116) VE LOCATION N N P LOCATION (117) V Di PERSON ARTIFACT (118) VC PA PERSON ARTIFACT PA (119) VH  X  Na OCCUPATION PERSON (120) VH  X  ORGANIZATION OCCUPATION PERSON (121) VH  X  LOCATION OCCUPATION PERSON (122) ORGANIZATION Na PERSON (123) ORGANIZATION Na Na PERSON  X  (124) ORGANIZAITON ORGANIZATION Na (125) ( LOCATION Nc LOCATION (126) VH  X  OCCUPATION PERSON B. Stop Word List for SCO-QAT 12: 28  X  C.-W. Lee et al.

