 Matrix approximation is one of the most effective meth-ods for collaborative filtering-based recommender systems. However, the high computation complexity of matrix fac-torization on large datasets limits its scalability. Prior so-lutions have adopted co-clustering methods to partition a large matrix into a set of smaller submatrices, which can then be processed in parallel to improve scalability. The drawback is that the recommendation accuracy is lower as the submatrices only contain subsets of the user-item rating information.

This paper presents WEMAREC, a weighted and ensem-ble matrix approximation method for accurate and scal-able recommendation. It builds upon the intuition that (sub)matrices containing more frequent samples of certain user/item/rating tend to make more reliable rating predic-tions for these specific user/item/rating. WEMAREC con-sists of two important components: (1) a weighting strategy that is computed based on the rating distribution in each submatrix and applied to approximate a single matrix con-taining those submatrices; and (2) an ensemble strategy that leverages user-specific and item-specific rating distributions to combine the approximation matrices of multiple sets of co-clustering results. Evaluations using real-world datasets demonstrate that WEMAREC outperforms state-of-the-art matrix approximation methods in recommendation accuracy (0.5 X 11.9% on the MovieLens dataset and 2.2 X 13.1% on the Netflix dataset) with 3 X 10X improvement on scalability. H.3.4 [ Systems and Software ]: User profiles and alert services R ecommendation; matrix approximation; weighted; ensem-ble
Chao Chen and Dongsheng Li contributed equally to this work.
  X 
Collaborative filtering (CF), which predicts users X  item ratings based on the ratings of other users with similar taste, has been shown to preform well in many recommender sys-tems [1]. Among existing CF solutions, matrix approxima-tion has become increasingly popular. It formulates the rec-ommendation problem as missing entry prediction using ex-isting entries in a user-item rating matrix, i.e., attempting to predict the missing entries in a partially observed matrix. Given m users and n items, the user-item rating matrix M  X  R m  X  n is typically of low-rank, then M can be approx-imated by a r -rank matrix  X  M = UV T , where U  X  R m  X  r is the set of user features, V  X  R n  X  r is the set of item fea-tures, and r min( m, n ). Then, the rating of the i -th user on the j -th item can be predicted by the inner product U
V T j . Using matrix approximation, the user/item feature vectors are reduced to lower dimensions, which helps to ad-dress the  X  X ata sparsity X  issue, a challenge to memory-based CF methods [1, 24]. Recent studies have shown that matrix approximation based CF methods outperform many other CF solutions [10, 17, 22, 28].

However, existing matrix approximation based CF meth-ods exhibit poor scalability due to the high computation complexity of matrix factorization on large user-item rat-ing datasets [10, 22, 16, 11, 28]. Recent work adopted co-clustering methods [8, 29, 26] to partition the large user-item rating matrix into a set of smaller submatrices, which can then be processed in parallel to improve system scalability. However, this usually leads to lower recommendation accu-racy. Co-clustering tries to find coherent submatrices each of which contains a subset of users who share similar interests on a subset of items. In the ideal case, recommendations based on such submatrices can be as accurate as recommen-dations based on the original large matrix while requiring much less computation overhead. However, the submatrices obtained by co-clustering methods are not perfect, as a small fraction of user-item ratings may not follow the distribution of majority ratings. As a result, recommendation accuracy on such user-item ratings will degrade, affecting the overall recommendation accuracy. Our study shows that, for the MovieLens dataset with 1 million ratings, the recommenda-tion RMSE (root mean square error) increases from 0.8645 to 0.9 when the co-clustering setting varies from 1  X  1to 5  X  5. Therefore, a better matrix approximation solution that achieves both high accuracy and high scalability for recommendation is needed.

In this work, we have developed WEMAREC, a weighted and ensemble matrix approximation method for accurate and scalable CF-based recommendation. The intuition is that, (sub)matrices only contain partially-sampled informa-tion of user/item/rating; if a submatrix contains more sam-ples for a certain user or item or rating, this submatrix can probably make more reliable predictions on the specific user or item or rating. This applies not only to the submatri-ces generated from a single co-clustering process, but also to the multiple sets of submatrices generated using different co-clustering constraints. Since these submatrices contain different user-item rating information, an intelligent combi-nation of these submatrices can yield better recommenda-tion quality while still enjoy the benefit of high scalability due to parallel processing the co-clustering submatrices.
This work makes the following contributions: (1) iden-tification of the unbalanced predication power on different users/items/ratings due to the partial information that is contained in (sub)matrices; (2) development of a submatrix-based weighting strategy to capture rating-specific predic-tion power and combine submatrices into the approximation of a single user-item rating matrix; (3) development of an ensemble matrix approximation method that uses different co-clustering constraints to generate and combine multiple sets of submatrices with different rating prediction power for different users and items; and (4) evaluation using two large-scale real-world datasets which demonstrates WEMAREC X  X  improvement in recommendation accuracy and scalability over state-of-the-art matrix approximation techniques.
The rest of this paper is organized as follows. Section 2 formulates the problem. Section 3 describes the proposed WEMAREC method in detail. Section 4 analyzes the er-ror bounds of the proposed method. Section 5 presents the evaluation results. Section 6 discusses the related work, and finally Section 7 concludes this work.
This section provides necessary background for the ma-trix approximation problem. It then presents case studies to motivate the challenge faced by existing matrix approxi-mation methods. The case studies presented in this section are conducted on the MovieLens (1M) dataset and the stan-dard singular value decomposition (SVD) algorithm is used in matrix approximation.
In this paper, upper case letters such as M,U,V denote matrices. For matrix M  X  R m  X  n ,wedenote M i  X  as the i -th row vector, M  X  j as j -th column vector, and M ij as the entry in the i -th row and j -th column. We denote M as a submatrix of M , i.e., both the rows and columns in M are subsets of those in M .A r -rank approximation of M is denoted as  X  M = UV T , where U  X  R m  X  r ,V  X  R n  X  r and r min( m, n ). In addition, [ n ] denotes the list of { 1 ,...,n } ,  X  denotes the set of observed entries in the user-item rating matrix M ,i.e.,  X  ( i, j )  X   X , M i,j = 0. Then, is denoted as the total number of observed entries in M .
Three matrix norms are used in this paper. The Frobenius norm is denoted as: The nuclear norm is denoted as the sum of singular values:
RMSE Time (s) Figure 1: The tradeoff between recommendation accuracy and runtime efficiency when varying the number of co-clusters.
 The max norm is denoted as:
Two methods have been used for low-rank matrix approx-imation  X  M of M , i.e., SVD and compressed sensing [5, 6]. The SVD method is based on minimizing the sum-squared distance  X  Frobenius norm: M = arg min where each I ij is the indicator function that equals to 1 if M ij is observed and equals to 0 otherwise. The compressed sensing method is based on minimizing the nuclear norm: As shown in [22], the problem defined in (1) is a difficult non-convex optimization problem and an iterative method may converge to a local minimum. In contrast to SVD, the problem defined in (2) is convex and can be casted as a semi-definite program [6].

Rating Distribution RMSE (w/o weighting) RMSE (w/ weighting) Table 1: Rating-specific RMSE when running SVD without (w/o) or with (w/) weighting on a subma-trix.
Next, we present case studies to demonstrate the accuracy issue of existing co-clustering based matrix approximation methods, and provide insights on why user-item rating dis-tribution can be leveraged to improve the recommendation accuracy.
Co-clustering is an effective method to improve the scal-ability of matrix approximation based CF methods [8, 29], because these submatrices can be processed in parallel. In addition, co-clustering tries to find coherent submatrices, each of which containing a subset of users who share similar interests on a subset of items. In the ideal case, users X  com-mon interests in each submatrix can be accurately predicted. Unfortunately, the submatrices obtained by co-clustering are not perfect. Within each submatrix, a subset of user-item ratings may not follow the distribution of majority ratings. As a result, the corresponding recommendation accuracy of those minority user-item ratings may be poor, which in turn affects the overall recommendation accuracy.

To evaluate how the recommendation accuracy varies with co-clustering granularity, we apply Bregman co-clustering [2] on the MovieLens dataset. As demonstrated in Figure 1, as-suming the co-clustering submatrices are processed in par-allel, the scalability of matrix approximation increases when the number of co-clusters increase from 1  X  1to5  X  5( k  X  k ). On the other hand, the recommendation error (mea-sured as RMSE) increases from 0.8645 to 0.9 as the number of co-clusters increases. Therefore, in order to utilize co-clustering based scalable matrix approximation methods in recommender systems, one must address the accuracy issue.
Although the submatrices obtained through co-clustering are not informative enough to build accurate recommenda-tion models for all users and all items in each submatrix, they can still be utilized to build  X  X eak X  recommendation models which can accurately predict the common interests shared by users in the same submatrix. In this study, we analyze (1) which part of information in each submatrix rep-resents users X  common interests and thus can be utilized to build  X  X eak X  recommendation models and (2) how we can make these  X  X eak X  recommendation models more accurate when predicting users X  common interests in a submatrix.
Table 1 shows the distribution of different ratings in a submatrix obtained from Bregman co-clustering, as well as the recommendation quality when applying standard SVD algorithm on the submatrix. As shown in the third column, the RMSE varies by the specific rating and lower RMSEs (i.e., better recommendation accuracy) are achieved for rat-ings that occur more frequently in the submatrix, such as 3 and 2 ratings. This makes sense because a learning model usually does a better job capturing samples that occur more frequently in the train data. Based on this observation, we can train  X  X eak X  recommendation models from the subma-trices, which can at least make accurate recommendations on users X  common interests, i.e., ratings that occur most fre-quently in the corresponding submatrix.

Given the biased prediction power of the  X  X eak X  mod-els towards ratings that occur more frequently in a subma-trix, we could potentially boost the recommendation accu-racy by weighting the user-item ratings differently, assigning higher (lower) weights to ratings that occur more frequently (rarely) in the submatrix. The expectation is that we could obtain more accurate recommendations on the majority of the ratings and the overall recommendation accuracy can be improved. The fourth column in Table 1 shows the recom-mendation quality after adding such weights to differentiate the user-item ratings. Clearly, we can see that the  X  X eak X  recommendation model built by SVD with weighting can in-deed make more accurate recommendations on the ratings that occur more frequently (i.e., 2, 3 and 4) than the SVD method without weighting. More importantly, the over-all recommendation quality also increases after weighting, Figure 2: WEMAREC design overview. The orig-inal user-item rating matrix M is described by z low-rank matrices, which are based on z different k  X  l co-clustering settings. For all pairs ( u, i )  X  [ m ]  X  [ n ] , the entry M ui is computed based the cor-responding entries in the co-cluster-based subma-trices (denoted as shaded regions). The equation describes how to compute a unified matrix approxi-mation  X  M from z co-clustering based approximations {  X 
M 1 ,  X  M 2 ,...,  X  M z } . i.e., RMSE decreases from 0.9517 to 0.9479. These results demonstrate that rating-specific weighting has the potential to boost the accuracy of more frequently-occurring ratings and enhance the overall accuracy as well.
In this section, we present the design details of WEMAREC whichcanachievebothhighrecommendationaccuracyand high scalability for matrix approximation based CF meth-ods. As illustrated in Figure 2, WEMAREC consists of three key steps: 1. Co-clustering and submatrices generation. The original user-item rating matrix is first divided into a set of submatrices by Bregman co-clustering, so that the scal-ability issue can be addressed by factoring all submatrices in parallel. Also, different co-clusterings can be obtained by varying the constraints in Bregman co-clustering, which naturally offers us the ability to exploit the advantages of dif-ferent co-clusterings to achieve better recommendation ac-curacy. 2. Submatrices-based weighting and matrix approx-imation. A new weighting strategy is proposed, which is computed based on each individual submatrix and assigns higher weights to ratings that occur more frequently in a given submatrix. The weighted submatrices from the same co-clustering setting are then used to generate a single ma-trix approximation. 3. Ensemble of multiple matrix approximations. Dif-ferent co-clustering constraints can lead to different subma-trices and thus different matrix approximations. Since each  X  X eak X  recommendation model can only make accurate rec-ommendations on some of the user-item ratings, an ensemble strategy is proposed, which utilizes the advantages of dif-ferent  X  X eak X  models to realize a  X  X trong X  recommendation model which can achieve high accuracy.
Co-clustering is a popular technique which allows simul-taneous clustering of both the rows and columns in a given matrix. By applying co-clustering methods on user-item rat-ing matrix in recommender systems, users and items corre-spond to a co-cluster (submatrix) are highly correlated, i.e., these users will have similar opinions on these items. More formally, let submatrix M = { M ui | u  X  X  ,i  X  X } denote the ratings of a subset of users U on a subset of items I we properly choose a set of very similar users U and a set of very similar items I , then M can be reconstructed by fewer number of parameters, i.e., lower rank. Such co-clustering can be beneficial in two aspects: 1) these submatrices can be approximated simultaneously via parallel computing so that high scalability can be achieved and 2) each submatrix will have lower rank than original user-item rating matrix, so that low-rank matrix approximation can be computed more efficiently for each submatrix.

In order to find such coherent submatrices in the user-item rating matrix, we consider to simultaneously partition all users and items into disjoint user clusters {U 1 ,..., and item clusters {I 1 ,..., I l } , and a co-cluster ( U sponds to one desired submatrix. Bregman co-clustering [2] is adopted in this paper to achieve such partitioning. It views the co-clustering as a lossy data compression prob-lem, and attempts to obtain as much information as pos-sible about the original matrix with a few number of crit-ical statistics for co-clusters, such as the row and column averages of each co-cluster. Following common approaches in Bregman co-clustering, we should firstly choose a set of statistics of original matrix that need to be preserved, e.g., the average rating of each user or the average rating of each item, etc., and each statistic can be viewed as a constraint. We denote C as the constraint set, and six of the most pop-ular non-trivial constraint sets are described as follows: where I r and I c are the random variables of row and column indices, which take values over { 1 ,...,m } and { 1 ,...,n respectively.  X  I r and  X  I c are the random variables of row andcolumnclusters,whichtakevaluesover { 1 ,...,k } and { 1 ,...,l } in a k  X  l co-clustering. More specifically, C that the average values of each row and each column should be preserved, C 2 means the average values of all entries in-side each co-cluster should be preserved, and similarly for the other constraint sets. Besides the constraints, we also need to select appropriate Bregman divergence to evaluate a co-clustering, which is defined as follows: for z 1 ,z 2 d ( z 1 ,z 2 )=  X  ( z 1 )  X   X  ( z 2 )  X  &lt;z 1  X  z 2 ,  X  ( z  X  is the gradient of differential function  X  . Two popu-lar Bregman divergences are I-divergence and Squared Eu-clidean distance, defined respectively as follows: Finally, the row and column clustering will be achieved by an iterative meta algorithm, in which the row and column cluster updates can be obtained from the optimal Lagrange multipliers in parallel. The details of the meta algorithms for achieving Bregman co-clustering can be found in [7, 8]. Since the meta algorithms are not the contributions of this paper, details of these algorithms are omitted.
As described earlier, performing standard matrix approx-imation on submatrices will not be accurate due to the fact that each submatrix only holds partial information of users/items/ratings. Therefore, we can only learn a  X  X eak X  recommendation model from each submatrix, which can make accurate recommendation on a majority of ratings in the corresponding submatrix. As demonstrated in the motivat-ing examples (Section 2.3.2), by associating higher weights to ratings that occur more frequently in a given submatrix, we could potentially improve not only the recommendation accuracy for the frequent ratings, but also the overall rec-ommendation accuracy.

Based on the idea above, we propose a new method for low-rank matrix approximation, which weighs the individual user-item rating differently based on the rating distribution in a submatrix. Specifically, we compute the probabilistic distribution of different ratings in each submatrix and con-struct a weighted norm by adding the probabilistic informa-tion. Since the weight is a function of the rating distribution, we can construct the weighting function p ( x ): F  X  R with Taylor X  X  formula as follows: Since Pr[ x ]  X  [0 , 1], the residual term r p , which is super lin-ear to Pr[ x ], can be omitted. Without loss of generality, we can assume that C 0 = 1 by scaling all the parameters, then there is only one unknown parameter  X  0 in the weight-ing function. The value of  X  0 should be trained such that optimal recommendation accuracy can be achieved. How-ever, this is not straightforward, because recommendation is one-step further after matrix approximation. Therefore, we choose the optimal  X  0 by brute force search, in which we check every  X  0 value in the linear function p ( x ). The sensitivity analysis of  X  0 is presented in Section 5.2.
After defining the weighting function p ( x ), we present the extended SVD and compressed sensing matrix approxima-tion methods here, in which weight W ij is p ( M ij )ifthe entry is observed and 0 otherwise. It should be noted that the standard low-rank matrix approximation methods, such as SVD, can be regarded as special cases of the proposed method by setting  X  0 =0.

Extension of SVD : M = arg min
Extension of Compressed Sensing : M = arg min The two optimization problems describe how to estimate  X  M from observed entries in an original submatrix M . Then, missing entries in the original submatrix M can be obtained from  X  M , i.e., user ratings on unrated items can be predicted from  X  M as in other matrix approximation methods.
After applying Bregman co-clustering on the user-item rating matrix M ,a k  X  l co-clustering ( k is the number of user clusters and l isthenumberofitemclusters)can be obtained. Then, we can perform the proposed low-rank matrix approximation on each submatrix. Here, we present a gradient decent learning algorithm for approximating the user-item rating matrix based on the k  X  l co-clustering. As described in Algorithm 1, the weight p ( x ) for each entry in each submatrix is first computed. Then, the proposed low-rank matrix approximation is achieved by a gradient decent method with L 2 regularization. At last, we combine all the resulting submatrix approximations, so that the approxi-mated matrix  X  M can be obtained by re-locating each entry in the k  X  l submatrices.
 Algorithm 1 Co-clustering-based Matrix Approximation Input: All co-clustering submatrices M ( t )  X  M ( t  X  [ kl ]), Output: Approximated user-item rating matrix  X  M . 1: for each t  X  X  1 ,...,kl } in parallel do 2: // Computing weights 3: Compute the rating distribution on F in M ( t ) . 4: for each observed entry ( u, i )in M ( t ) do 5: W ui = p ( x ), if M ui = x . 6: end for 7: // Updating model 9: while not converged do 10: for each observed entry ( u, i )in M ( t ) do 12: for each z  X  X  1 ,...,r } do 15: end for 16: end for 17: end while 18: end for 19: for each ( u, i )  X  [ m ]  X  [ n ] do 20: Locate ( u, i ) in its corresponding submatrix and let 22: end for 23: return  X  M
As described in Section 3.1, a Bregman co-clustering con-sists of three components: a constraint set C ,aBregman divergence d  X  , and the number of row clusters k and column clusters l . For convenience, we denote a 3-tuple ( C ,d  X  as a co-clustering. It should be noted that different 3-tuples (
C ,d  X  ,k  X  l ) could lead to different matrix approximation re-sults  X  M because each entry in  X  M is generated based on the corresponding co-cluster. In Section 3.1, we described six constraint sets and two Bregman divergences, which means that we can construct 6  X  2 = 12 different  X  M s by combining different C and d  X  given fixed k and l . Therefore, we propose an ensemble method to intelligently combine the user-item rating predictions obtained from multiple matrix approxi-mations based on different co-clustering settings. Our goal is to further enhance the recommendation accuracy.
To recover a global approximation  X  M from z low-rank ap-proximations  X  M ( t ) ( t  X  [ z ]), we adopt the weighted mean of  X  M determined by the confidences of both users and items. And a predicted rating from a  X  X eak X  recommendation model should be considered as more important if 1) the corre-sponding user frequently gave such rating to items before and 2) the corresponding item was frequently rated by such rating before. More formally, the i -th user ( M i  X  ) and the j -th item ( M  X  j ) can be viewed as discrete random variables over a finite-field F with unique distributions Pr( x ; M Pr( x ; M  X  j ), x  X  F . Furthermore, the prediction of M should be considered more confident if user i and item j have (been) rated x many times in M . Therefore, the en-semble weight q ( x ): F  X  R can be regarded as a function of Pr( x ; M i  X  ) and Pr( x ; M  X  j )asfollows: Again, by omit the small residual term r q and scaling the constant term to 1, we can obtain the final ensemble weight as Equation 12. Then, the proposed ensemble method can be performed as follows: where Q ( t ) ui = q ( M ui ). Based on Equation 13, we present Al-gorithm 2 to describe how to predict missing values in user-item rating matrix for recommendation. We can clearly see that the global matrix approximation  X  M can be efficiently computed because the computation for all entries in  X  M just requires weighted averaging.
 Algorithm 2 WEMAREC Ensemble ( u, i ) Input: Resulting matrix approximations  X  M ( t ) ( t  X  [ z ]) Output: The predicted rating of user u on item i :  X  M ui 1: // Computing weights 2: for t  X  [ z ] do 4: end for
The proposed weighted and ensemble matrix approxima-tion method (WEMAREC) is faster than many state-of-the-art matrix approximation algorithms, although its overall computational complexity is nearly z times larger than solv-ing a regularized SVD problem. The reasons why the pro-posed WEMAREC method can run faster are as follow: (1) Every co-cluster is independent from each other, and ma-trix approximation on each co-cluster can be computed in parallel; (2) standard low-rank algorithms have a computa-tion complexity of  X ( rmn ) per-iteration, whereas the pro-posed WEMAREC method significantly reduces the compu-tation complexity to  X ( r |U||I| ) per-iteration because user clusters and item clusters are not overlapping in Bregman co-clustering; and (3) the users inside each co-cluster are highly similar, and so are the items. Therefore, lower rank ( r ) is required to achieve accurate matrix approximation in the proposed method than other methods, which further re-duces its running time. Besides theoretical analysis, we also analyze the scalability of the proposed WEMAREC method in Section 5.
This section analyzes the generalization error bounds of the proposed method. We use the root mean squared error (RMSE), one of the most widely adopted accuracy measures in recommender systems [1], as the evaluation metric: where M  X  F m  X  n and Pr[max( F )  X   X  M ui  X  min( F )] = 1. Then, the following proposition establishes the error bound of the proposed weighted matrix approximation method, i.e., the RMSE of the weighted low-rank matrix approximation method on each co-cluster is bounded, so that we can still find optimum submatrix factorization for recommendation by optimizing the extended optimization problems (Equa-tion 8 and 9).

Proposition 1. For any M  X  F m  X  n , m, n &gt; 2 ,  X &gt; 0 , with probability at least 1  X   X  over choosing a subset  X  of entries in M uniformly,
Proof. Since the entries of  X  are chosen independently and uniformly, it is reasonable to assume each loss (  X  M =( M ui  X   X  M ui ) 2 is a random variable and satisfies which  X  =max( F )  X  min( F ). Hence, based on the Hoeffding Inequality, we have Pr[ D (  X  M )  X  X   X  (  X  M )  X  ]  X  e i.e., Pr[ D (  X  M)  X D  X  (  X  M) + log  X   X  Therefore, the errors of the new problems are bounded.
Next, we theoretically analyze the generalization error bounds of the proposed co-clustering based matrix approx-imation algorithm (Algorithm 1) and the ensemble method (Algorithm 2). Since the error bound of SVD based low-rank matrix approximation method has been well analyzed [19, 21], we focus on analyzing the error bound of compressed-sensing based method (defined in Equation 9) by using sim-ilar analysis techniques as in [5, 6]. As shown in [5, 6], we can recover a rank r matrix M  X  R m  X  n ( n  X  m )with probability at least 1  X  n  X  3 , if the number of observed en-tries is |  X  | X  C X  2 nr log 6 n , where C is a constant and  X  is the strong incoherence parameter. However, this result is not applicable in our case, because the matrix M is approxi-mated by multiple low-rank submatrices. Hence, we develop a new error bound based on a variant of the aforementioned conclusion.

The following analysis makes the following assumptions: (a) every submatrix M is a rank r matrix that satisfies the strong incoherent properties, and (b) the observed entries are uniformly distributed in submatrices such that the den-sity of the observed entries in every submatrix is consistent with each other (i.e., = |  X  | mn ). Without loss of generality, we assume n  X  m , and denote  X  =max( F )  X  min ( F ), where F is the set of ratings. We start by analyzing the error bound of the co-clustering-based model  X  M in Proposition 2. Then, based on Proposition 2, we proceed to derive an error bound on the global approximation  X  M in Proposition 3.
Proposition 2. If the density of the observed entries is largeenoughsuchthat |  X  | X  C X  2 nr log 6 n ,thenwithproba-bility of at leat 1  X   X  ,  X  M corresponding to a k  X  l co-clustering satisfies where  X  =(2 kln )  X  3 .
 Proof. For every user-item pair ( u, i ), an observation M ui is equal to  X  M ui + Z where Z is a random variable whose absolute error is bounded by W  X  ( M  X   X  M )  X   X  (1 +  X  0 )( M  X   X  M )  X   X  (1 +  X  0 )  X . By applying Theorem 7 in [5] to matrix completion problem with bounded noise, we get with probability greater than 1  X   X   X  3 that every co-cluster-based approximation  X  M will satisfy where  X  =max( |U| , |I| ),  X  = min ( |U| , |I| ). For one k clustering, there are kl different submatrices M t , X  ( t ) [ kl ], and obviously t  X  [ kl ]  X  ( t )  X  m . Using Cauchy-Schwarz inequality, we get Therefore, we can bound the approximation error as follows:
W  X  ( M  X   X  M ) F in which (a) holds due to the triangle inequality of Frobenius norm; and (b) holds due to (14); and (c) holds due to (15). Since for all ( u, i )pairs,W ui  X  1. Then, we have Combining (16) and (17), we established the error bound of  X  M as stated above. In order to adjust the confidence level, we take a union bound of the events W X  ( M X   X  M ) F  X  (1 +  X  0 )  X  (4  X  (2+ ) + 2) for each submatrix M ( t ) , then we have i.e., the inequation in Proposition 2 holds with probabilities at least1  X   X  (  X  = 3
Proposition 3. If Proposition 2 holds, then with prob-ability of at leat 1  X   X  ,the  X  M based on z different k co-clustering settings satisfies: where  X  = z (2 kln )  X  3 .

Proof. By Proposition 2, we bound the error of  X  M as follows: ( b ) holds due to Equation 13; ( c ) holds due to the triangle inequality of Frobenius norm; ( d ) holds due to Equation 16. Finally, by dividing proof. The confidence level is adjusted to z (2 kln )  X  3 the union bound property as in Proposition 2.
This section evaluates the proposed method on real-world datasets. The first study conducts sensitivity analysis. The proposed method consists of a set of parameters, i.e., rank of matrices r , and the number of row clusters k and column clusters l . This study evaluates how the recommendation accuracy of the proposed method is affected by these pa-rameters. The second study compares the recommendation accuracy of the proposed method against six state-of-the-art matrix approximation based CF methods using PREA toolkit [13]. The third study evaluates the runtime scalabil-ity of the proposed method against standard SVD method.
The experimental study uses three real-world datasets that have been widely used for evaluating recommendation algo-rithms  X  1) MovieLens 1M (10 6 ratings of 6 , 040 users on 3 , 706 items); 2) MovieLens 10M (10 7 rating of 69 , 878 users on 10 , 677 items); and 3) Netflix (10 8 rating of 480 , 189 users on 17 , 770 items). For each dataset, we split it into train and test sets randomly by setting the ratio between train set and test set as 9 : 1. The results are presented by averaging the results over five different random train-test splits.
We use learning rate v =0 . 002 for gradient decent method,  X  =0 . 01 for L 2 -regularization coefficient, =0 . 0001 for gra-dient descent convergence threshold, and T = 100 for max-imum number of iterations. The proposed method (WE-MAREC) is compared against six state-of-the-art matrix approximation based CF methods, which are described as follows:  X 
NMF [Lee et al., NIPS X  01]: assumes the data and compo-nents are non-negative and every entry follows the Poisson distribution. Then the approximation is achieved by maxi-mizing the log-likelihood.  X 
Regularized SVD [Paterek et al., KDD X  07]: is a stan-dard matrix factorization method inspired by the effective methods of natural language processing, in which user/item features are estimated by minimizing the sum-squared error.  X 
BPMF [Salakhutdinov et al., ICML X  08]: is a Bayesain extension of probabilistic matrix factorization, in which the model is trained using Markov chain Monte Carlo methods.  X 
APG 1 [Toh et al., PJO 2010]: views the recommendation task as a matrix completion problem, and computes the ap-proximation by solving a nuclear norm regularized linear least squares problem.  X 
DFC 2 [Mackey et al., NIPS X  11]: divides a large-scale matrix factorization task into smaller subproblems, and uses the techniques from randomized matrix approximation to combine the subproblem solutions.  X 
LLORMA [Lee et al., ICML X  13]: relaxes the low-rank as-sumption, and assumes that the original matrix is described using multiple low-rank submatrices, which are constructed using techniques from non-parametric kernel smoothing.
We first show how WEMAREC performs with different combinations of parameters. MovieLens (10M) and Netflix datasets are used in this study with randomly selected 90% of data as training data and the rest 10% as test data.
Figure 3 presents the effects of the weighted function p ( x ) (Eqn. 5) with  X  0 varying in [0 , 2 . 0] on three artificially se-lected datasets (from MovieLens (10M)) with different rat-ing distributions. The detailed characteristics of the three selected datasets are presented in Table 2. As we can see, the RMSEs on all three datasets first decrease as  X  0 in-creases from 0, and increases after the optimal accuracies are achieved. We also observe that optimal  X  0 s on more un-even datasets are smaller than those on more even datasets. The reason is that the frequency of different ratings are close on even datasets, so that a greater  X  0 is required to make the weights of different ratings more different. Based on the above study, we choose  X  0 =0 . 4 for the following ex-periments because the submatrices generated by Bregman co-clustering are always uneven.

Figure 4 and 5 analyze the effects of Bregman co-clustering by changing the rank r andthenumbersofrowandcolumn clusters k and l on MovieLens (10M) and Netflix dataset. We can see from the results that recommendation accura-cies increase when the rank r increases from 5 to 20 in Fig-ure 4. And the accuracies on the left (I-divergence) and middle (Euclidean-distance) are worse than those on the right (combination of these two distances), which indicates that the combination of different approximations  X  M leads to better recommendation accuracy than both original ones. Figure 4 also shows that the recommendation accuracy de-creases when k and l increase (from 2  X  2to3  X  3). This is due to the fact that each co-cluster based submatrix consists of less user-item ratings when k and l increase, resulting in insufficient training data for both model training and pre-diction. However, the accuracy first increases when k and l increase (from 2  X  2to2  X  3) in Figure 5, and then decrease when k  X  l =3  X  3. This implies that larger dataset can be divided into more clusters to further improve the efficiency with improved accuracy. Based on this study, we choose rank r =20and k  X  l as 2  X  2or3  X  2intheensemble method, which offers the best recommendation accuracy for WEMAREC. http://www.math.nus.edu.sg/~mattohkc/NNLS.html http://www.cs.ucla.edu/~ameet/dfc/ entropy means that the dataset is more uneven). median-uneven(middle), low-uneven(right) datasets, with  X  MovieLens (10M). Netflix.

Figure 6 analyzes the effect of ensemble weight function q ( x ) (Eqn. 12) by selecting different  X  1 and  X  2 .Andwe can see that the accuracies of the proposed weighted aver-age methods always outperform the simple average method without weighting (i.e.,  X  1 =0 . 0 , X  2 =0 . 0). It seems that larger  X  1 will lead to better accuracy, but we also observe that the RMSE becomes stable when  X  1 &gt; 40. Therefore, we adopt  X  1 =3 . 0and  X  2 = 40 in the following experiments.
This study evaluates the accuracy of the proposed meth-ods by comparing it with the six state-of-the-art matrix ap-proximation based CF methods summarized in Section 5.1, i.e., NMF [11], Regularized SVD (RSVD) [16], BPMF [18], APG [25], DFC [14], LLORMA [12]. Each of the method is configured using the same parameters provided by the orig-inal paper. For the proposed WEMAREC method, we con-sider 2  X  2  X  2 = 8 resulting matrix approximations, which are constructed by varying C X  X C 2 , C 5 } , d  X   X  X  I -divergence , Euclidean -distance } and k  X  l  X  X  2  X  2 , 3  X  2 } . The Movie-Lens (10M) and Netflix datasets are used in this study. Table 3 presents the RMSEs of all these matrix approx-WEMAREC 0.7769  X  0.0004 0.8142  X  0.0001 Table 3: RMSE on MovieLens (10M) and Netflix of NMF (r=50) [11], Regularized SVD (r=50) [16], BPMF(r=30) [18], APG [25], DFC [14], LLORMA (r=20) [12], WEMAREC (r=20). imation based CF methods on the MovieLens (10M) and Netflix datasets. This study shows that the proposed WE-MAREC method outperforms all the other six matrix ap-proximation based CF methods on both the datasets. The reason why the proposed method can further improve the recommendation accuracy is due to 1) the new low-rank ma-trix approximation method can build more accurate models on submatrices because most often appeared ratings (major Figure 6: Effects of ensemble weighted function q ( x ) on the performance of WEMAREC with different  X  1 and  X  2 on MovieLens 10M. Figure 7: Efficiency comparisons of SVD method and WEMAREC method with different sizes of user-item rating matrix and different numbers of row clusters and column clusters. interests) of users are treated more importantly; and 2) the ensemble method can effectively take advantage of the high-quality recommendation results from different co-clusterings to further improve the recommendation accuracy.
This study evaluates how the WEMAREC method can speedup the recommendation process by leveraging paral-lel computing. The MovieLens (1M) dataset is used in this study. Leveraging parallel computing, all the submatrices are processed in parallel, and the execution time of WE-MAREC is determined by that of the largest submatrix.
Figure 7 shows the execution time of WEMAREC by changing the co-clustering settings. The SVD method is included in this study for comparison purpose. This study shows that the execution time of SVD and WEMAREC both increases as the rank increases, and the performance of the WEMAREC method improves as the sizes of the submatri-ces decrease. WEMAREC outperforms SVD by 3X  X  10X when the co-clustering setting varies from 2  X  2to5  X  5, and the speedup increases as the number of submatrices increases. This study demonstrates that WEMAREC can effectively improve the recommendation system scalability on large datasets.
Matrix factorization methods have been widely adopted in many applications [27], as well as recommendation sys-tems [10]. Billsus et al. [4] initially introduced SVD to col-laborative filtering context. Then, Srebro et al. [23] pro-posed a maximum-margin matrix factorization (MMMF) method, which can be formulated as a semi-definite pro-gramming problem for achieving matrix approximation based CF. Rennie et al. [17] investigated a direct gradient-based optimization method for achieving MMMF based CF, which can effectively improve the efficiency of MMMF method. Singh et al. [20] introduced a collective matrix factorization method based on relational learning to generalize existing matrix factorization methods and yielded new large-scale optimization algorithms for these problems. Yu et al. [28] proposed a non-parametric matrix factorization method, to make matrix approximation based CF methods applicable on large-scale datasets. Salakhutdinov et al. [15] extended matrix factorization to probabilistic algorithms by proposing a Probabilistic Matrix Factorization (PMF) method, which can scale linearly with the number of observations in the ma-trix. Based on the above work, a fully Bayesian treatment of PMF is present By Salakhutdinov et al. [18], which can train the user-rating matrix in recommender systems using Markov chain Monte Carlo methods.

In addition to single matrix factorization, ensemble meth-ods have also been investigated in the literature. The Net-flix Prize winners Bell et al. [3] and Koren et al. [9] utilized the combination of memory-based and matrix factorization methods to improve recommendation accuracy. Different from the above work, Mackey et al. [14] introduced a Divide-Factor-Combine (DFC) framework, in which the expensive task of matrix factorization is randomly divided into smaller subproblems which can be solved in parallel using an arbi-trary base matrix factorization algorithms. Lee et al. [12] proposed a local low-rank matrix approximation (LLORMA) method, which generalized the DFC method in a way that a metric structure is used on the original matrix and the matrix partitions are constructed by kernel smoothing.
The DFC method and LLORMA method share similar idea with our method in that ensemble methods are adopted to boost recommendation accuracy. A significant difference between DFC and LLORMA is the construction of subma-trices. In DFC, each submatrix is constructed by random sampling, while in LLORMA the submatrix is made of near-est neighbors within certain range. Different from both of them, the submatrices in our method are constructed via partitional co-clustering, so that each submatrix has low-parameter structure with less users and items, i.e., the sub-matrices in our method are of lower rank. Therefore, matrix approximation on such submatrices can be performed more efficiently. In addition, a submatrix-based weighting strat-egy is proposed to capture rating-specific prediction power of each submatrix, so that more accurate recommendations can be generated on more frequent samples in each subma-trix. Therefore, the overall recommendation accuracy can be improved in each submatrix. Finally, the ensemble strat-egy in the proposed method can leverage both user-specific and item-specific rating distributions to combine the approx-imation matrices, which considers much more information compared with simple averaging method in DFC and kernel smoothing method in LLORMA. Moreover, the proposed en-semble method is more efficient than the LLORMA method, because the kernel distance used in LLORMA method is based on the cosine distances between the rows of factor matrices, requiring extra singular value decompositions.
Matrix approximation methods have shown great success in recommender systems. However, tradeoff between scala-bility and accuracy must be made for most existing matrix approximation based CF methods. In this paper, a weighted and ensemble matrix approximation method (WEMAREC) is proposed to improve both recommendation accuracy and scalability. In WEMAREC, the user-item rating matrix is partitioned into a set of submatrices, which are then pro-cessed in parallel to improve system scalability. To opti-mize recommendation accuracy, a submatrix-based weight-ing strategy and an ensemble strategy are proposed. The weighting strategy improves the accuracy of the majority ratings of individual submatrices. The ensemble strategy improves the overall recommendation accuracy by combin-ing multiple sets of co-clustering results based on the user-specific and item-specific rating distributions. Experimental study on MovieLens and Netflix datasets demonstrates that the proposed method can outperform state-of-the-art ma-trix approximation based CF methods on recommendation accuracy and scalability. This work was supported in part by the National Natural Science Foundation of China under Grant No. 61233016, and the National Science Foundation of USA under Grant Nos. 1251257, 1334351 and 1442971.
