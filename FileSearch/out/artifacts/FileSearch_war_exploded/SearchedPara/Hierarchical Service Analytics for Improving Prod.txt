 Modern day service centers are the building blocks for highly efficient and productive business systems in a knowledge economy. In these service systems, accurate and timely delivery of pertinent information to serv ice representatives becomes the cornerstone for delivering efficient customer service. There are two main steps in achieving this obj ective. The first step concerns efficient text mining to extract critical and pertinent information from the very long service request (SR) documents in the historical database. The sec ond step concerns matching new service requests with previously stored service requests. Both lead to efficiencies by minimizing time spent by service personnel in extracting Intellectual Capital (IC). In this paper we present our text analytics system, the Service Request Analyzer and Recommender (SRAR), which is designed to improve the productivity in an enterprise service center for computer network diagnostics and support. SRAR uni fies a text preprocessor, a hierarchical classifier, and a service request recommender, to deliver critical, pertinent, and categorized knowledge for improved service efficiency. The novel feature we report here is identifying the components of the diagnostic process underlying the creation of the original text documents. This identification is crucial in the successful design a nd prototyping of SRAR and its hierarchical classifier element. Equally, the use of domain knowledge and human expertise to generate features are indispensable synergistic elements in improving the accuracy of the text analysis toward identifying the components of the diagnostic process. The evaluation and comparison of SRAR with other benchmark approaches in the literature demonstrate the effectiveness of our framework and algorithms. This framework can be generalized to be applicable in many service industries and business functions that mine te xtual data to achieve increased efficiency in their service delivery. We observe significant service time responsiveness improve ments during the first step of IC extraction in network service center context at Cisco. H.4.0 [ Information Systems Applications ]: General; I.7.0 [ Document and Text Processing ]: General Algorithms, Design, Experime ntation, Human Factors Text classification, text mi ning, recommendation, service requests, intellectual capital, domain knowledge, expertise, diagnosis process Enterprises today generate and collect a significant amount of data in the ongoing process of doi ng business. The generation of operational data, such as that pr oduced in customer relationship management (CRM), click-through, query logs, and transactions, has resulted in an explosion of information. It is widely understood that such data represents a significant advantage in terms of its potential to be mined so as to provide business insight and guidance. More and more en terprises tend to build business by extracting knowledge and insights from their data [1][2][3][4]. However, there are various difficulties in this process, such as extracting knowledge from noisy data, factoring in the business process, and addressing the diverse user needs. In the service center of a company supporting myriad products and technologies, customer support engineers (CSEs) have the task of providing assistance to customers to address their questions and solve their problems. When a customer calls into the service center with a question or an issue, a Service Request (SR) is created in the database to record the information about the customer, the statement of th e question or problem, and any supporting documentation provided. In a large-scale service environment, thousands of such SRs are created or updated on a daily basis. Once the SR is assigned to an engineer, all communication, tests, and test re sults that resulted from the start to the final close of the request are recorded against the SR. Most of this information is recorded as unstructured text as the content consists of internal or extern al email correspondence, a written description of a phone conversati on, or the text output to a command from a device. Some SRs are simple where the length is a few pages, and some are more complex where the length could be a hundred pages or more. The SR in its completed form after closure has duplicated text ar ising from email correspondences that contain repetitive threads sent back and forth, a form of writing that is not edited for grammatical accuracy and has no restrictions on syntax or typogra phic errors, verbiage that is acronym and abbreviation intensiv e, text outputs from commands issued on devices that do not c onform to standard writing, along with miscellaneous text such as internal notes and other correspondences. Within this la rge volume of text is the Intellectual Capital (IC) that describes the true problem, together with the troubleshooting steps taken to narrow it down to its true cause and the action plan that resolved it. Such refined information when extracted can be classified as knowledge and serves as a foundation for further applications.
 Figure 1 describes the phases in the knowledge extraction process used in the delivery of Smart Services. A customer device, when enabled for Smart Services, is configured to send messages to the back-end systems of Cisco to repor t the anomalies detected within the device. The messages that it sends are machine representations of a customer reporting the same problem verbally. So, for established pr oducts, there is a high probability that such problems had been solved by a CSE in the Global Service Center already. Figure 1. Knowledge Sources, Extraction and Application Smart Services Engineers (SSEs) have a role of creating a database of refined IC, ma pping messages from devices to recommendations that can potentially solve the problems. To create such a database, SSEs try to retrieve relevant SRs from the service request database to determine if these SRs already have a proven solution that can be used to create such a refined knowledge store. They also consu lt a number of other sources of IC such as internal and external documentation in this process. This database is subsequently us ed by the Smart Services back-end systems to provide a set of recommendations when an incoming message from a customer device reports an anomaly. SSEs need to manually read through the voluminous detail in the SRs to extract the IC. However, manual extraction of IC is time-consuming and difficult work for SSEs, because the IC they are looking for is hidden in the midst of massive amount of text of the SR, which would contain releva nt, irrelevant, and duplicated information. Hence, automati on or semi-automation of IC extraction process by data mini ng techniques is an effective shortcut to improve the produc tivity of the Smart Services engineers. Although data mining techniques have attracted a great deal of attention and tools are rapidly em erging, there are several gaps between these and business appli cation needs. Data mining tools can automatically search for patterns and discover correlations based on certain techniques and metr ics, but transforming data for analysis and interpreting the resu lts to actionable intelligence still require expertise and domain know ledge. For example, in text clustering analysis, tools could pr esent the word clusters from a technical report, but it is difficult to specify which cluster is the most pertinent to the theme of the report and which words can be used as keywords, because of the lack of contextual understanding. Clearly the role of domain knowledge and human expertise is indispensable in this process, but it is also quite challenging, if not impossible, to automate this combined information extraction process that captures expertise. We assert that a clear understa nding of this business process aspect is essential in designing a system to help choose the correct tools and techniques that can pr ovide the best results. Business analytic modules should go beyond data mining by combining domain knowledge, understanding that part of the text which corresponds to certain elements of the diagnostic business process, and addressing diverse us er needs. This process should end by presenting useful data, text patterns and structures to the expert in a manner that e xpedites understanding and helps interpretation. In this paper we present the Service Request Analyzer and Recommender (SRAR) for automation in the context of improving productivity of a service center. Our contributions in the context of text analysis for extremely complex and long documents are four-fold: (1) hypothesizing and validating the presence of a diagnostic business process that is inherent in an SR; (2) extracting the four components of this business process using a hierarchical classification scheme with features generated from documented domain knowledge a nd human expertise; (3) an efficient SR recommender based on language models (LMs), the components of the diagnostic business process, and domain knowledge; and (4) a discussion of our experience and the lessons learned in developing and depl oying the system, which could potentially create new areas for CIKM research. We have hypothesized and validated a structured ontology in the text, based on the knowledge diagnostic process, to enable effective information extraction. Based on this structured ontology we have designed the n ecessary system elements, and have either chosen or proposed appropriate algorithms or considered the factors that influence system performance, and introduced domain knowledge and expertise capture. The strong experimental results confirm that our hypothesis indeed discovers the latent structure from the heterogeneous text. Our results in this paper are app licable to network services and knowledge extraction from service requests. We anticipate that these results can be generalized further and that their use can be extended into many new domai ns such as semiconductor manufacturing, aerospace system operation, automotive system design, and network design diagnostic s. This expectation is based on prior related work by us in the semiconductor [9], aerospace [5], automotive [6], and networks [8] contexts. Case-Based Reasoning (CBR) is a problem solving paradigm that aims to solve a new problem by remembering a previous similar situation and by reusing relate d information and knowledge of that situation [7]. A general CBR cy cle consists of four processes: retrieval of the most similar case, reuse of the information in that case to solve the problem, revisi on of the proposed solution, and retainment of this experience for future problem solving [7]. In the service center scenario, CBR appears to have the same objective as the one we have stated in the paper. The processes of retrieval and reuse appear to be common between CBR and SRAR. In the retrieval process, assessi ng similarity between textually represented cases is an important research question. Burke et al. developed FAQ-Finder, a ques tion-answering system [20]. Conceptually, each of the question-answer pairs in FAQ files is treated as problem and solution in a CBR framework. Given a typed question as an input, it can retrieve textual answers. FAQ-Finder uses techniques that comb ine statistical and semantic knowledge to measure similarity. In addition to the vector space retrieval model, it includes a semantic definition of similarity between words, which is based on the concept hierarchy in WordNet. An evaluation demons trated that adding semantic information concerning the background knowledge led to performance improvements. Lenz and Burkhard took a different approach in FA LL Q, another question-answering system that compares textual cases through th e meanings of terms [21]. The program processed the free text to identify Information Entities (IEs) which are keywords. Domain knowledge is required to identify such task-specific IEs. The similarity assessment evaluates IE similarity usi ng a manually constructed domain ontology as well as a generic thesaurus. The SPIRE system [22] combines CBR and Information Retr ieval (IR). It first performs CBR analysis of the input problem on a small, manually prepared case corpus. The important cases in the results of this analysis are selected by the user and then are used as queries in the text-based IR system to retrieve results from a large, unprepared corpus with greater precision. In the reuse process, research has focused on developing methods to map textually expressed cases into structured representations. Weber et al. introduced a semi-automated approach to populate case templates from legal documen ts [23]. The method is domain-specific and requires knowledge e ngineering from domain experts to elicit which attributes are relevant in the target domain, the way to identify them, and the variations in which they might occur in the domain-specific texts. This know ledge is then used to feed template mining methods that extr act the feature values from text to characterize the cases. Although CBR and SRAR have some commonality, they differ in several aspects and details, which are summarized in Table 1. Without incorporating the methodol ogy of SRAR which we have developed, CBR would appear not to work well in this service center environment, because it doe s not explicitly address issues addressed by SRAR. The specific challenges are described in Section 3 and our methodology is elaborated in Section 4. In this section, we list the pain points that the SSEs endure in their daily work and the research challenges we experienced in developing the system around improving work efficiency and accuracy. Correspondingly, we attempt to solve these challenges when designing the system com ponents detailed in Section 4. Unlike the structured information from operational applications and transaction systems, unstruc tured text provides situational context around an event or set of events that aids answering the questions  X  X hat was the problem? X ,  X  X hy did it occur? X , and  X  X ow was it solved? X  essentially filling in the details of the  X  X roblem, cause, correction X  IC cycle. In our service center scenario, the majority of the SR content is unstructured text from which the SSEs have to find the  X  X hat X ,  X  X hy X , and  X  X ow X  to uncover hidden relationships, evaluate events, discover unforeseen patterns, and facilitate problem identification for rapid resolution. However, the most impor tant facts or concepts are not always readily available, but are veiled in the myriad of details and noise that accompany them. Extracting IC from the unstructured text is daunting. Si mply transplanting data mining and CBR algorithms may not provide satisfactory results. Because a service request records the textual information about the interaction between a customer and CSEs, it contains substantial duplicates and near-duplicates originating from the quotation of previous messages, the repetition of email signatures (name, title, telephone number, addr esses, and so on), and system notes. They account for 31% of the unstructured text in SRs, nearly all of which is useless and even of negative value for tracking and assessing similarity between SRs. Even after the duplicates are rem oved, there is still a significant amount of remaining text that the SSEs will not need, such as the email header (sender address, recipient address, subject, date, and so on), email signature, system notes, greeting and closing sentences, and self-introduction, as well as paragraphs such as  X  X f you need any assistance, please ... X  These pieces of content are irrelevant to IC, but take up to 39% of the SR on average. If this text were not removed, the system will treat it as part of the text that has IC. Detecting and removi ng the irrelevant text reduces the SR to only that text where the IC is located. However, this is not a simple task, because not all of this irrelevant text has obvious patterns to identify. From the perspective of ontology , the IC embedded in SRs is composed of many categories of information corresponding to  X  X hat X ,  X  X hy X , and  X  X ow X . Engin eers might pick different parts for use according to their needs in different situations. For example, some might be interested in just the  X  X hat X  and the  X  X ow X , while others might be in terested in the  X  X hy X  as well. These diverse demands are one r eason for us to hypothesize the diagnostic business process inherent in SRs. In brief, we believe that granular IC can help respond to or accommodate diverse needs in practice. The state-of-the-art systems for text categorization use learning algorithms in conjunction with th e bag-of-words features. The bag-of-words approach is inhere ntly limited, as it can only use pieces of information such as words that are explicitly mentioned in the documents. Specifically, this approach has no access to vital domain knowledge. Consequently , the data mining system is unable to function effectively when required to respond to the facts and terms not mentioned in th e training set. Hence, we have proposed a solution that captures features from (1) bag of words; (2) domain knowledge, by using t echnical documents describing the terms and acronyms being refe rred to in SRs in generating semantic equivalents; and (3) expertise, by using features suggested by the experts for each component of the diagnostic business process. A machine learning algorithm might fail in text mining for the following reasons. (1) Acronyms and abbreviations are widely used in domain-specific documents such as service requests. Since an acronym is at best defined when it first occurs, a machine learning algorithm might fail to associate the concept with its acronym in subsequent processing, because of the lack of domain knowledge, unless this is retained in memory. (2) Lack of a semantic processor, which incl udes a terminology dictionary or thesaurus. This results in the in ability of the system to identify semantically equivalent words and associated concept dependencies. In other words, the causality between problem and the underlying cause, the connec tion among problem components, and the relationship of different problems, are all left undetected. A similar problem applies to term inology. What is more, without domain knowledge and semantic understanding, the technical terms are treated as independent symbols by the machine, undifferentiated from other common wo rds. In this sense, mining functions only at the word level, which is the surface layer of SRs, instead of discovering hi dden dependencies among concepts. Thus, we are interested in iden tifying the linguistic, or actually, structural features, which are semantically useful in the business context of the SR, to categorize IC, and we are also concerned about the concept dependency embe dded in SRs. To address the above issues of incorporati ng indispensable domain knowledge, we incorporate the Cisco Intern etworking Terms and Acronyms Dictionary (ITAD) to incorporate networking domain knowledge in our project. ITAD not only expands the acronym, but also gives a description of the acronym or te rm, which helps the data mining algorithm understand its semantic meaning, function and related concepts. An example of an entry from ITAD is: ARP: Address Resolution Protocol. In ternet protocol used to map an IP address to a MAC address. Defined in RFC 826. Compare with RARP. See also proxy ARP. To create the refined IC repository, an SSE will need to read several SRs to determine all the possible solutions to the current problem. Thus, these SRs should address the same or similar problems but have different troubl eshooting steps and solutions. It might be suboptimal to get the SSE to read the SRs in the order given by the search engine. Even highly ranked SRs are not necessarily guaranteed to satisfy the user X  X  need. The reason is that SSEs would like the search engine to recommend SRs that are highly similar to the curre nt problem in the problem description but with different st eps in the troubleshooting and the solution steps. However, a typical search engine matches the search query with all the text in an SR without the required discrimination and is consequen tially ineffective. We have developed an SR recommender to cater to this requirement of matching based on more granul ated and segmented text. We have designed the system architecture and elements as shown in Figure 2 with the objective of addressing the challenges we have described above to improve the work efficiency. This design is based on the identification of the diagnostic business process inherent in SRs. The servi ce request containing both the structured and unstructured conten t is retrieved in XML format from the database. The unstructu red text containing IC is extracted from the XML file, and the contents are tidied up in the Preprocessor (Section 4.1). The deduplicated text is then input into a Hierarchical Classifier (Section 4.2), which removes the irrelevant information and extr acts the relevant information during the first step. In the second step, the system categorizes the paragraphs by labels in accordance with the type of content in that paragraph. We have enriched th e features with domain knowledge and human expertise, to achieve high classification accuracy. To expand the bag-of-words method, we have incorporated the ITAD to use domain knowledge and also combined human expertise to incorporate the insights of the engineers. An enhanced Feature Generator is described in S ection 4.3. An LM-based SR Recommender (Section 4.4) has been proposed for situations in which the engineers are not satisfi ed with the IC extracted from a single SR alone, in which case the recommender can suggest other SRs with a similar context. When an SR is retrieved in XML format, the system returns the entire SR in which the content is described through tag types. Structured data has unique tag ty pes. However, all unstructured text detailing the correspondence between the customer and the engineer that includes relevant, irrelevant, and duplicates, is returned together with the same tag type. Such storage mandates the need for a CIKM approach to sift through the content and extract only that which has IC. At the first step, we have used a preprocessor to handle XML files, extract the unstructured text, separate text into paragraphs, and remove duplicates. We compare two approaches for deduplication. First we evaluated the hash algorithm. It can detect identical contents, but it was very brittle, as potentially any change in formatting, word order, or slight variations in content produced a different value. In fact  X  X lmost X  identical content with a few characters added or removed between two pieces of text also constitutes a duplicate for humans. This near-duplicate is also part of the content in an SR. For example, most near-dupli cates in the email quotation are not identical to the original text; line duplication indicated by the email reply symbol &gt; is a good example. The hash method was unable to detect such duplicates . This inability necessitated the use of the more flexible resemblance approach [10]. Resemblance measures the fuzzy similarity between two documents. If a threshold is exceeded, the documen ts are considered duplicate. The objective of SRAR is to present the desired IC in a compact, easy-to-understand fashion to facilitate the work of an SSE. After deduplication, the IC is still not evident as it is hidden in a massive amount of heterogeneous, irrelevant information. This irrelevant information cannot be reduced as easily and accurately as using the deduplication as it l acks universal patterns. What is more, even if the irrelevant information could be removed completely, the IC, presented as a set of paragraphs, is still not easy for the SSEs to digest and absorb. To build a system that will be the most helpful to the engin eers, we investigated the routine elements of their work and analyzed the ontology of the networking domain. The finding was that the engineers usually had a clear purpose in mind when reading the SR, and they probably needed one or more specific kinds of IC from the functional perspective, such as  X  X hat was the problem? X ,  X  X hy did it occur? X , or  X  X ow was it solved? X  Hence we concluded that if the IC was granulated into several functional categories that are aligned with the diagnostic busine ss process, this process could assist engineers to obtain the information they needed more effectively and rapidly. To handl e the heterogeneous contents of SRs and the diverse needs of engineers, we adopted a supervised learning algorithm to analyze IC and proposed a hierarchical classifier to improve prediction accuracy. Our most significant contribution in the current paper is in exploiting this diagnostic busine ss process structure to achieve effective information extraction. The hierarchical classifier is one possible method, but there are ma ny possible methods, which we describe in more detail under less ons learned and further research. Our contribution is in identifying hierarchical classes which are meaningful for the task and result in enhanced performance. We confirmed the hierarchical structure which we had hypothesized for the SRs. When this structure was utilized, the classification problem could be d ecomposed into a set of smaller problems and with reduced noise. Our classifier first learns to distinguish between relevant and i rrelevant paragraphs at the top level; then lower-level IC distinc tions are learned only within the relevant paragraphs. Currently , we have defined four IC categories in terms of the engin eers X  need: Problem Description, Troubleshooting, Inference, and Action Plan. Each of these subclasses can be classified more efficiently and more accurately. Koller and Sahami [15] first researched the hierarchical text classification on Reuters-22173. They found the advantage of hierarchical models over the fl at models using na X ve Bayes. Dumais and Chen [19] applied SVM to hierarchically classify text into large structures and reported quite a high efficiency. McCallum et al. [18] used na X ve Bayes combining shrinkage for parameter estimation and reduced the error by up to 29% over traditional flat classifiers. Another reason for us to prefer the hierarchical classifier comes from its ability to resist the class bias problem [13]. For highly skewed data, the class distribution is biased toward the majority in the sense that most classifiers would lean to predict the major class in order to obtain a higher overall accuracy. In service requests, the irrelevant inform ation accounts for between 3 and 14 times as much as one IC category. Flat classifiers will misclassify most IC as irrelevant in this case, which is counter to our objective of predicting the minor classes to achieve a low false negative rate while maintaining overall accuracy [14]. Feature generation is essential to text categorization. The widely used bag-of-words approach is limited by the fact that it is a limited representation of the information in sentences. Quite a few attempts have been made to deviate from the bag-of-words paradigm. In particular, repres entations based on phrases, named entities, and term clustering, have been explored. However, none of these techniques has overcome the underlying problem in enterprise data mining X  X ack of domain knowledge. For example,  X  X ddress X  and  X  X rotocol X  are im portant terms in networking and are very effective indicators of IC. However,  X  X RP X  is deemed a trivial word, because, without domain knowledge, the machine can by no means understand its semantic meaning X  X he acronym of  X  X ddress Resolution Protocol X . To better represent textual information in feature space, we built a feature generator using domain-specific ITAD to enrich the bag of words with new, more informative features. Besides the expansion of the acrony m, ITAD also briefly explains the meaning or function of a technical term, in which other related terms are introduced. We argue that this dependency could be used to generate stronger features for classification by using some of these terms. Example terms such as  X  X nternet X ,  X  X P X , and  X  X AC X  mentioned in the  X  X RP X  entry, are all good indicators of the IC, and further increase the probability of a paragraph that includes the term  X  X RP X  belonging to IC as well as potentially to a specific category of IC. Human expertise provides insights, observations and experience. Since experts have a very strong learning capability and high intelligence, the features genera ted from expertise are usually effective. They are beneficial complements to the machine-generated features. In SRAR, we propose a feature generator to enhance the representation of the textual information in feature space. Domain knowledge and human expertise ha ve been combined with the bag-of-words approach to ach ieve a better performance. We defined one irrelevant class and four IC classes to characterize the components of the diagnostic business process (see Section 5 for definition) and requested experts to provide indicative patterns or features they observed when reading SRs. Table 2 shows some exampl es and the motivations. Engineers sometimes want to r ead several SRs discussing the same or similar problem so that they can understand the possible ways that such a problem had been addressed and solved in the past. In SRAR we propose an SR recommender to address this need. This recommender performs matching using the components of the diagnostic busine ss process. The similarity of two SRs is measured by evaluating the similarity between the corresponding structured fields (title, technology, and subtechnology) and granulated IC categories (see Figure 3). High similarity of structured fields and problem description indicates that both SRs probably discuss si milar problems involving similar products and techniques. Meanwhile , if engineers want to learn about different solutions, the recommender could measure the similarity of troubleshooting and action plan categories. A low score implies that these two SRs might have different solutions. Hence, in these problem contex ts, a method to measure text similarity effectively is critically needed. We now consider developing a mode l to determine the similarity of two SRs in the current context. Again, we incorporated the domain knowledge. Acronyms and terms are expanded by the original names and explanations in ITAD so as to increase overlapping words. Table 3 shows IC excerpts from two SRs that address a similar problem with the expansion from ITAD in brackets. The overlapping word s between unexpanded excerpts are marked in blue italics, while those introduced by ITAD are in red boldface. Obviously, the similarity can be substantially enhanced if the domain knowledge is applied. Representing documents in vector space and computing their Cosine Similarity (CS) is one method widely employed to measure text similarity. Langua ge models [16][17] in the literature are known to have better performance than the CS models. Originally, the language model was proposed to measure the relevance between a query and a document. Here, we have modified it as an approach to measure SR similarity. Given two pieces of text t 1 , t 2 , and the vocabulary w defined below and an intuitive explanation for them follows a little further below. where estimate of the probability, ( , ) ij cw t is the number of occurrences of w i in t j , and (,) (|) estimation of w i with respect to the corpus C [16]. p ( t the probability of generating t 1 from the same model that has generated t 2 , and vice versa for p ( t 2 | t 1 ). This probability represents the text relevance. Since both pieces of text are observed and a similarity metric should have th e symmetry property, we define the similarity metric between t 1 and t 2 by a mutual generating probability as In the approach here, for a given SR, a ranked list of related SRs is recommended to engineers ba sed on the mutual generating probability defined above, where the SR with the highest value of Sim LM is awarded the highest recommendation. We investigated the effectiveness of SRAR in its ability to help SSEs capture IC quickly and accurately. We first measured the improvement of productivity by using SRAR in the routine of an SSE. We then evaluated the SR Analyzer and SR Recommender separately. We have demonstrated the advantages of the hierarchical classifier, the feature generator and the LM-based recommender over the benchmark approaches. In all our experiments, a paragraph from an SR was treated as the atomic unit to present IC. Each paragraph is classified as belonging to one of four categories, depending on the stage of the problem solving process. These four categories are: The data set included 52 service requests that the CSEs created by solving customer problems. On average each SR was 12 pages long, and some SRs even extende d up to 56 pages with a very large number of  X  X oisy X  phrases and terms. After preprocessing and deduplication, a total of 2868 paragraphs across 52 SRs were manually labeled by human exper tise. Table 4 shows the skewed distribution of paragraph cla sses across both relevant and irrelevant classes, as a percentage of the total. We evaluated the SR Analyzer using this data set as a test bed. Moreover, multiple SRs in the data set were dealing with a similar problem, and consequently we were able to evaluate the ability of the SR Recommender to identify those SRs sharing the same or similar problem statement. Since SRAR was designed to improve the engineers X  work efficiency, we compared the time (minutes) spent by SSEs in reading the SR before using SRAR with the time after using it, namely reading the original SR versus reading the categorized IC. We also observed the change in the length of the SR during IC analysis, and used it as an estimate of the resulting workload placed on the SSEs. We chose nine device messages that had to be mapped to recommendations a nd measured the time required by the SSEs to create them. In the initial stages of their work on a given problem, the SSEs searched for the existing SRs addressing a similar problem and read them quickly to assess the relevance. If an SR was relevant, the SSEs would read it thoroughly, try and understand the cause and the resolution, and consider the potential of the solution described in this SR as a candidate for the recommendation. If an SR was i rrelevant or did not provide enough information, the SSEs woul d continue looking for related SRs. We measured the time to assess the relevance of each SR and the total time to create the mapping to a solution respectively. Figure 4 shows the comparison of the time used to assess the relevance for each of the nine given problems. On average, the time to assess the relevance of previous SRs to the current problem is reduced by 60%, from 27 minutes per SR to 11 minutes. This reduction is attributable to the deduplication and the IC categorization steps of our approach. The SSEs were not distracted by the large volumes of irrelevant and noisy information and were able to flexibly work with any SR and the associated IC they felt was useful in the context of their work. Figure 5 shows the comparison of the time used to complete the mapping. Besides the time to assess relevance, it also includes the time to understand IC and the time to read multiple SRs and other sources of IC as needed. On average, SSEs spent 97 minutes to create recommendations to a device message by reading original SRs, while it took only 67 minutes to do if they used SRAR. The productivity was improved by 45%. Extra time was saved in this process because the SSEs usually read SRs back and forth, and the explicit representation of IC helps them with easy tracking. Figure 6 displays the average number of SR pages given by SRAR at the points of removing duplicates and removing irrelevant information. Hash dedup lication was able to remove the identical duplicates and reduce the contents by 50%, but it failed to identify the near-duplicates. The resemblance method took a more flexible notion of duplica tion defined by similarity and reduced the SR contents by 76% , where the removed contents included the duplicate and near-duplicate information. The text remaining after the resemblance based deduplication still contained a great deal of content irrelevant to IC. We used the hierarchical classifier we developed to finally filter out almost all undesired (because of its noisy and non-informative nature) information and locate the IC portion of the total text which was only 10.6% of the original SR. Accurate classification presents SSEs with only that information which describes the essence of the service request as well as prevents IC from being classifi ed as irrelevant and hence discarded. Porter stemming and stop-words removal were applied before building an inverted inde x. In our feature generator, described in Section 4.3, the disc riminative words were chosen by the information-gain-based feature selection method [12]. Considering the fact that nearly all words were useful features for text categorization [11] and the capability of na X ve Bayes to handle high dimension data, we selected 12,000 word features expecting to improve generalization accuracy and to avoid overfitting. Another 36 features that we used were suggested by experts as promising, based on their observation and work experience. We adopted Weka 1 as the learning tool, employed the na X ve Bayes classifier as our choice for its speed and good performance on text, used a 10-fold cross-validation to fit the model and evaluated its predicti ons with the labels given by experts. We adopted a  X 1-vs-a ll X  approach to handle multiclass classifications. We measured the performance of classification schemes varying in structures, f eatures, and classifiers, including Hierarchical classification (H) versus Flat classification (F), Feature Generator (FG) versus Bag-of-Words (BW), and Na X ve Bayes (NB) versus Logistic Re gression (LR). Table 5 shows the empirical results on precision, recall and F-1 measure. The  X  X verall X  is the weighted average on all five categories, while the  X  X C Overall X  is the weighted average on the four IC categories. Our first finding was that all hierarchical classifiers performed much better than the flat classifiers on every metric. In particular, the H-NB-FG achieved strong results. This is attributable to the hierarchical framework which reduced much of the irrelevant information at the top level and brought two benefits. Firstly, the class bias problem was avoided (Section 4.2). This problem is quite obvious for flat classifiers, where the minor classes  X  X  X  and  X  X  X  were poorly classified, much worse than the major class  X  X  X , and even not comparable with  X  X  X  or  X  X  X . On the contrary, all IC categories in hierarchical schemes were classified very accurately. Secondly, the irrelevant information that negatively affected the IC classification at the bottom level was obviously reduced. The second finding was that domain knowledge and human expertise indeed helped IC analysis. From Table 5, H-NB-FG comprehensively outperformed H-NB-BW in all categories by F-measure. This demonstrates that our Feature Generator is superior to the traditional bag-of-words. ITAD was able to associate more words characterizing a category by expanding terminologies and thus enhanced the discriminative power of the extracted feature. Experts used their intelligence to propose insightful patterns and features. In fact, the top three effective features from information gain were all from the experts. They are the relative position of a paragraph in SR, the length of a paragraph, and the number of times that the word  X  X roblem X  appeared in one paragraph. The third finding was that logistic regression worked better in flat framework (F-NB-FG vs. F-LR-FG), but na X ve Bayes dominated logistic regression in a hierarchical framework (H-NB-BW vs. H-LR-BW). This resulted from the re moval of irrelevant content, which reduces the dependent, noisy features to alleviate the conditional independence assu mption of na X ve Bayes. The SR recommender helps to identify the SRs addressing a problem similar to that being worked on by the SSEs. We designed the experiment by ranki ng the similarity score of two SRs given by the recommender. Because we were already aware http://www.cs.waikato.ac.nz/ml/weka/ of the SRs that were addressing th e same or similar problem, such SR pairs were expected to be ra nked higher in the list. Since we did not know whether all engineer s require the related SRs to contain the similar problem descri ption but different solutions, the experiment was designed for fi nding the related SRs without comparing the solutions. Hence only the fields and problem description paragraphs were used he re. We were also interested in whether SR Analyzer could he lp enhance recommendation, so another experiment was performed on the original SR for comparison. We adopted the Lemur toolkit 2 to build language models, and evaluated the propos ed LM-based recommender and the cosine similarity recommender by precision, recall and mean reciprocal rank (MRR). There are 1326 pairs generated by the 52 SRs, out of which 35 are related. Figure 7 shows the precision and recall of the rank lists given by the recommenders using different combinations (that is CS-SR, CS-IC, CS-IC-ITAD, LM-SR, LM-IC and LM-IC-ITAD). The corresponding MRR values are 2.42, 3.36, 3.67, 2.45, 3.72 and 3.90, respectively. We had thr ee findings: (1) both recommenders achieved much better results when categorized IC was used. This demonstrated that SR Analyzer indeed improved the accuracy of the SR recommendation and, more importantly, validated the inherent diagnostic business pro cess which we had hypothesized as crucial to the performance enhancement. (2) LM-based recommender outperformed CS recommender. All of the top 15 ranked pairs generated by LM-IC-IT AD were related pairs, while only 13 were related as indicat ed by CS-IC-ITAD (14 by LM-IC versus 10 by CS-IC). The good performance of LM-based recommender results from that la nguage models with Dirichlet smoothing represented the text be tter for similarity measurement than the vector space model. (3) Using domain-specific reference ITAD further improves retrieval results, because ITAD disambiguated the word sense a nd introduced more terms related http://www.lemurproject.org/ to the problem so as to increase the words overlap between semantically similar documents. Many enterprises are building new applications by utilizing existing textual information to offer value-added services. The problem of rapidly and accurately presenting meaningful content to users is an essential part of this process. In this paper, we have described the Service Request Analyzer and Recommender which we designed and deployed for the Smart Services team in Cisco, together with the elements addressing various practical challenges. This system is de signed based on successfully mining the diagnostic business process inhe rent in data. The preprocessor filters out a large amount of duplicate and near-duplicate contents. The hierarchical classifier further removes the irrelevant information, identifies the granul ated business process types, and delivers the categorized IC with high accuracy to satisfy the diverse demands from different us ers. Its strong performance is attributable to both the hierarchical classification structure and the feature generator incorporating domain knowledge and human expertise, both of which are the crucial elements to solve specific problems in an enterprise e nvironment. Besides, the SR recommender helps ensure that th e engineers retrieve relevant SRs that have a high probability of providing a solution to the current problem. The synergy of SR Analyzer and SR Recommender improves the productivity in a service center environment. This framework can also be generalized to other domains and business functions that utilize textual data to improve service efficiency and productivity. The primary learning has been the validation of the hypothesis that modeling the diagnostic process using domain knowledge enables greatly enhanced performance. The next learning was that a service request could in actuality contain several problems, root causes, and associated solutions, rather than the single purported problem reported by the customer and its associated solution. In complex service requests, until all root causes are identified and solved, the original problem is not solved. In this case, the difficulty in problem resolution manifests itself through the presence of multiple inferences a nd action plans in the body of the SR. While our approach works well in instances when only one problem and the associated problem resolution steps are present in an SR, the model breaks down when more than one problem and all the associated problem resolution steps are present in the same SR. This is because any algorithm based on the four steps for a single problem would be misled by a presence of eight or more steps from more than one probl em resolution process. When classification results were review ed, we found that the classifiers usually performed worse on the SRs that addressed multiple subproblems within a main problem . Intermediate inferences and action plans were misclassified as troubleshooting steps. These occurred as a result of heavily weighting the feature of paragraph position as the main indicator by training on the data set where most of SRs addressed only one problem. This indicates the need to integrate topic detection into the algorithmic process. Resemblance deduplication performed better than the hash method in experiments, but we found that it removed a small amount of critical text that was important. This mainly occurs in the context of command-line outputs from devices that contain highly similar words and symbol s between two consecutive lines of a device configuration or a command output. Setting a high similarity threshold alleviates this problem, but also affects the performance. This requires us to design a component to detect those outputs as a cluster, where we preserve the contents of a cluster but remove duplicat e instances of clusters. SRAR is a project in progress. We have finished the small-scale deployment on SR Analyzer and will begin a small-scale deployment on SR Recommender in the next phase. If every component performs well, we plan to incorporate the SRAR in the large-scale deployment. Our long range objective is to build a system to retrieve documents and extract parts relevant to a new request. This is a classic IR pr oblem. However, the variation in our context is very challenging and novel for reasons discussed earlier. We have analyzed the data in developing an empirically driven approach. This approach serves as the foundation to both solve current research problems and enable solutions to the next generation of research problem. Bu ilding the work described here, we plan to pursue the following research agenda: (1) Extraction of metadata to characterize an SR, including topic (2) SR recommender systems that incorporate topic models, user (3) A hierarchical net to represent more detailed networking [1] Park, Y. and Gates, S. 2009. Towards Real-Time [2] Bhattacharya, I., Godbole, S., and Gupta, A. 2009. Enabling [3] Forman, G., Kirshenbaum, E ., and Suermondt, J. 2006. [4] Akella, R., Xu, Z., Barajas, J., and Caballero, K. 2009. [5] Srivastava, A., Akella, R., Diev, V., Kumaresan, S., [6] Voit, J., Akella, R., Kishore, R. 2003. Triggered Learning [7] Aamodt, A. and Plaza, E. 1994. Case-Based Reasoning: [8] Xu, Z. and Akella, R. 2008. A Bayesian Logistic Regression [9] Shindo, W., Wang, E., Akella, R., Strojwas, A. J. 1999. [10] Broder, A., Glassman, S., Ma nasse M., and Zweig, G. 1997 [11] Joachims, T. 1998. Text Categor ization with Support Vector [12] Yang, Y. and Pedersen, J. 1997. A Comparative Study on [13] Batista, G., Prati, R., and Monard, M. 2004. A study of the [14] Weiss, G. M. 2004. Mining with rarity: a unifying [15] Koller, D. and Sahami, M. 1997. Hierarchically classifying [16] Zhai, C. and Lafferty, J. 2004. A study of smoothing [17] Liu, X. and Croft, W. B. 2002. Passage retrieval based on [18] McCallum, A. K. , Rosenfeld, R., Mitchell, T. M., and Ng, [19] Dumais, S. and Chen, H. 2000. Hierarchical classification of [20] Burke, R., Hammond, K., Ku lyukin, V., Lytinen, S., [21] Lenz, M. and Burkhard, H. 1997. CBR for Document [22] Risslandand, E. and Daniels, J. 1996. The synergistic [23] Weber, R., Martins, A., and Ba rcia, R. 1998. On legal texts 
