 Prasad M. Deshpande  X  Salil Joshi  X  Prateek Dewan  X  Karin Murthy  X  Mukesh Mohania  X  Sheshnarayan Agrawal Abstract In today X  X  enterprise world, information about business entities such as a cus-tomer X  X  or patient X  X  name, address, and social security number is often present in both rela-tional databases as well as content repositories. Information about such business entities is generally well protected in databases by well-defined and fine-grained access control. How-ever, current document retrieval systems do not provide user-specific, fine-grained redaction ofdocumentstopreventleakageofinformationaboutbusinessentitiesfromdocuments.Leav-ing companies with only two choices: either providing complete access to a document, risking potential information leakage, or prohibiting access to the document altogether, accepting potentially negative impact on business processes. In this paper, we present ZoRRo, an add-on for document retrieval systems to dynamically redact sensitive information of business entities referenced in a document based on access control defined for the entities. ZoRRo exploits database systems X  fine-grained, label-based access-control mechanism to identify and redact sensitive information from unstructured text, based on the access privileges of the user viewing it. To make on-the-fly redaction feasible, ZoRRo exploits the concept of k -safety in combination with Lucene-based indexing and scoring. We demonstrate the efficiency and effectiveness of ZoRRo through a detailed experimental study.
 Keywords Sanitization  X  Redaction  X  Security and protection 1 Introduction A large percentage of digital data are unstructured in nature, and these data are growing much more rapidly than structured data [ 1 ]. Digital data in an enterprise encompass sensitive busi-ness entities and other highly confidential information, whose disclosure to everyone might not be desirable. Protecting such sensitive data from unauthorized access becomes more and more important and is difficult to scale. While there exist well-formulated and highly efficient techniques such as role-based access control policies to protect structured information, tech-niques for protecting information in unstructured documents are still primitive. Most current document retrieval systems feature access privileges of the form Read Only, Read/Write, No Access . There is no provision to define what content (or part of content) should be acces-sible to different users. In addition, even the basic access control to documents is often not based on the actual document content, leading to inadvertent leakage of sensitive information contained in a document.

Consider ZoRRoCorp, a large organization with a few million customers. ZoRRoCorp X  X  centralserverstoresdetailedinformationabouteachcustomer.Recently,ZoRRoCorpdecided to outsource customer support services to CallAll Global, a third-party service provider. Since then, ZoRRoCorp X  X  customer support queries are directly forwarded to CallAll Global. In order to provide appropriate answers to ZoRRoCorp X  X  customers, CallAll Global requires partial access to customer documents stored in ZoRRoCorp X  X  central server. Note that partial access is sufficient as the amount of information stored at ZoRRoCorp far exceeds what is needed for CallAll Global to provide basic customer support services. Also notice that customers writing a query to support@ZoRRoCorp.com may be unaware that this query is handled by a third party. As a consequence, customers might include confidential information in their support queries that is not meant to be viewed by a third party.

Even within CallAll Global, a support query may be handled by multiple people with different information needs. An employee at a managerial level might require access to more content than a call center or a customer support executive, hence demanding fine-grained access control even within CallAll Global itself. A solution to all these requirements is essential for CallAll Global to conform with the regulatory compliance of ZoRRoCorp.
Similarly, in the medical domain, a patient X  X  medical file contains potentially sensitive information such as medication, symptoms, diseases diagnosed, along with certain personal financial information or contact information. While access to financial information should be prohibited for a doctor, a hospital administrator should not have access to detailed medical information. Figure 1 illustrates how the same document should be redacted differently based on the person viewing the document.

Both scenarios highlight that information present in unstructured format is vulnerable to leakage and needs a document retrieval system that supports fine-grained access control. Allowing various users to view specific content based on their respective roles or privileges helps companies meet important industry compliance requirements [ 2 ].

Prior work in the area of document sanitization [ 3  X  6 ] has mainly focused on redacting documents based on static dictionaries and patterns defined manually. These techniques all address general document sanitization and do not consider user-specific access-control information from structured databases.

In contrast, we present ZoRRo, an add-on to document retrieval systems, specifically designed to control the amount of sensitive information exposed to a user based on her respective privileges and access level in the enterprise. ZoRRo dynamically determines the information that needs to be redacted for a user, each time a particular document is accessed by the user. The ZoRRo framework exploits the well-formulated and fine-grained access-control mechanism present in database systems to implement an access-control solution for unstructured documents. ZoRRo extracts the access privileges of a user from a database system and maps the same privileges onto unstructured information. Essentially, whatever information a user is not allowed to view in a database is hidden from her in the document, too.
The key idea of ZoRRo is to identify the set of entities (rows) and attributes (columns) that are mentioned in the document and apply the policies corresponding to those rows and columns in a relational database. Identifying specific entities being mentioned in a document consists of two steps X  X dentifying the attribute values mentioned in the document and finding the best matching entities by comparing with the structured data.

ZoRRo indexes the entities using Lucene and exploits its scoring function to find the matching entities. Note that there might be a large number of entities sharing the same values occurring in a document. For example, a document may mention the city  X  X hicago, X  potentially matching thousands of Chicago-based customer entities in the database. Checking the access-control information for each such entity in the database is prohibitively expensive. To make the solution feasible, ZoRRo exploits the concept of k -safety [ 7 ] to identify document content that is not k -safe. Only the entities corresponding to this content need to be checked, and by definition, the number of such entities for any such content cannot be more than k . We describe this aspect in more detail in Sect. 4 .

In summary, the major contributions of ZoRRo are:  X  A first attempt, to the best of our knowledge, to apply existing and well-trusted access- X  A practical approach to prevent information leakage from documents that provides a  X  An efficient method to identify sensitive information in documents that makes on-the-fly  X  A detailed experimental study demonstrating the efficiency and effectiveness of ZoRRo. The remainder of the paper is structured as follows: Section 2 covers related work and introduceslabel-basedaccesscontrol.Section 3 setsthecontextbycharacterizinginformation leakage, spelling out security model assumptions, and providing a problem formulation for identifying sensitive information. Section 4 describes how the concept of k -safety can be used to improve the efficiency of redaction. Section 5 describes the architecture of the ZoRRo system and presents the algorithm for identifying sensitive information. Section 6 describes the experimental setup and results. Finally, we conclude and discuss some future work in Sect. 7 . 2 Preliminaries 2.1 Related work Priorworkintheareaofdocumentsanitization[ 5 ]hasmainlyfocusedonredactingdocuments based on static dictionaries and manually defined patterns. These algorithms were exten-sively applied in the medical domain [ 3 , 6 ] to identify sensitive information such as names, social security numbers, and health-related information. In addition, semantic-preserving text-anonymization techniques using t -plausibility have been proposed in [ 4 ]. Instead of redacting, sensitive terms are replaced with more general but semantically related terms to preserve the information content in the document. All of these techniques address general document sanitization and do not consider user-specific access-control information from structured databases.

A technique by Bettini et al. [ 8 ] aims at restricting the outgoing information flow at the boundary of information systems. It employs a semi-supervised learning system which restricts the users from accessing certain documents based on the access-control rules defined in structured data stores. Similar approaches revolving around the concept of  X  X nformation release control X  are discussed in [ 9 ]and[ 10 ] among others. The applicability of these tech-niques is limited given the manual intervention required and the fact that they do not support term-level redaction within documents.

Some recent work aims at protecting the information in composite documents such as java archive files and Mime HTWeb archives by encrypting the documents within the composite with role-based keys [ 11 , 12 ]. A public-key encryption mechanism is employed for this pur-pose. Again, the protection takes place at a document level not at the level of terms within a document.

Attribute-level encryption proposed by Sahai and Waters [ 13 ] grants individual users X  keys that permit the decryption of a keyword if and only if their key matches certain attributes specified in the ciphertext X  X  creation. The technique has been successfully used for appli-cations, such as protecting electronic medical records [ 14 ] and cloud storage services [ 15 ]. However, the current implementation primarily employs rule and policy-based protection, instead of harnessing the existence of a structured dataset with defined access control.
A related technique semiautomatically redacts text documents using machine-learning techniques and standard NLP algorithms [ 16 ]. The goal is to redact documents in such a way that correct classification of the document becomes difficult. The authors propose redaction approaches along the lines of k -anonymity introduced by Sweeney [ 17 ]. They present a semiautomated system which uses human intervention to enhance the set of keywords to be redacted. Again, human intervention limits the approach to smaller document collections.
Cumby et al. [ 18 ] introduced a privacy framework for protecting sensitive information in text data, while preserving known utility information. The authors present a framework called Text Inference Control which gives the user fine-grained control over the level of privacy needed for sensitive concepts present in the data. The authors also considered a variant of k -anonymity they termed k -confusability. However, the approach does not consider user-specific access control and redaction.

An extension of k -anonymity includes k -safety proposed by ERASE [ 7 ]. ERASE presents a framework for redacting text for protected entities based on k -safety and focuses on finding a minimal set of words to be redacted to make a document k -safe. This problem is shown to be NP-hard, and various approximation algorithms are presented. Using this method, the time required to redact a document is in the order of a few seconds and thus not suitable for an online setting where documents have to be redacted dynamically. Also, in the experimental setup, the number of entities was small, in the order of a few thousands. In ZoRRo, we use the concept of k -safety. However, rather than focusing on finding a minimal set of words to redact, which is very expensive, we use k -safety to reduce the number of entities whose access control needs to be checked in the database. This makes ZoRRo highly scalable and enables it to handle large entity databases.

In contrast to k -anonymity, differential privacy [ 19 ] allows formal reasoning about what an adversary can learn from released data. Privacy-preserving query answering [ 20 ]and data mining with privacy [ 21 ] are recent advances in differential privacy. However, while differential privacy provides a stronger notion of privacy than k -safety, we show that the latter can be used in online settings providing interactive response times.

Current industrial solutions for data redaction in unstructured text work on statically defined policies. For example, the InfoSphere Guardium Data Redactor (earlier known as Optim Data Privacy Provider [ 22 ]) redacts documents based on exclusive policies defined as part of a solution. Sensitive data such as credit card or social security numbers are replaced with a fake value retrieved from a static dictionary or simply blacked out.

In order to identify sensitive information, ZoRRo needs to identify the entities whose infor-mation is contained in the documents. This task is related to techniques for linking structured data with unstructured text. For example, [ 23 ] creates a bridge for integrating information from semi-structured sources with existing large database with multi-relational entities. Other systems include EROCS [ 24 ] that efficiently links text documents with structured data and EUTC [ 25 ] that identifies entity-related information from documents and links documents to entities in a master data management system. ZoRRo X  X  technique for identifying entities is similar to the one used in EUTC [ 25 ]. 2.2 Label-based access control Most database systems come equipped with a powerful Label-Based Access Control (LBAC) for defining access levels for various users. LBAC can be customized to suit various security requirements of an organization. It provides fine-grained access control on rows, columns, and tables. Both row-and column-level access control can be used simultaneously as well as separately. LBAC uses security labels defined on objects to specify what access privilege is needed to access the object X  X  information. If a user wants to access a particular object, the security label of the object needs to match her access labels, otherwise access is denied.
As a simple example of LBAC, consider a policy with labels of type  X  X IGH, X   X  X EDIUM, X  and  X  X OW, X  denoting high, medium, and low levels of sensitivity. Each row in a table is associated with a label, as-is each user. The access rule can specify that a user with label  X  X EDIUM X  should be allowed to access rows with labels  X  X EDIUM X  and  X  X OW, X  whereas a user with label  X  X OW X  can only access rows with label  X  X OW. X 
For ZoRRo, we assume that the structured data in the database are protected using the standard implementation of LBAC in IBM X  X  DB2 database [ 26 ]. 3 Problem formulation 3.1 Types of information leakage Consider a structured database table T with records shown in Table 1 .Let u be a user trying to access document d showninTable 2 . The document contains information about the entity e with name  X  X anish P. X  If u does not have access to entity e 2 but is able to access document d , he can gain information that he is not authorized to have. The information leakage is of two types:  X  Leaking sensitive attribute values In general, the attributes of an entity can be classified into two types: identifying attributes such as Name and City which are public knowledge and sensitive attributes such as Salary whose access is restricted to users who are explicitly granted access to that information. For example, document d leaks the value of the sensitive attribute Salary for entity e 2 ,ifuser u is allowed unrestricted access to document d .For structured data sources approaches for releasing sensitive data while preserving privacy [ 30 ].  X  Leaking non-attribute information In addition to entity information also covered by the attributes in the structured data source, each document may have additional sensitive information about an entity. Such additional information can be very open ended and is difficult to characterize. For example, document d may leak the information that e 2 visited
Delhi in May if user u is allowed unrestricted access to document d and e 2  X  X  visit to Delhi is not public knowledge. In general, additional information can be sensitive and its linkage to a specific entity may also need to be protected.
 3.2 Security model essentials Preventing information leakage from documents is difficult since documents have no specific structure and can contain any kind of information. To make the problem tractable, we make a few simplifying assumptions. Note that ZoRRo still works even if those assumptions do not hold, but its precision may drop (that is, ZoRRo may no longer prevent all information leakage).  X  We assume that all attribute values mentioned in a document that, alone or in some com-bination, identify an entity, are also present in the entity X  X  information in the structured data source. (In privacy literature such a set of attribute values is called a quasi-identifier [ 17 , 28 ].) For example, the terms  X  X anish P X  and  X  X angalore X  in document d reveal that the document talks about entity e 2 . In some cases, it is possible that a document contains additional identifying attributes not covered by the structured data source. For example, if the document contained  X  X anish P, Distinguished Engineer, visited Delhi in May. X , a reader might still identify e 2 , even though the data in T are insufficient to identify that the entity mentioned is e 2 .  X  We assume that if a user has access to a particular entity in the structured data (row-level access in LBAC), she is allowed to access any additional information about the entity that may be present in the document. In general, the policies regarding access to such additional information may depend on the type of additional information. However, characterizing and classifying such additional information is challenging.  X  We assume that a user does not know any sensitive information about entities he is not allowed to access and has no additional domain knowledge. Additional domain knowledge can lead to inference and is difficult to model. For example, if a user knows that only Sara ( e 5) has a salary greater than 100 K , then a document that mentions a salary figure of 150 K can easily be associated with e 5, even if the name Sara is redacted from the document. This way, the user can learn that the actual salary of Sara is 150 K .

The first two assumptions are valid in many application domains where structured data are captured separately and needs to be protected in documents. Often, official documents describe structured entities in a standardized way, eliminating the need for complex language processing. For example, a chemical paint manufacturing company 1 describes the chemical components using standard nomenclature. Another example is a production support company that needs to remove account-specific information such as server names from problem tickets before sharing those tickets across accounts. In such cases, edit-distance-based fuzzy match-ing is sufficient to identify which entities to redact, without understanding the semantics of the text.

The third assumption pertains to domain knowledge which can vary from reader to reader and can not be ingested. In practice, domain knowledge does not lead to critical information leakage since such information is already captured by the business processes in structured format. Moreover, our system further reduces this leakage by redacting the structured clues that might lead to such deduction. 3.3 Identifying sensitive information The content of a document is sensitive for a particular user, if the content pertains to some entity that the user does not have access to. Thus, to identify sensitive information, we need to identify the entities whose information is contained in the documents. In our running example, this amounts to identify that entity e 2 is mentioned in the document. One way to do this is to build entity annotators that identify the entities along with their attribute values mentioned in the document. These entities are then compared with the entities in the structured data to resolve which entities were actually mentioned. However, considerable effort needs to be invested to build annotators with high precision and recall. False negatives in identifying entities can lead to information leakage. For example, if the annotator fails to identify that Bangalore is associated with Manish P in the document d , it will not identify that entity e 2 is associated with the document. As a result, d will be released as-is even if a user does not have access to e 2 .

ZoRRo takes a conservative approach and finds the set of possible entities associated with a document purely based on matching the terms in the document with the identifying attribute values of the entities. This may lead to more entities matching a document, but guarantees that no entity is missed. For example, for the terms in document d , the potentially matching or more of the terms {Manish P, Bangalore, Delhi} from the document. Entity e 5 does not have any matching attribute value and thus can be ignored. Note that ZoRRo employs fuzzy matching to account for variations between the spelling of a term in the structured data and the spelling in the document. For example, even if the document talked about  X  X anosh P, X  entities e 1 and e 2 are identified as potential matches.

Based on this, the problem of identifying sensitive information can be formulated as follows. Consider a database D containing a set of tables { T i } . The information about an relationship. Let T denote a join over all the tables. That is, each row of T corresponds to an entity. The set of identifying attribute values of the entity is referred to as the context C ( e ) the set of users that have access to the entity row according to the LBAC policies. Let U A ( a ) denote the set of users that can access attribute a , according to the column labels in LBAC. Let w denote a word in the document and w a denote the attribute corresponding to w .
When a user u tries to access a document d , the system should mask a set of words M ( d , u ) from the document such that the following conditions are satisfied: 1. w  X  d  X  u /  X  U A (w a )  X  w  X  M ( d , u ) 2. w  X  d  X  X  X  e  X  T : (w  X  C ( e )  X  u /  X  U E ( e ))  X  w  X  M ( d , u )
We explain these conditions using the running example from Tables 1 and 2 whereauser is trying to access document d . The access rights of the user according to the LBAC policies are shown by shading the cells that are blocked. The columns Name and City are exempt since they are publicly available identifying information. The column-level policy protects the column AccountNum from the user. The row-level policy allows the user to access only row e 2 .

The first condition checks whether there are some attribute values in the document that the user should not have access to (column-level LBAC access) and blocks those. In this exam-ple, the user does not have access to the AccountNum column, so the word 918044088778 corresponding to an account number is masked.
 The second condition checks whether the user has access to all the entities (row-level LBAC access) that can be associated with the document. If a user does not have access to some entity, the identifying information pertaining to that entity is masked. This is to prevent inadvertent leakage of information. In this example, the terms Bangalore and Delhi need to be masked, since they can be associated with entities e 1 , e 3 ,and e 4 for which the user does not have access. The masked document is shown in Table 3 . 4Using k -safety to improve efficiency and utility 4.1 Background The process described so far prevents information leakage but has drawbacks in terms of utility and efficiency. Let us first consider the utility aspect. In the example, the terms Manish P , Bangalore ,and Delhi were masked, rendering the document almost useless. This may be unnecessary, since the masked terms by themselves are not very selective. There are several entities with a City attribute of value Bangalore or Delhi .

The second aspect is efficiency. The set of entities potentially matching the document is { e , e 2 , e 3 , e 4 } , and the access control needs to be checked for each of these entities which can be quite expensive. Again, this is due to frequent terms such as Bangalore and Delhi matching many entities. However, if multiple entities match the identifying information in the document, an adversary can not distinguish which entity is actually meant in the document. To avoid unnecessary redaction of such sets of terms, the notion of k -safety was introduced by Venkatesan et al. [ 7 ].
 Definition 1 k -safety: Let W  X  d be a set of terms. Let N ( W ) be the number of entities, whose context contains W ,thatis, N ( W ) = | { e : W  X  C ( e ) } | . The set of terms W is said to be k -safe, if N ( W )  X  k . A document is considered k -safe if all subsets of terms in the document are k -safe.

Consider a user with access to a document d .Let I denote the sensitive information in the document X  X his includes sensitive attributes and any other sensitive information. Sensitive information can be associated with an entity only if there is some identifying information about that entity in the document. Let W be the maximal set of terms in d which relate to an entity e .Thatis, W = d  X  C ( e ) . The confidence with which the user can associate the information I with the entity e is bound by the confidence with which the entity e can be entities (including e itself) whose context have the same set of terms W . There is no way to bounds the confidence with which a user can associate sensitive information with any given entity to 1 k . 4.2 Identifying sensitive information with k -safety To make the process of identifying sensitive information more efficient, ZoRRo relaxes the condition of information leakage. Instead of preventing complete information leakage, it aims to limit the confidence with which a user can associate sensitive information with an entity she does not have access to. Using the concept of k -safety, this confidence can be limited to k . Algorithm 1 shows an outline of the algorithm. The process of masking is similar to before, except that for a matching entity e for a document d ,if C ( e )  X  d is k -safe, the access control on e need not be checked. We call such an entity k -safe. Only entities that match at least one non-k -safe set of terms need to be considered during redaction. We call such entities non-k -safe entities.
In the example, the matching entities for the term {Delhi} are e 1 and e 4 .If k =2, {Delhi} is k -safe since there are two entities matching the term {Delhi}. Thus, we need not look up the access control for e 1 and e 4 . The matching entities for the term {Bangalore} are e 2 and e 3 , and thus {Bangalore} is also k -safe. However, for the terms {Manish P, Bangalore}, there is exactly one entity, e 2 , that matches. Thus, the terms {Manish P, Bangalore} are not k -safe, and the access control of e 2 needs to be checked. Since the user is allowed to access e , nothing needs be masked. The set {Manish P, Delhi} is not k -safe either, since it matches exactly one entity, e 1 . As the user does not have access to e 1 , at least one of the terms needs to be masked. Masking {Delhi} leaves the term {Manish P}, which is k -safe. Once {Delhi} is masked in the document, any set of terms remaining is either k -safe or the user has access to the corresponding entities. The masked document shown in Table 4 has higher utility than the masked document shown earlier in Table 3 and still does not leak information.
Note that for a non-k -safe set of terms, it is necessary to check the access control for all the matching entities (Line 4). Checking only whether the users have access to at least one matching entity may lead to information leakage.
 Theorem 1 It is necessary to check the access control on all the matching entities for a non-k-safe set of terms.
 Proof Let W be a set of terms that is not k -safe and E w be the set of entities matching on W , that is E w ={ e : W  X  C ( e ) } .Let N = | E w | be the number of such entities. The user can determine N since the identifying attributes that form the context are all publicly accessible. Since W is not k -safe, we have N  X  k . Let there be at least one entity e in E w which the user does not have access to. If the document is released without redacting any term in W ,the confidence with which the user can associate information in the document with e can be 1 N , which is greater than 1 k . Thus, the user can learn about an entity he is not supposed to have access to with a confidence higher than 1 k , which violates our goal of protecting information. Alg. 1 Identifying values to redact 5 The ZoRRo system 5.1 Architecture As discussed earlier, the aim of ZoRRo is to address the problem of information leakage from unstructured text, given that the same information is also present in structured form. To aid the problem discussed, we define a three-layer system architecture. The user interface (UI) layer provides the user with a means to specify a set of keywords or a document ID to search the content repository. Once the documents are redacted by ZoRRo, the UI can present the user with a list of hyperlinks to the redacted documents. The processing layer performs the necessary pre-processing and runtime processing to redact the document(s) requested by the user. The data layer encompasses both the structured database as well as the document repository.

In practice, most companies will want to directly integrate the ZoRRo functionality into their existing enterprise content management (ECM) system. Figure 2 shows the architecture for ZoRRo along with such an integration. In this case, all UI functionalities are provided by the content management system. We are currently working on integrating ZoRRo into the FileNet content management system [ 31 ]. Figure 3 shows a screenshot of the integration. The menu on the left-hand side of the figure shows the ProtectedFolder which stores the sensitive files. When a user requests a file from this folder, the file is redacted using ZoRRo before the user can access it. Figure 3 also shows the redacted document presented to the user.
Independent of the architectural choice, the main components of ZoRRo are the Indexer, the Protected Data Discoverer, and the Redactor. The Protected Data Discoverer analyzes the text content and identifies content sensitive for the user based on the LBAC policies. The sensitive content is passed to the Redactor, which redacts the identified keywords from the original document, preserving the format of the input document. The Indexer helps in counting the number of entities that match a set of terms. In our implementation, we use a Lucene [ 32 ] index and exploit the scoring function in Lucene to efficiently find non-k -safe sets of terms. To reduce the runtime of ZoRRo, both indexing as well as building appropriate dictionaries are done as a pre-processing step.

We have implemented the ZoRRo APIs in Java to make integration with content manage-ment systems easy. ZoRRo relies on third-party libraries such as Apache Tika [ 33 ], Apache PDFBox [ 34 ], and Apache POI [ 35 ] to extract information from and modify content of arbi-trary documents. For our implementation, we assume that LBAC policies, LBAC protected data, and user-access labels are stored in IBM DB2 [ 36 ].

We now discuss the main components of ZoRRo in more detail. 5.2 Pre-processing Indexing The Entity Index is created from the entities stored in the database. A unified entity view is created by joining all the related tables in the database. For each entity, a document is created that includes all the attribute values corresponding to that entity. An inverted index is then created from the set of these entity documents. The Entity Index is used to quickly identify the set of entities related to a document, that is, those entities whose information is potentially contained in a document. Creating the index is a one-time step and can be done efficiently using Lucene. Triggers ensure that the Entity Index is updated whenever the entity information changes in the database.

For efficiency, we could also include the LBAC labels associated with that entity in the document. This would eliminate the need to go back to the database at runtime, even for access-control information. However, it requires updates to the index whenever LBAC labels change.
 Dictionaries and patterns To identify the entities associated with a document, we associate either a dictionary or a regular expression with each attribute of an entity. A dictionary for an attribute is simply the collection of all values of that attribute in the database. For example, the dictionary for the Name : attribute for the entities shown in Table 1 consists of the values {Manish P, Tom S, Jim S, Sara L} . Note that for ZoRRo purposes, we do not need to identify any names that do not occur in the database.

For some attribute types, dictionary-based annotation may not be appropriate. In this case, a regular expression can be associated with the respective attribute. For example, there are many different ways to express a date, making it advisable to associate a regular expression with attributes of type date such as the attribute BirthDate . These regular expressions can be either specified by the user or learned automatically by ZoRRo. For that purpose, any standard regular expression learning method based on positive examples [ 37  X  39 ] can be integrated with ZoRRo, Allowing ZoRRo to automatically associate a regular expression with a specific attribute, using all the different values for this attribute present in the structured dataset as positive examples. 5.3 Runtime processing To be dynamic and support on-the-fly user-specific redaction, sensitive information that needs to be redacted can only be identified at runtime.

TheProtectedDataDiscovererdetermineswhatneedstobemaskedinthedocument,based on the access rights of the user. The first step is to identify the words in the document that correspond to attribute values. Each annotator associated with an entity attribute identifies the respective attribute values occurring in the document. Dictionary-based matching employs fuzzy matching to account for minor spelling variations. The output of this step is a set M
D of key:value pairs, where key denotes the attribute name and value is the attribute value occurring in the document.

The next step is to check whether the identified attribute values are sensitive for the user based on row and column labels. Algorithm 2 lists the algorithm for identifying the values Alg. 2 Identifying values to redact that need to be redacted. It takes as input the key:value pairs identified in the document and the safety parameter k and outputs a subset of key:value pairs corresponding to the values that need to be protected in the document.

First, the column labels of the attributes occurring in the document are compared with the labels of the user (lines 2 X 4). If access to a column is not allowed for the user, the corresponding key:value is added to the set of values B that need to be redacted. Row-level checking is more expensive and entails counting the number of matching entities for each set of terms to check for k -safety (see Algorithm 1 ). We use the Entity Index to find the matching entities and exploit Lucene X  X  scoring mechanism to reduce the number of results that need to be processed.

Rather than finding the matching entities for each set of terms separately (which leads to many queries), the algorithm constructs a single query that is an OR of all the values in M D (lines 5 X 6). This returns all entities matching any term in the document in decreasing order of their scores. The Lucene API allows for specifying a limit on the number of matches to be returned. In our implementation, we use a limit MAX which is 5% of the total number of entities in the database. The assumption is that entities beyond the top 5% are such weak matches that their information in the document cannot be sensitive. Note that for sensitive applications, this limit can be raised to consider such weak matches. The algorithm then iterates over the results returned by Lucene. For each result entity e , it computes the set S of matching terms (lines 7 X 8). The entity e is recorded in the matching entities corresponding to S in the EntityMap (line 9). Entities which already matched on a superset of S are also added to the set of entities for S (lines 10 X 11). When a k -safe singleton set S is found, the rest of the results are skipped (lines 12 X 13). We exploit the scoring function of Lucene to ensure that it is fine to skip the rest of the results as soon as a singleton k -safe set is found. We discuss the correctness of this approach in Sect. 5.4 .

For each non-k -safe set of terms in EntityMap , the LBAC labels of all the entities matching on that set of terms are compared with the labels corresponding to the user (lines 14 X 16). If the user is not allowed to access an entity, terms are selected from S to be redacted, until the set of remaining terms leads to a score of less than Threshold ( k ) (lines 18 X 21). Threshold ( k ) is the score Lucene gives for a match on a term that occurs exactly k times in the entity dataset. Terms already selected for redaction are accounted for while computing the score (line 17), in order to minimize the number of terms to be redacted.

Finally, the {Key:value} pairs and the document are given to the Redactor module which searches for the values in the document and replaces them with a fixed string such as  X  X XXXX. X  5.4 Exploiting Lucene scoring The algorithm described in Sect. 5.3 only works correctly if we can guarantee that once an entity matching on a k -safe term is found, any further matches beyond it are also k -safe.
To achieve this, we exploit the scoring function that Lucene uses. The default scoring function in Lucene is based on the weighted vector space model, where the weights are TF-IDF values. The scoring function is listed below:
In this equation, tf ( t , d ) represents the term frequency and idf ( t ) represents the inverse documentfrequency. Lucene allows customization of its scoring function. We setallof coord , queryNorm , tf , get Boost ,and norm to always return 1. The default idf function is defined docFreq is never 0 in our case, so there is no need to add 1). The scoring function is now:
The following property holds for entities that have exactly one term matching with the query: Property 1 Any entity with exactly one k -safe matching term will have a score lower than any entity with exactly one non-k -safe matching term.
 Proof Let e 1 be an entity matching on a non-k -safe term t a and e 2 be an entity matching on since both entities match on only one term each.
 This property implies that as soon as we find an entity matching on one k -safe term, we can skip the rest of the matches with one term, since they will also be k -safe. However, it may not always hold for entities matching on more than one term.

If we assume that the attributes are independent of each other, the property holds even for p ( t ) = docFreq numDocs and if there are m matching terms for an entity e ,weget: In other words, a matching entity e 1 will have a higher score than a matching entity e 2 if docFreq ( e 1  X  q )&lt; docFreq ( e 2  X  q ) . This implies that an entity matching with a k -safe set of terms can never get a score higher than an entity matching on a non-k -safe set of terms. In practice, the assumption of independence might not hold in some cases, and to be conservative, we stop processing the records only once we reach a k -safe record which matched the query on a single term. 5.5 Improving recall using fuzzy matching We briefly discuss how employing fuzzy matching during the annotation process strictly increases the recall of ZoRRo. That is, ZoRRo will redact sensitive terms even if their spelling in a document does not exactly match the spelling in the database. Once additional variations of a term in a document are identified, the exact value and all detected variations are included in the Lucene query. The additional terms result in more results, which in turn can bring in new non-k -safe sets of terms. Those additional non-k -safe sets of terms may lead to extra terms being redacted, depending on the access rights of the user. While this approach may sometimes lead to over-redaction, it guarantees that spelling variations do not cause leakage of information.

Assume that the sample database given in Table 1 contains one more entity e 6 where the name is  X  X anosh P X  and the city is  X  X angalore. X  In this case, the dictionary will contain both variations and the term  X  X anish P X  will be annotated in the document both due to  X  X anish P X  as well as due to  X  X anosh P. X  As a consequence, ZoRRo will query Lucene including both variations of the name. When deciding which terms to redact, both (Manish P, Bangalore) and (Manosh P, Bangalore) appear in the list of non-k -safe terms. If checking the access control for e 6 reveals that the user does not have access to entity e 6 , the term  X  X anish P X  will be additionally redacted from the document. For employing fuzzy matching, we use the edit-distance-based scheme described in [ 25 ]. 6 Experiments and results In this section, we describe the experiments we performed to evaluate ZoRRo. Specifically, Sect. 6.2 discusses performance results and Sect. 6.3 the redaction quality of ZoRRo. 6.1 Experimental setup All experiments were conducted in a Windows 7 Ultimate environment using IBM X  X  DB2 Express-C version 9.7.4 on an Intel Core 2 Duo processor with 2 GB RAM. Lucene version 3.5 was used for indexing. We measured various parameters such as average time taken to redact a document, number of non-k -safe entities related to the document, and number of terms redacted. The numbers reported are an average over a set of 300 documents. 6.2 Performance results We conducted a comprehensive set of experiments to understand the effect of various para-meters on the time taken to redact a document. 6.2.1 Synthetic dataset To study the performance of ZoRRo, we needed a large dataset with structured records and related unstructured documents. We used the TPC DS 2 [ 40 ] benchmark schema and data generator to create a dataset including the sales table and all related tables. Entities correspond to sales transactions and all attributes were treated as identifying attributes. In addition, we generated related documents by randomly picking terms from a dictionary of words and adding terms from the attribute values in the database. For each document, we randomly selected a set of entities and a set of attributes from those entities to add to the document. For example, if the generated number of entities was 3 and the number of attributes was 10, we picked 3 entities from the database, selected 10 attributes in total from these 3 entities and added those attribute values to the document. Unless otherwise mentioned, the number of records used in the experiments was 100,000. The number of terms was set to 150, the number of attributes to 10, and the number of entities per document to 3. The MAX parameter in Algorithm 2 was set to 5,000, thus limiting the number of matching entities to 5,000. 6.2.2 Effect of parameter k in k-safety To study the effect of k in k -safety, we varied k from 2 to an unbounded value. A value of 2 for k means that any set of terms that matches at least two entities is considered k -safe, leading to the least amount of redaction. An unbounded value of k implies that no set of terms is k -safe; thus, access control needs to be checked for all matching entities, leading to the maximum amount of redaction. Figure 4 shows the average time taken to redact, the average number of terms redacted, and the average number of non-k -safe entities per document, when varying k from 2 to 25. As expected, as k increases, the number of non-k -safe entities increases from 4 to 35, thus increasing the time required to redact a document. The number of actual terms redacted saturates at k = 3 . 45. This is due to the fact that many of the non-k -safe entities match on the same set of terms. The time required to redact a document when k is unbounded was 79 seconds. The unbounded k case reflects the baseline and shows that naive methods without the optimization of k -safety are impractical for redacting documents on the fly. 6.2.3 Effect of entity database size To study the effect of the size of the structured database on the performance, we varied the number of entities from 25,000 to 125,000, while keeping k constant at 10. Figure 5 shows the results. As the number of entities increases, the time taken to redact a document also increases. There are two reasons for this behavior. As the entity database size increases, more entities are expected to match each document and more access-control information needs to be looked up. In addition, the Entity Index size increases and thus the time taken to look up the index increases. Note, however, that the time for redaction increases at a much slower rate than the increase in the database size, indicating that ZoRRo scales well to large database sizes. 6.2.4 Effect of document content To study the effect of the document content on the redaction process, we varied the number of matching attributes when generating documents. We kept the number of entities constant at 3 but picked a varying number of terms from those entities. The results are shown in Fig. 6 a, b. Summarizing our results, we observe that as the number of attributes increases, the time taken to redact a document also increases. If there are more attribute terms in each document, we observe more matching entities and more terms that need to be redacted. Both effects increase the time taken. In fact, we can see that the number of non-k -safe entities does increase. The number of terms redacted is also increasing since the number of sensitive terms in a document increases with the number of matching attributes.

We also varied the number of entities per document when generating documents while keeping the number of attributes constant at 10. The results are shown in Fig. 7 a, b, for a comparison against the time taken and the ratio of non-k -safe entities, respectively. Summa-rizing our results, we observe that the time to redact does not vary much with the number of entities per document. The number of non-k -safe entities depends heavily on the number of matching terms, which we kept constant at 10 for this experiment. 6.2.5 Discussion Our experimental results confirm that ZoRRo X  X  two main strategies of reducing the complex-ity in determining which terms to redact from a document, work well in practice. 1. As explained, not all rows are relevant to redacting a document. Using an inverted index 2. As explained, the set of matching entities can be very large due to some non-selective 6.3 Redaction quality To test the quality of redaction, we conducted a set of experiments to measure the precision and recall of the redaction done by ZoRRo on manually tagged data. Earlier works on text sanitization such as [ 4 ] have defined the utility of a document as the similarity between the original document and the sanitized document. However, since ZoRRo does user-specific document redaction with the goal of allowing each user to perform his or her role, the utility of a redacted document varies based on the role of the user. For example, in the medical domain, a document redacted for a doctor may not have any utility for an administrator, whereas a document redacted for an administrator is unlikely to be useful to a doctor. Thus, instead of adhering to a particular definition of utility, we define our results only in terms of precision and recall. 6.3.1 Real-world dataset We picked a dataset from a public-safety scenario where entities are people and documents may be news articles, emails, and other notes about the people. The structured dataset consists of person data released by the US Department of Treasury in the Specially Designated Nationals (SDN) List [ 41 ]. We used a subset of attributes for the entities and generated a table T with attributes Name, Address1, City1, Country1, Address2, City2, Country2, Alias1, Alias2, Alias3, Alias4, Alias5 and Alias6 . There are 10,000 entities in the dataset.
The documents cover press releases related to these entities published by the Treasury department. We manually tagged 20 such documents. Manually deciding which term(s) to remove to avoid a non-k -safe set of terms (for varying k) is very time-consuming and not deterministic. Thus, even though this does not result in a minimal redaction, we decided to redact all terms if a set of terms is non-k -safe. For each set of related terms, we counted the entities matching in the database and tagged the terms with that count. For example, if Manish and Bangalore are related in the document and there are 10 entities matching { Manish, Bangalore } in the database, we tagged the terms as Manish_10 and Bangalore_10 . The LBAC policy was set such that the user did not have access to any entity. Thus, a word tagged with a count c should be redacted for any value of k greater than c . 6.3.2 Precision and recall We ran ZoRRo on the test documents for various values of k 3 and computed two sets of precision and recall. One set is with respect to manual redaction for the same value of k , and the second set is with respect to manual redaction for unbounded k . The unbounded k corresponds to the case where no leakage of sensitive information is allowed.

As a result of tagging the documents, we had the set T = ( t , c ) , consisting of terms t associated with the number of entities c in which they appear. For a given k -safety parameter If R k denotes the set of redactions marked by ZoRRo for a given value of k , precision and recall for the first set of results can be computed as: Similarly for the second set of results,
The results are shown in Table 5 . The high values for recall show that ZoRRo achieves its primary goal of preventing information leakage and does not miss out on redacting terms. The results also show that ZoRRo X  X  efficiency comes at the cost of reduced precision.
Comparing with the unbounded k case, one can see that as k increases, recall increases and precision reduces. This is expected since increasing k amounts to a conservative approach that leads to more redactions.

There are two effects that lower ZoRRo X  X  precision. The main cause is that ZoRRo consid-ers each document as a bag of words and conservatively checks the k -safety of each subset of terms, even though some terms may not be related in the document. To control unnecessary redactions due to this, large documents can be split into smaller chunks. This can also reduce the processing time, because of reduced attribute value combinations. Additionally, spurious fuzzy matches may lead to a decrease in precision.

There are also two reasons ZoRRo may not achieve perfect recall at lower k values. First, fuzzy matching sometimes fails to identify some variants of attribute values, causing ZoRRo to miss the term when redacting. Second, as explained in Sect. 5.3 , ZoRRo stops processing results returned by the Entity Index once it reaches a k -safe record that matched on a single term.

To verify that this does not lead to significant information leakage, we quantified the non-k -safe records found after we stopped processing the results (skipped records). Remember Note, however, that an actual leakage only happens if a skipped record contains a term that did not already get redacted based on other non-k -safe term sets found earlier (missed terms). Table 6 shows that the number of terms ZoRRo misses is very low and rapidly diminishes with increase in k .

Note that for sensitive applications, unbounded k value with a given fuzzy matching threshold guarantees 100% recall across all entities which match within the given threshold. 6.3.3 Effect of fuzzy matching To understand the effect of fuzzy matching on redactions done by ZoRRo, we quantified the results for the first set (i.e., precision k and recall k with respect to manual redactions for the same value of k ) by setting edit distance to 0. The results for this setting under different values of k are summarized in Table 7 .
 As seen in the table, fuzzy matching improves recall while penalizing precision for ZoRRo. This is expected since fuzzy matching may retrieve spurious matches apart from retrieving desired terms missed by exact matching. Whereas the desired terms can improve recall, the spurious matches may reduce the overall precision as mentioned earlier. 6.3.4 Effect of user-specific redactions The precision and recall tabulated in Table 5 were computed after tagging documents as per the access control for a user who does not have access to any entity from the dataset. To understand the effect of user-specific redaction, we varied the percentage of entities a user can access. To contrast against existing approaches that can only perform user-agnostic redaction, we compared redactions done for each user as per their respective access control against redactions done by considering that the user does not have access to any entity. These results are summarized in Table 8 . Each row in the table shows precision and recall achieved by ZoRRo (i.e., user-specific setting) and user-agnostic setting for a given percentage of entities that the user has access to. The entities which the user can access were randomly chosen, and the value of k was set to  X  (i.e., Unbounded ).

Table 8 shows that as the user X  X  access control becomes less restrictive, the precision under user-agnostic setting reduces drastically and finally converges to 0% in case of a user who has complete access. This is expected since the user-agnostic setting for unbounded k redacts very conservatively. On the other hand, the user-specific redactions by ZoRRo perform quite well across all entity-access levels. 7Conclusion In this work, we presented ZoRRo, a system and method to prevent leakage of information from unstructured text documents by utilizing the well-formulated access-control techniques in database systems. ZoRRo specifically targets use cases where the same information exists in structured as well as unstructured form.

To make ZoRRo work in practice, we employed a simple bag-of-words approach instead of relying on information extraction and sophisticated NLP techniques to understand the content of the document. Building appropriate annotators to extract entities and all their related information from text requires considerable effort and is very domain-specific. In ordertoguaranteethatallsensitiveinformationisredacted,annotatorsneedtohavehighrecall and precision. In contrast, ZoRRo is completely domain independent and can be employed out-of-the-box without heavy customization effort.

While ZoRRo X  X  conservative bag-of-words approach may result in unnecessary redaction, experiments show that ZoRRo X  X  redaction precision is still over 65% with a recall very close to 100%. Thus, given the choice between not revealing a document at all and revealing a slightly over-redacted document, companies gain significantly by using ZoRRo. Also, while ZoRRo does not guarantee to prevent any information leakage, it significantly reduces the risk as compared to revealing unredacted documents.

We demonstrated that by using the concept of k -safety, we can restrict the number of entities to be considered, thus making on-the-fly redaction based on a user X  X  access privileges feasible.OurexperimentsshowthatZoRRocanprotectsensitiveinformationfromdocuments with a high recall while still preserving interactive response times.

There are several ways to further enhance ZoRRo. Since the set of non-k -safe entities for a document is independent of the user, computing this set could be done as a pre-computation step. Only the entities that have been updated since the last pre-computation need to be con-sidered at runtime. Also, without making the assumption of attribute independence, ZoRRo first k -safe entity that matches on a single term. Eliminating the need for this assumption and handling correlation between attributes more efficiently is an interesting research problem. References
