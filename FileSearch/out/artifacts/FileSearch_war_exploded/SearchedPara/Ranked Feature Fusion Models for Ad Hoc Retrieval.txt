 We introduce the Ranked Feature Fusion framework for in-formation retrieval system design. Typical information re-trieval formalisms such as the vector space model, the best-match model and the language model first combine features (such as term frequency and document length) into a uni-fied representation, and then use the representation to rank documents. We take the opposite approach: Documents are first ranked by the relevance of a single feature value and are assigned scores based on their relative ordering within the collection. A separate ranked list is created for every feature value and these lists are then fused to produce a final document scoring. This new  X  X ank then combine X  ap-proach is extensively evaluated and is shown to be as effec-tive as traditional  X  X ombine then rank X  approaches. The model is easy to understand and contains fewer parameters than other approaches. Finally, the model is easy to extend (integration of new features is trivial) and modify. This ad-vantage includes but is not limited to relevance feedback and distribution flattening.
 H.3.3 [ Information Search and Retrieval ]: Retrieval Models Algorithms, Theory Feature Ranking, Metasearch, Score Normalization
The modeling of the ad hoc information retrieval pro-cess using principled formalisms has a long history. Over the years, many models have been proposed, from the vec-tor space model [12], to best-match tf . IDF models [9], to Copyright 2008 ACM 978-1-59593-991-3/08/10 ... $ 5.00. inference networks [16] and language models [8] and even more recent  X  X earning to rank X  approaches [17]. These mod-els combine raw statistical features from the collection (e.g. term frequencies, document lengths) into a single score that determines the retrieval rank of the document. Indeed, it is in the cleverness with which a model is able to combine the various features to produce a unified document score (e.g. by using document length as a normalization factor for term frequency) that a model succeeds or fails.

We take a different view. Rather than integrating raw statistical features directly, we first rank documents by the relevance of each feature independently, e.g. the frequency of a single query term, or the length of a document that contains a query term. We then combine the ranked lists that each feature produced. We name this general approach  X  X anked-feature fusion. X  The model is not defined by any particular ranking mechanism. The fusion strategy is also not a defining characteristic, as strategies other than the one we adopt in this paper are possible. What distinguishes the current approach is that multiple ranked lists of documents are created, one for each feature. These ranked lists are then combined to produce the final ranking. This work in-troduces an important notion: The value of a single feature is not relative to other features in the document in which it resides; it is relative to its relevance-ranked position in a list of documents that contain the same feature. We will show that this approach not only yields results that are as good as a well-known retrieval model, but does so with no parame-ter tuning and in a manner that is easier to understand and modify. We argue that this model offers an interesting and fertile new foundation for retrieval algorithm design.
Consider a two-word query: q 1 q 2 . When these query terms appear in a document, a combine then rank approach produces a single score for that document based on the com-bination of all features (e.g. term frequency tf and docu-ment length dl ) from the whole query. To slightly oversim-plify, this typically translates to the sum or the product of the term frequencies, normalized by the document length:
Our rank then combine approach, on the other hand, treats each of these features (both tf and dl for terms q 1 and q as independent retrieval subsystems. Each subsystem com-pletes its own ranking of all the documents in the collection by that feature. The query above produces four ranked lists: R documents sorted by tf ( q 1 ). R dl ( q 2 ) is a ranked list of doc-uments sorted by dl ( q 2 ), the lengths of all the documents that contain q 2 . These ranked lists are then combined, or fused, to produce the final document ranking R , using a fu-sion strategy such as CombSUM [1]. Let R tf ( q 1 ) ( d )bethe normalized rank at which document d is found in R tf ( q 1 The final score for d is:
Some research has hinted at using term frequency [7] for fusion, but we are not aware of any experiments on such low level features. In this section we perform one such ex-periment to motivate the problem, and in later sections we expand this approach into a full retrieval model.
We compare sum tf and borda tf that use only one class of feature, tf , the term frequency in a document. However, the first system uses raw term frequency, whereas the second system uses ranked term frequency. While better results can be obtained by using document lengths and document frequencies, comparing two systems using only the tf feature should give us insight into the rank-then-combine approach. (In Section 5 we will compare full-strength models.) The first system, sum tf , is a combine-then-rank approach. The overall score for a document is the combination of the sum of term frequencies. Documents are ranked by this combined score:
The second system, borda tf , is a rank-then-combine ap-proach. Documents are ranked by term frequency; the final score for a document is the sum of the borda counts, rather than the sum of the raw tf .Let borda tf ( t ) be a list of doc-uments sorted by tf ( t ) and normalized by Borda counts [1]. Borda assigns a score to each document based on its relative position in the list: max (1000  X  r, 1), where r is the orig-inal rank. The top ranked tf receives a score of 1000, the second ranked frequency a score of 999 and so on. If there are more than 1000 unique frequencies, a minimal score of 1 is assigned to all ranks past 1000. borda tf ( t ) ( d ) is the rank-based score for document d in that list:
We did a full-scale evaluation of borda tf versus sum tf ,us-ing 400 queries from TREC1 through TREC8 (topics 51-450) on their appropriate full volumes. The queries are title-only, except for TREC4, which has a short description field. The results are found in Table 1. The sum of the tf ranks (rank-then-combine) far exceeds the performance of the sum of the raw tf (combine-then-rank) only.

Naturally, in full-strength retrieval systems various forms of document length normalization are used to account for the differences in tf magnitudes. The purpose of this example is simply to show that there are advantages to the ranked feature fusion approach.
There are many ways in which ranked list fusion has been used to combine multiple sources of relevance evidence. For this discussion we refer to Figure 1 that illustrates the three conceptual levels at which fusion can take place. Typi-cally ranked list fusion has been used as a foundation for metasearch [13, 1]: the same query is fed to multiple search engines that make different est imates of relevance. The com-bination of ranked lists from these engines increases both the precision and coverage of the resulting list. Metasearch fu-sion is sometimes also known as external fusion because the entire retrieval process is completed by the engines before their outputs are combined.

Internal fusion operates within single search engine [1, 7]. Documents contain various types of information such as bibliographic data, hyperlinks or citations, images, and unstructured text (see Figure 1). The output of separate, specialized search engines for each of these document sub-content types is fused into a single ranked representation.
Ranked feature fusion takes this notion a step further. It is possible, within a single subsystem, to rank the features that comprise that subsystem. Such approaches are not uncom-mon in multimedia retrieval domains, where the large state space often makes it easier to first classify (rank) lower-level features and then combine, rather than inventing a formal probabilistic model that unified all features [15]. However, Figure 1: Traditional (external) fusion merges the in standard text retrieval there is a trend is in the opposite direction, toward more complex modeling or combination functions (e.g. probabilistic graphical models, support vec-tor kernels). Some research does hint at pushing the fusion deeper into the retrieval process by using term frequencies as a retrieval subsystem [7] but we are not aware of any studies implementing that or similar sub-internal, ranked feature, retrieval subsystems. The goal of this work is to use the same raw features as other systems, namely tf , dl and IDF , and show that a simple yet effective retrieval algorithm can be composed from feature ranking and fusion.
Section 2 contains an interesting example of rank-then-combine effectiveness using term frequencies only. In this section we show how a complete system can be built from fusion components.

Aslam and Montague [1] distinguish between two different types of unsupervised fusion: fusion schemes that operate on ranked list information only (e.g. Borda count) and fusion schemes that make use of the magnitude of the relevance score returned from each engine (e.g. CombSUM); Comb-SUM was shown to outperform Borda count, and so we chose it as a foundation upon which the retrieval system is built [13, 1, 7]. The purpose of this work is not to introduce a new fusion model. It is to show how fusion provides a solid foundation for retrieval using only the basic features tf , dl and IDF , and to show how feature ranking creates new op-portunities for system design. Three stages are necessary to describe the model: feature re levance ranking, ranked list normalization, and fusion.
In external and internal fusion (see Figure 1) a retrieval subsystem ranks documents using whatever information is available. Ranked feature fusion does the same, but only uses a single feature, such as tf ( q )or dl ( q ).
To rank documents by relevance using only one feature, that feature needs to have different values in different doc-uments. A feature that has the same value in every docu-ment does not discriminate between documents and cannot be used for ranking. Both tf and dl are suitable for relevance ranking: Query terms have differing term frequencies in dif-ferent documents, and documents containing those query terms have different lengths. However, IDF alone cannot be used for ranking. Rather, it appears in the fusion model as a weighting factor (Section 4.3).
Relevance ranking by tf is straightforward: documents with a higher frequency for a query term are ranked above documents with a lower frequency. Most of modern IR lit-erature is based upon the notion that term frequency is positively correlated with relevance and we do not justify it further. Table 2 contains an example, TREC topic#63: machine translation . Columns 1 and 5 show tf ,sortedby relevance. In the interest of space, multiple documents with the same value/rank are removed only the top and bottom twelve values are shown.
Relevance ranking by dl , though on the surface not im-mediately obvious, is just as straightforward. One need only realize that most, if not all, combine-then-rank models (e.g. BM25 or language models) use document length as a tf nor-malization factor, i.e. 1 dl . Document length is often inversely correlated with relevance, which means that shorter docu-ments rank higher than long documents. Columns 9 and 13 in Table 2 show dl , sorted by relevance.

Using separate dl -based rankings for every query term means that one document has non-equal rank values in dif-ferent lists. In our example, both terms appear in a doc-ument of length 21, but the document with that length is ranked 12 th in the machine list and 7 th in the transla-tion list. The corresponding ranked-based document length scores are therefore different. To our knowledge, this is the first information retrieval model in which a single doc-ument length score is given multiple values, depending on which term in that document is being matched. It is beyond the scope of this paper to investigate this further, but this non-intuitive X  X nd potentially useful X  X ituation arises directly from the ranked-feature approach.

While in many cases short documents are preferred to longer ones, there exist collections for which longer docu-ments contain better results. Whereas combine-then-rank approaches have to accommodate for this in complex ways (see [5], for example), our formalism requires only that the documents be re-ranked in order of ascending, rather than descending, document length. The document length prefer-ence can be specified by the user at query time to support exploratory search.
Now that we have subsystems for tf -and dl -based rel-evance ranking, we need a way of normalizing the ranks to fuse the lists [6]. In CombSUM fusion normalization is done via linear transformation [4, 6]; the underlying rele-vance scores are shifted and scaled to the min-max [0..1] range. The min-max score for document ranked by feature f is given by the following equation, where f ( d )istheorig-inal value of the feature in the document and rank f [ 1 ]and rank f [ n ] are original feature values in the first and last doc-uments in the relevance-ordered list:
In our implementation, rather than shifting and scaling to [0..1] range, we shift and scale to the [1..1000] range. We set the minimum to 1.0 because even the lowest ranked item in a tf or dl or any other relevance-ranked retrieval list should have a non-zero value. 1 For easy comparison of these (M)inmax values in Table 2 the maximum is set to 1000 to bring it onto the same scale as Borda count. This scaling is irrelevant, however: every ranked list is scaled by the same factor and therefore has no effect on the final rankings. Thus for term frequencies of query term q the score is:
This is not smoothing in a language modeling sense. Shift-ing the minimum to 1.0 still does not account for estimates of zero probability, i.e. a term not being found in a docu-ment. How a system deals with missing query terms in the ranked-feature fusion approach is an interesting question but beyond our current scope.
Similarly, for document lengths:
The general form of CombSUM is based on a set of re-trieval subsystems S and subsystem mixture weights  X  s minmax s is the ranked list produced by subsystem s and minmax s ( d ) is the rank-normalized score of document d in that list.
For each term q in a query Q there are two retrieval subsystems, minmax tf ( q ) and minmax dl ( q ) In external fu-sion applications of CombSUM, the mixture weights  X  s are typically set through supervised training. Recall from Sec-tion 4.1 that we cannot rank documents by IDF because it varies across terms, rather th an across documents. There-fore,itisusedasan unsupervised mixture weight of the ranked lists produced by the various term-based features. For more direct comparison with our baseline (BM25) in the next section, we use IDF [11], where N is the total number of documents in the collection, and n is the num-ber of documents in which the query term is found (which is also equivalent to the length of a ranked list for that feature, from Equation 3 above):
To give this score a mixture weight interpretation, we nor-malize by the sum of IDF scores used for fusion, i.e. two per query term (one for the tf ranked list, one for the dl list):
Finally, the core retrieval algorithm, weighted CombSUM, is simply  X  -weighted combination of all ranked lists, minmax and minmax dl ( q ) :
Other than the +0.5 value inside of the IDF score no ex-ternal parameters are set or tuned, or even needed. Every-thing about the model, from the relevance-ordered ranking to the min and max values for each subsystem, are com-pletely specified by statistics from the collection. As in Section 2, the evaluation was done on 400 queries, TREC1 through TREC8 (topics 51-450). Stopwords were removed, but no stemming was done on either system. For our baseline, we chose Okapi BM25. Naturally, tuning the parameters k 1 and b affects performance. We use the val-ues of k 1 =2 . 0and b =0 . 75, deemed by Robertson and Sp  X  arck Jones [10] as effective values on TREC collections: BM25 ( d )= We compare against BM25 for a specific reason. The the IDF function used in BM25 is the same function, Equa-tion 5, that we use as a mixture for weighted fusion. In fact, the denominator in Equation 6 is somewhat unneces-sary because it is the same for every ranked list being fused. Replacing  X  idf ( q ) with IDF ( q ) and refactoring Equation 7 yields: score ( d )=
Comparing Equations 7 and 8 shows that what distin-guishes the BM25 model and our implementation of ranked-feature fusion is the treatment of tf and dl . This allows us to compare directly a  X  X ombine-then-rank X  approach against a  X  X ank-then-combine X  approach, i.e.: tf ( q , d )+ k 1 ((1  X  b )+ b dl avgdl )
Table 2 contains the results of this experiment plus the p-values for a sign test, t-test and wilcoxon signed-rank test. While MAP of ranked-feature fusion is slightly higher than BM25, none of the differences are statistically significant. The two systems perform equally well, on average. This is interesting, almost surprising. Standard CombSUM fusion of tf and dl retrieval subsystems performs just as well as the more complicated BM25.

Sometimes the value of an information retrieval model comes not through increased system performance, but from providing a way to think about the problem in a new man-ner [8]. Ranked-feature fusion offers this possibility. Rather than basing the retrieval score on probabilities, vector spaces, or other combine-then-rank models, documents are ranked by the relevance of single features. The final ranking then is nothing more than fusion of these ranked lists.
This model is easy to understand and conceptually straight-forward, and is easy to extend and modify. Adding a feature to the model is primarily a matter of creating a relevance-ranking subsystem for that feature. Furthermore, the rank-centric approach more easily allows altering the effects of individual features by changing how that feature is ranked. In the next section we will show one method for modifying the model. The model can also be modified by changing the manner in which fusion is done. We propose and evaluate an alternate form of fusion based on the hedge algorithm as a method for implementing relevance feedback. In Section 7 we suggest many more extensions.
We have shown that our model produces search results that are comparable to a well-regarded model. While these results are encouraging, they are not sufficient to base a research agenda on a formalism. We must also show that the formalism can be extended fruitfully to support useful operations. In this section, we evaluate two such extensions.
Term frequency distributions are Zipfian, which means that there is a small number of documents in which a term occurs quite frequently and a long tail of documents in which the term occurs less frequently. Term frequency is a valuable indicator of relevance, but researchers have long observed that this contribution should not be linear; the marginal util-ity of term frequency as a diagnostic of document relevance declines as term frequency increases. Some systems utilize log ( tf ) to model this decreasing utility, whereas BM25 uses more of an tf tf + c approach, which also exhibits asymptotic behavior at higher tf values.

These existing methods all introduce non-linearity as func-tion of the raw data. How these ad hoc functions are chosen, and with what parameter values, are not intuitively clear. Our ranked feature approach offers an alternative: selec-tion of an order-statistic. Recall from Equation 3 that a relevance-ranked feature list is normalized using the values at the first and last positions in the ranked list. To remove high tf outliers, we instead take the approach of selecting the k th item in the ranked feature list, and setting every value that is ranked higher than this value to the maximum value in the normalized range [1..1000], i.e. we  X  X lip X  the top k items in the list. We normalize the remainder of the ranked list using the k th value as the new maximum. flattened f ( d )= Figure 3 plots this effect for one term X  X  tf values and Table 2 gives the scores for the ongoing machine transla-tion query example. Order statis tic-based transformations of ranked lists have the following properties: 1. The dominance of high-frequency terms is reduced. 2. k th value renormalization increases the dynamic range, 3. It is easier to choose order statistics than to than to 4. Different features have different normalization values. 5. This clipping level can be set interactively, allowing Figure 3: Ranked lists for tf of translation .Com-parison of Min-max versus Flattened normalization.
Therefore, outlier flattening (Table 2) is done not only on term frequency features but also on document length fea-tures, or on any ranked feature list. For our experiments, we chose a threshold k of5toreflectthecommonPrecision@5 metric and repeated the retrieval evaluation described in the previous section. This simple transformation resulted in improvements in overall system performance, as shown in Table 3 in the (RFM+Flat) column.

On average, the transformation resulted in a 4.41% im-provement (p &lt; 0.01) in recall with no statistically-significant decrease in MAP . The transformation showed statistically-significant (p &lt; 0.01) improvements in precision at 100 doc-uments and higher, which is likely the result of the discrim-inatory spreading mentioned above. These results suggest that the approach has some merit; simply using the k th or-der statistic from a ranked-feature list as a normalizing value improves retrieval.
Relevance feedback is another extension that suggests the robustness of a model. Our goal in this experiment is not to demonstrate that relevance feedback works; this fact is well-studied in the literature. Our goal is to show that the ranked-feature fusion model is capable of supporting rele-vance feedback, and furthermore does so in a non-traditional manner that opens up new ways of thinking about retrieval algorithm design.

This approach is similar to the online hedge algorithm for ranked list fusion [2]. In external fusion of multiple search engine lists, [2] dynamically alters the weight assigned to each engine X  X  list in accordance with the number of rele-vant documents found while traversing that list. As certain ranked lists show themselves to be more  X  X rustworthy X  by containing larger numbers of relevant documents, a higher weight is given to the documents coming from those lists. This hedge algorithm uses loss functions and learning rates to update the mixture weights.

We adapt this intuition to the ranked feature fusion model in the same online manner. As the ranked list for a partic-ular feature (e.g. minmax dl ( q ) ) finds more relevant docu-ments than another feature X  X  list, weights change to favor that list. Documents that have already been seen cannot be re-ordered, but remaining unseen documents in the subsys-tem queues can. This produces a final ranking, the overall order in which the documents have been traversed.
However, instead of using loss functions and learning rates, there is another alternative. Recall from Equation 5 in Sec-tion 4.3 that the mixture weights for ranked feature subsys-tems are set using  X  idf ( q ) = IDF ( q ). This can be adapted when relevance information is available. A well-known ap-proach is [9]. R is the number of documents thus far known to be relevant to a topic and r q is the number of documents that contain query term q :
We use Equation 10 as the mixture weight for the minmax tf and minmax dl ( q ) by reinterpreting r q as r tf ( q ) and r These are the numbers of relevant documents in the ranked lists for feature tf ( q )and dl ( q ), respectively. Table 3 in the (RFM+Rel) column shows these improvements over the RFM baseline. Unsurprisingly, it works quite well; there is a 10-11% statistically significant (p &lt; 0.01) improvement in recall, MAP, and precision at various recall points. And, likely due to the online nature of the fusion, the more rel-evant documents found during the traversal of fused lists the better the fusion becomes. Again, the point here is not to demonstrate that relevance feedback works; it is to demonstrate that it works within the structure and form of ranked-feature fusion models.
Finally, we can combine the online relevance feedback of the previous section with the outlier flattening transforma-tion described in Section 6.1. One can be performed inde-pendently of the other; we now evaluate whether the effects are cumulative.

The results in the (RFM+Flat+Rel) column of Table 3 show that the combination of the flattening with relevance feedback outperforms either extension alone, and is better than a linear combination of improvements. This makes sense: One of the effects of flattening was to increase the dis-criminatory power between terms in a subsystem ranked list. Relevance feedback alters the mixture weight on a ranked list. A small shift in mixture weights therefore has a larger effect on the documents within a list.

The analyses in this section demonstrate two of the many possible extensions of the basic model. They also demon-strate how easily the extensions can be combined. These extensions improve the performance of the model and lead us to believe that the formalism is more than a clever hack that yields good results for a specific problem. It offers a new framework in which to think about the retrieval problem.
There are many possibilities for model extensions and im-provements. The previous section suggested a few of these and we now offer an overview of three directions future work may develop. These are alternative rankings, improved fu-sion, and additional features.
In this work documents were ranked by their literal term frequencies and document lengths. Even when normalized to the [1..1000] range, this ranking is still linear with respect to the underlying feature. Flattening was one way of intro-ducing non-linearity into the ranking function. Naturally, a more principled specification of the flattening function, e.g. learning the best threshold t , will need to be derived and tested experimentally. The point is simply that ranked-feature fusion offers a new way of thinking about retrieval algorithm design. Improvements to precision and recall are made not by tweaking a term weight; they are made by se-lecting an order statistic. In addition to flattening, it may also be possible to derive a continuous function based on feature-specific information-theoretic considerations.
We also note the abstract similarity of outlier flattening (Section 6.1) to pivoted document length normalization [14]. The act of choosing an order statistic as a renormalization point has the effect of pivoting not only document lengths minmax dl ( q ) but term frequencies as well minmax tf ( q is a lot of work around pivoted document length normal-ization that could be applied to the problem of choosing effective order statistic renormalization points.
This work used weighted CombSUM as a fusion mecha-nism. There are many more possibilities, such as Bayes Op-timal fusion, Condorcet fusion, CombMNZ, etc. Although we apply fusion at a different conceptual level than metasearch engines, existing research is applicable to this domain and better fusion strategies are likely available. At the very least, one next step using the current approach is to de-couple the  X  idf ( q ) parameter. Currently both minmax tf and minmax dl ( q ) retrieval subsystems use the same  X  idf It may be possible to decouple these parameters and use different weights on each list.
Many low-level features can be computed on documents, not just the tf and dl features used in this work. In gen-eral, any feature that discriminates between documents can be used for ranking. For example, the dl feature used in this paper was a function of the number of terms in a docu-ment. It is possible to use multiple forms of dl ,e.g. length as a function of sentences, paragraphs, text tiles [3], etc., to create separate ranked lists for each. The same can be done for tf : term frequency can be a function not of the number of words in that  X  X ontain X  t , but the number of sentences, paragraphs and text tiles that contain t .Adoc-ument that ranks highly by all four tf ( q ) subsystems, i.e. a document that outranks most other documents for term fre-quency, sentence frequency, paragraph frequency and text tile frequency is more likely to be relevant than a document that ranks high on term frequency but low on paragraph frequency. Multiple term frequency and document length metrics can improve overall rankings. The feature-rank fu-sion model can incorporate multiple types of tf , seamlessly and simultaneously.

Stemming, synonyms, hypernyms and hyponyms are also natural extensions of the feature list fusion model. For mor-phological variants (e.g. giraffe versus giraffes )inacombine-then-rank approach there is no difference between tf ( giraffe ) + tf ( giraffes )and tf ( giraffe  X  giraffes ). However, in a rank-then-combine model, separate ranked lists with very different distributional properties exist for tf ( giraffe ), for tf ( giraffes )andfor tf ( giraffe  X  giraffes ). The same doc-ument will not necessarily rank as high in one list as in another. Documents that rank high in all three lists are more likely to be relevant than documents that only rank high in a single list.

Proximity and co-occurrence o perators can be integrated as well. Documents can be ranked by the number of times query words co-occur within a given window. Alternatively, documents can be ranked on the proximity or density of query terms. Documents with a low spread will rank higher than those with a higher spread.
We described an information retrieval model based on fus-ing ranked lists computed from primitive features of docu-ments. We evaluated the effectiveness of this algorithm by comparing it with BM25: we replaced BM25 X  X  dl -normalized tf score with the fusion of tf and dl ranked lists and showed that there was no difference in system performance. We showed that term reweighting can be expressed as a func-tion of rank list renormalization based on order statistics, and that relevance feedback may be expressed as hedge fu-sion. Both extensions improved system performance, and the combination was better than a sum of individual im-provements.

The model we describe is conceptually simple, and unlike other models that use parameters to tweak system behavior, it relies on features whose values get relevance-ranked and fused. The value of a single feature is not relative to other features in the document in which it resides; it is relative to its relevance-ranked position in a list of documents that contain the same feature. This has two advantages: retrieval logic is separated from document structure and features can be customized to documents or collections. This model of-fers a flexible and fertile new foundation for retrieval algo-rithm design. [1] J. A. Aslam and M. Montague. Models for [2] J. A. Aslam, V. Pavlu, and R. Savell. A unified model [3] M. Hearst. Texttiling: Segmenting text into [4] J.-H. Lee. Analyses of multiple evidence combination. [5] D. Losada and L. Azzopardi. An analysis on [6] M. Montague and J. A. Aslam. Relevance score [7] M. Montague and J. A. Aslam. Condorcet fusion for [8] J. M. Ponte and W. B. Croft. A language modeling [9] S. Robertson, S. Walker, S. Jones, [10] S. E. Robertson and K. S. Jones. Simple, proven [11] G. Salton and C. Buckley. Term weighting approaches [12] G. Salton and M. McGill. Introduction to Modern [13] J. A. Shaw and E. A. Fox. Combination of multiple [14] A. Singhal, C. Buckley, and M. Mitra. Pivoted [15] C. G. M. Snoek, M. Worring, and A. W. M.
 [16] H. Turtle and W. B. Croft. Evaluation of an inference [17] J. Xu and H. Li. Adarank: a boosting algorithm for
