 1. Introduction
The enforced hill climbing (EHC) method, a heuristic search strategy which has been employed in a number of recent AI planners, was originally introduced in the Fast Forward (FF) planning system ( Hoffmann and Nebel, 2001 ). By using EHC and some other techniques, such as relaxation and helpful action pruning ( Russell and Norvig, 2003; Hoffmann and Nebel, 2001 ), FF came first in the fully automatic track of the 2nd International Planning Competition (IPC). In a similar fashion, the Metric-FF ( Hoffmann, 2003 ), a descendant of FF, was a top performer in STRIPS and simple numeric tracks of the 3rd IPC. EHC was a useful search strategy in these planning systems due to its simplicity and efficiency.

In addition to FF and Metric-FF, a number of other planning systems, such as Conformant-FF ( Hoffmann and Brafman, 2006 ), Probabilistic-FF ( Domshlak and Hoffmann, 2007 ), MACRO-FF ( Botea et al., 2005 ), MARVIN ( Coles and Smith, 2007 ), POND ( Bryce, 2006 ), and recently JAVAFF ( Coles et al., 2008 ) and FF ( Keyder and Geffner, 2008 ) have employed EHC as their main search strategy. EHC has also been employed in applications such as web service composition ( Meyer and Weske, 2006; Lautenba-cher and Bauer, 2007; Hoffmann et al., 2009 ) and real time robot planning ( Ulusar and Akin, 2006 ). Thus, EHC is a widely applicable search technique, the improvement of which can benefit many planning systems.

One of the main challenges in a heuristic search algorithm is the proper selection of the next node to expand. Since state evaluation is costly, EHC non-deterministically searches for the first better next state (if there exists any) instead of the best successor state. In the case of a local minimum, EHC searches the next layer of successors in a breadth-first order. This blind node selection can reduce the efficiency of EHC, especially in domains with high branching factors and costly distance estimate functions.

To improve the efficiency of EHC, we have developed a new hill climbing strategy called guided enforced hill climbing (GHC), which employs an ordering function to propose more promising successor states, facilitating earlier selection of more valuable next states. Like EHC, GHC uses a distance heuristic function to evaluate successor states, but according to the order proposed by the ordering function. The idea of using information from previous states of the search has also been used in other research areas such as Constraint Programming ( Boussemart et al., 2004; Grimes and Wallace, 2007; Stergiou, 2009 ). Comparing the results of EHC and GHC in several STRIPS planning domains shows that in many domains, GHC explores much fewer states. GHC is also faster than EHC, especially when tackling larger problems.
A preliminary version of this work was presented in Akramifar and Ghassem-sani (2009) . However, in this paper we explain about several different aspects of our proposed algorithms in more details. These include a detailed account of the ordering function, the complexity analysis of the presented algorithms, and the results of additional experiments that have been performed.
The remainder of this paper is organized as follows. Section 2 includes a brief review of ordinary EHC to provide the necessary background. Section 3 details our new approach to enforced hill climbing, and Section 4 explains the adaptive branch ordering that is employed by GHC. Section 5 provides the results of several experiments demonstrating the advantages of GHC. This section also compares the results of EHC with that of two variants of GHC, applied to a set of benchmark domains. In this section, we also provide a comparison of GHC and two other related approaches.
The last section concludes. 2. Background
In this section, we present a brief description of the EHC method and the state evaluation process of the FF planning system. 2.1. Enforced hill climbing (EHC)
Hill climbing has been widely used in artificial intelligence areas such as AI planning ( Ghallab et al., 2004 ), machine learning ( Mitchell, 1997 ), and optimization ( Rich and Knight, 1991; Russell and Norvig, 2003 ). It attempts to minimize (or maximize) a function h ( s ), where s are discrete search states. These states are typically represented by vertices (or nodes) in a graph, where edges (or actions) in the graph indicate nearness or similarity of vertices. Hill climbing traces the search graph, vertex by vertex, locally decreasing (or increasing) the value of h , until a local minimum (or maximum) state is reached.

The enforced form of hill climbing is a combination of hill climbing and breadth-first search. It can also be regarded as an extension of ordinary hill climbing, proposed to tackle the main problem of hill climbing, i.e., getting trapped in local minima. In each planning state s c , hill climbing only searches the immediate successor states of s c . In the case of facing a local minimum, ordinary hill climbing uses techniques such as random restart ( Russell and Norvig, 2003 ) to resolve the problem. However, until an appropriate state (i.e., exit node) is found, enforced hill climbing searches the next layer of successor states in a breadth-first order.

To better understand the behavior of EHC, formal representa-tions of a planning problem and the EHC algorithm are needed. A detailed account of the algorithm was initially given in Hoffmann and Nebel (2001) . Here, we introduce a recursive representation of EHC, which will be later used to represent GHC.

In the STRIPS representation ( Fikes and Nilsson, 1971 ), a are, respectively, the initial state, domain actions, and problem goals. Each action a is represented by three sets of propositions: a where del  X  a  X  D pre  X  a  X  . A state s is defined as a finite set of propositions. An action a is said to be applicable to a state s if pre  X  a  X  D s . The result of applying a to s is defined as: by a set of propositions G . The planning task is to find a sequence resulting state will contain goals G .

Fig. 1 shows the recursive form of the EHC search algorithm. The input to the algorithm is a planning problem
P  X  / state s , actions A , goals G S . EHC includes three main functions: h (lines 1, 5), successors _ of (lines 2, 8), and pop _ front _ of (line 4). The function h ( s , A , G ) is a distance estimate function. Inputs to the function are state s , action set and goal set G . It returns a distance value between s and a state containing G , using some actions from A . In Section 2.2, we provide a detailed account of the distance estimate function of
EHC. The function successors _ of(s) returns the successor states of the input state s . A successor state s i of a search state s , action a to state s . The function pop _ front _ of(Open _ the standard list pop function that removes and returns the first item of Open _ list .

Without line 8, EHC Algorithm 1 turns into ordinary hill climbing. It replaces the current state by the first better successor state in a recursive call to the main algorithm (line 7). Since hill climbing is an incomplete search strategy, when facing a local minimum, it cannot find a solution, and returns a failure (line 9).
Without line 7, EHC Algorithm 1 turns into breadth-first search. EHC tackles the problem of local minima by switching to breadth-first search when facing a local minimum. If the evaluation of a state shows that it is not better than the current state, all of its direct successors will be added to the end of
Open _ list (line 8). Hence, when all direct successors are evaluated and none is better than the current state, EHC will start evaluating the successors of successors. This will be continued (lines 3 X 8) until either a better state is found (line 6) or Open _ list becomes empty, in which case a failure is reported (line 9).

The main limitation of EHC is that it gets trapped in a dead-end in certain domains. If a planning task contains a state through which some goals are unachievable (i.e., a dead-end state), EHC may fail to find a solution. In such cases, EHC-based planners usually invoke a complete heuristic search (i.e., best first) to solve the task from scratch. 2.2. State evaluation
The main task in state evaluation is distance estimation, which is an approximation of the distance between a given state and a goal state. As it was mentioned earlier, EHC estimates a goal distance through the function h . Given a state, function h provides the search with a goal distance estimate, and additionally with a set of promising successors of the state which is called helpful actions ( Hoffmann and Nebel, 2001 ).
 To estimate goal distance, h uses some relaxed planning tasks.
A relaxed planning task A 0 can be obtained by ignoring the delete lists of all actions of A . Using relaxed planning tasks, the estimation process is performed in two steps. First, a relaxed planning graph is constructed in the forward direction, and then a relaxed plan is extracted in the backward direction. The length of the relaxed plan (i.e., the number of actions) would be used as a distance estimate.
 The cost of planning graph construction for each state s is and j A 0 j is the number of relaxed actions in the domain; the cost of relaxed plan extraction is O  X j A 0 j X  ( Hoffmann and Nebel, 2001 ). If EHC explores n states to solve a problem, the total cost will be O  X  n j s j X  n l j A 0 j X  , where j s j is the average number of proposi-tions in each state.

To speed up the search, EHC incorporates an action pruning technique, by which only a subset of the applicable actions of each state, i.e., helpful actions, are examined. Actions of the first step of the generated relaxed plan are regarded as being helpful. Although this technique reduces the branching factor, it does not preserve the completeness of the search ( Hoffmann and Nebel, 2001 ).

FF uses a technique to reorder helpful actions before append-ing successor states to Open_list. For this purpose, it counts the number of goals and subgoals deleted by each helpful action (i.e., propositions in level 0 of the relaxed planning graph). Helpful actions which delete fewer number of goals and subgoals will precede those which delete more.
 The helpful action ordering used by FF is just a partial ordering. It first reorders the helpful actions of the state currently being evaluated. It then appends the states produced by the helpful actions to the end of Open_list, in the order of production. In Open_list, the newly appended states reside after all the existing states. In other words, the sequence of those states already in Open_list is not altered by adding the new states. Since the resulting Open_list is not totally ordered, helpful ordering is more appropriate for usual steps of hill climbing with a more promising state among immediate successors.

Furthermore, ordering helpful actions is a suitable technique for smaller problems. However, as the problem size (i.e., number of domain actions, goals and domain propositions) increases, the effectiveness of this technique decreases. This is because larger problems usually have larger states and evaluation of such states normally leads to the more helpful actions. Consequently, the number of helpful actions with the same priority will increase. This in turn increases the number of choices in the non-deterministic selection of the actions to be examined in subsequent steps. Hence, the helpful-action ordering technique is less effective in larger problems.
 State evaluation is the most time-consuming part of EHC. There are two possible approaches for reducing the overall time spent for state evaluation: (1) creating a faster state evaluation module, or (2) reducing the number of states to be evaluated. In the next section, we introduce a new method called guided enforced hill climbing (GHC), which is based on the latter approach. 3. Guided enforced hill climbing
Guided enforced hill climbing is an extension of enforced hill climbing. GHC includes two important modifications to EHC: ordering the expansion of successor states according to an ordering measure, and obtaining information from each evaluated state to improve the ordering measure.

Ordering the successor nodes is an effective way of reducing the total cost of the search. Since state evaluation is the most time consuming part of EHC, generally, any decrease in the total number of evaluated states will decrease the search time. An appropriate ordering will decrease the number of state evalua-tions by suggesting the more promising states first.

The adaptive branch ordering technique that we propose consists of two complementary parts. The ordering part reorders all the successors according to an ordering measure d . To have an appropriate ordering, d should be as precise as possible. A proper approach to obtaining such a measure is to use some contextual (domain) information from the search process. Therefore, the second part of adaptive branch ordering in GHC is based on the information acquired from state evaluation efforts. As it was mentioned in Section 2.2, the cost of state evaluation for each states is O  X j s j X  l j A 0 j X  .

The ordering measure of GHC is based on the history of actions during the search. In fact, an action is an important part of a planning search space. To produce a promising state during the search, some actions succeed and some fail; some actions are applicable on some search states, some of which are also helpful. Therefore, in each search state, some actions have a history that shows the performance of actions in previous search steps.
Since d is an action-based measure, adaptive branch ordering reorders the helpful actions of each search state. Based on this action ordering, the resulting states will also be reordered.
In the next sections, GHC X  X  main algorithm and the adaptive branch ordering technique used are described. 3.1. GHC algorithm
Fig. 2 shows the main algorithm of GHC. This algorithm is similar to that of EHC, except for using of two new functions: pop _ best _ of (line 4) and update _ ordering _ measure (line 9). Function pop _ best _ of replaces for function pop _ front EHC. Unlike pop _ front _ of , which selects the first element of
Open _ list , pop _ best _ of selects the most appropriate state of
Open _ list based on the ordering measure which is calculated from the information obtained from previously evaluated states. pop _ best _ of is the ordering part of adaptive branch ordering. Another part of adaptive branch ordering, i.e., update ordering _ measure , is a learning function that acquires information from each evaluation effort of the next _ state in s .

In this paper, we have considered two variants for GHC, represented by GHC br and GHC be . Similar to EHC, GHC br combination of hill climbing and breadth-first search; whereas GHC be employs a best-first search strategy when facing a local minimum. Applying a best-first search in EHC was first used in MARVIN ( Coles and Smith, 2007 ). Nonetheless, the best-first search of GHC be is different from that of MARVIN in that the former uses previously acquired information, whereas the latter reuses results of the main distance heuristic function h to suggest the best node.

The algorithm of GHC be is the same as that of GHC br , except for the inputs to function pop _ best _ of . In GHC br , the input to this function is the set of those states in Open _ list that are at the closest level to the root node in the search tree (i.e., a breadth-first-style Open _ list ); whereas the input to the function in GHC be is the set of all the states in Open _ list (i.e., a best-first style Open _ list ).

Based on the results obtained from the adaptive branch ordering function, GHC be may even recommend the evaluation of a successor at a level farther from the current state prior to the evaluation of some of its immediate successors. For example, suppose s 1 and s 2 are two immediate successors of the current state s c . These states are appended to Open _ list of s explored next. Also, suppose s 11 and s 12 are two successors of s both EHC and GHC be evaluate s 1 first, and if the distance value of s is estimated to be more than that of s c , both EHC and GHC append s 11 and s 12 to the end of Open _ list . EHC evaluates the states in Open _ list in a breadth-first order. Thus, it will evaluate state s 2 next; whereas based on the distance heuristic function,
GHC be evaluates states in the order suggested by the function pop _ best _ of , and it may well be the case that s 12 , for example, would be evaluated before s 2 and s 11 .

To work with non-immediate successor states, we define the notion of an enforced successor state as a reachable state in the search tree. A state s i , is an enforced successor state of a state, s of s , or a successor of an enforced successor of s c . In other words, s is a direct or indirect successor of s c , produced by a i
For each enforced successor s i of s c , we also define Producer ( s as the first action that produces s i in a part of planning search space starting from s c . 3.2. The adaptive branch ordering function
The adaptive branch ordering function of GHC guides the enforced hill climbing process to a potentially appropriate node.
In other words, GHC evaluates successor states in the order proposed by the function. For example, suppose that s i and s two states of the Open _ list of state s c , and the two actions a a are producers of s i and s j , respectively. According to the ordering measure d , whenever a i is more promising than a j , GHC will evaluate s i before s j . Subsequently, if the estimated distance of s less than that of s c (i.e., h  X  s i  X  o h  X  s c  X  ), s i current state; otherwise, GHC appends the successors of s
Open _ list of s c .

In the following section, an appropriate heuristic measure named least-failed-first is introduced. Using other measures is also possible. For instance, two other alternatives (i.e., a measure based on the number of action appearances in previous relaxed plans, and a measure based on the number of action appearances in previous helpful actions) have been presented in Akramifar and
Ghassem-sani (2007) . In the case of the first alternative, actions that have more occurrences in previous relaxed plans are considered to be more appropriate. Similarly, for the second alternative, actions that have more occurrences in previous helpful actions are considered more appropriate.

We have selected the least-failed-first heuristic, because of its less computational overhead in a number of planning domains.
The overhead of creating the least-failed-first measure (i.e., O (1) for each search step) is much less than that of the other two, i.e., O (number of helpful actions), and O (number of actions in the relaxed plan), in each search step. In other words, the least-failed-first heuristic is more informative and cost-effective than both relaxed-plan-counting-based and helpful-action-counting-based heuristics.

To improve the time efficiency of GHC, d needs to be a very low-cost and highly accurate function. There is a trade-off between efficiency and accuracy. This trade-off has to be considered when creating an efficient guided enforced hill climber. One way to minimize the overhead is to use, as much as possible, the past information that has directed the planner to its current state.

Even if the output of an ordering function in the GHC algorithm is really an appropriate successor state, it needs to be verified and accepted by the distance heuristic function h . We regard a state s proposed by d as an acceptable state, if h ( s i ) is less than h ( s
Otherwise, state s i is regarded as an unacceptable state. Therefore, we should only focus on an ordering measure that would mainly rely on the distance estimate function, h . We define the acceptability value of a state s i as V ( s c , s i )  X  h ( s which s i is a subsequent state of the current plan state s 4. The least-failed-first heuristic
The least-failed-first heuristic is based on a negative reward assignment approach. During the search, this measure is created through the assignment of some negative weights to those actions that produce unacceptable states. The main intuition behind this measure is that if the previous states generated by an action, a , were often unacceptable, new states generated by a are likely to be unacceptable, too. This measure will then be used to reorder the successors of each search state.

An action that has failed to produce a promising state after being applied to some search states, may also fail in subsequent states. Since the application of an action to a search state s usually changes only a few propositions, a successor state t is usually acceptable successor of s (i.e., h  X  t  X  o h  X  s  X  ), and an action a applicable to both states, the states resulting from applying a to s and t , i.e., s i  X  Result ( s , a i ) and t i  X  Result ( t , a similarities. Thus, if s i is an unacceptable successor of s (i.e., Fig. 3 ).

In other words, if the following relations hold: h  X  t  X  o h  X  s  X  and h  X  s  X  r h  X  s i  X  then h  X  t  X  o h  X  s i  X  Since t is considered to be a highly similar successor of s , its will be approximately equal to h ( s i ), and it may be also the case that h  X  t  X  o h  X  t i  X  An action a i that has failed in some state s , may also fail in a subsequent state t . Therefore, the least-failed-first heuristic may propose more appropriate states. We will return to the definition of this function after giving the definition of the action-failure-weight measure. The failure weight of an action a i is correlated to the number of unacceptable states produced by a i during previous search iterations. Formally: Definition 1 ( Action failure weight ). In any intermediate state s of search, the failure weight of an action, a i , that produces an enforced successor state s i is recursively defined as f new  X  a i  X  X  f old  X  a i  X  X  h  X  s c  X  h  X  s i  X  1  X  if  X  T  X  where T  X  X  h  X  s i  X  4 h  X  s c  X  X  and s i  X  ^ R esult  X  s c , a
The failure weight of all the actions is initially set to zero. These would then be updated during the search. Thus, GHC tunes its ordering measure d during the search by using action failure weights. Based on Definition 1, in any state of the search, the failure weights for producer actions of all unacceptable successor states will be increased. We use the acceptability value V ( s change the failure weight, because it reflects the error scale of the distance estimation.

The function update _ ordering _ measure of the GHC algo-rithm ( Fig. 2 ) is based on Definition 1. In the each iteration, GHC updates the failure weight of one grounded action only. Since the time complexity of weight updating in each evaluation process is O (1), the function update _ ordering _ measure does not greatly increase the cost of GHC.
 Definition 2 ( Least failed first heuristic ). In any intermediate state s of planning a problem, the least failed first heuristic, d defined as follows: d  X  s c  X  X  a i , iff ( a i A Producer  X  Open _ List  X  s c  X  X  and 8 a j A Producer  X  Open _ List  X  s c  X  X ) f  X  a i  X  r f  X  a j  X  X  2  X  Here, we have supposed that the Producer of a set of states is a set of actions. In each planning state, the least-failed-first ordering function determines the successor state with the highest priority, to then be evaluated by the main heuristic function h . 4.1. Complexity of the algorithm Function pop _ best _ of of GHC ( Fig. 2 line 4) is based on Definition 2. Since this function finds the most appropriate state from Open _ list in each iteration, its time complexity in the worst case (i.e., for the GHC be method) is O  X j Open _ list j X  whereas in EHC, the time complexity of function pop _ front is O (1). The extra cost of GHC is much less than the cost of distance estimate function (i.e., O  X j s j X  l j A 0 j X  ).
The time complexity of function update _ ordering _ measure ( Fig. 2 line 9) is O (1). That is because in each search state this function updates the weights of only one failed action.
Suppose that in order to solve a problem, GHC explores m states, then the total time complexity would be complexity of GHC is O  X  m j s j X  m l j A 0 j X  . As it was mentioned in Section 2.2, the total cost of EHC is O  X  n j s j X  n l j A 0 j X  . Comparing these two methods, because GHC, due to using its extra ordering function, explores fewer states (i.e., m o n ), it will be faster than EHC in solving planning problems.

Since each action has a failure weight, the extra memory cost of GHC is O  X j A 0 j X  . Therefore, the GHC function uses a bounded memory size with the aid of the least-failed-first heuristic. The size of extra memory remains unchanged during the search. 4.2. An example
We will demonstrate the main idea of GHC by considering a small problem from the Logistics domain and provide a comparison between EHC and GHC based on their behaviors in planning this problem. Suppose that there are two cities, C1 and C2. C1 has one location L1 and one airport Ap1, and C2 has only one airport Ap2. C1 contains a truck T1 that can move between L1 and Ap1. There is also one airplane, Aip, which can move between airports. The trucks and the airplane can load and unload packages in every location that they visit. In the initial state, one package is located at Ap2, the truck is located at L1, and the airplane is located at Ap1. The goal is to move the package to L1. Fig. 4 shows the problem in PDDL ( McDermott, 1998 ).
Fig. 5 shows the detailed account of the planning process for this small problem using EHC. Horizontal moves were created using hill climbing, and vertical moves were created using breadth-first search. Details of each state are demonstrated in the lower part of the figure. The nodes of the tree in the figure demonstrate states, and the edges demonstrate actions. The actions that have resulted in a previously evaluated state of the breadth-first search tree, e.g., A -5 in state S 1 , have been omitted, and only helpful actions have been demonstrated.

Both of the plans generated by EHC and GHC include the following eight actions: A -12: (FLY-AIRPLANE Aip Ap1 Ap2) A -10: (LOAD-AIRPLANE Obj Aip Ap2) A -5: (FLY-AIRPLANE Aip Ap2 Ap1) A -8: (UNLOAD-AIRPLANE Obj Aip Ap1) A -14: (DRIVE-TRUCK T1 L1 Ap1 C1) A -4: (LOAD-TRUCK Obj T1 Ap1) A -7: (DRIVE-TRUCK T1 Ap1 L1 C1) A -2: (UNLOAD-TRUCK Obj T1 L1) For planning this small problem, EHC explores 11 states, whereas GHC explores 10. In other words, for this problem, GHC and EHC explore the same states except for S 5 , which is not explored by GHC. To find a state better than S 4 , EHC explores S 5 before S Since the estimated distance of S 5 is not less than that of S then evaluates S 6 (produced by A -8).
 On the other hand, the function pop _ best _ of of GHC reorders
A -8 before A -14 when in state S 4 , based on the least-failed-first measure. As a result, GHC finds a more promising state in the very first try, before the expansion of A -14. This advantageous reordering is due to a negative history obtained from state S 0 , in which applying A -14 produced the unacceptable state S 2 . GHC reduces the priority of A -14 in subsequent states, through the function update _ ordering _ measure ( Fig. 2 , line 9).

Without helpful action ordering, EHC may explore even more states. For instance, in states S 1 and S 3 , EHC may inappropriately expand A -14 first. In both S 1 and S 3 , the states produced by A -14 is not more promising than the preceding state; whereas using the history heuristic, GHC assigns a low priority to A -14 and saves the extra effort spent by EHC on S 1 and S 3 .

There are some important points in this example: (1) Action A -14, which was an applicable and helpful action in S was also applicable and helpful in four subsequent states.
Therefore, subsequent states are similar to the actions applicable. (2) The distance value of each of the states produced by A -14 was not less than that of its predecessor state S 0 , and this is also the case in subsequent states. Therefore, the history of actions in one state can be used in exploring subsequent states. (3) This example was just a very small problem with a low number These points highlight the main ideas behind GHC. 5. Experiments
In order to evaluate the performance of our method, we have applied EHC and the two variants of GHC (i.e., GHC br and GHC to several different planning benchmarks and have compared the results. All configurations were tested on the same set of problems under identical conditions (hardware, software, and planning system). The software and hardware used included a Linux Fedora core on a Pentium(R) 4 CPU running at 2.40GHz, with 1GB of main memory. To obtain a comparison between different search strategies on standard planning domains, we augmented an existing EHC based planner, i.e., FF, as the test-bed planner.

In some domains, EHC (and consequently, GHC br and GHC be fails to solve certain problems. If a planning task contains a state through which some goals are unachievable (i.e., a dead-end state), a hill-climbing search method (and consequently EHC) may fail to find a solution. Since EHC for being highly efficient explores a subset of applicable actions named helpful actions, its search space may be incomplete. Therefore, enforced hill climbing may run into a dead end. In such cases, FF uses a complementary best-first algorithm to deal with these special cases.

FF also employs a goal ordering technique in the preprocessing phase of planning. By means of the goal ordering technique, in some domains, FF distributes the goals into ordered clusters. Then it uses EHC to solve each single class in the determined order. In other words, FF invokes EHC iteratively according to the number of goal clusters. Although the goal ordering increases the efficiency of the planner, it is a separate technique, and not a part of EHC itself. Therefore, to obtain a comparison with the original EHC search method, we have ignored the goal ordering task of FF.

Our main objectives for the experiments were to evaluate the effect of adaptive branch ordering on the performance of enforced hill climbing using breadth-first and best-first search strategies. 5.1. Benchmarks
The benchmark set used included many of the STRIPS planning problems from the fully automated tracks of the first five international planning competitions, hosted by AIPS 1998, AIPS 2000, 2 AIPS 2002, 3 ICAPS 2004, 4 and ICAPS 2006. 5 of benchmark domains is presented in Table 1 . In total, the benchmark suite consisted of 633 problems.
 We applied FF with three different search strategies (i.e., EHC, GHC be , and GHC br ) to all the 633 benchmark problems. Table 2 presents the number of unsolved problems in each domain. The column  X  X #failed X  X  demonstrates the number of failed problems. Since enforced hill climbing is an incomplete search strategy, it may fail to solve some problems. In order to preserve its completeness, in such cases, FF starts from the beginning and tries to solve the problem this time with a best first search. This strategy was preserved in all three search methods that we have compared in our tests.

The column  X  X #exceeded X  X  demonstrates the number of pro-blems that could not be solved in the maximum allowed time (i.e., 10min). The number of unsolved problems are equal to the number of failed and exceeded problems in each domain. According to the results in Table 2 , in some domains, EHC, GHC br , and GHC be could not solve many of the problems. Therefore, none of these enforced hill climbing variants seems to be an appropriate search strategy for such domains. To perform a more precise comparison, we have chosen the domains in which each configuration has solved more than 50% of the problems. These domains are MPRIME from IPC1, FREECELL and LOGISTICS from IPC2, DEPOTS from IPC3, OPTICAL-TELEGRAPH and SATELLITE from IPC4, and OPENSTACKS, PATHWAYS, ROVERS, and
TPP from IPC5. 5.2. Results
The results of applying the search methods to the benchmark problems have been demonstrated in Figs. 6 X 8 . In the left hand side of the figures, the horizontal axises show the problem number, and the vertical axises display the CPU time on a logarithmic scale. In the right hand side of the figures, the horizontal axises again show the problem number, and the vertical axises depict the total number of actions in generated plans.

Since the impacts of the ordering function cannot be determined in the failure cases (i.e., problems that the base planner fails to solve), we have not used such problems in the comparisons. Also, Table 3 presents the average planning times and plan lengths for each domain. Average planning time can be a useful indicator for planning efficiency and average plan length (in terms of number of actions in plans) can be a useful indicator for quality of produced plans.

As it is presented in Table 3 , in all domains with the exception of PATHWAYS, GHC be is more efficient than the other two. In addition, in all domains GHC br is at least as good as EHC. As it can be seen in Fig. 7 , the poor performance of GHC be in PATHWAYS is a consequence of its inefficiency in solving problem 24, which is an exception in this domain. Without this problem, the average planning time for EHC is 0.7s, which is significantly smaller than that of the other two: 1.5s for EHC and 1.6s for GHC br . 5.3. Discussion
In this section, we discuss about the relation between the two domain features and the performance of GHC. The first feature is the average branching factor of the domain  X  bf  X  . As we know, GHC X  X  main goal is to order applicable actions for each state. So there maybe a relation between the average number of possible actions per state in a domain (that is bf ) and the performance of the applied algorithm.

GHC reuses information acquired in a state for its successor states. Clearly, this information will be applicable only if successor states are similar enough to their predecessors. In other words, if two states are similar, one can expect that the same actions could be recommended for both of them. Thus, the average similarity of subsequent states in a domain is the other feature that we would like to analyse its impact on the performance of GHC.

For this goal, we use the following definition of the similarity between two states s and t . similarity  X  s , t  X  X  j s \ t j Then the average similarity measure for a problem  X  similarity  X  can be computed as the average of similarities between any two subsequent states s and t that the algorithm processes during its search.

Table 4 presents the average similarity and average branching factor for all the problems solved using EHC. Rows are ordered according to the order of domains in Figs. 6 X 8 .
 As presented in the table, the three domains MPRIME, TPP, and OPENSTACKS, had the least average branching factors. On the other hand, the three domains SATELLITE, LOGISTICS, and OPTICAL-TELEGRAPH, had the highest average branching factors.
According to the experiments, the adaptive branch ordering function d lff , used in GHC br along with a breadth-first search, did not significantly improve the efficiency of EHC in the first three domains. An informed branch ordering with breadth-first search is more effective in states with high branching factors. In such states, many successors will be pruned through using the informed branch ordering function, thereby decreasing the number of evaluated states. On the other hand, in states with low branching factors, even though the function orders the successors appropriately, not only little improvement is achieved, but also the computational overhead of the function can outweigh its improvements. Therefore, the adaptive branch ordering function of GHC br did not significantly improve EHC in the benchmark domains MPRIME, TPP, and OPENSTACKS.

The adaptive branch ordering function of GHC br fared differ-ently in the three domains with the highest branching factors. According to the experiments, this function significantly im-proved the efficiency of EHC in the LOGISTICS and OPTICAL-TELEGRAPH domains; whereas improvement was not significant in the SATELLITE domain. Although branching factor is important for the ordering part of the function, state similarity ratio is important for the accuracy of the ordering measure. In other words, the ordering measure has a lower accuracy in domains which lack highly similar states. Comparing the three domains,
SATELLITE has a low state similarity; whereas the other two enjoy a high degree of average state similarity. Therefore, the efficiency improvement in EHC through the adaptive branch ordering d was not as significant in SATELLITE as in the other two domains.
In addition, the GHC be method, which uses the adaptive branch ordering d lff with a best-first search, performed almost as good as
GHC br in the SATELLITE domain, proving the low accuracy of the ordering measure in this domain.

In the remaining six domains, adaptive branch ordering d lff improved the efficiency of EHC. Overall, the d lff function with breadth-first search used in EHC is appropriate for problems with high branching factors and highly similar states.

According to the experiments, the d lff method with a best-first search used in GHC be also improved the efficiency of EHC in many domains, even in the TPP and OPENSTACKS domains, which have low branching factors. Since GHC be can search deep into the search tree (through the best-first search), it can fare better than
EHC in local minima where none of the immediate successors are more promising. Therefore, in this method, the accuracy of the ordering measure is more important than the branching factor.
In only one of the domains, i.e. MPRIME, the adaptive ordering function with a best-first search did not improve the efficiency of EHC. It seems that the low similarity ratio of this domain is the main reason behind this. Besides, MPRIME had a low branching factor, further decreasing the effectiveness of the ordering function. 5.4. Comparison with other approaches
In this section, the least-failed-first heuristic is compared with two other well-known heuristics: the least-bad-first heuristic ( Coles and Smith, 2007 ), and the history-heuristic ( Schaeffer, 1989 ).

Using the best-first search strategy on local minima, instead of the breadth-first search strategy of EHC, was first introduced in MARVIN ( Coles and Smith, 2007 ). The notion of best refers to the successor with the heuristic value closest to that of the parent state. The heuristic was called least-bad-first  X  d lbf  X  because no chosen successor can improve on the heuristic, but some choices are less negative than others.

There are some similarities and differences between GHC be MARVIN X  X  EHC search strategy. Both incorporate a best-first search into enforced hill climbing. In addition, both employ the main distance heuristic function h of FF. However, GHC be MARVIN X  X  EHC search strategy are different in that GHC be adaptive branch ordering measure d (called least-failed-first: d )  X  which is based on using the planning history  X  to order the successor states. In other words, d lff is different from d the former uses previously acquired information about actions, whereas the latter reuses results of the main distance heuristic function h to suggest the best state.

The second related work is the history heuristic , which enhances the alpha X  X eta algorithm by reducing the size of minimax trees. The history heuristic improves the order in which branches are considered at interior nodes of the search tree. This heuristic maintains information on whether there is a correlation between an action and any success that the action has had in achieving a goal.

The least-failed-first ordering is different from the history heuristic ( hh ), because the former uses negative histories as the ordering measure  X  d lff  X  , while the latter employs positive histories failed actions is equal to increasing the priority of unevaluated or successful actions. Therefore, d lff can be more general than d which increases the priority of successful actions only. 5.4.1. Comparison with the least-bad-first heuristic
In order to obtain a comparison between d lff and d lbf ,we applied these heuristics in GHC be as ordering measures. Table 5 presents the overall results of applying GHC be on the benchmark domains through d lff and d lbf . The results are shown in terms of: (1) the number of problems solved by each approach, (2) the average time (in s) taken to solve one problem, and (3) the average plan length. Instances solved by both methods are used to calculate 2 and 3. Using d lbf resulted in a faster planning in only two of the domains: MPRIME and PATHWAYS; whereas d lff was superior in the other domains. Furthermore, 359 problems were solved through d lbf , whereas 407 problems were solved through d .

When using d lbf in the MPRIME domain, nine of the larger problems were not solved in the predetermined time limit (i.e., 600s), demonstrating that d lbf is appropriate for the smaller problems of this domain, whereas d lff is more appropriate for larger ones.

In PATHWAYS, the average time of applying d lff is biased towards the time of solving problem 24, which is an exception. Without considering this problem, the average times using d and d lff were 0.1 and 0.6s, respectively. Although the average time of using d lbf is more promising than that of d lff , their difference has been reduced. Since the maximum time for solving a problem of this domain was 76s for problem 24 using d lff , the benchmark problems seem to have been rather small. Based on experiments on larger instances, it can be predicted that d lff will perform as well as d lbf in the larger problems of this domain.

In many of the domains; the average length of plans found by d lbf was slightly smaller than that of d lff . A detailed tracing of the search process shows that larger problems have several states with least-bad estimations. Furthermore, in such states, the distance estimate function of many states returns an estimate equal to that of their parents. Therefore, d lbf will expand the least-bad-first nodes at levels of the search tree that are closer to the root. In these cases, d lbf does not have a tendency to move in depth, whereas d lff is independent from function h , and may search deep into the search tree. Since finding an appropriate state deep inside a search tree increases the number of plan actions, the plan quality of GHC be when using d lff is slightly inferior to that of GHC be when using d lbf . Conversely, the time efficiency of using d lff is more promising than that of using d 5.4.2. Comparison with the history heuristic
To evaluate d hh against d lff , we have added d hh to GHC as d lff . According to d hh , actions that have had more success in previous states are considered more appropriate; whereas in d the least failed actions (introduced in Section 4) are considered more appropriate.

It is evident from the experimental results that in many of the domains, applying GHC be through d lff has been faster than GHC through d hh . As presented in Table 6 , only in three of the domains (i.e., PATHWAYS, and OPENSTACKS) the overall results of using d were slightly better than using d lff ; whereas in other domains, d fared significantly better than d hh . For instance, in domains
OPTICAL-TELEGRAPH, TPP, and ROVERS, searching by d lff was, respectively, 355.5, 24.2, and 15.2 times faster than searching by d
In PATHWAYS benchmark problems, it seems that d hh is more effective than d lff . However, average searching times by d biased towards the time of problem 24, which is an exception.
Without considering this problem, average searching time using d is 0.7 second, whereas average searching time using d hh is 1.8s. Therefore, in PATHWAYS, d hh is not more appropriate
The best performance observed for d hh was in the OPENSTACKS domain. According to the results of applying EHC in this domain, d (like d lff ) was much faster than EHC. In fact, the d hh appropriate for those search spaces that include a number of frequently used actions. As presented in Schaeffer (1989) , d is appropriate for game domains which include repeatedly used actions (such as  X  X  X ove X  X ). In OPENSTACKS, a grounded action  X  X  X pen_new_stack X  X  can be repeatedly used. For instance, the generated plan for the first benchmark of OPNESTACKS included 25 actions, in which  X  X  X pen_new_stack X  X  was repeated five times.
Also, the plan for problem 14 had 100 actions, including 20 occurrences of  X  X  X pen_new_stack X  X .

Overall, d lff had a better performance than d hh in the bench-mark domains. In our benchmark domains, the frequency of grounded actions in the plans generated was relatively low, except for OPENSTACKS. Hence, d hh does not significantly improve
EHC in these domains. d lff is appropriate for domains in which d is appropriate (i.e., OPENSTACKS and PATHWAYS). On the other hand, d hh is not necessarily appropriate for domains in which d is appropriate. 6. Conclusion
Enforced hill climbing is the main search strategy of the fast forward planner (FF). To find a more appropriate next state, EHC examines successor states according to a partial ordering technique. We have implemented a new extension to EHC, which we call guided enforced hill climbing (GHC). GHC evaluates successor states in the order determined by an adaptive branch ordering function. We have also introduced a typical ordering measure, called the least-failed-first heuristic. The least-failed-first heuristic, d lff , keeps track of state evaluation efforts and learns from the failures. This is a domain-independent heuristic, and our experiments have shown that it is appropriate for various domains including LOGISTICS, OPENSTACKS, DEPOTS, FREECELL, OPTICAL-TELEGRAPH, PATHWAYES, ROVERS, and TPP.

We have tested two different forms of GHC on a large number of standard planning benchmarks. The two forms of GHC are:
GHC br , which uses a breadth-first search with hill-climbing, and
GHC be , which uses a best-first search with hill-climbing. The experimental results in several planning domains demonstrate that in almost all of the domains, GHC br and GHC be examine fewer states than EHC, and are faster. GHC be is also faster than GHC all benchmark domains. However, in some planning domains,
GHC X  X  plan quality is slightly inferior to that of EHC and GHC
This paper demonstrates that an adaptive branch ordering function that includes two parts (i.e., an ordering function, and an information acquisition X  X daptation-function), can significantly improve the efficiency of enforced hill climbing in planning domains. The main advantages of this function are as follows: (1) An appropriate ordering function provides better performance in searching problems that have a search space with high branching factors. In such situations, the ordering function prunes many of the branches and explores fewer numbers of nodes in various domains. For example, the average number of nodes explored by the GHC in LOGISTICS, OPENSTACKS, and
ROVERS domains are, respectively, 63% 58%, and 52% fewer than the number of nodes explored by EHC.

Ordering the successor nodes is an effective way of reducing the total cost of the search. Since state evaluation is the most time consuming part of EHC, generally, any decrease in the total number of evaluated states will decrease the searching time. An appropriate ordering will decrease the number of state evaluations by suggesting the more promising states first. (2) An adaptive ordering measure d , that is created through acquiring information from search states, is more appropriate when subsequent search states are highly similar. In highly similar subsequent states, knowledge acquired from one state is valuable when evaluating its successors. (3) Larger problems usually have larger search states, and consequently many helpful actions. Branch selection in such states is more non-deterministic than in smaller states, even when using a partial ordering technique as in FF. On the other hand, larger states are more similar to each other and have higher branching factors, thus creating an advantage for the adaptive ordering function of GHC. Therefore, when GHC uses d lff , it outperforms EHC in larger problems. Furthermore, in larger states, the ordering measure of GHC has a better chance for knowledge acquisition, leading to a more informative ordering measure.

Using an adaptive branch ordering to guide the search, as employed in GHC, is a rather general idea. As a future work, one can devise other ordering measures for this purpose; especially, appropriate measures for solving the problems for which GHC failed to find a solution. It is also interesting to see how one can apply this idea to other powerful heuristic search strategies such as A n . The similarities between search strategies can help us in this regard.
 References
