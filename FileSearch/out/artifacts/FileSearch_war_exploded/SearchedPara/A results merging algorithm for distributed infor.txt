 1. Introduction
With the proliferation of the web, it has become increasingly difficult for users to find relevant information to satisfy their information needs. Often, they are faced with the decision of choosing amongst several information sources in order to find the most appropriate that will provide the necessary information. A solution to this problem is provided by search engines, which give users a starting point in finding the information they need. General purpose search engines, nonetheless, only offer a limited solution. They cannot index the whole of the
WWW because of its prohibitive size and rate of growth and thus index only parts of it. In addition to that, a large number of web sites, collectively known as invisible web ( Bergman, 2001; Raghavan &amp; Garcia-Molina, 2001 ), are either not reachable by search engines or do not allow their content to be indexed by them and offer their own search capabilities. Studies by Bergman (2001) have indicated that the size of the invisible web may be 2 X 50 times the size of the web crawlable by search engines. Lastly, there are vast amounts of corporate knowl-edge accumulated in privately owned and managed networks that remain outside their reach. Thus, a user pos-ing a query to general purpose search engines may be missing on relevant and highly qualitative information. Distributed Information Retrieval (DIR) ( Callan, 2000, chap. 4 ) also known as collection fusion ( Voorhees, problems by offering users the capability of simultaneously searching remote document collections (i.e. search engines or specialized web sites) through a single interface.

The challenge posed by DIR is on how to combine the results from multiple, independent, heterogeneous document collections into a single merged result in such a fashion that the effectiveness of the combination approximates or even surpasses the effectiveness of searching the entire set of documents as a single collection, if one was possible. This process can be perceived as four separate but often interleaved sub-processes: Source representation ( Callan &amp; Connell, 2001; Si &amp; Callan, 2003a, 2003b ), in which surrogates of the available remote collections are created. Source selection ( Callan, 2000, chap. 4 ; Powell, French, Callan, Connell, &amp;
Viles, 2000; Si &amp; Callan, 2003a, 2003b ), in which a subset of the available information collections is chosen to process the query. Query submission, in which the query is submitted to the selected sources and results merging ( Craswell, Hawking, &amp; Thistlewaite, 1999; Si &amp; Callan, 2003a, 2003b; Paltoglou, Salampasis, &amp;
Satratzemi, 2007 ), in which the separate results are combined into a single merged result list which is returned to the user. The focus of this paper is on the last part of the distributed information retrieval process, results merging. Previous research by Craswell et al. (1999) and Si and Callan, 2003a, 2003b has shown that the results merging phase is vital to the overall effectiveness of the retrieval process. Even if the most appropriate information sources have been chosen in the previous stages, an ineffective merging will deteriorate the overall quality of the retrieval process. This importance is augmented particularly in the web environment where users rarely look past the top 20 results ( Jansen, Spink, &amp; Saracevic, 2000 ).

Algorithms that make use of regression have sustainably proved to be more effective than other estimation approaches ( Si &amp; Callan, 2003a, 2003b; Paltoglou et al., 2007 ), such as weighted score merging. Their drawback is that they rely on a significant number of overlap documents in order to function effectively and make use of document relevance scores returned from the remote collections in order to achieve maximum performance. The above facts make the utilization of regression methodologies at least problematic in realistic web environments.
On the other hand, download approaches are based on downloading all the returned documents, completely or partially, in order to locally estimate their relevance. In the context that they were tested by Craswell et al. (1999) , they proved to be more effective than estimation approaches. Their main disadvantage is the increased computation, time and bandwidth overhead that they pose on the retrieval process.

The proposed algorithm presents an approach that combines the advantages of the above methodologies, while trying to minimize their drawbacks. It is based on a selective and limited download of documents, with the help of which it estimates the relevance of the rest. In addition to that, the algorithm does not require doc-ument relevance scores being reported from remote collections, making it practicable in realistic environments where remote collections return only ranked lists of documents. In effect, the algorithm suggests a limited com-promise of efficiency, in exchange for a significant benefit in performance.

The rest of the paper is divided as follows. Section 2 reports on prior work. Section 3 describes the new methodology proposed in this paper. Section 4 describes the setup of the experiments conducted. Section 5 reports and discusses the results obtained and Section 6 concludes the paper, summarizing the findings. 2. Prior work
Significant research has been made in distributed information retrieval in recent years. Although most of the focus has been on source selection, significant progress has also been made in results merging.
The STARTS ( Gravano, Chang, Garcia-Molina, &amp; Paepcke, 1997 ) initiative is an attempt to facilitate the task of querying multiple document sources through a commonly agreed protocol. It provides a solution for acquiring resource descriptions in a cooperative environment and thus facilitates source selection and results merging. It requires that all remote collections transmit predefined information regarding their content both regularly and upon query time. Unfortunately, the protocol requires that all available information sources cooperate and does not guarantee that the information being transmitted is accurate, thus it is susceptible to manipulation. It is therefore most suited to environments where all available sources are administered by a single authority, such as small to medium sized corporate networks.

When cooperation from collections is not available (i.e. isolated environments ), techniques have been devel-oped that allow for the estimation of their contents. Callan and Connell (2001) proposed a query-based sam-pling approach that creates samples of the collections through multiple one-term queries. Through them, statistics concerning the contents of the collections (such as terms, term frequencies, document frequencies etc.) can be inferred. Estimation of the size of the collections is also possible through various techniques (i.e. sample X  X esample ( Si &amp; Callan, 2003a, 2003b )). Thus, the combination of these two techniques can com-pensate for the lack of information about the content and the size of the remote collections in isolated environments.

Merging the result lists from individual collections is a complex problem not only because of the variety of retrieval algorithms that may be utilized at the remote collections, but also because of the diversity of individ-ual corpus statistics. Voorhees et al. (1994) was one of the first to conduct experiments in results merging. In that work two approaches where tested: one simple interleaving algorithm and a probabilistic biased c-faced die algorithm. The interleaving approach is based on the assumption that all chosen collections have the same number of relevant documents and works by simply interleaving their results one by one. It was found to be highly ineffective since the above assumption is rather improbable in most environments. The biased c-faced die approach produced better results and was considered the most sophisticated technique that could be adopted in isolated environments in the absence of both sample collections and relevance scores. The proba-bilistic nature of the algorithm was later re-examined by Yager and Rybalov (1998) and various deterministic approaches were presented.

In environments where the remote collections return not only ranked lists of documents but also document relevance scores, a variety of approaches have been proposed. Raw score merging merges the results as they are returned from the remote collections, but it was found to be ineffective by Callan, Lu, and Croft (1995) since it required that the scores are within a common range (i.e. between 0 and 1). The problem of incomparable scores was overcome by normalizing the returned scores at a common range. This approach produced better results, but the problem of different corpus statistics, eventually resulted in incomparable scores. For example, in a collec-tion that is mainly about sports a document containing the term  X  X  X omputer  X  will rank high if that term appears in the query, while the same document would rank lower in a computer science related collection.
Weighted scores merging overcomes the above issue by assigning each document a score which is based both on the relevance of the document itself and the collection where it belongs. This way, high scoring doc-uments from low scoring collections (as in the above example) rank lower than highly relevant scores from highly relevant collections. The CORI results merging algorithm that was proposed by Callan et al. (1995) is such a weighted scores merging algorithm and is considered state-of-the-art. The final score of each docu-ment is calculated as shown below:
Eqs. (1) and (2) normalize the collection and document scores respectively to a range of 0 X 1 while Eq. (3) as-signs the final relevance score to each document. Specifically, C the source selection algorithm, C min and C max are the minimum and maximum scores respectively that any col-lection can be assigned by the source selection algorithm and C is the relevance score given to a document at the remote collection (the initial document score), D are the maximum and minimum document scores that could be assigned by the collection and D ument normalized score. Note that D max and D min require cooperation from the remote collection to be set. In case when this cooperation is not available, they are set to the relevance score achieved by the most and least relevant document respectively. Finally, D 00 is the final document score, which again is normalized (thus the division by 1.4) between 0 and 1.

An attempt to estimate the probability of relevance of documents returned from remote collections by mak-ing use only of their ranking has been made in the past by Calve  X  and Savoy (2000) , but it required an extensive training phase before it could be utilized successfully. In addition to that, the approach followed in that work created a single model for each collection, regardless of the query being posed. The approach presented here builds a model on-the-fly per query for each collection, without requiring any training phase, thus making it practicable in volatile and rapidly evolving environments, such as the web.

Semi-supervised learning (SSL) by Si and Callan (2003a, 2003b) is a results merging algorithm that is based on linear regression. It takes advantage of a centralized sample index, comprised of all the sampled documents from the remote collections. By locating the matching documents between the centralized index and the remote collections, it estimates a linear regression model per collection between the two corresponding rele-vance scores. Utilizing the estimated model, it assigns scores to the rest unmatched documents. It was found to perform sustainably better than CORI in the majority of settings.

The disadvantages of the algorithm is that it relies heavily on the discovery of a significant number of matching documents between the remote collections and the centralized sample index, which is comprised of all the sampled documents indexed together, in order to estimate an accurate model and makes use of doc-ument relevance scores returned from remote collections in order to function effectively. Usually, 300 X 1000 documents are requested from each collection, in order for the algorithm to locate the needed matching doc-uments. However, in most modern information retrieval environments, there is a limit of 100 results per page so obtaining enough training data would require multiple interactions with the remote collection, increasing the time and bandwidth requirements of the algorithm considerably. In addition to that, studies by Jansen et al. (2000) have indicated that users rarely go past the first 10 X 20 results, so under this context, requesting an excessive number of documents can be regarded as a significant  X  X  X verkill  X  . On the other hand, the usage of document relevance scores from the remote collections limits the practicality of the algorithm in semi-coop-erative environments, where they are provided. In uncooperative environments, where only ranked lists of documents are returned without scores, the algorithm is forced to make assumptions about the mapping of rankings to scores ( Avrahami, Yau, Si, &amp; Callan, 2006 ) by assigning artificial scores to the returned docu-ments, that do not necessarily hold, thus having deteriorated effectiveness.

Alternatively, a number of approaches proposed by Craswell et al. (1999) download on-the-fly, partially or completely, the returned documents in order to produce a final ranking. The advantage of these methods is that they can locally estimate the relevance of documents and do not have to rely on the ranking or the scores that they received at the remote collections. In addition to that, they can function quite effectively with a lim-ited amount of returned documents from collections (10 X 100). Their disadvantages are that they are very  X  X  X xpensive  X  in terms of computation, time and bandwidth in order to download, index and score all the returned documents, even if the download is only partial.

The results merging problem is often confused with the metasearch problem ( Aslam &amp; Montague, 2001 ), where a number of information retrieval algorithms pose a query to a single document collection or multiple similar collections and subsequently merge the results of the individual algorithms in one final list. Most of the approaches under that context make use of the matching documents returned from the various sources in order to find the most relevant. They are based on the concept that the more a document appears at the returned lists of the individual algorithms, the more relevant it is. Lee (1997) proposed various techniques like COMB-
SUM or COMBMNZ that estimate the final score of a document as the sum of the scores obtained by individual document collections, or by multiplying this sum by the number of collections which had non-zero scores.
Similarly, the work presented by Wu and Crestani (2004) refers to partially overlapping document remote collections. The approach presented in that work takes advantage of any common documents between the remote collections when they exist and creates  X  X  X hadow  X  documents for the rest of the non-common docu-ments. The work presented in this paper differs from both of above approaches in that it assumes that the remote document collections have no documents in common (i.e. are non-intersecting), thus making the uti-lization of the above algorithms inappropriate. 3. A hybrid approach
The motivation behind the new results merging algorithm is to function effectively and efficiently in realistic information seeking web environments. It combines regression and download methodologies (thus the name
Hybrid), taking advantage of the benefits of both, while trying to minimize their disadvantages. Instead of requesting an excessive number of documents from the remote collections or alternatively downloading every single document, it selectively downloads a limited number of documents that are used as training data for the regression model.

The goal of algorithm is to estimate on-the-fly a model for each collection, on a per query basis, in order to map collection-specific rankings (the ranking that documents attain at the remote collection in respect to a particular query) to collection-independent relevance scores (the relevance scores that documents would attain if the query had been posed on a single global index). It functions by considering the centralized sample index, which is comprised of all the sampled documents indexed together, as a representative of the single global index that would be created if all the documents were available for crawling and indexing. The centralized sample index is therefore utilized as a  X  X  X eference statistics database  X  in a way similar to Craswell et al. (1999) , that provides estimates of global collection statistics (such as global idf), necessary for the estimation of collection-independent document relevance scores.

Additionally, the algorithm actively tries to maximize its effectiveness while at the same time minimizing any occurring efficiency issues. Considering that the most useful documents from each collection are the ones usually returned at the top 10 X 20 ranks, a hypothesis that is later proved by the conducted experiments, it requests a limited number of documents from the remote collections. Under this context, it is considered as granted that there would not be enough training data (i.e. matching documents between the remote collections and the centralized sample index) discovered at the returned list in order to efficiently train the regression model. Thus, it incorporates the decision of downloading documents into the algorithm itself, explicitly min-imizing the produced overhead, while maximizing the gained accuracy.

One should also note that in the web environment, a list of 100 results is probably much more  X  X  X xpensive  X  to view in terms of bandwidth than one with 10 or 20 documents. Likewise, in most cases it can be considered equally, or at least not overwhelmingly more, expensive to download (completely or partially) two or three documents from the result list of the first 10 documents than viewing 300 or more results from a remote col-lection, that would require multiple interactions with the remote collection. Therefore, the downloading of a limited number of documents can be considered as an acceptable solution that does not pose an excessive over-head to the process in comparison to other existing approaches. 3.1. Mapping collection-specific ranks to collection-independent relevance scores
One of the main issues concerning the proposed algorithm is the fact that an appropriate model would have to be determined that provides an accurate mapping of rankings to relevance scores. In previous approaches such in Avrahami et al. (2006) it was assumed that this mapping is linear. Specifically, artificial scores were assigned to returned documents in an heuristic manner, giving a score of 0.6 to the 1st ranked document and decreasing at equal intervals until assigning a score of 0.4 to the last.

Calve  X  and Savoy (2000) proposed a logistic function in order to provide an accurate mapping between rank and relevance. According to that work, the probability that document D by the equation
Fig. 1 demonstrates the general expected correlation between probability of relevance and ranking using a lo-gistic function with various parameters.

Two are the important properties of the logistic function that make it attractive to this model as pointed out by Nottelmann and Fuhr (2003) . First of all, the results are always in the [0,1] area, as a probability of rel-evance should be. Secondly, it can produce a large variety of curves, by simply modifying a and b. This is par-ticularly important since given a specific query and the number of relevant documents, the actual curve for each collection can vary significantly. 3.2. Curve fitting 3.2.1. Mapping rankings to relevance scores
Influenced by the work by Calve  X  and Savoy (2000) , we hypothesize that the correlation between the rank X of a document and relevance score Y is given by a logistic function Applying the following transformations, we are able to modify the above equation into a liner one:
We will need to estimate the parameters a , b of the above model in order to estimate the S -curve for each col-lection, that will map collection-dependent rank X to collection-independent relevance score Y . Since Eq. (8) is a linear one, the estimation can be accomplished through linear regression. 3.2.2. Linear regression
Linear regression models are used when the relationship between two variables, dependent ( y ) and indepen-dent ( x ) or their transformation can be expressed with a linear function. The model can be formally stated as where a and b are the parameters of the model and e is the observed error.

The aim of the model is to estimate the parameters a and b that minimize the error e which represents the difference between the observed values of y and the ones estimated through the model. The best way to accom-plish this is through least-squares regression analysis. In particular, the algorithm aims at minimizing the sum of squared residuals S The problem can be formalized using matrix terminology where The optimal solution for parameters a and b is the one that minimizes S in Eq. (10) and is given by
The observations that are used for the above estimation are tuples  X  x the remote collection of the i th downloaded document (see below on which documents are utilized for regres-sion), y i is the relevance score of the document using the centralized sample index as a reference statistics data-base and n is the number of downloaded documents. 3.3. Selective document download
One of the most important aspects of the proposed algorithm is the integration of the download process as an indispensable part of the estimation of an accurate model. The aim of the process is to selectively download a limited number of documents from the remote collections, maximizing the effectiveness of the model. Exces-sive download would negate the efficiency advantages of the algorithm, while a very limited one would create an inaccurate model. Two were the main issues concerning the details of the download decision. First of all, the rankings of the documents that would be downloaded and therefore utilized for regression had to be deter-mined. Since most of the relevant documents are expected to appear at the top ranks it was evident that the download process would have to start from the top ranks. A decision had to be made so that appropriate sam-ple data would be produced. Two approaches were possible. We could either download documents at a steady rate r (at ranks n r , where n 2 N ) or at unequal intervals (i.e. at ranks 2 approach produced a more uniform sampling space, so it was adopted as the default choice. Further exper-iments about the effects of downloading documents at unequal intervals were also conducted and are also reported. In order to further minimize the download overhead, we also took advantage of any matching doc-uments between the returned lists from the remote collections and the centralized sample index. Specifically, should any matching document be found at ranks we did not download the document at rank n r from the remote collection, but instead used the matching document (and its corresponding rank) for regression. In effect, we divided the rank space into segments and requested that at least one document represent each segment. This step helped in reducing the download overhead into a minimum. Evidently, the actual existence of matching documents depend on a number of fac-etc., but the process was designed to take advantage of them, aiming at minimizing the download overhead, regardless. In the experiments that were conducted r was set to 3, although early experiments showed that the algorithm X  X  performance remains stable with other values as well. In order to provide a complete coverage of the algorithm, we also present those experiments. Additionally, we also conducted experiments focusing on the number of downloaded documents, to demonstrate the efficiency gains of using already sampled documents.
Secondly, a stopping rule had to be determined that would end the download process, when the model was deemed sufficiently accurate. An adaptive download process was designed that estimated how accurate was the produced model after each document was downloaded and decided whether to continue or cease the down-load. Since we were interested at estimating the  X  X  X oodness of fit  X  of the regression, the coefficient of determi-nation R 2 provided the appropriate solution where SS
E is the sum of squared errors (i.e. the squared sum of the differences between the observed values y predicted values ^ y i ) and SS T is the total sum of squares (i.e. the squared sum of the differences between the observed values y i and their mean y ). Eq. (13) calculates the coefficient of determination of the fit. The closer the value is to 1, the better the model fits the data.

In order to avoid overfiting the curve to the top 1 or 2 documents, the algorithm by default utilizes at least three top ranked documents (downloaded or pre-sampled) at the ranks determined by the above pro-cess and inserts an artificial document at rank 4 j N j with relevance score 0.001, where j N j is the number artificial document aims at better simulating the decline at the end of the graph ( Fig. 1 ). The choice of the exact coordinates of the artificial document were set empirically and experiments (not presented here) showed that they did not affect the performance of the algorithm. It was nonetheless important that the arti-ficial document was defined outside the actual rank space set by the actual documents so that it would not interfere with the data provided by them (thus the choice of 4 j N j for the x -coordinate) and that it was close to zero (thus the choice of 0.001 for the y -coordinate) so that the final estimated curve would approach the x -axis in an asymptotical manner.

The documents are added to the centralized sample index and their relevance scores are calculated. We used the INQUERY retrieval algorithm ( Callan, Croft, and Harding, 1992 ), but any effective algorithm would do.
Next, the algorithm estimates the parameters of model (Eq. (5) ) through regression using the obtained data and calculates the R 2 value. Should that value exceed a threshold, the model is deemed good enough and no further download occurs. If not, the algorithm proceeds at downloading the next document and re-esti-mates the R 2 value. The process continues until either the set threshold is exceeded or a maximum of 5 doc-uments is downloaded. The threshold was heuristically set to 0.95, which produced a  X  X  X ood enough  X  model, without being too demanding on the process.

Note that the downloaded documents can afterward either be kept at the centralized sample index, prac-tically updating it, or be deleted. In the line of experiments that were conducted, the second approach was adopted so that previous queries would not effect the results of subsequent queries.

The above process is repeated for each collection chosen by the source selection algorithm. Applying Eq. (5) with the estimated parameters for each remote collection, the algorithm assigns relevance scores to all the doc-uments returned based on their ranking. 4. Experiment setup
We used a variety of testbeds to evaluate the proposed algorithm. The TREC123 and TREC4 testbeds ( Powell et al., 2000 ) have been used extensively in distributed information retrieval experiments, so we briefly present them below:
Trec123-100col-bysource: The documents in TREC 1, 2, 3 CDs are divided into 100 non-intersecting col-lections, organized by source and publication date. The contents of the collections are somewhat heterogeneous.

Trec4-kmeans: The documents in TREC 4 CD are divided in 100 non-intersecting collections by a k -means clustering algorithm. The collections are very homogeneous and the word distribution is very skewed.
The advantages of the above testbeds are that they offer qualitative content in a way similar to authoritative web resources, such as hidden web or enterprise resources. In addition to that, each one of the collections in the testbeds contains a significant number of documents, effectively creating non-trivial content-oriented clus-ters. Their drawback is that they are artificially made, not web-based and they both offer the same limited degree of distribution (100 collections). In order to better evaluate the proposed algorithm a more natural test-bed had to be manufactured, ideally one that is both web-based and presents a more natural separation of collections. We therefore engineered a new test collection, based on the WT10g ( Bailey, Craswell, and Hawk-ing, 2003 ):
WT10g-1000col-byUrl: The documents from the WT10g collection are divided into 11.653 collections based on their URLs and the 1000 collections with the largest number of documents were selected. The collection are very diverse both in word distribution as well as in size.

The advantages of the last test collection are numerous. It is web-based, naturally divided into collections as they were created by their authors and offers a much greater distribution than the standard trec collections. More details on the collections are provided in Table 1 .

Details about the queries used for each test collection are provided in Table 2 . An added advantage of the newly proposed test collection is that it has an average of two words per query, which is common for actual web queries, as stated by Jansen et al. (2000) .
 In order to create representatives for the remote collections, we used query-based sampling ( Callan and
Connell, 2001 ), sending 75 one-word queries and downloading the first four documents. The above process, resulted in representatives of approximately 300 documents per collections, which is customary for distributed information retrieval experiments. We used the CORI algorithm at the source selection stage, which has been used extensively in research and is one of the best known and better performing algorithms: df is the number of docs in collection C i that contain term r r , cw is the number of terms in C i , avg cw is the average cw , j DB j is the number of available collections and b is the default belief, set to the default value of 0.4.

An important factor that had to be considered was the information retrieval algorithm that would be used at the remote collections. Two strategies were possible; we could either assume that all the remote collections employ the same algorithm, or that those differ. In order to make the experiments more realistic, the second approach was adopted. Three retrieval algorithms were implemented: INQUERY ( Callan et al., 1992 ), KL divergence ( Zhai and Lafferty, 2001 ) and okapi ( Robertson, Walker, Hancock-Beaulieu, Gull, and Lau, 1994 ) and were assigned to the remote collections in a round robin fashion. All the algorithms, including our own, were implemented using the Lemur Toolkit.

Although the environment that we are particularly interested in, is a completely uncooperative one (i.e. the remote collections do not return documents relevance scores), in order to present the performance of CORI and SSL under the best possible light, we allowed them to make use of scores. It is generally expected that the performance of those algorithms in score-lacking environments, where estimates of documents relevance scores need to be made based on their ranking, will be lower than the performance reported here.
The Download approach downloads every returned document (except for those that were already sampled, in which case the sampled document was used) and scores them locally, using the inquiry retrieval algorithm.
The centralized sample index was again used as a  X  X  X eference statistics database  X  in order to estimate global statistics. Neither the Download or the Hybrid approaches made any use of document relevance scores from the remote collections. 5. Results
In distributed information retrieval environments it is usually inefficient to retrieve all the relevant docu-ments scattered in the remote collections. Especially when the focus of the experiments and the retrieval is on the results merging part of the process, the preferable measure is precision. Tables 3 X 5 report the results of the initial experiments. Each table refers to one testbed. The left column indicates the number of documents that are requested from each remote collection (10, 100 or 1000 documents per collection). We tested the
Download and the new Hybrid approaches only at the first two settings. This decision was based on the assumption that for the Download approach it is highly inefficient to download 1000 documents from each collection while the new algorithm was explicitly designed to function effectively with a limited number of returned documents. We provide the last setting of 1000 documents mainly for comparison reasons with the estimation approaches, especially SSL that cumulatively builds a more accurate model when there are more documents available.

One of the first things that can be noted, applicable to all the test collections, is that the performance of most algorithms remains ineffected by the gradual increase of returned documents from the remote collections.
The findings support our initial hypothesis that most of the relevant documents are returned on the top 10 results and very few relevant documents are added to the final merged list thereafter. In addition to that, the excess documents may introduce noise to the final merged lists, effectively deteriorating performance. In a realistic web environment it would potentially be much more beneficial to the retrieval process if more col-lections are added, potentially increasing the diversity and completeness of the final merged list, instead of requesting an increasing number of documents from a same limited number of collections. The only exception is, as expected, SSL whose performance generally increases as more documents are returned.

A second conclusion that is also common in all the test collections is that both of the approaches that incor-porate some sort of downloading, Download or Hybrid, achieve performance that is persistently better than that of any estimation approach at most settings, with few exceptions. The above observation may imply that collection-dependent document relevance scores provide insufficient evidence for collection-independent doc-ument scores. Therefore, approaches that do not rely on such scores but take a more direct approach, such as downloading all or some, completely or partially (as it will be shown below) of the returned documents, thus not being susceptible to such estimation errors, are able to perform more adequately regardless of the perfor-mance of the underlying remote collections.

The intent behind the new Hybrid algorithm is to approximate the effectiveness of the Download approach, with as little overhead as possible. Clearly, the algorithm cannot avoid downloading some documents, which means that some overhead, even a limited, will be introduced to the retrieval process. What the Hybrid algo-rithm suggests is a limited compromise of efficiency, in exchange for a significant benefit in performance. The amount of introduced overhead will be studied subsequently and a comparison with the Download approach will be presented.

In the first setting of the Trec123-100col-bysource testbed ( Table 3 ), where each collection returns 10 doc-uments, the difference between the CORI  X  SSL and the Download  X  Hybrid groups is rather distinct and sta-tistically significant, while the intra group differences are not (i.e. the difference between Download and Hybrid is not statistically significant). In the absence of sufficient training data SSL performs worse than CORI, although not in a statistically significant manner. The performance of CORI, Hybrid and Download remains mostly unchanged at the other settings, noting only small fluctuations, thus retaining their statistical signifi-cance. SSL X  X  performance is surprisingly unstable, noting a decline in the second setting despite the additional documents and reaching a peak at the last. It is only at that setting, when the collections return 1000 docu-ments each, that its performance comes near that of the Hybrid  X  Download group at the previous settings, but still doesn X  X  manage to surpass it despite the excessive amount of documents and training data.
The trec4-kmeans testbed ( Table 4 ) has been characterized as more difficult than the trec123 testbed, because of the heterogeneity of the individual collections and the skewness of the word distributions. It is pos-sibly because of this fact that the Download approach attains such a notable performance gain, especially at P@5, while it retreats closer to the performance of the Hybrid in the following ranks. The performance of the
Hybrid at the first setting is again above the performance of the CORI  X  SSL group, in a statistically signif-icant manner. Again, CORI performs better than SSL, a behavior that does not persist in the second setting, where SSL X  X  performance increases (in comparison to the trec123 testbed). Again, the Hybrid algorithm out-performs the best estimation approach, although in a statistically significant manner only at P@5. At the last setting, where each collection returns 1000 documents the performance of SSL approximates very closely that of the Hybrid  X  Download approaches.

The newly introduced WT10g-1000col-byUrl testbed ( Table 5 ) presents very interesting challenges because of the level of distribution that it offers and the diversity of the sizes and topicality of the underlying collec-tions. In order to have meaningful measurements, we decided to select 100 collections per query, which even though may seem excessive is at the same percentile scale as the previous testbeds (10% of the available col-lections). Early experiments showed that only few relevant documents were returned at the first 10 collections, making the comparison between the algorithms trivial. In this testbed, the performance differences between the
CORI  X  SSL and the Hybrid  X  Download group are more profound in comparison to the previous testbeds, giving a possible hindsight about the actual performance of the approaches in realistic web environments. In the first setting, where each collection returns 10 documents, the difference between the Hybrid algorithm and the best performing estimation algorithm varies around +85% at any precision measurement, a statistically significant difference. In comparison, the difference between the Hybrid and the Download approach is not statistically significant. At the second setting, the difference between the two groups is reduced, but still remains at significant levels. SSL X  X  performance at the last setting surprisingly remains at the same level as the second, possibly because the collections do not return as much as 1000 documents, thus not approaching the performance of the Hybrid algorithm at any setting.
 Overall, the performance of the algorithms that incorporate some sort of downloading of documents,
Hybrid or Download, almost always exceeds that of estimation approaches and in most cases with a statisti-cally significant difference. It is only in the, unrealistic, case that each collection returns 1000 documents per query that SSL approximates that performance, but only rarely does it surpass it. 5.1. Studying the efficiency issues
In this section, we will study the overhead that the Hybrid and the Download approaches introduce to the retrieval process. Thus, we counted the number of documents that each approach downloads at the above set-tings. As noted above, the Hybrid algorithm incorporates a progressive download criterion, while the Down-load approach downloads every returned document, unless it has already been sampled in the query-sampling phase. Results are presented in Table 6 . As expected, when remote collections are requested to return 100 doc-uments, the Download approach downloads an excessive number of documents, while the Hybrid algorithm utilizing the progressive download methodology described above, limits the number of downloads to a min-imum. Taking into consideration that the effectiveness gains in this setting are trivial when compared to the first setting where collections return 10 documents we will focus our attention to that setting, which is also more fair for comparison to the Download algorithm. The results are presented here only for completeness reasons.

When the remote collections return 10 documents each, the Download algorithm downloads on average 9.3 documents in the trec123 testbed and 8.8 documents in the trec4 testbed. In comparison, the Hybrid algorithm downloads a maximum of 2.5 documents in either setting, noting a efficiency gain of more than 70% in both cases. A question may arise here since as it was mentioned above, the Hybrid algorithm by default utilizes three documents from each collection to avoid overfitting the curve to the top 1 X 2 documents. The answer lies in the usage of the already sampled documents, which provide some of the necessary representatives of the divided rank space thus demonstrating that using already sampled documents indeed help the efficiency of the algorithm. In addition to that, in most cases the initially produced model was above the set R and therefore the algorithm did not resort to additional downloading. The new wt10g testbed presented some surprising results. The query-based sampling process in this testbed proved to be unexpectedly efficient, since the Download algorithm had to download only 5.4 documents on average per collection. Again, the Hybrid algorithm proves to be much more efficient, resorting to downloading only 1 document per collection. The skewness of the size distribution of the collections may provide the explanation for the above results, poten-tially making the most the useful documents from the collections already available at the query-sampling phase, thus minimizing the needed downloading on query time.

The above results demonstrate the efficiency gains of the algorithm in comparison to the Download approach. Let it be noted, that in realistic web environments both the algorithms could download the cached version of the returned document which most search engines provide, instead of the actual one, thus reducing any delay in producing the final results. Still even in this case, the Hybrid approach remains a much more effi-cient solution, downloading on average only 2 X 3 documents per collection. 5.2. Attempting a partial download approach
For the next line of experiments, we compared the performance of the Hybrid algorithm with partial down-load approaches. Under this context, instead of downloading documents completely, only a part is down-loaded, based on a size threshold. The above approach aims at laxing the efficiency constraints of the original Download approach, in which every returned document is completely downloaded. In parallel, we also tested the Hybrid algorithm under the partial download approach, in which the selected documents are also downloaded partially, in order to test its robustness. It should be noted that the partially downloaded documents are only utilized to produce the final merged document list and not be served to the user, should he/she chooses to view the document. Should that happen, the user would be directed to the actual remote document. Under this context, a partial document approach is always applicable when there are significant constrains on the download overhead that can introduced into the process.

Since the initial experiments showed that requesting more than 10 documents from the remote collections did not attribute to a significant increase in precision, we tested the algorithms only in this setting. Taking into consideration that the average size of a single trec document is roughly 6kbytes, two size thresholds were tested: 2kbytes and 3kbytes, which approximately translate into downloading one third and half of the returned documents respectively. Results for all the test collections are reported in Tables 7 X 9 .
Generally, the performance of both algorithms using partial downloading is decreased, but not in a signif-icant manner. The performance of the Hybrid algorithm is always at par with the Download approach, as in the previous settings where documents are completely downloaded. We will focus our attention on the com-parison of the results of this setting to the ones produced by the estimation approaches ( Tables 3 X 5 ) to examine whether the partial downloading has a significant effect on the performance of the Hybrid algorithm or whether the conclusions drawn at the initial experiments are still valid in this setting.

In the trec123 testbed, when only 2kbytes from the selected documents are downloaded, the performance of the Hybrid remains above the CORI  X  SSL group ( Table 3 ). It is only at the setting where each collection returns 1000 documents that the SSL manages to overpass the Hybrid approach, a result that does not persist when the threshold is increased to 3kbytes.

It is worth making a point here concerning the bandwidth requirements of the algorithms. In this setting for example, the Hybrid algorithm roughly downloads 10 results lists of 10 documents each plus 75kbytes of doc-uments (3kbytes per document * 2.5 documents (on average, see Table 6 )
SSL algorithm needs to view 1000 results, which translate into 10 interactions with each remote collection, each interaction returning 100 results. Although the actual sizes of the results lists of search engines may vary from one to another, one can assume that the latter method would be at least problematic in realistic web environments.
 In the trec4 testbed, the conclusions are generally similar as the ones observed in the trec123 collection.
Hybrid manages to outperform the CORI  X  SSL group in all the settings ( Table 4 ), with the exception of the last where collections return 1000 documents in which SSL performs better, but not in a statistically sig-nificant manner.

Finally, in the wt10g testbed, the performance of the Hybrid algorithm, even at the 2kbyte setting steadily remains well above that of any estimation algorithm. The differences increase when the size threshold is set to 3kbytes, but only marginally.

In general, the results demonstrate the robustness of the algorithms that incorporate a downloading pro-cess, even if documents are downloaded only partially. Additionally, the performance of the Hybrid algorithm steadily remains above that of estimation approaches in most settings, making it a very efficient and robust approach. 5.3. Downloading at different intervals
In the experiments that were presented this far, the Hybrid algorithm was set to download documents at equal intervals. Additionally, the rate was heuristically set to 3, based on preliminary experiments. It would be interesting to see whether adopting a different rate would effect the performance of the algorithm. One would expect that it remains robust to such modifications, not recording great differences in precision.
In this section, we present the effect of downloading documents at different equal intervals. We ran the experiments on the same testbeds described above requesting 10 documents from each remote collection, vary-ing only the interval. The original algorithm used a rate of 3, so we tested the algorithm with rates of 2, 4 and 5, as these provided more than one tuple in the first 10 results.
 The results are presented in Table 10 . The original adopted rate is also presented for comparison reasons.
The percentages in parenthesis report the differences in precision in reference to the original rate. The best per-formance is marked with bold in each precision setting.

In general, the performance of the algorithm remains mostly stable on all testbeds. Varying the interval of downloading documents does produce some fluctuations in precision as expected, but at no point do those differences alter the conclusions drawn earlier about the effectiveness of the algorithm in respect to other approaches.

Specifically, in the trec123 and the trec4 testbed, with the exception of P@5 in the latter, the differences range between 3% and +4%, which are rather insignificant. One minor exception is the P@5 at the trec4 test-bed, where the difference between the original algorithm and an interval of 4 reaches a 6% drop, but is later equalized at P@10 and the following precision measurements.

In the wt10g, the differences are somewhat more substantial, noting significant drops, particularly at P@5, while are mostly leveled in latter precision measurements. The heterogeneity of the particular testbed and the skewed distribution of relevant documents may justify for the observed results, but more research may be needed in order to fully understand the noted fluctuations.

The above results demonstrate the robustness of the algorithm in reference to the adopted rate for down-loading documents. Although some differences in performance are noted when the interval is changed, these are mostly insignificant. The optimal rate does seem to be 3, which has the most stable performance across all testbeds, justifying the original choice. This is probably because it successfully combines focusing in the top ranked documents with a somewhat dense and uniform sampling space across the ensuing ranks. 5.4. Downloading at unequal intervals
In order to further examine the robustness of the algorithm, we tested its performance when downloading documents at unequal intervals. We experimented with a variety of functions to provide a diversity of down-loading schemes. Specifically, we downloaded documents at ranks 2 8), 2 2 n 1 (i.e. ranks 1, 2, 8) and 3 n (i.e. ranks 1, 3, 9).

Each one of those functions was chosen because of the focus they assign to different ranks. The first one focuses particularly heavily on the top ranked documents, the second one on the first two top ranked docu-ments and also utilizes a last one toward the end of the result list and the third one provides a rather sparse sampling space. Results are reported in Table 11 . Again, percentages in parenthesis report the differences in precision in reference to the original reported algorithm. The performance of the algorithm again remains mostly stable. The observed differences between the various downloading schemes and the original approach lie between 4% and +6% and are not statistically significant. The 2 what better at all the testbeds in comparison to the original approach, while the rest of the approaches are mostly at par with the original version of the algorithm, performing sometimes slightly better and sometimes slightly worse, always within an acceptable margin.

In summary, we tested the algorithm with a variety of different schemes for downloading documents, both at equal and unequal intervals and we found that its performance remains mostly stable, noting minor fluc-tuations which rarely become statistically significant. The above results demonstrate the robustness of the pro-posed approach regardless of the parameters originally chosen. 5.5. Using a different reference statistics database
In the experiments that were conducted so far, we utilized the documents that where downloaded locally during the query-based sampling phase as a  X  X  X eference statistics database  X  , as this seemed like a natural choice given the fact that they were readily available. The added advantage of this choice was a reduced download overhead, since some of the documents that were returned during the experiments were already sampled.
The primary reason for the utilization of a reference statistics database is to provide a representative for a single global database, thus aiding in the estimation of global-wide statistics, such as inverse document frequency. One may view the prerequisite of the creation of such a database during the sampling phase as a serious drawback of the proposed results merging algorithm, making it dependable on the effectiveness (or not) of previous phases of the distributed information retrieval process. Additionally, we were also very inter-ested in examining the effects of using a different, potentially more general, document corpus as a reference statistics database. Should the performance of the algorithm remain the same, that would effectively mean that the algorithm is independent of the query-based sampling phase, and could thus be utilized by making use of a random document corpus as a reference database. For this reason, we conducted the following line of exper-iments. We interchanged the centralized sample indexes from the trec123 and wt10g testbeds and tested the performance of the algorithm using them as reference databases. In effect, we utilized the centralized sample index created for the wt10g during the query-sampling phase in experiments with the trec123/trec4 testbeds as a reference statistics database, and vice versa. Note that the difference of the two corpora is rather substantial, based on the nature of the actual testbeds. The documents in the wt10g collection are web documents, cover-ing various subjects, while the trec documents are newswire articles, written in a more authoritative manner. More on the differences of the two testbeds can be found on Section 4 .

Additionally, since previous research by Shokouhi, Zobel, Scholer, and Tahaghoghi (2006) has indicated that query-based sampling does not necessarily provide a true random sample of documents and is rather biased toward long documents, we also experimented with a true random selection of documents from the trec123 and wt10g collections. Thus, we ran two experiments using different references statistics databases; one using a database that was produced with query-based sampling on a different corpus (we named this set-ting  X  X  X t10g  X  QryBased  X  and  X  X  X rec  X  QryBased  X  respectively) and a second, using a database comprised of randomly selected documents ( X  X  X t10g  X  Random  X  and  X  X  X rec  X  Random  X  respectively) again from a different corpus.

The goal of those experiments is to see whether the use of a completely different document corpus (both biased and unbiased) would still provide helpful estimations for global-wide statistics, or whether the pro-duced statistics would be so inaccurate, resulting in a significant drop of effectiveness of the algorithm.
We also tested the algorithm in two additional settings, as controls. In the first, we utilized no centralized sample index at all. Only the documents that were downloaded at each query were used as a reference statistics database. It is clear, that in this case the database is heavily biased toward documents that are at least some-what relevant to the query (potentially all the documents would contain the query terms, thus providing lar-gely inaccurate idf estimations) and in no way does it provide a representative of a single global collection, but it would be very interesting in exploring the effect of using such a database on the algorithm, testing whether the algorithm can function in a heavily biased setting.

Lastly, slightly altering the above setting, we started with an empty centralized sample index, but kept the documents that were downloaded at each query, effectively building a reference statistics database incremen-tally (we named this setting  X  X  X ynamic  X  ). The last setting could be employed in a realistic environment where the algorithm would start functioning without any locally-stored documents and use the downloaded docu-ments as reference for future queries.

Results are reported in Table 12 . Note that we used the original version of the algorithm for this line of experiments. In the trec123 testbed, the lack of a reference statistics database ( X  X  X one  X  ) has a significant impact on the algorithm. The estimates produced are obviously inaccurate, producing an ineffective final rank-ing. On the contrary, the use of a different reference database has little impact on the algorithm indicating that the algorithm could indeed function with a completely different document corpus in producing global-wide statistics.

What is surprising is the performance at the  X  X  X ynamic  X  setting, which is only slightly deteriorated compared to the original. Conducting a per query comparison, we found out that the differences were not so substantial at the initial queries as one may have expected, as at these queries the reference corpus is still at a very initial stage, but instead they were scattered amongst the queries, in a rather unpredictable manner.

Combining these results with the significantly deteriorated performance of the algorithm when no reference database is available, one may reach the conclusion that for some more difficult queries (i.e. where the doc-ument frequency of the query terms vary significantly) a good reference corpus is helpful, while for others it is unnecessary.
The results are quite different at the trec4 testbed. There are no substantial differences between the approaches, including the case where no reference database is being used. These results are somewhat surpris-ing and partly come in contrast with the results from the tre123 testbed, potentially indicating that in well con-tent-based defined collections with a limited relevant document distribution amongst collections, the use of a reference database is indeed redundant. We believe that this is because any relatively successful source selec-tion strategy will be able to locate the most promising collections, thus providing a significant number of rel-evant documents for the final merging. Still, even at this setting one should not underestimate the importance of the results merging phase, as demonstrated by the significant differences between the download and the esti-mation approaches in Table 4 .

Lastly, in the wt10g testbed, there is a general drop of effectiveness at P@5, which usually does not persist at later precision measurements, closely approximating the original recorded performance. The only exception to the above is when the algorithm utilizes a random sampling from the trec123 collection, where the perfor-mance remains notably low (but still much higher than any estimation approach). The reasons why this may be happening are not very clear, but we assume that a random selection of documents from the trec123 collection, may produce a bad selection of documents.

In summary, the above experiments indicate that the use of some sort of reference database does aid the performance of the algorithm. The utilized corpus need not be specific to the testbed, but may as well be a general corpus. The absence of any reference database is not suggested as the performance of the algorithm becomes rather unstable at this setting. 6. Conclusions and future work
In this work, a new results merging algorithm for distributed information retrieval environments was pre-sented. The algorithm offers a hybrid solution to the two directions from which the problem has been approached in research, estimation and download , by combining their strengths and minimizing their draw-backs. It is based on downloading a limited number of selected documents from the remote collections with the help of which it creates a model on-the-fly on a per query basis for the estimation of the relevance of the rest through regression methodologies. In addition to that, the algorithm does not make use of document rel-evance scores from remote collections, making it practicable in completely uncooperative environments where they are not provided.

In the experiments that were conducted it was shown that the effectiveness of the proposed algorithm is almost always above the best of the estimation algorithms, while approximating that of download approaches.
Even if the selected documents are partially downloaded, the impact on the performance of the algorithm is not significant, making it a robust solution in realistic web environments. It was also demonstrated that the new approach is much more efficient in terms of bandwidth overhead that download approaches, downloading on average 78% less documents.

Further experiments demonstrated that the algorithm is robust to variations of the originally chosen parameters, noting mainly minor fluctuations in performance. Additionally, it was shown that the algorithm is not dependent on the query-based sampling phase and could function quite effectively by making use of a general, but carefully chosen, document corpus.

Additionally, a model was suggested for mapping ranks to relevance scores without the need of a training phase. Although, in the context that the mapping was utilized in this work, it was used to assign collection-specific rankings to collection-independent relevant scores, we believe that there are significant merits in the general approach that was followed and we intent to further study frameworks under which it can be utilized. Acknowledgements
This paper is part of the 03ED404 research project, implemented within the framework of the  X  X  X einforce-ment Programme of Human Research Manpower  X  (PENED) and co-financed by National and Community
Funds (25% from the Greek Ministry of Development-General Secretariat of Research and Technology and 75% from EU  X  European Social Fund).
 References
