 1. Introduction
Electricity consumer classi fi cation is a prominent scheme in the deregulated markets, since it can serve as the basis for designing suitable tariffs that directly re fl ect the generation costs. The con-sumers of the same class present similar daily demand patterns and different pricing policies can be applied, leading to increased pro ability of retailers ( Mahmoudi-Kohan et al., 2010 ). Load pro tool that provides the required information about the demand behavior among various kinds of consumers, so that the aforemen-tioned actions can be effectively implemented.

Given a set of metered load data, the task of grouping them in a sophisticated manner is a subject of many requirements ( Panapakidis et al., 2013 ). First of all, a representative sample of load curves must be obtained when dealing with the load pro fi ling of either an individual consumer or a group of c onsumers. After gathering the metered data, we need to specify the load condition. This refers to the periodic attributes of the demand. For example, the load pro may consider only working days excluding holidays and weekends.
In our work, we formulated nine yearly daily load data sets. Next, a suitable preprocessing of the data is necessary. The data are normal-ized in the appropriate range, since in the clustering operation we are concerned mainly with the load curve shape similarities. The load pro fi ling process involves the proper selection of one or more clustering algorithms. Each algorithm has unique characteristics, such as input parameter requirements, speed and computational complexity. The purpose of the clustering procedure is the search of structures within a data sample. The formation of groups takes place, wherethepopulationwithinthesamegroupshowsmoresimilar characteristics compared with the members of the other groups. The clustering procedure is data driven. In the majority of the applica-tions there is no a priori knowledge about the relations between the data, their geometry or special attributes. 2. Literature survey and contributions clusters is unknown. Therefore a load pro fi ling problem is formu-latedasanunsupervisedlearningtask( Panapakidis et al., 2013 ).
Clustering based load pro fi ling is a two-stage process. In the stage, each consumer is studied separately. His daily load curves are clustered and certain classes are formed. Each class is represented by the average daily load curve, which is actually the normalized load pro fi le. In the second stage, a speci fi cloadpro fi each consumer and the second clustering occurs that generates consumer classes ( Tsekouras et al., 2007 ). Otherwise, the clustering can be skipped and the load pro fi le that represents the consumer can be an averaged load curve, which corresponds separately on weekdays and weekends ( Chicco et al., 2004 ). rithms and the performance of the algorithms is tested by a set of adequacy measures, which are indications of the within-cluster distances of the features that belong to the same cluster and the cen-troid (center) of it and the distances between the centroids of the different clusters. These researches can be generally categorized based on the type of the algorithm that they used and based on the characteristics of the data sets under study. The fi rst category includes studies that involve: Partitional clustering algorithms, like the K-means, Weighted
Fuzzy Average (WFA) K-means, Improved Weight Fuzzy Average (IWFA) K-means and modi fi ed  X  follow-the-leader  X  ( Tsekouras et al., 2007; Chicco et al., 2004; Bidaki et al., 2010; Kohan et al., 2009; Cao et al., 2013; Ramos et al., 2013; Kwac et al., 2014 ).
Hierarchical clustering algorithms, like the family of agglomerative algorithms and the COBWEB ( Tsekouras et al., 2007; Chicco et al., 2004; Bidaki et al., 2010; Gerbec et al., 2002a, 2002b, 2003 ; Nizar et al., 2006 ; Hino et al., 2013; Notaristefano et al., 2013; Kwac et al., 2014 ).

Fuzzy clustering algorithms, like the Fuzzy C-Means (FCM) ( Tsekouras et al., 2007; Chicco et al., 2004; Bidaki et al., 2010;
Chicco and Ilie, 2009; Anuar and Zakaria, 2012; Gerbec et al., 2002b; Zakaria and Lo, 2009; Iglesias and Kastner, 2013 ). Neural Network (NN) based clustering algorithms, like the Self-
Organizing Map (SOM), Adaptive Vector Quantization (AVQ) trained NN and Hop fi eld NN ( Tsekouras et al., 2007; Chicco et al., 2004; Bidaki et al., 2010; Rodrigues et al., 2003; Chicco and Akilimali, 2009; Chicco and Ilie, 2009; R X s X nen et al., 2010; Figueiredo et al., 2005; L X pez et al., 2011; Verdu et al., 2004; Wang et al., 2013 ).
 Algorithms that do not belong to the above categories, like the Expectation Maximization (EM), Support Vector Clustering (SVC), Renyi entropy (Nizar et al., 2006; Chicco and Akilimali, 2009;
Chicco and Ilie, 2009 ), Ant Colony Clustering ( Chicco et al., 2013 ) and a set of Subspace and Projection Clustering Methods ( Piao et al., 2014 ).

The second category distinguishes among studies that involve load data sets from:
Low voltage consumers ( R X s X nen et al., 2010; Figueiredo et al., 2005; L X pez et al., 2011; Rodrigues et al., 2003 ; Nizar et al., 2006, Anuar and Zakaria, 2012; Hino et al., 2013; Cao et al., 2013; Iglesias and Kastner, 2013; Piao et al., 2014 ).

Medium voltage consumers ( Tsekouras et al., 2007; Chicco et al., 2004; Bidaki et al., 2010; Kohan et al., 2009; Gerbec et al., 2002a, 2002b, 2003; Chicco and Akilimali, 2009; Chicco and Ilie, 2009; Notaristefano et al., 2013; Chicco et al., 2013; Ramos et al., 2013; Wang et al., 2013 ).

In spite of the extended research in the fi eldofloadpro fi more sophisticated segmentation o f recorded load data is still req-uired. The contribution of the present paper can be summarized as follows: 1. A detailed comparison of the most commonly used algorithms in the load pro fi ling is performed. We also introduce a new type of clustering concept. The Minimum Entropy Clustering (minCEntropy) is an objective-function-oriented algorithm where the task is to minimize the conditional entropy within the clusters ( Vinh and Epps, 2010 ). The algorithm has already shown robust performance in other clustering applications. In this study, we introduce a modi fi ed version of this algorithm, which includes hybridization with the K-means. The latter is used to provide the initial centroids of the minCEntropy. The superiority of the hybrid K-means/minCEntropy algorithm over the others already considered in the literature is demonstrated. 2. This is the fi rst study that examines all the clustering validity indicators or adequacy measures of the literature used to evaluate the algorithms operation. An effectual partitioning of the metered samples should lead to low values of intra-connectivity and inc-reased degree of inter-connectivity of the generated clusters. The clusters should be well-separated, so that a post-processing of the grouped data would be feasible and accurate. The performance of the different validity indicators for the different algorithms is investigated, but also their implementation in de fi ning the optimal number of clusters. 3. So far in the literature, the clustering is only evaluated in a strictly mathematical manner (i.e. considering the values of the indicators). Several additional performance criteria are intro-duced in this paper, like the computation time of the algorithms, their complexity as expressed by the input parameters require-ments, whether a parameter updating towards the simulations is needed, whether the clustering results are exploitable by a speci fi cloadpro fi ling application and others. 4. While the majority of the related studies terminate their ana-lysis in the comparison of the algorithms, we move a step for-ward to a load pro fi ling application, providing a framework for the usage of the load pro fi ling fi ndings in the design of the appropriate Demand Side Management (DSM) measure for the consumer under study.

The majority of the respective researches deals with the pro of the medium voltage (MV) consumers; the load pro fi ling of high voltage (HV) industrial consumers have not been proposed so far in the literature ( Chicco, 2012; Zhou et al., 2013 ). The analysis of the present study is focused on the daily load curves of an existing HV metal industry located in Greece. T he HV consumers' consumption in Greece was 6.61 TWh in 2011, corresponding to the 12.63% of the total consumption of the Greek electric sector ( Regulatory Authority for Energy, 2012 ). Due to the contractual DSM policies, HV industrial consumers are a group of special interest to the utilities. Since 2011 an ongoing discussion between the Public Power Corporation of Greece S.A. (PPC S.A.), which is the sole s upplier of the HV industries, and the Regulatory Authority of Energy of Greece takes place, regarding tariffs offered to the HV customers. Load pro fi ling of HV consumers can therefore help in the design and evaluation of sophisticated DSM measures and the analysis of the paper is oriented towards this goal. 3. Load pro fi ling framework 3.1. Data collection
The data set used in this work is composed by nine subsets, corresponding to different years, between 2003 and 2011. To provide a general overview of the yearly demand, Table 1 registers the minimum, maximum and average load and daily consumed energy per year, together with the annual variation from the base year of 2003. The null values of Table 1 refer to temporary terminations of the industrial activity. It can be noticed that the maximum load continually increases until 2007. Then for the next two years the maximum annual load is reduced and fi nally in 2010 and 2011 rises again. The annual variation of the maximum load is below 10% while in 2011 the maximum load increase is 34.44%. This high increment of the demand is also recorded in the corresponding maximum daily consumed energy. Regarding the average yearly load, more fl tions between the years are observed. For three years the average yearly load is reduced. This fact is re fl ected to the average daily consumed energy. Therefore the industry load does not follow a constant trend and Table 1 revealsthatitisnecessarytousean expanded data set to provide a more robust analysis for the patterns of electricity usage.
 The load pro fi ling process consists of several steps as shown in
Fig. 1 .The fi rst stage deals with the collection of data. The data used in this study were obtained by the Power Exchange Transactions
Department of Independent Power Transmission Operator of Greece ( Independent Power Transmission Operator , Athens, Greece). Accord-ing to the respective legislation, all the HV consumers in Greece are monitored since they are eligible for application of primary DSM actions, i.e. pre-negotiated load reductions during transmission system peaks. In this study no prior processing of the data took place, such as the elimination of atypical metering records. Addi-tionally, no prior distinction of the daily load curves between work-ing days and weekends/holidays took place, since the data set corresponds to a single consumer. This approach would however help in a better partitioning in cases of many consumers within a similar demographic, economic and operational environment. 3.2. Data processing
The 2nd stage of the demand pro fi ling of the industrial con-sumer is the appropriate representation of the data sample. The time-series modeling of the load data corresponds to a H -dimen-sional vector. Each daily load curve is expressed by a vector with 24 elements, where each one corresponds to the mean hourly load demand; the dimension of each pattern corresponds to the meter-ing sampling interval. The vector of the m -th day, m  X  1,2, p  X  X  p m 1 ;:::; p m 24 T  X  1  X  where H  X  1 ; 2 ;:::; 24 is the respective hour. The set that contains the load data is denoted as P  X f p m ; m  X  1 ;:::; M g ; with M indicating the number of the daily load curves, i.e. M  X  365 : Since the com-plete data set contains nine subsets for every year, the total population of load curves is M total  X  3285 : Note that a different set P is considered for every year.

In this study, load data of 9 subsequent years are used. The large amount of data improves the credibility of the pro fi analysis and the clustering results and conclusions can be general-ized. Furthermore a proper benchmark tool can be formulated via into the existing classes.

Since the focus of interest is the load shape and not the load magnitude, each vector should be normalized in the proper range before entering the main clustering process. Also, the normalization aims to improve the interpretability of the clustering outcome. Let p be the maximum demand of the set P . The data are scaled in the [0,1] range, according to the following formula: x  X  p md p
The set of the normalized patterns is denoted as X  X f x m 1 set X in order to fi nd a number, say K ; of clusters, W k  X  1 ;:::; K ; 1 r K r M g : The partition matrix of size K M can be represented as U  X  X  u km ; k  X  1 ;:::; K and m  X  1 ;:::; M element u km de fi ned the membership of x m to the clusters W The aim is to group together the data set X such that ( Wang and Zhanga, 2007 ): W k a  X  for k  X  1 ;:::; K  X  3  X 
W \ W j  X   X  for i  X  1 ;:::; K ; j  X  1 ;:::; K and i a j  X  4  X 
In the case of crisp clustering, the following condition holds: u  X 
In the case of soft clustering, each pattern can be a member of different clusters with different membership that lies in the [0,1] range. 3.3. Algorithm selection
The main load pro fi ling process involves the selection and imp-lementation of one or more clustering algorithms. The algorithms should be executed for a different number of iterations and possibly for a variable number of clusters. Below is a description of the clust-ering algorithms used in this study.

K-means is the most popular partition clustering algorithm, due to its speed, effectiveness and computational simplicity. In the reg-ular K-means the initial centroids are chosen randomly. After the selection of centroids, each of the remaining patterns is assigned to the closest. The next step is the re-calculation of the centroids, which are actually the centers of gravity of the newly formatted clusters. The procedure is repeated until there are no transpositions of patterns between the clusters. From a mathematical point of view theproblemistominimizethefollowingobjectivefunction:
O  X   X  K where d e denotes the Euclidean distance norm and w k is the centroid of the k -th cluster.

Hierarchical agglomerative clustering algorithms start with M singleton clusters, i.e. each cluster has one pattern and after the computation of the M M proximity matrix, the minimum dis-tance between the centroids is tracked and merged to form a new cluster. The matrix is updated and the distances are re-computed.
The process terminates when one cluster is left. The merge of a pair of centroids depends on the distance function between the current centroid and the newly formed one. The family of agg-lomerative algorithms corresponds to different types of the dist-ance function. The general scheme of the agglomerative sequence is summarized in the following: 1) Starting from an initial population of M singleton clusters, the
M M proximity matrix is computed. 2) Let C m ; C l be two different random singleton clusters of the data set X . The minimal distance within the matrix is derived, d  X  C i ; C j  X  X  min where d is the distance function and C i ; C j are the clusters which are combined to form a new cluster C ij : The distance between C i and C j is the minimum distance between all the random pairs C m ; C l : 3) The matrix is updated by calculating the distances between the cluster C ij and the others. 4) The steps 2 and 3 are repeated until one cluster is left.
Merging a pair of clusters depends on the distance function between the cluster C j and the newly formatted one, C ij general form of the distance function is, where a i ; a j ;  X  and  X  are coef fi cients which depend on the algo-rithm used. Their values are assigned according to Table 2 for the different algorithms used, either explicitly or as a function of M ; M j and M l representing the populations of the clusters C and C l ; respectively. The family of agglomerative algorithms cor-responds to different types of the distance function and inclu-des the Single Linkage (SL), Complete Linkage (CL) the Unweighted Pair Group Method Average (UPGMA), Weighted Pair Group Method Average (WPGMA), Weighted Pair Group Method Centroid (WPGMC), Unweighted Pair Group Method Centroid (UPGMC) and Minimum Variance Method (MVM).

The SOM is an unsupervised learning neural network that consists of a layer of neurons that is arranged in a geometrical top-ology such as a line, plane, torus or sphere. Usually, the layer is a one-dimensional (1D) or a bi-dimension al (2D) lattice. Every neuron is connected via weights with the input layer and receives a complete and modi fi ed copy of the input. A neuron affects positively (excita-tion) the neighboring neurons and negatively (inhibition) the most distant ones.

Generally, when the layer is supplied with an input, a competi-tion between the neurons takes place, which neuron will respond more to the speci fi c input. A weight updating takes place for the winner neuron, w  X  t  X  1  X  X  w k  X  t  X  X   X   X  t  X  h  X  k  X  c  X  t  X  X  x m  X  t  X  w where t is the respective epoch (i.e. the training step), w w  X  t  X  1  X  are the weights of the winning neuron i at epoch t and neighborhood kernel around neuron w k  X  t  X  and the random input vector at time t , respectively. The map resembles the grouping of the input patterns. Speci fi c regions on the map represent the groups. This means that the SOM projects the multi-dimensional input data in, usually, a two dimensional space.

Fuzzy clustering is a generalization of crisp clustering and involves partial membership over the clusters; this concept is app-ropriate for handling the degree of fuzziness of the original data. The most widely used fuzzy clustering algorithm is the FCM. The initial population of patterns is grouped in k segments in a way that a cost function (dissimilarity metric) is minimized. The diss-imilarity metric to be minimized is given by, J  X   X  K where q A [1, 1 ) is the fuzziness index. The centroid of the k -th cluster depends not only on the fuzziness index but also on the membership degree u km of the m -th pattern to the cluster and it can be calculated by ( Xu and Wunsch, 2006 ): w
The minCEntropy algorithm belongs to the category of partitional clustering algorithms. Its iterative operation, just like the K-means, aims to the minimization of an objective function. The algorithm stops if the number of user de fi ned epochs is reached or when there are no changes in the partitions. The difference with K-means lies in the form of the objective function. The K-means minimizes the total sum of squares, i.e. the distances between the patterns and the respective centroids, where in the minCEntropy the objective is to minimize the conditional entropy criterion. Let W be the space of all possible partitions of X : The problem is to identify a partition W
W ; which maximizes the mutual information between X and W ; minimizes the conditional entropy,
W n  X  arg max
W n  X  arg min sidering the Havrsa-Charvat's entropy in (14), the conditional entropy minimization is expressed by,
CE  X  W  X  X   X  K where M k is the population of the k -th cluster and  X  is the Gaussian kernel width parameter. The CE is a measure of the quality within a cluster. The minimum conditional entropy criterion aims to the maximization of the weighted sum average of intra-cluster similarity, i.e. the pairwise distances between the members of the same cluster. 3.4. Clustering assessment framework An effective load pro fi ling scheme requires a reliable validity check.
In this paper all the adequacy measures of the literature are presented and implemented ( Chicco, 2012 ). Each measure examines speci characteristics of the generated partitions, like the compactness, the separation and others. Below is the de fi nition of the measures:
The Mean Square Error or Error Function J , which refers to the distance of each pattern from its cluster centroid, is de
J  X  1 M  X  where S k is the subset of X that includes the population of the k th cluster, where w k is the centroid.
 The Mean Index Adequacy (MIA), given by:
MIA  X 
The MIA is the average of the distances between each input vector assigned to the cluster and the cluster center.

The Clustering Dispersion Indicator (CDI), which is the ratio of the mean intra-set distance between the input vectors in the same cluster and the inter-set distance between the clusters centroids:
CDI  X  The ratio of Within Cluster Sum of Squares to Between Cluster
Variation (WCBCR), which is the ratio of the distance of each pattern from its cluster centroid and the distances between the 4. Experimental results 4.1. Sensitivity analysis for the calibration of the algorithms parameters the initialization conditions. The random choice of the centroids is the main drawback for achieving a fast convergence and a global minimum of the corresponding objective function. The fi rst step in current analysis is the investigation of the sensitivity of the algorithms for varying performance parameters. For this analysis the selected data set includes only 365 daily load curves for one year, namely year 2011 throughout this Section. The J measure was used for the validation of all algorithms since it represents the most common quality characteristic of the formatted clusters, i.e. the cohesion, which is expressed by the similarities between the patterns and the centroids.

The K-means converges when the minimum amount of imp-rovement of the objective function between two successive itera-tions, as de fi ned by a pre-set tolerance of  X   X  10 6 , is met. The process is also terminated after a pre-set maximum number of iterations, in case of no convergence.

The fi rst test for the FCM aims to determine the appropriate value of the exponential parameter in (11), which controls the fuzziness of membership of each pattern. The variation of the parameter is between 1.10 and 4 with a step of 0.10. As the fuzz-iness index receives lower values, i.e. the generated partitions are characterized by higher uncertainty, the operation of the FCM becomes less ef fi cient. The effect of the fuzziness index on the clustering error becomes more evident for large number of clusters. After a set of experiments, the optimal value of the index is obtained. The next investigation on the FCM is to examine the necessary number of iterations for convergence. Results showed that the algorithm needed some more iterations compared to K-means, however not exceeding 25 in all cases examined.
Next the performance of the minCEntropy algorithm has been checked. Note that our implementation of the minCEntropy refers to a hybrid combination of the K-means and the main minCEn-tropy. Using the K-means as the 1st member of a hybrid combina-tion the overall ef fi ciency becomes higher. The K-means provides a solution to the random initialization of the minCEntropy. The output centroids of the K-means are used as the initial ones for the minCEntropy. However, the hybrid combination corresponds to high computation time. Apart from the number of iterations, another issue that has been analyzed is the effect of the parameter on the performance of the minCEntropy algorithm. The  X  sigma  X  parameter is the Gaussian kernel width ( Vinh and Epps, 2010 ). Our results are in accordance with ( Vinh and Epps, 2010 ), where the optimal value appears to be  X  0  X  0 : 50 :
In contrary to the previous algorithms, the robust function of the SOM needs a large number of parameters to consider. It should be noted that only one-dimensional maps are considered since the two-dimensional maps require another clustering to obtain the fi nal centroids ( Tsekouras et al., 2007 ; Chicco et al., 2004 ; R X s X nen et al., 2010 ). The optimal parameters of the neural network are shown in Table 3 where  X   X  ; T ;  X  t and d cl are the initial learning rate, total number of epochs, neighborhood radius at time t and distance between the neurons c and l of the competitive layer, respectively. In the sequential type training, only a single pattern is selected randomly and led into the network, while in the batch type all patterns are led at each epoch. According to our simulations for the problem under study, the sequential type results in better clustering. Before the beginning of a new training cycle, the learning rate and the size of the neighborhood decrease. Both learning and neighborhood are decreasing functions of time. The training is implemented into phases. During the rough ordering phase a small number of epochs, large initial learning rate and initial neighborhood function radius are required. The fi ne tuning phase involves a large number of epochs and small initial learning rate and radius. The neighborhood function de the neighborhood size around the winning neuron ( Helsinki University of Technology, 2000 ). 4.2. Comparative analysis of the algorithms
After the data processing, the next task is the implementation of the classi fi cation framework. Considering a similarity metric, the main pro fi ling operation aims at the organization of the pat-terns in groups. This allows the continuous import of new patterns and grouping them into clusters. Furthermore, the requirement of an effective clustering algorithm is the successful handling of noisy data and outliers. Also, a well-functioning algorithm should be comprehensible and easily modi fi able. Note that in the current load pro fi ling application there is no indication about the optimal number of clusters for the data sample under study. Therefore, the present problem is a purely unsupervised machine learning task. Only the inherent information of the data is used to extract accurate and exploitable information about the demand patterns. To sum up, the robustness of a clustering algorithm applied in a load pro fi ling problem should satisfy the following conditions:
Condition1 : Superior performance as measured by the most, if not all, clustering validity indicators.

Condition2 : Minimum number of parameters that need to be speci fi ed.

Condition3 : Minimum requirement for parameter updating. The optimal parameters obtained by the parametric analysis, should remain stable when considering different data sets.
Condition4 : Comprehensible and well-documented type of operation.
 Condition5 : High execution speed.

Condition6 : Generation of exploitable information about the fi nal load curve classes.

The comparative analysis of the algorithms is based on the satisfaction of the aforementioned conditions separately. The sole tool to evaluate the main classi fi cation of the data is the validity framework. We included all the adequacy measures of the litera-ture in order to investigate their properties, not only on the evaluation of the algorithms but also their capacity to indicate the optimal number of clusters. A series of repeated executions of the algorithms have taken place, where the number of clusters varied between 2 and 30. Each time the values of the validity indicators were checked. Depending on the measure, the super-iority of an algorithm over the others is demonstrated when it leads to lower or higher values for a speci fi c number of clusters. The behavior of the adequacy measures is illustrated in Fig. 2 .
The J, MIA, CDI, WCBCR, SI and IAI measures are decreasing with increasing number of clusters. The DBI, SMI, SMI2, MDI, IEI and CH measures show an unstable behavior. Here a robust algorithm should result in lower variations of the values of the measures for the different number of clusters. The Error Function J is an indication of the compactness of the clusters. When the number of the segmentations is high, the sum of the distances in each cluster is small, due to the lesser terms within the distance itself. Another indicator of the compactness is the MIA which shows similar behavior with the J. The CDI and the WCBCR combine the degree of the compactness and the separation of the clusters. They decrease monotonically with the number of clusters.
The IAI measures the sum of the distances between the patterns and the centroids and is de fi ned similar to J.

Table 4 presents the two most ef fi cient algorithms, as resulted by the overall comparison per adequacy measure. Regarding the
J measure, the minCEntropy seems to be the most effective algorithm followed by the K-means and the SOM. The SL hier-archical algorithm appears to be the less effective with notable higher J values. Similar conclusions are drawn from the IAI mea-sure. Measuring the performance of the eleven algorithms with the MIA, it can be noticed that the SL algorithm leads to lower values. Here the minCEntropy is the next more ef fi cient while the
UPGMC who follows in the respective order presents very close performance to the minCEntropy. Generally, all the algorithms result in to very close MIA curves. The degree of compactness of the formatted groups is represented by the similarities of their members and the similarities between the members and the groups' representatives (centroids). The J, IEI and MIA are suitable to represent this. Aside from the compactness, the CDI also measures the separation of the clusters, i.e. the distances between the centroids. When the CDI results in lower values, either the compactness is high (i.e. the distances within the clusters are small) or the separation is high (i.e. the distances between the clusters representatives are large) or fi nally, both of these condi-tions hold. Again the minCEntropy algorithm groups the data in the most optimal manner. Competitive performance is shown by the CL, K-means and SOM. In the case of the WCBCR, the UPGMC is preferable until the 12th cluster. For the remaining number of clusters, the SL leads to the lowest values. Here the minCEntropy outperforms only the CL, K-means, MVM, SOM and FCM.
 that makes the evaluation rather dif fi cult. An ef fi cient clustering should however lead to limited fl uctuations. To show the corre-sponding results more clearly, the SMI and SMI2 curves are mag-ni fi ed in the area of 10 to 30 clusters. The most preferable algo-rithms are the UPGMC and the SL. The SL and the UPGMC algorithms also exceed the performance of the rest when the validation is done using the DBI. The minCEntropy leads to subs-tantially lower MDI values.
 corresponds to higher values. This measure is related to the degree of spreading of the centroids from the arithmetic mean and eventually between themselves. Here the SL algorithm leads to the largest IEI values. The CH is a combination of IAI and IEI. By de fi nition, the CH should be maximized for better performance.
However this is not the case with the results in Fig. 2 . The non-monotonous behavior may lead to contradictive conclusions as to the performance of the algorithms, although the minCEntropy and the K-means seem to win the competition for some cluster number areas. Finally, in the case of the SI the most ef fi algorithms are the WPGMC and the SL. To sum up, the minCEn-tropy algorithm results in the lowest error when the J, CDI, IAI,
MDI, CH are considered and is among the most effective algo-rithms in the case of MIA and the WCBCBR. However, when the measures include the arithmetic mean of the sample (IEI and SI) its performance is not satisfactory.
 6 present the mean values of the measures and the standard dev-iations, respectively. Note that the information provided in these
Tables is indicative. Apart from examining the values of the mea-sures in different cases, i.e. the values for 2 to 30 clusters, mean values or other statistical indices, the comparison of the CH algorithms should take into consideration other parameters like the complexity, availability, exploitable interpretation of the results and other. It is up to the user to decide on the strengths and weaknesses of the algorithms.

While Condition1 refers to the clustering performance, Condi-tion2 and Condition3 deal with the computational complexity of the algorithms. The parameters of the algorithms that need to be properly de fi ned prior to the algorithms executions are shown in Table 7 . The agglomerative hierarchical clustering requires an external termination condition. This means that the user is responsible for providing this term. If this is not provided, hier-archical clustering will terminate when all input patterns will merged into a single cluster. Another characteristic of partitional clustering algorithms like the K-means, FCM and minCEntropy is their dependence of the selection of the initial centroids. The common forms of K-means and FCM do not involve any approach for the initial condition selection. The operation of the SOM req-uires a large number of the parameters that need to be properly de fi ned. This is a disadvantage in cases where the simplicity of the clustering algorithm is an important factor.

Finally, another approach in the comparative framework of the clustering models is to study the yearly fl uctuations of the mea-sures. The fl uctuation is tested by the standard deviation curve of the measure corresponding to the 9 years period of the full data set. Indicatively, the standard deviation curve of the IAI measure is given by the following expression: s where IAI k i is the measure's value of the i -th year corresponding to the k -th cluster, IAI k the measure's average value for the k -th cluster and i  X  1, ... , n (with n  X  9) is the indicator denoting the correspond-ing year. The proposed criterion when dealing with the demand pro fi ling of a consumer with a large number of available load data, is to validate the ef fi ciency of an algorithm when it leads to lower standard deviation values. As an example, the reader is referred to
Fig. 3 , where the deviation curves of the algorithms for 2 to 30 clusters are shown. Using this approach, the 9 IAI curves are modeled by a single one. Just like the original IAI, the superiority of the minCEntropy is once again evident followed by the K-means and the SOM. The most ef fi cient hierarchical clustering is the MVM. load of the consumer under study is taken as the basis for the normalization of the load data. Although in the examined cases of a single consumer with yearly data the resulting data scaling is similar, this approach may lead to inaccurate data representation since the peak value may correspond to a metering error. There-fore the use of the average or median load could be a better choice in cases of multiple data sets. In order to examine the in of the different scaling values, Figs. 4 and 5 present the compar-ison of the algorithms using the IAI measure, implementing the mean and the median values of the set as the normalization base, respectively. Using the median value as the basis, the WPGMA and UPGMA algorithms change position in the ranking of the algorithms ef fi ciency. All the other ranks remain the same. Using the mean value as the basis, the WPGMC and UPGMC change position in the ranking.
 select the proper clustering approach. The implementation of all algorithms can be based on commercial software packages and no in-house coding is further needed.
 plexity. In cases of databases with large amount of stored data, a fast algorithm is required in order to provide real-time results.
Accurate and easily obtained results can serve as the basis of a real-time, bi-directional fl ow of information between the utility and the end consumer. Table 8 presents the comparison of the algorithms' performance with respect to the execution time of the
K-means, used here as a reference. All calculations have been done in a 2.30 GHz Pentium s T4500 Dual Core  X  with 4 GB RAM 64-bit system. The computational time refers only to the execution of the algorithms for variable number of clusters with no further calcula-tion of any adequacy measure. Generally, all algorithms do not need large execution time. However, computational time can be a signi fi cant factor when an algorithm is to be applied in very big data sets involving many consumers.

The algorithms are executed for 2 to 30 clusters and the data refers to the year 2011. Here, hierarchical algorithms outperform all the rest. SOM presents high execution time, making this algorithm unsuitable for real time applications. The minCEntropy also requires large times since it is actually a hybrid combination of two algo-rithms, with the K-means providing the initial centroids. 4.3. Extraction of the typical load curves
The demand pro fi ling of the HV consumer postulates a reliable method to de fi ne the optimal number of clusters. The decreasing tendency of the WCBCR curve makes the measure appropriate for applying the  X  knee  X  point detection method ( Panapakidis et al., 2013 ). The concept is to track a speci fi c point in the curve, after which the WCBCR decreases with lower rate. Let ( x , y ) be the coordinates of random point in the measure curve. We considering and the two last {( x 29, y 29), ( x 30, y 30)}of the curve, where the coordinates refer to the corresponding number of clusters. The point of the intersection corresponds to the optimal number. After this point, the measure decreases with a lower rate. Another imp-ortant issue is the selection of the adequacy measure not only to evaluate the clustering procedure but to derive the optimal number of load curve clusters. Table 9 shows the cluster charact-eristic that each measure refers to. Additionally, the last column registers which measure can be used to de fi ne the optimal number if clusters with the  X  knee  X  detection method.

The determination of the optimal number of clusters of the daily load curves of the industrial consumer highly depends on the valid-ation criterion. Evaluating the clustering process with a different measure, the optimal number is different. The  X  knee  X  point detection method offers a mathematical option to derive this number, when a measure presents monotonically decreasing or increasing trend. A large number of clusters is generally not desirable, since it increases the complexity of the interpretation and the utilization of the load pro fi ling outcomes. The J, MIA, CDI, WCBCR, IAI and SI measures are suitable for deriving the optimal cluster number, while the rest measures are best suited for algorithm comparisons. Finally the use of the CDI and WCBCR is suggested since they are related with two qualities of the clusters, namely the compactness and the mutual separation. For IEI and CH measures , the superiority of an algorithm over the others is demonstrated when it leads to higher values of these measures. For the all the rest of the measures, the superiority refers to lower values while the number of clusters varies.
Indicatively, the load pro fi les of 2011 will be examined. By applying the  X  knee  X  point detection method on the WCBCR sub-plot for the curves generated by the minCEntropy algorithm, the 365 daily load curves of the industry are optimally grouped together at 12 clusters. The respective pro fi les are illustrated in Fig. 6 . The day type distribution generated by the minCEntropy, the K-means, the SL and the CL are shown in Table 10 , respectively. Considering Fig. 6 and Table 10 , it is observed that there are two main types of curve shapes. Pro fi les w1, w7, w11 and w12 present almost no fl uctuations, while the rest present a similar daily pattern. The demand is high during the fi rst morning hours until 10:00  X  11:00 h. A period of low consumption follows until 14:00 h, where a local peak appears up to 18:00 h. After a fall of the load level, it rises again after 20:00 h and remains high during the night hours. It should be stated that the demand shows daily, weekly and monthly periodicities. The consumption is high during the night hours and during weekends and holidays, due to the offered electricity prices by the sole supplier of the industry, namely the PPC S.A. Cluster 1 is the most populated one and contains 44 almost equally distributed working days per type.
Condition6 refers to the main pro fi ling application. A robust algorithm should provide exploitable information about the clusters and the respective centroids. Comparing the day type distribution of Table 10 , it can be noticed that the minCEntropy leads to clusters with comparable number of members. The SL and
CL algorithms are able to track special demand patterns, as the patterns of the anomalous days within the year, i.e. holidays and working days with atypical demand patterns. Since load pro is application dependent, special care should be placed in the selection of the clustering algorithm ( Panapakidis et al., 2012 ). For example, hierarchical algorithms that can isolate the atypical pat-terns are more suitable in the data pre-processing of a load fore-casting problem, where this isolation acts as a fi ltering of the data.
The atypical patterns can be then excluded from the data and are not used in the training of the forecasting model. For applications related to DSM measures for the industrial consumer, the shapes and the amplitude of the pro fi les is important, since the DSM measures will be applied to the corresponding days that belong to a speci fi c cluster. Therefore a good distribution of the members in the clusters is important for such applications and therefore the most suitable technique should be chosen. Following this concept, minCEntropy satis fi es better Condition6 .
 sity of the shape during the day, we employed the Load Shape
Factors (LSFs) ( Verdu et al., 2004 ). The LSFs deal with speci
Hour aspects of the load curve and characterize indirectly and univo-cally the magnitude of the load.

The 24 h period is classi fi ed into the following zones: day which refers to the full 24 h period, daylight which includes the hours from 07:30 h to 18:30 h, lunchtime which includes the hours from 13:30 h to 15:30 h and night which refers to period from the imum, minimum and average load of the day period, respectively,
P the lunchtime period and P max ; night ; P min ; night ; P the night period, respectively. The LSF are de fi ned as follows: f f  X  1 3 P f  X  P
Table 11 presents the LSFs of the pro fi les of Fig. 6 . By obtaining a macroscopic picture of the consumption norms through the year, suitable load shape objectives can be designed and tested. The load shape objective, i.e.  X  peak clipping  X  ,  X  valley fi lling servation  X  ,  X  fl exible load shape  X  ,  X  load building  X  determines the selection of the DSM measures, either energy eff-iciency or demand response interventions, that will meet the consumption standards ( International Institute for Energy Cons-ervation, 2006 ). For example, low load factors (i.e. the factor f correspond to partial loading of the generation units. The increase of f can be achieved usually through valley fi lling, load building and shifting. The pro fi les w1, w7 and w12 present similar LSF values.
There are no discrete peaks through the 24 h period and differences between the consumption of the distinct periods. The average load in each period is comparable, as shown by the low values of f f ,aswellasthehighvaluesoff 7 .Fortherestofthepro fi les, there is a mutual relation between some of the LSFs. Again there are no discrete peaks, a fact that makes peak clipping of limited applic-ability. The high consumption of the night hours is indicated by the values of f 7 which exceed unity and the low values of f 4 denotes the fact that the peak load occurs at night. In the case of mandatory loads, portions of the demand can be transferred to early afternoon hours, through load shifting measures. Low f 3 suggests that there is a large difference between the minimum and the average demand. To increase this ratio, valley fi lling and load buil-ding-type DSM measures are suitable. 5. Discussion and summary
The main contributions of the paper can be summarized in the following:
The studies in the literature present comparisons among several algorithms only in terms of minimizing the clustering error. In this study, fi ve additional conditions are introduced in order to provide a more in depth comparative analysis of the perfor-mance of the algorithms.

The most commonly used algorithms of the literature are consi-dered. It is shown that to optimize their performance, a parameter calibration prior to their application should be implemented.
A new concept of clustering approach is introduced, namely the hybrid K-means/minCEntropy algorithm, which shows superior performance in most cases.

All the adequacy measures of the literature are examined and a distinction about their usage in determining the optimal number of clusters is proposed.

A new clustering validity approach is introduced when dealing with large data sets that cover many consequent years.

While most related studies are concerned only with the classi cation of the load curves, this paper examines a load pro application. It is shown that based on the pro fi le shapes, special DSM measures can be recommended.

The outline of the results demonstrates that different conclusions can be drawn when clustering algorithms and adequacy measures are considered. The clustering procedure and the generated parti-tions depend on the choice of the algorithm and the measure that assesses the process. A number of experiments are needed to reach a safe decision about the more ef fi cient demand pro fi ling approach. For the above reasons, the de fi nition of the appropriate clustering algorithms is a rather complex procedure. The main fi ndings of this work can be summarized in the following:
With respect to Condition1 : Our experiments show that the conditional entropy clustering re sults, in most cases, in the lowest clustering error. Therefore it is considered as the most appropriate algorithm. The UPGMC and the SL also shows high ef fi ciency and arerecommendedasthenextpreferablealgorithms.
 With respect to Condition2 and Condition3 : Apart from the SOM, all algorithms do not need a wide parameter calibration.
In terms of simplicity, hierarchical algorithms need only one parameter to be de fi ned. Due to large number of parameters, the SOM is generally not recommended for utilization.

With respect to Condition4 : All the algorithms are well-docum-ented and available via commercial software packages.

With respect to Condition5 : Hierarchical algorithms are recom-mended, since they require less computational time. Out of the partitional clustering algorithms, the minCEntropy is 10 times slower than the K-means. The reason for this delay is the hybrid implementation that the authors used for the simulation.
Finally, the use of the SOM algorithm is not suggested due to its high computational burden.

With respect to Condition6 : For the application under study, the minCEntropy is recommended. Hierarchical clustering is able to isolate the atypical patterns which lead to clusters with few members. This situation limits the design of DSM measures, since we have load patterns corresponding to few days within the year.
Finally, the FCM does not properly satisfy Condition6 .TheFCM generates clusters where every input pattern has a partial member-ship in each cluster. While this fuzziness can fi nd application in the solution of other problems, the techno-economic assessment of the DSM should be concise and straight-forward.

With the respect to the selection of the validity indicator: Here two conditions should be satis fi ed. An adequacy measure should provide as more information as possible about the fi nal clusters.
Thus, the measures that deal with more than one cluster char-acteristic are recommended. The second condition is the use of the indicator for the determination of the optimal number of clusters. Under these considerations it is concluded that the
CDI and the WCBCR measures are the more appropriate in the load pro fi ling studies.

With the respect to the macroscopic view of the demand patterns of the high voltage consumer: According to the extracted load pro fi les, the demand of the high voltage industry presents daily, weekly and seasonal variation. The 365 daily load curves are optimally grouped together in 12 clusters. The industry presents high activity in the weekends and the holidays of each year due to the lower offered electricity price. Within the day period, the demand is high during night hours again due to tariff issues.
There are generally 2 types of pro fi les regarding the curves shapes. The different sub-types within these types differ mainly in the magnitude. The load pro fi les do not present intense and sudden peaks. Appropriate DSM measures should target at the overall load reduction, load shifting to periods of low consump-tion and load building on speci fi cperiods.

Load pro fi ling is a tool that provides, among others accurate informationaboutthedemand,helpsinthedetectionofatypical demand patterns and identi fi es the daily, weekly and seasonal variations of the demand. Robust demand pro fi ling is an essential tool for suppliers, distribution sys tem operators, retailers, aggre-gators and energy service companies. Within this context, the approach of the present work can be expanded in future research by considering applications in sh ort-term load forecasting, dem-and response measures, scheduling of distributed energy resou-rces and power line congestion management. Another objective of further investigation is the application of techniques that can reduce the size of the available data set without decreasing the pro fi ling ef fi ciency.
 References
