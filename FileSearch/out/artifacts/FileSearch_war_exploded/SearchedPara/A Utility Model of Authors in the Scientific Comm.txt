 Why do we write? As researchers, we write pa-pers to report new scientific findings, but this is not the whole story. Authoring a paper involves a huge amount of decision-making that may be influenced by factors such as institutional incen-tives, attention-seeking, and pleasure derived from research on topics that excite us.

We propose that text collections and associated metadata can be analyzed to reveal optimizing be-havior by authors. Specifically, we consider the ACL Anthology Network Corpus (Radev et al., 2013), along with author and citation metadata. Our main contribution is a method that infers two kinds of quantities about an author: her associ-ations with interpretable research topics, which might correspond to relative expertise or merely to preferences among topics to write about; and a tradeoff coefficient that estimates the extent to which she writes papers that will be cited versus papers close to her preferences.

The method is based on a probabilistic model that incorporates assumptions about how authors decide what to write, how joint decisions work when papers are coauthored, and how individual and community preferences shift over time. Cen-tral to our model is a low-dimensional topic rep-resentation shared by authors (in defining prefer-ences), papers (i.e., what they are  X  X bout X ), and the community as a whole (in responding with ci-tations). This method can be used to make predic-tions; empirically, we find that: 1. topics discovered by generative models out-2. such models do better at that task without mod-3. the author utility model leads to better pre-
This method can also be used for exploration and to generate hypotheses. We provide an in-triguing example relating author tradeoffs to age within the research community. In the following, a document d will be represented by a vector  X  d  X  R K . The dimensions of this vec-tor might correspond to elements of a vocabulary, giving a  X  X ag of words X  encoding; in this work they correspond to latent topics.

Document d is assumed to elicit from the scien-tific community an observable response y d , which might correspond to the number of citations (or downloads) of the paper.
 Each author a is associated with a vector  X  a  X  ments. Below, we will refer to this vector as a  X  X   X  X references, X  though it is important to remember that they could also capture an author X  X  expertise , and the model makes no attempt to distinguish be-tween them. We use  X  X references X  because it is a weaker theoretical commitment. We describe the components of our model X  author utility (  X  3.1), coauthorship (  X  3.2), topics (  X  3.3), and temporal dynamics (  X  3.4) X  X hen give the full form in  X  3.5. 3.1 Modeling Utility Our main assumption about author a is that she is an optimizer: when writing document d she seeks to increase the response y d while keeping the con-tents of d ,  X  d ,  X  X lose X  to her preferences  X  a . We encode her objectives as a utility function to be maximized with respect to  X  d : where d,a is an author-paper-specific idiosyn-cratic randomness that is unobserved to us but as-sumed known to the author. (This is a common assumption in discrete choice models. It is often called a  X  X andom utility model. X )
Notice the tradeoff between maximizing the re-sponse y d and staying close to one X  X  preferences. We capture these competing objectives by formu-lating the latter as a squared Euclidean distance between  X  a and  X  d , and encoding the tradeoff between extrinsic (citation-seeking) and intrinsic (preference-satisfying) objectives as the (positive) coefficient  X  a . If  X  a is large, a might be un-derstood as a citation-maximizing agent; if  X  a is small, a might appear to care much more about certain kinds of papers (  X  a ) than about citation.
This utility function considers only two partic-ular facets of author writing behavior; it does not take into account other factors that may contribute to an author X  X  objective. For this reason, some care is required in interpreting quantities like  X  a . For example, divergence between a particular  X  a and  X  d might suggest that a is open to new topics, not merely hungry for citations. Other motivations, such as reputation (notoriously difficult to mea-sure), funding maintenance, and the preferences of peer referees are not captured in this model. Sim-ilarly for preferences  X  a , a large value in this vec-tor might reflect a  X  X  skill or the preferences of a  X  X  sponsors rather than a  X  X  personal interest the topic.
Next, we model the response y d . We assume that responses are driven largely by topics, with some noise, so that where  X  d  X  N (0 , 1) . Because the community X  X  interest in different topics varies over time,  X  is given temporal dynamics, discussed in  X  3.4.
Under this assumption, the author X  X  expected utility assuming she is aware of  X  (often called  X  X ational expectations X  in discrete choice models), is:
E [ U (  X  d )] =  X  a  X  &gt;  X  d  X  (This is obtained by plugging the expected value of y d , from Eq. 2, into Eq. 1.)
An author X  X  decision will therefore be  X   X  d = arg max Optimality implies that  X   X  d solves the first-order equations  X   X  j  X  (  X   X  d,j  X  (  X  a,j + d,a,j )) = 0 ,  X  1  X  j  X  K Eq. 5 highlights the tradeoff the author faces: when  X  j &gt; 0 , the author will write more on  X  d,j , while straying too far from  X  a,j incurs a penalty. 3.2 Modeling Coauthorship Matters become more complicated when multiple authors write a paper together. Suppose the docu-ment d is authored by set of authors a d . We model the joint expected utility of a d in writing  X  d as the where the  X  X ost X  term is scaled by c d,a , denoting the fractional  X  X ontribution X  of author a to docu-ment d . Thus, a latent categorical distribution to be inferred. The first-order equation becomes 3.3 Modeling Document Content As noted before, there are many possible ways to represent and model document content  X  d . We treat  X  d as (an encoding of) a mixture of topics. Following considerable past work, a  X  X opic X  is de-fined as a categorical distribution over observable tokens (Blei et al., 2003; Hofmann, 1999). Let w d be the observed bag of tokens constituting docu-ment d . We assume each token is drawn from a mixture over topics: p ( w d |  X  d ) = where N d is the number of tokens in document d , z d,i is the topic assignment for d  X  X  i th token w d,i , and  X  1 ,...,  X  K are topic-term distributions. Note that  X  d  X  R K ; we define p ( z |  X  d ) as a categorical draw from the softmax-transformed  X  d (Blei and Lafferty, 2007).

Using topic mixtures instead of a bag of words provides us with a low-dimensional interpretable representation that is useful for analyzing authors X  behaviors and preferences. Each dimension j of an author X  X  preference is grounded in topic j . If we ignore document responses, this component of model closely resembles the author-topic model (Rosen-Zvi et al., 2004), except that we assume a different prior for the topic mixtures. 3.4 Modeling Temporal Dynamics Individual preferences shift over time, as do those of the research community. We extend our model to allow variation at different timesteps. Let t  X   X  1 ,...,T  X  index timesteps (in our experiments,  X  a denote the community X  X  response coefficients, author a  X  X  preferences, and author a  X  X  tradeoff co-efficient at timestep t .

Again, we must take care in interpreting these quantities. Do changes in community interest drive authors to adjust their preferences or exper-tise? Or do changing author preferences aggregate into community-wide shifts? Or do changes in the economy or funding availability change authors X  tradeoffs? Our model cannot differentiate among these different causal patterns. Our method is use-ful for tracking these changes, but it does not pro-vide an explanation for why they take place.
Modeling the temporal dynamics of a vector-valued random variable can be accomplished us-ing a multivariate Gaussian distribution. Follow-ing Yogatama et al. (2011), we assume the prior cision matrix  X (  X , X  )  X  R T  X  T :  X (  X , X  ) =  X  The two hyperparameters  X  and  X  capture, respec-tively, autocorrelation (the tendency of  X  ( t +1) be similar to  X  ( t ) proach to modeling time series allows us to cap-ture temporal dynamics while sharing statistical strength of evidence across all time steps.

We use the notation T (  X , X  )  X  N ( 0 ,  X (  X , X  )) for this multivariate Gaussian distribution, in-stances of which are used as priors over response coefficients  X  , author preferences  X  a , and (trans-formed) author tradeoffs log  X  a . .
 3.5 Full Model Table 1 summarizes all of the notation. The log-likelihood of our model is: We adopt a Bayesian approach to parameter esti-mation. The generative story, including all priors, is as follows. Recall that T (  X  ,  X  ) denotes the time series prior discussed in  X  3.4. See also the plate diagram for the graphical model in Fig. 1. 1. For each topic k  X  X  1 ,...,K } : 2. For each author a  X  A , draw (transformed) 3. For each timestep t  X  { 1 ,...,T } , and each
Eq. 9 captures the choice by authors a d of a dis-tribution over topics  X  d . Assuming that the d,a s are i.i.d. and Gaussian, from Eq. 7, we get Figure 1: Plate diagram for author utility model. Hyperparameters and edges between consecutive time steps of  X  ,  X  and  X  are omitted for clarity. and the linear additive property of Gaussians gives us
In  X  3.1 we described a utility function for each author. The model we are estimating is similar to those estimated in discrete choice economet-rics (McFadden, 1974). We assumed that authors are utility maximizing (optimizing) and that their optimal topic distribution satisfies the first-order conditions (Eq. 7). However, we cannot see the idiosyncratic component, d,a , which is assumed to be Gaussian; as noted, this is known as a ran-dom utility model. Together, these assumptions give the structure of the distribution over topics in terms of (estimated) utility, which allows us to naturally incorporate the utility function into our probabilistic model in a familiar way (Sim et al., 2015). Exact inference in our model is intractable, so we resort to an approximate inference technique based on Monte Carlo EM (Wei and Tanner, 1990). During the E-step, we perform Bayesian inference over latent parameters (  X  ,  X  , z ,  X  , c ,  X  ) using a Metropolis-Hastings within Gibbs algo-rithm (Tierney, 1994), and in the M-step, we compute maximum a posteriori estimates of  X  by directly optimizing the log-likelihood function. Since we are using conjugate priors for  X  , we can integrate it out. We did not perform Bayesian pos-terior inference over  X  because the coupling of  X  would slow mixing of the MCMC chain.
  X  d blockwise using the Metropolis-Hastings algo-rithm with a multivariate Gaussian proposal distri-bution, tuning the diagonal covariance matrix to a target acceptance rate of 15-45% (see appendix  X  A for sampling equations).

For z , we integrate out  X  and sample each z d,i directly from p ( z d,i = k |  X  d ,  X  k )  X  exp(  X  d,k ) w is associated with topic k , and the number of tokens associated with topic k respectively.
We run the E-step Gibbs sampler to collect 3,500 samples, discarding the first 500 samples for burn-in and only saving samples at every third it-eration.
 M-step. We approximate the expectations of our latent variables using the samples collected dur-gradient. The gradient of the log-likelihood with respect to  X  ( t )  X  L (with vague priors) at the end of the M-step. We fix the symmetric Dirichlet hyperparameter  X  = During initialization, we randomly set the topic as-signments, while the other latent parameters are set to 0. We ran the model for 10 EM iterations. Inference. During inference, we fix the model parameters and only sample (  X  , z ) for each doc-ument. As in the E-step, we discard the first 500 samples, and save samples at every third iteration, until we have 500 posterior samples. In our ex-periments, we found the posterior samples to be reasonably stable after the initial burn in. Data. The ACL Anthology Network Corpus contains 21,212 papers published in the field of computational linguistics between 1965 and 2013 and written by 17,792 authors. Additionally, the corpus provides metadata such as authors, venue and in-community citation networks. For our ex-periments, we focused on conference papers pub-texts, tagged the tokens using the Stanford POS tagger (Toutanova et al., 2003), and extracted n -grams with tags that follow the simple (but effec-tive) pattern of (Adj|Noun)  X  Noun (Justeson and Katz, 1995), representing the d th document as a bag of phrases ( w d ). Note that phrases can also be unigrams. We pruned phrases that appear in &lt; 1% or &gt; 95% of the documents, obtaining a vocabulary of V = 6,868 types. The pruned corpus contains 5,498 documents and 2,643,946 phrase tokens written by 5,575 authors. We let re-sponses y d = log(1 + # of incoming citations in 3 years )
For our experiments, we used 3 different ran-dom splits of our data (70% train, 20% test, and 10% development) and averaged quantities of in-terest. Furthermore, we remove an author from a paper in the development or test set if we have not seen him before in the training data. 5.1 Examples of Authors and Topics Table 2 illustrates ten manually selected topics (out of 64) learned by the author utility model. Each topic is labeled with the top 10 words most likely to be generated conditioned on the topic (  X  k ). For each topic, we compute an author X  X  topic preference score:
TPS ( a,k ) =  X  ( t d ) where Softmax ( x ) = exp( x ) the author X  X   X  preferences by the relative num-ber of citations that the author received for the topic. This way, we can account for different  X  s over time, and reduce variance due to authors five authors with the highest TPS are displayed in the rightmost column of Table 2. These top-ics were among the roughly one third (out of 64) that seemed to coherently map to research topics within NLP. Some others corresponded to parts of a paper (e.g., explaining notation and formulae, experiments) or to stylistic groups (e.g.,  X  X atio-nal words X  including rather , fact , clearly , argue , clear , perhaps ). Others were not interpretable to us. 5.2 Predicting Responses We compare against two baselines for predicting in-community citations. Yogatama et al. (2011) is a strong baseline for predicting responses; they in-corporated n -gram features and metadata features in a generalized linear model with the time series prior discussed in  X  3.4. 6 We also compare against a version of our model without the author utility component. This equates to replacing Yogatama et al. X  X  features with LDA topic mixtures, and per-forming joint learning of the topics and citations; we therefore call it  X  X imeLDA. X  Without the time series component, TimeLDA would instantiate su-pervised LDA (McAuliffe and Blei, 2008). Fig-ure 2 shows the mean absolute error (MAE) for the three models.

With sufficiently many topics ( K  X  16 ), topic representations achieve lower error than surface features. Removing the author utility component from our model leads to better predictive perfor-mance. This is unsurprising, since our model forces  X  to explain both the responses (what is Figure 2: Mean absolute error (in citation counts) for predicted citation counts ( y -axis) against the number of topics K ( x -axis). Errors are in ac-tual citation counts, while the models are trained with log counts. TimeLDA significantly outper-forms Yogatama et al. (2011) for K  X  64 (paired t -test, p &lt; 0 . 01 ), while the differences between Yogatama et al. (2011) and author utility are not significant. The MAE is calculated over 3 random splits of the data with 809, 812, and 811 docu-ments in the test set respectively. evaluated here) and the divergence between author preferences  X  a and what is actually written. The utility model is nonetheless competitive with the Yogatama et al. baseline. 5.3 Predicting Words  X  X iven a set of authors, what are they likely to write? X   X  we use perplexity as a proxy to mea-sure the content predictive ability of our model. Perplexity on a test set is commonly used to quan-tify the generalization ability of probabilistic mod-els and make comparisons among models over the same observation space. For a document w d writ-ten by authors a d , perplexity is defined as perplexity ( w d | a d ) = exp  X  and a lower perplexity indicates better generaliza-tion performance. Using S samples from the in-ference step, we can compute p ( w d | a d ) = where  X  s is the s th sample of  X  , and  X  s is the topic-word distribution estimated from the s th sample of z .
 We compared the Author-Topic model of Rosen-Zvi et al. (2004). The AT model is simi-lar to setting  X  a = 0 for all authors, c d = 1 and using a Dirichlet prior instead of logistic nor-mal on  X  a . Figure 3 present the perplexity of these labeled each of these topics. Figure 3: Held-out perplexity (  X  10 3 , y -axis) with varying number of topics K ( x -axis). The differ-ences are significant between all models at K  X  64 (paired t-test, p &lt; 0 . 01 ). There are 523,381, 529,397, 533,792 phrase tokens in the random test sets. models at different values of K . We include a ver-sion of our author utility model that ignores tem-poral information ( X  X  X ime X ), i.e., setting T = 1 and collapsing all timesteps. We find that perplex-ity improves with the addition of the utility model as well as the temporal dynamics. 5.4 Exploration: Tradeoffs and Seniority Recall that  X  a encodes author a  X  X  tradeoff between increasing citations (high  X  a ) and writing papers on topics a prefers (low  X  a ). We do not claim that individual  X  a values consistently represent authors X  tradeoffs between citations and writing about preferred topics. We have noted a number of potentially confounding factors that affect au-thors X  choices, for which our data do not allow us Figure 4: Plot of authors X  median  X  (blue, solid) and mean citation counts (magenta, dashed) against their academic age in this dataset (see text for explanation). to control.

However, in aggregate,  X  a values can be ex-plored in relation to other quantities. Given our model X  X  posterior, one question we can ask is: do an author X  X  tradeoffs tend to change over the course of her career? In Figure 4, we plot the me-dian of  X  (and 95% credible intervals) for authors at different  X  X ges. X  Here,  X  X ge X  is defined as the number of years since an author X  X  first publication
A general trend over the long term is observed: researchers appear to move from higher to lower  X  . Statistically, there is significant dependence between  X  of an author and her age; the Spear-man X  X  rank correlation coefficient is  X  =  X  0 . 870 with p -value &lt; 10  X  5 . This finding is consis-tent with the idea that greater seniority brings increased and more stable resources and greater freedom to pursue idiosyncratic interests with less concern about extrinsic payoff. It is also consistent with decreased flexibility or openness to shifting topics over time.

To illustrate the importance of our model in making these observations, we also plot the mean number of citations per paper published (across all authors) against their academic age (magenta lines). There is no clear statistical trend between the two variables (  X  =  X  0 . 017 ). This suggests that through  X  , our model is able to pick up evi-dence of author X  X  optimizing behaviors, which is not possible using simple citation counts.

There is a noticeable effect during years 5 X 10, in which  X  tends to rise by around 40% and then fall back. (Note that the model maintains consider-able uncertainty X  X ider intervals X  X bout this ef-fect.) Recall that, for a researcher trained within the field and whose primary publication venue is in the ACL community, our measure of age cor-responds roughly to academic age. Years 5 X 10 would correspond to the later part of a Ph.D. pro-gram and early postgraduate life, when many re-searchers begin faculty careers. Insofar as it re-flects a true effect, this rise and fall suggests a stage during which a researcher focuses more on writing papers that will attract citations. How-ever, more in-depth study based on data that is not merely observational is required to quantify this effect and, if it persists under scrutiny, determine its cause.

The effect in year 24 of mean citations per paper (magenta line) can be attributed to well cited pa-pers co-authored by senior researchers in the field who published very few papers in their 24th year. Since there are relatively few authors in the dataset at that academic age, there is more variance in mean citations counts. Previous work on modeling author interests mostly focused on characterizing authors by their style (Holmes and Forsyth, 1995, inter alia ), 8 through latent topic mixtures of documents they have co-authored (Rosen-Zvi et al., 2004) and their collaboration networks (Johri et al., 2011). Like our paper, the latter two are based on topic models, which have been popular for modeling the content of scientific articles. For instance, Gerrish and Blei (2010) measured scholarly impact using dynamic topic models, while Hall et al. (2008) an-alyzed the output of topic models to study the  X  X is-tory of ideas. X 
Predicting responses to scientific articles was explored in two shared tasks at KDD Cup 2003 (Brank and Leskovec, 2003; McGovern et al., 2003) and by Yogatama et al. (2011), which served as a baseline for our experiments and whose time-series prior we used in our model. Furthermore, there has been considerable research using topic models to predict (or recommend) citations (in-stead of aggregate counts), such as modeling link probabilities within the LDA framework (Cohn and Hofmann, 2000; Erosheva et al., 2004; Nal-lapati and Cohen, 2008; Kataria et al., 2010; Zhu et al., 2013) and augmenting topics with discrimi-native author features (Liu et al., 2009; Tanner and Charniak, 2015).

We modeled both interests of authors and re-sponses to their articles jointly, by assuming authors X  text production is an expected utility-maximizing decision. This approach is similar to our earlier work (Sim et al., 2015), where au-thors are rational agents writing texts to maximize the chance of a favorable decision by a judicial court. In that study, we did not consider the unique preferences of each decision making agent, nor the extrinsic-intrinsic reward tradeoffs that these agents face when authoring a document.

Our utility model can also be viewed as a form of natural language generator, where we take into account the context of an author (i.e., his prefer-ences, the tradeoff coefficient, and what is popu-lar) to generate his document. This is related to natural language pragmatics, where text is influ-problem of generating text under pragmatic cir-cumstances from a planning and goal-orientation perspective, while Vogel et al. (2013) used multi-agent decision-theoretic models to show cooper-ative pragmatic behavior. Vogel et al. X  X  models suggest an interesting extension of ours for future work: modeling cooperation among co-authors and, perhaps, in the larger scientific discourse. We presented a model of scientific authorship in which authors trade off between seeking citation by others and staying true to their individual pref-erences among research topics. We find that topic modeling improves over state-of-the-art text re-gression models for predicting citation counts, and that the author utility model generalizes better than simpler models when predicting what a particular group of authors will write. Inspecting our model suggests interesting patterns in behavior across a researcher X  X  career.
 The authors thank the anonymous reviewers for their thoughtful feedback and members of the ARK group at CMU for their valuable com-ments. This research was supported in part by an A*STAR fellowship to Y. Sim, by a Google re-search award, and by computing resources from the Pittsburgh Supercomputing Center; it was completed while NAS was at CMU.
 We sample each  X  a,j , for j = 1 ...K , and  X  a blockwise across time steps using Metropolis-Hastings algorithm with a multivariate Gaussian proposal distribution and likelihood:  X  exp  X   X   X  exp  X   X  Likewise,  X  d is sampled blockwise for each docu-ment with a multivariate Gaussian distribution and likelihood: For c d , we first sampled each c d from a multivari-ate Gaussian distribution, and applied a logistic transformation to map it onto the simplex. The likelihood for c d is:
