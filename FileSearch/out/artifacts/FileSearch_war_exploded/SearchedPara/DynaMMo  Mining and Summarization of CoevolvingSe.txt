 Given multiple time sequences with missing values, we pro-pose DynaMMo which summarizes, compresses, and finds latent variables. The idea is to discover hidden variables and learn their dynamics, making our algorithm able to function even when there are missing values.

We performed experiments on both real and synthetic datasets spanning several megabytes, including motion cap-ture sequences and chlorine levels in drinking water. We show that our proposed DynaMMo method (a) can suc-cessfully learn the latent variables and their evolution; (b) can provide high compression for little loss of reconstruction accuracy; (c) can extract compact but powerful features for segmentation, interpretation, and forecasting; (d) has com-plexity linear on the duration of sequences.
 Categories and Subject Descriptors: H.2.8 Database applications: Data mining I.2.6 Artificial Intelligence: Learn-ing -parameter learning General Terms: Algorithms; Experimentation.
 Keywords: Time Series; Missing Value; Bayesian Network; Expectation Maximization (EM).
Time series data are abundant in many application ar-eas such as motion capture, sensor networks, weather fore-casting, and financial market modeling. The major goal of analyzing these time sequences is to identify hidden pat-terns so as to forecast the future trends. There exist many mathematical tools to model the evolutionary behavior of time series (e.g. Linear Regression, Auto-Regression, and AWSOM [15]). These methods generally assume completely available data. However, missing observations are hardly rare in many real applications, thus it remains a big chal-lenge to model time series in the presence of missing data.
We propose a method to handle the challenge, with occlu-sion in motion capture as our driving application. However, as shown in the experiments, our method is capable of han-Figure 1: Reconstruction for a jump motion with 322 frames in 93 dimensions of bone coordinates.
 Blue line: the original signal for rootbone z-coordinate -the dash portion indicates occlusion from frame 100 to 200. The proposed DynaMMo, in red, gets very close to the original, outperforming all competitors. dling missing values in diverse settings: sensor data, chlorine levels in drinking water system, and other similar coevolving sequences.

Motion capture is a technique to produce realistic mo-tion animation. Typical motion capture system use cameras to track passive markers on human actors. However, even when multiple cameras are used, some markers may be out of view  X  especially in complex motions like handshaking or modern dance. Handling occlusions is currently a manual process, taking hours/days for human experts to fill in the gaps. Figure 2 illustrates a case of motion-capture data, with occlusions: A dark cell at row j ,andcolumn t denotes a missing value, for that specific time ( t -th frame/column) and for that specific joint-angle ( j -th row).

The focus of our work is to handle occlusions automati-cally. Straightforward methods like linear interpolation and spline interpolation give poor results (see Section 4). Ideally we would like a method with the following properties: 1. Effective: It should give good results, both with re-2. Scalable: The computation time of the method should 3. Black-outs: It should be able to handle  X  X lack-outs X ,
In this paper, we propose DynaMMo , an automatic method Figure 2: Occlusion in handshake motion. 66 joint angles (rows), for  X  200 frames. Dark color indi-cates a missing value due to occlusion. Notice that occlusions are clustered. to learn the hidden pattern and handle missing values. Fig-ure 1 shows the reconstructed signal for an occluded jumping motion. Our DynaMMo gives the best result close to the original value. Our main idea is to simultaneously exploit smoothness and correlation. Smoothness is what splines and linear interpolation exploit: for a single time-sequence (say, the left-elbow x-value over time), we expect successive en-tries to have nearby values ( x n  X  x n +1 ). Correlation reflects the fact that sequences are not independent; for a given mo-tion (say,  X  X alking X ), the left-elbow and the right-elbow are correlated, lagging each other by half a period. Thus, when we are missing x n , say, the left elbow at time-tick n ,wecan reconstruct it by examining the corresponding values of the right elbow (say, y n  X  1 ,y n ,y n +1 ). This two-prong approach canhelpushandleeven X  black-outs  X , which we define as time intervals where we lose track of all the time-sequences.
The main contribution of our approach is that it shows how to exploit both sources of redundancy (smoothness and correlation) in a principled way. Specifically, we show how to set up the problem as a Dynamic Bayesian Network and solve it efficiently, yielding results with the best reconstruc-tion error and agreeing with human intuition. Furthermore, we propose several variants based on DynaMMo for ad-ditional time series mining tasks such as forecasting, com-pressing, and segmentation.

The rest of the paper is organized as follows: In Section 2, we review the related work; the proposed method and its discussion are presented in Section 3; the experimental re-sults are presented in Section 4. Section 5 discusses addi-tional benefits of our method: interpretation, compression and segmentation. Finally, Section 6 concludes the paper.
Interpolation methods, such as linear interpolation and splines, are commonly used to handle missing values in time series. Both linear interpolation and splines estimate the missing values based on continuity in a single sequence. While these methods are generally effective for short gaps, they ignore the correlations among multiple dimensions.
Singular Value Decomposition (SVD) and Principal Com-ponent Analysis (PCA) [20] are powerful tools to discover linear correlations across multiple sequences, with which it is possible to recover missing values in one sequence based on observations from others. Srebro and Jaakkola [18] have proposed an EM approach (MSVD) to factor the data into low rank matrices and approximate missing value from them. We will describe MSVD in appendix, and we show that it is a special case of our model. Brand [1] further develop an incremental algorithm to fast compute the singular de-composition with missing values. Similar to the missing value SVD approach, Liu and McMillan [13] have proposed a method that projects motio n capture markers positions into linear principal components and reconstructs the miss-ing parts from the linear models. Furthermore, they pro-posed an enhanced Local Linear method from a mixture of such linear models. Park and Hodgins [16] have also used PCA to estimate the missing markers for skin deformation capturing. In another direction, Yi et al [21] have proposed a online regression model over time across multiple dimen-sion that is in extension to Autoregression (AR), thus could handle missing values.

There are several methods specifically for modeling mo-tion capture data. Herda et al [5] have used a human body skeleton to track and reconstruct the 3-d marker positions. If a marker is missing, it could predict the position using three previous markers by calculating the kinetics. Hsu et al [6] have proposed a method to map from a motion control specification to a target motion by searching over patterns in existing database. Chai and Hodgins [2] uses a small set of markers as control signals and reconstruct the full body motion from a pre-recorded database. The subset of mark-ers should be known in advance, while our method does not assume fixed subsets observed or missing. As an alter-native non-parametric approach, Lawrence and Moore [9] model the human motion using hierarchical Gaussian pro-cesses. [13] provides a nice summary of related work on occlusion for motion capture data as well as of techniques for related tasks such as motion tracking.

There are many related work in time series representa-tion [14, 12, 17], indexing [8], classification [3, 19] and out-lier detection [10]. Mehta et al [14] proposed a represen-tation method for time varying data based on motion and shape information including linear velocity and angular ve-locity. With this representation, they track the tangible fea-tures to segment the sequence trajectory. Symbolic aggre-gate approximation (SAX) [12] is a symbolic representation for time series data, and later generalized for massive time series indexing (iSAX) [17]. Keogh et al use uniform scaling when indexing a large human motion database [8]. Lee et al [10] proposed the TRAOD algorithm to identify outliers in a trajectory database. In their approach, they first parti-tion the trajectories into small segments and then use both distance and density to detect abnormal sub-trajectories. Gao et al [3] proposed an ensemble model to classify the data streams with skewed class distributions and concept drifts. Their approach is to undersample the dominating class, oversample or repeat the rare class and then partition the data set and perform individual training. The trained models are then combined evenly into the resulting classifi-cation function. However, none of these methods can handle missing values.

Our method is also related to Kalman Filters and other adaptive filters conventionally used in tracking system. Jain et al [7] have adapted Kalman Filters for reducing commu-nication cost in data stream. Tao et al [19] have proposed a recursive filter to predict and index moving objects. Li et al [11] used Kalman filter to stitch motions in a natural way. While our method includes Kalman Filter as a special case, DynaMMo can effectively cope with missing values.
Given a partially observed multi-dimensional sequence, we propose DynaMMo , to identify hidden variables, to mine their dynamics, and to recover missing values. Our moti-vation comes from noticing two common properties of time series data: temporal continuit y and spatial correlation. On one hand, by exploiting continuity as many interpolation methods do, we expect that missing values are close to ob-servations in neighboring time ticks and follow their moving trends. On the other hand, by using the correlation be-tween difference sequences as SVD does, missing values can be inferred from other observation sources. Our proposed approach makes use of both, to better capture patterns in coevolving sequences.
We will first define the problem of time series missing value recovery, and then present our proposed DynaMMo . Table 1 explains the symbols and annotations.

Definition 1. Given a time sequence X with duration T in m dimensions, X = { x 1 ,..., x T } , to recover the missing part of the observations indicated by W . w t,k =0 whenever X  X  X  k -th dimensional observation is missing at time t ,and otherwise w t,k =1 .Let us denote the observed part as X g and the missing part as X m .

We build a probabilistic model (Figure 3) to estimate the expectation of missing values conditioned on the observed parts, E [ X m |X g ]. We use a sequence of latent variables (hid-den states), z n , to model the dynamics and hidden patterns of the observation sequence. Like SVD, we assume a linear projection matrix G from the latent variables to the data se-quence (both observed and missing) for each time tick. This mapping automatically captures the correlation between the observation dimensions; thus, if some of the dimensions are missing, they can be inferred fro m the latent variables. For example, the states could correspond to degrees of freedom, the velocities, and the accelerations in human motion cap-ture data (although we let DynaMMo determine them, au-tomatically); while the observed marker positions could be calculated from these hidden states.To model temporal con-tinuity, we assume the latent variables are time dependent with the values determined from the previous time tick by a linear mapping F . In addition, we assume an initial state for latent variables at the first time tick. Eq (1 -3) give the mathematical equations of our proposed model, with the Figure 3: Graphical Illustration of the Model. z 1  X  X  X  4 : latent variables; x 1 , 2 , 4 :observations;x 3 : partial ob-servations. Arrows denote Gaussian distributions. parameters  X  = { F , G , z 0 ,  X  ,  X  ,  X  } . where z 0 is initial state of the latent variables. F implies the transition and G is the observation projection.  X  0 ,  X  and i ( i =1 ... T) are multivariate Gaussian noises with the following distributions: The model is similar to Linear Dynamical System except that it includes an additional matrix W to indicate the miss-ing observations. The joint distribution of X m , X g and given by P ( X m , X g and Z )= P ( z 1 )  X 
Given an incomplete data sequence X and the indication sequence W ,wepropose DynaMMo method to estimate: 1. the governing dynamics F and G ,aswellasotherpa-2. the latent variables  X  z n = E [ z n ], ( n =1 ... T); 3. the missing values of the observation sequence E [ X m
The goal of parameter estimation is achieved through max-imizing the likelihood of observed data, L (  X  )= P ( X g ever, it is difficult to directly maximize the data likelihood in missing value setting, instead, we maximize the expected log-likelihood of the observation sequence. Once we get the model parameters, we use belief propagation to estimate the occluded marker positions. We define the following objective function as the expected log-likelihood Q (  X  )withrespectto the parameters  X  = { F , G ,z 0 ,  X  ,  X  ,  X  } : where D () is the square of the Mahalanobis distance D ( x , y ,  X ) = ( x  X  y ) T  X   X  1 ( x  X  y )
Our proposed DynaMMo searches for the optimal solu-tion using Expectation-Maximization [4]. The optimization algorithm is actually an iterative, coordinate descent pro-cedure: estimating the latent variables, maximizing with respect to parameters, estimating the missing values, and iterating until convergence.

To estimate the parameters, taking the derivatives of Eq 7-8 with respect to the components of  X  new and setting them to zero yield the following results:
The calculation of optimal parameters in Eq 9-14 requires estimation of latent variables, which includes our second goal. We use a belief propagation algorithm to estimate the posterior expectations of latent variables, similar to mes-sage passing in Hidden Markov Model and Linear Dynam-ical Systems. The general idea is to compute the posterior distribution of latent variables tick by tick, based on the computation of previous time tick.

Finally, the missing values are easily computed from the estimation of latent variables using Markov property in the graphical model (Figure 3). We have the following equation:
The overall algorithm is described in Algorithm 1, omit-ting details explained in Appendix A.1. Model Generality: Our model includes MSVD, linear in-terpolation, and Kalman filters as special cases: Penalty and Constraints: In the algorithm described above, Eq. (7) is error term for the initial state. Eq. (8) is trying to estimate the dynamics for the hidden states, Algorithm 1 : DynaMMo Input : Observed data sequence: X = X g ,
Missing value indication matrix: W the number of latent dimension H
Output :
Initialize  X  X with X g and the missing value filled by linear interpolation or other methods; Initialize F , G ,z 0 ;
Initialize  X  ,  X  ,  X tobeidentitymatrix;  X   X  X  F , G ,z 0 ,  X  ,  X  ,  X  } ; repeat until converge ; while Eq. (8) is getting the best projection from observed motion sequence to hidden states. Eq. (8) is penalty for the covariance, similar to model complexity in BIC. It is easy to extend the model by putting a further penalty on the model complexity through a Bayesian approach. For example, we could constraint the covariance to be diagonal  X  2 I ,whichis used in our experiments, since it is faster to compute. Time Complexity: The algorithm needs time linear on the duration T , and specifically O (#( iterations )  X  T  X  m 3 we expect it to scale well for longer sequences. As a point of reference, it takes about 6 to 10 minutes per sequence with several hundreds time ticks, on a Pentium class desktop.
We evaluate both quality and scalability of DynaMMo on several datasets. To evaluate the quality of recovering missing values, we use a real dataset with part of data treated as  X  X issing X  so that it enables comparing the real observations with the reconstructed ones. In the following we first describe the dataset and baseline methods, and then present the reconstruction results.
We use linear interpolation and Missing Value SVD (MSVD) as the baseline methods. We also compare to spline inter-polation.

Missing Value SVD involves iteratively taking the SVD and fitting the missing values from the result [18]. This method is very easy to implement and already used on mo-tion capture datasets in [13] and [16]. In our implementation (appendix A.2), we initialized the holes by linear interpola-tion, and use 15 principal dimensions (99% of energy). Chlorine Dataset (Chlorine): The Chlorine dataset (see sample in Figure 6(a)) was produced by EPANET 2 1 that models the hydraulic and water quality behavior of water distribution piping systems. EPANET can track, in a given water network, the water level and pressure in each tank, the water flow in the pipes and the concentration of a chem-ical species (Chlorine in this case) throughout the network within a simulated duration. The data set consists of 166 nodes (pipe junctions) and measurement of the Chlorine concentration level at all these nodes during 15 days (one measurement for every 5 minutes, a total of 4310 time ticks). Since the water demand pattern during the 15 days follows a clear global periodic pattern (daily cycle, dominating res-idential demand pattern), EPANET would correctly reflect the pattern in the Chlorine concentration with a few excep-tions and slight time shifts.
 Full body motion set (Motion): This data set contains 58 full body motions of walking, running, and jumping mo-tions from subject #16 of mocap database 2 . Each motion spans several hundred of frames with 93 features of bone positions in body local coordinates. The total size of the dataset is 17MB. We make random dropouts and reconstruct the missing values on the data set.
We create synthetic occlusions (dropouts) on the Motion data and evaluate the effectiveness of reconstruction by Dy-naMMo . To mimic a real occlusion, we collected the occlu-sion statistics from handshake motions. For example, there are 10.44% of occluded values in typical handshake motions, and occlusions often occur consecutively (Figure 2). To cre-ate synthetic occlusions, we randomly pick a marker j and the starting point (frame) n for the occlusion of this marker; we pick the duration as a Poisson distributed random vari-able according to the observed statistics, and we repeat, un-til we have occluded 10.44% of the input values.
We present three sets of results, to illustrate the quality of reconstruction of DynaMMo .
Figure 1 shows the reconstructed signal (root bone z-coordinate) for a jump motion. Splines find a rather smooth curve which is not what the human actor really did. Lin-ear interpolation and MSVD are a bit better while still far from the ground truth. Our proposed DynaMMo (with 15 hidden dimensions) captured both the dynamics of the mo-tion as well as the correlations across the given inputs, and achieved a very good reconstruction of the signal.
For each motion in the data set, we create a synthetic occluded motion sequence as described above, reconstruct using DynaMMo , then compare the effectiveness against linear interpolation, splines and MSVD. To reduce random effects, we repeat each experiment 10 times and we report the average of MSE. To evaluate the quality, we use the MSE: Given the original motion X , the occlusion indica-tion matrix W and the fitted motion  X  X , the MSE is the http://www.epa.gov/nrmrl/wswrd/dw/epanet.html http://mocap.cs.cmu.edu Figure 5: Average error for missing value recovery on a sample mocap data (subject#16.22). Average rmse over 10 runs, versus average missing length  X  (from 10 to 100). Randomly 10.44% of the values are treated as  X  X issing X . DynaMMo (in red solid line) wins. Splines are off the scale. average of squared differences between the actual ( X )and reconstructed (  X  X ) missing values -formally: Both MSVD and our method use 15 hidden dimensions ( H = 15). Figures 4(a)-4(c) show the scatter plots of the average reconstruction error over 58 motions in the Motion dataset, with 10% missing values and 50 average occlusion length. It is worth to noting that the reconstruction grows little with increasing occlusion length, compared with other alternative methods (Figure 5). There is a similar result found in exper-iments on Chlorine data as shown in Figure 6(b). Again, our proposed DynaMMo achieves the best performance among the four methods.
As we discussed in Section 3, the complexity of DynaMMo is O (#( iterations )  X  T  X  m 3 ). Figure 7 shows the running time of the algorithm on the Chlorine dataset versus the sequence length. For each run, 10% of the Chlorine concentration levels are treated as missing with average missing length 40. As expected, the wall clock time is almost linear to sequence duration.
In addition to recovering the missing observations, our (a) Sample Chlorine data. Figure 6: Reconstruction experiment on Chlorine with 10% missing and average occlusion length 40. Figure 7: Running time versus the sequence length on Chlorine dataset. For each run, 10% of the values are treated as  X  X issing X . proposed DynaMMo method can be easily extended for further data mining tasks . Here we propose several Dy-naMMo extensions for time series compression, segmenta-tion, and forecasting.
One of the advantages of our proposed DynaMMo is that its learnt representation of a data stream contains informa-tion about behavior of patterns and trends, as illustrated in the following examples. As shown in top of Figure 8, we create a sinusoid sequence with a cycle of 32, then learn the parameters  X  = { F , G ,z 0 ,  X  ,  X  ,  X  } using DynaMMo with hidden dimension of 6, and then generate a simulated signal  X  x using only the parameter z 0 , F , G :  X  z 1 = z 0 ,  X  z  X  x n = G  X  z n . The simulated signal in Figure 8 presents an identical periodic pattern to the original signal, with negli-gible difference in amplitude. Note it is easy to forecast the future if we continue ticking the time. As this simple case demonstrates, DynaMMo is able to learn dynamics from a data stream and reproduce its behavior even without ob-servations. This suggests that it could be used to compress time sequences, as we will discuss in the next section.
Time series data are usually real valued which makes it hard to achieve a high compression ratio using lossless meth-ods. However, lossy compression is reasonable if it gets a high compression ratio and low recovery error. As de-scribedinSection3, DynaMMo produces three outputs: Figure 8: Simulated signal versus original signal. The top is the original signal (blue curve), while the bottom (red curve) is the generated using pa-rameters learnt from original with a hidden dimen-sion of 6. Note the almost identical pattern in both generated and original signal, and will continue if prolonged. model parameters, latent variables (posterior expectation) and missing values. To compress, we record some of the hidden variables learned from DynaMMo instead of stor-ing direct observations. By controlling the hidden dimension and the number of time ticks of hidden variables to keep, it is easy to trade off between co mpression ratio and error. We provide three alternatives for compression.

Here we first present the decompression algorithm in Al-gorithm 2.
The fixed compression will first learn the hidden variables using DynaMMo and store the hidden variables for every k time ticks. In addition, it stores the matrix F ,  X  , G and  X  . Both covariance  X  and  X  are constrained to  X  2 I and  X  I respectively. It also stores the number k .
 The total space required for fixed compression is S f =  X  H + H 2 + H  X  m +3, where k is the gap number given.
The adaptive compression will first learn the hidden vari-Algorithm 2 : DynaMMo Decompress Input :  X  z S , hidden variables, indexed by S  X  [1  X  X  X  T],
Output : The decompressed data sequence  X  x 1  X  X  X  T y  X   X  z 1 ; for n  X  1 to T do ables using DynaMMo and store the hidden variable only for the necessary time ticks, when the error is greater than a given threshold. Like fixed co mpression, it also stores the matrix F ,  X  , G and  X  . Both covariance  X  and  X  are con-strained to  X  2 I and  X  2 I respectively. For each stored time tick, it also records the offset of next storing time tick.
The total space required for adaptive compression is S a = l  X  ( H +1)+ H 2 + H  X  m +2 where l is the number of stored time ticks.
The optimal compression will first learn the hidden vari-ables using DynaMMo and store the hidden variables for the time ticks determined by dynamic programming, so as to achieve the smallest error for a given number of stored time ticks. Like the fixed compression, it also stores the matrix F ,  X  , G and  X  . Both covariance  X  and  X  are constrained to  X  2 I and  X  2 I respectively.

The total space required for optimal compression is S d = l  X  ( H +1)+ H 2 + H  X  m +2 where k is the number of stored time ticks.
We use a combined method of SVD and linear interpo-lation as our baseline. It works as follows: given k and h , it first projects the data into h principle dimensions using SVD, then records the hidden variables for every k time ticks. In addition, it will also record the projection matrix from SVD. When decompressing, the hidden variables are projected back using the stored matrix and the gaps filled with linear interpolation.
 The total space required for baseline compression is S b =  X  h + h  X  m + h +1, where k is the gap number given, and h is the number of principle dimensions.

For all of these methods, the compression ratio is defined as
Figure 9 shows the decompre ssion error (in terms of RMSE) with respect to compression ratio compared with the base-line compression using a combined method SVD and Linear Interpolation. DynaMMo d wins especially in high com-pression ratio.
As a further merit, our DynaMMo is able to segment the data sequence. Intuitively, this is possible because Dy-naMMo identifies the dynamics and patterns in data se-quences, so segments with different patterns can be expected Figure 9: Compression for Chlorine dataset: RMSE versus compression ratio. Lower is better.
 DynaMMo d (in red solid) is the best. to have different model parameters and latent variables. We use the reconstruction error as an instrument of segmenta-tion. Note that since there might be missing value in the data sequences, a normalization procedure with respect to the number observation at each time tick is required. We present our segmentation method in Algorithm 3.
 Algorithm 3 : DynaMMo Segment Input : Data sequence: X ,
With or without missing value indication matrix: W the number of latent dimension H
Output : The segmentation position s { G ,  X  z 1  X  X  X  m } X  DynaMMo ( X, W, H ); for n =1 to m do find the split: s  X  arg max
To illustrate, Figure 10 shows the segmentation result on a sequence composed of two pieces of sinusoid signals with different frequencies. Our segmentation method could cor-rectly identify the time of frequency change by tracking the spikes in reconstruction erro r. Figure 11 shows the recon-struction error from segmentat ion experiment on a real hu-man motion sequence in which an actor running to a com-plete stop. Two (y-coordinates of left hip and femur) of 93 joint coordinates are shown in the top of the plot. Note the spikes in the error plot coincide with the slowdown of the pace and transition to stop.
Given multiple time sequences, we propose DynaMMo (Dynamics Mining with Missing values), which includes a learning algorithm and its variant extensions to summarize, compress and find latent variables. The idea is to auto-matically discover a few, hidden variables and to compactly describe how the hidden variables evolve by learning their transition matrix F . Our algorithm can even work when there are missing observations, and includes Kalman filters as special case.

We presented experiments on motion capture sequences and chlorine measurements and demonstrated that our pro-Figure 10: Segmentation result on one dimensional synthetic data. Top is a sequence composed of two pieces of sinusoid signals with different frequencies 64 and 128 respectively. Bottom is the reconstruc-tion error per time tick. Note the spike in the middle correctly identify the shifting of frequencies. posed DynaMMo method and its extensions (a) can suc-cessfully learn the latent variables and their evolution, (b) can provide high compression for little loss of reconstruc-tion accuracy, and (c) can extract compact, but powerful features, for sequence forecasting, interpretation and seg-mentation, (d) scalable on duration of time series.
Acknowledgments. This material is based upon work supported by the National Science Foundation under Grants No.DBI-0640543, by the iCAST project sponsored by the NSC, Taiwan, under the Gr ants No. NSC97-2745-P-001-001 and by the Ministry of Economic Affairs, Taiwan, under the Grants No. 97-EC-17-A-02-R7-0823, also, under the aus-pices of the U.S. Department of Energy by University of Cal-ifornia Lawrence Livermore National Laboratory under con-tract DE-AC52-07NA27344 (LLNL-CONF-404625), subcon-tracts B579447, B580840. The data used in this project was obtained from mocap.cs.cmu.edu supported by NSF EIA-0196217. This work is also partially supported by an IBM Faculty Award, a Yahoo Research Alliance Gift, a SPRINT gift, with additional funding from Intel, NTT and HP. Any opinions, findings, and conclusions or recommendations ex-pressed in this material are those of the author(s) and do not necessarily reflect the views of the funding parties. [1] M. Brand. Incremental singular value decomposition [2] J. Chai and J. K. Hodgins. Performance animation [3] J.Gao,B.Ding,W.Fan,J.Han,andP.S.Yu.
 [4] Z. Ghahramani and M. I. Jordan. Supervised learning Figure 11: Reconstruction error plot for segmen-tation on a real motion capture sequence in 93 di-mensions (subject#16.8) with 250 frames, an actor running to a complete stop, with left hip and femur y-coordinates shown in top plots. The spikes in bot-tom plot coincide with the slowdown of the pace and transition to stop. [5] L.Herda,P.Fua,R.Pl  X  ankers, R. Boulic, and [6] E.Hsu,S.Gentry,andJ.Popovi  X  c. Example-based [7] A. Jain, E. Y. Chang, and Y.-F. Wang. Adaptive [8] E. Keogh, T. Palpanas, V. B. Zordan, D. Gunopulos, [9] N. D. Lawrence and A. J. Moore. Hierarchical [10] J.-G. Lee, J. Han, and X. Li. Trajectory outlier [11] L. Li, J. McCann, C. Faloutsos, and N. Pollard. [12] J. Lin, E. Keogh, S. Lonardi, and B. Chiu. A symbolic [13] G. Liu and L. McMillan. Estimation of missing [14] S. Mehta, S. Parthasarathy, and R. Machiraju. On [15] S. Papadimitriou, A. Brockwell, and C. Faloutsos. [16] S. I. Park and J. K. Hodgins. Capturing and [17] J. Shieh and E. Keogh. isax: indexing and mining [18] N. Srebro and T. Jaakkola. Weighted low-rank [19] Y. Tao, C. Faloutsos, D. Papadias, and B. Liu. [20] M. E. Wall, A. Rechtsteiner, and L. M. Rocha. [21] B.-K. Yi, N. D. Sidiropoulos, T. Johnson, H. V.
In Algorithm 1, the estimation (line 1) is to find the marginal distribution for hidden state variables given the data, e.g.  X  z n = E [ z n |X g , X m ]( n =1 ,..., T). Since both prior and conditional distributions in the model are Gaus-sian, the posterior up to current time tick p ( z n | x 1 should also be Gaussian, denoted by  X   X  ( z n )= N (  X  n Let p ( x n | x 1 ,..., x n  X  1 ) denoted as c n ,Wehavethefollow-ing propagation equation:
From Eq 17 we could obtain the following forward passing ofthebelief. Themessageshereare  X  n , V n and P n  X  1 (needed in later backward passing).
 The initial messages are given by:
For the backward passing, let  X  ( z n ) denote the marginal posterior probability p ( z n | x 1 ,..., x N ) with the assumption: The backward passing equations are:
Hence, the expectation for Algorithm 1 line 1 are com-puted using the following equations: where the expectations are taken over the posterior marginal distribution p ( z n | y 1 ,..., y N ).

From these estimations, the new parameter  X  new is obtain by maximizing the Equations 7-8 with respect to the com-ponents of  X  new given the current estimate of  X  old , yield the Algorithm 1 line 2.
 Algorithm 4 : Missing Value SVD Input : Observed data matrix: X g ,
Occlusion indication matrix: W the number of hidden dimensions H Output : Estimated data matrix:  X  X
Initialize  X  X with X g and the missing value filled by linear interpolation; repeat until converge ;
