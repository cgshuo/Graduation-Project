
Recently a number of modeling techniques have been developed for data mining and machine learning in rela-tional and network domains where the instances are not in-dependent and identically distributed (i.i.d.). These meth-ods specifically exploit the statistical dependencies among instances in order to improve classification accuracy. How-ever, there has been little focus on how these same de-pendencies affect our ability to draw accurate conclusions about the performance of the models. More specifically, the complex link structure and attribute dependencies in net-work data violate the assumptions of many conventional statistical tests and make it difficult to use these tests to assess the models in an unbiased manner. In this work, we examine the task of within-network classification and the question of whether two algorithms will learn models which will result in significantly different levels of perfor-mance. We show that the commonly-used form of evalua-tion (paired t-test on overlapping network samples) can re-sult in an unacceptable level of Type I error. Furthermore we show that Type I error increases as (1) the correlation among instances increases and (2) the size of the evalua-tion set increases (i.e., the proportion of labeled nodes in the network decreases). We propose a method for network cross-validation that combined with paired t-tests produces more acceptable levels of Type I error while still providing reasonable levels of statistical power (i.e., Type II error).
The seminal work of Dietterich [3] focused on enumer-ating the types of statistical questions that analysts could ask of the models and algorithms that they develop and/or learn. His work outlined a taxonomy of questions that dif-ferentiates between algorithm and model performance, and whether the goal is to estimate accuracy or to choose be-tween models/algorithms. Within this taxonomy, Dietterich formulated the question that is most central to data min-ing and machine learning research: Given two learning al-gorithms A and B and a dataset of size S from a domain D, which algorithm will produce more accurate classifiers when trained on other datasets of size S drawn from D? This question explicitly formulates the notion of generalization and provides a means to test the notion statistically.
Within this framework, Dietterich analyzed the char-acteristics of five statistical tests that can be used to as-sess generalization performance and showed that two of the tests in widespread use (at that time) had a high probabil-ity of Type I error (i.e., the tests will likely lead to an er-roneous conclusion of algorithm difference when there is none). Overall, Dietterich X  X  work showed that the over-lap in training/test sets combined with imbalanced sam-ples can lead to higher Type I errors due to biased esti-mates of mean performance difference between two algo-rithms. Therefore, a methodology that reduces the overlap between the training and test sets leads to lower Type I er-rors. Based on this analysis, Dietterich developed a novel 5x2 cross-validation test, which has lower Type I error than the standard cross-validation test but slightly worse statisti-cal power (i.e., higher Type II error).

However, Dietterich X  X  work only considered i.i.d. data where the instances are independent. In this work, we con-sider the task of comparing algorithm performance on the task of within-network relational learning. Within-network relational learning aims to generalize within a single rela-tional data graph X  X odels are learned on a partially labeled network and then applied to predict the class labels in the remainder of the network (i.e., the unlabeled portion). In many real world applications, relational learning tasks fall naturally into the within-network classification setting. For example, in the task of research paper classification, new papers to be classified usually have citation links to papers in the past whose topics are known. Similarly, in fraud de-tection, brokers whose fraud status is yet to be determined might associate with other brokers who have already been identified as fraudulent or not.

Within-network relational learning tasks have two char-acteristics that can complicate the application of conven-tional statistical tests for comparing generalization perfor-mance. First, the instances in the network are not inde-pendent. Indeed, relational learning algorithms are specifi-cally trying to exploit the dependencies among instances to improve prediction accuracy. The dependencies among in-stances, however, tend to result in correlated errors among the instances. These correlated errors can increase the im-balance between network samples and this can lead to in-creased Type I errors. Second, the size of the training and test sets are dependent and thus as the proportion of labeled data decreases, the size of the test set increases. This re-sults from the fact that the models are learned/applied to a partially labeled network with varying levels of labeled instances and the full set of unlabeled instances are typi-cally used for evaluation. As the size of the unlabeled set increases, the dependencies between samples increases and this can also lead to increased Type I errors.

In this paper, we consider the following question: Given two learning algorithms A and B and a partially-labeled network from domain D, and with S L labeled instances and S U unlabeled instances ( S = S L + S U ), which algo-rithm will produce more accurate classifiers when trained on other partially-labeled networks of size S drawn from D? We investigate the performance of a number of com-mon statistical tests, using both simulated and real classi-fiers, and both synthetic and real datasets. The experimen-tal methodology and empirical results for each combination can be found in the following sections: Synthetic Data Section 4 Section 5
Our findings indicate that a commonly-used method of statistical assessment X  X aired t-tests on repeated samples of randomly selected network samples (labeled training set and unlabeled test set) X  X esults in unacceptably high levels of Type I error. We propose a method for network cross-validation that combined with unpaired t-tests produces low levels of Type I error at the expense of reduced statistical power. Combining network cross-validation with paired t-tests is a good compromise, resulting in both acceptable lev-els of Type I error and reasonable levels of statistical power. The contributions of this work include:  X  Formulation of important statistical questions for com- X  Discussion and demonstration of the challenges of net- X  A proposed solution, network cross-validation , which  X  Empirical evaluation of statistical test characteristics
Table 1. Error correlation and relational autocorrela-In the previous section we described two sources of Type I error in within-network classification: 1. Inter-instance dependencies lead to correlated errors. 2. Small training sets lead to large test sets, increasing the
In Section 3.1 we describe how existing resampling pro-cedures create dependent samples. Here, we demonstrate that within-network classifiers produce correlated errors.
To test the conjecture that within-network classifiers pro-duce correlated errors, we experimented with several rela-tional classifiers and real-world classification tasks, using the  X  coefficient to measure the correlation of 0-1 errors over all pairs of related (i.e., linked) instances. We used a non-learning relational neighbor classifier [13] and a learn-ing link-based classifier [12]. We ran each classifier both with and without collective inference on a number of pre-diction tasks. Table 1 shows the amount of measured er-ror correlation for each task, averaged over all classifiers though we report averages, we should note that all trials (i.e., tasks/classifiers) exhibited some degree of error cor-relation. We can observe that the level of error correlation is correlated with the level of relational autocorrelation (see e.g., [18]) in the class label. Since autocorrelation has been shown to be essentially ubiquitous in relational data, this suggests that error correlation is widespread as well.
Statistical tests for comparing classifiers generally con-sist of two parts: (1) The resampling procedure dictates how the available data is partitioned into training and test sets for estimation of classifier performance (i.e., how many times is the classifier trained and tested? , which data is used to train the classifier? , which data is used to test the classi-fier? ) and (2) The significance test takes the classification results from the resampling trials and makes a determina-tion as to whether observed differences reflect a true differ-ence in classifier performance or whether it is likely to have occurred by chance alone.
Given a fully labeled network of size S , we consider three resampling procedures to generate training (labeled set S L ) and test (unlabeled set S U ) sets to evaluate within-network classification algorithms: simple random resam-pling (RRS), equal-instance random resampling (ERS), and network cross-validation (NCV). The first two methods have been used extensively in past work on relational learn-ing algorithms (see Section 3.3 for more detail). The third method is a new approach, based on the incremental cross-validation procedure outlined in Cohen [2] for generating learning curves, which will be more robust to Type I error.
Tables 2 and 3 outline the procedures for RRS and ERS, respectively. Both methods involve repeated random draws from the sample population to generate the training/test splits; and, therefore, produce overlapping test sets. How-ever, ERS ensures that each instance in the original sample occurs in exactly the same number of test sets in the collec-tion of resamples.

Table 4 outlines the NCV procedure that eliminates over-lap between test sets altogether. The procedure samples for k disjoint test sets that will be used for evaluation. Then for each test set fold, the remaining folds are merged together and the training set of size S L is randomly sampled from the merged set. When the training set size is less than the size of the merged folds (i.e., S L &lt; ( k  X  1) S k ), this will leave a set of unlabeled nodes that are neither in the test set nor the training set. Since these unlabeled instances will likely be connected to nodes in the test set, we will run col-lective inference over the full set of unlabeled nodes (the inference set), and then only evaluate model performance on the nodes assigned to the test set.

NCV addresses a limitation of standard cross-validation for within-network classification tasks. Namely, standard CV forces us to label k  X  1 of every k instances. So, if k = 10 , we are forced to experiment with 90% labeled data. The NCV approach accommodates a lower proportion of labeled instances because it samples a smaller labeled set from the k  X  1 non-test folds. However, since NCV applies collective inference to the full unlabeled portion of the net-work but only evaluates the model on the disjoint test set instances, it will not suffer the same problems experienced by resampling due to overlapping test sets.
Once a sampling procedure is chosen to create train-ing/test splits within a network, the algorithms are learned on each training set and then the models are applied for col-lective inference over the associated unlabeled portion of the network. The predictions on the test set instances are evaluated to generate an estimate of algorithm performance (e.g., accuracy, AUC, squared loss). The training/test splits results in a set of performance measurements for each al-gorithm and a significance test is then used to determine whether the observed performance differences are signifi-cantly different than what would be expected if the perfor-mance measures were drawn from the same underlying dis-tribution (i.e., the algorithms perform equivalently).
In this work, we investigated the following three sta-tistical tests: (1) paired t-test, (2) unpaired t-test, and (3) Wilcoxon signed rank test. Both the paired t-test and Wilcoxon signed rank test assume independence of the paired differences between classifiers. We observed no sub-stantive differences due to the use of the Wilcoxon test vs. the t-test. Therefore, we focus on the more commonly used t-test for the remainder of this paper.
Over the past 10 years, there has been a great deal of work on classifiers for relational domains. Here we survey the methodological design of 23 research papers most rel-evant to our work [1, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]. Relevant papers compare the performance of two or more classifiers on a re-lational classification task. Based on our survey, there are two common evaluation methodologies which emerge: (1) Independent set size . The salient feature of the inde-pendent set size methodology is that there is no dependency between training and test set sizes. The problem may be either within-network or across-network classification (i.e., there may or may not be relations between instances in the labeled and unlabeled set). For across-network classifica-tion, classifiers are generally trained on a fully labeled train-ing network and then evaluated on a disjoint (partially la-beled) test network. In this case, the proportion of labeled data available in the training and test networks may be var-ied independently. This means that there is no dependence between training and test set sizes. For within-network classification, there is a single network and classifiers are trained on the labeled instances and evaluated on the unla-beled instances. So, the only way to achieve independent set sizes in within-network classification is to fix the pro-portion of labeled data available in the network. All of the papers in our survey that employ independent set sizes (15 out of the 23) use some form of random resampling or cross-validation for evaluation. (2) Dependent set size . This methodology applies to within-network classification only, where training and test sizes are dependent. Any change in the labeling proportion over the network will affect the number of instances avail-able for both training and testing. All of the papers in our survey that employ dependent set sizes (9 out of the 23) use some sort of random resampling for evaluation (i.e., test sets overlap). Note that standard cross-validation is not possible here since it assumes a fixed proportion of labeled data.
Both of the above methodologies generally address the statistical question that we consider in our work: Given two
Table 5. Experimental characteristics of 23 surveyed learning algorithms A and B and a partially-labeled net-work from domain D, and with S L labeled instances and S U unlabeled instances ( S = S L + S U ), which algorithm will produce more accurate classifiers when trained on other partially-labeled networks of size S drawn from D? How-ever, the independent set size methodology makes a rather strong assumption about the value of S L . In particular, most studies that employ independent set sizes use 10-fold cross validation, which means that their results only generalize to graphs where 90% of the data is labeled to begin with. This is limiting because: (1) most interesting real-world prob-lems have far less than 90% of data labeled to begin with and (2) many algorithms that perform well at 90% labeled will perform poorly at sparser labelings (e.g., 10%). The dependent set size methodology is more general and pow-erful since it generalizes over different values of S L , so we focus on this version in our work.

Table 5 provides a summary of related work along a number of methodological dimensions. Note that counts in each section do not necessary sum to 23 since papers may fit in more than one category. The majority of the studies in our survey (13/23) make use of resampling procedures that pro-duce overlap between test sets. This includes all resampling procedures except cross validation and temporal sampling. Temporal sampling involves training on past instances (e.g., previously published papers with known topics) and evalu-ating on present instances (e.g., a newly submitted paper with unknown topic). Controlled random sampling proce-dures attempt to control or account for the amount of over-lap between test sets (e.g., as in the equal-instance resam-pling procedure described in Section 3.1).

In our survey, within-network classification tasks are more common than across-network tasks (13 papers vs. 8). However, in a substantial number of cases, it is unclear ex-actly how the experiments are set up. For example, authors will often say something like: we split the network into training and test sets. It is not clear from this description whether the links are preserved between instances in the training set and instances in the test set. In other cases, au-thors are explicit regarding whether such links are retained or removed.

The majority of studies in our survey (13/23) do not vary the proportion of labeled data available. Of the studies that do vary the proportion of labeled data, most (8/10) are within-network studies (i.e., dependent set size ).
The most common number of resampling folds used in the surveyed papers is 10. As Dietterich notes in his origi-nal study, the probability of Type I error for random resam-pling procedures increases with the number of resampled folds [3]. Our simulation experiments confirm this finding, but we do not replicate the result here.

Half of the studies in our survey do not make use of an explicit significance test. However, of these, about half do report standard deviation, variance, or standard error ( StDev/Var/StErr in Table 5). Finally, both accuracy and AUC are common measures of classifier performance, with precision/recall-based measures being much less common.
Type I errors occur when a statistical test incorrectly re-jects the null hypothesis (i.e, the test concludes that there is a significant difference between two classifiers when there is none). We run simulation experiments in order to as-sess the contributions of two key factors in Type I errors for within-network classification: (1) correlation of errors among related data instances and (2) dependence between samples. We also assess the potential of various statistical tests to produce Type I errors in a within-network classifi-cation setting.

Our method preserves the basic structure of Diet-terich X  X  [3], but introduces a group-based model to more easily represent sets of related instances and vary the degree of error correlation among them. We also ran Dietterich X  X  original procedure with qualitatively similar results.
As we have seen, real classifiers exhibit correlated errors on sets of related instances. To simulate this behavior, we divide all data instances into disjoint groups such that clas-sification errors are more likely to be correlated on instances within a group than on instances from different groups.
Table 6. Simulation algorithm. See Table 7 for
Table 6 outlines our simulation algorithm for both the re-sampling procedures and the network cross-validation pro-cedure. The two procedures differ only in that: (1) they use different resampling algorithms to create their test sets, and (2) NCV uses the same samples and folds across all proportions of labeled data, whereas the resampling proce-dures choose a different sample and random split for each trial and proportion labeled.

We simulate drawing a network sample s from an under-lying population by creating 300 instances and assigning one of 10 groups to each instance uniformly at random (a skewed group-size distribution produced qualitatively sim-ilar results). We then resample s and run a simulated clas-sification experiment on each resampled train/test split (see Table 7). For each trial, we apply a significance test to either accept or reject the null hypothesis. We calculate the pro-portion of trials for which the null hypothesis was rejected. Since the simulation is designed so each classifier has the same error rate in the underlying population, any rejections of the null hypothesis represent Type I errors. In addition, we measure the degree of error correlation for each trial. We use the  X  coefficient to measure the pairwise correlation of 0-1 errors among all instances in the same group, averaged over all groups.
For all experiments, we present average Type I error rates for various statistical tests over 10 simulations of 1000 tri-als each, on data samples of size 300 instances. Unless Table 7. Group-based classifier simulation algorithm. otherwise noted, our default experimental parameters are: classifier error rate P ( err ) = 0 . 1 , error correlation param-eter errCorr = 0 . 9 , and proportion of labeled instances propLabeled = 0 . 9 . The errCorr parameter determines the likelihood of misclassifying instances in the chosen mis-classification group vs. instances in other groups.
Figure 1(a) shows the effects of varying the proportion of labeled data available (0.1, 0.3, 0.5, 0.7, 0.9). For both resampling procedures, the Type I error rate increases as propLabeled decreases. This result is expected since the degree of overlap between test sets increases as the test sets become larger due to the larger number of unlabeled instances. Since NCV disallows overlapping test sets by design, it is not susceptible to this problem, achieving low Type I error rates across the range of propLabeled values.
Figure 1(b) shows the effects on measured error cor-relation and Type I error rate as we vary P ( err ) = can observe that the Type I error rate of the resampling pro-cedures increase as the error correlation increases. This is not surprising since increased error correlation is expected to lead to increased imbalance between samples. In addi-tion, we note that our experiments showed, for a fixed value of P ( err ) ( errCorr ), both the measured type I error rate and the measured error correlation increased monotonically with errCorr ( P ( err ) ). Overall, NCV is less affected by imbalanced samples since test sets do not overlap; so it ex-hibits much lower levels of Type I error. The Type I er-ror rates of equal-instance resampling (ERS) are lower than simple random resampling, however since the improvement is not sufficient to make it competitive with NCV, we do not consider ERS further.
This section describes our investigation of the charac-teristics of statistical tests when comparing real relational learning algorithms. We consider two collective inference models and compare their performance on synthetic data. The synthetic data generation enables the simulation of multiple draws of networks from the same distribution. We evaluate the Type I error and power of statistical tests, as the performance of the two models is varied.
In order to run experiments at the scale needed to as-sess Type I error and power rates, we choose to investi-gate two simple and efficient collective models described in Macskassy and Provost [14].

The first model is the weighted-vote relational neighbor (wvRN), which estimates class label probabilities by as-suming the existence of homophily. Given the unlabeled nodes in a network v i  X  V U , wvRN estimates P ( y i as the average of the class probabilities of the instances in N i ( v i  X  X  neighbors): P ( y i = + | N i ) = 1 Z P v j  X  N + | N j ) , where Z is a normalizing constant.

The second model is the network-only Bayes classifier (nBC), which estimates class label probabilities for v i with a multinomial naive Bayesian model, based on the classes [ malizing constant and  X  y j is the class observed at node v
The wvRN model does not require any learning. To es-timate the parameters of the nBC model, we use maximum likelihood estimation over the labeled part of the network. For collective inference, we use relaxation labeling with both models.
The synthetic datasets are generated with a latent group model (LGM) [17]. Each network is generated with 300 nodes (instances). The nodes are generated as members of (hidden) groups and group membership determines the bi-nary class label values and link existence for each node. The average group size is 10 and there are two types of groups: A and B . The network is skewed towards A groups, P ( A ) = 0 . 75 . Members of A groups are more likely to have a positive class label, P (+ | A ) = 0 . 9 . Mem-bers of A groups also have higher intra-group linkage with P
A ( e ij = 1 | i  X  g A k  X  j  X  g A k ) = 0 . 6 and lower inter-group linkage with P A ( e ij = 1 | i  X  g A k  X  j /  X  g A Members of B groups are more likely to have a negative class label, P (+ | B ) = 0 . 1 . Members of B groups have relatively lower intra-group linkage with P B ( e ij = 1 | i  X  g k  X  j  X  g B k ) = 0 . 4 and higher inter-group linkage with P
B ( e ij = 1 | i  X  g B k  X  j /  X  g B k ) = 0 . 013 . The resulting net-works have an average autocorrelation of 0.40 and a class prior of P (+) = 0 . 70 .

Our choice of data generation parameters was designed to create networks where the wvRN and nBC would make different classification errors. Many of the nodes in type B groups have more links to nodes in type A groups so the networks does not fully meet the assumption of homophily which underlies the wvRN model. The nBC should thus more accurately learn how to classify the type B nodes, while the wvRN will likely be more accurate on the type A nodes. However, since wvRN does not learn the concept of homophily, it will not experience variance due to small labeled sets.
To estimate Type I error, we need two models with equal performance on partially-labeled networks of the same size, drawn from the same domain. To achieve this, we mea-sured the average accuracy of the wvRN and the nBC mod-els on the synthetic data and handicapped the better model (wvRN) until the performance difference of the models was  X  0 . 005 . The better performing model was handicapped by randomly selecting c % of it X  X  predictions and perturbing those probabilities toward the opposite class. To set c , we generated 50 networks for use as a calibration set. Each of the 50 networks was sampled into 10-fold network cross-validation sets, resulting in 500 training/test set splits on which we measured average accuracy of each model. Us-ing this calibration set, we searched for a value of c that resulted in a performance difference of  X  0 . 005 between the two models.

To estimate power, we need to vary the performance dif-ference between the two models. To achieve this, we per-turbed the predictions of the worse performing model (nBC) to increase the mean difference in performance between the two models. For the power experiments, we used perturba-tion rates of c = [0 . 025 , 0 . 075 , 0 . 15 , 0 . 3] .
To measure Type I error rates and power of the statisti-cal tests, we used four synthetic networks (in addition to the calibration set). On each network, we considered four levels of labeling: [0.1, 0.2, 0.3, 0.4]. At each level of labeling, we sampled the network 10 times, either by repeated sampling or cross-validation. On each of the ten samples, we learned the nBC model on the labeled portion of the network and then we applied both models to the unlabeled portion of the network with the perturbation rate c . We measured the ac-curacy of each model on the ten test sets and then assessed the difference in performance using a t-test.

Figure 1(c) plots the Type I error rates for four combi-nations of sampling method (RS, NCV) and statistical test (paired and unpaired t-test). Type I error rates for each dataset are measured over 100 trials and averaged. Re-call that we use the calibration set to choose a value for c that makes the average performance of the wvRN and nBC equal at the given level of labeling. Thus any trial in which the t-test assesses the observed performance difference as significant corresponds to a Type I error.

All the tests have high Type I error rates with 10% of instances labeled. This error generally decreases as the amount of labeled data increases (and thus the size of the test set decreases). Since we are using relatively simple classifiers, as the number of labeled data increase model performance does indeed converge and the two models make similar classification errors at labeling rates greater than 40%. In reality, when comparing relational models with different representations and different complexity, we expect Type I error to occur at all levels of labeling.
Notably, the repeated sampling approach experiences as high as 50% Type I errors (at 20% labeling). This means that half the time the method is concluding a significant performance difference between the two models, when in fact there is none. For data mining and machine learning re-searchers that are investigating the tradeoffs between learn-ing algorithms, this is an unacceptable level of error. On the same data, the network cross-validation procedure error rate is only 15% with the paired t-test X  X  70% reduction in er-ror. Clearly, the network cross-validation approach results in a more accurate comparison of model performance. Note that in the simulated classifier experiments (see Figures 1(a)-1(b)), NCV across the proportions labeled is equivalent to 10-fold CV at 90% labeled, since performance in the simulated classifier experiments does not depend on: (1) the number of labeled neighbors available during infer-ence, and (2) the number of instances available during train-ing. These additional dependencies contribute to the higher Type I error observed for NCV with real classifiers.
Although the Type I error of NCV combined with a paired t-test is much lower than resampling, it is still higher than the generally accepted level of 5%. To investigate this behavior, we examined the estimates used in the t-test calculation: (1) the estimate of the mean performance dif-ference between the two models:  X  diff =  X  A acc  X   X  B acc and (2) the estimate of the variance of the differences: V ar diff = V ar ( { A acc  X  B acc } i ) . The error correlation increases the variance of the estimated  X  diff , but the esti-mates are not biased. On the other hand, the error corre-lation decreases the variance of the estimated differences, resulting in a biased underestimate of V ar diff . We consid-ered the unpaired t-test to adjust for this bias. The unpaired t-test uses a pooled estimate of variance over the perfor-mance measurements { A acc } and { B acc } , instead of an es-timate of variance of the differences. The pooled estimate of variance is higher than the variance of the differences, so it can offset the bias in variance estimation due to error correlation. This is indeed the case X  X ombining NCV with unpaired t-tests results in Type I error rates of less than 0.05 (when proportion labeled is greater than 10% ).

Figure 1(d) plots the power of each statistical test as we varied the average performance difference be-tween wvRN and nBC. More specifically, we used c = [0 . 025 , 0 . 075 , 0 . 15 , 0 . 3] and measured the average perfor-mance difference between the two models on the calibra-tion set. This gives us the mean performance difference that is plotted on the x-axis. Then for each of the four evaluation networks, we sampled the network 10 times and learned/applied/evaluated the models as described above. Again the results are measured over 100 trials. Since the two models do perform differently, any trial in which the t-test does not conclude that the observed performance dif-ference is significant corresponds to a Type II error. Power is defined as the proportion of trials in which the t-test cor-rectly concludes that the two models are different. We only plot the results for proportion labeled of 30% . The results for other levels of labeling are qualitatively similar.
The power results illustrate an additional challenge in evaluating the performance difference between the models. Even when there is 5% difference in the mean performance of the two algorithms, it is sobering to note that repeated sampling can detect this difference less than 80% of the time. Network cross-validation is significantly worse X  X he paired t-test can detect the difference less than 30% of the time and the unpaired t-test less than 5% of the time. This may be due to the difference in test set size used by the two approaches. Recall that repeated sampling uses all the unlabeled data for evaluation, so at 30% labeling this cor-responds to 210 nodes. On the other hand network cross validation uses only 10% of the nodes for evaluation (i.e., 30 nodes) regardless of the level of labeling in the network.
To explore this issue, we increased the dataset size to 600 and measured the power of each approach again. Fig-ure 1(e) graphs the resulting power rates. In general, power of any test will be increased as you increase the sample size. However, here we can see that the gains for network cross-validation are relatively larger than for repeated sampling. It is difficult to compare across the two sets of results due to different mean performance of the models. However, if we interpolate between the results in Figure 1(e) to assess the power at 5% mean performance difference and compare to Figure 1(d) at 5%, we can see that doubling the dataset size reduced the error of repeated sampling by 10% but the net-work cross-validation was reduced by 45% (paired t-test) and 70% (unpaired), respectively.
This section describes our investigation of the character-istics of statistical tests when comparing relational learning algorithms on real-world network data. To confirm the be-havior we observed in the synthetic datasets, we compared the performance of wvRN and nBC on data from the Na-tional Longitudinal Study of Adolescent Health [10].
The Adolescent Health (Add Health) data consists of sur-vey information from 144 middle and high schools, col-lected in 1994-1995. The survey questions queried for the students X  social networks along with myriad behavioral and academic attributes. In this paper, we consider the social networks of six schools with similar autocorrelation and link patterns. The classification task is to predict whether the student smokes based on the behavior of their friends in the social network. The six schools we selected have sizes ranging from 300-700 nodes, average degree of 7-8, and au-tocorrelation in the range [0.25,0.35].

To assess the Type I error characteristics of the mod-els, we used a procedure similar to the one described in Section 5. Each trial considers one school network as the evaluation set, then we calibrate the models on the remain-ing 5 school networks under the assumption that these net-works were drawn from the same distribution. We sam-pled each of these five networks 10 times into 10-fold NCV sets, producing a calibration set of 500 training/test splits. As described previously, we searched for a value of c that resulted in a performance difference of  X  0 . 005 between the two models. We considered five levels of labeling: [0.1,0.2,0.3,0.4,0.5], calibrated the models at each level of labeling, and measured the Type I error on the held out net-work. The models converge in performance at 50% labeling (i.e., c =0 ) so we do not consider labeling rates &gt; 50% .
Figure 1(f) shows Type I error for each combination of sampling method and statistical test, measured over 50 tri-als. As expected, the statistical tests exhibit similar behavior on the Add Health data and the synthetic data. Again re-sampling produces unacceptable levels of Type I error (up to 40%) and network cross-validation has more reasonable error rates. Overall error decreases as the proportion of la-beled data increases to 50% (i.e., test set size decreases). Recall, however, that we are investigating simple models that are nearly equivalent on a restricted task involving only the class label and no other attribute/link features. In prac-tice, as the complexity of models and concepts increase, Type I errors are likely to occur at all levels of labeling.
In this paper we examined the characteristics of statis-tical tests for comparing within-network classification algo-rithms. We presented three resampling procedures and three significance tests; performed experiments on both real and synthetic data using real and simulated classifiers.
Our analysis shows that a commonly-used form of eval-uation in relational learning (paired t-tests on overlapping network samples) can result in unacceptably high levels of Type I error (as high as 50%). High Type I error indicates that many algorithm differences will be judged incorrectly as significant when in fact performance is equivalent. Al-though for efficiency reasons we considered relatively sim-ple relational models for this work, our findings apply to evaluations of more complex relational models as well X  since any relational model that attempts to exploit relational autocorrelation is likely to produce correlated errors.
Furthermore, we demonstrated that Type I error in-creases as (1) the correlation among instances increases and (2) the size of the evaluation set increases (i.e., the propor-tion of labeled nodes in the network decreases).

Although we investigated the properties of significance tests for within-network classification, the findings are also applicable to across-network tasks and other forms of hy-pothesis testing (e.g., standard error bars will be underes-timated). The extent of the effect will depend on the level of observed autocorrelation (which will cause error correla-tion), as well as the amount of overlap between samples.
We proposed a method for network cross-validation that reduces the overlap between samples. We note that al-though the method creates disjoint test sets, the predictions for those test set instances will be influenced by other pre-dictions in the unlabeled inference set (due to the collective inference process). This means that there is still some de-pendency between test sets (since the inference sets overlap) which could increase the Type I error of NCV.

Our empirical evaluation shows that NCV combined with unpaired t-tests results in low levels of Type I error. However, this low error is achieved at the expense of statis-tical power (i.e., Type II error). NCV combined with paired t-tests produces more acceptable levels of Type I error while still providing reasonable levels of statistical power.
Promising research directions include: (1) using patterns (such as communities) in relational data to split train/test data (e.g., stratified by community, or biased by commu-nity); (2) an investigation of non-random labeling patterns and their impact on error correlation for different collective inference methods; and (3) investigating how characteristics of relational data affect the power of statistical tests (i.e., Type II error).

We thank Rongjing Xiang for her assistance in exper-imental implementation. This work was performed under the auspices of the U.S. Department of Energy by Lawrence Livermore National Laboratory under contract DE-AC52-07NA27344. This was also supported by DARPA and NSF under contract numbers NBCH1080005 and SES-0823313.
