
Redmond, WA 98052, USA used during training. For example for classification tasks, an error rate for a binary SVM classifier might be reported, although the cost function used to train the SVM only very loosely models the number of errors on the training set, and similarly neural net training uses smooth costs, such as MSE or cross entrop y. Thus often in machine learning tasks, there are actually two cost functions: to mak e the optimization task tractable (smooth, con vex etc.), and it should approximate the desired that are commonly used in information retrie val, all of which depend only on the sorted order of the documents 2 and their labeled rele vance. The tar get costs are usually averaged over a lar ge number of queries to arri ve at a single cost that can be used to assess the algorithm. These tar get costs present severe challenges to machine learning: the y are either flat (ha ve zero gradient with respect to the model scores), or are discontinuous, everywhere. It is very lik ely that a significant mismatch the algorithm. In this paper , we propose one method for attacking this problem. Perhaps the first approach that comes to mind would be to design smoothed versions of the cost function, but the inherent  X  X ort X  mak es this very challenging. Our method bypasses the problems introduced by the sort, by defining a virtual gradient on each item after the sort. The method is simple and very general: it can be used domain. We sho w that the method gives significant benefits (for both training speed, and accurac y) for applications of commercial interest.
 Notation: for the search problem, we denote the score of the ranking function by s general cost function is denoted C ( f s n , and where l documents sorted by score. We will drop the query inde x i when the meaning is clear . Rank ed lists that high rank means closer to the top of the list, we will tak e  X  X igher rank X  to mean  X  X o wer rank inde x X . Terminology: for neural netw orks, we will use  X  X prop X  and  X  X ackprop X  as abbre viations for a forw ard pass, and for a weight-updating backw ard pass, respecti vely . Throughout this paper we also use the term  X  X mooth X  to denote C 1 (i.e. with first deri vatives everywhere defined). We list some commonly used quality measures for information retrie val tasks: see [10 ] and refer -ences therein for details. We distinguish between binary and multile vel measures: for binary mea-a binary measure where for each rele vant document, the precision is computed at its position in the ordered list, and these precisions are then averaged over all rele vant documents. The corresponding quantity averaged over queries is called  X  X ean Average Precision X . Mean Reciprocal Rank (MRR) is also a binary measure: if r then the MRR is just the reciprocal rank, averaged over queries: MRR = 1 used, for example, in TREC evaluations of Question Answering systems, before 2002 [14 ]. Winner Takes All (WT A) is a binary measure for which, if the top rank ed document for a given query is rel-evant, the WT A cost is zero, otherwise it is one. WT A is used, for example, in TREC evaluations of Question Answering systems, after 2002 [14]. Pair-wise Correct is a multile vel measure that counts the number of pairs that are in the correct order , as a fraction of the maximum possible number of as the AUC, which has led to work exploring optimizing the AUC using ranking algorithms [15 , 3]. bpref biases the pairwise correct to the top part of the ranking by choosing a subset of documents from which to compute the pairs [1, 10]. The Normalized Discounted Cumulativ e Gain (NDCG) level [6]. For a given query Q where r ( j ) is the rele vance level of the j  X  X h document, and where the normalization constant N chosen so that a perfect ordering would result in N which the NDCG is computed. The N well suited to Web search applications because it is multile vel and because the truncation level can be chosen to reflect how man y documents are sho wn to the user . For this reason we will use the NDCG measure in this paper . structured outputs. Our approach is very dif ferent, howe ver, from recent work on structured outputs, choice of a suitable inner product), but the best output is found by estimating the argmax over all possible outputs. The ranking problem also maps outputs (documents) to the reals, but solv es a much simpler problem in that the number of documents to be sorted is tractable. Our focus is on a very dif ferent aspect of the problem, namely , finding ways to directly optimize the cost that the user ultimately cares about. As in [7], we handle cost functions that are multi variate, in the sense that the number of documents returned for a given query can itself vary , but the key challenge we Ho we ver, we emphasize that the method also handles the case of multi variate costs that cannot be represented as a sum of terms, each depending on the output for a single feature vector and its label. We call such functions irr educible (such costs are also considered by [7]). Most cost functions used in machine learning are instead reducible (for example, MSE, cross entrop y, log lik elihood, and the costs commonly used in kernel methods). The ranking problem itself has attracted increasing attention recently (see for example [4, 2, 8]), and in this paper we will use the RankNet algorithm of [2] as a baseline, since it is both easy to implement and performs well on lar ge retrie val tasks. One approach to working with a nonsmooth tar get cost function would be to search for an optimiza-tion function which is a good approximation to the tar get cost, but which is also smooth. Ho we ver, cost depends on only the top few rank ed positions after sorting, the sort itself depends on all docu-of all the returned documents. We therefore consider a dif ferent approach. We illustrate the idea with an example which also demonstrates the perils introduced by a tar get / optimization cost mis-match. Let the tar get cost be WT A and let the chosen optimization cost be a smooth approximation to pairwise error . Suppose that a ranking algorithm A is being trained, and that at some iteration, for a query for which there are only two rele vant documents D D parameters of A are adjusted so that D maximized, but the number of pairwise errors has been reduced by n 4 . No w suppose that at the next iteration, D to mo ve it to top position is clearly less (possibly much less) than the change in D to mo ve it to top position. Roughly speaking, we would prefer A to spend a little capacity mo ving D and j an optimization cost C that has the property that whene ver j much easier to specify rules determining how we would lik e the rank order of documents to change, after sorting them by score for a given query , than to construct a general, smooth optimization cost we are defining the gradients of an implicit cost function C only at the particular points in which we are interested. Second, the rules can encode our intuition of the limited capacity of the learning algorithm, as illustrated by Eq. (2). Let us write the gradient of C with respect to the score of the document at rank position j , for the i  X  X h query , as The sign is chosen so that positi ve reduce the cost. Thus, in this frame work choosing an implicit cost function amounts to choosing suitable of all the documents. We will call these choices the functions. At this point two questions naturally arise: first, given a choice for the functions, when does there exist a function C for which Eq. (3) holds; and second, given that it exists, when is C con vex? We have the follo wing result from multilinear algebra (see e.g. [11 ]): then every closed form on S is exact.
 Note that since every exact form is closed, it follo ws that on an open set that is star -shaped with respect to the origin, a form is closed if and only if it is exact. No w for a given query Q corresponding set of returned D (fix ed) labels l Then assuming that the scores are defined over R n , the conditions for the theorem are satisfied and = dC for some function C if and only if d = 0 everywhere. Using classical notation, this amounts to requiring that the deri vatives: the Jacobian (that is, the matrix J given that such a cost function C does exist, then since its Hessian is just the abo ve Jacobian, the condition that C be con vex is that the Jacobian be positi ve semidefinite everywhere. Under these constraints, the Jacobian looks rather lik e a kernel matrix, except that while an entry of a kernel matrix depends on two elements of a vector space, an entry of the Jacobian can depend on all of the scores s other choices that give rise to symmetric J , positi ve definiteness can be imposed by adding diagonal regularization terms of the form LambdaRank has a clear physical analogy . Think of the documents returned for a given query as point masses. are met, then the forces in the model are conserv ative, that is, the y may be vie wed as arising from a potential ener gy function, which in our case is the implicit cost function C . For example, if the  X  X  are linear in the outputs s , then this corresponds to a spring model, with springs that are either compressed or extended. The requirement that the Jacobian is positi ve semidefinite amounts to the requirement that the system of springs have a unique global minimum of the potential ener gy, which systems of springs). The physical analogy pro vides useful guidance in choosing functions. For example, for a given query , the forces (  X  X ) should sum to zero, since otherwise the overall system (mean score) will accelerate either up or down. Similarly if a contrib ution to a document A  X  X  is computed based on its position with respect to document B , then B  X  X  should be incremented by Finally , we emphasize that LambdaRank is a very simple method. It requires only that one pro vide rules for the deri vatives of the implicit cost for any given sorted order of the documents, and as we will sho w, such rules are easy to come up with. RankNet [2] uses a neural net as its function class. Feature vectors are computed for each query/document pair . RankNet is trained on those pairs of feature vectors, for a given query , for which the corresponding documents have dif ferent labels. At runtime, single feature vectors are fpropped through the net, and the documents are ordered by the resulting scores. The RankNet cost tak es the form given in Eq. (8) belo w. Training times for RankNet thus scale quadratically with the mean number of pairs per query , and linearly with the number of queries.
 The ideas proposed in Section 4 suggest a simple method for significantly speeding up RankNet training, making it also approximately linear in the number of labeled documents per query , rather the method works for any ranking method that uses gradient descent and for which the cost depends on pairs of items for each query . Most neural net training, RankNet included, uses a stochastic per query (that is, the weights are updated for each query). We present the idea for a general ranking function f : R n 7! R with optimization cost C : R 7! R . It is important to note that adopting batch training alone does not give a speedup: to compute the cost and its gradients we would still need to fprop each pair . Consider a single query for which n documents have been returned. Let the output scores of the rank er be s pairs of document indices used for training be P . The total cost is C deri vative with respect to w It is con venient to ref actor the sum: let P let D be the set of document indices. Then we can write the first term as fprops are performed to compute the s be where the sort on the scores is performed); then for each i = 1 ; : : : ; n the are computed; then to compute the gradients @s i props are done. The key point is that although the overall computation still has an n 2 dependence computation required to perform the 2 n fprops and n backprops. Thus we have effecti vely replaced a
O ( n 2 ) algorithm with an O ( n ) one 3 . We performed experiments to (1) demonstrate the training speedup for RankNet, and (2) assess whether LambdaRank impro ves the NDCG test performance. For the latter , we used RankNet as a baseline. Ev en though the RankNet optimization cost is not NDCG, RankNet is still very effecti ve at optimizing NDCG, using the method proposed in [2]: after each epoch, compute the NDCG on a validation set, and after training, choose the net for which the validation NDCG is highest. Rather than attempt to deri ve from first principles the optimal Lambda function for the NDCG tar get cost (and for a given dataset), which is beyond the scope of this paper , we wrote several plausible -functions and tested them on the Web search data. We then pick ed the single function that gave the this is described belo w. 6.1 RankNet Speedup Results Here the training scheme is exactly LambdaRank training, but with the RankNet gradients, and with no sort: we call the corresponding function G . We will refer to the original RankNet training as V1 and LambdaRank speedup as V2. We compared V1 and V2 in two sets of experiments. In the first we used 1000 queries tak en from the Web data described belo w, and in the second we varied the number of documents for a given query , using the artificial data described belo w. Experiments were run on a 2.2GHz 32 bit Opteron machine. We compared V1 to V2 for 1 layer and 2 layer (with 10 hidden nodes) nets. V1 was also run using batc h update per query , to clearly sho w the gain (the con vergence as a function of epoch was found to be similar for batch and non-batch updates; furthermore running time for batch and non-batch is almost identical). For the single layer net, on the Web data, LambdaRank with G was measured to be 5.1 times faster , and for two layer , 8.0 times faster: the left panel of Figure 1 sho ws the results (where max validation NDCG is plotted). Each point on the graph is one epoch. Results for the two layer nets were similar . The right panel sho ws a log log plot of training time versus number of documents, as the number of documents per query varies from 4,000 to 512,000 in the artificial set. Fitting the curv es using linear regression gives the slopes of V1 and V2 to be 1.943 and 1.185 respecti vely . Thus V1 is close to quadratic (but not exactly , due to the fact that only a subset of pairs is used, namely , those with documents whose labels dif fer), and V1 is close to linear , as expected.
 6.2 -function Chosen for Ranking Experiments To implement LambdaRank training, we must first choose the function (Eq. (3)), and then sub-stitute in Eq. (5). Using the physical analogy , specifying a function amounts to specifying rules for the  X  X orce X  on a document given its neighbors in the rank ed list. We tried two kinds of func-query), and those where its depends only on its nearest neighbors in the sorted list. All functions were designed with the NDCG cost function in mind, and most had a mar gin built in (that is, a force exceeds that mar gin). We investig ated step potentials, where the step sizes are proportional to the NDCG gain found by sw apping the pair; spring models; models that estimated the NDCG gradient using finite dif ferences; and models where the cost was estimated as the gradient of a smooth, pair -wise cost, also scaled by NDCG gain from sw apping the two documents. We tried ten dif ferent we will use the one that work ed best on the Web validation data for all experiments. This function used the RankNet cost, scaled by the NDCG gain found by sw apping the two documents in ques-tion. The RankNet cost combines a sigmoid output and the cross entrop y cost, and is similar to the to be rank ed higher than document j , then the RankNet cost is [2]: and if the corresponding document ranks are r combining with Eq. (1) gives where N is the reciprocal max DCG for the query . Thus for each pair , after the sort, we increment each document X  s force by , where the more rele vant document gets the positi ve increment. 6.3 Ranking for Sear ch Experiments We performed experiments on three datasets: artificial, web search, and intranet search data. The data are labeled from 0 to M , in order of increasing rele vance: the Web search and artificial data have M = 4 , and the intranet search data, M = 3 . The corresponding NDCG gains (the numerators in Eq. (1)) were therefore 0, 3, 7, 15 and 31. In all graphs, 95% confidence interv als are sho wn. In all experiments, we varied the learning rate from as low as 1e-7 to as high as 1e-2, and for each experiment we pick ed that rate that gave the best validation results. For all training, the learning rate was reduced be a factor of 0.8 if the training cost (Eq. (8), for RankNet, and the NDCG at truncation level 10, for LambdaRank) increased over the value for the pre vious epoch. Training was done for 300 epochs for the artificial and Web search data, and for 200 epochs for the intranet data, and training was restarted (with random weights) if the cost did not reduce for 50 iterations. 6.3.1 Artificial Data We used artificial data to remo ve any variance stemming from the quality of the features or of the labeling. We follo wed the prescription given in [2] for generating random cubic polynomial data. real datasets, and more data, all to more realistically approximate a Web search application. We used 50 dimensional data, 50 documents per query , and 10K/5K/10K queries for train/v alid/test respecti vely . We report the NDCG results in Figure 2 for ten NDCG truncation levels. In this clean dataset, LambdaRank clearly outperforms RankNet. Note that the gap increases at higher rele vance levels, as one might expect due to the more direct optimization of NDCG.
 6.3.2 Intranet Sear ch Data This data has dimension 87, and only 400 queries in all were available. The average number of documents per query is 59.4. We used 5 fold cross validation, with 2+2+1 splits between although LambdaRank gave uniformly better mean NDCGs, the overlapping error bars indicate that on this set, LambdaRank does not give statistically significantly better results than RankNet at 95% confidence. For the two layer nets the NDCG means are even closer . This is an example of a case that more powerful statistical tests would find a dif ference here also). 6.4 Web Sear ch Data This data is from a commercial search engine and has 367 dimensions, with on average 26.1 doc-uments per query . The data was created by shuf fling a lar ger dataset and then dividing into train, validation and test sets of size 10K/5K/10K queries, respecti vely . In Figure 3, we report the NDCG scores on the dataset at truncation levels from 1 to 10. We sho w separate plots to clearly sho w the dif ferences: in fact, the linear LambdaRank results lie on top of the two layer RankNet results, for the lar ger truncation values. We have demonstrated a simple and effecti ve method for learning non-smooth tar get costs. Lamb-daRank is a general approach: in particular , it can be used to implement RankNet training, and it furnishes a significant training speedup there. We studied LambdaRank in the conte xt of the NDCG tar get cost for neural netw ork models, but the same ideas apply to any non-smooth tar get cost, and to any dif ferentiable function class. It would be interesting to investig ate using the same method starting with other classifiers such as boosted trees.
 Ackno wledgments We thank M. Taylor , J. Platt, A. Laucius, P. Simard and D. Me yerzon for useful discussions and for pro viding data.

