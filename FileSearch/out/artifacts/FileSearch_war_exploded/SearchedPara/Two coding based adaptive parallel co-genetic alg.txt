 1. Introduction
Optimization algorithm is very important for machine learning, complex combination optimization, feature selection, operational as a good optimization method is its implicit parallelism which is a more likely to jump out of local optima to search for global optima.
However, with the searched space and dimensions increasing, the complexity of searching increases greatly, it becomes easier to fall into local optima. In order to improve its performance, many researchers performed many works on it, and proposed many modified GAs ( Goldberg, 1989 ; Holland, 1992 ; Michalewicz and Fogel, 2000 ; Kherallah et al., 2008 ).

Srinivas and Patnaik (1994) proposed one GA with adaptive parameters (AGA); the parameters are related with fitness value of the whole population, thereby keeping the diversity of the whole population. Dun-wei et al. (2002) proposed one genetic algorithm that can effectively avoid similar individuals X  crossover (SFGA). However, this algorithm only considers the adaptive crossover process. Zhong et al. (2004) introduced lattice-like agent structure into GA, and modified the selection operator, crossover operator, and mutation operator, thereby proposing one multi-agent GA (multi-agent genetic algorithm, MAGA). The relative experimental results show that the MAGA can obtain better optimization results than some other popular GAs for some complex benchmark test functions. The proposition of MAGA shows the potential of the combination of agent structure and GA.
However, in MAGA, there are some things needing to be improved. Firstly, the neighborhood competition selection pro-cess is not dynamic enough. When the neighboring agents compete with the current agent to obtain new agent, maybe some of the neighboring agents have been updated. With MAGA, the competition between the current agent and its neighboring agent are done no matter whether the neighboring agents are updated or not. Secondly, the four-neighborhood individuals X  values occupy too many nodes (i.e. local top advantage) over early. If current agent has higher fitness value than its neighbor-ing agents, when the current agent competes with its neighboring agents, the current agent can replace its neighbors and occupy the nodes of its neighbors. So, it is hard to keep whole population X  X  diversity, thereby leading to over early convergence. Based on this, we did some improvements, proposed dynamic neighbor-hood competition operator and chain-like agent structure (cycle chain agent structure), thereby designing one new agent
GA-dynamic chain-like agent genetic algorithm (CAGA) ( Zeng et al., 2008 ). According to some popular benchmark functions, the optimization result is better than MAGA to some extent. During the research of CAGA, we found that the improvement on time cost is still limited. For the complex optimization problems, even if we combine the characteristics of search space into optimization algorithm to reduce the algorithm X  X  time cost, the improvement on time cost is still limited and is not very apparent.
In order to reduce the time cost and improve the optimization speed greatly, multi-population genetic algorithm is a good choice to be applied to realize parallel optimization. We can adopt multi-
CPUs (computing processing unit, CPU) to realize the multi-population genetic algorithm, each CPU realize one population.
Apparently, the time cost will be reduced greatly. Currently, there are two realization modes for multi-population genetic algorithm.
One kind of realization mode is to decompose the optimization problem into many sub-problems; every sub-problem uses one
GA with single population. The shortcomings of the mode are in: firstly, it is hard to know whether the decomposition is reason-able. Secondly, because the optimization result of every genetic algorithm is optimal partial solution, the assembly of the optimal partial solutions into one optimal full solution is necessary.
However, the assembly is very hard and skillful for the corresponding designer. Usually, the simple combination of all optimal partial solutions is not one optimal full solution for the whole optimization problem. Another kind of realization mode is to design one multi-population genetic algorithm, every sub-population searches optimal full solution in parallel, and the sub-populations exchange evolution information each other during genetic operation to search for global optimal full solution. Olsson (2001) proposed the co-evolutionary search algorithm in asym-metric space, obtaining satisfying results to some extent, but the searching speed and precision for complex searched space are not satisfying enough. Potter and De Jong (2000) proposed one collaboration co-genetic algorithm. They introduced multi-popu-lation idea into genetic algorithm to improve GA X  X  optimization performance. However, it just considers collaboration between sub-populations but does not consider competition between sub-populations. Ying et al. (2006) proposed one collaboration co-particle swarm optimization algorithm, but the algorithm has similar drawbacks as the one in the paper ( Potter and De Jong, 2000) . Su and Hou (2008) proposed another multi-population genetic algorithm for parameter optimization problem. The GA has two sub-populations, each sub-population optimizes different object function and two optimization results within one generation are obtained. After that, the two results are put first realization mode discussed above.

Based on the analysis above, this paper proposes one multi-population co-genetic algorithm with double chain-like agent structure (close chain-like agent structure and cycle chain-like agent structure), combining chain-like agent structure and multi-population parallel searching. In this algorithm, inside every sub-population, close chain-like agent structure is applied; the individuals of some sub-population are connected with one close chain as agents. Every sub-population is connected with one cycle chain and shares some common agents (they are called shared agents). The sub-populations evolve themselves and cooperate and compete with each other. Besides, the genetic operators are improved, they include dynamic neighborhood competition selection operator, neighborhood orthogonal crossover operator and adaptive mutation operator to effectively keep and enhance the diversity of the sub-population, being good to searching for global optima in complex and high dimensional search space.
Except global numerical optimization problems, GAs are often used for feature selection problems as optimization algorithm. Feature selection problems are themselves optimization pro-blems; they can be looked as searching of optimal feature subset (optimal solution). Therefore, as good optimization algorithm, GA is widely used for feature selection problems. The feature selection performance (feature selection precision and speed) can evaluate the performance of some GA or some other optimization algorithm. Therefore, in this paper, we will discuss how to use MPATCGA for feature selection and organize some feature selection experiments to evaluate the performance of MPATCGA.

In our previous work, based on global optimization problems, we have proposed a similar modified genetic algorithm (MPAGA) with real coding ( Li and Zeng ). Based on feature selection problem, we have proposed a similar modified genetic algorithm (MPAGAFS) with binary coding ( Yongming et al. ). But, our motivation to write this paper is in that, firstly, the two modified genetic algorithm has same agent structure, so it is necessary to discuss the two genetic algorithms together in order to let the interested authors know what is same thing and difference between the two genetic algorithms and how to use them correctly. Secondly, the genetic algorithms are parallel agent genetic algorithms; all the sub-populations can search in parallel. In the real word, the task (search space) on each sub-population cannot be same. Therefore, the size of sub-population should be adaptive. This paper modifies our previous work with the self-adaptability and gives the relevant experimental results. Thirdly, we slightly modify the dynamic competition strategy with very similar time cost and slight improved precision. Fourthly, for global optimization, some experiments need to be further done; therefore, we give the more experimental results in this paper. Fifthly, for feature selection, some experiments need to be further done, such as multi-class dataset; therefore, we give the more experimental results in this paper.

The paper is organized as follows: Section 2 describes the process of the design of the MPATCGA. In Section 3, we prove that the MPATCGA can be better than other optimization algorithm for some optimization problems in terms of NFL theorem. In Section 4, we evaluate the efficiency of the proposed MPATCGA for numerical optimization and feature selection through comparing it with some other popular genetic algorithms. Finally, some conclusions are offered in Section 4. 2. Two coding based multi-population co-genetic algorithm with double chain-like agents structure (MPATCGA) 2.1. Multi-population cycle chain-like agent structure
The model of optimization problem of functions can be expressed as follows: min -f  X  x domain R n , f  X  x - X  is object function, B B the definition domain of x i :[ B li , B ui ]).

Multi-population cycle chain-like agent structure means that in terms of the position information of agents, the whole population is divided into some sub-populations. The agents inside sub-population are located in close chain-like agent structure and cooperate with each other. Each sub-population is connected with other sub-population through  X  X hared agents X  and in the form of cycle chain-like agent structure; they cooperate with each other though sharing the information of  X  X hared agents X .
Suppose the number of shared agents is S , the agent that is located in the j th node in the i th sub-population is expressed as L i =1,2, y , M , j =1,2, y , L . The neighborhood domain of L
L  X 
Fig. 1 shows the multi-population cycle chain-like agent structure with 6 agents per sub-population and 2 shared agents.
The motivation of the agents for evolution is to augment their power, so they cooperate and compete with each other. Finally, the agent with low power will die, and new agent will occupy its position. Inside the sub-population, the cooperation and competition take place between the agent and its neighbors, the introduction of the shared agents will supply the genetic information of other sub-populations, thereby improving the efficiency of the evolution. Within the structure, each ring represents a ring-like agent structure. In the ring-like agent close chain-like environment, L , which is called an agent ring. The size of L is 1 L size , where L size is an integer, 1 means one dimensional agent structure. Each agent is fixed on a ring-point and it can only interact with its neighbors ( Fig. 2 ). 2.2. Genetic operators 2.2.1. Dynamic neighborhood competition selection operator
Definition of the energy : an agent, a, represents a candidate solution to the optimization problem in process. The value of its energy is defined as follows:
Eng  X  L 1 ; i  X  X  fitness  X  L 1 ; i  X  X  3  X  where fitness () means the fitness value of some individual in the population. For numerical optimization, fitness () means the Shared agent corresponding function needing to be optimization; for feature selection, it corresponds to some evaluation criteria.
As can be seen, each agent stands for an individual. In order to realize the local perceptivity of agents, the environment is constructed as a chain-like structure as mentioned above.
Suppose that the current agent is located at(1, i ), L 1, i l , energy among the neighbors of L 1, i , where n means the number of genes. l i , n means the nth gene of ith individual L 1, i chromosome), m 1, n means the nth gene of Max 1, i . That is,
If L 1, i satisfies formula (5), then it still live in the agent chain. Or else, it will die, and its chain-point will be occupied by Eng  X  L 1 ; i  X  Z Eng  X  Ma x 1 ; i  X  X  4  X 
Dynamic competition strategy : during competition process, ascending order, after the competition of the 1st agent, the 1st agent is updated. Assuming the i th agents before competition and determined by the following procedures: if i =1 elseif i = L size else
Within one sub-population, when the i th agent is calculated, if any of the neighboring agents of the ith agent is updated, the after updated agent rather than before updated agents cooperate with the current agent to obtain the new agent Max 1, i . For example, if 5th agent is calculated. The neighboring agents of the current agent are 4th agent and 6th agent. Assuming the before updated 4th agent is L pre 1 ; 4 , the before 6th agent is L pre 1 ; 6 agent is L post 1 ; 4 and the after updated 6th agent is L agent is updated when the 5th agent is being processed, traditional method is that the L pre 1 ; 4 , L pre 1 ; 5 and L other to obtain Max 1,5 . However, with the dynamic competition strategy, the corresponding method is that the L post 1 ; 4 compete each other to obtain Max 1,5 . The major advantage is that with similar computational complexity, the 4th agent is used twice and dynamically, the efficiency is higher. the operator with real coding is for numerical optimization and is described as follows.

For formula (6), Max 1, i has two strategies to occupy the chain-point, and the strategies vary based on competition probability
P selected. Here, the  X  U (0,1) X  means a random number generator, it
Max 1, i first generates a new agent, New 1, i =( ne i ,1 then Max 1, i is put on the chain-point.

In strategy 1, New 1, i is determined by ne where i means which agent in whole population; k means which dimension; l means the lower bound; u means the upper bound; m i , k means the biggest value in the neighboring agents of kth dimension; and l i , k means the value of the current agent of kth dimension.

In strategy 2, New 1, i is determined by ne where the p match means the inoculation probability, it is realized with rand /2. During the concrete realization, the corresponding formula is simplified, the number of parts is 2, formula (6) changes as follows: ne perhaps has some useful informat ion, so occupying strategy 1 in favor of reserving some information of a loser. It puts emphasis on and assemble the parts back into ne i , k .Ithasthefunctionof random searching, but is better than random searching in that it makes use of the information of a winner. It puts emphasis on exploration. 2.2.1.2. Discussion on selection operator with binary coding. Normally, the operator with binary coding is for feature selection and is described below.

The neighborhood competition selection operator is described as follows: suppose the order of competition selection is from following formula: 8 &gt; &gt; &lt; &gt; &gt; : In formula (9), 3 means competition selection between agent 1 ; i and L t 1 ; i 1 , the two agents consist of lots of genes: L L means j th gene of L t 1 ; i , c t i 1 ; j means j th gene of L t number of genes of single agent. The competition selection as follows: round  X  U  X  0 ; 1  X  X  .

Where, U (0,1) means random number generator and is within the domain [0,1]. 2.2.2. Neighborhood orthogonal crossover operator
Two crossover operators for binary coding and real coding are described here, respectively. Orthogonal crossover operator with real coding can be used for numerical optimization, and adaptive crossover operator with binary coding can be used for feature selection. Both the two operators are used between some agent and its neighboring agents.
 2.2.2.1. Neighborhood orthogonal crossover operator with real cod-ing . The orthogonal crossover operator was described in Leung and
Wang (2001) . An orthogonal array can specify a small number of individuals that are scattered uniformly in the search space; the orthogonal crossover operator can generate a small but re-presentative sample of potential individuals. In CAGA, this op-erator is performed between L 1, i and Max 1, i to achieve the both sides X  cooperation. Combining this paper, the design of this op-erator is described briefly as follows, and for details, please see Leung and Wang (2001) .

Assuming that the search space defined by L 1, i and Max 1, i [ x , x i , u ] as follows: x x (
The domain of the b th dimension is quantized into b b ,1 b , b  X  8 &gt; &gt; &gt; &lt; &gt; &gt; &gt; :
Specially, we randomly generate F 1 integers k 1 , k 2 , y ing F factors for any chromosome a =( x 1 , x 2 , x n ): f  X  X  x 1 ; ... x k 1  X  f ^ f 8 &gt; &gt; &gt; &gt; &lt; &gt; &gt; &gt; &gt; :
The f th factor f f with Q 2 factor is determined as follows: f f ^ f 8 &gt; &gt; &gt; &gt; &lt; &gt; &gt; &gt; &gt; :
The orthogonal matrix L M 2  X  Q F 2  X  X  X  b i ; j M following M 2 chromosomes (here they are called as agents):  X  f  X  f ^  X  f 8 &gt; &gt; &gt; &gt; &lt; &gt; &gt; &gt; &gt; :
Finally, among the M 2 agents, the one with the biggest energy is selected to replace L 1, i . 2.2.2.2. Adaptive neighborhood crossover operator with binary cod-ing. In the crossover process, the crossover probability p culated adaptively. The corresponding formula is as follows: p  X  8 &gt; &lt; &gt; :
Here, p c , i means the probability of crossover about the cross-of both the individuals, f max means the maximum value of all the value of all the individuals. The crossover procedure is as follows: if U (0,1) o p c , i do single point crossover processing between L 1, i and Max else keep L 1, i no change 2.2.3. Adaptive mutation operator
Two mutation operators for binary coding and real coding are coding can be used for feature selection, and adaptive crossover operator with real coding can be used for numerical optimization. 2.2.3.1. Adaptive mutation operator with real coding. A new agent,
New 1, i =( ne 1 , ne 2 , y , ne n ) is generated as ne  X  where p m means mutation probability, G (0,(1/ t ))is a Gaussian random number generator, t is the evolution generation. Ac-cording to Michalewicz and Fogel (2000) , the suitable p m vised to be set as 1/ n . 2.2.3.2. Adaptive mutation operator with binary coding. In the crossover process, the mutation probability p m is calculated adaptively based on the length of chromosome ( Zhong et al., 2004). The crossover procedure is as follows: if U (0,1) o 1/ n do single point mutation processing between L 1, i (namely, some gene changes its value from 1 to 0 or vice versa randomly) else keep L 1, i no change where n means number of genes 2.3. Stopping criterion f ave can reflect the evolution of the current population. f for the best average fitness value since beginning. k stop counter, it counts the number that f best has no change. If k search stops. The setting of k is described in experiments section. 2.4. Elitism strategy
Agents have knowledge which is related with the problem that they are designed to solve. With elitism strategy, the agent can inherit the good solution from the former generation. This method can make the best solution within i th generation better than or equals to the best solution in the former ( i 1) generations. In can be found in Section 3. 2.5. Self-adaptability of MPATCGA
As discussed above, the MPATCGA can realize the parallel optimization because of its multiple sub-populations. In the real problem, it cannot be guaranteed that the workload on each sub-population is same. For some sub-population, when its workload become smaller, if the size of the sub-population remains same, the precision does not become better, but the time cost will become more. When its workload become heavier, if the size of the sub-population remains same, the time cost does not become less, but the precision will become worse since it seems the number of agents is not enough.

Based on the discussion, the self-adaptability of MPATCGA is applied into the MPATCGA. As we know, an evaluation criterion is needed to guide automatic modification of size of sub-population.
The evaluation criterion is diversity measurement of sub-popula-tion and described as follows: div _ sub  X  where the f i , j means value of the jth agent in the i th sub-i th sub-population; n i means the size of the i th sub-population (number of the agents in the i th sub-population). When the div _ sub becomes lower than some threshold T , the size of sub-population becomes smaller with an agent (i.e. number of the agents minus 1); when the div _ sub becomes higher than some threshold T , the size of sub-population becomes larger (i.e. number of the agents plus 1). Our rationale is that if workload on some sub-population becomes larger, it means the search space for the sub-population becomes heavier; the diversity of the agents in the sub-population becomes larger. Therefore the size of the sub-population needs to be increased. If workload on some sub-population becomes smaller, it means the search space for the sub-population becomes smaller; the diversity of the agents in the sub-population becomes smaller. Therefore the size of the sub-population needs to be decreased. 2.6. Realization of MPATCGA algorithm The MPATCGA algorithm can be described as follows.
 Procedure of MPATCGA
Notes : Suppose the best individual in the whole population in the k cnt_whole , the upper boundary of k cnt_whole is TIMEs_OUT. The stopping criterion here is as follows: compare the in d i k cnt_whole equals to TIMEs_OUT, quit the whole evolution and output the final optimization .
 Begin End Procedure of MPATCGA_IN
Notes : In MPATCGA_IN, the neighborhood competition operator is applied on each agent. As a result, the agents with lower energy are cleaned out from the agent chain so that there is more developing space for the promising agents. The neighboring crossover operator and the mutation operator are applied on each agent, respectively. At the end of ith generation, the best agent in this generation competes with the best agent in (i 1)thgeneration, and in d t best (the best agent during t genera-tions X  evolution) is updated.

Begin End Comment :L t represents the agent chain in the tth generation, and
L t +1/3 and L t +2/3 are the mid-chains between L t and L the agent chain after mutation processing in the tth generation . agent in L t .p c and p m are the probabilities to perform the neighboring crossover processing and the mutation processing.
The optimization precision and time cost are two important indices to show the performances of optimization algorithms. Whether MPATCGA can have better optimization precision is still unknown. We did many modifications to enhance its optimization performance: Dynamic neighborhood competition operator is similar to as the principle of  X  X ood ones win and bad ones lose X  in the nature. The individuals with high fitness values are kept, the individuals with low fitness values are not kicked out simply, but are improved with their neighbors to obtain new individuals, and the diversity of the whole population is kept and enhanced. Neighborhood orthogonal operator can obtain different indivi-duals as possible, thereby keeping and enhancing the diversity of the population. Besides, with the shared agents, the sub-populations can share genetic information with each other, the optimization precision can be assured. Besides, the optimization necessary to consider how to combine the partial solutions into a full solution, and the error occurred during combination of a full solution can be avoided. However, the modifications cannot prove its advantage of optimization precision directly. Therefore, it is very necessary to verify the optimization precision of the algorithm and compare it with other popular Ga through empirical insights. 2.7. Computational complexity As we know, the MPATCGA can realize parallel optimization. Suppose each CPU (computing processing unit) implements one sub-population, CPU shares genetic information with other CPUs through shared agents. Compared with the time cost needed for genetic operation, the time cost for exchanging genetic informa-tion is little and can be neglected. Suppose the time cost needed by each sub-population is equal, the time cost with each CPU should be average time cost TimeAvg : TimeAvg=Time/k n_sub Time means the time cost of whole population with MPATCGA, k n _ sub means number of sub-populations. Suppose the time complexity of MAGA is O ( gp ), if MPATCGA is realized by multi-CPUs in parallel, the time complexity of MPATCGA is O ( gp/k approximately, where g means the generations of iteration, p means the size of whole population. 2.8. MPATCGA and the NFL theorem
Briefly speaking, the no-free-lunch (NFL) theorem for optimi-zation proposed by Wolpert and Macready states that any two blackbox algorithms (deterministic or stochastic) have the same average performance over all problems ( Wolpert and Macready, 1997). Random search is one such blackbox algorithm, so all blackbox algorithms have the same average performance as random search over all problems. One immediate implication of this theorem means that when a new algorithm is shown to perform better than others over some problems, then the new algorithm will perform worse over all remaining problems on average.

However, it has been seen that in practice one does not need an algorithm that performs well on all possible functions, but only on a subset that arises from the constraints of real-world problems. For example, it has been shown that for pseudo-
Boolean functions restrictions of the complexity can lead to subsets of functions on which some algorithms perform better than others ( Whitley, 1999 ). Therefore, if we can prove the NFL theorem cannot be applied in the functions (or problems) under investigation in this paper, the theorem can be ignored. Recently, several researchers ( Kimbrough et al. ) claimed their algorithms can perform well over some kinds of functions (or problems) under investigation with the way.

As we know, Whitley has proven that NFL results hold for any under permutation (c.u.p.) ( Schumacher et al., 2001 ). Based on this important result, classes of functions where NFL does not hold can be derived simply by proving that these classes are not c.u.p.

The functions under investigation by this paper are from the popular benchmark functions (such as those in Yao et al., 1999).
These functions are multimodal functions, and can verify the searching performance of the given optimization algorithms, therefore they are used in many researches widely (for example, seen in Zhong et al., 2004 ; Pan and Kang, 1997 ; Tu and Lu, 2004 , and so on). Fig. 3 shows the functions under 2 dimensions (will be used in Section 3). As the figure shows, in the two functions, there are many local minima, but the practical number of local minima of any one of these functions is smaller than the maximal possible number. Without question, if we prove that these kinds of functions are not c.u.p, then the NFL theorem is invalid here.
Let us do some necessary definitions. We consider a finite I = Y
X be the set of all objective functions f : X -Y to be optimized (also called fitness, energy, or cost functions). Given a function f and a neighborhood relation on X , we define l max ( f ) as the maximal number of minima that functions with the same Y -histogram as f can have (i.e. functions where the number of X -Thus, it follows:
Theorem. If the number of local minima of every function f in a nonempty subset F C I is constrained to be smaller than the maximal possible max f A F l max ( f ), then F is not c.u.p.
 minima, s max ( f ) means the practical maximal local minima. Apparently, for these functions here, s max ( f ) o max f If F is c.u.p (that means any two functions f , g have the same Y -histogram).
 Then the function g p is in F , there exists permutation p (that means f p = g ).

This function has s max  X  f  X  X  s max  X  g 3 p  X  X  l max  X  g  X  X  max which contradicts the local minima constraint. 0 5 10 15 20 25 30 0 2 4 6 8 0 5 10 15 20 25 30 -5 0 5
Therefore, F is not c.u.p, the NFL theorem cannot be applied these functions. &amp;
Some researchers have done several similar researches about the conditions that obviate the no free lunch theorems for optimization, interested authors can find more detailed content in their papers (for example, Koehler, 2007 ; Igel and Toussaint, 2003, and so on).

When the constraints are fixed and NFL does not apply, a given algorithm may be better or worse on average than random search.
MPATCGA is such an algorithm. Whether it performs better or worse than random search for these functions is unknown on average. So we have to do experiments to verify it. 3. Experiments and analysis of results
From the flow chart of MPATCGA, it is seen that the algorithm can realize multi-sub-population parallel optimization, so the time cost can be reduced a lot. Suppose the number of shared agents is 1, the size of sub-population is n , and the size of whole population is N , the population can be divided into N /( n 1) sub-populations with size of n . Compared with the time cost needed by genetic operation within each sub-population, the time cost of transmission of the values of the shared agents between neighboring sub-populations is a little. Therefore, theoretically the time cost of MPATCGA can be reduced to ( n 1)/ N times of that of agent GA with single population. Without question, the parallel optimization can obtain faster optimization speed and lower time cost.

However, the optimization precision is very important and cannot be verified theoretically, so it needs to be verified through experiments. In order to show the optimization precision performance of this algorithm, global numerical optimization experiments and feature selection experiments were organized. 3.1. Global numerical optimization experiments
Some popular test functions were used in Table 1 for comparing MPATCGA and MAGA ( Yao et al., 1999). f 1 f 4 are multimodal functions with many local optima (traps). Fig. 3 functions have a lot of local optima, which trap optimization method. It is not easy to find out the global optima or near-global optima. It means that the closer to the global optima, the better the algorithm will be.

The reasons for using MAGA for comparison are: firstly, it is also one agent GA with agent structure, the comparison of MPATCGA and MAGA can show the advantage of the agent structure of MPATCGA directly. Secondly, MAGA is an agent GA with single population, the comparison of MPATCGA and MAGA can show the MAPGA cannot only realize parallel optimization, thereby reducing the time cost needed greatly, but also obtain the precise optimization results as well as or even sometimes better than MAGA. Thirdly, the paper ( Zhong et al., 2004 ) said that MAGA performed better than some well-known algorithms such as OGA/ Q, AEA, FEP, BGA, so the comparison with it can show MPATCGA better performance over those genetic algorithms indirectly.
Since the algorithm is parallel processing through multi-sub-population with smaller size, the corresponding runtime is smaller than the other genetic algorithm with single population including MAGA. However, the optimization precision of this algorithm needs to be evaluated. Therefore, lots of experiments about the optimization precision were organized to compare MPATCGA and MAGA. Since the major purpose of these experi-ments is to evaluate the optimization precision of MPATCGA, the experiments were implemented based on single PC platform. Because the implementation of this algorithm is based on single CPU, the parallel mechanism was changed as order parallel mechanism. Within one generation, each sub-population evolves in sequence. So the corresponding experimental process can be looked as fake parallel mode. The fake parallel mode can simplify the experimental process and is easy to be implemented by any interested authors no matter how many PCs he/she has. Theoretically, for optimization precision, the fake parallel mode is same as the true parallel mode, so the following experiments can show the optimization precision of MPATCGA and MAGA truly.

For the MPATCGA, The size of sub-population and the number of shared agents are adjustable. For fixed size of whole population, with the size of sub-population and the number of shared agents changes, the number of sub-population changes. The Table 2 shows the possible number of sub-populations with different shared agents and size of sub-population. Here, we discuss the number of sub-populations, size of sub-population, and number of shared agents based on one premise. The premise is that the whole population is same (however, the size of whole population in the experiments in this paper varies from 63 to 66 because the number of sub-populations should be integer).

In the following experiments in Sections 3.1.1 X 3.1.3, we set 6 as the size of sub-population and 2 as the number of shared agents. The setup of the other parameters is as follows: the size of whole population is 66, the probability of crossover is p probability of mutation is P m =0.05, the upper limit of evolution generation is T =1000, TIMEs_OUT=10. The relevant condition about PC platform is CPU (central processing unit, CPU) with mainframe of 2.8GHz, memory of 0.99GB. In the experiments in
Section 3.1.4, the size of whole population is 63 X 66; the size of sub-population and the number of shared agents are adjustable.
The other parameters of MPATCGA do not change. 3.1.1. Low and middle dimensional optimization experiments of MPATCGA
We used MPATCGA and MAGA to optimize the tested functions results were obtained after 50 running times and are listed in
Table 3 . The corresponding performance indices are as follows:  X  X ave X  means average global optima,  X  X eneration X  means the average number of generation when optimization stops,  X  X ime X  means average running time,  X  Time_Avg  X  means average running time for each sub-population or for each CPU (here, the Time_Avg is obtained by the formula: Time_Avg=Time/k n_sub , where  X  X ime X  means the time cost of whole population with MPATCGA,  X  k means number of sub-populations).

From Table 3 , we can find that under middle dimensions, the optimization of these functions is not very hard, so both the
MPATCGA and MAGA can find similar optimization precision. For some test functions, the average optimization precision of
MPATCGA is better than MAGA slightly. For most of test functions, the average running time of MPATCGA is shorter than that of
MAGA. These advantages of MPATCGA are mainly in that: firstly, the whole population is divided into several sub-populations, the genetic operation becomes simpler, the relevant time is shor-tened. Secondly, within each sub-population, the number of neighboring individuals is changed from 4 to 2, the relevant genetic operation becomes less, the relevant time cost will become less, besides, the possibility of some individuals with high fitness value occupying the whole population over early becomes lower, that means the average optimization precision will become higher. Thirdly, the ring-like agent structure allows the genetic information propagate along the ring, which means for each sub-population, all the individuals in the sub-population communicate with other individuals. Just with some individuals, the whole sub-population can exchange genetic information with other sub-populations; the relevant genetic operation becomes less. Besides, if multi-CPU is used for realizing MPATCGA, the time cost can be reduced greatly. 3.1.2. High dimensional optimization experiments of MPATCGA
The following experiments will increase to 100 dimensions (see Table 4 ). The experimental conditions are similar as the low and middle dimensional optimization experiments (Section 3.1.1).
With dimensions increasing, the coupling degree of variables becomes stronger, and the relevant optimization becomes more and more complex. The advantage of MPATCGA becomes more and more apparent. It means that for those high dimensional complex test functions, MPATCGA can obtain better optimization performance.

From the table, it can be seen that for high dimensional functions, MPATCGA has satisfied optimization precision. With dimensions increasing, the optimization precision falls down to some degree. It is because the increase of dimensions will lead to the increase of coupling degree of variables, at the same time, the search space will increase in positive proportion, approximately as Q i  X  1  X  u i l i  X  . However, MPATCGA still can find the more precise optimization results than MAGA. For example, for high dimen-
MPATCGA can have good global optimization capability for those high deceptive and multi-trap optimization problems. The ring-like (that is close chain-like) agent structure decreases the number of the neighboring individuals from 4 to 2, thereby reducing the probability of some individuals with high fitness value occupying the whole population over early, the diversity of population is kept. The structure is more effective for those functions with multi-local optima than lattice-like agent structure adopted in Zhong et al. (2004) . Besides, the dynamic neighboring selection operator, neighboring crossover operator and adaptive mutation operator can contribute to the improvement. Similar to
Section 3.1, if multi-CPU is used for realizing MPATCGA, the time cost can be reduced greatly. 3.1.3. Analysis of convergence performance
In order to show the convergence performance of MPATCGA and compare it with MAGA, the test functions listed in Table 1 were tested under several dimensions. Fig. 3 shows the conver-gence performance of MPATCGA and MAGA on function f 3 , the experimental condition is similar to Sections 3.1.1 and 3.1.2. The function f 3 is complex high deceptive multimodal function, and very suitable for testing optimization performance. In Fig. 3 , lattice means MAGA, cell means MPATCGA.

Seen from Fig. 3 , convergence curve can be divided into two parts: the fast falling part (part 1) that can be looked as global searching part and the slow falling part (part 2) that can be looked as local searching part. In part 1, more strongly the fitness value changes, the better the global searching capability of this algorithm will be. From the figure, under several dimensions, MPATCGA show its good convergence performance. Taking
Fig. 3 (d) as example, part 1 is within 10 generations, but part 1 of MAGA is extended to be 50 generations. The speed of falling of MPATCGA is faster than that of MAGA in part 1. It means
MPATCGA can have better global searching capability, especially for those high deceptive multimodal functions. In part 2, the major task is to fix the optimal area. If the global searching capability of some algorithm is not strong, it is easy to fall into local trap for this algorithm. If the diversity of population can be kept well, it is easy to jump out of local trap to locate the global optima, which are very close, so the optimization of the function belongs to high deceptive problem. Seen from the experimental results, MPATCGA shows its good global searching capability. In part 2, good algorithm can tell different local optima within local searched area. As for MPATCGA, its neighboring selection operator and crossover operator keep the individuals different each other as much as possible, thereby effectively distinguish different 0 20 40 60 80 100 120 140 160 180 0 200 400 600 800 1000 1200 1400 1600 1800 0 200 400 600 800 1000 1200 0 500 1000 1500 2000 2500 optima. Besides, the adaptive mutation operator can reduce the search area gradually, thereby improving the searching precision and saving searching time. For f 3 under 100 dimensions, part 2 of MPATCGA is 16 generations (from 4th to 20th generation), but
MAGA does not find the near global optimal. It means MPATCGA has better local searching capability than MAGA. 3.1.4. The study of the number of shared agents and size of sub-populations Through the experiments above, the optimization capability of
MPATCGA have been verified. The major reason that sub-populations can co-evolve is that they can exchange genetic information each other through shared agents, so it is necessary to study the number of shared agents. In order to study the relationship between the number of shared agents and optimiza-tion capability, different numbers of shared agents are adopted; they are 1, 2, and 3. The size of sub-population is 6; the setup of the other parameters is similar to the experiments above. The test be seen in Table 5 . Time_Avg means the average running time for each CPU if the MPATCGA is realized by multi-CPUs in parallel.
From Table 5 , it can be seen that with the number of shared agent increasing, the optimization result does not change apparently. Within the sub-population, the genetic information is propagated along ring, so any individual can obtain the information, so theoretically, one shared agent can exchange genetic information between different sub-populations. That is why different number of shared agent leads to similar optimiza-tion precision. Secondly, from the Time_Avg , we can find out, with the number of shared agents increasing, the time cost needed by each sub-population decreases. For example, for test function f when number of shared agents increases from 1 to 2, the
Time_Avg decreases from 0.6381 to 0.6284s. When number of shared agents increases from 2 to 3, the Time_Avg decreases from 0.6284 to 0.5677s. It is because the increase of shared agents can quicken the propagation of genetic information between sub-populations. However, from Table 2 , we know that with number of shared agents increasing, number of sub-populations increases accordingly. It means more CPUs are needed, more computational resource is needed. Therefore, number of shared agents cannot be too many or too few; suitable number of shared agents should be made certain based on practical application.

Fig. 4 shows the convergence performance of MPATCGAs with different number of shared agents and compare them with MAGA. In the figure, lattice stands for MAGA, cellga-6-1 stands for
MPATCGA whose size of sub-population is 6 and whose number of shared agents is 1, cellga-6-2 stands for MPATCGA whose size of sub-population is 6 and whose number of shared agents is 2, cellga-6-3 stands for MPATCGA whose size of sub-population is 6 and whose number of shared agents is 3.

From the figure, it can be seen: firstly, with the number of shared agent increasing, the optimization result does not change apparently. Both the three kinds of MPATCGAs can obtain the global optima or near global optima better than MAGA. Secondly, with number of shared agents increasing, the convergence speed quickens. In other words, in most cases, the convergence speed of the MPATCGA whose number of shared agents is 3 is fastest; it can reach the near global optima area most quickly. It is because the increase of shared agents can quicken the propagation of genetic information between sub-populations.

Tables 6 and 7 show the optimization performance with different number of sub-population under 10 and 100 dimensions. From the tables, we can see that, firstly, with size of sub-population increasing, the time cost needed by each sub-population increases.
It is because the individuals within one sub-population increases, time complexity increases accordin gly. Secondly, with size of sub-population increasing, the optimi zation precision does not change of whole population is same or similar. Thirdly, when the number of shared agents is fixed, with size of sub-population increasing, average time needed by each CPU increases, but number of CPUs decreases accordingly. Therefore, size of sub-population cannot be too big or too small; the suitable size of sub-populations should be made certain based on practical application. 3.1.5. Self-adaptability in global numerical optimization
In order to verify the self-adaptability of the MPATCGA in global numerical optimization, we organized two experiments. The 1st experiment is to verify the self-adaptability of the
MPATCGA when the search space of some sub-population is bigger than other sub-populations very apparently. The 2nd experiment is to verify the self-adaptability of the MPATCGA when the search space of some sub-population is smaller than other sub-populations very apparently. Table 8 shows the results.
In the table, case 1 represents the 1st experiment and case 2 represents the 2nd experiment. We use the test function 2 for optimization with 100 dimensions. In case 1, we divide the search space into n i sub-search spaces, each sub-search spaces is for one sub-population for searching. But the 3rd sub-search space is very huge compared with other sub-search spaces. It means the workload on the 3rd sub-population is more than other sub-populations apparently. In case 2, we divide the search space into n sub-search spaces, each sub-search spaces is for one sub-population for searching. But the 4th sub-search space is very small compared with other sub-search spaces. It means the workload on the 4th sub-population is less than other sub-populations apparently.

From the table, we can see that in case 1, if the size of sub-population cannot change, the 3rd sub-population X  X  size remains no change. For the huge workload, the presetting size of 3rd sub-population (number of agents) is not enough apparently, so the precision cannot be guaranteed. But with self-adaptability of MPATCGA, the 3rd subpopulaiton X  X  size can change accordingly, so the precision can be guaranteed. Seen from the table, with self-adaptability, the 3rd sub-population can obtain the global optima (0), but without self-adaptability, the 3rd sub-population just can obtain the local optima (0.7749). Although, with self-adaptability, is very worthy considering the apparent improvement on precision. The case 2 is an opposite case. For smaller workload than other sub-populations, the agents seem more redundant. Although more agents are good to more precision, too redundant population cannot obtain higher precision linearly. The time cost will increase rapidly. So it is necessary to remove the redundancy. Self-adaptability can be helpful for this. Seen from the table, with self-capability, the size of 4th sub-population shrinks accordingly, the time cost decreases accordingly (0.4370 o 0.6190s). But the precision can be guaranteed, the best result is 0 (global optima). 3.2. Feature selection experiments
We know that the feature selection means the searching of the optimal features combination through optimization method. There-fore, good optimization algorithm is essential for feature selection method. Here, four genetic algorithms including AGA ( Srinivas and Patnaik, 1994 ), MAGA ( Zhong et al., 2004 ), SFGA ( Dun-wei et al., 2002), and SGAE ( Michalewicz and Fogel, 2000 )areadoptedtobe compared with MPATCGA. The reasons for choosing these genetic algorithms are as follows: firstly, SGAE is a traditional genetic algorithm with elitism strategy, and it is often used in feature selection and performs well, so it is suitable to be compared with other improved genetic algorithms. Secondly, AGA is a representa-tional adaptive genetic algorithm and can keep the diversity of population effectively, so some GAs used in feature selection is adaptive GAs. The comparison with it can shows MPATCGA has more powerful searching capability, can keep the diversity of population more effectively and avoid premature convergence to get near-global optima. Thirdly, FSGA is another improved genetic algorithm with adaptive crossover operator. It can adaptively adjust performs well for optimization problems. Fourthly, MAGA is an improved genetic algorithm with lattice-like agent population structure proposed recently. In Zhong et al. (2004) ,theMAGAis described to perform better than some well-known algorithms such as OGA/Q, AEA, FEP, BGA, so the comparison with it can show MPATCGA better performance over those genetic algorithms indirectly.Thisgroupofexperimentsisconductedtoshowthe satisfying search capability of MPATCGA for feature selection.
The datasets are selected from popular international UCI database, including binary-class and multi-class datasets, please see Table 9 . Some of them are not tried before. This algorithm in this paper is applied in the new datasets not tried before. For different groups of experiments, some information changes and is not involved in this table. The fitness value of a chromosome or selected feature subset is evaluated using some kind of classifier and 5-fold cross validation (5-fold CV).

Here one evaluation criterion is used, and its corresponding ability-correlation. The fitness function below is adopted for feature selection here: fitness function (evaluation criterion): features; S b means between-classes variance, S b =( m 1 m means the first class speci mens under some feature; m 2 means the second class specimens under the same feature; S w =( s class 1 and the second feature vector within the first class p_corr1 ,then calculate the correlation of the first feature vector and the second feature vector within the second class p_corr2 , after that, the is added to corr 2. With the same processing, the correlation of the second feature and the third feature can be gotten and be added to this time, the corr 2isobtained. 3.2.1. Feature selection experiments with filter methods
Table 10 lists statistical experimental results (average results) of the 10 times experiments based on the former three datasets.
From the table, it can be seen that the average number of features from MPATCGA is less than that from MAGA and SFGA, the variance of the average number of features from MPATCGA is lowest. According to the fitness function of evaluation criterion involved, the average best fitness value from MPATCGA is highest and most stable. For letter dataset, during 10 times experiments, the best fitness value does not change; it means that MPATCGA can search for the optima stably. For wave dataset, the variance of best fitness value just is 7 0.06. For both the two datasets, the average classification accuracy obtained by MPATCGA is highest, and the variance of the average classification accuracy is lowest. 3.2.2. Feature selection experiments with wrapper methods In order to evaluate the feature selection precision of MPATCGA for wrapper feature selection, the previous GAs are BP neural network. Table 11 lists the experimental results; the relevant data are statistical results after ten times experiments.
From Table 11 , it can be seen that with wrapper method, all the five GAs obtain improved feature selection precision than filter version. In terms of ANF, the MPATCGA can obtain least number of selected features, and is very stable. In terms of classification accuracy, the MPATCGA can obtain the highest classification accuracy (i.e. feature selection precision or optimi-zation precision). For the wave dataset, all the five GAs X  classification accuracy falls down. However, the MPATCGA still can obtain the highest classification accuracy. The experimental results show that the MPATCGA not only can easily be used for wrapper feature selection, but also can have satisfying classifica-tion accuracy and number of features compared with some other popular GAs. It means MPATCGA has good optimization precision over these datasets. For the multi-class dataset (Letter-4), the results are similar. 3.2.3. Feature selection experiments with different classifiers
In order to evaluate the sensitivity of optimization perfor-mance of MPATCGA to different classifiers, three frequently used classifiers are used here for comparison; they are BP, RBF, and SVM. The MPATCGA uses the three classifiers for wrapper feature relevant data are statistical results after ten times experiments.
From Table 12 , some things can be found: regardless of different classifiers, the average number of features and average classification accuracy does not change a lot. It means the
MPATCGA can have good optimization performance regardless of according to the concrete characteristics and requirements of the feature selection problems, the designers will choose the suitable different classifier. Since the MPATCGA is not sensitive to category of classifiers, the MPATCGA is a good optimization algorithm for different feature selection problems. For the multi-calss dataset, the results are similar. 3.2.4. The study of the number of shared agents and size of sub-populations
For the MPATCGA, The size of sub-population and the number of shared agents are adjustable. For fixed size of whole population, with the size of sub-population and the number of shared agents changes, the number of sub-population changes. Here, we discuss the number of sub-populations, size of sub-population, and number of shared agents based on one premise. The premise is that the whole population is same (however, the size of whole population in the experiments in this paper varies from 63 to 66 since the number of sub-populations should be integer). Similar as Section 3.1.4, here, the considered possible numbers of shared agents are 1, 2, and 3, respectively; the considered possible sizes of sub-population are 4, 6, and 8, respectively. The purpose of this group of experiments is to study the effect of different values of parameters on the feature selection performance of MPATCGA.
Here, the classifier used is BP neural network. Table 13 lists the ten times experiments.

From Table 13 , we can find: firstly, with the number of shared agents and size of subpopulation changes, the feature selection performance changes accordingly. Theoretically, different number of shared agents means different capability of exchange genetic information, different size of sub-population means different searching capability of global optima. Secondly, the changes are not too much. Theoretically, when number of shared agents is one, the share agent still can exchange genetic information between sub-populations. Thirdly, with number of the shared agents increasing, the feature selection precision becomes better.
This is maybe because more shared agents will speed up the propagation of genetic information between sub-populations.
Fourthly, with size of sub-population becomes larger, the feature selection precision becomes better. This is maybe because larger size of sub-population has more agents than smaller size of sub-population; naturally, the sub-population with larger size can obtain higher precision possibly. 3.2.5. Comparison with parallel feature selection method
Currently, some researchers study the parallel feature selec-tion algorithm based on optimization algorithms such as GA.
Some of them divide the feature space (search space) into several sub-feature spaces ( Silva and Fred, 2007 ). For each sub-feature representative method is compared with MPATCGA. The relevant papers do not state how to divide the feature space in detail, so we list two kind of division; they correspond to two kinds of methods (methods 1 and 2). For the first one, it fixes the former several features to divide the feature space into several sub-spaces; here we suppose the number of fixed features is 3, so the number of feature subspace is 2 3 =8. For the second one, it fixes the latter several features to divide the feature space into several subspaces; here we suppose the number of fixed features is 3, so the number of feature subspace is 2 3 =8 too. Including MPATCGA, all the three feature selection method use the same classifier (BP neural network) for wrapper feature selection. Table 14 lists the experimental results; the relevant data are statistical results after ten times experiments.

Table 14 shows that this algorithm MPATCGA is better than other two methods apparently in terms of number of features and classification accuracy. In terms of number of features, according to the three datasets, the MPATCGA can obtain least features, and the variance is lowest. In terms of classification accuracy, according to the three datasets, the MPATCGA can obtain best classification accuracy, and the variance is lowest. The reason maybe is that the other two methods divide the feature space into several sub-feature spaces, the division is fixed and cannot changes. However, the distribution of local optima and complex-ity within the feature space is not even, so the number of local optima and complexity for each sub-feature space is quite different. As we know, the size of sub-population for each sub-feature space is fixed and equal, so for the sub-feature space with most complexity and local optima and with global optima (in other words, different sub-feature space means different compu-tational load or computational cost), the corresponding sub-population is hard to obtain satisfying precision. For MPATCGA, the case is quite different. Although MPATCGA search the optimal feature subset in parallel, it has shared agents connecting the different sub-populations. With the shared agents, the genetic information can spread between different sub-populations. If one sub-population faces the too complex feature space, the shared agents will help other agents in other sub-populations to cooperate with the agents in this sub-population to search for optimal feature subset. In other words, the shared agents can balance the different computational load of different sub-popula-tions. For the multi-class dataset, the results are similar. 3.2.6. Self-adaptability in feature selection
In order to verify the self-adaptability of the MPATCGA in feature selection, we organized two experiments. The 1st experiment is to verify the self-adaptability of the MPATCGA when the search space of some sub-population is bigger than other sub-populations very apparently. The 2nd experiment is to verify the self-adaptability of the MPATCGA when the search space of some sub-population is smaller than other sub-popula-tions very apparently. Here, the  X  X earch space X  means feature space. Table 15 shows the results. In the table, case 1 represents the 1st experiment; case 2 represents the 2nd experiment. We use the wave dataset for the two experiments. In case 1, we divide the search space into n i sub-search spaces, each sub-search spaces is for one sub-population for searching. But the 3rd sub-search space is very huge compared with other sub-search spaces. It means the workload on the 3rd sub-population is more than other sub-populations apparently. In case 2, we divide the search space into n i sub-search spaces, each sub-search spaces is for one sub-population for searching. But the 4th sub-search space is very small compared with other sub-search spaces. It means the workload on the 4th sub-population is less than other sub-populations apparently.

From the table, we can see that in case 1, if the size of sub-population cannot change, the 3rd subpopulaiton X  X  size remains no change. For the huge workload, the presetting size of 3rd sub-population (number of agents) is not enough apparently, so the precision cannot be guaranteed. But with self-adaptability of
MPATCGA, the 3rd subpopulaiton X  X  size can change accordingly, so the precision can be guaranteed. Seen from the table, with self-adaptability, the 3rd sub-population can obtain the near global optima (1.4434), but without self-adaptability, the 3rd sub-population just can obtain the local optima (1.1478). Although, with self-adaptability, the time cost will increase a little, the sacrifice of a little time cost is very worthy considering the apparent improvement on precision. The case 2 is an opposite case. For smaller workload than other sub-populations, the agents seem more redundant. Although more agents are good to more precision, too redundant population cannot obtain higher preci-to remove the redundancy. Self-adaptability can be helpful for population shrinks accordingly, the time cost decreases accord-ingly (2.9891 o 4.8971s). But the precision can be guaranteed, the best result is 1.3789, the corresponding classification accuracy is 81.89%. 4. Conclusions
In order to improve the deficiency of GAs with single population, this paper proposed one multi-population co-genetic algorithm with double chain-like agents structure (MPATCGA) for parallel numerical optimization, thereby reducing optimization time cost and keeping high optimization precision. This paper described the process of the design of this algorithm, and organized several experiments to test the algorithm X  X  optimization perfor-mance based on global numerical optimization experiments and feature selection experiments. Apparently, MPATCGA realizes the parallel optimization with multi-populations, so it can shorten the optimization time cost without question, but the advantage of optimization precision of MPATCGA needs to be verified. The numerical optimization experimental results show that MPATCGA has higher optimization precision and shorter optimization time cost than MAGA averagely, especially for those complex multi-modal optimization problems under high dimensions. The feature selection experimental results show that MPATCGA has higher optimization precision than some well known GAs averagely, is not selection than some other optimization algorithms. It means that
MPATCGA is very suitable for complex optimization problems such as global numerical optimization, feature selection, and so on, with satisfying optimization precision and speed. Within experiments, the number of shared agents and size of sub-population have been studied carefully both for numerical optimization and feature selection problems. With self-adaptability, the MPATCGA can deal with different kinds of imbalanced optimization problems with satisfying precision and time cost. The future works are to perfect this algorithm further and use it in the practical applications such as decision support system, data mining, bioinformatics, and so on.
 Acknowledgement
The authors want to thank the reviews X  and the editor X  comments and the support of Chongqing Natural Science Foundation (NO: CSTC, 2008BB2164, CSTC, 2008BB2322) and Innovation Ability Training Foundation of Chongqing University (CDCX018).
 References
