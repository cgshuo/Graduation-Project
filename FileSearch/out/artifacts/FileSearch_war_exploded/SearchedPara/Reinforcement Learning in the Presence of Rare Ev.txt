 We consider a practically important class of control tasks, in which rare (potentially catastrophic) events might take place. For example, in a computer network, links and nodes can fail, causing traffic to be undelivered and large penal-ties to be incurred. A robot exploring a rugged terrain may be caught by a sudden gust of wind which rolls it over. An investment agent may be faced with a market that is in tur-moil due to a sudden unforeseen event. In such cases, the rare events occur independently of the actions of the agent, with some small probability. However, such rare events can have a disproportionate effect on the agent X  X  utility. If such events are sampled on-line, as is the case in most reinforce-ment learning (RL) applications, they may not occur often enough to obtain an accurate estimate of the value function. In this paper, we formalize this problem and propose solu-tion algorithms. We assume that learning will be done in a simulation environment in which the probability of the rare event can be set to desired levels. In most safety-critical applications, training in a simulated environment is a com-mon approach. In this case, we can sample rare events more often, and use importance sampling corrections similar to (Precup et al. 2000, 2001) to evaluate a given policy. How-ever, importance sampling can cause high variance in the learning updates. We propose to use an adaptive algorithm in which the sampling rate for the rare event is adjusted in such a way as to minimize variance. For the case in which the value function is represented as a table, we show that the algorithm converges and provide a bias-variance analy-sis, based on (Mannor et al., 2007) . For the case of linear function approximation, we prove convergence. We note that a bias-variance analysis for this case is not even avail-able for TD-learning without importance sampling. We il-lustrate the performance of our approach on two domains: random Markov Decision Processes (MDPs), and a large network planning task. Our approach proves quite success-ful when compared to on-line TD-learning.
 The literature on simulation of rare events is vast; see (Bucklew, 2004; Asmussen &amp; Glynn, 2007) for compre-hensive reviews. There are many Markov (or Markov-like) models that have been studied in the simulation community including queues, inventory control problems, call centers, communication systems, etc. The main objective of these works is to estimate the probability of a rare event by sim-ulating the system under an alternative probability mea-sure, and then use importance sampling to unbias the re-sults. The search for the optimal change of measure can be done in several ways, including the cross-entropy method (Rubinstein &amp; Kroese, 2004) and stochastic approxima-tion. Variance reduction has also been studied within the RL community. In particular, (Baxter &amp; Bartlett, 2001) considered adaptive control-variates for policy gradient al-gorithms.
 The explicit modeling of rare events in reinforcement learning-style algorithms was studied in (Bhatnagar et al., 2006). Their objective is to find an optimal control policy conditioned on the occurrence of a rare event. A model that closely resembles our approach is presented in (Ahamed et al., 2006). However, they assume that the model of the transition probabilities is known, and can be arbitrar-ily modified. We make a less restrictive assumption: the only parameter of the simulator that can be modified is the rate at which the rare events occur. Also, the bias-variance analysis and discussion of the function approximation case are novel.
 The rest of the paper is organized as follows. In Section 2 we provide the essential background on RL and MDPs. In Section 3 we formally describe the rare events model used in this paper. We review RL algorithms that use importance sampling in Section 4. The learning algorithm we propose is described in Section 5. In Section 6 we present bias-variance results for learning in MDPs with rare events. Sec-tion 7 presents a learning algorithm with function approx-imation and a proof of convergence in this case. The em-pirical results of our approach are presented in Section 8. Finally, Section 9 presents conclusions and avenues for fu-ture work. We use the standard RL framework (Sutton &amp; Barto, 1998) in which an agent interacts with its environment at discrete time steps t = 0 , 1 , 2 ,... . At time t , the agent finds itself in a state s t  X  S , chooses an available action a t  X  A s then receives a numerical reward, r t + 1  X  R and observes the next state s t + 1 . We denote A = S s  X  S A s . If the environment is modeled as an MDP, its dynamics are characterized by the stationary transition probability distribution: and a bounded, real-valued reward function r ( s , a , s 0 | r ( s , a , s 0 ) | X  R max &lt;  X  ,  X  s , s 0  X  S , a  X  A . We are concerned with the problem of policy evaluation in discounted infinite horizon problems with discount fac-tor  X   X  ( 0 , 1 ) . The agent chooses its actions according to a stationary policy  X  ( s , a ) = Pr { a t = a | s t = s } . We are inter-ested in computing the state-value function V  X  : S  X  R for the given policy  X  . This value function is the solution to the well-known Bellman equations
V  X  ( s ) =  X  In RL, this value function is often estimated on-line using the well-known TD-learning algorithm (Sutton, 1988). If the actions are chosen using the desired policy  X  , after ob-serving transition ( s , a , r , s 0 ) , the estimate of the value func-tion, V , can be updated as: In control tasks, the objective is to find the policy that max-imizes V  X  ( s ) at all states s . We are concerned with problems involving rare, significant events that occur as a result of environmental factors, and which are independent of the current action taken by the agent. We model this using a mixture of two separate tran-sition probability distributions: f ( s 0 | s , a ) , which captures the environment dynamics during  X  X ormal X  operating con-ditions, and g ( s 0 | s ) , which is the  X  X are event X  transition dis-tribution. We assume that at every state s  X  S , there is a small probability,  X  ( s ) , that an unusual event might occur from this state. In this case, the transition to the next state is determined exclusively by g . If such an event does not occur, the next state is drawn according to f , and depends on both the current state and the agent X  X  action. Hence, the transition probability in the environment can be re-written as: Without loss of generality, we will assume that the  X  X or-mal X  states (reachable by f ) and the  X  X are event X  states (reachable by g ) are disjoint. Hence, the transition prob-ability distribution can be re-written as: where T  X  S is the set of  X  X are event X  states.
 We are concerned with rare events that have a signifi-cant impact on the state-value function for a given policy. Therefore we define the rare events state set as follows. Definition 3.1. A subset of states T  X  S is called a rare events state set if the following three properties hold: 2. There exists s  X  S, s 0  X  T such that g ( s 0 | s ) &gt; 0 (i.e., T 3. Let V  X  f denote the value function obtained by replac-The last condition means that the states in the rare event state set must (collectively) have a large impact on the state-value function. We define rare events to be transitions into the rare event state set. For convenience, we will refer to the states that are not in the rare event state set, S \ T , as the normal states.
 We note that we use the term  X  X are event X  loosely from the point of view of the simulation community (Bucklew, 2004), because our definition is not based solely on the probability of the event. We deviate from the typical defi-nition due to the fact that there may be events that occur in-frequently but do not have a noticeable effect on our value function estimates, and we are not concerned with these events. The TD update (2) is based on the idea that the right-hand side of the Bellman equations (1) can be approximated us-ing samples of the next transition, r ( s , a , s 0 )+  X  V ( s a  X   X  ( s ,  X  ) and s 0  X  p (  X | s , a ) . However, in an environment with rare events, if  X  ( s ) is very small, a very large number of samples will be needed in order for the rare events to be averaged properly in the value function estimates. Instead, we investigate a sampling distribution which allows these events to be sampled preferentially, and then we use im-portance sampling corrections to account for this in the TD updates.
 Importance sampling is a variance-reduction technique commonly used in statistics, as well as in the simulation community (Bucklew, 2004). The main idea is that in-stead of obtaining samples from the true distribution p , they will be drawn from a different distribution q , called the proposal distribution , in which events of interest oc-cur more frequently. If q is devised well, then using these samples will reduce the variance of the estimator. Pre-cup, Sutton &amp; Singh (2000) extended this approach to TD-learning. They studied the case in which a target pol-icy  X  is evaluated based on data generated by a differ-ent behavior policy. In this case, they showed that a TD-learning algorithm can still be used, in which the TD targets are adjusted by using the appropriate importance sampling weights: w ( s , a , s 0 )( r ( s , a , s 0 )+  X  V ( s 0 )) , where: In their case, the change of measure is induced by the be-havior policy, and the importance sampling weights are the likelihood ratios of the probabilities of action a under the two policies.
 Ahamed, Borkar &amp; Juneja (2004) use the same idea but with the goal of changing the next-state probabilities in a discrete-time finite-state Markov chain with positive costs. They assume that the transition probabilities are known and can be modified at will, and propose an adaptive impor-tance sampling algorithm (ASA) which finds an alternative set of transition probabilities in order to minimize the vari-ance of the value function estimator. They provide a con-vergence proof (assuming a tabular representation of the value function), a discussion of convergence rates, and sim-ulation results. The ASA algorithm assumes that we have full knowledge of the transition model, and can completely control the transition probabilities, so all the transition probabilities can be tilted towards the zero-variance importance sam-pling distribution. In this paper, we relax this assumption because it is difficult to achieve in practical applications. We assume that the true rare event probability  X  is known (e.g., as the mean of a Poisson process that generates fail-ures in a network, or the weight of the tail of a distribution in which rare events occur). We assume that the system dynamics, f and g are unknown and cannot be modified, but that the probability with which rare events are gener-ated can be changed as the simulation proceeds. In general, with only this parameter at our disposal, we cannot achieve the zero-variance importance sampling distribution; how-ever, we can tilt the transition probability distribution p to-wards the zero-variance distribution, and therefore reduce the variance of our estimates.
 We define  X   X  : S  X  [ 0 , 1 ] to be the probability of a rare event occurring from every state during the simulation. Hence, the next states will be sampled from a proposal distribution given by: where f and g remain unchanged. By considering that the state space S is separated into disjoint normal and rare event subsets of states, we note that the importance sampling cor-rections w ( s , a , s 0 ) can be computed by: Following a similar argument as in the development of the ASA algorithm, we can determine the following optimal form for the rare event sampling distribution:  X  ( s )=  X  ( s )  X  s 0  X  T Fortunately, the values  X   X  ( s ) can be estimated on-line using samples.
 Algorithm 1 is our proposed approach for learning in the presence of rare events. We call this algorithm rare events adaptive stochastic approximation (REASA). It is based on the observation that we can rewrite  X   X  ( s ) as follows:
T  X  ( s ) =  X  ( s )  X  is the contribution to the value of s by the rare event state set T , and U ( s ) = ( 1  X   X  ( s ))  X  is the contribution to V  X  ( s ) from the normal states. In the algorithm, T ( s ) is an unbiased estimator of T  X  U ( s ) is an unbiased estimator of U  X  ( s ) . It follows that as t  X   X  , from Equation (6), for every state s  X  S . Since we use importance sampling to calculate  X  V  X  ( s ) , we also have that as t  X   X  ,  X  V  X  ( s ) from standard stochastic approximation arguments under some mild assumptions on the MDP structure. We summarize the result in the following proposition. Proposition 1. Using Algorithm 1 and assuming that the MDP is unichain for  X  =  X  1 we have that: Moreover,  X  s s.t.  X   X  ( s )  X  (  X  , 1  X   X  ) we have that  X   X  ( s )  X   X   X  ( s ) almost surely.
 We note that we guarantee that we have enough persistent exploration by requiring that  X   X  ( s ) is bounded from below by  X  and from above by 1  X   X  (step 5h in Algorithm 1). Although the treatment above is assuming positive rewards (for ease of notation), our algorithm is actually formulated for the general case in which rewards can be both positive and negative, which is an extension of the ASA algorithm. Algorithm 1 Rare-event Adaptive Importance Sampling
Input: Rare event set T  X  S , true rare-event probabilities  X  ( s ) , and parameter  X  &gt; 0, used to keep the sampling distribution non-zero everywhere. For simplicity, let us assume that  X  ( s ) =  X  for all states s  X  S (all the analysis can be done without this assumption, but becomes more tedious). Let R  X  denote the vector of imme-diate rewards for every state, with entries: and P  X  be an | S | X | S | transition matrix under  X  , with en-tries: From (3), we can re-write P  X  as: where F  X  is the transition matrix corresponding to staying in the normal states, and G is the matrix corresponding to transiting into the rare event states. Note that according to our assumptions, G does not depend on  X  . Similarly, the reward vector can be decomposed into two components,
F and R  X  G . We use two sequences, { X k }  X  k = 1 and { Y of geometrically distributed random variables, with means ( 1  X   X  )  X  1 and  X   X  1 respectively, to represent the amount of time between transitions from the normal states and the rare event states respectively. We also assume that the initial state is a normal state. Hence, the simulation starts in some normal state and stays in the set of normal states for X 1 steps, at which point it transitions to a state in the rare event set, where it stays for Y 1 time steps, then transitions back to the normal set for X 2 time steps, etc. We make two further simplifications. First, we assume that after each excursion into the rare event state set, the system  X  X umps back X  to the normal state in which it was before entering; that is, wards for transitioning to states in the normal set are similar ysis can be done without these assumptions, but it becomes more tedious. These assumptions are reasonable because in general the rare events model failures in the system, such as a failed link in a network, and when the failure is no longer present, the system resumes from the state prior to the failure. We define  X  ( k ) =  X  k  X  1 i = 1 X k and  X  The value function estimate, V  X  , can be re-written as: V  X   X  E When the value function is estimated from data using TD-learning, we can analyze the bias and variance of this es-timate by considering that all the model component esti-mates are affected by noise components (Mannor et al., 2007). The estimate of the value function,  X  V  X  can be bro-ken up into two components; the first component ignores rare events, and the second takes rare events into account. The first component is: where  X  R  X  F ,  X  F  X  represent the noise estimates in the normal part of the model. The bias and variance of this estimate can be derived directly as in (Mannor et al., 2007), noting that  X  ( k ) and  X  ( k ) are sums of independent geometrically distributed variables, and are therefore distributed accord-ing to a negative binomial distribution.
 The second component is: Note that the noise components  X  G ,  X  F  X  ,  X  R  X  G depend on the number of transitions observed in the environment. If we observe N transitions, then the expected number of transi-tions observed in the normal state set is ( 1  X   X  ) N and the expected number of transitions observed in the rare event state set is  X  N . Hence, we assume that the noise compo-nent  X  F  X  is negligible compared to  X  G , and  X  R  X  G establish bias-variance estimates for  X  V  X  G , we need to look at E ( G +  X  G )( R  X  G +  X  R  X  G ) . Similarly to (Mannor et al., 2007), we assume that E [  X  G ] = 0 and E [  X  R  X  G ] = 0. Hence, the re-maining term which will determine the bias and variance is E [  X 
G  X  R  X  G ] , which captures the correlations between the tran-sition and model estimates, due to the fact that they are estimated from the same samples. This expectation can be derived directly from the formulas in (Mannor et al., 2007). We would like to point out that we could also have ap-plied the analysis of (Mannor et al., 2007) directly to P  X  . However, this would lead to very loose bounds, because their results depend on the inverse of the minimum number of samples obtained for any transition, and we expect that there will be very few transitions into the rare event set. In our analysis, only the second term depends on numbers of transitions into the rare events states, so we can focus our analysis on the effect of the rare events on the bias and variance in our estimates.
 Also, note that the purpose of the algorithm is to sample rare events proportionately to their contribution to the value function for all states. Hence, intuitively, it will reduce bias and variance in the second component by oversampling the rare events, and thus decreasing the noise components  X  and  X  R  X  G . Given the same amount of data, the errors in and  X  R  X  F , but not by much. If the state space is very large or continuous, function ap-proximation must be used to estimate the value function. Here, we are concerned with the case of linear function ap-proximation, in which the value of a state is estimated as: where  X  is a parameter vector that needs to be estimated and  X  ( s ) is a set of features corresponding to state s . In this case, the eligibility traces are also represented as a vector e of the same size as  X  . We now extend the REASA algo-rithm to deal with this case. First, note that in this case, we may not be able to have a state-dependent probability of obtaining a rare event state, because specifying this on a state-by-state basis would be too expensive. Hence, we will assume for the moment, without loss of generality, that the true rare event probability  X  is constant over the entire state space. We discuss possible extension to this in Sec-tion 9. The algorithm will estimate a parameter  X   X  by taking the view that, at a high level, the agent switches between the normal states S \ T and the rare-event states T . These are now treated as two states in a high-level MDP, and  X   X  estimated like in REASA, on this 2-state system.
 Algorithm 2 presents the approach, which adapts the algo-rithm of Precup et al. (2001). Unlike in the tabular case, here importance sampling corrections have to be made to account for the difference in the distribution of observed features, as well as for the difference in the TD target. As explained in Precup et al. (2001), these corrections, which are collected in the trajectory weight c , can result in high variance. However, since we assume that the sets of normal and rare event states are disjoint, we can assume, without loss of generality, that they are represented by disjoint fea-tures as well. In this case, step 7i of Algorithm 2 can be eliminated, and variance will be greatly improved. Proposition 2. Under standard stochastic approximation conditions, Algorithm 2 converges in the limit, with proba-bility 1, to the same estimates as the on-policy TD-learning algorithm.
 Algorithm 2 Rare-event Adaptive Importance Sampling with Function Approximation
Input: Rare event set T  X  S , true rare-event probabil-ity  X  , and parameter  X  &gt; 0, used to keep the sampling distribution non-zero everywhere. 8.1. Random MDPs We first compare the performance of REASA to on-line TD(  X  ) and to ASA on a testbed of randomly generated Markov chains. Each environment contains 10 regular states and one rare event state. Each regular state can tran-sition to seven other regular states (chosen randomly) with probabilities drawn from a uniform distribution, and to the rare event state with probability  X  = 0 . 001. The rewards for transitioning between the regular states and from the rare event state to the regular states are drawn from a nor-mal distribution with mean 1 . 0 and standard deviation 0 . 5, with negative values being discarded (so that we can run ASA). The rewards for transitioning to the rare event state are drawn from a normal distribution with mean 10 /  X  and standard deviation 1 /  X  . The initial state is state 0, and the discount factor is  X  = 0 . 7.
 In the following results, a step is considered to be one tran-sition for both ASA and REASA, but for TD(  X  ), a  X  X tep X  actually consists of 2300 real time steps. We chose this number of steps so that the probability of observing at least one rare event transition in each episode is approximately 0 . 9. Therefore, we put TD(  X  ) at a significant advantage in terms of the number of samples that it is provided. In Figure 1 we plot the estimate for the value function at the initial state over time, averaged across 70 independent runs. We use a value of  X  = 0 . 7 and the learning rates are on de-creasing schedules that have been tuned separately for each of the algorithms. Figure 2 shows the root mean squared er-ror for the value function estimate at the initial state, again averaged across 70 independent runs.
 The learning and error curves for REASA and ASA are nearly indistinguishable, and both outperform TD(  X  ). We note that in the case of ASA, the original transition prob-ability distribution is needed, and the algorithm has full control over the transition probabilities that are used in the simulation (an unlikely case in many practical appli-cations). We observe that despite the fact that REASA can only know and control the rare event probability, it per-forms nearly as well as ASA. 8.2. Policy Evaluation for Network Planning In order to demonstrate REASA in a practical setting with a large state space, we use a network planning task in which a reinforcement learning agent has to build and maintain a telecommunications network linking ten North American cities. Each pair of cities has a certain traffic demand, rang-ing from 3GBs 2 to 60GBs initially, and this demand grows stochastically at a rate of approximately 3% per year. The goal is to place links between the cities in order to deliver this data. Links consist of bundles of fiber optic cables, and each fiber can carry a specific unit of bandwidth. Build-ing links between the cities incurs a large one-time cost of $500k/mile. Once a link has been built, the capacity of the link can be increased by activating fibers, in units of 25GBs; this incurs a cost of $30k/mile. The revenue from traffic is generated daily: traffic delivered generates a re-ward of $1k/GBs/mile, and undelivered traffic is penalized at a rate of $200k/GBs/mile every hour.
 Link failures occur with a small probability, completely severing a link for a short period of time. Without consider-ing link failures, a minimum spanning tree (MST) could be built, with enough activated fibers to carry the traffic. How-ever, in such a network, any link failure would disconnect the network, which would lead to undelivered traffic and a high penalty. Hence, link failures in a network that lacks robustness are rare events according to our definition. On each day, each link goes down with probability 1 / 1460, or approximately once every four years. When a link fails, it remains down for a random amount of time that is normally distributed with a mean of 12 hours and standard deviation of 2 hours. In a tree network with 9 links, this is equivalent to seeing at least one link fail with probability of approx-imately 0 . 00896 each day during the 10 year simulation period; this is our rare event probability. We implemented a network planning agent with a simple heuristic policy, which first builds a tree network and then monitors the links, adding capacity when the utilization of a link reaches 90%. We use REASA and TD(  X  ) to estimate the value of this policy. We represent the network state as a vector of binary features, and use linear function approx-imation to represent the value function. In order to cope with the high dimensionality, we use a fairly coarse state representation, consisting of: indicator variables regarding whether each of the possible links have been built; indicator variables for each link, which are true if the link is currently failing; and the percentage utilization of each link, parti-For our 10 node network, this corresponds to 270 binary features plus an additional bias feature.
 We use a discount factor of 0 . 95 and we set  X  = 1 . 0. We use a decaying schedule for the learning rate parameter  X  starting with a value of  X  0 = 2  X  15 for T = 100 episodes, then using  X  0 / 2 for 2 T episodes,  X  0 / 4 for 4 T episodes, etc. We note that  X  0 is extremely small due to the fact that the rewards often have large magnitude and can vary between  X  10 7 and 10 5 . In the following results, an episode consists of a simulated 10-year time span.
 In Figure 3, we show the value estimate for the initial tree network state. We see that REASA converges quickly, while the TD(  X  ) estimates have high variance and converge quite slowly. On longer runs, the TD(  X  ) estimates do con-verge to the same value as REASA. REASA estimates the optimal failure probability to be 0 . 155, which in a tree net-work with 9 links corresponds to each link going down ap-proximately every 54 days; this is quite far from the origi-nal failure probability of once every 1460 days.
 The rate of convergence is crucial for applications such as the network task. Here, each episode corresponds to a sim-ulated 10 year period, and these simulations are computa-tionally expensive to run, because on each day, a routing algorithm has to be run to determine the reward. Hence, the gains obtained by REASA are significant. We presented an approach for reinforcement learning in en-vironments with rare events, aimed at reducing the variance of RL algorithms. Our algorithm modifies the sampling probability of the rare events, and makes minimal assump-tions on the simulator available to the agent. The empirical results demonstrate the viability of our approach for solv-ing large-scale problems. Future work will include mea-suring empirically the bias and variance of the algorithm. We would also like to lift the assumption that the rare event probability is constant for the function approximation case. Note that Algorithm 2 can be easily adapted to compute  X  T and  X  U as a function of the features available. Hence, if a representation of  X  ( s ) as a function of the available features  X  is given, we could estimate  X   X  as a function of features as well. It is possible also to learn the true rare event proba-bilities  X  from data, but we anticipate that in practice this may be difficult.
 Acknowledgments The authors gratefully acknowledge the support of NSERC and the Canada Research Chairs program.

