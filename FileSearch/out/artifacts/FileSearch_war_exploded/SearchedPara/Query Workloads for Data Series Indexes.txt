 Data series are a prevalent data type that has attracted lots of interest in recent years. Most of the research has focused on how to e ciently support similarity or nearest neighbor queries over large data series collections (an important data mining task), and several data series summarization and in-dexing methods have been proposed in order to solve this problem. Nevertheless, up to this point very little atten-tion has been paid to properly evaluating such index struc-tures, with most previous work relying solely on randomly selected data series to use as queries (with/without adding noise). In this work, we show that random workloads are inherently not suitable for the task at hand and we argue that there is a need for carefully generating a query work-load. We define measures that capture the characteristics of queries, and we propose a method for generating workloads with the desired properties, that is, e  X  ectively evaluating and comparing data series summarizations and indexes. In our experimental evaluation, with carefully controlled query workloads, we shed light on key factors a  X  ecting the perfor-mance of nearest neighbor search in large data series collec-tions.
 H.2 [ Database Management ]; H.3.1 [ Information Stor-age and Retrieval ]: Content Analysis and Indexing data series; indexing; similarity search; workloads
Work done while at Cornell University. This work was funded by NSF Grants IIS-0911036 and IIS-1012593, and by the iAd Project from the National Research Council of Nor-way. Any opinions, findings and conclusions or recommen-dations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors.
Data series (ordered sequences of values) appear in many diverse domains ranging from audio signal [15] and image data processing [33], to financial [29] and scientific data [14] analysis, and have gathered the attention of the data man-agement community for almost two decades [24, 30, 5, 23, 10, 11, 36]. Note that time series are a special case of data series, where the values are measured over time, but a series can also be defined over other measures (e.g., mass in Mass Spectroscopy).

Nearest neighbor queries are of paramount importance in this context, since they form the basis of virtually every data mining and analysis task involving data series. How-ever, such queries become challenging when performed on very large data series collections [6, 26]. The state-of-the-art methods for answering nearest neighbor queries mainly rely on two techniques: data summarization and indexing . Data series summarization is used to reduce the dimension-ality of the data [17, 24, 25, 21, 1, 21, 16, 8, 22] so that they can then be e ciently indexed [24, 30, 5, 32, 2, 28].
We note that despite the considerable amount of work on data series indexing [12, 25, 8, 30, 16], no previous study paid particular attention to the query workloads used for the evaluation of these indexes. Furthermore, since there exist no real data series query workloads, all previous work has used random query workloads (following the same data dis-tribution as the data series collection). In this case though, the experimental evaluation does not take into account the hardness of the queries issued.

Indeed, our experiments demonstrate that in the query workloads used in the past, the vast majority of the queries are easy. Therefore, they lead to results that only reveal the characteristics of the indexes X  performance under examina-tion for a rather restricted part of the available spectrum of choices. The intuition is that easy queries are easy for all indexes, and therefore queries cannot capture well the di  X  er-ences among various summarization and indexing methods (the same also holds for extremely hard queries as well). In order to understand how indexes perform for the entire range of possible queries, we need ways to measure and con-trol the hardness of the queries in a workload. Being able to generate large amounts of queries of predefined hardness will allow us to stress-test the indexes and measure their relative performance under di  X  erent conditions.
In this work, we focus on the study of this problem and we propose the first principled method for generating query workloads with controlled characteristics under any situa-tion, irrespective of the data series summarization used by the index or the available test dataset. 1 To this end, we investigate and formalize the notion of hardness for a data series query. This notion captures the amount of e  X  ort that an index would have to undertake in order to answer a given query, and is based on the properties of the lower bounding function employed by all data series indexes. Moreover, we describe a method for generating queries of controlled hard-ness, by increasing the density of the data around the query X  X  true answer in a systematic way.

Intuitively adding more data series around a query X  X  near-est neighbor forces an index to fetch more raw data in that area for calculating the actual distances, which makes a query  X  X arder X . In this paper, we break down this problem into three subproblems.
The proposed method leads to data series query workloads that e  X  ectively and correctly capture the di  X  erences among various summarization methods and index structures. In ad-dition, these workloads enable us to study the performance of various indexes as a function of the amount of data that have to be touched. Our study shows that queries of in-creased hardness (when compared to those contained in the random workloads used in past studies) are better suited for the task of index performance evaluation.

Evidently, a deep understanding of the behavior of data series indexes will enable us to further push the boundaries in this area of research, developing increasingly e cient and e  X  ective solutions. We argue that this will only become possible if we can study the performance characteristics of indexes under varying conditions, and especially under those conditions that push the indexes to their limits.
Our contributions can be summarized as follows.
A data series x =[ x 1 ,...,x d ] is an ordered list of real values with length d . In this paper, we also call data series as points . Given a dataset D = { x i } N 1 of N points and a query set Q = { q i } M 1 of M data series, a query workload W is defined as a tuple ( D , Q ,k,DIST ), where each query point q i 2 Q is a k nearest neighbors ( k -NN) query and DIST (  X  ,  X  ) is a distance function. When the context is clear, we use x ( k ) to denote k -th nearest neighbor of a query q .
In this work, we focus on the nearest neighbor query, i.e., k = 1, and define MINDIST ( q )as DIST ( x (1) , q ). How-ever, our methods can be naturally extended to higher values Website: http://disi.unitn.it/~zoumpatianos/edq of k by employing the distance to the k -th nearest neighbor. For the rest of this study, we consider the Euclidean dis-tance DIST ( x , y )= k x y k 2 , due to its wide application in the data series domain [5, 30, 32]. Table 1 summarizes the notations in this paper.
Since data series are inherently high-dimensional, di  X  erent summarization techniques are used in order to reduce the total number of dimensions. Popular techniques not only include well known transforms and decompositions such as DFT [24, 25, 21, 1], DHWT [21, 16, 8], PCA and SVD [19, 27], but also data series specific data summarization tech-niques such as SAX [22], PAA [17], APCA [7] and i SAX [30, 5]. We briefly describe the most prominent ones below.
Piecewise Aggregate Approximation (PAA) [17] approximates a data series by splitting it into equal seg-ments and calculating the average value for each segment.
Discrete Fourier Transform (DFT) [24, 25, 21, 1] uses Fourier transforms to convert a data series to the fre-quency domain and represents it as a list of coe cients. 2
Discrete Haar Wavelet Transform (DHWT) [21, 16, 8] uses Haar wavelets in order to transform a data series into a list of coe cients.

Symbolic Aggregate approXimation (SAX) [22] is built above PAA with the addition that the value space is also discretized, leading to a symbolic representation with very small memory requirements.

To use such summarizations and make exact query an-swering feasible, indexes use lower and upper bounds of the distances between two data series in the original data space. These bounds are computed based on the summarizations of the data series. Throughout our study, we refer to the lower bounding function of a given summary as function L ; given two summarized data series, L returns a lower bound of their true distance in the original space.
Nearest neighbor search can be an intensive task; the na  X   X ve approach requires a full scan of the dataset. Fortu-nately, lower bounding functions on summarizations along with indexing make it possible to significantly prune the search space.

Indexes are built by hierarchically organizing data series in one or many levels of aggregation. At each level multi-ple groups of data series are summarized under a common representation. This is illustrated in Figure 1. Data series indexes that support exact nearest neighbor search can be divided into three broad categories as follows.
In this work, we use the well known FFT algorithm. Figure 1: An index structure built above a set of data series, pruning the search space for a query.
Summarization &amp; spatial access method . The first category involves the use of a summarization technique and a (general) spatial access method. Previous work has pro-posed the use of R-Trees with summarizations like DFT [1, 12, 24, 25], DHWT [8] and Piecewise Linear Approximation (PLA) [9].

Data series specific summarization &amp; index . The second category involves the use of a summarization method specific to data series, and a specialized index that is built on top of it. Such indexes include TS-Tree [2] (based on a symbolic summarization), DS-Tree [32] (based on APCA), ADS [35] and i SAX index [30, 5, 6] (built on an indexable version of SAX), and SFA index [28] (it uses a symbolic summarization of data series in the frequency domain based on DFT).

Summary reorganization &amp; multi-level scan . This last category skips the step of building an index structure; it rather relies on carefully organizing and storing the data se-ries representations on disk. Using this approach, data can be read in a step-wise function, where distance estimations for all data series are gradually refined as we read the sum-marizations in increasing detail. Both the DFT and DHWT summarizations have been studied in this context [21, 16].
Although the problem of data series indexing has attracted considerable attention, there is little research so far in prop-erly evaluating those indexes. In this work, we focus on studying the properties of data series query workloads. The aim is to better understand the characteristics of di  X  erent queries, and how these can be used to e  X  ectively test a data series index under di  X  erent, but controllable conditions.
When navigating an index, we make use of the lower bounds (computed based on the summarizations) of the true distances of data series in the original space. This technique guarantees that there will be no false negatives in the candi-date set, but it does not exclude the false positives. There-fore, the indexes need to fetch the raw data series as well, and check them before returning the answer in order to filter out the false positives, and thus guarantee the correctness of the final answer.

The use of lower bounds can be conceptually thought of as the cut-o  X  point in the distance between two summarized data series. Below this point, the corresponding raw data have to be checked. To capture this notion, we can use the (a) Points in  X  -area (  X  =1 . 0) (b) # of points in  X  -area Figure 2: Two random queries with nearest neigh-bors depicted with  X   X   X .
 Tightness of Lower Bound ( TLB ) [31], which is measured as the average ratio of the lower bound over the true distance. We formalize this notion by first introducing here the Atomic Tightness of Lower Bound ( AT LB ), which is the same ratio, but defined for a specific query and data series pair. Definition 1 (Atomic Tightness of Lower Bound).
 Given a lower bounding function L ,theatomictightnessof lower bound ( AT LB )betweentwodataseries q and x is defined as
Definition 2 (Tightness of Lower Bound). Given asummarizationwithlowerboundingfunction L ,asetof queries Q and a set of data series D ,thetightnessoflower bound ( TLB )forthissummarizationisdefinedas
Note that the TLB is small for an inaccurate summariza-tion, i.e., the summarization tends to significantly under-estimate the distance. As a result, data series under this summarization will look much closer than they actually are. Consequently, the index will have to check more raw data, leading to a longer execution time.

Example 1. Figure 2(a) demonstrates the implications of the TLB .Forsimplicity,werepresenteachdataseries as a point in a two-dimensional space, i.e., d =2 .Inthis example, we plot two queries q 1 , q 2 and mark their nearest neighbors with a bold  X   X   X . Assume MINDIST ( q 1 )=0 . 33 , MINDIST ( q 2 )=0 . 26 ,andalldataseriesaresummarized using the same summarization method. Let the AT LB be-tween the queries and any data point be 0.5, i.e., the lower bound of the distances between q 1 or q 2 and all other points is 0.5 times their actual distance. According to the definition of AT LB ,apoint x cannot be pruned if This means that for q 1 ,allpointswhoseactualdistanceis within a radius  X  = 0 . 33 0 . 5 from q 1  X  X  nearest neighbor can not be pruned, because their lower bound distances are less than the distance to the answer. Since AT LB ( L, x , q ) 2 (0 , 1] , the right hand side of Inequality (4) is always no less than MINDIST ( q ) .TheserangesaredepictedasdisksinFig-ure 2(a).

Since di  X  erent summarizations (and hence di  X  erent TLB s) can be employed, we need a method to generate query work-loads that are summarization-independent so that it does not accidentally favor one summarization over another. To achieve that, we start by formalizing Example 1 using the notion of minimum e  X  ort that an index has to undertake for a specific query. We then generalize this concept to a mea-sure that is summarization/index-independent, called hard-ness , for a given query and a given set of data series, and describe the connection between hardness and AT LB / TLB .
We define Minimum E  X  ort ( ME ) as the ratio of points over the total number of series that an index has to fetch to answer a query.

Definition 3 (Minimum Effort). Given a query q , its MINDIST ( q ) and a lower bounding function L ,the minimum e  X  ort that an index using this lower bounding func-tion has to do in order to answer the query is defined as
As we have seen in Example 1, given a fixed ATLB be-tween the query and data series, data series that contribute nearest neighbor and these data series cannot be pruned. The size of this radius is inversely proportional to AT LB and proportional to MINDIST ( q ).

It is important to clarify that this is the minimum possible e  X  ort that an index will have to undertake, and in most cases it will be smaller than the actual e  X  ort that the index will actually do. This is because the search for the solution hardly ever starts with the real answer as a best-so-far.
Recall that our goal is to generate query workloads that are summarization-independent. Since minimum e  X  ort is tied to a specific summarization, we need a more general notion to capture how hard a query is. Intuitively, the hard-ness of a query is related to the number of points around its nearest neighbor (true answer). Given this intuition, we define the  X  -Near Neighbors (  X  -NN) of a query q as follows.
Definition 4 (  X  -Near Neighbors). Given  X  0 ,the  X  -near neighbors of a query q is N  X  ( q )= { x 2 D| DIST ( x , q )  X  (1 +  X  ) MINDIST ( q ) } ,i.e.,allthepointsin D that are within (1+  X  ) MINDIST ( q ) of the query X  X  nearest neighbor.
The  X  -NN naturally defines a hypersphere around the near-est neighbor of the query. In the rest of this paper, we will refer to this hypersphere as the  X  -area . Now we define the hardness of a query as follows. 3
Definition 5 (Hardness). Given  X  0 ,thehardness of a query q is  X   X  ( q )= |N  X  ( q ) | |D| ,i.e.,thefractionof D that is within (1 +  X  ) MINDIST ( q ) of the query. A similar definition can also be found in [4].

Note that for an  X  -area to cover all the points that cannot be pruned,  X  needs be no less than 1 AT LB ( L, x , q ) 1 according to Inequality (4). Using the example in Figure 2(a) and assuming the total number of points in the dataset is 100, since AT LB is 0.5,  X  =1 . 0 defines the area where no points can be pruned. In this case,  X   X  ( q 1 )=0 . 06 and  X   X  ( q 0 . 18, i.e., q 1 is a much easier query than q 2 .
Even though we have drawn a connection between AT LB and hardness, it is worth noting that hardness and AT LB (and hence TLB ) are two intrinsically di  X  erent notions. While we can use the AT LB as a way to calibrate our  X  values, TLB is tied to a specific index structure and summarization technique combination. Hardness on the other hand is a property of a dataset; when the hardness of a query is high, there are more points distributed at a close distance to the query X  X  nearest neighbor, and as a result if the TLB is low, the index will have to do more e  X  ort, since it will have to check a larger amount of data.
Figure 2(b) shows the number of points in the  X  -area when  X  increases for both q 1 and q 2 . These two queries show very di  X  erent behavior; the hardness of q 2 increases significantly as  X  increases while the hardness of q 1 stays relatively still for large  X  . As a result,  X  X asy X  queries such as q 1 will be easy for a reasonably  X  X mart X  summarization and index, and therefore lead to similar performance. On the other hand, queries such as q 2 are good candidates to test various sum-marizations under examination because 1) it is reasonably  X  X ard X  so that any stress test could be conducted and 2) the number of points around the true answer increases almost linearly as  X  increases, which makes it easier to observe the subtle di  X  erences for various summarizations. Recall that we could think of the lower bounding function of a summa-rization as a cut-o  X  point, below which one cannot prune any points. When points are roughly uniformly distributed in the  X  -area, such  X  X ut-o  X   X  point can be better captured.
In this section, we review some common datasets and their corresponding workloads that have been used in the litera-ture. We use the same datasets in our experimental section as well. For capturing trends and shapes, we z-normalize (mean=0, std=1) all data series following common practice.
RandWalk [1, 12, 24, 8, 30, 2, 5, 16, 20, 6, 28, 35]. This dataset is synthetically produced using a random walk data series generator. The step size in each data series varies according to a Gaussian distribution. We start with a fixed random seed and produce 200,000 data series of length 1024, 256 and 64.

EEG [34, 18, 2, 20, 26] . We use the electroencephalo-grams dataset from the UCI repository [3], and sample 200,000 data series of length 1024, 256 and 64 from the dataset to be used as the dataset.

DNA [5, 6, 35] . We use the complete Human DNA (Homo Sapiens), obtained from the Ensembl project. 4 We sample the dataset to create 200,000 data series of length 1024, 256 and 64. ftp://ftp.ensembl.org/pub/release-42/ (a) RandWalk (64) (d) RandWalk (256) (g) RandWalk (1024)
The query workloads that have been used in all past stud-ies are generated in one of the following two ways. 1. A subset of the dataset is used for indexing and a dis-2. The entire dataset is used for indexing. A subset of
In our study, we shu  X  e the datasets, and use half of each dataset (100,000 data series) as queries, and the other half (100,000 data series) as the indexed dataset.
One of our key requirements is the ability to test how indexes scale as they need to check an increasing amount of data. This is the case with hard queries, for which indexes are not able to easily identify the true nearest neighbor. In this subsection, we choose 1,000 random queries from our initial query set and evaluate the hardness of each one of them for  X  2 { 0 . 25 , 0 . 5 , 1 } .

The results are depicted in Figure 3. As the histograms show, for all data series (length 64, 256, 1024) the query workloads are mainly concentrated towards easy queries. For  X  =0 . 5, the average hardness is less than 0.1, while for  X  =1 . 0, the average of hardness is less than 0.25. Addi-tionally, as the  X  decreases to 0, the hardness of the queries drops very rapidly for both the RandWalk and the DNA datasets. These low hardness values further motivate us for the need of a controlled way to generate workloads with queries of varying hardness.
As we demonstrated in the previous section, all the widely-used (i.e., randomly generated) query workloads are biased Figure 4: Example of 3 queries, where the  X  -area of q 1 and q 2 intersect. As a result we cannot control the hardness of these two queries independently, as densifying each one of the two zones might a  X  ect also the hardness of the other query. Figure 5: Maximal clique formed by q 1 , q 2 ,and q 5 . towards easy queries. In this paper, we argue that an e  X  ec-tive query workload should contain queries of varying hard-ness. Since most existing queries are easy, we start with those easy queries and make them harder by adding more points in their  X  -areas.

We start with a list of di  X  erent hardness values in non-decreasing order [  X   X  1 ,...,  X   X  n ] with respect to some  X  that is provided by the user ( and an input sample query set Q that contains many easy queries (produced through random generation). Therefore, for each hardness value  X   X  i , we need to densify (i.e, adding more points) a certain region in D so that we could select a subset of queries of size n from Q such that their hardness values match the predefined values  X   X  i for i 2 { 1 ,...,n } .
The key problem here is to ensure the  X  -areas of selected queries do not intersect with each other so that when we densify the  X  -areas the hardness value will stay exactly as required. Figure 4 shows an example that q 1 and q 2 inter-sect. In this case, we can either choose q 1 and q 3 ,or q q . The next step is to identify the amount of points we need to add in each  X  -area. Finally, we spread these points in such a way that as the TLB of the index gets worse, the minimum e  X  ort captured by the workload increases, follow-ing our intuitions described in Section 3 and Example 1.
Given an initial sample of queries Q , we want to find the largest subset of Q , where it holds that the  X  -areas around each query do not intersect.

Our first step is to calculate the radius of each  X  -area. In order to do this we need to find the distance to the near-est neighbor and multiply it by (1 +  X  ). Since we are using Euclidean distance (a metric), we can use the triangle in-equality in order to find non-intersecting queries. Given a distance function DIST in metric space, we set R ( q )=(1+  X  ) MINDIST ( q ) as the radius of the  X  -area. Two queries q i , q j 2 Q , q i 6 = q j are non-intersecting if and Algorithm 1 FindNonIntersectingQueries 1: R  X  createRadius ( Q , D ,  X  ) 2: g createV erticesF romQueries ( Q ) 3: for ( q i , q j ) 2 Q  X  Q do 4: if DIST ( q i , q j ) &gt;R  X  ( q i )+ R  X  ( q j ) then 5: g.addEdge ( q i , q j ) 6: V g.getSortedV ertices () { Sorted by ascending de-7: Q 0 ; 8: for q 2 V do 9: if isCompatible ( q , Q 0 ) then 10: Q 0 Q 0 [ { q } 11: return Q 0 only if the following holds (by the triangle inequality):
In order to validate this constraint, we first need to cal-culate all the pairwise distances for all the queries in Q , and for each query q , the distance to its nearest neighbor MINDIST ( q ).

Given the set of queries and their pairwise distances, we can create a graph G , where each vertex represents a query, and an edge exists if two queries q i and q j do not interfere with each other. Now it is clear that our problem is closely related to the maximum clique problem. Figure 5 illustrates an example graph with 6 queries, where queries q 1 , q 2 , q form the maximum clique, being mutually non-intersecting.
Note that finding the maximum clique in graph G is NP-complete, we therefore employ a greedy approach to select queries by assigning a query q to some  X   X  i (denoted as q (  X  if its current hardness is smaller than  X   X  i and if its  X  -area does not intersect with the  X  -areas of all previously assigned queries. This ensures that when densifying the  X  -area for q (  X   X  i ), the hardness of other selected queries q (  X  will remain una  X  ected.
 Algorithm 1 describes how to find non-intersecting queries. The algorithm sorts the vertices of the graph based on their degree. The intuition is that high-degree vertices have more compatible vertices. We then keep reading vertices in that order, adding compatible ones to a list while skipping in-compatible ones.
As we discussed, it is important to generate a benchmark that has queries of varying hardness. In the earlier section, we have established the notion of hardness and have dis-cussed how to select a set of candidate queries given a query set. In this section, we will describe how to determine the number of points to densify.

Given the list of n input hardness values [  X   X  1 ,...,  X  respect to some  X  . Each hardness value now has a corre-sponding query, and we need to identify the number of points to densify to the  X  -area of each query in order to achieve the target hardness. Let x i be the number of points to add for N ( q (  X   X  i )) and N i = |N  X  ( q (  X   X  i )) | is the current number of points in q (  X   X  i ) X  X   X  -area, we have the following linear system. Representing this linear system in matrix form, we have where and This linear system can be easily solved and it will tell us how many points to densify in the  X  -area for each selected query.
In this section we describe how to densify the  X  -areas for the selected queries. Given that most summarization and in-dex methods require the data to be z-normalized, we could not directly add random points in the  X  -area since after z-normalization those points could be outside of the  X  -area. In-stead, we utilize existing points in the  X  -area. Therefore, the candidate strategies for densification are the following: (a) randomly choosing a point in N  X  ( q (  X   X  i )) and adding noise to create a new point; (b) adding noise to the query X  X  near-est neighbor (ignoring all other points in its  X  -area); and (c) adding points as uniformly as possible in the  X  -area.
To demonstrate di  X  erent densification strategies, we gen-erate queries of hardness 0.2 (  X  =1 . 0) for a dataset of 100,000 data series. According to Section 3.2, this  X  allows us to test less tight representations with TLB saslowas0.5. To evaluate the e  X  ort for every query, we use four standard data series summarization techniques (SAX, FFT, DHWT, PAA) at various resolutions, ranging from 8 to 64 bytes per data series. The data series are of length 256, and for each summarization we measure the minimum e  X  ort required.
Random densification . A na  X   X ve method to increase the hardness in an  X  -area is to choose random points from this area and add noise to them, thus producing the desired amount of extra points. A property of this method is that the original distribution of the points will not change sig-nificantly. The problem with this method, however, is that for very good summarization methods (large TLB values), as we increase the number of points in the  X  -area, the mini-mum e  X  ort will not necessarily increase relatively to it. As a result, indexes with di  X  erent TLB values might have the same e  X  ort to answer a query.

The result of a query generated with this method can be seen in Figure 6(a). The histogram at the top of the figure displays the distribution of the points in the densi-fied  X  -area. As we can see the further away we get from the nearest neighbor, the more points we find at each area. The heat map in the center represents the locations of the points that contribute to the minimum e  X  ort, i.e., L ( x , q )  X  MINDIST ( q ). The color represents the portion of these points in the corresponding bucket of  X  . Finally, the vertical graph on the right side represents the minimum e  X  orts of the di  X  erent summarization methods.

As expected, the results show that crude summarizations (SAX-8, DHWT-8, FFT-8, PAA-8) that use less bytes for representing the data series have much larger minimum ef-forts. From the plot we could infer that a significant portion e  X  ort located at the corresponding bucket of  X  . of points that contribute to minimum e  X  ort may not be in-cluded by this  X  -area. On the other hand, fine summariza-tions (SAX-64, DHWT-64, FFT-64, PAA-64) are well cap-tured by this  X  . Actually, we only need  X  =0 . 6 to capture all points contributing to the minimum e  X  orts. With the his-togram on the top, it is easy to see that the minimum e  X  ort is related to the distribution of points in the original space. For example, while the heat map for FFT-64, DHWT-64 and PAA-64 spans a larger range of  X  values, their minimum e  X  ort is not much greater than that of SAX-64, which spans a much smaller range. This is because, as we can see in the histogram at the top, there is a very small amount of data within  X  =0 . 5 and it does not increase too much as  X  increases. This situation is more pronounced with another query example shown in Figure 6(b), where the distribution of points in the  X  -area is even more skewed. 1NN densification . Another na  X   X ve method for increas-ing hardness in the  X  -area is by just adding noise to the query X  X  nearest neighbor itself. This will force all summa-rizations to make (almost) the same e  X  ort, as the area very close to the nearest neighbor is now very dense and all the rest of the  X  -area is very sparse. In this case, all e  X  orts for all summarizations are almost identical. An example 1NN densified query is shown in Figure 6(c).

Equi-densification . As discussed in Section 3.3, we want to ensure that the hardness points are distributed as uni-formly as possible within the  X  -area corresponding to each possible ATLB value. This ensures that we capture the sub-tle di  X  erences for various summarizations. To this end, we propose equi-densification that aims to distribute the extra points we need to add in such a way that buckets that are originally almost empty get a large number of points, and buckets that are almost full get a small number of points.
In order to achieve this, we bucketize ATLB values, and accordingly the  X  values are bucketized (in a non-uniform way). For each ATLB bucket we want to make sure there is an equal amount of points. Densification is done by creating linear combinations of points located within and outside of each bucket. This ensures the diversity of the generated data series, allowing us to control the location of the data points in the  X  -area, and also ensures that the resulting data series after z-normalization will fall in the desired location with high probability. A query produced with equi-densification is depicted in Figure 6(d). The histogram on the top shows that the first few buckets have more points, while the last few buckets have less. This happens because  X  is inversely proportional to ATLB , and as a result  X  bucket ranges are small for large ATLB values and large for small ATLB values. For example for ATLB values in [0 . 5 , 0 . 6] the corresponding  X  values are in  X  values are in [0 . 43 , 0 . 67] As we can see in the heat map, the e  X  ort points are now evenly distributed in the  X  -areas. Note also that as the bounds of a summarization get worse, we need to increase the  X  to include all points that contribute to the minimum e  X  ort.

Therefore, equi-densification achieves the desired result, accurately capturing the relative di  X  erences among di  X  erent summarizations, and consequently leading to correct per-formance comparisons of indexes based on their TLB .We further validate this claim in the experimental evaluation.
In this section, we provide an experimental evaluation of the proposed method. We generate query workloads on the three datasets in Section 4 using our method described in the previous section. All our datasets contain 100,000 data series with length 256. Given a set of desired hardness values,  X  , and the densification mode, our method produces a new dataset that is the original dataset with extra points, and a set of queries that forms the workload that matches the desired hardness values.

We performed three sets of experiments. The first inves-tigates the amount of non-interfering queries we can find for each dataset. The second is intended to compare the three di  X  erent densification methods with regards to the mini-mum e  X  ort of various common summarization techniques, i.e., PAA, FFT, DHWT and SAX. For each one of the sum-marizations, we used 8, 16, 32 and 64 bytes to represent each data series. In the third set of experiments, we used two real world indexes, i SAX 2.0 [6] and the R-Tree [13] using PAA as a summarization method. This last experiment aims to show the impact of our benchmark on these indexes com-pared to choosing random points from the dataset (queries are left outside of the indexed data). A comprehensive ex-perimental comparison of various data series indexes is out of the scope of this study and is part of our future work.
In this experiment, we used 100,000 data series from each dataset as the indexed data, and 100,000 data series as sam-ple queries. We generated sets of 1,000 (100 sets), 2,000 (50 sets) and 4,000 (25 sets) queries, and run our non-interfering queries discovery algorithm on each one of them queries against the corresponding dataset, and we report the average number of queries found per dataset, query set size and  X  , as well as the corresponding error bars.

The results are depicted in Figure 7 (error bars are very small). We observe that for RandWalk and DNA , using a large  X  only allows us to find an average of 7-10 non-interfering queries, and as  X  decreases we can find up to 300-600 queries. For the EEG dataset, the data distribution allows us to find a much higher number of non-interfering queries, which is in the order of thousands. Note that since we are mainly interested in generating queries with high hardness values, we do not need too many queries. Further-more, the constraint hard queries that we could produce for one dataset. For ex-ample, we can generate 5 queries of hardness 0.2, but only 2 queries of hardness 0.5 and 0.5, respectively. Since this number of queries is small for a comprehensive benchmark, the solution is to use multiple datasets with corresponding query workloads; even in the case of  X  =1 . 0, we can run our algorithm 100 times to get 100 di  X  erent output datasets with at least 3 di  X  erent queries each, for a total of 300 queries.
In this experiment, we generated 3 di  X  erent queries with a hardness of 0.2 for each one of them (  X  =1 . 0). For each query we used a di  X  erent densification method. Our goal is to measure how well the di  X  erent densification methods cap-ture the relative summarization errors of di  X  erent summa-rization techniques. We use 1 TLB as the summarization error for each technique. This number intuitively captures how far the lower bound of a summarization is from the true distance. We report the relative summarization errors in the results (normalized by the smallest summarization error). In our experiments, the summarization with the smallest error was SAX (64 bytes). The TLB s for each summariza-tion were computed by comparing the distances to the lower bounds for 100 random queries against all the other points of the dataset, for each of the datasets we generated.
Figure 8 shows the average relative summarization errors for each dataset (averaged over the 100 di  X  erent benchmarks generated). The results show that 1NN densification results to almost equal e  X  ort for all summarizations, while random densification tends to over-penalize bad summarizations and favor good ones. Both situations are not desirable and can-not be useful. In contrast, equi-densification has an e  X  ort much more closely related to the summarization error across all datasets. As a result, equi-densification well captures the actual pruning power of each summarization and does not over-penalize or under-penalize any of the summarizations.
In our last experiment, we generated 85 datasets with 3 equi-densified queries corresponding to each one of them, which we will refer to as EDQ, and 3 additional queries (per dataset) that were randomly selected from the input queries sample without any densification. The 3 EDQ queries have hardness values of 0.1, 0.3 and 0.5 (  X  =1 . 0). We generated 510 queries in total, half of which were random and half equi-densified. Our goal for this study is to demonstrate the qualitative di  X  erence of using our query workload versus a workload of randomly generated queries.

Figure 9 shows the histograms of the distribution of the hardnesses for the queries on each workload for every dataset for  X  =1 . 0. Again, the random workloads are concen-trated on easy queries with only a very small number of hard queries. On the contrary, the EDQ workload has been de-signed to produce queries of varying hardness values, and as a result their histograms contain equal number of queries in the 0.1, 0.3 and 0.5 bucket. This confirms that our method produces queries with desired properties.

In order to specifically evaluate the e  X  ect of hard queries, we further split the random workload into two sets, result-ing in 3 di  X  erent workloads: Random, where we use all the randomly selected queries, Random-H, where we only use queries with hardness larger than 0.5, and EDQ generated by our method. We indexed all three datasets with both i SAX [30] and R-Trees [13] with PAA [17], and measured the average query answering time per workload. Figure 10 illustrates the normalized query answering time. The results show that when the Random workload is used, queries are on average easy, and consequently, the two indexes seem to have similar performance. The same observation also holds when only the hard queries are selected using Random-H, in-dicating that simply selecting the hard queries of a randomly generated query workload cannot lead to a good query work-load.

The real di  X  erence comes when the workload becomes harder using the EDQ workload. In this case, the di  X  er-ences between the indexes become more prominent. The reason behind this can be intuitively seen in Figure 11, where we plot the distribution of the distances to the query X  X  nearest neighbor in the  X  -area for the three di  X  erent work-loads. We can see that with random queries, (Random and Random-H), the vast majority of the points are lo-cated towards the large  X  values. The di  X  erence between Random and Random-H is just on the number of points in each bucket. As we discussed earlier, such a distribution of points cannot capture the relative TLB of di  X  erent indexes, as there are fewer points in small range to the true answer and many more points in larger range. On the other hand, the distribution of EDQ is very di  X  erent from the others, which ensures there are roughly equal number of points for the corresponding ATLB bucket.
In this work, we focus on the problem of how to system-atically characterize a data series query workload, and sub-sequently, how to generate queries with desired properties, which is a necessary step for studying the behavior and per-formance of data series indexes under di  X  erent conditions. We demonstrate that previous approaches are not viable solutions as they are biased toward easy queries. We for-mally define the key concept of query hardness and conduct an extensive study on hardness of a data series query. Fi-nally, we describe a method for generating data series query workloads, which can be used for the evaluation of data se-ries summarizations and indexes. Our experimental evalu-ation demonstrates the soundness and e  X  ectiveness of the proposed method. % of queries (a) RandWalk 0 5 10 15 20
R  X  Tree iSAX R  X  Tree iSAX R  X  Tree iSAX
Avg. query time (norm.) (a) RandWalk Figure 10: Average query answering time compar-ison between i SAX (256 characters, 16 segments) and R-Tree (PAA with 8 segments) normalized over i SAX. (a) Random Figure 11: Distribution of points in  X  =1 . 0 area for 3 types of queries for RandWalk.

