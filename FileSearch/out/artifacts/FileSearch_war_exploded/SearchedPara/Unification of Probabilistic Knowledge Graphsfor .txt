 An event consists interactions among objects (including people) over a period of time that are of interest to an observer . It is the basic semantic unit by which human per-ceives the real world . Event recognition is also the target of commonsense reasoning [ 1 ] in general and machine perception in domains such as video surveillance and au-tonomous vehicles. A major difficulty in event recognition is uncertainty. U ncert ain exists in the real world and in the recognition process itself . One effective way to cope with uncertainty is to combine the recognitions from distinct recognizers . If the recognizer s  X  errors are not correlated, a pool of recognizers can outperform any single recognizer [ 2 , 3 ]. In many machine perception tasks that employ distributed devices, including video surveillanc e and autonomous driving, a pool of recognizer is readily available. 
A typical video surveillance system consists of many cameras that cover a critical area from different angles and distances . Increasingly, the goal is to detect some target events, for e xample, TREVID 2016 ( http://www -nlpir.nist.gov/projects/tv2016/tv2016.html ) has two tasks dedicated to event recogni-tions . Since e vent recognition is much more difficult than object classification due to a large number of appearance variations , combining t he results from multiple cameras can overcome the deficiency of single cameras . 
For an autonomous vehicle to navigate the local streets safely, it needs to reliably recognize interact ions among objects in a scene . For example, an autonomous vehicle should treat a group of kids playing balls on the sidewalk differently from a group of kids playing a computer game on the sidewalk , as the first event poses more risk than the second one . The event recognition from one vehicle may no t be accurate due to view point, occlusions, illumination and distance . However, multiple vehicles and roadside sensors can share their recognitions in real -time to improve the accuracy .
In both cases, we need an efficient way to represent, transmit and c ombine event recognitions from distributed devices . The representation should be at the semantic level and can incorporate uncertainty for both objects and interactions. For this pur-pose , we propose Probabilistic Event Graph (PEG) model that represent s int eractions and objects with logic and uncertainties about the representations with probability . PEG can be translated to RDF [ 4 , 5 , 6 ] , a W3C standard markup language, and shared between the decentralized devices over a variety of networks with low latency and bandwidth requirement . 
Based on the PEG model , we develop an efficient method to unify PEG documents from different devices . Given two PEG documents X and Y , and a taxonomy S , the method produces in polynomial time a PEG documents Z that entails X and Y under S . The unification method uses probability mixture model to combine the probability distributions in X and Y , and Akaike Information Criteria (AIC) [ 7 ] to minimize the size of Z based on the mixture probability distributions . Preliminary experimental results on large set of simulated PEG documents show that the approach is feasible and promising.
 The rest of the paper is organized as follows. Sect ion 2 reviews the related work. Section 3 presents the PEG model with entailment and Section 4 describes the PEG unification me thod. Section 5 discusses the experiments results, and we conclude with Section 5. Russel [ 8 ] provides a concise survey on recent developments to inte grate logic and probability models . A key distinction is made between the closed world assumption , in which all the objects are known and do not change , and open world model assump-tion , in which not all the object are known and may change over time . Using the pos-sible world model , one can determine the entailment relations between logic expres-sions .
 Predoiu [ 9 ] provides a comprehensive survey on research efforts to e xtend the W3C standard RDF [ 4 ] , RDF Schema (RDFS) [ 5 ] and OWL [ 6 ] languages with prob-ability to deal with incomplete, inconsistent and vague information over the Web . Two types of approaches are evaluated: 1) those that directly extend the W3C lan-guages; and 2) those that extend the Description Logic (DL) underlying the W3C languages , including probabilistic extensions to DL and probabilistic logic systems that are diff erent from DL . 
Probabilistic RDF ( pRDF ) [ 10 ] is a direct extension to RDF and RDFS with prob-ability , such that a RDF document can contain both deterministic triple s and probabil-istic quadruples . pRDF defines a possible world semantics and entailment for proba-bilistic RDF document s . Our PEG can be regarded as an extension to pRDF with new probabilistic assert ions .

Unification of D irected Acyclic Graphs (DAG) has been widely used in logic rea-soning and unification grammars [ 11 ] . Give two DAG X and Y and a taxonomy S , the unification procedure merges X and Y , such that th e modified X and Y are the most general DAG subsumed by X and Y given S . However, the DAG unification procedure cannot be used to unify PEG , because the procedure cannot handle the probabilities in PEG . Furthermore, the properties in a DAG are distinct symbols , whereas the proper-tie s in PEG can be related concepts in the taxonomy . 
PEG unification is related but different from Ontology Matching [ 12 ] . P EG unifi-cati on merges different documents of the same ontology, whereas Ontology Matching finds equivalent concepts between different ontologies. Although such equivalent relations allow us to treat ontologies X and Y as one ontology Z , Z does not combine the probabi lities in X and Y as PEG unification does . PEG unification can use Ontolo-gy Matching to align documents if they are from different ontologies.

S everal recent projects aim to extract, combine and query knowledge graphs using to determine the probability of an extracted RDF triple being true . The same classifi-er is also used to propose new RDF triples based on priors estimated from the existing knowledge graphs. In [ 16 ], a classifier is trained and used to align properties between the knowledge graphs based on 4 features, where the alignments can be one -to -many. Our approach differ from these as PEG unification does not re quire a trained clas sifier but relies on probabilistic inference to combine knowledge graphs . 
Ther e have been some effort s to develop ontologies for representing events ob-served in real world using markup languages [ 17 , 18 , 19 ]. However, these languages do not incorporate probability for uncertainty associated with event recognitions . Event recognition can be regarded as a classification problem . G iven a set of classes and relations defined in a taxonomy , a recognizer assign s each object a most likely class and each pair of objects a most likely relation . To handle uncertainty, the recog-nizer can assign n -best classes and relations ranked by probabilities. This is the prac-tice adopted by computer vision community. For example, ImageNet ( http://www.image -net.org/ ) , a popular imag e dataset for object classification task , is based on the WordN et taxonomy, which can be represented by RDF ( http://wordnet -rdf.princeton.edu/ ) .

Under this assumption , our PEG model allows probabilistic assertions about ob-jects and relations as summarized in Table 1 . W e use lower cases letters to denote entities ( RDF instances), upper c ase letters denote classes and relations (RDF classes and properties) The meaning of these assertions are summarized below: 1. C(x) : e ntity x belong s to class C with conditional probability P(C|x) ; 2. R(x,y) : e ntity x has property R with entity y with conditional probability P(R,y|x) ; 3. x X  X  : e ntity x , a rdf:Bag or rdf:Alt container [ 5 ] , has member entity y with condi-tional probability P(y|x) ; 4. C X  X  : c lass D has subclass C with conditional probability P(C|D) .

Among the 4 probabilistic assertions, only the first one occurs in pRDF [ 10 ]. PEG and pRDF use different probabilistic models for all assertions. PEG uses conditional probabilities whereas pRDF uses joint probabilities. The conditional probability mod-el ties PEG mode l and taxonomy to Bayesian Network [ 20 ]. As the result, PEG doc-ument s and the taxonomy can be joined into one Bayesian Network. Fig. 1 illustrates a PEG document created with assertions 1, 2 and 3 in Table 1 . The PEG has 4 entities 1 -4 c olored in red, where entity 1 is the root node. The class assertions C(x) are colored in cyan. The relation assertions R(x,y) are colored in red. The member assertions x X  X  are colored in green. This PEG document describes the uncertain entities and relati ons with probabilistic options, which are (key, probability) pairs. For example, entity 1 has 3 probabilistic class options: {(adult, 0.6), (senior, 0.3), (child, 0.1)} . Entity 1 has a relation with entity 2, and the relation has 2 probabil-istic options: { (pushes, 0.6), (hold, 0.4)} . Entity 4 has 2 probabilistic member options: {(mary, 0.4), (john, 0.6)}. 
Fig. 2 illustrates a toy taxonomy constructed with assertion C X  X  . The edges in the taxonomy are labeled with conditional probabilities of the child given the parent. A taxonomy defines a partial order &lt; between the concepts. For any concepts C and D , C&lt;D if there is a chain of rdfs:subclassOf or rdfs:subproperty relati ons from C to D . For example, kid &lt;person , but kid  X  adult .
 An important relation between PEG documents is entailment [ 8 ] . We denote PEG X entails PEG Y by X |= Y . Let W(X) denote all the worlds in which PEG document X is true, then X |= Y if and only if  X  (  X  )  X   X  (  X  ) . For a PEG document X , the set W(X) can be determined from the structure of X . To illustrate the idea , let PEG Y con-tain only entity 1 and its 3 classes in Fig. 1 . There are three worlds in which Y can be true , one world for each class option . Let PEG X contain only entity 1 and two clas-ses: adult and senior . It is evident that X |= Y because W( X ) contains two worlds and W( X )  X  W( Y ) . Partial order &lt; in a taxonomy can influence entailment . For example, if PEG X ={ stroller (x)} , and PEG Y = { cart (x)} , then X |= Y , because stroller&lt;cart guar-and z are constant and y=z , or y is a constant and z is a variable. 
L et f be a function that maps each entity x of X to the corresponding entity y=f(x) in Y . To determine if PEG X entails PEG Y , we test if the classes, members and rela-tions of every x entail those of y . Since the classes, members and relations are proba-bilistic options, a ll these tests depend on if probabilistic options U entails V under the partial order &lt; . More formal ly, we de termine if entities x |= y as follows : 1.  X  X  X  X  X  X  X  X  X  (  X  X  X  X  X  X  X  X  X  X  (  X  ) ,  X  X  X  X  X  X  X  X  X  X  (  X  ) ) , where  X  X  X  X  X  X  X  X  X  X  (  X  ) = { (  X  ,  X  ) |  X  (  X  )  X   X  } ; 2.  X  X  X  X  X  X  X  X  X  (  X  X  X  X  X  X  X  X  X  X  (  X  ) ,  X  X  X  X  X  X  X  X  X  X  (  X  ) ) , where  X  X  X  X  X  X  X  X  X  X  (  X  ) = { (  X  ,  X  ) |  X   X   X   X  X  X   X  } ; 3. For every relation A (x,c) in  X  X  X  X  X  X  X  X  X  X  X  X  X  (  X  ) = {  X  (  X  ,  X  ) |  X  (  X  ,  X  )  X   X  } , { (  X  ,  X  ) |  X   X   X  } ; 4.  X  X  X  X  X  X  X  X  X  (  X  ,  X  ) : for every u  X  U , there exists v  X  V , such that u  X  v .
The above tests can be carried on X by a Depth -First Traversal procedure that starts from the root of X and visit s each entity once following the relation edges . The func-tion f can be determined by the unification procedure, which will be introduced in the next section. The purpose of PEG unification is to combine partial event recognitions from distrib-uted event recognizers without introducing unwarranted information. To minim i ze memory and storage, we wish the combined PEG to be as small as possible without losing critical information. Since each event recognizer has different recognition ac-curacy, we need a way to e xpress our confidence on the partial recognitions as well . To satisfy these requirements, we formalize the PEG unification as a constrained op-timization problem.

Problem Definition: G iven two PEG documents X with weight wx and Y with weight wy , and a probabilistic taxonomy S , produce a PEG Z , without modifying X or Y , to minimize AIC(L) , such that Z |= X and Z |= Y under S , where AIC(L) = 2K X  X ogP(Z|X,Y,S) and K is the number of probabilistic options in Z .
 The we ights express confidence on X and Y in the mixture of experts model [ 20 ]. The entailment constraints ensure that Z only contain s assertions derived from X , Y and S . Minimization of the AIC [ 7 ] leads to the smallest and most likely Z . 
Fig. 3 and Fig. 4 show two PEG documents X and Y derived from the PEG in Fig. 1 . Fig. 5 depicts the PEG Z produced by unifying X and Y . To guarantee the entail-ment constraints, the unif ication procedure unions the relations in X and Y , but inter-sects the classes and members of X and Y . As the result, all the relation (red) edges in X and Y occur in Z , while some class (cyan) and member (green) edges in X and Y are discarded by Z . Th e probabilities in Z are a mixture of the probabilities in X and Y based the weights wx and wy . In these examples, wx=wy=0.5 . 
The numbers inside the brackets at each entity in Z point to the origin entities in X and Y that merge into the Z entity. For example, label  X 11 [1,6] X  in Z indicates that entity 11 is merged from entity 1 in X and entity 6 in Y . These origin numbers define the function f from Z to X and Y needed for the entailment procedure in Section 3. 4.1 Unification Procedure
The unification of PEG X and Y into PEG Z is carried by unifying the entit ies in X and Y into the entities in Z . To unify entity x in X and entity y in Y into entity z in Z , we intersect the classes and members of x and y , while union their relations , in order to satisfy the entailments . For example, if X={A(x)} and Y={B( y )} , then Z does not include A( z ) or B( z ) , as it would violate the entailment constraints. Instead, Z only includes (A X B)( z ) , where A X B denotes the Greatest Lower Bound (GLB) [ 11 ] of A and B in a taxonomy. In the taxonomy in Fig. 2 , we have person X kid =kid and in-fant X kid = null . In a taxonomy with multiple inheritance, the GLB of two concepts A and B may be a third concept C such that C&lt;A , C&lt;B and C is closest to both A and B .
To unify the classes of x and y , we intersect every class option A in classes(x) with every class optio n B in classes(y) to find their GLB A X B , and compute the conditional probability of A X B given A , B and S . This process can produce between 0 and |classes(x)||classes(y)| GLB classes. T o unify the members, we apply the same idea. More formally, the classes and members of entity z are derived from x and y by equa-probabilities in descending order . Equation (4) calculates the conditional probability of C =A X B given co ncept A ( or B) , by summing the probabilities of all the shortest paths from A ( or B ) to C in the taxonomy. 
To unify the relations of x and y , we need to union their relations as they may over-lap . Two relations, A in relations(x) and B in relations(y) , are regarded as equivalent derived from A and B . The more a i  X b j exist in A X B with high P ij , the more likely A and B are equivalent. T he degree of match between A and B is defined in (5) , where probability P z = P ij is defined in (3) .
Since A can be equivalent to only one B (or nothing) , finding the equivalent rela-tions between two sets of relations becomes an instance of the assignment pro blem [ 21 ] . To solve this problem , (6) defines the cost between A and B to be negatively correlated with their degree of match, such that minimum cost assignments result in maximum matches . The relation assignment process produces three disjoint relations for z . Relations of x and y that do not intersect ( A X  and B X  ) have disc ounted probabili-ties because the options occur in only one PEG . Rela tions that intersect ( C ) retain the GLB options with probability mixtures. All three relations are selected by the k -max function .  X   X   X  =  X  - X  X  X  X  { (  X  ,  X   X   X   X  ) | (  X  ,  X   X  )  X   X  } ;  X   X   X  =  X  - X  X  X  X  { (  X  ,  X   X   X   X  ) | (  X  ,  X   X  )  X   X  } ;  X   X  =  X  - X  X  X  X  { (  X  ,  X   X  (  X  |  X  ,  X  ,  X  ) ) |  X  =  X   X   X  , (  X  ,  X   X  )  X  A , (  X  ,  X   X  )  X   X  } . To unify X and Y with more than one entity node, a Depth -First T raversal procedure is used. The procedure starts from the root entities of X and Y , unifies t heir classes, members and relations as described above . The procedure then recursively traverses to the child entities of the roots and repeat s the entity unification procedure , until all the entities of X and Y are unified . The procedure keeps track of which entities in X and Y have been unified to an entity in Z and reuse s the results. Since the procedure needs to compare all the relations of two entities, t he time complexity of the unifica-tion procedure is O(|V(X)|+|V(Y)|+|E(X)||E(Y)|) , where V(.) denote s the entities and E(.) denotes the relations of a PEG document. 4.2 AIC Minimization The use of function k -max to select the top k options in classes, members and relations is justified by AIC. We adapt AIC to measure the complexity of a PEG document based on equations (7) -(11) , where e and r are the entities and relations of Z . 
Let AIC(L , K ) denote AIC (L) with K =kc+km+ko total options, then it can be shown that  X  X  X  X  (  X  ,  X  + 1 )  X   X  X  X  X  (  X  ,  X  ) = 2  X  log ( 1 +  X  ) &gt; 0 , where  X  X 1 . This property suggests that increasing K will always increase AIC regardless of the proba-bility distributions. Therefore, to minimize AIC, we should reduce K as much as pos-sible and increas e the log probabilities LC , LM and LO for a given K . Function k -max implements this strate gy by selecting the highest probabilities for a given K . Merging two probabilistic options into a GLB always reduces K by 1 and decreases AIC. For this reason, unification of probabilistic options based on GLB is justified by both the entailment constrain ts and AIC. A prototype system was developed in Python 3.5 based on the proposed entailment and unification procedure s . The prototype system uses python -igraph package ( http://igraph.org/python/ ) to visualize PEG documents. All the te sts were run on a Windows 7 Professional Dell Latitude E7450 notebook computer with 2.6Ghz dual cores and 8GB memory. randomly generated PEG documents. The workflow of our experiments is depicted in Fig. 6 . The PEG Generator generates random PEG documents of a given number of entiti es. The PEG Sampler uses Markov random walk to randomly select certain per-centages of classes, members of relations from each entity of G . The PEG Unifier unifies X and Y into Z . The entailment procedure validate s that Z |= X and Z |= Y . The test use s a du mmy taxonomy that only check equality of concepts .
 We generated 10 sets of PEG documents of N=10 to 100 entities. Each set contains 100 random PEG documents of the same size N . In each PEG document of size N , determined by uniform distributions. X and Y always selects 60% classes, members and relations of G at random . For each test set, the PEG Unifier selects k =3, 4 and 5 best options for classes, members and relations for Z . For each N and k , we recorded the processing time (millisecond) for unifications averaged over the 100 documents and the results are shown in Fig. 7 . 
The chart shows that the processing time increases with the size of the PEG docu-6 0], [70, 100] but jumps between the ranges . We suspect this is due to the increase of relations in the documents. The chart also indicates that different k values have no significant effect on the performance as the three trends virtually overlap for all the N values. This suggests that function k -max is efficient when the PEG size increases. This paper presents a probabilistic knowledge model ca lled Probabilistic Event Graph, a method to test the entailment between PEG documents and a method to uni-fy PEG docum ents . For future work, we plan to improve the entailment and unifica-tion algorithms for large size of realistic knowledge graphs. 
As machi ne perception is moving from object classifications toward semantic event analysis, and from standalone to collaborative recognitions, we think logic and probabilistic based models will play increasing ly important role for machine percep-tion.

