 In the current Web 2.0 era, the popularity of Web resources fluctu-ates ephemerally, based on trends and social interest. As a result, content-based relevance signals are insufficient to meet users X  con-stantly evolving information needs in searching for Web 2.0 items. Incorporating future popularity into ranking is one way to counter this. However, predicting popularity as a third party (as in the case of general search engines) is difficult in practice, due to their lim-ited access to item view histories.

To enable popularity prediction externally without excessive cra-wling, we propose an alternative solution by leveraging user com-ments, which are more accessible than view counts. Due to the sparsity of comments, traditional solutions that are solely based on view histories do not perform well. To deal with this sparsity, we mine comments to recover additional signal, such as social influ-ence. By modeling comments as a time-aware bipartite graph, we propose a regularization-based ranking algorithm that accounts for temporal, social influence and current popularity factors to predict the future popularity of items. Experimental results on three real-world datasets  X  crawled from YouTube, Flickr and Last.fm  X  show that our method consistently outperforms competitive base-lines in several evaluation tasks.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval -information filtering, retrieval models; Popularity Prediction; Item Ranking; Bipartite Graph Ranking; Com-ments Mining; BUIR
The era of static webpages has been surpassed over a decade ago with the advent of Web 2.0. Users now not only post content in This research is supported by the Singapore National Research Foundation under its International Research Centre @ Singapore Funding Initiative and administered by the IDM Programme Office. Web 2.0, but also participate in a spectrum of means  X  comment-ing, voting, forwarding, and tweeting  X  among other social actions. This characteristic of the new Web has led to a more dynamic and immediate sense of popularity, as the demand for items is influ-enced by such social actions in groups and by real world events. To satisfy the constantly evolving information needs of users in the Web 2.0 setting, it is indispensable for ranking engines to prop-erly account for these temporal dynamics. However, we find that current search engines do not utilize popularity as effectively as they could. Figure 1 gives an illustrative example, which shows the YouTube results from Google for the query  X  X he Voice of China X  on the 24th of July, 2013. The Voice of China is a popular Chinese reality talent show that premiered in 2012, and kicked off its sec-ond season on 12th July 2013. Given this background, we believe that most users who queried  X  X he Voice of China X  in July 2013 are looking for videos of the second season. However, almost all the top results returned by Google are past popular videos from the first season. The first relevant result of the second season is ranked 16th,  X  X elow the fold X  for many searchers. Inspecting the view count of these search results over the next three days, we find that the top three results receive less than 10,000 views, while the top-ranked video of the second season (ranked 16th) was viewed over 100,000 times. This indicates that at least for this particular query, Google did not make optimal use of future popularity, and hence did not satisfy many users X  search expectations.

Along similar lines, Gon X alves et al. [13] previously observed that popularity had not been well utilized in the blog search. They found that popularity is quite different from the importance mea-sured by PageRank [27] on a Web graph. Importantly, they show that search effectiveness and user satisfaction are significantly im-proved when incorporating popularity into ranking. Therefore, we believe that user experience can be improved, particularly for time-sensitive queries, if search engines can predict which items will become more popular in the near future and rank them accordingly.
Modeling and predicting the popularity of Web content can ben-efit many downstream applications such as online marketing [20], cache managing [1], search ranking [13], social network model-ing [4], and so on. We equate estimating popularity with the task of predicting future view count, a direct and objective means to assess the interest of users. Though previous works have focused on popularity prediction, their primary strategy is to mine the view history of items [32, 28, 1]. However, for some external services which are not content providers, previous solutions are infeasible because they require full access to the item X  X  view count histo-ries [13]. While many Web 2.0 sites often provide a current view count for items, repeated crawling to build and maintain such view histories is expensive. This method also does not allow prediction for newly crawled items, due to insufficient view history. Figure 1: The top three search results for the query  X  X he Voice of China X  on 24th July 2013, from Google, restricted to the YouTube.com domain.

To address these challenges faced by external observers, we pro-pose an alternative approach by exploiting user comments, which are more easily accessible than view counts. Comments are a rich source of information, containing not only the opinions of users, but also timestamps which allow us to deduce the view history, and usernames for mining potential social influence. As commenting (or interchangably,  X  X eviewing X ) is a basic social action enabled for most Web 2.0 sites, our solution is generally applicable for Web 2.0 sites. Although many works have studied the use of comments, in-cluding summarization [16], ranking [30], clustering [15] and rec-ommendation [31, 38], there have been little work that use com-ments to predict item popularity. However, comments are much sparser than views: a user viewing an item often does not com-ment on it. As such, simply using comment counts in a time series approach is insufficient, especially for less popular items with few user comments. Thus, it is important to mine and incorporate addi-tional popularity signals from the comments in prediction.
We propose to predict item future popularity from comments based on three hypotheses about the 1) temporal, 2) social, and 3) current popularity factors. We model comments as a time-aware bipartite graph, on which we propose a regularization-based algo-rithm Bipartite User-Item Ranking (BUIR) to rank items by captur-ing the three hypotheses. We evaluate BUIR extensively on three real-world datasets that represent a spectrum of different media: YouTube (videos), Flickr (photos) and Last.fm (music). Experi-mental results show that BUIR consistently outperforms competi-tive baselines in predicting item future popularity. We further ana-lyze the prediction quality on specific item subsets  X  on per-query and tiered popularity bases  X  to deconstruct the efficacy of BUIR in complementing Web search ranking, and to provide more insights into how to use comments for popularity prediction.

This paper is organized as follows. We first review related work in Section 2. In Section 3, we show the feasibility of using com-ments for popularity prediction in an initial analysis over YouTube data. In Section 4, we detail the BUIR method and evaluate it in Section 5. We conclude the paper in Section 6.
In this section, we review work on popularity prediction first, then discuss Web 2.0 user comment mining.
Popularity prediction can be classified into three broad types: statistics-based, classification-based, and model-based approaches.
Statistics-based Prediction . These approaches assume that past popularity is a good predictor of future popularity. Szabo and Hu-berman [32] analyzed the popularity growth of YouTube videos and Digg stories, finding a strong correlation between the logarithmi-cally transformed past popularity and current popularity. They pro-posed a univariate linear model to capture this correlation. Later, Pinto et al. [28] extended the univariate model to a multivariate one by incorporating additional historical points and features. Radin-sky et al. [29] proposed several time series prediction methods of user behaviors based on state-space models. All of these techniques require access to the view histories of items, which are difficult for third parties to obtain in practice, as described earlier in Section 1.
Classification-based Prediction . These approaches transform the popularity prediction problem into a discrete classification task by using different classifiers such as k -nearest neighbors [17], de-cision tree [17, 34], and support vector machine [17, 34]. Various features derived from the textual content, time series, and commu-nity structure are distilled as input features for the classifiers. The output of such methods are unfortunately, too coarse-grained for many applications, such as Web search ranking.

Model-based Prediction . Model-based approaches are difficult to formulate, but often yield more insight and higher accuracy. Yin et al. [37] ranked potentially popular items from early votes. They modeled each user X  X  voting behavior as a constrained random process. Recently, Ahmed et al. [1] predicted popularity by mod-eling the temporal evolution of online content. They first split an item X  X  history into time windows, and then generated clusters of items in each window. Based on the clusters, they built a transi-tion graph to predict the most promising cluster for an item in the future.

The above methods, however, only model a user X  X  past behav-ior or an item X  X  history individually, and do not account for social signals, an important criterion in Web 2.0. Lerman et al. [22] an-alyzed friending actions in Digg as a way of propagating user be-havior to influence other users. They modeled user voting behavior as a stochastic model, considering both social influence and web-site layout to predict story popularity. This work lends evidence that users exert varying levels of influence on others, and that such social factors need to be taken into account to predict popularity. However, their work is too specific to Digg; parts their model do not easily transfer to other sites (requiring Digg specific page view dis-tribution and other internal factors), making the lessons drawn from their study difficult to port to other Web 2.0 sites. For items with longer lifecycles where external events may exert a strong influ-ence, such as unexpected view bursts (i.e., YouTube videos), their approach may not work well.
User comments, as one of the most common sources for user-generated content, have received much attention in recent years.
Descriptive Information Mining . The descriptive information contained in user comments are leveraged in many applications. Mishne et al. [25] found that incorporating comments into blog search improved recall by 5 X 15%. Hu et al. [16] integrated com-ments to improve the summarization of blogs. Noise in comments is a well-known source of difficulty, yet the content of comments still shows utility in IR applications when properly handled: Fillip-pova et al. engaged comments for video classification [12], while He et al. [15] did similarly for Web 2.0 item clustering.
Sentiment Information Mining . Sentiment latent in user com-ments can also be utilized to rank items. Wijaya and Bressan [35] ranked movies based on the sentiment ( positive or negative ) of re-views. Their obtained ranking was highly correlated with the gross income of movies. Pedro et al. [30] ranked images from an aes-thetic perspective by extracting image features and opinions from comments. In recent work, Zhang et al. [38] performed phrase-level sentiment analysis of user comments to improve the accuracy and explainability of recommender system.

Although these works have mined comments, they focused ex-clusively on textual content. In particular, timestamps and user-names are additional important features that can be extracted from comments, which have been neglected in existing work. We be-lieve that there is important knowledge contained in the timestamps and user communities (evidenced by usernames), and that popular-ity prediction can be refined to utilize these relevance signals for search ranking. We thus propose to exploit user comments for pop-ularity prediction. There are two works most closely aligned to our proposal, but they also have shortcomings. In [33], due to lack of ground truth, they used comments as the popularity metric, which we show differs from actual view count in Section 5.2.1. In Ja-mali et al.  X  X  work [17] on Digg, they transformed the prediction problem into one of classification tasks, using Digg-score as the popularity index, rather than the views.
We now conduct a feasibility study on a YouTube dataset to val-idate our idea of using comments for popularity prediction.
YouTube captures detailed video statistics, which include the view, comment and favoriting history up to the current date, as charts (Figure 2). YouTube creates these charts via the Google Chart API, which exposes the data points in the request URL. This allows us to obtain all of the data points used to create the charts, following the methodology in [11].

Since we want to study the feasibility of comment-based popu-larity prediction, a general sample of videos is sufficient to form a proof-of-concept. We use ten general queries, drawing from the most popular tags at collection time (9th August 2012), to generate a corpus of videos:  X  X nimal X ,  X  X ar X ,  X  X ood X ,  X  X ootball X ,  X  X ame X ,  X  X ovie X ,  X  X usic X ,  X  X ba X ,  X  X lympic X  and  X  X eople X . We collect the YouTube pages containing the videos using the YouTube API, re-questing the top 1,000 videos using three different order-by sort-ing criteria: ranking by relevance, view count and published time. From this preliminary corpus, we remove (1) duplicate videos, (2) videos with a low number of comments and views (thresholds set to 10 and 20, respectively), and (3) videos that lack statistics (some videos do not allow commenting, or choose to keep these statistics private). The analysis in the remainder of this section is based on this final corpus of 14,509 videos.
We conjecture that the comment history and view history are highly correlated, as exemplified by the view and comment curves in Figure 2. We wish to gauge the quality of their correlation to see whether we can use the comment history as a surrogate to predict future views. While prior works [25, 6] have shown that comments do exhibit a strong correlation with views, this is only done for a particular temporal snapshot ( i.e. , on some given day d , how highly correlated are the total cumulative view count with the comment count). To ensure the feasibility of our approach, we need to ana-lyze how the histories of views and comments on individual items evolve over time. To the best of our knowledge, there has not been any studies on such correlation.

The historical views of a video form a time series. We first calcu-late raw counts per time point, then measure the similarity between the comment history and view history using the correlation [5]: where x 1 ,...,x n and y 1 ,...,y n denote the comment series and the view series, x and y denote the mean of the two series, respectively.
The mean correlation coefficient for each item is 0.76, with a standard deviation of 0.3. Figure 3 shows the cumulative distri-bution function (CDF) of videos given their correlation. As the figure shows, more than 45% have a correlation greater than 0.9 (strong correlation), and more than 80% have a correlation greater than 0.5 (good correlation). We conclude that comment history is highly correlated with view history , which lends supports for our comment-based prediction proposal.
The strong correlation between the comment history and view history indicates that we can substitute  X  X iew X  for  X  X omment X . Can past comments be used to predict future views, as we propose?
To answer this question, we perform an autocorrelation analysis of the comment series. The autocorrelation coefficient measures the correlation of a time series with itself over different lags. Given the time series x 1 ,...,x n and a lag k , the autocorrelation coefficient of the series { x i } at lag k is the correlation of series x and series x k +1 ,...,x n . It is usually approximated as follows [5]:
Figure 4 shows the mean autocorrelation of the comment series at different lag k ( 0  X  k  X  97 ). The figure exhibits a short-term correlation characterized by a large value, acr 1 = 0 . 64 , followed by a few further coefficients which are successively smaller ( acr 0 . 51 ,acr 3 = 0 . 43 ). Values of acr k for longer lags ( k  X  40 ) are approximately zero. We thus conclude that comment histories can reflect future comment in the near-term , and that its predictive abil-ity decreases with a larger lag .
Most applications seek to determine an item X  X  ranking relative to other items. We thus focus on the relative ranking of items  X  rather than exact popularity prediction ( cf. Section 5.2.1)  X  to reflect their potential popularity in the future (as in [37]).
Having shown a strong correlation between the comment and view series, an intuitive solution is to apply any time series pre-diction approach on the comment series. However, we notice that comments are relatively sparse compared with views, e.g. , many items do not have any comments in a particular time unit at all. This has an adverse impact on regression. We argue thus that in case of comments sparsity, just using the counts of comments and applying traditional time series prediction approach is insufficient; it is essential to incorporate other factors for prediction, such as so-cial influence. To account for such latent signals in user comments, we first model user comments as a time-aware bipartite graph, and Figure 3: CDF of videos with respect to their correlation co-efficient.
 then predict future popularity based on this graph using a regular-ization framework [39], which enables the incorporation of multi-ple factors in a principled manner.
Let G = ( U  X  P,E ) be a bipartite graph, where U and P represent users and items respectively, and the edges E represent comments (Figure 5). Each edge carries a weight w , modeling its contribution towards an item X  X  future popularity. As our analysis shows a strong near-term correlation, we assign w based on tempo-ral considerations. We model recent (older) comments as contribut-ing more (less) to an item X  X  future popularity, by assigning edge weight as a monotonically decreasing exponential decay function: where  X  is the decay parameter that controls the rate at which w changes with time, t 0 is the ranking time and t ij is the commenting time of user u i on item p j . a and b are constants, to be tuned for the particular media and site. Time units are arbitrary; they can be assigned as minutes, hours, days, weeks or other units, depending on the temporal resolution and the domain of items to rank. If no edge exists between u i and p j , then w ij is zero.

Exponential functions have been widely used to model the di-minishing impact of past behavior over time ( e.g. , [10]). Due to its simplicity and interpretability, we have purposefully chosen it as a proof-of-concept of our BUIR solution in this work; however, more accurate decay functions do exist [8] ( i.e. , polyexponential decay and sliding window function) and may further improve our model. We leave this possibility for future work.
We now present our proposed ranking method in the bipartite user-item graph. We describe the hypotheses that form the basis for our regularization function first before presenting our solution.
Generally speaking, we have three hypotheses about the future popularity of an item, and wish to incorporate into our model:
H1. Temporal Factor: If an item receives many recent com-ments, it is more likely to be popular in the next time step ( cf. our study of YouTube).

H2. Social Influence Factor: If the users commenting on an item are more influential, the item is more likely to receive more views in the future. This is enabled by the Web 2.0 social interfaces that propagate a user X  X  comments to friends and followers. Such social factors have been shown to be useful in popularity prediction and recommendation [22, 23].

H3. Current Popularity Factor: If an item is already popular ( i.e. , has accumulated a large amount of views), it is likely to garner more views in the future. This is effected by the ranking functions and recommendation interfaces in Web 2.0: the more views an item has, the more likely it will be suggested by the system. This  X  X ich-get-richer X  effect has been observed in some Web 2.0 systems [6]. H1 has been studied in our initial analysis of YouTube dataset. We further validate H2 and H3 through experiments in Section 5.3. We now devise regularizers to capture these three hypotheses. Our goal is to devise a ranking function f : P  X  U  X  R , which maps each vertex in G to a real number such that the value reflects the vertex X  X  popularity (for items) or influence (for users).
Capturing H1 and H2 . Combining H1 and H2 together yields an equivalent formulation: if an item is reviewed by many influen-tial users recently, it should be given a high score . We model this through the regularized term R 1 ( f ) : where w ij is the edge weight defined in Eq. (3); n and m denote the number of items and users, respectively; d p j and d u i are the weighted degrees ( i.e. , sum of edge weights) of item p j and user u malization, respectively.

We now discuss the relationship between R 1 ( f ) and our hy-potheses. First, minimizing R 1 ( f ) forces p j  X  X  normalized score ( i.e. , f ( p j ) / q d p j ) to be similar to the normalized scores of all its connected users. Thus, if p j is commented on by influential users, its normalized score will be large (as in H2 ). Second, note that the score of p j is normalized by q d p j , which is proportional to the de-gree of p j . Hence, in constraining the normalized score of p be similar to the scores of its neighbors, f ( p j ) is large when the degree of p j is large (as in H1 ). Therefore, minimizing Eq. (4) simultaneously captures both H1 and H2 .

Capturing H2 . We have enforced the social influence of user commenting behaviors on the popularity of items, however, we have not distinguished influential users. Intuitively, if a user has more friends, his behavior is likely to influence more users. Thus, we set a user X  X  initial influence score proportional to the log value of his number of friends: where g i is user u i  X  X  number of friends at the ranking time. We use add-1 smoothing to address the case where a user has no friends. We can now define the regularized term R 2 ( f ) to encode initial user influence:
We note that more accurate social influence models do exist ( e.g. , [3]), but we have purposefully chosen to rely just on the single fea-ture of the number of friends to make our method easily generaliz-able to a wide range of Web 2.0 systems.
Capturing H3 . To capture the potential  X  X ich-get-richer X  effect, we define the initial score of an item as: where v j is the total view count of item p j at the ranking time. Similarly, the corresponding regularizer to capture the current pop-ularity factor of items is defined as:
Regularization function . Having defined the regularizer for each hypothesis, we combine them linearly to obtain a final reg-ularization function: where the regularization parameters  X  and  X  determine the trade-off among these three terms. The first term is a smoothness term, that helps to rank items such that high scores are assigned to items that have been recently reviewed by many influential users ( H1 and H2 ). The second and third terms are for consistency , that assert that the final rankings should not overly deviate from their initial scores, which encode our hypotheses H2 and H3 .
The regularization function Q ( f ) defined by Eq. (9) needs to be solved (minimized) to obtain the final ranking. As two types of variables ( p j and u i ) exist in the function, we can find the solution using alternating optimization. Differentiating Q ( f ) with respect to p j and u i , respectively, and letting the derivatives be 0, we have: This is the iterative solution of the objective function Q ( f ) . It is guaranteed to find the global minimum, as Q ( f ) is strictly con-vex in both the p j and u i variables (the Hessian is positive semi-definite). We show the proof in the Appendix. Other standard optimization techniques ( e.g. , gradient descent) can also be used; alternating optimization has the advantage of quick convergence.
As the updating rules shown in Eq. (10) are linear transforma-tions of f ( p j ) and f ( u i ) , they can be equivalently written in ma-trix form. Let the ranking vectors be p = [ f ( p j )] n  X  1 [ f ( u i )] m  X  1 , and the initial vectors be p 0 = [ p [ u i ] m  X  1 . Let matrix S w be [ neat matrix form of Eq. (10):
By further reducing Eq. (11), we obtain a nice closed-form solu-tion:
Although the closed form can be obtained, in practical cases  X  especially when there is a large number of items to rank  X  the iter-ative solution is preferable, as the matrix to inverse is n  X  n . In our experiments, the iterative solution usually converges in fewer than 30 iterations, which is sufficiently efficient. Therefore, in subse-quent experiments, we implement the iterative solution of Eq. (11) and adopt the name BUIR ( Bipartite User-Item Ranking ) to refer this specific instance.
It is easy to show that a direct implementation of the iterative solution in Eq. (11) has a O ( mn ) time complexity, mainly due to the multiplication of S T w u and S w p . However, note that S cally sparse, as a non-zero entry denotes a comment by a user on an item. A representation of sparse matrix only needs to account for non-zero entries, instead of all mn entries. As such, the whole time cost of BUIR is O ( lc ) , where c denotes the number of comments, and l denotes the number of iterations executed to converge.
If one only aims to rank items without requiring a completed ranking for users, then the time cost can be further reduced through improved implementation. Embedding the update rule of u into the update rule for p in Eq. (11), we obtain: p = S The above can then be solved with simply iterating the update rule Eq. (13) until convergence. Note that transition matrix in the first term ( S T w S w ) and the entire second term remain unchanged be-tween iterations, thus they can be pre-computed offline. As such, the online ranking reduces to the straightforward power iteration algorithm for computing the stationary distribution of a Markov chain. Without any optimization, the time complexity is O ( ln Our experiments on our largest dataset (YouTube) with over 7M comments took 7.4 seconds to complete on a modest commodity desktop (Intel quad-core 3.40GHz CPU and 8GB RAM). And in our Flickr and Last.fm datasets, BUIR only takes 0.2 seconds to finish ranking. Coupled with previous work [24] that can further accelerate computation, we believe that BUIR can be applied in real-world large-scale online item ranking.
It is instructive to interpret how BUIR ranks items in relation to other graphical algorithms that have been adopted in IR.

From the iterative solution in Eq. (11), we can see that BUIR essentially captures the mutual reinforcement between users and items. The first term shows that the comment by a user will increase the target item X  X  score; and in return, the target item increases the user X  X  score. The number of items a user has commented on re-flects his engagement, and indirectly his influence. The second term shows that the score of items and users is partially determined by prior belief. To sum up, BUIR determines a user X  X  social in-fluence based on two source of evidence: his level of activity and his number of friends. Analogously, BUIR determines an item X  X  fu-ture popularity based on four aspects: the quantity of its comments, their timing (recency), the influence of its commenting users, and its current accumulated popularity.

Our proposed BUIR can be seen as a variant of PageRank [27] that treats the two types of vertices in the bipartite graph differ-ently. To show the connection, we first focus on the first equa-tion of Eq. (11). Assuming p and u represent the same set of ver-tices (removing the bipartite graph property), then the equation is equivalent to the personalized PageRank algorithm [14], where S (after normalization) serves as the transition matrix, and p Table 1: Statistics of our three Web 2.0 datasets. Avg C:I de-notes the average number of comments per item.
 personalized vector. As PageRank was originally proposed for ho-mogeneous graphs, where vertices uniformly represent entities of the same type, direct use of PageRank in our bipartite scenario will mix the weights of two types of entities, and may lead to unex-pected results. There are other graph ranking algorithms that are specifically designed for bipartite graphs which are more relevant, such as HITS [19], SALSA [21], Co-HITS [9] and CoRank [36]. It is known that HITS and SALSA will fall short when the graph is disconnected (tightly knit community effect [21]). As our user-item graph is built from sparse user comments, resulting in many discon-nected components, direct use of either HITS or SALSA may not lead to expected results. Co-HITS and CoRank are designed for different semantics  X  although they work on bipartite graphs, they consider the influence of like vertices of the same type, which is explicitly not used in BUIR (no influence among items or users).
Our solution forms a general framework, easily extendable to in-corporate other factors beyond what we have described. For new features related to individual comments, such as content and sen-timent relevance, we can estimate their weight in contributing to each item X  X  popularity and integrate them into the definition of w in Eq. (3). For new features related to individual items or users, we can model them within BUIR X  X  bipartite regularization framework (Eq. (9)), by adding corresponding regularized terms.
As BUIR is a general method which does not require any domain-specific knowledge, we provide a comprehensive assessment of its prediction quality over a wide variety of different Web 2.0 media. We crawl three real-world datasets from well-known Web 2.0 sites (demographics in Table 1) to assess our proposed BUIR solution. 1. YouTube (21,653 videos): The dataset used is identical to the one used in the preliminary analysis in Section 3.1, but omitting the third filter that drops videos that lack statistics. 2. Flickr (26,815 images): We follow the same collection method as in the YouTube case, using the same ten queries. We do not apply any frequency filter as this dataset is more sparse than YouTube. 3. Last.fm 1 (16,284 artists): As Last.fm X  X  search API differs from the two other datasets, we collect this dataset by obtaining data about artists: obtaining at most 100 similar artists for each of the top 1,000 most popular artists. For the query-specific evalua-tion, we query on the top 10 tags that describe a music style:  X  X las-sical X ,  X  X ountry X ,  X  X lectronic X ,  X  X olk X ,  X  X ip-hop X ,  X  X ndie X ,  X  X azz X ,  X  X etal X ,  X  X op X  and  X  X ock X . We assign each artist to the single tag that is used most often to describe the artist by Last.fm users.
We choose the three datasets for ease of evaluation, as these all provide item view count. Our datasets are crawled on two dif-ferent dates: for graph construction ( t 0 ) and for obtaining ground truth (GT) for evaluation ( t 3 ), which is 3 days after t observed that ephemeral trends are important to capture, we specif-ically aim to evaluate short-term prediction and chose 3 days as the
In lieu of view count, Last.fm provides a  X  X crobble X  count, which is the number of times Last.fm users listen to a track by the target artist. This differs from the view count of an artist X  X  page, but we argue more indicative of an artist X  X  popularity. For convenience, we use  X  X iew count X  to refer to scrobble count in Last.fm. target period to evaluate. The initial crawl t 0 for YouTube, Flickr and Last.fm is on 9th August 2012, 3rd September 2012 and 24th October 2012, respectively. For items in Flickr and Last.fm, we crawl the view count, the number of friends and the list of com-ments on the two dates. For YouTube, due to its privacy policy, we cannot obtain a user X  X  number of friends, so we set the initial score for users uniformly. As older items may have accumulated many past comments but which would not significantly contribute in BUIR, we discard comments older than five months before t for efficiency. If an item is commented by the same user multiple times, we only keep the most recent comment when calculating the edge weight. This also helps to avoid problems when users have tangential conversations via comments.
To assess the predicted ranking with the ground truth ranking, we employ ranking correlation in the standard form of the Spearman coefficient [2]. It measures the agreement between two rankings defined as follows: where N is the number of items in the ranking, s 1 ,i and s positions of the i th item in two rankings R 1 and R 2 , respectively. It ranges from -1 to 1, where 1 (-1) means a perfect agreement (dis-agreement) between the two rankings and 0 means no correlation.
While the Spearman coefficient is indicative of the agreement between two rankings, it does not reflect the importance of getting the top ranks correct, which are crucial for many applications such as Web search ranking. To address this, normalized discounted cu-mulative gain (nDCG) [18]  X  which rewards relevant results in the top ranks more highly than those ranked lower  X  is widely used for evaluating query-dependent rankings. As such, in our query-specific evaluation (Section 5.2.2), we also employ nDCG @ k to evaluate the top k rankings for each query. As nDCG takes rele-vance levels into account, we define the top 10% of items found in the ground truth ranking as relevant, where higher ranked posi-tions are accorded more relevance, computing a relevance score of 1  X  i 0 . 1  X  N [37] for the i th ranked item and a score of 0 for items beyond the top 10%.

We compare our BUIR with the following five baselines: 1. View Count (VC) : Rank based on the current view count of items. This corresponds to our belief in Hypothesis H3 alone. 2. Comment Count in the Past (CCP) : Rank based on the num-ber of comments received in the 3-day period prior to t 0 to t 0 ), corresponding to our Hypothesis H1 . 3. Comment Count in the Future (CCF) : Rank based on the number of new comments received in the three days after t t ). This is an oracular method with access to future comments. 4. Multivariate Linear model (ML) [28] : We implement this method on the comment series of the 30 days prior to t 0 , aggregat-ing comment counts into 3-day windows, each contributing a fea-ture, for a total of 10 features. This is the state-of-the-art statistical method for predicting the popularity of Web content. 5. PageRank (PR) [27] : Our temporal user-item graph is bi-partite, which could cause the random walk to become periodic and non-stationary [26]. To work around this, we use the stan-dard method to set a uniform self-transition weight w ii = 1 for all nodes, and then convert the weight matrix to a probabilistic one for use with PageRank. For the damping factor, we vary its value from 0 to 1 with step size 0 . 05 . Experimental results show consistently good performance when the damping factor is in the range 0.1 to 0.9; we set it to 0 . 85 as suggested in [27].
In our BUIR solution, there are two sets of parameters to be spec-ified: 1) ones for assigning edge weights, and 2) ones for the regu-larization. Edge weights are assigned intuitively: for the time unit of YouTube and Last.fm, as comments are rich and reflective of popularity, we set it to 1 day; for Flickr, we find that the comments are posted less frequently, thus we set it to 3 days; for the time decay function in Eq. (3), we empirically set  X  = 0 . 85 , a = 1 , and b = 0 for all datasets. As for the regularization parameters  X  and  X  in Eq. (11), we randomly held out 10% of the dataset as development for parameter tuning. We use grid search to set the best parameters on the development portion, and then evaluate all methods on the remaining 90% test portion.

Note that although the first two baselines are heuristic and sim-ple, they do produce reasonable results for short-term popularity prediction, thus forming competitive baselines (see [29]). For all methods, if items receive the same score, we break ties by ranking based on their current view count.
We first evaluate the prediction of all items in each dataset. As the overall ranking of all items does not tell the whole story, we then further dissect the results through evaluating on subsets of items, in order to better understand the task and results. Specifi-cally, we assess the performance on individual queries, and study the performance over different popularity levels.
Table 2 shows that BUIR achieves the highest fidelity in ranking items of the test datasets, among all methods. Further experimenta-tion of 10-fold cross validation shows that BUIR obtains very con-sistent performance, significantly outperforming all other methods ( p &lt; 0 . 01 , via one-sample paired t-test). BUIR is followed by CCF and CCP, where the difference between CCP and CCF are insignificant. VC also obtains a good performance in general, indi-cating the effectiveness of H3 . PageRank (PR) performs poorly for Flickr and Last.fm, indicating that just the centrality of an item in the user-item temporal graph is insufficient for prediction. We also used BUIR X  X  initial vector p 0 and u 0 as the personalized vector of PageRank, which also results in poor performance. This lends evidence that separately handling the two vertex types (users and items) in the bipartite graph is important.

It is surprising that the state-of-the-art ML approach underper-forms CCP, as ML leverages more information: comments in the recent 30 days compared with CCP X  X  access to only three days. There are two possible reasons for this: 1) short-term prediction, and 2) ML X  X  optimization criterion. As the prediction task is a short-term one, the most recent data carries the most signal  X   X  X hat happened yesterday will happen tomorrow. X  Radinsky et al. [29] concurs with this observation, showing that in the short-term pre-diction of query and URL clicks, considering only the last value of the time series generally outperforms other regression methods, such as using power weighting function and linear weighting func-tion. This also indicates the effectiveness and competitiveness of simple baselines in near-term popularity prediction. The second cause may stem from ML X  X  use of minimizing the mean Relative Squared Error (mRSE) [28]) as its optimization criterion. We note Figure 6: Improvement in Spearman coefficient between BUIR and the best baselines of query-specific evaluation. that using mRSE as the optimization metric may favor evaluations on items with a small number of current views, as the relative popu-larity growth to learn are larger compared to items with a large cur-rent views 2 . As a result, the parameters learned may not be mean-ingful: we find that optimized weights are sometimes non-sensical ( i.e. , negative) and that the weights for recent time units can be smaller than the earlier ones, also contradicting intuition. We also find that when we decrease the number of features to learn, perfor-mance increases. Thus, although ML does provide a better estima-tion of future popularity than CCP in terms of mRSE, we believe this criterion does not fit well with the goal of relative ranking. This also highlights the difference between the task of predicting the exact popularity and ranking items by the predicted popularity. Although the (exact) popularity prediction problem is more chal-lenging compared with the relative ranking problem, we believe that the ranking problem is more suited for applications where the ordering (and not the exact numeric quantity) is important: such as search ranking, recommendation and online advertising.

It is worth noting that correlation levels dramatically differ in each dataset. YouTube shows the highest correlation while Flickr is the lowest. This indicates that comments in YouTube are generally richer and thus better reflect trending and popularity growth. Flickr users, as a whole, are less active than YouTube users (as can be seen from the comment statistics in Table 1). More specifically, many items do not receive sufficient comments to reflect their future pop-ularity; some items even do not receive any comment within our 5 -months window. In these cases, H1 does not hold, which leads to the degraded performance of comment-based prediction methods. Let us dissect the ranking lists to gain additional insight. In Last.fm, we notice that BUIR incorrectly ranks two items very high, while their GT ranks are low. Looking into the data, we find that the two abnormal items are two well-known artists  X  Lady Gaga and Madonna  X  ranked 4th and 7th, while their GT rank is 170th and 178th, respectively. After observing the comments, we find that the two artists receive many recent comments, but do not receive a proportional play count. Many comments are about two artists as a persona or just express praise, rather than their music. In Flickr, a similar phenomenon occurs with a few images that are ranked high but have low GT ranks. One 3 has 1,891 comments but only 4,115 views; the other 4 has 1,276 comments but only 3,299 views. Examining the details, we find that many users leave com-ments for participating in Flickr group activities (  X  X ood work! I like it!! This photo definitely deserves a Bronze Trophy! FLICKR BRONZE TROPHY GROUP Post X  ), which is the cause for the ex-cessive ratio of comments to views. In both the Last.fm and Flickr cases, the items are ranked incorrectly as the comments are not re-flective of their intrinsic popularity.
The results of tiered popularity evaluation (Section 5.2.3) reflect this: ML performs better on less popular items in general. Figure 7: Comment and view statistics (mean) for items re-turned by the ten queries.
 We recap our example query about  X  X he Voice of China X  from Figure 1. As shown in the figure, the top results of Google X  X  search are all past popular videos that do not touch on the second season until the 16th search result. Looking into the comments, we find the top three videos receive very few comments close to the crawl date, while videos concerning the second season in contrast, received many recent comments. We import the top 20 results (along with their comment streams) of Google X  X  search for this query, providing to our BUIR algorithm for reranking. BUIR ranks the video about the second season in the top position (which we view as the correct ranking). This example also shows the relevance signals resided in user generated comments, which complement the search ranking when other signals are insufficient to rank well.

From our two case studies, we can see that if an item is experi-encing a burst and the burst is reflected in comments, BUIR suc-cessfully ranks it high. However, in the case of items receiving a disproportionally high number of comments to views, disobeying H1 , BUIR is misled into making incorrect rankings. We also evaluated prediction quality on a per-query basis to test BUIR X  X  variability for specific queries and its feasibility for use with Web search ranking.

On our datasets, per query, BUIR needs to rank between 500 to 3 , 500 items. Figure 7 shows the average number of comments and views of items for each query, which highlights the variability in comment and view count between queries; it is not necessary that items with many views have a corresponding number of comments, or vice versa (seen the case of query  X  X op X  and  X  X ock X  of Last.fm).
Table 3 shows the average performance over all queries. We per-form one-sample paired t -test ( p -value = 0 . 05 ) to assess statistical significance. Supporting our previous results in Table 2, BUIR per-forms the best in all datasets. Specifically, as judged by the Spear-man coefficient, BUIR outperforms all baselines except the case of CCF in YouTube, where they are statistically comparable. Surpris-ingly, for nDCG@10, VC achieves comparable performance (the same significance level) with BUIR in all datasets. As nDCG@10 only evaluates the ranking of the top 10 positions, of which are all popular items, we hypothesize that the current view count is a good indicator of popular items. This motivates the need to analyze pre-diction at other popularity levels (detailed later in Section 5.2.3).
We further examine the performance for each query. Figure 6 shows the percentage of improvement in Spearman coefficient be-tween BUIR and the best baselines (CCF, ML, and VC for YouTube, Flickr, and Last.fm, respectively). As can be seen, BUIR bests the Figure 9: Comment statistics (mean and standard deviation) for items in the ten popularity tiers. baselines compared in all cases with the exception of  X  X lympic X  in YouTube and  X  X lassical X  in Last.fm. We investigate the cause for these performance exceptions.

For  X  X lympic X , CCF and CCP show a significant improvement over other methods ( 0 . 80 for CCF and CCP; 0 . 72 and 0 . 34 for BUIR and VC, respectively). The YouTube dataset is crawled on 9th August 2012, during the London Olympic Games. Many col-lected videos of the query  X  X lympic X  from YouTube are indeed about the London Olympic Games. These videos are rather new, such that they have not accumulated enough view count to reflect their popularity ( cf. Figure 7 X  X  view statistics). However, the recent comments are more reflective, as users are actively commenting on the events. From the user comments, we observe that users watch videos largely according to their interests or perhaps their country X  X  medaling in an event. In this case, H2 (Social Influence Factor) does not strictly hold. Hence, our method does not give the bet-ter result. For such new items, we postulate that performance may be improved with a more fine-grained time unit for BUIR. Chang-ing the time granularity to an hourly basis, BUIR X  X  performance improves (from 0 . 72 to 0 . 76 ), although still underperforming CCP and CCF. This lends tentative support to our idea, but which needs further investigation in future work.

For the results of  X  X lassical X  in Last.fm, VC obtains the highe-set Spearman coefficient ( 0 . 781 ), followed by BUIR ( 0 . 780 ) and CCF ( 0 . 765 ). The query  X  X lassical X  reflects a wide range of classic musicians, such as Fr X d X ric Chopin and The Beatles. Such items have existed for a long time, and have already accumulated many views and reached a steady state in attracting views. In these cases, current view count (VC) reflects their future popularity well.
While BUIR performs well overall, does it perform consistently on items of different popularity? To answer this question, we study the prediction quality over different popularity levels. We first sort items by descending view count at t 0 and then split into ten equal-sized subsets: Tier-1 (most popular) to Tier-10 (least popular). We report the results for ranking correlation (note that as each tier ac-counts for a popularity range, nDCG is already considered).
Figure 9 reports the comment statistics (mean and standard devi-ation) of the ten tiers. Both the YouTube and Last.fm datasets show the same trend: the average number of comments decreases when moving to higher (less popular) tiers, while all tiers in Flickr do not show much difference. This is because Flickr users largely refrain from making comments compared to YouTube and Last.fm users. As a result, popular items with high view count do not necessarily mean that they will have a high number of comments. From the comment statistics, we can see that the items in high tiers are less popular items with a low number of comments in general.

Figures 8 shows the performance broken down by tier. In gen-eral, we observe the same trends over all three datasets. Firstly, BUIR consistently performs well and the improvements over the comment-based baselines (CCP and CCF) are more noticeable for higher tiers, corresponding to less popular items. Secondly, current view count (VC) performs well for low tiers while suffers signifi-cantly for high tiers, and is worse than CCF and CCP. As VC ranks well for most popular items ( cf. nDCG@ 10 of query-specific evalu-ation, in Table 3), we conclude that the current view count is a good predictor for popular items, but not for less popular items. Further-more, we also note CCF does not always outperform CCP, although CCF utilizes the future knowledge. This indicates the limitation of simply using the comment count for popularity prediction, and mo-tivates the necessity of mining more signals from user comments for prediction.

For Flickr, BUIR improves over CCP and CCF significantly in all tiers; while in the Last.fm case, BUIR shows slight improvement in lower tiers (less than 5), which represent more popular items. To be precise, the average improvement over CCF in Tiers 1 X 5 is 5.0%, while in Tiers 6 X 10, the improvement is 12.1%. We note that the average number of comments in Tier 1 X 5 is 30 . 0 , while in Tier 6 X  10 is only 4 . 3 . This indicates that for items in the top tiers (which can be said to have already accumulated sufficient comments), tak-ing social influence into account may not capture much additional signal. Conversely, this highlights social influence as a good signal for prediction of less popular items, earlier given as H2 . To conclude the above three sets of experiments, we recap the key findings to predict popularity based on user comments:
In this final subsection, we wish to validate the necessity for modeling all three comment-based hypotheses in BUIR. As H1 is intuitive and has been studied in Section 3, we concern ourselves Table 4: Spearman coefficient of overall prediction and perfor-mance decrease of different parameter settings.
 primarily with the H2 (social influence) and the H3 (current popu-larity) factors.

In BUIR, there are two regularization parameters,  X  and  X  , which determine the weight of H3 and part of H2 (social influence factor captured by users X  initial score) in prediction. Table 4 shows the prediction performance when regularization parameters are set to 0 (to be clear, a  X 0 X  setting nullifies the corresponding factor). As can be seen, when either  X  or  X  is set to 0, BUIR suffers and does not predict well; when both  X  and  X  are zeroed, the performance further decreases. These results provide additional support to vali-date our hypotheses H3 and H2 . As such, we conclude that every factor captured in BUIR  X  H1 , H2 and H3  X  is necessary for high-quality popularity prediction based on user comments.
In this work, we systematically investigate how to best leverage user comments for predicting the popularity of Web 2.0 items. We show that simply applying time series methods on the comment se-ries does not predict well, and that it is important to mine additional signals from comments. To remedy this, we propose three hypothe-ses, that separately accounting for temporal, social influence and current popularity factors. We introduce a new ranking algorithm, Bipartite User-Item Ranking (BUIR), that realizes these hypotheses under a regularization framework. Extensive experiments on three different Web 2.0 media  X  YouTube, Flickr and Last.fm  X  show the effectiveness of our proposed method. Detailed analysis reveals that the factors individually only predict well for some subset of items, while combining all under the proposed BUIR methodology yields the highest quality predictions. Importantly, our proposed solution is general: it is easily extended to incorporate additional factors, and is applicable to ranking items when user comments are available.
 The current work on BUIR ignores the content of the comments. In the future, we will study how to optimally incorporate the con-tent analysis of user comments. We believe the proper modeling the relevance and sentiment of comments towards an item will aid prediction. As evidenced in our dataset, some items with unusu-ally high comment-to-view ratio have shown the need for relevance analysis. Finally, we plan to operationalize our comment-based prediction in real-world Web search applications, such as ranking, contextual advertising and recommender systems.
We prove the convexity of regularization function Q ( f ) in Eq. (9) by showing its Hessian is positive semi-definite.

The second order derivative of Q ( f ) is: Let the matrix A be the ( m + n )  X  ( m + n ) weighted adjacency matrix of the user-item bipartite graph. Then, the Hessian of Q ( f ) H can be written as: where I is the identity matrix, D is a diagonal matrix where each entry D ii is the weighted degree of i -th vertex (can be an item or a user). M is a diagonal matrix that each entry M ii is  X  or  X  , depend-ing on the i -th vertex denotes an item or a user. Note that the matrix ( I  X  D  X  1 2 AD  X  1 2 ) is the normalized Laplacian matrix of the graph. By spectral graph theory [7], the normalized Laplacian matrix of a graph is positive semi-definite. Meanwhile, M is also positive semi-definite because its eigenvalues are all non-negative (eigen-values of a diagonal matrix are its diagonal values). Finally, the addition of these two positive semi-definite matrices is also posi-tive semi-definite, concluding that the Hessian matrix H positive semi-definite.
 We would like to thank the anonymous reviewers for their valu-able comments, and wish to acknowledge the additional discus-sions with Jun-Ping Ng, Aobo Wang, Tao Chen, and Jinyang Gao.
