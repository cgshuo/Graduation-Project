 Nowadays instant messaging (IM) is getting very co mmon in everyday life especially in casual contexts. Studying emotional communication in this channel is still growing. The main focus of this study is how four emotional states (relaxed, angry, happy, sad) influence the type and quantity of emotion -relate d cues used during informal conversations between college friends in IM. Results of the analysis revealed that the happy condition led to more use of nonverbal cues than the other three conditions, including more punctuation, vocal spellings, lexical surro gates, and minus features. Understanding how emotions affect emotional cues users apply in IM has implications for future research on emotion communication via CMC, as well as for the design of the next generation of IM tools that can facilitate communicat ing those emotional cues.
 H.4.3. Communications Applications: Computer conferencing, teleconferencing, and videoconferencing J.4. Social and Behavioral Sciences: Psychology Experimentation, Human Factors Computer -Mediated Communication (CMC), Instant Messaging (IM), Emotional Cues, Lens Model Instant messaging (IM) continues to grow in everyday life. According to data from the Pew Internet and American Life project [13] over one -third of Internet users utilize instant messaging. This channel of communication has especially become popular among teens and college students [15]. Compared to an average Internet user, college students are twice as likely to use instant messaging [12]. Entertainment, affection, inclusion, and sociability are some of the main motivations for students using IM [4, 24].
 Given the absence of visual and aural nonverbal behaviors as the main limitation of instant messaging communication, many theories have been developed on how this medium can influence emotional communication. A growing number of studies also investigated how people express emotions during text -based computer -mediated communication (e.g. [ 10, 11, 28, 31, 32,) . To fill a gap in the text -based CMC theory with respect to emotional communication, the main purpose of this study is to test how people use text -based emotional cues in IM under four distinct emotional states. These results reveal relevant psychological factors to be taken into account for future research and design of text -based CMC systems for personal and professional applications. Early research on text -based CMC compared it to other existing communication modalities and mainly characterized it as task -oriented and impersonal [28]. The lack of visual and aural nonverbal cues was given as the main cause of the impersonality of this type of communication [5, 14]. Acc ording to Mehrabian [16], in everyday communications, only 7% of the people X  X  emotional understanding stemmed from the words spoken, whereas 38% was attributed to verbal tone and 55% was related to facial expression. However, more recent research argues th at text -based CMC can be a highly interpersonal means of communication and users can compose and express their emotions if they have the time and technological affordances.
 Social Information Processing (SIP) theory argues that people are indeed able to e mploy a variety of active and passive strategies to convey and perceive nonverbal behaviors in text -based CMC [31]. Supporting the SIP theory, Walther et al. [32] showed that likable and dislikable partners were identified in text -based CMC as accurately as face-to-face. Hancock et al. [11] suggested that individuals adapt their positive and negative emotion expression to a text-based communication environment through four strategies: degree of agreement, negative affect terms, punctuation, and verbosity. Hancock et al. [10] also showed that people in a negative mood produced fewer words and used more negative terms. Another study added further detail to these findings, by investigating the combined effects of negative and positive emotional states and situ ational stress on emotional communication cues [23]. The results revealed several patterns of expression associated with specific situational conditions, such as participants under stress producing significantly more negative emotion words but fewer vocal spellings (altering spelling to mimic a specific vocal inflection such as weeeell or soooo) than non-stressed participants. Continuing on the trajectory of this prior research, the main goal of this study is to investigate the effect of users X  emotional s tate on different emotional cues they apply in text -based IM. However, this study seeks to address some limitations of the previous studies and extends their findings in several aspects. The first aspect is the emotion elicitation strategy. In previous stu dies [ 11, 32] , participants were asked to act likable or unlikable, and happy or sad. As Hancock et al. [10] explained, the role -playing nature of their emotional expression might have affected their emotional communication and made it unnatural or exagger ated. In the study reported here, short video clips were used to induce four specific moods, followed by open -ended text -based chat accompanied by memory elicitation designed to maintain those moods and encourage participants to express their emotions more naturally during chat. Therefore text -based IM communication was explored by cultivating controlled emotionally -laden situations in which participants were more likely to engage in natural emotional conversation and apply text -based emotional cues to express those emotions. The second innovative aspect of this study is the wider range of moods studied to investigate the emotional cues in text -based IM. Compared to the previous studies [10, 11, 32], which examined a limited range of positive and negative emotions (generally happy or sad), this study investigated a range of four distinct emotions of relaxed, sad, happy, and angry. These are four major emotional states that can be expected to occur frequently in daily life. In Russell X  X  [26] circumplex model of affect, these four different emotions occupy four separate quadrants in a two-dimensional space composed of pleasure -displeasure and degree of activation. Th e third aspect of this study is the large set of text -based emotional cues selected for detection and analysis. A relatively new and novel set of verbal and nonverbal cues were extracted from Boonthanom X  X  [2] study of asynchronous CMC (email) . This set inc ludes emotion words (e.g. happy, angry), vocal spelling (altering spelling to mimic a specific vocal inflection, e.g. weeeell or soooo ), lexical surrogates (textual representations of vocal sounds that are not words, e.g. uh huh, haha), spatial arrays or emoticons (pictographs constructed from punctuation and letters, e.g. : -( for a sad face, or : -D to indicate laughing), manipulation of grammatical markers (alterations of the presentation of words, e.g. all capital letters, strings of periods or commas) , a nd minus features (deliberate or inadvertent neglect of conventional formatting elements, e.g. abbreviation and acronyms, lack of capitalization or paragraphing) . Additional cues were derived from the existing literature on the text -based CMC. Hancock et a l. [11] showed that degree of agreements, verbosity, and punctuation are three strategies participants use to express positive versus negative emotion in text -based CMC. Therefore following categories were also explored in this study: assent (e.g. agree, O K, yes), negation (e.g. no, not, never) , punctuation, and number of words per conversations as an indicator of verbosity.
 Note that, based on Boonthanom X  X  [2] cue taxonomy , vocal spelling, lexical surrogates, spatial arrays/emoticons, punctuation, minus fe atures, and manipulation of grammatical markers are called nonverbal emotional cues since they mimic visual and aural nonverbal behaviors in face -to-face communication, such as facial expression, tone of voice, body gesture, or posture [2,9]. We categorize d positive, negative, and swear words as verbal emotional cues. In their study, Pirzadeh and Pfaff [23] provided empirical support for Brunswik X  X  lens model [3] in text -based CMC, which argues that situational context and personal traits of the encoder c an affect emotional communication. This approach models the process of encoding (expression), transmission, and decoding (impression) of emotional communication. Indeed, all communication, including IM, is embedded in a framework of culture, social rules, situational context, and individual differences. Therefore, given the need for more comprehensive knowledge about emotional communication in synchronous text -based CMC, this study applied the modified version of the lens model to characterize the influenc es of four emotional states (relaxed, angry, happy, sad), on the proportions of different emotion -related cues used during informal IM conversations between friends in controlled emotionally -laden situations . Twenty college students in ten pairs of friends, three males and 17 females, ranging from 18 to 31 years old, received a $10 gift card for their participation in this study. We required partners to be friends who knew each other for at least six months so they were more likely to IM each other in real life, to express their emotions, and engage in natural emotional conversation [ 30, 20, 7 , 24]. Each pair of friends arrived at the laboratory together and were randomly assigned to separate rooms equipped with similar equip ment (computer, table, and chair). After sitting at the computers, they were asked to sign a consent form and answer several demographic questions. Participants were informed that the purpose of this study was to learn about text -based communication, with no mention of emotion. Telling participants the actual goal of the study might have increased the demand effects in which participants might not naturally achieve the desired emotional state and by either resisting or pretending to be in the mood.
 The stu dy used a within-subjects design. The experiment included two phases, repeated for each of the four emotional conditions of relaxed, angry, happy, and sad. The first phase was the mood induction. For each mood induction, a short video clip was selected to elicit the condition mood. Partners were asked to watch the video, followed by a manipulation check survey using the 28 emotion items from Russell [26]. Among different procedures to elicit emotions or induce moods, such as imagination, images, film/story, sound/music, or social interaction, the meta -analysis results by Westermann et al. [33] argued that the film/story mood induction procedure was the most effective procedure when subjects are treated individually.

Figure 1. Modified version of Brunswik X  X  lens model (adapted The relaxed clip was treated as the baseli ne condition and always shown first. To avoid carryover effects, suggested by Rottenberg, Ray, and Gross [25], video clips of the same valence were shown in a blocked order. Therefore, the happy video clip was shown as the third video and anger and sad video clips were shown randomly in second and fourth position.
 The second phase was informal chat via IM . The goal of this phase was to keep part icipants in the emotional state that was elicited using the video clip and eng aging them in a chat conversation likely to include expression of their emotions. A memory elicitation technique was used to reach the goal of this phase. Participants were asked to trigger each other X  X  memory to remember different life experiences they had related to the emotion of the film they just watched, and talk about them for ten minutes via Google Chat before watching the next mo vie. The memory elicitation technique is favored by several researchers since they directly trigger individual experiences of an emotion [17]. Participants were given some sample questions to use for memory elicitation (e.g.  X  X ave you had any experiences s imilar to what you watched in the video? X   X  X hat makes you relaxed/happy/sad/angry? X ). After watching and chatting about all four movies, participants answered a short survey on how satisfied they were expressing their emotions in chat conversation and why . They were asked to report their satisfaction in 7 -point scale of 1 (totally dissatisfied ) to 7 (totally satisfied ). Other questions were also asked such as  X  X hich emotion was the hardest/easiest to express through text -based chat ? Why? X  and  X  X hat differences do you see between expressing your emotion in IM compared to face -to-face? X  At the end, participants were compensated and dismissed. For the manipulation check, participants reported their feelings after watching the video clip on a 5 -point scale of 1 (slightly or not at all) to 5 (extremely), for each of the 28 emotion terms in Russell X  X  circumplex model [26]. Clusters of emotions were created from the four highe st-rated emotions in each condition. This created four roughly orthogonal emotion clusters, one in each quadrant of the two -dimensional circumplex model, in which the horizontal axis represents emotional valence (unpleasant on the left and pleasant on the right) and degree of activation on the vertical axis (Figure 2). Relaxed mood was calculated by taking mean of four emotions of at ease , serene , calm , and relaxed . The same process was done for measuring happy ( happy , glad , pleased , delighted ), angry ( annoyed , frustrated , angry , tense ), and sad ( gloomy , sad , depressed , and miserable ) mood s. All four moods of relaxed, anger, happy, and sad were calculated for each of the four conditions. Verbal emotional cues including positive emotion words, negative emotion words (angry, sad, anxiety), and swear words were counted using the Linguistic Inquiry and Word Count (LIWC) software [29]. LIWC was also used to count the assent and negation words, big words (words &gt; 6 letters), fillers (blah, I mean, you know) , cognitive (e.g. know, think), and perceptual (e.g. see, hear, feel). All nonverbal cues except punctuation (vocal spelling, lexical surrogates, spatial arrays/emoticons, minus features, and g rammatical markers) were counted manually by the researchers. Punctuation was counted using LIWC. Since verbal cues identified by LIWC represent percentages of total words produced in each condition, we calculated nonverbal cues as percentages of total wor ds produced in each category. The mood manipulation was checked first to test whether the levels of each emotion (relaxed, angry, happy, and sad) were significantly higher than the other three for each of the corresponding mood inductions. Since the distribution of moods in none of the conditions was normal, the manipulation check was done by conducting Friedman X  X  ANOVAs for all four emotions in each condition. The results were significant in all conditions at p &lt; .001: relaxed (  X  2 (3) = 43.43), angry (  X  2 (3) = 45.65), happy (  X  = 5 4.71), and sad (  X  2 (3) = 52.14). Results of pairwise comparisons of the means using a Bonferroni correction are in Table 1.
 The results of the pairwise comparisons for each mood among four conditions showed that the desired emotional state was highest comp ared to the other three (noted in bold), though more in terms of valence than activation. Participants reported the negative emotion sad significantly higher than the other three emotions in the sad condition. However, in the remaining conditions, the two positive emotions were significantly different from the two negative emotions in their respective conditions, but the two same -valenced emotions were not significantly different from each other. This suggests that the manipulation was highly successful in terms of emotional valence (positive or negative), Table 1. Mean emotion levels by condition (SD in parentheses). Measure Relaxed Angry Happy Sad
Relaxed 2.82 a
Angry 0.17b 
Happy 1.76a circumplex model, showing valence as the horizontal axis, and but only partially successful in terms of activation, as only the sad condition produced an emotional response significantly less activated than the angry emotion. Alternately, the emotion clusters simply may not have included items high enough in activation to produce a significant difference. This may also be due to an emotional ceiling effect. Participants generally arrive in the laboratory in a positive mood, causing negative mood manipulations to have a much stronger effect than positive mood manipulations. However, even without a neat and mutually exclusive division of the four emotional responses, the four mood conditions successfully produced four clearly distinguishable and appropriate mood profiles , which was the goal of the manipulation. Table 2 shows the results of Friedman repeated -measures ANOVAs for each of the cues categories. There were significant differences among all categories, except big words (words &gt; 6 letters), filler (blah, I mean, you know), emoticons, anxie ty, cognitive, perceptual, and negation words in the four conditions. Follow -up pairwise comparisons were conducted for each condition using a Bonferroni corrected level of significance (.008). Overall, participants were satisfied (average response : 5.9 /7) with emotion expression to their friends via IM communication. The main reason of their satisfaction was reported as comfortable conversations they had with friends that have known them for a long time. For example,  X  X t helps to know the person yo u're chatting with in the first place. If it was with a stranger it would be harder but I found it easy because of the way we communicate already and then we just transferred that to this chat room X   X  X  feel very comfortable talking to (friend X  X  name). It a lmost felt as though we were chatting at home. X   X  X  feel like (friend X  X  name) knows me well enough to understand what tone I am using when I use text -based chat X . More than half of the participants (14/20) reported happiness as the easiest emotion to expre ss, since they can apply different strategies such as emoticons, lol, haha, and punctuations to show their emotions. They also reported that they tend to IM with their friends more when they are happy, so they know how to express happiness and joy to their friends. For example,  X  X t is very easy to joke around, use a  X  X aha X  or  X  X ol X  or emoticons to express happiness while chatting X ,  X  X ECAUSE YOU CAN DO THISSSS! :D HAPPPPINNNNESSSSSS X , or  X  X he use of smiley faces, "lol", and "hahah" helps to easily show joy a nd ease. Also, usually when I talk to (friend X  X  name) I'm happy so I knew how to tell her I was happy X . The rest of participants (6/20) reported anger as the easiest emotion to express in their conversations. They reported emotion words, emoticons, and upp er case letters as the main cues that make it easy for them to express anger . For example,  X  X t's very easy to portray when you're angry and type angry, using different words, visual cues and using all caps. X  The answers to the most difficult emotion to ex press, however, were more diverse. Nine out of twenty participants reported sadness as the hardest emotion to express. Lack of emoticons was one of the reasons they reported. They also reported that sadness is a deep emotion and they usually communicate it through subtle facial cues such as eye expression, which are missing in IM.  X  X or me, sadness is displayed in my face and my eyes. When I'm sad, I don't want to talk about it. So when the only way of communicating my sadness is through message, that make s it difficult to do. X ,  X  X adness is a very personal emotion so you have to be around people to feel it X , or  X  X  think sadness is the hardest because, the only way (friend X  X  name) would know I was sad is if I put a sad face ( :( : \ ) X . Seven out of twenty pa rticipants reported contentment/relaxed as the hardest emotion to express, since they were in a neutral state and had no extreme emotion to express. The rest of participants, four out of twenty, reported anger as a hard emotion to express. Some of their an swers were:  X  X hen it comes to deeper emotions like sadness or anger I tend to use a lot of gestures, facial expressions and I seek them in the respondent. Not being able to utilize those aspects of conversation was frustrating X ,  X  X  like to focus on why I'm angry before expressing my anger X ,  X  or  X  X  like to vent anger physically, through sports and lifting weights rather than emotionally X .
 Condition Cue Categories Relaxed Angry Happy Sad  X  2 (3) Verbal Emotional Cues Affect words 8.66ab (2.68) 8.01a (2.93) 11.22b (3.02) 8.78ab (2.27) 9.60* Positive words 7.46ab (2.17) 4.01a (1.77) 9.49b (2.71) 3.98a (1.91) 27.42*** Negative words 1.15a (0.83) 4.00b (1.99) 1.70a (0.89) 4.80b (1.47) 45.72*** Anger words 0.30a (0.44) 2.36b (1.50) 0.29a (0.33) 1.22b (1.02) 30.02*** Sad words 0.30a (0.59) 0.42a (1.12) 0.37a (0.36) 2.64b (1.49) 41.44*** Swear words 0.00a (0.00) 0.44b (0.49) 0.15ab (0.26) .50b (0.54) 19.04*** Nonverbal Emotional Cues Vocal Spellings 1.24ab (1.37) 1.04a (0.93) 2.23b (1.94) 0.67a (1.03) 18.28*** Lexical Surrogates 2.34ab (1.88) 2.09ab (1.71) 2.95a (1.86) 1.38b (1.75) 9.09* Minus Features 1.92ab (2.40) 1.26ab (1.43) 2.05a (2.83) 0.76b (0.89) 8.08* Punctuation 16.64ab (9.14) 13.61a (7.84) 21.27b (11.30) 13.11a (8.82) 11.59** Grammatical Markers 0.38a (0.66) 0.42a (0.82) 1.32b (1.30) 0.65ab (1.01) 17.19** Other verbal cues Assent words 3.06ab (1.66) 2.78ab (1.61) 4.65a (2.67) 2.28b (1.38) 10.18* Overall, the results of this study provide empirical support for Brunswik X  X  lens model [3] in synchronous text -based CMC, demonstrating that emotional state of encoder can affect emotional communication and the usage of text -based cues in IM. The first interesting point was the number of words per conversation and wor ds per minute participants used among four conditions. Similar to Hancock [11] participants in all four conditions produced words at approximately the same rate. However, inconsistent with their results that showed participants in negative mood used fewer words compared to a positive mood, we found no significant differences in the number of words per conversation among four conditions. A possible explanation could be the substantially different informal context of our experiment and using participants that knew each other for a quite some time that could talk about positive and negative emotions. Since the number of words per conversation was consistent across conditions, this increased our interest in exploring the relative proportions of the number of cue s that were used in different conditions. As expected, in the happy condition, participants used more positive emotion words compare to the other three conditions. Similarly, in the angry condition participants used more angry words compared to the other three conditions, and likewise for the sad mood. Of course, this is partially influenced by the topics of their conversations in those conditions, which were life experiences specifically portraying events that would be described with those words. These results demonstrated that memory elicitation in the second phase of the experiment could maintain users in the emotional states we required. Consistent with the Hancock [11] study, participants used more punctua tion in the happy condition than the sad and angry conditions. Participants also used a significantly higher number of vocal spellings (e.g. sooo, weeell) in the happy condition compared to the sad and angry conditions. This result from informal chat align s with a previous task -based study [23] in which participants used less vocal spelling during time -pressured conditions that were designed to frustrate participants through frequent task switching. The proportion of manipulations of grammatical markers (al terations of the presentation of words, e.g. all capital letters, strings of periods or commas) were higher in the happy condition than the angry and relaxed condition, but not the sad condition. The proportion of lexical surrogates (e.g. uh huh, haha) and minus features (abbreviations and acronyms) were used significantly higher in happy than sad condition. Cue categories such as punctuation, vocal spelling, lexical surrogates, and manipulation of grammatical markers are noteworthy in how they attempt to mimic real speech [9]. Participants seem to apply these types of emotional cues to adapt the prosody of face -to-face communication to text -based CMC, especially when they are in a happy mood more than the other three moods of relaxed, angry, and sad. As su ch, these results extend to CMC the face-to-face findings of Scherer [27] and Ekman [8], which showed prosody (e.g. tone of voice, frequency, pitch) is one of the main cues to emotional expression and is exhibited differently across various emotions. These results may have implications for the automatic detection of positive mood, as these groups of emotional cues are relatively easy to capture in real time during a chat conversation.
 Another interesting result is the usage of emoticons. Emoticons were defi ned as symbols that resemble facial expression and body movements and are used very often especially in instant messaging [7]. Derks et al. [6] showed that participants used more emoticons in positive than negative emotional states. The results of this stu dy, however, showed no significant difference in the number of emoticons participants used in four conditions. One possible explanation of this result may be the diversity of emoticons (both positive and negative) we had in our experiment, since in their s tudy [6] a limited number of emoticons for negative emotional expression was provided. Overall, the happy condition led to more use of nonverbal cues than the two negatively -valenced conditions of sad and angry. The manipulation check after watching the v ideo clips confirmed that participants were in the desired mood at the beginning of the chat conversation and it persisted throughout each chat session as shown by the use of significantly different numbers of positive, angry, and sad words. Yet the main nonverbal manifestation of the negative moods (angry and sad) in the chat logs was simply a reduction in the number of nonverbal cues used compared to the happy condition. It suggests that a happy mood promotes an overall increase in nonverbal emotional exp ressivity in IM, which could be a possible explanation of why most of the participants reported happiness as the easiest emotion to express. This increase in nonverbal emotional expressivity has three potential explanations. The first is that IM, as it is currently implemented in common desktop and web-based applications such as Google Chat, may not support negative expressivity sufficiently. It may be that letters, numbers, punctuation, and even emoticons are ill -suited to express negative emotions. This explanation is in line with the survey results, in which some of the participants reported that sadness and anger are difficult to express in IM because of the lack of cues to communicate facial expression in this medium. This explanation also put SIP theo ry in a new perspective with respect to the negative emotions, since the results showed that users were not able to find any strategies to convey nonverbal behaviors to express their negative emotions. The second explanation is that participants expressed negative emotions using cues not among those captured in this analysis. Lastly, the third explanation is that perhaps sad and angry partners chatting informally turn their focus inward and become less expressive overall. This explanation is in line with th e survey results, in which some participants reported they want to focus more on the reason that made them angry before expressing their anger. This is also in line with the results of Pfaff X  X  [22] study showing an increased inward focus of participants i n a negative mood in the NeoCITIES simulation, a six -person team decision -making task. This behavior manifested as reduced attentiveness to their partners. Participants used a significantly higher number of assent words (e.g. agree, OK, yes) in the happy condition than the sad condition. Hancock [11] also showed the degree of agreement as one of the main strategies that participants used to express their positive emotion compared to negative emotions in CMC. However, their data showed that it is the frequency of negation words (e.g. not, no, never), rather than the frequency of assent words, which differentiates positive emotion from negative. Our data showed no significant difference in the frequency of negation words in different conditions. A possible explanation for not having significant difference among negation words can be explained by the conversation context of this study. Participants mainly talked about their personal experiences and they might have not had many opportunities to expr ess disagreement, compared to the task -based context in Hancock [11] study. This study specifically investigated how different emotional states influence cues individuals apply in instant messaging. The results of this study prov ide empirical support for Brunswik X  X  lens model [3] in synchronous text -based CMC, demonstrating that emotional state of encoder can affect emotional communication in IM. This model can be used as a framework for future studies to explore how unique patter ns of emotional cues in addition to other factors (such as personal traits, culture, or social relationships) can characterize different emotional states in IM. Understanding the usage patterns of nonverbal emotional cues has implications for future research on emotion communication via CMC, as well as for the design of the next generation of IM tools that can facilitate a wider range of emotional expression. Results of this study also contribute toward sentiment analysis and automatic extraction of opinions and emotions from text. Detection of emotional cues applied in text -based CMC can inform different models which are employed in text analysis [1,19]. The rela tively small sample size (ten couples) was one limitation of this study, though the within -subjects design across four conditions produced forty conversations total, an ample amount of data. Significant results found in this sample shows that this topic me rits further consideration and future studies need to re -examine these findings with chat data from a bigger sample. The higher number of female compared to male participants is another limitation of this study. Although Hancock et al. [10,11] found no gender affect on emotion expression and detection in instant messaging, future studies should reexamine the results of this study in a more diverse participant pool (in terms of gender, age, and ethnicity) than this overwhelmingly young female sample.
 The ma in goal of this study was, through a quantitative approach, to see whether pre -defined cues, such as those provided by Boonthanom [2], would appear in different proportions under different emotional conditions. The advantage of the quantitative approach was the speed and efficiency to mine large data sets, though they only capture content, not context. Therefore studies are planned that take a qualitative approach and apply conversation analysis to preserve the complexity and richness of the content. These studies will focus more intently on the emotional cues participants used and the reasons behind using those cues in different emotional states and specifically explore why IM supports happy emotion more richly than negative emotions of sadness and anger. F urther analysis of the content of statements can also provide a strong research foundation for designers to develop solutions to support different emotional cues in emotional communication in IM. Overcoming the limitations of prior work on emotion expressi on in text -based CMC [11, 32] that used role -playing to make users express their emotions, this study explored the topic by cultivating emotionally-laden situations, in which participants were more likely to engage in natural emotional conversation, by usi ng video clips and memory elicitation techniques. Future studies need to apply other creative techniques to increase the emotion expression in natural conversation among friends. An alternate approach is to acquire a large natural corpus of IM data and use content analysis to classify conversations into appropriate emotional categories to then be analyzed for verbal and nonverbal cues.
 Continuing with the need to better understand IM users, prior work demonstrates the importance of considering individual personality traits when assessing communication behaviors [34]. Extraversion [27] and Emotional Intelligence [21] are the most likely factors to explain some of the variation between individuals in their use of verbal and non -verbal cues to express emotion i n IM, and should be considered in future research in this area. Future studies also need to consider that emotional or social cueing (verbal and nonverbal) is not the only mechanism that connects people together in text -based CMC. Whittaker [34] explains different cognitive cues such as turn taking, availability, shared attention, and interactivity are additional types of cues that facilitate expressive communication. Nardi [18] goes beyond cognitive and social cueing to discuss social connection and differ ent categories of activities for social bonding (affinity, commitment, and attention) that need to be considered in any type of communication, including text -based CMC in the absence of traditional nonverbal cues. Longer -term goals are to study text-based CMC beyond emotional expression to explore different categories of cues and activities that individuals use to connect and develop social bonds. Applying the knowledge of the different cues individuals apply in text -based CMC with a participatory design pr ocess will help to develop different technological strategies that facilitate effective and transparent emotion expression in text -based CMC. [1] Alm, C. O., Roth, D., and Sproat, R. 2005 . Emotions from [2] Boonthanom, R. 2004. Computer -mediated communication [3] Brunswik, E. 1956. Perception and the Representative [4] Chung, D., and Nam C. 2007 . An analysis of the variables [5] Culnan, M. J. and M. L. Markus 1987. Information [6] Derks, D., Bos, A. R., and von Grumbkow, J. 2008. [7] Derks, D., Fischer, A. H., and Bos, A. E. R. 2008. The role [8] Ekman, P. 1982. Emotion in the human face. Cambridge [9] Hancock, J.T. 2004. Verbal irony use in computer -mediated [10] Hancock, J. T., Gee, K., Ciaccio, K., and Lin, J. M. H. 2008. [11] Hancock, J. T., Landrigan, C., and Silver, C. 2007. [12] Jones, S. 2002. The Internet goes to college: How student s [13] Jones, S., and Fox, S. 2009. Generations online in 2009. Pew [14] Kiesler, S., Siegel, J., and McGuire, T. 1984. Social [15] Leung, L . 2001. College student motives for chatting on [16] Mehrabian, A. 1972. Nonverbal commmunication . Chicago: [17] Morris, W. N. 1989. Mood: The Frame of Mind. New York: [18] Nardi, B. 2005. Beyond bandwidth: Dimensions of [19] Pang, B., and Lee, L. 2008. Opinion Mining and Sentiment [20] Parkinson B, Fischer AH, and Manst ead ASR. 2005. [21] Petrides, K., Pita, R., &amp; Kokkinaki, F. 2007. The location of [22] Pfaff, M. S. 2012. Negative affect reduces team awareness: [23] Pirzadeh, A., and Pfaff, M. S. 2012. Emotion expression [24] Ramirez, A., and Broneck, K. 2009 .  X  X M me X : Instant [25] Rottenberg, J., and Ray, R. D., &amp; Gross, J. J. 2007. Emotion [26] Russell, J. A. 1980. A circumplex model of affect. Journal of [27] Scherer, K.R. 1978. Personality inference from voice quality: [28] Short, J, Williams E, and Christie, B. 1976. The social [29] Tausczik, Y. R., and Pennebaker , J. W. 2010. The [30] Wagner, H. L., and Smith, J. 1991. Facial expression in the [31] Walther, J.B. 1992. Interpersonal effects in computer -[32] Walther , J.B., Loh, T., and Granka, L. 2005. Let me count [33] Westermann R, Spies K, Stahl G, Hesse FW. 1996. Relative [34] Whittaker, S. 2003. Theories and Methods in Mediated 
