 With the actual huge and continuously growing of World Wide Web, the amount of information in the public domain grows explosively. As a result, there is a vast demand for new technologies that can eff ectively process information. Document summarization is an essential technology to overcome this obstacle in technolog-ical environments [1].

The main motivation of document summarization is to help users capture the major topics of a document with les s effort [2]. Different summarization tasks make the query-oriented summary different from the generic summary [3]. Besides, document summarization can be categorized as abstract-based and extract-based summaries [1]. In this pa per, we aim to gener ate extract-based generic summary, i.e., to select a combination of sentences which are the most important for the overall understanding of the document[4].

Many algorithms, supervised and unsupervised, have been applied for doc-ument summarization. Jones [5] gives a review of the research on automatic summarization over the last decade. Based on our study, we argue that a good summary should be compact while cove r as many aspects in the document as possible. We propose a feature called entropy to measure the coverage from the inner-sentence level, and another featur e called relevance to indicate the com-pactness from the intra-sentence level:
Entropy  X  X his feature denotes the quantity of information implied by the sentence. As you may notice, long senten ces are likely to cover more aspects in the document than short sentences. Note that a long sentence usually has a comparably larger entropy than a short sentence. Hence, a large entropy of sentence possibly implies a large converge.

Relevance  X  X his feature measures the intra-sentence relationships between sentences. On the whole, sentences sharin g a considerable number of words with other sentences often have high relevances . Therefore, choosing sentences corre-lated with each other probably leads to a compact summary.

As you may notice, leveraging the above two features may generate summaries that prefer long sentences which are stro ngly related to other sentences in the document. We aim to balance the importance within the sentence and between the sentences to generate compact su mmaries covering as many aspects of the document as possible.

In order to testify the effectiveness o f leveraging the two features, or say heuristic rules[6], we firstly score sentences on the strength of these two features and perform a simple sentence selection m ethod for unsupervised summarization. Furthermore, we combine the two feature s with other features [2,7] extracted from sentences and apply regression meth ods for supervised summarization.
We evaluate the performance of our methods from DUC01(http://duc.nist.gov) on an open benchmark data set. Experiments show that leveraging the two pro-posed heuristic rules are contributive to summary generation from both the un-supervised and the supervised perspective.

The rest of the paper is organized as fo llows: Section 2 introduces the prelim-inary knowledge and gives the problem de finition, Section 3 pr esents the details of the algorithms, Section 4 gives experimental results and comparisons of our methods with baseline methods, Section 5 describes the related work, and we conclude our paper in Section 6. 2.1 Feature Extraction Since sentences could be viewed as a vector of words, in order to analyze the significance within the sentence quantitatively, we adopt entropy as a metric: where x i denotes the i -th sentence, I ( x i ) denotes the amount of information covered by the i -th sentence, p ij denotes the probability of the j -thwordinthe i -th sentence. From the above formula, we can see that a longer sentence is more likely to have a larger entropy. Therefor e, a sentence of large entropy is very likely to cover more aspects of the document than a sentence of small entropy.
Moreover, we extract another feature ca lled relevance to show the compactness by calculating the relationship of a s entence between other sentences: between the j -th sentence and the i -th sentence based on directed backward graph according to Mihalcea X  X  experiments[8], Overlap ( x j ,x i ) denotes the num-ber of words co-occurring in the j -th and the i -th sentence, and length ( x i )is the length of the i -th sentence. Hence, sentences with high relevance probably compose more compact summari es than sentences without.

Besides, for regression methods, we ex tract entropy and relevance together with other typical features from the document as [2,7] in Table 1. 2.2 Problem Formulation the i -th sentence, and X denotes the space of all the documents.

For a single document, our target is to extract the most representative y = { s 1 ,s 2 , ..., s k summary.
 Unsupervised Summarization. As discussed in Section 2.1, a large I ( s i ) of sentence probably suggests a large coverage, and large R ( s i )s indicate that sentences are relevant which probably le ads to compact summaries. Therefore, in order to generate compact summaries with a large coverage, we aim to maximize the following objective function: where S  X  denotes the summary. Take a deep look at Equation (3), substitute the I ( s i )and R ( s i ) with Equation (1) and (2), we have: where x m is the m -th sentence in the document. After removing stopwords, the number of words in sentences is small, while the effect of R ( s i ) grows exponen-tially as the number of sentences increase s. Therefore, if we use relevance in its balance, we adjust our objective function as follows: Supervised Summarization. Given a document x = { x 1 ,x 2 , ..., x N } X  X  ,for is in the summary. Now assume we have all the training data preprocessed as: regression problem, y i denotes the ROUGE-2-P score of the i -th sentence, which will be described in Section 4.2. We aim to construct a discriminant function such that: We attempt to find the  X  T with the least generalization error through the train-ing process. The predicted summary y would be extracted according to the following equation: The sentences with the highest values of y will constitute the final summary. 3.1 Entropy-and-Relevance-Based Summarization As we have given the objective function in Section 2.2, in order to select the most important sentence in the sense of entropy and relevance. As we can see, to maximize the objective function is to find the sentences with highest I ( s i )+ (  X / X  )  X  log( R ( s i )).
 Algorithm 1. Entropy and Relevance based Summarization
For a better balance between entropy and relevance, we adopt different  X  and  X  to see the effect of leveraging the two features. Specifically, for simplicity, we consider the parameter tuning to be 1 and  X / X  . Experimental results will be giveninSection4.
 3.2 Regression-Based Summarization From the supervised perspective, we adop t the linear regression [9] and Extreme Learning Machine (ELM) [10] regression to leverage the entropy and relevance. For linear regression, we simply apply the following model:
Given the whole document space X = { x 1 ,x 2 , ..., x N } , the feature vector x We use the minimum squared errors to estimate  X  , therefore, we have: In Equation (10), to minimize the RSS with respect to  X  , Assume X is full column rank, let  X  X SS  X  X  = 0, we can obtain a solution of  X  On the other hand, the ELM regression differs from linear regression in the way how  X  is obtained. For the single hidden l ayer feedforward networks (SLFNs) with M hidden neurons, the problem could be modeled as follows: data with zero means. Therefore, for all the N training sample, the goal is to minimize the cost function: Under the ELM model, Huang et al. [10] formularized the problem as:
H ( w 1 , ..., w M ,b 1 , ..., b M ,x 1 , ..., x N )= According to Equation (15), through the use of the Moore  X  Penrose generalized inverse, a solution of  X  is [11]: where, H  X  is the Moore  X  Penrose generalized inverse of H . Detail steps of de-duction could be found in [10] and [11].

For both kinds of regression, the predictive summary of x could be calculated through: For each document, we select the sentences with the highest values of y  X  into the final summary. As we have mentioned, the goal of using regression methods is to learn how to balance different features through the training procedure. Effectiveness of the leveraging is shown in the next section. 4.1 Experimental Setting Experimental Data: The DUC(Document Understanding Conference) 2001 data set is used to evaluate the effectiv eness of leveraging the two features. The whole data set includes 147 documents of 6921 sentences together with the corresponding ground-truth summaries.
 Baseline Methods: For unsupervised methods, we implement the Luhn X  X  method, RANDOM[7], LSA and HITS to compare with our entropy and rel-evance based summarization. For supe rvised methods, the Linear Regression and ELM regression are applied based on different feature sets for evaluation. Evaluation Metric: The ROUGE evaluation toolkit [12] adopted by DUC for automatic summarization evaluation is highly correlated with human evalua-tions. In this paper, we employ this toolkit to evaluate the performance of our proposed methods. There are several ki nds of ROUGE metrics, here we introduce the most commonly used sub-metrics as follows: 1. ROUGE-N-R is the recall rate of summary from the n  X  gram point of view. It can be calculated as follows [12]: 2. ROUGE-N-P is the precision rate summary from the n  X  gram point of view. It can be calculated as follows : 3. ROUGE-N-F is the F 1 metric of ROUGE-N-R and ROUGE-N-P and could be calculated as follows: In the above equations, N is the length of words in n-gram, s is the sentence in summary, y  X  denotes the generated summary and y is the ground  X  truth summary. Count match ( gram n ) is the number of n-gram co-occurring between y  X  and y . Count ( gram corresponding summary. 4.2 Performance Evaluation Results of Unsupervised Methods: We implement several unsupervised methods for summarization to compare with our ERBS summarization. The results are shown in Table 2.

From Table 2, we can find that ERBS achieves the best performance among the unsupervised methods and gains a significant improvement over all baseline methods. This shows the effectiveness of t he proposed heuristic rules X  entropy and relevance. Moreover, we give the de tail results of ERBS with different pa-rameters in Fig. 1.

From the left figure in Fig. 1, we can see that ROUGE-scores changes with different gama, where gama=  X / X  . From the figure on the right, we can see the best result is obtained when gama=1.2, as is displayed in Table 2. When gama=0, the ERBS just selects sentences with the highest entropy, as gama grows, the ERBS pays more and more attention to relevance. The result of ERBS converges to selecting sentences with the highest relevance when gama grows infinite. As we can see from the figures, leveraging entropy and relevance achieves better performance than emphasizing one single heuristic rule.
 Results of Supervised Methods. To further investigate the effect of en-tropy and relevance, we also listed the compared results of LR, LR+E, LR+R, LR+E+R, ELM, ELM+E, ELM+R and ELM+E+R in Table 3. LR denotes Linear-Regression dealing with features excluding entropy and relevance, while LR+E includes entropy, LR+R includes relevance and LR+E+R includes all the features. The methods based on ELM-regression are named similarly.
For a better comparison, we also employ the weighted longest common sub-sequence (ROUGE-W). And the weight parameter is set to be 1.2 in the exper-iments. Comparing the LR+E+R with LR, LR+E and LR+R, we can see that LR+E+R performs the best. And it is the same with the ELM. This suggests that leveraging the two features enhances the regression based summarization. Luhn [13] firstly proposed automatic summarization and addressed the signifi-cance of sentences for sentence extraction in the 1960s. Moreover, LSA (Latent Semantic Analysis) was applied to identify semantically important sentences for summarization based on the Singular Vector Decomposition(SVD) of matrix[14] one decade ago. Recently, Lee et al. [1] proposed an unsupervised document summarization method using the Non-negative Matrix Factorization (NMF).
Also, ranking algorithms such as HITS and PageRank could be applied for summarization since documents could be r epresented as graphs [6]. Mihalcea [8] implemented HITS based on undirected graph, forward directed graph and back-ward directed graph. Among all the graphs, the method based on the backward directed graph performs best.

Meanwhile, supervised algorithms have also been applied for document sum-marization. Generally, these methods fi rst extract a set of features from the document, and then train a summarizer to predict whether a sentence should be selected into the summary [6]. Such features include linguistic features and sta-the document, the sentence length, and so on [2,16]. Some complex features are included in order to improve the performance of supervised algorithms such as, the LSA score of the sentence, the HITS score of the sentence [3], the PageRank score of the sentence [7], and so on.

Learning methods such as Naive Bayes (NB), C4.5 [16], Logistic Regression (LR), and Neural Network (NN) could be applied to train summarizers based on features extracted from sentences [2]. Rijsbergen et al. [17] introduces an SVM-based method aiming at constructing a decision boundary between summary and non-summary sentences. Moreover, HMM( Hidden Markov Models) was proposed by Conroy and O X  X eary [3] in 2001 based on three extracted features.
However, HMM could not fully exploit linguistic features of sentences since its independence assumption. Shen et al. [2] proposed Conditional Random Fields(CRF) for summarization, which considers the summa rization task as a sequence labeling problem. In order to e nhance diversity, coverage and balance for summarization, Li et al. [7] proposed another supervised method through a structure learning framework.

For supervised methods, whether the extracted features indicate the useful in-formation for summarization strongly affects the quality of summaries. Moreover, the capability of the adopted method also influences the quality of summarization. To generate compact summaries with possibly large coverage, we extract entropy and relevance from sentences for summari zation. Then we perform unsupervised summarization named ERBS and supervised summarization utilizing Linear Re-gression and ELM regression in order to validate the effectiveness of leveraging the two features. Experimental results show that the ERBS outperforms other baseline unsupervised methods. Moreover, the results of linear regression and ELM regression based summarization also indicate that leveraging the two fea-tures is beneficial for aut omatic summarization.
 This work is supported by the National Science Foundation of China (No.60675010, 60933004, 60975039), 863 National High-Tech Program (No.2007AA01Z132), National Basic Research Priorities Programme (No.2007CB311004) and National Science and Technology Support Plan (No.2006BAC08B06).
