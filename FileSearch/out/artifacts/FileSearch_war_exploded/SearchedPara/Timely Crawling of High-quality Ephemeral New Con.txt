 In this paper, we study the problem of timely finding and crawling of ephemeral new pages, i.e., for which user traffic grows really quickly right after they appear, but lasts only for several days (e.g., news, blog and forum posts). Tradi-tional crawling policies do not give any particular priority to such pages and may thus crawl them not quickly enough, and even crawl already obsolete content. We thus propose a new metric, well thought out for this task, which takes into account the decrease of user interest for ephemeral pages over time.

We show that most ephemeral new pages can be found at a relatively small set of content sources and suggest a method for finding such a set. Our idea is to periodically recrawl content sources and crawl newly created pages linked from them, focusing on high-quality (in terms of user interest) content. One of the main difficulties here is to divide re-sources between these two activities in an efficient way. We find the adaptive balance between crawls and recrawls by maximizing the proposed metric. Further, we incorporate search engine click logs to give our crawler an insight about the current user demands. The effectiveness of our approach is finally demonstrated experimentally on real-world data. Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval Keywords : Crawling; ephemeral pages; content sources
User traffic to many of the newly created Web pages (e.g. news, blog and forum posts) grows really quickly right after they appear, but lasts only for a few days. It was discussed in several papers that the popularity of news posts decreases exponentially with time [7, 9]. This observation naturally leads to distinguishing two types of new pages appearing on Figure 1: Typical user interest patterns for ephemeral and non-ephemeral new pages the Web: ephemeral and non-ephemeral pages. We clus-tered user interest patterns of some new pages discovered in one week (see Section 3 for details), and Figure 1 shows the centroids of the obtained clusters. The cost of the time delay between the appearance of such ephemeral new pages and their crawl is thus very high in terms of search engine user satisfaction. Moreover, if a crawler fails to find such a page during its period of peak interest, then there might be no need to crawl it at all.

It was reported in [5], that 1-2% of user queries are ex-tremely recency sensitive, while even more are also recency sensitive to some extent. The problem of timely finding and crawling ephemeral new pages is thus important, but, to the best of our knowledge, is not studied in the literature. Indeed, different metrics were suggested to measure the cov-erage and freshness of the crawled corpus [2, 3, 10], but they do not take into account the degradation of the profit to a search engine contributed by these pages. Crawling policies based on such metrics may then crawl such new pages not quickly enough, and even crawl already obsolete content. Thus, we propose a new quality metric, well thought out for this task, and a crawling algorithm optimized to maximize this metric over time, that takes into account this degrada-tion of pages X  utility.

Our daily experience of using the Web also suggests that such ephemeral new pages can be found from a relatively
The authors are given in alphabetical order
The extended version of this paper can be found at http: //arxiv.org/abs/1307.6080 small set of  X  X ubs X  or content sources . Examples of content sources are main pages of blogs, news sites, category pages of such news sites, RSS feeds, sitemaps. One needs to pe-riodically recrawl such sources in order to find and crawl ephemeral new pages way before their peak of user interest. However, frequent recrawling of all these sources and all new pages found on them requires a huge amount of resources and is quite inefficient. In order to solve this problem effi-ciently, we analyze the problem of dividing limited resources between different tasks (coined as holistic crawl ordering by Oslton and Najork in [10]), i.e., here between the task of crawling ephemeral new pages and the task of recrawling content sources in order to discover those new pages.
In this paper, we propose a new algorithm that dynami-cally estimates, for each content source, the rate of new links appearance in order to find and crawl newly created pages as they appear. As a matter of fact, it is next to impossible to crawl all these new pages immediately due to resource constraints, therefore, a reasonable crawling policy has to crawl the highest quality pages in priority.

The quality of a page can be measured in different ways, and it can, for example, be based on the link structure of the Web graph (e.g., in-degree [8] or PageRank [4, 1]), or on some external signals (e.g., query log [6, 11, 12] or the number of times a page was shown in the results of a search engine [11]). We propose to use the number of clicks for this purpose in order to estimate the quality of pages, and predict the quality of newly created pages by using the quality of pages previously linked from each content source. In this way, we are able, in fact, to incorporate user feedback into the process of crawling for our algorithm to find and crawl the best new pages.

Note that we review related work in the extended version of this paper.
As we discussed in the introduction, we deal with pages for which user interest grows within hours after they appear, but lasts only for several days. The profit of crawling such ephemeral new pages thus decreases dramatically with time.
Assume that for each page i , we know a decreasing func-tion P i ( X  t ), which is the profit of crawling this page with delay  X  t seconds after its creation time t i (by profit, one can mean the expected number of clicks or shows on SERP). If, finally, each page i was crawled with a delay  X  t i define the dynamic quality of a crawler as: In other words, the dynamic quality is the average profit gained by a crawler per second in a time window of size T .
The dynamic quality defined above can be useful to un-derstand the influence of daily and weekly trends on the per-formance of a crawler. Let us now define the overall quality of a crawler, which allows to easily compare different algo-rithms over larger time windows. It is natural to expect that if T is large enough then the influence of season and weekly trends of user interest will be reduced. In other words, the function Q T ( t ) tends to a constant while T increases. Thus, we can consider the overall quality : Figure 2: Average cumulative number of clicks depending on page X  X  age which does not depend on t and T .

In this paper, by P i ( X  t ) profit of crawling a page i at time t +  X  t , we mean the total number of clicks this page will get on a SERP after this time (ignoring any indexing delay). In this way, we can approximate the relevance of a page to the current interests of users. From a crawler X  X  perspective, it is thus an appropriate measure of the contribution of new pages to a search engine performance (given a specific rank-ing method). Note, that this metric can only be evaluated with some delay because we need to wait until the crawled pages are not shown anymore to take their profit, i.e., their number of clicks, into account. We are going to use this metric to validate our algorithm in Section 5.

However, we do not know the function P i ( X  t ) for pages that just appeared, but we can try to predict it. It is nat-ural to expect that pages of similar nature exhibit similar distributions of user interest. In order to demonstrate this, on Figure 2 we plot the average number of cumulative clicks depending on the page X  X  age for all pages published on a news site and a blog (both being randomly chosen) over 1 week. We can see that almost all clicks appear in the first week of a page X  X  life, and that the dependency of the cu-mulative number of clicks gathered by a page on the page X  X  age is pretty well described by the function: P 1  X  e  X   X   X   X  t where P is the total number of clicks a page gathers during its life. We thus propose the following approximation of the profit P i ( X  t ) (i.e., the number of future clicks): where the rate of decay  X  i and the profit P i are content-source-specific and should be estimated using historical data (see Section 4.2 for details). We use this approximation in Section 4 in order to analyze the problem under considera-tion theoretically.
Our hypothesis is that one can find most of ephemeral new pages appearing on the Web at a small set of content sources, but links from these sources to new pages are short living so a crawler needs to frequently recrawl these sources to avoid missing links to new pages.

We used the toolbar logs continuously collected at Yan-dex (Russia X  X  most popular search engine) to monitor user visits to Web pages. In this way, we can easily track the appearance of new pages that are of interest to at least one user, know which content sources referred them, and also follow the evolution of user interest over time for each of these new pages. This data is a representative sample of the Web as this toolbar is used by millions of people across different countries.

Using this toolbar data, we randomly sampled 50K pages from the set of new pages that appeared over a period of one week and were visited by at least one user. For each page, we computed its number of daily visits for a period of two weeks after it appeared in the logs. Then, using this 14-dimensional (one per day) feature vector (scaled to have its maximum value equal to 1), we clustered these pages into 6 clusters by applying the K-means method 3 . Let us note that when we tried less clusters, non-ephemeral pages were not assigned to one cluster. Finally, we obtained only  X  non-ephemeral pages. The percentage of new pages that are ephemeral (and were visited at least once) for this week is thus 96%, which is really significant. Centroids of these clusters are plotted on Figure 3 (in Section 1 we showed only two of them).

We also extracted all links (users X  transitions between pages) found in the logs pointing to one of the ephemeral new pages and obtained  X  750K links. Using these links, we studied the percentage of ephemeral new pages reached depending on the number of content sources. We obtained that only 3K content sources are required to cover 80% of new content, which validates our hypothesis about content sources (see the extended version of this paper for details). 42% of these 3K content sources are main pages of web sites, while 44% are category pages, which can be accessed from the main page. So, overall, 86% of them are at most 1 hop away from the main page.

Motivated by these results we propose to regard the main page of each host of interest and all pages linked from it (i.e., all pages 1 hop away from the main page) as content sources. This simple method fits our usage scenario considering that our crawling algorithm (described in the next section) fo-cuses on high-quality ephemeral new content. Therefore, even if content sources that produce low quality content or almost no new content at all are made available to this al-gorithm, they will almost never be crawled or crawled just in case much later, when the crawler is least loaded. See the extended version of this paper for details. http://en.wikipedia.org/wiki/K-means_clustering
In this section, we assume that we are given a relatively small set of content sources, which regularly generate new content (see Section 3 for the method for finding such a set). Our current aims are to (1) find an optimal schedule to recrawl content sources in order to quickly discover high-quality ephemeral new pages, and (2) understand how to spread resources between the tasks of crawling new pages and recrawling content sources.
Assume that we are given a set of content sources S 1 ,...,S and let  X  i be the rate of new links appearance on the source S , i.e., the average number of links to new pages, which appear in one second.
 Let us consider an algorithm, which recrawls each source S i every I i seconds, discovers links to new pages, and also crawls all new pages found. We want to find a schedule for recrawling content sources, which maximizes the overall quality Q (see Equation (2)), i.e., our aim is to find optimal values of I i . Suppose that our infrastructure allows us to crawl N pages per second ( N can be non-integer). Due to these resource constraints, we have the following restriction: On average, the number of new pages linked from a source S is equal to  X  i I i , therefore every I i seconds we have to crawl 1 +  X  i I i pages (the source itself and all new pages found). We want to maximize the overall quality (see Equation (2)), i.e.,
Note that this expression is exactly the average profit per second. Content sources may not be equal in quality, i.e., some content sources may provide users with better content than others. We now assume that, on average, the pages from one content source exhibit the same behavior of the profit decay function and hence substitute P j ( X  t j ) by the the profit P i and the rate of profit decay  X  i as the parameters of each content source S i . Thus, we obtain: here p i = P i we can assume that p 1  X  ...  X  p n . We now want to max-imize Q ( x 1 ,...,x n ) subject to (3). We use the method of Lagrange multipliers: where w is a Lagrange multiplier.

Note that I i = 1 x The function g ( x ) = 1  X  (1 + x ) e  X  x increases monoton-ically for x &gt; 0 with g (0) = 0 and g (+  X  ) = 1. If we are given 0 &lt;  X  &lt; p i , then we can find the unique value  X  g  X  lead to bigger values of  X  i I i . That is why P monotonic function of  X  and we can apply a binary search algorithm 4 to achieve the condition P i 1 I This algorithm is described deeper in the extended version of this paper.

Algorithm 1: Find an optimal recrawl schedule
The value of  X  may be interpreted as the threshold we apply to content sources X  utility. Actually, we can find the minimal crawl rate required for the optimal crawling policy not to completely refuse to crawl content sources with the least utility.

We completely solved the optimization problem for the metric suggested in Section 2, the solution of (4) is theo-retically optimal (we use the name ECHO-based crawler for http://en.wikipedia.org/wiki/Binary_search_ algorithm the obtained algorithm, where ECHO is an abbreviation for Ephemeral Content Holistic Ordering).
Let us describe a concrete algorithm based on the results of Section 4.1. First, we use the results from Section 3 to obtain an input set of content sources. Then, in order to apply Algorithm 1 for finding an optimal recrawl schedule for these content sources, we need to know for each source its profit P i , the rate of profit decay  X  i , and the rate of new links appearance  X  i . We propose to estimate all these values dynamically using the crawling history and search engine logs. Since these parameters are constantly changing, we need to periodically re-estimate time intervals I Algorithm 1), i.e., to update the crawling schedule.
For this part, we need search engine logs to analyze the history of clicks on new pages. We want to approximate the average cumulative number of clicks depending on the page X  X  age by an exponential function. This approximation for two chosen content sources is shown on Figure 2.
Let us consider a cumulative histogram of all clicks for all new pages linked from a content source, with the histogram bin size equals to D minutes. Let s i be the number of times all N new pages linked from this content source were clicked during the first iD minutes after they appeared. So, s i the average number of times a new page was clicked during the first iD minutes.

We can now use the least squares method, i.e., we need to find: arg min
It is hard to find an analytical solution of (5), but we can use the gradient descent method to solve it (see the extended version of this paper for the detailed description of this method).

From the production point of view, it is very important to decide, how often to push data from search engine logs to re-estimate the values of  X  i and P i as it is quite an expensive operation. We denote this logs push period by L .

The rate of new links appearance  X  i ( t ) may change during the day or during the week. We thus dynamically estimate this rate for each content source. In order to do this, we use historical data: we consider the number of new links found at each content source during the last T crawls. We analyze how different values for T affect the performance of the algorithm in Section 5.

Finally, we should solve the following problem: in reality we cannot guarantee that we find exactly  X  i I i new links after each crawl. We can find more links than expected after some recrawl and if we crawl all of them, then we will deviate from the schedule. Therefore, we cannot both stick to the schedule for the content sources and crawl all new pages. So we propose the two following variants to deal with this problem.
ECHO-newpages. In order to avoid missing clicks, we always crawl newly discovered pages right after finding them. If there are no any new pages in the crawl frontier, we try to come back to the schedule. We crawl the content source, which is most behind the schedule, i.e., with the highest value of I 0 i /I i , where I 0 i is time passed after the last crawl of the i -th content source.

ECHO-schedule. We always crawl content sources with intervals I i and when we have some resources to crawl new pages, we crawl them (most recently discovered first).
We selected the top 100 most visited Russian news sites and the top 50 most visited Russian blogs using publicly available data from trusted sources 5 . For each such site, we selected content sources by applying the method discussed in Section 3 and obtained about 3K content sources overall.
Then, we crawled each of these content sources every 10 minutes for a period of 3 weeks (which is frequent enough to be able to collect all new content appearing on them before it disappears). The discovery time of new pages we observed is thus at most delayed by these 10 minutes. We considered all pages found at the first crawl of each source to be old and discovered  X  415K new pages during these 3 weeks. Keeping track of when links to new pages were added and deleted from the content sources, we created a dynamic graph that we use in the following experiments. This graph contains  X  2 . 4M unique links.

Additionally, we used search engine logs of Yandex to col-lect user clicks for each of the newly discovered pages in our dataset for the same period of 3 weeks plus 1 week for the most recent pages to become obsolete. We observed that  X  20% of the pages were clicked at least once during this 4 weeks period.
 Our data is publicly available. 6
There are no state-of-the-art algorithms for the specific task we discuss in this paper, but one can think of several natural ones:
Breadth-first search (BFS) We crawl content sources sequentially in some fixed random order. After crawling each source, we crawl all new pages linked from this source, which have not been crawled yet.

The two following algorithms help to understand the im-portance of 1) the holistic crawl ordering and 2) the usage of clicks from search engine logs.

Fixed-quota This algorithm is similar to ECHO-schedule , but we use a fixed quota of 1 2 for recrawling con-tent sources and for crawling new pages that have not been crawled before.

Frequency This algorithm is also similar to ECHO-schedule , but we do not use clicks from search engine logs, i.e., all con-tent sources have the same quality and content sources are ordered only by their frequency of new pages appearance.
We also propose a simplification of our algorithm, based on Section 4, which could be much easier to implement in a production system. http://liveinternet.ru/rating/ru/media/ http://blogs.yandex.ru/top/ http://yadi.sk/d/AWUixF6g7jaIj Algorithm N = 0.05 N = 0.10 N = 0.20 Frequency 0 . 014  X  0 . 004 0 . 39  X  0 . 04 0 . 61  X  0 . 06 BFS 0.24  X  0.04 0.46  X  0.03 0.62  X  0.03 Fixed-quota 0.43  X  0.04 0.59  X  0.03 0.69  X  0.03 ECHO-greedy 0.60  X  0.03 0.68  X  0.03 0.69  X  0.03 ECHO-schedule 0.52  X  0.02 0.69  X  0.03 0.71  X  0.03 ECHO-newpages 0.62  X  0.04 0.69  X  0.03 0.71  X  0.03 Upper bound 0.72 0.72 0.72 Table 1: Average dynamic profit for a 1-week window.
ECHO-greedy We crawl the content source with the highest expected profit, i.e., with the highest value of  X  where I 0 i is the time passed since the last crawl of the content source,  X  i is its rate of new links appearance, and P i is the average profit of new pages linked from the content source. Then, we crawl all new pages linked from this source, which have not been crawled yet, and repeat this process.
We simulated, for each algorithm, the crawl of the dy-namic graph described in Section 5.1, using the content sources as seed pages. Each algorithm can thus, at each step, decide to either crawl a newly discovered page or to recrawl a content source in order to find new pages. We used three crawl rates N = 0 . 05, N = 0 . 1 and N = 0 . 2 per second.

We evaluated the influence of all algorithms X  parameters and, based on our experiments, observed that only the logs push period has some influence, but only during the warm-up period (as shown on Figure 5). See the extended version of this paper for details. We thus took the crawl history of size 7 and the logs push period of 1 hour (randomly), and compared ECHO-based crawlers with other algorithms on three different crawl rates. In order to compare our algo-rithms during the last week of our observations (after the warm-up period) we measured the dynamic profit every two minutes using a time window of one week (enough to com-pensate daily trends). Table 1 shows average values and their standard deviations. Note that we also include the up-per bound of algorithms X  performance that we computed us-ing BFS algorithm with an unbounded amount of resources, which allows to crawl all new pages right after they appear. This upper bound therefore does not depend on the crawl rate and equals 0 . 72 of profit per second (see the last column of the table).

ECHO-newpages shows the best results, which are really close to the upper bound, although the crawl rate used is much smaller than the rate of new links appearance. This means that our algorithm effectively spends its resources and crawls highest quality pages first. Note that the smallest crawl rate that allows BFS to reach 99% of the upper bound is 1 per second (this value is measured, but not present in the table), as BFS wastes lots of resources recrawling con-tent sources to find new pages, while ECHO-newpage and ECHO-schedule reach this bound with crawl rate 0.2 per second (see the last column of the table).

Note that the profit of ECHO-greedy is also high. This fact can be a good motivation for using it in a production system, where ease of implementation is a strong require-ment (as it is much easier to implement). First, it only requires a priority queue of content sources rather than a recrawl schedule updated using the binary search method from Algorithm 1. Second, it does not use  X  i , so P i is thus simply the average number of clicks on pages linked from the i -th content source, and can therefore be computed eas-ier than by using the gradient descent method.

Let us show a representative example (at N = 0 . 1) demon-strating the advantage of ECHO-based algorithms over the baselines (see Figure 6). One can observe that ECHO-based algorithms perform the best most of the time. It is interest-ing to note though that during the night BFS shows better results. It happens as BFS is  X  X atching up X  by crawling pages, which were crawled by other algorithms earlier. This follows how the dynamic profit is defined: we take into ac-count the profit of the pages, which were crawled during the last 5 hours. We also see that the algorithm with fixed quota for crawl and recrawl perform well during the week-end because less new pages appear during this period and the crawl rate we use is thus enough to crawl practically all good content without additional optimizations.
To the best of our knowledge, the problem of timely and holistic crawling of ephemeral new content is novel. In this paper, we introduce the notion of ephemeral new pages, i.e., pages that exhibit the user interest pattern shown on Fig-ure 1, and emphasize the importance of this problem by showing that a significant fraction of the new pages that are appearing on the Web are ephemeral.

We formalized this problem by proposing to optimize a new quality metric, which measures the ability of an algo-rithm to solve this specific problem. We showed that most of the ephemeral new content can be found at a relatively small set of content sources and suggested a method for find-ing such a set. Then, we proposed a practical algorithm, which periodically recrawls content sources and crawls newly created pages linked from them as a solution of this prob-lem. This algorithm estimates the quality of content sources using user feedback. Finally, we compared this algorithm with other crawling strategies on real-world data and demon-strated that the suggested algorithm shows the best results according to our metric.

Special thanks to Gleb Gusev for his careful reading and useful comments.
