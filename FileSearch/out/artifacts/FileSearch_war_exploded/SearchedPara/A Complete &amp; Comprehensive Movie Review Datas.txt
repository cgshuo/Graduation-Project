 Online review sites are widely used for various domains in-cluding movies and restaurants. These sites now have strong influences towards users during purchasing processes. There exist plenty of research works for review sites on various as-pects, including item recommendation, user behavior anal-ysis, etc. However, due to the lack of complete and com-prehensive dataset, there are still problems that remain to be solved. Therefore, in this paper we assemble and publish such dataset (CCMR) for the community. CCMR outruns existing datasets in terms of completeness, comprehensive-ness and scale. Besides describing the dataset and its col-lecting methodology, we also propose several potential re-search topics that are made possible by having this dataset. Such topics include: (i) a statistical approach to reduce the impacts from fake reviews and (ii) analyzing and modeling the influences of public opinions towards users during rating actions. We further conduct preliminary analysis and exper-iments for both directions to show that they are promising.  X  Information systems  X  Test collections; Spam de-tection; Personalization; Test Collection, User Behavior, Review Sites
Online review sites are now widely used for various do-mains. For example we have IMDb and MovieLens for movies, Yelp for restaurants, TripAdvisor for hotels and attractions. When making choices, nowadays lots of people refer to these review sites for guidance. These sites have significant influ-ences towards users during their purchasing processes.
Due to the importance, there are plenty of research works in this direction. Most of existing works focus on rating pre-diction and user behavior analysis. For rating predictions, the goal is to improve user experiences by implementing cor-responding recommender systems [8]. Matrix factorization is the most widely used technique for this task [3]. For user behavior analysis, researches mainly focus on detecting mali-cious users and bots in review sites (or fake review detection) [7]. Because higher rating in review sites may lead to more purchases, some merchants hire malicious users or bots to give untruthful ratings. These fake reviews highly jeopardize the user experiences and bring review sites into disrepute. There are research works that tackle this by rating pattern analysis [2], linguistic analysis [5] and other techniques.
Despite the satisfying research outcomes, there are still research problems remain to be solved. For example, most existing works on fake review detection focus on identify-ing each malicious user or fake review instance instead of targeting directly at the final aggregated ratings, which are the most important information the review sites provide for users. Also, there is no work on analyzing the influences of public opinions towards users (users are aware of the aggre-gated rating for the item before rating it in most sites). For these directions, one major obstacle is the lack of suitable dataset. To model the aggregated ratings, we requires com-plete rating logs. To analyze the influences of public opin-ions, we need snapshots of aggregated ratings right before each rating action. However, currently there is no dataset that satisfies these requirements.

Therefore, we assemble and publish a complete and com-prehensive dataset CCMR for the community. Here com-plete indicates complete logs over all users instead of only a subset of users, and comprehensive indicates the availability of side information such as timestamps, item-side informa-tion, review texts and snapshots of the aggregated ratings.
In the following sections, we first describe the dataset and its collecting methodology, and then compare it with exist-ing datasets. Further, we propose two potential topics that benefit from CCMR. We also conduct preliminary analysis and experiments according to these topics.
The CCMR dataset can be freely accessed online 1 . For privacy concerns, we reorder the user ids as well as the movie ids in CCMR. For review texts, we replace each word with corresponding word id.

CCMR consists of movie rating logs from one of China X  X  largest movie review sites. The data is collected and dumped in May, 2015. All rating logs before the collection time are http://dataset.apexlab.org/ccmr included. In total, it covers 190,129 movies and 4,920,695 active users (with at least one rating action). Among those users and movies, we have 283,775,314 rating actions. On average we have 1,471.92 rating logs per movie and 58.10 per active user. Besides the rating matrix, we also include the corresponding review text, the date of the rating action as well as the snapshot of aggregated statistics for the target movie right before the rating action. For aggregated statis-tics, -1 is given when no statistic is available due to insuffi-cient rating histories. We list and explain all data columns for rating logs in Table 2. For each movie, we also pro-vide director(s), actors, movie genre (action/romance/etc.), release date and nation.
 stat distrib 1 stat distrib 5
We plot the distribution of user X  X  rating counts as well as movie X  X  with log-log plots in Figure 1, from which we can notice the power law or the long tail phenomenon. We can also notice that in log-log plot for user, there is an unusual streak above the main streak, i.e. an unusual large number of users have exactly the same number of rating logs. By detailed analysis, ratings from each of these users are mostly conducted on the same day. So they are highly likely the malicious users or bots. Figure 1: Log-Log Plot for Movie/User Action Frequency
One of the most well known task based on rating logs is rating prediction, which is also the core for recommender systems in item-based sites. Matrix factorization is the most widely accepted and used technique for this task [3]. As this task has been heavily studied, there also exist several well assembled datasets, e.g. Book-Crossing, EachMovie and MovieLens from GroupLens 2 and the Netflix dataset [1]. Comparing to these datasets, CCMR X  X  advantage is its scale, completeness and comprehensiveness. Therefore, CCMR can be employed for wider scenarios and research topics. A detailed comparison is listed in Table 1.
Because review sites now play important parts during user X  X  purchasing processes, the ratings are critical for the product X  X  revenues. Driven by benefits, some merchants hire malicious users or bots to generate fake reviews to increase the rating of their products or to discredit their competitors. As analyzed previously (Figure 1), malicious users widely exist and have noticeable influences on the overall statistics.
Fake reviews not only have negative impact on user ex-periences but also bring the review sites disrepute. There-fore, this problem has aroused people X  X  attention and heated discussion. There are works aiming at identifying the fake reviews or malicious users. Jindal et al. focus on unusual review patterns in [2]. Linguistic features in the review text are also considered in [5]. Mukherjee et al. further go be-yond single user and target at group spammers [6].

Despite the promising performance of existing works, they only focus on identifying specific user or review instance. On the other hand, users normally do not go through each re-view instance in detail. Instead, most users only focus on the aggregated ratings. So we can eliminate the fake reviews X  in-fluences by directly recovering the unbiased aggregated rat-ings. Another advantage of this is that spammers can not easily find the rules and evolve accordingly while they can for traditional approaches. Therefore, this would be a great future direction for fake review detection. However, to the best of our knowledge, currently there is no research work following this direction.

One major reason is that currently we have no suitable dataset to conduct this topic. We need the complete rating logs to model the aggregated ratings, while current datasets only provide logs over a subset of users instead of complete logs.

Another problem is the lack of ground truth (the unbi-ased public opinion towards the item). By analyzing the identified fake reviews (by analysis in Figure 1 and existing techniques), we find that most fake reviews are close to the http://grouplens.org/datasets/movielens/ movie X  X  release date (few weeks). Specifically, we define f to be the ratio of fake reviews posted within k days after the release date ( f +  X  =1), and n k accordingly for normal reviews. We plot f k /n k in Figure 2 to show that fake re-views significantly shift towards the release date comparing to normal reviews. This finding also matches with intuition because only the ratings in first few weeks are crucial for the movie X  X  box office. Therefore we consider the aggregated rat-ing at a long time (years) after the movie X  X  release date as a good approximation of the unbiased public opinion.

So the task can be formally stated as: given rating logs within k days after the movie X  X  release date, estimate the un-biased public rating (approximated by the aggregated rating at least one year after release date). We model the task as regression problem and employ linear regression for it. For features, we include the average ratings on day 1, day 2 up to day k , and also movie-side information including direc-tors, actors, genre and nations. Except for the real-valued average ratings, one-hot encoding is used for the categorical information.

For comparison, we implement baseline method by di-rectly use the average rating as the estimation, which is the method used in most review sites. By using Mean Absolute Error (MAE) as the metric, we show the results in Figure 3. As we can notice, performance of linear regression is notice-ably better than the baseline. We believe there still exist much space for improvement by employing more advanced techniques. The results also indicate that the task is rather challenging, especially for the very first few days. Figure 3: Preliminary Result for Recovering Truthful Rating
In this section we focus on behavior analysis over legiti-mate users instead of malicious users. We propose another future direction: to analyze and model the influences of pub-lic opinions towards each individual user in review sites.
The users are aware of the public opinion towards the item before rating it. Recall the online rating process. In most scenarios, users conduct the rating actions on the item X  X  de-tail page, in which the aggregated rating (given by aver-age rating and rating distribution) is normally highlighted. However, there is no research analyzing whether and to what extent are users influenced by such public opinions.
There are several works in similar directions. Krishnan et al. define this influence as Social Influence Bias (SIB) in [4], and find statistically significant evidence of SIB on a political-related rating system (a rather small dataset and specific scenario). Behavior patterns behind voting actions is targeted in [9]. They claimed that user actually votes for whether the instance is ranked lower/higher than it deserves according to his/her opinion.

However, there is no work modeling this in large-scale re-view sites. The main reason is that no existing dataset pro-vides the snapshots of the aggregated ratings before each rat-ing action. By assembling CCMR, we provide such dataset for the community.

To show the existence of such influence (or SIB) in review sites, we conduct the following preliminary analysis. We randomly select popular movies with rather large variation in their aggregated rating. For each movie, we represent their rating logs as ( r i ,p i ) where r i indicates the rating of i th action and p i indicates the public opinion (the aggre-gated rating) right before the action. We group the logs by similar p i , and report the rating results from each group by averaging. If users are not effected by public opinion, then r and p i should be independent and the mean of r i within each group should be the same, which represents the true public opinion towards the movie. We plot the results for three movies in Figure 4. As we can see, dependency exists. The phenomenon widely exists in most movies. Therefore, we claim that the public opinion do have an noticeable in-fluence on users.

To gain more insight, we model the user X  X  final rating by linear combination over user X  X  actual preferences and the public opinion. The combination parameter for each user varies as some users are susceptible while the others are not. We further integrate the model with traditional matrix factorization (MF) [3]. The overall model as well as the loss function are now as follows: where r ij is the predicted rating for user i on movie j . U models the user X  X  true preferences by user and item X  X  latent factor according to MF. p ij is the public opinion for movie j just before the user i rates it. w i is the parameter indicating to what extent user i is influenced by public opinion. Large w i indicates the user to be more susceptible.

Evaluating it as rate prediction model, it has slight im-provement comparing to MF. A relative improvement of 1 . 74% is achieved according to RMSE metric. As our goal is to understand the influence of public opinion instead of rating prediction, we focus on parameter w i . We plot the analytical results regarding w i and the number of rating ac-tions made by user i in Figure 5. On the left we plot each user instance. For users with few actions, the distribution seems like Gaussian. However, when shifting to users with heavy activities, the distribution begins to fan out, espe-cially to higher w . To be clear, we also plot the mean and variance of w for users with different activity level. From which we conclude that users with heavy activities are tend to be more conservative and susceptible to public opinions. And for normal users, the susceptible level is mostly due to the user X  X  personality.

By these preliminary analysis, we show that influences of public opinion do exist and there are interesting phe-nomenons in user behaviors. We believe that following this direction, there are plenty of behavior patterns to be re-vealed. By understanding such patterns, we can model user preferences more accurately and further improve personal-ized services such as recommender systems. In this paper we assemble and publish a benchmark dataset CCMR for the community. CCMR is a complete and com-prehensive movie review dataset with over 283 million rat-ing actions made by 4.9 million users over 190k movies. The data is complete as it includes actions from all users instead of only users in sampled subset. For comprehensiveness, we provide timestamp, item-side information as well as the snapshot of aggregated statistics for the item before each rating action. Therefore, our dataset CCMR can support a wider range of tasks. Further, we propose two novel tasks that are made possible by employing CCMR dataset. One is to eliminate effect of fake reviews by directly recovering the unbiased aggregated rating, and the other is to analyze and model the influences of public opinions towards each user. We also conduct preliminary analysis and experiments for these tasks. We will keep maintaining and updating this dataset. For any suggestions to extend the dataset, please contact the first author. [1] J. Bennett and S. Lanning. The netflix prize. In [2] N. Jindal, B. Liu, and E.-P. Lim. Finding unusual [3] Y. Koren, R. Bell, and C. Volinsky. Matrix [4] S. Krishnan, J. Patel, M. J. Franklin, and [5] T. Lappas. Fake reviews: The malicious perspective. [6] A. Mukherjee, B. Liu, J. Wang, N. Glance, and [7] A. Mukherjee, V. Venkataraman, B. Liu, and N. S. [8] P. Resnick and H. R. Varian. Recommender systems. [9] R. Sipos, A. Ghosh, and T. Joachims. Was this review [10] C.-N. Ziegler, S. M. McNee, J. A. Konstan, and
