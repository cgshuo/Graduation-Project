 Argumentation mining attracts much attention re-cently: it is an important building block of applica-tions like automated decision making (Bench-Capon et al., 2009) or pro-and-con search engines (Cabrio and Villata, 2012c). In such applications, argumen-tation mining usually consists of solving three tasks for each document: (1) Identifying all argumenta-tive text segments in the document, (2) classifying the type of each segment, and (3) classifying rela-tions between the segments.

In this paper we focus on the first task taking on the retrieval perspective of a search engine: Given a large-scale collection of documents (e.g., the web) and a query on some topic, return all argumenta-tive text segments relevant to the topic. Among oth-ers, a classifier is needed for this task that can dis-tinguish argumentative from non-argumentative seg-ments. Since we cannot presuppose a specific do-main or register within a general retrieval scenario, the classifier needs to robustly deal with documents from diverse domains and registers. In this regard the following two key issues arise.

First, existing approaches to classifying argu-mentativeness usually focus on specific text do-mains (e.g., education) and registers (e.g., student essays). Therefore, many used features capture not only local linguistic properties of a text segment, but also global document properties (e.g., that a segment is part of the introduction). Such kinds of features tend to be effective only within a certain domain or a particular register while often failing for others.
Second, all major existing approaches follow a su-pervised learning scheme based on manual annota-tion of argumentative text segments. However, the annotation of arguments is particularly intricate and thus expensive due to the complex linguistic struc-ture and the partly subjective interpretation of ar-gumentativeness. Different types of argumentative and non-argumentative segments may come in any order, segment boundaries are not always unam-biguous, and parts of an argument may be implicit. Studies reveal that annotators need multiple train-ing sessions to identify and classify argumentative segments with moderate inter-annotator agreement, and crowdsourcing-based annotation does not help notably (Habernal et al., 2014). I.e., a high-quality manual annotation will not scale to large numbers of documents from diverse domains and registers.
We propose a solution to the outlined issues. In particular, we follow the idea of distant supervision to construct a large-scale corpus of text segments from diverse domains and registers annotated with respect to argumentativeness. Distant supervision is a well-known idea for training robust statistical clas-sifiers. Here, we exploit online debate portals that (1) contain argumentative and non-argumentative text segments for several controversial topics, and that (2) are organized in a semi-structured form, al-lowing to derive annotations from it.

In several experiments we compare classifiers trained on the constructed corpus to those trained on existing corpora for argumentation mining. We classify argumentativeness using a rich set of lexical, syntax, and indicator feature types. Our results sug-gest that the new corpus is the most robust resource for classifying argumentative text segments across domains and registers. In addition, we observe that n-grams seem to be most domain-dependent, while syntax features turn out to be more robust.

The contribution of this paper is three-fold: First, through distant supervision we acquire a large cor-pus with 28,689 argumentative text segments from the online debate portal idebate.org. The corpus covers 14 separate domains with strongly varying feature distributions. It will be made freely avail-bust classifier for argumentativeness, providing ev-idence that distant supervision does not only save money and time, but also benefits the effectiveness of cross-domain and cross-register argumentation mining. Third, we evaluate X  X or the first time X  the robustness of several features in classifying ar-gumentativeness across domains and registers.
Altogether, the paper serves as a starting point for bringing argumentation mining to practice. We ex-pect that a robust identification of arguments will be a core module of future search engines, as it allows to provide rationales for retrieved documents. To this end, the search engines also need to identify the most relevant arguments for a given topic. The pa-per concludes with ideas on how to assess argument relevance with resources that are obtained through applying our proposed distant supervision technique to other datasets. Argumentation mining is still in an early stage of in-vestigation, although several promising approaches have been proposed in the last years. Our survey of the argumentation mining literature especially cov-ers three respects: (1) favored domains and regis-ters, (2) techniques for annotation acquisition, and (3) the exploitation of debate portals. We combine these research lines in our approach to tackle argu-mentativeness classification across domains.
The existing argumentation mining approaches achieve classification accuracies ranging from 73% and 86% (Stab and Gurevych, 2014b; Levy et al., 2014; Palau and Moens, 2009) but they deal with texts from one register or one narrow domain only. For instance, Palau and Moens (2009) address the legal domain, Cabrio and Villata (2012b) as well as Boltu X i  X  c and  X najder (2014) investigate online de-bates and discussions, Aharoni et al. (2014) examine Wikipedia articles, Villalba and Saint-Dizier (2012) as well as Wachsmuth et al. (2014a) work on product reviews, Stab and Gurevych (2014a) focus on per-suasive essays, and Peldszus (2014) on microtext. In (Wachsmuth et al., 2015), we studied the generality of sentiment-related argumentative structures across domains. In contrast, here we aim at effectiveness in cross-domain argumentation mining, which is useful for practical applications such as argument retrieval from diverse web-scale document collections. All mining approaches above proceed as follows. Starting point is a complex and often expensive man-ual annotation of argumentative text segments in a collection of documents, including the segments X  roles (e.g., premise or conclusion) and their relations (e.g., support or attack). Then, the classification of argumentativeness, roles, and relations is achieved via supervised machine learning using different lin-guistic and statistical features. Our approach avoids manual annotation. Instead, we apply distant super-vision to automatically acquire annotations.
Distant supervision is a technique to automati-cally harvest annotations from data that has been compiled and structured intentionally by a user com-munity on the web. Most approaches employing dis-tant supervision so far address the problems of rela-tion extraction (Mintz et al., 2009; Hoffmann et al., 2011) or event extraction (Reschke et al., 2014). A few others target at sentiment analysis (Marchetti-Bowick and Chambers, 2012) and emotion detec-tion (Purver and Battersby, 2012). In case of the latter, annotations are derived from strong textual in-dicators like emoticons. In this paper, we exploit metadata from the debate platform idebate.org for mapping texts from the platform to argumentative and non-argumentative classes.

The idea of relying on idebate.org for argument annotation acquisition is in line with related research of Cabrio and Villata (2012c) and Gottipati et al. (2013). In these papers, however, the debate por-tal is used to infer text-level knowledge only (e.g., stances in debates), but not to generate a complete annotated dataset for argumentativeness.

The work that is most related to ours is the pro-posal of a method to exploit debate portals for semi-supervised argumentation mining by Habernal and Gurevych (2015). In particular, the authors use word embedding techniques for projecting the texts from debate portals into an annotated argument space, re-lying on the argument model of Toulmin (1958). On this basis they identify argumentative text segments and their roles. A clear difference to our approach is that Habernal and Gurevych (2015) consider all content of debate portals as argumentative. As a consequence, their approach concentrates mainly on exploiting the debate portals for improving the clas-sification of segment roles, with minor impact on ar-gumentativeness. Moreover, while being compara-bly effective, our approach aims for simplicity. The reason is that we apply distant supervision to derive a robust resource from the metadata of debate por-tals only. Thus, we allow for a rich feature space without requiring to use advanced machine learning methods. Finally, Habernal and Gurevych (2015) evaluate their approach only on one dataset from the educational domain, whereas we explicitly aim at robustness across domains. Accordingly, we con-duct several experiments on different available cor-pora (including theirs). We propose an approach based on the distant super-vision paradigm. Our goal is to obtain a classifier that can robustly mine argumentative texts across domains. More precisely, we focus on the task of classifying each segment of a text as being argumen-tative or not. We assume the text to be separated into segments already.

Our approach consists of three high-level building blocks: (1) Mapping functions that allows an auto-matic acquisition of argumentativeness annotations from debate portals. (2) A corpus with argumen-tative and non-argumentative text segments created using the functions. (3) A classifier that can distin-guish the two classes of text segments. All building blocks are detailed in the following. Figure 1 depicts an overview of the approach. 3.1 Argumentativeness Mapping Functions The basic idea of distant supervision is to gener-ate annotations by automatically mapping unlabeled source data to a set of predefined class labels. This requires resources that are related to the given task as well as effective heuristic labeling functions. Typ-ical resources comprise large amounts of data, of-ten in form of user-generated content with semi-structured or structured metadata. Ideally, the re-source X  X  metadata substantially eases the mapping to the predefined labels.

In the context of argumentation mining, online de-bate portals serve as a rich source of argumentative texts on diverse topics. These portals are typically managed by user communities. Textual content can be added via a structured interface that already spec-ifies metadata (e.g., what constitutes a topic or an argument). Thus, mapping text segments from de-bate portals to classes for argumentation mining is a promising instance of distant supervision.

In particular, we rely on idebate.org. This debate portal has an established community of experienced debaters and volunteers who take care of editing and monitoring semi-structured discussions on various controversial topics, subsumed under 14 high-level themes. A discussion (called  X  X ouse X  in the por-tal X  X  terminology) starts with a one-sentence stance on the respective topic, followed by a more verbose introduction to the topic. Afterwards, points for and against the stance are opposed, both given as a list of arguments. Each argument in turn comes along with points (the argument itself) and counterpoints (counterarguments). Table 1 shows an example.
We downloaded all available discussions from idebate.org. For each discussion, the stance on the topic, the introduction, and the points are extracted from the URL of the web page of the respective dis-cussion. Based on the structure exemplified in Ta-ble 1, we stipulate on the following assumptions to automatically map components from the debate por-tal to annotated argumentativeness instances. [Component]: Introduction  X  [Assumption]: The introduction explains the  X  [Mapping]: Each sentence in the introduction [Component]: Points for &amp; Points against  X  [Assumption]: Each point from these lists rep- X  [Mapping]: Each point is an instance of the ar-[Component]: Point &amp; Counterpoint  X  [Assumption]: The main objective of a point  X  [Mapping]: Each sentence in a point / counter-
To optimize the mapping quality, we manually an-alyzed 50 discussions and then derived three tailored cleansing rules from them: (1) We remove all liter-ature references from the argumentative instances. (2) We delete all special brackets and symbols from the argumentative instances. (3) We delete some keywords from the non-argumentative instances that are used by the community to organize a discussion (e.g.,  X  X his house X  or  X  X his debate X ). 3.2 The Webis-Debate-16 Corpus As a result of applying the defined mapping func-tions, we obtained a large argumentation mining corpus, called Webis-Debate-16 . The corpus con-tains 28,689 text segments from the 14 themes of idebate.org (23,880 argumentative, 4809 non-argumentative). Each theme is assumed to represent one domain. Table 2 lists the distribution of docu-ments over the domains in the corpus. Regarding the number of annotated text segments, Webis-Debate-16 is the largest dataset published so far for argu-mentation mining. While our review corpus from (Wachsmuth et al., 2014b) is even larger, its anno-tations are restricted to sentiment-related argumen-tation. Table 3 compares Webis-Debate-16 to other real argumentation mining corpora, namely, the Es-says corpus (Stab and Gurevych, 2014a), the Web discourse corpus (Habernal and Gurevych, 2015), the European Court of Human Rights (ECHR) cor-pus (Palau and Moens, 2009), and the Araucaria cor-pus (Reed and Rowe, 2004). The Webis-Debate-16 3.3 A Classifier for Argumentativeness A wide range of statistical and linguistic features has been suggested for argumentation mining and related tasks such as discourse parsing. We em-ploy supervised machine learning to train an argu-mentativeness classifier based on the features em-ployed by Stab and Gurevych (2014a), Palau and Moens (2009), and Habernal and Gurevych (2015) that cover the following: Token n-grams : Unigrams, bigrams, and trigrams as Boolean features. In general, n-grams are the most powerful feature type in many related text clas-sification problems (e.g., sentiment analysis). Discourse markers : Features that represent the ex-istence of words such as  X  X ecause X , which are fre-quently used in argumentative texts.
 Syntax: This feature category contains the number of sub-clauses and production rules.  X  Number of sub-clauses: Counter for the num- X  Production rules: Boolean features capturing Part of speech: Features that capture information related to the parts of speech in a text segment:  X  Verbs: A boolean feature capturing whether a  X  Adverbs: A boolean feature capturing whether  X  Modals: A boolean feature capturing whether  X  Verb tense: Boolean features capturing whether  X  First person pronouns: Pronouns such as  X  X  X 
Using these features, we train a binary statistical classifier for argumentativeness. Given a set of text segments, the classifier decides for each text seg-ment whether it is argumentative or not. We now report on several in-domain and cross-domain experiments with the classification of ar-gumentativeness. The goals are (1) to demonstrate the effectiveness and robustness of training on the Webis-Debate-16 corpus for cross-domain classifi-cation, and (2) to analyze the effectiveness of the proposed features across domains and registers. 4.1 Experimental Setup To evaluate the effect of using the Webis-Debate-16 corpus for training, appropriate argumentation cor-pora are needed for comparison. We consider an available corpus as appropriate if (1) the corpus is annotated in a way that allows the distinction of ar-gumentative from non-argumentative text segments, and if (2) the corpus comes with clear annotation guidelines and reported inter-annotator agreement. In addition, we aim at corpora that differ in terms of the covered domains and registers to provide an adequate cross-domain setting. While the Araucaria corpus does not meet the second requirement (Reed and Rowe, 2004), two recently published corpora fulfill both; we refer to them as the Essays corpus and the Web discourse corpus.
 Essays: The Argument Annotated Essays corpus of Stab and Gurevych (2014a) consists of 90 man-ually annotated persuasive student essays from the education domain. Argumentative text segments are assigned with their type (major claim, claim, or premise). Following Stab and Gurevych (2014b), we consider all sentences that do not have an annota-tion as being non-argumentative, and the annotated segments as argumentative.
 Web discourse: The Argument Annotated User-generated Web Discourse corpus of Habernal and Gurevych (2015) consists of 340 documents from six different topics and four registers. The anno-tation of arguments is conducted based on the ar-gument model of Toulmin (1958) using five types (claim, premise, backing, rebuttal, and refutation). Again, we consider all annotated text segments as being argumentative and sentences without annota-tion as being non-argumentative.

Only in case of the Essays corpus, the authors al-ready provide a split into a training and a test set (72 essays for training and 18 for testing). For both the Web discourse corpus and our corpus, we ran-domly split the document set into 80% for training and 20% for testing. As a result, the training set of the Web discourse corpus consists of 272 docu-ments, and its test set of 68 documents, while the training and test sets of our corpus consist of 356 and 89 documents, respectively.

We train classifiers for each of the above feature types and for the full feature set on the training set of each corpus using the default configuration of the naive Bayes implementation of Weka (Hall et al., 2009). Since all corpora are imbalanced in terms of the number of argumentative and non-argumentative text segments, we perform undersampling for all training sets X  X n effective technique for largely im-balanced datasets (Japkowicz and Stephen, 2002). All feature values are computed based on the out-put of the StanfordNLP library (Manning and Klein, 2003). For the different classifiers, we measure the resulting classification performance on all three test sets in terms of accuracy and F 1 -score. 4.2 In-Domain Results Table 4 shows the results of the in-domain experi-ments. For the full feature set, the achieved F 1 -score of 0.922 and the accuracy of 0.918 on the Webis-Debate-16 corpus are high compared to those on the Essays and Web discourse corpus. This might be a result of guidelines suggested by the debate portal community, which make the corpus quite homoge-neous in terms of style.

Using the full feature set leads to the best results on all three corpora. N-grams denote the most effec-tive single feature type on the Essays copus and on the Webis-Debate-16 corpus, while the syntax fea-tures outperform the n-grams on the Web discourse corpus. On the Essays and on the Webis-Debate-16 corpus, the syntax features are sometimes better and sometimes worse than the part of speech features. The discourse markers are the least effective single feature type, largely failing on all test sets, especially in terms of F 1 -score.

Note that a comparison to the exact values re-ported by Stab and Gurevych (2014b) for the Es-says corpus and by Habernal and Gurevych (2015) for the Web discourse corpus is not be meaning-ful due to their experimental set-ups with different class sets. However, their reported results for the non-argumentative class are comparable to the per-formance we achieved: Stab and Gurevych (2014b) achieve an F 1 -score of 0.275 with lexical features and 0.426 with syntax features on the Essays corpus, while Habernal and Gurevych (2015) obtain an F 1 -score of 0.718 with lexical features and 0.671 with syntax features on the Web discourse corpus. 4.3 Cross-Domain Results Table 5 shows the results of the cross-domain ex-periments. For comparison, we again show the in-domain results in grey color.

As usual, the obtained cross-domain effective-ness values are lower than the in-domain values in most cases and the full feature set usually outper-forms feature subsets. One notable exception are the results for the part of speech features on the Es-says corpus. The cross-domain effectiveness trained on the Webis-Debate-16 corpus is about six points higher than the in-domain effectiveness in terms of F -score and four points in terms of accuracy. For testing on the Web discourse corpus, training on the Webis-Debate-16 corpus using the full features gives the best cross-domain performance. For test-ing on the Webis-Debate-16 corpus, training on the Web discourse corpus using the n-gram feature type achieves the best cross-domain performance.

Overall, the best corpus for cross-domain classifi-cation in our evaluation is clearly the Webis-Debate-16 corpus. Training on Webis-Debate-16 leads to the best cross-domain results for the full feature set and three out of four of the single feature types (n-grams, syntax, and part of speech). Only for the discourse markers, the Web discourse corpus performs better in the cross-domain scenario.

Finally, we observe that the n-grams feature type turns out to be the most domain-dependent in our evaluation. In contrast, both the syntax and the part of speech features appear quite robust across do-mains. The performance of the discourse markers greatly depends on how frequent they are used in the target domain and register.

Although combining the Webis-Debate-16 corpus to the training datasets of the Essays or the Web dis-course corpus increased the performance compared to training only on Webis-Debate-16, it did not out-perform the in-domain performance for both cor-pora. For conciseness, we therefore omit to report the results of our respective experiments here. 4.4 Discussion of our Approach to Robustness As expected, our experiments reveal the domain de-pendence of feature distributions in classifying argu-mentativeness. This finding emphasizes the impor-tance of explicitly dealing with domain robustness in argumentation mining whenever more than one do-main (in terms of a topic, register, or similar) is of interest. To achieve robustness, we have proposed a simple but effective approach that applies distant supervision to create a corpus for classifying argu-mentativeness. Our results are promising: Classifi-cation clearly improves across domains when being trained on our Webis-Debate-16 corpus instead of other available argumentation mining corpora.
The obtained results suggest that our approach can be effectively leveraged to achieve domain ro-bustness. One reason is probably the larger size and domain coverage of our Webis-Debate-16 corpus compared to the other tested corpora. This makes our corpus and the underlying distant supervision idea a valuable resource for research on argumen-tation. More noise reduction might even further in-crease the performance of training on the corpus.
In its current form, our corpus contains annota-tions for distinguishing argumentative from non ar-gumentative text only. While more fine-grained an-notations of argumentative texts, such as premise vs. claim, are important for argumentation mining, they cannot be obtained directly from the metadata of idebate.org. Still, the positions of segments in some parts of the debate portal (e.g, point and coun-terpoint) often indicate whether they are claims or premises. We plan to investigate the exploitation of such information for future versions of the corpus.
So far, we have shown how to create an annotated corpus classifying argumentativeness exploiting one specific debate portal via distant supervision. In principle, our approach is rather general and, thus, could also be applied to other argumentation re-sources and tasks. Indeed, idebate.org is only one of many web resources with lots of argumentative texts and argumentation-relevant metadata. Aside from debate portals, one such resource is given by Wikipedia talk pages. Very recently, Wikipedia in-troduced markups within these article discussions, such as support or oppose . While still being in an early stage, this metadata seems promising to de-rive argumentative relations from it. We plan to use our distant supervision approach for classifying ar-gumentative relations on such resources. This can then be an important next step to enable the assess-ment of argument relevance  X  X  core building block of an argument retrieval system. 4.5 From Argumentativeness to Relevance As motivated in the introduction, a retrieval system for arguments not only requires the identification and classification of argumentative text segments. A successful future search engine taking argument features into account additionally needs a way of ranking arguments according to their relevance. In this regard, we propose a  X  X ageRank for arguments X  based on the link network of support and attack re-lations between arguments.

In particular, given robust algorithms to identify arguments and their relations across web pages (e.g., via distant supervision), we could build an argu-ment graph for the web. Related research has al-ready used the argumentation framework of Dung (1995) to find accepted arguments based on such a graph on a much smaller scale (Cabrio and Villata, 2012a). However, the size of the web would allow for recursive analysis of the graph with statistical ap-proaches like the famous PageRank algorithm (Page et al., 1999), enabling an assessment of argument relevance. Several research questions arise from this idea (e.g., how to balance support and attack within the analysis) but argument relevance forms a very important future research direction. Most existing approaches tackle argumentation min-ing in a supervised manner trained on manually an-notated documents from a specific domain. Such approaches neither tend to be effective on docu-ments from other domains, nor do they scale to ap-plications that deal with huge document collections, such as search engines. In this paper, we investi-gate how to achieve robust performance for argu-mentation mining across domains, focusing on the classification of the argumentativeness of text seg-ments. In particular, we approach the data side of this problem, namely, we apply distant supervision to automatically create a large annotated corpus with argumentative and non-argumentative text segments from several domains, exploiting metadata from the online debate portal idebate.org.

Based on the created corpus and on common manually annotated corpora, we conduct several in-domain and cross-domain argumentativeness exper-iments. Our results clearly indicate that training on the created Webis-Debate-16 corpus yield the most robust cross-domain classifier. Thereby, our approach serves as a starting point for bringing ar-gumentation mining to practical applications like search engines. The corpus as well as an implemen-tation of the approach will be made freely available. Besides a robust identification of argumentative seg-ments, search engines will also need to decide which arguments are the most relevant to a given query X  a very promising future research direction in the field of argumentation mining.

