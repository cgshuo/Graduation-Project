 1. Introduction
Video is a rich medium for carrying information, far richer than text, image or audio. Video in digital for-mat started to receive much attention when personal computers became powerful enough to handle video, namely when they were able to capture, compress and store video. Because technology now allows us to easily capture, compress, store, edit, transmit, and render video, and this results in large collections of video, it fol-lows that managing collections of video information is also now important.

There are two broad approaches for managing digital video information, or indeed managing any kind of digital information: using manually-assigned tags and metadata and automatically processing video content in order to derive content descriptors ( Smeaton, 2004 ). In the case of video, archivists have, for years, been using manual annotations and metadata (date, time, location, etc.) as the basis for systems which manage video con-tent in broadcasters X  libraries, news archives, video stock shot vendors, etc. The second approach of automat-ically processing video data in order to derive content descriptors for use as a basis for retrieval is a relatively recent development. Even in the few years since such video analysis techniques have reached the stage of being efficient, scalable and accurate as well as also being computationally tractable, a variety of approaches or modalities for automatic content description have emerged, which we shall see later.

We are now at an interesting point in the development of content-based retrieval from digital video archives in that the easy things like searching the spoken dialogue have been researched, prototypes have been devel-oped and are under evaluation, and we are now facing having to tackle the harder things in terms of video processing such as matching tracked video in shots if we want to make further progress in this area. This requires us to now propose and develop new modalities for video retrieval as these new video analysis and matching technologies turn into tools which are usable in video retrieval. As we move headlong into tackling the  X  X  X ard X  X  video retrieval techniques, a question we should ask is whether these will be worth the effort X  X o we know what kinds of modalities for retrieval that users want, or will even find useful? Because video is so different to say text, simply replicating techniques based on statistical word occurrences developed for text-based IR on video is not acceptable because it would not necessarily leverage the richness of information asso-ciated with video when compared to text or other media.

To address the question of whether, and perhaps in what order, different modalities for video retrieval are actually useful in retrieval, in this paper we set out to examine how a range of such video retrieval techniques are used in a certain type of video retrieval scenario. By limiting ourselves in the domain and type of video we use, and also in the type of search a user is asked to perform, we are able to extend the range of search/browse modalities available into a richer set of retrieval options which we would expect to see developed generally. Thus, restricting the video genre and search type gives us a glimpse of what we can expect on general purpose video sometime in the future. Operating in a controlled search environment, we are able to monitor how a set of users each runs a set of given search tasks and on this we carry out a usage study analysing what features of search our users use, and when. This gives us some guidelines as to what kinds of video retrieval people will actually use.
The rest of paper is organised as follows. The next section contains a brief review of contemporary approaches to video IR. This mostly covers shot-level retrieval, i.e. retrieval of video shots rather than single frames, video scenes or whole programmes. Our review includes text-based retrieval on the closed captions or speech as appear-ing in the video dialogue or in video OCR, using a sample image/keyframe as a query for video shot retrieval, indexing video by semantic concepts and the growing interest in ontologies for video retrieval and the different approaches to video browsing. A recent development in available video retrieval techniques is object-based retrieval where a user uses a video object or a component of an object as part of a query. In Section 3 , we examine the problem of object detection and what is currently feasible in that area, including ongoing work on object-based video retrieval. Section 4 gives a brief overview of a system we have built which supports search and brows-ing of a 21-hour corpus of video using a range of retrieval options. We describe the video corpus, user queries, system and we conclude by highlighting the contribution of this paper. 2. Approaches to video retrieval
In this paper we are interested in video retrieval where metadata such as date and time, location, title, etc., or other manually assigned descriptors, is not sufficient to give the richness of retrieval and interaction we are interested in. Such a situation occurs when a user has already narrowed down a video archive using such meta-data and there is a considerable amount remaining to be searched or the user X  X  information need cannot take advantage of existing metadata to narrow the search scope. In these cases we need to use search and/or brows-ing techniques based directly on the video content and we refer to this as content-based video retrieval .
Contemporary approaches to content-based video retrieval usually involve a combination of techniques including text matching against the spoken dialogue or against video text OCR, image matching against shot keyframes, or using semantic features to index video shots. Text matching is the traditional approach of com-paring a user X  X  input query against text obtained from the video such as from closed captions (if available), recognized speech, or against video OCR, which is text recognized from a video frame where it appears as a caption or as part of the actual image itself like a road sign or name on the side of a building ( Sato, Kanade,
Hughes, &amp; Smith, 1998 ). The usual text-based IR approaches can be used and term weighting approaches from text IR such as BM25 have been applied, as well as attempts to develop language models for video text retrieval. Text-based video retrieval works adequately for certain types of user search where the focus of the search is the actual item under discussion in the dialogue from the video, but when the focus of the search is to be found in the visual content then image matching and indexing techniques based on semantic features such as indoor, outdoor, or the number of faces appearing, are more suitable.

One of the first steps in video analysis is automatic structuring of video into units referred to as shots . These correspond to the video footage taken by a single camera in a contiguous time period and may involve camera motion (zoom, pan, tilt, etc.) and/or the movement of objects in the video. Boundaries between shots can be  X  X  X ard X  X , where the last frame of the finishing shot is followed directly by the first frame of the incoming shot, or the shot transition can be  X  X  X radual X  X  as in fades, cross-fades, etc. Automatic techniques for shot boundary detection are now very reliable with greater than 95% precision and recall being reported for hard cuts and greater than 80% precision and recall for gradual transitions using some techniques ( Smeaton, Over, &amp; Kraaij, 2004 ).

Shot boundary detection is usually accompanied by the automatic extraction of a keyframe or single image as a representative for each shot. In video retrieval, this representative image is selected using somewhat inju-dicious mechanisms, the most popular being to simply choose the keyframe in the middle of the shot! Key-frames, however they are chosen, are used in video retrieval for two purposes: to present a visual summary of a shot to a user during video browsing, and to act as a representative of a shot X  X  contents for image-based shot retrieval. In the case of using keyframes to browse shots this can include browsing sets of keyframe images representing sets of shots. Other approaches to browsing sets of shots such as fast-forward and advanced displays of keyframe images have been tried, especially in the interactive search task in the annual
TRECVid benchmarking activity ( Smeaton et al., 2004 ). In the case of image-based shot retrieval then using the simplest kind of image matching will match images on the basis of low-level image features such as global or local colour, texture, edges or perhaps shapes ( Smeulders, Worring, Santini, Gupta, &amp; Jain, 2000 ). However using the poor approaches to keyframe selection that we use, it is not surprising that matching a user X  X  query image(s) against keyframes using low-level features can be successful only if the user X  X  information need can be captured in a still image, and the keyframe is genuinely representative of a shot. Queries such as a  X  X  X ocket launch X  X  with a query image consisting of a blaze of flames coming from a rocket against a blue sky back-ground will be visually similar to keyframes taken from rocket launch footage, so there are cases when this type of retrieval works well.

Apart from text-based and image matching approaches, the automatic detection of semantic concepts and the relationship among these concepts and how they can be structured into an ontology has received much attention from the video retrieval research community. The common approach is to manually annotate a large corpus of shots as having feature X or not having feature X and then use a machine learning algorithm to learn an automatic classifier. Examples of features which have been used for this include  X  X  X ndoor/outdoor X  X ,  X  X  X uildings X  X ,  X  X  X ountryside/cityscape X  X ,  X  X  X eople present X  X , various types of camera motion,  X  X  X icycle X  X ,  X  X  X utomo-bile X  X ,  X  X  X aterside/beach X  X , and so on ( Rautiainen, Ojala, &amp; Sepp, 2004 ). Clearly the range of possible features which we could hope to automatically detect is enormous but building such concept feature detectors is a dif-ficult task for a number of reasons. Firstly, the definition of what constitutes a semantic concept is debatable.
Is a shot taken indoors but including outdoor scenery through a window indoor or outdoor? A shot of a park with trees and grass and a lake, but with a New York city skyline in the background, is it countryside or city-scape? How much of a bicycle needs to be visible for a shot to be deemed as featuring a bicycle? A second reason why building feature detectors for video retrieval is difficult is that the selection of a set of concepts which is broad enough to cover a large range of information needs yet discriminative enough to be useful in user searching, is an open question. Finally the task of collecting enough positive and negative examples of a feature concept and then learning a classifier is something that requires much effort, all of it manual, and thus is expensive. Nevertheless, if a concept feature detector is available and has been applied to a corpus of video with a high level of accuracy then it can certainly be useful in helping a users search. The way in which such semantic features can be most usefully used is to arrange the semantic features into an ontology, usually hierarchical in nature and to allow the user to navigate this ontology to help focus their information need ( Snoek et al., 2005 ). The problem with using semantic features and ontologies is that there are only a small number of classifiers built for these features and many have a level of accuracy which is less than that required for reliable retrieval. This is shown each year in TRECVid where the average precision of some feature detec-tions can be as low as 20% for the best-performing groups ( Smeaton et al., 2004 ).

One other type of video shot retrieval which has not received much attention to date is retrieval based on the presence, or absence, of given video objects. Sometimes we want to retrieve video shots based on an iden-tifiable object in the video such as a car, a dog, the Eiffel Tower or a motorbike. In such cases we can hope that the dialogue will mention that what is on-screen is a dog in a car beside a bicycle in front of the Eiffel Tower, but we cannot rely on this as it usually does not happen at all. We could also hope that reliably detected and useful features will help narrow down the corpus so that we can do keyframe browsing based on filtering shots as being outdoor and containing cars but we cannot rely on this either. Another approach is to find an exam-ple image of a dog or a car or the Eiffel Tower and match this example image containing our desired object(s) against all keyframes with the match being based on colour, texture or edges. This is likely to be useful only for very few shots where the colour, texture and edges are very predictable such as a rocket launch against a blue sky but the range of colours, textures and edges possible in a shot of the Eiffel Tower is huge. What would be really useful for this kind of retrieval is if we could specify one or more actual objects, segmented from their backgrounds, and use those as part of the query ( Sivic &amp; Zisserman, 2003 ), which is exactly what we do in this paper as we shall see later.

With a range of search modalities available for video retrieval, and each working best only on certain search types, clearly the best overall approach is to make as many modalities as possible available to a user and let the user use these, possibly in some weighted combination. A recent approach in ( Yan et al., 2004 ) has been to automatically assign a user X  X  search to one of four pre-defined search types, and for each search type the sys-tem will have previously been trained to determine the optimal combination of different retrieval tools such as those outlined above. In the case of ( Yan et al., 2004 ) this was done by training search types using TRECVid data ( Smeaton et al., 2004 ) which has known relevance assessments. This appears to be accepted as the best overall approach to video shot retrieval but it does not allow the user to interactively adjust the dynamics of the search depending on whether he/she prefers to use one modality more than another.

In the work reported here we are interested in how a user chooses for himself/herself from among the dif-ferent search modalities on offer in video retrieval. We are particularly interested in how a user would use, or choose not to use, video objects as part of retrieval. With so much effort going into object segmentation and tracking, and the promise that object-based retrieval offers, we are keen to try to understand whether object-based retrieval will actually be useful in practice. In the next section we summarise prior work in video object segmentation and object-based retrieval. 3. Video objects: segmentation and retrieval
Segmentation, tracking and compression of objects in video underpins the MPEG-4 compression standard ( Ebrahimi &amp; Horne, 2000 ) and when it is achieved reliably, accurately and on a large scale, then object detec-tion is supposed to open up many interaction possibilities between users and objects appearing in vide. It will also offer large gains in video compression. When the MPEG-4 standard was specified in the late 1990s that was the goal and this remains the objective behind much current research in video and image processing, an objective which is proving hard to achieve because of the complex nature of object segmentation in the general case. We will now examine work in video object segmentation and then look at how this can be used in object-based retrieval. 3.1. Video object detection and segmentation
Despite much focus and attention, fully automatic object detection, segmentation and tracking across video is not yet achievable for natural video though detecting objects in images and video is typically used by humans to understand visual media and is thus very important for content access to such media ( Duygulu, Barnard, de Freitas, &amp; Forsyth, 2002 ). Available object segmentation systems usually require user involvement and are thus semi-automatic. What this means in practice is that such systems are assisted by a human who draws either a rough sketch or an exact boundary contour around an object of interest and the system will detect and segment this object, separate it from its background, and track it temporally through the remainder of the video shot. Fully automatic detection of objects has had some success ( Adamek &amp;
O X  X onnor, 2003 ), but only in limited shot types such as in-studio news anchorpersons and field sports players against grass backgrounds. It is limited by the availability of training data and of models of objects which work across different shot types.

Object based feature extraction, which is an important part of the work in this paper is an enabling tech-nology for the type of video retrieval we are interested in. It is primarily based on edge information computed from the image or from a video frame. An edge is defined as a sharp variation in intensity or colour at some local part of an image which corresponds to the boundaries between objects appearing in the image such as the boundaries between trees and sky, people and grass, cars and buildings, etc. One of the main reasons why edge is more important than other image features for segmentation is that the colours of an object can vary greatly under different lighting conditions making object detection a haphazard affair, and while colour information is useful for object detection, it is extracted separately. The following requirements for object detection from nat-ural video explain why this is the case:
Object detection must be invariant to scale as an object could fill the entire frame or be in a small area of the frame; Object detection must be invariant to rotation as a matching object in the image might be rotated along the X -, Y -or Z -axes and we need to take account of this during detection;
Part of an object may be occluded either by another object or the object itself and so a detection algorithm needs to be able to fill in the missing data and in many cases there could be more than one possibility;
Object detection must be able to cater for variations of the same object type which generate very different comparisons, such as a car object for example. If we picture all the different types and colours of cars we can think of it is easy to see how different object matches would be produced and indeed one approach to object segmentation is based on using templates for shapes of objects as viewed from different angles;
Detection will have to deal with noise such as edges from other objects in the image and/or errors in the edge detection process itself;
Images and frames are 2D representations of a 3D world without depth information. For a symmetrical object like a vase it does not matter what angle we look at it from as the shape will always be the same, but most objects are not like this;
Each object for detection requires a number of positive and negative training examples which illustrate true and false occurrences of the object.

In light of the difficulties outlined above it is not surprising to see that detection of all possible object types is generally considered a computationally infeasible problem. Broadly speaking, there are three classes of approach to object segmentation. The first approach decomposes an input image into a tree or graph of constituent sub-regions and then performs graph and sub-graph matching to detect objects such as demon-strated in ( Tu, Saxena, &amp; Hartley, 1999 ). In this approach we detect a bicycle, for example, by detecting wheels and a frame as sub-components which are matched against a graph for a typical bicycle. The second approach is based around computing some kind of correspondence measure between regions of an image and models of the objects being detected. A number of templates of the object to be detected are con-structed in advance and possible matches against image regions are extracted from the input image. Shape matching is often used during the comparison/detection to compare each image against the available templates ( Adamek &amp; O X  X onnor, 2003; Doufour &amp; Miller, 2002 ). The third approach is the most crude and is to try identify objects by their active movement. The idea behind this approach is that foreground objects will move from frame to frame to a greater degree than their backgrounds which will exhibit less movement.

In this paper we take the second approach to object segmentation based on template matching because we carry out our experiments on animated video content, and this approach performs better on such animated content compared to natural video for the following reasons: (1) In animated content, the shape and colour of the characters, objects and backgrounds do not tend to (2) In animated content, the characters usually have a limited number of detectable and repeatable facial (3) In animated content the boundaries for each of the objects are very clear and detectable because a
To illustrate these differences, Figs. 1 and 2 show a set of edges automatically detected from an animated image and from a natural scene which have more or less the same composition and arrangement of people, yet the edges from the animated image are much more pronounced and clear. If we were to take the first approach to segmenting objects then we would have a pre-defined template for  X  X  X ead and shoulder X  X  recognition and we would search for occurrences of eyes, mouths, head and shoulder silhouettes, etc. which we would map onto the pre-defined template. in the second approach we would have a template for an entire head-and-shoulders view and we would search for occurrences of that template and all other templates, in the image or video. In the third approach we would detect any movement of the characters in the sequence of frames and use this as a basis for object segmentation. One of the disadvantages to the approach of using animated video content and template matching for object detection is that the set of objects which are to be segmented, and then used in user querying, must be pre-defined. This reduces user querying using objects to just those set of pre-defined objects and in a more realistic object-based retrieval scenario a user would need to identify, or perhaps seg-ment, objects at query time. 3.2. Video object retrieval: related work
Despite the technical difficulties with object segmentation there are some examples of work done which sup-port object-based retrieval from video. Some of these are briefly summarised here.

The QIMERA system is a collaborative video segmentation platform that incorporates natural object detection using a number of colour and texture features to identify object boundaries. QIMERA also demonstrates user-assisted object segmentation on a number of standard MPEG test sequences such as that in Fig. 3 ( Adamek &amp; O X  X onnor, 2003 ).

In ( Hohl, Souvannavong, Merialdo, &amp; Huet, 2004 ) the work presented segments images into regions, visu-ally, and refers to a set of homogeneous regions as an object, though these are not objects in the semantic sense. An  X  X  X bject X  X  retrieval system based on this approach is evaluated on a short cartoon rather than on nat-ural content, with a ground truth of object occurrences and in ( Hohl et al., 2004 ) they demonstrate the accu-racy of an approach to locating objects in video based on a user query specified as a set of regions. Similarly in ( Erol &amp; Kossentini, 2005 ) there is another proposal for locating arbitrary-shaped objects, this time based on shapes and shape deformations over time, with another set of evaluation figures on measuring the accuracy of locating query objects in video sequences as measured against humans locating the same objects.

Sivic and his colleagues at the University of Oxford take an approach whereby the user also does the object segmentation in the query and this is then matched and highlighted against similar objects appearing in shots.
The approach uses contiguous frames within a shot to improve the estimation of objects and addresses changes in viewpoint, illumination and object occlusion. The approach is illustrated working on the movie
Groundhog Day in ( Sivic, Shaffalitzky, &amp; Zisserman, 2004 ) and a more detailed presentation of the image pro-cessing can be found in ( Sivic &amp; Zisserman, 2003 ) on the movie Run Lola Run where again, the task is to locate a user-specified visual object.

Work reported in ( Liu &amp; Ahuja, 2004 ) addresses object segmentation and retrieval based on a complex approach to motion representation and concentrates on the object tracking without actually segmenting the semantic object (whether a person, a bird or a car). The paper reports some preliminary experiments where similar objects which have a similar trajectory to the query clip and appear in similar video compositions, can be located, though a thorough evaluation is needed. Similar work is reported in ( Smith &amp; Khotanzad, 2004 ) where they automatically segment video frames into regions based on colour and texture, and then track the largest of these  X  X  X bjects X  X  through a video sequence, though they are more like blobs than objects. A user query is not a still object but an object appearing in a query video clip. The work reported in ( Smith &amp; Khotanzad, 2004 ) will search using a query video clip to find video sequences similar in terms of object motion, as well as edges, texture, and colours and this has been tested on a corpus of natural video. In this paper we are more interested in information retrieval from video where the user is not quite sure what their query object might look like, though video object detection as described here, will be part of such a system.

Finally, while most of the work mentioned above is quite recent and suggests that object-based video retrie-val is a new development, this is not quite true, with work on video-object retrieval being reported more than 10 years ago (e.g. Oomoto &amp; Tanaka, 1993 ). Clearly the notion of using video objects for retrieval has been desirable for some time, but only very recently has technology started to allow even very basic object-location functions on video. Our interest in video object retrieval is partly in the challenge of the technique itself, but more so in the way in which it can be used by users in searching for video information, and in trying to mea-sure the actual advantages it offers in searching. In the next section we shall introduce the system for video retrieval we have built, including details of how it segments objects to support object-based retrieval. 4. The F X   X  schla  X  r X  X impsons system
The television show  X  X  X he Simpsons X  X  has been a phenomenon since it started broadcasting in 1990. With nearly 360 episodes completed in 16 seasons, the show is hugely popular. The 360 episodes of the Simpsons add up to 144 hours or 6 full days of content. The basic premise of the show revolves around the exploits of a dysfunctional American family, the Simpsons, and their hometown of Springfield. The Simpsons family mem-bers are Homer, Marge, Bart, Lisa, Maggie, Grandpa Abe Simpson, Patty and Selma. Over the years, the show has evolved both in terms of the visual quality of the animation and the storylines. The first two years of the show focussed on Bart as the main character but in the second year this focus shifted to Homer who became (and remains) the most popular character. After the first season, Homer became more stupid and con-tinues to get worse as seasons progress. After the first season Lisa X  X  character changed too, becoming more intelligent.

We have built a complete video indexing, searching and browsing system for a corpus of video of the Simp-sons. For our corpus we used 52 episodes (20.8 hours) taken from seasons 2 and 3, and from themed DVDs covering seasons 4 X 12. These were taken from DVDs and transcoded into MPEG-1 for image analysis, stor-age and playback. Extraction of the closed captions presented a challenge as the DVD subtitle text was stored as a series of images and therefore, optical character recognition (OCR) was required to convert the image text into machine-readable text.

The video system had several general requirements, including the ability to support searching based on a range of search modalities and the ability to support iterative relevance feedback. In preparing our video con-tent for indexing and search, we first applied a shot boundary detection algorithm to this corpus ( Browne &amp;
Smeaton, 2004 ) to determine the 20,529 shots regarded as units of retrieval. For each shot we identified a sin-gle keyframe as the frame in the middle of the shot and we extracted this as a still image. In terms of user searching, we support text search through the closed captions manually extracted from the DVD where they are available as tagged text files ( Browne &amp; Smeaton, 2004 ). Each word in the extracted text was associated with a shot based on the timing information. Stemming and stopword removal were then applied to the extracted text, and the remaining terms were indexed based on a standard TF*IDF method. Text comparison between a given text query and the text used to index each shot was based on our implementation of the vector space retrieval model ( Ferguson, Gurrin, Wilkins, &amp; Smeaton, 2005 ) and the resulting comparison scores were ranked in descending order in order to generate a shot ranking from a text query.

In addition to supporting text search, we also support keyframe-based image matching whereby a keyframe can be used as a query to find similar keyframes based on visual appearance. In our system we used the fol-lowing four low-level visual features to index each shot keyframe and facilitate visual searching similar to how we perform keyframe matching in our participation in TRECVid ( Cooke et al., 2004 ): 4 region * 18-bin hue histogram, 9 region * average colour, 9 region * median colour, 4 region * 16-bin edge histogram.

As the key interest in the work reported here is to explore how useful object-based retrieval can be as part of video shot retrieval we decided to automatically detect the presence of the major characters on-screen as video objects. Our approach was to have a number of shape templates for each character manually selected from representative images, to extract all yellow coloured objects from each keyframe and then to match these yel-low objects against the character templates. This procedure for object detection is straightforward because all
Simpsons characters are always yellow and have a small number of facial poses each. Each of our character templates is compared against all yellow objects in each keyframe using a shape matching algorithm which generates a number of comparison scores and scores above a pre-defined threshold are regarded as a positive match. Colour plays an important part in the shape matching process, as positive matches require yellow areas to match the templates. All other colours are ignored and this reduces false detection considerably ( Adamek &amp; O X  X onnor, 2003; Browne, Smeaton, O X  X onnor, Marlow, &amp; Berrut, 2000 ).
 Fig. 4 is a screenshot taken from the template-based shape matching application we used ( Adamek &amp;
O X  X onnor, 2003 ). At the bottom of the screen, we can see 5 members of the Simpsons family and their representative templates. At the top left of the screen we can see the keyframe image that is being compared while the top right shows positive template matches for Homer, Lisa and Marge. The choice of which Simpsons characters to detect automatically was based primarily on their popularity and on the availability of representative examples for template matching. The template matching technique needs multiple training examples to detect each character, and the overall accuracy of the detection needs to be high in order for sub-sequent searching to be reliable. To measure the accuracy of the object detection we used a development cor-pus of 12 episodes (4.8 hours, 6525 shots) and manually tagged each shot which had any of our target characters (objects). Table 1 gives a list of the 10 characters detected automatically using this approach, their occurrence in the development corpus, and the accuracy of detection measured against a manually tagged ground truth from the development corpus using precision and recall. The results of our object detection accu-racy test on the development corpus shows that precision is acceptably high for almost all characters with an average overall value of 0.947 (weighted by the number of occurrences per character/object), but recall is low at 0.421, indicating we are missing many character detections. These results (high precision, low recall) are similar to those observed in other work on video object detection (e.g. Erol &amp; Kossentini, 2005; Hohl et al., 2004 ), and are mostly due to the character not being present or being occluded in the keyframe but pres-ent in some other part of the shot.

The F X   X  schla  X  r X  X impsons system we built allows a user to search for video shots using three different modal-ities namely by text matching, by similarity between a query image and a shot keyframe, and by the presence or absence of combinations of video objects, the 10 major characters in the Simpsons in our case. The three different modalities, or any combination of them can also be combined into one search as most often happens in practice once a search has commenced. At each iteration of a search, whatever input the user has provided for the query whether text, positive or negative example frames or positive or negative example objects, sep-arate shot rankings are generated for each component, text, image and object, and these three separate rank-ing are combined using late fusion ( Mc Donald &amp; Smeaton, 2005 ). Fig. 5 shows a screen from the system showing the shot context screen in which the 5 shots preceding and following a given ranked shot are pre-sented with the ranked shot in the centre of the screen. For this given ranked shot any video objects detected are presented with a white border and can be added to the query as positive or negative feedback by clicking with either mouse button. The F X   X  schla  X  r X  X impsons system is described in more detail in ( Browne &amp; Smeaton, 2005 ). 5. Results of usage analysis
In order to assess the importance and usefulness of video objects as part of a user X  X  query strategy, we con-ducted a series of user experiments with our F X   X  schla  X  r X  X impsons video retrieval system. The user interacts with the system by starting a search, which can use either text, a keyframe, or an object from a keyframe as the query and this generates a ranked list of shots. The user can then browse a shot X  X  context (surrounding shots) and add other shot keyframes as positive or negative feedback in which case an image similarity measure is used to re-rank unseen shots, or a user can add a segmented object (a character from the Simpsons) as positive or negative feedback in which case a re-ranking is also generated. In this way a user X  X  query can be composed of a combination of text, keyframe(s) and/or object(s). Our interest here is in examining the usage of different types of search features, namely text, image and object searching.

To evaluate our system, 15 users, not familiar with the system but knowledgeable about the Simpsons, each ran the same 12 topic searches. The users were a mixture of undergraduate and graduate students of comput-ing and were thus experienced computer users and all were either regular viewers of the Simpsons or had watched the programme on several occasions. We formulated the 12 topics shown in Appendix A which could be narrow (N), general (G), or broad (B) which we define as depending on the number of relevant shots to be found in the evaluation corpus of 20,529 shots. Appendix A also indicates whether text searching (T), low-level image keyframe matching (L), object-object matching (O) or simple keyframe context browsing (X) are likely to be useful for each of the topic types. The nature of these topics broadly reflects the nature of top-ics created in each year of the TRECVid experiments in terms of the mix of topic types (broad, general, etc.).
For each topic we manually determined relevant shots from within the corpus in order to establish the ground truth for retrieval. While this was time-consuming, given our familiarity with the content we believe we have ensured a high degree of completeness in these relevance judgments.

Our 15 users were each shown a demonstration of the system and they then they went through a training period by searching for 3 topics under the supervision of the system developer. Each user then searched for each of the 12 topics in turn with 7 minutes allocated per topic. Users were not grouped together for either the demonstration or search sessions and performed these solo. Users were allowed to use any search features (i.e. text-, image-or object-based querying) individually or in combination, that they considered suitable at any point of their iterative search process. Topic ordering was rotated among users in a Latin squares, follow-ing the guidelines for interactive search in TREC and in TRECVid. The rationale for a time-constrained search was to simulate the case where an enduser is required to find some shots, but not necessarily all shots, which satisfy some search constraint and the answer set is required as soon as possible. In the TRECVid inter-active search task the time limit is set to 15 minutes, but we felt that with a corpus of just under half the size of a TRECVid search corpus we should also reduce the allocated search time. All searches by all users for all topics used up the full 7 minutes.

With a maximum of 7 minutes per topic it is clear that for some topics, such as topic 11 with 631 relevant shots, not all these could possibly be found whereas for others such as topic 6 with only 22 relevant shots, locating all shots is possible. In the case of the broad type of topic the aim is to find as many of the 631 shots as a user can (since all relevant shots are treated equally) and so high recall is not possible, while for topics like topic 6 a combination of high precision and high recall is a realistic target. That means an overall averaged performance of both high precision and high recall is not attainable. In Fig. 6 , we see the precision for each topic i.e. the proportion of relevant shots in the set retrieved for each topic. Average recall across the topics is approximately 28% reflecting the fact that there are very many relevant shots and only a limited time to com-plete searches. However, in this paper we are primarily interested in what search features were used, and espe-cially how object-based retrieval was used in the 180 searches (15 users, 12 searches each) which were logged.
Table 2 summarises those results. As we can see, text search was the most common search type with keyframe matching following close by. Object-based searches were used in 54% of searches and there was good use of objects as both positive and negative relevance feedback judgments whereas keyframe image similarity was used mostly for positive rather than negative feedback. The distribution of object searching across the 12 top-ics indicates it was naturally highly concentrated among topics in which named characters were also among the detected objects (topics 3, 4, 5, 6 and 12). Table 3 shows the total number of search types at each iteration over the 180 searches, this is shown graphically as Fig. 7 . Each time a user marks an object as a relevant or non-relevant object, or adds positive or negative feedback for a shot keyframe, or changes the content of their text search by adding or deleting terms, their query expands and the iteration level increments. When the user restarts with a new search then the iteration level is reset to zero. Looking at the search types we can see that users try a number of different query searches during the 7 minutes as the total number of searches (1690) is far greater than the total number of topics completed by users (180). The maximum number of iterations com-pleted by a user is 22, and the most popular search strategy employed by users is to start with a text search followed by keyframe match in the subsequent iteration, while the second most popular method is to start with a text and then an object search. This can be explained by the fact that users like to start doing visual search as soon as they can rather than have an initial text search followed by another text search, for example. Eighty percent of feedback judgments are in the first 8 iterations while 40% of feedback is in the first 3 judgments. 6. Conclusions
In this paper we have combined text-, image-, and object-based searching into an iterative video shot retrie-val system, and evaluated its interactive use with users in a controlled experiment. This was done in an attempt to gauge how useful object-based search is in comparison with keyframe matching and text searching. Fifteen users each completed 12 different searches, each in a controlled and measured environment with a maximum 7 minutes time limit to complete each search. The study does have limitations in terms of the relatively small size of the sample and the artificiality of the search tasks but does reveal some interesting patterns on how object-based video searching can be used.

The findings from the log data analysis shows that text-based search is the most popular method for initial querying and was used in 97% of query topics. This confirms our expectation and we have found that the retrieval performance of text quickly gets people to relevant areas of the content at which point keyframe (image) retrieval can be used. Object-based searching was performed in over 50% of queries, and this shows that when image objects are available and appropriate for the query topic, users will use them.

This result goes some way towards validating the approach of allowing users to select objects as a basis for searching video archives when the search dictates it as appropriate, though the technology to do this on nat-ural as opposed to animated video, is still under development for scalable applications. Our approach of using animated cartoons to allow us to do object interaction, allowed users in our work to manipulate video objects as part of their searching and browsing interaction.
 Acknowledgement
The authors wish to acknowledge the support of the Informatics Directorate of Enterprise Ireland. This work was supported by Science Foundation Ireland under Grant No. 03/IN.3/I361.
 Appendix A. Evaluation query topics
Table to illustrate query topics and their nature where topics are narrow (N), general (G), or broad (B) and the potentially useful search modes are text searching (T), low-level image keyframe matching (L), object-object matching (O) or simple keyframe context browsing (X).

Topic ID Relevant shots Topic type Topic description 1 149 G(TL) Find shots of any Simpsons character smoking. Shots where the 2 153 G(TX) Find shots of people hurt and in pain. The sound must be audible 3 43 N(OLT) Find shots of Bart, Lisa and Maggie together in the sitting room References Appendix A ( continued )
Topic ID Relevant shots Topic type Topic description 4 221 G(OLT) Find shots of Homer working at the nuclear power plant.
 5 225 G(OX) Find shots of Bart sad, upset, or worried X  X motion should be 6 22 N(O) Find shots that show Bart and a teacher. Valid results will 7 45 N(TX) Find shots showing an explosion; sound must be audible 8 349 B(TLX) Find shots that show Homer X  X  neighbour Flanders 9 62 N(TLX) Find shots of the  X  X ew X  owners of the Springfield 10 155 G(TLX) Find shots of the Simpsons character that proposes to Marge X  X  11 631 B(TL) Find shots of video that feature any sporting event. A valid 12 99 N(OL) Find shots of Homer and Marge in the Kitchen Further reading
