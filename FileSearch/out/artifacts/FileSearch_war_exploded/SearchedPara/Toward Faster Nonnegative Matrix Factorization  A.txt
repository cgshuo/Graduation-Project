 Nonnegative Matrix Factorization (NMF) [12, 17] has attrac ted much attention during the past decade as a di-mension reduction method in machine learning and data minin g. NMF is considered for high dimensional data where each element has a nonnegative value, and it provides a lower rank approximation formed by factors whose vision [14], and cancer class discovery [4, 9].
 nonnegative elements such that A  X  W H . The factors W and H are commonly found by solving the optimization problem: hopes to find a local minimum.
 on the alternating nonnegative least squares (ANLS) framew ork. They used a subroutine for the nonnegativity every limit point of the sequence of solutions in iterations is a stationary point [15]. ANLS framework. Previous NMF algorithms using the ANLS fram ework include the active set method [10], projected quasi-Newton methods apply traditional techniq ues for unconstrained optimization with modifications that improve the block principal pivoting method and then bu ild a new algorithm for NMF. exhibits the best performance for NMF computation.
 and their interpretation are provided in Section 5. We concl ude the paper in Section 6 with discussions. We describe the alternating nonnegative least squares (ANL S) framework for solving Eqn. (1). The ANLS frame-work is a simple Expectation Maximization type algorithm wh ere variables are divided into two groups that are updated in turn. The framework is summarized as follows. (1) is non-convex, the subproblems in Eqns. (2) are convex pr oblems for which optimal solutions can be found. subproblems in Eqns. (2), and we will revisit this point in la ter sections.
 updates, at each iteration, a solution which is not optimal f or the subproblem.
 a straightforward way. Faster algorithms were recently dev eloped by Bro and de Jong [3] and Van Benthem and Keenan [1], and Kim and Park made them into a NMF algorithm [10 ].
 solution to the feasible nonnegative orthant at every itera tion.
 We now describe this method in detail. this to handle multiple right-hand sides efficiently. For the moment, let us focus on the NNLS problem with a single r ight-hand side vector which is formulated as basic building block for an algorithm for Eqns. (2).
 solution of Eqn. (3).
 Let x submatrices of C with corresponding column indices. Initially, we assign x y of x F and y G is done as follows. Algorithm 1 Block principal pivoting algorithm for the NNLS with single right-hand side (Eqn. (3)) One can first solve for x complementary basic solution if it is obtained by Eqns. (5).
 solution ( x Eqn. (4c) does not hold. Formally, we define the following ind ex sets and update F and G by the following rules: where  X  H If  X  H number of infeasible variables (i.e., | H rule [20] where only one variable is exchanged. As soon as the backup rule reduces the number of infeasible terminates in a finite number of iterations [20].
 variable x variable x initial solution with x  X  0 while the active set algorithm does.
 rule mentioned earlier. We used three as a default value of p which means that we can try the block exchange algorithm is very efficient for the NNLS [20, 5]. we can reduce the computational effort to solve Eqn. (9). The subproblems in Eqns. (2) comprise of NNLS problems with m ultiple right-hand side vectors. Suppose we need to solve the following NNLS problem: 1) for each right-hand side vector b Jong [3] and Van Benthem and Keenan [1] suggested these ideas for active set methods, and we employ them in the context of block principal pivoting methods here.
 (5a) by a normal equation and Eqn. (5b) can be rewritten as Note that we only need to have C T from NMF, the matrix C is typically very long and thin. In this particular case, con structing matrices C T the beginning and reuses them in later iterations. One can ea sily see that C T storage needed for C T C and C T B is not an issue.
 Alg. 1 for many right-hand side vectors. At each iteration, w e have the index sets F j  X  X  1 ,  X  X  X  , r } , and we must compute x F that share the same index sets F solving Eqn. (9). Figure 1 illustrates this reordering idea .
 right-hand sides case. for NMF. Implementation issues such as a stopping criterion are discussed in Section 4.2. Algorithm 2 Block principal pivoting algorithm for the NNLS with multip le right-hand sides (Eqn. (8)) applications [9]. Sparse NMF [9] is formulated as, when the s parsity is considered for H factor, We can solve Eqn. (11), as shown in [9], by solving the followi ng subproblems alternatingly: where e where I As shown in [10], Eqn. (12) can also be recast into the ANLS fra mework. We can iterate solving where 0 until convergence.
 (12), and therefore, can be used for faster sparse or regular ized NMF as well. the papers where they are presented. We compare the following algorithms for NMF. Deciding when to stop a NMF algorithm is based on whether we ha ve reached a local minimum of the objective function k A  X  W H k (KKT) optimality condition for Eqn. (1).
 These conditions can be simplified as where the minimum is taken component wise [6]. We use a normal ized KKT residual as where of the sizes of W and H . Using this normalized residual, the convergence criterio n is defined as where  X  comparing several algorithms. matrix W and k  X  n matrix H with 40% sparsity. Then, we computed A = W H and added Gaussian noise to each element where the standard deviation is 5% of the average magnitude of elements in A . Finally, we normalized synthetic matrices that have latent sparse nonnegative fac tors.
 news articles from various sources such as NYT, CNN, VOA, etc . in 1998. The corpus is manually labeled across TF.IDF indexing and unit-norm normalization [16]. We obtai ned a 12617  X  1491 term-document matrix. grey level. We obtained 10304  X  400 matrix. principal pivoting algorithm in MATLAB. For other existing NMF algorithms, we used MATLAB codes presented Pentium4 Xeon EMT64 machines with Linux OS. shared in all algorithms, and the average results using 10 di fferent initial values are shown in the Table 1. in worse approximations as the residual values show; when k = 20 , multi and als gave larger average residuals compared to ANLS type algorithms. Since the number of iterat ions exceeded the limit and the execution times were among the slowest, we did not include these algorithms i n the following experiments. more or less similar to each other except in projgrad method. This is because that projgrad and projnewton the tolerance value. On the other hand, activeset and blockpivot exactly solves the NNLS subproblem at every iteration. The difference might lead to a variation in the nu mber of iterations of their NMF algorithms. in bold type. For slow algorithms, experiments with larger k values take too much time and are omitted. The residual is computed as k A  X  W H k execution time of lsqnonneg as shown in Table 1 was much longer than in other ANLS algorith ms. Among remaining ANLS type algorithms, i.e., projnewton , projgrad , activeset , and blockpivot , projnewton method These results imply that projnewton is slower in solving the subproblems in Eqns. (2). 20, the execution time required for activeset or projgrad was comparable to that of blockpivot . However, as k becomes larger, the difference between blockpivot and the other two growed to be nontrivial. As these three algorithms show the best efficiency, we focus on comparing th ese algorithms with large real datasets below. synthetic datasets, we focused on comparing the three algor ithms.
 blockpivot is generally superior to the other two algorithms.
Table 2: Experimental results on 12617  X  1491 text dataset with = 10  X  4 . For each k , all algorithms were executed with the same initial values, and the average results from using 10 different initial values are shown in the table.
 The residual is computed as k A  X  W H k time (sec) 5 107.24 81.476 82.954 iterations 5 66.2 60.6 60.6 residual 5 0.9547 0.9547 0.9547 datasets and different problem sizes although they are both inferior to blockpivot in all cases. trend is what we expect from a dimension reduction method, as mentioned in Section 2. We emphasize that the long and thin structure of the NNLS problems arising from NMF is a key feature that enables us to use speed-up techniques explained in Section 3.2 and consequently gives the successful experimental results of blockpivot . 2. Note that projgrad was comparable to or faster than blockpivot when a loose tolerance was given. When approximation by a tight tolerance. In this paper, a new algorithm for computing Nonnegative Mat rix Factorization (NMF) based on the alternating nonnegative least squares (ANLS) framework is proposed. Th e new algorithm is built upon the block principal Figure 2: Execution time with respect to tolerance values on 12617  X  1491 text dataset. All algorithms were mulations such as sparse NMF or regularized NMF. Experiment al comparisons with most of the NMF algorithms efficient method for computing NMF.
 remedy this problem making these algorithms generally appl icable for computations of NMF. The work of authors was supported in part by the National Scie nce Foundation grants CCF-0732318 and CCF-0808863, and the Samsung Scholarship awarded to Jingu Kim. A ny opinions, findings and conclusions or rec-National Science Foundation.

