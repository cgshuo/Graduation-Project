
The one-class classification problem is a type of unsuper-vised learning problem. A boundary that separates a target class from other classes is estimated from an unlabeled train-ing dataset in which a large part of the examples included in the dataset is thought to belong to the target class. There are many applications for one-class classifiers, such as outlier detection, anomaly detection, novelty detection, concept learning, and so on, and a lot of research have been done in these areas [1], [2]. Among these, anomaly detection is one of the most actively addressed problems. The basic idea of anomaly detection with one-class classifiers assumes that there is a system that we want to monitor in order to check whether or not the system operates without any problems: 1) Collect data that characterizes the system when the system is operating normally, 2) Estimate a one-class classifier from the collected dataset to obtain a region of data inside which the system is considered to be working normally, 3) Monitor the system with the estimated classifier. An alert warning is issued if the classifier detects the data that lies outside the normal region, which indicates that something is wrong with the system. In such a situation, the training dataset typically increases because it is not difficult to collect while the system is believed to be operating normally. However, the training dataset may contain some abnormal data, because it is difficult to confirm manually whether the system has worked normally during the data collection. Therefore, a one-class classifier must be scalable and robust toward noises in a training dataset. Another difficulty comes from the fact that abnormal data is rarely available in anomaly detection, and even if some abnormal data is available, it is considered to be strongly biased, because when the system is relatively complex, it is almost impossible to list every possible fault of the system beforehand. There is a risk of estimating a biased classifier if we use biased abnormal data to tune the classifier. As a result, it is better to adjust a classifier using the training dataset only, which is considered to contain a large number of normal examples.

In this paper, we propose a novel approach for one-class classification problems in which a one-class classifier is estimated as a Boolean function, which is represented by a binary decision diagram. In the proposed method, each item of data in a training set is converted into a logical form and stored into a Boolean formula, one after another. After all the data is stored into a formula, we approximate it so that the data is described optimally in terms of the minimum description length principle. The operation of storing data into a formula and approximating the formula is done quite efficiently through binary decision diagram manipulations. The proposed method is parameter-free [3], because it has only one parameter to be tuned, and it is possible to select the parameter from an unlabeled training set via the minimum description length principle. In addition, the proposed method is robust toward noises in a training set.

This paper is organized as follows: Section II defines the problem considered in this paper and intuitively describes the basic idea of the proposed method. Section III gives an overview of binary decision diagrams, and Section IV de-scribes an efficient implementation of the proposed method based on binary decision diagrams. Section V describes a parameter selection via the minimum description length principle. Section VI shows some experimental results. Re-lated works are discussed in Section VII, and Section VIII summarizes this work and discusses future work.

In this section, we define the problem to be considered in this paper and illustrate the basic idea of our approach. Note that the algorithms mentioned in this section are used only for explaining the idea of our approach; their efficient implementations are discussed in Section IV. A. Problem Setting
Let D be an unlabeled dataset defined upon a set of u continuous attributes x =( x 1 ,...,x u ) and v categorical attributes y =( y 1 ,...,y v ) . Assume that D includes N data and that the i -th data in D is denoted by ( x ( i ) ,y ( i ) 1 ,...,N ) . Let | y i | ( i =1 ,...,v ) be the number of elements included in the domain of y i . We assume that there are no missing values in D .

We consider a situation where a large part of the data in D belongs to a particular data class A , and we want to construct a one-class classifier occ such that using only an unlabeled dataset D as a training dataset. B. Constructing an Initial Region Set
We consider a u -dimensional hypercube C whose side length is 2 m , where m is an arbitrary positive integer, and C is defined as [0 , 2 m ) u . Let  X  be a function that returns a u -dimensional unit hypercube that subsumes x , which is defined as follows: where  X  is the floor function. Figure 1 shows an example of C and  X  ( x ) , where u =3 ,m =3 , and x =0 . Let  X  be a map R u  X  R u , such that  X  ( x ( i ) )  X  X  for every i in { 1 ,...,N } . An example of  X  is given in the appendix. Let  X  be a minimal perfect hash function that receives a categorical data y as a key and returns a hash value. From the definition of y , the range of  X  has M = v i =1 | y i values. Assume without a loss of generality that the range of  X  is { 1 ,...,M } . When v =0 , which means that D has no categorical attributes, it is assumed that M =1 and  X  (  X  )=1 .

In our approach, we first construct a region set G from a dataset D using Algorithm 1. In Algorithm 1, G i becomes a u -dimensional region inside C that subsumes a projected data Algorithm 1 Construct a region set G from D 1: for i =1 to M do 3: for i =1 to N do 5: return G = { G 1 ,...,G M }  X  ( x ( j ) ) if  X  ( y ( j ) )= i for i =1 ,...,M and j =1 ,...,N . Then, G is defined as a set of G i ( i =1 ,...,M ) .
Given a region set G , a one-class classifier occ G can be constructed as follows: However, occ G works rarely if we use a region set G ob-tained by Algorithm 1 directly, because G will be too coarse if we set m too small. On the other hand, G will be too fine if we set m too large. In our approach, we set m large enough, and approximate G obtained by Algorithm 1 according to its hierarchical local density, which is mentioned in the next section.
 C. Approximating the Region
Let G 1 = { G 1 1 ,...,G 1 M } be a set of regions obtained using Algorithm 1. We approximate G 1 using Algorithm 2, where  X  is a one-dimensional threshold parameter whose domain is [0 , 1] . In Algorithm 2, for each i in { 1 ,...,M Algorithm 2 Approximate G with a threshold  X  1: for i =1 to M do 3: return G = { G 1 ,...,G M } 4: procedure A PPROX ( G i , h ,  X  ) 5: if h is a unit hypercube then 6: return G i 7: if D ENSITY ( G i , h )  X   X  then 9: return G i 10: Split h into 2 u hypercubes, denoted by h k ( k = 11: for k =1 to 2 u do 13: return G i 14: procedure D ENSITY ( G i , h ) 15: return we first focus on a region defined by C , and if the volume ratio of G i  X  X  toward C is larger than or equal to  X  , C added to G i , and the procedure is terminated. Otherwise, is split into 2 u hypercubes of a size, then, the region, which is defined by each split hypercube, is focused recursively until the split one becomes a unit hypercube. Figure 2 shows an example of G 1 i and its approximated results with various  X  , denoted by G  X  i , via Algorithm 2 where u =2 and m =3 . In Figure 2, the dark gray region means the initial region, the light gray region means the region added by Algorithm 2, and the bold square signifies a hypercube whose density is larger than or equal to the threshold  X  . The smaller the threshold  X  becomes, the larger the region G  X  becomes monotonically, as can be seen in Figure 2. Using G occ G is expected to be a sound one-class classifier.
Although the computations in the above mentioned al-gorithms might be complex, it is possible to process them efficiently using binary decision diagram-based implemen-tations mentioned in Section IV. For example, the problem with u =8 ,v =0 , and N = 34018 , which is described in Section VI, took 4.7 seconds for Algorithm 1 and 1.2 sec-onds on average for Algorithm 2 where we set m =16 , which means that the volume of C is 2 16  X  8 . Furthermore, we propose a method for selecting a proper threshold  X  from a given unlabeled training set via the minimum description length principle, which is mentioned in Section V.
A Binary Decision Diagram (BDD) is a compressed representation of a Boolean formula. In particular, a Reduced Ordered BDD (ROBDD) has an important feature that it is canonical for a particular formula and variable order, which has been applied in fields such as model checking [4]. Hereafter, we refer to an ROBDD as a BDD for the sake of simplicity. Another important feature of a BDD is that it is possible to perform most logical operations directly in the compressed form, and they can be implemented by polynomial-time algorithms [5]. For illustrative purposes, we consider a Boolean formula F 0 , which is defined by a set of four Boolean variables { b 11 ,b 12 ,b 21 ,b 22 } as follows: Figure 3 shows a BDD representation of F 0 , where the variable order is b 11  X  b 21  X  b 12  X  b 22 . In Figure 3, we refer to square nodes as constant nodes, ellipsoidal nodes as variable nodes, double-squared nodes as function nodes, solid arrows as true edges, dashed arrows as false edges and double-lined arrows as root edges. Variable nodes are labeled 2 ,..., 7 and their corresponding variables are on the left side, for example, both Node 5 and Node 6 correspond to b 21 . A variable node that is directed by a root edge is also called a root node. The constant nodes consist of the 1 constant node and the 0 constant node which are referred to as Node 1 and Node 0, respectively. A path from the root node to Node 1 corresponds to a conjunction of literals. For example, the path { Node 7, Node 6, Node 4, Node 1 } in Figure 3 corresponds to b 11  X  b 21  X  b 12 . A Boolean formula is given as a disjunction of these paths.

In some implementations of BDDs, such as CUDD [6], complement edges are used. BDDs can be expressed or operated more efficiently by this technique. Although the BDD-based algorithms mentioned in the following sections cannot be directly applied when complement edges are used in BDDs, it is possible to adapt them so that complement edges can be used. For the sake of simplicity, we explain our algorithms without the use of complement edges.
In this section, we propose efficient implementation of the algorithms mentioned in Section II.
 A. Constructing an Initial Boolean Formula
Let C ODE Y ( y ) be a function that receives a v dimensional categorical vector y and that returns a logical formula defined upon L Boolean variables { d 1 ,...,d L } that code y . For example, suppose that y = { y 1 ,y 2 } and the domain of y 1 and y 2 are { blue , red , yellow } and { ON , OFF spectively. By setting L =3 , the elements of the domains of y 1 and y 2 can be coded as { d 1  X  d 2 ,d 1  X  d 2 , d and { d 3 , d 3 } , respectively. Then, C ODE Y ( y ) returns d d 2  X  d 3 for y = { red , ON } .If v =0 , it is assumed that C
ODE Y (  X  )=1 . Let C ODE Z ( z ) be a function that receives a u -dimensional integer vector z = { z 1 ,...,z u } , which must be inside C , and returns a logical formula that codes z . For each i in { 1,...,u } , m Boolean variables { b i 1 ,... b are used to code z i in the manner of an unsigned-integer-type coding, where b i 1 is the most significant bit and b is the least significant bit. For example, C ODE Z ( z ) returns b 11  X  b 12  X  b 21  X  b 22 for z = { 2 , 3 } , where m =2 . We propose Algorithm 3, which constructs a formula F from a dataset D . In Algorithm 3, F is defined using the Algorithm 3 Construct a boolean formula F from D 2: for i =1 to N do 5: return F L + mu Boolean variables. Note that formula F constructed via Algorithm 3 is the informational equivalent of a region set G constructed via Algorithm 1, because C ODE Y ( y ) and C ODE Z ( z ) correspond to  X  ( y ) and  X  (  X  ( x )) respectively. BDDs are used to represent F and to perform logical operations in Algorithm 3. It is assumed that the variable order satisfies the following condition: [ d 1 ,...,d L ]  X  [ b 11 ,...,b u 1 ]  X  ...  X  [ b 1 m ,...,b where the variables inside the square brackets can be in arbitrary order. The ordering in Equation (3) is based on the significance of the bits representing the attributes. Using a BDD representation, a region set G can be expressed very efficiently, because a common pattern in G is com-pressed and expressed as a node in a BDD. For example, F 0 in Figure 3 represents the region shown in Figure 4a where the horizontal axis is z 1 and the vertical axis is z 2 because the minterms of F 0 are b 11  X  b 21  X  b 12  X  b 22 b and b 11  X  b 21  X  b 12  X  b 22 and they correspond to { z 1 { 3 , 3 } , { 3 , 2 } , { 2 , 2 } , { 3 , 0 } , and { 1 , 2 a minterm is a logical product of variables in which each variable appears once. In Figure 4a, there is a common as Node 3 in Figure 3.
 Given a formula F obtained via Algorithm 3, a one-class classifier occ F can be constructed as follows: occ F ( x, y )= which corresponds to a classifier defined by Equation (1). B. Approximating the Boolean Formula
Let F 1 be a Boolean formula obtained by Algorithm 3, and T F 1 be a BDD representation of F 1 . The number of minterms of a formula, which is expressed as a node in T
F 1 , is calculated recursively by the following equation [7]: where  X  X is the number of minterms of node X , and T and E are the destination nodes of the true and false edges of node N , respectively. The density of node N , denoted by  X 
N , is defined as The level of node N , denoted by  X  N , is defined as where w N is a Boolean variable to which node N corre-sponds. For example, Table I shows  X  N ,  X  N and  X  N of the nodes in Figure 3, where L =0 , m =2 , and u =2 . Figure 4 shows regions that nodes in Figure 3 represents where the horizontal axis is z 1 , the vertical axis is z a bold square means a common pattern caused by  X  X on X  X  care X  variables. For example, Node 4 in Figure 3 signifies a formula b 12  X  b 12  X  b 22 , where b 11 and b 21 are don X  X  care variables, and as a result, common patterns appear in four regions that are defined by b 11  X  b 21 , b 11  X  b 21 , b b 11  X  b 21 , as seen in Figure 4d. Note that the pattern in the region b 11  X  b 21 in Figure 4d also appears in the same region in Figure 4a, because it is possible to reach Node 4 from Node 7 through the edges that are signified by b 11  X  b 21 Figure 3.
 We propose Algorithm 4, which approximates a formula F through the direct manipulation of T F . Algorithm 4 is Algorithm 4 Approximate F with threshold  X  2: B DD A PPROX (the root edge of T F ,0,  X  ) 3: return T F 4: procedure B DD A PPROX ( e , l ,  X  ) 5: N X  the destination node of e 6: if N is a constant node then 7: return 8: if  X  N  X   X  and  X  N &gt;l then 9: Modify the destination of e from N to Node 1. 10: return 11: else 12: B DD A PPROX ( the true edge of N , X  N , X  ) 13: B DD A PPROX ( the false edge of N , X  N , X  ) 14: return an implementation of Algorithm 2, where the region set is given as a BDD. In Algorithm 4, C OUNT M INTERMS ( T F ) first calculates the number of minterms of each node in T based on Equation (5). Then, all the edges in T F are searched in a depth first manner and reconnected to Node 1 if the density of the destination node is greater than or equal to threshold  X  . This reconnection works as the manipulation of line 8 in Algorithm 2. The levels of nodes are considered, so that the reconnection happens only at the hypercube level. For example, Figure 5a shows a BDD, denoted by F 0 , which is obtained via Algorithm 4 from the BDD in Figure 3 with threshold  X  =0 . 75 . The edge from Node 6 to Node 4 in Figure 3 is modified by Algorithm 4 because the density of Node 4 is equal to  X  , and the level of Node 6 is smaller than that of Node 4, as seen in Table I. Figure 5b shows the region of F 0 . The hypercube whose density is equal to  X  in Figure 4a is added to the region by Algorithm 4, as seen in Figure 5b. Algorithm 4 works very efficiently, because the approximation is directly manipulated on a compressed form of a BDD. The order of the computational amount of Algorithm 4 is proportional to the number of edges in T F Therefore, Algorithm 4 can be executed in an applicable time if T F is small enough to store in the memory. A one-class classifier can be constructed by replacing F in Equation (4) with the approximated formula obtained by Algorithm 4.
 If we want to obtain the approximations with various  X  by Algorithm 4, it is enough to execute C OUNT M INTERMS (  X  just once at the start of the procedure and then exe-cute B DD A PPROX (  X  ) repeatedly with various  X  , because C
OUNT M INTERMS (  X  ) does not depend on  X  . Moreover, if the approximation with some parameter  X   X  results in the 1 constant BDD, the approximation with  X  which is less than  X   X  also results in 1 constant BDD from the monotonicity of the algorithm as mentioned in Section II.

Cross-validation-based techniques are often used to select a parameter of a model. However, it is difficult to apply these techniques when no labeled data is available, which is the situation considered in this paper. One possible solution is to apply likelihood-based cross-validation [8] if the model can be viewed as a probabilistic model. A main drawback of cross-validation-based methods is that they are computa-tionally expensive because the problem must be solved n -times if n -fold cross validation is applied. Here we propose a method for selecting a parameter of the proposed classifier via the Minimum Description Length (MDL) principle [9]. In the MDL principle, the optimal parameter is chosen so as to minimize the MDL, which is the sum of the code length that describes the model and the code length that describes the data encoded by the model. The MDL is defined as where L is the likelihood,  X  is the number of parameters describing the model, and  X  is the number of data. Unlike cross-validations, the problem needs to be solved only once to evaluate the MDL.

Let G 1 be an initial region set obtained via Algorithm 1, and let G  X  be an approximated region set obtained via Algorithm 2. In addition, let T F 1 and T F  X  be BDDs obtained via Algorithm 3 and Algorithm 4 that represent G 1 and G  X  , respectively. Now, let us consider that a set of unit hypercubes in G 1 is a data and T F  X  is a model that encodes the data. Given { T F 1 ,T F  X  } , the parameters in Equation (7) are calculated as follows: where V 1 and V  X  are the volumes (sizes if u =2 and lengths if u =1 )of G 1 and G  X  , which are equivalent to the number of minterms of T F 1 and T F  X  , respectively, and | T
F  X  | is the number of variable nodes in T F  X  . The number of data  X  is given by Equation (10) because the volume of G 1 is equivalent to the number of unit hypercubes in G 1 . The assumption behind Equation (8) is that the probability density is equally distributed inside the region G  X  , then the probability of each unit-hypercube is given as ( V  X  )  X  1 the likelihood is given as Equation (8) because there are V unit hypercubes in G 1 . The number of model parameters is given by Equation (9) because the information about the variable name and the destination nodes of the true edge and the false edge of each node is needed to store a BDD.
In this section, we present experimental results to demon-strate the working of the proposed method. First, we show the result with synthetic data; then, we describe the results with real datasets, which include Shuttle dataset and KDD Cup 99 dataset, which are taken from the UCI machine learning repository [10]. We implemented the proposed method a s a C program with the help of CUDD [6]. Although the algorithms mentioned in the previous sections assume no use of complement edges, they were modified so as to apply complement edges, because CUDD assumes them. Hereafter, we refer to the proposed classifier as the Binary Decision Diagram-based One-Class Classifier (BDDOCC). We used m =16 and  X  , which is described in the appendix, for the BDDOCC in the experiments. For comparison purposes, real problems were also solved by the One-Class Support Vector Machine (OCSVM) [11] and the Parzen Density Estimator (PDE) [12], where e1071 [13] was used to solve the OCSVM, and the PDE was implemented on R [14]. The Gaussian kernel is used in both OCSVM (3.20 GHz, 2 GB RAM). A. Synthetic Data
A dataset that has no categorical attributes and two continuous attributes was generated and named MoonStar. MoonStar consists of N = 10000 data, 95 % of which are randomly distributed inside the moon-shaped region as moon data, and the remainin g5%of which are ran-domly distributed outside the region as star data, as seen in Figure 6a. The star data is considered as a noise in the training set. The initial region G 1 is constructed via Algorithm 3. Figure 6 shows the result of approximations by Algorithm 4 with various  X  , each of them is denoted by G  X  . Figure 6 gives an intuitive explanation of how the proposed method works. Table II shows the numerical results of approximations where the number of BDD nodes after approximation, the size of the region after approximation, and the MDL are listed. We can see that  X  =10  X  4 . 9 achieves the lowest MDL in Table II. The BDD nodes are reduced from 46137 to 1153, and the region is expanded from 10 4 to 10 8 . 8 in size by Algorithm 4, when  X  =10  X  4 . 9 From Figure 6c, it seems that the approximated region with  X  =10  X  4 . 9 represents the moon-shaped region quite properly and the region outside the moon is kept small, which means that the proposed method is robust toward the noise in the training set. The computational time for constructing the initial region G 1 via Algorithm 3 was about 0.3 seconds, and it took approximately 0.1 seconds to execute C OUNT M INTERMS (  X  ) in Algorithm 4. Finally, the computational time needed to execute B DD A PPROX (  X  in Algorithm 4 was less than 0.01 seconds on average. The smaller the threshold  X  , the earlier the B DD A PPROX terminates, because edge modifications tend to occur in the depthless layers of the BDD when a smaller  X  is used. Therefore, it will take 0 . 3+0 . 1+0 . 01  X  100 = 1 . 4 seconds to search among a hundred kind of parameters in the BDDOCC, because only B DD A PPROX (  X  ) is needed to be executed repeatedly to search an optimal threshold.
Figure 7 shows the results of the MDL optimal approx-imations when m =8 , 10 , 32 are used instead of m =16 . The threshold parameters selected by the MDL principle are From Figure 7, it seems that the approximation becomes better as m gets larger, and the choice of m have little effect on the result when m is large enough, in this case m  X  10 . B. Shuttle Data
The Shuttle data contains eight continuous attributes and no categorical attributes, note that time attribute was ex-cluded in our experiment, and each of them is classified into seven classes. The number of examples in the training set and the test set are 43500 and 14500, respectively, in which approximately 80 % of them belong to class 1. We now consider class 1 as a target class, and construct a one-class classifier that identifies whether or not the data belongs to class 1. The class 1 data was extracted from the training set and is used to learn the classifier, then the number of examples in the training set becomes 34108. The estimated classifiers are evaluated with the test set to check if they can distinguish the class 1 data from the other data.
Table III shows the classification results of the test data by the BDDOCC. In Table III, F 1 and F MDL are used as F in Equation (4), where F 1 is constructed via Algorithm 3, and F
MDL is obtained via Algorithm 4 with  X  =10  X  14 . 5 , which minimizes the MDL. From Table III, we can see that the classification with F 1 results in too many false negatives, because the region that F 1 represents is too fine to detect class 1 robustly. On the other hand, the classification with F
MDL achieves fewer false negatives, while the number of false negatives is kept low. For comparison purposes, the same problem is solved using the OCSVM. However, there is no way to choose the proper parameters for the OCSVM from only the unlabeled training set, so we tried various parameters on a grid, with reference to Hsu [15], and eval-uated the AUC [16] with the test set instead of constructing a one-class classifier. The same problem was also solved by the PDE, where the kernel parameter was selected based on likelihood cross-validation. Because the PDE estimates the probability density of the data distribution, it returns the probability as a score toward the given test data. Therefore, a score threshold is necessary to classify the data into either +1 or -1; however, it is not clear how to select this threshold properly. Thus, we also evaluated the PDE result in terms of the AUC. Table IV shows the AUC of each method. From Table IV, we can see that the BDDOCC performs better than the others, and the OCSVM provides quite poor results for a particular parameter setting although it works well on average.

The computational time of constructing F 1 via Algo-rithm 3 was about 4.7 seconds, and it took about 1.1 sec-onds to execute C OUNT M INTERMS (  X  ) in Algorithm 4. The computational time needed to execute B DD A PPROX (  X  ) in Algorithm 4 was 0.12 seconds on average. Therefore, it will take 4 . 7+1 . 1+0 . 12  X  100 = 17 . 8 seconds to search among a hundred kind of parameters in the BDDOCC. It took less than 1 second to classify the test set using the BDDOCC. The computational time of solving the OCSVM for a particular parameter set was 16.2 seconds on average, and it took more than 10 minutes to evaluate the likelihood of the PDE for a particular kernel parameter. Therefore, it is supposed that the BDDOCC is superior to the compared methods from the point of view of computational effort. C. KDD Cup 99 Data
The KDD Cup 99 data is network access data that is related to network intrusion problem. It consists of seven categorical attributes and 34 continuous attributes, and each data is labeled either normal or as one of the 21 types of attacks ( back , buffer overflow , and so on). The training set and the test set contain approximately five million and 310 thousand examples, respectively. In reference to Yamanishi [17], we employ the following problem settings: 1) The data that satisfies logged in =1 is extracted from the original data. The extracted examples become 703067 and 53645 in the training set and the test set respectively. A one-class classifier that distinguishes normal access from attacks is estimated from the training set. Note that the training set contains 3377 (0.5%) attacks, which are considered as noises in the training set. 2) Four of the original 41 attributes { service , duration , src bytes , and dst bytes } are used to construct a classifier because they are thought to be the most basic attributes. Service is the only categorical attribute that has 41 original levels that are reassigned into five levels { http , smtp , ftp , ftp data , and others } in the experiment. Because the continuous attributes are concentrated around 0, they are transformed by log( x +0 . 1) .

Table V shows the classification results of the test data using the BDDOCC. In Table V, F 1 is constructed via Algorithm 3, and F MDL is obtained via Algorithm 4 with  X  =10  X  4 . 6 , which minimizes the MDL . From Table V, we see that the classification by F 1 suffers from high false negatives, but F MDL achieves fewer false negatives, while keeping the false negatives as few as that of the result by F Most data whose attack type is back could not be detected by either F 1 or F MDL , as seen in Table V. The attacks that could not be detected by F 1 would never be detected by F MDL , because F MDL always has a larger area than F 1 . Therefore, we need to add new attributes that can identify back attacks from normal accesses or decrease noises included in the training set, if we want to improve the classifier so that it can classify back attacks correctly. In the case of Table V, the latter may work better, because approximately 65 % of the noise included in the training set belongs to back . The computational time for constructing F 1 via Algorithm 3 was approximately 32.5 seconds, and it took approximately 1.0 second to execute C OUNT M INTERMS (  X  ) in Algorithm 4. The computational time needed to execute B DD A PPROX (  X  in Algorithm 4 was 0.14 seconds on average. Therefore, it will take 32 . 5+1 . 0+0 . 14  X  100 = 47 . 5 seconds to search among a hundred kind of parameters in the BDDOCC, which is quite applicable, even though the size of the training set is rather large. Although we tried to solve the same problem using the OCSVM and the PDE, neither of them terminate even after an hour, so we do not mention the AUC-based comparison of these methods here.

Many methods have been proposed related to outlier detection, anomaly detection, novelty detection, concept learning, and so on [1], [2]. Most of these techniques can be applied to one-class classification problems, with or without modifications. The features of some of the existing methods are listed in Table VI and compared with the features of the proposed method. In Table VI, the column model settings contain the settings that must be determined by users beforehand, but whose choice is less sensitive to the final performance and for which there exists a common choice, for example, the Gaussian kernel for kernel-based methods, and the Euclidian distance for distance-based methods. The column magic parameters contains the parameters that must be set by users beforehand and that have a strong influence on the final performance, such as variance parameters for the Gaussian kernel, and the number of hidden units for neural networks [2]. The column output score contains the type of value returned by the model toward a particular input, for example, the Parzen density estimator returns the probability and the  X  LSIF returns the importance of given data. Finally, the column necessity of score threshold indicates whether some score threshold is required to classify a data into either +1 or -1, for example, the one-class SVM does not require any score threshold because it outputs either +1 or -1, while the LOF requires a threshold for the output, because it returns a continuous score. From Table VI, we can see that most methods, except for the Parzen density estimator and the proposed method, have no means for selecting their magic parameters with an unlabeled training set. Note that the  X  LSIF requires both a training set and a test set instead of requiring a labeled training set to learn its magic-parameters. Although it is possible to select the magic parameter of the Parzen density estimator through likelihood cross-validation, some score threshold is needed to construct a one-class classifier, and no explicit criterion exists for choosing the threshold properly. On the other hand, the magic parameter is determined with the help of the MDL principle in the proposed method, and it does not require any score threshold to construct a one-class classifier, because it outputs either +1 or -1. Therefore, the proposed method is superior to other methods in terms of the model selection with an unlabeled training set. Furthermore, some methods in Table VI, such as the one-class SVM and the Parzen density estimator, become computationally intractable when the number of examples in the training set is large, as seen in the experiment in Section VI-C. In comparison, the proposed method can deal with a large training set practically as long as the created BDD can be stored in memory.

Few applications use BDDs as their basis in the fields of data mining and machine learning. For example, Minato and Arimura [21] applied Zero-suppressed BDDs (ZDDs), which is an extension of BDDs, for frequent item set mining. Loekito and Bailey [22] applied ZDDs for the analysis of contrast patterns of two item sets. As far as we know, BDDs have not been applied to classification problems in the past.
Some algorithms that approximate a Boolean formula have been proposed in the literature. Ravi et al. [23] proposed an approximation algorithm that approximates a Boolean formula that is expressed as a BDD, by manipulat-ing edges so as to minimize the ratio of minterms toward the number of nodes. Although we conducted some experiments that use the approximation algorithm proposed by Ravi et al., which is implemented as a function in the CUDD, instead in Algorithm 4, it ended in a poor result for the purpose of classification.

In this work, we proposed a novel approach toward the one-class classification problem. The region of the target class is expressed as a Boolean formula that is stored as a binary decision diagram. One of the features of the pro-posed method is that a one-class classifier can be produced thoroughly by using an unlabeled training set. There is no need to prepare a labeled set to tune the classifier, which is essential in some applications, such as anomaly detection. Model selection can be done quite efficiently because the proposed method has only one intuitive parameter to be tuned via the minimum description length principle.
This work can be extended in many directions. We have considered only an over-approximation algorithm in this paper, which is an approximation in which the approximated region always becomes greater than or equal to the original region. However, it is also possible to under-approximate the region by modifying the proposed algorithm slightly, which leads to other applications, such as noise removal in a training set. Furthermore, we believe that the proposed approach can be extended to two or more class classification problems by developing an evaluation scheme that is suitable for these problems based on the minimum description length principle.

Although the proposed method is scalable to the number of examples in a training set, it may become intractable when the number of attributes is very large because the size of the created binary decision diagram tends to be huge in such a case. One simple solution toward this problem is to embed some dimension-reduction techniques, such as principal component analysis, into the projection function  X  . Another possible solution is to employ techniques that improve the scalability of binary decision diagrams, such as partitioning methods [24].

For example,  X  is given as follows: where is an arbitrary small positive number and where  X  i and  X  i are the mean and the standard deviation of the i -th continuous attribute respectively, which are calcu-lated from x ( i ) | i =1 ,...,N .

