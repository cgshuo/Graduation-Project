 This paper is concerned with the study of information retrieval (IR) on Accumulative Social Descripti ons (ASDs). ASDs refer to Web texts that accumulated by many Web users describing certain Web resources, such as anchor texts, s earch logs and social annotations. There have been some studies working on leveraging ASDs for improving search performance in both internet and intranet. However, to the best of our knowledge, no prior study has concerned the specific generation features of ASDs, wh ich are the focus point of this paper. Specif ically, we consider the generation features from two perspectives, the generation processes and the generated distributions . Further, three probabilistic IR models are derived based on them. The three models are firs t demonstrated with one toy dataset and then empirically evaluated with two real datasets: an internet dataset c onsisting of 90,295 Web pages, with 25,845,818 social annotations cr awled from Del.icio.us and 31,320,005 pieces of anchor text s crawled through Yahoo! API, and an intranet dataset cons isting of 179,835 Web pages with 1,245,522 annotations dumped from th e intranet tagging system in IBM, named as Dogear. Extensive experimental results show that the proposed methods, which fully leverage the generation features of ASDs, improve the pe rformance of both internet and intranet search significantly. H.3.3. [ Information Storage and Retrieval ]: Information Search and Retrieval  X  Retrieval models Algorithms, Performance, Expe rimentation, Human Factors Information Retrieval, Accumulative Social Descriptions, Social Document, Generation Proce ss, Power-Law Distribution Nowadays, Web users have been the dominant forces in driving the evolvement of the Web. On one hand, they get information by browsing Web resources. On the other hand, they bring new descriptions to the Web resources explicitly or implicitly through their interactive activities. For example, search engine users generate click through histories; social bookmarking and tagging users create high quality tags for Web resources; Web site owners bring anchor texts in Web pages. The Web activities from a single user do not bring much knowledge. However, when there are hundreds of thousands of such users keeping active day by day, the knowledge they bring will fo rm a huge collect ion. Formally, we refer to such kind of user accumulated descriptions for resources on the Web as accumulative social descriptions (ASDs). To facilitate statements below, we call the integration of all the descriptions corresponding to a given Web resource a social document , and each element of a social document a descriptor term . There have been some research studies that discuss the usage of certain types of ASDs for IR, including click through histories [5, 11, 18], anchor texts [4, 13] and social annotations [2, 25]. They generally use traditional IR models to leverage ASDs as new metadata to improve the retrie val quality on Web page contents. However, nearly none of them st udies the new generation mode of ASDs, which is a key characteristic of ASDs that differentiates themselves from the traditional documents. Particularly, we conclude two main distinguishing generation features of ASDs: 1) accumulative generation. The contents of each traditional document are typically generated at a certain time, while social documents are not generated at one blow, but continually grow with time; 2) social generation. Each traditional document is generally generated by one author or a group of co-authors, while each social document is typically accumulated by a huge amount of Web users. Inspired by these vital differences, we try to analyze the potential of the generation features of ASDs for IR. Though some traditional IR models, e.g. language model and BM25, make some assu mptions about corpus generation, they do not emphasize the importance of this feature, since there is no salient characteristic in the generation of traditional document corpuses. Quite differently, the generation mode of ASDs is a fundamental characteristic that distinguishes social documents from traditional documents. An appropriate modeling of the generation of ASDs may benefit the IR tasks. In this paper, we propose to st udy new models which can capture the generation features of ASDs more accurately. After a detailed analysis of ASDs, we identify tw o perspectives of the generation features. 1) The generation processes . Two generation processes are investigated and formally m odeled in terms of probabilistic simulation. 2) The generated distributions . The distributions formed by the generation proce sses follow the power law. The analysis and the modeling from th e two perspectives both embody the accumulative generation feat ure and the social generation * Part of this work was done wh ile Lichun Yang and Shengliang Xu were interns at IBM China Research Lab. feature. Though the properties of the generation processes and the generated power-law distributions have been mentioned in some related work, while in this paper, we further bridge the relevance relations between the queries and Web resources from the two perspectives separately and deri ve three IR models. From the generation process perspective, tw o retrieval models are derived: 1) Social Query Generation Model. The retrieval is conducted as selecting the queries for the Web resources. 2) Social Document Selection Model. As the name sugge sts, we consider the retrieval as the selection of Web resources for the given query. From the power-law distribution perspective, we derive the third retrieval model: 3) Social Power Law Model, which takes advantage of the global distribution information of ASDs. As for experiments, we first illustrate the intuition of the proposed models with a toy dataset. Since there are no standard ASDs test sets available, we compile two real datasets. One is a set of 90,295 internet Web pages with two kinds of ASDs, i.e. social annotations crawled from Del.icio .us and anchor texts crawled through Yahoo! API. The data set, which is called as ASDDataSet, is published on the Web 1 . The other is a set of 179,835 intranet Web pages with one kind of ASDs, i.e. the internal annotations assigned by worldwide employees from IBM during their uses of the Dogear Tagging System [15]. Statistical analysis verifies the fitness of the proposed models on the real datasets. For quantitative evaluation, we furthe r developed 100 queries for the internet, which is also published in the same URL as the ASDDataSet, and intranet datasets, respectively. Extensive experimental results show th at the three proposed models generally outperform the classic IR models (such as language model and BM25) significantly. The rest of the paper is organize d as follows: Section 2 surveys the related work. Section 3 gives a detailed analysis of the generation features of ASDs. In S ection 4, we show the derivation of our models. The experimental re sults are provided in Section 5. Finally, we make conclusions a nd discuss the future work in Section 6. Much work has been done on both Accumulative Social Descriptions and Information Retr ieval Models. In this section, we will review some most related work on each side. In traditional Web environment, the social property of data is not well studied. While the anchor texts and search logs are two clear examples of ASDs. The idea of propagating anchor text to the page it refers to was first pub lished and implemented by McBryan [13] in 1994. Brin and Page [3] implemented the first prototype of the Google search engine. They found that one of the useful properties of anchor text is that it often provides more accurate descriptions of Web pages than the pages themselves. Craswell and Hawking studied anchor text in the context of finding specific Web sites with navigational queries [4]. As for search logs, Raghavan and Sever proposed to re use past optimal queries to improve search by reformula ting new queries [18]. Joachims presented a method of utilizing click-through data to learn a retrieval function [11]. In [25], the authors proposed to better estimate the similarity between queries by taking into account the temporal information of click-through data. http://www.apexlab.org/apex_wiki/ASDDataSet With the fast evolvement of Web 2.0, more and more users are involved in the generation of ASDs . Social annotation is one of the most popular representatives. Al-Khalifa and Davis analyzed the semantic value of social anno tations and concluded that social annotations are semantically richer than keywords extracted from page content [1]. Bao et al. prop osed to improve Web search by using social annotations to estim ate both similarity and popularity of Web pages from Web users X  perspective [2]. Recently, Zhou et al. presented a new approach to enhance the language model by exploring the topics within social annotations [26]. In spite of much research work considering certain types of ASDs for IR, they generally follow traditional IR approaches and take ASDs as new metadata to improve retrieval quality and do not look into the specific generation f eatures of ASDs. Differently, in this paper, we propose to study IR models on only ASDs data. Besides, we highlight the specific generation features of ASDs and conclude that they are beneficial for IR. The probabilistic ranking principle, proposed by S. E. Robertson [20], can be viewed as a fundame ntal basis for probabilistic IR models. There are many approaches along this line, such as [6, 9, 23]. Among these approaches, the 2-Poisson model [21] is one of introduced, which partitions th e document set into elite and non-elite sets, and Poisson distri bution of term frequencies in either subset is assumed. Because of the difficulties in estimating the parameters, they further simplify the model and derived BM25 [19]. Ponte and Croft proposed a  X  X uery generation X  model for IR and gained practical success [17]. In their approach, a  X  X anguage model X  is estimated for each doc ument, and the ranking procedure is to order documents by the probability that the query may be generated by each document X  X  mode l. Because of data sparseness problem, a technique of smoothing is widely used. The basic idea is to use the background informa tion of the collection to smooth the model of each document. In [24], Zhai and Lafferty further investigated and compared three popular smoothing methods. Recently, instead of using multinomial distribution to model the generation of query, Mei et al studied the Poisson process of query generation [14]. In [12], the connection between the classic probabilistic IR approaches and the language model approach for IR is discussed. Lafferty and Zhai demonstrated that both of them can be viewed as derived from the probabilistic ranking principle. While successful as above models are, they are all derived from the traditional document X  X  point of view. Differently, this paper studies the generation features of ASDs and based on which, three new models are derived and extensively evaluated. In this section, we first study two main features in the generation of ASDs and then consider the generation features from two perspectives, the generation processes and the generated distributions. Finally, we di scuss the connections between generation features and the releva nce calculation for IR on ASDs. Comparing with traditional docu ments, ASDs have their own special features in generation, of which we conclude two here: 1) The accumulative generation feature. The contents of each traditional document are typically generated at a certain time, while social documents are not generated at one blow, but continually grow with time. Users are always freely to add any descriptions for any Web resources at any time. 2) The social generation feature. Each traditional document is generally generated by one author or a group of co-authors, while each social document is typically accumulated by a huge amount of Web users. The actions of large group of users may somehow show social properties, i.e. properties that result from the implicit relations or interacti ons between users. Based on the two generation feat ures above, we implement the modeling of the generation of AS Ds from two perspectives, as shown in the following two subsections. Formally, the generation of ASDs involves three sets of entities, Web users u 1 , u 2 , ..., Web resources wr 1 , wr terms dt 1 , dt 2 , .... Each ASDs data corpus is composed of a large set of individual user activities. Generally, we may think of every single activity by users as a pair, ( wr i , dt k resource wr i and a descriptor term dt k . For different types of ASDs, the entities of Web resources are different, e.g. Web pages, images, videos etc. For descriptor s, we consider single terms. The upper half of Figure 1 illustrates the accumulation of ASDs from each single user activity, which is denoted as a pair of a descriptor and a Web resource surrounded by a dashed circle. The lower half of Figure 1 gives a bipartite structure of ASDs, where W are weightings of the edges, which can be the number of users who generated the corresponding pair. To formally model the generation of ASDs via probabilistic simulation, we further study the users X  activities. Every single two steps: 
StepWR: a user selects a Web resource wr i ; StepDT: a user select s a descriptor term dt k . Two generation processes (GP) can be formed using the two steps, as shown in Figure 2. Consider th at one user comes and wants to add a new pair ( wr , dt ) to the ASDs dataset, which may be conducted in either of the two generation processes: 
GP1(DT-WR): A user may first have an information need of a specific topic corresponding to some descriptor terms, and then may find some Web resources valuable. Formally, this scenario can be modeled as selecting a descriptor term first, which may be an existing descriptor term or a totally new one (denoted as 
StepDT in GP1 ). And then, she/he will continue to select a Web resource from the whole corpus (denoted as StepWR in GP1 ). It is quite common in the generation of anchor texts. 
GP2(WR-DT): On the other hand, a user first may find an interesting Web resource by surfing the Web without any specific need, and then may choose a descriptor term for the 
Web resource. This scenario can be modeled as selecting a Web resource that interests he r/him first (denoted as StepWR in GP2 ). 
And then, she/he will continue to select an existing or a new descriptor term (denoted as StepDT in GP2 ). It is quite common in the generation of social annotations. Now the only problem we face is how to estimate the probabilities in the above steps. Considering the accumulative generation feature and the social generation feature, we believe that previous users X  choices somehow affect fu ture users X  selection. More specifically, during the accumula tion process, social users X  interactions will spread their common interests. Therefore, the process will show a typical so ciology phenomenon,  X  X he Matthew effect X , i.e. the more times that a term or a Web resource has been used by others before, the more likely it will be used in the future. Based on these, we introduce a pr eferential selection process to model the steps, similar to [8]. Let us take StepDT in GP2(WR-DT) as an example, we make two generation assumptions as the following to facilitate the estimations: 
The probability of selecting a new descriptor term for a Web resource is a document spec ific constant, denoted as p new a parameter of the process. 
The probability of selecting an ol d descriptor term is in direct ratio to the times that it has been used. It has been proved that the generation processes proposed above will lead to the power-law distributions [22]. A discrete quantity n is said to follow the power law if it is drawn from a probability distribution as follows: where exp is a constant parameter know n as the scaling parameter scaling parameter, known as the Riemann zeta-function. As has been demonstrated [16], many accumulatively as well as socially generated data lead to power-law distributions, such as the in-links of Web sites, the citations of papers, the popularity of names, population of ci ties, etc. We believe that the power-law distributions somehow embody the accumulative generation feature and the social generation feature. All above analysis of the generati on features can be utilized to enhance the estimation of rele vance between queries and web resources. The following two sec tions studies the relevance estimation from generation processe s X  and generated distributions X  points of view respectively. The two generation processes, GP1(DT-WR) and GP2(WR-DT) both bring links between descriptor terms and Web resources directly. On one hand, since th e ASDs are accumulated manually by the Web users, though there may be noise, it is intuitive that an observed pair ( wr i , dt k ) indicates the hidden relevance of the Web resource wr i and the descriptor term dt k to some extent. On the other hand, the queries can be decomposed to query terms and the query terms share quite similarity with descriptor terms. Thus, the relevance between queries and Web resources can be built through descriptor terms. Figure 3 gives an illustration. The generated power-law distributions are not so intuitive for bridging the relevance relations between queries and Web resources comparing with the generation processes. The derivation of the Okapi BM25 retrieval model [21] gives us the inspiration. The main idea is a ssuming two non-intersected sets of Web resources for each descriptor term. One is called the  X  X lite Set X , which includes the Web resources that are  X  X elevant X  to the given descriptor term, the other is called the  X  X on-Elite Set X , which includes all the other Web resources. Further, we assume that the occurrence frequency of each descriptor term follows a power-law distribution in its Elite Set and follows another power-law distribution in its Non-Elite Set. Actually, both the two sets are hidden for each descri ptor term. What we only observe are the occurrence frequencies or the non-occur of the descriptor terms in the social documents. Based on these assumptions and the observed occurrence data, we can estimate quantitatively the relevance between a descriptor te rm and a Web resource. Further, through the decomposition of queries to query terms, the relevance between queries and We b resources can be estimated. Figure 4 gives a simple illustration, where Occur Set denotes the set of Web resources in the social document of which the descriptor term dt i occurs. 
Figure 4: The Elite Set, the Non-Elite Set, and the Occur Set In this section, we derive three IR models, two from the generation process perspective and one from the distribution perspective. Figure 5 gives an overview of our derived models. From the generation process pe rspective, focusing on term selection, we derive Social Query Selection Model (Section 4.1); focusing on Web resource selection, we derive Social Document Selection Model (Section 4.2). Fr om the generated distribution perspective, we derive Social Power Law Model (Section 4.3). 
Figure 5: Overview of the three IR models derived from the The notations for the statistics of data are shown in Table 1. Social Query Selection Model (SQS M) leverages the analysis of StepDT, i.e. the selection of terms, to inference the relevance between queries and Web resour ces. We derive the model based on the language model for IR, i. e. to rank the documents according to Pr( q | d ). Moreover, we ma ke the independence assumptions among terms and apply the typical smoothing method: Pr( | ) where, Pr( q i | C ) is the collection model and  X  dependent coefficient, which is simply a normalization of the probabilities. In this way, the problem reduces to the modeling of the documents and the collection, i.e. Pr seen ( t | d ) and Pr( t | C ). we take advantage of the analys is of StepDT in GP2 (WR-DT) instead of simple maximum likelihood. Recall the generation assumptions in Section 3.1. We model social documents according to them: The collection model, i.e. the probability Pr( t | C ), means the probability that the collection generates the new term t . The estimation should be ba sed on the analysis of StepDT in GP1 (DT-WR). Again, by taking the generation assumptions, the estimation for the collection generation is quite similar: Some of the classic smoothing methods like Dirichlet smoothing [24] are not suitable here, sinc e the generation assumption of the query terms is changed. We appl y the Jelinek-Mercer smoothing, which makes a linear mixture between the document language model and the collection model. where  X  is the mixture parameter for smoothing. As proposed in [24], we try to estimate  X  via maximizing the likelihood of generating the given query. Let  X  i be the unknown mixing weights, which are the prior probabilities of choosing each document. We use the EM algorithm to calculate the parameters. In the E-step, we estimate the unknown parameter  X  i . In the M-step, we update the parameter  X  . where | q | represents the query length. A detailed derivation and the discussion of this estimation can be found in [24]. The calculation of the normalization factor  X  d is derived as: Social Document Selection Model (SDSM) focuses on StepWR. Symmetric to the basis of Social Query Selection Model, we rank method is also similar: Pr( | ) Note again that d stands for social docume nt. Further, we interpret social document d . Since every social document corresponds to a Web resource, we may leverage the analysis of StepWR to estimate the probability. For Pr( d | t ), it is clearly StepWR in GP1(DT-WR) , Similarly, for Pr( d | C ), we estimate it according to StepWR in GP2(WR-DT): As for smoothing, we also apply the Jelinek-Mercer smoothing method: The estimation for the mixture parameter and the calculation of the normalization factor are just similar to those in Social Query Selection Model. Social Power Law Model (SPLM) estimates relevance according to the distributions of term frequ encies. The model basis is similar to the well known Okapi BM25. We rank the documents according to the probability ranking to denote irrelevance). This time we consider each social document as a vector of all de scriptor terms X  in-document to focus on the frequencies of te rms and further make independent assumptions among terms. Moreover, we assume that for any non-query term, the probabilities given relevant and given terms t ). In this way, we simplify the ranking formula and focus on the probability of the query terms X  in-document frequencies. Inspired by the 2-Poisson model in [21], a key concept of eliteness is introduced as have discussed in Section 3.3.2. Given eliteness, we assume that relevance and term frequency are independent. where e and e denote eliteness and non-e liteness respectively. we can reduce the problem to the estimation of Pr( tf non-elite sets. After introducing eliteness, we partition the social documents into two subsets for any given term t . where  X  denotes the proportion of social documents that belong to the elite set. Different from th e 2-Poisson model, we do not assume Poisson distribution of term frequencies, but power-law distributions instead. where exp e and exp n are the scaling parameters of the power-law distributions in elite and non-elite set respectively. Following the Equation (10), the weighting formula can be derived as: The weighting of a document is the sum of all the partial weighting of the query terms. To make the retrieval process simpler, it is convenient to set the partial weighting to be zero when a document does not contain a certain query term [21]. Otherwise, in the ranking proce ss, we have to calculate the weighting for all the documents in the collection, whatever it contains the query terms or not, which is time consuming. The following formula shows the solution: where w 0 ( q i , q , R ) denotes the weighting of any document for query q with the query term q i absent. By means of adding this document-independent constant ite m, the partial weighting for a document without the query term is set to zero. Since the power-law distributi on is over the documents that contain the specific term, the case of zero is left undefined. We simply assume that the weightin g should be a continuous stretch of the general case. By the definition of eliteness, weighting for an unseen query term. In this way the final weighting function for a matched term is: For the parameters of the distribu tion, we use the EM algorithm to maximize the likelihood of the data. A formal derivation can be found in Appendix A. For any given term, the eliteness of each document is a hidden variable for us. As suggested in [21], we try to calculate the expected value of it. For a given term and a given document, we can calculate the probability that the document is elite with respect to the term, based on th e number of occurrences of the term in the document. Generally speaking, for any query, the relevant documents form a tiny small subset of the global collection. Based on this, we may simply consider every document as non-relevant and make estimation for p en . Then for p er we use an indirect wa y. As shown in [21], p somehow reveal the term X  X  importa nce, embodied by the standard inverted document frequency value. Thus, we can get the estimation for p er from the estimated value of p and the standard inverted document frequency value . In the former derivation, we make independence assumption among terms, focus on every single term and then estimate the probabilities. However, it is obvious that document length is an important issue. As proposed in [10], the document length has two aspects of influence on the term frequency within document: 1) the same term usually occurs re peatedly in long document; and 2) a long document has usually a large size of vocabulary. For social documents, we should also consider the document length issue. The independent assumption among terms is too strong and needs to be revised. It has been pointed out that powe r-law distributions have the scale free property, i.e. the proportions remain constant. For evidence, Golder and Huberman find that the proportions of frequencies of annotations within a given documen t stabilize over time [7]. We then derive document length norma lization for the original term frequency. documents in the collection. In th is way, the term frequency of a document with arbitrary length is normalized to the virtual document with a constant length. For the ease of understanding our modeling, we introduce a toy dataset, and demonstrat e how the models work. SumDTs 40 40 40 40 160 We represent the dataset via a matrix, shown in Table 2. The columns represent Web resources and the rows represent the represents the number of users who attached the i -th descriptor term to the j -th Web resource. Note that the last row,  X  X umDTs X , shows the total number of descriptor terms in each social document. Correspondingly, the la st column,  X  X umWRs X , shows the total frequency of each descriptor term across the whole dataset. We first demonstrate the modeli ng of the generation processes. Taken the assumptions in Section 3.1, we are able to estimate each step quantitatively. We still take StepDT in GP2(WR-DT) (the selection of descriptor te rms after selecting a certain Web resource) as an example, i.e. we first focusing on the column of the matrix corresponding to the We b resource and then select an entry corresponding to a certain term. For the case of assigning dt to wr 3 and the observed toy dataset given in Table 2, we may derive that the probability is (1-p new )*10/40. Here, p process parameter. We simply estimate it via maximum likelihood: Pr( | , ) log log rdq in the 40 generated terms of wr 3 , there are 2 distinct terms, dt dt (i.e. for 2 times out of 40, th e process generated a new term), thus p new can be estimated as 2/40. The probabilities in other steps are similar. There is no mean to show the distributions of the toy dataset distributions of our real dataset in Section 5.3. In the following, we illustrate the intuitions of the derived models with the toy dataset. Suppose we get a query q ={ dt 2 } and focus on the weighting score of wr 2 and wr 3 . SQSM analyze StepDT, the selection of terms. The estimations are made through certain columns of the matrix. Firstly, we look into the wr 2 and wr 3 columns and estimate the probability of selecting dt 2 . Then, we analyze the  X  X umWRs X  column to smooth the estimation. Traditional approaches generally give the same same document length and the same inverted document frequency. Quite differently, SQSM ranks wr 3 above wr 2 . Specifically, the score of wr 3 and wr 2 in the document model part are (1-2/40)*10/40 and (1-3/40)*10/40 respectively, while the smoothing part is the same. SQSM takes the number of distinct terms in a document into consideration. 2 ( dt 2 and dt ( dt 1 , dt 2 and dt 4 ) in wr 2 indicates that wr 3 is more concentrated on the topics expressed by the existi ng terms. Thus, we may conclude that wr 3 is more relevant to dt 2 . score of wr 1 and wr 2 . Symmetric to SQSM, SDSM focuses on StepWR, the selection of Web resources; and the estimati ons are made through the rows of the matrix. Firstly, we analyze the rows of dt 2 and dt the probability that these terms are attached to wr then, we smooth the estimation via the analysis of the  X  X umDTs X  row. For example, the estimation of given dt 2 selecting wr (1-2/4)*10/20. Note that beside s the typical term frequency in document, both the number of doc uments containing the term ( df ( t , C ) ) and the total number of the term across the collection ( tf are considered simultaneously. Fo r traditional documents, it is quite unreasonable to consider the problem in this way, since the dataset is typically generated doc ument by document and it is clearly not a process of  X  X electi ng X  documents. Some weighting schemes, such as the inverted document frequency, also use df or tf ( t , C ) , but they are just used for estimating the importance of the term. score of wr 3 and wr 4 . SPLM first analyzes the distribution of dt 1 and dt weighting schemes. For a small toy dataset, it is hard to show the differences between distributions. We conclude the intuition here: dt generally appears more times than dt 2 ; hence, it is more common to see dt 1 with high frequency. Therefore, on one hand, if dt for dt 1 ; on the other hand, if we consider the weighting as a models, the inverted document frequency somehow provides the former functionality; however, they typically do not consider the latter one. Moreover, they simp ly count the document frequency, while SPLM further looks into the distributions among documents. Since there is no standard test set available for our task, we build a new test bed containing two data sets. 1) The first one is an internet set containing 90,295 URLs. We crawled two types of ASDs for it. One is the social a nnotations (called the internet annotation set). We craw led all the Del.icio.us records of the Web pages up to May, 2008. The other is the anchor texts of the URLs (called the internet anchor se t), crawled through Yahoo! API. belonging to the 90,295 Web pages. We are not able to crawl the whole collection because the Yahoo! API only returns up to top 1,000 backward links. 2) The second one is an intranet set (called the intranet annotation set). The annotations are from the employees of IBM during their uses of the Dogear Tagging System. Some statistics of the datasets are illustrated in Table 3. Note that our original internet set was quite big. However, we found it difficult to crawl anchor texts for so many URLs. Finally, we get subset of 90,295 URLs by in tersecting the original set with the URLs in Open Directory Projec t (ODP). This results in that the intranet annotation set contains more URLs. Contrarily, there are much more tokens in the internet anchor set and internet annotation set than those in the in tranet annotation set. The reason is that there are much more users all over the Web than those in a company. Note that we consider IR tasks on ASDs only (without the contents of Web pages), since we would like to emphasize the specific generation features of ASDs comparing to traditional documents and study whether th e modeling of the specific generation may benefit IR or not. Moreover, this task is meaningful in many applications . To name a few, systems that collect users X  ASDs may be directly applied to search the Web resources, without spending time an d effort analyzing the contents of the Web resources, such as Del.icio.us. Moreover, when the Web resources are non-text, such as images in Flickr or videos in YouTube, it would be more helpful to build an IR system using ASDs data of these Web resources than to extract information from non-text contents. A group of CS students are asked to help us collect queries and label the relevant Web resources. For the internet anchor set and the internet annotation set, the URLs are the same and we use the same set of queries. Finally, we get 100 queries for the two internet sets and the average query length is 2.78; and another 100 queries for the intranet set and the average query length is 2.51. In order to achieve good quality of manually labeling, we use the pooling technique. We built 2 retr ieval models, a Jelinek-Mercer smoothed Language Model for IR, and an Okapi BM25 model. For the internet anchor set and the internet annotation set, we annotations from Del.icio.us, OD P category and ODP description; while for the intranet annotation se t, we obtain 3, i.e. Web page contents, title and th e IBM employee X  X  annotations. The queries contain 1,451 relevant Web pages for the internet anchor set and the internet annotation set, and 1,217 relevant Web pages for the intranet annotation set. Two state-of-the-art models are selected as the baselines, i.e. Jelinek-Mercer Smoothed Language Model for IR (JMLM) and BM25. SQSM and SDSM are very similar to JMLM in that they all model the retrieval as local activities, i.e.  X  X election X  in SQSM and SDSM and  X  X eneration X  in JMLM. Thus, we set them as a comparing group in the experi ments. Besides, SPLM and BM25 are also very similar in that th ey all model the retrieval through global distributions. Natu rally, we set them as a comparing pair in the experiments. For the mixtur e parameter of JMLM, we tune from 0.0 to 1.0 with 0.1 as step le ngth, selecting the one that leads to the best MAP. For BM25, we use the default parameters, k =1.0 and b =0.75. The main evaluation metric is Mean Average Precision (MAP). MAP is defined as the mean of average precision over queries. Another evaluation metric is Precision at N (P@N), which is the precision of the top N results. We select P@5 and P@10. Figure 6: Sample distributions in our Del.icio.us dataset. Note In Figure 6, we list four sample distributions from our social annotation dataset corresponding to the four steps in Section 3.1. Figure 6.A corresponds to the  X  X umWRs X  column of the data matrix just like shown in Table 2, which results from users X  global selection of terms, i.e. StepDT in GP1(DT-WR). Similarly, Figure 6.B, 6.C and 6D results from St epDT in GP2(WR-DT), StepWR in GP2(WR-DT) and StepWR in GP1(DT-WR), respectively. As we can see directly from the fi gures, the distributions generally follow the power law. These are the theoretical foundations of Social Power Law Model. As has been proved in [8], the preferential selection process will lead to the power-law distributions. Thus, these results also indicate the correctness of our modeling for the generation processes. We take JMLM as the baseline for SQSM and SDSM, since all of them are derived from the micro selection or  X  X eneration X  processes. Table 4 shows the results. The improvements with a star suggest that the st atistical signific ance according to the T-test at the level of 0.05. The MAP, P@5 and P@10 values tell that both SQSM and SDSM outperform JMLM on each dataset, which in return verify the usefulness of the generation processes for IR. The table also shows that both SQSM and SDSM get the highest improvements on the internet a nnotation set, the second highest on the intranet annotation set and the worst on the internet anchor comparatively better than anchor texts; and since the internet annotation set contains richer information than the intranet annotation set, it has the highes t quality. Another reason is the anchor text collection used in the experiments is just a small portion of all the anchor texts be longing to the Web resources in our test bed across the whole Web. Next, we further make a comparison between SQSM and SDSM. As shown in Table 4, we find the results of SDSM are similar to SQSM on the internet annotation set and the intranet annotation set, while the result of SQSM is a bit worse than that of SDSM in the anchor text set. The reason is that comparing to social annotations, anchor texts are more likely to be generated by the process of document selection. Th erefore, SDSM is more suitable than SQSM for anchor texts. To evaluate SPLM, we take BM25 as the baseline since both of them use the global di stribution information. Before IR experiments fo r BM25 and SPLM on ASDs, we conduct a fitting experiment to compare the fitness of the 2-Poisson distribution [21] assumed by BM25 and the 2-power-law distribution assumed by SPLM on the term distributions in ASDs. Figure 7 give s the result of the comparison, in which we selected the term  X  X oftware X  as a sample for experiment. The left figure shows the fitting result in the internet annotation set; the right figure shows the fitting result in the internet anchor set; and the bottom figure shows the fitting result distribution assumption fits the data much better than the 2-Poisson distribution assumpti on. That means our SPLM model better captures the generated distribution. Now we discuss the IR experi ments on BM25 and SPLM. Table 5 shows the results. SPLM outperfor ms BM25 all the three datasets for all the three evaluation metrics. Again, we find the best performance improvements on the internet annotation set. Last, we compare the models that derived from different perspectives. On the internet anchor set and the internet annotation set, SPLM is better than SQSM as well as SDSM; while on the intranet set, SPLM is worse than SQSM or SDSM. We conclude that this is due to the differences in modeling perspectives. The model basis of SPLM is the global distribution of data. It gives a modeling of the generation of ASDs from a macro perspective. As was illustrated, comparing to the internet anchor set and the internet annotation set, the intranet annotation set contains more documents but much less terms. Hence, the distribution over documents is much clearer in the internet anchor set and the internet annotatio n set, where SPLM works well. SQSM and SDSM are built from a micro perspective. Although they use some additional collect ion information, they mainly focus on local generation proce sses, without looking into the global data distributions. Ther efore, SQSM and SDSM are less affected by the adequacy of data and achieve better performance than SPLM on the intranet annotation set. In addition, the two baselines models, JMLM and BM 25, are also derived from the micro generation perspective and the macro distribution perspective, respectively. Their results also lead to the consistent conclusion. From the above experiments, we conclude that a data-oriented modeling of the generation of th e underlying dataset benefits information retrieval a lot. Moreover, when the data contain adequate information and the distribution feature is quite clear, models that leverage global di stribution information generally better capture the generation features; while when the data are quite sparse, models that derive d from a micro perspective would be better. In this paper, we studied the problem of information retrieval on accumulative social descriptions (ASDs) using the generation features. The generation mode of ASDs is a fundamental characteristic that distinguishes ASDs from the traditional documents. Specifically, we iden tified two generation processes and analyzed the generated power-law distributions. We then derived three probabilistic IR m odels from two perspectives: two models from the perspective of detailed generation processes and one model from the perspective of generated distributions. Experimental results show that th e generation features benefit IR significantly. The main contributions can be concluded as follows: (1) The proposal of studying IR for ASDs using the generation (2) The derivation of IR models for ASDs. We analyze the (3) Extensive experiments on both internet and intranet dataset. As for future work, we would like to further test the universality of our models on more types of ASDs data such as query logs. The authors would like to thank IBM China Research Lab for its continuous support to and cooperation with Shanghai Jiao Tong University. We would also like to express our gratitude to D.R. Millen and J. Feinberg from IBM Watson Research Center for providing us the Dogear social ta gging data corpus. Besides, we appreciate the valuable suggestions and the helpful comments from some Apex Lab members and the anonymous reviewers. [1] H. S. Al-Khalifa and H. C. Davis. Measuring the Semantic [2] S. Bao, X. Wu, B. Fei, G. Xue, Z. Su, Y. Yu. Optimizing [3] S. Brin and L. Page. The anatomy of a large-scale [4] N. Craswell, D. Hawking and S. E. Robertson. Effective site [5] H. Cui, J. R. Wen, J. Y. Nie, and W. Y. Ma. Query [6] N. Fuhr. Models for Retrieva l with Probabilistic Indexing. [7] S. Golder and B. A. Huberman. The Structure of [8] H. Halpin, V. Robu and H. Shepherd. The Complex [9] S. P. Harter. A Probabilistic Approach to Automatic [10] B. He, L. Ounis. A Study of Parameter Tuning for Term [11] T. Joachims. Optimizing Sear ch Engine using Clickthrough [12] J. Lafferty and C. Zhai. Probabilistic Relevance Models [13] O. A. McBryan. GENVL and WWWW: Tools for taming the [14] Q. Mei, H. Fang and C. Zhai. A study of Poisson query [15] D. Millen, J. Feinberg, a nd B. Kerr. Dogear: Social [16] M. E. J. Newman. Power laws, Pareto distributions and [17] J. Ponte and W. B. Croft. A language modeling approach to [18] V. V. Raghavan and H. Sever. On the reuse of past optimal [19] S. E. Robertson and S. Walker. Some simple effective [20] S. E. Robertson. The Probabilistic Ranking Principle in IR. [21] S. E. Robertson and C.J. van Rijsbergen and M.F. Porter. [22] H. A. Simon. On a Class of Skew Distribution Functions. [23] H. Turtle and W. B. Croft. Efficient Probabilistic Inference [24] C. Zhai and J. Lafferty. A study of smoothing methods for [25] Q. Zhao, S. Hoi, T. Liu, S. Bhowmick, M. Lyu and W. Ma. [26] D. Zhou, J. Bian, S. Zheng, H. Zha and C. L. Giles. The 2-power-law distribution can be described in three parameters, expectation of the log likelihood function: The eliteness of each document is a hidden variable. In the E-step, we estimate it according to th e parameters in the previous iteration.  X  X  Then, we get the expectation of the log likelihood. For the M-step, we update the parameters to maximize the expectation. For  X  , we set partial derivative function to be zero, and get the updating formula. For the exponent parameters of th e power-law distri bution, if we follow the same way, the result is quite complex. Since there is a Riemann zeta-function on the parameter, it is quite difficult to find the direct solution by setting partial derivative approximation approach. Finally we get: For exp n the result is similar. 
