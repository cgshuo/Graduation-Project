 In this paper we study the general affine rank minimization problem (ARMP), where A is an affine transformation from R m  X  n to R d .
 The affine rank minimization problem above is of considerable practical interest and many important machine learning problems such as matrix completion, low-dimensional metric embedding, low-rank kernel learning can be viewed as instances of the above problem. Unfortunately, ARMP is NP-hard in general and is also NP-hard to approximate ([22]).
 Until recently, most known methods for ARMP were heuristic in nature with few known rigorous guarantees. In a recent breakthrough, Recht et al. [24] gave the first nontrivial results for the for all X  X  R m  X  n of rank at most k , The above RIP condition is a direct generalization of the RIP condition used in the compressive sensing context. Moreover, RIP holds for many important practical applications of ARMP such as image compression, linear time-invariant systems. In particular, Recht et al. show that for most natural families of random measurements, RIP is satisfied even for only O ( nk log n ) measurements. Also, Recht et al. show that for ARMP with isometry constant  X  5 k &lt; 1 / 10 , the minimum rank solution can be recovered by the minimum trace-norm solution.
 In this paper we propose a simple and efficient algorithm SVP (Singular Value Projection) based on the projected gradient algorithm. We present a simple analysis showing that SVP recovers the minimum rank solution for noisy affine constraints that satisfy RIP and prove the following guar-antees. (Independent of our work, Goldfarb and Ma [12] proposed an algorithm similar to SVP . However, their analysis and formulation is different from ours. They also require stronger isometry assumptions,  X  3 k &lt; 1 / Furthermore, SVP outputs a matrix X of rank at most k such that  X A ( X )  X  b  X  2 2  X   X  and  X  X  X  X As our SVP algorithm is based on projected gradient descent, it behaves as a first order methods and may require a relatively large number of iterations to achieve high accuracy, even after iden-tifying the correct row and column subspaces. To this end, we introduce a Newton-type step in our framework ( SVP -Newton ) rather than using a simple gradient-descent step. Guarantees sim-ilar to Theorems 1.1, 1.2 follow easily for SVP -Newton using the proofs for SVP . In practice, SVP -Newton performs better than SVP in terms of accuracy and number of iterations. We next consider an important application of ARMP : the low-rank matrix completion problem ( MCP ) X  given a small number of entries from an unknown low-rank matrix, the task is to complete the missing entries. Note that RIP does not hold directly for this problem. Recently, Candes and Recht [6], Candes and Tao [7] and Keshavan et al. [14] gave the first theoretical guarantees for the problem obtaining exact recovery from an almost optimal number of uniformly sampled entries. While RIP does not hold for MCP, we show that a similar property holds for incoherent matrices an analysis similar to that of Theorem 1.1 immediately implies that SVP optimally solves MCP. We provide strong empirical evidence for our hypothesis and show that that both of our algorithms recover a low-rank matrix from an almost optimal number of uniformly sampled entries. In summary, our main contributions are:  X  Motivated by [11], we propose a projected gradient based algorithm, SVP , for ARMP and show  X  We introduce a Newton-type step in the SVP method which is useful if high precision is criti- X  As observed in [23], most trace-norm based methods perform poorly for matrix completion when  X  We show that the affine constraints in the low-rank matrix completion problem satisfy a weaker  X  We evaluate our method on a variety of synthetic and real-world datasets and show that our In this section, we first introduce our Singular Value Projection ( SVP ) algorithm for ARMP and present a proof of its optimality for affine constraints satisfying RIP (1). We then specialize our algorithm for the problem of matrix completion and prove a more restricted isometry property for the same. Finally, we introduce a Newton-type step in our SVP algorithm and prove its convergence. 2.1 Singular Value Decomposition (SVP) Consider the following more robust formulation of ARMP ( RARMP ),
The hardness of the above problem mainly comes from the non-convexity of the set of low-rank matrices C ( k ) . However, the Euclidean projection onto C ( k ) can be computed efficiently using singular value decomposition (SVD). Our algorithm uses this observation along with the projected gradient method for efficiently minimizing the objective function specified in (RARMP). computing the top k singular values and vectors of X .
 In SVP , a candidate solution to ARMP is computed iteratively by starting from the all-zero ma-trix and adapting the classical projected gradient descent update as follows (note that  X   X  ( X ) = A T ( A ( X )  X  b ) ): Figure 1 presents SVP in more detail. Note that the iterates X t are always low-rank, facilitating faster computation of the SVD. See Section 3 for a more detailed discussion of computational issues. Algorithm 1 Singular Value Projection ( SVP ) Algorithm Require: A ,b, tolerance  X  ,  X  t for t = 0 , 1 , 2 ,... 1: Initialize: X 0 = 0 and t = 0 2: repeat 4: Compute top k singular vectors of Y t +1 : U k ,  X  k , V k 6: t  X  t + 1 7: until  X A ( X t +1 )  X  b  X  2 2  X   X  Analysis for Constraints Satisfying RIP Theorem 1.1 shows that SVP converges to an  X  -approximate solution of RARMP in O (log  X  b  X  2  X  ) steps. Theorem 1.2 shows a similar result for the noisy case. The theorems follow from the following lemma that bounds the objective function after each iteration.
 Lemma 2.1 Let X  X  be an optimal solution of (RARMP) and let X t be the iterate obtained by SVP at t -th iteration. Then,  X  ( X t +1 )  X   X  ( X  X  ) +  X  2 k (1  X   X  isometry constant of A .
 The lemma follows from elementary linear algebra, optimality of SVD (Eckart-Young theorem) and two simple applications of RIP . We refer to the supplementary material (Appendix A) for a detailed proof. We now prove Theorem 1.1. Theorem 1.2 can also be proved similarly; see supplementary material (Appendix A) for a detailed proof.
 Proof of Theorem 1.1 Using Lemma 2.1 and the fact that  X  ( X  X  ) = 0 , it follows that  X  X 0 = 0 , i.e.,  X  ( X 0 ) =  X  b  X  2 2.2 Matrix Completion We first describe the low-rank matrix completion problem formally. For  X   X  [ m ]  X  [ n ] , let P  X  : R  X  and ( P  X  ( X )) ij = 0 otherwise. Then, the low-rank matrix completion problem ( MCP ) can be formulated as follows,
Observe that MCP is a special case of ARMP , so we can apply SVP for matrix completion. We update for matrix-completion: do not satisfy RIP in general. Thus Theorems 1.1, 1.2 above and the results of Recht et al. [24] do not directly apply to MCP . However, we show that the matrix completion affine constraints satisfy RIP for low-rank incoherent matrices.
 Definition 2.1 (Incoherence) A matrix X  X  R m  X  n with singular value decomposition X = U  X  V T is  X  -incoherent if max i,j | U ij | X  The above notion of incoherence is similar to that introduced by Candes and Recht [6] and also used by [7, 14]. Intuitively, high incoherence (i.e.,  X  is small) implies that the non-zero entries of X are not concentrated in a small number of entries. Hence, a random sampling of the matrix should provide enough global information to satisfy RIP .
 Using the above definition, we prove the following refined restricted isometry property. Theorem 2.2 There exists a constant C  X  0 such that the following holds for all 0 &lt;  X  &lt; 1 ,  X   X  1 , n  X  m  X  3 : For  X   X  [ m ]  X  [ n ] chosen according to the Bernoulli model with density p property holds for all  X  -incoherent matrices X of rank at most k : Roughly, our proof combines a Chernoff bound estimate for  X P  X  ( X )  X  2 F with a union bound over low-rank incoherent matrices. A proof sketch is presented in Section 2.2.1.
 of Theorem 1.1 can be used to show that SVP achieves exact recovery for low-rank incoherent matrices from uniformly sampled entries. As supported by empirical evidence, we hypothesize that the iterates X t arising in SVP remain incoherent when the underlying matrix X  X  is incoherent. Figure 1 (d) plots the maximum incoherence max t  X  ( X t ) = left singular vectors of the intermediate iterates X t computed by SVP . The figure clearly shows n and density p throughout the execution of SVP . Figure 2 (c) plots the threshold sampling density p beyond which matrix completion for randomly generated matrices is solved exactly by SVP for theoretic bound [14] of  X ( k log n/n ) .
 Motivated by Theorem 2.2 and supported by empirical evidence (Figures 2 (c), (d)) we hypothesize that SVP achieves exact recovery from an almost optimal number of samples for incoherent matrices. Conjecture 2.3 Fix  X ,k and  X   X  1 / 3 . Then, there exists a constant C such that for a  X  -incoherent matrix X  X  of rank at most k and  X  sampled from the Bernoulli model with density Moreover, SVP outputs a matrix X of rank at most k such that  X P  X  ( X )  X  X   X  ( X  X  )  X  2 F  X   X  after O 2.2.1 RIP for Matrix Completion on Incoherent Matrices We now prove the restricted isometry property of Theorem 2.2 for the affine constraints that result from the projection operator P  X  . To prove Theorem 2.2 we first show the theorem for a discrete collection of matrices using Chernoff type large-deviation bounds and use standard quantization arguments to generalize to the continuous case. We first introduce some notation and provide useful lemmas for our main proof 1 . First, we introduce the notion of  X  -regularity.
 Definition 2.2 A matrix X  X  R m  X  n is  X  -regular if max i,j | X ij | X   X   X  mn  X  X  X  X  F . Lemma 2.4 below relates the notion of regularity to incoherence and Lemma 2.5 proves (3) for a fixed regular matrix when the samples  X  are selected independently.
 Lemma 2.4 Let X  X  R m  X  n be a  X  -incoherent matrix of rank at most k . Then X is  X  to the Bernoulli model, with each pair ( i,j )  X   X  chosen independently with probability p , While the above lemma shows Equation (3) for a fixed rank k ,  X  -incoherent X (i.e., (  X  X using Lemma 2.4), we need to show Equation (3) for all such rank k incoherent matrices. To handle this problem, we discretize the space of low-rank incoherent matrices so as to be able to use the above lemma and a union bound. We now show the existence of a small set of matrices matrix from the set S (  X , X  ) .
 of rank k with  X  X  X  2 = 1 , there exists Y  X  S (  X , X  ) s.t.  X  Y  X  X  X  F &lt;  X  and Y is (4  X  We now prove Theorem 2.2 by combining Lemmas 2.5, 2.6 and applying a union bound. We present a sketch of the proof but defer the details to the supplementary material (Appendix B). Proof Sketch of Theorem 2.2 Let S  X  (  X , X  ) = { Y : Y  X  S (  X , X  ) ,Y is 4  X  S (  X , X  ) is as in Lemma 2.6 for  X  =  X / 9 mnk . Let m  X  n . Then, by Lemma 2.5 and union bound, for any Y  X  S  X  (  X , X  ) , [  X P where C 1  X  0 is a constant independent of m,n,k . Thus, if p &gt; C X  2 k 2 log n/ X  2 m , where C = 16( C 1 + 1) , with probability at least 1  X  exp(  X  n log n ) , the following holds Now, by Lemma 2.6 there exists Y  X  S  X  (  X , X  ) such that  X  Y  X  X  X  F  X   X  . Moreover, Proceeding similarly, we can show that Combining inequalities (4), (5) above, with probability at least 1  X  exp(  X  n log n ) we have, The theorem follows using the above inequality. 2.3 SVP-Newton In this section we introduce a Newton-type step in our SVP method to speed up its convergence. Recall that each iteration of SVP (Equation (1)) takes a step along the gradient of the objective function and then projects the iterate to the set of low rank matrices using SVD. Now, the top k  X  Note that as A is an affine transformation,  X  k can be computed by solving a least squares problem the objective function more than SVP . This observation along with straightforward modifications of the proofs of Theorems 1.1, 1.2 show that similar guarantees hold for SVP -Newton as well 3 . Note that the least squares problem for computing  X  k has k 2 variables. This makes SVP-Newton computationally expensive for problems with large rank, particularly for situations with a large number of constraints as is the case for matrix completion. To overcome this issue, we also consider the alternative where we restrict  X  k to be a diagonal matrix, leading to the update We call the above method SVP-NewtonD (for SVP-Newton Diagonal). As for SVP-Newton, guar-antees similar to SVP follow for SVP-NewtonD by observing that for each iteration, SVP-NewtonD decreases the objective function more than SVP . The general rank minimization problem with affine constraints is NP-hard and is also NP-hard to approximate [22]. Most methods for ARMP either relax the rank constraint to a convex function such as the trace-norm [8], [9], or assume a factorization and optimize the resulting non-convex problem by alternating minimization [4, 3, 15].
 The results of Recht et al. [24] were later extended to noisy measurements and isometry constants up to  X  3 k &lt; 1 / 4 Lee and Bresler [17] proposed an algorithm (ADMiRA) motivated by the orthogonal matching pur-suit line of work in compressed sensing and show that for affine constraints with isometry constant  X  cient for large datasets and when the rank of the optimal solution is relatively large. For the matrix-completion problem until the recent works of [6], [7] and [14], there were few meth-ods with rigorous guarantees. The alternating least squares minimization heuristic and its variants [3, 15] perform the best in practice, but are notoriously hard to analyze. Candes and Recht [6], Candes and Tao [7] show that if X  X  is  X  -incoherent and the known entries are sampled uniformly imum rank solution. Keshavan et.al obtained similar results independently for exact recovery from uniformly sampled  X  with |  X  | X  C (  X ,k ) n log n .
 Minimizing the trace-norm of a matrix subject to affine constraints can be cast as a semi-definite program (SDP). However, algorithms for semi-definite programming, as used by most methods for minimizing trace-norm, are prohibitively expensive even for moderately large datasets. Recently, a variety of methods based mostly on iterative soft-thresholding have been proposed to solve the trace-norm minimization problem more efficiently. For instance, Cai et al. [5] proposed a Singular Value Thresholding (SVT) algorithm which is based on Uzawa X  X  algorithm [2]. A related approach based on linearized Bregman iterations was proposed by Ma et al. [20], Toh and Yun [25], while Ji and Ye [13] use Nesterov X  X  gradient descent methods for optimizing the trace-norm. While the soft-thresholding based methods for trace-norm minimization are significantly faster than SDP based approaches, they suffer from slow convergence (see Figure 2 (d)). Also, noisy measure-ments pose considerable computational challenges for trace-norm optimization as the rank of the intermediate iterates can become very large (see Figure 3(b)). Figure 1: (a) Time taken by SVP and SVT for random instances of the Affine Rank Minimization Problem (ARMP) with optimal rank k = 5 . (b) Reconstruction error for the MIT logo. (c) Empirical estimates of the sampling density threshold required for exact matrix completion by SVP (here C = 1 . 28 ). Note that the empirical bounds match the information theoretically optimal bound p and sizes n . Note that the incoherence is bounded by a constant, supporting Conjecture 2.3. Figure 2: (a), (b) Running time ( on log scale ) and RMSE of various methods for matrix completion problem with sampling density p = . 1 and optimal rank k = 2 . (c) Running time ( on log scale ) of various methods for matrix completion with sampling density p = . 1 and n = 1000 . (d) Number of iterations needed to get RMSE 0 . 001 .
 For the case of matrix completion, SVP has an important property facilitating fast computation of the main update in equation (2); each iteration of SVP involves computing the singular value decomposition (SVD) of the matrix Y = X t + P  X  ( X t  X  X  X  ) , where X t is a matrix of rank at most k whose SVD is known and P  X  ( X t  X  X  X  ) is a sparse matrix. Thus, matrix-vector products computing packages such as PROPACK [16] and ARPACK [19] that only require subroutines for computing matrix-vector products. In this section, we empirically evaluate our methods for the affine rank minimization problem and low-rank matrix completion. For both problems we present empirical results on synthetic as well as real-world datasets. For ARMP we compare our method against the trace-norm based singular value thresholding (SVT) method [5]. Note that although Cai et al. present the SVT algorithm in the context of MCP, it can be easily adapted for ARMP . For MCP we compare against SVT, ADMiRA [17], the OptSpace (OPT) method of Keshavan et al. [14], and regularized alternating least squares minimization (ALS). We use our own implementation of SVT for ARMP and ALS, while for matrix completion we use the code provided by the respective authors for SVT, ADMiRA and OPT. We report results averaged over 20 runs. All the methods are implemented in Matlab and use mex files. 4.1 Affine Rank Minimization We first compare our method against SVT on random instances of ARMP . We generate random matrices X  X  R n  X  n of different sizes n and fixed rank k = 5 . We then generate d = 6 kn random affine constraint matrices A i and compute b = A ( X ) . Figure 1(a) compares the computational time and shows that our method requires many fewer iterations and is significantly faster than SVT. Next we evaluate our method for the problem of matrix reconstruction from random measurements. As in Recht et al. [24], we use the MIT logo as the test image for reconstruction. The MIT logo we use is a 38  X  73 image and has rank four. For reconstruction, we generate random measurement matrices A i and measure b i = Tr ( A i X ) . We let both SVP and SVT converge and then compute the reconstruction error for the original image. Figure 1 (b) shows that our method incurs significantly smaller reconstruction error than SVT for the same number of measurements.
 Matrix Completion: Synthetic Datasets (Uniform Sampling) We now evaluate our method against various matrix completion methods for random low-rank ma-Figure 3: (a) : RMSE incurred by various methods for matrix completion with different rank ( k ) solutions on Movie-Lens Dataset. (b) : Time( on log scale ) required by various methods for matrix completion with p = . 1 , k = 2 and 10% Gaussian noise. Note that all the four methods achieve similar RMSE. (c) : RMSE incurred by various methods for matrix completion with p = 0 . 1 , k = 10 when the sampling distribution follows Power-law distribution (Chung-Lu-Vu Model). (d) : RMSE incurred for the same problem setting as plot (c) but with added Gaussian noise. trices and uniform samples. We generate a random rank k matrix X  X  R n  X  n and generate random Bernoulli samples with probability p . Figure 2 (a) compares the time required by various methods (in log -scale) to obtain a root mean square error (RMSE) of 10  X  3 on the sampled entries for fixed k = 2 . Clearly, SVP is substantially faster than the other methods. Next, we evaluate our method for increasing k . Figure 2 (b) compares the overall RMSE obtained by various methods. Note that SVP-Newton is significantly more accurate than both SVP and SVT. Figure 2 (c) compares the time required by various methods to obtain a root mean square error (RMSE) of 10  X  3 on the sampled entries for fixed n = 1000 and increasing k . Note that our algorithms scale well with increasing k and are faster than other methods. Next, we analyze reasons for better performance of our methods. To this end, we plot the number of iterations required by our methods as compared to SVT (Fig-ure 2 (d)). Note that even though each iteration of SVT is almost as expensive as our methods X , our methods converge in significantly fewer iterations.
 Finally, we study the behavior of our method in presence of noise. For this experiment, we generate random matrices of different size and add approximately 10% Gaussian noise. Figure 2 (c) plots time required by various methods as n increases from 1000 to 5000 . Note that SVT is particularly arising in SVT can be fairly large.
 Matrix Completion: Synthetic Dataset (Power-law Sampling) We now evaluate our methods against existing matrix-completion methods under more realistic power-law distributed samples. As before, we generate a random rank-k = 10 matrix X  X  R n  X  n and sample the entries of X using a graph generated using Chung-Lu-Vu model with power-law distributed degrees (see [23]) for details. Figure 3 (c) plots the RMSE obtained by various methods for varying n and fixed sampling density p = 0 . 1 . Note that SVP-NewtonD performs significantly better than SVT as well as SVP. Figure 3 (d) plots the RMSE obtained by various methods when each sampled entry is corrupted with around 1% Gaussian noise. Note that here again SVP-NewtonD performs similar to ALS and is significantly better than the other methods including the ICMC method [23] which is specially designed for power-law sampling but is quite sensitive to noise.
 Matrix Completion: Movie-Lens Dataset Finally, we evaluate our method on the Movie-Lens dataset [1], which contains 1 million ratings for 3900 movies by 6040 users. Figure 3 (a) shows the RMSE obtained by each method with varying k . For SVP and SVP-Newton, we fix step size to be  X  = 1 /p For SVT, we fix  X  = . 2 p using cross-validation. Since, rank cannot be fixed in SVT, we try various values for the parameter  X  to obtain the desired rank solution. Note that SVP-Newton incurs a RMSE of 0 . 89 for k = 3 . In contrast, SVT achieves a RMSE of 0 . 98 for the same rank. We remark that SVT was able to achieve RMSE up to 0 . 89 but required rank 17 solution and was significantly slower in convergence because many intermediate iterates had large rank (up to around 150 ). We attribute the relatively poor performance of SVP and SVT as compared with ALS and SVP-Newton uniformly distributed samples.
 Acknowledgements: This research was supported in part by NSF grant CCF-0728879.
