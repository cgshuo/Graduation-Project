 Choosing good features to represent objects can be crucial to the success of supervised machine learning methods. Recently, there has been a great interest in applying data mining techniques to construct new classification features. The rationale behind this ap-proach is that patterns (feature-value combinations) could capture more underlying semantics than single features. Hence the inclu-sion of some patterns can improve the classification performance. Currently, most methods adopt a two-phases approach by generat-ing all frequent patterns in the first phase and selecting the discrim-inative patterns in the second phase. However, this approach has limited successbecause it is usually very difficult to correctly iden-tify important predictive patterns in a large set of highly correlated frequent patterns
In this paper,we introduce the minimal predictive patterns frame-work to directly mine a compact set of highly predictive patterns. The idea is to integrate pattern mining and feature selection in or-der to filter out non-informative and redundantpatterns while being generated. We propose some pruning techniques to speed up the mining process. Our extensive experimental evaluation on many datasets demonstrates the advantage of our method by outperform-ing many well known classifiers.
 I.2.6 [ LEARNING ]: General Algorithms, Experimentation, Performance
Classification and pattern mining are two very important prob-lems in data mining and machine learning. Recently, there has been an increasing interest in studying the combination of these problems. Earlier studies mainly focused on associative classifi-cation , where a rule based classifier is built from high-support and high-confidence association rules [20, 18, 10, 27, 26]. Recently, the focus was more on using discriminative frequent patterns to de-fine new features in order to improve the quality of the learning algorithm [8, 12, 5].

Let us explain the rationale of using patterns in classification in terms of a linear classifier, like the famous SVM classifier [25]. Linear SVM tries to learn the optimal (maximum margin) hyper-plane that separates the classes. While this classifier can achieve very high accuracies on some data, it may completely fail on other data due to the fact that the classes cannot be linearly separated in the original feature space. One approach to overcome this problem is to use the kernel trick [11], which implicitly maps the data into a higher dimensional space. Pattern based classification is another more data-driven approach to solve the problem by trying to con-struct new classification features using pattern mining techniques.
Consider the example in Figure 1. The two classes c 1 and c cannot be separated by any linear classifier in the original 4 dimen-sional space. Now assume an oracle told us that this data contain junction of feature-value pairs). Using this information, we can map the data into a higher dimensional space by defining two ad-ditional binary features b 1 and b 2 ,where b i simply indicates the presence or absence of pattern P i in each data instance. After per-forming this dimensionality expansion, it becomes very easy for a linear model to separate the classes.
 Figure 1: An example illustrating the use of patterns in classi-fication
Frequent pattern mining is a very popular data mining technique to extract patterns from the data. Since their introduction in [1], fre-quent pattern and association rule mining have received a great deal of attention and have been successfully applied to various domains [30, 5, 17]. The key strength of frequent pattern mining is that it searches the space of patterns thoroughly by examining all patterns that occur frequently in the data. However, its main disadvantage is that the number of frequent patterns is usually very large. More-over, many of these patterns are redundant because they are only small variations of each other. Hence, it is not practical to use all frequent patterns in classification. The purpose of this study is to present a new way to intelligently select a small subset of frequent patterns that are useful for the classification task.

Most of the methods that use patterns in classification adopt a two-phases approach [8, 12, 5, 18, 20] by first mining all frequent patterns and then selecting the top k discriminative patterns to be used by the classifier. However, there are many inherent problems with this approach. First, it is not clear what is the optimal num-berofpatterns( k ) that should be used by the classifier. Second, the number of frequent patterns can be very large, making the ap-plication of an effective feature selection method computationally expensive. Third, the algorithm may falsely select many spurious patterns, where a spurious pattern is a pattern that seems interesting by itself, but is completely redundantwith respect to other patterns.
Having discussed these problems, it is important to develop a method to directly mine a small set of patterns that are highly pre-dictive and at the same time contain low redundancy . To achieve this goal, we first introduce the concept of minimal predictive pat-terns (MPP). The idea is to exploit the nested structure of the pat-terns in order to assure that every pattern in the result offers a sig-nificant predictive advantage over all of its generalizations (simpli-fications). After than, we propose an effective algorithm to directly mine these patterns. In contrast to the two-phases approach, our algorithm integrates pattern mining and feature selection and eval-uates each pattern as soon as it is generated. Finally, we present a very efficient pruning technique to approximately mine the MPP set. Our experimental evaluations clearly demonstrate the benefits of our method to efficiently learn very accurate classifiers.
Assume a dataset with only categorical features (attributes): all numeric features should be first discretized. Each ( feature , value ) pair is mapped to a distinct item in  X = { I 1 , ..., I l } a conjunction of items: P = I q 1  X  ...  X  I q k where I q a pattern contains k items, we call it a k-pattern (an item is a 1-pattern ). Assume an item I =( fea,val ) ,where fea is a feature and val is a value. Given a data instance x , we say that I fea ( x )= val and that P  X  x if  X  I j  X  P : I j  X  x .
 Given a dataset D = { x i } n i =1 , the instances that contain pattern P define a group D P = { x j | P  X  x j } .If P is a subpattern of P ( P  X  P ), then D P is a supergroup of D P ( D P  X  D P ). Note that the empty pattern  X  defines the entire population. The support of P is defined as: sup ( P )= | D P | / | D | .

In our framework, we are interested in mining patterns that are predictive of the class variable. So for pattern P ,wecandefinea rule R : P  X  c with respect to class label c . The confidenceof R is the posterior probability of c in group D P . Note that confidence of  X   X  c is the prior probability of c . We say that rule R : P is a subrule of rule R : P  X  c if c = c and P  X  P .

Given a dataset D = { x i ,y i } n i =1 in d dimensional feature space and a set of patterns  X = { P 1 , ..., P m } , D is mapped into a higher dimensional space with d + m dimensions as follows: x and b i,j =0 if P j /  X  x i The classification model is then learned in the new expanded fea-ture space D = { x i ,y i } n i =1 .
We say that pattern P is a frequentpattern if sup ( P )  X  where min_sup is a defined threshold. Frequent patterns obey the monotonicity property:  X  X f pattern P is not frequent, then all its su-perpatterns ( P  X  P ) are not frequent X . This property implies that if P is frequent, then all its subpatterns ( P  X  P ) are also frequent.
There are two important reasons for using frequent patterns in classification:
Even though some frequent patterns can be important predictors, using all frequent patterns in the classifier is not a good option for the following reasons:
Example 1: Consider a Bayesianbelief network example in Fig-ure 2, where we have a causal relation between feature F 1 class variable C . All other features are independentof C : F i  X  X  2 ..n } . Assume that pattern (item) F 1 = 1 is highly predictive of class c 1 , so that the posterior Pr( C = c 1 | F 1 = 1 larger than the prior Pr( C = c 1 ). Let us denote this pattern as P Clearly, P M is the only predictive pattern in this data. Figure 2: Illustrating the problem of spurious patterns in fre-quent pattern mining
Now consider a spurious pattern P S : F 1 = 1  X  F q 1 = v of variable F q i . The network structure implies that Pr( C = c  X 
Pr( C = c 1 | P M ). The problem is that if we evaluate the patterns individually (without considering the nested structure of the pat-terns), we may falsely think that P S is a predictive pattern because is totally redundant given P M . Clearly, the number of these spu-rious patterns can become huge. For example, if we assume that all variables are binary, there are 3 n  X  1  X  1 spurious patterns in this network structure.

The two-phases approach: This approach [8, 12, 5, 18, 20] performs feature selection on the result of a frequent pattern min-ing algorithm. However, the above analysis shows that many spu-rious patterns can be considered predictive using most interesting-ness measures [15]. To circumvent this problem, [8] suggested us-ing a redundancy score (e.g. the Jaccard score [15]) and selecting the features in an incremental way: a feature is added to the set of features if it is both predictive and has low redundancy to the fea-tures already selected. We can see that performing effective feature selection in the two-phases approach is:
Objective: Having discussed these problems, it is important to develop a principled way to directly mine the classification patterns. Basically, the method should return a small set of patterns that are highly predictive and at the same time contain low redundancy .In order to satisfy these requirements, we define the minimal predic-tive patterns concept. Definition We say that pattern P is a minimal predictive pattern (MPP) if there is a class label c such that P predicts c significantly better than all of its subpatterns (including the empty pattern  X  ).
We call these patterns minimal becauseremoving any non-empty combination of items from the pattern would cause a significant drop in its predictive power. We can see that the MPP definition prefers simple patterns over more complex patterns (the Occam X  X  Razor principal) becausepattern P is not an MPP if its effect on the class distribution  X  X an be explained X  by a simpler pattern ( P that covers a larger population.
Assume our task is to check whether pattern P is an MPP with respect to class c . Let us define rule R : P  X  c . Assume that group D P contains N instances, out of which N c instances belong to class c .Let P c be the highest confidenceachieved by any subrule of R : P c =max P  X  P Pr ( c | D P ) . The null hypothesis presumes that N c is generated from N according to the binomial distribution with probability P c . The alternative hypothesis presumes that the true underlying probab ility that generated N c is significantly higher than P c . Hence, we perform a one sided significance test (we are interested only in increases in the proportion of c ) using the bino-mial distribution and calculate its p-value as follows: If this p-value is smaller than a significance level  X  (commonly  X  =0 . 05 ), we concludethat P significantly improves the predictabil-ity of c over all its subpatterns, hence P is an MPP.
Geometrically, we can see group D P of pattern P as defining a distinct region in the feature space. Hence, when P is a composite pattern ( P contains more than one item), its region is the intersec-tion of the regions of all of its subpatterns: D P = Our objective is to identify those regions in the space that are im-portant for classification. Since this can be seen similar to the ob-jective of a decision tree, so let us start by illustrating the difference between these two approaches.

A decision tree partitions the space recursively by c hoosing at each time the best item to split the data. Therefore, the space is partitioned into non-overlapping regions. However, because the partitioning is performed greedily, it is very likely for a decision tree to miss important regions in the space. In comparison, frequent pattern mining performs a more complete search by exploring all possible frequent combinations of items. Therefore, it produces a large number of highly overlapping regions.

Now pattern P is an MPP if P predicts a class c significantly better than all of its subpatterns. This means that we cannot express the class distribution in region D P by any convex combination of the distributions in the regions that contain D P : [
C | D P ] = Therefore, P describes a distinct discriminative region D space where the class distribution in D P cannot be explained by any of its containing regions.
Let us go back to example 1 (Figure 2) to explain how the MPP framework tackles the problem of spurious patterns in frequent pat-tern mining. Pattern P S : F 1 = 1  X  F q 1 = v q 1 ...  X  F q MPP because P S will not be significantly more predictive than its subpattern P M : F 1 = 1 .

More generally, for pattern P : F q 1 = v q 1 ...  X  F q k an MPP, there should exist a path from each variable F q i class C that is not blocked (d-separated) by the other variables in the pattern: F q i not  X   X  C |{ F q 1 , ... , F q i  X  1 , F Because this is not the case in example 1, we expect these spurious patterns to be filtered out. In this section we explain our algorithm for mining the MPP set. The algorithm is outlined in Figure 3. Briefly, the algorithm ex-plores the spaceby performing an Apriori-like level wise search. At each level ( l ), we first remove the candidate l-patterns that do not pass the minimum support threshold (line 6). Then we extract all MPPs from those frequent l-patterns (line 7). Finally, we generate the candidates for the next level (line 8). This process is repeated until no more candidates can be generated.
 Algorithm 1: Mine all MPPs Input: Dataset: D , minimum support: min_sup Output: minimal predictive patterns: MPP //global variables 01: MPP =  X  , tbl_max_conf = hashtable() //the prior distribution of the class 02: tbl_max_conf [ h (  X  )]= calculate_class_distribution ( 03: Cand = generate_1_patterns() 04: l =1 05: while ( Cand = X  ) 06: FP [ l ] = prune_infrequent ( Cand , D , min_sup ) 07: extract_MPP ( FP [ l ], D ) 08: Cand = generate_candidates ( FP [ l ]) 09: l = l +1 10: return MPP
Figure 3: The algorithm for mining the MPPs from a dataset
The process of testing if frequent pattern P is an MPP is not triv-ial because the definition requires us to check the pattern against all its subpatterns. This requires checking all 2 l  X  1 subpatterns if P has length l . Our algorithm avoids this inefficiency by caching the statistics needed to perform the MPP test within the (l-1)-subpatterns from the previous level. This part of the algorithm is outlined in Figure 4.

To explain the method, it is useful to envision the progress of the algorithm as gradually building a lattice structure level by level, starting from the empty pattern  X  . An example lattice is shown in Figure 5. Every frequent l-pattern P is a node in the la ttice with l children: one child for each of its (l-1)-subpatterns .
The key idea of our algorithm is to store in each node P the maximum confidence score for every class that can be obtained by the patterns in the sublattice with top P (including P itself): max _ conf P [ c ]= max (Pr( c | D P )):  X  P  X  P .These max_conf values are computed from the bottom up as algorithm progresses.
Initially, max _ conf  X  for the empty pattern is set to be the prior distribution of the class variable. In order to compute max _ conf for pattern P , we first compute conf P (line 2), the distribution of the class variable in group D P : conf P [ c ]=Pr( c | we use the max_conf values of P  X  X  direct children to compute max _ conf _ children P (line 3) so that: max _ conf _ children P [ c ]= max (Pr( c | D P )):  X  P  X  we compute max _ conf P by taking the element-wise maximum of two arrays: conf P and max _ conf _ children P (line 4).
Now we want to check if P is an MPP. So for each class label c , we perform the MPP significance test to check if P predicts c
Algorithm 2: extract_MPP ( FP [ l ], D ) //add pattern P  X  FP [ l ]to MPP if P is significantly more predictive than all its subpatterns 1: for each P  X  FP [ l ] 2: conf = calculate_class_distribution ( P , D ) 3: max_conf_children = max { tbl_max_conf [ h ( S l  X  1 ) 4: max_conf = max { conf , max_conf_children } 5: tbl_max_conf [ h ( P ) ]= max_conf 6: if ( is_MPP ( P , max_conf_children , D )) 7: MPP = MPP  X  P 8: lossless_pruning ( P , max_conf , D , FP [ l ]) Function is_MPP ( P , max_conf_children , D ) //return true if P predicts any class significantly better than all its subpatterns N = count ( P , D ) for each class label c return false Function lossless_pruning ( P , max_conf , D , FP [ l ]) //prune P if it cannot produce any MPP for any class for each class label c remove ( P , FP [ l ]) Figure 4: The algorithm for extracting the MPPs from the fre-quent patterns at level l Figure 5: An illustrative example showing the frequent pattern lattice associated with I 1  X  I 2  X  I 3 . The MPPs are shaded significantly better than max _ conf _ children P [ c ]. If the test is positive, we add P to the MPP set (line 7).

Please note that in our implementation, we do not explicitly build the frequent pattern lattice. Instead, we use a hash table tbl_max_conf to provide direct access to the max_conf values. So that tbl_max_conf [ h ( P )] = max _ conf P ,where h is a hash function. Also note that none of the functions ( calculate_class_distribution , is_MPP and lossless_pruning )requires anotherscan on the data be-cause we can collect the class specific counts during the same scan that counts the pattern.

Figure 5 illustrates the algorithm using a small lattice on a dataset that contains 200 instances from class c 1 and 300 instances from class c 2 . For each pattern P (node), we show the number of in-stances from each class in group D P , the distribution of the classes in D P ( conf ) and the maximum confidence from P  X  X  sublattice ( max_conf ). Let us look for example at pattern I 1  X  I 2 .Thispat-tern is predictive of class c 2 : conf ( I 1  X  I 2  X  c 2 )=0 ever, it is not an MPP because it does not significantly improve the predictability of c 2 over subrule I 2  X  c 2 : Pr binomial 100 , 0 . 7) = 0.16 (not significant at significance level  X  The MPPs from this example are: I 1 , I 2 and I 1  X  I 3 .
We say that pattern P is pruned if we do not examine any of P  X  X  superpatterns ( D P  X  X  subgroups). This can be seen as excluding the entire sublattice with bottom P from the pattern lattice.
The Apriori algorithm relies only on the support of the patterns and prunes the infrequent patterns according to the monotonicity property. However, frequent pattern mining becomes computation-ally very expensive when:
All of these reasons cause the frequent pattern lattice to become extremely huge. One simple way to speed up the algorithm is to set min_sup very high. However, this solution is not very effective be-cause the algorithm may miss many important patterns. In fact, [8] argued that the discriminative power of very high support patterns is boundedby a small value 1 . Therefore, we need a method that can utilize the discriminative ability of the patterns in order to further restrict the search space. We present two pruning techniques: the first one is lossless (does not miss any MPP), while the second is lossy . The MPP significance test can help us to prune the search space. This pruning is implemented by the lossless_pruning function in Figure 4. The idea is to prune pattern P if we guarantee that P cannot produce any MPP. However, since our algorithm is applied in a level-wise fashion, we do not know what subgroups P will generate further in the lattice. To overcome this, we define the optimal subgroup D P  X  c the subgroup that contains all the instances from c i and none of the instances from any other classes. Clearly, P cannot generate any subgroup better than D P  X  c prune P if for every class c i , P  X  c i does not significantly improve
This is analogous to the stop words in document retrieval or text categorization. the predictability of c i with respect to the current best prediction ( max_conf [ c i ]). Please note that this pruning technique does not miss any MPP because the lossless_pruning test is anti-monotonic.
As an example, consider pattern P = I 1  X  I 2  X  I 3 in Figure 5. This pattern contains 15 examples, 5 from class c 1 and 10 from class c 2 .Both P  X  c 1 and P  X  c 2 are not significant at  X  =0.05 with re-spect to the current best predictions 0.6 and 0.75 (for c respectively). Therefore, we can safely prune P .
This techniqueperforms lossy pruning, which meansthat it speeds up the mining, but at the risk of missing some MPPs. We refer to the set of patterns mined using the lossy pruning method as A-MPP (Approximated MPP). The idea is to prune pattern P if P does not show any sign of being more predictive than its subpatterns. To do this, we simply perform the MPP significance test, but at a higher significance level  X  2 than the significance level used in the original MPP test:  X  2  X  [  X , 1] .If P does not satisfy the MPP test with respect to  X  2 , we prune P .

We can see that  X  2 is a parameter that controls the tradeoff be-tween efficiency and completeness. So if we set  X  2 = 1 ,thenwe do not do any lossy pruning. On the end of the spectrum, if we set  X  2 =  X  , then we prune every non-MPP pattern, i.e., we require every subpattern of every MPP to be MPP as well.
In this example, we discuss the effect of item correlation on the efficiency of pattern mining. High correlation between items causes many long patterns to becomefrequent, and therefore severely slows down the mining algorithm. For instance, assume items I 1 , ..., I n occur often together in the data. Let P = I 1  X  ...  X  I n .if P is frequent, then the algorithm has to generate all patterns in the lattice with top P . However, because of the high correlation, all of these patterns are going to cover very similar groups (regions) in the data. Therefore, the class distribution will be very similar: [
C | D P ]  X  [ C | D P ] :  X  P ,P  X  P . Applying the lossy prun-ing can save us a lot of computation because P  X  X  lattice will not be explored beyond the second level. The reason that any 2-pattern P = I i  X  I j  X  P is likely to fail the MPP test with  X  2 &lt; because P will not show any potential of being different from its parents: [ C | D I i  X  I j ]  X  [ C | D I i ]  X  [ C | D I j Consider the XOR example, where we have two binary features F 1 and F 2 and a class variable defined as: C = F 1 XOR F 2 . Learning any linear classifier on this data is hopeless because no hyperplanecan separate the two classes. Besides,trying to par tition the space using a decision tree is also unlikely to succeed. Clearly, the data contains the 4 MPPs: { F 1 = 0  X  F 2 = 0 , F 1 =  X 
F 2 = 0 , F 1 = 1  X  F 2 = 1 } . By adding these MPPs as features, it becomes extremely easy for a linear classifier to perfectly classify the data 2 .

However, the bad news is that the lossy pruning method will miss all of these MPPs. The reason is that the distribution of the class variable in the group of any item is the same as the prior distribu-tion: [ C | D I j ] = [ C ] : I j  X  X  F 1 = 0 , F 1 = 1 , F
A kernel-based classifier is also likely to succeed on this toy ex-ample. none of these items will show any predictab ility signal and the al-gorithm will falsely prune all of them for any  X  2 &lt; 1
However, in real-world data, it is not very common for the class to follow an XOR-like distribution and our experiments will demon-strate that the lossy pruning can achieve orders of magnitude speed up while recovering most MPPs.
In this section we present our experimental evaluation on 20 dif-ferent UCI classification datasets [3]. Columns 1 to 4 in Table 1 show the main characteristics of the datasets (number of features, number of items, number of records and number of classes).
The experiments compare the performance of the following meth-ods:
We used the SVM implementation provided by the LIBSVM [7] package. For methods that use patterns in classification ( two-phases , DT_fea , MPP , A-MPP ), we learn a linear SVM classifier in the space of the original features plus the induced patterns.
For two-phases , MPP and A-MPP , we set the minimum support parameter ( min_sup ) to 10% the number of instances in the dataset. For MBST , we found that the performance of the algorithm (execu-tion time and classification accuracy) is very sensitive to the invoca-tion min_sup parameter. Unfortunately, the authors did not provide any guidance on how to set this parameter. We tried many values and found that se tting the invocation min_sup to 30% is a reason-able choice for our datasets. Please note that setting min_sup = 10% for MBST is impractical because the algorithm becomes extremely inefficient even on simple datasets.

All classification results are reported using a 10 folds cross-validation scheme, where we use the same train/test splits for all of the com-peting methods.
Table 1 compares the classification accuracy of A-MPP against several famous classifiers. We can see that A-MPP is the best per-forming method on most datasets and achieves on average the high-est classification accuracy. It is interesting to notice that the uncon-ventional method of using decision tree patterns with SVM is able to produce good classifiers on several datasets. However, due to its restricted heuristic-based search, DT_fea is often outperformed by A-MPP . Also note that the performance of SVM_RBF is not consis-tent. It is the best method on some datasets (nursery, car and E.coli) and the worst method on other datasets (wine, heart-disease, credit and parkinson). This suggests that pattern based SVM can be con-sidered an attractive alternative to kernel based SVM.

Let us discuss the behavior of the different classifiers on the tic-tac-toe dataset. We choose this data because the high interaction between the features greatly illustrates the usefulness of patterns in classification. This dataset encodes the different board configura-tions at the end of the tic-tac-toe game. Each data instance (board configuration) has 9 features, one for each of the 9 squares. Each feature takes one of the 3 values: 1=player x has taken, 2=player o has taken and 3=blank. The goal is to learn the target concept  X  X in for player x X . Although this task may seem easy for a human, it is not straightforward for a machine learning method to automatically learn the concept from data.

Table 1 shows that SVM_linear is the worst performing method on tic-tac-toe because the classes (win vs loose) cannot be linearly separated in the original 9 dimensional space. DS is the second worst method because even by applying the boosting technique, decision stumps (1-patterns) are still too simple to explain the re-lation between the features and the class. SVM_RBF improves the accuracy over SVM_linear because of its ab ility to form complex decision boundaries. However, it is s till not able to clearly explain the model. Decision trees ( DT and fea_DT ) have acceptable per-formance because they are able to capture some predictive patterns in this data. However, they fail to extract all important patterns. Finally, A-MPP is able to identify most of the important patterns (achieving an accuracy of 98.33%) and it outperforms all other methods by a wide margin.
Table 2 shows the classification accuracy and the execution time (in seconds)for two-phases , MBST , MPP and A-MPP . The reported execution time is measured on a Dell Precision T7500 machine with an Intel Xeon 3GHz CPU and 16GB of RAM. All methods are implemented using matlab. The entries in the table with  X  X A X  indicate that the method run for 15 hours without finishing! The last three columns in table 2 show the total number of closed pat-terns (the first phase of two-phases ), the number of MPPs and the number of approximated MPPs.

Let us first discuss the classification accuracy of the different methods. We can see that MPP and A-MPP are the best performing methods on most datasets. Notice that the performance of MBST is not consistent. It performs very good on some datasets (e.g. breast cancerand mammographic), while it performs much worse than the other methods on other datasets (e.g. glass, pen-digits, tic-tac-toe and car).

Now Let us now focus on the efficiency aspect of the different methods. The results in table 2 show clearly that efficiency is a big concern for the two-phases , MBST and MPP methods. For instance, all of these methods fail to complete the mining on the mushroom, WDBC, image-seg and parkinson datasets using the 15 hours time budget. On the other hand, we can see that A-MPP is extremely fast (the longest execution time for A-MPP is on the mushrooms dataset, which took around 7 minutes).

Let us considerthe lymphography dataset. Mining all closed fre-quent patterns took 23,473 seconds (around 6.5 hours). Applying the lossless pruning in MPP helps reducing the execution time to 6,293 seconds. Finally, applying the lossy pruning made the min-ing almost instant (4 seconds). It is interesting to see that this huge computational savings was at the cost of missing only 2 out of 33 MPPs. In fact, for many datasets, A-MPP did not even miss any MPP!
These results show that A-MPP is a very accurate method and achieves orders of magnitude speed up over the state-of-the-art pat-tern based classification methods.
Frequent pattern mining is a very important research area in data mining. The original framework [1] has been extended to mine several types of data, including temporal [30], time series [5], text [21] and graph datasets [17, 12].

A lot of research has been conducted to reduce the output size of frequent patterns. [4] proposed the concept of closed frequent pat-terns to perform lossless compression of frequent patterns. More powerful compressionusually rely on lossy compressionusing tech-niques like maximal patterns [19], clustering [28], pattern profiles [29], MDL compression [23] or maximum entropy patterns [24]. However, most of these methods are employed in an unsupervised setting to summarize the large collection of frequent patterns using fewer patterns. The objective of our work is different because we are mainly interested in selecting patterns that are important for the classification task.

The usage of frequent patterns in classification has been explored by many recent studies. Earlier approaches mainly focused on as-sociative classification [20, 18, 10, 27, 26], which mines predic-tive association rules and builds a rule-based classifier. The results in [20, 18] showed that associative classification can be more ac-curate than heuristic C4.5 [22]. Top-k rule mining [10] discovers top-k covering rules from high dimensional gene expression pro-files. Harmony [27] uses an instance-centric approach and assures that the highest confidence rule for each training is included in the classifier. [26] applies a lazy classification philosophy and mines the rules on demand by focusing only on the test instance.
Recently, the focus was more on using discriminative frequent patterns to map the data into a new feature space in order to im-prove the quality of the classifier. Most of the met hods [8, 12, 5] first mine all frequent patterns and then select the discriminative ones. [13, 9] are two very recent studies that do not apply the two-phases approach. The Model Based Search Tree (MBST) method [13] builds a decision tree using discriminative frequent patterns. Our experiments showed that we are able to outperform MBST in terms of both classification accuracy and efficiency. The Direct Discriminative Pattern Mining (DDPMine) method [9] is similar to [13] in that it mines the most discriminative patterns on progres-sively shrinking subsets of the data. The main difference is that DDPMine [9] uses the sequential covering paradigm instead of de-cision tree and performs some pruning to speed up the mining.
In this paper, we present a novel feature construction method us-ing minimal predictive patterns. Our framework applies a statistical test to ensure that every pattern in the result offers a significant pre-dictive advantage over all of its subpatterns. We show that this can effectively filter out many spurious patterns and produce a compact set of predictive patterns. Geometrically, each pattern P in the re-sult represents a distinct region D P in the feature space where the class distribution is surprisingly skewed and cannotbe explained by the regions that contain D P . Our classification approach is to iden-tify these predictive regions, model them explicitly as new features and then learn a linear classifier in the new expanded space.
We present an algorithm to directly mine the MPPs. Our algo-rithm works in a level-wise fashion by examining the general pat-terns first and gradually testing and adding more specific patterns to the result. In contrast to the traditional two-phases approach, we integrate pattern mining and feature selection by evaluating each pattern with respect to its subpatterns as soon as it is generated. Finally, we present a very efficient version of the algorithm to ap-proximately mine the MPP set.

Experimental results clearly demonstrate the benefits of our ap-proach by outperforming many well known classifiers. Besides, we show that applying the lossy pruning technique makes the mining process very efficient without sacrificing the classification perfor-mance.
This research work was supported by grants 1R21LM009102-01A1, 1R01LM010019-01A1, and 1R01GM088224-01 from the NIH. Its content is solely the responsib ility of the authors and does not necessarily represent the official views of the NIH. [1] R. Agrawal, T. Imielinski, and A. Swami. Mining association [2] R. Agrawal and R. Srikant. Fast algorithms for mining [3] A. Asuncion and D. Newman. UCI machine learning [4] Y. Bastide, N. Pasquier, R. Taouil, G. Stumme, and [5] I. Batal, L. Sacchi, R. Bellazzi, and M. Hauskrecht. [6] L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. [7] C.-C. Chang and C.-J. Lin. LIBSVM: a library for support [8] H. Cheng, X. Yan, J. Han, and C. wei Hsu. Discriminative [9] H. Cheng, X. Yan, J. Han, and P. S. Yu. Direct discriminative [10] G. Cong. Mining top-k covering rule groups for gene [11] C. Cortes and V. Vapnik. Support-vector networks. Machine [12] M. Deshpande, M. Kuramochi, N. Wale, and G. Karypis. [13] W. Fan, K. Zhang, H. Cheng, J. Gao, X. Yan, J. Han, P. Yu, [14] Y. Freund and R. Schapire. A short introduction to boosting. [15] L. Geng and H. J. Hamilton. Interestingness measures for [16] J. Han, J. Pei, and Y. Yin. Mining frequent patterns without [17] M. Kuramochi and G. Karypis. Frequent subgraph discovery. [18] W. Li, J. Han, and J. Pei. CMAR: Accurate and efficient [19] D.-I. Lin and Z. M. Kedem. Pincer-search: A new algorithm [20] B. Liu, W. Hsu, and Y. Ma. Integrating classification and [21] H. Lodhi, C. Saunders, J. Shawe-Taylor, N. Cristianini, [22] J. R. Quinlan. C4.5: programs for machine learning . Morgan [23] L. M. Siebes A, Vreeken J. Item sets that compress. In [24] N. Tatti. Maximum entropy based significance of itemsets. [25] V. Vapnik. The Nature of Statistical Learning Theory . [26] A. Veloso, W. Meira Jr., and M. J. Zaki. Lazy associative [27] J. Wang and G. Karypis. Harmony: Efficiently mining the [28] D. Xin, J. Han, X. Yan, and H. Cheng. Mining compressed [29] X. Yan, H. Cheng, J. Han, and D. Xin. Summarizing itemset [30] M. J. Zaki. Spade: an efficient algorithm for mining frequent
