 1. Introduction
Patent classification schemes are the basic reference criteria used to search for the right collection of patent documents for a given patent specification of a patent examiner. The patent examiner first wants to locate a collection of relevant patent documents in a classification that matches the applied specification, and then to refine the search. The accurate classification of patent documents is a crucial first step in an accurate search, even for the novice searcher.
A number of statistical classifications and machine learning techniques have been applied to text categori-zation, including nearest neighbour classifiers ( Lam &amp; Ho, 1998; Yang, 1999 ), decision trees ( Apte, Damerau, &amp; Weiss, 1998 ), Bayesian probabilistic approaches ( Baker &amp; Mccallum, 1998; McCallum &amp; Nigam, 1998 ), support vector machines ( Joachims, 2002 ), and neural networks ( Ng, Goh, &amp; Low, 1997; Wiener, Pedersen, &amp; Weigend, 1995 ). Because a patent is a text, these techniques can be applied to patent categorization.
However, as patent documents are structural documents with their own distinguishing characteristics from general documents, these characteristics should be considered in the patent categorization. The characteristics of patent documents are as follows ( Iwayama et al., 2003 ): 1. Patents are structured by claims, purposes, effects, embodiments of the invention, and so on. 2. To enlarge the scope of invention, vague or general terms are often used in claims. 3. Patents include much technical terminology. Applicants may define and use their original terms not used in other patents. 4. There are large variations in length. In 340,512 Japanese patents issued in 1996, the length of the longest patent is 420,039 bytes (containing about 60,000 Japanese words,) and the length of the shortest patent is 308 bytes (containing about 45 Japanese words). The average length of the patents is 9805 bytes (containing about 1522 Japanese words).

In this paper, we categorize Japanese patent documents automatically, focusing on the first characteristic from the above.

Japanese patent documents are structured into a sequence of normative sections (or large narrative text of Drawings  X  , and  X  Drawings  X  as in Table 1 .

Some sections, such as  X  Abstract  X  and  X  Description  X  , consist of more detailed components (or elements) with names like [prior art], [application field], [means of solving problems], [effects of invention], [examples of embodiment], and so on. Such detailed components are used to improve the readability, but their tags are named by the patent applicant and have some variations even though they should have one meaning.
In this context, we will call detailed components  X  X  X pplicant elements X  X  and refer to their names as  X  X  X ppli-cant-defined tags X  X .

For example, components with applicant-defined tags, like [prior art] and [application field], can be more helpful to classify patent documents than those of the other components because they include more informa-tion related to the background and field. Because they represent the whole patent document and are so often used in the  X  Abstract  X  section, the contents of components with [purpose of invention] and [means of solving problems] are as important as the  X  Claims  X  section. Therefore, if the detailed components are considered as major features for patent categorization, high performance can be achieved. This is the basic claim of this paper.

In previous works, the structural characteristic of patent was not properly considered. Larkey (1999) cre-ated a tool for classifying by US patent codes based on a k-Nearest Neighbors (k-NN) approach. She indexed patents. Koster et al. (2003) used the Winnow algorithm ( Grove, Littlestone, &amp; Schuurmans, 2001 ) to classify patent applications. They used only abstracts instead of the full-texts of the patents as features, but were unable to show better performance. Fall, To  X  rcsva  X  ri, and Karetka (2003) indexed various fields of the patent documents and then performed categorization tests on them. The titles (a) or the claims sections (b) are first used to describe each document. Next, the first 300 words from the titles, inventors, applicants, abstracts, and ing each patent. The most precise categorization was achieved when they indexed the first 300 words of each document.

These previous methods used the some sections or the front part of document without the semantic analysis of the structural document. In this paper, we categorize a given patent document by using a k -NN approach.
That is, the given patent is classified into the categories of k documents similar to it. When we retrieve similar documents, this paper will try to compare the contents of same semantic elements instead of the whole texts; however, because there are many applicant-defined elements (components with applicant-defined tags) that should be mapped to one semantic element in patent documents, it is first necessary to cluster these applicant-defined elements in order to contribute them to a small fixed set of meaningful  X  X  X emantic elements (fields) X  X  for effective categorization.

The rest of this paper is organized as follows. In Section 2 , we will describe the whole system. Section 3 shows experiments on our model in the patent categorization. Section 4 is devoted to the discussion, conclu-sion and the future work. 2. Patent categorization method 2.1. Preliminaries: patent document structure We observed that a patent document consists of sections (or large fields) with fixed name tags, like
 X  Abstract  X  ,  X  Claims  X  , and so on. Each section may be divided into several components that have named tags provided by the patent applicant. Such components are called  X  X  X pplicant element X  X  and their tags (or titles) are named  X  X  X pplicant-defined tag X  X . However, we now assume a semantic structure mapped from the above will be described in later subsections. 2.2. Overall system description
Fig. 1 shows the overall system architecture. The system is composed of an indexing phase, retrieval phase, and categorization phase.

In the indexing phase (Phase 1, shown in Fig. 1 ), patent documents in the training set are indexed in order to retrieve similar documents for a given query document. Before indexing, we re-organize and divide each document in the training set by previously defined semantic tags. Index files are respectively built by keywords extracted from each divided semantic field. In Fig. 1 , the semantic fields are represented by Field kit 1 (versions 3.1) was used for document indexing and retrieval in this paper.

In the retrieval phase (Phase 2, shown in Fig. 1 ), we retrieve similar documents for a given query document which should be classified by using indexing files built in the previous phase. Like the indexing phase, we also re-organize and divide a query document by the previously defined semantic tags. Next, keywords are extracted from each divided field, and then respectively corresponding queries are made for the retrieval.
We retrieve similar documents for each query by using indexing files for each semantic field. These retrieval results are merged and then generate a list of similar N documents. This list is used as an input in the next phase.

Finally in the categorization phase (Phase 3, shown in Fig. 1 ), we assign classification codes to the query documents by using classification codes of similar N documents retrieved in the previous phase. When we cal-culate the score of the classification code, the similarity score and the rank of retrieved documents are considered.
 2.3. Document indexing
In the indexing phase, we index patent documents in the training set in order to retrieve similar documents for a given query document. Before we index, it is necessary to observe the structure of a patent document because a patent document has its own characteristics. As mentioned in the introduction, Japanese patent doc-uments are structured into a sequence of normative sections for the  X  Bibliography  X  ,  X  Abstract  X  ,  X  Claims  X  , of the more detailed elements with names like [prior art], [application field], [means of solving the problems], [effects of the invention], [examples of embodiment], and so on. While the titles of sections are fixed, the names of the detailed elements are applicant-defined. Because applicants decide the names of the detailed components with important words that represent the contents of the components, it can be said that names of the applicant elements have applicant-defined meanings; therefore, we call them  X  X  X pplicant-defined tags X  X .
Even if applicants write about the same content, they can label tags differently. Actually, 3516 applicant-defined tags are used in the  X  Abstract  X  and  X  Description  X  sections among 347,227 Japanese patent documents issued in 1993. Table 2 shows examples of applicant-defined tags with high frequency and Fig. 2 shows the distribution chart of applicant-defined tags. Most of them have a low frequency while several tags are used frequently in the patent documents.

From Table 2 and Fig. 2 , we can infer that patent applicants have a tendency to describe their inventions by using the tag naming of the important components. However, in order to utilize these applicant-defined tags for our purposes, we should classify them into several fixed classes of  X  X  X emantic tags X  X . The basic process is based on the assumption that the head nouns of applicant-defined tags are the major feature of classification into a semantic tag. Firstly, we extract head nouns from applicant-defined tags by using a heuristic rule: the last simple noun of tag is a head noun (e.g. N N head (N head head nouns according to their frequency in applicant-defined tags. From all applicant-defined tags, 1475 head nouns are extracted (see Table 3 ). Note that the 100 most frequent head nouns out of the total 1475 are found in 1940 applicant-defined tags out of 3516 in total. However, those 1940 applicant-defined tags, including only 100 high frequent head nouns, cover 99.85% of the total cumulative occurrences of applicant-defined tags.
This shows that the most frequently occurring head nouns of applicant-defined tags are the most crucial fea-ture of classification.

According to observations on the most frequently 100 head nouns with high frequency, the patent appli-cants describe their inventions with the following basic structure: Title of invention.
 Purpose of invention.
 Background (Prior art, Background of invention).
 Application field.
 Problems to be solved.
 Means of solving problems.
 Claims.
 Detailed explanation (effects of invention, Composition, Operation, Advantage, etc .).
 Embodiment examples.

The problems to be solved by the invention are almost always included in the  X  X  X urpose of invention X  X  field, and both the  X  X  X ackground X  X  and  X  X  X pplication field X  X  fields include information related to the technical back-ground or field. These assumptions are verified through the similarity computation between description pat-terns or keywords most commonly used in each field. Therefore, we treat  X  X  X urpose of invention X  X  and  X  X  X roblems to be solved X  X  field equally.  X  X  X ackground X  X  and  X  X  X pplication field X  X  are also equally dealt with, and because  X  X  X urpose of invention X  X  represents the invention itself, it can be said that  X  X  X urpose of invention X  X  is related to the title of the invention. Therefore, we decided that applicant-defined tags are classified into six semantic tags, such as  X  X  X echnical field X  X ,  X  X  X urpose X  X ,  X  X  X ethod X  X ,  X  X  X laim X  X ,  X  X  X xplanation X  X , and  X  X  X xample X  X .
We manually classify 1940 applicant-defined tags into six semantic tags by their most frequently occurring 100 head nouns. Some useless applicant-defined tags, such as (equation), (table) and (picture), are not classified and removed. Table 4 shows examples of classified applicant-defined tags which were changed into semantic tags.

It is possible to classify one applicant-defined tag into multiple semantic tags if it has a coordinate conjunc-tion or a pause, such as  X  X  (the means of solving the problem and the operation) X  X .

Although we can classify content without any applicant-defined tag by using machine learning technology, by using the description patterns or keywords in each applicant element, this paper does nothing but classi-fication based on the head nouns of the applicant-defined tags. We, therefore, ignore other unclassified appli-cant-defined tags.

In summary, Fig. 3 shows the re-organization of patent documents into six semantic tags. Some applicant elements may be deleted due to ignored applicant-defined tags, and some elements can be assigned to more than one semantic field due to the multiple classifications of applicant-defined tags. Keywords are extracted from each element and build index files, respectively for retrieval. We restrict keywords to single nouns extracted by ChaSen 2 , Japanese morphological analyzer. 2.4. Document retrieval In the retrieval phase, we retrieve similar documents for a given query document which should be classified.
Like the indexing phase, the query document is re-organized into six fields with the previously defined six semantic tags. That means six queries are generated for the retrieval, while the weights of the keywords are assigned by the term frequency of the query document.

The unimportant terms are deleted from the keywords of the query. From 500 nouns with high document frequency, 67 stopwords are collected by hand (e.g.  X  X  (thing) X  X ,  X  X  (invention) X  X ,  X  X  (object) X  X ,  X  X  (problem) X  X ,  X  X  (problem) X  X ,  X  X  (claim) X  X ,  X  X  (mention) X  X ).

When retrieving the similar patents for the given query patent, each field of the meaningful pairs of seman-tic tags are compared instead of the whole texts. The two documents can be considered similar if they are in the same technical classes and have the same (or similar) problem and solution (method). The simplest form of similarity computation is to retrieve the target documents whose semantic fields are respectively similar pair-wise to those of the query document, as shown in Fig. 4 .

However, if we exclusively compare semantic fields with the same tag in a pair-wise manner, the retrieval performance can decrease for the following reasons: 1. To enlarge the scope of invention, vague or general terms are often used in claims. If we compare the claim of the query with that of the target document, the recall goes down. 2. We cannot fully trust user-defined applicant-defined tags because the described content can be different from the content represented by the applicant-defined tag. For example, some writers describe the problem and method together, even using the single tag name  X  X  X he problem of the invention X  X . 3. We cannot fully trust semantic tags because they are semi-automatically classified based on the head nouns, and a semi-automatic process can cause errors. For example, although  X  X  ( i.e. , explanation of the problem) X  X  should be classified into  X  X  X urpose X  X , it is classified into  X  X  X xplanation X  X  by the previously described classification method based on the head nouns.

Cross comparison helps avoid the errors of the previous classification of semantic titles. Actually, in our experiment, cross comparison brought a 2.6% improvement of accuracy. We will describe this result in Chap-ter 3 detail.

Therefore, we allow cross comparison, which is document retrieval by possible 36 pairs between 6 queries and 6 index files, as in Fig. 5 .

Firstly, six results are retrieved from six index files in the document set for one query and merged. For one query, M similar documents are retrieved from one index file. This merging procedure is repeated for six que-ries. Six merged results are merged again, and then a list of similar N documents for a query document is gen-erated finally. In this paper, we set both M and N to be 200. A list of similar N documents for a query document is used as an input in the next phase.
 Thirty six retrieval results produced by cross comparison are merged by the following equation:
We let R ( Q i , T j ) represent a retrieval result found in the index file T similarity scores for M retrieved documents. w i is the weight value for the query Q for the index file T j when query is Q i . All weight values are learned in a development set (a training set for developing a system). For example, the weight w tm is calculated by Eq. (2) . 2.5. Document categorization
Finally in the categorization phase, classification codes are proposed for the given query document by using the classification codes of the similar N documents retrieved in the previous phase, as in Fig. 6 .
Classification codes are ordered by their scores and selected top C (=100 in this paper) codes as an final result for patent categorization. When we calculate the score of the classification code, score the similarity score and the rank of retrieved documents are considered as shown in the following equation: score doc ( c , d ) is the similarity score of a document d in retrieved documents D and weight document d where d is included in the category c . score category given query document is assigned to the several categories that have the high values in all of score
Because we thought top r documents are more meaningful among N retrieved documents, we changed uments are considered with small weights. In our experiment, we achieved good performance when the value of r ranged from 5 to 20, and a was 0.1. 3. Experimental results
The effectiveness of our patent categorization method, which was introduced in Section 2 , was evalu-ated using the  X  X  X ormal run query set of the Classification Subtask in NTCIR-5 Patent Retrieval Task
Between the two subtasks of  X  X  X heme Categorization X  X  4 and  X  X  X -term Categorization X  X  Subtask, we deal with only the  X  X  X heme Categorization X  X  subtask in this paper.

The query set consists of 2008 patent documents. A ranked list of 100 possible themes among 2519 themes for each query is submitted as the output of the classification. A patent document set that consists of 1.7 mil-lion documents issued from 1993 to 1997 can be used for training.

Recall and precision values are calculated for each query document, and these are summarized in the MAP (Mean Average Precision) value. This is the standard TREC-style evaluation method for document retrieval.
In our case, a query corresponds to a query document, and its retrieved documents correspond to assigned categories. MAP values are averaged over query documents. 3.1. Experimental results in the development set
Before conducting an experiment, we verified our method in the small scaled development set and learned the weight values used in the cross comparison. The development set consisted of 1000 query documents and 50,000 training documents. The query documents were randomly extracted from the training set of year 1995, and the training documents were collected from the training set of year 1993.
 We compared three kinds of systems as follows:
BaseSystem : the baseline system using indexing files and queries constructed by terms extracted from whole texts in documents.

FSystem (FieldByFieldSystem) : the system using indexing files and queries constructed by terms extracted from some detailed semantic fields of documents (proposed system in this paper).

SSystem (SectionBySectionSystem) : the system using indexing files and queries constructed by terms extracted from some normative sections of documents (a system for comparison with proposed system).
Three systems use same parameter values ( r = 10 and a = 0.1). The simplest way to retrieve similar docu-ments for a given patent document is to compare the whole text. BaseSystem does not use the structural infor-mation of the patent at all. The MAP value of this system is 0.2939 in the development set.
 Table 5 shows the categorization result produced by the cross comparison in FSystem.

The first cell located in the first row and the first column indicates the MAP of the categorization result file. The performance of some results, such as R ( Q t , T performance of the results using the whole text as an index file, although the size of index files is smaller. We can observe that the  X  X  X echnical Field X  X ,  X  X  X urpose X  X , and  X  X  X laim X  X  fields have the more useful information for categorizing patent documents than other fields.
 Table 6 shows categorization results merged by cross comparison with various weight values.

The first row of Table 6 indicates the result merged into by six retrieval results ( R ( Q
R ( Q t , T m ), R ( Q t , T c ), R ( Q t , T exp ), R ( Q various weight values are assigned. The first column means that six retrieval results are merged with the same weight values and the second column means that only three retrieval results retrieved from three index files ( T
T p and T c ), which showed good performance values in Table 5 , are merged. The third column means that six retrieval results are merged with different weight values which are learned from the MAP of the Table 5 . For example, the weight w pp is calculated by the following equation: For all queries, the merged result is better than the one using the whole text as index file. The last two rows of
Table 6 indicate the final results. The best performance is achieved when we merge retrieval results produced by three queries ( Q t , Q p , and Q c ). We achieved a 74% improvement of categorization performance (MAP) over BaseSystem under the same condition.
 of the patents in order to show the effectiveness of our method. In this experiment, applicant-defined tags under the large text fields (or sections) are not considered. Table 7 shows the result of the comparative experiment.
 The first cell located in the first row and the first column indicates the MAP of the categorization result by the performance of the categorization decreases if the  X  Description  X  section is used as the query or indexing file. This is why the  X  Description  X  section has a great deal of various information which can turn out to be noise. The highest value of MAP in this experiment is smaller than that achieved by FSystem. We can conclude that it is helpful for patent categorization to use the meaningful components extracted from the nor-mative sections through the analysis of semantic tags like FSystem.

An additional comparative experiment is conducted using pair-wise comparison in order to show the effec-tiveness of cross comparison. Table 8 shows the result of the comparative experiment.

We can observe that the performance of cross comparison is better than the one of pair-wise comparison for all query X  X arget pairs. Cross comparison brought a little improvement of accuracy.

In the k-NN approach, the size of the training set influences the performance. Fig. 7 shows the changes of the MAP according to the size of training document set.

As the size of documents increases, the performance of the categorization also increases. The k-NN approach may be a good method to categorize the patents because it does not have the limitation of vector size, unlike other machine learning technology. 3.2. Experimental results in the test set The effectiveness of our patent categorization method was evaluated using the  X  X  X ormal run query set of the
Classification Subtask in NTCIR-5 Patent Retrieval Task X  X . Four participating groups submitted 31 runs (sys-tems) to a Theme Categorization Subtask. We submitted the five results produced by the method described in this paper. The five results were differently produced according to the value of k (10, 20, 30, 50, 100) when a is 0.1.

We used two years documents (1993, 1997) among given five years training documents as a training set. The pair-wise comparison by using three semantic fields (technological field, purpose, method) is carried out and three retrieval results are merged with equal weights. Although we used a smaller training set and only three semantic fields without claim field, our system achieved the best performance. Table 9 shows the evaluation for the submitted results. Only the best result (in MAP) from each participating group is on table. k-NN Model showed comparatively good performance in Theme Categorization Subtask. We can observe that there is the contrast between results produced by MEM based (BOLA6) and k-NN based classification (BOLA1) although similar features are used. We believe that k-NN method could filter noisy data more effi-ciently by eliminating patents which were not similar to a query document from the training data. Most of the participating groups used informative components selected from each full text. FXDM3 and
NICT used semantic titles like our method, but they did not perform well. We believe that our field-by-field comparison brought the improvement of accuracy.

Fig. 8 shows the macro averaged recall/precision curves for submitted results based on the Table 9 . 4. Conclusion
This paper showed that the semantics of patent document structure is one of the most important features for categorization purposes. Although there are many semantic factors in one shared intention of patent appli-cations, the author X  X  (i.e., patent applicant X  X ) intention is assumed to be manifested in their freedom of tags in order to not only clarify their claims, but also hide the intention of their patent application for defensive purposes. Such semantic discrepancy according to the applicant X  X  confused intention reveals two points: one is that the semantics of documents can be understood by considering the author X  X  named tags of document components and the other is that the syntactic comparison of semantic tags is violated by the patent appli-cant X  X  underlying intention to hide the patent contents. While observing such phenomena manually and experimentally in this paper, the result of the method leads to two summarized strategies: (1) re-organization by reducing (or clustering) the physical document structure into a semantic structure that is represented by a small set of semantic fields and (2) cross comparison of specific semantic fields rather than a straight pair-wise comparison between semantic fields.

The first strategy of the problem reduction from the physical document structure into semantic fields is real-ized by the properties of the head nouns in the applicant-defined tags. Even the authors wrote them in different ways, the purpose of their patent applications is the same: to gain approval from the patent examiner. There-fore, applicant-defined tags can be normalized and clustered into the reduced fixed set of semantic tags. The faults in this paper may be the highly sophisticated justification on why the set of semantic tags are adequate, although the tags represent the metadata of the general scientific and technical headings like  X  X  X ield X  X ,  X  X  X urpose X  X ,  X  X  X laim X  X ,  X  X  X ethod X  X ,  X  X  X xplanation X  X , and  X  X  X xample. X  X  While experimenting on these semantic tags,  X  X  X ethod X  X  and  X  X  X xplanation X  X  were confusing because their meanings should be carefully divided, and they could not contribute to the performance upgrade. These two confusing fields would have been better if they are combined into one and run into the cross comparison to acquire a similarity score with the other pair of semantic fields. The second strategy of cross comparison between two different semantic fields was helpful to discover the patent writer X  X  hidden or confusing intention. Because some contents are found under the mismatched tags, the cross pairing of different tags could raise the performance beyond our expectation.

In the aspects of classification techniques, there is a list of techniques in the literature. Among them, we experiment the k-NN, MEM (Maximum Entropy Model), SVM (Support Vector Machine), etc. on the semantic re-organized structure. SVM showed good performance in many classification problems. However, this task. We could not avoid the lengthy calculations of this task. However, the k-NN approach always out-performed MEM in our experimentation. The current conclusion is that the transparent semantic handling was possible in k-NN, but not in the other methods. We would like to find the right way to handle the seman-tic structure in the other machine learning methods. All of the fields in the patent document have been observed and computed manually and then experimented from the small scale to the whole. By this procedure, it was not possible to touch the whole set of patent archives, but we believe that our works could be a good foundation of the further improvement of patent document classification tasks.
 References
