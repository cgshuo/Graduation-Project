 The traditional association rule mining framework produces many redundant rules. The extent of redundancy is a lot larger than previously suspected. We present a new framework for associations based on the concept of closed frequent itemset-s. The number of non-redundant rules produced by the new approach is exponentially (in the length of the longest fre-quent itemset) smaller than the rule set from the traditional approach. Experiments using several  X  X ard X  as well as  X  X asy X  real and synthetic databases confirm the utility of our frame-work in terms of reduction in the number of rules presented to the user, and in terms of time.
 H.2.8 [ Database Management ]: Applications X  Data Min-ing ; I.2.6 [ Artificial Intelligence ]: Learning Association rule discovery, a successful and important mining task, aims at uncovering all frequent patterns among transac-tions composed of data attributes or items. Results are present-ed in the form of rules between different sets of items, along with metrics like the joint and conditional probabilities of the antecedent and consequent, to judge a rule X  X  importance. It is widely recognized that the set of association rules can rapidly grow to be unwieldy, especially as we lower the fre-quency requirements. The larger the set of frequent itemsets the more the number of rules presented to the user, many of which are redundant. This is true even for sparse datasets, but for dense datasets it is simply not feasible to mine all possi-ble frequent itemsets, let alone to generate rules, since they typically produce an exponential number of frequent itemsets; finding long itemsets of length 20 or 30 is not uncommon [2]. Prior research has mentioned that the traditional association rule mining framework produces too many rules, but the ex-tent of redundancy is a lot larger than previously suspected. More concretely, the number of redundant rules are exponen-tial in the length of the longest frequent itemset. We present a new framework for association rule mining based on the con-cept of closed frequent itemsets. The set of all closed frequent itemsets can be orders of magnitude smaller than the set of all frequent itemsets, especially for real (dense) datasets. At the same time, we don X  X  loose any information; the closed item-sets uniquely determine the set of all frequent itemsets and their exact frequency. Note that using the maximal frequent itemsets results in loss of information, since subset frequency is not available. We show that the new framework produces exponentially (in the length of the longest frequent itemset) fewer rules than the traditional approach, again without loss of information. Our framework allows us to mine even dense datasets, where it is not feasible to find all frequent itemsets. Finally, the rule set we produce is a generating set, i.e., al-l possible association rules can be inferred from them using operations like transitivity and augmentation.
 Experiments using several  X  X ard X  or dense, as well as sparse databases confirm the utility of our framework in terms of re-duction in the number of rules presented to the user, and in terms of time. We show that closed itemsets can be found in a fraction of the time it takes to mine all frequent itemsets (with improvements of more than 100 times), and the number of rules returned to the user can be smaller by a factor of 3000 or more! (the gap widens for lower frequency values). There has been a lot of research in developing efficient algo-rithms for mining frequent itemsets [1, 2, 4, 9, 15, 21]. Most of these algorithms enumerate all frequent itemsets. Using these for rule generation produces many redundant rules, as we will show later. Some methods only generate maximal frequen-t itemsets [2, 9]. Maximal itemsets cannot be used for rule generation, since support of subsets is required for confidence computation. While it is easy to make one more data scan to gather the supports of all subsets, we still have the problem of many redundant rules. Further, for all these methods it is simply not possible to find rules in dense datasets which may easily have frequent itemsets of length 20 and more [2]. In contrast the set of closed frequent itemsets can be orders of magnitude smaller than the set of all frequent itemsets, and it can be used to generate rules even in dense domains. In general, most of the association mining work has concen-trated on the task of mining frequent itemsets. Rule generation has received very little attention. There has been some work in pruning discovered association rules by forming rule cover-s [17]. Other work addresses the problem of mining interesting association rules [8, 3, 10, 12]. The approach taken is to incor-porate user-specified constraints on the kinds of rules generat-ed or to define objective metrics of interestingness. As such these works are complimentary to our approach here. Further-more, they do not address the issue of rule redundancy or of constructing a generating set.
 A preliminary study of the idea of using closed frequent item-sets to generate rules was presented by us in [20]. This paper substantially improves on those ideas, and also presents exper-imental results to support our claims. Independently, Pasquier et al. have also used closed itemsets for association mining [13, 14]. However, they mainly concentrate on the discovery of frequent closed itemsets, and do not report any experiments on rule mining. We on the other hand are specifically inter-ested in generating a smaller rule set, after mining the fre-quent closed itemsets. Furthermore, we recently proposed the C
H ARM algorithm [19] for mining all closed frequent item-sets. This algorithm outperforms, by orders of magnitude, the AClose method proposed by Pasquier et al [14], as well as the Apriori [1] method for mining all frequent itemsets. In this pa-per we do not present the C H ARM algorithm, since our main goal is rule generation; we simply use the output it produces. The notion of closed frequent sets has its origins in the ele-gant mathematical framework of formal concept analysis (F-CA). A number of algorithms have been proposed within F-CA for generating all the closed sets of a binary relation [5]. However, these methods have only been tested on very small datasets. Further, these algorithms generate all the closed set-s, and thus have to be adapted to enumerate only the frequent concepts. The foundations of rule generation (in FCA) were studied in [11], but no experimentation on large sets was done. Our characterization of the generating set of association rules is different, and we also present an experimental verification. Other work has extended the FCA approach to incorporate in-cremental rule mining [6], and recent work has addressed the issue of extracting association rule bases [16].
 The rest of the paper is organized as follows. Section 2 de-scribes the association mining task. Section 3 presents the notion of closed itemsets. Section 4 looks at the problem of eliminating redundant rules. We experimentally validate our approach in Section 5. The proofs for all theorems have been omitted due to lack of space; these are available in [18]. The association mining task can be stated as follows: Let I = f 1 ; 2 ; ;m g be a set of items, and let T = f 1 ; 2 ; ;n g be a set of transaction identifiers or tids . The input database is a binary relation  X  IT . If an item i occurs in a transaction t , we write it as ( i; t ) 2  X  , or alternately as i X  t . Typically the database is arranged as a set of transactions, where each transaction contains a set of items. For exam-ple, consider the database shown in Figure 1, used as a run-ning example in this paper. Here I = f A; C ; D ; T ; W g , and T = f 1 ; 2 ; 3 ; 4 ; 5 ; 6 g . The second transaction can be repre-sented as f C X  2 ;D X  2 ;W X  2 g ; all such pairs from all transac-tions, taken together form the binary relation  X  . A set X I is called an itemset , and a set Y T is called a tidset .For convenience we write an itemset f A; C ; W g as AC W , and a tidset f 2 ; 4 ; 5 g as 245 . The support of an itemset X , denoted ( X ) , is the number of transactions in which it occurs as a subset. An itemset is frequent if its support ( X ) minsup , where minsup is a user-specified minimum support threshold. An association rule is an expression A p ! B , where A and B are itemsets. The support of the rule is ( A [ B ) (i.e., the joint probability of a transaction containing both A and B ), and the confidence p = ( A [ B ) = ( A ) (i.e., the conditional probability that a transaction contains B , given that it contains A ). A rule is frequent if the itemset A [ B is frequent. A rule is confident if p minconf , where where minconf is a user-specified minimum threshold.
 Association rule mining consists of two steps [1]: 1) Find all frequent itemsets, and 2) Generate high confidence rules. Finding frequent itemsets This step is computationally and I/O intensive. Consider Figure 1, which shows a bookstore database with six customers who buy books by different au-thors. It shows all the frequent itemsets with minsup = 50% (i.e., 3 transactions). AC T W and CDW are the maximal fre-quent itemsets (i.e., not a subset of any other frequent itemset). Let jI j = m be the number of items. The search space for enu-meration of all frequent itemsets is 2 m , which is exponential in m . One can prove that the problem of finding a frequen-t set of a certain size is NP-Complete, by reducing it to the balanced bipartite clique problem, which is known to be NP-Complete [20]. However, if we assume that there is a bound on the transaction length, the task of finding all frequent itemsets is essentially linear in the database size, since the overall com-plexity in this case is given as O ( r n 2 l ) , where jT j = n is the number of transactions, l is the length of the longest frequent itemset, and r is the number of maximal frequent itemsets. Generating confident rules This step is relatively straightfor-ward; rules of the form Y p ! X Y , are generated for all frequent itemsets X , for all Y X , Y 6 = ; , and provid-ed p minconf . For example, from the frequent itemset AC W we can generate 6 possible rules (all of them have sup-pictorially in Figure 2. Notice that we need access to the sup-port of all subsets of AC W to generate rules from it. To obtain all possible rules we need to examine each frequent itemset and repeat the rule generation process shown above for AC W . Figure 2 shows the set of all other association rules with con-fidence above or equal to minconf = 80%.
 For an itemset of size k there are 2 k 2 potentially confident rules that can be generated. This follows from the fact that we must consider each subset of the itemset as an antecedent, except for the empty and the full itemset. The complexity of the rule generation step is thus O ( f 2 l ) , where f is the number of frequent itemsets, and l is the longest frequent itemset. In this section we describe the concept of closed frequent item-sets, and show that this set is necessary and sufficient to cap-ture all the information about frequent itemsets, and has small-er cardinality than the set of all frequent itemsets. Let ( P; ) be an ordered set with the binary relation , and let S be a subset of P . An element u 2 P ( l 2 P )isan upper bound ( lower bound )of S if s u ( s l ) for all s 2 S . The least upper bound is called the join of S , and is denoted as and is denoted as the join, and x ^ y for the meet. An ordered set ( L; ) is a lattice , if for any two elements x and y in L , the join x _ y and meet x ^ y always exist. L is a complete lattice if S exist for all S L . Any finite lattice is complete. Let P denote the power set of S (i.e., the set of all subsets of S ). The ordered set ( P ( S ) ; ) is a complete lattice, where the meet is given by set intersection, and the join is given by set union. For example the partial orders ( P ( I ) ; ) , the set of all possible itemsets, and ( P ( T ) ; ) , the set of all possible of all frequent itemsets we found in our example database. Let the binary relation  X  IT be the input database for association mining. Let X I , and Y T . The mappings t : I 7 !T ; t ( X )= f y 2T j 8 x 2 X; x X  y g i : T 7 !I ; i ( Y )= f x 2I j 8 y 2 Y; x X  y g define a Galois connection between P ( I ) and P ( T ) .We denote a X; t ( X ) pair as X t ( X ) , and a i ( Y ) ;Y pair as i ( Y ) Y . Figure 4 illustrates the two mappings. The map-ping t ( X ) is the set of all transactions (tidset) which con-tain the itemset X , similarly i ( Y ) is the itemset that is con-tained in all the transactions in Y . For example, t ( AC W )= 1345 , and i (245) = CDW . In terms of individual elements t ( X ) = t ( AC W )= t ( A ) \ t ( C ) \ t ( W ) = 1345 \ 123456 \ 12345 = 1345 . Also i (245) = i (2) \ i (4) \ i (5) = CDW \ AC D W \ AC D T W = CDW . The Galois connection satisfies the fol-lowing properties (where X; X 1 ;X 2 2 P ( I ) and Y; Y 1 2P ( T ) ): 1) X 1 X 2 ) t ( X 1 ) t ( X 2 ) ,2) Y 1 Y 2 i ( Y 1 ) i ( Y 2 ) ,3) X i ( t ( X )) and Y t ( i ( Y )) . For ex-ample, for AC W AC T W , t ( AC W ) = 1345 135 = t ( AC T W ) .For 245 2456 , i (245) = CDW CD = i (2456) . Also, AC i ( t ( AC )) = i (1345) = AC W . Let S be a set. A function c : P ( S ) 7 ! P ( S ) is a closure operator on S if, for all X; Y S , c satisfies the following properties: 1) Extension: X c ( X ) . 2) Monotonicity: if X Y , then c ( X ) c ( Y ) . 3) Idempotency: c ( c ( X )) = c ( X ) . A subset X of S is called closed if c ( X ) = X . Let X I and Y T . Let c it ( X ) denote the composition of the two mappings i  X  t ( X )= i ( t ( X )) . Dually, let c t  X  i ( Y ) = t ( i ( Y )) . Then c it : P ( I ) 7 ! P ( I ) and c P ( T ) 7 !P ( T ) are both closure operators.
 We define a closed itemset as an itemset X that is that same as its closure, i.e., X = c it ( X ) . For example the itemset AC W is closed. A closed tidset is a tidset Y = c ti ( Y ) . For example, the tidset 1345 is closed.
 The mappings c it and c ti , being closure operators, satisfy the three properties of extension, monotonicity, and idempotency. We also call the application of i  X  t or t  X  i a round-trip . Fig-ure 4 illustrates this round-trip starting with an itemset X .For example, let X = AC , then the extension property say that X is a subset of its closure, since c it ( AC ) = i ( t ( AC )) = i (1345) = AC W . Since AC 6 = c it ( AC ) = AC W ,we conclude that AC is not closed. On the other hand, the idem-potency property say that once we map an itemset to the tidset that contains it, and then map that tidset back to the set of items common to all tids in the tidset, we obtain a closed item-set. After this no matter how many such round-trips we make we cannot extend a closed itemset. For example, after one round-trip for AC we obtain the closed itemset AC W .Ifwe perform another round-trip on AC W , we get c it ( AC W )= i ( t ( AC W )) = i (1345) = AC W .
 For any closed itemset X , there exists a closed tidset given by Y , with the property that Y = t ( X ) and X = i ( Y ) (con-versely, for any closed tidset there exists a closed itemset). We can see that X is closed by the fact that X = i ( Y ) , then plug-ging Y = t ( X ) , we get X = i ( Y ) = i ( t ( X )) = c it thus X is closed. Dually, Y is closed. For example, we have seen above that for the closed itemset AC W the associated closed tidset is 1345 . Such a closed itemset and closed tidset pair X Y is called a concept .
 A concept X 1 Y 1 is a subconcept of X 2 Y 2 , denoted as X denote the set of all possible concepts in the database. Then the ordered set ( B (  X  ) ; ) is a complete lattice, called the Ga-lois lattice. For example, Figure 5 shows the Galois lattice for our example database, which has a total of 10 concepts. The least element is C 123456 and the greatest elemen-tis AC D T W 5 . The mappings between the closed pairs of itemsets and tidsets are anti-isomorphic, i.e., concepts with large cardinality itemsets have small tidsets, and vice versa. The concept generated by a single item x 2 I is called an item concept , and is given as C i ( x )= c it ( x ) t ( x ) . Simi-larly, the concept generated by a single transaction y 2T is called a tid concept , and is given as C t ( y )= i ( y ) c For example, the item concept C i ( A ) = i ( t ( A )) t ( A ) = i (1345) 1345 = AC W 1345 . Further, the tid concept C (2) = i (2) t ( i (2)) = CDW t ( CDW )= CDW 245 .
 In Figure 5 if we relabel each node with the item concept or tid concept that it is equivalent to, then we obtain a lattice with minimal labelling , with item or tid labels, as shown in the fig-ure in bold letters. Such a relabelling reduces clutter in the lattice diagram, which provides an excellent way of visualiz-ing the structure of the patterns and relationships that exist be-tween items. We shall see its benefit in the next section when we talk about high confidence rules extraction.
 It is easy to reconstruct the concepts from the minimal label-ing. Consider the tid concept C t (2) = X Y . To obtain the closed itemset X , we append all item labels reachable be-low it. Conversely, to obtain the closed tidset Y we append all labels reachable above C t (2) . Since W , D and C are all the labels reachable by a path below it, X = CDW forms the closed itemset. Since 4 and 5 are the only labels reachable above C t (2) , Y = 245 ; this gives us the concept CDW 245 , which matches the concept shown in the figure. We begin this section by defining the join and meet operation on the concept lattice (see [5] for the formal proof): The set of all concepts in the database relation  X  , given by ( B (  X  ) ; ) is a (complete) lattice with join and meet given by meet: ( X 1 Y 1 ) ^ ( X 2 Y 2 )=( X 1 \ X 2 ) c ti ( Y 1 [ Y For the join and meet of multiple concepts, we simply take the unions and joins over all of them. For example, consider the join of two concepts, ( AC D W 45) _ ( CDT 56) = c ( AC D W [ CDT ) (45 \ 56) = AC D T W 5 . On the other hand their meet is given as, ( AC D W 45) ^ ( CDT 56) = ( AC D W \ CDT ) c ti (45 [ 56) = CD c ti (456) = CD 2456 . Similarly, we can perform multiple concept join-s or meets; for example, ( CT 1356) _ ( CD 2456) _ ( CDW 245) = c it ( CT [ CD [ CDW ) (1356 \ 2456 \ 245) = c it ( CDT W ) 5= AC D T W 5 .
 We define the support of a closed itemset X or a concep-t X Y as the cardinality of the closed tidset Y = t ( X ) , i.e, ( X )= j Y j = j t ( X ) j . A closed itemset or a concept is frequent if its support is at least minsup . Figure 6 shows all the frequent concepts with minsup = 50% (i.e., with tidset cardi-nality at least 3). All frequent itemsets can be determined by the join operation on the frequent item concepts. For example, since join of item concepts D and T , C i ( D ) _C i ( T ) , doesn X  X  exist, DT is not frequent. On the other hand, C i ( A ) _C AC T W 135 , thus AT is frequent. Furthermore, the support of AT is given by the cardinality of the resulting concept X  X  tid-set, i.e., ( AT )= j t ( AT ) j = j 135 j =3 .

L EMMA 1. An itemset X  X  ( X ) support is equal to the sup-port of its closure, i.e., ( X )= ( c it ( X )) .
 This theorem (independently reported in [13]) states that al-l frequent itemsets are uniquely determined by the frequent closed itemsets (or frequent concepts). Furthermore, the set of frequent closed itemsets is bounded above by the set of fre-quent itemsets, and is typically much smaller, especially for dense datasets. For very sparse datasets, in the worst case, the two sets may be equal. To illustrate the benefits of closed itemset mining, contrast Figure 3, showing the set of all fre-quent itemsets, with Figure 6, showing the set of all closed frequent itemsets (or concepts). We see that while there are only 7 closed frequent itemsets, in contrast there are 19 fre-quent itemsets. This example clearly illustrates the benefits of Recall that an association rule is of the form X 1 where X 1 ;X 2 I . Its support equals j t ( X 1 [ X 2 ) j , and its confidence is given as p = P ( X 2 j X 1 )= j t ( X 1 [ X We are interested in finding all high support and high confi-dence rules. It is widely recognized that the set of such associ-ation rules can rapidly grow to be unwieldy. In this section we will show how the closed frequent itemsets help us form a gen-erating set of rules, from which all other association rules can be inferred. Thus, only a small and easily understandable set of rules can be presented to the user, who can later selectively derive other rules of interest.
 Before we proceed, we need to formally define what we mean by a redundant rule. Let R i denote the rule X i say that a rule R 1 is more general than a rule R R 1 R 2 provided that R 2 can be generated by adding addi-tional items to either the antecedent or consequent of R 1 be a set of rules, such that all their confidences are equal, i.e., p = p; 8 i . Then we say that a rule R j is redundant if there exists some rule R i , such that R i R j . The non-redundant rules in the collection R are those that are most general. We now show how to eliminate the redundant association rules, i.e., rules having the same support and confidence as some more general rule. In the last section, we showed that the sup-port of an itemset X equals the support of its closure c it Thus it suffices to consider rules only among the frequent con-cepts. In other words the rule X 1 p ! X 2 is exactly the same as the rule c it ( X 1 ) p ! c it ( X 2 ) .
 Another observation that follows from the concept lattice is that it is sufficient to consider rules among adjacent concepts, since other rules can be inferred by transitivity, that is:
L EMMA 2. Transitivity: Let X 1 ;X 2 ;X 3 be frequent closed itemsets, with X 1 X 2 X 3 .If X 1 p ! X 2 and X 2 q ! X In the discussion below, we consider two cases of association rules, those with 100% confidence, i.e., with p = 1 : 0 , and those with p&lt; 1 : 0 .
L EMMA 3. An association rule X 1 1 : 0 ! X 2 has confi-dence p =1 : 0 if and only if t ( X 1 ) t ( X 2 ) .
 This theorem says that all 100% confidence rules are those that are directed from a super-concept ( X 1 t ( X 1 )) to a sub-concept ( X 2 t ( X 2 ) ),i.e., down-arcs, since it is in precisely these cases that t ( X 1 ) t ( X 2 ) (or X 1 X 2 ). Consider the item concepts C i ( W ) = CW 12345 and C i ( C ) = C that if we take the itemset closure on both sides of the rule, but since the antecedent and consequent are not disjoint in this rules are exactly the same. Figure 7 shows some of the other rules among adjacent concepts with 100% confidence. (CT x 1356) We notice that some down-arcs are labeled with more than one rule. In such cases, all rules within a box are equivalent, and we prefer the rule that is most general. For example, consider the latter two are obtained by adding one (or more) items to we can say that the addition of C to either the antecedent or the consequent has no effect on the support or confidence of the rule. Thus, according to our definition, we say that the other two rules redundant.

T HEOREM 1. Let R = f R 1 ; ;R n g be a set of rules with 100% confidence ( p i =1 : 0 ; 8 i ), such that I 1 = c ) , and I 2 = c it ( X i 100% confidence rule I 1 1 : 0 ! I 2 . Then all the rules R are more specific than R I , and thus are redundant . Let X  X  apply this theorem to the three rules we considered above. For the first rule c it ( TW [ A ) = c it ( AT W ) = AC T W . Similarly for the other two rules we see that c it ( TW [ AC )= c ( AC T W )= AC T W , and c it ( CT W [ A )= c it ( AC T W )= AC T W . Thus for these three rules we get the closed itemset I = AC T W . By the same process we obtain I 2 = AC W . All three rules correspond to the arc between the tid concept C (1 ; 3) and the item concept C i ( A ) . Finally TW 1 : 0 ! A is the most general rule, and so the other two are redundant. A set of such general rules constitutes a generating set , i.e., a rule set, from which all other 100% confidence rules can in-ferred. Note that in this paper we do not address the question of eliminating self-redundancy within this generating set, i.e., there may still exist rules in the generating set that can be de-rived from other rules in the set. In other words we do not claim anything about the minimality of the generating set; that is the topic of a forthcoming paper. See [7, 11, 16] for more information on generating a base set (or minimal generating set) of rules.
 Figure 7 shows the generating set in bold arcs, which includes produce rules that cannot be written with disjoint anteceden-t and consequent. For example, between C t (2) and C and consequent are not disjoint, as required by definition, we discard such rules). All other 100% confidence rules can be derived from this generating set by application of simple infer-all the 18 100% confidence rules produced by using frequent itemsets, as shown in Figure 2, can be generated from this set of 5 rules, produced using the closed frequent itemsets! We now turn to the problem of finding a generating set for association rules with confidence less than 100%. As before, we need to consider only the rules between adjacent concepts. But this time the rules correspond to the up-arcs, instead of the down-arcs for the 100% confidence rules, i.e., the rules go from sub-concepts to super-concepts.
 Consider Figure 8. The edge between item concepts C i ( C ) adjacent concepts can be derived by transitivity. For example, for C p ! A we can obtain the value of p using the rules C 2 = 3=0 : 67 .

T HEOREM 2. Let R = f R 1 ; ;R n g be a set of rules with confidence p&lt; 1 : 0 , such that I 1 = c it ( X i c I . Then all the rules R i 6 = R I are more specific than R thus are redundant .
 This theorem differs from that of the 100% confidence rules to account for the up-arcs. Consider the rules produced by the up-arc between item concepts C i ( W ) and C i ( A ) . We find that for all three rules, I 1 = c it ( W )= c it ( CW )= CW , and I c ( W [ A )= c it ( W [ AC )= c it ( CW [ A )= AC W . The support of the rule is given by j t ( I 1 [ I 2 ) j = j t ( AC W ) j =4 , and the confidence given as j t ( I 1 [ I 2 ) j = j t ( I are redundant. Similarly for the up-arc between C i C in the box are redundant! The set of all such general rules forms a generating set of rules from which other rules can be inferred. The two bold arrows in Figure 8 constitute a generating set for all rules with 0 : 8 p &lt; 1 : 0 . Due to the transitivity property, we only have to consider arcs with confidence at least minconf = 0 : 8 . No other rules can be confident at this level.
 By combining the generating set for rules with p =1 : 0 , shown in Figure 7 and the generating set for rules with 1 : 0 &gt;p 0 : 8 , shown in Figure 8, we obtain a generating set for al-l association rules with minsup = 50%, and minconf = 80%: It can be easily verified that all the association rules shown in Figure 2, for our example database from Figure 1, can be derived from this set. Using the closed itemset approach we produce 7 rules versus the 22 rules produced in traditional as-sociation mining. To see the contrast further, consider the set of all possible association rules we can mine. With minsup = 50%, the least value of confidence can be 50% (since the max-imum support of an itemset can be 100%, but any frequent subset must have at least 50% support; the least confidence value is thus 50/100 = 0.5). There are 60 possible association rules versus only 13 in the generating set (5 rules with p =1 : 0 in Figure 7, and 8 rules with p&lt; 1 : 0 in Figure 8) The complexity of rule generation in the traditional framework is O ( f 2 l ) , exponential in the length l of the longest frequent itemset ( f is the total number of frequent itemsets). On the other hand using the closed itemset framework, the number of non-redundant rules is linear in the number of closed itemsets. To see how much savings are possible using closed frequen-t itemsets, lets consider the case where the longest frequent itemset has length l ; with all 2 l subsets also being frequent. In the traditional association rule framework, we would have to consider for each frequent itemset all its subsets as rule an-tecedents. The total number of rules generated in this approach is given as On the other hand the number of non-redundant rules produced using closed itemsets is given as follows. Let X  X  consider two extreme cases: In the best case, there is only one closed item-set, i.e., all 2 l subsets have the same support as the longest frequent itemset. Thus all rules between itemsets must have 100% confidence. The closed itemset approach doesn X  X  pro-duce any rule; it just lists the closed itemset with its frequency, with the implicit assumption that all possible rules from this itemset have 100% confidence. This corresponds to a reduc-In the worst case, all 2 l frequent itemsets are also closed. In this case there can be no 100% confidence rules and all ( &lt; 100% confidence) rules point upwards, i.e., from subsets to their immediate supersets. For each subset of length k we have k rules from each of its k 1 length subsets to that set. The total number of rules generated is thus number of rules by of a factor of O (2 l =l ) , i.e., asymptotically exponential in the length of the longest frequent itemset. All experiments described below were performed on a 400MHz Pentium PC with 256MB of memory, running RedHat Linux 6.0. Algorithms were coded in C++. Table 1 shows the char-acteristics of the real and synthetic datasets used in our eval-uation. The real datasets were obtained from IBM Almaden (www.almaden.ibm.com/cs/quest/demos.html). All datasets ex-cept the PUMS (pumsb and pumsb*) sets, are taken from the UC Irvine Machine Learning Database Repository. The PUM-S datasets contain census data. pumsb* is the same as pums-b without items with 80% or more support. The mushroom database contains characteristics of various species of mush-rooms. Finally the connect and chess datasets are derived from their respective game steps. Typically, these real datasets are very dense, i.e., they produce many long frequent itemsets even for very high values of support.

Database # Items Record Length # Records chess 76 37 3,196 connect 130 43 67,557 mushroom 120 23 8,124 pumsb* 7117 50 49,046 pumsb 7117 74 49,046 T20I12D100K 1000 20 100,000 T40I8D100K 1000 40 100,000 T10I4D100K 1000 10 100,000 T20I4D100K 1000 20 100,000 We also chose a few synthetic datasets (also available from IB-M Almaden), which have been used as benchmarks for testing previous association mining algorithms. These datasets mimic the transactions in a retailing environment. Usually the syn-thetic datasets are sparse when compared to the real sets. We used two dense and two sparse (the last two rows in Table 1) synthetic datasets for our study. Consider Table 2 and 3, which compare the traditional rule generation framework with the closed itemset approach pro-posed in this paper. The tables shows the experimental results along a number of dimensions: 1) total number of frequent itemsets vs. closed frequent itemsets, 2) total number of rules in the traditional vs. new approach, and 3) total time taken for mining all frequent itemsets (using Apriori) and the closed frequent itemsets (using C H ARM).
 Table 2 shows that the number of closed frequent itemsets can be much smaller than the set of all frequent itemsets. For the support values we look at here, we got reductions (shown in the Ratio column) in the cardinality upto a factor of 45. For lower support values the gap widens rapidly [19]. It is note-worthy, that C H ARM finds these closed sets in a fraction of the time it takes Apriori to mine all frequent itemsets as shown in Table 2. The reduction in running time ranges upto a fac-tor of 145 (again the gap widens with lower support). For the sparse sets, and for high support values, the closed and all fre-quent set coincide, but C H ARM still runs faster than Apriori. Table 3 shows that the reduction in the number of rules (with all possible consequent lengths) generated is drastic, ranging from a factor of 2 to more than 3000 times! Incidentally, these ratios are roughly in agreement with the complexity formula we presented in Section 4.3. For example, consider the mush-room dataset. At 40% support, the longest frequent itemset has length 7. The complexity figure predicts a reduction in the number of rules by a factor of 2 7 = 7 = 128 = 7 = 18 , which is close to the ratio of 15 we got empirically. Similarly for 20% support, we expect a reduction of 2 15 = 15 = 2185 , and empirically it is 3343.
 We also computed how many single consequent rules are gen-erated by the traditional approach. We then compared these with the non-redundant rule set from our approach (with pos-sibly multiple consequents). The table also shows that even if we restrict the traditional rule generation to a single item con-sequent, the reduction with the closed itemset approach is still substantial, with upto a factor of 66 reduction (once again, the reduction is more for lower supports). It is worth noting that, even though for sparse synthetic sets the closed frequent item-sets is not much smaller than the set of all frequent itemsets, we still get upto a factor of 5 reduction in the number of rules generated.
 The results above present all possible rules that are obtained by setting minconf equal to the minsup . Figure 9 shows the effect of minconf on the number of rules generated. It shows that most of the rules have very high confidence; as the knee of the curves show, the vast majority of the rules have confidences between 95 and 100 percent! This is a particularly distressing result for the traditional rule generation framework. The new approach produces a rule set that can be orders of magnitude smaller. In general it is possible to mine closed sets using C
H ARM for low values of support, where it is infeasible to find all frequent itemsets. Thus, even for dense datasets we can generate rules, which may not be possible in the traditional approach. This paper has demonstrated in a formal way, supported with experiments on several datasets, the well known fact that the traditional association rule framework produces too many rules, most of which are redundant. We proposed a new framework based on closed itemsets that can drastically reduce the rule set, and that can be presented to the user in a succinct manner. This paper opens a lot of interesting directions for future work. For example we plan to use the concept lattice for interactive visualization and exploration of a large set of mined associa-tions. Keep in mind that the frequent concept lattice is a very concise representation of all the frequent itemsets and the rules that can be generated from them. Instead of generating all pos-sible rules, we plan to generate the rules on-demand, based on the user X  X  interests. Finally, there is the issue of developing a theory for extracting a base, or a minimal generating set, for all the rules.
