 The Cranfield evaluation method has some disadvantages, including its high cost in labor and inadequacy for evaluating interactive retrieval techniques. As a very promising alter-native, automatic comparison of retrieval systems based on observed clicking behavior of users has recently been stud-ied. Several methods have been proposed , but there has so far been no systematic way to assess which strategy is better, making it difficult to choose a good method for real applications. In this paper, we propose a general way to evaluate these relative comparison methods with two mea-sures: utility to users(UtU) and effectiveness of differenti-ation(EoD). We evaluate two state of the art methods by systematically simulating different retrieval scenarios. In-spired by the weakness of these methods revealed through our evaluation, we further propose a novel method by con-sidering the positions of clicked documents. Experiment re-sults show that our new method performs better than the existing methods.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Search and Retrieval]: Information Search and Retrieval; H.3.4 [System and Software]:Performance Evaluation General Terms: Experimentation Measurement Keywords: information retrieval, implicit feedback, evalu-ation
Evaluation of an information retrieval (IR) system is crit-ical for improving search techniques. So far, the dominant method for IR evaluation has been the Cranfield evaluation method. However, it has some disadvantages such as high cost in labor and inadequacy for iteractive retrieval evalu-ation. As a promising alternative, automatic evaluation of retrieval systems based on the implicit feedback of users has recently been studied [2, 4]. There are two main categories of methods based on  X  X bsolute metric X  and  X  X elative compar-ison test X , respectively. In the methods of the first category, it infers the absolute relevance of the retrieved documents of clickthroughs. Because such a strategy inevitably makes mistakes in predicting which system is better, we would need to aggregate the predictions over many queries in order to reach a reliable conclusion.

Any method for comparing systems can be regarded as logically consisting of two functions: (1) an interleaving function which determines how to combine the ranked lists of results returned from two systems to generate a single ranked list of results for the user, and (2) a comparison func-tion which decides which ranked list is preferred by the user based on the collected user clickthroughs and how the inter-leaved list was formed. In general, the interleaving function and the comparison function are  X  X ynchronized X  to work to-gether to support relative comparison of retrieval systems.
So far, there have been two major methods proposed to in-stantiate these two functions: the balanced method [2] and the team-draft method [4]. The balanced method merges two lists by taking a document from each list alternatively so as to ensure that the numbers of documents taken from both lists are as close as possible; indeed, it can be shown that they can be ensured to differ by at most one. To pre-dict which system(list) performs better, it takes the mini-mal rank of the last clicked document in these two lists as threshod position, and then measures the quality of each list by the number of clicked document above this thresh-old position. On the other hand, team-draft method fetchs one document from each list in one iteration to compose the merged list. From each list, it fetchs the most perferred document that is not in merged list and appends it to the merged list. To compare the performance of the lists, it counts number of clicked documents fetched from each of the lists. Though previous work showed they can be used to compare the retrieval systems, both make mistakes in some scenarios.
In this section, we propose a simulation-based approach for evaluating and comparing different interleaving methods more systematically. Our basic idea is to first systemati-cally generate many test cases, each being a pair of ranked lists of documents, and then apply each interleaving method to these test cases and evaluate its performance. Thus our overall approach consists of two components: (1) metrics to be used for measuring the performance of an interleav-ing method or comparing two methods, and (2) simulation strategies to be used to generate the test cases. We now present these two components in more detail.
Intuitively, an method should merge the two lists in such a way that we can differentiate the ranking accuracy of the two systems accurately based on the collected clickthroughs. Thus our first criterion is the effectiveness of differentia-tion (EoD) of a method. Meanwhile, it is also important that the utility of the merged list of results from a user X  X  perspective is high. Thus a s econd criterion that we con-sider is the utility to users (UtU) of a method. Note that in order to achieve high effectiveness of differentiation, an in-terleaving method may compromise its utility to users, and vice versa.

To quantify the utility to users of an interleaving method, we may apply any existing IR relevance evaluation mea-sures(MAP in this paper) to the merged result list generated by the interleaving method. Since we adopt a simulation strategy, information about which document is relevant is available to us, so computing MAP is straightforward once we generated a merged list.
 how an interleaving method performs in different scenar-ios. Thus each possible scenario should be simulated sep-arately to understand relative strengths and weaknesses of two methods in these different scenarios.

The following are the different scenarios that we would simulate in our experiments: Known-item search vs. high-recall search: This can be simulated by setting n r to 1 or a large number. Easy vs. difficult topics: This can be simulated by con-trolling the precision in the top ranked documents (e.g., pre-cision at top 10 documents) for both ranked lists. Incremental updating of the same system vs. two different systems: Thi can be simulated by controlling the correlation of two document ranking results( Kendall s  X  in this paper).
 Comparison of systems with similar and different retrieval accuracy: This can be simulated by regulating the difference in retrieval performance of the two ranked lists. In this paper, the similarity of accuracy is defined by where AP A and AP B are the average precision of the two ranked lists respectively.
 Patient users vs. impatient users: This can be simu-lated by varying the parameter K which indicates how many documents a user is expected to view.

We can stop the sampling process when we have sufficient test cases to obtain a relatively reliable estimate of the UtU and EoD of the interleaving methods being evaluated. For example, we may stop when all the average measure score values achieve high confidence in a small interval.
In this section, we use the proposed evaluation method to evaluate the balanced and team-draft methods.
Given a set of simulation test cases, we evaluate an inter-leaving method with two measures: (1) Average precision of the merged ranked list of results generated by the inter-leaving method(for UtU). (2) Cost of prediction made by the method regarding which system is better(for EoD). To summarize the performance of a method in these two dimen-sions, we compute the mean(MAP and MCP, respectively) and variance of the values of each measure over an entire set of test cases. We repeat sampling until both the MAP and MCP values are relatively stable (i.e., reaching a 95% confidence interval of 0 . 01).

The generation of a test case is controlled by five param-eters: (1) length of result lists ( n ); (2) number of relevant documents ( n r ); (3) range of correlation of the two ranked lists (  X  ); (4) range of average precisions of the two ranked lists ( AP A , AP B ,and RAP ); and (5) number of result doc-umentsassumedtobeviewedbyauser( K ). In our exper-iments, we control the first parameter n by setting it to 10 and vary all the other parameters to simulate different eval-uation scenarios. The ground truth about which system is better is decided based on the average precision of the top 10 documents for each system. Variations of other parameters and the scenarios simulated are summarized in Table 2.
We show the results of the two methods in all the different special scenarios in Table 3 and Table 4, for MAP and MCP respectively. We first see that the balanced method and team-draft method have identical MAP in all the special Search result difference: There seem to be no consistent patterns associated with the difference of the search results of the two systems.
 User patience: Intuitively, the cost should become smaller when the number of viewed documents is larger. The results show that the performance pattern of the team-draft method indeed conforms to this intuition well. However, the bal-anced method did not show such a consistent pattern. This reveals a major deficiency of the balanced method in not being able to exploit clickthrough information efficiently.
In general, the balanced method is preferred to the team-draft method. For known-item search, it X  X  always better to use the balanced method. For searches with more relevant documents, if users are expected to view very few top docu-ments, the balanced method is also preferred, but when the user is patient and willing to view more documents, team-draft may be more appropriate.

It has revealed some interesting weaknesses of each of them. Specifically, a main problem of the team-draft strat-egy is that it assigns one clicked document to only one ranked list, which can easily cause wrong preditions. The main problem of the balanced method is that it cannot ben-efit from more observed documents. The reason is that the balanced method does not use the clickthroughs efficiently because it only counts how many clickthroughs have been made to each list, but does not consider the positions of those clickthroughs. In some sense, it is only sensitive to position of the last clicked document, but not to the posi-tions of the documents clicked earlier.

Thus the common drawback of both existing methods is that they are not sensitive to the positions of the clicked documents in the ranked lists. In the next section, we pro-pose an improvement to the balanced method to overcome this limitation.
In this section, we propose an improvement over the bal-anced method that would better exploit the position in-formation of clicked documents. Specifically, in this new method (called preference-based balanced method), we would interleave the ranked lists in the same way as the balanced method but make prediction about which system is bet-ter based on a new preference-based comparison function ( ppref ).

A preference relation between two documents indicates that one document is more relevant than the other with respect to a query (denoted by d i &gt; p d j ). Although click-throughs are often biased, it has been shown that some pref-erence relations extracted from clickthrough data are reliable [3].

The original balanced method predicts which system is better based on only the number of clicked documents con-tributed by each system. Our goal is to consider the rank-ing positions of all these clicked documents in the original ranked lists. To achieve this goal, we would first try to convert the collected clickthroughs from the user into a set of preference relations. Specifically, we infer the preference relations based on two rules: (1) a clicked document are preferred to the skipped documents above it; (2) a clicked document is more relevant than the next unclicked one[3]. Now, our key idea is to design a preference-based measure to score each ranked list by treating these inferred incom-plete preference relations between documents as our gold standard. In particular, we will use the precision of prefer-ence ( ppref ) measure proposed in [1] for evaluation based on manual preference judgments.

