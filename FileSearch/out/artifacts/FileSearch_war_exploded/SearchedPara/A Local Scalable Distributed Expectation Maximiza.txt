 This paper offers a local distributed algorithm for expecta-tion maximization in large peer-to-peer environments. The algorithm can be used for a variety of well-known data min-ing tasks in a distributed environment such as clustering, anomaly detection, target tracking to name a few. This technology is crucial for many emerging peer-to-peer ap-plications for bioinformatics, astronomy, social network ing, sensor networks and web mining. Centralizing all or some of the data for building global models is impractical in such peer-to-peer environments because of the large number of data sources, the asynchronous nature of the peer-to-peer networks, and dynamic nature of the data/network. The distributed algorithm we have developed in this paper is provably-correct i.e. it converges to the same result com-pared to a similar centralized algorithm and can automat-ically adapt to changes to the data and the network. We show that the communication overhead of the algorithm is very low due to its local nature. This monitoring algorithm is then used as a feedback loop to sample data from the network and rebuild the model when it is outdated. We present thorough experimental results to verify our theore t-ical claims.
 H.2.4 [ Database Management Systems ]: Distributed databases; H.2.8 [ Database Applications ]: Data mining; G.3 [ Probability And Statistics ]: Multivariate statistics; C.2.4 [ Distributed Systems ] Algorithms, Performance, Experiments peer-to-peer, distributed data mining, local algorithms, EM, GMM
Expectation Maximization (EM) is a powerful statistical and data mining tool which can be widely used for a vari-ety of tasks such as clustering, estimating parameters from the data even in the presence of hidden variables, anomaly detection, target tracking and more. In 1977, Dempster et al. [10] presented the seminal work on EM and its appli-cation for estimating the parameters of a Gaussian Mixture Model (GMM). The authors showed that given a sample of data, there is a two step process which can estimate certain unknown parameters of the data in the presence of hidden variables. This is done by maximizing the log likelihood score of the data and assuming a generative model. Thus the classical EM algorithm is well-understood and produces satisfactory estimates of the parameters when the data is centralized.

However, there exist emerging technologies in which the data is not located at a central location but rather dis-tributed across a large network of nodes or machines con-nected by an underlying communication infrastructure. The next generation Peer-to-Peer (P2P) networks such as Gnutel la, BitTorrents, e-Mule, Kazaa, and Freenet offer some exam-ples. As argued on several occasions, P2P networks can no longer be viewed as an isolated medium of data storage and dissemination; recent research on P2P web community for-mation [24] [9], bioinformatics 1 and diagnostics [13][32] has shown that interesting information can be extracted from the data in such networks. However data analysis in such environments calls for a new breed of algorithms which are asynchronous, highly communication efficient and scalable. To solve this problem, in this paper we develop a local, P2P distributed ( PeDEM ) and asynchronous algorithm for monitoring and subsequent reactive updating of a GMM model using an EM style update technique. Our algorithm is provably correct i.e. given all the data, our algorithm will converge to the same result produced by a similar centralize d algorithm. The algorithmic framework is local , in the sense that the computation and communication load at each node is independent of the size or the number of nodes of the net-work. This guarantees high scalability of the algorithm pos -sibly to millions of nodes. The proposed methodology takes a two-step approach for building and maintaining GMM pa-rameters in P2P networks. The first step is the monitoring phase in which, given an arbitrary estimate of the GMM parameters, our local asynchronous algorithm checks if the y are valid with respect to the current data to within user-http://smweb.bcgsc.bc.ca/chinook/index.html specified thresholds using different metrics, as we explain later. If not, this algorithm raises a flag, thereby indicat-ing that the parameters are out-of date. At this point, we employ a convergecast-broadcast technique to rebuild the model parameters. This step is known as the computation phase . The correctness of the monitoring algorithm guar-antees that a peer need not do anything if the flag is not raised  X  thus reducing communication and computation costs. When the data undergoes a change in the underlying distribution and the GMM parameters no longer represent it, the feedback loop indicates this and the parameters are recomputed. The specific contributions of this paper are as follows:
The rest of the paper is organized as follows. Section 2 presents the motivation of this work. Related background material is presented in Section 3. Section 4 provides some necessary background material on the EM algorithm and then introduces the notations and problem definition. Sec-tion 5 discusses the main theorem and its application for developing the algorithm. The monitoring of the param-eters are presented in Section 6. Section 7 discusses the computation problem. Theoretical analysis of the algorith m is presented in Section 8 followed by experimental results i n Section 9. Finally, Section 10 concludes this paper.
Monitoring models in large distributed environments can be done in three different ways: (1) periodic, (2) incremen-tal, or (3) reactive. In the periodic mode, a model is build at fixed intervals of time. While this approach is simple, one needs to come up with optimal value of the interval. Too small a value of the interval may unnecessarily build models when not needed, thereby wasting resources ( e.g. when data is stationary) while a longer interval may not update the model often in case of dynamic data. The in-cremental approach adjusts to the model whenever the data changes, thereby keeping the model up-to-date. However, developing incremental algorithms may be difficult. The third approach, what we take in this paper and also shown in [4][36][8], is to update a model only when the data no longer fits it. If the data is piecewise stationary, it has bee n shown that this approach may be both simple and efficient.
The suitability of the model and the data can be checked using several metrics: L2-norm,  X  2 , log-likelihood etc. In this paper we have developed a local distributed algorithm for monitoring the GMM parameters using (1) log-likelihood of the data, and (2) norm difference between the parame-ters. Local algorithms rely on a set of data dependent rules, thereby deciding when a peer can stop sending messages and output the result even it has communicated with only a handful of immediate neighbors. A peer can do nothing even if its data has changed as long as its local rules are satisfied.

Practical scenarios in which distributed EM can be used
Work related to this research can be subdivided into two major areas  X  distributed EM algorithms and computation in large distributed systems a.k.a P2P systems. We pro-vide a brief exposure to each of the topics in the next two subsections.
In standard EM algorithm, the task is to estimate some unknown parameters from the given data in the presence of some unobserved or hidden variables. The seminal work by Dempster et al. [10] proposed an iterative technique al-ternating between the E -step and M -step that solves this estimation problem. The paper also proved the convergence of the EM algorithm. In the E -step, the expected value of assumed hidden variables are generated using an estimate of the unknown parameters and the data. In the M -step, the log-likelihood of the unknown parameters given the data and hidden variables (as found in the previous step) are maxi-mized. This two-step process is repeated until the paramete r estimates converge. Furthermore, for gaussian mixture mod -els (GMM), the updates for the M-step can be written in a closed form as a computation of the weighted combination of all data points. This decomposable nature of the problem makes it tractable in a distributed setup.

In the distributed setup, we need to find decentralized im-plementations of the E-step and M-step. Distributed imple-mentation of the E-step is straightforward and communicati on-free: given the estimates of the parameters at each node, simply evaluate the estimates of the hidden variables based on only its local data. So the focus of most distributed EM research is to efficiently compute the parameters in the M-step in a distributed fashion. The naive approach of simply aggregating all the data at a central location does not scale well for large asynchronous networks. Next, we discuss sev-eral implementations of the distributed M-step.

In 2003, Nowak [31] proposed a distributed EM (DEM) algorithm with the execution of the M-step as follows. A ring topology is overlaid over the original network encom-passing all the nodes in the network. Due to this ring, the updates in the M-step proceed in a cyclical fashion whereby at iteration t , node m gets the estimates of the parameters from its neighbor in the ring, updates those estimates with its local data, and passes them to the next neighbor at clock tick t + 1. The paper proves that DEM monotonically con-verges to a local maxima and because of the incremental update, it converges more rapidly than the standard EM al-gorithm. However, this technique is not likely to scale for large asynchronous networks due to the strict requirement o f the overlay ring topology covering all the nodes in the net-work. Moreover, the algorithm is highly synchronized, with each round of computation taking time which is proportional to the size of the network. This becomes problematic espe-cially if the data changes or a node fails or joins whence the entire computation needs to be started from scratch.
To overcome this problem, several techniques have been developed. Recalling that for GMM, the updates for the M-step can be written as weighted averages over all the nodes X  data, any distributed averaging technique can be used for performing this computing over a large number of nodes. In the literature there are two basic types of distributed aver ag-ing techniques: (1) probabilistic i.e. gossip style  X  [17][6][20] in which a node repeatedly selects another node in the net-work and averages its data with the selected node and (2) deterministic i.e. graph-laplacian [26] or linear dynamical systems based [34]  X  in which a node repeatedly communi-cates with its immediate neighbors only and updates its stat e with the information it gets from all its neighbors. While the first class of algorithms probabilistically guarantee t he correct result, the deterministic algorithms converge to t he correct result asymptotically. Newscast EM [21] is the algo -rithm proposed by Kowalczyk and Vlassis which uses gossip-style distributed computation to compute the parameters of the M-step. At each iteration, any peer P i selects another peer P j at random and both compute the average of their data. It can be shown that, if the peer selection is done uniformly at random, any such gossip-based algorithm con-verges to the correct result exponentially fast. For such a technique to work in practice, the network must be fully con-nected i.e. any node must be able to select any other node in the network. Using deterministic averaging technique, G u [12] proposed an EM algorithm for GMM. In this algorithm peers communicate with immediate neighbors only. At any timestamp t , whenever a peer P i gets the current estimates from all of its immediate neighbors, it updates its own es-timate based on its own data and all that it has received. Then it moves to the next timestep t + 1 and broadcasts its own estimate to its immediate neighbors. This process continues forever. However, the major criticism for both of these techniques is that they are highly synchronous and hence not scalable for large asynchronous P2P networks.
In this paper we take a different approach. Assuming a previous estimation of the parameters, we monitor if the parameters are still valid with respect to the current globa l data. Our algorithmic framework guarantees correct result s (with respect to centralization) at a low communication cos t.
Several applications for distributed EM algorithms have also been proposed in the literature. Multi-camera trackin g [27], acoustic source localization in sensor networks [19] , dis-tributed multimedia indexing [30] are some of the examples.
Based on the type of computation performed in P2P sys-tems, this section can be subdivided into approximate algo-rithms and exact algorithms.
Approximate algorithms, as the name suggests, computes the approximate data mining results. The approximation can be probabilistic or deterministic .

Probabilistic algorithms use some variations of graph ran-dom walk to sample data from their own partition and that of several neighbors X  and then build a model assuming that this data is representative of that of the entire set of peers . Examples for these algorithms include the P2P k -Means al-gorithm by Banyopadhyay et al. [2], the newscast model by Kowalczyk et al. [20], the ordinal statistics based dis-tributed inner product identification for P2P networks by Das et al. [9], the gossip-based protocols by Kempe et al. [17] and Boyd et al. [6], and more.

Researchers have proposed deterministic approximation technique using the method of variational approximation [16][14]. Mukherjee and Kargupta [28] have proposed dis-tributed algorithms for inferencing in wireless sensor net -works. Asymptotically converging algorithms for comput-ing simple primitives such as mean, sum etc. have also been proposed by Mehyar et al. [26], and by Jelasity et al. [15].
In exact distributed algorithms, the result produced are exactly the same if all the peers were given all the data. They can further be subdivided into convergecast algorithms, flooding algorithms and local algorithms.

Flooding algorithms, as the name suggests, flood whatever data is available at each node. This is very expensive espe-cially for large systems and more so when the data changes.
In convergecast algorithms, the computation takes place over a spanning tree and the data is sent from the leaves up the root. Algorithms such as [33] provide generic solutions  X  suitable for the computation of multiple functions. These algorithms are, however, extremely synchronized.

Local algorithms are a class of highly efficient algorithms developed for P2P networks. They are data dependent dis-tributed algorithms. However, in a distributed setup data dependency means that at certain conditions peer can cease to communicate with one another and the result is exact . These conditions can occur after a peer has collected the statistics of just few other peers. In such cases, the over-head of every peer becomes independent of the size of the network and hence, local algorithms exceptionally suitabl e for P2P networks as well as for wireless sensor networks.
In the context of graph theory, local algorithms were used in the early nineties by Linial [23] and later Afek et al. [1]. Naor and Stockmeyer [29] asked what properties of a graph can be computed in constant time independent of the graph size. Local algorithms for P2P data mining include the ma-jority voting and protocol developed by Wolff and Schuster [37]. Based on its variants, researchers have further propo sed more complicated algorithms: facility location [22], outl ier detection [7], meta-classification [25], eigen vector moni tor-ing [8], multivariate regression [4], decision trees [5] an d the generic local algorithms [36].

Communication-efficient broadcast-based algorithms have been also developed for large clusters such as the one devel-oped by Sharfman et al. [35]. Since these algorithms rely on broadcasts as their mode of communication, the cost quickly increases with increasing system size.
In this section we present some background material nec-essary to understand the PeDEM algorithm that we have developed.
EM [10] is an iterative optimization technique to estimate some unknown parameters  X  given some data U . It is also assumed that there are some hidden variables J . EM algo-rithm iteratively alternates between two steps to maximize the posterior probability distribution of the parameters  X  given U : In order to apply the above two rules for estimation, we need closed form expressions for the E-and M-steps. Fortunately , closed form expressions exist for a widely popular estimati on problem viz. Gaussian mixture modeling (GMM). We dis-cuss this in details next as we will use it for the rest of the paper for developing our distributed algorithm.

A multidimensional Gaussian mixture for a random vector  X  X  X  x  X  R d is defined as the weighted combination: of k gaussian densities where the s -th density is given by p (  X  X  X  x | s ) = 1 each parameterized by its mean vector  X  X  X  s = [ s. 1 s. 2 . . . and covariance matrix C s =( x  X  s )( x  X  s ) T .  X  s = p ( s ) defines a discrete probability distribution over the k compo-nents. Given n multi-dimensional samples X = {  X  X  X  x 1 , . . . , the task is to estimate the set of parameters by maximizing the log likelihood of the parameters given the data: L ( X  | X ) = log Using EM for GMM, the E-step and the M-step can be writ-ten as: E-step (estimate the contribution of each point): M-step (recompute the parameters): where N (  X  X  X  x a ;  X  X  X  s , C s ) denotes the pdf of a normal distribu-tion with input  X  X  X  x a , mean  X  X  X  s and covariance C s . Note that the above computation needs to be carried out for all the k Gaussian components.

In the next few sections we shift our focus to distributed computation of these parameters and discuss some assump-tions and necessary background material.
Let V = { P 1 , . . . , P p } be a set of peers connected to one another via an underlying communication infrastruc-ture such that the set of P i  X  X  neighbors,  X  i , is known to P Each peer communicates with its immediate neighbors (one hop neighbors) only. At time t , let G denote a collection of data tuples which have been generated from the k gaussian densities having unknown parameters and unknown mixing probabilities. The tuples are horizontally distributed ov er a large (undirected) network of machines (peers). The local data of peer P i at time t is S i = [  X  X  X  x i, 1 ,  X  X  X  x i, 2  X  X  X  x number of data tuples at P i and d denotes the dimensional-ity of the data. The global input at any time is the set of all inputs of all the peers and is denoted by G =
Our goal is to develop a framework under which each peer (1) checks if the current parameters of the GMM are up-to-date with respect to the global (all peers X ) data, and (2) re-computes the models whenever deemed unfit. The network that we are dealing with can change anytime i.e. peers can join or leave. Moreover, the data is dynamic and is only as-sumed to be piecewise stationary. The proposed algorithm is designed to seamlessly adapt to network and data changes in a communication-efficient manner.

We assume that communication among neighboring peers is reliable and ordered. These assumptions can be imposed using heartbeat mechanisms or retransmissions proposed el se-where [11][18][36][5]. Furthermore, it is assumed that dat a sent from P i to P j is never sent back to P i . One way of ensuring this is to assume that communication takes place over a communication tree  X  an assumption we make here (see [36] and [5] for a discussion of how this assumption can be accommodated or, if desired, removed).
When all the data is available at a central location, the update equations for the iterative EM algorithm are given by Equations 1 X 4. However, in the distributed setup, all the data is not available at a central location. Therefore, for a ny peer P i , the log likelihood and the update equations, can be written as: E-step: M-step: Note that computation in the E-step is entirely local to a peer. However, for the log-likelihood and the M-step, a peer needs information from all the nodes in the network in order to recompute the parameters. In this paper, we consider a monitoring version of this problem: given a time-varying data set and pre-computed initial values of these parameter s (build from a centralized or sampled data) to all peers, does these parameters describe the union of all the data held by all the peers?
In other words, we focus on a monitoring and subsequent reactive updating of the GMM parameters. Given all the data at a central location, an admissible solution to the GMM problem occurs when the estimated parameters (given by Equations 2 X 4) become equal to the true parameters. However, for the distributed setup, since we are consider-ing a dynamic scenario, we relax this criteria and consider the solution to be admissible when it is within a user defined threshold  X  of its true value. For the monitoring problem, let c  X  , c  X  X  X  s and c C s denote the parameters that were calculated offline based on some past data, and disseminated to all the peers. The monitoring problem is to check if these param-respect to the current data of all the peers. We use two dif-ferent metrics to perform this (1) monitor the log-likeliho od of the data, and (2) monitor the parameters themselves. Be-low is a formal problem definition.
 Problem Definition: Given a time varying dataset S i , user defined thresholds  X  1 ,  X  2 , and  X  3 , and pre-computed itoring problem is to check if: for every gaussian s  X  { 1 , ..., k } , where kk F denotes the Frobenius norm of a matrix. In many cases, thresholding the log-likelihood of the data may be enough. However, there are situations where monitoring the parameters may prove beneficial, as we discuss later.
As a building block of PeDEM, we use an efficient, prov-ably correct, and local algorithm for monitoring functions of average vectors in R d , where the vectors are distributed in a P2P network. Here we present a brief summary; interested readers are referred to [3][36] for details.

Peers communicate with one another by sending sets of points in R d or statistics as defined later in this section. Let X i,j denote the last sets of points sent by peer P i to P j suming reliable messaging, once a message is delivered both P and P j know X i,j and X j,i . Below we present definitions of several sets which are crucial to the monitoring algorith m. Definition 4.1. The knowledge of P i is the union of S i with X j,i for all P j  X   X  i and is denoted by K i = S i [ K i can also be initialized using combinations of vectors de-fined on S i (instead of only S i ) as we will present in the next section.

Definition 4.2. The agreement of P i and any of its neighbors P j is A i,j = X i,j  X  X j,i .

Definition 4.3. The subtraction of the agreement from the knowledge is the withheld knowledge of P i with respect to a neighbor P j i.e. W i,j = K i \A i,j .

The next section presents a theorem which shows how we can convert this monitoring problem into a geometric problem for an efficient solution. For this we need to split the domain into convex regions since the stopping condition we describe later (Theorem 5.1) relies on this. The followin g definition states the properties of these convex regions.
Definition 4.4. A collection of non-overlapping convex regions R F = { R 1 , R 2 , . . . , R  X  , T } is a cover of region R invariant with respect to a function F : R d  X  O (where O is an arbitrary range), if (1) every R i  X  X  F (except T ) is con-and (3) T denotes the area of the domain, not encompassed by
Finally, for any  X  X  X  x  X  R d we denote R F (  X  X  X  x ) the first region of R F which includes  X  X  X  x . The precise specification of the convex regions will depend on the definition of F . Monitor-ing of the GMM parameters will require us to invoke three separate monitoring problems with three separate convex regions as we show in Section 6.

The goal is to monitor and compute mixture models de-fined on G . Since G is a hypothetical quantity, not available to any peer, each peer will estimate G based on only the sets of vectors defined above. However, these sets can be large, thereby making communication expensive. Fortunately, un-der the assumption that communication takes place over a tree topology imposed on the network, it can be shown that the same sets can be represented uniquely by only two suf-ficient statistics which we define next.
 Set Statistics: For each set, define two statistics: (1) the average which is the average of all the points in the respec-tive sets ( e.g. S i , K i , A i,j , W i,j , X i,j , X j,i the weights of the sets denoted by  X  ( S i ),  X  ( X i,j ),  X  ( X these two statistics for each set. We can write the following expressions for the weights and the average vectors of each set: Knowledge Agreement Withheld Note that these computations are local to a peer. The gen-eral methodology for computing F ( G ) requires us to cover the domain of F using non-overlapping convex regions. For the GMM, we show the convex regions that we need for monitoring the three parameters.

Our next section presents a general criteria which a peer can use to decide the correctness of the solution based on only its local vectors.
The goal of the monitoring algorithm is to raise a flag whenever the estimates of the parameters are no longer valid with respect to the union of all data i.e. G . The EM moni-toring algorithm guarantees eventual correctness, which m eans that once computation terminates, each peer computes the correct result as compared to a centralized setting. The fol -lowing theorem allows a peer to stop sending messages and achieve a correct termination state i.e. if F ( G ) &gt;  X  or &lt;  X  solely based on K i , A i,j , and W i,j .

Theorem 5.1. [Termination Criteria] [36] Let P 1 , . . . , P be a set of peers connected to each other over a spanning tree G ( V, E ) . Let G , K i , A i,j , and W i,j be as defined in the pre-vious section. Let R be any region in R F . If at time t no messages traverse the network, and for each P i , K i  X  R and for every P j  X   X  i , A i,j  X  R and either W i,j  X  R or W i,j =  X  , then F ( G )  X  R .

Proof. We omit the proof here. Interested readers are referred to [36].

The above theorem allows a peer to stop the communica-tion and output F ( K i ) which will eventually become equal to F ( G ). A peer can avoid communication even if its local data changes or the network changes as long as the result of the theorem is satisfied. Indeed, if the result of the theorem holds for every peer, and all messages have been delivered, then Theorem 5.1 guarantees this is the correct solution. Otherwise, if there exists one peer P z for which the condi-tion does not hold, then either of the two things will happen: (1) a message will eventually be received by P z or, (2) P will send a message. In either of these two cases, the knowl-edge K z will change thereby guaranteeing globally correct convergence.

Using this Theorem, we now proceed to monitor each of the parameters of the GMM using the distributed EM.
In this section we present the monitoring of the log like-lihood of the data and the three parameters given in Equa-tions 7 X 9.
Monitoring  X  s implies thresholding the absolute difference between the current  X  s (implied by the current data) and the calculated one c  X  s is with respect to a user defined constant  X  . Denoting this difference as Err (  X  s ), we can write This can be monitored using the framework presented in Section 4.4. Note that the quantity is the average of the estimates in the E-step ( q i,s,a  X  c  X  across all the peers. Each peer subtracts c  X  s from each of its Figure 1: (A) the area inside an  X  2 circle (B) A ran-dom vector (C) A tangent defining a half-space (D) The areas between the circle and the union of half-spaces are the tie areas. local q i,s,a . This forms the input S i for this monitoring prob-lem. However, due to the presence of the modulus operator, two concurrent monitoring problems need to be run instead of just one. Let these instances be M  X  s 1 and M  X  s 2 where M 1 and M  X  Err (  X  s ) &lt;  X  1 respectively. Since this monitoring problem is in R , the convex regions are subsets of the real line. There-fore, for monitoring  X  s , the following initializations need to be carried out:
Following a similar argument, monitoring  X  X  X  s is equivalent to thresholding the following quantity:
Err (  X  X  X  s ) =  X  X  X  s  X  c  X  X  X  s 2 &lt;  X  2 The quantity q in this case is not taken with respect to the number of tu-ples in the dataset S i , but rather over all the q i,s,a result, we set | S i | = lem, the geometric interpretation to the monitoring proble m is to check if the L2-norm of the vector difference between  X  X  X   X  c  X  X  X  s lies inside a circle of radius  X  2 . L2 norm threshold-ing of average data vector was first proposed in our earlier paper [36]. In R 2 , the problem can be depicted using Fig-ure 1. The area in which Err (  X  X  X  s ) &lt;  X  2 , is inside the circle (sphere) and hence is already convex in R 2 ( R d ). However, the other region outside the circle (sphere) is not convex. Hence random tangent lines (planes) are drawn on the sur-face of the circle (sphere) by choosing points c u 1 , . . . , c u the circle (sphere) (the same points across all peers). Each of these half-space is convex. To check if an arbitrary point  X  X  X  z is inside the sphere, a peers simply checks if k  X  X  X  z k &lt;  X  To check if it is outside, a peer selects the first point b u that  X  X  X  z b u i &gt;  X  2 . The following denotes the initialization
The last parameter that we need to monitor is the covari-ance matrix C s . A natural extension of the L2-norm in this case is the Frobenius norm. Let  X  X  X  y i,a =  X  X  X  X  x i,a  X   X  X  X 
C Therefore, By taking the square root and re-substituting  X  X  X  y i,a , we get, Thus thresholding problem is to check if k C s k F  X  c C s function, but C n s is. Thus we monitor the latter one instead. of the inequality is not true, i.e. C n s &gt;  X  3 ; k C s  X  . Thus using C n s for thresholding is more conservative: in the worst case we will have more number of false alerts for building new models, but will not miss any alert.
Let Err ( C n s ) = C n s  X  c C s . Let g : R 2 d  X  R be defined as follows:  X  ( s 1 , . . . , s 2 d )  X  R , g ( s 1 , . . . , s P g Each peer can locally compute the 2-d dimensional vector q down to zero-thresholding g applied to the average of local vectors. The last thing to prove is that g (or  X  g ) is a convex function. Taking the Hessian of  X  g it can be easily shown that  X  g is convex. We can therefore apply our tangent line technique for monitoring Err ( C n s ).

Note that the inside of g is already convex. The outside can be decomposed into convex regions using tangent lines placed at random locations on g . For a 2-dimensional case, Figure 2 shows the function (a parabola) and the possible define tangent lines. Checking if g ( K i ) &lt;  X  3 is equivalent to checking if K i lies inside g . If not, we find the first point b u such that K i b u i &gt; k b u i k . We then apply the theorem for half space defined by b u i .

Now since Err ( C n s ) can be both positive or negative, we need to check if | Err ( C n s ) | &lt;  X  3 . Therefore, we need two monitoring instances denoted by M C s 1 and M C s 2 . The fol-lowing denotes the datasets and convex regions for this mon-itoring problem.
Having discussed each of the monitoring problems, we are now in a position to present the algorithms for monitoring the parameters. For each gaussian s  X  { 1 , . . . , k } , we need to run the following monitoring problems separately:
In order to use Theorem 5.1 for developing a monitoring algorithm, the following steps must be followed: 1. Specify the input to the algorithm ( i.e. S i ) Figure 2: (A) the area inside a parabola (B) The area covered by the half-space (C) A tangent defin-ing a half-space. 2. Specify the cover i.e. R F For each of the monitoring problem, these are already speci-fied in the previous sections. Algorithm 1, 2, 3 and 4 present the pseudo-code. We describe the algorithm with respect to monitoring  X  s only. The other two are almost identical. Input :  X  1 , R F , S i ,  X  i , L and c  X  s
Output : Set
Initialization : On MessageRecvd X,  X  ( X ) , id from P j On any Event:
Algorithm 1 : Pseudo code for monitoring  X  s for any peer P i .

For any peer P i , the input to the algorithm are  X  , R F S ,  X  i , L and c  X  s (we describe L later). The output for each of the monitoring instance is a flag which is set if the cor-responding K i exceeds the threshold. In the initialization phase, it initializes its local statistics K i , A i,j and W cording to the equations in Section 4.4. The algorithm is entirely event driven. Events can be one of the following: a change in local data S i , message received or a change in the set of neighbors  X  i . If one of these things happen, a peer calls the ProcessEvent method (Algorithm 4). The goal of this method is to make sure that the conditions of Theorem 5.1 are satisfied by the peer which runs it. First peer P i finds the active region: the region R  X  X  F in which K i lies i.e. R = R F ( K i ). If, R = T , i.e. the knowledge lies in the tie region, the condition of the theorem does not guarantee a solution and hence the only correct solution is flooding all of its data. On the otherhand, if for all P j both A i,j  X  R and W i,j  X  R , P i does nothing and can rely on the result of the theorem for correctness. If A i,j /  X  R or Input :  X  2 , R F , S i ,  X  i , L and c  X  X  X  s
Output : Set On MessageRecvd X,  X  ( X ) from P j On any Event:
Algorithm 2 : Pseudo code for monitoring monitoring ~ s for any peer P i .
 W i,j /  X  R , the result of the theorem dictates P i to send a message to P j . Other than these two cases, a peer need not send any message even if its local data has changed. Input :  X  3 , R F , S i ,  X  i , L and c C s
Output : Set
Initialization : Initialize two monitoring instances M  X  s On MessageRecvd X,  X  ( X ) , id from P j On any Event: Algorithm 3 : Pseudo code for monitoring monitoring
C s for any peer P i .

Function ProcessEvent ( M , R F ,  X  i , L , LastMsgSent ) begin end
Message sending is performed in the ProcessEvent method itself. When R = T , the peer has to flood whatever knowl-edge it has. Thus it sets X i,j and  X  ( X i,j ) equals to its knowledge minus what it had received from P j previously. It then sends this to P j . However, when R 6 = T , a peer can refrain from sending all data. As shown by Wolff et al. [36] and Bhaduri et al. [4], this technique of sending all the data has adverse effects on the communication in a dynamic data scenario. This is because if a peer communicates all of its data, and the data changes again later, the change is far more noisy than the original data. So we always set X i,j | X i,j | such that some data is retained while still maintain-ing the conditions of the theorem. We do this by checking with an exponentially decreasing set of values of |W i,j | until either all K i , A i,j and W i,j  X  R , or |W i,j | =0. If the lat-ter happens, then there exist no condition for which a peer can have witheld data and it has to send everything. The conditions stated in the ProcessEvent method are exhaus-tive; a peer only sends a message if one of these conditions are violated. This guarantees eventual correctness based o n the theorem. Similarly, whenever it receives a message ( X and | X | ), it sets X j,i  X  X and | X j,i |  X  | X | and calls the ProcessEvent method again.

To prevent message explosion, in our event-based system we employ a  X  X eaky bucket X  mechanism which ensures that no two messages are sent in a period shorter than a constant L . This technique is not new but has been used earlier [4][36]. The basic idea is to maintain a timer. Whenever a peer sends a message, the timer is started. If later, the peer wants to send another message, it checks if L time units has passed since the timer was started. If yes, it sends the message and resets the timer to reflect the current time. Otherwise, the peer waits time difference between L and the  X  X imer X  time. When the timer expires, the peer checks the conditions for sending messages and decides accordingl y. Note that this mechanism does not enforce synchronization or affect correctness; at most it might delay convergence. We explore its effect thoroughly in our experiments.
In the next section we describe how we can use this moni-toring algorithm to update the models, if they are outdated.
Once the models ( c  X  s , c  X  X  X  s and c C s ) are monitored to within  X  of their true values, the next step is to rebuild the models if they are found outdated. The monitoring algorithm present in the previous section generates an alert whenever one of the following occurs for any s  X  X  1 ...k } : Building global models in a distributed environment is com-munication intensive. Here we rely on the outcome of our correct and efficient local algorithm to generate a trigger dictating the need for re-building the model. Given enough time to converge, the correctness of our monitoring algo-rithm ensures that even simple techniques such as best-effor t sampling from the network may be sufficient to produce good results. If it does not, the underlying monitoring al-gorithm would eventually indicate that and a new model building will be triggered.

The idea of model computation in the network is very simple. Peers engage in a convergecast-broadcast process. The monitoring algorithm can be viewed as a flag which is raised whenever the model is misfit with respect to the global data. If this happens for any peer, it does the fol-lowing. First it waits for a specific amount of time which we call the alert mitigation time  X  to see if the alert is in-deed due to a data change or random noise. If the alert exists even after  X  units of time, the peer checks if it has received data from all its neighbors except one. If yes, it generates a sample of user-defined size B from its own data and each of its children weighing each point inversely as the size of its subtree such that each point has an equal chance of being included in the sample. It then sends the sample to its parent and marks its state as convergecast. Whenever a peer receives data from all peers, it becomes the root of the convergecast and employs a centralized EM algorithm to build new model parameters. It then sends these mod-els to itself and marks its state to broadcast. Whenever a peer gets new models it forwards the models to all its neigh-bors (except the one from which it received) and moves from convergecast to broadcast phase. Because we do not impose the root of the tree, it may so happen that two peers get all the data simultaneously. We break the tie in such sce-narios using the id of the nodes. Only the peer with the highest id is allowed to propagate the model in the net-work. Algorithm 5 presents the pseudo code of the overall EM algorithm. As shown, there are three types of messages: Monitoring Msg , P attern Msg and Dataset Msg . The monitoring message is passed to the underlying monitoring algorithm. The pattern message is received as part of the broadcast round while the datasets are received when the peer engages in convergeast round.
In this section we prove that (1) the PeDEM algorithm is correct, and (2) local .
 Lemma 8.1. PeDEM is eventually correct .

Proof. In termination state, i.e. when all nodes in the network have stopped sending messages and there are no messages in transit, the knowledge K i of each peer P i will converge to one of these states: (1) K i = G , or (2) K i and W i,j are in the same R  X  R F for every neighbor P j . In the first case, K i = G  X  F ( K i ) = F ( G ). In the second case, by Theorem 5.1, K i , A i,j , and W i,j  X  R  X  G  X  R . By Definition 4.4, F is invariant in R and hence F ( G ) = F ( K Thus the PeDEM algorithm is eventually correct.

Lemma 8.2. PeDEM is local . In this section we demonstrate the performance of the Pe-DEM algorithm on synthetic dataset. Our implementation of the algorithm was done in Java using the DDMT 2 toolkit developed at UMBC. For the topology, we used the BRITE topology generator 3 . We experimented with the Barabasi Albert (BA) model since it generates realistic edge delays (in millisec), thereby simulating the internet. We convert the http://www.umbc.edu/ddm/Sftware/DDMT/ http://www.cs.bu.edu/brite/
Input :  X  1 ,  X  2 ,  X  3 , R F , S i ,  X  i , L , c  X  s , c  X  X  X 
Output : New model such that error is less than Initialization : Initialize vectors; Set LastDataAlert  X  X  X  ; Datasent  X  false ; On Receiving a message:
MsgType, Recvd P j  X  MessageRecvdFrom( P j ) if MsgT ype = Monitoring Msg then end if MsgT ype = P attern Msg then end if MsgT ype = Dataset Msg then end if S i ,  X  i or K i changes then end edge delays to simulator ticks for time measurement since wall time is meaningless when simulating thousands of com-puters on a single PC. On top of each network generated by BRITE, we overlay a communication tree.
The input data of a peer is a set of vectors in R d generated according to multi-dimensional GMM. More specifically, for a given experiment, we fix the number of gaussians k , their means  X  X  X  1 , . . . ,  X  X  X  k and covariance matrices C 1 , . . . , C also the mixing probabilities  X  1 , . . . ,  X  k . Every time a sim-ulated peer needs an additional data point, it first selects a gaussian s with corresponding probability  X  s and then gen-erates a gaussian vector in R d with mean and covariance  X  X  X  , C s . The means and the covariances are changed ran-domly at controlled intervals to create an epoch change.
In our experiments, the two most important parameters for measurement are the quality of the result and the cost of the algorithm.

For the regression monitoring algorithm, quality is mea-sured in terms of the percentage of peers which correctly compute an alert, i.e. , the number of peers which report that K i &lt;  X  when E G &lt;  X  and similarly K i &gt;  X  when E We also report the overall quality which is average of the qualities for both less than and greater than  X  and hence lies in between those two. Moreover, for each quality graph in Figures ?? , ?? , ?? , ?? , ?? and ?? we report two quantities  X  (1) the average quality over all peers, all epochs and 10 independent trials (the center markers) and (2) the standar d deviation over 10 independent trials (error bars). For the r e-gression computation algorithm, quality is defined as the L2 norm distance between the solution of our algorithm and the actual regression weights. We compare this to a centralized algorithm having access to all of the data.

We refer to the cost of the algorithm as the number of normalized messages sent, which is the number of messages sent by each peer per unit of leaky bucket L . Hence, 0.1 normalized messages means that nine out of ten times the algorithm manages to avoid sending a message. We report both overall cost and the monitoring cost (stationary cost) , which refers to the  X  X asted effort X  of the algorithm. We also report, where appropriate, messages required for converge -cast and broadcast of the model.
A typical experiment is shown in Figure ?? . In all the experiments, about 4% of the data of each peer is changed every 1000 simulator ticks. Moreover, after every 5  X  10 5 simulator ticks, the data distribution is changed. There-fore there are two levels of data change  X  (1) every 1000 simulator ticks we sample 4% of new data from the same distribution (stationary change) and (2) every 5  X  10 5 clock ticks we change the distribution (non-stationary change). To start with, every peer is supplied the same regression coefficients as the coefficients of the data generator. Figure ?? shows that for the first epoch, the quality is very high (nearly 96%). After 5  X  10 5 simulator ticks, we change the weights of the generator without changing the coefficients given to each peer. Therefore the percentage of peers re-porting K i &lt;  X  drops to 0. For the cost, Figure ?? shows that the monitoring cost is low throughout the experiment if we ignore the transitional effects. This work was supported by the NASA Aviation Safety Pro-gram, Integrated Vehicle Health Management Project. (a) Variation of L ( X  |G ) vs. time for the synthetic dataset. The red line refers to  X  . and number of gaussians. [1] Y. Afek, S. Kutten, and M. Yung. The Local [2] S. Bandyopadhyay, C. Giannella, U. Maulik, [3] K. Bhaduri. Efficient Local Algorithms for Distributed [4] K. Bhaduri and H. Kargupta. A Scalable Local [5] K. Bhaduri, R. Wolff, C. Giannella, and H. Kargupta. [6] S. Boyd, A. Ghosh, B. Prabhakar, and D. Shah. [7] J. Branch, B. Szymanski, C. Gionnella, R. Wolff, and [8] K. Das, K. Bhaduri, S. Arora, W. Griffin, K. Borne, [9] K. Das, K. Bhaduri, K. Liu, and H. Kargupta.
 [10] A. Dempster, N. Laird, and D. Rubin. Maximum [11] J. Garcia-Luna-Aceves and S. Murthy. A [12] D. Gu. Distributed EM Algorithm for Gaussian [13] Q. Huang, H. J. Wang, and N. Borisov.
 [14] T. Jaakkola. Tutorial on Variational Approximation [15] M. Jelasity, A. Montresor, and O. Babaoglu.
 [16] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and [17] D. Kempe, A. Dobra, and J. Gehrke. Computing [18] P. M. Khilar and S. Mahapatra. Heartbeat Based [19] N. Kitakoga and T. Ohtsuki. Distributed EM [20] W. Kowalczyk, M. Jelasity, and A. E. Eiben. Towards [21] W. Kowalczyk and N. A. Vlassis. Newscast EM. In [22] D. Krivitski, A. Schuster, and R. Wolff. A Local [23] N. Linial. Locality in Distributed Graph Algorithms. [24] K. Liu, K. Bhaduri, K. Das, P. Nguyen, and [25] P. Luo, H. Xiong, K. L  X  u, and Z. Shi. Distributed [26] M. Mehyar, D. Spanos, J. Pongsajapan, S. H. Low, [27] T. Mensink, W. Zajdel, and B. Kr  X  ose. Distributed EM [28] S. Mukherjee and H. Kargupta. Distributed [29] M. Naor and L. Stockmeyer. What Can be Computed [30] A. Nikseresht and M. Gelgon. Gossip-Based [31] R. D. Nowak. Distributed EM Algorithms for Density [32] N. Palatin, A. Leizarowitz, and A. Schuster. Mining [33] M. Rabbat and R. Nowak. Distributed Optimization [34] D. Scherber and H. Papadopoulos. Distributed [35] I. Sharfman, A. Schuster, and D. Keren. A Geometric [36] R. Wolff, K. Bhaduri, and H. Kargupta. A Generic [37] R. Wolff and A. Schuster. Association Rule Mining in
