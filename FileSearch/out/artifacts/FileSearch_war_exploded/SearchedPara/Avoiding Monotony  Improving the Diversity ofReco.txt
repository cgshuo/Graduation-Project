 The primary premise upon which top-N recommender sys-tems operate is that similar users are likely to have sim-ilar tastes with regard to their product choices. For this reason, recommender algorithms depend deeply on similar-ity metrics to build the recommendation lists for end-users. However, it has been noted that the products offered on rec-ommendation lists are often too similar to each other and attention has been paid towards the goal of improving di-versity to avoid monotonous recommendations.

Noting that the retrieval of a set of items matching a user query is a common problem across many applications of in-formation retrieval, we model the competing goals of max-imizing the diversity of the retrieved list while maintaining adequate similarity to the user query as a binary optimiza-tion problem. We explore a solution strategy to this opti-mization problem by relaxing it to a trust-region problem. This leads to a parameterized eigenvalue problem whose so-lution is finally quantized to the required binary solution. We apply this approach to the top-N prediction problem, evaluate the system performance on the Movielens dataset and compare it with a standard item-based top-N algorithm. A new evaluation metric ItemNovelty is proposed in this work. Improvementsonbothdiversityandaccuracyareob-tained compared to the benchmark algorithm.
 h.3 [ INFORMATION STORAGE AND RETRIEVAL ]: Information Search and Retrieval; h.m [ MISCELLANEOUS ] Theory, Algorithms, Performance diversity, accuracy, novelty, recommender system, metrics
Recommender systems have been very effective to help users find interesting and relevant items from a large infor-mation space. Most research has been driven by the goal of improving the accuracy of recommender systems and rela-tively little has focused on improving the utility of the rec-ommendation list, considering other desirable system char-acteristics. The accuracy metrics typically used to measure system performance are largely directed at average perfor-mance over the entire user set, such as MAE [2], or precision and recall [4]. Predictions are deemed successful so long as the predicted set overlaps with the user X  X  real preferences. However, these metrics do not account for other qualities of the prediction, in particular, they do not consider the diffi-culty of the prediction. It is reasonable to assume that a user will be more satisfied with a system that offers recommen-dations from a less obvious set of items that the user likes, than one that always offers items from a core set that the user has frequently used or purchased in the past. Users may become frustrated that there is little variety in the products offered by the system.

Take an academic paper recommender system as an ex-ample. Suppose all of the recommendations made were for papers written by a single author that the user has read in the past. Even if the user is strongly interested in these pa-pers and the system is very good at ranking them in order of preference, it is a poor recommender system because it is easy for the user to find these papers by other means. In fact, the academic paper recommender system may be re-garded as underperforming when evaluated by the standard metrics, if it offers less common topics that the user likes, instead of topics the user has already read and knows well.
We tackle this lack of variety issue head-on, by proposing methods for maximizing the diversity of a recommendation list, while maintaining an acceptable level of matching qual-ity. In this paper, we show how these competing concerns can be presented as constrained binary optimization prob-lems and propose solution strategies for these problems.
The contributions we make in this paper are as follows: Sets are represented by uppercase letters such as C and R . Column vectors are represented by bold lowercase letters such as x . The transpose of a column vector to a row vector is written as x T and the dot product of two vectors x and y is y T x . x column vector whose entries are all 1 is represented as 1 .
The drawbacks of developing recommender systems with accuracy as the single goal have been articulated in recent research. For example, in [14], informal arguments that the recommender community should move beyond the conven-tional accuracy metrics and their associated experimental methodologies are made and new user-centric directions for evaluating recommender systems are proposed. One direc-tion that has drawn recent interest is the evaluation of dif-ferent system characteristics, such as the diversity of the system output. In fact, in the context of case-based reason-ing systems, [15] argues that diversity can sometimes be as important as similarity to the target query. In this work, a definition of set diversity as average dissimilarity  X  X def-inition that we adopt in this paper  X  X s introduced. The authors propose three heuristic algorithms for selecting re-trieval sets that combine similarity and diversity. The best of these methods X  greedy selection  X  X dds cases one at a time to the retrieval set, according to a heuristic measure combining diversity and similarity. This is in contrast to our approach where the entire retrieval set is considered as a whole and an optimal set according to a well-defined objec-tive function is found. Furthermore, [15] does not examine the impact that additional diversity has on retrieval perfor-mance. In Section 7.5, we examine this issue by explicitly evaluating the system X  X  ability to recommend novel items.
In [17] the issue of diversification of recommendation lists is tackled. The authors propose a similarity metric using a taxonomy-based classification and use it to compute an intra-list similarity metric to determine the overall diversity of the recommended list. Intra-list similarity is analogous to Smyth X  X  [15] notion of subset diversity, except that it is a decreasing rather than increasing function of diversity. The authors provide a heuristic algorithm to increase the diver-sity of the recommendation list. Their results show that lists ordered for greater diversity perform worse on accuracy mea-sures than unaltered lists, but nevertheless users preferred the altered lists. In fact, depending on the user X  X  intentions, the makeup of items appearing on the list affects the user X  X  satisfaction with the recommender more than changes in the accuracy of the items on the list.

Another paper [6] examined the conditions in which diver-sity can be increased without loss of similarity, and presented an approach to deliver such similarity-preserving increases in diversity when possible. The role of diversity in conver-sational recommender systems is clarified in [5], highlight-ing the pitfalls of naively incorporation current diversity-enhancing techniques into existing recommender systems. Finally, [1] examines the effect of recommender systems on the diversity of sales. It uses a measure of statistical disper-sion called the Gini coefficient to measure sales diversity.
The general problem of finding a sub-set of items that best match a query or request issued by a system user is com-mon across many applications of information retrieval. The items in question may be products offered for purchase in a recommender system, or documents returned by a search engine for instance.

Let Q be a set of possible queries which may be issued to the system and I the item-set. Denote a query issued by user u as q u  X  X  and let R  X  X  be the sub-set returned by the system. We will refer to R as the recommended list . We can model the system algorithm as the application of a matching function f m : Q X  2 I  X  R , such that f m ( q u ,R )isa real-valued matching value associated with sub-set R ,where, without loss of generality, we assume that higher match-ing values correspond to better matches. The system may then return a sub-set R of some fixed size which maximizes f ( q u ,R ), or the smallest sub-set R whose matching value is above a given threshold.

In this paper, we consider matching functions where the matching value of a sub-set is formed as the aggregate of matching values of items contained in the sub-set. That is, there exists a function g m : Q X I X  R , such that
Generally, the query q u is an inaccurate or coarse rep-resentation of the user X  X  true requirements (e.g. a set of keywords to describe a particular type of document that the user wishes to retrieve). The matching function is a heuris-tic to capture the user X  X  subjective opinion on how well an item agrees with the requirements and is unlikely to be fully accurate. So the best matched items according to g m do not correspond necessarily to the most desirable items from the user X  X  point of view. For this reason, many practical sys-tems are interactive, allowing the user to scroll through a set of choices to make a decision. However, in recent research, it has been observed that, because all items in R are well matched to the same query q u ,itislikelythattheyarealso very similar to each other . This can lead to an unsatisfac-tory system from the user X  X  point of view, since the choice offered by the system has limited variety and motivates the goal of adding greater choice or diversity to the set R .The argument that suggests that the standard approach leads to recommended lists with low diversity also suggests that lists with greater diversity will also have greater variance in the matching values of the items contained in them. Clearly a system that offers poorly m atched items, with low g m ( q value, is unlikely to produce satisfactory results. So, the joint goals of offering a set R of high diversity and offering aset R of high matching value, stand in opposition to each other. In the following, we will present objective functions that capture the trade-offs between these goals and show how the maximization of these objective functions can be represented as a binary quadratic programming problem.
A number of different definitions of set diversity have been proposed in the literature. One approach is to model diver-sity as the average rarity of the elements in the set [10]. Another, which has been discussed in other application con-texts, (e.g. [8, 11]) and which has been used in a similar context to ours in [15], is to model diversity as the aggregate or equivalently average dissimilarity of all pairs of items in the set. Specifically, given a distance function, d : I X I  X  R such that d ( i, j ) is the distance or dissimilarity between el-ements i, j  X  X  , the diversity f D ( R ) is given as the average dissimilarity of all pairs of elements contained in R .That is, where p = | R | . Here we assume that this distance function is symmetric ( d ( i, j )= d ( j, i )).

The distance function is application-dependent and may correspond for example to a distance between feature vec-tors under a mapping of I into a feature space. Often, the matching function g m ( q u ,i ) is a decreasing function of some distance metric and, if the same metric is used to mea-sure dissimilarity, then the opposition of the competing re-quirements of diversity and high matching are immediately clear. However, in the following, all that is required is that that item-item dissimilarities and query matching values can be calculated, without restriction on the feature space or method used to calculate these values. Let C  X  X  be a set of size | C | = M .Lettheelementsof C be listed as C = { c 1 ,...,c M } . It is useful to represent subsets R  X  C using an M -dimensional indicator vector y , such that y ( i )=1if c i  X  R and y ( i )=0otherwise. Moreover, let D be the M  X  M distance matrix with ( i, j )-element d ( c i ,c j ). Then, the diversity of R may be written as the quadratic form 1
Moreover the matching function may be written as the linear expression where m u is the M -dimensional vector with elements m ( i )= g ( q u ,c i ).

With these expressions, it is possible to express the trade-offs between diversity and matching as constrained optimiza-tion problems.
 Find the vector y  X  defined as In this optimization problem, we seek to maximize the di-versity of the set R , under the constraint that the matching value of R be greater than some matching tolerance m tol . The final two constraints specify that y is a binary vector with p non-zero values i.e. | R | = p .
 Find the vector y  X  defined as In this optimization problem, we seek to maximize the match-ing value of the set R , under the constraint that the diversity of R be greater than some diversity tolerance d tol . Again, the final two constraints specify that y is a binary vector with p non-zero values i.e. | R | = p .
Note that this equivalence depends on d ( c i ,c i )=0  X  i .In fact nothing changes in the analysis, as long as d ( c i ,c constant  X  i . Given a fixed parameter  X   X  [0 , 1], find the vector y  X  defined as Here, the trade-off between the two objectives is explicitly expressed using the parameter  X  that represents the impor-tance given to the matching value in comparison to that given to diversity.  X  and  X  are normalization parameters to ensure that diversity and matching measures are normal-ized to the same scale, for example, that both lie in the range [0 , 1].
Problems 1 and 3 above correspond to binary quadratic programming problems with linear constraints. Problem 2 is a binary linear programming problem with quadratic and linear constraints. There is a vast literature on solution strategies for such problems [16]. A general strategy is to search through a space of solutions, using for example branch and bound search, where the solutions are generated by solv-ing relaxations of the binary problem to problems with real-valued solutions. Writing x  X  R M as the solution to the real-valued problem, a quantization strategy must then be applied to map this solution to a binary vector y .Inthe following, we will focus mainly on Problem 3, where the objective represents an explicit trade-off between our two concerns. Additionally, we will pursue a solution to Prob-lem 1, through a two-stage process of initially identifying a candidate set C in which all sub-sets of size p satisfy (3) and then pursuing solutions to the quadratic program with-out constraint (3), which is equivalent to solving Problem 3 with  X  =0 . 0.
A key step in the solution of a binary quadratic program-ming problem is the choice of relaxation to a real-valued problem. Indeed, the real-valued problem may itself be a hard problem. While polynomial time solutions exist when the Hessian matrix, D, is positive semi-definite (i.e. has no negative eigenvalues), otherwise the real-valued quadratic program is NP-complete. A common relaxation of (2) is spectral relaxation , where the relaxed problem is and is solved by finding the eigenvector corresponding to the maximum eigenvalue of D. It is clear that any feasible solution y of (2) is a feasible solution of (8), so that x is an upper bound on the maximum value of (2). Given that fast eigensolvers for large-scale systems exist, this approach has the advantage of speed. However, in many cases (see [9], for example) the solution x  X  is far from the best binary solution y  X  and quantization yields a poor binary solution.
Spectral relaxation can be equally applied to Problem 3, through a standard modification of the problem. Extending y to an M +1 dimensional vector, we can write the objective (7) as the quadratic form ( y T ,y ( M +1)) (1 with the additional constraint y ( M +1)=1. However, the corresponding spectrally rela xed problem imposes no con-straint on x ( M +1). Hence, if D is a negative definite matrix (e.g. D  X  S for some positive definite similiarity matrix, S), then, in the limit as  X   X  0, the best relaxed solution is x  X  =(0 ,..., 0 ,  X  p ) T , which clearly cannot be converted to a good binary solution of (10).

It is noteworthy that spectral relaxation is essentially the basis underlying the method in [7], where spam users are detected through a principal components analysis (PCA) of the similarity matrix. Here, a subset of maximally similar users is extracted using a user-user similarity matrix, while in our context, we are locating maximally dissimilar items using an item-item dissimilarity matrix.

At the other end of the scale from spectral relaxation, semi-definite programming relaxations can yield higher qual-ity solutions but can be inefficient. An alternative relaxation to (7) and the one which we use for our application is for which a fast solution is presented in [12]. Note that in [12], the constraint is actually x 2  X  p , but adding k ( p  X  x T x ) to (11) for a sufficiently large penalty k , will yield a solution with x 2 = p . A code for solving this problem, called LSTRS, is described in [13]. The method is based on a reformulation of a trust-region sub-problem as a parameterized eig envalue problem: with  X  a scalar parameter. LSTRS relies on matrix-vector products only and has low and fixed storage requirements, features that make it suitable for large-scale computations.
Once the real-valued solution x  X  to the relaxed problem is found, it is necessary to quantize the values to a candi-date binary solution y . One approach is to consider the dot product y T x  X  . Indeed, writing  X  max (resp.  X  min the maximum (resp. minimum) eigenvalue of D, if x  X  is a solution to the spectrally relaxed problem and we write e = y  X  x  X  , it is easy to check that Next, using the fact that e T D e  X   X  min e T e for all vectors e , we have that Thus maximizing y T x  X  maximizes a lower bound on the quadratic form. Since y is binary and noting that  X  x  X  is also a valid solution, the dot product is maximized by setting y ( i ) = 1, wherever x ( i )isoneofthe p highest elements of + x or  X  x , depending on which gives the larger value. While relaxation and quantization can form just one step in a larger branch-and-bound search, for efficiency reasons in the results that follow, we do no further searching once a feasible binary solution is generated.
We apply our diversity analysis to top-N recommenda-tion. Given a past history of purchases made by a set of users, the problem of top-N recommendation is to find a subset R  X  X  of N products to recommend for purchase to a given user u .Let U = { u 1 ,...,u P } be the set of possi-ble system users and I = { i 1 ,...,i Q } the set of items or products available for purchase, with P = |U| the total pop-ulation of system users and Q = |I| , the total number of products. For each user u ,the user profile P u  X  X  is the set of items purchased by u .

The framework we use for computing a top-N recommen-dation list for a user u is given in Figure 1. These steps en-capsulate the SUGGEST recommendation engine [3]. In the case of SUGGEST an item-item similarity matrix is learned during a training phase. Using an input parameter K , C is formed as the intersection of the sets of K most similar items to each item in the user profile. The sorting function then computes the similarity of each candidate item to all items in the profile and the N best items are recommended. The general framework allows the introduction of diversity as an additional criterion for selecting and ranking C .
To produce recommendation lists that are both similar to the user profile and diverse, we solve the optimization problems of Section 4. To do so, we note two steps within the framework at which diversity criteria can be applied. The first is the selection of the candidate set C . Solving Problem 3, for a given  X  and a given size p = l , allows the selection of l items from I X  P u , which are diverse as well as similar to the user profile. Alternatively, or additionally, from an intermediate set I of items in C that are most similar to the user profile, we can solve Problem 3, to extract the most diverse subset of size p = N from I . The intermediate set is introduced for efficiency reasons only. Diversity criteria can be applied to the entire set C (i.e. I = C ) and this is in fact the approach used in our evaluation in Section 7.5.
To apply our method, it is necessary to compute the dis-similarity matrix D and determine the matching function g ( q u ,i ). Depending on what application-level information is available, there are many potential ways to do this. For example, in the movie recommendation domain, the dissim-ilarity function could be based on genre information associ-ated with each movie. For our analysis however, we assume that only purchase transaction information is available. In particular, given the item-item cosine correlation matrix  X  learned from the data using a method such as that given in [3], define the item-item similarity matrix as S  X  . In keeping with [15], the distance matrix D may be chosen as D 1  X   X . Alternatively, without signficantly changing the optimisation problem, we may simply take D  X   X .

Note that, for the top-N problem that we are considering, the transaction matrix consists of user-item entries that are 1 if the corresponding user has purchased the item and 0 otherwise. As a consequence,  X  ij  X  [0 , 1]. Each of the above matrices is M  X  M ,where M is the size of the super-set from which items are selected. Writing the user profile as the M -dimensional vector u such that u ( j )=1ifuser u has purchased the j th item in the super-set and zero otherwise, the matching vector m u , giving the similarity of each item to the user profile u ,is m u S u . Hence, the matching value of a sub-set specified by the indicator vector y is  X  u T where the normalization parameter  X  1 / ( p 1 T u ) ensures that the matching value of a sub-set is contained in the same absolute range ([0 , 1]) as the diversity value (1).
When solving the optimisation problems in this section, we have taken D  X   X . However, so that direct comparison can be made with other work in this area, we have plotted diversity using the more usual definition, D 1  X   X . Since y (1  X   X ) y = p 2  X  y T  X  y for any binary vector y of norm p , the binary solution using either definition is the same. However, the optimal for the real-valued relaxed problem differs depending on the definition of D so that each defini-tion could lead to different solutions, although we would not expect a significant difference in results.
The purpose of introducing diversity into a recommenda-tion engine is to increase the probability of recommending products that the user likes but which are not too alike other products the user has purchased. It is useful to introduce a measure to capture this notion. For any sub-set R  X  X  ,we define the novelty of an item i  X  R as n
R ( i ) p ( f D ( R )  X  f D ( R  X  X  i } )) = that is, the amount of additional diversity that i brings to the set R . Highly novel items are also difficult items for a collaborative filtering algorithm to recommend.
Our diversity method is evaluated using the Movielens 2 dataset. This is a standard evaluation dataset consisting of 1 , 000 , 000 ratings that 6 , 040 users gave for 3 , 900 movies. The dataset is highly sparse, containing only 4% of all pos-sible ratings. As we are interested in the top-N prediction http://www.movielens.org Figure 2: Moving average plot of window size 100 of the diversity of the user profiles P u in the Movielens dataset against the user profile degree. The variance of the diversity is indicated by two lines that are one standard deviation above and below the mean diver-sity plot. Also shown is the diversity of a randomly selected item-set of the same size and the maximum novelty of items in the user profile. problem, we follow [3] and do not consider the rating value when calculating similarities, instead, any rating is accepted as evidence that the movie was watched (and hence  X  X iked X  or  X  X urchased X ).

The collaborative filtering algorithm has been shown to work well on this dataset, under the usual metrics. Thus, we expect that there is strong similarity between items in user profiles i.e. between movies that a user has rated. Nev-ertheless, highly novel items do exist in the dataset and the probability that standard algorithms will recommend such items is very small.

It is interesting to examine the diversity of user profiles in the Movielens dataset. In Figure 2, user profile diver-sity is plotted against profile size in a moving average plot, alongside the diversity of randomly chosen item sub-sets of the same size. It is clear that there is a high degree of inter-profile similarity exhibited in the Movielens dataset, indicated by the fact that the diversity is well below that of a randomly selected item-set. However, plotting the maxi-mum novelty of all items in the user profile in this plot also shows that profiles do contain items which are significantly more difficult to recommend than the average items in the dataset. Following [3], we split the dataset into a training dataset Y
T and a test dataset Y P by randomly assigning 50% of the user profiles to each set. The training dataset is used to learn an item-item correlation matrix  X , by applying the cosine similarity method. The values in  X  lie in the range [0 , 1], and  X  is symmetric positive definite.

To evaluate the method, a large number of user profiles P are selected at random from Y P . For each profile,  X  | P are removed from the profile and remainder of Y P is used to make a top-N prediction for u .Here  X   X  [0 , 1] represents the percentage of the user profile that is removed. We refer to the removed items as the profile test-set , written as T u where  X   X  [0 , 1] is a parameter representing the difficulty of the prediction probl em. In particular, T u (  X  ) is selected as follows: Our primary performance metric is precision , defined as where U is the set of all tested users.
We firstly observe the diversity and matching quality of sets produced by solving the optimization problems of Sec-tion 4. In particular, (7) is solved to select the candidate set C , from the set of all possible items I X  P u . The candidate set size is fixed to l = 500 items. Figure 3 shows the resulting diversity of the candidate set, averaged over all tested users, plotted against the parameter  X  . For comparison purposes, also shown in Figure 3 is the average diversity of the candi-date set when chosen according to the SUGGEST algorithm. We set K = 20 in the SUGGEST algorithm, so that the can-didate set is constructed through the intersection of the 20 most similar items to each item in the user profile. To allow direct comparison, we terminate the SUGGEST algorithm as soon as the candidate set contains l = 500 items. Lastly, we show the diversity of a candidate set where l = 500 items are chosen with equal probability from I X  P u .InFigure4, the average similarity of the candidate sets to the user profile is plotted. From these plots, it is clear that we can use our method to control diversity and matching quality of the gen-erated sets. As  X   X  0, our method produces sets of greater diversity than random sets, while as  X   X  1, our method produces sets of higher similarity than the SUGGEST algo-rithm. The interesting range for  X  is  X   X  [0 . 5 , 0 . 8], in which sets of greater diversity can be selected at a cost of reduced matching quality.
We now examine whether using diversity can produce bet-ter system performance, particularly for recommendation of novel items. In this experiment, we use the standard SUG-GEST algorithm to select a candidate set C of l = 500 items. Again, the SUGGEST parameter K = 20 is chosen. We take theintermediateset I = C ,setthesizeof R to N = 20, and thus select R directly from C , by one of three methods: 1. Random :The items in R are selected with equal prob-2. SUGGEST : The candidate set is sorted according to 3. Theta : The optimization problem (7) is solved with Figure 3: Mean diversity of a candidate set of size l = 500 , plotted against  X  . Also shown is the mean diversity of a randomly chosen candidate set and of a candidate set chosen using the SUGGEST algo-rithm. Figure 4: Mean similarity of a candidate set of size l = 500 , plotted against  X  . Also shown is the mean similarity of a randomly chosen candidate set and of a candidate set chosen using the SUGGEST algo-rithm.

Because novel items have low similarity to the other items on user profile, the SUGGEST algorithm is highly unlikely to recommend them, even if they are in the candidate set. The Random algorithm is likely to perform better for novel items, since once the item is in the candidate set, it has the same chance as all other items of being recommended. We take  X  =0 . 1, (i.e. the profile test set size | T u | is 10% of the total user profile size | P u | ) and examine how the algorithms perform for different values of the novelty parameter  X  .So, for example, when  X  =0 . 3, if the user profile is made up of 100 items, then the profile test set contains 10 items with these items drawn from the 30 most novel items in the pro-file, according to the item novelty value n P u ( . ), calculated from (13).
 The diversity and similarity of the recommendation list R is shown in Figure 5. The plots follow the same shape as for the candidate set, with the interesting values of  X  in the range [0 . 5 , 0 . 9]. Figure 6 plots the precision  X  (  X  ) against  X  with  X  varied in the range [  X , 1 . 0], for the SUGGEST al-Figure 5: Mean similarity and diversity of the rec-ommended set R , plotted against  X  . Figure 6: Precision of the recommendation algo-rithms, plotted against the novelty value of the test set. gorithm, the Random algorithm and the Theta algorithm using two different values,  X  =0 . 0and  X  =0 . 7. Firstly, it is interesting to observe the behavior of the SUGGEST al-gorithm. For  X   X  0 . 3,  X  (  X  ) = 0, that is, the algorithm fails completely when the test set is drawn from nearly 1 / 3of the user profile containing items of highest novelty. Up to  X  =0 . 5thevalueof  X  (  X  ) is still near to 0. This illustrates how standard performance measures can hide important de-tail about the quality of the recommendation. The standard performance result usua lly reported corresponds to  X  =1 . 0, but the recommendation quality falls off quickly once the novelty of the recommendation problem is increased. The Theta algorithm, with  X  =0 . 0 corresponds closely to the random algorithm, since in this case, the most diverse set R is selected by the optimization algorithm, without any con-sideration of matching quality. For very novel test sets (i.e. low  X  ), both algorithms perform better than SUGGEST, as expected.

When  X  =0 . 7, a strong improvement over the SUGGEST algorithm can be observed. In particular, it may be observed that the performance drops less sharply as  X   X  0thanit does for the SUGGEST algorithm. In fact, when  X  =0 . 3, the performance has dropped by 76% from that achieved at  X  =1 . 0 and while this is a significant drop, it compares very favorably with SUGGEST where the performance has been completely lost at this point. Figure 7: Precision of the recommendation algo-rithms, plotted against  X  with novelty value  X  =0 . 5 .
In Figure 7 we plot precision against  X  for a fixed novelty value  X  =0 . 5, showing the full range of  X  in which a clear performance improvement is achieved over both the Random and SUGGEST algorithms.
In this paper we have shown how to express the compet-ing concerns of maximizing the diversity of a retrieved list while also maximizing its similarity to the target query, as a binary optimization problem. Furthermore, we have given an approach for solving this optimization problem. Apply-ing this theory to the top-N recommendation problem, we have shown how to control the diversity and matching qual-ity of recommendation lists. Our evaluation on the Movie-lens dataset has shown that our method can increase the likelihood of the system recommending novel items, while maintaining good performance on the core items. It is no-table however that, even with diversity, the probability of recommending novel items is very low and this will be the case whenever similarity is used as a primary selection cri-terion. Future work will apply the methodology presented here to other problems in information retrieval. We will also consider other methods for enabling novel but relevant recommendations.
 This work is supported by Science Foundation Ireland, grant number 07/RFP/CMSF219. [1] D. Fleder and K. Hosanagar. Recommender systems [2] J.L.Herlocker,J.A.Konstan,L.G.Terveen,and [3] G. Karypis. Evaluation of item-based top-N [4] C. D. Manning, P. Raghavan, and H. Schutze.
 [5] L. McGinty and B. Smyth. On the role of diversity in [6] D. McSherry. Diversity-conscious retrieval. In [7] B. Mehta, T. Hofmann, and P. Fankhauser. Lies and [8] K. Nehring and C. Puppe. A theory of diversity. [9] C. Olsson, A. P. Eriksson, and F. Kahl. Solving large [10] G. P. Patil and C. Taillie. Diversity as a concept and [11] D. Reynolds. Comparison of background [12] M. Rojas, S. A.Santos, and D. C.Sorensen. A new [13] M. Rojas, S. Santos, and D. Sorensen. LSTRS: [14] J. R. Sean M. McNee and J. A. Konstan. Accurate is [15] B. Smyth and P. McClave. Similarity vs. diversity. In [16] L. N. Vicente, J. J. Judice, and P. M. Pardalos. [17] C. Ziegler, S. M. McNee, J. A. Konstan, and
