 It is very difficult for many users of Information Retrieval (IR) system to select appropriate query terms to represent the ir information need. This difficulty cause the mismatch between query terms and information need; e.g., the query terms are ambiguous and it is difficult to focus on relevant documents only; a number of relevant documents are difficult to retrieve because they have only a small part of initial query terms.

To reduce this mismatch, many IR systems use query term expansion tech-niques to find better query terms[1]. However, the effectiveness of this technique depends on the quality of initial query terms. Cronen-Townsend et al. [2] used a query clarity score based on a language model to decide if the query terms contain relevant information for the query term expansion; this approach was shown to be effective.

The Reliable Information Access (RIA) Workshop [3] conducted a failure analysis [4] for a set of topics using seven different popular IR systems and proposed a topic categorization based on the types of failures they encountered. They also conducted a relevance feedback experiment using a different IR sys-tems [5]. This study, however, did not examine the relationship between topic difficulty based on the mismatch and the effect of relevance feedback. Because the relevance feedback technique is used for reducing the mismatch between the initial query and information need, it is important to determine the effectiveness of relevance feedback when used with query expansion.

For characterizing topic difficulty based on the mismatch, query term expan-sion subtask was designed for evaluating topic difficulties from the viewpoint of query term expansion in the NTCIR-5 Web task [6]. In this subtask, several fea-ture quantities that characterize this mismatch were investigated for analyzing the topic difficulty, but single quantity is not enough to characterize this topic difficulty.

Therefore, in this paper, I analyze the effect of query term expansion technique of IR experiment data in topic-by-topic manner and discuss how these quantities are affect the quality of this technique.

The remainder of this paper is divided into four sections. Section 2 briefly review various statistical features for defining the topic difficulty and the ef-fectiveness of the query expansion term. In Section 3, the result of query term expansion subtask in the NTCIR-5 web task is briefly summarized. Section 4 an-alyzes the experimental results, and Section 5 gives the conclusions of the paper. Buckley et al. [7] hypothesized a possible reason why query expansion improves the query performance as follows. 1. one or two good alternative words to original query terms (synonyms) 2. one or two good related words 3. a large number of related words that establish that some aspect of the topic 4. specific examples of general query terms 5. better weighting to original query terms
The first four reasons relate to query t erm expansion. Reasons 1, 2, and 4 can be evaluated using a thesaurus. However, since Voorhees [8] confirmed simple automatic query term expansion based on a general thesaurus did not improve query performance, it may be inappropriate to use a general thesaurus for this evaluation.

Therefore, in the query term expansion subtask in the NTCIR-5 Web task, a mismatch between different information-need expressions (query terms and relevant documents) were used for this evaluation. 2.1 Feature Quantities for Characterizing Mismatch Between Initial When a user carefully selects good query terms, query term expansion may not improve the retrieval performance. Therefore, it is crucial for this subtask to evaluate the quality of the initial query based on the mismatch between the initial query and the relevant documents.
In the NTCIR-4 web test collection, each query has a Boolean operator infor-mation. In order to estimate the information need precisely, it is better to use this Boolean information 1 .

When the query is represented with a Boolean operator, this mismatch is char-acterized as a mismatch between the documents that satisfy this query and the relevant documents. When the initial query is precise enough, documents that satisfy the Boolean query (Boolean satisfied documents) and relevant documents are equivalent ((1) and (3) in Figure 1 are an empty set).
However, because it is difficult to construct good queries, (1) and (3) are empty sets in almost no query. The sizes of (1) and (3) characterizes the quality of the query from the viewpoint of query expansion. For example, when there are many documents in (1), the initial query is too general and requires new query terms that define the context of the query. Conversely, when (3) has many documents, the initial query is too strict and it is necessary to determine alternative words to relax the query.

Since the number of documents in (1) and (3) are affected by the number of relevant documents, the following two feature quantities were used for this evaluation.
 R &amp; B/R The ratio between the size of relevant documents that satisfy the R &amp; B/B The ratio between the size of relevant documents that satisfy the Since a set of B is a result of the Boolean IR system without term weighting, R &amp; B/R and R &amp; B/B represents the appropriateness of initial query quality from the view point of recall and precision respectively.

These two feature quantities are the values that represent the characteristics of the set of initial query terms. Therefore, this value is also effective for the different IR models (probabilistic, vector-space, and etc.) evaluation even though it is based on the Boolean IR model. 2.2 Feature Quantities for Evaluating Effectiveness of the Query Feature quantities proposed in 2.1 can also be used to evaluate a query term when the Boolean query is constructed using only this term.

In addition to these features, the following three criteria were proposed for selecting feature quantities. 1. Appropriateness of the alternative term for each initial query term. 2. Appropriateness of the context definition term for the query. 3. Appropriateness of the term that characterizes the relevant documents.
A good alternative term should exist for relevant documents that do not contain the initial query term. Therefore, the number of documents that have a query expansion term and do not have an initial query term are useful for evaluation.

A good term for context definition is a distinct term that exists in relevant documents. Therefore, the number of documents that have a query expansion term in the relevant documents, the Boolean satisfied documents, and total documents are useful for evaluation.

The following feature quantities are defined for each query expansion term. total : Rel The number of relevant documents that have a query expansion term. total : Bool The number of Boolean satisfied documents that have a query ex-total : R &amp; B The number of Boolean satisfied relevant documents that have a total : All The number of documents that have a query expansion term in the
Feature quantities that are based on mutual information content were used for evaluating the distinctiveness of each term [9]. These quantities are the mutual information content between relevant documents r and the term w . p ( w )isthe probability of the term w in the document database and p ( w | r ) is the probability of the relevant documents.

When term w exists explicitly in the relevant documents, MI ( w )increases. The NTCIR-5 query term expansion subtask was designed for evaluating the ef-fect of query term expansion technique by using NTCIR-4 web test collection[6]. In order to evaluate the query term expansi on technique by itself, several feature quantities discussed in previous section were introduced for analyzing the topic difficulty. 3.1 The NTCIR-4 Web Test Collection The NTCIR-4 Web test collection [11] is a set of 100 gigabytes of html document data and 80 topics for retrieval experiments. 35 out of 80 topics are for the survey retrieval topics and the other 45 are for the target retrieval topics. The survey retrieval topics are designed for finding most relevant documents and the target retrieval topics are for finding just one, or only a few relevant documents of the highly ranked documents. Since the target retrieval topics may miss relevant candidate documents, only the survey retrieval topics (topic numbers 1, 3, 4, 6, 84, 86, 88, 91, 95, 97, 98, and 99) were used for this query term expansion subtask.
Figure 2 shows a sample topic in this test collection. &lt; TITLE &gt; includes 1-3 terms with Boolean expressions. The attribute  X  X ASE X  in &lt; TITLE &gt; , &lt; ALT0 &gt; , &lt; ALT1 &gt; , &lt; ALT2 &gt; , &lt; ALT3 &gt; means: (a) All the terms are related to one another by the OR operator. (b) All the terms are related to one another by the AND operator. (c) Only two terms can be related using the OR operator; the rest are specified
For the sample topic described in Figure 2, the Boolean query ( # (offside) and ( (soccer) or (rule))) from TITLE and ( # (offside) and (soccer) and (rule)) is formulated from ALT3. 3.2 Feature Quantities of Topics in the NTCIR-4 Web Survey The graph in Figure 3 shows a characteristic of the topics in the Survey Retrieval Topics by using Boolean formula defined in title field. The X axis of the graph corresponds to R &amp; B/R and the Y axis corresponds to R &amp; B/B . The radius of each circle indicates the number of the relevant documents.

All statistical values were calculated using an organizer reference IR system named  X  X ppropriate Boolean Query Reformulation for Information Retrieval X  (ABRIR) [9]. Such values may differ according to the method of extracting index keywords from the documents.

From this graph, all initial queries were not sufficiently appropriate to dis-tinguish all relevant documents from the other documents. For the topics that have higher R &amp; B/R and lower R &amp; B/B , such as topics 1, 4, 6, 55, and 98, the term for context definition may be good query expansion terms. For the topics that have lower R &amp; B/R and higher R &amp; B/B , such as 65, 76, and 82, alternative terms may be good query expansion terms. The topics that have lower R &amp; B/R and lower R &amp; B/B , such as 45, 62, 63, 80, and 84, may require various types of query expansion terms. 3.3 Retrieval Experiments in the Subtask Four participants, listed below in alphabetical order of RUN-id, and one orga-nizer reference system submitted their completed run results.

Several query term expansion techniques and information retrieval models were used in these runs for which results were submitted.
 JSWEB[12]. Experimented with relevant document vectors that were gener-NCSSI[13]. Experimented with a clustering technique for the initial retrieval R2D2[14]. Experimented with Robertson X  X  Selection Value (RSV) for select-ZKN [15]. Experimented with Larvenko X  X  relevance model for selecting query ABRIR: Organizer Reference System [9]. Experimented with mutual in-
In order to evaluate the effect of the query term expansion technique, all participants submitted a set of run with and without query term expansion. In addition, following two different (pseudo-)relevant document selection methods were used for evaluating the effect of the quality of the feedback documents.  X  Automatic selection: auto  X  Simulation of the user relevance document selection: user
Each participant submitted sets of retrieval results, expansion term candidates lists, and document lists that were used for query term expansion. 3.4 Summary of Overall Evaluation Results Table 1 shows the overall evaluation of the submitted runs. In most of the runs, the query term expansion technique improved the retrieval performance on av-erage, but there was no run that improved the query performance for all topics.
Table 2 shows the number of Run IDs where query performance improved by using query term expansion techniques. There is no direct correlation be-tween the effectiveness of the query term expansion technique and the mismatch between the initial query and the relevant documents showed in Figure 3.
It is interesting that there are several topics whose number of Run IDs for a user is lower than that for the automatic (e.g., 21, 58, 76, 82). Common char-acteristics of those topics is that they have higher R &amp; B/B values compared with other topics. It means that good query expansion terms for these topics are related terms to find out terms that are alternative to the initial query terms.
This results show that it is not necessary to use real relevant documents to find out these expansion terms. 4.1 Topic-by-Topic Analysis of the Organizer X  X  Reference System In NTCIR-5 query term expansion subtask, the correlation between topic dif-ficulties based on Figure 3 and effect of query term expansion is not so clear. So topic-by-topic analysis is conducted by using organizer X  X  reference system (ABRIR) run.

Table 3 shows the comparison results b etween query expansion run (300 terms expansion and 10 terms expansion) and no expansion run of the organizer X  X  reference system. In most of the topics, retrieval performance improves with use of query term expansion. It is also difficult to find a direct correlation between the effectiveness of the query term and the mismatch between the initial query and the relevant documents shown in Figure 3.

The correlation between the pseudo-relevant document quality and improve-ment of retrieval performance using query term expansion is investigated.
Table 4 shows the quality of pseudo-relevant documents used in the automatic feedback( X  X  X  is highly relevant,  X  X  X  is relevant,  X  X  X  is partially relevant, and  X  X  X  is non-relevant). From this table, I confirmed that there are topics that improve query performance without real relevant documents (e.g., Topic 21 and 91). On the contrary, there are topics that degrade query performance with real relevant documents (e.g., Topic 76 and 82).

The mismatch between the initial query terms and the terms that are distinc-tive terms in relevant document sets is also checked. In 33 out of 35 topics, initial query terms are included in the top 5 distinctive terms. Two topics (topic 29: (( ( +  X  X orld Heritage X  and  X  X apan X )) do not have initial query terms in this list. Considering retrieval performance of these two topics, topic 29 de-grades retrieval performance with query term expansion but topic 98 improves. I assume this difference comes from the difference of the pseudo-relevant docu-ments; e.g., there are no real relevant documents in a pseudo-relevant document set of topic 28 and there are two real relevant documents (one  X  X  X  and one  X  X  X ) in one of topic 98 (Table 3). From this result, I assume that topics with inappro-priate initial query terms are easily affected by the quality of the pseudo-relevant documents.

Before discussing the effect of the pseudo-relevant documents quality, I would like to discuss the characteristics of the query expansion terms generated from pseudo-relevant documents. Since pseudo-relevant documents contains initial query terms, good related terms to find out document without initial query terms may found in the documents. However, in order to find out context terms that improves the precision of the retrieval performance, it is necessary to use real relevant documents.

Based on these understandings, retrieval performance difference for the top-ics that do not use real relevant documents in a pseudo-relevant document set is checked. There are six topics which use no real relevant documents in a pseudo-relevant document set. In this topic sets, two topics (topic 21 (  X  X al-ue of Writer X  and S Z  X  X UKUDA Kazuya X ) and topic 91 (TOEIC and !  X  X igh score X  and  X  X ethod X )) improve all retrieval performance measure. Since these topics have smaller B &amp; R/R (Figure 3), addition of related terms ex-tracted from non-relevant documents may improve the query performance.
In contrast, there are two topics (topic 76 ( 6 5-5  X  X ittgenstein X  and ( N  X  X hought X ) and topic 82 ( c  X  X ocialist market economy X  and  X  X hina X )) which degrade performance by adding query expansion terms from relevant documents. These topics have smaller B &amp; B/B (Figure 3). In such cases, addition of related terms works well com-pared to the addition of context terms. 4.2 Discussion Since there are varieties of mismatches between the initial query and relevant documents list, most of the query term expansion techniques are effective on av-erage. In addition, there is no topic where all query expansion technique degrade retrieval performance. From this results, I conclude that it is difficult to define simple topic difficulties measure from the viewpoint of query term expansion.
However, topic-by-topic analysis tells that several factors (a type of the mis-match between query and relevant documents, quality of pseudo-relevant docu-ments, etc.) affects the retrieval performance.

Table 5 summarize the type of appropriate expansion terms and candidate strategies for query term expansion according to the type of the mismatch based on this experiment.

This table suggests that the analysis on the effect of simple query term expan-sion technique (e.g., query term expansion by using thesaurus only) in average may be biased the number of each type of topics stored in the test collection.
Therefore, it is necessary to consider the characteristics of the test collection for simple query term expansion technique. It is also important to estimate the type of the mismatch for integrating these different strategies to establish good query term expansion technique. In this paper, I briefly review previous research on feature quantities that charac-terize the quality of the initial query that are defined in this study for evaluating topic difficulties from the viewpoint of query term expansion, and summarize the results of the NTCIR-5 query term expansion subtask. I also describe detailed analysis concerning how these feature quantities affect the query term expan-sion technique. From this analysis, I found that the effect of the simple query term expansion technique may vary according to the characteristics of the test collection and it is necessary to select appropriate test collection for evaluating the simple query term expansion technique.

Further analysis is necessary to establish a framework to evaluate the query term expansion technique in isolation. For example, proposal of feature quan-tities other than R &amp; B/R and R &amp; B/B may be useful. In addition, in order to integrating several query term expansion strategies, it is also important to make a framework to estimate the type of the mismatch. This research was partially supported by a Grant-in-Aid for Scientific Research on Priority, 16016201 from the Ministry of Education, Culture, Sports, Science, and Technology, Japan.

I would also like to thank all participants of the NTCIR-5 query expansion subtask for their fruitful contribution.

