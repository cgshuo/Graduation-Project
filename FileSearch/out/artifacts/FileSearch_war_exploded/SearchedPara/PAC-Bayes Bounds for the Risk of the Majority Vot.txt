 The PAC-Bayes approach, initiated by [1], aims at providing PAC guarantees to  X  X ayesian-like X  learning algorithms. Within this approach, we consider a prior 1 distribution P over a space of classifiers that characterizes our prior belief about good classifiers (before the observation of the data) and a posterior distribution Q (over the same space of classifiers) that takes into account the additional information provided by the training data. A remarkable result, known as the  X  X AC-Bayes theorem X , provides a risk bound for the Q -weigthed majority-vote by bounding the risk of an associated stochastic classifier called the Gibbs classifier . Bounds previously existed which showed that you can de-randomize back to the Majority Vote classifier, but these come at a cost of worse risk. Naively, one would expect that the de-randomized classifier would perform better. Indeed, it is well-known that voting can dramatically improve performance when the  X  X ommunity X  of classifiers tend to compensate the individual errors. The actual PAC-Bayes framework is currently unable to evaluate whether or not this compensation occurs. Consequently, this framework can not currently help in producing highly accurate voted combinations of classifiers.
 In this paper, we present new PAC-Bayes bounds on the risk of the Majority Vote classifier based on the estimation of the mean and variance of the errors of the associated Gibbs classifier. These bounds allow to prove that a sufficient condition to provide an accurate combination is (1) that the error of the Gibbs classifier is less than half and (2) the mean pairwise covariance of the errors of the classifiers appearing in the vote is small. In general, the bound allows to detect when the voted combination provably outperforms its associated Gibbs classifier. We consider binary classification problems where the input space X consists of an arbitrary subset of R n and the output space Y = { X  1 , +1 } . An example z def = ( x , y ) is an input-output pair where x  X  X  and y  X  X  . Throughout the paper, we adopt the PAC setting where each example z is drawn according to a fixed, but unknown, probability distribution D on X  X Y .
 We consider learning algorithms that work in a fixed hypothesis space H of binary classifiers (de-fined without reference to the training data). The risk R ( h ) of any classifier h : X  X  Y is defined as the probability that h misclassifies an example drawn according to D : where I ( a ) = 1 if predicate a is true and 0 otherwise.
 Given a training set S , m will always represent its number of examples. Moreover, if S =  X  z 1 , . . . , z m  X  , the empirical risk R S ( h ) on S , of any classifier h , is defined according to: After observing the training set S , the task of the learner is to choose a posterior distribution Q over H such that the Q -weighted Majority Vote classifier, B Q , will have the smallest possible risk. On any input training example x , the output, B Q ( x ) , of the Majority Vote classifier B Q (also called the Bayes classifier) is given by: where sgn( s ) = +1 if real number s &gt; 0 and sgn( s ) =  X  1 otherwise. The output of the determin-istic Majority Vote classifier B Q is thus closely related to the output of a stochastic classifier called the Gibbs classifier. To classify an input example x , the Gibbs classifier G Q chooses randomly a (deterministic) classifier h according to Q to classify x . The true risk R ( G Q ) and the empirical risk R S ( G Q ) of the Gibbs classifier are thus given by: The PAC-Bayes theorem gives a tight risk bound for the Gibbs classifier G Q that depends on how far is the chosen posterior Q from a prior P that must be chosen before observing the data. The PAC-Bayes theorem was first proposed by [2]. The bound presented here can be found in [3]. Theorem 1 ( PAC-Bayes Theorem ) For any prior distribution P over H , and any  X   X  ]0 , 1] , we have where KL( Q k P ) is the Kullback-Leibler divergence between Q and P : and where kl( q k p ) is the Kullback-Leibler divergence between the Bernoulli distributions with prob-ability of success q and probability of success p : kl( q k p ) def = q ln q p + (1  X  q ) ln 1  X  q 1  X  p . This theorem has recently been generalized by [4] to the sample-compression setting. In this paper, however, we restrict ourselves to the more common case where the set H of classifiers is defined without reference to the training data.
 A bound given for the risk of Gibbs classifiers can straightforwardly be turned into a bound for the risk of Majority Vote classifiers. Indeed, whenever B Q misclassifies x , at least half of the classifiers (under measure Q ), misclassifies x . It follows that the error rate of G Q is at least half of the error some small positive  X  ) has been proposed by [5] for large-margin classifiers. For a suitably chosen prior and posterior, [5] have also shown that R S ( G Q ) is small when the corresponding Majority Vote classifier B Q achieves a large separating margin on the training data. Consequently, the PAC-Bayes theorem yields a tight risk bound for large margin classifiers.
 Even if we can imagine situations where R ( B Q ) &gt; R ( G Q ) , they have been rarely encountered in practice. In fact, situations where R ( B Q ) is much smaller than R ( G Q ) seem to occur much more In this case R ( B Q ) = 0 whereas R ( G Q ) can be as high as 1 / 2  X   X  for some arbitrary small  X  . The situations where R ( B Q ) is much smaller than R ( G Q ) are not captured by the PAC-Bayes theorem. In the next section, we provide a bound on R ( B Q ) that depends on R ( G Q ) and other properties that can be estimated from the training data. This bound can be arbitrary close to 0 even for a large for which their errors are sufficiently  X  X ncorrelated X . All of our relations between R ( B Q ) and R ( G Q ) arise by considering the Q -weight W Q ( x , y ) of classifiers making errors on example ( x , y ) : Clearly, we have: Pr Hence, Pr and where cov err (h 1 , h 2 ) denotes the covariance of the errors of h 1 and h 2 on examples drawn by D . The next theorem is therefore a direct consequence of the one-sided Chebychev (or Cantelli-Chebychev) inequality [6]: Pr ( W Q  X  a + E ( W Q ))  X  Var ( W Q ) Var ( W Theorem 2 For any distribution Q over a class of classifiers, if R ( G Q )  X  1 / 2 then we have We will always use here the first form of C Q . However, note that 1  X  2 W Q = is just the margin of the Q -convex combination realized on ( x , y ) . Hence, the second form of C Q is simply the variance of the margin divided by its second moment! The looser two-sided Chebychev inequality was used in [7] to bound the risk of random forests. However, the one-sided bound C Q is much tighter. For example, the two-sided bound in [7] diverges when R ( G Q )  X  1 / 2 , but C Q  X  1 whenever R ( G Q )  X  1 / 2 . In fact, as explained in [8], the one-sided Chebychev bound is the tightest possible upper bound for any random variable which is based only on its expectation and variance.
 The next result shows that, when the number of voters tends to infinity (and the weight of each voter tends to zero), the variance of W Q will tend to 0 provided that the average of the covariance of the risks of all pairs of distinct voters is  X  0 . In particular, the variance will always tend to 0 if the risk of the voters are pairwise independent.
 Proposition 3 For any countable class H of classifiers and any distribution Q over H , we have The proof is straightforward and is left to the reader. The key observation that comes out of this result is that is uniform on H with |H| = n . Then q = for each pair of distinct classifiers in H , then Var ( W Q )  X  1 / (4 n ) . Hence, in these cases, we have that C Q  X  X  (1 /n ) whenever 1 / 2  X  R ( G Q ) is larger than some positive constant independent of n . Thus, even when R ( G Q ) is large, we see that R ( B Q ) can be arbitrarily close to 0 as we increase the number of classifiers having non-positive pairwise covariance of their risk.
 To further motivate the use of C Q , we have investigated, on several UCI binary classification data been obtained with the Adaboost [9] algorithm used with  X  X ecision stumps X  as weak learners. Each data set was split in two halves: one used for training and the other for testing. In the chart relating clear correlation between R ( B Q ) and R ( G Q ) . We also see no clear correlation between R ( B Q ) and Var ( W Q ) in the second chart. In contrast, the chart of C Q vs R ( B Q ) shows a strong correlation. Indeed, it is almost a linear relation! A uniform estimate of C Q can be obtained if we have uniform upper bounds on R ( G Q ) and on the variance of W Q . While the original PAC-Bayes theorem provides an upper bound on R ( G Q ) that holds uniformly for all posteriors Q , obtaining such bounds for the variance of a random vari-able is still an issue. To achieve this goal, we will have to generalize the PAC-Bayes theorem for expectations over pairs of classifiers since E ( W 2 Q ) is fundamentally such an expectation. Definition 4 For any probability distribution Q over H , we define the expected joint error ( e Q ), the expected joint success ( s Q ), and the expected disagreement ( d Q ) as usual, i.e., c e Q def = E It is easy to see that Thus, we have e Q + s Q + d Q = 1 and 2 e Q + d Q = 2 R ( G Q ) . This implies, Moreover, in that new setting, the denominator of C Q can elegantly be rewritten as The next theorem can be used to bound separately either e Q , s Q or d Q .
 Theorem 5 For any prior distribution P over H , and any  X   X  ]0 , 1] , we have: where  X  Q can be either e Q , s Q or d Q .
 In contrast with Theorem 5, the next theorem will enable us to bound directly Var ( W Q ) , by bound-ing any pair of expectations among e Q , s Q and d Q .
 Theorem 6 For any prior distribution P over H , and any  X   X  ]0 , 1] , we have: Pr where  X  Q and  X  Q can be any two distinct choices among e Q , s Q and d Q , and where is the Kullback-Leibler divergence between the distributions of two trivalent random variables Y q The proof of Theorem 5 can be seen as a special case of Theorem 1. The proof of Theorem 6 essen-tially follows the proof of Theorem 1 given in [4]; except that it is based on a trinomial distribution From the two theorems of the preceding section, one can easily derive several PAC-Bayes bounds of the variance of W Q and therefore, of the majority vote. Since C Q is a quotient. Thus, an upper bound on C Q will degrade rapidly if the bounds on the numerator and the denominator are not tight  X  especially for majority votes obtained by boosting algorithms where both the numerator and the denominator tend to be small. For this reason, we will derive more than one PAC-Bayes bound for the majority vote, and compare their accuracy. First, we need the following notations that are related to Theorems 1, 5 and 6. Given any prior distribution P over H , Since v v + a = 1 1+ a/v , it follows from Theorem 2 that an upper bound of both Var ( W Q ) and R ( G Q ) will give an upper bound on C Q , and hence on R ( B Q ) . Hence, a first bound can be obtained, from Equation 9, by suitably applying Theorem 5 (with  X  Q = e Q ) and Theorem 1.
 PAC-Bound 1 For any prior distribution P over H , and any  X   X  ]0 , 1] , we have Pr Since Bound 1 necessitates two PAC approximations to calculate the variance, it would be better if we could obtain directly an upper bound for Var ( W Q ) . The following result, which is a direct consequence of Theorem 6 and Equation 9, shows how it can be done.
 PAC-Bound 2 For any prior distribution P over H , and any  X   X  ]0 , 1] , we have Pr As illustrated in Figure 2, Bound 2 is generally tighter than Bound 1. This gain is principally due to the fact that the values of e and s , that are used to bound the variance, are tied together inside the kl() and have to tradeoff their values ( e  X  X ries to be X  as large as possible and s as small as possible). Because of this tradeoff, e is generally not an upper bound of e Q , and s not a lower bound of s Q . In the semi-supervised framework, we can achieve better results, because the labels of the examples do not affect the value of d Q (see Definition 4). Hence, in presence of a large amount of unlabelled data, one can use Theorem 5 to obtain very accurate upper and lower bounds of d Q . This combined with an upper bound of e Q , still computed via Theorem 5 but on the labelled data, gives rise to the following semi-supervised upper bound 3 of Var ( W Q ) . The bound on R ( B Q ) then follows from Theorem 2 and Equation 10.
 PAC-Bound 3 ( semi-supervised bound ) For any prior distribution P over H , and any  X   X  ]0 , 1] : Pr Pr We see, on the left part of Figure 2, that Bound 2 on Var ( W Q ) is much tighter than Bound 1. We can also see that, by using unlabeled data 4 to estimate d Q , Bound 3 provides an other significant improvement. These numerical results were obtained by using Adaboost [9] with decision stumps on the Mushroom UCI data set (which contains 8124 examples). This data set was randomly split into two halves: one for training and one for testing. As illustrated by Figure 2, Bound 2 and Bound 3 are (resp. for the supervised and semi-supervised frameworks) very tight upper bounds of the variance. Unfortunately they do not lead to tight upper bounds of R ( B Q ) . Indeed, one can see in Figure 2 that after T = 8 , all the bounds are degrading even if the true value of C Q (on which they are based) continues to decrease. This drawback is due to the fact that, when the value of d Q tends to 1 / 2 , the denominator of C Q tends to 0 . Hence, if d Q is close to 1 / 2 , Var ( W Q ) must be small as well. Thus, any slack in the bound of Var ( W Q ) has a multiplicative effect on each of the three proposed PAC-bounds of R ( B Q ) . Unfortunately, boosting algorithms tend to construct majority votes with expected an disagreement d Q just slightly under 1/2. Based on the next proposition, we will show that this drawback is, in a sense, unavoidable. Proposition 7 ( Inapproachability result ) Let Q be any distribution over a class of classifiers, and let B &lt; 1 be any upper bound of C Q which holds with confidence 1  X   X  . If R ( G Q ) &lt; 1 / 2 then is an upper bound of R ( G Q ) which holds with confidence 1  X   X  . a bound on C Q ), gives a PAC-bound on R ( G Q ) which is just slightly lower (  X  0 . 5% ) than the classical PAC-Bayes bound on R ( G Q ) given by Theorem 1. Since any bound better than Bound 3 for C Q will continue to improve the bound on R ( G Q ) , it seems unlikely that such a better bound exists. Moreover, this drawback should occur for any bound on the majority vote that only considers Gibbs X  risk and the variance of W Q because, as already explained, C Q is the tightest possible bound where d Q is closed to 1 / 2 , one will have to consider higher moments. However, it is not clear that this will lead to a better bound of R ( B Q ) because, even if Theorem 5 generalizes to higher moments, its tightness is then degrading. Indeed, for the k th moment, the factor 2 that multiplies KL( Q k P ) in Theorem 5 grows to k . However, it might be possible to overcome this degradation by using a generalization of Theorem 6 as we have done in this paper to obtain our tightest supervised bound for the variance (Bound 2). Indeed, if we evaluate the tightness of that bound on the variance (w.r.t. its value on the test set), and compare it with the tightness of the bound on R ( G Q ) given by Theorem 1, we find that both accuracies are at about 3% . This is to be contrasted with the tightness of Bound 1 and seems to indicate that we have prevented degradation even if the variance deals with both the first and the second moment of W Q ; whereas the Gibbs X  risk deals only with the first moment. We have derived a risk bound for the weighted majority vote that depends on the mean and variance of the error of its associated Gibbs classifier (Theorem 2). The proposed bound is based on the one-sided Chebychev X  X  inequality, which is the tightest inequality for any real-valued random variables given only the expectation and the variance. As shown on Figures 1, this bound seems to have a strong predictive power on the risk of the majority vote.
 We have also shown that the original PAC-Bayes Theorem, together with new ones, can be used to obtain high confidence estimates of this new risk bound that hold uniformly for all posterior distribu-tions. Moreover, the new PAC-Bayes theorems give rise to the first uniform bounds on the variance of the Gibbs X  X  risk (more precisely, the variance of the associate random variable W Q ). Even if there are arguments showing that bounds of higher moments of W Q should be looser, we have empirically found that one of the proposed bounds (Bound 2) does not show any sign of degradation in compar-ison with the classical PAC-Bayes bound on R ( G Q ) (which is the first moment). Surprisingly, there is an improvement for Bound 3 in the semi-supervised framework. This also opens up the possibility that the generalization of Theorem 2 to higher moment be applicable to real data. Such generaliza-tions might overcome the main drawback of our approach, namely, the fact that the PAC-bounds, based on Theorem 2, degrade when the expected disagreement ( d Q ) is close to 1 / 2 . Acknowledgments: Work supported by NSERC Discovery grants 262067 and 0122405.

