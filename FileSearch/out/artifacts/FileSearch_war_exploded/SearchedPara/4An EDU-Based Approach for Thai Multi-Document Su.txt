 NONGNUCH KETUI, THANARUK THEERAMUNKONG, and Everyday a gigantic number of news articles are produced in various languages all over the world. Such a situation where too much information is available makes it difficult for us to find needed information and to assemble an accurate and complete record of current events from many related news articles derived from several sources with similar and different facts. To solve this problem, it is necessary to study an ef-ficient and effective method to create a summary from multiple news articles. In an early work, Mani [1997] proposed an approach for multi-document summarization by graph search and matching under similarities and dissimilarities among document pairs. Later, Carbonell and Goldstein [1998] and Goldstein and Carbonell [1998] pre-sented a method of combining query relevance with information novelty as topic-driven summarization with a maximal marginal relevance (MMR) measure. For this purpose, a number of works [Mani and Bloedorn 1999; McKeown and Radev 1999] explored the similarities and differences among related documents. McKeown et al. [1999] and Radev et al. [2000] proposed centroid-based techniques to generate a composite sen-tence from each cluster by using a number of syntactic-based features. Following tech-niques in Barzilay et al. [1999] and Cai and Li [2011], summarization is formulated as a clustering problem. Several works [Kuo and Chen 2008; Okazaki et al. 2005] con-sidered a sentence ordering method that helps to extract the important information and then generate a new summary. Generally, much research focuses on extractive summarization [Alguliev et al. 2011; Aliguliyev 2009; Ferreira et al. 2013] and then updates the information for summary generation [Wang and Zhou 2012].

In Thai, there have been very few works on summarization since Thai texts are structurally flexible and complicated, and also because techniques and tools for basic text processing in Thai are still in their infancy stage. As for work on Thai text sum-marization, Jaruskulchai and Kruengkrai [2003] proposed a paragraph-based sum-marization that selects the top-n paragraphs by formulating the importance level of a paragraph with the integration of local and global scores calculated from words in the paragraph. However, some limitations of this approach are that its process-ing unit is set to paragraph, duplication of units are not taken into account, and its primary target is summarizing a single document. As another work on paragraph-based summarization, Thangthai and Jaruskulchai [2004] studied effects of param-eters on paragraph selection for generating a Thai news summary in three genres of news documents, with consideration of Latent Semantic Analysis (LSA) as dimen-sionality reduction. Later, Suwanno et al. [2005] proposed a compound noun extraction method and news structure (headline) consideration to enhance paragraph-based sum-marization with a better score ranking method. Moreover, Ketui and Theeramunkong [2010] presented a more sophisticated weighting system with iterative weight calcu-lation. They also proposed two summarization methods, called inclusion-based and exclusion-based selection, to pick up a set of candidate paragraphs from multiple news documents for a summary. However, summarization based on paragraphs is forced to select a whole paragraph for summary, even though only some parts in a paragraph are important. To avoid this restriction, two recent works have proposed methods to select, instead of paragraphs, significant segments for a Thai single-document summa-rization [Chongsuntornsri and Sornil 2006; Sornil and Gree-ut 2006].

However, using only punctuation marks for splitting a running text into segments and then selecting a subset of segments for summary may not be realistic, since a summary generated from such a subset of segments is usually not readable. As a so-lution, Sukvaree et al. [2007] have presented an EDU-based summarization approach that extracts Elementary Discourse Units (EDUs) [Carlson et al. 2003] from a text, then finds their discourse coherence and organizes them into a spanning tree based on the well-known rhetorical structure theory. In this work, EDUs are extracted by Thai discourse markers. The result showed that Thai discourse coherence can refer to different meanings, so-called word sense ambiguity . For example, some words can be both a conjunction and a marker cue. However, the graph-based approach is able to specify the maximal number of common and different sentences to control the output. Several works [Carbonell and Goldstein 1998; Erkan and Radev 2004; Mihalcea 2004] have applied graph-based approaches to order the document and produce summaries. This method uses these structures to actually compose abstractive summaries, rather than to extract sentences from the text.

In this work, we introduce Thai Elementary Discourse Units (TEDUs) and com-mon phrases (COMPs) and then present a three-stage method of Thai multi-document summarization, that is, unit segmentation, unit graph formulation, and unit selec-tion and summary generation for summarization. To evaluate our methods, we inves-tigate three different granularities of units: (1) TEDU+COMP; (2) combined TEDU; and (3) paragraph. Our proposed methods can be parameterized by three factors: (1) importance-based selection; (2) redundancy avoidance; and (3) postselection weight recalculation. A number of experiments are conducted using 50 sets of Thai news documents; a summary of performance evaluations is also given. Three measures of ROUGE-1, ROUGE-2, and ROUGE-SU4 are used as performance metrics. Then, we apply a Thai multi-document summarization model to develop automatic Thai news summarization.
 The rest of the article is organized as follows. Section 2 defines Thai Elementary Discourse Unit (TEDUs). Our method on Thai multi-document summarization is pre-sented in Section 3. Methods and procedures are shown in Section 4. Experimental results and discussions are described in Section 5. A summarization engine and GUI are given in Section 6. Finally, conclusions and future work are given in Section 7. In the past, there have been a number of works on extracting Elementary Dis-course Units (EDUs) from Thai texts, such as extraction from an agriculture corpus [Charoensuk et al. 2005] and a written family law text [Sinthupoun and Sornil 2010]. Considering the definition of Thai EDUs (TEDUs) in these works and the definition of English EDUs in Carlson et al. [2003], we have defined a set of TEDUs to reflect special characteristics in the Thai language in this section. In our framework adapted from Ketui et al. [2012], a TEDU is defined to represent a single event and usually contains a predicate (i.e., a verb). A Thai text is basically composed of continuously connected TEDUs, and sometimes there is a common phrase (COMP), such as a spa-tial or temporal phrase, a conjunction phrase, and an embedded phrase, between two TEDUs in order to specify relations among them. However, in Thai, some word se-quences look syntactically similar to a TEDU by containing a verb as their component, but they are not TEDUs. Later called a TEDU-like phrase/word (TEDU-LP), some ex-amples of such sequences are clausal subjects/objects or synthetic nominal compounds. On the other hand, a verbal unit may be a simple verb and a verb with some auxiliary units. Moreover, in Thai texts, it is possible to have concatenated verbs, this is called a serial verb, which signifies a relative action in order. In this work, since we princi-pally set one verb as one TEDU, serial verbs will be broken down into several units as a basic procedure. To cope with units in a Thai running text, we have defined six types of TEDUs (TEDU-1 to TEDU-6), four types of COMPs (TEMPP, SPATP, CONJP, and EMBP), and two types of TEDU-LPs (TEDU-LP-1 and TEDU-LP-2) as follows. In order to detect units of such three fundamental types, namely, TEDUs, COMPs, and TEFU-LPs, a set of patterns and clues are defined for each type, as shown in Figure 1. (1) TEDU-1 (Simple Clauses) refers to a simple clause composed of a subject ( S ), (2) TEDU-2 (Subject Zero-Anaphora Clauses) refers to a clause with its subject omit-(3) TEDU-3 (Clauses with Attribution Verb) refers to a clause with an attribution (4) TEDU-4 (Comparative Clauses) refers to a clause with a special verb ( V (5) TEDU-5 (Question Clauses) refers to an interrogative sentence, that is a clause (6) TEDU-6 (Embedded Conjunction Clauses) refers to an embedded conjunction (7) TEMPP (Temporal Phrases) refers to an expression of date, time, or duration. It (8) SPATP (Spatial Phrases) refers to the location/place ( L ) in the clause. Sometimes, (9) CONJP (Conjunction Phrases) refers to a single conjunction ( C ), a conjunction (10) EMBP (Embedded Phrases) refers to a relative pronoun E (11) TEDU-LP-1 (Clausal Subjects/Objects) refers to a unit that has a structure of a (12) TEDU-LP-2 (Synthetic Nominal Compounds) refers to a unit that has a structure This section presents our Thai multi-document summarization model that is composed of three processes: unit segmentation, unit graph formulation, and unit selection and summary generation. Figure 2 displays the framework, composed of three main pro-cesses and their subprocesses. In the unit segmentation (process 1 in Figure 2), a Thai running text is segmented into a sequence of tractable units and tagged with part-of-speech (POS) and named entities (NEs) (subprocess 1.1 in Figure 2). Three alternative forms, called TEDUs, combined TEDUs, and paragraphs, are proposed for segmenting a Thai running text into units (subprocess 1.2 in Figure 2). In the unit graph formula-tion (process 2 in Figure 2), a graph of units is constructed by conceptualizing a unit as a weighted node in a graph (subprocess 2.1 in Figure 2) and a relationship of two nodes is formulated as a weighted link between the nodes (subprocess 2.2 in Figure 2). The weight of a node or link is determined by considering its importance, that is, its contribution in the graph. In the unit selection (process 3 in Figure 2), a number of important nodes and links are selected by considering the importance level of nodes or links, together with redundancy among units (nodes) (subprocess 3.1 in Figure 2), and focusing on the node weight recalculation (subprocess 3.2 in Figure 2). In the final step (subprocess 3.3 in Figure 2), a summary can be generated by ordering selected units under a set of predefined criteria. In this work, for sake of simplicity, the summary is generated by displaying selected units under two general conditions: (1) temporal order among documents and (2) occurrence order in each document. The details are discussed in the next sections. Since Thai language has no sentence boundary (or even word boundary), it is difficult to define a unit for summarization. However, paragraphs as units for Thai document summarization are too large, since a paragraph may contain heterogeneous contents and because such a paragraph-based approach does not allow to exclude some unim-portant parts in a paragraph. With this issue in mind, TEDUs detection is useful, but requires some forms of word segmentation and POS/NE tagging. Towards this issue of TEDUs detection, it is necessary to perform word segmentation and POS/NE tag-ging. Besides TEDUs, an alternative, namely a combined TEDU, is proposed under the hypothesis that two TEDUs can be merged to form a larger unit if there exist some clues of connection between them. In the rest of this section, word segmentation and POS/NE tagging (process 1.1 in Figure 2) as well as unit segmentation (process 1.2 in Figure 2) are described.  X  Word Segmentation and POS/NE Tagging. In the past, a number of tech- X  Unit Segmentation. For this step, we apply a set of simple metarules, called left-After a running text is split into tractable units, namely, TEDU+COMP, CTEDU, or paragraph, we need a model to determine the importance of units and to select the most suitable ones for a summary. Towards this end, a graph model is proposed to express units and their relations extracted from multiple targeted documents for sum-marization. In our approach, a node in a graph corresponds to a unit while a link in a graph expresses relation connections between two units (processes 2.1 and 2.2 in Figure 2).  X  Node Weight Calculation. As a well-known method, it is possible to apply TF/IDF  X  Link Weight Calculation. Besides node weights, it is worth investigating the re-Given the unit graph derived from the process described in the previous section, unit selection and summary generation are tasks to select a set of suitable units for con-structing a summary (process 3 in Figure 2) and to generate a summary using the selected units. In the past, several works [Goldstein and Carbonell 1998] applied a straightforward method to select units based on their weights. In this work, we uti-lize a variant of the inclusion-based summarization method proposed in Ketui and Theeramunkong [2010]. Our unit selection starts from iteratively selecting potential units based on priority (weight), but trying not to include a unit if it is duplicated with some selected units in the summary S . Finally, the weights of the rest of the nodes u in the graph of units in documents G are recalculated. As shown in Algorithm 1, the number of units to be selected is set. The most important nodes are repeatedly added one-by-one into the summary S and then these nodes are deleted from a set of unselected units U until the number of selected units reaches a predefined compres-sion rate. During the node addition, the weight of each unselected unit is recalculated. When the number of selected units satisfies the predefined compression rate, the algo-rithm will return the graph of summary S . Three basic concepts of our inclusion-based summarization approach (subprocesses 3.1 and 3.2 in Figure 2) are discussed next.  X  Importance-based selection. A unit with a higher weight (importance) has a higher  X  Redundancy avoidance. Two units with identical or highly similar content should  X  Postselection weight recalculation. After a unit is selected to include in a summary,
According to the first concept on weighting units, the unit selection can be formu-lated as follows (here, the original weighting of a unit W centroid-related and a redundancy-related factor, as shown in Eq. (2)). The best unit (  X  u ) can be selected by maximizing the value in the equation, where the three terms in-dicate original weight, centroid-related, and redundancy-related factors, respectively. Let u indicate the current unit. The unselected unit is represented by u the whole unselected set | U | , where i is the i -th unselected unit while s selected unit in a summmary, where j is the j -th selected unit. When more than one unit has the same highest score, the most preceding one in the latest document will be selected.
For the first term, TF/IDF can be used to express such importance levels of words by using term (word) frequency and inverse document frequency. The second term ex-presses the average similarity between the current unit and other units; a high value indicates the closeness to the centroid of the unselected units. The third term rep-resents the level of content redundancy. As an extreme case, if the unit has similar content to that in the set of selected units, the maximum is 1 and the term will become 0. As the third concept, after a unit is selected to include in a summary, its selection affects the possibility that other units will be selected. As shown in Eq. (3), recalcula-tion of node weights ( W re t ( u ) ) is done by decreasing the weight of an unselected node W re t ( u ) by the factor of the ratio of similarity between this node ( u ) and the selected node s j in set S and other nodes ( u i ).
 Then the new node weight W t + 1 ( u ) is normalized by W (select, add, delete, and recalculate) in the algorithm are iterated to select the next node until the compression rate reaches a predefined value. After the unit selection, a summary can be generated by ordering selected units under a set of predefined criteria. Later the summary can be revised manually for readability improvement. However, in this work, for sake of simplicity, we simply generate a summary by ordering the selected units in the temporal relations among documents and the ordinal relations in the original documents.
 For clarity, Figures 3 to 5 are provided to illustrate a trace of three processes with two news text samples. Figure 3 shows two original news texts ( 02 &gt; ) with their glosses in the upper part and the corresponding word-segmented and POS/NE-tagged texts in the lower part. In Figure 3, segmentation is expressed by a vertical bar while the POS or NE type of each segmented word is given inside parenthe-ses. English word-by-word glosses are provided in square brackets. Figure 4 displays the process of forming a sequence of one or more segmented words into units (either TEDUs or COMPs). Note that, since each TEDU-LP becomes a part of an EDU, it is not included in the result. In the lower part of Figure 4, there are eight TEDUs for 01 &gt; and 11 TEDUs with one COMP for &lt; news-02 &gt; . Figure 5 illustrates the process of unit graph formulation, unit selection, and summary generation. In the middle part of Figure 5, three sample nodes (i.e., units) (NW-0101, NW-0102, and NW-0212) and two linking edges (EW-0101-0102 and EW-0101-0212) are shown with their weights (i.e., 1.459, 0.622, and 1.219 for the nodes and 0.101 and 0.688 for the edges). Finally in the lower part, a summary (at the compression rate of 0.4) is generated by selecting a set of nodes (TEDUs or COMPs) with a criterion (such as higher weights) and list-ing them under two general conditions, that is, the temporal order among documents and the occurrence order in each document. Here, two different dashed boxes show the resulting TEDUs or COMPs of &lt; news-01 &gt; and &lt; news-02 This section describes the dataset, evaluation method, and experimental settings. As for the evaluation criteria, a number of ROUGE-based measures are used to define the differences between manual summary and system summary. To evaluate the perfor-mance of our methods, four experiments are conducted.
 This work utilizes the THAI-NEST corpus developed in Theeramunkong et al. [2010] comprised of 10,000 news articles in seven categories: crimes, sports, foreign affairs, politics, entertainment, economics, and education. These are from Thai news agen-cies such as Daily News 1 , Thairath 2 ,and Matichon 3 . Later, a method for discovering document relations in Kittiphattanabawon et al. [2010] is applied to find relations among news documents and to group highly related news documents into a dataset for summarization. In this work, we randomly select 50 sets of related documents with completely related (CR) and somehow related (SH) relations for testing our proposed graph-based summarization approach. Each set of related documents are tagged with NEs and POS by Thai E-Class [Tongtep and Theeramunkong 2013].
 Later our Thai running text with POSs and NEs tagging are segmented into TEDUs, COMPs, and TEDU-LPs by using 446 context-free grammar rules (CFG rules) with chart parsing [Ketui et al. 2012; 2013]. We applied three groups of CFG rules, namely, 342 rules for TEDUs, 95 rules for COMPs, and 9 rules for TEDU-LPs. Conceptually, the TEDUs and COMPs can be detected after recognizing TEDU-LPs while a CTEDU can be constructed by merging two related TEDUs using COMPs. Utilizing results in Ketui et al. [2013], the longest matching technique was found to outperform the maximum matching technique. The performance of the longest matching is displayed as a graph in Figure 6. Here, a pattern column shows the precision value, a gray column illus-trates the recall value, and a black one displays the F-score value. The last type of units, paragraph, can be simply detected by line breaks and indents. If the text is in HTML format, common markers are &lt; p &gt; or &lt; br &gt; mary (i.e., model summary) for evaluation, a number of Thai language experts at the Faculty of Liberal Arts, Thammasat University manually constructed an abstractive-based summary with a size of 50 X 100 words for each of the 50 datasets as the gold standard for evaluating system results. The summaries contain main contents in the original documents, including What, Where, Who, When, and How (4W1H). Some dis-course markers are added to connect clauses. These reference summaries will be used for evaluating a summary obtained from a system.

Table I displays characteristics of the 50 experimental datasets grouped by number of documents per set, including document size, number of words, number of TEDUs, number of COMPs, number of CTEDUs, number of paragraphs, and number of docu-ments as well as the size of reference summary and number of words in the reference summary. The details show summation, average, maximum, and minimum by dataset. Our 50 datasets used for experiments contain 144 documents and 23,781 words. The size of all datasets is approximately 483.1KB and the average size per set is 9.7KB. The maximum size equals 31.0KB while the minimum is 4.0KB. The number of TEDUs is greater than twice that of CTEDUs, implying CTEDUs are constructed from two TEDUs. At least two paragraphs appear in each set while the average number of para-graphs in each set is 7 units. This tendency is also true for the numbers of words, TEDUs, COMPs, CTEDUs, and paragraphs. The last two columns in Table I show characteristics of the reference summary of all datasets. The average size of a dataset of the reference summary is 1.3KB. The maximum size equals 2.8KB, while the min-imum is 0.7KB. The number of words of the reference summary is less than 7 original dataset. This trend is also true for the number of words in the reference sum-maries. Table II illustrates the detailed results of unit segmentation of the 50 datasets, including number of words and units grouped by types of TEDUs, COMPs, and TEDU-LPs. Considering all units, there are 23,781 words and 9,887 TEDUs+COMPs in total. In detail, the table presents the summation, average, maximum, and mini-mum of each type. The most frequent units are those of TEDU-2 (i.e., 3,886 units). On the other hand, TEDU-4 is the least frequent type. There are only five TEDU-4 units for the 50 datasets but each unit is quite long (i.e., on average 25 Although we have more TEDU-3 units than TEDU-1 units, on average, a TEDU-3 unit (2,731  X  1,453 = 1.9 words/unit) is shorter than a TEDU-1 unit (3,846 3.1 words/unit). Moreover, TEDU-4, TEDU-5, and TEDU-6 units appear in only some datasets. For COMPs, the common phrases of conjunctions (CONJP) occur the most frequently in terms of units. They usually have one word per unit and are used to connect two TEDUs. The temporal and spatial phrases (TEMPP and SPATP) include, on average, two words (1,033  X  562 = 1.8 and 1,304  X  716 = 1.8) while the embedded phrases (EMBP) consist of only one word. Moreover, 572 TEDU-LP-1 units and 345 TEDU-LP-2 units are embedded in TEDUs or COMPs. To examine performance of the proposed methods, we have conducted four experi-ments using 50 sets of related news documents containing 23,781 words. The first experiment aims to investigate summarization performance according to three unit types, three summarization factors, and five compression rates. Here, the three types of units are: (1) TEDU+COMP; (2) CTEDU; and (3) PARA (as stated in Section 3.1). The three summarization factors we considered are: (1) importance-based selection; (2) redundancy avoidance; and (3) postselection weight recalculation. For importance-based selection, we consider the simple highest-weight priority (H) and an extension of the highest-weight priority with centroid preference (C). As for redundancy avoid-ance, we compare the consideration of redundancy penalty (P) to the nonconsideration version (D). For postselection weight recalculation, we also compare the recalculation case (R) with its nonrecalculation one (N). For the compression rate, we examine five rates of 0.1, 0.2, 0.3, 0.4, and 0.5 since a summary should not be larger than half of the original documents. Here, the compression rate is defined as the ratio of number of units in a summary to that of units in the original documents. Moreover, as an opti-mal case, we calculate upper bound performance (UPB) of summarization by starting from selecting the unit that obtains the highest ROUGE, adding it into the summary, and then selecting the next unit that achieves the best performance when added into the summary. This greedy-based selection is performed repeatedly until reaching the target compression rate. The average of results of all eight methods is also provided for reference. Moreover, we compare our summarization methods with a traditional graph-based ranking model called TexRank [Mihalcea 2004]. The sentence scoring function is known as the PageRank algorithm [Page et al. 1998]. The second experiment compares performance of the combinations of the three factors. For each factor, two alternatives are investigated by varying other factors and then comparing their results. Moreover, the performance of the combinations is summarized and ranked to clarify which factor combination is optimal. The third experiment performs a two-tailed t-test with sig-nificance value 0.05 to check win/loss/tie (W/L/T) among all eight methods in order to make a detailed comparison. Lastly, the fourth experiment investigates the quality of generated summaries by humans. For this task, six Thai linguists from Faculty of Liberal Arts of Thammasat University are requested to read the generated texts of 50 datasets and evaluate their readability. Since this task is labor intensive, we have evaluated only two systems that obtain the best ROUGH performance.

To evaluate a summary output from a system, we use the reference summaries as described in Section 4.1. For each set of related news articles, the reference summary is constructed by requesting Thai linguists to read the articles and then manually make their abstractive summary. In this work, we utilize ROUGE [Lin 2004] to eval-uate a system X  X  summarization result by comparing it with its reference summary. Among various types of ROUGE, this work uses R-1 (unigram-based co-occurrence statistics), R-2 (bigram-based co-occurrence statistics), and R-SU4 (skip-bigram plus unigram-based co-occurrence statistics) that are commonly used for evaluation. Origi-nally developed by NIST, the ROUGE is a variant of ROUGEs that consider precision (R-1 P ,R-2 P ,R-SU4 P ), recall (R-1 R ,R-2 R ,R-SU4 R ) and F-score (R-1 Here, we utilize the result of the five compression rates (0.1 X 0.5) for performance com-parison under consideration of the three ROUGE values (R-1, R-2, R-SU4) across the eight methods. This section reports four experimental results: (1) investigation of summarization performance on three unit types, three summarization factors, and five compression rates; (2) performance comparison of the eight combinations of the three factors with pairwise comparison and overall ranking; (3) win/loss/tie (W/L/T) checking among the eight methods by a two-tailed t-test with significance value 0.05; and (4) quality assessment by humans. To obtain insight into the results, error analysis is made and reported at the end of this section. The results in three F-score ROUGEs (R-1 F ,R-2 F , and R-SU4 Table III. This section provides observations on the results based on unit types. As for TEDU+COMP, CPR outperforms other methods with R-1 at 29.00 at compression rate 0.5, while CPN is superior with the best R-2 of 11.99 and R-SU4 of 12.61 at com-pression rate 0.2. Compared to UPB (the optimal case), around 20% X 30% performance is achieved. The PageRank (henceforth PRK) is used as our baseline. At compression rate 0.5, PRK achieves the highest R-1 of 28.54 while obtaining the highest R-2 and R-SU4 of 9.89 and 10.21. Its performance is lower than our method. As for CTEDU (the second unit type), HPR gets the best R-1, R-2, and R-SU4 (29.07, 9.77, and 11.24) at compression rate 0.4. The performance of the eight methods tends to be high at compression rate 0.4 while the highest values of UPB are at compression rate 0.2 and then drop at compression rate 0.3. The average of R-1 and R-SU4 reaches high-est levels at compression rate 0.5 (28.07 and 10.89) and the second highest values at compression 0.4 (27.81 and 10.84). The average of R-1, R-2, and R-SU4 over all meth-ods is higher than the baseline method (PRK) at the whole range of compression rates. Considering a larger unit called paragraph (PARA), R-1, R-2, and R-SU4 are low at compression rate 0.1 because only in some datasets could we select the unit to gener-ate a summary. It is possible that the multiplication of the total number of paragraphs and the compression rate is less than 1. Moreover, R-1 values of the eight methods are slightly higher at around 11% from compression rate 0.1 to 0.2 and then approx-imately increase to 7% and 3% at compression rate 0.3 and 0.4, respectively. HPN gets the highest performance of R-1, R-2, and R-SU4 at compression rate 0.5 while achieving the second highest at compression rate 0.4. On average, R-1, R-2, and R-SU4 become the highest at compression rate 0.5 for PARA, while PARA with PRK gets low performance of R-2 and R-SU4. To compare the performance of three units using eight different methods at compression rates between 0.1 and 0.5, the average of R-1 values of CTEDU (24.97) is greater than TEDU+COMP (23.86) and PARA (19.93), that is, CTEDU &gt; TEDU+COMP &gt; PARA. Unlike R-1, the ranking of average of R-2 values is TEDU+COMP &gt; CTEDU &gt; PARA (9.59, 8.51, and 6.06). R-SU4 follows the same pattern as R-2 (10.03, 9.61, and 8.18).

To conclude, we found that both the highest and second highest ROUGE values are at compression rates between 0.4 and 0.5. The performance of TEDU+COMP is generally greater than CTEDU and paragraph. It suggests that the length of TEDU+COMP is suitable for summarization since TEDU+COMP might be short and contains important information (keywords) that overlaps with the reference summary. CTEDU comes from TEDU and COMP and also obtains a high performance. Sum-marization using paragraph units may not be effective, since a paragraph may include heterogeneous content, where some should be in summary and some not. Our proposed methods can achieve 30% X 50% of UPB (optimal case). The performance of our methods is relatively higher than the baseline graph-based approach (PRK). However, the re-sults in Table III show that the performance of the proposed methods obtains a similar value. To distinguish them, we calculate the average F-score of three ROUGE values (R-1 F ,R-2 F , and R-SU4 F ) and rerank these scores as shown in Table IV. In this table, we see that HPR has the best performance for R-1 but is lower than HDR and HPN for R-2. Both HPR and HDR achieve the highest F-score for R-SU4. Moreover, there is no difference on the average F-score of three ROUGE values for HPR, HDR, and HPN but most methods outperform PRK. CDN achieves the lowest value. From this result, we cannot determine which method certainly wins out. Therefore, we perform another experiment in Section 5.3 and explore the effect of three factors on summarization performance in Section 5.2. This section explores the effect of three factors on the methods. For each factor, two al-ternatives are investigated by varying other factors and then comparing their results. Table V demonstrates the performance comparison with a two-tailed t-test with 5% significance among three factors. Each comparison contains 150 cases (three units 50 datasets) and applies a t-test score. The average of ROUGE-1 ROUGE-SU4 F is used to find the number of wins, losses, and ties (W/L/Ts).
Table V shows that the simple highest-weight priority (H**) tends to outperform the highest-weight priority with centroid preference (C**). One possible reason is that the centroid preference (that selects the common unit from unselected units) may suggest a unit that overlaps with the currently selected units for the summary. When compar-ing the methods with (*P*) and without redundancy avoidance (*D*), we found that *P* outperforms *D* with two wins and two ties. For example, CPR beats CDR by 0.27 (13.41 versus 13.14) with 5% significance. This indicates the effectiveness of re-dundancy removal in multi-document summarization. The redundancy can be found when we form a unit graph in our second step, showing the effectiveness of the graph formulation. As for the last factor, the versions with postselection weight recalculation (**R) outperform the non-recalculation version (**N) with two wins and two ties. The weight recalculation will adjust the weight of a node to a more suitable value.
In conclusion, three factors, namely, the simple highest-weight priority, the redun-dancy avoidance, and the postselection weight recalculation, have positive effect on the performance of the summarization method. With a suitable combination, the per-formance can be improved. In more detail, the performance of our proposed method is investigated by varying the three factors and comparing them with a two-tailed t-test with 5% significance, as discussed in the next section. In this experiment, we rank the average ROUGE F values of our eight methods and perform a two-tailed paired t-test at 5% significance level. Table VI shows that each method is tested with 105 pairs (5 compression rates  X  3 ROUGE values Following a normal schema, we set the scores of a win, loss, and tie to 3, 0, and 1, respectively. There are 150 cases from 50 datasets and three types of units. The row and column headers show the method names, and the cell values represent the number of wins, losses, and ties. The value in each cell shows the number of compression rates where the method in the row is superior (win), inferior (loss), and comparable (tie) on the column.

For each evaluation metric, we noticed that HPR performs better than the other methods, except for HPN, in terms of R-1 and R-SU4. This method only loses to HPN in terms of R-2. HDR has 23 wins, comparable to CPR, but its number of losses is lower than CPR. Specifically, for HPN there are 19 wins, 3 losses, and 83 ties. CDN never won, indicating that the highest-weight priority with centroid prefer-ence does not perform well. Furthermore, the performance of CPR increases, since the effect of the highest-weight priority with centroid preference, the redundancy avoidance, and the postselection weight recalculation can help improve the perfor-mance. Although the previous result showed this method is at the fifth rank (see Table V), we found that the average ROUGE values at the initial compression rate are very low and the values slightly increase at compression rates 0.4 X 0.5. In ad-dition to CPR, even if they have the centroid-related consideration and either the redundancy avoidance or the postselection weight recalculation, this is not enough to get high accuracy. Since the centroid of the unselected units is considered the common information as the important unit, redundancy avoidance is necessary. In-cluding the postselection weight recalculation may reduce the weight of unselected unit, similar to the previously selected units. Besides this, we found that postse-lection weight recalculation is a part of top-3 methods, therefore this factor is very important for summarization tasks. The ranking of the eight methods is HPR &gt; CPR &gt; HPN &gt; HDN &gt; CDR &gt; CPN &gt; CDN. Note that the ranking of the methods in Tables IV and VI are not the same. We conclude that HPR has the best performance at compression rates between 0.1 and 0.5 and obtains the best performance for CTEDU, while CPN and CPR work well with TEDU+COMP. In this experiment, we evaluate the text summarization quality. The evaluation is con-ducted by asking six Thai linguists (H 1  X  X  6 ) to rate news summaries in four aspects. We select HPR and CPR for quality assessment by humans since they are the best methods for CTEDU and TEDU+COMP, respectively. Moreover, by this means, we can evaluate simultaneously the effect of unit types on summary quality. Table VII displays the average scores of text quality measures of CTEDU with HPR and TEDU+COMP with CPR summarization methods. Four aspects of text quality evaluated include the following. (1) Grammaticality. Text summarization should not contain the non-textual items, (2) Nonredundancy. Text summarization should not contain redundancy content. (3) Reference Clarity. Text summarization should contain the nouns and pronouns (4) Readability and Coherence. Text summarization should have a good summary or
The Thai linguists gave rating scores of 1 X 5 (1 = fail; 2 = poor; 3 = average; 4 = good; 5 = very good) to each summary from 50 datasets of Thai news articles. The results show that three aspects of text quality (grammaticality 3.54, nonredundancy 3.69, readability and coherence 3.43) of CTEDU with HPR get average ratings, and good rating (3.75) for referential clarity. On the other hand, for TEDU+COMP with CPR, three aspects of text quality (2.04, 2.23, and 2.20, respectively) are quite poor while referential clarity is close to average (2.65). For CTEDU with HPR, the average of four aspects of text quality (3.64) is greater than TEDU+COMP with CPR (2.28). Fur-thermore, one additional observation is that CTEDU seems to gain better summary quality than TEDU+COMP since the larger units, that is, CTEDUs obtaining conjunc-tive cohesion, are more understandable than TEDU+COMP units. According to the linguists X  suggestions, the unit ordering should be improved because the disordered summary may make readers misunderstand the content. In this work, the summary is easily generated by listing the selected units in the temporal relations among doc-uments and the ordinal relations in the original documents. However, a summary is likely to obtain better performance if semantic components are considered when it is automatically generated. This is left as our future work. In all, we ranked the 50 datasets based on their performance (average F-score over all ROUGEs and compression rates). Some observations can be drawn. First, datasets with a very low ratio of document size between the reference summary and the orig-inal seem to obtain low performance in summarization. This implies the difficulty in selecting suitable phrases/words for a summary due to the large number of candidates. Second, datasets with heterogeneous contents or somehow related (SH) relations tend to get low performance. For instance, while one of the datasets consists of 15 docu-ments, only five documents are strongly related and the others trivially so. The task of summarizing heterogeneous content documents is difficult. Third, datasets that oc-cupy documents with a small number of paragraphs tend to get low performance for paragraph-based units (PARA). When unit selection is done based on the paragraph, it is highly possible to include words/phrases that are not suitable to be included in the summary since the paragraph-based method will select the whole paragraph as one unit. Lastly, datasets with improper TEDU+COMP or CTEDU segmentation may trigger lower performance in ROUGE-2 and ROUGE-SU4. Selection on missegmented units is not efficient, resulting in low performance. Up to now, most applications for multi-document summarization have usually been related to the domain of news including both public and commercial sites. For example, Google News 4 and Columbia NewsBlaster 5 are public and commercial sites while News In Essence 6 is a commercial one. In this work, we have developed a Thai automatic summarization application that is part of the AllNews system cept of unit segmentation, unit graph formulation, and unit selection. For our imple-mentation, the following software and hardware requirements are used. The system was developed using C++ language and shell script. GCC (GNU Compiler Collection) version 4.6.3, which is an integrated distribution of compilers for several major pro-gramming languages, is used. We edited all programs with Eclipse version Juno. We installed Java and Perl language programming for supporting unit segmentation and evaluation methods. The Web interface of the system was constructed by PHP (Per-sonal Home Page) programming. A MySQL database was used for storing original texts and the summarization results. The system ran on Ubuntu 10.04, Intel Xeon 3.3 MHz, 8GB RAM, and 1TB HDD.

In the system of Thai automatic summarization, the original text can be gathered from online news articles. Figures 7 to 10 show user interface (UI) snapshots when the system generates a summary from related news articles. Figures 11 and 12 display the manual input. Details and descriptions for Figures 7 through 12 are as follows. (1) Figure 7 displays of Thai news articles from eight categories (crimes, sports, for-(2) Figure 8 shows current popular news titles (B-2) grouped into the eight categories, (3) Figure 9 Illustrates the news title (C-1), original news text (C-2), publisher and (4) Figure 10 demonstrates the title and summarized date (D-1), summarized re-(5) Figure 11 shows instructions for an input of original news text (E-1). The format-(6) Figure 12 depicts of summarization results with the manual input (F-1). The This article provided a definition of the Thai Elementary Discourse Unit (TEDU) and then presented our three-stage method of Thai multi-document summarization, that is, unit segmentation, unit graph formulation, and unit selection and summary generation. In the unit segmentation process, we investigated three different units: TEDU+COMP, CTEDU, and paragraph. For TEDU+COMP, a Thai running text is seg-mented into TEDUs, COMPs, and TEDU-LPs. The unit graph formulation represents units as weighted nodes in a graph and their relationships as weighted links among nodes, according to their importance and contribution in the graph. The unit selection is performed to include important nodes and links by considering redundancy among units (nodes) and content difference among units. After the unit selection, a summary is generated by ordering the selected units in the temporal relations among documents and the ordinal relations in the original documents. Three factors that have been con-sidered include importance-based selection, redundancy avoidance, and postselection weight recalculation. The 50 sets of Thai news articles are used for evaluation. The re-sults show that TEDU+COMP yields the best performance in terms of R-2 and R-SU4 while CTEDU is superior in terms of R-1. When the average ROUGE F-score is used for ranking, HPR and HDR have superior performance. In other words, it is highly effec-tive to select units based on their weights with consideration for redundancy avoidance and weight recalculation.

As for future work, we will analyze the relations among TEDUs in order to form combined TEDUs with considerations of semantics. Moreover, we plan to improve ini-tialized node weights and investigate more semantic-based unit selection, including consideration of discourse structure via conjunctions or discourse markers, as well as TEDUs and COMPs. Moreover, it is worth exploring a query-based approach where a query is provided to find related documents before summarization. An investiga-tion on a larger dataset will help us evaluate our method in a more practical way. Finally, we plan to make some improvements on our news summarization application, such as an easy-to-use GUI, a mobile and real-time version, and an XML-compatible implementation.

