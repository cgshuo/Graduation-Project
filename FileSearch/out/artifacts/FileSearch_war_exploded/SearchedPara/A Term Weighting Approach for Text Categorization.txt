 predefined categories [1,2]. One of the most commonly investigated applications of categorization have been focused on the method of computation by classifier models and the extraction of learning features [3,4]. For classifier models, there are statistical approaches, machine learning approaches, and information retrieval techniques. As for the representation of categories and input documents, one of the basic assumptions in probabilistic model is that a set of terms frequently occurring in a document would category. 
Probabilistic learning methods are the most dominant techniques to build a classi-fier. Basically, text categorization system estimates the generation probability of input document by the classifiers of pre-defined categories. The generation probability is calculated by the terms that are extracted from input document. Through the machine learning techniques like NB, SVM, kNN, and LLSF, the pre-classified documents are proaches have so far been using a document representation in a word-based document vector together with frequency-based statistics [5,6,7]. 
A large number of classification models have been explored, together with feature selection and reduction [8,9,10]. Despite the numerous attempts to explore more so-phisticated document representation techniques, the simple  X  X ag-of-words X  assump-tion remained very popular. This assumption has been used to distinguish the catego-ries together with the number of words that occur in training documents. As an effort ference factors have been explored. These approaches are to capture the discriminat-ing capability of the classifiers [11,12]. 
One of the important issues of text categorization is to extract good terms and to est imate the term probabilities on the document. The first step to text categorization is to extract the classifiers from the documents, which typically are a sequence of words or terms. And then, classifiers are trained by categorization model. In statistical models, t erm frequency and inverse document frequency are used to calculate the weight value s of classifiers, and features are automatically trained by machine learning methods. F eature learning system is based on the terms and term frequencies. Because of the co mmon terms of high frequency, both tf and idf measure is combined as a weighting sc heme of the terms. 
Though frequency-based term weighting is a common technique, it is not sufficient for the representation of the document. Also, in the text categorization problem, the ef forts to improve the performance of the categorization algorithm have a limitation on t he frequency-based weighting scheme. The standard frequency-based estimation of te rm probabilities may not the best solution of document representation and it would be better if more sophisticated document representation techniques are introduced. We fe el that making unwarranted parametric assumptions on standard frequency-based term weighting will not lead to better performance than the knowledge engineering approa ch. Furthermore, making prior assumptions about the similarity of documents is not w arranted either. In this paper, we give a further evidence to the usefulness of a more so phisticated text representation method, which is based on the content-based term weig hting with a smoothing technique, that we improved the accuracy of the text categoriz ation system. Na X ve Bayesian(NB) classifier is to use the joint probabilities between terms and categories to determine categories for input documents. Na X ve Bayesian model as-other words. We complement the weakness of the word independence assumption by combining term weighting method to estimate the probabilities of terms. 2.1 Term Weighting Algorithm In general, the construction of a document vector depends on both term frequency and inverted document frequency. We introduced a content-based term weighting method for the Korean language [13]. It is an analytic approach that analyses the contents of a document to extract a keyword list from the document and term weights are evaluated by considering thematic factors as a keyword. Term weighting method is based on the relative importance of terms in a document. As for assigning relevance values to terms, terms are identified by term-type, term-length, thematic role in a sentence, location, and frequency count. The weight value of a term is calculated by combining all these term-weighting factors, and terms are ordered by the scores. phrase or a clause, sentence location, and sentence type. From the rhetoric word in a sen-tence, the importance of a sentence is computed and weight is added to the corresponding which terms are extracted. Initial weight by term type is 0.0~1.0 that is given by intuition: compound noun 0.9, common noun 0.5, and verbalized noun 0.2. 
Algorithm TermWeighting(text) begin for (each sentence in text) { end 
For each term, base score is assigned by the term-type, and then weighting factor is analysis of a document. Content analysis is performed through the morphological analyser and base-noun chunking. Base noun chunking is needed for identifying multi-word compound nouns and assigns the same scores for component nouns. Rela-scoring process, term score is converted to a weight value of 0 to 1. 
Term weighting is a more sophisticated method of assigning weight values to terms. Terms and their weight values represent a document and document vector for input document or classifiers for categorization are extracted in the same manner. The term ranking method considers term and document features such as term locations morpho-syntactic analyser. So, there is a possibility to improve the term weighting through deep analysis of a document and the document type features like title-abstract-conclusion in a research paper. 2.2 Probability Estimation of Terms The probability estimation of terms is the core part of categorization system. We have been explored a term weighting schemes that is a combination of weighting metrics with smoothing and inverse category frequency. First three methods of TF-1, TF-2, and TF-3, are the variants of frequency-based techniques. The only differences of them are the smoothing technique (TF-2) and inverse category frequency (TF-3). TF-1 is a standard Na X ve Bayesian model with a smoothing technique by collection fre-quency. In TF-2, smoothing value was fixed to a constant  X  that is the minimum probabilities of terms in TF-3. As a new approach of applying term-weighting model, two methods are proposed. One of them is the replacement of frequency-based metric with term weighting (TW-the same one as that of TF-3. The other one is to apply the term weighting metric both (TW-2). We performed an experiment to evaluate the effects of content-based term weighting on the selection of terms and text categorization. Through the experiment, we tried to term weighting method for text categorization with the feature selection method of ment are news group data collection. This data collection consists of 10,331 docu-ments for 15 categories [14]. We used 7,224 articles for training data and 3,107 arti-cles for test data. The result showed that the combination meth od of content-based term weighting and inverted category frequency with a smoothing of minimum value achieved the best performance. The performance of the hybrid method TW-2 achieved 6.9%~11.1% improvement in F 1 -measure compared to the base-line model (TF-1). The perform-ance is monotonically increasing according to the number of features in the experi-ments. TF-1 and TF-2 are pure Naive Bayesian classifiers with collection frequency smoothing (TF-1) and minimum value smoothing (TF-2), respectively. TF-2 of mini-mum value smoothing achieved 5.3%~6.8% improvement compared to the collection frequency smoothing. When we adopted term weighting and inverted category fre-quency to the probabilities of terms in TF-3 resulted in 0.8%~4.2% improvements. It is common that information retrieval model is designed and implemented on the probability estimation by frequency-based metrics that is one of the important factors for term weighting. In this paper, we explored a new approach of term weighting method, which will overcome the limitation of frequency-based weighting scheme. Term weighting is performed by content analysis of the document. It is based on the combination of term importance metrics in a sentence or paragraph together with frequency-based metrics. Term weights are calculated considering several factors such as location information, grammatical roles in a sentence, and type of terms. 
We applied term-weighting model to Na X ve Bayesian model of text categorization and evaluated the results. Experimental results showed that term weighting approach to Na X ve Bayesian model outperformed the standard frequency-based model. Also, we found that smoothing and inverted category frequency have a great role for perform-ance improvement, and we also showed that term weighting in document representa-tion technique is an important factor for performance enhancement of the text catego-rization system.

