 Exploiting lexical and semantic relationships in large unstructured text collections can significantly enhance managing, integrating, and querying information locked in unstructured text. Most notably, named entities and relations between entities ar e crucial for effective question answering and other informatio n retrieval and knowledge management tasks. Unfortunately, the success in extracting these relationships can vary for differ ent domains, languages, and document collections. Predicting extraction performance is an important step towards scalable and intelligent knowledge management, information retrieval and information integration. We present a general language modeli ng method for quantifying the difficulty of information extractio n tasks. We demonstrate the viability of our approach by predicti ng performance of real world information extraction tasks, Nam ed Entity recognition and Relation Extraction. H.3.1 [INFORMATION STORAGE AND RETRIEVAL]: Information Search and Retrieval General Terms Algorithms, Experimentation. Keywords Language modeling, information extraction, named entity extraction, relation extraction, context language model ing. The vast amount of information that exists in unstructured text collections is still primarily accessible via keyword quer ying at the document level. Unfortunately, this method of information access largely ignores the underlying lexical and semantic relationships between terms and entities in the text. These relationships can be extremely valuable for answering questions , browsing the documents, and managing information associated with the entities of interest. Additionally, document retrie val relevance could be improved if we detect meaningful terms (e. g., named entities such as dates, persons, organizations, and locations); and related entities (e.g., pairs of entities such as  X  person  X  X  birth date  X  and  X  person who invented a device  X ) could be used directly to answer questions. Furthermore, indexing entities and relationships would support more intelligent document browsing and navigation, and would allow for richer interactions with the document collections Hence, being able to reliably extract such relationships from text may be of vital importance to knowledge management and information retrieval. However, real collections can exhi bit properties that make them difficult for information extractio n. At the same time, tuning an information extraction system for a given collection, or porting an information extraction syste m to a new language, can require significant human and computational effort. Hence, predicting if an extraction task will be succe ssful (i.e., the required information can be extracted with high accuracy) is extremely important for adapting, deploying, and maintaining information extraction systems, and, ultimately, for managing and retrieving information in large text collections . We observe that document collection properties, such as typica l text contexts surrounding the entities or relation tuples, can affect difficulty of an extraction task. In this paper, we pr esent a first general approach to use context language models for predicting whether an extraction task will succeed for a given document collection. More specifically, we consider two crucial information ext raction tasks: Named Entity Recognition , and Relation Extraction .  X  Named Entity Recognition (NER) is a task of identifying  X  Relation Extraction (RE), is a task of identifying semantic Most state of the art NER and RE systems rely on local context to identify entities or determine the relationship between ta rget entities. In NER, contextual patterns such as  X  Mr.  X  or  X  mayor of  X  are often used for hypothesizing occurrences of entities and classifying such identified entities, especially when they are polysemous or of a foreign origin. The local context is al so important for the RE task. Intuitively, if the context surroundi ng the entities of interest for a given relation looks simil ar to the general text of the documents (i.e., there are no consistent and obvious  X  X lues X  that the entities or relationships of interes t are present), then the RE task for that relation will be hard. Whil e NER systems can resort to dictionary lookups in some cases (e.g., for the  X  X ocation X  entities, dictionaries can be particularl y helpful), for others (e.g., people X  X  names or organizations) high accuracy may not be possible. In contrast, if the text context around entities in the collection tends to contain telltale cl ues, such as  X  X r. X  preceding a person name, the extraction task is expected to be easier, and higher accuracy achievable. Our approach formalizes and exploits this observation by building two language models for the collection  X  a task-specific context language model for the extraction task, and the overall background model for the collection. We can then compare the two language models and compute the divergence of the task-specific language model from that of the overall collecti on model. If the divergence is high (i.e., the task-specific l anguage model is different from the overall model), the extraction task is expected to be easier than if the divergence is low (i.e., the task-specific language model is very similar to the document lang uage model). Interestingly, our approach can be potentially helpful for other applications, including better term weighting for information retrieval, and supporting active learning for interactive information extraction. For example, we could derive improved term weighs for domain-specific retrieval tasks such as bi rthday finding by incorporating context model weights. We will discuss other promising future directions of this work in Section 5. The rest of this paper is organized as follows. In the next s ection we review related work. In Section 3 we present our formal model and the algorithms for building the language models. In Section 4 we present experimental results for the NER and RE tasks over large document collections. In Section 5 we prese nt our conclusions, and discuss potential future research directions. Our work explores language modeling for information extraction and thus touches on areas of information retrieval, informati on extraction, and language modeling. Our approach is partly inspired by the work of Cronen-Townsend, Zhou, and Croft [9] on predicting query performance by measuring the similarity of a language model LM from the retrieved documents for a query and a language model for the whole target collection of documents LM Coll . Using simple unigram language models, they showed that the relative entropy between the query and collection language models correlates with the average precision in several TREC test collecti ons. In this paper, we apply similar language modeling techniques to the task of predicting information extraction performance. Language modeling, typically expressed as the problem of predicting the occurrence of a word in text or speech, has been a n active area of research in speech recognition, optical char acter recognition, context-sensitive spelling, and machine translatio n. An in-depth analysis of this problem in natural language processing is presented in [20], Chapter 6. Language modeling has also been used to improve term weighting in information retrieval (e.g., [22, 30] and others). However, in previous w ork LM was used as a tool for improving the specific system performance, whereas in our work we attempt to predict performance for general extraction tasks. An important distinction of our work is that we consider task-specific contexts. As our results indicate, using the locality in the overall document collection may not be sufficient, as local context models can become similar to the background model for overall document collection. Our approach is similar in spir it to the use of entity language models described in [23] for classifying and retrieving named entities. Our work is complementary as we present a general approach for modeling the performance of extraction tasks including both named entity recognition and relation extraction. For the named entity recognition task, numerous ways of exploiting local context were proposed, from relatively sim ple character-based models such as [11] and [19] to complex models making use of various lexical, syntactic, morphological, orthographical information, such as [6] and [10]. In this work, we show that we can predict the difficulty of identifying several types of named entities by using relatively simple context language models. This study can be viewed as complementary to Collins X  work [8] on the difficulty of identifying named entity boundaries, regardless of entity type. Relation extraction systems rely on variety of features (e.g., syntactic, semantic, lexical, co-occurrence), but all depend heavily on context. Once the entities are identified, it is t he textual context that expresses the relationship between the entities. Partially supervised relation extraction systems (e.g., [1], [12], [18], [24], and others) rely on the text contexts of example facts to derive extraction patterns. For relation extraction, the task difficulty was previously analyzed by considering the complexity of the target extractio n templates ([2] and [17]). Another promising approach described in [13] modeled the task domain variability by considering the different paraphrases used to express the same information in the text. In contrast, our work quantifies the difference between the contexts around the entities and unrelated text contexts. If the contexts of the example facts are similar to the background te xt, an extraction system is expected to have more difficulty deri ving extraction patterns and recognizing the relevant entities. In this section we describe the general approach we take for modeling the difficulty of an extraction task, and hence the expected performance of an extraction system on the task (Section 3.1). Then, in Section 3.2, we describe the algorithms for computing the language models to make our predictions. As we discussed, the textual context (i.e., the local properti es of the text surrounding the entities and relations of interest) can be of crucial importance to extraction accuracy. Intuitively, if the contexts in which the entities occur are similar to the gener al text then extraction is expected to be difficult. Otherwise, if ther e are strong contextual clues, the extraction should be easier and we should expect higher extraction accuracy. To quantify the notion of context, we use a basic unigram language model, which is essentially a probability distribution over the words in the text X  X  vocabulary. In this study, we deri ve this probability distribution from the histogram of words occurring in the local context of target entities by using maximum likelihood estimation. Our purpose is to compare the language model associated with an entity type or relationship LM C with a background language model for the whole target text, denoted by LM BG . Therefore, no smoothing of these models is necessary. Intuitively, if the background language model for t he collection is very similar to the language model construct ed from the context of the valid entities then the task is expected to be hard. Otherwise (if LM C is very different from LM BG expected to be easier. A common way to measure the difference between two probability distributions is relative entropy, also known as the Kullback-Leibler divergence: In Information Theory, KL-divergence represents the average number of bits wasted by encoding messages drawn from the distribution LM C using as model the distribution LM BG Alternatively, we can measure how different two models are by using cosine similarity, which represents the cosine of the a ngle between the two language models seen as vectors in a multidimensional space in which each dimension corresponds to one word in the vocabulary: The closer the cosine is to 1, the smaller the angle and thus, the more similar the two models. Hence, to measure the differ ence of the two models LM C and LM BG we define CDist as: to maintain symmetry with the KL metric, with larger values indicating larger difference between models. We now describe how to construct a language model for a give n extraction task. For clarity, we describe a unigram language model, but our methodology can be extended to higher-order features. For syntax-based extraction systems, we could parse the text and incorporate that information into the model as in [6]. However, as we will show experimentally, a simple unigram model is sufficient to make useful predictions. To construct the task-specific context language model LM search the collection for occurrences of valid entities (o r relation tuples). While for the NER and RE tasks LM C is constructed slightly differently (as described below), the overall appro ach is to consider the text context to be the K words surrounding the entities in question. More specifically, the language model for NER is construct ed as outlined in Figure 3.1. We scan the document collection D, searching for occurrences of each known entity E i entity is detected, we add to LM C up to K terms to the right and to the left of the entity. The algorithm for constructing a task-specific language mode l for RE is outlined in Figure 3.2. The procedure is similar to the N ER algorithm above. We scan the document collection D, searching for occurrences of each known example tuple T i for the target relation. For this, we search for all attributes of T all entities are present, and occur within K words of each other, we increment the LM C counts of all the words between the leftmost and the rightmost entities. If the entities in a r elation tuple are close together (i.e., there are fewer than K words separating the entities in the text), we include all the terms separating the entities. Algorithm 3.1: NER Context language model construction . Algorithm 3.2: RE Context language model construction . Unfortunately, we don X  X  have all the valid entities available (i.e., when predicting whether a task will succeed without going through the complete extraction process). Hence, our model is build based on sampling the collection using a small (20-40) sample of the known entities or tuples by providing only these example entities as input to the NER and RE language model construction algorithms above. For a large corpus, the sampl e-based model is expected to be a reasonable approximation of the complete task specific language model. The background language model, LM BG is derived through maximum likelihood estimation using the word frequencies in each document collection. When we discard stopwords from LM we also discard them from LM BG . In order to interpret the divergence of a task specific langua ge model LM C from the background language model, we build a reference context language model LM R (also denoted as RANDOM). We construct LM R , by taking random samples of words in the vocabulary (excluding stopwords) of the same size as the entity samples. We then use these words input to Algorithm 3.1. Using LM R we can then compute the  X  X eference X  divergence of a context language model from the background model for a given sample size. For large sample sizes, LM expected to approximate the background model. Indeed, Figure 3.1 reports that for larger random word sample sizes, LM becomes more similar to the background model, and the divergence steadily decreases. We also use LM R to normalize our divergence measures to be robust to different entity sample sizes and collection sizes . For this, we compute the normalized divergence as the ratio of the KL-divergence value of LM C , and the KL-divergence value of LM R , as compared to the background distribution LM similarly compute normalized cosine distance as the ratio of the CDIST values of LM C and LM R compared to LM BG . Constructing the context models LM C and LM R can be done efficiently by using any off-the-shelf search engine and considering only the documents retrieved by search for the example entities or tuples, and run Algorithms 3.1 and 3.2 only over these reduced document sets. Having described constructing the language models for extraction tasks, we now turn to experimental evaluation. We evaluated our prediction for two real-world tasks: Named Entity Recognition (NER) and Relation Extraction (RE). We f irst describe the experimental setup (Section 4.1), including the datasets, entity and relation types, and parameter settings w e considered. Then we describe our experiments for predicting NER difficulty (Section 4.2), followed by our experiments on predicting RE difficulty (Section 4.3). In order to design a realistic evaluation we focused on two extraction tasks, NER and RE, over large document collectio ns. The overall goal of the experiments is to determine if the language models, constructed from a realistically small s ample of the extractions of interest, can make useful predictions about t he observed accuracy of the extraction task for that collectio n. The document collections used for these experiments are reported in Table 4.1. The Reuters RCV1 documents were drawn from the collection used in the CoNLL 2003 [17] NER shared task evaluation. The EFE (Spanish) and the De Morgen (Dutch) documents were the datasets used in the CoNLL 2002 NER shared task evaluation. Note that while the Spanish and the Dutch collections are small, they are a  X  X tandard X  dataset for NER evaluation. For the RE experiments, we used Encarta, a large online encyclopedia document collection. For all experiments, we start with a small sample (10-40) o f entities or relation tuples, drawn at random from a list of known valid entities or tuples. In Table 4.2 we report the specific s of the extraction tasks used for the experiments. To validate our extraction performance predictions for the NE R task, we used as reference the top performing systems in the CoNLL shared task competition, which were evaluated over a manually annotated subset of news articles from the same RCV 1 corpus as described above. Moreover, we built the samples of named entities by randomly sampling the set of named entities present in the training set provided by the CoNLL competition organizers (described in [26] and [27]). To validate our performance predictions on the RE task, we us ed a bootstrapping-based extraction system similar to Snowball [ 1], which is heavily dependent on the example entities and the text context in which they appear to derive extraction patterns. For comparison, we also report RANDOM, the divergence of the random keyword sample-based language model, LM R . In our experiments we explored the following parameters:  X  Context size: number of words to the left and to the right of  X  Maximum distance separating the entities (for RE task)  X  Divergence metric, CDist or KL : The language model  X  Example set size S : number of randomly drawn entities (or  X  Random sample size R : number of randomly drawn terms to  X  Stopwords : we analyze two cases, when stopwords (common  X  N-gram size N : We considered word unigrams, bigrams and In order to explore the parameter space and evaluate the accura cy of our predictions, we use as reference the reported performa nce of the top five systems in the CoNLL 2003 shared task competition [27], which is summarized in Table 4.3. According to the reported numbers, the Person (PER) and Location (LOC) entities are the  X  X asiest X  to extract, whereas the Misce llaneous (MISC) and Organization (ORG) are relatively difficult. LOC 91.15 91.12 89.98 89.54 89.26 90.21 MISC 80.44 79.16 80.15 75.87 78.54 78.83 ORG 84.67 84.32 80.48 80.46 79.41 81.86 PER 93.85 93.44 90.72 90.44 88.93 91.47 Overall 88.76 88.31 86.31 85.50 85.00 86.77 We report the results of our system on predicting NER diffic ulty in Tables 4.4a, 4.4b, and 4.5. The first two tables present the results obtained on a smaller subset of the Reuters RCV1 cor pus, of 3.5 million words, while the latter shows the results obta ined for a 10 times bigger subset of the same corpus (35 million words). It is remarkable that the language models estimated on the smaller corpus make extremely similar predictions to tho se estimated on a corpus 10 times larger. Our ranking identifies ORG and PER entities as  X  X asy X  to extr act entity types and LOC and MISC as hard to extract. These correlate with the results reported by the participants in the CoNLL 2003 Shared Task competition (Table 4.2), with the exception of the LOC entities. We believe this happens for thr ee reasons: first, the location entities in the test set ove rlap to a large degree with the locations in the training data; second: more than for the other entity types considered, indicative contexts of LOC entities are represented by stopwords (e.g. in , from , to ), third, all systems shown in Table 4.3 except [19] used extensive lists of gazetteers, which were likely to contain most l ocations that news articles may talk about and thus, covering most of the locations in the test. Tables 4.4a and 4.4b report the prediction results using stopwords in the language model (Table 4.4a) and discarding the stopwords (Table 4.4b). As expected, the context language models are more similar to the background model when stopwords are included, but in both cases the conclusions are the same. This is encouraging, as it shows that our approach may work even for languages where no lexical information (such as stopwords) is known a priori . A drawback of our current approach is that we do not consider how easy it is to identify entities of a given type based o n sources of information other than context, such as morphology, internal capitalization, or gazetteer lists. Consequently, our syst em may not be able to predict accurately the extraction performance o f fully-featured systems for entities with various intrinsic properties that make them easier or harder to identify independent of context (e.g. the real performance for MISC is somewhat lower than expected based on our prediction due to the fact that capitalization and length varies to a much larger degree for MISC entities than for the other entity types). We now consider the sensitivity of our results to sample si ze (Figure 4.1) and N-gram size (Figure 4.2). Figure 4.1 reports normalized KL divergence for RCV 1/100 for seed sample sizes of 10, 20, 30, 40, and 50. As we can see, for sample sizes greate r than 20 our predictions do not change. Hence, sample size of 20 seed entities will be used for our subsequent experiments. Figure 4.2 reports the normalized KL divergence for RCV 1/100 for language models created with N-grams of size 1, 2, and 3 words. Interestingly, the single-word (unigram) model appears to be as predictive as the two-word (bigram) and the three-word (trigram) models. In general, higher order N-gram models t end to be sparse, and hence may not be useful for our problem. Therefore, we will report results for the simpler one wor d models for the subsequent experiments. We now show that our prediction technique also applies to languages other than English, by evaluating it for the named entity task on the Spanish and Dutch collections used in the CoNLL 2002 shared task evaluation [24]. As we discussed, porting information extraction systems to new domains and new languages can require significant effort. For languages other than English, annotated data and other language specific resources ar e less readily available. Hence, developing systems for thes e languages is typically more difficult, and this is also show n by the lower performance of state-of-the-art NER systems o n Spanish and Dutch (Tables 4.6 and 4.7). Tables 4.6 and 4.7 report the F-measure performance of the top three systems participating in the CoNLL 2002 evaluation on the NER task for Spanish and Dutch, respectively. Tables 4.8 and 4.9 report the average KL divergence for LOC, MISC, ORG, and PER entities for the two languages by using random samples of 20 entities of each type. As we can see, o ur model predicts that PER entities are much easier to extrac t based on context than the other entities in both Spanish and Dutch. LOC 2.86 1.18 2.53 1.39 2.17 1.42 MISC 4.19 1.73 3.86 2.12 3.60 2.35 ORG 3.44 1.42 2.90 1.59 2.51 1.64 PER 4.86 2.01 4.21 2.31 3.91 2.56 RANDOM 2.42 1.82 1.53 LOC 3.73 1.44 3.11 1.65 2.75 1.61 MISC 5.11 1.97 3.82 2.02 3.26 1.91 ORG 3.96 1.53 3.52 1.86 3.28 1.92 PER 5.82 2.25 4.97 2.63 4.44 2.60 RANDOM 2.59 1.89 1.71 This is confirmed by the actual results for Spanish. For Dutch, the best performing systems in CoNLL 2002 also performed much better for PER than for ORG and MISC, LOC being, similarly to English, an outlier. Our system predicts that LOC is a difficult entity type to extract based on context for all l anguages mainly because many of the relevant corresponding contexts in these languages are stopwords, which occur frequently throughout the text. However, real systems were able to ide ntify the LOC entities because the percentage of actual entities s een both in the training and in the test is typically greater than f or the other entity types. This aspect of the problem is not modeled by our approach, and can be addressed in future work. We now turn to predicting performance of relation extraction tasks (RE). The goal is to predict which relations are  X  X if ficult X  to extract, and which ones are  X  X asy X . Table 4.10 reports the actual extraction accuracy on the RE task using a simple bootstrapping-based information extraction system similar to Snowball [1] and KnowItAll [12]. We report the precision on each task estimated by sampling 100 facts from the extracted relation instances. As we can see, the BORN and DIED relat ions are  X  X asy X  for the extraction system (exhibiting precision o f as high as 97%), whereas INVENT and WROTE are relatively  X  X ard X  (exhibiting precision as low as 50%). BORN 0.73 0.96 Easy DIED 0.34 0.97 Easy INVENT 0.35 0.64 Hard WROTE 0.12 0.50 Hard Table 4.10 reports the absolute and normalized KL divergence values computed from the models built by discarding common English stopwords. As we can see, the KL divergence values of the BORN and DIED relations are higher than the KL values for the INVENT and WROTE relations, predicting that the former should have higher accuracy than the latter. Hence, KL correctly predicts the  X  X asy X  relations vs.  X  X ard X  relations to extra ct. Also note that if only one word of context is considered, our model incorrectly predicts that DIED relation is more difficult than the INVENT relation. As we can see, context size of at leas t two words is needed to correctly predict the extraction difficulty . Relation Context size 1 Context size 2 Context size 3 BORN 13.88 2.02 13.54 2.17 13.84 2.39 DIED 12.98 1.89 11.61 1.86 10.60 1.83 INVENT 13.33 1.94 10.92 1.75 9.96 1.72 WROTE 10.92 1.59 9.92 1.59 8.86 1.53 RANDOM 6.87 6.24 5.79 To further investigate the required effort needed for robust prediction on the RE task, in Figure 4.3 we report the predictions for varying the seed sample size from 10 to 40 relation tuples . As we can see, our predictions remain relatively stable for s ample sizes of at least 20 seed tuples. Interestingly, adding additio nal seed tuples beyond 20 does not improve the overall prediction accuracy as our approach: while our method distinguishes between the  X  X asy X  and  X  X ard X  relations correctly, it is no t able to further distinguish between the two  X  X ard X  relations, namely that the WROTE relation is more difficult to extract than t he INVENT relation. We presented a general, domain and language independent approach for predicting extraction performance. We have shown that our language modeling approach is effective for predicting extraction accuracy for tasks such as named entity recognitio n and relation extraction, both tasks crucial for high accuracy a nd domain-specific information retrieval and information management. As our experiments indicate, starting with even a small sampl e of available entities can be sufficient for making a reasona ble prediction about extraction accuracy. Our results are particula rly encouraging as we consider a relatively simple model that do es not require extra information to that typically available to modern NER and RE systems. Extending our method to use more sophisticated language models can further improve our predictions. For languages where reliable NLP tools are available, one promising direction would be to incorporate syntactic features, and to apply techniques suc h as co-reference resolution to build richer and more accurate context language models. Additionally, incorporating gazetteer lists similar to those typically used by the NER systems can further improve prediction accuracy. Another interesting direction for future work is to correlate our predictions wi th the actual accuracy values for more fine-grained predictions. Furthermore, our results could be applied for building interactive information extraction systems that could guide the user by requesting more examples for the extraction tasks predicted to be  X  X ifficult X . Such an interactive system could more effectiv ely focus valuable human effort on the  X  X ifficult X  extraction tasks, where it is most sorely needed. As we have shown, our approach is general and language-independent. With amounts of new information available in text increasing daily, our techniques could be extremely valuable for developing, maintaining, and deploying information extraction technology for better information access. We thank Luis Gravano for ideas and discussions that inspired this work. We also thank Eric Brill and the anonymous referee s for their insightful comments. [1] E. Agichtein and L. Gravano, Snowball: Extracting Relations [2] A.Bagga, Analyzing the complexity of a domain with respect [3] E. Brill, S. Dumais, and M. Banko. An Analysis of the [4] X. Carreras, L. M X rques and L. Padr X , Named Entity [5] X. Carreras, L. M X rques and L. Padr X , A Simple Named [6] C. Chelba and F. Jelinek, Exploiting Syntactic Structure for [7] H. L. Chieu and H. T. Ng, Named Entity Recognition with a [8] M. Collins, Ranking Algorithms for Named Entity Extraction: [9] S. Cronen-Townsend, Y. Zhou, W. B. Croft, Predicting Query [10] S. Cucerzan and D. Yarowsky, Language Independent Named [11] S. Cucerzan and D. Yarowsky, Language Independent NER [12] O. Etzioni, M. J. Cafarella, D. Downey, S. Kok, A. M. [13] I. Dagan and O. Glickman, Probabilistic textual entailment: [14] R. Florian, Named Entity Recognition as a House of Cards: [15] R. Florian, A. Ittycheriah, H. Jing, and T. Zhang, Named [16] R Gaizauskas, Y Wilks, Information Extraction: Beyond [17] S. Huttunen, R. Yangarber, and R. Grishman, Complexity of [18] R.Jones, A.McCallum, K. Nigam, and E. Riloff, [19] D. Klein, J. Smarr, H. Nguyen, and C. D. Manning, Named [20] C. D. Manning and H. Sch tze, Foundations of Statistical [21] J. Perez-Carballo and T. Strzalkowski, Natural Language [22] J. M. Ponte and W. B. Croft, A Language Modeling Approach [23] H. Raghavan, J. Allan, and A. McCallum, An exploration of [24] D. Ravichandran and E. Hovy, Learning Surface Text Patterns [25] E. Riloff and R. Jones, Learning Dictionaries for Informat ion [26] E. Tjong and K. Sang, Introduction to the CoNLL-2002 [27] E.F. Tjong, Kim Sang and F. De Meulder, Introduction to the [28] E.M. Voorhees, Natural Language Processing and Information [29] D. Wu, G. Ngai, M. Carpuat, J. Larsen and Y. Yang, Boosting [30] J. Xu and W. B. Croft, Improving the effectiveness of [31] T. Zhang and D. Johnson, A Robust Risk Minimization based 
