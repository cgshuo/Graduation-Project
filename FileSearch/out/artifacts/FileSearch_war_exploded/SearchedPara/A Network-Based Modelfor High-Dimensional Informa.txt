 The Vector Space Model has been and to a great extent still is the de facto choice for profile representation in content-based Information Filtering. However, user profiles repre-sented as weighted keyword vectors have inherent dimen-sionality problems. As the number of profile keywords in-creases, the vector representation becomes ambiguous, due to the exponential increase in the volume of the vector space and in the number of possible keyword combinations. We argue that the complexity and dynamics of Information Fil-tering require user profile representations which are resilient and resistant to this X  X urse of dimensionality X . A user profile has to be able to incorporate many features and to adapt to a variety of interest changes. We propose an alternative, network-based profile representation that meets these chal-lenging requirements. Experiments show that the network profile representation can more effectively capture additional information about a user X  X  interests and thus achieve signif-icant performance improvements over a vector-based repre-sentation comprising the same weighted keywords.
 H.3.4 [ Information Storage and Retrieval ]: Systems and Software Algorithms, Experimentation, Performance Content-based Information Filtering, User Profiling, Curse of Dimensionality
When today on the WWW everyone can be both a con-sumer, but also a producer of information, a dual informa-tion overload problem arises. On one hand, it is impossible to keep track of the information that is being dynamically generated and disseminated in the context of the so called real-time Web, or to spot interesting sources of information out of the available glut. On the other hand, nobody can en-sure the individual publisher that broadcasted information will reach the right audience. Information Filtering (IF) and the personalisation of information delivery that it achieves, canhavearadicalimpactonthewayweinteractwithin-formation media. Already, Collaborative Filtering (CF) has been successfully deployed for calculating recommendations of movies, music tracks and books, but is admittedly not well suited for dynamic domains, like news publishing. Unlike CF, content-based IF has not yet produced similar success stories. After two decades of research on content-based IF, there is a surprising lack of publicly available and broadly adopted content-based IF applications.

Some of the reasons for this absence are discussed in [10], where we argued that IF is a complex and dynamic prob-lem with its own particular characteristics and requirements, which differentiate it from Information Retrieval and Text Classification. IF is complex and dynamic because user in-terests and the information environment are complex and dynamic. Unlike Text Classification, the notion of a  X  X opic X  of interest is not that distinct in the case of IF. A user is typically interested in a variety of topics, which are fluid and interrelated. Over time, the level of interest in each topic may vary, new topics of interest can emerge and a previously interesting topic may wane and even become obsolete. Fur-thermore, there is an immense variety of topics to choose from in the information space. From general topics of in-terest, such as news categories (e.g., economy, technology, etc.) to a  X  X ong-tail X  of more personal and specific interests. Of course, the information space itself continuously changes with the new material dealing with new combinations of concepts, even new concepts, the development of new tech-nologies, the occurrence of temporal events, etc. Successful IF requires a user profile representation that can capture the various topics of interest and can continuously adapt to interest drifts and changes in the information space.
One significant implication of the above specification is that a user profile has to be able to incorporate a large number of features. For example, if we focus on textual information, then a larger number of keywords is required to represent multiple topics of interest than to represent a single topic. More keywords are also required when the top-ics are specific rather than general. But, as we will further discuss in the next section, the Vector Space Model (VSM), the most popular choice for profile representation in IF, has inherent problems when the number of keywords, i.e., the dimensions of the vector space, increases. This  X  X urse of dimensionality X  [3] has forced vector-based approaches to IF to adopt radical dimensionality reduction techniques and to approach the modelling of user interests by requiring a separate profile for each topic. Furthermore, the VSM is typically coupled with the  X  X ag of words X  assumption and hence any information encoded in the correlated placement ofwordsintextisnotcaptured.
 In this paper, we propose an alternative to the VSM. The user profile is no longer a weighted feature vector but a weighted network of descriptive features, which has been assigned to, or can be automatically extracted from interest-ing information items. Links in this network represent cor-relations between features appearing within the same con-text. Relevance evaluation of information items is not per-formed with trigonometric measures of similarity between keyword vectors, but with a directional spreading activation process. In the case of textual information, the user profile is a weighted network of keywords extracted from the con-tent of interesting documents. We argue and experimentally support that the proposed network-based profile can incor-porate a large number of keywords, more effectively than a vector-based profile. In doing so, the network-based pro-file captures additional information about a user X  X  multiple interests, it becomes more specific and achieves significant performance improvements. We substantiate this claim in section 4. This resistance to the  X  X urse of dimensionality X  is a significant property of the proposed profile representation, offering many practical advantages and new perspectives.
In the rest of this paper we first identify and discuss the causes for the inherent dimensionality problems of vector-based approaches to IF. Then in section 3 we describe the proposed network-based representation. The experiments in section 4 have been performed with a methodology that adopts the Reuters-21578 and simulates users with multiple topics of interest. We compared a vector-based to a network-based profile comprising the same weighted keywords. The results indicate that as the number of keywords in these profiles increases, the existence of links in the network profile contributes to an increase in performance of up to 50% on average, compared to the vector-based profile. We discuss the implications of these positive results and we conclude with a summary and future research plans in section 5.
The VSM [15] is the most popular choice for profile rep-resentation and has had fundamental impact for research in IF. According to the VSM both documents and profiles are represented as, typically weighted, keyword vectors in a mul-tidimensional space with as many dimensions as the number of keywords in the documents X  vocabulary. This abstraction allows the application of trigonometric measures of similar-ity, like the inner product or cosine similarity, for assessing how  X  X lose X  to a user profile a given document is [6]. The profile X  X  goal is to define decisi on bounda ries between rele-vant and non-relevant documents, or represent regions in the vector space which are dense with interesting documents. In multidimensional spaces however, the discriminatory power of pairwise distances is significantly affected.

In [7], the authors thoroughly analyse and discuss in the context of Artificial Immune Systems, the issues that under-mine vector-based approaches in multidimensional spaces. Their argumentation is directly applicable to vector-based approaches to IF and so here we recapitulate some basic points:
We would like to complement the above issues with an intuitive argument. Let X  X  assume that a user is interested in two topics:  X  X ong river X  X nd X  X ank holidays X . If we use a key-word vector, containing these four words, then we equally represent any possible combination of these four words, in-cluding for instance the combinations,  X  X iver banks X  and  X  X ong holidays X , which do not necessarily represent topics of interest to the user. In general, as the number of keywords in a user profile increases, the number of possible combinations increases exponentially and the profile becomes ambiguous, because the majority of keyword combinations will be irrel-evant to the user X  X  interests. To counteract this effect one should at least avoid the common term-independence as-sumption, which is inherent to the VSM and its orthogonal dimensions, and move away from the  X  X ag of words X  simpli-fication. In the opposite case, valuable information about the user X  X  interests is lost and the profile X  X  specificity drops. Our experimental results, clearly support this argument.
Although in this paper we concentrate on a static IF prob-lem, we will briefly touch on dynamic aspects. As both the user X  X  interests and the information space change over time, a fixed keyword space becomes an inadequate choice. Tack-ling the problem X  X  dynamics requires a fluid keyword space where additional dimensions can be added and removed. But even if this is the case, the adoption of a common vec-tor space where all profiles and documents are represented is still impractical. If an IF system needs to accommodate a large number of users, then it is only safe to assume that their interests will vary. To cover all possible topics of in-terest with a common vector space, a very large number of keywords and hence dimensions are required and the com-putational and memory costs would significantly increase. For instance, experiments performed in [9] demonstrate that covering 23 of the topics in Reuters-21578 requires a com-mon vector space comprising more than 30000 documents.
Although seldom clearly stated, the above dimensional-ity problems are evident in current vector-based IF prac-tices. They heavily depend on dimensionality reduction techniques, such as stop word removal, stemming, term weight-ing [23], Latent Semantic Indexing (LSI) [5] and more. This pre-processing of documents takes place in advance and typ-ically results in a fixed vector space with manageable dimen-sions. Furthermore, they tend to break up the problem into distinct pre-defined topics and built a separate single-topic profile for each individual topic [21, 22]. This practice has been inherited from Text Classification and is reflected by evaluation standards for IF [14]. In reality though, a user X  X  topics of interest cannot be easily predefined and they are definitely not fixed. Even within a general topic (e.g.,  X  X con-omy X ) different users will develop specific interests in various subtopics (e.g.,  X  X redit card fraud X ,  X  X quity markets X  etc.). Furthermore, given the plenitude of documents (e.g., news stories, blog posts, etc.) being published daily on a topic, a user is only interested in and has the time to read a very small percentage. So specificity is essential for successful IF, but it requires profiles with the ability to capture any avail-able information about a user X  X  interests. Although outside the scope of the current work, we would also like to note that many machine learning algorithms, such as Rocchio X  X  linear learning algorithm, which have been a popular solution for profile adaptation [17, 24], lack an inherent mechanism for adding or removing keywords to a user profile. This means that they assume a fixed vector space that predefines the possible repertoire of profile keywords. The algorithm can only modify the weights of keywords in the profile X  X  vector. It has also been argued, that learning algorithms cannot easily cope with radical interest shifts [20]. A new profile is typically generated whenever a new topic of interest emerges and a profile that corresponds to a no longer interesting topic is destroyed [21]. This is of course only a partial solution to the problem that unnecessarily complicates the task with additional system parameters and in any case, no profile will be able to represent a topic that is not already covered by the keywords in the predefined vector space.
To alleviate the above issues, we propose an alternative to the VSM. We need a model for IF that satisfies the following basic requirements:
In the proposed model, the user profile is a weighted net-work with nodes representing features extracted from inter-esting information items and links representing their cor-relations. Since we will concentrate on textual information, nodes will represent terms (i.e., single words) extracted from the content of documents and links will represent correla-tions between terms in text. The discussion however can be easily extended to any type of descriptive feature that can be automatically extracted from the content of infor-mation items, or have already been assigned to them (e.g., tags). Every node is assigned a weight that measures the importance of the corresponding term given the user X  X  in-terests. Every link between two nodes is assigned a weight measuring the degree of correlation between the respective terms. The weights are of course important, but the way they are calculated is not constrained by the model itself. Any keyword weighting method can be used to calculate the weights of nodes in the profile X  X  network. To calculate the weights of links, co-occurence statistics between terms appearing in the same context are required. Various ap-propriate methods appear in the literature and have been used to construct similar network structures for capturing term correlations in Text Retrieval and even Information Filtering. For example,  X  X ollocation maps X  [12] and  X  X epen-dence trees X  [19] have been proposed for query expansion and  X  X oncept hierarchies X  [16] for navigating document col-lection and search results. In IF, correlations between terms have been generally neglected. One notable exception is the adoption of an associative graph for capturing syntactic cor-relations between terms that appear next to each other and of a spreading activation process for document evaluation in [18].

The essential contribution of the proposed model is not the network itself, but a new way of using the weighted network to evaluate the relevance of documents. We treat content-based IF as the general problem of assigning a rel-evance score to each document based on its content, rather than making a binary classification between relevant and non-relevant documents. Document evaluation is based on a non-iterative , directional , spreading activation process that takes into account not only the weight of profile nodes (terms), but also the weight of links between them. The process can be deployed to assign a relevance score to any portion of a document X  X  text, ranging from a single sentence to the com-plete document. The document is not treated as a  X  X ag of words X  and it does not have to be represented as a keyword vector. Nevertheless, the terms in the text can be weighted with methods such as Term Frequency Inverse Document Frequency (TFIDF). To assign a relevance score to a por-tion of text T , each term in the profile X  X  network that also appears in T is assigned an initial activation equal to the term X  X  weight in T . The activation phase is followed by a dissemination phase, which starts with the activated node with the smaller weight in the profile X  X  network and proceeds sequentially with the remaining activated nodes in increas-ing weight order, until the activated node with the largest weight is reached. Every activated node is triggered once to disseminate part of each current activation to the activated nodes with larger weights that it is linked to. The amount of activation that is disseminated between two nodes is propor-tional to the weight of the link between them. The relevance
Here we focus on symmetric links but the model could also account for non-symmetric links. n + c kn a k -c km -c km (D) node m disseminates. score of T is calculated as the weighted sum of the final ac-tivation of nodes.
 Figure 3 illustrates the above process. The portion of text T activates the initially idle nodes k , m , n , out of the com-plete profile network, which is not depicted in the figure. The weights of the three nodes are w k , w m , w n respectively, with 0 &lt;w k &lt;w m &lt;w n , and the weights of the links be-tween the three nodes have weights w kn , w km and w mn with positive values. The initial activation of the three nodes ( a a m , a n ) is equal to the weight of the corresponding terms in T . The dissemination process starts with the activated node with the least weight 2 .Node k disseminates an amount of activation equal to c kn = a k  X  w kn to node n and an amount equal to c km = a k  X  w km to node m . To avoid the situation where a node disseminates more than its current activation, i.e., when the sum of the weights of links to activated nodes is more than one, we normalise the weight of these links so that they add up to one. Once node k has disseminated its activation, the new activation of the three nodes k , m and n , becomes a k  X  c kn  X  c km , a m + c km and a n + c kn respectively. It is now the turn of the next node in the order of increasing weight to disseminate part of its current activation to acti-vated nodes with larger weight that it is linked to. So node m disseminates the amount c mn =( a m + c km )  X  w mn to node n and their respective activation becomes a m + c km  X  c mn and a n + c kn + c nm . At this point the dissemination process terminates because node n is not linked to any other acti-vated nodes with larger weights. The relevance score R T of T is given by the following formula: Note that the first term in the above sum, is actually the in-ner product between a weighted keyword vector of the three profile terms and a weighted keyword vector of the three terms in T . In other words, the above formula specialises to the inner product if there are no links between the activated nodes ( w kn = w km = w mn = 0). However, when the acti-vatednodesarelinkedthentherelevancescoreof T increases by an additional amount equal to the sum of the last three terms in equation 1. It is clear that this additional amount is
If two terms have the same weight then they are ordered alphabetically positive because all weights are positive and w k &lt;w m So every time a portion of text activates correlated nodes in the profile X  X  network and not isolated ones, it receives an additional relevance reward. In this way the profile becomes more specific, because it concentrates on those term combi-nations that are relevant to the user X  X  interests and it does not equally represent every possible combination of terms in the profile. Our experimental results clearly show that this property results in significant improvements in filtering accuracy, especially when the number of terms in the profile increases.

Overall, the proposed model, as described so far, satisfies the first two of the three requirements described earlier. The user profile is not represented as a weighted keyword vector on a common, central, predefined and fixed space. For each individual user, the profile is a separate network structure containing only those terms that are representative of the user X  X  interests. Furthermore, the number of profile terms is neither predefined nor fixed. It is dynamically controlled during the profile X  X  adaptation to interest changes. The pro-file X  X  network captures correlations between terms in text and a directional spreading activation process takes them into account during document evaluation. Note also, that unlike existing spreading activation processes that are typ-ically iterative and involve the complete network [18], thus increasing the computational cost, the proposed approach involves only the subset of activated profile nodes and its di-rectionality ensures that each activated node is visited only once and thereafter, each link between activated nodes is also traversed only once. So in the worst case of a fully con-nected and fully activated network, its complexity is O ( N where N p is the number of profile terms. The inclusion of links increases the complexity of the user profile, in com-parison to the linear complexity O ( N L ) of a vector-based profile in a N L -dimensional space, using the inner product for document evaluation. Note however, that if a common vector space is used to represent the profiles of many users with a variety of interests, then a large number of keywords would be required to cover all possible interests. In contrast, the proposed model is inherently distributed and comprises only the terms required to represent the interests of a single user. So N p &lt;&lt;N L and the difference in complexity be-tween O ( N 2 p )and O ( N L ) is alleviated. If needed, the com-putational and memory requirements of each profile can be controlled with upper limits on the number of profile terms and links. In any case, if the user profile resides on the user X  X  machine, scalability issues do not arise.
The theoretical background of the proposed model is dis-cussed in detail in [8] and is biologically-inspired. The user profile is modelled after the network of interacting antibod-ies in the immune system of vertebrates and through the chains of suppression and reinforcement that the spreading activation process generates, it defines the host organism X  X   X  X elf X , i.e., the user X  X  interests. In the same paper, we de-scribe an algorithm that allows the profile to continuously adapt to a variety of interest changes, through a biologically-inspired process of self-organisation. The algorithm adjust the profile X  X  structure in response to user feedback, through variations in the weight of profile terms, recruitment of new terms that cover emerging topics of interest and removal of terms that correspond to no longer interesting topics. So the profile is not constrained by the terms in a pre-defined vector space. Experiments show that through this process the pro-file can adapt to both short-term variations and more long-term, radical changes in user interests, while autonomously controlling both its size and connectivity [8]. Furthermore, comparative experiments show that this algorithm outper-forms the popular Rocchio X  X  learning algorithm on a contin-uous learning problem [11]. So, although outside the scope of the current work, the third of the aforementioned require-ments can also fulfilled.
In this section, we evaluate experimentally a specific reali-sation of the model and compare it with a vector-based pro-file. In [10], we argued that existing evaluation methodolo-gies do not accurately reflect the particularities of content-based IF, mainly because they usually treat it as a Text Classification problem, with each user profile representing a single topic of interest and trained with a large number of rel-evant documents. Furthermore, since the removal of the fil-tering track from the Text Retrieval Conference (TREC) in 2001, there is no established evaluation standard for content-based IF. Here we adopt a stripped down version of the methodology in [11], which simulates users with multiple interests, but ignores interest changes.

The methodology uses the Reuters-21578 document col-lection, but could be applied for any pre-classified collection of documents. The evaluation concentrates on the 23 topics in Reuters-21578 with more than 100 relevant documents (table 1). Each topic is assigned a serial number (in paren-thesis) to identify it when presenting the experimental re-sults. For each topic we use the first fifty relevant documents in the collection for training and the complete collection as a test set. This is a significant departure from current prac-tices for the evaluation of Text Classification systems, such as the ModApte split, that uses three quarters of the doc-uments for training and the remaining quarter for testing. By using the same small number of training documents per topic and given that the number of relevant documents range from 112 to 3987 (table 1), a variable percentage of the rel-evant documents is used to build the profile. So the user profile has to be specific for topics with a small number of relevant documents and exhaustive for topics with a large number of relevant documents.

The most significant contribution of the proposed method-ology, is the simulation of users with multiple interests. In particular, we simulated users with parallel interest in one, two, three, four and five topics. For instance, to simulate a user interested in two topics we train a single profile for each combination of two consequent topics (e.g., earn and acq (1:2), acq and money-fx (2:3), money-fx and crude (3:4) and so on). We combine consequent topics with similar sizes to avoid biases towards topics with a larger number of rel-evant documents. For each topic combination we use the first 50 relevant documents per topic to train a single pro-file, which is then used to evaluate the complete collection. The 21578 documents are then ranked according to their relevance score and the Average Uninterpolated Precision (AUP) measure is calculated for each individual topic and also for their combination, i.e., an aggregate topic that in-cludes all documents relevant to the constituent topics. A topic X  X  AUP is defined as the sum of the precision  X  i.e., the percentage of documents relevant to that topic  X  at each point in the ordered list where a relevant document appears, divided by the topic X  X  size. We use an evaluation list com-prising all documents in the collection and not just the best 1000 scoring documents (as in TREC X  X  routing subtask). This way, we obtain more accurate and unbiased measure-ments, since a list of the best 1000 scoring documents can be easily populated when a topic of interest has a far larger number of relevant documents. Furthermore, such a list can be biased towards one of the topics in a combination, because it can be dominated by the topic X  X  relevant docu-ments at the expense of documents relevant to the rest of the topics. For similar reasons, we preferred Reuters-21578 and not the more recent RCV1, because the latter causes eval-uation problems due to the very large number of relevant documents per topic in the collection [14].

Overall, the methodology defines a challenging IF task that more accurately reflects the problem X  X  complexity and proposes an alternative to existing practices. As the number of topics of interest increases from one to five the necessary number of profile terms also increases. Our aim is to exper-imentally support our argument regarding the effectiveness of the proposed model in comparison to the VSM when this happens.
The documents in the collection are first pre-processed using stop word removal and stemming with Porter X  X  algo-rithm. We then used Information Gain (IG) [4] to weight the remaining terms in the training documents. Terms with positive weight were extracted to build a user profile for each topic or topic combination. Two different types of pro-file were constructed. The baseline profile is a vector-based Figure 2: AUP scores for the five experiments: one-topic to five-topic.
 profile comprising the extracted weighted terms. The sec-ond type, is the proposed network-based profile comprising exactly the same weighted terms, but an additional process is deployed to generate the links between profile terms and calculate their weights. In particular, a sliding window ap-proach is used to define the context of terms in text and identify their co-occurences. The window defines a span of 10 contiguous terms 3 . Every time two profile terms appear within the window in the training documents, a link between them is established. The weight w kn of the link between two terms k and n is calculated using the following equa-tion, which is similar to the one adopted in [12], extended with an additional factor based on the average distance, i.e., the number of terms that intervene between k and n in the sliding window. fr k and fr n are respectively the number of occurrences
To evaluate each document in the collection, the same sliding window is deployed, so that only terms found in the same context get activated. Terms in the document are not weighted. Every position of the window defines a portion of the document X  X  text. In the case of the vector-based profile, we assign a relevance score to the window by calculating the inner product between the profile and a keyword vector com-prising the terms in the window. The network-based profile assigns a relevance score to the window using the proposed spreading activation process. In both cases, the result of the document evaluation process is a relevance score for each po-sition of the window in the document X  X  text, which indicates the distribution of relevance through out the document. For practical purposes though, we calculate a single relevance score for each document as the sum of the individual win-dow scores, normalised to the logarithm of the number of terms in the document. It is important to note, that since bothtypesofprofilecompriseexactlythesameweighted terms, any difference in their performance is due only to the additional relevance that links contribute to documents, accordingtoequation1.
Figure 2 includes five graphs one for each of the single-topic, two-topic, three-topic, four-topic and five-topic ex-periments. The x-axis shows the serial numbers of topics or topic combinations (e.g., 1:2 corresponds to earn:acq) and
The window X  X  size was chosen based on systematic experi-ments. Figure 3: Average AUP for different numbers of profile terms. the y-axis the corresponding AUP value, or combined AUP value in the case of multi-topic experiments. Table 2 sum-marises for each of the five experiments the average percent increase, the standard deviation, the p value of the paired, two-tailed t-test, and the average number of profile terms.
The results clearly show that as the number of topics of interest increases, causing an increase in the number of profile terms, the network-based profile achieves significant performance improvements of up to 50.2% on average, over the vector-based profile. These differences are consistent through out the 23 topics, as highlighted by the standard deviation, and are statistically significant, since all the p val-ues of the t-test are less than 0.05. It is also evident, that the difference is more pronounced for topics with a small number of relevant documents in the collection, where specificity is more important. As expected, the achieved AUP values get smaller as the number of topics of interest increases, because the IF task becomes more difficult.

The observed differences in performance are not only sub-stantial, but also of computational interest, because they are due only to the existence of links in the network-based pro-file, since both types of profile comprise exactly the same weighted terms. Furthermore, although not reported here due to space limitation, we have also performed the same experiments with a different term weighting method, called Relative Document Frequency (RelDF) [13]. The results achieved for RelDF were overall worse than those presented here for IG, but the network-based profile achieved improve-ments of up to 75% on average, over the vector-based pro-file 4 . It is also evident, that the observed differences in AUP scores relate to the increase in the number of profile terms required to represent multiple topics of interest.
To further investigate this effect we repeated the exper-iments for different numbers of profile terms. In particu-lar, we present here results for the three-topic experiments, where we progressively increase a threshold and we only ex-tract from the training documents terms with weight larger than this threshold. Table 3, summarises the average AUP values of the vector-based and network-based profiles for dif-ferent numbers of profile terms, the percent increase, the standard deviation and the p value of the paired, two-tailed t-test. Figure 3 presents a plot of the average AUP (columns 3 and 4 in table 3) on the y-axis, as the average number
All experimental results can be found at http://www.scribd.com/IF SIGIR10 Table 3: 3-Topic Experiment: overall results for dif-ferent threshold values of profile terms (column 2 in table 3) increases on the x-axis. When the number of profile terms is small the AUP scores of the two types of profile are also small, because there is a limited scope for capturing three topics using a small number of terms. As the number of profile terms in-creases the AUP scores of both profile types increases, but in the case of the vector-based profile the curve quickly flat-tens. Approximately, after extracting the first 400 terms with the highest weights the remaining 2000 terms do not essentially contribute to the profile X  X  accuracy. On the con-trary, the AUP score of the network-based profile increases more rapidly with the increase in the number of terms and keeps on increasing until all terms with positive weight have been incorporated. The p-values show that as the number of profile terms increases the confidence in the comparison also increases. Although not reported here due to space limita-tions, these differences are more pronounced for more topics of interest.

These findings are revealing, because they demonstrate that the network-based profile can effectively incorporate a large number of terms (or features in general). Unlike the vector-based profile that does not distinguish between rele-vant and non-relevant term combinations, the term network can exploit the additional information about the user X  X  in-terests, that a large number of terms and their correlations encode. We are confident that these findings generalise be-yond textual information, to any type of information that can be described or associated with correlated features. A user profile that is  X  X esistant X  to dimensionality problems can not only represent multiple topics of interest more ac-curately than a vector-based profile, but in principle, it can incorporate a greater diversity of features, including context dependent ones. A large number of features is no longer a problem, but an advantage that can be exploited to incor-porate, in the profile, additional features (such as tags) that have been assigned to, or can be automatically extracted from information items. In this way, the scope of IF can be easily extended beyond textual information and the speci-ficity of a user profile can be augmented.
The problem of information overload still impedes the dis-semination of information on today X  X  Web, where everyone can be both a receiver and a transmitter of information. IF has an important role to play in achieving personalised information delivery to ensure that the right information reaches the right people. However, unlike the success sto-ries of Collaborative Filtering that produced popular appli-cations for recommending books, movies and music tracks, content-based IF has not yet lead to the development of widely adopted Web applications for personalised informa-tion delivery. In this paper, we argued that one possible explanation is the reliance on the VSM, which leads to in-herent dimensionality problems. Instead, we proposed an alternative network-based model for profile representation that, in the case of textual information, captures correla-tions between terms appearing in the same context. The network profile can evaluate any portion of text with a di-rectional spreading activation process.

We performed a series of experiments comparing the net-work profile to a vector-based profile containing the same weighted terms. Unlike existing evaluation practices, our ex-perimental methodology simulates users with multiple con-current interests. Representing multiple topics of interest requires a large number of profile terms and reveals the di-mensionality problems of the VSM. The existence of links allows the network profile to capture additional information about the user X  X  interests, become more specific and achieve significant performance improvements. We specifically in-vestigated the effect of the number of terms on the profile X  X  performance and found out that after just a few hundred terms the vector-based profile reaches its maximum repre-sentational capacity, while the network profile can effectively incorporate thousands of terms.

This is a significant property that extends beyond tex-tual information and terms as features. We envision user profiles that do not only incorporate the necessary number of terms for representing a user X  X  multiple interests, but are also hybridised with additional features, such as tags or even user ratings. In this way, the user profile will become more specific to the user X  X  interests and will enable a variety of personalisation services. Given that the proposed model is also distributed and dynamic, it proposes a new perspec-tive towards IF and may establish a new research paradigm. This paper is part of ongoing effort towards this direction, that involves further experiments, theoretical analysis and real world prototype implementations. [1] C. C. Aggarwal, A. Hinneburg, and D. A. Keim. On [2] T. Ault and Y. Yang. knn, rocchio and metrics for [3] R. Bellman. Adaptive Control Processes: A Guided [4] L.Breiman,J.H.Friedman,R.A.Olshen,andC.J.
 [5] P. W. Foltz. Using latent semantic indexing for [6] W. P. Jones and G. W. Furnas. Pictures of relevance: [7] C. McEwan and E. Hart. Representation in the [8] N. Nanas and A. De Roeck. Autopoiesis, the immune [9] N. Nanas, S. Kodovas, and M. Vavalis. Revisiting [10] N. Nanas, M. Vavalis, and A. De Roeck. What [11] N. Nanas, M. Vavalis, and L. Kellis. Immune learning [12] Y. C. Park and K.-S. Choi. Automatic thesaurus [13] M. F. Porter. Implementing a probabilistic [14] S. Robertson and I. Soboroff. The TREC 2001 [15] G. Salton and M. J. McGill. Introduction to Modern [16] M. Sanderson and B. W. Croft. Deriving concept [17] R. Schapire, Y. Singer, and A. Singhal. Boosting and [18] H. Sorensen, A. O X  Riordan, and C. O X  Riordan. [19] C. J. van Rijsbergen. A theoretical basis for the use of [20] G. I. Webb, M. J. Pazzani, and D. Billsus. Machine [21] D.H.Widyantoro,T.R.Ioerger,andJ.Yen.An [22] Y. Yang, A. Lad, N. Lao, A. Harpale, B. Kisiel, and [23] Y. Yang and J. O. Pedersen. A comparative study on [24] Y. Zhang. Using bayesian priors to combine classifiers
