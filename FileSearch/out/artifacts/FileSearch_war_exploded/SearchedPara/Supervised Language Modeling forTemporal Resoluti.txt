 We investigate temporal resolution of documents, such as deter-mining the date of publication of a story based on its text. We describe and evaluate a model that build histograms encoding the probability of different temporal periods for a document. We con-struct histograms based on the Kullback-Leibler Divergence be-tween the language model for a test document and supervised lan-guage models for each interval. Initial results indicate this language modeling approach is effective for predicting the dates of publica-tion of short stories, which contain few explicit mentions of years. H.3.1 [ Content Analysis and Indexing ] Algorithms, Design, Experimentation, Performance time, temporal information, text mining, document dating
There is a long tradition of work on temporal analysis of texts. In computational linguistics, the primary focus has been on the fine-grained level of temporal ordering of individual events necessary for deep natural language understanding [1, 24]. In Information Retrieval (IR), research has investigated time-sensitivity document ranking [9, 17], time-based organization and presentation of search results [2], how queries and documents change over time [16], etc.
This paper describes temporal analysis of documents using meth-ods rooted in both computational linguistics and IR. While accu-rate extraction and resolution of explicit mentions of time (absolute or relative) is clearly important [2], our primary focus here is the challenge of learning implicit temporal distributions over natural language use at-large. As a concrete example, consider the word wireless . Introduced in the early 20th century to refer to radios, it fell into disuse and then its meaning shifted later that century to describe any form of communication without cables (e.g. internet access). As such, the word wireless embodies implicit time cues, a notion we might generalize by inferring its complete temporal distribution as well as that of all other words. By harnessing many such implicit cues in combination across a document, we might fur-ther infer a unique temporal distribution for the overall document.
As in prior document dating studies, we partition the timeline (and document collection) to infer an unique language model (LM) underlying each time period [10, 14]. While prior work consid-ered texts from the past 10-20 years, our work is more historically-oriented, predicting publication dates for historical works of fiction.
After estimating a similar LM underlying each document [20], we measure similarity between each document X  X  LM and each time period X  X  LM, yielding a distribution over the timeline for each doc-ument [10, 14]. In addition to document dating applications, this distribution has potential to inform work in computational humani-ties, specifically scholars X  understandings of how a work was influ-enced by or reflects different time periods.

We predict publication dates of short stories from the Gutenberg project 1 published between 1798 to 2008. Gutenberg stories are labeled by publication year. Our inference task is then to predict the publication year given the story X  X  text. These short works of fiction typically use relatively few explicit temporal expressions or mentions of real-life named entities. We refer to the stories as docu-ments , with Gutenberg defining a document collection c consisting of N documents: c = d 1: N . Our best model achieves a median error of 23 years from the true publication dates.
Annotation and corpora. Recent years have brought increased interest in creating new, richly annotated corpora for training and evaluating time-sensitive models. TimeBank [21] and Wikiwars [19] are great exemplars of such work. They have been used for tasks like modeling event structure (e.g. work of [7] on TimeBank).
Document dating. Prior work on LM-based document dating [10, 14] partitioned the timeline (and document collection) to infer a unique LM underlying each time period. A LM underlying each document [20] was also estimated and used to measure similarity vs. each time period X  X  LM, yielding a distribution over the timeline for each document. However, while prior work focused on the past 10-20 years, our work is more historically-oriented, modeling the timeline from the present day back to the 18th century.

Foundational work by de Jong et al. [10] considered Dutch news-paper articles from 1999-2005 and compared language models us-ing the normalised log-likelihood ratio measure (NLLR), a vari-ant of KL-divergence. Linear and Dirichlet smoothing were ap-http://www.gutenberg.org plied, apparently to the partition LMs but not the document LMs. They also distinguish between output granularity of time (to be pre-dicted) and the granularity of time modeled. Kanhabua et al. [14] extended de Jong et al. X  X  model with notions of temporal entropy, use of search term trends from Google Zeitgeist, and semantic pre-processing. Temporal entropy weights terms differently based on how well a term distinguishes between time partitions and how im-portant a term is to a specific partition. Semantic techniques in-cluded part-of-speech tagging, collocation extraction, word sense disambiguation, and concept extraction. They created a time-labeled document collection by downloading web pages from the Internet Archive spanning an eight year period. In follow-on work inferring temporal properties of queries [15], they used the New York Times annotated corpus, with articles spanning 1987-2007.

IR Applications. IR research has investigated time-sensitivity query interpretation and document ranking [17, 9], time-based or-ganization and presentation of search results [2], how queries and documents change over time [16], etc. One of the first LM-based temporal approaches by Li and Croft [17] used explicit document dates to estimate a more informative document prior. More recent work by Dakka et al. [9] automatically identify important time in-tervals likely to be of interest for a query and similarly integrate knowledge of document publication date into the ranking function. The most relevant work to ours is that by Alonso et al. [2], who provide valuable background on motivation, overview and discus-sion of temporal analysis in IR. Using explicit temporal metadata and expressions, they create temporal document profiles to cluster documents and create timelines for exploring search results.
Time-sensitive topic modeling. There has been a variety of work on time based topic-analysis in texts in recent years, such as Dynamic Topic Models [6]. Subsequent work [5] proposes proba-bilistic time series models to analyze the time evolution of topics in a large document collection. They take a sequential collection of documents of a particular area e.g. news articles and determine how topics evolve over time -topics appearing and disappearing, new topics emerging and older ones fading away. [25] provide a model to evaluate variations in the occurrence of topics in large corpora over a period of time. There have been other interesting contributions such as work by [18] which studies the history of ideas in a research field using topic models, by [8] which provides the temporal analysis of blogs and by [27] which gives models for mining cluster evaluation from time varying text corpora.
Geolocation. Temporal resolution can be seen as a natural pair-ing with geolocation: both are ways of connecting texts to simple, but tremendously intuitive and useful, models of aspects of the real world. There has been a long-standing interest in finding ways to connect documents to specific places on the earth, especially for geographic information retrieval [12, 3]. Of particular relevance to our paper is Wing and Baldridge X  X  LM based method of measuring similarity of documents with language models for discrete geodesic cells on the earth X  X  surface [26].

Authorship attribution. A final relevant work is the LM-based authorship attribution work by Zhao et al. [28]. They similarly par-tition the corpus by author, build partition-specific LMs, and infer authorship based on model similarity computed with KL-divergence and Dirichlet smoothing. They also consider English literature from the Gutenberg Project. Unlike us, they train directly on this corpus instead of applying LMs from another domain.
Following aforementioned prior work [2, 10, 14], we quantize continuous time into discrete units. Our terminology and formal-ization most closely follow that of Alonso et al. [2]. The small-est temporal granularity we consider in this work is a single year, though the methods we describe can in principle be used with units of finer granularity such as days, weeks, months, etc.

A chronon is an atomic interval x upon which a discrete time-line is constructed [2]. In this paper, a chronon consists of  X  years, where  X  is a tunable parameter. Given  X  , the timeline T  X  posed into a sequence of n contiguous, non-overlapping chronons x = x 1: n , where n =  X   X  .

We model the affinity between each chronon x and a document d by estimating the discrete distribution P ( x | d ) , a document-specific normalized histogram over the timeline of chronons x 1: n next section, we use P ( x | d ) to infer affinity between d and differ-ent chronons as well as longer granules. We describe an LM-based approach for inferring affinity between chronon x and document d as a function of model divergence between latent unigram distribu-tions P ( w | d ) and P ( w | x ) (similar to [10, 14]).
We model the likelihood of each chronon x for a given docu-ment d . By forming a unique  X  X seudo-document X  d x associated with each chronon x , we estimate  X  x from d x and estimate P ( x | d ) by comparing the similarity of  X  d and  X  x [10, 14]. Whereas prior work measured similarity via NLLR ( X 2), we compute the unnor-malized likelihood of some x given d via standard (inverse) KL-divergence D ( X  d ||  X  x )  X  1 and normalize in straight-forward fash-ion over all chronons x 1: n : We generate the  X  X seudo-document X  d x for each chronon x by in-cluding all training documents whose labeled span overlaps x .
As in prior work [10, 14, 28], we adopt Dirichlet smoothing to regularlize partition LMs. However, rather than adopt a sin-gle fixed prior for all documents and chronons, we instead de-fine document-chronon specific priors. Let | V d x  X  V d document-chronon specific vocabulary for some collection docu-ment d i and pseudo-document d x . For each document-chronon pair, we define the prior to be a uniform distribution over this spe-cific vocabulary: 1 | V document d , we perform Dirichlet smoothing of form: Rather than specify the hyper-parameter  X  directly, however, we introduce another hyper-parameter  X  , where  X  =  X  | V
The intuition for this smoothing is that mass is provided only for the words that are present in the collection document or the chronon X  X  pseudo-document, ignoring other words. Thus, the di-vergence calculation is done only with respect to either the docu-ment or the chronon we have evidence for. There are many chronons for which we have little textual evidence; if these are smoothed with respect to all words in the collection, then those terms dominate the divergence calculation. When a short document is evaluated against a low-evidence chronon, smoothing over all words leads to many terms (few of which actually occur in the document or the chronon) having similar probabilities, leading to low divergence. To infer a representative chronon x d for each document, M AX -C
HRONON simply selects the most-likely chronon under P ( x | d ) : We use M AX C HRONON to predict a chronon for the document and then select the first year in that chronon as the predicted year.
Data. We use 678 Gutenberg short stories, split into devel-opment and test sets of 333 and 345 each, respectively. Strictly speaking, this split is not actually necessary for our reported ex-periments since we tune out-of-domain on a separate corpus (see below). Nonetheless, we also plan to tune in-domain in our future work, so we have gone ahead and used the division now for direct comparison to later results. The average, minimum and maximum word count of these stories are 14k, 11k and 100k respectively. All numeric tokens and standard stopwords are removed.

Wikipedia Biographies . We tune parameters using an external corpus of biographies from the September 4, 2010 dump of English Wikipedia 2 (our use of Wikipedia here is motivated by other pre-liminary work on it to be further developed in our future work). We consider individuals whose complete lives occurred within the year range 3800 B.C. to 2010 A.D. We extract the lifetime of each indi-vidual via each article X  X  Infobox birth_date and death_date fields. We exclude biographies which do not specify both fields or which fall outside the year range considered. Given the lifetime of each individual, we take the midpoint of individual X  X  lifetime as the gold standard year to match for the biography. We note that as is often typical of Wikipedia coverage, the distribution of biographies is quite skewed toward recent times.

As examples of the kinds of distributions we obtain for Wikipedia biographies, Figure 1 shows graphs of P ( x | d ) for (a) Plato and (b) Abraham Lincoln. Recall that these are based on no explicit tempo-ral expressions. For Plato, there is a clear spike around the time he was alive, along with another rising bump toward the current day, reflecting modern interest in him. For Lincoln, there is a single peak at 1835 X  X ery close to the 1837 midpoint of his life.
Parameters and Estimation . We set W ORD A FFINITY  X  X  smooth-ing parameter  X  = 1 without any tuning (left for future work). The chronon size  X  was tuned through experimentation to optimize performance on the Wikipedia biography collection. Two salient points of note are: (a) the task will be more challenging since we have a tune/test corpus mismatch between Wikipedia vs. Guten-berg, and (b) a particular challenge of this is differences in vocabu-lary selection between recently written Wikipedia articles and his-torical fiction in the Gutenberg stories.

As in prior work [10, 14], we smooth chronon pseudo-document language models but not document models. While smoothing both may potentially help, smoothing the former is strictly necessary to prevent division by zero in the KL-divergence calculation.
Year Prediction . The gold standard to match for each document is a labeled year (publication year for Gutenberg, lifetime midpoint for Wikipedia). Given document d , we predict the year to be the first year from x d mc , the most-likely chronon for d over the timeline.
When predicting a single year for a document, a natural error measure between the predicted year  X  y and the actual year y difference |  X  y  X  y  X  | . We compute this difference for each document, then compute and report the mean and median of differences across documents. Similar distance error measures have also been used with document geolocation [13, 26].

Baseline . As a simple baseline, we consider a fixed prediction of the year 1903 for all stories: the midpoint of publication date range (1798-2008) in the collection. This assumes that one knows a rough range of possible publication dates, which might be reasonable in many cases and provide a useful starting point for comparison.
Development Set Tuning and Results . Using the Wikipedia biographies, we tune  X   X  X  1 , 2 ,..., 100 } and examine mean error Figure 2: Chronon size  X  vs. resultant mean error of year pre-dictions for lifespan midpoints in Wikipedia biographies. in predicted year. Error roughly varies between 85-155, with  X  = 40 seen to yield optimal mean year prediction error (see Figure 2). Next, we use  X  = 40 on the Gutenberg collection. In comparison to the 1903 baseline, mean prediction error is reduced from 36 to 28 years, with median prediction error cut from 50 to 32 years.
Test Set Results . While results show only a modest reduction of mean error in year predictions for the model vs. the 1903 baseline (37 to 34), median error is more than halved, dropping from 50 to 23. This provides a promising validation of our initial work that words which are not temporal expressions still convey important temporal information that can be exploited for historical document dating, even when (a) the model was trained on a different domain and (b) there is only a range of 210 years under consideration and the baseline represents the exact midpoint.
We have shown that it is possible to perform accurate temporal resolution of texts by combining evidence from both explicit tem-poral expressions and the implicit temporal properties of general words. We create a partitioned timeline with language models in each partition; these are used to determine a probability over the timeline for new test documents. The models are effective enough that they can predict publication dates on out-of-domain stories from a 210 year period to within 32 years on average.

There are a number of ways to improve the present approach. For example, we might smooth LMs for the chronons themselves such that we infer similar temporal distributions proportional to proxim-ity between chronons. Similar intuition has appeared in LM-based IR work in various forms: similar queries should have similar LMs, similar documents should have similar LMs [23], and similar doc-uments should receive similar scores when compared to to some other fixed model [11].

Another, obvious idea is to consider n-grams rather than uni-grams: e.g., civil war has different temporal properties from civil and war on their own. Ultimately, one of our interests in this gen-eral line of inquiry is to create models of word meaning that are grounded . In most computational models of word meaning, a word is characterized in terms of other words X  X  circular (though use-ful) model. However, there is recent interest in models that connect words to properties of the real world, such as conceptual represen-tations [4] and models of space [22] and time. A relevant task for this idea is word sense disambiguation. For example, the word ap-ple could be a reference to the fruit (general over time), the record company formed by the Beatles (1960-1970s, primarily), or Ap-ple Inc. (1976-present). Identifying the temporal properties of the text can be part of disambiguating such terms and help us keep a computer or record company from falling on Isaac Newton X  X  head. We thank the anonymous reviewers for their valuable feedback and suggestions. This work was partially supported by a John P. Com-mons Fellowship for Matthew Lease and by a grant from the Morris Memorial Trust Fund of the New York Community Trust.
