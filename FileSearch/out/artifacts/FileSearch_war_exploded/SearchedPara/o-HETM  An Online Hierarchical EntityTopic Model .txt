 Recently, once a thing of great concern such as  X  X he Missing Flight MH370 X  hap-pens, massive news articles 1 covering a wide spectrum of aspects ranging from the missing situation to possible causes, to investigation and searching will be contin-ually published. Though many web sites have created a  X  X pecial Topic X  web page for organizing all the related news article URLs together, it is still impossible for users to go through all the contents and manually identify the newly emerging and important sub-topics. This necessitates a dynamic structured summarization for the streaming news. Using a hierarchical structure which presents topics at differ-ent levels of granularity can enable the users to feel free to find whatever they are interested as soon as possible.
 popular researches is topic detection which aims to discover the topics reported in news articles and group them in terms of their topics [ 1  X  3 ]. Most of the state-of-art topic detection systems are based on probabilistic generative models, clus-tering techniques and Vector Space Model (VSM) model. Under the frameworks, researchers have proposed several practical methods such as LDA (Latent Dirichlet Allocation), single-pass clustering and incremental K-means clustering etc. However, most of them focused on flat topical structures. Some researches [ 4 ]con-structed hierarchical document-level clusters instead of hierarchical theme-level topics. Hierarchical topic models have been successfully applied for topic hierar-chy construction in text mining [ 5 , 6 ]. Considering that the news is time sensitive and named entities are critical in conveying when, where, who in news articles, we present a new hierarchical entity topic model as well as an online inference method for streaming news.
 subtopics from the streaming news articles about a special topic. For example, Fig. 1 shows part of the topic hierarchy about  X  The Missing Flight MH370  X . Each topic is represented as ranked lists of words and entities. The task is not trivial due to various challenges in analyzing streaming news data. First, the whole data cannot be fit into memory at once and has to be processed incrementally. Second, the algo-rithm need to be efficient due to real-time response rate requirements. In addition, news data is time-sensitive and puts emphasis on named entities (persons, loca-tions, organizations and time) which convey the information of when, where and who [ 7 ]. These challenges necessitate algorithms that should meet the following demands: 1) be incremental; 2) run in real time; 3) take the arriving time of news documents into consideration; 4) model the relationship between topics and enti-ties. To this end, we develop an online Hierarchical Entity Topic Model (o-HETM) to automatically construct topic hierarchies from fast-coming news streams. Specif-ically, we incorporate the time factor into the well-known nCRP (nested Chinese Restaurant Process) and construct a non-parametric hierarchical topic model based on it. The topic model distinguishes entities from topics and models the relation-ship between topics and entities. To adapt the model to online news streams, we present an online Gibbs sampler for fast inference. The main contributions of this work can be summarized as follows: (1) We propose a nonparametric hierarchical entity topic model HETM for topic hierarchy construction from news data. The model considers the timeliness of news data and the relationship between topics and entities. (2) We propose online HETM (o-HETM) by presenting a fast online inference algo-rithm for HETM to adapt it to streaming news data. (3) Experimental results on real datasets demonstrate that our model HETM sig-nificantly improves the topic quality and time efficiency, compared to HLDA (Hierarchical Latent Dirichlet Allocation). With the online inference algorithm based on Gibbs sampling, o-HETM further improves the time efficiency dra-matically and can be used for streaming news. In this section, we detail our proposed online nonparametric hierarchical entity topic model o-HETM for dynamic topic hierarchy construction from news streams. We first introduce the Time-Dependent nCRP which considers the timeliness of news data for constructing nonparametric hierarchical topic models in Section 2.1 . Then we present the graph representation and document generative process of HETM basedonthe Time-DependentnCRP in Section 2.2 . The model combines the advan-tages of the hierarchical topic model HLDA [ 8 ] and the entity topic model CorrLDA [ 9 ]. Last, we construct the model o-HETM by presenting an online inference algo-rithm for HETM in Section 2.3 . 2.1 Time-Dependent nCRP As described in HLDA [ 8 ], the nCRP places priors over trees and does not limit the branching factors or the depths of the trees. It supposes that there are an infinite number of infinite-table Chinese restaurants. A customer enters the root restaurant with infinite tables where each refers to another restaurant and each restaurant is referred to exactly once. He chooses a table according to CRP (Chinese Restau-rant Process) which is a  X  X referential attachment X  way. Specifically, the ( n +1) customer chooses a new table with probability  X  n +  X  and chooses an already exist-ing i th table with probability proportional to the number of people sitting there, namely n i n +  X  . Then he reaches another restaurant which is referred to by this table. And so forth, the structure repeated infinite times will reach an infinitely branched and deep tree.
 In this paper, considering that news data is time-sensitive, we present a variant of nCRP , Time-Dependent nCRP . It supposes that the much former customers will have less influence to the current customer [ 10 ]. Therefore, in our Time-Dependent nCRP , we use a common time discount function in drawing a table for a customer. We use t to denote time (period). The ( n +1) th customer at time t chooses a new table with probability  X  n +  X  where n denotes the discounted number of customers before time t . He chooses an already occupied table with probability n time t and n is the total discounted number of customers at time t .Thisexpres-sion defines a time-decaying function parameterized by  X  (width) and  X  (decay factor). The Time-Dependent nCRP supposes that only the previous customers in the time period from t  X   X  to t will influence the current customer at t .The parameter  X  controls the decay rate of the influence of previous customers with time. When  X  =0,the Time-Dependent nCRP only considers the previous cus-tomers at current time t . When  X  = t and  X  =  X  , it degenerates to a common nCRP without considering the factor of time. 2.2 Online Hierarchical Entity Topic Model In this section, based on the Time-Dependent nCRP , we can construct our hier-archical entity topic model HETM. Considering a document as a customer, the model first chooses a table for the customer in the root restaurant. Then the cus-tomer enters another restaurant at deeper level referred to by the table. And so forth, we can finally get a path for the document and each node on the path repre-sents a topic. The depth of the infinite tree is controlled by Stick-Breaking Process parameterized by ( m,  X  )inwhich m  X  (0 , 1) controls the mean of the stick lengths and  X &gt; 0 determines the variance of the stick lengths [ 8 ]. The process supposes that there is a stick whose length equals to 1. We sample stick lengths ranging in (0 , 1) according to V i Beta ( m X , (1  X  m )  X  ). In most applications, we fix the depth of a tree as L . Therefore, for the last level L , the stick length is 1 These lengths correspond to the probabilities of topics on the path and form the prior distribution of the document over the topics. Then words are sampled from the L topics along that path. After sampling all the words of the document, entities are drawn according to the topics of words.
 graph representation of our hierarchical entity topic model HETM is illustrated in Figure 2 . The model assumes a tree with infinite branches but fixed depth of L levels. It combines the advantages of the hierarchical topic model HLDA [ 8 ]and the entity topic model CorrLDA [ 9 ][ 7 ], namely, it not only generates a structured topic hierarchy for the news data but also models the relationship between topics and entities. Thus, it can better fit the news data.
 1. For each table k  X  T in the infinite tree, draw a word distribution  X  and entity distribution  X   X  k  X  Dir (  X   X  ) 2. For each document d  X  S  X  X raw c d  X  Time  X  Dependent nCRP (  X ,  X  )  X  draw a distribution over levels in the tree,  X  d | m,  X  3. For each word w  X  d  X  choose a level z d,n |  X  d  X  Mult (  X  d )  X  X hooseaword w d,n | z, c d , X   X  Discrete (  X  c d [ z d,n 4. For each entity e  X  d  X  choose a level  X  z d,n  X  Unif ( z w 1 ,z w 2 , ..., z  X  choose an entity e d,n  X  Discrete (  X   X   X  z d,n ) As shown in the generative process, first, we associate each table (topic) in the tree with a prior word distribution and entity distribution. When generating a docu-ment d , we first determine its path according to the Time-Dependent nCRP process parameterized by  X  and  X  . Then the topic distribution of the document is drawn from a stick-breaking process parameterized by ( m,  X  ). The following process of generating words and entities is similar to the entity topic model CorrLDA [ 9 ]. First, we sample topics for words w and then sample topics for entities e based on the sampling results of word-topic assignments. Therefore, we can learn the rela-tionship between topics and entities. 2.3 Online Inference Algorithm In this section, we introduce the online inference algorithm for our model to fit the streaming news. We apply Gibbs sampling which uses p ( z the intractable posterior distribution p ( z | w ). The desired Gibbs sampler runs a Markov chain for enough iterations and then it converges to the desired posterior distribution. Inspired by the online LDA [ 11 ], we have the following extension for our o-HETM: In Algorithm 1 , we first apply batch Gibbs Sampler [ 8 ] on the first 10% data because its content can cover most content of the later coming data [ 11 ] (Line 1).
Algorithm 1. Online Gibbs Sampler for o-HETM Afterwards, we sample topics of each new word by conditioning words of previous documents observed so far (Lines 2-7). To improve the accuracy of topics, we resam-ple the topics of some previous words (Lines 8-10). The probability distribution p ( c d | w , c  X  d , z ) is used for path sampling, where c before time of document d , p ( z i | z  X  i , w ), p (  X  z pling where z  X  i denotes all the topics of all words except the i They can be derived as: where the first term is the prior on paths implied by the Time-Dependent nCRP and the second term denotes the probability of the data given a particular choice of path. We can refer to [ 8 ] for details. For sampling topics for words: where the first term is the conditional topic distribution given all other words X  mined by the prior distribution and the word-topic assignments. The prior distri-bution is the truncated stick breaking process parameterized by m,  X  where larger m indicates a larger probability of higher levels. The second term is the word dis-tribution given all the other variables. After sampling all the words, we sample levels for entities according to the already known topic distribution among words and the distributions of the topics over enti-ties. Formally, Due to space limitation, we don X  X  present how to commutate the distributions in detail. For more details, we can refer to HLDA [ 8 ]. Key sentences are selected to form a summary for each topic. We note that the title of a news article briefly summarizes the content of the article. Therefore, we utilize the titles of the news articles belonging to the same topic to construct the candi-date sentences for the topic X  X  summary. The representative sentences are selected as follows. First, a topic signature word set TW z and entity set TE by extracting top 10 words and 10 entities with highest p ( w for each sentence s ,awordset TW s and an entity set TE s informative words (i.e., noun, verb, adj, and adv.) and entities from the the sen-tence [ 12 ]. Third, each sentence is ranked by measuring the weighted average score of the similarity between its word set and the topic signature word set and the sim-ilarity between its entity set and the topic signature entity set. Jaccard Similarity is used as the similarity metric. Formally, Sim ( s, z )=  X   X   X  Sim Jac ( TE s ,TE z ). The higher the similarity, the higher the rank. The weight parameters  X  1 and  X  2 allow the users to freely control the importance of entities compared to words. In this work, they are empirically set as 0.4 and 0.6 respectively to emphasize entities more. 4.1 Datasets Due to restrictions of data crawling on many websites, it is difficult to collect data for our experiments. We collect three news datasets about different topics. The first dataset (in Chinese) and the third dataset (in English) were respectively crawled from the news agency Sohu and Sina , while the second dataset (in English) was collected from the well-known news agency The Guardian . For each news docu-ment, we keep the publication time, title and body content. For all the datasets, we sort the documents by their publication time and perform preprocessing as follows: 1) word segmentation (for only Chinese dataset) and entity recognition with ICT-CLAS or StandfordNER 3 ; 2) removal of stop words (e.g., X  X  X ,  X  X he X ,  X  X f X , etc.). Statistics of the three datasets including the number of documents, vocabulary size and entity vocabulary size after preprocessing are summarized in Table 2 . 4.2 Experimental Setup Our evaluation of the efficacy of our proposed online hierarchical entity topic model is threefold: 1) comparison with state-of-the-art method HLDA implemented by Chua et al. [ 13 ]; 2) comparison with gold standards constructed according to the manually created tables of contents in related Wikipedia articles; 3) a case study. Comparison with State-of-the-art Methods. For fair comparison, on one hand, the common hyper-parameters shared by these methods are set as the same, ies [ 8 ]. For additional hyper-parameters  X   X  of HETM and o-HETM, we set them as same as  X  . We set the maximum level of all the models to 3. On the other hand, as HLDA didn X  X  consider the factor of time, we set the time width as  X  = t and the decay factor as  X  =  X  which makes Time-dependent nCRP degenerate to nCRP. We compare our models HETM and o-HETM with HLDA in terms of time effi-ciency and topic coherence.
 ComparisonwithGoldStandards. We construct a gold standard for each topic by leveraging the Contents table in Wikipedia articles (e.g.,  X  2010 Chile earth-quake  X  4 ). The Contents tables summarize the topics about the thing that the title stands for. We ask five college students to work together to pick up the effec-tive topics which forms the gold standard hierarchy. Only when all of them reach a consensus, we will consider a topic in the tables of contents as effective. Then we in our results with the the gold standards manually.
 CaseStudy. For qualitative analysis of topic hierarchies generated by our method, we present the result of the largest dataset  X  2012 US Election  X  generated by our method as a case study. 4.3 Evaluation Metrics TopicCoherence. We use topic coherence to evaluate the topic quality [ 14 ]. Given a list of words, the more often the words co-occur, the larger the topic coherence is and the list is more likely to represent a topic. Formally, it is computed as described using top N words with the largest probabilities, W D ( w i ) is the document frequency of word w i , D ( w i frequency of word w i and w j . suppose .
 Recall. The recall of our topic hierarchy is defined as R ( H )=( where T H and T H S are respectively the topic sets of our topic hierarchy and the gold standard. 4.4 Results and Analysis Comparison with State-of-the-art Methods. Table 3 shows the average topic coherence scores and running time of different methods. As we can see, our model HETM significantly outperforms HLDA in terms of both topic quality and time efficiency. It shows that distinguishing entities from words can not only discover the relations between topics and entities but also improve the time efficiency. With the online inference algorithm, o-HETM further improves the time efficiency by more than 20-50 times. The time of dealing with a document reaches 2-50 milliseconds, which meets the demand of real-time news processing. However, without surprise, the topic quality of o-HETM is of inferior quality, compared to HLDA in terms of topic coherence. The significant improvement of the time efficiency is made at the expense of the topic quality.
 Comparison with the Gold Standards. The recall values of our topic hierar-chies about the three events  X  X he Missing Flight MH370 X ,  X 2012 US Election X  and  X 2010 Chile Earthquake X  are respectively 71.4%, 62.5% and 90.9%. Consid-ering that our topic hierarchies are generated from the real news data while the gold standard are constructed manually without referring to the news, the recall of 62.5%-90.9% has demonstrated the effectiveness of our method. For example, the gold standard about the  X  2012 US presidential election  X   X  primaries  X ,  X  campaigns  X  and  X  races  X , most of which can be found in our topic hierarchy as shown in Fig. 3 . In addition to that, our method discovers many pop-ular topics such as  X  tax  X  and  X  scandals  X  and provides a more complete view. The gold standard of  X  The Missing MH370  X  contains some specific topics that don X  X  occur a lot in news articles such as  X  electrical fire speculation  X  and thus cannot be discovered by our method. In future work, we can leverage the  X  table of contents  X  for semi-supervised topic modeling of news to improve our results. The recall vale on the dataset of  X 2010 Chile earthquake X  is the highest. There are 10 aligned top-ics (e.g., tsunami, damage, government response and so on). Only one topic, i.e.,  X  prison escape  X  is not in our topic hierarchy. However, our model discovers hot topics such as  X  copper  X .
 Case Study. Due to space limitation, we present only the main part of the topic hierarchy about the  X  2012 US election  X  in Fig. 3 . As we can see, the topic of  X  econ-omy  X  is the most hot topic, which includes more than 2 / 3 of the documents and has subtopics of  X  job  X  and  X  primaries  X . Another small but hot topic is  X  X ffair about sex X . It has subtopics of  X  sexual harassment  X  and  X  Corruption  X . Other top-ics which are not shown in the figure are in smaller size. In terms of topic summary, we can see an example that the most related news title to the topic of  X  X ob X  is  X  US politics live blog: Rick Perry X  X  jobs policy, New Hampshire v Nevada, Herman Cain X  X  9-9-9 tax plan  X . Overall, our results accord well with our common sense. However, some expected topics such as  X  debate  X  and  X  X oting X  are mixed with the root topic of  X  X ampaign X  as we can see the top 10 words contain  X  debate  X  and  X  voters  X . All the models have the problems. The reason may be that these topics are too related to each other and thus cannot be separated.
 Parameter Analysis. We also test the model with different values of the param-eters  X  and  X  , and find that the quality of the topic hierarchy is highly insensitive to the parameters. However, smaller  X  and  X  are likely to result in more specific topics. A user can choose different parameters according to the datasets and prac-tical demands. In terms of the parameter of online-algorithm, if Count is larger, the model will be more approximate to the algorithm of batch sampling, and will get better results at the expense of time. Topic discovery from streaming data has been studied a lot. For example, [ 2 ]pre-sented general probabilistic methods for discovering and summarizing the evolu-tionary patterns of themes in a text stream. Topic detection and tracking aims to discover topics and group the streaming documents in terms of their topics [ 1  X  3 ]. However, most of the studies focused on flat structures of the topics. Some studies focus hierarchical structures in streams [ 4 , 15 ], but their focus is document level clustering, while we perform theme level word clustering.
 Topic hierarchy construction is another problem relevant to our work. A lot of hierarchical topic models, e.g., HLDA [ 5 ], HPAM [ 6 ], and hHDP [ 16 ] have been successfully applied in text mining. Different from these studies, we focus on the streaming time-sensitive news data which puts emphasis on named entities, and incorporate the timeliness of news data and the relationship between topics and entities into hierarchical topic modeling. There is also considerable work on entity topic models [ 7 , 17  X  19 ]. However, they extract flat topic structures. Fitting a topic model given a set of documents requires approximate inference techniques that are computationally expensive. Therefore, our work is also rele-vant to studies about efficient inference of topic models [ 11 , 20 ]. Inspired by online inference with LDA [ 11 ], we develop an online inference algorithm for our o-HETM in order to deal with the streaming news in real time. In this paper, we present an online hierarchical entity topic model o-HETM to dynamically construct topic hierarchies from news streams. The model consid-ers the timeliness of news data and the relationship between topics and entities, which are very important for news data. The fast online inference algorithm signif-icantly improves the time efficiency of the model and thus adapt it to the streaming news. Extensive experiments have verified the effectiveness and efficiency of the proposed model, compared to the baseline model HLDA. In future work, we can investigate and visualize the hierarchical topic evolutionary patterns based on the current work.

