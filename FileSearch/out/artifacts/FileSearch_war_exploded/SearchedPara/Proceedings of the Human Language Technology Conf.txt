 ity scores to recorded call center con versations. The system com-bines speech recognition, pattern matching, and maximum entrop y classification to rank calls according to their measured quality . Calls at both ends of the spectrum are flagged as  X  X nteresting X  and made available for further human monitoring. In this process, the ASR transcript is used to answer a set of standard quality control questions such as  X  X id the agent use courteous words and phrases,  X  and to generate a question-based score. This is interpolated with the probability of a call being  X  X ad,  X  as determined by maximum entrop y operating on a set of ASR-deri ved features such as  X  X ax-imum silence length X  and the occurrence of selected n-gram word sequences. The system is trained on a set of calls with associated manual evaluation forms. We present precision and recall results from IBM X  s North American Help Desk indicating that for a given amount of listening effort, this system triples the number of bad calls that are identified, over the current polic y of randomly sam-pling calls. The application that will be demonstrated is a research prototype that was built in conjunction with IBM X  s North Ameri-can call centers. Ev ery day , tens of millions of help-desk calls are recorded at call centers around the world. As part of a typical call center operation a random sample of these calls is normally re-played to human monitors who score the calls with respect to a variety of quality related questions, e.g. This process suf fers from a number of important problems: first, the monitoring at least doubles the cost of each call (first an opera-tor is paid to tak e it, then a monitor to evaluate it). This causes the second problem, which is that therefore only a very small sample of calls, e.g. a fraction of a percent, is typically evaluated. The third problem arises from the fact that most calls are ordinary and uninteresting; with random sampling, the human monitors spend most of their time listening to uninteresting calls.
 that addresses these problems. Automatic speech recognition is used to transcribe 100% of the calls coming in to a call center , and def ault quality scores are assigned based on features such as key-w ords, key-phrases, the number and type of hesitations, and the average silence durations. The def ault score is used to rank the calls from worst-to-best, and this sorted list is made available to the human evaluators, who can thus spend their time listening only to calls for which there is some a-priori reason to expect that there is something interesting.
 part because of the variability in how hard it is to answer the ques-tions. Some questions, for example,  X  X id the agent use courteous words and phrases? X  are relati vely straightforw ard to answer by looking for key words and phrases. Others, howe ver, require es-sentially human-le vel kno wledge to answer; for example one com-pan y X  X  monitors are ask ed to answer the question  X  X id the agent tak e ownership of the problem? X  Our work focuses on calls from IBM X  s North American call centers, where there is a set of 31 ques-tions that are used to evaluate call-quality . Because of the high de-gree of variability found in these calls, we have investigated two approaches: We have found that both approaches are workable, and we present final results based on an interpolation between the two scores. These results indicate that for a fix ed amount of listening effort, the number of bad calls that are identified approximately triples with our call-ranking approach. Surprisingly , while there has been significant pre vious scholarly research in automated call-routing and classification in the call center , e.g. [1, 2, 3, 4, 5], there has been much less in automated quality monitoring per se. 2.1. Data The speech recognition systems were trained on approximately 300 hours of 6kHz, mono audio data collected at one of the IBM call centers located in Raleigh, NC. The audio was manually tran-scribed and speak er turns were explicitly mark ed in the word tran-scriptions but not the corresponding times. In order to detect speak er changes in the training data, we did a forced-alignment of the data and chopped it at speak er boundaries. The test set consists of 50 calls with 113 speak ers totaling about 3 hours of speech. 2.2. Speak er Independent System The raw acoustic features used for segmentation and recognition are perceptual linear prediction (PLP) features. The features are Table 1 . ASR results depending on segmentation/clustering and adaptation type.
 mean-normalized 40-dimensional LD A+MLL T features. The SI acoustic model consists of 50K Gaussians trained with MPE and uses a quinphone cross-w ord acoustic conte xt. The techniques are the same as those described in [6]. 2.3. Incr emental Speak er Adaptation In the conte xt of speak er-adapti ve training, we use two forms of feature-space normalization: vocal tract length normalization (VTLN) and feature-space MLLR (fMLLR, also kno wn as con-strained MLLR) to produce canonical acoustic models in which some of the non-linguistic sources of speech variability have been reduced. To this canonical feature space, we then apply a discrim-inati vely trained transform called fMPE [7]. The speak er adapted recognition model is trained in this resulting feature space using MPE.
 incremental adaptation. For the former , the transformations are computed per con versation-side using the full output of a speak er independent system. For the latter , the transformations are updated incrementally using the decoded output of the speak er adapted sys-tem up to the current time. The speak er adapti ve transforms are then applied to the future sentences. The adv antage of incremental adaptation is that it only requires a single decoding pass (as op-posed to two passes for off-line adaptation) resulting in a decoding process which is twice as fast. In Table 1, we compare the per -formance of the two approaches. Most of the gain of full offline adaptation is retained in the incremental version. 2.3.1. Segmentation and Speak er Clustering We use an HMM-based segmentation procedure for segmenting the audio into speech and non-speech prior to decoding. The rea-son is that we want to eliminate the non-speech segments in order to reduce the computational load during recognition. The speech segments are clustered together in order to identify segments com-ing from the same speak er which is crucial for speak er adaptation. The clustering is done via k-means, each segment being modeled by a single diagonal covariance Gaussian. The metric is given by the symmetric K-L divergence between two Gaussians. The im-pact of the automatic segmentation and clustering on the error rate is indicated in Table 1.
 3.1. Question Answering This section presents automated techniques for evaluating call quality . These techniques were developed using a train-ing/de velopment set of 676 calls with associated manually gen-erated quality evaluations. The test set consists of 195 calls. tati ves is commonly assessed by having human monitors listen to a random sample of the calls and then fill in evaluation forms. The form for IBM X  s North American Help Desk contains 31 questions. A subset of the questions can be answered easily using automatic methods, among those the ones that check that the agent follo wed the guidelines e.g. But some of the questions require human-le vel kno wledge of the world to answer , e.g. We were able to answer 21 out of the 31 questions using pat-tern matching techniques. For example, if the question is  X  X id the agent follo w the appropriate closing script? X , we search for  X  X HANK YOU FOR CALLING X ,  X  X NYTHING ELSE X  and  X  X ER VICE REQ UEST X . An y of these is a good partial match for the full script,  X  X hank you for calling, is there anything else I can help you with before closing this service request? X  Based on the answer to each of the 21 questions, we compute a score for each call and use it to rank them. We label a call in the test set as being bad / good if it has been placed in the bottom/top 20% by human evaluators. We report the accurac y of our scoring system on the test set by computing the number of bad calls that occur in the bottom 20% of our sorted list and the number of good calls found in the top 20% of our list. The accurac y numbers can be found in Table 2. 3.2. Maximum Entr opy Ranking Another alternati ve for scoring calls is to find arbitrary features in the speech recognition output that correlate with the outcome of a call being in the bottom 20% or not. The goal is to estimate the probability of a call being bad based on features extracted from the automatic transcription. To achie ve this we build a maximum entrop y based system which is trained on a set of calls with asso-ciated transcriptions and manual evaluations. The follo wing equa-tion is used to determine the score of a call C using a set of predefined features: where class  X  { bad, not  X  bad } , Z is a normalizing factor , are indicator functions and {  X  i } { i =1 ,N } are the parameters of the model estimated via iterati ve scaling [8].
 we used a hand-guided method for defining features. Specifi-cally , we generated a list of VIP phrases as candidate features, e.g.  X  X HANK YOU FOR CALLING X , and  X  X ELP YOU X . We also created a pool of generic ASR features, e.g.  X  X umber of hes-itations X ,  X  X otal silence duration X , and  X  X ongest silence duration X . A decision tree was then used to select the most rele vant features and the threshold associated with each feature. The final set of fea-tures contained 5 generic features and 25 VIP phrases. If we tak e a look at the weights learned for dif ferent features, we can see that if a call has man y hesitations and long silences then most lik ely the call is bad .
 Table 3 sho ws the accurac y of this system for the bottom and top 20% of the test calls.
 one that relies on answering a fix ed number of evaluation ques-tions and a more global one that looks across the entire call for hints. These two scores are both between 0 and 1, and therefore can be interpolated to generate one unique score. After optimizing the interpolation weights on a held-out set we obtained a slightly higher weight (0.6) for the maximum entrop y model. It can be seen in Table 4 that the accurac y of the combined system is greater that the accurac y of each indi vidual system, suggesting the com-plementarity of the two initial systems. 4.1. Application This section describes the user interf ace of the automated quality monitoring application. As explained in Section 1, the evalua-Fig . 2 . Interf ace to listen to audio and update the evaluation form. tor scores calls with respect to a set of quality-related questions after listening to the calls. To aid this process, the user interf ace pro vides an efficient mechanism for the human evaluator to select calls, e.g. The automated quality monitoring user interf ace is a J2EE web application that is supported by back-end databases and content management systems 1 The displayed list of calls pro vides a link to the audio, the automatically filled evaluation form, the overall score for this call, the agent X  s name, serv er location, call id, date and duration of the call (see Figure 1). This interf ace now gives the agent the ability to listen to interesting calls and update the answers in the evaluation form if necessary (audio and evaluation form illustrated in 2). In addition, this interf ace pro vides the eval-uator with the ability to vie w summary statistics (average score) and additional information about the quality of the calls. The over-all system is designed to automatically download calls from mul-tiple locations on a daily-basis, transcribe and inde x them, thereby making them available to the supervisors for monitoring. Calls spanning a month are available at any given time for monitoring purposes. 4.2. Pr ecision and Recall identification of  X  X ad X  calls. The test set consists of 195 were manually evaluated by call center personnel. Based on these manual scores, the calls were ordered by quality , and the bottom 20% were deemed to be  X  X ad.  X  To retrie ve calls for monitoring, we sort the calls based on the automatically assigned quality score and return the worst. In our summary figures, precision and recall are plotted as a function of the number of calls that are selected for monitoring. This is important because in reality only a small number of calls can recei ve human attention. Precision is the ratio Fig . 3 . Precision for the bottom 20% of the calls as a function of the number of calls retrie ved. of bad calls retrie ved to the total number of calls monitored, and recall is the ratio of the number of bad calls retrie ved to the total number of bad calls in the test set. Three curv es are sho wn in each plot: the actually observ ed performance, performance of random selection, and oracle or ideal performance. Oracle performance sho ws what would happen if a perfect automatic ordering of the calls was achie ved.
 monitoring regime where only a small fraction of the calls are monitored, we achie ve over 60% precision. (Further , if 20% of the calls are monitored, we still attain over 40% precision.) volume monitoring, the recall is midw ay between what could be achie ved with an oracle, and the performance of random-selection. our automated ranking to the number found with random selection. This indicates that in the low-monitoring regime, our automated technique triples efficienc y. 4.3. Human vs. Computer Rankings As a final measure of performance, in Figure 6 we present a scatterplot comparing human to computer rankings. We do not have calls that are scored by two humans, so we cannot present a human-human scatterplot for comparison. This paper has presented an automated system for quality moni-toring in the call center . We propose a combination of maximum-entrop y classification based on ASR-deri ved features, and question answering based on simple pattern-matching. The system can ei-ther be used to replace human monitors, or to mak e them more Fig . 5 . Ratio of bad calls found with QTM to Random selection as a function of the number of bad calls retrie ved. efficient. Our results sho w that we can triple the efficienc y of hu-man monitors in the sense of identifying three times as man y bad calls for the same amount of listening effort. [1] J. Chu-Carroll and B. Carpenter ,  X  X  ector -based natural lan-[2] P. Haf fner , G. Tur, and J. Wright,  X  X ptimizing svms for com-[3] M. Tang, B. Pellom, and K. Hacioglu,  X  X all-type classifica-[4] D. Hakkani-T ur, G. Tur, M. Rahim, and G. Riccardi,  X  X nsu-[5] C. Wu, J. Kuo, E.E. Jan, V. Goel, and D. Lubensk y,  X  X mpro v-[6] H. Soltau, B. Kingsb ury , L. Mangu, D. Po vey, G. Saon, and [7] D. Po vey, B. Kingsb ury , L. Mangu, G. Saon, H. Soltau, [8] A. Ber ger , S. Della Pietra, and V. Della Pietra,  X  X  maximum
 special treatment in automatic processes. Appropriate processing of names is essential to achieve high-quality information extraction, speech recognition, machine translation, and information management, yet most HLT applications provide limited specialized processing of names. Variation in the forms of names can make it difficult to retrieve names from data sources, to perform co-reference resolution across documents, or to associate instances of names with their representations in gazetteers and lexicons. Name matching has become critical in government contexts for checking watchlists and maintaining tax, health, and So-cial Security records. In commercial contexts, name matching is essential in credit, insurance, and legal applications.
 be clear that much of the material applies to other languages and to names of places and organizations. Case studies will be used to illustrate problems and approaches to solutions. Arabic names illustrate many of the issues encountered in multilingual name matching, among which are complex name structures and spelling variation due to morphophonemic alternation and competing transliteration conventions. 1. Name matching across languages, scripts, and cultures 2. Evaluation of Name Search and Matching Systems This tutorial is intended for those with interest in information retrieval and entity extraction, identity reso-lution, Arabic computational linguistics, and related language-processing applications. As a relatively un-studied domain, name matching is a promising area for innovation and for researchers seeking new projects. Keith J. Miller received his Ph.D. in Computational Linguistics from Georgetown University. He spent several years working on various large-scale name matching systems. His current research activities cen-ter around multicultural name matching, machine translation, embedded HLT systems, and component and system-level evaluation of systems involving HLT components.
 Sherri Condon received her Ph.D. in Linguistics from the University of Texas at Austin. In addition to several years of work in multilingual name matching and cross script name matching, she is a researcher in discourse/dialogue, entity extraction, and evaluation of machine translation and dialogue systems.
 literature will be provided. 1. Introduction to the Bayesian Paradigm 2. Background Material  X  Graphical Models (naive Bayes, maximum entropy, HMMs)  X  Expectation Maximization  X  Non-Bayesian Inference Techniques 3. Common Statistical Distributions  X  Uniform  X  Binomial and Multinomial  X  Beta and Dirichlet  X  Poisson, Gaussian and Gamma 4. Simple Bayesian Inference Techniques  X  Inference = Integration  X  Integration by Summing  X  Monte Carlo Integration 5. Advanced Bayesian Inference Techniques  X  Markov Chain Monte Carlo Integration  X  Laplace Approximation  X  Variational Approximation  X  Others (Message Passing Algorithms) 6. Survey of Popular Models  X  Latent Dirichlet Allocation  X  Integrating Topics and Syntax  X  Matching Words and Pictures 7. Pointers to Literature on Other Topics 8. Conclusions retrieval. for NLP X  at the Conference for Neural Information Processing Systems.
 1. Graph-based Algorithms Basics * Vectors, matrices, graphs * Graph representations and notations * Algorithms for graph traversal * Minimum path length * Minimum spanning trees * Min-cut/max-flow algorithms * Graph-matching algorithms * Eigenvector analysis * Node-ranking algorithms * Graph-based centrality * Graph-based clustering * Machine learning on graphs 2. Information Retrieval applications * Web-page ranking * Text classification and clustering 3. Natural language processing applications * Word sense disambiguation * Semantic classes * Textual entailment * Sentiment classification * Dependency parsing * Prepositional attachment * Keyword extraction * Text summarization research is supported by NSF, Google, and the state of Texas. Dragomir X  X  work has been funded by NSF, NIH, and ONR.
 * Scenarios where technology is expected to be useful: * Characteristics * Meta-data annotation * Past work (HP SpeechBot, BBN, TREC, PodZinger, etc.) * Examples (OCW, CSJ, MICASE) * Characteristics * Challenges and opportunities * Examination of vocabulary statistics and coverage * Vocabulary expansion from supplemental materials * Spontaneous conversational speech vs. read speech * Appropriateness of written materials * Language model adaptation * Speaker independent modeling * Speaker dependent modeling * Supervised and unsupervised adaptation * Methods for recognizing OOV words * Phonetic transcription of OOV words * TF-IDF/vector space methods * Probabilistic methods * Large scale web search (Google) * Inverted indexing; query processing/language. * Word/phone/OOV-models for generation * Lattice accuracy vs. 1-best accuracy *  X  X oft X -indexing with pruning to control size * Proximity * Tuning precision/recall at query run-time * Basic Metrics: Precision/Recall * Ordered list metrics: Kendall-Tau, Spearman * Issues with evaluating speech data * Pro X  X /con X  X  for displaying transcription * Segmentation: topic boundaries, keywords, summaries
 aim to close this gap.
 useful, and on the other hand, providing feasible learning algorithms. have been applied to NLP tasks. 1. Introduction  X  Inductive vs. Transductive learning  X  Generative vs. Discriminative learning 2. Mixtures of Generative Models  X  Analysis  X  Stable Mixing of Labelled and Unlabelled data  X  Text Classification by EM 3. Multiple view Learning  X  Co-training algorithm  X  Yarowsky algorithm  X  Co-EM algorithm  X  Co-Boost algorithm  X  Agreement Boost algorithm  X  Multi-task Learning 4. Semi-supervised Learning for Structured Labels (Discriminative models)  X  Simple case: Random Walk  X  Potential extension to Structured SVM 5. NLP tasks and semi-supervised learning  X  Using EM-based methods to combine labelled and unlabelled data  X  When does it work? Some negative examples of semi-supervised learning in NLP  X  Semi-supervised learning for domain adaptation in NLP in NLP.
 available at http://www.cs.sfu.ca/ anoop. His email address is anoop@cs.sfu.ca ghaffar1@cs.sfu.ca
 1. Introduction  X  What is semantic role labeling?  X  Why is SRL important?  X  Existing corpora: FrameNet &amp; PropBank  X  Corpora in development  X  Relation to other tasks 2. Survey of Existing SRL Systems  X  History of the development of automatic SRL systems  X  Pioneering Work  X  Basic architecture of a generic SRL system  X  Major components  X  Machine learning technologies  X  CoNLL-04 and CoNLL-05 shared tasks on SRL  X  Details of several CoNLL-05 systems  X  Overall comparisons of CoNLL-05 systems 3. Analysis of Systems and Future Directions  X  Error Analysis  X  Influence of parser errors  X  Per argument performance  X  Directions for improving SRL 4. Applications  X  Information Extraction  X  Textual Entailment  X  Machine Translation understanding. best system in the CoNLL-05 shared task.
 Stanford was the runner-up system in the CoNLL-05 shared task.
