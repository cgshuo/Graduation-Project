 User query is an element that specifies an informat ion need, but it contextual factors that strongly influence the inte rpretation of a query. Recent studies have tried to consider the us er X  X  interests by creating a user profile. However, a single profile for a user may not be sufficient for a variety of queries of the u ser. In this study, we propose to use query-specific contexts instead of user-centric ones, including context around query and context within query The former specifies the environment of a query suc h as the domain of interest, while the latter refers to cont ext words within the query, which is particularly useful for the sel ection of relevant term relations. In this paper, both types of contex t are integrated in an IR model based on language modeling. Our experim ents on several TREC collections show that each of the cont ext factors brings significant improvements in retrieval effect iveness. H.3.3 [ Information storage and retrieval ]: Information Search and Retrieval  X  Retrieval Models Algorithms, Performance, Experimentation, Theory. Query contexts, Domain model, Term relation, Langua ge model. Queries, especially short queries, do not provide a complete specification of the information need. Many relevan t terms can be absent from queries and terms included may be ambig uous. These issues have been addressed in a large number of pre vious studies. Typical solutions include expanding either document or query representation [19][35] by exploiting different res ources [24][31], using word sense disambiguation [25], etc. In these studies, however, it has been generally assumed that query i s the only element available about the user X  X  information need . In reality, query is always formulated in a search context. As it has been found in many previous studies [2][14][20][21][26], contextual factors have a strong influence on relevance judgme nts. These factors include, among many others, the user X  X  doma in of interest, knowledge, preferences, etc. All these elements spe cify the contexts around the query. So we call them context around query in this paper. It has been demonstrated that user X  X  query should be placed in its context for a correct interpretation.
 Recent studies have investigated the integration of some contexts around the query [9][30][23]. Typically, a user pro file is constructed to reflect the user X  X  domains of intere st and background. A user profile is used to favor the doc uments that are more closely related to the profile. However, a sin gle profile for a user can group a variety of different domains, whic h are not always relevant to a particular query. For example, if a user working in computer science issues a query  X  Java hotel documents on  X  Java language  X  will be incorrectly favored. A possible solution to this problem is to use query-r elated profiles or models instead of user-centric ones. In this paper, we propose to model topic domains, among which the related one(s) will be selected for a given query. This method allows us t o select more appropriate query-specific context around the query . Another strong contextual factor identified in lite rature is domain knowledge, or domain-specific term relations, such as  X  program  X  computer  X  in computer science. Using this relation, one would be able to expand the query  X  program  X  with the term  X  computer  X . However, domain knowledge is available only for a few domains (e.g.  X  Medicine  X ). The shortage of domain knowledge has led to the utilization of general kno wledge for query expansion [31], which is more available from resources documents [24][27]. However, the use of general kno wledge gives rise to an enormous problem of knowledge ambiguity [31]: we are often unable to determine if a relation applies to a query. For example, usually little information is available to determine whether  X  program  X  computer  X  is applicable to queries  X  program  X  and  X  TV program  X . Therefore, the relation has been applied to all queries containing  X  program  X  in previous studies, leading to a wrong expansion for  X  TV program  X . Looking at the two query examples, however, people can easily determine whether the relation is applicable, by co nsidering the context words  X  Java  X  and  X  TV  X . So the important question is how appropriate relations to apply. These context words form a within query . In some previous studies [24][31], context words in a query have been used to select expansion terms su ggested by term relations, which are, however, context-indepen dent (such as  X  program  X  computer  X ). Although improvements are observed in some cases, they are limited. We argue that the pro blem stems from the lack of necessary context information in r elations themselves, and a more radical solution lies in the addition of contexts in relations. The method we propose is to add context words into the condition of a relation, such as  X  X   X  computer  X , to limit its applicability to the appropriate co ntext. This paper aims to make contributions on the follow ing aspects:  X 
Query-specific domain model: We construct more spec ific domain models instead of a single user model groupi ng all the domains. The domain related to a specific query is selected (either manually or automatically) for each query.  X 
Context within query: We integrate context words in term relations so that only appropriate relations can be applied to the query.  X 
Multiple contextual factors: Finally, we propose a framework based on language modeling approach to integrate mu ltiple contextual factors. Our approach has been tested on several TREC collec tions. The experiments clearly show that both types of context can result in significant improvements in retrieval effectiveness , and their effects are complementary. We will also show that i t is possible to determine the query domain automatically, and this results in comparable effectiveness to a manual specification of domain. This paper is organized as follows. In section 2, we review some related work and introduce the principle of our app roach. Section 3 presents our general model. Then sections 4 and 5 describe respectively the domain model and the knowledge mod el. Section 6 explains the method for parameter training. Exper iments are presented in section 7 and conclusions in section 8 . There are many contextual factors in IR: the user X  X  domain of interest, knowledge about the subject, preference, document recency, and so on [2][14]. Among them, the user X  X  domain of interest and knowledge are considered to be among t he most important ones [20][21]. In this section, we review some of the studies in IR concerning these aspects. Domain of interest and context around query A domain of interest specifies a particular backgro und for the interpretation of a query. It can be used in differ ent ways. Most often, a user profile is created to encompass all t he domains of interest of a user [23]. In [5], a user profile con tains a set of topic categories of ODP (Open Directory Project, http://d moz.org) identified by the user. The documents (Web pages) c lassified in these categories are used to create a term vector, which represents the whole domains of interest of the user. On the o ther hand, [9][15][26][30], as well as Google Personalized Sea rch [12] use the documents read by the user, stored on user X  X  co mputer or extracted from user X  X  search history. In all these studies, we observe that a single user profile (usually a stati stical model or vector) is created for a user without distinguishin g the different topic domains. The systematic application of the us er profile can incorrectly bias the results for queries unrelated to the profile. This situation can often occur in practice as a user can search for a variety of topics outside the domains that he has p reviously searched in or identified. A possible solution to this problem is the creation of multiple profiles, one for a separate domain of interest. Th e domains related to a query are then identified according to the query. This will enable us to use a more appropriate query-spec ific profile, instead of a user-centric one. This approach is use d in [18] in which ODP directories are used. However, only a sma ll scale experiment has been carried out. A similar approach is used in [8], where domain models are created using ODP categorie s and user queries are manually mapped to them. However, the e xperiments showed variable results. It remains unclear whether domain models can be effectively used in IR. In this study, we also model topic domains. We will carry out experiments on both automatic and manual identifica tion of query domains. Domain models will also be integrated with other factors. In the following discussion, we will call the topic domain of a query a context around query to contrast with another within query that we will introduce.
 Knowledge and context within query Due to the unavailability of domain-specific knowle dge, general knowledge resources such as Wordnet and term relati ons extracted automatically have been used for query expansion [2 7][31]. In both cases, the relations are defined between two s ingle terms such as  X  t 1  X  t 2  X . If a query contains term t 1 , then t as a candidate for expansion. As we mentioned earli er, we are faced with the problem of relation ambiguity: some relations apply to a query and some others should not. For example,  X  program  X  computer  X  should not be applied to  X  TV program even if the latter contains  X  program  X . However, little information context is appropriate. To remedy this problem, approaches have been propos ed to make a selection of expansion terms after the applicatio n of relations [24][31]. Typically, one defines some sort of globa l relation between the expansion term and the whole query, whi ch is usually a sum of its relations to every query word. Althoug h some inappropriate expansion terms can be removed becaus e they are only weakly connected to some query terms, many oth ers remain. For example, if the relation  X  program  X  computer enough,  X  computer  X  will have a strong global relation to the whole query  X  TV program  X  and it still remains as an expansion term. It is possible to integrate stronger control on the utilization of knowledge. For example, [17] defined strong logical relations to encode knowledge of different domains. If the appli cation of a relation leads to a conflict with the query (or wit h other pieces of evidence), then it is not applied. However, this ap proach requires encoding all the logical consequences including con tradictions in knowledge, which is difficult to implement in pract ice. In our earlier study [1], a simpler and more genera l approach is proposed to solve the problem at its source, i.e. t he lack of context information in term relations: by introducing stric ter conditions in a relation, for example  X  X  Java, program }  X  computer  X  X  algorithm, program }  X  computer  X , the applicability of the relations will be naturally restricted to correct c ontexts. As a result,  X  computer  X  will be used to expand queries  X  Java program or  X  program algorithm  X , but not  X  TV program  X . This principle is similar to that of [33] for word sense disambiguati on. However, make differences between word usages in different c ontexts. From this point of view, our approach is more similar to word sense discrimination [27]. In this paper, we use the same approach and we will integrate it into a more global model with other context factors . As the context words added into relations allow us to expl oit the word context within the query, we call such factors context within query. Within query context exists in many queries. In fa ct, users often do not use a single ambiguous word such as  X  Java  X  as query (if they are aware of its ambiguity). Some context words are often used together with it. In these cases, contexts wit hin query are created and can be exploited.
 Query profile and other factors Many attempts have been made in IR to create query-specific profiles. We can consider implicit feedback or blin d feedback [7][16][29][32][35] in this family. A short-term fe edback model is created for the given query from feedback documents , which has been proven to be effective to capture some aspects of the user X  X  intent behind the query. In order to create a good query model, such a query-specific feedback model should be inte grated. There are many other contextual factors ([26]) that we do not deal with in this paper. However, it seems clear that ma ny factors are complementary. As found in [32], a feedback model c reates a local context related to the query, while the general kno wledge or the whole corpus defines a global context. Both types o f contexts have been proven useful [32]. Domain model specifies yet another type of useful information: it reflects a set of specifi c background terms for a domain, for example  X  pollution  X ,  X  rain  X ,  X  for the domain of  X  Environment  X . These terms are often presumed when a user issues a query such as  X  waste cleanup  X  in the domain. complementarity among these factors. It is then use ful to combine them together in a single IR model. In this study, we will integrate all the above fact ors within a unified framework based on language modeling. Each component contextual factor will determines a different ranki ng score, and the final document ranking combines all of them. This i s described in the following section. In the language modeling framework, a typical score function is defined in KL-divergence as follows: where  X  D is a (unigram) language model created for a docume nt  X  a language model for the query Q , and V the vocabulary. Smoothing on document model is recognized to be cru cial [35], and one of common smoothing methods is the Jelinek-Mercer interpolation smoothing: where  X  is an interpolation parameter and  X  C the collection model. In the basic language modeling approaches, the quer y model is estimated by Maximum Likelihood Estimation (MLE) wi thout any limited to keyword matching, according to a few wor ds in the query. To improve retrieval effectiveness, it is im portant to create a more complete query model that represents better the information need. In particular, all the related an d presumed words should be included in the query model. A more compl ete query model by several methods have been proposed using f eedback documents [16][35] or using term relations [1][10][ 34]. In these cases, we construct two models for the query: the i nitial query model containing only the original terms, and a new model containing the added terms. They are then combined through interpolation. In this paper, we generalize this approach and inte grate more models for the query. Let us use 0 Q  X  to denote the original query model, F Q  X  for the feedback model created from feedback model created by applying term relations. 0 Q  X  can be created by MLE. F Q  X  has been used in several previous studies [16][35]. In this paper, F Q  X  is extracted using the 20 blind feedback  X  in Section 4 and 5. Given these models, we create the following final q uery model by interpolation: where X= {0, Dom, K, F } is the set of all component models and  X  (with 1 =  X  Then the document score in Equation (1) is extended as follows: each component model. Here we can see that our stra tegy of enhancing the query model by contextual factors is equivalent to document re-ranking, which is used in [5][15][30]. The remaining problem is to construct domain models and knowledge model and to combine all the models (para meter setting). We describe this in the following section s. As in previous studies, we exploit a set of documen ts already classified in each domain. These documents can be i dentified in two different ways: 1) One can take advantages of a n existing domain hierarchy and the documents manually classif ied in them, such as ODP. In that case, a new query should be cl assified into the same domains either manually or automatically. 2) A user can define his own domains. By assigning a domain to hi s queries, the system can gather a set of answers to the queries a utomatically, which are then considered to be in-domain documents . The answers could be those that the user have read, bro wsed through, or judged relevant to an in-domain query, or they c an be simply the top-ranked retrieval results. An earlier study [4] has compared the above two str ategies using TREC queries 51-150, for which a domain has been ma nually assigned. These domains have been mapped to ODP cat egories. It is found that both approaches mentioned above are e qually effective and result in comparable performance. The refore, in this study, we only use the second approach. This choice is also motivated by the possibility to compare between man ual and automatic assignment of domain to a new query. This will be explained in detail in our experiments. Whatever the strategy, we will obtain a set of docu ments for each domain, from which a language model can be extracte d. If maximum likelihood estimation is used directly on t hese documents, the resulting domain model will contain both domain-specific terms and general terms, and the former do not emerge. Therefore, we employ an EM process to extract the s pecific part of the domain as follows: we assume that the documents in a domain are generated by a domain-specific model (to be ext racted) and general language model (collection model). Then the likelihood of a document in the domain can be formulated as follo ws: where c ( t ; D ) is the count of t in document smoothing parameter (which will be fixed at 0.5 as in [35]). The EM algorithm is used to extract the domain model the domain), that is: This is the same process as the one used to extract feedback model in [35]. It is able to extract the most specific wo rds of the domain from the documents while filtering out the common w ords of the language. This can be observed in the following tab le, which shows some words in the domain model of  X  Environment  X  before and after EM iterations (50 iterations). Given a set of domain models, the related ones have to be assigned to a new query. This can be done manually by the us er or automatically by the system using query classificat ion. We will compare both approaches. Query classification has been investigated in sever al studies [18][28]. In this study, we use a simple classifica tion method: the selected domain is the one with which the query X  X  K L-divergence score is the lowest, i.e.: This classification method is an extension to Na X ve Bayes as shown in [22]. The score depending on the domain mo del is then as follows: Although the above equation requires using all the terms in the vocabulary, in practice, only the strongest terms i n the domain model are useful and the terms with low probabiliti es are often noise. Therefore, we only retain the top 100 strong est terms. The same strategy is used for Knowledge model. Although domain models are more refined than a sing le user making the domain model too large. This is particul arly true for large domains such as  X  X cience and technology X  defi ned in TREC queries. Using such a large domain model as the bac kground can introduce much noise terms. Therefore, we further c onstruct a sub-domain model more related to the given query, by us ing a subset of in-domain documents that are related to the quer y. These documents are the top-ranked documents retrieved wi th the original query within the domain. This approach is indeed a combination of domain and feedback models. In our e xperiments, necessary in some cases, but not in all, especially when Feedback model is also used. In this paper, we extract term relations from the d ocument collection automatically. In general, a term relation can be represented as A  X  B . Both B have been restricted to single terms in previous s tudies. A single containing that term. As we explained earlier, this is the source of many wrong applications. The solution we propose is to add more context terms into A , so that it is applicable only when all the terms in A appear in a query. For example, instead of creating a context-independent relation  X  Java  X  program  X , we will create  X  X 
Java , computer }  X  program  X , which means that  X  program selected when both  X  Java  X  and  X  computer  X  appear in a query. The term added in the condition specifies a stricter co ntext to apply the relation. We call this type of relation context-dependent relation. In principle, the addition is not restricted to one term. However, we will make this restriction due to the following reasons:  X 
User queries are usually very short. Adding more te rms into the condition will create many rarely applicable relati ons;  X 
In most cases, an ambiguous word such as  X  Java  X  can be effectively disambiguated by one useful context wor d such as  X  computer  X  or  X  hotel  X ;  X 
The addition of more terms will also lead to a high er space and time complexity for extracting and storing term rel ations. The extraction of relations of type  X  X  t j ,t k }  X  t i  X  can be performed using mining algorithms for association rules [13]. Here, we use a simple co-occurrence analysis. Windows of fixed siz e (10 words in our case) are used to obtain co-occurrence count s of three terms, and the probability ) | ( where ) , , ( In order to reduce space requirement, we further ap ply the following filtering criteria:  X 
The two terms in the condition should appear at lea st certain time together in the collection (10 in our case) an d they should be related. We use the following pointwise mutual i nformation as a measure of relatedness ( MI &gt; 0) [6]:  X 
The probability of a relation should be higher than a threshold (0.0001 in our case); Having a set of relations, the corresponding Knowle dge model is defined as follows: where ( t j t k )  X  Q means any combination of two terms in the query. This is a direct extension of the translation model proposed in [3] to our context-dependent relations. The score accor ding to the Knowledge model is then defined as follows: Again, only the top 100 expansion terms are used. There are several parameters in our model:  X  in Equation (2) and  X  ( i  X  {0, Dom, K, F }) in Equation (3). As the parameter  X  only affects document model, we will set it to the same value in all our experiments. The value  X  = 0.5 is determined to maximize the effectiveness of the baseline models (see Section 7 .2) on the training data: TREC queries 1-50 and documents on D isk 2. The mixture weights  X  i of component models are trained on the same training data using the following method of li ne search [11] to maximize the Mean Average Precision (MAP): each parameter is considered as a search direction. We start by se arching in one the values in other directions unchanged. Each dire ction is searched in turn, until no improvement in MAP is ob served. In order to avoid being trapped at a local maximum, we started from 10 random points and the best setting is selec ted. The main test data are those from TREC 1-3 ad hoc a nd filtering tracks, including queries 1-150, and documents on D isks 1-3. The choice of this test collection is due to the availa bility of manually specified domain for each query. This allows us to compare with an approach using automatic domain identification. Below is an example of topic: We only use topic titles in all our tests. Queries 1-50 are used for training and 51-150 for testing. 13 domains are def ined in these queries and their distributions among the two sets of queries are shown in Fig. 1. We can see that the distribution v aries strongly between domains and between the two query sets. We have also tested on TREC 7 and 8 data. For this series of tests, used for testing. Some statistics of the data are d escribed in Tab. 2. All the documents are preprocessed using Porter ste mmer in Lemur and the standard stoplist is used. Some queri es (4, 5 and 3 in the three query sets) only contain one word. Fo r these queries, knowledge model is not applicable. On domain models, we examine several questions:  X 
When query domain is specified manually, is it usef ul to incorporate the domain model?  X 
If the query domain is not specified, can it be det ermined automatically? How effective is this method?  X 
We described two ways to gather documents for a dom ain: either using documents judged relevant to queries i n the domain or using documents retrieved for these queries. How do they compare? On Knowledge model, in addition to testing its effe ctiveness, we also want to compare the context-dependent relation s with context-independent ones. Finally, we will see the impact of each component m odel when all the factors are combined. Two baseline models are used: the classical unigram model without any expansion, and the model with Feedback. In all the experiments, document models are created using Jeli nek-Mercer smoothing. This choice is made according to the obs ervation in [36] that the method performs very well for long qu eries. In our case, as queries are expanded, they perform similar ly to long queries. In our preliminary tests, we also found th is method performed better than the other methods (e.g. Diric hlet), especially for the main baseline method with Feedback model. T able 3 shows the retrieval effectiveness on all the collections. This model is combined with both baseline models (w ith or without feedback). We also compare the context-depe ndent knowledge model with the traditional context-indepe ndent term relations (defined between two single terms), which are used to expand queries. This latter selects expansion terms with strongest global relation to the query. This relation is meas ured by the sum of relations to each of the query terms. This metho d is equivalent Co-occurrence model in Table 4. T-test is also perf ormed for statistical significance. As we can see, simple co-occurrence relations can p roduce relatively strong improvements; but context-depende nt relations can produce much stronger improvements in all cases , especially when feedback is not used. All the improvements ove r co-occurrence model are statistically significant (thi s is not shown in the table). The large differences between the two t ypes of relation clearly show that context-dependent relations are m ore appropriate for query expansion. This confirms the hypothesis w e made, that by incorporating context information into relations , we can better determine the appropriate relations to apply and th us avoid introducing inappropriate expansion terms. The foll owing example can further confirm this observation, where we show the strongest expansion terms suggested by both types o f relation for the query #384  X  space station moon  X : In comparison with the baseline model with feedback (Tab. 3), we see that the improvements made by Knowledge model a lone are slightly lower. However, when both models are combi ned, there are additional improvements over the Feedback model , and these improvements are statistically significant in 2 cas es out of 3. This demonstrates that the impacts produced by feedback and term relations are different and complementary. In this section, we test several strategies to crea te and use domain models, by exploiting the domain information of the query set in various ways. Strategies for creating domain models: C1 -With the relevant documents for the in-domain queries: this strategy simulates the case where we have an existi ng directory in which documents relevant to the domain are included . C2 -With the top-100 documents retrieved with the in-domain queries: this strategy simulates the case where the user specifies a domain for his queries without judging document rel evance, and the system gathers related documents from his searc h history. Strategies for using domain models: U1 -The domain model is determined by the user manually . U2 -The domain model is determined by the system. queries 51-150 is used in turn as the test query wh ile the other queries and their relevant documents (C1) or top-ra nked retrieved documents (C2) are used to create domain models. Th e same method is used on queries 1-50 to tune the paramete rs. We also compare the domain models created with all the in-domain documents ( Domain ) and with only the top-10 retrieved documents in the domain with the query ( Sub-Domain tests, we use manual identification of query domain for Disks 1-3 (U1), but automatic identification for TREC7 and 8 (U2).
 models can generally improve retrieval effectivenes s in all the cases. The improvements on Disks 1-3 and TREC7 are statistically significant. However, the improvement scales are sm aller than using Feedback and Relation models. Looking at the distribution of the domains (Fig. 1), this observation is not su rprising: for many domains, we only have few training queries, th us few in-domain documents to create domain models. In additi on, topics in the same domain can vary greatly, in particular in large domains such as  X  X cience and technology X ,  X  X nternational po litics X , etc. Second, we observe that the two methods to create d omain models perform equally well (Tab. 6 vs. Tab. 5). In other words, providing relevance judgments for queries does not add much a dvantage for the purpose of creating domain models. This may see m surprising. An analysis immediately shows the reason: a domain model (in the way we created) only captures term distribution in the domain. Relevant documents for all in-domain queries vary g reatly. Therefore, in some large domains, characteristic te rms have variable effects on queries. On the other hand, as we only use term distribution, even if the top documents retrieved f or the in-domain queries are irrelevant, they can still contain doma in characteristic terms similarly to relevant documents. Thus both st rategies produce very similar effects. This result opens the door for a simpler method that does not require relevance judg ments, for example using search history. Third, without Feedback model, the sub-domain model s constructed with relevant documents perform much be tter than the whole domain models (Tab. 5). However, once Feedbac k model is used, the advantage disappears. On one hand, this c onfirms our earlier hypothesis that a domain may be too large t o be able to suggest relevant terms for new queries in the domai n. It indirectly validates our first hypothesis that a single user m odel or profile may be too large, so smaller domain models are pref erred. On the other hand, sub-domain models capture similar chara cteristics to Feedback model. So when the latter is used, sub-dom ain models become superfluous. However, if domain models are c onstructed with top-ranked documents (Tab. 6), sub-domain mode ls make much less differences. This can be explained by the fact that the domains constructed with top-ranked documents tend to be more uniform than relevant documents with respect to ter m distribution, as the top retrieved documents usually have stronge r statistical correspondence with the queries than the relevant d ocuments. It is not realistic to always ask users to specify a domain for their queries. Here, we examine the possibility to automa tically identify query domains. Table 7 shows the results with this strategy using both strategies for domain model construction. We c an observe that the effectiveness is only slightly lower than those produced with manual identification of query domain (Tab. 5 &amp; 6, Domain models). This shows that automatic domain identific ation is a way to select domain model as effective as manual ident ification. This also demonstrates the feasibility to use domain mod els for queries when no domain information is provided. Looking at the accuracy of the automatic domain ide ntification, however, it is surprisingly low: for queries 51-150 , only 38% of the determined domains correspond to the manual ide ntifications. This is much lower than the above 80% rates reporte d in [18]. A detailed analysis reveals that the main reason is t he closeness of several domains in TREC queries (e.g.  X  X nternationa l relations X ,  X  X nternational politics X ,  X  X olitics X ). However, in this situation, wrong domains assigned to queries are not always ir relevant and useless. For example, even when a query in  X  X nterna tional relations X  is classified in  X  X nternational politics  X , the latter domain can still suggest useful terms to the query. Theref ore, the relatively low classification accuracy does not mean low usefu lness of the domain models. The results with the complete model are shown in Ta ble 8. This model integrates all the components described in th is paper: Original query model, Feedback model, Domain model and Knowledge model. We have tested both strategies to create domain models, but the differences between them are very small. So we only report the results with the relevant doc uments. Our first observation is that the complete models p roduce the best results. All the improvements over the baseline mod el (with feedback) are statistically significant. This resul t confirms that the integration of contextual factors is effective. Com pared to the other results, we see consistent, although small in some cases, improvements over all the partial models. Looking at the mixture weights, which may reflect t he importance of each model, we observed that the best settings i n all the collections vary in the following ranges: 0.1  X   X  0  X  important factor is Feedback model. This is also th e single factor which produced the highest improvements over the or iginal query model. This observation seems to indicate that this model has the highest capability to capture the information need behind the query. However, even with lower weights, the other models do have strong impacts on the final effectiveness. Thi s demonstrates the benefit of integrating more contextual factors in IR. Traditional IR approaches usually consider the quer y as the only element available for the user information need. Ma ny previous studies have investigated the integration of some c ontextual factors in IR models, typically by incorporating a user profile. In this paper, we argue that a single user profile (or model) can can be incorrectly biased. Similarly to some previo us studies, we propose to model topic domains instead of the user.
 Previous investigations on context focused on facto rs around the query. We showed in this paper that factors within the query are also important  X  they help select the appropriate t erm relations to apply in query expansion. We have integrated the above contextual factors, to gether with feedback model, in a single language model. Our exp erimental results strongly confirm the benefit of using conte xts in IR. This work also shows that the language modeling framewor k is appropriate for integrating many contextual factors . This work can be further improved on several aspect s, including other methods to extract term relations, to integra te more context words in conditions and to identify query domains. It would also be interesting to test the method on Web search usi ng user search history. We will investigate these problems in our future research. [1] Bai, J., Nie, J.Y., Cao, G., Context-dependent term relations [2] Belkin, N.J., Interaction with texts: Information r etrieval as [3] Berger, A., Lafferty, J., Information retrieval as statistical [4] Bouchard, H., Nie, J.Y., Mod X les de langue appliqu X  s  X  la [5] Chirita, P.A., Paiu, R., Nejdl, W., Kohlsch X tter, C ., Using [6] Church, K. W., Hanks, P., Word association norms, m utual [7] Croft, W. B., Cronen-Townsend, S., Lavrenko, V., Re levance [8] Croft, W. B., Wei, X., Context-based topic models f or query [9] Dumais, S., Cutrell, E., Cadiz, J., Jancke, G., Sar in, R., [10] Fang, H., Zhai, C., Semantic term matching in axiom atic [11] Gao, J., Qi, H., Xia, X., Nie, J.-Y., Linear discri minative [12] Goole Personalized Search, http://www.google.com/ps earch. [13] Hipp, J., Guntzer, U., Nakhaeizadeh, G., Algorithms for [14] Ingwersen, P., J X verlin, K., Information retrieval in context: [15] Kim, H.-R., Chan, P.K., Personalized ranking of sea rch [16] Lavrenko, V., Croft, W. B., Relevance-based languag e [17] Lau, R., Bruza, P., Song, D., Belief revision for a daptive [18] Liu, F., Yu,C., Meng, W., Personalized web search b y [19] Liu, X., Croft, W. B., Cluster-based retrieval usin g language [20] Morris, R.C., Toward a user-centered information se rvice, [21] Park, T.K., Toward a theory of user-based relevance : A call [22] Peng, F., Schuurmans, D., Wang, S. Augmenting Naive [23] Pitkow, J., Sch X tze, H., Cass, T., Cooley, R., Turn bull, D., [24] Qiu, Y., Frei, H.P. Concept based query expansion. [25] Sanderson, M., Retrieving with good sense, Inf. Ret., [26] Schamber, L., Eisenberg, M.B., Nilan, M.S., A re-[27] Sch X tze, H., Pedersen J.O., A cooccurrence-based th esaurus [28] Shen, D., Pan, R., Sun, J-T., Pan, J.J., Wu, K., Yi n, J., Yang, [29] Shen, X., Tan, B., Zhai, C., Context-sensitive info rmation [30] Teevan, J., Dumais, S.T., Horvitz, E., Personalizin g search [31] Voorhees, E., Query expansion using lexical-semanti c [32] Xu, J., Croft, W.B., Query expansion using local an d global [33] Yarowsky, D. Unsupervised word sense disambiguation [34] Zhou X., Hu X., Zhang X., Lin X., Song I-Y., Contex t-[35] Zhai, C., Lafferty, J., Model-based feedback in the language [36] Zhai, C., Lafferty, J., A study of smoothing method s for 
