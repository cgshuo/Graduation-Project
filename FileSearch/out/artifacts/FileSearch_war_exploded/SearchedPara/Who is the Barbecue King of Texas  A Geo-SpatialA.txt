 This paper addresses the problem of identifying local experts in so-cial media systems like Twitter. Local experts  X  in contrast to gen-eral topic experts  X  have specialized knowledge focused around a particular location, and are important for many applications includ-ing answering local information needs and interacting with com-munity experts. And yet identifying these experts is difficult. Hence in this paper, we propose a geo-spatial-driven approach for identify-ing local experts that leverages the fine-grained GPS coordinates of millions of Twitter users. We propose a local expertise framework that integrates both users X  topical expertise and their local authority. Concretely, we estimate a user X  X  local authority via a novel spatial proximity expertise approach that leverages over 15 million geo-tagged Twitter lists. We estimate a user X  X  topical expertise based on expertise propagation over 600 million geo-tagged social con-nections on Twitter. We evaluate the proposed approach across 56 queries coupled with over 11,000 individual judgments from Ama-zon Mechanical Turk. We find significant improvement over both general (non-local) expert approaches and comparable local expert finding approaches.
 H.2.8 [ Database Applications ]: Data Mining Twitter; expert finding; local expert; social tagging; crowdsourcing
We tackle the problem of finding local experts in social media systems like Twitter. Local experts bring specialized knowledge about a particular location and can provide insights that are typ-ically unavailable to more general topic experts. For example, a  X  X oodie X  local expert is someone who is knowledgeable about the local food scene, and may be able to answer local information needs like: what X  X  the best barbecue in town? Which restaurants locally source their vegetables? Which pubs are good for hearing new bands? Similarly, a local  X  X echie X  expert could be a conduit to connecting with local entrepreneurs, identifying tech-oriented neighborhood hangouts, and recommending local talent (e.g., do  X  Answer: Daniel Vaughn (@BBQsnob) you know any good, available web developers?). Indeed, a recent Yahoo! Research survey found that 43% of participants would like to directly contact local experts for advice and recommendations (in the context of online review systems like Yelp), while 39% would not mind being contacted by others [1].

And yet finding local experts is challenging. Traditional expert finding has focused on either small-scale, difficult-to-scale cura-tion of experts (e.g., a magazine X  X  list of the  X  X op 100 Lawyers in Houston X ) or on automated methods that can mine large-scale in-formation sharing platforms. Indeed, many efforts have focused on finding experts in online forums [29], question-answering sites [18], enterprise corpora [3, 5], and online social networks [8, 11, 22, 25, 30]. These approaches, however, have typically focused on finding general topic experts, rather than local experts .
In this paper, we investigate new approaches for mining local expertise from social media systems. Our approach is motivated by the widespread adoption of GPS-enabled tagging of social media content via smartphones and social media services (e.g., Facebook, Twitter, Foursquare). These services provide a geo-social overlay of the physical environment of the planet with billions of check-ins, images, Tweets, and other location-sensitive markers. This massive scale geo-social resource provides unprecedented opportunities to study the connection between people X  X  expertise and locations and for building localized expert finding systems. Figur e 1: Heatmap of the location of Twitter users who have listed @BBQsnob or @JimmyFallon Concretely, we propose a local expertise framework  X  Local-Rank  X  that integrates both a person X  X  topical expertise and their local authority. The framework views a local expert as someone who is well recognized by the local community , where we estimate this local recognition via a novel spatial proximity expertise ap-proach that leverages over 15 million geo-tagged Twitter lists. Fig-ure 1(a) shows a heatmap of the locations of Twitter users who have labeled Daniel Vaughn (@BBQsnob) on Twitter. Vaughn  X  the newly-named Barbecue Editor of Texas Monthly  X  is one of the foremost barbecue experts in Texas. We can see that his ex-pertise is recognized regionally in Texas, and more specifically by local barbecue centers in Austin and Dallas. In contrast, late-night host Jimmy Fallon X  X  heatmap suggests he is recognized nationally, but without a strong local community. Intuitively, Daniel Vaughn is recognized as a local expert in Austin in the area of Barbecue; Jimmy Fallon is certainly an expert (of comedy and entertainment), but his expertise is diffused nationally.
Toward identifying local experts, this paper makes the following contributions.  X  First, we propose the problem of local expert finding in social media systems like Twitter and propose a novel expertise frame-work  X  LocalRank. The framework decomposes local expertise into two key components: a candidate X  X  topical authority (e.g., how well is the candidate recognized in the area of Barbecue or web de-velopment?) and his local authority (e.g., how well do people in Austin  X  the area of interest  X  recognize this candidate?).  X  Second, to estimate local authority , we mine the fine-grained geo-tagged linkages among millions of Twitter users. Concretely, we extract Twitter list relationships where both the list creator and the user being labeled have revealed a precise location. The first lo-cal authority method considers the distance between an expert can-didate X  X  location and the location of interest, capturing the intuition that closer candidates are more locally authoritative. However, in many cases, an expert in one location may actually live far away  X  e.g., Daniel Vaughn is an expert in Austin Barbecue although he lives 200 miles away in Dallas. To capture these cases, we propose and evaluate a local authority method that considers the distance of the candidate expert X  X   X  X ore audience X  from the location of interest (that is, to reward candidates who have many labelers near the lo-cation of interest, even if the candidate lives far away). So, if many people in Austin consider Daniel Vaughn an expert, then his Austin local authority should reflect that.  X  Third, to estimate topical authority , we adapt a well-known language modeling approach to expertise identification, but aug-ment it to incorporate the distance-weighted social ties of 24 mil-lion geo-tagged Twitter users. In this way, topical expertise can be propagated through the social network to identify local experts that are well connected to, and recognized by the local community in the topic.  X  Finally, we evaluate the LocalRank framework across 56 local expertise queries coupled with 11,000 individual judgments from Amazon Mechanical Turk. We see a significant improvement in performance (35% improvement in Precision @10 and around 18% in NDCG @10 ) over the best performing alternative approach. We observe that the local authority approaches that consider the loca-tions of a candidate X  X   X  X ore audience X  perform much better than an alternative that only considers the distance between the candidate X  X  location to the query location. In addition, we see that the expertise propagation through the social network can improve the baseline local expert finding approach.

These results demonstrate the viability of mining fine-grained geo-social signals for expertise finding, and highlight the poten-tial of future geo-social systems that facilitate information flow be-tween local experts and the local community.
The emergence of online geo-social systems provides unprece-dented opportunities to bridge the gap between people X  X  online and offline presence. However there are key challenges associated with these opportunities including location sparsity [2, 7] and location privacy [9, 28]. Given the geo-social footprints from these ser-vices, researchers have analyzed the spatio-temporal properties of these footprints [23], studied the semantics associated with these footprints [26], and investigated new location recommendation sys-tems [27, 31].

Expert finding is an important task that has seen considerable re-search. Lappas et al. [16] provided a comprehensive survey about expert finding in social networks, and grouped the related work into two categories: (i) using text content posted by expert candidates; and (ii) using the expert candidates X  online social connections. For example, Balog et al. [3] proposed two generative probabilistic models  X  a user model generated using documents associated to an expert, and a topic model generated using documents associated to the topic  X  to detect topic experts. Based on their evaluation over the TREC Enterprise corpora, the authors observed that the topic Figure 2: Our goal is to identify Local Experts (the red stars in the top-right section) model outperforms the user model and other unsupervised tech-niques. On the other hand, Zhang et al. [29] applied link analysis approaches like PageRank and HITS to identify top experts in a Java forum, observing that both link analysis and network structure are helpful in finding users with extensive expertise.
 Along the direction of expert finding in online social networks, Weng et al. [25], proposed a link-analysis based approach to iden-tify top experts in a topic. They considered both topical similar-ity between users and social connections. The authors observed their approach outperforms Twitter X  X  system, PageRank, and topic-sensitive Pagerank. Similarly, Pal and Counts [22] introduced a probabilistic clustering framework to identify top authorities in a topic using both nodal and topical features. The Cognos system built by Ghosh et al. [11] leveraged Twitter lists to identify the can-didate X  X  expertise, and the authors reported that their system works as well as Twitter X  X  official system (i.e., WTF: Who To Follow) to identify top users for a particular topic. Other works include expert finding in online forums [29], question-answering sites [18], enter-prise corpora [5, 3], and online social network services [8, 11, 22, 25, 30].

In the context of local experts, Antin et al. [1] recently presented a survey designed to examine people X  X  attitudes about local knowl-edge and personal investment in local neighborhoods. They ob-served that over 52% of the participants claimed having both lo-cal knowledge and personal investment in their local area. And in an encouraging direction, they found that many participants would like to contact local experts for advice (43%) and many would not mind being contacted by others (39%). To understand people X  X  lo-cal expertise, some recent effort [17] proposed to apply points of interests as a possible categorization of expertise.
In this paper, we are interested in finding local experts with par-ticular expertise in a specific location. We assume there is a pool of expert candidates V = { v 1 ,v 2 ,...,v n } , that each candidate v has an associated location l ( v i ) and a set of areas of expertise de-scribed by a feature vector ~v i . Each element in the vector is asso-ciated with a expertise topic word t w (e.g.,  X  X echnology X ), and the element value indicates to what extent the candidate is an expert in the corresponding topic. As presented in our previous work [6], we define the Local Expert Finding problem as:
D EFINITION 1. (Local Expert Finding) Given a query q that includes a query topic t ( q ) , and a query location l ( q ) , find the set of k candidates with the highest local expertise in query topic t ( q ) and location l ( q ) .

A location l ( q ) can correspond to different spatial granularities, depending on the goal of expert finding  X  a region (e.g., Texas), a city (e.g., Austin), a neighborhood (e.g., downtown), or a latitude-longitude coordinate.
Identify a local expert requires that we can accurately estimate not only the candidate X  X  expertise on a topic of interest (e.g., how much does this candidate know about barbecue), but also that we can identify the candidate X  X  local authority (e.g., how well does the local community recognize this candidate X  X  expertise). Hence, we propose to decompose the local expertise for a candidate v two related dimensions:  X  Topical Authority : which captures the candidate X  X  expertise on  X  Local Authority : which captures the candidate X  X  local author-
To illustrate, Figure 2 shows example candidates in this two-dimensional space for a particular topic (say, Barbecue) and a par-ticular location (say, Austin):  X  Nobodies [bottom-left] : For a particular area of interest, these  X  Locals [bottom-right] : These candidates have high local au- X  Experts [top-left] : Candidates with high topical authority, but  X  Local Experts [top-right: red stars] : both great topical author-
Note that a candidate is evaluated per-topic and per-location, so a local expert in one place may be considered as just an expert or even a nobody in a different location.
To identify these local experts (the red stars), we propose to ex-ploit the geo-social information embedded in Twitter lists to find candidates who are well recognized by the local community . Twit-ter lists are a form of crowd-sourced knowledge, whereby aggre-gating the individual lists constructed by distinct users can reveal the crowd perspective on how a Twitter user is perceived [11]. Con-cretely, for each expert candidate v i , we assume that there is a set of people V l ( v i ) that recognize v i  X  X  expertise, and label v own lists. We refer to the set of people as candidate v i or more concisely labelers . Candidate v i is the labelee . Unique to this study in comparison with previous efforts that use Twitter lists, for each labeler v j (such that v j  X  V l ( v i ) ), we assume that v location l ( v j ) is known . For example, Figure 3 shows a geo-tagged Twitter list relationship in which the list labeler (@jerry from San Antonio) has placed the labelee @BBQsnob (from Dallas) on his  X  X BQ X  list.

But how do we sample such geo-tagged list relationships? Are there sufficient users to support local expertise finding? And if so, do these lists actually reveal topics of potential expertise interest, or are they focused mainly on other dimensions (e.g., for organizing a user X  X  friends)? In the following, we present our Twitter geo-tagged data collection (summarized in Table 1) and address the potential of geo-tagged lists to support local expertise finding, before turning to the development of our local expert finding approach.
 Geo-Locating Users. We sample 54 million Twitter user profiles based on the ID range of 12 (starting from Twitter co-founder Jack Dorsey @Jack) to 100 million, as well as 3 billion geo-tagged tweets we collected earlier [14]. For each user, we seek to as-sign a home location ; however, it is widely observed that many Twitter users reveal overly coarse or no location at all in the self-reported location field (see, e.g., [7], [13]). While no approach guarantees a perfect geo-location assignment for each user (due to noise and sparsity in self-reported locations), we adopt a home Figure 3: Example: @jerry lists @BBQsnob with label  X  X BQ X . finding method that relies on a user X  X  geo-tagged tweets akin to a similar approach previously used for check-ins and geo-tagged im-ages [20]. First, we group the user X  X  locations where he posted his tweets into squares of one degree latitude by one degree longitude (covering about 4,000 square miles). Next, we select the square containing the most geo-tagged tweets as the center, and select the eight neighboring squares to form a lattice. We divide the lattice into squares measuring 0.1 by 0.1 square degrees, and repeat the center and neighbor selection procedures. This process repeats un-til we arrive at squares of size 0.001 by 0.001 square degrees (cov-ering about 0.004 square miles). Finally, we select the center of the square with the most geo-tagged tweets as the  X  X ome X  of the user. In total, we geo-locate about 24 million out of the 54 million users (about 45.1%) with fine-grained latitude-longitude coordinates (us-ing a minumum of 5 geo-tagged tweets per user).
 Data Type Total # of Records User Profiles 53,743,459 Geo-Tagged User Profiles (45.1%) 24,252,450 Lists 12,882,292 User List Occurrences 85,988,377 Geo-Tagged List Relationships (17.2%) 14,763,767 Friendship Links 166,870,858 List-peer Relationships 430,186,408 Geo-Labeled List Relationships. Of the 24 million geo-tagged Twitter users, we collect 13 million lists that these users occur on or that these users have created. In total, the 24 million users occur 86 million times in the 13 million lists. Among these 86 million occurrences of a user in a list, almost 15 million of them are geo-tagged, indicating a direct link from a list creator X  X  location to a list member X  X  location. In addition to this network of list relationships, we additionally collect two additional networks around these users: (i) 167 million friendship links connecting these geo-tagged users; and (ii) 430 million links connecting a pair of geo-tagged users that co-occur in the same list.
 Expertise Potential of List Names. We parse the list names that are associated with all 14 million geo-tagged list labeling relation-ships (i.e., links connecting list creator to list member). Table 2 shows the most frequent unigrams. We are encouraged to see that 15 of the 21 most frequent unigrams are related to either people X  X  expertise or interests (the others focus on friendship and celebrity); as has been observed by Kwak et al. [15], Twitter serves as a form of news media as well as a social network, so there is good potential for expertise mining.
 Spatial Patterns of Expertise. What do these geo-tagged lists re-veal? For four example topics  X   X  X ech X ,  X  X ntertain X ,  X  X ravel X , and  X  X ood X   X  we plot in Figure 4 the cumulative distribution of fre-quency of list labeling relationships over distance. That is, how far apart are list labelers from the list labelees? We observe almost 40% of Twitter users who are labelees in a  X  X ood X -relevant list are within a hundred miles to the labelers. However, only about 10% to 15% of the labelees in a list of other three topics are within a hun-dred miles to their labelers. In addition, the average distance be-tween a pair of list labeler and list labelee for  X  X ood X  is also much Table 2: Most frequent words in list names of geo-tagged list labeling relationships entertain 0.50% web 0.48% travel 0.47%
Figure 4: Cumulative frequency of list relationship distances smaller than the average distance for other topics. These obser-vations suggest that certain topics are inherently more  X  X ocal X  and that identifying local experts in topics that are inherently more local could be easier than identifying local experts in other topics.
Based on these encouraging observations  X  (i) that there is a wealth of geo-tagged list data in Twitter; (ii) that these lists tend to focus on areas of potential expertise; and (iii) that distance im-pacts list labeling (and possibly revealing the localness of particular topics)  X  we turn in the next two sections to developing methods for identifying local experts.

Recall that we propose to measure a candidate v i  X  X  local exper-tise by a combination of both the candidate X  X  topical authority and local authority. While there are many ways to integrate these two scores, we propose a simple combination in this first study. We formally define candidate v i  X  X  LocalRank (LR) s ( v i ,q ) in query q as: where s l ( l ( v i ) ,l ( q )) denotes the Local Authority of v cation l ( q ) , and s t ( ~v i ,G,t ( q )) denotes the Topical Authority of v in query topic t ( q ) that is estimated using the candidate X  X  expertise vector ~v i , and the social graph G that the candidate is involved in. In the following two sections we investigate how to estimate these values.
In this section, we present three approaches for estimating a can-didate expert X  X  local authority . The first local authority method considers the distance between an expert candidate X  X  location and the location of interest, capturing the intuition that closer candi-dates are more locally authoritative. The latter two leverage the fine-grained geo-tagged linkages among the sampled Twitter users as revealed through list relationships, where both the list creator and the user being labeled have revealed a precise location. Candidate Proximity. The first (and perhaps most intuitive) ap-proach to estimate candidate v i  X  X  local authority for query q is to use the distance between candidate v i  X  X  location l ( v i location l ( q ) . For example, if we are looking for experts on Austin Barbecue, then all candidates located in Austin will be considered more authoritative than candidates outside of Austin. We define this Candidate Proximity ( s l CP ) as: where d ( l ( v i ) ,l ( q )) denotes the distance between l ( v (using the Haversine formula which accounts for the curvature of the earth), and we set d min = 100 miles. In this case  X  indicates how fast the local authority of candidate v i for query location l ( q ) diminishes as the candidate moves farther away from the query lo-cation. This first local authority approach captures the intuition that closer candidates are more locally authoritative. Figure 5(a) shows a candidate expert in Baltimore (the green pentagon); if we are looking for an expert in New York (the gold star), such a Balti-more candidate X  X  local expertise will be a function of the distance from Baltimore to New York. While simple, this approach cannot capture local expertise of candidates who do indeed live far from a location of interest. As we have mentioned before, Daniel Vaughn is an expert in Austin Barbecue although he lives 200 miles away in Dallas. 1
To capture these cases where expertise is not dictated solely by distance from a candidate to an area of interest, we next propose two local authority methods that consider the distance of the candi-date expert X  X   X  X ore audience X  from the location of interest (that is, to reward candidates who have many labelers near the location of interest, even if the candidate lives far away).
 Spread-based Proximity. The first of these geo-tagged list meth-ods is the Spread-based Proximity that measures the  X  X pread X  of a candidate X  X  core audience X  X  locations compared to the query loca-tion: where v l j denotes one of the core audience V l ( v i ) of candidate v Basically, the  X  X pread X  it measures considers how far candidate v core audience are from the query location l ( q ) on average. If the core audience of a candidate is close to a query location on aver-age, the candidate gets a high score of s l SP . For example, in Figure 5(b), the green pentagon and the gold star represent the expert can-didate X  X  location and the query location, respectively. However, the spread-based proximity for the candidate in the query location em-phasizes the distance of the links (plotted as red arrows) between the candidate X  X  list labelers X  locations (plotted as blue dots) and the query location.
 Focus-based Proximity. In some cases, the spread-based proxim-ity approach may underestimate a candidate X  X  local authority. For example, for a couple of  X  X oodies X  v a and v b both in New York City, suppose v a has a large audience in New York City recognizing his food expertise, and is well appreciated by a lot of people on the west coast, and even abroad; while v b is much less well recognized by the local community in New York City, but has more people rec-ognizing his expertise in mid-east United States, North Carolina, and Florida. Despite a much better local community recognition in New York, user v a has a lower value of spread-based core audi-ence query spatial proximity, due to the higher spatial spread of his
In addition, the home location of an expert candidate may not even be accurate: recall that our home locator estimates a location based on a single user X  X  geo-tagged tweets. In contrast, the following two local authority methods consider the aggregated perspectives of many list labelers, so there is a clearer signal of a candidate X  X  location of expertise. labelers. To overcome this type of expertise underestimation, we propose the Focus-based Proximity as: where r ( l ( q )) represents a radius around a location l ( q ) . This focus-based proximity measures how focused a candidate X  X  audi-ence is in the query location by measuring the percentage of the core audience that resides within the radius of the query location. For example, in Figure 5(c), 4 out of 7 labelers (blue dots) for the candidate (green pentagons) are within the radius (plotted as the red dashed circle) of the query location (gold star), and the focus-based proximity in this case is 4 7  X  0 . 57 .

These two local authority methods  X  the spread-based and focus-based approaches  X  are designed to capture the expert candidate X  X  spatial influence measured via collective intelligence contributed by the people who labeled the candidate.
In this section, we discuss how we estimate the topical authority score of candidate v i being as a local expert in query q . Specifi-cally, we propose to use both the crowd-sourced geo-tagged labels and the social connections between people to quantify a candidate X  X  topical expertise score given a query.
We begin with a topical authority approach that leverages the directly labeled expertise of candidate v i , as revealed through the sampled Twitter lists. Specifically, we adapt the user-centric model that Balog et al. proposed in [3] to estimate the Topical Author-ity Score s t ( ~v i ,G,t ( q )) of v i with respect to the query topic t ( q ) (ignoring for now the social graph G ). Balog et al. applied the user-centric model to identify an expert X  X  knowledge based on the documents (emails and web pages) that they are associated with. In our scenario, we apply the user-centric model to identify expert candidates X  expertise based on the list labels that the crowd has ap-plied to them.

The model is built on standard language modeling techniques: a user v i can be represented by a multinomial probability distribu-tion over the vocabulary of topic words (i.e., p ( t w |  X  denotes a user model). In this case, for each user v i , we infer a user model  X  v i such that the probability of a topic word t to occur in user v i  X  X  list labels can be estimated via p ( t w |  X  v
Given user v i  X  X  user model  X  v i , for a query q , user v cal Authority Score s t ( ~v i ,G,t ( q )) in query q is measured as the probability of query text t ( q ) to be generated from the users X  user model: where t w denotes a topic word in query text t ( q ) .

Since we are expecting that most of the users will be labeled by a small number of unique labels, most of the topic words will have zero probabilities for a particular user v i . Thus we smooth p ( t w |  X  v i ) using the probability of the topic word to occur in the whole corpus of labels p ( t w |  X  C v ) when estimating p ( t Here  X  represents the extent of smoothing. A large value of  X  in-dicates that the probability p ( t w |  X  v i ) is more weighted towards the probability of the topic word t w to occur in the corpus p ( t In the experiments, we fix the value of  X  to 0.1.
In addition to the directly labeled expertise derived from our collection of geo-tagged Twitter lists, we are interested to explore whether the social and list-based connections of Twitter users also provide strong signals of expertise. Specifically, we consider three graphs that include three types of connections: (i) User Friendship; (ii) List-labeling Relationship; and (iii) List-peer Relationship (see the data collection described in Table 1). Recall that each user v characterized as a vector ~v i of his topical expertise generated from the directly labeled expertise method. Can we enrich the expertise signals from the Twitter lists by propagating expertise along these three graphs? The intuition is that people with particular exper-tise have a higher likelihood to be connected to other people with the same expertise, and that having multiple connections to peo-ple with a particular expertise raises the possibility of an individual also having that expertise.
 User Friendship. The first expertise propagation approach is based on user friendship, as represented by a direct link e ( v user v i to user v j . In Figure 6, we show nine expert candidates (plotted as blue dots that are labeled from v 1 to v 9 ). Here, a friend-ship link (plotted as an orange arrow) connects a candidate to an-other candidate that he follows, and an example would be the or-ange arrow on the bottom left from v 4 to v 8 . The motivation for propagation along friendship links is that a candidate has a higher likelihood to be an expert in query topic t ( q ) if he has friend(s) that are also expert(s) in query topic t ( q ) .

Given users X  friendship linkages, we can generate the friendship graph G f ( V,E ) for a set of users V , and a set of friendship links E that connect users in V . For every edge e f ( v i ,v j w ( v i ,v j ) is simply 1 | E of out links from user v i .

In addition, from the perspective of the  X  X irst Law of Geogra-phy X  [24] that  X  X verything is related to everything else, but near things are more related than distant things X , we hypothesize that a user knows a friend nearby better than a friend farther away. Thus, we generate an alternative G f 0 ( V,E ) to reflect the effect of dis-tance between a pair of connected users v i , and v j on how well user v i knows v j (i.e., how much credit v j gets from v troducing the local authority score to the calculation of the weight w 0 ( v i ,v j ) for edge e ( v i ,v j ) : List-labeling Relationship. The second expertise propagation ap-proach considers the list-labeling relationship derived from the sam-pled Twitter lists. The motivation for the propagation here is: if an expert v i in a topic t ( q ) labels another user v j as an expert in the same topic, user v j also has a high likelihood to be an expert in the topic.

For example, user v i lists user v j as a tech expert in one of his lists on Twitter, generating a direct link e l ( v i ,v j dicating a relationship connected by expertise recognition. In this way, a graph G l capturing the expertise recognition can be con-structed. Returning to Figure 6, we show this list-labeling relation-ship (plotted as a red arrow) that links a list labeler to the candidate that he listed, and an example would be the red arrow on the top left from v 1 to v 2 with a list label  X  X eek X .

As in the friendship case, we can similarly construct two graphs  X  one with the weight w l ( v i ,v j ) and the other one with the distance-based weight w l 0 ( v i ,v j ) for the link e l ( v i number of out links from v i in G l , and G l 0 respectively. List-peer Relationship. Finally, we can propagate expertise along peers that appear on the same list. Returning to Figure 6, this list-peer relationship (plotted as a blue arrow) indicates a connec-tion between two candidates that appear on the same list, and ex-amples in the figure are the blue arrows in the middle between v and v 6 with a list label  X  X ech X . This list peer relationship carries an important signal: a person X  X  co-appearance with several top experts on lists further strengthens her topical authority.

Here, we have the link e lp ( v i ,v j ) that directly connects user v to user v j in a list on Twitter. We can measure the weight w lp ( v i ,v j ) for the link e lp ( v i ,v j ) according to the number of out links from v i in G lp . Using all the list peer relationship, we gen-erate a social graph G lp that captures the signals of expertise prop-agated from list peers. We can also generate the corresponding distance-weighted list peer graph G lp 0 .
 Topical Authority Score from Expertise Propagation. Given these three perspectives, we propagate expertise along these graphs through a random walk based on topic-sensitive PageRank (TSPR) [12]. Again, our intuition is that people with particular expertise have higher likelihood to be connected to other people with the same expertise. The random walk approach leverages this intuition by propagating expertise along links in the graph, and by resetting back to the candidates with high directly labeled expertise. Thus, for each particular social graph G described above (that is: G G 0 , G l / G l 0 , or G lp / G lp 0 ), we apply TSPR on the specific social graph to identify the most influential users for a particular query topic t ( q ) . The stabilized TSPR score for each user v ered as user v i  X  X  topical authority score s t ( ~v topic t ( q ) . In our experiments, we explore using both the general social graph, and the distance weighted social graph to identify top local topical experts for a given query.
In this section, we evaluate the proposed local expert finding framework. We seek answers to the following questions:  X  What impact does the choice of local authority have on the  X  Do the three types of expertise propagation over social and list- X  How well does LocalRank perform compared to alternative lo- X  Finally, how do the approaches perform in finding top local ex-
Assessing local expertise is difficult since there is no explicit ground truth data that specifies a user X  X  local expertise given a query (location + topic). Hence, in this section we describe how we constructed our testbed: we first describe the location + topic queries and then introduce the specific expert finding approaches we tested. We discuss how we gathered ground truth to evaluate these approaches, and how we measured approach effectiveness. Queries. In total, we evaluate local expert finding using 56 queries (16 general topic queries and 40 finer topic queries). We consider four general query topics coupled with four locations, totaling 16 topic-location queries. Specifically, we look for local experts in the areas of  X  X echnology X ,  X  X ntertainment X ,  X  X ood X , and  X  X ravel X  in New York City, Houston, San Francisco, and Chicago. We also consider 10 refined topics under the general umbrella of  X  X ood X  and  X  X tartup X , again in the same locations, totaling 40 topic-location queries. These refined topics are  X  X arbecue X ,  X  X eafood X ,  X  X izza X ,  X  X inery X , and  X  X rewery X  under the  X  X ood X  scenario, and  X  X enture capital X ,  X  X ncubator X ,  X  X ounder X ,  X  X ntrepreneur X , and  X  X ngel in-vestor X  under the  X  X tartup X  scenario. By considering both general-topic and finer-topic local expertise queries, our goal is to investi-gate differences in local expertise finding at varying granularities of expertise.
 Approaches for Finding Local Experts. In addition to the pro-posed local expert finding approaches presented in this paper, we consider five alternative baselines. The first considers only a can-didate X  X  topical authority (ignoring local authority):  X  Directly Labeled Expertise ( DLE ): Rank candidates by topical
The next three consider only a candidate X  X  local authority (ignor-ing topical authority):  X  Nearest ( NE ): Rank candidates by distance to the query loca- X  Most Popular in Town by Followers Count ( MP (follower) ):  X  Most Popular in Town by Listed Count ( MP (list) ): Rank can-
The final baseline combines simple versions of topical and local authority:  X  Most Popular in Town by Listed Count on Topic ( MP (on-topic) ):
We compare these five baselines with the proposed LocalRank approach presented in this paper. For LocalRank, we investigate the three approaches for estimating local authority  X  by Candidate Proximity (CP), Spread-based Proximity (SP), and Focus-based Prox-imity (FP)  X  and the Directly Labeled Expertise (DLE) and Exper-tise Propagation (EP) approaches for estimating topical authority. When applying both the Candidate Proximity , and Spread-based Proximity , we preset the d min to be 100 (miles), and alpha to be 2.0. We calculate the local expertise score using the normalized topical authority score and the normalized local authority score. Gathering Ground Truth. To gather ground truth, we employ human raters on Amazon Mechanical Turk. Since there are com-binatorially too many approach + query topic + query location + candidate expert variations, we rely on a sampling method whereby for each experimental setting (an approach + a query topic + a query location), we retrieve the corresponding top-10 local expert candi-dates with the highest local expertise scores, and have human raters on Mechanical Turk label to what extent an expert candidate has local expertise in the query topic and the query location. For each expert candidate, 5 relevance assessors label the candidate X  X  local expertise using a four-scale local expertise rating:  X  Extensive Local Expertise [+2]: The candidate has extensive  X  Some Local Expertise [+1]: The candidate has some expertise  X  No Evidence [0]: The candidate has no clear evidence to be  X  No Local Expertise [-1]: The candidate has neither any exper-
For each assessment, we provide the assessor with the candi-date X  X  user profile, a word cloud generated using the labels that people used to describe the candidate, a heatmap showing the lo-cations of the candidate X  X  labelers, the candidate X  X  most retweeted 5 tweets and 5 most recent tweets. To ensure the quality of these assessments, we follow the conventions suggested by Marshall and Shipman [19]. Each individual HIT (Human Intelligence Task) in-cludes 10 query / expert candidate pairs randomly selected from all the pairs of query and expert candidate. 2 out of the 10 pairs for each HIT are manually labeled by domain experts in order to eval-uate the quality of the feedback from assessors. If an assessor picks a significantly different answer comparing to ours for either one of the two particular pairs, the feedback for the HIT will be discarded. For a particular pair of query and expert candidate, we use the best judgment (i.e., the most voted rating) out of the 5 assessors as the final rating for the pair.

We investigate the inter-judge agreement using both kappa statis-tic and Accuracy . Since we have more than two annotators (five in our scenario) for each query-candidate pair, we adopt Fleiss X  kappa [10], which ranges from 0 (when the agreement is not better than chance) to 1 (when the two annotators agree with each other per-fectly). Following Brants [4] and Nowak et al. [21], we define Accuracy as: Accuracy ( Q pairs ) = where Q pairs represents the set of query and candidate pairs, in which each pair q pair includes both a query q , and an expert candi-date c . An ideal Accuracy would be 1.0 that all the assessors pick the same local expertise rating for every particular pair of query and candidate. For example, an Accuracy of 0.6 indicates that for a query-candidate pair, 60% of the human raters voted for the major-ity choice.
 Metrics. To evaluate each local expert finding approach, we mea-sure the average Rating@10 , Precision@10 , and NDCG@10 across all queries in our testbed. For the following experiments, we con-sider all the 0 and -1 ratings as 0s.

Rating@10 measures the average local expertise ratings by the human-raters for the top 10 ranked local experts across all the queries: where Q pairs represents the set of all query pairs, and rating ( c denotes the most voted local expertise rating for candidate c query q . Rating @10 ranges between 0 to 2, and an ideal ap-proach will have a Rating @10 value 2, which all identifies local experts with extensive local expertise in the query topics and lo-cations. Conversely, the worst performing approach will have a Rating @10 value 0, indicating that the approach only identifies local experts as those with no local expertise or no evidence.
Precision@10 measures the average percentage of candidates that are relevant to the query topic and query location in the top 10 candidates across all the queries. It is defined as: In this paper, we consider expert candidates with both  X  X xtensive local expertise X , and  X  X ome local expertise X  as relevant, while we consider both  X  X o local expertise X  and  X  X o evidence X  as irrelevant. A perfect local expertise estimator has a Precision @10 value of 1.0.

NDCG@10 (Normalized Discounted Cumulative Gain@10) mea-sures how well the predicted local expert rank order is compared to the ideal rank order (i.e., candidates are ranked according to their actual local expertise) for the top 10 results across all the query pairs. NDCG @10 ranges between 0 and 1, and a higher value indicates an approach that generates better rank orders.
Overall, we have 11,285 individual judgments made by the hu-man raters. How consistent and reliable are these judgments? We report the kappa (  X  ) and Accuracy values in Table 3. When consid-ering 3 rating categories for each pair (2:  X  X xtensive local exper-tise X , 1:  X  X ome local expertise X , and 0: either  X  X o local expertise X  or  X  X o evidence X ), the overall Accuracy for agreement is 0.716, in-dicating that for a pair of query and candidate, on average 71.6% of the human raters voted for the majority vote. This demonstrates good user agreement and is significantly higher than accuracy by chance (33.3% for three categories). When considering only 2 rat-ing categories (2 and 1 as relevant, and 0 as irrelevant), the over-all Accuracy increases to 82.2%, which is also much higher than the accuracy by chance (50% for two categories). For kappa, we see that the overall value is 0.280 in the 3 rating category case. For the binary rating case, the overall kappa value is 0.397. Both kappa statistics are typically considered  X  X air X  inter-judge agree-ments. Together, these kappa and Accuracy values suggest that these human raters have a fairly reasonable agreement. And we observe both much higher Accuracy and kappa value for binary rating categories, which indicates that raters find it easier to decide whether a candidate has local expertise or not, rather than deter-mining the extent of a candidate X  X  local expertise.
In this subsection, we seek answers for the questions brought up in the beginning of this section, with four set of experiments: (i) General Topics Accuracy  X  Accuracy  X  0.715 0.279 0.818 0.393 Table 4: LocalRank: Evaluating the three local authority ap-proaches Local Authority Rating @10 Precision @10 NDCG @10 evaluating the performance of local authority metrics; (ii) studying the impacts of expertise propagation; (iii) comparing the perfor-mance of baseline approaches and the LocalRank approaches; and (iv) evaluating the performance of expert finding via finer topics.
To begin with, we seek to understand the impact of the local authority approach on the quality of local expert finding in Local-Rank. Specifically, we fix the LocalRank topical authority as the Directly Labeled Expertise, while we vary the local authority across the three approaches presented in Section 4: Candidate Proxim-ity (CP), Spread-based Proximity (SP), and Focus-based Proximity (FP). Our goal is to understand to what degree the local authority affects local expert finding, and to assess if (and how much) the crowdsourced geo-tagged list labels impact local authority. We present in Table 4 the Rating @10 , Precision @10 , and NDCG @10 for each of the three local authority approaches. We observe that both of the approaches (SP and FP) that utilize the lo-cations from the candidates X  core audience significantly improve the performance of local expert finding in comparison with the candidate proximity approach (CP) that only takes the candidate X  X  physical location into consideration. Using candidate proximity (CP), the LocalRank approach only identifies true local expert 55% of the time on average among the top 10 candidates. Similarly, we see comparatively low values of Rating @10 as 0.952, and NDCG @10 as 0.685. In contrast, the Spread-based Proximity (SP) and Focus-based Proximity (FP) approaches reach Precision @10 of almost 85%, Rating @10 over 1.33, and NDCG @10 of 0.90. This indicates the core audience for an expert candidate is crucial to estimating a candidate X  X  local authority. And in absolute terms, the rating scores for both approaches range between  X  X ome local expertise X  (1) and  X  X xtensive local expertise X  (2), indicating that these approaches can identify candidates who are actually local experts. Interestingly, we see for this evaluation framework that the two approaches perform nearly equally well, although they capture two different perspectives on local authority (recall that SP con-siders the average distance of labelers, whereas FP considers the fraction of labelers within a radius).
Given these results for local authority, we next consider the im-pact of expertise propagation on the topical authority (and ulti-mately on the quality of local expert finding). As described in Sec-tion 3.2, we explore whether the three types of social and list-based connections of Twitter users do indeed provide strong signals of expertise. We consider the (i) friendship graph, (ii) list-labeling relationship graph, and (iii) list-peer relationship graph. For each graph (both with and without distance-weighted edges), we apply the topic-sensitive PageRank algorithm to propagate expertise. For each particular graph as well as a particular type of edge weight, we iterate the damping factor from 0.10 to 0.30 to 0.50 to study how the damping factor affects the task of finding top local experts. A smaller damping factor indicates less score propagation and more random walking among more topic-relevant nodes in the graph. We find that the conventional damping factor value (0.85 or 0.90) finds only national celebrities like @JimmyFallon (Jimmy Fallon, host of talk show Late Night with Jimmy Fallon), @TheEllenShow (Ellen Degeneres, host of the Ellen Degeneres Show), and @Jack (Jack Dorsey, Twitter and Square co-founder) no matter what the query topic is. With a smaller value of damping factor, we hope to iden-tify more topical relevant local experts.

We present in Figure 7 the local expertise results for expertise propagation using the Friendship graph as input, coupled with cor-responding parameter settings. We vary the choice of local author-ity (CP, SP, and FP), the use of distance-weighted links or not, as well as the choice of damping factor. This figure focuses on Precision @10 , while the subsequent Table 6 includes Rating @10 , Precision @10 , and NDCG @10 for all graph types. First, in terms of the damping factors, we see that across all settings (0.10, 0.30, and 0.50), that the best performing result is comparable. How-ever, we do observe a significant performance drop for damping factor 0.50 using regular edge weight that does not consider dis-tance between the nodes as a factor. Upon investigation into the top local expert candidate under this setting, we observe that many of the top local candidates are national celebrities (e.g., @Jimmy-Fallon, @TheEllenShow, and @Jack), compared to the candidates retrieved using a damping factor of 0.10 or 0.30. We attribute this result to the higher weight on score propagation through general friendship edges. On the other hand, for a damping factor 0.10 or 0.30, most of the scores are propagated through topic-relevant nodes via random walking.

Second, we observe a slight improvement for distance-based edge weight when using a damping factor of 0.10 or 0.30 rather than us-ing the regular edge weight. And we observe a dramatic improve-ment of performance for distance-weighted edge weight using a damping factor of 0.50 than the alternative version. This indicates that giving local friends more credit (in terms of expertise propa-gation flowing more strongly to nearby friends than far away ones) does help improve the likelihood to find better top local experts.
Third, in terms of the choice of location authority metric, we observe a similar result to what we observed in the previous section  X  that the approaches (SP and FP) that utilize the locations from the candidates X  core audience significantly improve the performance of local expert finding.

Finally, compared to the simpler approach of not propagating expertise at all, but just using the directly labeled expertise, we see that the results are quite similar (with Precision @10 near 0.84). Given this result, we compared the lists of top-10 local experts re-turned by LocalRank using directly labeled expertise versus Lo-calRank using each one of the expertise propagation approaches. While the overall precision is similar, the experts that each ap-proach finds are different: we find an average Jaccard coefficient between local expert lists of around 60 to 80%. In other words, on average, 20 to 40% of the top-10 local experts for the same query are different, when we compare the directly labeled exper-tise approach versus a particular expertise propagation approach. This indicates that the expertise propagation approaches are bring-ing in new signals of local expertise from the social and link-based connections of users; in our continuing work we are investigating methods to integrate these two types of topical authority by finding more diverse experts from each of these alternative approaches.
So far we have investigated the impact of local authority and the impact of topical authority on the quality of local experts found by the LocalRank framework. In this section, we compare LocalRank to the five alternative local expert finding approaches described in the experimental setup over the set of 10 general topics.
We first report the results for the five baselines in Table 5. We see that relying solely on topical authority  X  Directly Labeled Ex-pertise (DLE)  X  with no notion of localness, results in a very low Rating @10 , Precision @10 , and NDCG @10 . Similarly, rely-ing solely on local authority  X  Nearest (NE), Most Popular in Town by Followers Count (MP followers), and Most Popular in Town by Listed Count (MP list)  X  with no notion of topical authority also leads to very poor results. Since local experts are defined both by their localness and their on-topic expertise, these results confirm our intuition driving the LocalRank approach to combine both fac-tors. The baseline that does incorporate both factors  X  Most Popular in Town by Listed Count on Topic (MP (on-topic))  X  captures this notion of local expertise by rewarding candidates who have been listed on many Twitter lists on the topic of interest within a partic-ular location. We see in the table that this approach significantly outperforms the single factor alternatives ( Rating @10 of 1.059, Precision @10 of 0.628, and NDCG @10 of 0.750).
 We compare all five of these baselines to two versions of Local-Rank. Both consider local authority based on Spread-based Prox-imity (SP); one uses directly labeled expertise (SP + DLE), while the other uses expertise propagation (SP + EP + Friendship) over the friendship graph. We see similar qualitative results when eval-uating Focus Proximity (FP) and alternative expertise propagation approaches. Both approaches significantly outperform the four sin-gle factor baselines, as well as significantly outperforming the best alternative incorporating both local and topical authority, MP (on-topic). We see for LocalRank (SP + DLE) a Rating @10 of 1.334, Precision @10 of 0.842, and NDCG @10 of 0.896. For Local-Rank (SP + EP + Friendship), we have Rating @10 of 1.354, Preci sion @10 of 0.838, and NDCG @10 of 0.884. These results con-firm the effectiveness of the LocalRank approach and the impor-tance of carefully leveraging the large-scale geo-tagged list rela-tionships on Twitter.

Continuing this investigation, we report the results of the dif-ferent LocalRank approaches versus the best performing baseline in in Table 6. We see that the Expertise Propagation approaches generally perform slightly better than the Directly Labeled Exper-tise approach in terms of Rating @10 and Precision @10 . This suggests that adding in social connections bring in more signals to identify top local experts. In particular, LocalRank with expertise propagation coupled with the social graph of list-labeling relation-ships generates the best performance, with Rating @10 of 1.354 (an improvement of 27.6% over MP (on-topic)), Precision @10 of 0.847 (an improvement of 34.9%), and NDCG @10 of 0.886 (an improvement of 18.1%). However, in terms of NDCG @10 , we see that the simpler DLE approach performs slightly better. But in all cases, the LocalRank approach outperforms the alternative. Table 7: Comparing LocalRank versus the best performing al-ternative over finer topics
Finally, we drill down from general topics to more fine-grained topics, to investigate the ability of local expertise finding approaches to handle these more specific cases. Here we evaluate the proposed LocalRank approaches via the refined topics under the  X  X ood X , and  X  X tartup X  scenarios. We report the performance using the best pa-rameter settings for each of the proposed approaches. In this exper-iment, we set local authority as using Spread Proximity and exper-tise propagation relies on a damping factor of 0.30.
 Table 8: How well does LocalRank perform on finer topics?
Query Topic Rating @10 Precision @10 NDCG @10 barbecue 0.631 0.404 0.787 seafood 0.825 0.525 0.868 pizza 0.775 0.425 0.712 brewery 1.178 0.738 0.928 winery 0.763 0.475 0.744 entrepreneur 1.248 0.800 0.921 venture capital 1.180 0.663 0.956 angel investor 0.923 0.638 0.846 incubator 0.660 0.413 0.732 founder 0.995 0.688 0.786
Table 7 presents the local expert finding results for the four types of LocalRank versus the best performing alternative (MP (on-topic)). We observe that once again the LocalRank approaches outperform the best-performing alternative in all cases. However, we notice that the performance for these finer topics is worse than what we observed for the more general topics. For example, LocalRank with Directly Labeled Expertise performs the best with Rating @10 of 0.924, Precision @10 of 0.583, and NDCG @10 of 0.851 over these finer topics. But the same approach over the more general topics results in an average Rating @10 of nearly 0.4 points higher. Similarly, we see improved performance over the other metrics in the general topic case. We believe these results reflect two chal-lenges: (i) First, it is fundamentally more challenging to identify local experts for more refined topics. For example, it may be eas-ier to assess whether someone is a  X  X ood X  expert, rather than that they are an expert in a specific topic like  X  X arbecue X . (ii) Second, there is inherent data sparsity at the level of these finer topics. The number of candidates for a finer topic in a query location is much smaller compared to the number of candidates for a general topic in the same query location. For example, we observe that the ap-proaches consider the probable No. 1 barbecue expert in Texas  X  Daniel Vaughn  X  as a local expert for barbecue for query locations of Chicago and San Francisco, in addition to his natural expertise in Houston. For these two distant locations, Vaughn is often a top choice since there are few barbecue candidates recognized in the location.

In our continuing work, we are investigating the contours of ex-pertise across the country, so that topics with a strong regional fac-tor (like Barbecue, with its traditional centers in Texas, North Car-olina, and the Midwest) can be balanced with topics of expertise MP (on-topic) 1.059  X  0.628  X  0.750  X  LR: DLE + Local Authority 1.334 26.0% 0.841 33.9% 0.897 19.6% LR: EP + Friendship Graph 1.354 27.6% 0.838 33.4% 0.884 17.9% LR: EP + List-labeling Graph 1.354 27.6% 0.847 34.9% 0.886 18.1%
LR: EP + List-peer Graph 1.345 27.0% 0.844 34.4% 0.887 18.3% that are found nearly everywhere (e.g., the more general  X  X oodies X ). Along these lines, we show in Table 8 the results of LocalRank (SP + DLE) for each of the fine-grained topics. As we observed in our original investigation of Twitter lists, where we observed topics like  X  X ood X  being more local than topics like  X  X echnology X , here we see great variation in local expertise finding across these different subtopics.
The exponential growth in social media over the past decade has recently been joined by the rise of location as a central or-ganizing theme of how users engage with online information ser-vices and with each other. Enabled by the widespread adoption of GPS-enabled smartphones, users are now forming a comprehen-sive geo-social overlay of the physical environment of the planet. In this paper, we have argued for leveraging these geo-spatial clues embedded in Twitter lists to power new local expert finding ap-proaches. We have proposed and evaluated the LocalRank frame-work for finding local experts, by integrating both a candidate X  X  local authority and topical authority. We have seen that assessing local authority based on the spread and focus-based proximity of a candidate X  X   X  X ore audience X   X  that is, the users who have labeled him  X  can lead to good estimates of local authority and ultimately to high-quality local expert finding. Through an investigation of 56 queries coupled with over 11,000 individual judgments from Amazon Mechanical Turk, we have seen high average precision, rating, and NDCG in comparison with alternatives. In our continu-ing work, we are interested to (i) further investigate the borders of  X  X ocalness X  by investigating when an expert is considered a local expert versus a regional expert; (ii) enhance our current LocalRank approach with temporal signals to capture expertise evolution; and (iii) incorporate the detected local experts into a prototype system that can direct information needs to local experts who are consid-ered authoritative and responsive on the local topic of interest.
This work was supported in part by NSF grant IIS-1149383. Any opinions, findings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsors.
