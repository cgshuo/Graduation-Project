 In visually-oriented specialized medical domains such as dermatology and radi-ology, physicians explore interesting image cases from medical image repositories for comparative case studies to aid clinical diagnoses, educate medical trainees, and support medical research. This image browsing and lookup could benefit from a grouping of medical images that is consistent with experts X  understand-ing of the image content. However, it is challenging, because medical image interpretation usually requires domain knowledge that tends to be tacit. There-fore, to make expertise more explicit we propose an interactive machine learning paradigm that has experts in the loop to improve image grouping. Particularly, dermatologists encode their domain knowledge about the medical images by grouping a small subset of images via an interface. Our learning algorithm auto-matically incorporates these manually specified connections as constraints for re-organizing the whole image dataset. In this way, the computational evolution of an image grouping model, its visualization, and expert interactions form a loop to improve image grouping. A user evaluation study shows that this paradigm improves image grouping based on expert knowledge.
 In order to minimize human efforts and provide experts with a good starting point to group images, we create an initial image grouping using a multimodal expert dataset described in Sect. 2 [ 18 ]. This initial image grouping is learned through a multimodal data fusion algorithm flexible to incorporate new images [ 11 ]. From here, the loop to improve image grouping begins (see Fig. 1 ). An expert can inspect the image grouping and choose to improve it through an interface. The interface design and the supported expert image manipulations are presented in Sect. 3 . The interface then parses expert manipulations as implicit constraints by the rules described in Sect. 5 and incrementally learns the model, and visualizes the new image grouping using the techniques in Sect. 4 .Anexpert-in-the-loop evaluation study is described in Sect. 6 . Related studies, including visual (text) analytics and interactive machine learning systems, are compared with our learning paradigm in Sect. 7 .
 The initial image grouping was learned from an offline collected expert dataset. To elicit expert data, 16 physicians were asked to inspect 48 medical images and describe the image content aloud towards a diagnosis, as if teaching a student who was seated nearby [ 17 ]. Their eye movements were recorded, as eye movement features highlight perceptually important image regions, which is especially useful in knowledge-rich domains [ 9 ]. In this paper, we use experts X  eye fixation map to filter image features (SIFT features [ 16 ]). See Fig. 2 for an example. A bag of visual words is created from the remaining image features, and each image is described by a histogram of the visual words. Physicians X  verbal image descriptions were also recorded concurrently, as they provide insights into experts X  diagnostic image understanding. Figure 3 shows a sample transcription. The medical concepts were extracted from the transcriptions using MetaMap, a medical language processing resource [ 1 , 10 ]. These concepts formed a high dimensional feature space, in which each image is described by the occurrences of these medical concepts.
 analysis (LSA) or latent Dirichlet allocation (LDA). LSA does not perform as well as Laplacian sparse coding to cluster images by object [ 3 ]. LDA is affected not only by an initial specification but also by the samples randomly generated at each iteration [ 4 ]. It does not support users to make incremental changes, due to the inconsistent results obtained from multiple runs. To initialize an image grouping based on the features extracted from multiple modalities, we adopt a data fusion framework based on Laplacian sparse coding [ 11 ]. The objective function is presented in Eq. ( 1 ). Matrices E V  X  R n v  X  m are eye gaze-filtered image features and verbal features, respectively ( n being the number of visual words, n v being the number of verbal features, and m being the number of images). This model provides flexibility to allow extra data modalities by adding terms like the first two in Eq. ( 1 ). The coefficient matrix C  X  R k  X  m ( k being the number of latent topics) stores the new image representations, each of which is a distribution of latent topics learned and stored in the basis matrices P  X  R n e  X  k and Q  X  R n v  X  k . The matrices P and Q reveal the transformation from the original feature spaces to latent topics. where S (  X  ) represents a sparsity constraint ( l 1 -norm), and graph-regularizer. These constraints form the Laplacian sparse coding that helps capture underlying semantics behind observations in both modalities [ 14 , 20 ]. W is a neighboring matrix that indicates similarities between pairs of data instances. A multimodal variation of the feature-sign search algorithm oped to selectively update some elements of each data instance to tackle the non-derivativeness of the l 1 -norm [ 14 ]. Since the sparse codes learned through general-purpose machine learning algorithms usually do not reflect ideal expert image understanding [ 12 ], we extend this framework with extra constraints from expert knowledge to improve semantic image representations. The initial image grouping purely based on offline collected expert data is first visualized in the Older Image Organization in Fig. 4 (panel 1-a) for experts to inspect and manipulate. In the case where domain expert users need further information on the current image grouping, we provide two extra visualizations. First, experts can see an image cluster and the top features contributing to this cluster (see Fig. 6 ). Second, experts can click the buttons in Fig. 4 (panel 3) to compare the image grouping obtained when using different subsets of features, such as only primary morphology terms 1 (Fig. 5 a), with that using the whole feature set; see Fig. 4 (panel 1-a).
 Experts have two options to improve the image grouping in each round. First, they can directly drag images toward or apart from each other in Fig. 4 (panel 1-a). The system parses such expert inputs and incorporates them for updat-ing the neighboring graph-based regularizer (see Sect. 5.1 ). Second, experts can select a topic from the listbox in Fig. 4 (panel 5), and indicate the least relevant image(s) according to the vocabulary distribution of the selected topic. Based on such expert inputs, the system updates the image-topic distribution matrix (see Sect. 5.2 ). After experts interact with the interface using either option, the image grouping in the previous round is copied to Fig. 4 (panel 1-a), and the improved one is shown in Fig. 4 (panel 1-b). In each round, both image groupings are visualized following the approaches discussed in Sect. 4 . To comprehensively visualize the image grouping, our interface presents both a graph view shown in Fig. 4 (panel 1) and a matrix view shown in Fig. 4 (panel 2). Both views are automatically updated during expert interactions. ding (t-SNE) algorithm [ 19 ]. It better visualizes the high dimensional struc-ture of image grouping in 2D graph view than other dimensionality reduction techniques, such as principal component analysis (PCA) [ 4 ]. We use a metaphor to imply to experts that more similar images are spatially closer. How-ever, this metaphor does not proportionally reflect all pairwise image similari-ties in high-dimensional space, because of the difficulty to retain the whole data structure for any dimensionality reduction algorithms. To tackle this issue, our interface allows experts to see an image and its high dimensional close neighbors in 2D visualization. The popup window visualizing these neighbors are illus-trated in Fig. 6 . The interface also presents a matrix view that serves to give an overview of the pairwise image similarities, because it is impractical that experts choose to see the close neighbors of all images in a 2D graph view. See Fig. 7 for a magnified matrix view. The matrix view provides a global indexing of pairwise image similarities in the learned representation. There are mainly two approaches in prior studies allowing user interac-tions to help improve learning a model: document-level interactions [ 4 , 15 ], or topic/cluster-level interactions [ 5 , 6 ]. In our scenario, to improve medical image grouping, the documents are images. To develop this interface, we prefer docu-ment (image)-level interactions for two reasons. On the one hand, the medical conditions are more intuitive in the form of images than texts to physicians. On the other hand, the topics we learned offline based on a multimodal expert dataset are not easily visualizeable nor interpretable by physicians. Below are two functions in the interface for receiving expert inputs and updating the model, both to support image-level interactions. 5.1 Constraint on Neighboring Matrix, W Let the images in the original feature space be denoted as x neighbor graph G with m vertices can be constructed. A heat kernel can be used to compute the element W ij in the neighboring matrix W of the graph G [ 2 ]. If x i and x j are identical, then W ij equals 1; and if they are extremely different, then W ij asymptotically approaches 0. The interface can encode expert image manipulations as a transformation of the neighboring matrix W . This transformation is determined by multiple factors, including previous image grouping and experts X  interpretation of it. The transformation of W can be simplified as F (  X  ,  X  )inEq.( 3 ) and be considered as a constraint set by experts to guide the learning process. where K denotes the set of images selected by an expert in Fig. 4 (panel 1-a). In this paper, we use hard constraints , i.e., by moving one image toward or away from another, experts can connect or disconnect them in the model. Such expert constraint essentially sets a boundary regarding pairwise image similarities. Once an expert begins to connect these images, the system sets all W i = j ) to be 1. Likewise, W ij  X  X  ( i, j  X  K , i = j )areallsettobe0,ifthey should be grouped differently. This rule is designed to update the neighboring matrix W in Eq. ( 2 ). Once all W ij  X  X  specified by the expert are updated, the algorithm will trigger the further learning process for the image representation C and the visual and verbal topics P and Q with respect to the objective function in Eq. ( 3 ). 5.2 Constraint on Topic-Coefficient Matrix, C Experts can also improve the image grouping through the task illustrated in Fig. 4 (panel 5). For each topic selected by experts in the listbox, its top terms in the topic-term distribution are listed. The list of top terms explains the gist of the topic to experts. The images that are considered highly relevant to the selected topic by the algorithm are then displayed at the bottom. The task for experts is to submit the least relevant image(s) to the topic to disconnect its/their link(s) to the topic. After experts have indicated the least relevant image(s), the system updates the coefficient matrix C according to the constraint in Eq. ( 4 ). where T is the collection of selected topics, and L ( i ) represents the least relevant images for topic i . In this paper, the element C ij will be set to 0, if image j is selected to be least relevant to topic i . Once all C ij  X  X  are updated, the algorithm begins to learn P , Q and C further with respect to Eq. ( 4 ).
 matrix C , the model is learned incrementally, and it is consistent between suc-cessive interactions. In order for experts to work on consistent image groupings, we also keep the visualization consistent between successive interactions. This is achieved by storing the 2D coordinates of images and using them as the starting point in the graph view (Fig. 4 (panel 1)) for the next interaction [ 19 ]. To evaluate the effectiveness of the paradigm per expert X  X  objectives, a domain expert (co-author) was asked to provide a reference image grouping matches her overall understanding of the relationships between medical images in the database. In particular, for each image she listed its most similar images in terms of their differential diagnoses. We designed an experiment to compare the image grouping performances between the results of fully automated machine learning and our expert-in-the-loop paradigm. For fully automated learning (case 1), the resulting image grouping was estimated by our model without expert inputs. In our paradigm (case 2), the physician interacted with the model in the loop towards a better image grouping result. She manipulated the images based on her medical knowledge and the clinical information presented in these images. To quantitatively evaluate the image grouping performances, we retrieved the image neighbors and compared them to the corresponding reference image group-ing for both cases.
 based on which the machine learns from experts. The image groupings with expert interactive constraints consistently outperform the traditional learning case. In particular, our paradigm performs much better than fully automated learning with verbal feature of correct diagnosis (CD). This suggests that diag-noses are the primary factor considered by the expert to group medical images. For both cases, eye tracking filters boost the performance of image features (e.g., 12.5 %  X  17.86 %). Furthermore, learning from multimodal features achieves the best performance for both cases. We also elicited the expert X  X  qualitative evaluation through an interview. The expert noticed the improvement of each iteration. Centroid-based clustering algorithms (e.g., K-means) and connectivity-based clustering algorithms (e.g., hierarchical clustering [ 7 ]) are used for com-parison purposes. Since these algorithms are not easily applied to the multiple modalities, their multimodal performances are omitted. Their performances fall behind that of Laplacian sparse coding, and this suggests that Laplacian sparse coding is a good learning framework. Density-based and distribution-based algo-rithms do not work because of the small number of data instances.
 During the paradigm evaluation, we also recorded the expert X  X  verbal labeling of the image groups. The labeling of image groups is useful to disclose her diag-nostic reasoning while grouping images. This can be incorporated in future work to optimize the semantic feature space. Another important part of our future work involves implementing our paradigm on a larger dermatological image data-base with more experts in the loop to test our paradigm X  X  robustness. An image hierarchy can be learned and visualized. For the ease of expert interactions, a few representative images can be selected from each group. In the case where new images do not even have offline annotations, they can still be positioned in an existing image grouping for further improvements, since single-modal features can be easily projected into the unified topic space [ 11 ].
 The presentation of image groupings could also be based on experts X  trade-off between various factors, such as the primary lesion morphology and the causes of the diseases. Our current visualization may not be feasible for a larger database. It is necessary to design a more effective visualization strategy to allow experts to explore both global structure and local details of image grouping. By replacing the hard constraints in Eq. 3 with soft ones, the parameters in neighboring graph can also be learned. In order to balance the influences between the offline collected expert data and online expert inputs, soft constraints could be applied by encoding expert interactions in a new penalty term. Existing systems that allow interactive user visual analysis usually adopt topic modeling techniques [ 4  X  6 , 13 , 15 ]. Original features are reduced to a lower-dimensional topic space, in which documents are grouped. One type of such system, including UTOPIAN [ 4 ] and iVisClustering [ 15 ], visualizes the topics, so that users can adjust the topic-term distribution at the term granularity. In contrast, our paradigm focuses experts on natural high-level image group-ing tasks and encodes expert image manipulations as constraints to improve the overall image grouping. Besides, in our domain the objects for experts to interact with are medical images rather than latent topics, which may be confusing to the experts. Another type of system, including LSAView [ 6 ] and iVisClassifier [ 5 ], involves document-level interactions. These systems require users to change the parameters of the algorithms. In contrast, our system updates the underlying topic model based on experts X  natural manipulations of the images. This paper presents an interactive machine learning paradigm with experts in the loop for improving image grouping. We demonstrate that image grouping can be significantly improved by expert constraints through incremental updates of the underlying computational model. In each iteration, our paradigm allows to accommodate our model to experts X  input. Performance evaluation shows that expert constraints are an effective way to infuse expert knowledge into the learning process and improve overall image grouping.

