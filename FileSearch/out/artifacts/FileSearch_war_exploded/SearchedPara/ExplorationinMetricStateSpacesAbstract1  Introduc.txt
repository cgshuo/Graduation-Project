 Gatsby Unit Uni versity Colle ge London London, England John Langf ord JCL @ CS . CMU . EDU Recent years have seen the introduction and study of a number of representational approaches to Mark ov Decision Processes (MDPs) with very lar ge or infinite state spaces. These include the broad family kno wn as function approxi-mation, in which a parametric functional form is used to ap-proximate value functions, and direct models of the under -lying dynamics and rewards, such as factored or Dynamic Bayes Net (DBN) MDPs. For each of these approaches, there are now at least plausible heuristics, and sometimes formal analysis, for problems of planning [2] and learning. Less studied and more elusi ve has been the problem of global explor ation , or managing the exploration-exploitation trade-of f. Here the goal is to learn a (glob-ally) near -optimal -step polic y in an amount of time that has no direct dependence on the state space size, but only on the comple xity of the chosen representation. Global exploration was first solv ed in the deterministic finite-state setting [9, 10] and then progress slo wed. It is only recently that pro vably correct and efficient al-gorithms for exploration in small nondeterministic state spaces became kno wn (such as the algorithm[4 ] and its generalizations[5 ]). This approach has been generalized to factored MDPs under certain assumptions [3], but there remain man y unresolv ed questions regarding efficient ex-ploration in lar ge MDPs, including whether model-based approaches are required 1 .
 algorithm has a polynomial dependence on the size of the state (see [7] for a more formal statement). Hence, to obtain near -optimal algorithms with sub-linear dependence on the size of the state-space further assumptions and restrictions on the MDP must be made. The factored algorithm [3] considers one restriction where the MDP are represented in terms of a factored graph ( ie a dynamic Bayes net). Here, the number of steps the agent must act in the MDP in order to obtain a -step near optimal polic y is polynomial in the representation size of the factored graph.
 In this work, we examine the problem of exploration in en-vironments in which there is a metric on state-action pairs with the property that  X  X earby X  state-actions can be useful in predicting state-action dynamics. Such conditions are common for navigation or control problems, but may be more broadly applicable as well. Given suf ficient  X  X earby X  experience to predict outcomes, we have an implicit non-parametric model of the dynamics in a neighborhood of the state-action space. These implicit models can be  X  X ieced together X  and used for planning on a subset of the global space.
 One natural approach in the lar ge-state space setting is ag-gre gate state methods which group states together and as-sume Mark ov dynamics on these aggre gate states [12 , 13]. Clearly , this approach is useful only if a compatible set of aggre gate states can be found which preserv e the Mark ov dynamics on these aggre gate states and where the size the aggre gate state space is considerably smaller than that of the underlying state space. A benefit of this approach is that planning under this model can be done with traditional dynamic programming approaches on the aggre gate states. Unfortunately , in man y navigation domains, it appears that sumption required for planning in aggre gate state methods (and we pro vide one such example later).
 The local modeling assumption is not equi valent to an ag-gre gate state method since we do not group any states to-gether and do not assume a Mark ov property holds for ag-way, unlik e in aggre gate state methods. Hence, the com-putational problem of planning is still with us strongly . As with factored , we assume a  X  X lack box X  planning algo-rithm to abstract away the dif ficulty of planning from that of exploration. This assumption is not meant to trivialize the planning problem, but is made in order to isolate and quantify the dif ficulty of exploration.
 Given the ability to plan, we pro ve that the local modeling assumption implies the time required for global exploration depends only on the metric resolution and not on the size of the state space. More precisely , we give a generalization of the algorithm for metric MDPs which learns a (glob-ally) approximately optimal -step polic y in time depend-ing only on the covering number s , a natural and standard notion of the resolution required for local modeling under the metric.
 Metric MDPs are a natural complement to more direct parametric assumptions on value functions and dynam-ics. These results pro vide evidence that, as for fac-tored environments[3 ], effecti ve exploration mechanisms are available for metric MDPs. We work in the standard MDP setting. Let be the probability of a state given an action and state . Let For simplicity , assume that all rewards are determinis-tic and fall in the interv al ward recei ved over steps starting from state while act-ing under in MDP We first formalize the assumption that there is a no-tion of distance that permits local modeling of dynam-ics. Thus, let tween two state-action pairs. The results require that this metric obe y metry ( i.e., ity. This is fortunate since demanding the triangle inequal-ity limits the applicability of the notion in several natural scenarios. Let the metric.
 We now pro vide a standard definition of coverings under a metric. An the property that for any such that the lar gest minimal such that the remo val of any longer a cover.
 Our first assumption is that the metric permits local mod-eling of dynamics of an MDP and reward function : Local Modeling Assumption. There exists an algo-rithm Model for the MDP if Model is given wards all maximum running time of Model .
 Thus, with a suf ficient number riences, Model can form an accurate approximation of the local environment. Note that there is no requirement that a destination state ask only that nearby state-actions permit generalization in nearby states. The next subsection pro vides natural exam-ples where the Local Modeling Assumption can be met, but we expect there are man y rather dif ferent ones as well. In addition to an assumption about the ability to build lo-cal (generati ve) models, we need an assumption about the ability to use such models in planning.
 Appr oximate Planning Assumption. There exists an al-gorithm, Plan , which given a generati ve model for an un-kno wn MDP erage reward , where plan upper bound the running time of Plan and bound the calls to the generati ve model.
 Note that the Local Modeling Assumption does not reduce computational resources may be required to meet the Ap-proximate Planning Assumption. The purpose is not to it away from the problem of exploration-e xploitation. The same approach was necessary in analyzing factored-. There are at least three broad scenarios where this assump-tion might be met. The first is settings where specialized planning heuristics can do approximate planning due to strong parametric constraints on the state dynamics. For example, the recent work on planning heuristics for fac-tored MDPs is of this form. The second is the spar se sam-pling [6] approach, in which it has been sho wn that the Ap-proximate Planning Assumption can in fact be met for arbi-trary finite-action MDPs by a polic y that uses a generati ve model as a subroutine. Here the sample comple xity exponential in per state visited (see [6]), but has no de-pendence on the state space size. The third setting requires a regression algorithm that is capable of accurately estimat-ing the value of a given polic y. This algorithm can be used iterati vely to find a near -optimal polic y [8]. At a high level, then, we have introduced the notion of a metric over state-actions, an assumption that this metric permits the construction or inference of local models, and an assumption that such models permit planning. We be-lieve these assumptions are broadly consistent with man y of the current proposals on lar ge state spaces. We now pro vide an example that demonstrates the role of covering numbers, and then sho w that these assumptions are suf fi-cient for solving the exploration-e xploitation problem in time depending not on the size of the state space, but on the (hopefully much smaller) covering numbers under the metric. 2.1 An Example We can imagine at least two natural scenarios in which the Local Modeling Assumption might be met. One of these is where there is suf ficient sensor information and adv ance kno wledge of the expected effects of actions that the local modeling assumption can be satisfied even with a simple example, people can typically predict the approxi-mate effects of most physical actions available to them im-mediately upon entering a room and seeing its layout and dictions for unf amiliar distant rooms. Consider the MDP where the state space is the Euclidean maze world sho wn in Figure 1.(a), and where the agent is equipped with a vi-namics can be predicted at any  X  X een X  location. To apply this analysis, we must first specify a metric. The obvious choice is sight between and and otherwise. Note that this (a) (b) (c) metric satisfies symmetry , but not the triangle inequality (which would be some what unnatural in this setting). For any number of points which can be positioned in the space so that no pair have line-of-sight. One maximal set is given by the dots in Figure 1.(b). Note that even though this a con-tinuous state space, the covering number is much smaller , and naturally determined by the geometric properties of the domain.
 It is unrealistic to assume that local dynamics are mod-eled at distant locations as well as near locations which im-plies that modeling error gro ws with distance. In this case, a reasonable alternati ve is to define a constant controlling the rate of modeling error with Eu-clidean distance. Using this metric, the covers sho wn in Figure 1.(c) might naturally arise. Note that (in general) we are free to use actions as well as states in defining the metric.
 The abo ve examples are applicable to the of the Local Modeling Assumption. The second natural case is the more general  X  X earning X  setting, in which the next-state dynamics permit some parameterization that is smooth with respect to the distance metric, thus allo wing a finite sample of an environment to pro vide enough data hood. For instance, if reward appeared stochastically in some region, it might be necessary to visit nearby states a number of times before this distrib ution is learned. Alter -of the state space. For instance, a skier mo ving down a hill has dynamics dependent on the terrain conditions, such as slope, sno w type, and other factors.
 Incidentally , Figure 2 illustrates the reason why standard state space aggre gation techniques [12 ] do not work here. In particular , for partitioning induced by a cover on a Eu-clidean spaces there exist  X  X orners X  where 3 (or more) sets meet. When taking actions  X  X o ward X  this corner from gate state set is inherently unstable. The algorithm, Metric-, is a direct generalization of the A crucial notion in is that of a  X  X no wn X  state  X  a state visited often enough such that the dynamics and rewards are accurately modeled at this state. When the agent is not in the current set of kno wn states, the agent wanders ran-domly to obtain new information. While at a kno wn state, it must decide whether to explore or exploit  X  a decision explore is made by determining how much potential reward the agent can obtain by  X  X scaping X  the kno wn states to get maximal reward else where. If this number is suf ficiently lar ge, the agent explores. This number can be computed by planning to  X  X scape X  in a fictitious MDP pro vides maximal reward for entering an unkno wn state. The crucial step in the proof of is sho wing that either the agent exploits for near optimal reward, or it can explore kno wn states. Since the size of the kno wn set is bounded, the algorithm eventually exploits and obtains near optimal reward.
 Metric has a few key dif ferences. Here, a  X  X no wn X  state-action is a pair Local Modeling Assumption  X  namely , any pair which the algorithm has obtained at least periences does not explicitly enumerate this set of kno wn states, but rather is only able to decide if a particular state-action is kno wn. Thus, in the most general version of our algorithm, our model of the MDP is represented simply by a list of all prior experience.
 As in the original , a key step in Metric-is the cre-ation of the known MDP  X  a model for just that part of the global MDP that we can approximate well. Here the kno wn MDP at any moment is given as a generati ve model that  X  X atches together X  in a particular way the generati ve models pro vided by the planning algorithm at kno wn states. More precisely , the appr oximate known MDP gener ative model tak es any state-action operates as follo ws: 1. If 2. Else give 3. If exploit is 1, set 4. If for some action 5. Else output a special state and reward and halt. Intuiti vely , we have described a generati ve model for two MDPs with identical transition dynamics, but dif fering re-wards according to the value of the exploit bit. In both tions are  X  X edirected X  to a single, special absorbing state , while all other transitions of the global MDP are preserv ed. Thus initially the kno wn MDP dynamics are a small subset of the global MDP , but over time may cover much or all of the global state space. For rewards, when exploit is 1, rewards from the real environment are preserv ed, whereas when exploit is 0, reward is obtained only at the absorb-ing state, thus rewarding (rapid) exploration (escape from kno wn state-actions). We shall use MDP corresponding to the generati ve model abo ve when the exploit input bit is set to 1, and MDP generated by setting exploit to 0.
 Note that under our assumptions, we can always simulate the approximate kno wn MDP generati ve model. We can also vie w it as being an approximate (hence the name) gen-erati ve model for what we shall call the true known MDP  X  the MDP whose generati ve model is exactly the same as described abo ve, except where the local modeling algo-rithm Model is perfect (that is, in the Local Modeling As-sumption, a partial model of the global MDP , but it has the true prob-abilities for all kno wn state-actions. We shall use to denote the MDP corresponding to the generati ve model abo ve with a perfect Model and the exploit input bit set to 1, and Model and exploit set to 0.
 No w we outline the full Metric-algorithm. It is impor -tant to emphasize that this algorithm never needs to explic-itly enumerate the set of kno wn state-actions.
 Algorithm Metric-Input: Output: A policy 1. Use random mo ves until encountering a state with 2. Ex ecute Plan twice, once using the generati ve model 3. If 4. Else, HAL T and output exploit .
 in sample comple xity and running time that depend only on the covering number under the metric. We now turn to the analysis. We first state the main theorems 2 of the paper . In the follo wing theorems, we use: 1. 2. is the time horizon 3.
 4. 5. is an accurac y parameter 6. a confidence parameter .
 Theor em 4.1 (Sample Comple xity) Suppose With probability outputs a policy suc h that This sho ws that the sample comple xity (the number of ac-tions required) is bounded in terms of the covering number bounding the sample comple xity , we bound the time com-ple xity .
 Theor em 4.2 (Time Comple xity) Let be the over all sample comple xity . Metric-runs in time at most A few lemmas are useful in the proofs. First we define The original Simulation Lemma for had a dependence on the size of the state space that we cannot tolerate in our setting, so we first need an impro ved version: Lemma 4.3 (Simulation Lemma) If appr oximation of Proof . Let paths. For and tively . Let ties under in approximation, for any state , . Then where we have used the triangle inequality and linearity of expectation. Induction on Since the rewards The result follo ws using the pre vious two equations. No w we restate the  X  X xplore-or -Exploit X  lemma from [4]. Lemma 4.4 (Explor e or Exploit) Let policy for the global MDP policy for the true known MDP Then for any state of either or the optimal policy at least of leaving the known states in steps in One subtle distinction from the original algorithm ex-ists. Here, although the algorithm plans to reach some un-kno wn state, by the time this state is reached, it might ac-tually be kno wn due to the Local Modeling Assumption. Note that in the maze world example, the agent might plan to escape by mo ving around a corner . Ho we ver, when ac-tually executing this escape polic y, the states around the corner could become kno wn before the y are reached in steps, if the y come into line of sight beforehand. We now establish that Metric-ceases to explore in a rea-sonable amount of time. In the original this was a con-sequence of the Pigeonhole Principle applied to the number of states. A similar statement holds here, but now we use the size of a cover under the metric. It is important to note that this lemma holds whether or not the covering number Lemma 4.5 (Explor ation Bound) Metric-encounter s at most Proof . First, consider the as follo ws: the state-action set Note that the state at time and so if the size of unkno wn state-action pairs encountered by the algorithm before time ment in remo ved from It follo ws that for all and hence the algorithm cannot encounter more than unkno wn state-actions.
 For the general sets, only one of the sets By an analogous argument, if a state-action is unkno wn, it is added to some the number of unkno wn state-actions encountered by the algorithm before time minimal for all tered by the algorithm is bounded by We now pro vide the proofs of the main theorems. Proof of 4.1. The exploration bound of Lemma 4.5 implies we encounter a kno wn state after a number of actions that is at most exploration attempts. Each attempted exploration occurs when cessful exploration is greater than most, the state spaces occurs with a chance of error . The total number of actions before halting is less than the sum of the exploration actions kno wn states and the actions tak en in unkno wn states.
 The decision to halt occurs when which implies to planning and simulation error . By the Explore or Exploit lemma Due to simulation and planning error in computing an op-timal polic y in The result follo ws since a polic y in than in Proof of 4.2. It is never necessary to evaluate the metric between two samples more than once. There are at most at most most times since at least one transition occurs before reentering step 2. One call to Plan requires time at most plan most most . The result follo ws by adding these times. metric-over because the impro vements are inher -ently dependent upon the exact form of the local model-ing assumption. In the extreme case where the state-action space is continuous and nite sample comple xity while metric-has a finite sample extreme case is not too unusual. Certainly , man y control problems are modeled using continuous (or virtually con-tinuous) parameters.
 The metric-analysis implies that local modeling re-quires weak er assumptions about the beha vior of the world of states to have Mark ovian dynamics in order to engage in successful exploration. Instead, all that we need is the ability to generalize via local modeling. Of course, when aggre gations of states do have Mark ovian dynamics, state aggre gation may work well.

