 Cross-domain recommender systems adopt different tech-niques to transfer learning from source domain to target domain in order to alleviate the sparsity problem and im-prove accuracy of recommendations. Traditional techniques require the two domains to be linked by shared character-istics associated to either users or items. In collaborative filtering (CF) this happens when the two domains have over-lapping users or item (at least partially). Recently, Li et al. [7] introduced codebook transfer (CBT), a cross-domain CF technique based on co-clustering, and presented experimen-tal results showing that CBT is able to transfer knowledge between non-overlapping domains. In this paper, we dis-prove these results and show that CBT does not transfer knowledge when source and target domains do not overlap. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Information filtering Algorithms and theory Accuracy measures; Cold start; Matrix factorization; Rat-ings aggregation; Sparsity
In 2009, Li et al. [7] proposed codebook-transfer (CBT), a cross-domain collaborative filtering (CF) recommender sys-tem based on co-clustering and tri-matrix factorization. The core of CBT is based on a two-step process: extraction of knowledge (the codebook ) from the source domain and in-jection of knowledge in the target domain. Both steps are performed by using a constrained tri-matrix factorization. the lack of sufficient information about a user or item. In a cross-domain setting, a recommender system may draw on information acquired from other domains to alleviate such problem. For instance, a user X  X  preferred movie genres are likely to be derived accurately from her/his preferred litera-ture genres. This example is based on the assumption that there are similarities or correspondences either between the user preferences or between the items in the source and tar-get domains. Cross-domain recommender systems leverage these dependencies through considering, for example, over-laps between the observed user or item sets, similarities of item attributes, features of rated items, or correlations be-tween user preferences in the domains. Then, they apply a variety of techniques for enriching the knowledge possessed by the target domain, and improving the quality of recom-mendations.

The research on cross-domain recommendation has gener-ally aimed to exploit knowledge from a source domain D S , to perform or improve recommendations in a target domain D
T . Respectively for such domains, let U S and U T be their sets of users, and let I S and I T be their sets of items. The users of a domain are those that have some ratings for items of the domain. In order to alleviate cold-start and spar-sity problems in the target domain, cross-domain systems recommend items in the target domain by exploiting knowl-edge from both source and target domains, i.e., recommend items in I T to users in U S by exploiting knowledge about U
S  X  X  T and/or I S  X  X  T . In general, to perform this type of recommendation, some data relations or overlaps between domains are needed, and approaches aim to establish explicit or implicit knowledge-based links between the domains.
As discussed by Fern  X andez-Tob  X  X as et al. [4], domains can be linked by means characteristics associated to users or items. Following the notation used in [4], let X U and X I be the sets of characteristics used to represent the users and items, respectively. The nature of these characteristics may be diverse, e.g. ratings, user demographics, item attributes. Two domains D S and D T are linked if X U S  X X U T 6 =  X  or X
S  X X I T 6 =  X  , i.e., if there are user or item characteristics shared by the domains.

In collaborative filtering systems, the user preferences are usually modeled as a matrix R  X  R |U| X |I| , in which an ele-ment r ui is the rating assigned by user u to item i . Users are described through the ratings they assign to items. Thus, X
S = I S and X U T = I T . Similarly, items are described in terms of the ratings received by users, i.e., X I S = U S and X
T = U T . Domains D S and D T overlap when I S  X  X  T 6 =  X  or U S  X  X  T 6 =  X  . Figure 1 shows the four scenarios deriving by the combination of user and items overlapping in cross-domain CF as identified by Cremonesi et al. in [2]: where is the entry-wise division and 1 is a vector of ones. The codebook contains the average rating of each co-cluster of users and items.

In the second step , the target rating matrix R T is filled by expanding the values in the codebook. To this purpose, users and items in the target domains are first mapped to the co-clusters identified in the codebook by minimizing the following quadratic loss function Each row in U T or V T is the cluster membership of the user or item. Note that, differently from what happens in the first step, the norm of this second minimization problem is computed only on the non-zero entries of the user-rating matrix. Once the user and item membership matrices U T and V T are obtained, all the zero elements of the target rating matrix R T are filled with the appropriate elements of the codebook where  X  denotes the entry-wise product and W is a binary mask of the same size as R T used to mask zero elements.
In the third step , the filled target matrix can be used as the training data set for CF.

The authors performed experiments using a dense sub-set of the EachMovie dataset to compute the rating pat-terns, which were transferred to the more sparse MovieLens and BookCrossing datasets. They compared the accuracy of a traditional CF algorithm on the target domain with and without the transfer of the codebook from the source domain. The algorithm adopted for the comparison was a user-based k -nearest-neighbors (kNN) collaborative filter-ing based on Pearson correlation. According to the results, rating predictions were improved with the transfer of the codebook.
In this section we describe the methodology adopted to evaluate if the improvement of accuracy experimented by Li et al. in [7] with the CBT algorithm is due to a transfer of learning from source to target domain.

The CBT algorithm consists of three steps: (i) construc-tion of the codebook from the source domain; (ii) filling of the missing ratings in the target domain by using the code-book; (iii) training a CF algorithm on the filled target do-main. The goal of the second step is to reduce the sparsity on the target domain and to help the final CF algorithms in providing better recommendations.

We have modified the CBT algorithm by removing step (i) and by generating a codebook B that is not based on the source domain. In other words, we drop equations (1) and (2) and keep equations (3) and (4). We have tested three different methods to generate a  X  X alse X  codebook: MAE 0.5216 0.5064 0.4963 0.5089 0.5109
RMSE 0.4736 0.4492 0.4380 0.4556 0.4578 MAE 0.7285 0.7090 0.7047 0.7085 0.7124
RMSE 0.8630 0.8208 0.8149 0.8245 0.8313 MAE 0.5580 0.5456 0.5388 0.5474 0.5486
RMSE 0.5437 0.5231 0.5113 0.5269 0.5290 MAE 0.7640 0.7446 0.7425 0.7397 0.7417 RMSE 0.9388 0.8988 0.8932 0.8915 0.8935 Table 1: Accuracy of different methods. Bold num-bers are significantly different from the baseline method kNN.

As expected, CBT clearly outperforms the baseline method for all the datasets. Surprisingly, the experiments with the random codebook present an accuracy which is identical to the accuracy of the CBT method, even in the lack of knowl-edge transfer from source to target domain.
In this section we try to investigate the surprisingly results obtained by the random codebook. By design, the random codebook does not transfer any knowledge from the source domain into the target domain. Therefore, the increase of accuracy introduced by the random algorithm cannot be ex-plained in terms of transfer of knowledge and we need to look for a different explanation.

The insight of the solution of this problem is from the second step of the CBT, described in (3). The goal of this step is to map users and items in the target domain to the clusters derived in the source domain (in the case of a code-book correctly created in the source domain). The structure of the optimization problem (3) closely resemble regularized SVD matrix-factorization, as the minimization is performed only on the non-zero elements of the user-rating matrix [6]: The main differences are the constraints on the U T and V T matrices and the presence of the codebook B .

We believe that the second step of the codebook transfer, which is supposed to simply map users and item to clusters, is in fact equivalent to the training of a matrix-factorization algorithm, provided that matrix B (the codebook) has a wide enough range of ratings to account for the constraints imposed on U T and V T .

We also believe that the enrichment of the target domain performed by (4) is equivalent to performing recommenda-tions with the trained matrix-factorization model.
We finally believe that the last step of the CBT method (i.e., running the kNN algorithm over the enriched target matrix) does not provide any additional improvement to the quality of the estimations, as it is well know in the literature that matrix-factorization CF outperform user-based CF in
