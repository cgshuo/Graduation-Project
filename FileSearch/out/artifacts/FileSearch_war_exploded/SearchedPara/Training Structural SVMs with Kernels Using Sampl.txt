 Discriminative training for structured outputs has found in-creasing applications in areas such as natural language pro-cessing, bioinformatics, information retrieval, and computer vision. Focusing on large-margin methods, the most gen-eral (in terms of loss function and model structure) training algorithms known to date are based on cutting-plane ap-proaches. While these algorithms are very efficient for lin-ear models, their training complexity becomes quadratic in the number of examples when kernels are used. To overcome this bottleneck, we propose new training algorithms that use approximate cutting planes and random sampling to enable efficient training with kernels. We prove that these algo-rithms have improved time complexity while providing ap-proximationguarantees.Inempirica levaluatio ns,ouralgo-rithms produced solutions with training and test error rates close to those of exact solvers. Even on binary classifica-tion problems where highly optimized conventional training methods exist (e.g. SVM-light), our methods are about an order of magnitude faster than conventional training meth-ods on large datasets, while remaining competitive in speed on datasets of medium size.
 I.2.6 [ Artificial Intelligence ]: Learning Algorithms, Experimentation, Performance Support Vector Machines, Kernels, Large-Scale Problems
Large-margin methods for structured output prediction like Maximum-Margin Markov Networks [13] and Structural SVMs [15] have recently received substantial interest for challenging problems in natural language processing [14], bioinformatics [19], and information retrieval [20]. As train-ing algorithms for these problems, cutting-plane approaches [15, 6] are among the most generally applicable methods that provide well-understood performance guarantees. First, cutting-plane methods can be use to train any type of structured linear prediction model for which inference and subgradi-ent can be computed (or at least approximated) efficiently. This makes them applicable to problems ranging from HMM training and natural language parsing, to supervised clus-tering and learning ranking functions. Second, they allow optimizing directly to non-standard loss functions that do not necessarily have to decompose linearly (e.g. Average runtime provably scales linearly with the number of train-ing examples for linear models. This makes cutting-plane methods not only attractive for training structured predic-tion models, but they are also orders of magnitude faster than conventional methods for training binary classifiers [5].
Unfortunately, the computational efficiency of cutting-plane methods becomes substantially worse for non-linear models that involve kernels. While it is possible to train kernel mod-els, the computational complexity scales quadratically with the number of examples, not linearly as in the non-kernel case. In particular, each iteration of the algorithm requires a quadratic number of kernel evaluations. This makes it infea-sible to train large-scale struc tural models that involve ker-nels, and it makes cutting-plane methods non-competitive for training kernelized binary classifiers compared to con-ventional decomposition methods like SVM-light.

In this paper we present new cutting-plane training meth-ods for structural SVMs that can be used to train kernel-ized models efficiently. These methods are equally broadly applicable, requiring only the ability to compute subgradi-ents efficiently, but exploit sparse approximations to each cut in order to limit the number of kernel computations. In particular, we present two new cutting-plane methods that exploit random sampling in computing a cut, so that the number of kernel evaluations depends only linearly on the number of examples in one algorithm, or is independent of the number of examples in the other algorithm. Instead, the number of kernel evaluations depends only on the quality of the solution that the user desires and that is sensible for the learning task. In addition to providing theoretical guar-antees regarding runtime and quality of the solutions, we also provide empirical results in comparison to conventional decomposition methods and a subspace method that uses a Cholesky decomposition. Structual SVMs are a method for learning rules h : X X  Y fromsom espace X ofcom plexandstructuredobjects x  X  X tosom espace Y ofcom plexandstructuredobjects y  X  Y (e.g.sentence s X topars etrees Y innaturalla nguage parsing). Given a labeled training sample structural SVMs learn a linear discriminant rule by minimizing a regularized version of the empirical risk R function  X . In the case of margin-rescaling [15, 13] we con-sider in this paper, training a structural SVM amounts to solving the following quadratic program.

Optimization Problem 1. (Struct SVM Primal)
We use the short-hand notation  X   X  i ( X  y ):= X ( x i ,y i ) While this program is convex, it has an exponential or in-finite number of constraints (i.e. proportional to | Y | most interesting problems, making naive approaches to its solution intractable. Fortunately, it can be shown that the cutting-plane Algorithm 1 can nevertheless solve OP1 to ar-bitrary precision .
 Algorithm 1 1-Slack Cutting Plane Algorithm 1: Input: S =(( x 1 ,y 1 ) ,..., ( x N ,y N )) ,C, 2: J = {} ,t =0 ,w = 0 , X  =0 3: ( c ( t ) ,g ( t ) )= Find Cutting Plane ( w ) 4: while c ( t ) + w  X  g ( t ) &gt; X  + do 5: J = J  X  X  ( c ( t ) ,g ( t ) ) } 6: t = t +1 7: ( w,  X  )= Solve QP ( J ) 8: ( c ( t ) ,g ( t ) )= Find Cutting Plane ( w ) 9: end while 10: return ( w,  X  ) 11: procedure Find Cutting Plane ( w ) 12: for i =1to N do 13:  X  y i = argmax  X  y  X  X  ( X ( y i ,  X  y )+ w  X   X ( x i ,  X  y )) 14: end for 16: end procedure 17: procedure Solve QP ( J ) 18: ( w,  X  )= argmin w, X   X  0 19: return ( w,  X  ) 20: end procedure
This cutting plane algorithm is currently one of the fastest solution method for large margin structural learning prob-lems. Its time complexity scales linearly with the number of examples N [5, 6] when the learned discriminant func-tion w  X   X ( x, y ) is linear. However, with the use of kernels, it becomes neccessary to work in the dual and Algorithm 1 now scales quadratic in the number of examples. To see this, let X  X  look at this dual optimization problem.
Optimization Problem 2. (Cutting-Plane Dual)
Theprimalanddualsolutionarerelatedvia w =  X  t i =1  X  i g ( One of the major issue with the dual algorithm is the compu-= 1 = 1 which involves O ( N 2 ) kernel computations. This makes Al-gori thm1impracticalevenif N isonlymoderatelylarge. Removing this bottleneck is central to our approach.
There has been many training methods proposed in the structural learning literature. The Maximum-Margin Markov Networks [13] use SMO[8] for training with linearly decom-posable loss functions, while the more general framework of structural SVM [15] introduces the cutting plane method as a training procedure. Subgradient methods [9] have also been proposed as an efficient training method for structural learning. Recently a faster 1-slack version of the cutting plane algorithm [6] has been introduced to solve large margin structural learning problems. A generalization of the cutting plane method called the bundle method [12] has also been recently proposed for the minimization of different convex loss functions in structural learning. Most of these works consider only linear discriminant functions. Our work con-tinues this line of research by extending the cutting plane method to structural learning with kernels.

Our work is also related to the use of stochastic optimiza-tion in structural learning. The work in [16] investigated the use of stochastic gradient in the training of Conditional Random Fields, while the work in [11] employed stochastic subgradient to train linear SVMs. In stochastic optimization methods, decreasing step sizes or more accurate estimates of the gradient is required as the optimization progresses. We aim to provide methods that automatically terminate when a solution with guaranteed precision is reached. We take a somewhat different approach by directly modifying the op-timization method.

Besides structural learning there have also been extensive work on speeding up kernel methods based on kernel matrix approximation. The Nystr  X  om method has been proposed in [18] to approximate the kernel matrix used for Gaussian Process classification. Low-rank approximation has been ex-ploited to speed up the training of kernel SVMs[2]. A greedy basis-pursuit-style algorithm is also proposed in [7] to build sparse kernel SVMs to speed up both training and classifi-cation.
Our main idea is to speed up the expensive double sum kernel computations in Equation 1 with approximate cuts that involve fewer basis functions. Such approximate cuts could be constructed by various methods such as greedy ap-proaches, but we take the simpler approach of sampling since it allows us to prove performance guarantees later. In the following we will present two different sampling strategies and analyze their complexity.
Our first algorithm has constant time scaling with re-spect to the training set size. Let us look at the new cut-ting plane oracle in Algorithm 2, modified from Algorithm 1. There are no other changes apart from the function Find Cutting Plane (). The vector S contains r indices sampled uniformly from 1 to N . Both the offset c ( t ) and the subgradient g ( t ) are constructed from these r examples instead of the full training set. In general, the approximate subgradient points in a different direction than the exact subgradient. If we regard the exact constraint as a state-ment of how we want the classifier to behave on the whole training set, we can regard the sampled cut as a statement on a bootstrap sample. Notice that the exit condition of the while loop on Line 4 of Algorithm 2 is now based on an es-timate of the loss from a small sample instead of the whole training set.
 Algorithm 2 Constant Time Cut Subsampling Algorithm for Structural SVM 1: Input: S =(( x 1 ,y 1 ) ,..., ( x N ,y N )) ,C, 2: J = {} ,t =0 , w = 0 , X  =0 3: ( c ( t ) , g ( t ) )= Find Cutting Plane ( w ) 4: while c ( t ) + w  X  g ( t ) &gt; X  + do 5: J = J  X  X  ( c ( t ) , g ( t ) ) } 6: t = t +1 7: ( w,  X  )= Solve QP ( J ) 8: ( c ( t ) , g ( t ) )= Find Cutting Plane ( w ) 9: end while 10: return ( w,  X  ) 11: procedure Find Cutting Plane ( w ) 12: Sample r examples uniformly for S 13: for j =1to r do 14:  X  y S j = argmax  X  y  X  X  ( X ( y S j ,  X  y )+ w  X   X ( x S j 15: end for 17: return ( c, g ) 18: end procedure
Since the optimization problem is solved in the dual, we focus on complexity analysis of the dual of Algorithm 2. We defer the analysis on the number of cutting planes required before convergence to the next section, and analyze the time and especially the number of kernel computations required in each iteration. The dual form of the argmax operation of line 14 in Algorithm 2 is:  X  y j = argmax  X  y  X  X   X ( y S j ,  X  y )+ Expanding the inner product in Equation 2, g  X 
 X ( x S j ,  X  y ))= 1 r This involves O ( tr ) kernel computations at iteration t when we sum up from k =1to t  X  1, provided the argmax compu-tation over  X  y involves only a small constant number of kernel computation overall for different  X  y . This is true for binary or multi-class classification, and also true for the case when the kernel function factorizes into components (e.g. MRF cliques). Since we need to compute this inner product for all the sampled examples  X  y j for j =1to r , the overall complex-ity of sampling a cut involves O ( tr 2 ) kernel computations.
For computing the Gram matrix G , we can update it in-crementally from one iteration to the next. At iteration t , it involves expanding G by computing G it for 1  X  i  X  t . Following from Equation 1 in the case of the exact algo-rithm, we can infer that the inner product of two sampled O ( tr 2 ) kernel computations overall since we need to do this for 1  X  i  X  t . We can see that the subsequent iterations are more expensive since the cost scales linearly with t .Ifit takes T iterations for the algorithm to terminate, then the overall complexity would be O ( T 2 r 2 ) kernel computations. We omit the time spent on the quadratic program in this analysis since in practice kernel computations account for over 95% of training time.
The previous sampling approach never looks at the whole training set, making the complexity independent of the train-ing set size N in each iteration. Our second sampling algo-rithm trades off additional work in each iteration for the ability to sample in a more targeted way. Let us consider Algorithm 3, especially the changes to the cutting plane or-acle. Like the exact algorithm, it computes the argmax and the loss over all examples. However, it only samples r of the examples with non-zero loss to construct the cutting plane. This has the effect of focusing on those examples that are more important to determining the decision surface. Two clusion in the optimization problem while the other is used for the stopping criterion.

In the case of a linear feature space this sampling is not needed because the cutting plane can be represented com-pactly by just adding up the N feature vectors returned by the argmax computation. But in the nonlinear kernel case, sampling helps because it reduces the number of basis func-tions used in the kernel expansion from O ( N )to O ( r ). Since the argmax computation is performed on all N examples, the algorithm has more information on the whole training set compared to the constant time algorithm, such as the average loss and the primal objective value. In particular we can use the exact cutting plane ( c , g ) as the stopping criterion of the algorithm.
Since we are computing the argmax over all N examples, it is possible to save computation in return for increased memory usage. Suppose we have a structure A ki to store 1  X  k  X  t ,1  X  i  X  N ,andforall X  y  X  X  . Thisisasingle numbers for multi-class classification if there are m classes, one for each class. For HMM with kernelized emissions, this involves storing the kernelized emission score at each position for each possible hidden state. In all of these cases it amounts to O ( N ) storage requirement for each cut. Algorithm 3 Linear Time Cut Subsampling Algorithm for Structural SVM 1: Input: S =(( x 1 ,y 1 ) ,..., ( x N ,y N )) ,C, 2: J = {} ,t =0 , w = 0 , X  =0 3: (( c ( t ) , g ( t ) ) , ( c , g )) = Find Cutting Plane 4: while c + w  X  g &gt; X  + do 5: J = J  X  X  ( c ( t ) , g ( t ) ) } 6: t = t +1 7: ( w,  X  )= Solve QP ( J ) 8: (( c ( t ) , g ( t ) ) , ( c , g )) = Find Cutting Plane 9: end while 10: return ( w,  X  ) 11: procedure Find Cutting Plane ( w ) 12: for i =1to N do 13:  X  y i = argmax  X  y  X  X  ( X ( y i ,  X  y )+ w  X   X ( x i ,  X  y )) 14: end for 15: I = { 1  X  i  X  N |  X ( y i ,  X  y i ) &gt; 0 } 17: repeat 18: Sample r examples uniformly from I for S 20: until c + w  X  g  X   X  + or c + w  X  g&gt; X  + 21: return (( c, g ) , ( c , g )) 22: end procedure
The dual form of the argmax operation in line 16 is: With the saved kernel computations in A ki for 1  X  k&lt;t ,the argmax computation requires no extra kernel computations since the term g ( k )  X   X ( x i ,  X  y ) can be retrieved from A
Updating A ti for a new iteration t involves computing g )  X   X ( x This requires O ( r ) kernel computations, assuming that com-puting and storing the information required for recontruct-ing the above inner product for each  X  y takes a constant num-ber of kernel computations and storage. As this has to be done for all N examples, the overall complexity is O ( Nr ) kernel compuations for each update in each iteration.
The Gram matrix G can be updated conveniently with the information stored in A ki ,since g This involves no new kernel computations since both g ( i
Therefore if the algorithm terminates in T iterations, the overall complexity is O ( TNr ) kernel compuations, with O ( TN ) storage required. Although storing each cut requires O ( N ) storage, it is still feasible even for large datasets if the num-ber of active cuts is small(e.g., less than 100). This is the basic assumption in this space-time tradeoff and is confirmed by our experiments in section 6.
In this section we analyze theoretically the termination and solution accuracies of the two algorithms. We first prove bounds on the number of iterations for the algorithms to terminate, and then use the results to prove error bounds on the solutions. We prove the results for the two algorithms under a general framework to show that these results could also apply to the design of other sampling schemes.
To prove termination for the above algorithms, we con-sider the following template of the cutting plane algorithm: Algorithm 4 Generic Cutting Plane Algorithm 1: J = {} ,t =0 , w (0) = 0 , X  =0 2: (( c ( t ) , g ( t ) ) , ( c , g )) = Find Cutting Plane 3: while c + w ( t )  X  g &gt; X  + do 4: J = J  X  X  ( c ( t ) , g ( t ) ) } 5: t = t +1 6: ( w ( t ) , X  )= Solve QP ( J ) 7: (( c ( t ) , g ( t ) ) , ( c , g )) = Find Cutting Plane 8: end while 9: return ( w ( t ) , X  ) Notice that what the above algorithm returns as solution de-pends crucially on the implementation of Find Cutting Plane However the specific detail of the implementation does not affect the termination property of the above cutting plane algorithm, and we have the following theorem:
Theorem 1. Assume the following holds for the cuts ( c ( t ( c , g ) returned by the cutting plane oracle Find Cutting Plane (i) 0  X  c ,c ( t )  X   X   X  (ii) g , g ( t )  X  R (iii) if c + w ( t )  X  g &gt; X  + ,then c ( t ) + w ( t )  X  Then Algorithm 4 terminates after at most 8 C  X   X  R 2 / 2 to the cutting plane oracle Find Cutting Plane ().
Proof. Consider the optimization problem solved by Solve QP on line 6 of the generic cutting plane algorithm:
Optimization Problem 3. Consider also the following optimization problem:
Optimization Problem 4. min s.t.  X  ( c, g )  X  X  ,c + w  X  g  X   X  where C = { ( c, g ) | c  X  R , 0  X  c  X   X   X  , g  X  X  , g  X  C contains all possible bounded cutting planes where c is bounded above by  X   X and g is bounded above in norm by R .
Since conditions (i) and (ii) hold for the cutting plane ora-cle, OP3 is always a relaxation of OP4. Therefore the value of the primal solution of OP3 is always smaller than the value of the primal solution of OP4, and hence the value of any feasible solution of OP4 upper bounds the value of any dual solution of OP3. As w = 0,  X  =  X   X  is a feasible solu-tion to OP4, the value of the dual solution of OP3 is upper bounded by C  X   X . By Proposition 17 of [15], the inclusion of each -violated constraint increases the dual objective of OP3 by at least 2 / 8 R 2 ,where R is the upper bound on the norm of any g . As the dual objective is bounded from above fore the cutting plane algorithm terminates.

Condition (iii) ensures that whenever we are not termi-nating the while loop, an -violated constraint ( c ( t ) , g always be added to the working set.
 Corollary 1. Let  X   X  = max i,y  X ( y i ,y ) and R =max i,y  X   X  i ( y ) . Algorithm 2 terminates after at most 8 C  X   X  R 2 / 2 calls to Find Cutting Plane ().

Proof. First of all notice that Algorithm 2 fits into the generic template of Algorithm 4. The cut ( c ( t ) , g ( t by Find Cutting Plane () in Algorithm 2 serves both as the cut to be included into the working cut set J and also as the cut for the termination criterion ( c , g ) as in line 3 of Algorithm 4 above. Therefore condition (iii) of Theorem 1 holds trivially. Since 0  X  c ( t ) = 1 r r j =1  X ( y S j and (ii) hold. Invoking Theorem 1, we can conclude that at most 8 C  X   X  R 2 / 2 calls are made to Find Cutting Plane before Algorithm 2 terminates.
 Corollary 2. Let  X   X  = max i,y  X ( y i ,y ) and R =max i,y  X   X  i ( y ) . Algorithm 3 terminates after at most 8 C  X   X  R 2 / 2 calls to Find Cutting Plane ().

Proof. The proof is very similar to the previous corol-lary. Algorithm 3 fits the generic template of Algorithm 4. First of all c ( t ) = c = 1 N N i =1  X ( y i ,  X  y i )  X   X  (i) of Theorem 1 is satisfied. It is also easy to see that g in norm by R , so condition (ii) holds as well. It is also easy to see that the exit condition of the repeat loop on line 20 of Algorithm 3 makes condition (iii) hold. Therefore we can invoke Theorem 1 and conclude that at most 8 C  X   X  R calls are made to Find Cutting Plane () before termina-tion. Moreover, the repeat loop in Find Cutting Plane () always terminate in finte expected time. When the ex-act cutting plane is -violated, we can always sample an -violated approximate cut with probability bounded away from 0 (for example, by sampling the worst violating exam-ple r times).
After proving termination and bounding the number of cutting planes required, we turn our attention to the accu-racy of the solutions. Specifically we will characterize the difference between the regularized risk of the exact solution and our approximate solutions. The main idea used in the proof is: if the error introduced by each approximate cut is small with high probability, then the difference between the exact and approximate solutions will also be small with high probability. Bounding the difference between the ex-act cut and the sampled cut can be done with Hoeffding X  X  inequality.

Let us start the proofs by defining some notation. Let f ( w )= max of the empirical risk, and let  X  f ( w ) = max 1  X  be an approximate cutting plane model, with (  X  c ( t ) , ing the approximate cutting planes. We have the following lemma:
Lemma 1. Let a fixed v in the RKHS H be given. Suppose for some  X &gt; 0 each of the cutting plane and its approximate counterpart satisfy for t =1 ...T .Then  X  f ( v ) &lt;f ( v )+  X  with probability at least 1  X  Tp  X  .

Proof. By union bound we know that (  X  c ( t ) + v  X   X  g ( c least 1  X  Tp  X  . The following chain of implications holds: Hence  X  f ( v ) &lt;f ( v )+  X  with probability at least 1 The lemma shows that the approximate cutting plane model does not overestimate the loss by more than a certain amount with high probability. Notice that T is a fixed number above. If T is a bounded random variable such as the termination iteration, then we can replace T by its upper bound  X  T and the lemma still holds. From the termination bound in sec-tion 5.1 we have  X  T =8 C  X   X  R 2 / 2 .

Now we are going to use this lemma to analyze the lin-ear time algorithm Algorithm 3. In the linear time algo-rithm we denote the exact cutting plane ( c ( t ) , g ( cut (  X  c ( t ) ,  X  g ( t ) )with( 1 N N i =1  X ( y i ,  X  y We can bound the difference between the exact cutting planes and the approximate cutting planes using Hoeffding X  X  in-equality in the following lemma:
Lemma 2. Let a fixed v  X  X  , v  X  t =1 ...T ,
Pr (  X  c ( t ) + v  X   X  g ( t ) )  X  ( c ( t ) + v  X  g ( t uniformly from the index set I , Z j  X  X  are independent with between [  X  and after some arithmetic we obtain the result.

Now we are ready to prove our main theorem relating the regularized risk of the optimal solution to our approximate solution. Let v  X  be the optimal solution to OP1. We have the following theorem:
Theorem 2. Suppose Algorithm 3 terminates in T iter-ations and return w  X  as solution. Then with probability at least 1  X   X  , 1 2 w  X  2 + CL ( w  X  )  X  1 where  X  T =8 C  X   X  R 2 / 2 ,and L ( w ) is the margin loss as in OP1.

Proof. With the exact cutting planes ( c ( t ) , g ( t ) )andap-we apply Lemma 1. Put v = v  X  ,and p  X  =exp(  X  r X  2 / 4 C (we omit  X  since it is bounded above by 1), we obtain  X  f ( v f ( v  X  )+  X  with probability at least 1  X   X  T exp(  X  r X  Inverting the statement and we have with probability at least 1  X   X  : Since w  X  is the optimal solution of min w 1 2 w 2 + C  X  the T th iteration, we have the following: 1 2 &lt; 1 2  X  1 2 The last line makes use of the subgradient property that f ( w )  X  L ( w ) for any exact cutting plane model f of a convex loss function L . Since we are using the exact cutting plane as the condition for exiting the while loop, so we must have at termination: c + w  X   X  g  X   X  + Therefore we have: with probability at least 1  X   X  .
 The theorem shows that as far as obtaining a finite precision solution to the regularized risk minimization problem is con-cerned, it is sufficient to use sampled cuts with sufficiently large sample size r to match the desired accuracy of the solution. We will see in the experiment section that fairly small values of r work well in practice.

We state a similar result for Algorithm 2. The proof is fairly similar with a few technical differences. We assign its proof to the appendix.

Theorem 3. Suppose Algorithm 2 terminates in T iter-ations with w  X  returned as solution. Then with probability at least 1  X  2  X  , 1 2 w  X  2 + CL ( w  X  )  X  1 where  X  T =8 C  X   X  R 2 / 2 .
While theory gives us the worst case bounds that are re-assuring, we now study the empirical behaviour of the algo-rithms.
We implemented Algorithm 2 and Algorithm 3 and eval-uated them on the task of binary classification with kernels. We choose this task for evaluation because binary classi-fication with kernels is a well-studied problem, and there are stable SVM solvers that are suitable for comparisons. Moreover, scaling up SVM with kernels to large datasets is an interesting research problem on its own [1].

In binary classification the loss function  X  is just the zero-one loss. The feature map  X  is defined by  X ( x, y )= y X  ( x ), where y  X  X  1 ,  X  1 } and  X  is the nonlinear feature map in-duced from a Mercer kernel (such as the commonly used polynomial kernels and Gaussian kernels).

We implemented the algorithms in C, using Mosek as the quadratic program solver and the SFMT implementation [10] of Mersenne Twister as the random number generator. The experiments were run on machines with Opteron 2.0Ghz CPUs with 2Gb of memory (with the exception of the control experiments with incomplete Cholesky factorization, which we ran on machines with 4Gb of memory).

For all the experiments below we fix the precision parame-ter at 0 . 001. We remove cuts that are inactive for 20 itera-tions. We found that the constant time algorithm has better performance if we use a more stringent stopping criterion. We terminate the algorithm only when for p consecutive it-erations, the sampled cut is not violated by more than .In the experiments below we use p = 4. For each combination of parameters we ran the experi ment for 3 runs using differ-ent random seeds, and report the average result in the plots and tables below. In section 6.4 we also investigate the sta-bility of the algorithms by reporting the standard deviation of the results.

In the experiments below we test our algorithms on three different datasets: Checkers, Adult, and Covertype. Check-ers is a synthetic dataset with 1 million training points, with classes alternating on a 4x4 checkerboard. We generated the data using the SimpleSVM toolbox [17], with noise level pa-rameter sigma set to 0.02. The kernel width for the Gaussian kernel used for the Checkers dataset was determined by cross validation on a small subsample of 10000 examples. Adult is a medium-sized dataset with 32562 examples, with a sam-ple of 22697 examples taken as training set. The Gaussian kernel width is taken from [8]. Covertype is a dataset with 522910 training points, the kernel width of the Gaussian kernel we use below is obtained from the study [1].
Our first set of experiments is about how the two algo-rithms scale with training set size. We perform the exper-iments on the two large datasets Checkers and Covertype. We pick C to be 1 multiplied by the training set size, since
Figure 1: CPU Time Against Training Set Size Figure 2: Training Set Error Against Training Set Size that is the largest value of C we could get SVM light to train within 5 days. For the linear time algorithm we fix the sam-ple size r at 400, and for the constant time algorithm we use a sample size r of 1000 to compensate for the less effi-cient sampling. We train SVM models on subsets of the full training sets of various sizes to evaluate scaling.
Figure 1 shows the CPU time required to train SVMs on training sets of different sizes on the Checkers and Cover-type dataset. We can observe that the linear time algorithm scales roughly linearly in the log-log plot, while the constant time algorithm has a roughly flat curve in both plots. This confirms the scaling behaviour we expect from the complex-ity of each iteration. SVM light shows superlinear scaling on both of these datasets.

Figures 2 and 3 show the tra ining and test set errors of Figure 3: Test Set Error Against Training Set Size Figure 5: Number of Iteration Against Sample Size the algorithms. In general SVM light has the lowest training and test set errors, followed by the linear time algorithm and then the constant time algorithm. Both the training and test set errors lie within a very narrow band, and they are never more than 0.5 percentage point apart even in the worst case.
The next set of experiments is about the effect of the sample size r on training time and solution quality. We investigate the effect of sample size using the Adult dataset, since on this dataset it is easier to collect more data points for different sample sizes. We use sample sizes r from { 100, 400, 1000, 4000, 10000 } and C from { 0 . 01, 0.1, 1, 10, 100 multiplied by the training set size 22697. The constant time algorithm did not finish the training within 5 days for the largest sample size 10000 and C  X  X  10 , 100 } , hence there are two missing data points in the figures.

In Figure 5 shows that the number of iterations required generally decreases with increasing sample size. However the decrease in the number of iterations to convergence does not result in overall savings in time due to the extra cost in-volved in each iteration with larger sample sizes. This can be observed from the CPU Time plots in Figure 4. In general, the linear time algorithm has better scaling behaviour with respect to sample size compared to the constant time algo-rithm. This is predicted by our complexity analysis. What is most interesting is the stability of training and test set errors with respect to chang es to sample size, as shown in Figures 6 and 7. Except for very small sample sizes like 100 or small values of C like 0.01 the sets of curves are essentially flat.
Figure 6: Training Set Error Against Sample Size
Table 1 shows a comparison of the two algorithms against two conventional training methods, namely SVM light and a sampling-based method that uses Cholesky decomposi-tion as described below. For each dataset we train differ-ent models using values of C  X  X  0 . 01 , 0 . 1 , 1 , 10 , 100 tipled by the size of the training set. We used the results of SVM light as a yardstick to compare against, and report the value of C for which the test performance is optimal for SVM light . For the larger datasets Checkers and Covertype, SVM light terminated early due to slow progress for C  X  10, so for those two datasets we use C =1.

First of all, we notice from Table 1 that all the solutions have training and test set error rates very close to the so-lutions produced by SVM light . For the constant time algo-rithm the error rates are usually within 0.3 to 0.5 above the SVM light solutions, while the linear time algorithm has error rates usually within 0.2 above the SVM light solutions. The error rates also have very small standard deviation, on the order of 0.1, which is the same as our tolerance parameter . We also notice when using the same sample size r ,the linear time algorithm provides more accurate solutions than the constant time algorithm due to its use of more focused sampling.

We also provide control experiments with Cholesky de-composition method, where we subsample a set of points from the training set, and then compute the projection of all the points in the training set onto the subspace spanned by these examples. Then we train a linear SVM using SVM perf (with options -t2-w3-b0 ) on the whole training set. Our implementation involves storing all the projected train-ing vectors, and this consumes a lot of memory, especially for large datasets like Checkers and Covertype. We can only do 250 and 500 basis functions on those datasets respectively without running out of memory on a 4Gb machine, and on the Adult dataset we can only do up to 10000 basis func-tions. An alternative implementation with smaller storage requirement would involve recomputing the projected train-ing vector when needed, but this would become prohibitively expensive.

We observe that the Cholesky decomposition is generally faster than all the other methods, but its accuracy is usu-ally substantially below that of SVM light and our sampling algorithms. Moreover, unlike our algorithms, the accuracy of the Cholesky method depends crucially on the number of basis functions, which is difficult to pick in advance. The accuracies of our sampling algorithms are more stable with respect to the choice of sample size, where decreasing the sample size ususally results in more iterations to converge without much loss in accuracy of the solutions.
We presented two methods that make cutting-plane train-ing of structural SVMs with kernels tractable through the use of random sampling in constructing a cut. The meth-ods maintain the full generality of the cutting-plane ap-proach, making it possible to kernelize any structural predic-tion problem where linear models are currently used. The theoretical analysis shows that these algorithms have lin-ear or constant-time termination guarantees while providing bounds on the solution quality. Empirically, the algorithms can handle datasets with hundred-thousands of examples, and they are competitive or faster than conventional de-composition methods even on binary classification problems, where highly optimized special-purpose algorithms exist.
The current algorithms can be improved along several di-rections. The two sampling methods presented here are cho-sen for their simplicity and ease of analysis. Sampling effi-ciency can be improved by designing alternative sampling schemes, for example, by having different sampling rates for bound support vectors and non-bound support vectors following the popular shrinking heuristic used in training SVMs. On the other hand, one major bottleneck in the speed of the current algorithm is the large number of cuts required before convergence. Recently [3] proposes a sta-bilized cutting plane algorithm for linear SVMs with much improved convergence, and it will be interesting to extend their techniques to improve the speed of our sampled-cut algorithm for kernels.
We would like to thank the reviewers for their careful read-ing and helpful comments for improving this paper. This work was supported in part by NSF Award IIS-0713483 and by a gift from Yahoo!.
