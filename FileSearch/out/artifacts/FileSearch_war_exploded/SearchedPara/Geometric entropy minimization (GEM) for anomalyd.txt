 Anomaly detection and localization are important but notoriously difficult problems. In such prob-lems it is crucial to identify a nominal or baseline feature distribution with respect to which statisti-cally significant deviations can be reliably detected. However, in most applications there is seldom enough information to specify the nominal density accurately, especially in high dimensional fea-ture spaces for which the baseline shifts over time. In such cases standard methods that involve estimation of the multivariate feature density from a fixed training sample are inapplicable (high dimension) or unreliable (shifting baseline). In this paper we propose an adaptive non-parametric method that is based on a class of entropic graphs [1] called K -point minimal spanning trees [2] and overcomes the limitations of high dimensional feature spaces and baseline shift. This method detects outliers by comparing them to the most concentrated subset of points in the training sample. It follows from [2] that this most concentrated set converges to the minimum entropy set of proba-bility  X  as N  X  X  X  and K/N  X   X  . Thus we call this approach to anomaly detection the geometric entropy minimization (GEM) method.
 Several approaches to anomaly detection have been previously proposed. Parametric approaches such as the generalized likelihood ratio test lead to simple and classical algorithms such as the Stu-dent t-test for testing deviation of a Gaussian test sample from a nominal mean value and the Fisher F-test for testing deviation of a Gaussian test sample from a nominal variance. These methods fall under the statistical nomenclature of the classical slippage problem [3] and have been applied to detecting abrupt changes in dynamical systems, image segmentation, and general fault detection ap-plications [4]. The main drawback of these algorithms is that they rely on a family of parameterically defined nominal (no-fault) distributions.
 An alternative to parametric methods of anomaly detection are the class of novelty detection algo-rithms and include the GEM approach described herein. Scholkopf and Smola introduced a kernel-based novelty detection scheme that relies on unsupervised support vector machines (SVM) [5]. The single class minimax probability machine of Lanckriet etal [6] derives minimax linear decision re-gions that are robust to unknown anomalous densities. More closely related to our GEM approach is that of Scott and Nowak [7] who derive multiscale approximations of minimum-volume-sets to esti-mate a particular level set of the unknown nominal multivariate density from training samples. For a simple comparative study of several of these methods in the context of detecting network intrusions the reader is referred to [8].
 The GEM method introduced here has several features that are summarized below. (1) Unlike the MPM method of Lanckriet etal [6] the GEM anomaly detector is not restricted to linear or even convex decision regions. This translates to higher power for specified false alarm level. (2) GEMs computational complexity scales linearly in dimension and can be applied to level set estimation in feature spaces of unprecedented (high) dimensionality. (3) GEM has no complicated tuning pa-rameters or function approximation classes that must be chosen by the user. (4) Like the method of Scott and Nowak [7] GEM is completely non-parametric, learning the structure of the nominal distribution without assumptions of linearity, smoothness or continuity of the level set boundaries. (5) Like Scott and Nowak X  X  method, GEM is provably optimal -indeed uniformly most powerful of specified level -for the case that the anomaly density is a mixture of the nominal and a uniform density. (6) GEM easily adapts to local structure, e.g. changes in local dimensionality of the support of the nominal density.
 We introduce an incremental Leave-one-out (L1O) kNNG as a particularly versatile and fast anom-aly detector in the GEM class. Despite the similarity in nomenclature, the L1O kNNG is different from k nearest neighbor (kNN) anomaly detection of [9]. The kNN anomaly detector is based on thresholding the distance from the test point to the k-th nearest neighbor. The L1O kNNG detector computes the change in the topology of the entire kNN graph due to the addition of a test sample and does not use a decision threshold. Furthermore, the parent GEM anomaly detection methodology has proven theoretical properties, e.g. the (restricted) optimality property for uniform mixtures and general consistency properties.
 We introduce the statistical framework for anomaly detection in the next section. We then describe the GEM approach in Section . Several simulations are presented n Section 4. The setup is the following. Assume that a training sample X n = { X 1 ,...,X n } of d -dimensional vectors X i is available. Given a new sample X the objective is to declare X to be a  X  X ominal X  sample consistent with X n or an  X  X nomalous X  sample that is significantly different from X n . This declaration is to be constrained to give as few false positives as possible. To formulate this problem we adopt the standard statistical framework for testing composite hypotheses. Assume that X n is an independent identically distributed (i.i.d.) sample from a multivariate density f 0 ( x ) supported on the unit d -dimensional cube [0 , 1] d . Let X have density f ( x ) . Anomaly detection can be formulated as testing the hypotheses H 0 : f = f o versus H 0 : f = f o at a prescribed level  X  of significance P (declare H 1 | H 0 )  X   X  .
 The minimum-volume-set of level  X  is defined as a set  X   X  in I R d which minimizes the volume |
 X   X  is defined as a set  X   X  in I R d which minimizes the R  X  enyi entropy H  X  ( X   X  )= 1 1  X   X  ln  X  subject to the constraint  X  almost everywhere.
 The test  X  X ecide anomaly if X  X   X   X   X  is equivalent to implementing the test function This test has a strong optimality property: when f 0 is Lebesgue continuous it is a uniformly most powerful (UMP) level  X  for testing anomalies that follow a uniform mixture distribution. Specif-ically, let X have density f ( x )=(1  X  ) f 0 ( x )+ U ( x ) where U ( x ) is the uniform density over [0 , 1] d and  X  [0 , 1] . Consider testing the hypotheses Proposition 1 Assume that under H 0 the random vector X has a Lebesgue continuous density f 0 and that Z = f 0 ( X ) is also a continuous random variable. Then the level-set test of level  X  is uniformly most powerful for testing (2). Furthermore, its power function  X  = P ( X  X   X   X  | H 1 ) is given by A sufficient condition for the random variable Z above to be continuous is that the density f 0 ( x ) have no flat spots over its support set { f 0 ( x ) &gt; 0 } . The proof of this proposition is omitted. There are two difficulties with implementing the level set test. First, for known f 0 the level set may be very difficult if not impossible to determine in high dimensions d 2 . Second, when only a training sample from f 0 is available and f 0 is unknown the level sets have to be learned from the training data. There are many approaches to doing this for minimum volume tests and these are reviewed in [7]. These methods can be divided into two main approaches: density estimation followed by plug in estimation of  X   X  via variational methods; and (2) direct estimation of the level set using function approximation and non-parametric estimation. Since both approaches involve explicit approximation of high dimensional quantities, e.g. the multivariate density or the boundary of the set  X   X  , these methods are difficult to apply in high dimensional problems, i.e. d&gt; 2 . The GEM method we propose in the next section overcomes these difficulties. GEM is a method that directly estimates the critical region for detecting anomalies using mini-mum coverings of subsets of points in a nominal training sample. These coverings are obtained by constructing minimal graphs, e.g. a MST or kNNG, covering a K -point subset that is a given pro-portion of the training sample. Points not covered by these K -point minimal graphs are identified as tail events and allow one to adaptively set a pvalue for the detector.
 and E = { e } is the set of edges of the graph. The total power weighted length, or, more simply, the length, of G is L ( X n )= e  X  E | e |  X  where  X &gt; 0 is a specified edge exponent parameter. 3.1 K-point MST The MST with power weighting  X  is defined as the graph that spans X n with minimum total length: where T is the set of all trees spanning X n .
 Definition 1 K-point MST : Let X n,K denote one of the n K subsets of K distinct points from X n . Among all of the MST X  X  spanning these sets, the K-MST is defined as the one having minimal length The K -MST thus specifies the minimal subset of K points in addition to specifying the minimum length. This subset of points, which we call a minimal graph covering of X n of size K , can be viewed as capturing the densest region of X n . Furthermore, if X n is a i.i.d. sample from a multivariate density f ( x ) and if lim K,n  X  X  X  K/n =  X  and a greedy version of the K -MST is implemented, this set converges a.s. to the minimum  X  -entropy set containing a proportion of at least  X  = K/n of the mass of the (Lebesgue component of) f ( x ) , where  X  =( d  X   X  ) /d . This fact was used in [2] to motivate the greedy K -MST as an outlier resistant estimator of entropy for finite n, K . Define the K -point subset selected by the greedy K-MST. Then we have the following As the minimum entropy set and min-imum volume set are identical, this suggests the following minimal-volume-set anomaly detection algorithm, which we call the  X  X -MST anomaly detector. X  [1] Process training sample : Given a level of significance  X  and a training sample X n = {
X 1 ,...,X n } , construct the greedy K-MST and retain its vertex set X  X  n,K . [2] Process test sample : Given a test sample X run the K-MST on the merged training-test sample X [3] Make decision : Using the test function  X  defined below decide H 1 if  X  ( X )=1 and decide H 0 if  X  ( X )=0 . When the density f 0 generating the training sample is Lebesgue continuous, it follows from [2, The-orem 2] that as K, n  X  X  X  the K-MST anomaly detector has false alarm probability that converges to  X  =1  X  K/n and power that converges to that of the minimum-volume-set test of level  X  . When the density f 0 is not Lebesgue continuous some optimality properties of the K-MST anomaly detec-tor still hold. Let this nominal density have the decomposition f 0 =  X  0 +  X  0 , where  X  0 is Lebesgue continuous and  X  0 is singular. Then, according to [2, Theorem 2], the K-MST anomaly detector will have false alarm probability that converges to (1  X   X  )  X  , where  X  is the mass of the singular compo-nent of f 0 , and it is a uniformly most powerful test for anomalies in the continuous component, i.e. for the test of H 0 :  X  =  X  0 , X  =  X  0 against H 1 :  X  =(1  X  )  X  0 + U ( x ) , X  =  X  0 . It is well known that the K-MST construction is of exponential complexity in n [10]. In fact, even for K = n  X  1 , a case one can call the leave-one-out MST, there is no simple fast algorithm for computation. However, the leave-one-out kNNG, described below, admits a fast incremental algorithm. 3.2 K-point kNNG Let X n = { X 1 ,...,X n } be a set of n points. The k nearest neighbors (kNN) { X i (1) ,...X i ( k ) } of a point X i  X  X  n are the k closest points to X i points in X n  X  X  X i } . Here the measure of nearest neighbors. The kNN graph (kNNG) over X n is defined as the union of all of the kNN edges { e Definition 2 K-point kNNG : Let X n,K denote one of the n K subsets of K distinct points from X n . Among all of the kNNG over each of these sets, the K-kNNG is defined as the one having minimal As the kNNG length is also a quasi additive continuous functional [11], the asymptotic KMST theory of [2] extends to the K-point kNNG. Of course, computation of the K-point kNNG also has exponential complexity. However, the same type of greedy approximation introduced by Ravi [10] for the K -MST can be implemented to reduce complexity of the K-point kNNG. This approximation to the K-point kNNG will satisfy the tightly coverable graph property of [2, Defn. 2]. We have the following result that justifies the use of such an approximation as an anomaly detector of level  X  =1  X   X  , where  X  = K/n : Proposition 2 Let X  X  n,K be the set of points in X n that results from any approximation to the K-point kNNG that satisfies the property [2, Defn. 2]. Then lim n  X  X  X  P 0 ( X  X  n,K  X   X   X  )  X  1 and lim n  X  X  X  P 0 ( X  X  n,K  X   X   X  )  X  0 , where K = K ( n ) = floor(  X n ) ,  X   X  is a minimum-volume-set of level  X  =1  X   X  and  X   X  =[0 , 1] d  X   X   X  .
 Proof : We provide a rough sketch using the terminology of [2]. Recall that a set B m  X  [0 , 1] d of resolution 1 /m is representable by a union of elements of the uniform partition of [0 , 1] d into hypercubes of volume 1 /m d . Lemma 3 of [2] asserts that there exists an M such that for m&gt;M the limits claimed in Proposition 2 hold with  X   X  replaced by A m  X  , a minimum volume set of resolution 1 /m that contains  X   X  .As lim m  X  X  X  A m  X  = X   X  this establishes the proposition.
 Figures 1-2 illustrate the use of the K-point kNNG as an anomaly detection algorithm.
 Figure 1: Left: level sets of the nominal bivariate mixture density used to illustrate the K point kNNG anomaly detection algorithms. Right: K-point kNNG over N=200 random training samples drawn from the nominal bivariate mixture at left. Here k=5 and K=180, corresponding to a significance level of  X  =0 . 1 . Figure 2: Left: The test point  X * X  is declared anomalous at level  X  =0 . 1 as it is not captured by the K-point kNNG (K=180) constructed over the combined test sample and the training samples drawn from the nominal bivariate mixture shown in Fig. 1. Right: A different test point  X * X  is declared non-anomalous as it is captured by this K-point kNNG. 3.3 Leave-one-out kNNG (L1O-kNNG) The theoretical equivalence between the K-point kNNG and the level set anomaly detector motivates a low complexity anomaly detection scheme, which we call the leave-one-out kNNG, discussed in this section and adopted for the experiments below. As before, assume a single test sample X = X n +1 and a training sample X n . Fix k and assume that the kNNG over the set X n has been computed. To determine the kNNG over the combined sample X n +1 = X n  X  X  X n +1 } one can execute the following algorithm: This algorithm will detect anomalies with a false alarm level of approximately 1 / ( n +1) . Thus larger sizes n of the training sample will correspond to more stringent false alarm constraints. Furthermore, the p-value of each test point X i is easily computed by recursing over the size n of the training sample. In particular, let n vary from k to n and define n  X  as the minimum value of n for which X i is declared an anomaly. Then the p-value of X i is approximately 1 / ( n A useful relative influence coefficient  X  can be defined for each point X i in the combined sample X The coefficient  X  ( X n +1 )=1 when the test point X n +1 is declared an anomaly.
 Using matlab X  X  matrix sort algorithm step 1 of this algorithm can be computed an order of magnitude faster than the K-point MST ( N 2 logN vs N 3 logN ). For example, the experiments below have shown that the above algorithm can find and determine the p-value of 10 outliers among 1000 test samples in a few seconds on a Dell 2GHz processor running Matlab 7.1. Here we focus on the L1O kNNG algorithm due to its computational speed. We show a few repre-sentative experiments for simple Gaussian and Gaussian mixture nominal densities f 0 . Figure 3: Left: The plot of the anomaly curve for the L1O kNNG anomaly detector for detecting deviations from a nominal 2D Gaussian density with mean (0,0) and correlation coefficient -0.5. The boxes on peaks of curve correspond to positions of detected anomalies and the height of the boxes are equal to one minus the computed p-value. Anomalies were generated (on the average) every 100 samples and drawn from a 2D Gaussian with correlation coefficient 0.8. The parameter  X  is equal to 1  X   X  , where  X  is the user defined false alarm rate. Right: the resampled nominal distribution ( X   X   X ) and anomalous points detected ( X * X ) at the iterations indicated at left. First we illustrate the L1O kNNG algorithm for detection of non-uniformly distributed anomalies from training samples following a bivariate Gaussian nominal density. Specifically, a 2D Gaussian density with mean (0,0) and correlation coefficient -0.5 was generated to train of the L1O kNNG detector. The test sample consisted of a mixture of this nominal and a zero mean 2D Gaussian with correlation coefficient 0.8 with mixture coefficient =0 . 01 . In Fig. 3 the results of simulation with a training sample of 2000 samples and 1000 tests samples are shown. Fig. 3 is a plot of the relative influence curve (3) over the test samples as compared to the most outlying point in the (resampled) training sample. When the relative influence curve is equal to 1 the corresponding test sample is the most outlying point and is declared an anomaly. The 9 detected anomalies in Fig. 3 have p-values less than 0.001 and therefore one would expect an average of only one false alarm at this level of significance. In the right panel of Fig. 3 the detected anomalies (asterisks) are shown along with the training sample (dots) used to grow the L1O kNNG for that particular iteration -note that to protect against bias the training sample is resampled at each iteration.
 Next we compare the performance of the L1O kNNG detector to that of the UMP test for the hypotheses (2). We again trained on a bivariate Gaussian f 0 with mean zero, but this time with identical component variances of  X  =0 . 1 . This distribution has essential support on the unit square. For this simple case the minimum volume set of level  X  is a disk centered at the ori-gin with radius  X  =(1  X  )  X  + (1  X  2  X  X  2 ln 1 / X  ) . We implemented the GEM anomaly detector with the incre-mental leave-one-out kNNG using k =5 . The training set consisted of 1000 samples from f 0 and the test set consisted of 1000 samples from the mixture of a uniform density and f 0 with parameter ranging from 0 to 0 . 2 . Figure 4 shows the empirical ROC curves obtained using the GEM test vs the theoretical curves (labeled  X  X lairvoyant X ) for several different values of the mixing parameter. Note the good agreement between theoretical prediction and the GEM implementation of the UMP using the kNNG. Figure 4: ROC curves for the leave-one-out kNNG anomaly detector described in Sec. 3.3. The labeled  X  X lairvoyant X  curve is the ROC of the UMP anomaly detector. The training sample is a zero mean 2D spherical Gaussian distribution with standard deviation 0.1 and the test sample is a this 2D Gaussian and a 2D uniform-[0 , 1] 2 mixture density. The plot is for various values of the mixture parameter . A new and versatile anomaly detection method has been introduced that uses geometric entropy minimization (GEM) to extract minimal set coverings that can be used to detect anomalies from a set of training samples. This method can be implemented through the K-point minimal spanning tree (MST) or the K-point nearest neighbor graph (kNNG). The L1O kNNG is significantly less com-putationally demanding than the K-point MST. We illustrated the L1O kNNG method on simulated data containing anomalies and showed that it comes close to achieving the optimal performance of the UMP detector for testing the nominal against a uniform mixture with unknown mixing para-meter. As the L1O kNNG computes p-values on detected anomalies it can be easily extended to account for false discovery rate constraints. By using a sliding window, the methodology derived in this paper is easily extendible to on-line applications and has been applied to non-parametric intruder detection using our Crossbow sensor network testbed (reported elsewhere).
 Acknowledgments This work was partially supported by NSF under Collaborative ITR grant CCR-0325571.
 [1] A. Hero, B. Ma, O. Michel, and J. Gorman,  X  X pplications of entropic span-[2] A. Hero and O. Michel,  X  X symptotic theory of greedy approximations to minimal k-point [3] T. S. Ferguson, Mathematical Statistics -A Decision Theoretic Approach . Academic Press, [4] I. V. Nikiforov and M. Basseville, Detection of abrupt changes: theory and applications . [5] B. Scholkopf, R. Williamson, A. Smola, J. Shawe-Taylor, and J. Platt,  X  X upport vector method [6] G. R. G. Lanckriet, L. El Ghaoui, and M. I. Jordan,  X  X obust novelty detection with single-class [7] C. Scott and R. Nowak,  X  X earning minimum volume sets, X  Journal of Machine Learning Re-[8] A. Lazarevic, A. Ozgur, L. Ertoz, J. Srivastava, and V. Kumar,  X  X  comparative study of anom-[9] S. Ramaswamy, R. Rastogi, and K. Shim,  X  X fficient algorithms for mining outliers from large [10] R. Ravi, M. Marathe, D. Rosenkrantz, and S. Ravi,  X  X panning trees short or small, X  in Proc. 5th [11] J. E. Yukich, Probability theory of classical Euclidean optimization , vol. 1675 of Lecture Notes
