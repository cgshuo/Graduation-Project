 Department of Information and Knowledge Engineering, University of Economics, W. Churchill Sq. 4, 130 67 Prague, Czech Republic Tel.: +420 224 095 493; Fax: +420 224 095 400; E-mail: berka@vse.cz 1. Introduction
Human learning is one of the most important characteristics of human intelligence. In a similar 1950s. However, it has become very attractive, especially recently, thanks to the Expert Systems (ES), Knowledge Discovery from Databas es (KDD), Data Mining (DM), and t heir respective developments and applications. There is a number of machine learning methods: induction of decision trees, rule learning algorithms, neural networks, instance-based learners, or Bayesian networks.  X  X f-then X  rules belong to the most popular formalism used to represent knowledge either obtained from and data mining).

The most commonly used approach to learning decision rules is the set-covering approach also called
The other way to create decision rules is the compositional approach. In this approach the covered examples are not removed during learning, so an example can be covered with more rules. Thus more numerical value is usually added to the rule. The work reported in this paper fi ts into this approach. 2. The KEX algorithm 2.1. Basic notions
KEX (Knowledge EXplorer) works with categorial data in the form of a data matrix. Rows correspond to objects, columns correspond to attributes. Objects can be patients, animals, experiments, sites in a of objects. Sometimes attributes are called variables or features.

Values of attributes describe different categories of objects. We will denote A ( v ) a category of objects which have value v in the attribute A . So, for example, the attribute-value pair (sex, female) corresponds to the category woman. We assume that all attributes are nominal, i.e., their values are mutually exclusive and without any ordering (e.g., hair color). When working with numerical data (e.g., temperature), categorization must be done before using the system.

Categories are used to create combinations . From the logical point of view, combination C of categories is conjunction of propositions which corre spond to single categories. Number of categories in a combination is the length of the combination. We will denote a combination of the length k . For example, hair color(yellow)  X  sex(female) is a combination of the We will denote it C .

For two combinations C 1 , C 2 with no common attribute we can formulate empirical implications form: where So, from the contingency table above of C 1 occurring together with C 2 (conditional probability P( C 2 | C 1 )): together with C 1 (conditional probability P( C 1 | C 2 )): to 1  X  p (100 p%).

The validity of an implication expresses how strong the left-hand side combination is related to the side, the validity equals to 1). The coverage of implication expresses how strong the right-hand side also ful fi ll the left-hand side, the coverage equals to 1).

Rules are in the form of implications with a fi xed right-hand side combination C , the class given by the user. Weight w  X  [0, 1] is assigned to every rule; this number (derived from the validity of the implication) is used during consultation to infer the class value of an example. 2.2. Motivation
The underlying idea of the KEX algorithm is to select such association rules (implications C 1  X  C 2 ) that can build a knowledge base of a diagnostic expert system [1]. To solve such a task, we have to decide about  X  the structure of the knowledge base;  X  the form of the used inference mechanism; and  X  the knowledge ac quisition method.
 The knowledge base (KB) of the created system consists of rules of the form where:
The inference mechanism uses forward chaining of the rules. During consultation (classi fi cation) for to the class i with highest value of w  X  i . To combine weights of the rules, we use a pseudobayesian (Prospector-like) combination function [6]:
The knowledge acquisition method is based on the idea of top-down re fi nement of a set o rules. A new rule is added into the knowledge base ( KB ), only if KB becomes inconsistent with the analyzed data. 2.3. Algorithm for two class problems
KEX works in an iterative way, in each iteration testing and expanding an implication Ant  X  C .This process starts with an  X  X mpty rule X  with weight as a relative frequency of C in data, and stops after minimal frequency of Ant , and minimal validity of Ant  X  C . The implications are evaluated according concepts, which correspond to different categories of a given attribute.
 signi fi cantly from the composed weight (value obtained when composing weights of all sub-rules of the implication Ant  X  C ), then this implication is added to the knowledge base. To test the difference between validity and composed weight, we use the chi-square goodness-of-fi t test. We thus compute the value By default,  X  =0 . 05 as this value is suitable for most applications, but the user can use the values 0.025 or 0.01 to perform stricter testing. In the formula above, T denotes the number of classes, Ant i denotes the number of examples of class i covered by Ant and w  X  i ( Ant ) denotes the composed weight of the test criterion, that can be used both for two-and multi-class problems. For two-class problems, T equals 2, and one of the classes is considered to be the target concept C (examples of this class are considered to be examples of the target concept while examples of the other class are considered to be counter-examples of the target concept).

If the value CHISQ is greater than the value of the  X  2 distribution, we add the implication Ant  X  C as and from the composed weight w  X  ( Ant ), in such a way that We compute the weight w using the inverse composing function, so where
The generalization (selecting implications using the  X  2 test) is usually very high. Typically, the resulting knowledge base consists only of a small fraction (several percent) of all implications which ful fi ll the input criteria.

While expanding, new implications are created by adding single categories to Ant . These categories of Ant ) in an ordered list of implications. So KEX generates every implication only once, and for any implication in question, all of its sub-implications have already been tested.

The input to the algorithm consists of input data D , goal combination C , maximal length of the of the rule P min . The output of the algorithm is knowledge base KB (see Algorithm 2). following properties:  X  KB is a knowledge base,  X  KB is minimal in the sense that KB is a part of each set of implications B , which is a knowledge It is important to stress that when visually interpreting the knowledge base, sometimes some  X  X bvious X  pieces of knowledge cannot be found. This is because the effect of the corresponding  X  X issing X  rule can be composed from its (more general) sub-rules, which are already included in the knowledge base. account as a whole.

The algorithm described in this Subsection can be extended to learn rules for multi-class problems as described in the next Subsection. 2.4. Extension to multi-class problems
Rule-learning algorithms can deal with multi-class problems in two ways: either the algorithm builds rules for each class separately (handling examples of one class as examples of the concept and examples simultaneously. KEX uses the latter option, creating a set of rules distribution of examples within this region to classes C i .

This extension requires several modi fi cations to the algorithm described in Subsection 2.3. The main reason is in the two different ways of KEX processing uncertainty and expressing the notion of  X  X nknown X . When working with weights (e.g., when using the combination function  X  ), the notion of  X  X nknown X  corresponds to the weight 0.5, while when working with validities (e. g., when computing the CHISQ criterion), the notion of  X  X nknown X  X orresponds to the relative frequency of uniformly distributed classes, i.e., to the probability 1/ T where T is the number of classes. We thus have to use a mapping weight 0.5 to probability 1/ T (and vice versa). Such a mapping (also used in the P ROSPECTOR expert system) can be de fi ned as follows (Fig. 1 illustrates this mapping for T =5 ): resp. This mapping is used in the following modi fi cations of Algorithm 2:  X  the  X  2 test is performed for the validity of the implication and composed weight transformed to  X  weight w i of the rule Ant  X  C i ( w i ) is computed from w i  X  w  X  i ( Ant )= P ( C i | Ant ) ,where 2.5. Handling numeric input attributes
The KEX algorithm can process only categorial attributes. Numeric data must thus been pre-processed ber of categories using equidistant cut-points, or categorization based on mean and standard deviation. number of class sensitive algorithms has been proposed (see, e.g., [7,8,12,13]).

Our discretization algorithm is closely related to the KEX rule learning method described in Sub-section 2.3. Nevertheless, it can be used to preprocess data for any machine learning algorithm that intervals from sequences of values, for which  X  X ost X  objects belong to the same class. See Algorithm 3 for a semi-formal description. The criterion used in ASSIGN is based on the chi-square value (that is also used in the KEX rule learning method, see Eq. (9)). Our algorithm is thus similar to ChiMerge, which uses the chi-square test as well [12]. But unlike ChiMerge, in the merging step our algorithm intervals to the immediate left ( Int i  X  1 ) and right ( Int i +1 ).

Within the KEX knowledge acquisition approach, this discretization will lead to rules with the de-scription of the interval on the left-hand side of the rule: It is important to notice that the number of resulting intervals is determined by the algorithm and not given by the user as an input parameter.

During discretization of an attribute, we can lose some information hidden in the data. We can thus estimate the maximum possible accuracy that can be reached on the data as: When computing the maximum possible accuracy before and after the discretization, we can assess the been used for the experiments further reported in Section 4. 2.6. KB evaluation and testing
The acquired knowledge base can be tested for its accuracy. The standard way to do this is to use some on the confusion matrix, which summarizes the results of classi fi cation of testing data. The standard format of this matrix is (for two classes) shown in Table 2. Here:  X  FP (false positive) is the number of examples that are incorrectly classi fi ed as belonging to class  X  FN (false negative) is the number of examples that are incorrectly classi fi ed as belonging to class number of correctly classi fi ed examples) not correctly classify any example of other classes. Therefore, other performance measures must be used. One straightforward extension of the total accuracy ACC is the accuracy computed for each class separately. This value is (for two classes  X  +  X  X nd X   X   X ) calculated as So when increasing the value of  X  , the total accuracy (computed only from classi fi ed examples) can (and usually will) improve, but, simultaneously, the number of classi fi ed examples will decrease. The best trade-off between accuracy and the number of classi fi ed examples for a given data set has to be found empirically. Such behavior allows us to adopt a semi-automated classi fi cation strategy in which
Another extension of the standard confusion matrix done by KEX is that KEX counts the number of examples for which no rule was applicable (these examples are considered as not predicted and are The extended confusion matrix is shown in Table 3.

There are a number of ways to obtain testing data to evaluate a classi fi er using measures described above. The current implementation of KEX in the LISp-Miner system (for details, see Section 3) uses training data, testing set, n-fold cross-validation and random split to get the testing examples. 3. Implementation of the KEX algorithm
The KEX algorithm is a part of the LISp-Miner system, an academic software tool developed at the University of Economics, Prague to extract association rules and decision rules from data. The system is a successor of the GUHA method, an original Czech approach to association rule mining from mid-1960s ([11]). Contrary to  X  X lassical X  association rules, GUHA (and LISp-Miner as well) more expressive. The antecedents and succedents are created in the form of Boolean attributes, where a sequence of intervals for numeric attributes.
 LISp-Miner consists of 7 proceduresfor mining of different types of associationrules and the procedure KEX [15]:  X  4ft-Miner Procedure mines for association rules  X   X   X  and for conditional association rules  X   X   X  SD4ft-Miner Procedure mines for SD4ft patterns  X   X   X  :  X   X  SD4FT  X / X  . This pattern means that  X  Ac4ft-Miner Procedure mines for action rules  X  St  X  (  X  I  X   X  F )  X  Ac  X  St  X  (  X  I  X   X  F ) / X  .This  X  KL-Miner Procedure mines for KL-patterns R  X  KL C / X  . A KL-pattern means that attributes R and  X  SDKL-Miner Procedure mines for SDKL-patterns  X   X   X  :R  X  SDKL C / X  . A SDKL-pattern  X  CF-Miner Procedure mines for CF-patterns  X  CF R / X  . A CF-pattern means that frequencies of  X  SDCF-Miner Procedure mines for SDCF-patterns  X   X   X  :  X  SDCF R / X  . A SDCF-pattern means Beside these  X  X nalytical X  procedures, LISp-Miner contains also a data preprocessing module called DataSource. This module allows us to perform various data manipulations, e.g., creating new attributes from existing ones or discretization of numeric attributes. Analyzed data are stored in MS Access. The concept of meta-data, also realized as a database in MS Access, allows us to store performed tasks and their results. LISp-Miner is thus closely related to the MS Windows and MS Of fi ce environment. From the CRISP-DM methodology point of view, LISp-Miner supports the data understanding, data preparation, modeling and evaluation steps. The system is freely available at http://lispminer.vse.cz.
The structure of LISp-Miner is shown in Fig. 2. The system consists of a number of separate modules with assigning a meta-data fi le to the data (using the LMAdmin module). The next step is to select relevant attributes and eventually pre-process the data using the DataSource module; this corresponds to the data preparation step of CRISP-DM. After pre-processing the data, the modeling step can start by invoking any of the  X  X ask X : module of any of the analytical procedures. The results can then be inspected using the corresponding  X  X esults X  module. The LMAdmin module needs to be invoked only once but DataSource and Task/Results modules can be used repeatedly for modifying and tuning various parameters of the procedures.

KEX bene fi ts from integration into LISp-Miner through th e possibility of using the DataSource module KexResults). KexTask is used to run the machine learning algorithm, KexResults is used to visualize and testing data table.

The input parameters for the KexTask procedure are (see Fig. 3 for a screenshot illustrating analysis of Monk1 data):  X  list of attributes that can occur in Ant (left-hand side of a rule),  X  class attribute C ,  X  l max (max. length of Ant ),  X  f min (min. frequency of Ant ),  X  p min (min. validity of a rule Ant  X  C ),  X  option not to insert the empty rule  X  X  X  C ( w ) into KB ,  X  option not to perform the  X  2 test (in this situation, every generated implication Ant  X  C with its  X  signi fi cance level for the  X  2 test,  X  method of testing (training data, cross-validation or random sample of a given size). To tune these parameters requires some experience with the system. KEX therefore uses default values that work reasonably well in most situations. Beside this, there are some recommended strategies for setting the parameters l max ,f min and p min , that mostly affect the resulting KB :  X  for minimal analysis (the default strategy), l max =1 , f min =1 example and p min =0 ,  X  for full analysis, l max = number of input attributes, f min =1 example and p min =0 ,  X  for strong analysis, p min =0 . 9 found under the menu item  X  X onsultation X  (again, see a screenshot in Fig. 4 for Monk1 data results). this data. 4. Empirical comparison with other rule learning algorithms
The empirical comparison is based on testing the KEX algorithm against some rule learning algorithms implemented in Weka, a freely available data mining system developed at University of Waikato, New Zealand [18]. The algorithm PART is a set-covering algorithm based on partial decision trees; to make turned into a rule [9]. PRISM is a set-covering algorithm that produces rules by rule specializations starting from a rule with empty left-hand side, and stops adding conditions when the rule reaches the accuracy equal to 1 [3]. Jrip is an implementation of the RIPPER algorithm. This algorithm performs and if-false rules [10].
 The data used for the experiments are taken from the Machine Learning Repository maintained at University of California, Irvine [17]. Table 4 shows basic characteristics of the data used (number of examples, number of attributes, number of c lasses). Some data sets (Monk1, Monk3, Mushroom, preprocessing. Other data sets (ACredit, Diab, Iris, JCredit) contain numeric attributes. In this case, the data have been discretized prior to use of the learning algorithms, using the method described in Subsection 2.5.

Table 5 summarizes the results of the experiments. For each of the learning algorithms used, the table shows the number of found rules and the classi fi cation accuracy averaged over 10 runs of a 10-fold cross-validation test. The tested algorithms are PART, PRISM, JRip and Ridor from Weka and KEX with two different settings. KEX1 denotes the minimal analysis strategy ( l min =1 , f min =0 %and P min =0 ), KEX2 denotes the strong analysis strategy ( l min =2 , f min =1 %and P min =0 . 9 ). When  X  =0 . 025 . The impact of different settings of this parameter on the total accuracy and the number of the claim from Subsection 2.6 that when increasing  X  , the total accuracy can improve but the number of classi fi ed examples will decrease.

The results in Table 5 show that KEX is comparable with the other machine learning algorithms the number of rules). KEX was the winner for 7 of the 10 analyzed data sets, and for three winning results the difference between KEX and the second best algorithm was greater than 3%. But KEX also generates the largest sets of resulting rules. 5. Conclusions
The knowledge (concept) acquisition from data i s the domain of machine learning. If we think about the KEX algorithm in this way, it performs symbolic empirical learning from examples, where the data, unknown values, redundancy and contradictions. Several experiments with different data show that differences between KEX and these algorithms: 1. because of the statistical test used, KEX needs a reasonable amount of input data; 2. KEX can create more than one rule to cover a speci fi cexample; 3. the knowledge base can contain both a rule and its sub-rule; and 4. during consultations, the systemcan recommend (infer with positive weight) more than one concept.
Another difference with a set-covering algorithm is that the resulting knowledge base has to be taken into account as a whole in a PROSPECTOR-like inference mechanism. Visual interpretation of single approaches, the role of the domain expert is in the data understanding, data preparation and evaluation steps of the data mining process. The rules itself are created and evaluated automatically using data. Acknowledgement
This paper was prepared with the support the gran t MSMT 1M06014 (from the Ministry of Education of the Czech Republic), the grant GACR 201/08/0802 (from the Grant A gency of the C zech Repub lic) and the bi-lateral project Kontakt MSMT ME913.
 References
