
It is estimated that less than 10 percent of the world X  X  species have been described, yet species are being lost daily due to human destruction of natural habitats. The job of describing the earth X  X  remaining species is exacerbated by the shrinking number of practicing taxonomists and the very slow pace of traditional taxonomic research. In this article, we tackle, from a novelty detection perspective, one of the most important and challenging research objectives in tax-onomy  X  new species identification. We propose a unique and efficient novelty detection framework based on statisti-cal depth functions. Statistical depth functions provide from the  X  X eepest X  point a  X  X enter-outward ordering X  of multi-dimensional data. In this sense, they can detect observa-tions that appear extreme relative to the rest of the obser-vations, i.e., novelty. Of the various statistical depths, the spatial depth is especially appealing because of its compu-tational efficiency and mathematical tractability. We pro-pose a novel statistical depth, the kernelized spatial depth (KSD) that generalizes the spatial depth via positive definite kernels. By choosing a proper kernel, the KSD can cap-ture the local structure of a data set while the spatial depth fails. Observations with depth values less than a threshold are declared as novel. The proposed algorithm is simple in structure: the threshold is the only one parameter for a given kernel. We give an upper bound on the false alarm probability of a dep th-based detector, which can be used to determine the threshold. Experimental study demonstrates its excellent potential in new species discovery.
Approximately 1 . 4 million species are currently known to science. However, estimates based on the rate of new species discovery place the total number of species on planet earth at 10 to 30 times this number. Human popu-lation expansion and habitat destruction are causing extinc-tions of both known and yet to be discovered species. The accelerated pace of species dec line has fueled the current biodiversity crisis [20], in which it is feared large percent-age of the earth X  X  species will be lost before they can be discovered and described.

The job of discovering and describing new species falls on taxonomists. The science of taxonomy has also been suf-fering from dwindling numbers of experts over the past few decades [25]. Moreover, the pace of taxonomic research, as traditionally practiced, is very slow. In recognizing a species as new to science, taxonomists use a gestalt recog-nition system that integrate s multiple characters of body shape, external body characteristics, and pigmentation pat-terns. They then make careful counts and measurements on large numbers of specimens from multiple populations across the geographic ranges of both the new and closely related species, and identify a set of external body charac-ters that uniquely diagnoses the new species as distinct from all of its known relatives. The process is laborious and can take years or even decades to co mplete, depending on the geographic range of the species.

We believe that the pace of data gathering and analysis in taxonomy can be greatly increased through the integration of machine learning and data mining techniques into taxo-nomic research. In this paper, we tackle one of the most important and challenging research objectives in taxonomy  X  new species discovery.
From a machine learning perspective, new species dis-covery is closely related to novelty detection. Novelty de-tection is one of the most challenging problems in data min-ing [8]. When  X  X ormal X  observations are given as a training data set, novelty detection can be formulated as finding ob-servations that significantly deviate from the training data, which is essentially a one-class learning problem.
A statistically natural tool for quantifying the deviation is the probability density of the normal observations. Roberts and Tarassenko [24] approximated the distribution of the training data by a Gaussian mixture model. For every ob-servation, an novelty score is defined as the maximum of the likelihood that the observation is generated by each Gaus-sian component. An observation is identified as novel if the score is less than a threshold. Schweizer and Moura [29] modeled normal data, background clutter in hyperspectral images, as a 3 -dimensional Gauss-Markov random field. Several methods are developed to estimate the random field parameters. Miller and Browning [18] proposed a mixture model for a set of labeled and unlabeled samples. The mix-ture model includes two types of mixture components: pre-defined components and nonpredefined components. The former generate data from known classes and assume class labels are missing at random. The latter only generate un-labeled data, corresponding to the novelty in the unlabeled samples. Parra et al. [19] proposed a class of volume con-serving maps that transforms an arbitrary distribution into a Gaussian. Given a decision threshold, novelty detection is based on the corresponding contour of the estimated Gaus-sian density, i.e., novelty lies outside the hypersphere de-fined by the contour.

Instead of estimating the probability density of the nor-mal observations, Sch  X  olkopf et al. [28] introduced a tech-nique to capture the support of t he probability density, i.e., a region in the input space where most of the normal obser-vations reside in. Hence novel observations lie outside the boundary of the support region. The problem is formulated as finding the smallest hypersphere to enclose most of the training samples in a kernel induced feature space, which can be converted to a quadratic program. Because of its similarity to support vector machines (SVM) [34] from an optimization viewpoint, the method is called 1-class SVM. Along the line of 1-class SVM, Campbell and Bennett [5] estimated the support region of a density using hyperplanes in a kernel induced feature space. The  X  X ptimal X  hyper-plane is defined as one that puts all normal observations on the same side of the hyperplane (the support region) and as close to the hyperplane as possible. Such a hyperplane is the solution of a linear program. R  X  atsch et al. [22] devel-oped a boosting algorithm for one-class classification based on connections between boosting and SVMs. Banerjee et al. [3] applied 1-class SVM for anomaly detection in hyper-spectral images and demonstrated improved performance compared with the method described in [23].

There is an abundance of prior work that applies stan-dard supervised learning techniques to tackle novelty de-tection [1, 11, 17, 32]. These methods generate a labeled data set by assigning one label to the given normal exam-ples and the other label to a set of artificially generated novel observations. In [17], a neural network-based nov-elty detector is trained based on normal observations and artificial novel examples gener ated by a uniform distribu-tion. Han and Cho [11] use artificially generated intrusive sequences to train an evolutionary neural network for intru-sion detection. Abe et al. [1] propose a selective sampling method that chooses a small portion of artificial novelty in each training iteration. In general, the performance of these algorithms depends on the choice of the distribution of the artificial examples and the em ployed sampling plan. Stein-wart et al. [32] provide an interesting justification for the above heuristic by converting novelty detection to a prob-lem of finding level sets of data generating density.
In this paper, we propose a new novelty detection frame-work based on the notion of statistical depths .Nov-elty detection methods that are based on statistical depths have been studied in statistics and computational geome-try [21, 27]. These methods provide a center-outward or-dering of observations. Novel observations are expected to appear more likely in outer layers with small depth values than in inner layers with large depth values. Depth-based methods are completely data-driven and avoid strong dis-tributional assumption. Moreover, they provide intuitive visualization of the data set via depth contours for a low dimensional input space. However, most of the current depth-based methods do not scale up with the dimension-ality of the input space. For example, finding peeling and depth contours, in practice, require the computation of d -dimensional convex hulls [21, 27], for which the compu-tational complexity is of magnitude O ( d/ 2 ) ,where is thesamplesizeand d is the dimension of an input space. The computational complexity for halfspace depth [33] and simplicial depth [16] is O ( d  X  1 log ) [26]; for projection depth [36], it is O ([ 2( d  X  1) d  X  1 /d ] 2 3 ) [9].
Of the various depths the spatial depth is especially ap-pealing because of its computa tional efficiency and math-ematical tractability [30]. Its computational complexity is of magnitude O ( 2 ) , independent of dimension d . Because each observation from a data set contributes equally to the value of depth function, spatial depth takes a global view of the data set. Consequently the novelty can be called as  X  X lobally X  novel observations. Nevertheless, many data sets from real-world applications exhibit more delicate struc-tures that entail identification of novelty relative to its neigh-borhood, i.e.,  X  X ocally X  novel observations.

We develop a novelty detection framework that avoids the above limitation of spatial depth. Specifically, we intro-duce a new depth function, kernelized spatial depth (KSD), which defines the spatial depth in a feature space induced by a positive definite kernel. By choosing a proper kernel, e.g., Gaussian kernel, the contours of a kernelized spatial depth function conform with the structure of the data set. Conse-quently the kernelized spatial depth can provide a local per-spective of the data set. The kernelized spatial depth of any observation can be evaluated directly from the data set with computational complexity O ( 2 ) . Observations with depth values less than certain thres hold are declared as novel. For a given kernel, the threshold on the depth value is the only parameter of the algorithm. We provide an upper bound on the false alarm probability of the detector, i.e., the proba-bility of misclassifying a normal observation as novel. The upper bound can be used to determine the threshold. We ap-ply the proposed novelty detector method to a small group of cypriniform fishes, comprising five species of suckers of the family Catostomidae and five species of minnows of the family Cyprinidae , in order to demonstrate its excellent po-tential in new species discovery.

The remainder of the paper is organized as follows. Sec-tion 2 motivates spatial depth-based novelty detection via the connection between spatial depth and spatial median. Section 3 introduces kernelized spatial depth. Section 4 presents an upper bound on the f alse alarm probability of the proposed kernelized spatial depth-based novelty detec-tor and provides an algorithmic view of the approach. In Section 5, we explain the experimental studies conducted and demonstrate the results. We conclude and discuss pos-sible future work in Section 6.
As Barnett and Lewis described [4],  X  X hat characterizes the  X  X utlier X  is its impact on the observer (not only will it ap-pear extreme but it will seem, to some extent, surprisingly extreme) X  . An intuitive way of measuring the extremeness is to examine the relative location of an observation with respect to the rest of the population. An observation that is far away from the center of the distribution is more likely to be novel than observations that are closer to the center. This suggests a simple novelty detection approach based on the distance between an observation and the center of a distri-bution.
Although both the sample mean and median of a data set are natural estimates for the center of a distribution, the me-dian is insensitive to extreme observations while the mean is highly sensitive. A single contaminating point to a data set can send the sample mean, in the worst case, to infin-ity, whereas in order to have the same effect on the median, at least 50% of the data points must be moved to infinity. Let x 1 ,..., x be observations from a univariate distribu-tion F and x (1)  X  ...  X  x ( ) be the sorted observations in an ascending order. The sample median is x (( +1) when is odd. When is even, any number in the inter-val [ x ( / 2) , x (( +1) / 2) ] can be defined to be the sample me-Next, we present an equivalent definition that can be natu-rally generalized to a higher dimensional setting.
Let s :  X  X  X  1 , 0 , 1 } be the sign function, i.e., For x  X  , the difference between the numbers of observa-tions on the left and right of x is i =1 s ( x i  X  x ) .There are an equal number of observations on both sides of the sample median, so that the sample median is Replacing the absolute value | X | with the 2-norm (Euclidean norm)  X  , the sign function is readily generalized to mul-tidimensional data: the spatial sign function or the unit vec-tor [6], which is a map S : n  X  n given by where x = the spatial sign function, the multidimensional sample me-dian for multidimensional data { x 1 , x 2 ,..., x } X  n is a straightforward analogy of the univariate version (1), i.e., it is any x  X  n that satisfies The median defined in (2) is named as the spatial median or the L 1 median [35]. Next we give another equivalent definition of the spatial median that motivates the depth-based novelty detection. The concept of spatial depth was formally introduced by Serfling [30] based on the notion of spatial quantiles pro-posed by Chaudhuri [7], while a similar concept, L 1 depth, was first described by Vardi and Zhang [35]. For a multi-variate cumulative distribution function (cdf) F on n ,the spatial depth of a point x  X  n with respect to the distribu-tion F is defined as For an unknown cdf F , the spatial depth is unknown and can be approximated by the sample spatial depth :
Figure 1. A contour plot of the sample spa-tial depth based on 100 random observations (represented by  X   X  X ) from a bi-variate Gaus-sian distribution. The depth values are indi-cated on the contours. The example marked with  X  represents a possible novel observa-tion. It has a very low depth value of 0 . 0219 . where X = { x 1 , x 2 ,..., x } and |X X  X  x }| denotes the cardinality of the union X X  X  x } . Note that both D ( x ,F ) and its sample version have a range [0 , 1] .

Observing (2) and (3), it i s easy to see that the depth value at the spatial median is 1 . In other words, the spa-tial median is a set of data points that have the  X  X eepest X  depth 1 . Indeed, the spatial depth provides from the  X  X eep-est X  point a  X  X enter-outward X  ordering of multidimensional data. The depth attains the maximum value 1 at the deepest point and decreases to zero as a point moves away from the deepest to the infinity. Thus it gives us a measure of the  X  X xtremeness X  of a data point, which can be used for nov-elty detection . From now on all depths refer to the sample depth. Figure 1 shows a contour plot of the spatial depth D ( x , X ) based on 100 random observations (marked with  X   X  X ) generated from a bi-varia te Gaussian distribution with mean zero and a covariance matrix whose diagonal and off-diagonal entries are 2 . 5 and 1 . 5 , respectively. On each con-tour the depth function is constant with the indicated value. The depth values decrease outward from the  X  X enter X  (i.e., the spatial median) of the cloud. This suggests that a point with a low depth value is more likely to be novel than a point with a high depth value. For example, the point on the upper right corner on Figure 1 (marked with  X  ) has a very low depth value of 0 . 0219 . It is isolated and far away from the rest of the data points. This example motivates a simple novelty detection algorithm: Identify a data point as novel
Figure 2. Contour plot of the sample spatial depths based on 100 random observations (denoted by  X   X  X ) of a ring shaped distribution.

The depth values are indicated on the con-tours. The example (denoted by  X  ) at the cen-ter represents a possible novel observation.

It is depth value is 0 . 9544 . if its depth value is less than a threshold .

In order to make this a practical method, the following two issues need to be addressed: (1) How can we decide the threshold? (2) Can the spatial depth function capture the structure of the data cloud? We postpone the discussion on the first question to Section 4 where we present a framework to determine the threshold. The second question is related to the shape of depth contours. The depth contours of a spatial depth function tend to be circular [12], especially at low depth values (e.g., the outer contour in Figure 1). For a spherical symmetric distribution, such contours fit nicely to the shape of the data cloud. It is therefore reasonable to view a data point as novel if its depth is low because a lower depth implies a larger distance from the  X  X enter X  of the data cloud. However, in general, the relationship between the depth and the novelty in a data cloud may not be as straight-forward as is depicted in Figure 1. For example, Figure 2 shows the contours of the spatial depth function based on 100 random observations generated from a ring shaped dis-tribution. From the shape of the distribution, it is reasonable to view the point (marked with  X  )inthecenterasanovel observation. However, the depth at the location of the  X  is as high as 0 . 9544 . In fact, all of the 100 normal observations have depth smaller than that of the  X  X ovel X  observation at the center.

The above example demonstr ates that the spatial depth function may not capture the structure of a data cloud in the sense that a point isolated from the rest of the population may have a large depth value. This is due to the fact that the value of the depth function at a point depends only upon the sum of the unit vectors, each of which represents the direc-tion from the point to an observation. This definition down-plays the significance of dist ance hence reduces the impact of those extreme observations whose extremity is measured in (Euclidean) distance, so that it gains resistance against these extreme observations . On the other hand, the acquire-ment of the robustness of the depth function trades off some distance measurement, resulting in certain loss of the mea-surement of similarity of the data points. The distance of a point from the data cloud plays an important role in re-vealing the structure of the data cloud. In the following, we propose a method to tackle this limitation of spatial depth by incorporating into the depth function a distance metric (or a similarity measure) induced by a positive definite ker-nel function .
In various applications of machine learning and pattern analysis, carefully recoding the data can make  X  X atterns X  standing out. Positive definite kernels provide a computa-tionally efficient way to recode the data. A positive definite kernel,  X  : n  X  n  X  , implicitly defines an embedding map via an inner product in the feature space , For certain stationary kernels, e.g., the Gaussian kernel  X  ( x , y )=exp x  X  y 2 / X  2 ,  X  ( x , y ) can be interpreted as a similarity between x and y , hence it encodes a similar-ity measure.

The basic idea of the kernelized spatial depth is to evalu-ate the spatial depth in a feature space induced by a positive definite kernel. Noticing that x  X  y 2 = x , x + y , y  X  2 x , y = x T x + y T y  X  2 x T y , with simple algebra, one rewrites the norm in (3) as Replacing the inner products with the values of kernel  X  ,we obtain the (sample) kernelized spatial depth (KSD) function
D  X  ( x , X )=1  X 
Figure 3. Contour plots of KSD functions based on 100 random observations (marked with  X   X  X ) from a ring-shaped distribution. The depth values are marked on the contours.

The depth is kernelized with a Gaussian ker-nel with  X  =3 . The example (marked with  X  ) at the center represents a possible novel ob-servation. It has a depth value of 0 . 2651 . where  X   X  ( x , y )=  X  ( x , x )+  X  ( y , y )  X  2  X  ( x , y ) .Anal-ogous to the spatial sign function at 0 ,wedefine for x = y or x = z .
 The KSD (4) is defined for any positive definite kernels. Here we shall be particularly interested in stationary kernels (e.g., the Gaussian kernel), b ecause of their close relation-ship with similarity measures. Figure 3 shows the contour plot of the KSD based on the same 100 random observations generated from the ring shaped distribution in Figure 2. The Gaussian kernel with  X  =3 is used to kernelize the spatial depth. Interestingly, unlike the spatial depth, we observe that the kernelized spatial dep th captures the shapes of the data cloud. Moreover, the depth values are small for the possible novelty. The depth values at the location of the 0 . 2651 . A threshold of 0 . 27 can separate the novel observa-tion from the rest of the ring data. The remaining question is how we determine the threshold. This is addressed in the following section.
The idea of selecting a threshold is rather simple, i.e., choose a value which controls the false alarm probability (FAP) under a given significance level. FAP is the probabil-ity that normal observations are classified as novel. In the following, we derive a probabilistic bound on FAP.
Novelty detection formulated as a one-class learning problem can be described as follows. We have observations X = { x 1 , x 2 ,..., x } X  n from an unknown cdf, F . Based on the observations X , a given datum x is classified as normal or novel according to whether or not it is gener-ated from F .Let g : n  X  [0 , 1] be a novelty detector where g ( x )=1 indicates that x is novel. The FAP of a novelty detector g , P FA ( g ) , is the probability that an obser-vation generated from F is classified by the detector g as novel, i.e.
 where R o = { x  X  n : g ( x )=1 } is the collection of all observations that are classified as novel. The FAP can be es-timated by the false alarm rate ,  X  P FA ( g ) , which is computed by
For a given data set X and kernel  X  ,wedefineanovelty detector g  X  ( x , X ) by where t  X  [0 , 1] is a threshold. An observation x is clas-sified as novel according to g  X  ( x , X )=1 . Denote F the expectation calculated under cdf F . It follows that We have the following theorem for the bound of the FAP. Theorem 1 Let X = { x 1 , x 2 ,..., x train } X  n and Y = { y 1 , y 2 ,..., y test } X  n be i.i.d. samples from a distribution F on n .Let g  X  ( x , X ) be a novelty detector defined in (5). Fix  X   X  (0 , 1) . For a new random observa-tion x from cdf F , the following bound holds with probabil-ity at least 1  X   X  :
It is worthwhile to note that there are two sources of randomness in the above inequality: the random sample Y and the random observation x . For a specific Y , the above bound is either true or false, i.e., it is not random. For a random sample Y , the probability that the bound is true is at least 1  X   X  . Theorem 1 suggests that we can control the FAP by adjusting the t parameter of the detector. Although t does not appear explicitly in (6), it affects the value of sample version of FAP.
 Note that the detector is constructed from the training set X and evaluated using an independent test set Y . A bound as such is usually called a test set bound [14]. The FAP is bounded by the false alarm rate, evaluated on the test set, plus a term that shrinks in a rate proportional to the square root of the size of the test set. For a given desired FAP, we should choose the threshold to be the maximum value of t such that the right-hand side of (6) does not exceed the de-sired FAP. A proof of Theorem 1 is given in the Appendix.
We summarize the above discussion in pseudo code. The input is a set of observations X = { x 1 , x 2 ,..., x train n from an unknown cdf F and a kernel  X  . The following pseudo codes determine whether an observation x is novel. Algorithm 1 Learning a Novelty Detector 1 FOR (every pair of x i and x j in X ) 2 K ij =  X  ( x i , x j ) 3 END 4 given input x 5 FOR (every x i in X ) 6  X  i =  X  ( x , x i ) 7  X  i =  X  ( x , x )+ K ii  X  2  X  i 8IF  X  i =0 9 z i =0 10 ELSE 12 END 13 END 14 FOR (every pair of x i and x j in X ) 14 K ij =  X  ( x , x )+ K ij  X   X  i  X   X  j 15 END 17 OUTPUT ( x is novel if D  X  ( x , X )  X  t )
In terms of the number of kernel evaluations and multi-plications, the cost of computing the KSD depth for a given observation is O ( 2 ) . The above pseudo code assumes that the kernel  X  is given. Specifically, for Gaussian kernel, which is used in our experimental study, this requires the knowledge of  X  value. Finding an optimal kernel for a given problem is an interesting research issue for its own sake, but is out of the scope of this paper. We propose the following method to select the  X  parameter for a given set of observa-tions.
 Algorithm 2 Deciding  X  for Gaussian Kernel 1 FOR (every observation x i in X ) 3 END 4 OUTPUT (  X  =median( d 1 ,d 2 ,...,d ))
We apply the proposed novelty detector method to a small group of cypriniform fishes, comprising five species (minnows). of suckers of the family Catostomidae and five species of minnows of the family Cyprinidae . In all the experiments, the KSD is computed using the Gaussian kernel with the  X  parameter being determined from Algorithm 2.
The data set consists of 989 specimens from Tulane Uni-versity Museum of Natural History (TUMNH). The 989 specimens include 128 Carpiodes carpio , 297 Carpiodes cyprinus , 172 Carpiodes velifer , 42 Hypentelium nigricans , 36 Pantosteus discobolus , 53 Campostoma oligolepis , 39 Cyprinus carpio , 60 Hybopsis storeriana , 76 Notropis pe-tersoni ,and 86 Luxilus zonatus . We assign identifiers 1 to 10 to the above species. The first five species belong to the family Catostomidae (suckers). The next five species belong to the family Cyprinidae (minnows). Both families are under the order Cypriniformes . Sample images of spec-imens from the above 10 known species are shown in Fig-ure 4.

Figure 5. Digitized 15 homologous landmarks using TpsDIG Version 1.4 ( c 2004 by F. James Rohlf).

Over the past decade, digital landmarking techniques have been widely used to analyze body shape variation, in a procedure called Geometric Morphometrics [15, 2, 31]. The landmarks (LM) are biologically definable points along the body outline, which are arguabl y related by evolutionary descent. The LM of each specimen are saved as two di-mensional coordinates. Non-shape related variation in LM coordinates can be removed using techniques such as Gen-eralized Procrustes Analysis [10, 13]. Figure 5 shows 15 homologous LM digitized on a fish specimen using the Tps-DIG software tool developed by F. James Rohlf of SUNY Stony Brook 1 . Various body shape characters can be ex-tracted from these LM and expressed in a fairly simple lan-guage of lengths, angles, areas, and ratios of these. For example,  X  the length of the snout  X  is directly related to the slope of the line connecting the tip of the snout (LM 1 )and the naris (LM 2 ), which can be computed as the angle be-tween the vertical axis an d the line connecting LM LM 2 .The X  slenderness of the body  X  can be defined as the ratio of the body depth (computed as the distance between LM 4 and LM 11 ) to the body length (computed as the dis-tance between LM 13 and LM 7 ).
 Digital images of all specimens are uploaded into the TpsDIG software tool, and 15 homologous LM are digi-tized along the body outline of each specimen (Figure 5). The LM of each specimen are saved as 2 -dimensional coor-dinates. Next, Generalized Procrustes Analysis [13] is used to remove non-shape related variation in LM coordinates. Specifically, the centroid of each configuration (based on the 15 LM associated with each specimen) is translated to the origin, and configurations are scaled to a common unit size. We then compute 12 features, x 1 ,...,x 12 , for each specimen using the 15 LM. The description of each feature is given in Table 1.
In the first experiment, we held specimens from one of the 10 species as  X  X nknown X  specimens and specimens of the other 9 species as known. Specimens from the 9 known species are then randomly divided into two groups of roughly equal size. One group is used to build the KSD function. The other group is used to compute the up-per bound on the false alarm p robability based on (6) for  X  =0 . 05 . The parameter t is chosen such that the upper bound on the FAP is equal to one minus the detection rate evaluated from the  X  X nknown X  specimens. We denote this critical value of the upper bound on the FAP by e  X  . The de-tection rate is therefore 1  X  e  X  . Loosely speaking, e  X 
Table 1. Features describing shape characters. LM denotes the coordinates of the i -th landmark.

Non-shape related variation has been removed from the landmarks.
Table 2. With probability at least 0 . 95 ,theFAP is less than e  X  , and the detection rate is 1  X  e  X  .
A smaller value of e  X  indicates a smaller FAP and a larger detection rate.
 that the FAP of the novelty detector is less than e  X  when its detection rate is 1  X  e  X  . Therefore, a smaller value of e indicates that a larger percentage of the  X  X nknown X  spec-imens are novel with respect to the known species, which in turn suggests the possibility that the unknown specimens represent a new species.

The results are given in Table 2. As you can see, the proposed novelty detector identifies most of the  X  X nknown X  species as novel, i.e.,  X  X ew X  with high detection rates and low FAPs: the detection rate of Cyprinus carpio is 0 . 949 and its FAP is less than 0 . 051 , the detection rate of Hypen-telium nigricans is 0 . 929 and its FAP is less than 0 . 071 , Pantosteus discobolus has a detection rate 0 . 917 and FAP less than 0 . 083 , Carpiodes velifer has a detection rate 0 . 808 and FAP less than 0 . 192 , Carpiodes cyprinus has a detec-tion rate 0 . 798 and FAP less than 0 . 202 , Carpiodes car-pio has a detection rate 0 . 742 and FAP less than 0 . 258 , and Campostoma oligolepis has a detection rate 0 . 698 and FAP le ss th a n 0 . 302 . On the other hand, the method does not produce good detection rate for Hybopsis storeriana , Notropis petersoni ,and Luxilus zonatus . The detection rate for Notropis petersoni is especially low at 0 . 408 .
We interpret the low detectio n rates for some species as a consequence of a  X  X asking X  effect as illustrated in Figure 6. The 20 novel observations, marked with  X   X  X , are i.i.d. obser-vations from a uniform distribution over [  X  1 , 1]  X  [  X  The 400 known observations come from one of the follow-ing Gaussian distributions: N 1  X  N ([2 , 2] T ,I ) (marked with  X   X  X ), N 2  X  N ([  X  2 , 2] T ,I ) (marked with  X  X ), N N ([2 ,  X  2] T ,I ) (marked with  X  X ), N 4  X  N ([  X  2 ,  X  2] (marked with  X  X ). Clearly, the novel observations are sub-merged into (or masked by) the known observations. If we construct the KSD function using 200 known observations and evaluate the upper bound on the FAP from the remain-ing 200 known observations, we obtain e  X  =0 . 85 ,i.e.,a detection rate 0 . 15 when the FAP is less then 0 . 85 (a de-crease in the upper bound on the FAP will further reduce the detection rate). However, if we consider one Gaussian at a time, we get (1) N 1 : detection rate 0 . 9 and FAP less than 0 . 1 ;(2) N 2 : detection rate 0 . 75 and FAP less than 0 . 25 ; (3) N 3 : detection rate 0 . 85 and FAP less than 0 . 15 ;(4) N detection rate 0 . 75 and FAP less than 0 . 25 . This suggests that one may reduce the masking effect via a pairwise test, i.e., testing the specimens from the unknown species against each known species separately.

We summarize the pair-wise test results of the above 10 species in Table 3. Since the sample size of several species is rather small, the upper bound derived using (6) is very loose. Instead of reporting e  X  values, we present equal error rates, the value at which the false alarm rate (the nus the detection rate. The ij -th entry of Table 3 presents the value of equal error rate at testing the unknown species i against the known species j (the numerical identifier of species is given at the beginning of Section 5.1). The three Figure 6. An example of the masking effect.

With high detection rate and low FAP, novel observations (marked with  X   X  X ) can be de-tected from a group of known observations generated by any one of the four Gaussian distributions (marked with  X   X  X ,  X  X ,  X  X , and union of them. species, Hybopsis storeriana , Notropis petersoni ,and Lux-ilus zonatus , which are masked when each is compared against the remaining species, are easily distinguished in the pair-wise tests (the last three rows in Table 3). Among all 90 comparisons, the top two largest equal error rates occur between Notropis petersoni and Luxilus zonatus : the detec-tion rate for Notropis petersoni is 0 . 7791 (FAP is 0 . 2209 ) when it is tested against Luxilus zonatus ; the detection rate for Luxilus zonatus is 0 . 75 (FAP is 0 . 25 )whenitistested against Notropic petersoni . The above results demonstrate high potential for applying the proposed novelty detection algorithm in taxonomic research, specifically, to problems of new species discovery.
We have proposed a new statistical depth function, the kernelized spatial depth (KSD), and a novelty detection method using the KSD function. The KSD is a general-ization of the spatial depth [30, 7, 35]. It defines a depth function in a feature space induced by a positive definite kernel. The KSD of any observation can be evaluated using a given set of samples. The depth value is always within the interval [0 , 1] , and decreases as a data point moves away from the center, the spatial median, of the data cloud. This motivates a simple novelty detection algorithm that identi-fies an observation as novel if its KSD value is smaller than a threshold. We derived an upper bound for the false alarm probability of a novelty detector, which can be applied to determine the threshold. Experimental results demonstrate high potential for applying the proposed novelty detection algorithm in taxonomic research, specifically, to problems of new species discovery.
 We need an inequality attributed to McDiarmid.
 Lemma 1 (McDiarmid) Let X 1 ,X 2 ,...,X n be indepen-dent random variables taking values in a set . Suppose that f : n  X  satisfies for constants c i , 1  X  i  X  n . Then for every &gt; 0 ,
Pr[ f ( X 1 ,...,X n )  X  f ( X 1 ,...,X n )  X  ]  X  e
Proof of Theorem 1: Because y i /  X  X  and g  X  is bounded by 1 , a change of one y i in 1 results in at most a change of 1 the McDiarmid X  X  inequality yields Pr F [ g  X  ( y 1 , X )]  X  Setting  X  =exp  X  2 2 and solving for , we obtain = ln(1 / X  ) / 2 . This completes the proof.

