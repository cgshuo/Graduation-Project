 the changing environment, with the development of a disease or under different treatment is an even greater challenge and critical to improve human life. As an important step, extracting the knowledge from heterogeneous types of gene expressions may provide a better insight into the biological role of gene interactions with disease development and drug effect at the molecular level. Heterogeneous types of gene expressions contain different experimental conditions. The experimental conditions may correspond to different time points under different dosages of a drug, measures from different individuals, different organs or different diseases. DNA microarrays allow the measurement of expression levels for thousands of genes, perhaps all genes of a cell or an organism, within a number of different experimental conditions. The dynamic patterns of genes expressed under different conditions can be useful indicators about gene state-trajectories and may reveal possible states and trajectories of disease and treatment effects. Also, the analysis of the gene state patterns can help identifying important and reliable predictors of diseases, such as cancer, in order to develop therapies and new drugs. Biologists, computer scientists and statisticians have had a decade of research on the use of microarrays to model gene expressions [6], [10], [11], [20], [23]. Most of the studies are interested in the genes that co-express in similar conditions with fewer results on the heterogeneous types of gene expressions. Moreover, most of these studies focus on the mean profiles of the gene expression time course, which can make the cluster or classification of gene expressions largely simplified but ignores the important time updated information. One feature of gene expression data in time series microarray experiment is that it includes a large number of attributes with high correlation and with high level noise. Because of its massive parallelism, potential for fault and noise tolerance, an Artificial Neural Network (ANN) based information processing is capable of taking the task to deal with this feature. ANNs can adapt their structure in response to the change of the gene expressions under different conditions in order to extract knowledge, which contributes to a deep understanding of gene interactions and identifies certain causal relationships among the genes with diseases and drugs [4], [9]. The study of the heterogeneous gene expressions under different experimental conditions in a multivariate nonlinear time series may involve the study of dynamic changing of the statistical variations of non-stationary processes of gene expressions. There are several types of artificial neural networks for temporal processing, which can be used to model the natural characteristics of the gene changing under different conditions and update the information in the training data over time. Recurrent Neural Networks (RNNs) has the ability of dealing with time varying input and output and it can define neurons as states of the network [18]. The output of the hidden layer is fed back to the input layer via time delay. An internal state of the network encodes a representation of some characteristics or a biological mechanism of gene interactions, based on the transition function of the state from a recursive neural network, eventually to control the production of the internal information. State space model can be viewed as a special case of RNN, which combines a stochastic process with observation data model uniformly based on the recursive neural network. Hidden Markov processes can also be used to model the gene activity systems in which the gene states are unobservable, but can be represented by a state transition structure determined by the state parameters and the state transition matrix while processing the patterns over time. Time Lagged Recurrent Neural Networks (TLRNNs) are extensions of conventional RNNs and outperform them in the terms of network size. A TLRNN use short memory structure instead of static topology networks to develop advanced classification systems and use a complex learning algorithm: Back-Propagation Through Time (BPTr) to learn the temporal pattern [24], [27]. This dynamic learning process is well adapted to the heterogeneous time series gene expression domain. TLRNNs can be used in nonlinear time series prediction, system identification and temporal pattern classification. The goal of this paper is to investigate the performance of heterogeneous types of multivariate time series data using time lagged recurrent neural networks with dynamic trajectory learning. The question we are interested in is whether the dynamic heterogeneous gene activity patterns can be well identified or classified through the trajectory learning with a time lagged recurrent neural network. 
The rest of the paper is divided as follows: in Section 2 we describe how the data was acquired and preprocessed. In Section 3 TLRNNs, statistical criteria for searching for the optimal model and related learning algorithms are presented. Experimental results are given in section 4. We survey related work in section 5 and finally we provide some concluding remarks in section 6. 
We used the widely studied set of yeast expression measurements data produced by Eisen et al [5], [7]. The total data contained 2465 genes. Each data point represented the base two logarithm of the ratio of expression levels of a particular gene under two different conditions (CY5 and CY3 with red and green fluorescence intensity). The gene matrix contained 79 experiments over a variety of experimental conditions. The data were generated from spotted arrays using samples collected at various time points during diauxic shift, the mitotic cell division cycle, sporulation, the temperature, reduing shocks, and was taken from the Stanford genome research web site (http://www-genome.stanford.edu). The training labels were extracted from 
Saccharomyces cerevisiae functional catalogue databases. The study divided the data into three cases:  X  Identify two classes of gene functional patterns: 121 ribosomal protein sample genes and 2346 non-ribosomal protein genes. We used two third of the data as training to construct the best model and the rest as testing the network performance.  X  Identify three different classes of functional gene table 1.  X  Identify multiple gene functional patterns from four to ten classes and see the network performance trends. Ten subclasses are also extracted from the original functional catalogue, the last class as negative control and the other nine as learnable classes. The selected data and corresponding classes are given in table 2. 
Table 2. CGe:ne functional dasses with heterogeneous data 
Figure 1 gives scatter plots of data under different experimental conditions. Figure 2 provides time series plots of gene expressions under different experimental conditions. Figure 1. ScAttter plots of three different dames of functional gene expressions (ribosomal, transcription and secretion) -8 Figure 2. Time series plot of three classes functional gene 
As we have seen in the time series plots in figure 2, the data contains high-frequency components and are non-stationary, which may make modeling difficult. To remove these factors, we processed the data by differencing the input series. After difference transformation, the time series were plotted in Figure 3. 
The figure shows that transformation can make the data more stationary. 
O .2 4 .8 
Figure 3. Time series plots after difference transformation for three classes functional gene expressions under heterogeneous 79 inputs may be too large for a recurrent neural network, which is difficult to train (particularly if the data is noisy) and may result in overfitting problems, which do not provide good general solution. In order to select the neural network inputs, a statistical analysis has been carried out to determine the correlation between the inputs (experimental conditions) and the outputs (the class or pattern of genes). We compute the Pearson correlation coefficients of inputs and outputs, then based on the p-values we setup the acceptance threshold: if the p-value of the correlation coefficient is less than 0.0001, then we consider correlation and accept it as input, otherwise we drop it. The selected inputs and computed correlation coefficients are given in table 3. The number of inputs was reduced from 79 conditions to 47. Several input permutation runs were also employed in order to find the combination, which produce the lowest error in the testing set. 
After we filtered out the low correlation inputs, the data were fed into the time lagged recurrent neural network. One advantage of the TLRNNs is that they can use the memory layer confined to the input, which can be used as further input preprocessors to reduce the redundant information. 417 The heterogeneous gene expression contains information in its time structure, i.e. how the gene expression changes with time. Time Lagged Recurrent Neural Networks are useful to identify the gene functional patterns with time information. TLRNNs are extensions of Multi-Layer Perceptrons (MLPs) with short-term memory structures. In our study we choose the network architecture with three layers, the feedback connection from the hidden layer back to the input layer. The input layer use the inputs delayed by L samples before presented in the network. The trainings of the TLRNN are processed using back-propagation through time with trajectory learning and the parameters can be learned by examples [27]. The advantage of the BPTr with trajectory learning is that the learning system is trained to recognize or capture a set of patterns, which may vary with time since the number of inputs can be varied with time. Therefore it perfectly meets the requirements of the knowledge discovery purpose for biological systems and living systems like gene expression microarray data. In this case the static topology neural networks must be extended to dynamic networks, i.e. have short-term memory structures in which they capture the gene information of temporal patterns. There are three memory structures at the input layer we can choose from: one sample delay, Gamma memory, and Laguerre memory. In our study we employed three cases to search for the best structure for the selected data. Based on Akaike and Bayesian information statistical criteria and classification accuracy the Gamma structure worked better than the other two. One way to achieve the best network structure without information loss and accuracy loss is constructing the learning algorithm based on its capability of decreasing the effective number of weights during training to reduce the danger of overfitting. There are two ways to achieve this, one is using back-propagation through structural learning with forgetting in which the small weights are dropped off through regularization parameters or decayed values [14]. The other way is using Back-Propagation Through Time (BP'VI') in which the number of parameters in the network can be confined with the memory structure [27]. The BPTT algorithm uses the derivatives from state updated functions and cost functions, then trains over a trajectory of the input space, enabling it to capture the temporal dynamics of the time series information. Trajectory learning is based on gradient information over time. BPTI" can adapt the depth of the memory using gradient descent, instead of changing the number of inputs. Initial memory depth in our study is setup to 10, which later can be adapted by the network according to the type of the memory structure. The learning rule for each layer applied back-propagation with momentum, where the momentum was setup to 0.7. Tangent sigmoid transfer function works well for the given data. The goal of model selection is to find the best network architecture that can achieve the balance between data fitting and model complexity in order to avoid over-fitting and to optimize generalization performance. In a time lagged recurrent neural network, the nx)del complexity level varies through trajectories with distinct types of common states. There are several dynamic parameters such as the number of hidden neurons, the memory taps (depth in the samples) and the number of trajectories in the search space that need to be optimized in order to achieve optimal models. The memory taps (the depth in samples) parameter can be adapted through BPTT. We designed two-way factorial arrays to search for the best values of the trajectory and the number of hidden neurons. In our study the number of trajectories is ranged from 2 to 20 and the number of hidden nodes from 2 to 20. To avoid a sequence of hypothesis testing for the model selection, we use statistical criteria such as the Akaike Information Criterion (AIC), or the Bayesian Information Criterion (BIC) to determine the optimal vales for optimal network size and structure [1]. All these statistical criteria share the common form of composition, which is the model complexity criterion (equals to log-likelihood) plus a complexity penalty term. The advantage of using statistical criteria like AIC/BIC other than using sequential tests for model selection as proved by Stone [23], is that there is an asymptotic equivalence of the choice by cross-validation and AIC. Therefore we can avoid the computational drawback of cross-validation and do not need to separate the data into cross-validation set when selecting the model. Note that the difference between AIC and BIC is that BIC includes the size of the input examples, and it may be more informative. The model with lowest AIC/BIC is considered to be the preferred one and if the model complexity is changed with the size of the sample, BIC is preferred, otherwise we prefer AIC [16]. The best neural network is the one with the highest classification accuracy and the lowest AIC/BIC. The best model was chosen for the rest of the gene classification and future predictions. We used TLRNN with one hidden layer and Gamma memory function to identify two classes. The learning rule used back-propagation with momentum with step size 1, the learning rate was 0.7, and tangent sigmoid function was used as transfer function. After 1000 epochs of training the MSE dropped below 0.000059. Classification accuracy for testing set provided the mean with standard deviation value 99.427%:t0.366% with 10 independent runs. This result is even better than the reported results by prediction algorithm: CLEAVER, with a classification accuracy of 99.229834 % for the same data [20]. We also employed Nearest Neighbor with Mahalanobis-Distances and Self-Orgardzed Map (SOM) methods for comparison study, which gave correct classification rates of 97.39% and 98.53%, respectively. Table 4 provides computed statistical criteria for model selection: values of AIC/BIC with average of five independent runs based on the number of trajectories and the number of hidden nodes for the case of three classes. Table 5 reports generalization rates for the same runs. As it can be seen, the AIC/BIC values increase nearly linearly with the number of hidden nodes, while their values are essentially independent of the number of trajectories. 
Yet it is arguable if the low AIC/BIC values reported at trajectories 2, 5, and 18 are significant. Regarding table 5 most of the low error rates are reported around the upper left comer, which corresponds to low number of hidden nodes with low number of trajectories, but the overall optimum can be found at 4 hidden nodes with 18 trajectories. Since the number of classes to be recognized is only three, it is not surprising that small number of hidden nodes and number of trajectories can provide good performance, Table 5 also shows that the learning capability of the model varies with the complexity of the gene patterns, which mainly depends on the number of patterns in the data and on the complexity of the trajectories. Our preliminary results show that by increasing the number of patterns (classes) to be recognized, the number of trajectories and the number of hidden nodes needs to be increased in order to get optimal performance. 'IMI 2 4 8 12 15 20 2 1861/2143 3300 X 4289 7661/8698 11398/12970 14273/16234 19021/21629 5 1846/2127 3828/4367 7638/8694 11404/12978 14316 X 16263 19092/21699 8 195312234 3788 X 4327 763618692 11444113018 14293/16254 19091/21700 10 1904/2184 3861/4400 7631/8687 11410112985 14154/16345 19046121653 12 1773/2250 3722/4582 7688/8744 11460 X 13033 14195/17325 19128/21735 15 1945/2225 3815/4354 7679 X 8735 11505 X 13078 14288/16250 19111/21719 18 1907/2187 3807/4346 7624 X 8680 11420/12992 14282/16243 19059/21666 20 1936/2217 3834 X 4373 7665/8721 11445/13018 14378/16339 19139 X 21747 'IMI 2 4 8 12 15 20 2 4.42 3.96 5.72 4.70 5.30 5.01 5 3.74 4.97 6.46 5.61 5.09 7.97 8 5.73 4.36 5.29 7.99 6.16 7.72 10 4.53 7.43 5.61 4.63 5.90 8.66 12 5.43 6.53 9.15 9.31 7.36 11.60 15 5.96 4.50 9.71 10.41 5.51 9.41 18 4.39 3.48 5.75 6.23 5.03 5.90 20 6.03 5.26 8.39 6.36 10.58 12.66 
Table 6 provides comparison results with some other popular machine learning approaches for gene expression classification: Nearest Neighbor with Mahalanobis Distances (NNMD), Self-
Organized Map and Support Vector Machine (SVM). The values given in the table provided us the confidence with the mean and standard deviation of the correct classification rate for five independent runs. SVM in our case did not provide higher performance than TLRNN as opposed to most gene expression studies. The reason may come from the heterogeneous expression data, TLRNN particularly performs well for this kind of time series data. We also computed the results with another recurrent neural network: the Jordan/Elman Recurrent Neural Network (JERNN). According to the results for the heterogeneous time series gene expression patterns with three classes the TLRNN works best. Table 6. Correct classification rates of different methods for 
The data distribution for more broad gene functional classes is given in table 2. The correct classification rates with TLRNN are given in table 7, which are based on the optimal structure given by the AIC/BIC. As it can be seen in the table the correct classification rate decreases with the number of classes, which is not surprising. Also, as we discussed earlier both the number of hidden nodes and the number of trajectories increases as the number of classes increases for achieving better performance. Table 7, The correct classification rates of TLRNN with BPTT with dynamic trajectory learning corresponding to the 
A large number of approaches have been proposed, implemented and tested by computer scientists and statisticians in order to discover or identify the gene functional patterns with microarray experiments. For example, a genetic network approach was discussed and developed by Thieffry and Thomas [23] and D'haeseleer, Liang, and Somogyi [6]. Time series was studied by 
Socci and Mitra [20]. Self-organized hierarchical neural network was done by Herrero, Valencia, and Dopazo [10]. Unsupervised neural network and associated memory neural network was done by Azuaje [2] and Bicciato, Pandin, Didone, and Bello [3]. We reported a Bayesian neural network approach earlier [17]. 
Previous study showed that traditional statistical models can provide some insight into gene expressions and has precise results, but the weaknesses of statistical models are that they can not capture the dynamic changing of gene expressions from time to time well and are sensitive to noise and assumptions. Neural networks are more efficient and flexible for studying gene expressions. We, as an addition to our efforts reported in this paper currently explore other kinds of neural network models for discovering correlation in gene patterns, and refine the 
Jordan/Elman neural network approach to study the heterogeneous time series gene expression patterns. 
In this paper we proposed and explored the use of TLRNNs with dynamic trajectory learning for investigating the gene functional patterns with heterogeneous microarray experiments. Results show that the TLRNN works better than the nearest neighbor with 
Mahalanobis Distances, SVM and SOM. For SVM, this is a little surprise since most well known results using SVM provided the highest performance, and it has properties of dealing with high level noise and large number of attributes, which both exist in the gene expression data. The possible reasons may be found in the heterogeneous time series gene expression data, since it involved different conditions with time information. Another reason is that 
TLRNN can iteratively construct the network, train the weights and update the time information. Results show that the best generalization capability largely depends on the complexity of the patterns in which TLRNN can be monitored by the complexity of the trajectory with distinct types of states. With the increase in the number of gene functional patterns, the generalization performance decreased. However, with changing the number of trajectories and the number of hidden nodes, the performance of the model can be improved based on the statistical criteria for model selection, in which two or three way factorial design can be employed in order to search for the best network architecture for prediction and medical diagnosis. [1] Akaike, H. A new look at the statistical model identification. 
IEEE Transactions on Automatic Control 19:716-723, 1974. [2] Azuaje, F. Unsupervised neural network for discovery of gene expression patterns in B-cell lymphoma", Online Journal of 
Bioinformatics, Vol 1: 26-41, 2001. [3] Bicciato, S., Pandin, M., Didone, G., and Bello, C. Analysis of an Associative Memory neural Network for Pattern 
Identification in Gene expression data. BIOKDD01, Workshop on Data mining in Bioinformatics, 2001. [4] Bishop, C. M. Neural Networks for Pattern Recognition. 
Oxford University Press, 1995. [5] Chu, S., DeRisi, J., Eisen, M., Mulholland, J., Botstein D., and Brown, P. O. The Transcriptional Program of Sporulation in 
Budding Yeast. Science 282, 699-705, 1998. [6]D'haeseleer, P., Liang S., and Somogyi, R. Gene expression analysis and genetic networks modeling. Pacific Symposium on Biocomputing, 1999. [7] Eisen, M. B., Spellman, P. T., Brown, P. O., and Botstein, D. 
Cluster analysis and display of genome-wide expression patterns. PNAS, Vol. 95, Issue 25, 14863-14868, Dec. 1998. [8] Gasch, A. P., Spellman, P. T., Kao, C. M., Carmel-Harel, O., Eisen, M. B., Storz, G., Botstein, D., Brown, P. O. Genornic Expression Programs in the Response of Yeast Cells to 
Environmental Changes..Mol. Biol. Cell 11: 4241-4257, 2000. [9] Haykin, S. Neural Networks. Prentice Hall Upper Saddle 
River, New Jersey, 1999. [10] Herrero, J, Valencia A., and Dopazo, J. A hierarchical unsupervised growing neural network for clustering gene expression patterns. Bioinfomaatics, 17:126-136, 2001. [11] Holter, N., Maritan, A., Cieplak, M., Fedoroff, N. Banavar, J. Dynamic modeling of gene expression data. Proc Natl. Acad. 
Sci. USA 98: 1693-1698, 2001. [12] Jansen R., and Gerstein, M., Analysis of the yeast transcfiptome with structural and functional categories: characterizing highly expressed proteins. Nucleic Acids 
Research, Vol. 28, No. 6 1481-1488, 2000. [13] Matlab User Manual, Release 6.0, Natick, MA: MathWorks, 
Inc., 2001. [14] Miller D. A., and Zurada, J. M. A dynamical system perspective of structural learning with forgetting, IEEE 
Transaction.,; on Neural Networks, vol. 9, no. 3, pp. 508-515, 1998. [15] Li, W., Haghighi, F., Falk, C. T. Design of artificial neural networks and its applications to the analysis of alcoholism data. Genet Epidemiol 17(Suppl):S223-s22g, 1999. [16] Li, W., Sherriff, A., and Liu, X. Assessing risk factors of human complex diseases by Akaike and Bayesian information criteria (abstract). Am J Hum Cenet 67(Suppl): $222, 2000. [17] Liang, Y., George, E. O., Kelemen, A. Bayesian Neural Network for Microarray Data. Proceeding of the IEEE 
International Joint Conference on Neural Network, Hawaii, 2002. [18] Pearlmutter,B.A. Gradient calculation for dynamic recurrent neural networks, 6(5):1212-1228,1995. [19] Qian, J., Stenger, B., Wilson, C. A., Lin, J., Jansen, R., Teichmann, S. A., Park, J., Krebs, W. G., Yu, H., Alexandrov, 
V., Echols, N., and Gerstein, M. PartsList: a web-based system for dynamically ranking protein folds based on disparate attributes, including whole-genome expression and interaction information. Nucleic Acids Research, VOL.29, No. 8 1750-1764, 2001. [20] Raychaudhuri, S., Sutphin, P. D., Stuart, J. M., and Altman, 
R. B. CLEAVER: A publicly available web site for supervised analysis of microarray data. [21] Socci N. D., and Mitra, P. Time series analysis of yeast S. cerevisiae cell cycle expression data. In E. Bronberg-Bauer, A. 
De Beucklaer, U. Kummer and U. Rost, editors, Proceedings of Workshop on computation of Biochemical Pathways and 
Genetic Networks, Heidelberg, 1999. [22] Spellman,. P. T., Sherlock, G., Zhang, M. W., Iyer, V. R., 
Anders, K., Eisen, M. B., Brown P. O., and Futcher, B. 
Comprehensive identification of cell cycle regulated genes of the yeast Saccharomyces cerevisiae by microarray hybridization. Mol. Biol. Cell 9: 3273-3297, 1998. [23] Stone, M. Cross-validatory choice and assessment of statistical predictions (with discussion). Journal of the Royal 
Statistical Society, Series B, 36, pp. 111-147, 1974. [24] Stometta,W. S., Hogg,T. Huberman,B.A. dynamic approach to temporal pattern processing, In Neural information processing systems, D. Z. Anderson (Ed.), 750-759, 1988. [25] Thieffry, D., and Thomas, R. Qualitative analysis of gene networks. Pacific Syrup. Biocomp, 3:77-88, 1998. [26] Yang, Y., Dudoit, S., Luu, P., Speed, T. Normalization for cDNA microarray data. Technical Report 589, Department of 
Statistics, UC-Berkeley, 2000. [27] Werbos, P.J. Backpropagation Through Time: What it does and how to do it, Proc. of the ICNN,SanFrancisco,CA,1993. 
