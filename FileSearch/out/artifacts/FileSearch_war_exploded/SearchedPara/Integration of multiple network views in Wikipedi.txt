 Guangyu Wu  X  P X draig Cunningham Abstract One of the challenges in network data analysis is the determination of the most informative perspective on the network to use in analysis. This is particularly an issue when the network is dynamic and is defined by events that occur over time. We present an example of such a scenario in the analysis of edit networks in Wikipedia X  X he networks of editors interacting on Wikipedia pages. We propose the prediction of article quality as a task that allows us to quantify the informativeness of alternative network views. We present three fundamentally different views on the data that attempt to capture structural and temporal aspects of the edit networks. We demonstrate that each view captures information that is unique to that view and propose a strategy for integrating the different sources of information. Keywords Social network analysis  X  Data quality  X  Classification  X  Wikipedia 1 Introduction Insight into objects embedded in a network of interactions can be derived from an analysis of the structure of that network. This fundamental idea has been demonstrated not only in How best to represent the network around an object is still a significant research challenge. Recently, profiling using network motif counts has emerged as a promising solution for characterizing networks [ 3 , 23 , 26 , 27 ].

In this paper, we analyse edit networks in Wikipedia and consider three alternative views on the data that are significantly different. We evaluate the effectiveness of motif-based characterizations of networks in these three views in terms of the effectiveness for predicting article quality. We also evaluate strategies for integrating these three representations in order to improve predictive accuracy.

The three views capturing temporal, structural and edit trajectory information are as fol-lows:  X  The temporal network view considers article revision history as a sequence of editor X  X   X  The ego network is a static bipartite network where major edits are represented as edges  X  The trajectory network represents article revisions through a sequence of editors (see
Using each of these three views, an article can be characterized by vectors of motif counts (vectors of length 125, 22 and 13, respectively). The objective of our study is to assess the effectiveness of these feature-based representations for predicting article quality using supervised machine-learning methods.

In the next section, we briefly review some relevant research on social network analysis and authoritativeness. We describe the three network views (temporal network, structural network and trajectory network) in Sects. 3 , 4 and 5 , respectively, and follow by showing their performance on three Wikipedia article collections in Sect. 6 . The two integration strategies are introduced in Sect. 7 , and we demonstrate the improvement resulting from integrating the three network views in Sect. 8 . The final Sect. 9 concludes with a summary of the integration work. 2 Related research The ultimate objective of the network analysis we propose here is to assess the quality and authoritativeness of user generated content (UGC) in the social Web. UGC has been enormously disruptive as resources such as Wikipedia, TripAdvisor and MenuPages replace traditional businesses that provided similar services. At the same time, this raises questions regarding the quality of UGC [ 4 ]. Is an answer on Yahoo! Answers correct? Are details presented in blogs or on Wikipedia credible? Can we trust reviews on TripAdvisor or on Amazon? These questions have received a good deal of attention in recent research. We focus on work related to Wikipedia here.
 In contrast to traditional encyclopedias, where authority derives from expert contributors, Wikipedia depends on collaboration and consensus to produce quality articles. There has been some controversial research that suggests that the quality of Wikipedia articles approaches that of established encyclopedias [ 9 ]. The famous quote from Surowiecki X  X  The Wisdom of Crowds is that  X  X nder the right circumstances, groups are remarkably intelligent X  [ 25 ]. The challenge when using Wikipedia is to determine whether the circumstances that produced the article in question were right or not.

The first requirement for a Wikipedia article to be authoritative is the many eyes idea [ 19 ] X  X  significant group of contributors must have cooperated to produce the article. It is also important that this collaboration has been constructive, and it is better if the editors have a reasonable reputation as contributors. Adler and De Alfaro [ 1 ] have pursued a content-driven strategy to assess editor reputation. They have used text survival and edit distance to quantify editor reputation. In later work [ 2 ], they show that edit longevity is a good measure of editor contribution. Korfiatis et al. [ 17 ] pursue a network-based strategy to evaluate authoritative sources in Wikipedia. They construct a two-mode network of articles and contributors. The article nodes are linked by hyperlinks, and contributors are linked if they have worked on the same article. Contributors are also linked to articles on which they worked. The study proposes article and contributor degree centrality as indicators of authoritativeness. This is similar in spirit to the strategy in our work as degree centrality is captured by a subset of the network motifs we consider.

Brandes et al. [ 7 ] have also analysed the collaboration structure in Wikipedia. Their work has focused on the edit interactions on individual articles. Edges between individual contributors represent delete , undelete and restore interactions. The main contribution of this work is to present the notion of bipolarity that captures the level of conflict between the contributors to an article. Thus, the work is more directed at the problem of Wikipedia vandalism than the issue of authoritativeness that is the subject of this paper. Laniado et al. [ 18 ] presented an algorithm that assigns scores to all contributors of a Wikipedia article according to their contribution and selects the top contributors to build a collaboration network of authors where edges represent the co-authorship between authors. Thus, the inexperienced authors are filtered out, and the co-authorship networks becomes more informative. With the exception of eigenvector centrality (where edge weights were considered), the features they extracted were taken from unweighted versions of the networks.
Recently, Jurgens et al. [ 13 ] proposed temporal motifs extracted from temporal bipartite networks using Wikipedia article revision histories. In the work, they defined editor interac-tions as minor add , major add , minor edit , major edit , minor delete , major delete and revert . A large set of temporal motifs was built of these interactions. They employed these motifs to classify pages as combative or cooperative, and in a further study, they analysed Wikipedia X  X  content growth with these temporal motifs.

Related to this, temporal network representation is the idea of a trajectory network [ 12 , 15 ]. Iba et al. [ 12 ] introduce the idea of a trajectory network for a Wikipedia article as the network of editors the article has passed though during its development (see Sect. 5 for more detail). They show that this representation helps highlight different editor roles including coolfarmers , the outstanding editors who coordinate the editing process and egoboosters who are not so cool. Keegan et al. [ 15 ] go on to show how this representation is effective for revealing characteristic patterns in breaking news collaborations in Wikipedia.

In earlier work, we have used a static bipartite network representation of edit activity around a Wikipedia article as an indicator of the quality of the article [ 26 , 27 ]. Articles are classified using a set of network motifs and motif count ratios. We have identified a subset of motifs that are predictive of Wikipedia quality as determined by the official Wikipedia quality scale that ranges from Stub and Start articles at the bottom end to Featured articles at the top.

There also exists non-network-based studies, for example, Lipka and Stein [ 20 ]used machine-learning techniques to identify featured articles using character trigram and part-of-speech trigram vectors. These features that are known to be characteristic of writing style out-performed alternatives in both a single domain and a domain-transfer situation with F -measure scores of 0.88 across domains and good performance on articles of varying length.
Dalip et al. [ 8 ] presented a comprehensive assessment of quality indicators in collab-orative content curation with a focus on Wikipedia. In their analysis, they considered 69 indicators including text features, review features and basic network features. They used a machine-learning approach to discover the most effective indicators and combination of indi-cators. They found that the easy-to-extract text-based features were most informative X  X ore informative than more complex features based on link analysis.
This work by Dalip et al. [ 8 ] and Lipka and Stein [ 20 ] is complementary to ours in that our network-based features can be combined with their content-based features to further improve classification accuracy. 3 Temporal network view In the network perspective proposed by Jurgens and Lu [ 13 ], a Wikipedia article is viewed as a sequence of revisions ordered by the edit times. These revisions can be tagged by edit types, such as add or revert . The source for this annotation can be seen in Fig. 1 . This shows a portion of the edit history of the Wikipedia page on  X  X frican nationalism X . It shows the date and time of the eight most recent edits, the resulting size of the page in bytes and the number of bytes added or deleted. According to the number of bytes changed in the edit and the comment provided by the editor, revisions are divided into five types: major add , minor add , major delete , minor delete and revert .An add -revision means the total number of characters in an article is increased after the revision is made. Conversely, if the size of the article is reduced, the revision is considered a delete -revision. A revision also can be seen as major or minor according to the absolute value of bytes changed. A major edit happens when a section of text has been inserted or removed, and a minor edit might be word spelling correction, adding or deleting few words. Jurgens and Lu [ 13 ] have shown that the correlation between edit size and page size is poor. Because of this, they use a fixed threshold to separate minor and major edits rather than having a threshold that varies with page size. In our experiments, we set 100 bytes as the minor/major threshold.

Some revisions are committed in order to revert an article to a previous stage, and in this case, editors usually write comments to declare that the revision is a  X  X evert X . We mark an revision as a revert if the comment contains strings such as  X  X evert X ,  X  X v X  or  X  X vv X . Clearly, reverts are significant events as they indicate that all is not going smoothly in the article development. In the extreme case, they can indicate an editing  X  X ar X .

Jurgens and Lu [ 13 ] consider a further edit type which they simply call edit . This covers situations where an edit session results in the insertion and deletion of a number of tokens (words). We do not include this category because these edit events cannot be identified without fetching the original papers to compare tokens. The five types we consider can be identified simply by analysing the edit log reflected in Fig. 1 . Jurgens and Lu X  X  edit events will typically be modelled as minor add or minor delete depending on the net impact on the page size. 3.1 Temporal network motifs In this scenario, an article is represented as a long sequence of edit symbols as shown in Fig. 2 where there are five symbols in the vocabulary. A sequence of three major add symbols might represent healthy progress, whereas a sequence of three revert events represents conflict between editors.

In contrast with the other network views, the motifs that best characterize this temporal data are sequence rather than network motifs. We consider motifs of length three with each motif covering a sequence of three revisions so a set of 125 temporal patterns is generated from the five edit types. In contrast with the work of Jurgens et al. [ 13 ], we did not capture whether the sequence revisions were edited by the same author as this results in more motifs. The temporal motifs are counted through the full revision history of an article with the total count of motif instances being equal to the total number of revisions minus two. The article is then represented as a vector of 125 motif counts representing the temporal motif distribution. 4 Ego networks Ego networks are static bipartite networks representing major edit events between authors and the ego node (the article) and also edits to other articles by those authors as shown in Fig. 3 [ 26 , 27 ]. To build these edit networks, we applied several rules to filter the raw data. Firstly, as some articles have long edit histories, we only consider the latest 2,000 revisions (most articles have less than 2,000 revisions). The revisions are sorted by size, and only the top 50% of revisions are considered further. The editors who made these major revisions are selected as  X  X ajor editors X  to include in the ego network. We also considered all articles that are connected by hyperlinks from the originating or ego article and involved the editors who made the major contributions on these articles. We retained the linked articles that have been edited by at least one of the ego article X  X  major editors. Editors often repeatedly save their changes during a short session, so we judged continuous revisions by the same editor as a single revision.
In Wikipedia, there are two type of editors, registered and unregistered users, where unregistered users are automatically labelled by the IP address they used when editing. We include both the registered and unregistered users to create the ego network.

Bots are allowed by Wikipedia to do some automatic editing and conventionally use names starting or ending with  X  X ot X . Bots perform a huge amount of small editing tasks so the bots are often very high-degree nodes in the networks. For this reason, we dropped the bots from the networks as their high-degree nodes distort our network motif counting results. Furthermore, we do not expect that the amount of attention from bots should impact on the quality of the article.

As with the temporal network representation, revisions can be add or delete actions. We represent an add action using an arrow from an editor to an article, and an edge from an article to an editor means a delete revision X  X ee Fig. 3 .

The ego network formulation only allows single edges between article and editor nodes so the direction of the edge is determined to be an add or delete based on the aggregate contribution of the editor to the article. So, a directed edit describes the overall behaviour of an editor on an article. 4.1 Ego network motifs In earlier work [ 27 ], we found that four basic undirected motifs, shown at the top in Fig. 4 , capture most of the classification power of five-node motifs. So here, we extend these motifs to consider all variants with directed edges. Each undirected motif can produce several directed motifs representing the add and delete actions between articles and editors X  X n all there are 19 directed motifs extended from the four undirected motifs as shown in Fig. 4 .
In addition to these 19 motifs, we also added three multi-edge motifs, which are shown in the right-hand column in Fig. 4 . The objective with these additional motifs is to capture sequential behaviour of editors, i.e. pick up on sequences of major additions or add/delete cycles. 5 Trajectory networks temporal network representation presented in Sect. 3 . From the first revision to create a new network, the nodes represent editors who made the revisions and a directed edge linking from editor A to editor B means that B  X  X  revision follows A  X  X  revision. Figure 5 demonstrates the construction of a trajectory network step by step. Editor A creates a new article following by editor B ,sowelinkthemfrom A to B . Editors may make contributions with a number of revisions, and this is reflected in the in-degree count for that editor node. Self-looping edges are ignored so multiple consecutive edits by the same editor are not captured in this representation.

Trajectory networks do not always look like chains but can get quite dense when editors work on articles through a number of revisions at different times. Figure 6 shows trajectory networks for articles at two different quality levels; the network on the top represents a Featured article, while the one on the bottom is a Start class article. 5.1 Trajectory network motifs It is clear from a cursory analysis of the networks in Fig. 6 that they differ in terms of some key statistics such as network diameter, clustering coefficient and betweenness centrality. While statistics such as these are the measures used by Keegan et al. [ 15 ], we continue with a motif-based strategy and use the triad network motifs introduced by Holland and Leinhardt [ 11 ](seeFig. 7 ). The 13 motifs cover all cases of three nodes connected with uni-or bi-directed edges. We did not extend to motifs of four or more nodes to keep the number of motifs reasonable as four nodes can produce 199 connected motifs.
 6 Evaluating relative performance Each of the three views provides a characterization of a Wikipedia article as a vector of motif counts. The objective now is to assess the effectiveness of these motif profiles in predicting article quality. Each motif profile is a feature-vector representation that can be tested in a supervised learning framework to assess how predictive the different motif views are of the quality class. In all the tests that follow the classification is done using either random forest or logistic regression X  X n earlier work, we found these to be the most effective classifiers with the ego-network representation [ 27 ].

The test is to score Wikipedia article quality as defined by the Wikipedia quality scale 1 which runs from high to low as Featured , A , Good , B , C , Start and Stub (see Table 1 ). It is worth noting that the vast majority of Wikipedia articles are Start or Stub class. While we take Start articles as the core of our  X  X ow X  quality article sets, Start class articles account for a lot of Wikipedia use.
 We construct two classification tasks, an easy task that compares Featured articles with Start class articles and a harder task that compares Featured and Good articles against C and Start class articles. (The A class was skipped because there are few articles in that class.) We gathered Wikipedia articles from three large projects X  History , USA and Meteorology in order to have enough Featured articles. In addition, we merged the articles from these three collections to build cross-domain datasets for both the easy and hard cases. The numbers of articles for each class and collection are listed below in Table 2 . 6.1 Classifications using individual networks Tables 3 and 4 show the classification accuracies and ROC area on each of the eight clas-sification tasks. The classification is performed using a random forest with 100 trees, and accuracy is measured using tenfold cross-validation.

There are some clear patterns in the results that can be summarized as follows:  X  The easy case is indeed easier than the hard case with classification accuracies higher by  X  Temporal and trajectory methods performed similarly with high accuracies, and both of  X  The meteorology data seem to be harder to classify than the other datasets. One expla- X  There is a clear correlation between simple accuracy and ROC area so accuracy only is 6.2 Comparison against content features Given that methods from text/document classification can be considered for this quality assessment task, it is worth comparing the performance of our network-based features with alternative content-based features. Dalip et al. [ 8 ] present a comprehensive evaluation where they consider three different types of features  X  Text features: length, structure, style and readability.  X  Review features: derived from editor and reviewer activity on the article.  X  Network features: centrality, assortativity and clustering coefficient.
 It is fair to say that these review features set out to capture similar information to the temporal network features in our study; however, the temporal network characterization should be more comprehensive. Similarly, our ego-network features should cover the collection of network features in their study.

They present results on a regression study where the quality classes map to numeric values as follows: {Featured Article, A Class, Good, B Class, Start, Stub} map to {5, 4, 3, 2, 1, 0}. They do not consider C class articles as that category did not exist when they gathered their data. On this regression task, they report mean squared error (MSE) scores between 0.92 and 1.19 for different combinations of their text features.
 With our data, we construct a similar regression task with the following mapping {Featured Article, Good, C Class, Start}  X  { 4, 3, 2, 1} on all the articles listed in Table 2 . A tenfold cross-validation on this regression task returns an average MSE is 0.96. This is in the same range as the performance of the text features in the study by Dalip et al. 7 Integration strategies One thing that emerges from the descriptions of the different views in Sects. 3 , 4 and 5 is that there are aspects of behaviour captured by some views that are not captured by others. Thus, if we integrate the three views, it should result in improved performance on our Wikipedia quality classification task.

This fusion of different perspectives has received a fair deal of attention in the research literature, and there are two clear alternatives [ 10 , 22 ]. The integration can be at the feature level which is called early integration or at the decision level which is termed late integration [ 10 ]. Early integration simply entails concatenating the feature vectors from each view into a composite vector and using that as input to a classifier. Late integration entails training classifiers for the individual views and then integrating the output of these classifiers, typically in an ensemble framework.

In order to assess the effectiveness of these integration strategies, we conduct an evaluation using just two views, the temporal view and the ego-network view. Early integration results in a vector of 147 features produced by combining the 125 temporal motifs with the 22 ego-network motifs. This representation is then tested with logistic regression and random forest classifiers.

Late integration is achieved using an ensemble of 20 members, 10 trained using temporal motifs and 10 trained using ego-network motifs. In order to ensure diversity in the ensemble, the classifiers are trained using feature subsets selected at random so that each subset contains two-thirds of the features. The decisions of the individual classifiers are integrated through majority voting with the majority class ( Start or CS ) selected in the event of a tie. 7.1 Performance of early and late integration The experiments for both strategies were performed using random forest (100 trees) and logis-tic regression classifiers on the combined datasets ( All in Table 2 ). Results for classification accuracy measured using tenfold cross-validation are shown in Fig. 8 .

The results are consistent across both datasets. For each classifier type, early integration is better than late integration. Early integration using random forest is the best, and late integration using logistic regression performs worst. So in the experiments that follow, we continue with early integration using random forest. 8 Integration of three views In this section, we present Venn diagrams to demonstrate the improvements in accuracy from integrating two and three network views. The Venn diagrams in Fig. 9 show the performance on the integrated datasets.  X  X ppendix X  shows the results for the remaining datasets.
The Venn diagrams are straightforward to interpret. The figures for classification using the individual views only correspond to those in Table 3 . Performance using all three views is 91% in the easy case and 78.7% in the hard case. The improvement in classification accuracy in the hard case is impressive representing an increase of 5.5% over the best single view.
An examination of all eight Venn diagrams shows that all three views make a contri-bution with no view being redundant. This is evident because in all cases, there is no combination of two views that is as effective as all three. In fact, the unique contribution of an individual view can be quantified in this way as illustrated in the radar charts in Fig. 10 .

For instance, in the easy case (the left Venn diagram) in Fig. 9 , the classification accuracy usingtheintegrationofegoandtemporalnetworkviewsis89.2%andthefull-viewintegration achieved 91%, so the unique contribution of the trajectory network view is 1.8%. This point is plotted on the radar chart in Fig. 10 . These radar charts offer a number of insights into the integration process. For instance, the improvements due to integration are greatest on the harder classification task. Also, the ego network makes its biggest contribution on the meteorology data. 8.1 Feature importance The radial plots in Fig. 10 show that all network views contribute to classification perfor-mance. We can analyse this further by scoring the predictiveness of individual features using information gain [ 28 , 29 ]. Figure 11 shows histograms of the 40 top scoring features on the integrated classification tasks.

The most obvious point to note is that the features from the ego networks are not con-tributing much on the easy task but are more prominent for the hard task. On the easy task the highest scoring features are triads 11, 7 and 6 from Fig. 7 . These are all open triads with bi-directional edges.

The same three motifs take second, third and fourth place on the hard classification task with a temporal network motif taking first place. This winning motif is the sequence of three major add edits in a row. The next two motifs in the ranking are Ego motifs, the first two multi-edge motifs shown at the bottom of Fig. 4 . 9 Summary and conclusions Determining appropriate network representations for dynamic collaborations such as the development of Wikipedia pages is a significant challenge. In this paper, we have described three alternative network representations for collaboration among Wikipedia editors and proposed a quality classification task in order to assess the effectiveness of the representations. It is clear from the evaluation that the different network views capture different aspects of the interaction so we have evaluated alternative strategies for integrating the views in classification. The evaluation shows that early integration using random forest was the best solution for the classification task.

In future work, we will seek to develop an integrated representation that captures repre-sentation power of the three strategies. We will perform a feature selection analysis across the integrated representation to identify which motifs (associated behaviours) are providing thepredictivepower.
 Appendix: Venn diagrams See Fig. 12 . References
