 Learning to rank from relevance judgment is an active re-search area. Itemwise score regression, pairwise preference satisfaction, and listwise structured learning are the major techniques in use. Listwise structured learning has been applied recently to optimize important non-decomposable ranking criteria like AUC (area under ROC curve) and MAP (mean average precision). We propose new, almost-linear-time algorithms to optimize for two other criteria widely used to evaluate search systems: MRR (mean reciprocal rank) and NDCG (normalized discounted cumulative gain) in the max-margin structured learning framework. We also demonstrate that, for different ranking criteria, one may need to use different feature maps. Search applications should not be optimized in favor of a single criterion, because they need to cater to a variety of queries. E.g., MRR is best for navigational queries, while NDCG is best for informational queries. A key contribution of this paper is to fold multiple ranking loss functions into a multi-criteria max-margin op-timization. The result is a single, robust ranking model that is close to the best accuracy of learners trained on individ-ual criteria. In fact, experiments over the popular LETOR and TREC data sets show that, contrary to conventional wisdom, a test criterion is often not best served by training with the same individual criterion.
 Categories and Subject Descriptors: I.2.6 [Computing Methodologies]: Artifical Intelligence  X  Learning ; H.3.3 [In-formation Systems]: Information Storage and Retrieval  X  Information Search and Retrieval .
 General Terms: Algorithms, Experimentation.
 Keywords: Max-margin structured learning to rank, Non-decomposable loss functions.
Learning to rank is an active research area where super-vised learning is increasingly used [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]. The main challenge in adapting supervised learning is that for ranking problems, the evaluation criterion or  X  X oss func-tion X  is usually defined over the permutation induced by the scoring function over response instances (see Section 2.2), and hence the loss function is not decomposable over in-stances as in regular Support Vector Machines (SVMs) [11]. Moreover, common ways to formulate learning with these loss functions result in intrinsically non-convex and  X  X ough X  optimization problems. This is a central challenge in learn-ing to rank. Algorithms for learning to rank may view instances Itemwise [10] and regress them to scores, then sort the Pairwise [2, 3], which is highly suited for clickthrough data, Copyright 2008 ACM 978-1-60558-193-4/08/08 ... $ 5.00. Listwise [4, 9], and use structured learning [12] algorithms, The large-margin structured learning framework [12] fits a model to minimize the loss of a whole permutation, not in-dividual or pairs of items. This approach has been used to optimize non-decomposable ranking criteria, like the area under the ROC curve (AUC) [4] and mean average pre-cision (MAP) [9]. However, other widely-used criteria in Information Retrieval and Web search, such as mean recip-rocal rank (MRR) [13] or normalized discounted cumulative gain (NDCG) [14], had no efficient direct optimizers.
The advantage of the structured learning approach is that it helps break down the difficult non-convex ranking loss optimization problem into a convex quadratic program and a combinatorial optimization problem which, as we shall see, is often tractable and simple for ranking applications.
The second framework approximates the non-convex and discontinuous ranking loss function with a somewhat more benign (but often still non-convex) surrogate, which is then optimized using gradient descent and neural networks [3, 7, 8, 15]. A very interesting variant is LambdaRank [6] which models only the gradient, with an unmaterialized objective. A potential problem with this family is that non-convex opti-mization behavior is tricky to replicate accurately in general, requiring many bells and whistles to tide over local optima. In fact, a recent approach using boosted ordinal regression trees ( McRank ) [10] has proved surprisingly competitive to gradient methods and put itemwise approaches back in the race. In this paper we will not focus on this family, except to compare the best structured learning approaches with McRank , to show that listwise structured learning remains very competitive.
Our first contribution is to augment the class of non-smooth ranking loss functions that can be directly and effi-ciently (in near-linear time) optimized for listwise structured learning. Specifically, we propose new, almost linear-time al-gorithms, SVMndcg for NDCG (Section 3.4) and SVMmrr for MRR (Section 3.6). Therefore, now we can optimize ef-ficiently for AUC, MAP, NDCG and MRR within the struc-tured ranking framework.

Structured ranking requires us to design a feature map  X  ( x,y ) over documents x and a proposed ranking y . Our second contribution is a close look (Section 3.1) at feature map design and feature scaling: all-important but some-what neglected aspects of structured ranking. Specifically, our feature maps for MRR and NDCG are different, and this affects accuracy (Section 4.3). It also greatly affects the numerical behavior of the optimizer (Section 4.2). We give some guidelines on how to check if a feature map, in con-junction with a loss function, is healthy for the optimizer.
We perform a thorough comparison, using standard public data sets (see Figures 1, 9), of test accuracies in terms of MAP, MRR, and NDCG when trained with structured rank learners that are optimizing for each of these loss functions separately. Conventional wisdom suggests that a system trained to optimize MAP should be best for test MAP, a system optimized for MRR should be best for test MRR, etc. Surprisingly, across five data sets, we see very little evidence of this. Often, the best test loss of a certain type is obtained by training with a different loss function. We conjecture that this is because conventional feature maps for ranking are not well-matched to commonly-used ranking loss functions (Section 4.2).

Web search users have diverse needs. Even if we could, it would be undesirable to ultra-optimize in favor of one criterion. MRR is best for navigational queries ( X  X BM X ) and factual questions ( X  X iraffe height X ), where only the first URL ( http://www.ibm.com ) or answer (18 feet) matter. In contrast, NDCG is best for collecting information about a topic.

Our third contribution is a robust max-margin optimiza-tion ( SVMcombo ) of a combined loss function (Section 3.7). We show that SVMcombo  X  X  test performance on any crite-rion (MAP, MRR, NDCG) is close to that of the best com-ponents of the combination (Section 4.5).

We also report on running time and scalability compar-isons between structured rank learners and a prominent re-cent contender in accuracy ( McRank  X  X oosted regression trees) and find, on the public LETOR data set [16], that structured learners are considerably faster (Section 3.8) while also being more accurate in two out of three data sets.
Figure 1 summarizes the leading algorithms and data sets on which they have been evaluated. RankSVM [2], Struct-SVM [12], and SVMmap [9] codes are in the public do-main. Implementations of ranking using gradient-descent and boosting are not public.

From Figure 1 it is evident that many of the data sets are proprietary, and many implementations are not readily available. As a result, there are hardly any data sets over which many algorithms have been compared directly. We have implemented SVMauc [4], SVMmap [9], DORM [18], McRank [10], as well as our new proposals SVMmrr and SVMndcg , in a common open-source Java testbed ( http: //www.cse.iitb.ac.in/soumen/doc/StructRank/ ).

We ran all the algorithms on the well-known LETOR benchmark [16] that is now widely used in research on learn-ing to rank. We also ran all algorithms but one on TREC 2000 and 2001 data prepared by Yue et al. [9]. More details are in Section 4.1. In many cases, the behavior of different al-gorithms differed on the three data sets. This highlights the importance of shared, standardized data to avoid potentially biased conclusions.
Suppose q is a query from a query set Q . For each docu-ment x i in the corpus, we use q together with x i to compute a feature vector we call x qi  X  R d whose elements encode various match and quality indicators. E.g., one element of x qi may encode the extent of match between q and the page title, while another element may be the PageRank of the node corresponding to the i th document in the Web graph. Collectively, these feature vectors over all documents, for fixed query q , is called x q .

Learning a ranking model amounts to estimating a weight vector w  X  R d . The score of a test document x is w &gt; Documents are ranked by decreasing score.

For evaluation, suppose the exact sets of relevant and ir-relevant documents, G q and B q , are known for every query q . For simplicity these can be coded as z qi = 1 if the i th docu-ment is relevant or  X  X ood X  for query q , and 0 otherwise, i.e., the document is  X  X ad X . Let n + q = | G q | and n  X  q = | B will drop q when clear from context.
For every query q , every good document x qi and every bad document x qj , we want x qi to rank higher than x qj denoted  X  x qi x qj  X  and satisfied if w &gt; x qi &gt; w preferences can be asserted even when absolute relevance values are unknown or unreliable, as with clickthrough data. Usually, learning algorithms [1, 2] seek to maximize over w (a smooth approximation to) the number of pair preferences satisfied. The number of satisfied pairs is closely related to the area under the receiver operating characteristic (ROC) curve [4].

A long-standing criticism of pair preference satisfaction is that all violations are not equal [19]; flipping the documents at ranks 2 and 11 is vastly more serious than flipping #100 and #150. This has led to several global criteria defined on the total order returned by the search engine.
In a navigational query, the user wants to quickly locate a URL that is guaranteed to satisfy her information need. In question answering (QA), many questions have definite answers, any one correct answer is adequate. MRR is well-suited to such occasions [13]. Suppose, in the document list ordered by decreasing w &gt; x qi , the topmost rank at which an answer to q is found is r q . (Note: For consistency with code all our ranks begin at zero, not one.) The reciprocal rank for a query q is 1 / (1 + r q ). Averaging over q , we get Often a cutoff of k is used: The ideal ranking ensures an MRR of 1 (assuming there is at least one relevant document for every query). Of recent interest in Information Retrieval and Machine Learning communities is NDCG, which, unlike MRR, does accumulate credit for second and subsequent relevant docu-ments, but discounts them with increasing rank. The DCG for a specific query q and document order is DCG( q ) = P document i for query q and D ( i ) is the discount factor given by [14] Note the cutoff at k . Suppose there are n + q good documents for query q , then the ideal DCG is pushing all the relevant documents to the top. Now define Public Not public Algorithm SVMauc , public, reimplemented here [4]  X   X   X   X  [9]  X  [9] [18] SVMmap , public, reimplemented here [9]  X   X   X   X  [9]  X  [9] SVMmrr , proposed here  X   X   X   X   X  SVMndcg , proposed here  X   X   X   X   X  RankNet , not public [3] [6] LambdaRank , not public [6] [6, 10] [10] [10] SoftRank , not public [8, 15] [15] and average NDCG( q ) over queries. G ( q,i ) is usually defined as 2 z qi  X  1. Because we focus on z qi  X  X  0 , 1 } , we can simply write G ( q,i ) = z qi .
For query q , let the i th (counting from zero) relevant or  X  X ood X  document be placed at rank r qi (again, counting from zero). Then the precision (fraction of good documents) up to rank r qi is (1 + i ) / (1 + r qi ). Average these over all good documents for a query: The ideal ranking pushes all good documents to the top and ensures a MAP of 1.
In structured ranking, the input is the set of documents x q and the output is a total or partial order y over all the documents.

Ranking is achieved through two devices: a feature map  X  ( x q ,y ) and a model weight vector w . The score of a ranking trained model w and documents x q , to return the best rank-given w , this maximization amounts to sorting documents by decreasing w &gt; x qi .

The ideal ranking for query q is called y  X  q . It places all good documents in G q at top ranks and all bad documents B q after that. The third component of structured learning is a loss function : Any order y incurs a ranking loss  X ( y  X  0.  X ( y  X  q ,y  X  q ) = 0 and the worse the ranking y , larger the value of  X ( y  X  q ,y ).

In structured rank learning the goal is to estimate a scor-ing model w via this generic StructSVM optimization [12]:  X  q,  X  y 6 = y  X  q : w &gt;  X  ( x q ,y  X  q )  X  w &gt;  X  ( x
Intuitively, if y is a poor ranking,  X ( y  X  q ,y ) is large and we want w to indicate that, i.e., we want w &gt;  X  ( x q ,y w &gt;  X  ( x q ,y ) in that case. If not, w needs to be refined.
At any step in the execution of a cutting plane algorithm [12], there is a working set of constraints, a current model w and current set of slack variables  X  q , and we wish to find a violator ( X  q,  X  y 6 = y  X   X  q ) such that where &gt; 0 is a tolerance parameter, and then add the the working set. This means we need to find, for each q , arg max If w were  X  X erfect X , maximizing w &gt;  X  ( x q ,y ) would give us an ideal y with very small  X . Intuitively, the maximization (2) finds weaknesses in w where a large w &gt;  X  ( x q ,y ) can coexist with a large  X ( y q ,y ). Then the next step of the cutting plane algorithm proceeds to improve w and adjust  X  suitably.
Applying StructSVM to a problem [9, 18] amounts to designing  X  ( x,y ) and giving an efficient algorithm for (2). The critical property of the cutting plane algorithm is that, for any fixed , a constant number of violators are consid-ered before convergence. Therefore, if each invocation of (2) takes linear time, the overall training algorithm is also linear-time. The details, which are now standard, can be found in [12, 4, 5].
The representation of x qi as a feature vector comes from domain knowledge, but the design of the feature map  X  ( x,y ) is an integral part of learning to rank. Here we review two known feature maps and propose one.
For pair preferences and AUC, the partial order feature map is a natural choice. If y encodes a partial order, it is indexed as y ij where z qi = 1 and z qj = 0. y ij = 1 if the partial order places x qi before x qj . y ij =  X  1 if the partial order (mistakenly) places x qi after x qj . If y ij = 0, x x qj are incomparable in the partial order. Note that y  X  ij for all i,j . With this coding of y , a common feature function used by Joachims and others [4, 9] is where g indexes a good document and b indexes a bad doc-ument.
In practice, the pair-averaging scale factor 1 / ( n + q n absolutely critical for any learnability of w ; without it, we get almost zero accuracy of all kinds (AUC, NDCG, MAP, MRR). Therefore, proper scaling across queries is also an integral part of feature map design.
With  X  po defined as above, consider Note that the optimization (1) sees only  X  X  x ( y  X  ,y ), never  X  ( x,y  X  ) or  X  ( x,y ) separately.
 Fact 3.1.  X  X  x ( y  X  ,y ) can be written as where  X  b g  X  means that y places the bad document indexed by b before the good document indexed by g .
 Proof. Consider a good document (index) g with two bad documents, b 1 g and g b 2 , in partial order y . Using (3),  X  ( x,y ) will include terms x b 1  X  x g and x g  X  x b 2 . In y will have g b 1 and g b 2 , so  X  ( x,y  X  ) will include terms x This shows that, despite the global  X  X ll pairs X  feel to (3),  X  po carries no information to the optimizer from documents lower than the lowest-ranked good document. We can use (4) to define an equivalent feature map that exposes the local nature of  X  po : and therefore  X  X  x ( y  X  ,y ) =  X  X  x ( y  X  ,y ).

Now consider MRR.  X  MRR depends not on all g , but only the top-ranking good document g 0 ( y ). So the sum over all g seems out of place. Accordingly, we will define Again,  X  mrr ( x,y  X  ) = ~ 0. There is no need to scale down by n , because only one good document is contributing to the sum. We are just soft-counting the number of bad docu-ments ahead of g 0 ( y ), so there is also no need to scale down by n  X  q .
Instead of expressing a partial order involving good-bad pairs, y may also encode a total order. A natural encoding { 0 ,...,n q  X  1 } . Given x q and a permutation y , the feature function is where A ( r ) is a heuristically designed decay profile [18]. E.g., the ranking evaluation measures we study here pay more attention to the documents in the top ranks. For NDCG and MRR, our attention is limited to the top k documents. To embed this knowledge in the feature function, one can set A ( r ) to various decay functions, like 1 /  X  dorm ( x q ,y ) increases the representation of top-ranked docu-ments. Unfortunately, there is no theoretical guidance to de-sign A . It is naturally of interest to see how  X  po , X  mrr perform at various tasks; to our knowledge such comparisons have not been done before.
It is easy to translate the ranking criteria reviewed in Sec-tion 2.2 into loss functions.  X  AUC ( y  X  ,y ) is the fraction of pair preferences that are violated by y ( y  X  violates none). This can be written as Next we consider MRR, NDCG and MAP. For any y , MRR is a number between 0 and 1;  X  MRR ( y  X  ,y ) is simply one minus the MRR of y . Note that the MRR of y  X  is 1 if there is at least one good document for every query, which we will assume.  X  NDCG ( y  X  ,y ) and  X  MAP ( y  X  ,y ) are defined similarly.
Consider optimization (2) using  X  po and  X  AUC [4]. For the current fixed w in a cutting plane algorithm, let us short-hand the current score s qi = w &gt; x qi , and omit q when fixed or clear from context. Then observe that In this case the best choice of y is obvious: y gb = sign( s s  X  1 2 ), and therefore the elements y gb can be optimized independently. Other ranking criteria, such as MAP, MRR and NDCG lead to more non-trivial optimizations.

SVMauc with  X  po and  X  AUC admits a very efficient op-timization of (2). Learning for MAP with  X  po and  X  MAP is not as simple, but the following insight can be exploited to design a greedy algorithm [9].
 Fact 3.2. There is an optimal total order y for (2) with  X  and  X  =  X  MAP such that the scores (wrt the current w ) of good documents are in non-increasing order, and the scores of bad documents are in non-increasing order.
 The proof is via a swap argument. The SVMmap algorithm of Yue et al. [9] greedily percolates score-ordered bad doc-uments into the score-ordered sequence of good documents, and this is proved to be correct.
For  X  po and  X  NDCG , we present a solution to optimiza-tion (2) that takes O  X  P q ( n q log n q + k 2 )  X  time, where n n q + n  X  q . It can be verified that Fact 3.2 holds for  X  po  X 
NDCG as well. However, the details of merging good and documents are slightly different from SVMmap .

Fix a query q and consider the good documents in a list G = x 0 ,...,x g ,...,x n +  X  1 sorted by decreasing current score wrt the current w . Also let B = x 0 ,...,x b ,...,x be the bad documents sorted likewise. We will insert good documents, in decreasing score order, into B (the opposite also works). Initially, it will be easiest to visualize this as a dynamic programming table, shown in Figure 2.

Cell [ g,b ] in the table will store a solution up to the place-ment of good document x g just before bad document x b , which means its rank in the merged list is g + b (counting from 0). The contribution of the cell [ g,b ] to the objective H comes in two parts: CellScore ( g,b ) from w &gt;  X  po is CellLoss ( g,b ) is  X  D ( g + b ) / DCG  X  ( q ): this is the negated 1: obtain sorted good list G and bad list B 2: initialize objective matrix S to zeros 3: for g = 0 , 1 ,...,n +  X  1 do 4: for b = 0 , 1 ,...,n  X   X  1 do 5: { g good and b bad before x g } 6: find cellLoss  X  CellLoss ( g,b ) (see text) 7: find cellScore  X  CellScore ( g,b ) (see text) 8: cellValue  X  cellLoss + cellScore 9: if g = 0 then 10: { first good doc } 11: S [ g,b ]  X  cellValue 12: else if g &gt; 0 and b = 0 then 13: { several good docs before first bad } 14: S [ g,b ]  X  S [ g  X  1 , 0] + cellValue 15: else 16: { general recurrence with g,b &gt; 0 } 17: p  X   X  arg max 0  X  p  X  b  X  S [ g  X  1 ,p ] + cellValue 18: S [ g,b ]  X  S [ g  X  1 ,p  X  ] + cellValue For clarity, Figure 2 shows a generic procedure that may also be useful for other loss functions. For the specific case of NDCG, the following observations simplify and speed up the algorithm considerably.
 Fact 3.3. The optimal solution can be found using a reduced table of size min { n + ,k } X  ( k +1) instead of the n +  X  n shown.
 Proof. We keep the first k columns 0 ,...,k  X  1 as-is, but columns k through n  X  can be folded into a single column representing the best solution in row g for  X  b  X  k  X . This is possible because, right of column k , cellLoss becomes zero, so the best cellValue is the best cellScore , which can be obtained by binary searching bad documents b k ,...,b n  X   X  with key s g . This reduces our table size to n +  X  ( k + 1). However, there is also no benefit to considering more than k good documents, so we can further trim the number of rows to k if k &lt; n + .
 Fact 3.4. Instead of the general recurrence which takes  X ( k 2 ) time per cell and O ( k 3 ) time overall, we can first find the best column for the previous row g  X  1 : and then set S [ g,b ] = max which will take k 2 time overall.
 Proof. This involves a swap argument similar to Yue et al. [9, Lemma 1] to show that b  X  g  X  1  X  b  X  g , i.e., even though the optimal column in each row is being found greedily, these columns will monotonically increase with g . This follows the same argument as Yue et al. [9] and is omitted.
 Therefore, we can execute each  X  X rgmax X  step of SVMndcg in O ( k 2 + n log n ) time, using  X  po and  X  NDCG . The initial sorting of good and bad documents by their current scores s qi dominates the time.

Because MAP needs to optimize over the location of all good documents, the  X  X rgmax X  (2) step in SVMmap took time P q O ( n + q n  X  q + n q log n q ). Because NDCG is clipped at rank k , SVMndcg can be faster, although, in practice, the O ( n q log n q ) term tends to be the dominating term. we see that in case of MRR and NDCG, no credit accrues for placing a good document after rank k , whereas in case of MAP, a good document will fetch some credit no matter where it is placed. This means that SVMmrr and SVM-ndcg get no signal when if improves the position of good documents placed beyond rank k . We can give SVMndcg the same benefit by effectively setting k =  X  . The dy-namic programming or greedy algorithms can be adapted, like SVMmap , to run in O  X  P q ( n + q n  X  q + n q log n We will call this option SVMndcg-nc , for  X  X o clip X .
A different feature encoding,  X  dorm described in Section 3.1.3, was used very recently by Le et al. [17, 18] to perform a Di-rect Optimization of Ranking Measures ( DORM ). In this case, optimization (2) takes the form This is equivalent to filling in a permutation matrix (a square 0/1 matrix with exactly one 1 in each row and column)  X  to optimize an assignment problem [20] of the form The Kuhn-Munkres assignment algorithm takes O ( n 3 ) time in the worst case, which is much larger than the time taken by SVMndcg . The time can be reduced by making A very sparse. E.g., we might force A ( r ) = 0 for r &gt; k , but the resulting accuracy is inferior to a smooth decay such as A ( r ) = 1 /
An important limitation of DORM , thanks to using the assignment paradigm, is that it cannot  X  X ount good docu-ments to the left of a position X , and so cannot deal with MAP or MRR at all.

As we shall see in Section 4.4, SVMndcg is substantially faster than DORM while having quite comparable accuracy.
Because of the change in feature map from  X  po to  X  mrr , we have to redesign the  X  X rgmax X  routine. The pseudocode for solving (2) in SVMmrr is shown in Figure 3. Below we explain how it works. 1: inputs: current w , x 1 ,...,x n , clip rank k 2: obtain sorted good list G = g 0 ,...,g n +  X  1 and bad list 3: maxObj  X  X  X  X  , argMaxOrder  X  null 4: for r = 0 , 1 ,...,k  X  1 do 5: initialize empty output sequence o 7: merge b r ,...,b n  X   X  1 and g 0 ,...,g n +  X  2 in decreasing 9: if obj ( o ) &gt; maxObj then 10: maxObj  X  obj ( o ) 11: argMaxOrder  X  o 12: build remaining output sequence o with r  X  k and 13: if obj ( o ) = 1 + w &gt;  X  mrr ( x,o ) &gt; maxObj then 14: maxObj  X  obj ( o ) 15: argMaxOrder  X  o 16: return optimal order o for generating new constraint Figure 3: SVMmrr pseudocode for one query. In an implementation we do not need to materialize o .
 Fact 3.5. With  X  mrr and  X  MRR , (2) can be solved for a query q in time O ( n q log n q + k 2 + k log n q ) . Proof. With  X  mrr and  X  MRR , instead of using Fact 3.2, we collect all solutions y for the objective w &gt;  X  mrr  X 
MRR ( y  X  ,y ) into clusters, each having a common value of  X 
Note that  X  MRR ( y  X  ,y ) can only take values from the set { 0 , 1 /k, 2 /k,..., 1 } , so we can afford to first optimize the objective within each cluster and take the best of these k +1 solutions.

Consider all solutions y with  X  MRR ( y  X  ,y ) = 1  X  1 / (1+ r ), i.e., the MRR of ordering y is 1 / (1 + r ) (0  X  r &lt; k ) because the first good document is at position r (beginning at 0). Inspecting  X  mrr in (5), it is easy to see that within this one that fills ranks 0 ,...,r  X  1 with bad documents having the largest scores, and then places the good document with the smallest score at rank r . What documents are placed after the first good document is immaterial to  X  mrr , and therefore we can save the effort.

The last cluster of orderings is where there is no good document in any of ranks 0 ,...,k  X  1 and the MRR is 0 and  X 
MRR = 1. In this case, clearly the k bad documents with the largest scores should occupy ranks 0 ,...,k  X  1. Now con-sider the good document x g with the smallest score s g . We should now place all bad documents with score larger than s , after which we place x g . Again, how other documents are placed after x g does not matter.
Conventional wisdom underlying much work on learning to rank is that it is better to train for the loss function on which the system will be evaluated. As we have argued in Section 1.2, search systems typically face a heterogeneous workload. It may not be advisable to ultra-optimize a rank-ing system toward one criterion. Moreover, our experiments (Section 4.5) suggest that a test criterion is not reliably op-timized by training with the associated loss function.
A related question of theoretical interest is, must one nec-essarily sacrifice accuracy on one criterion to gain accuracy in another, or are the major criteria (AUC, MAP, MRR and NDCG) sufficiently related that there can be a com-mon model serving them all reasonably well?
Once a model w is trained by optimizing (1), during test-ing, given x q we return f ( x q ,w ) = arg max y w &gt;  X  ( x fine the following empirical risk as In presence of multiple kinds of loss functions  X  l ,l = 1 ,...,L , we can modify learning problem (1) in at least two ways. Shared slacks: We define an aggregate loss  X ( y,y 0 ) = max l  X  l ( y,y 0 ), and then assert the same constraint as in (1). This is done by simply asserting more constraints, on behalf of each  X  l : At optimality, we can see that 1 | Q | P q  X  q  X  R ( w, max pirical risk, as P l R ( w,  X  l ), in which case, we have to de-clare separate slacks  X  l q for each query q and loss type l . These slacks have different  X  X nits X  and should be combined as (1 / | Q | ) P l C l P q  X  l q . For simplicity we set all C learning C l s is left for future work. As before, we can see that 1 | Q | P q  X  l q  X  R ( w,  X  l fore P l 1 | Q | P q  X  l q  X  P l R ( w,  X  l ). Because max P Therefore, if there is a mix of queries that benefit from dif-ferent loss functions, such as some navigational queries that are served well by MRR and some exploratory queries served better by NDCG, separate slacks may perform better, which is indeed what we found in experiments.
For completeness, we compare the structured learning ap-proaches ( SVMauc , SVMmap , SVMmrr , SVMndcg , DORM , SVMcombo ) against McRank , which is among the best of the lower half of Figure 1. Li et al. [10] have found Mc-Rank to be generally better than LambdaRank [6] and FRank [21]; SoftRank is comparable to LambdaRank [8] and both are generally better than RankNet [3].

McRank uses a boosted ensemble of regression trees [22] to learn a non-linear itemwise model for Pr( z | x qi ), i.e., the probability of falling into each relevance bucket (in this pa-per we have mostly considered z  X  { 0 , 1 } ). The interesting twist is that, instead of assigning relevance arg max z Pr( z | x McRank assigns a score P z z Pr( z | x qi ) to the i th document responding to query q , and then sorts the documents by de-creasing score. Note that McRank has no direct hold on true loss functions like MAP, MRR or NDCG. We imple-mented the boosting code in Java, taking advantage of the WEKA [23] REPTree implementation.
Inside the LETOR distribution [16] there are three data sets, OHSUMED (106 queries, 11303 bad documents, 4837 good documents), TD2003 (50 queries, 48655 bad docu-ments, 516 good documents) and TD2004 (75 queries, 73726 bad documents, 444 good documents). Each document has about 25 X 45 numeric attributes. These are scaled to [0 , 1] within each query as specified in the LETOR distribution. We observed that LETOR has many queries for which the same feature vector is marked as both good and bad. In addition there are feature vectors with all elements exactly equal to zero. A robust training algorithm is expected to take these in stride, but test accuracy falls prey to break-ing score ties arbitrary. This can give very unstable results especially given the modest size of LETOR. Therefore we eliminated all-zero feature vectors and good and bad vec-tors whose cosine similarity was above 0.99. Although this further reduced the number of queries, the comparisons be-came much more reliable. In our other data set obtained from Yue et al. [9], TREC 2000 has 50 queries, 218766 bad documents and 2120 good documents. TREC 2001 has 50 queries, 203507 bad documents and 2892 good documents.
Obviously, formulating a structured learning approach to ranking does not guarantee healthy optimization. The pur-pose of this section is to highlight that structured ranking al-gorithms suffer from various degrees of distress during train-ing, and offer some analysis.
 rithms, the value of (1 / | Q | ) P q  X  q (an upper bound on the training loss) when the optimizer terminates, against C . DORM , SVMmrr , and SVMauc show the most robust reduction in average slack with increasing C . Note that DORM and SVMmrr use custom feature maps. Also,  X  po is ideally suited for  X  AUC . When constraints are added in SVMauc , each term (1 /n + n  X  ) y gb ( s g  X  s b ) on the lhs w is matched to one term (1 /n + n  X  )(1  X  y gb ) on the rhs  X  Figure 4: Different behavior of (1 / | Q | ) P q  X  q at ter-mination for different algorithms as C increases.
SVMmap and SVMndcg have a harder time. We conjec-ture that this is caused by a mismatch between  X  MAP ,  X  NDCG and  X  po . The lhs of constraints now consist of sums of (vari-able numbers of) score differences, while the rhs have a much more granular loss  X  not sufficiently sensitive to the varia-tion on the lhs.
 Training objective: Let obj( w = ~ 0) be the value of the objective in (1) for w = ~ 0, and obj be the optimized training objective. For C = 0, w = ~ 0 is the optimum. A plot of obj / obj( w = ~ 0) against C (Figure 5) is an indication of how well w is adapting to increasing C . The results are related to Figure 4: DORM , SVMauc and SVMmrr adapt best, while SVMmap and SVMndcg stay close to w = ~ 0, but, luckily, not quite at w = ~ 0 X  X ee Figure 6. Figure 5: obj / obj ( w = ~ 0) for different algorithms as C increases. 1E-2 1E+0 1E+2 1E+4 svmC Figure 6: Different behavior of | w | at termination for different algorithms as C increases. Rightmost point arbitrarily scaled to 1 for comparison. Indirect support for our conjecture comes from Figure 7. It shows the benefits of using  X  mrr instead of  X  po for optimiz-ing MRR. Over all data sets, there is a consistent large gain in MRR when  X  mrr is used, compared to  X  po . However, from Figure 9, we see that training with some criterion other than MRR is almost always best for test MRR scores. Specifi-cally, SVMcombo almost always beats SVMmrr . Similar to Taylor et al. [8], we conjecture that this is because the  X  X rue X  loss function  X  MRR and  X  MRR are losing information from multiple good documents. SVMcombo  X  X edges the bet X  in a principled manner.
The three data sets inside the LETOR distribution have different sizes, which makes it easy to do scaling experi-ments. OHSUMED has a total of 16140 documents, TD2003 has 49171, and TD2004 has 74170; this is roughly 1:3:4.6. OHSUMED has 106 queries, TD2003 has 50 and TD2004 has 75.

In these experiments we gave DORM the benefit of a sparse A (  X  ) decay function decaying to zero after rank 30, which was what was required to approach or match the ac-curacy of SVMndcg . From Figure 8 we see that the total time taken by DORM is substantially larger than SVM-ndcg , and scales much more steeply than 1:3:4.6, which is expected from the nature of the assignment problem.
In contrast, the total time taken by SVMndcg is much smaller. For TD2004, SVMndcg took only 19 seconds while Figure 7: Comparison of  X  mrr against  X  po for the MRR criterion. Figure 8: Breakdown of DORM time into as-signment [20] and quadratic optimization; com-pared with breakdown of SVMndcg time into time taken by the  X  X rgmax X  calculations (Figure 2) and quadratic optimization.
 DORM needed 283 seconds. Obviously the gap will only grow with increasing data sizes. Proprietary training data mentioned in the literature [10] have millions of documents.
Also noteworthy is the very small time taken in the QP optimizer (invisible for DORM , barely visible for SVM-ndcg ). We used a very recent and fast implementation of LaRank [24]. This shows that solving the  X  X rgmax X  problem (2) quickly for large data sets is important, because the QP solver is not the bottleneck.

Figure 9 compares test NDCG at rank 10 for different training criteria. SVMndcg and DORM come out about even, but SVMndcg-nc is consistently better than DORM .
At this point, it is of interest to complete a table where each row corresponds to a training criterion, and each col-umn is a test criterion. Conventional wisdom suggests that the trainer that gives the best test NDCG will be the one that uses  X  NDCG and so on. Figure 9 shows that this is rarely the case! Specifically,
We finally consider the training speed and test accuracy of
McRank . Our WEKA-based implementation exceeded 2 GB of RAM for each of TREC 2000 and TREC 2001, and was unreasonably slow. So we limit our study to the LETOR data. In only two of the nine columns pertaining to LETOR does McRank show substantial advantage; in the remaining seven, one of the list-wise structured learning approaches is better.
 McRank  X  X  occasional lead comes at a steep RAM and CPU cost. The CPU time is dominated by the time to induce CART [22] style regression trees. The number of rounds of boosting was set between 1500 and 2000 by Li et al. [10]; we found this too slow (corroborated elsewhere [19]) and also unnecessary for accuracy. On LETOR more than 30 X 40 rounds sometimes hurt accuracy, so we set the number of boosting rounds to 30; this only tips the scales against us wrt performance. Even so, we find in Figure 10 that McRank can be computationally more expensive that structured learners by two orders of magnitude.
Using the structured learning framework, we proposed novel, efficient algorithms for learning to rank under the MRR and NDCG criteria. The new algorithms are com-parable to known techniques in terms of accuracy but are much faster. We then presented SVMcombo , a technique to optimize for multiple ranking criteria. SVMcombo may be preferable for real-life search systems that serve a hetero-geneous mix of queries. Our exploration revealed that struc-tured ranking often suffers from a mismatch between the fea-ture map  X  ( x,y ) and the loss function  X ( y,y 0 ). Designing loss-specific feature maps for better training optimization remains a central problem that merits further investigation. Yue for the TREC 2000 and TREC 2001 data, to Sundar Vishwanathan for discussions, and to Sunita Sarawagi for discussions and the LARank implementation.
 Figure 10: Break-up of McRank running time and comparison with SVMndcg and SVMmrr (times in seconds). SVMmap are among the most robust trainers.
