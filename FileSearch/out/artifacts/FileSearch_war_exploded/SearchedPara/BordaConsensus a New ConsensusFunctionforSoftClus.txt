 g Consensus clustering is the task of deriving a single lab eling by applying a consensus function on a cluster ensem ble. This work introduces BordaConsensus, a new consensus function for soft cluster ensem bles based on the Borda voting scheme. In con trast to classic, hard consensus functions that oper-ate on lab elings, our prop osal considers cluster mem bership information, thus being able to tackle multiclass clustering problems. Initial small scale exp erimen ts rev eal that, com-pared to state-of-the-art consensus functions, BordaConsen-sus constitutes a good performance vs. complexit y trade-o . I.2.7 [ Arti cial Intelligence ]: Natural Language Pro cess-ing| Text Analysis ; I.5.3 [ Pattern Recognition ]: Cluster-ing| Algorithms Algorithms, Design, Exp erimen tation, Performance Documen t clustering, soft cluster ensem bles, Borda voting
Being the unsup ervised coun terpart of classi er commit-tees, consensus clustering aims to com bine the results of sev eral clustering pro cesses, collected in the cluster ensem-ble , into a consensus lab eling through the application of a consensus function F . Typical applications include clus-tering reuse besides distributed and robust clustering [6].
Most consensus functions posed in the literature operate on hard cluster ensem bles made up of the labelings output by sev eral clustering pro cesses (e.g. [3, 5, 6]). However, they are also applicable on ensem bles built upon soft partitioning algorithms, by previously transforming clusterings into la-belings (i.e. assigning eac h documen t to the cluster with the largest mem bership probabilit y [4]). In either case, cluster mem bership information is ignored or discarded.

Alternativ ely, soft consensus functions have been prop osed [2] so as to mak e use of suc h information, besides making multiclass clustering consensus possible. Follo wing this ap-proac h, this work i) adapts existing hard consensus func-tions to the soft cluster ensem bles con text, and ii) introduces a novel consensus function for soft cluster ensem bles, named BordaConsensus, whic h is inspired in data fusion techniques based on the Borda voting scheme [1]. In order to evaluate our prop osal, the BordaConsensus function is compared to state-of-the-art consensus functions in terms of performance and time complexit y.
In this section, we describ e how the Bor da-fuse data fusion technique [1] is adapted to deriv e a novel consensus function for creating a consensus lab eling upon soft cluster ensem bles.
Henceforth, it is assumed that our aim is to group a col-lection of jD j documen ts into jC j clusters. Giv en a set of jP j indep enden t soft clustering pro cesses, a soft cluster en-sem ble is de ned as a jC jjP j jD j matrix made up of jP j mem bership probabilit y matrices: where T denotes matrix transp osition and M p is the jC j jD j documen t-to-cluster mem bership probabilities matrix result-ing from the p th soft partitioning of suc h corpus: where m p c is a column vector that con tains the mem bership probabilities of the jD j documen ts with resp ect to the c th cluster according to the p th soft clustering pro cess.
Data fusion (ak a metasearc h) techniques are designed to fuse the rank ed lists of documen ts returned by distinct searc h engines, aiming to impro ve retriev al results. In the litera-ture, metasearc h has often been compared to voting, regard-ing eac h searc h engine as a voter and eac h documen t, as a candidate [1]. Follo wing this analogy , we adapt the Bor da-fuse data fusion technique to the consensus clustering prob-lem, where the jD j documen ts, the jC j prede ned clusters and the jP j clustering pro cesses play the role of candidates, elections and voters, resp ectiv ely.
 scor e = zeros( jC j ; jD j ); for c = 1 : : : jC j , end M = softmax ( scor e ) or = argmax c ( scor e ) Table 2: Documen t corp ora subsets description
The application of the Borda positional voting scheme to the consensus clustering problem is as follo ws: rstly , docu-men ts are rank ed according to their mem bership probabilit y with resp ect to eac h cluster. Then, for eac h cluster, the top rank ed documen t receiv es jD j points, the second rank ed doc-umen t receiv es jD j 1 points, and so on. As a result, the BordaConsensus function can indistinctly yield a soft con-sensus clustering M (a mem bership probabilities matrix) or a consensus lab eling (by assigning the documen ts to the cluster that maximizes their score {see table 1).
To ensure the consistency of the voting pro cess, it is neces-sary to solv e a cluster corresp ondence problem across the jP j partitions prior to voting. In this work, this is accomplished through the correlation based approac h that maximizes the overlap between corresp onding clusters presen ted in [5].
The follo wing exp erimen ts have been conducted on sub-sets of the miniNewsgroups and OHSUMED collections, giv-ing rise to two single-class balanced clustering problems. Table 2 summarizes the main asp ects of both corp ora, in-cluding their vocabulary size jV j and the average num ber of terms per documen t, jT d j .

The application of 4 documen t represen tation techniques with 49 distinct dimensionalities and the execution of 10 runs of the k-means (KM) algorithm with random cen troids initialization on eac h corpus has given rise to 200 represen ta-tionally , partitionally and dimensionally-div erse cluster en-sem bles of sizes jP j = f 4 ; 10 ; 49 g . Note that the soft clus-ter ensem ble is built by transforming the documen t-to-cen troid distances returned by KM into mem bership proba-bilit y matrices by applying a softmax normalization.
The prop osed BordaConsensus function ( F BC ) is com-pared to sev eral state-of-the-art hard consensus functions: Evidence Accum ulation ( F EAC ) [3], Cluster-Similarit y Par-titioning Algorithm ( F CSPA ), Hyp er-Graph Partitioning Al-gorithm ( F HGP A ) and Meta-Clustering Algorithm ( F MCLA [6]. This comparison is twofold, as it involves not only the classic hard version of suc h consensus functions, but also a soft version (i.e. adapted to operate on soft cluster en-sem bles). Suc h adaptation is accomplished by substitut-ing the documen t or cluster similarit y matrices that these consensus functions deriv e from hard cluster ensem bles [3, Figure 1: F1 score vs. CPU time 2 -region plot comparing all the consensus functions. 6] for the probabilit y matrices T or T , resp ec-tively. Moreo ver, F BC is also compared to the only {to our kno wledge{ consensus function originally designed to oper-ate on soft cluster ensem bles, that we name F DWH after its authors [2]. This consensus function is based on sim ultane-ously matc hing clusters iterativ ely on a pairwise basis plus documen t-to-cluster assignmen t according to a prop ortional weigh ting voting strategy [2].
 The exp erimen ts have been run under Matlab 7.1 on a PC PIV (1.6 Ghz, 1GB RAM). The nal hard consensus lab eling is evaluated in terms of i) the macroa veraged F1 measure with resp ect to the true category lab els of eac h documen t, and ii) the CPU time (in seconds) required for execution.
Figure 1 depicts a F1 score vs. CPU time 2 -region plot comparing all the consensus functions. Tw o issues must be noted for both corp ora: rstly , the state-of-the-art hard consensus functions su er F1 score losses when they oper-ate on soft cluster ensem bles. And secondly , F DWH is, in general, the fastest function, as it sim ultaneously performs cluster corresp ondence solving and voting [2]. Regarding the prop osed F BC function, i) in the miniNewsgroups ex-perimen t, its performance is statistically signi can tly better than F DWH {in terms of ANO VA ( F (1 ; 398) = 4 : 36 ; p = : 0374){ while being clearly faster than the remaining func-tions, and ii) in the OHSUMED exp erimen t, F BC achiev es the best soft consensus performance (but statistically equiv-alen t to F DWH ), thus constituting a good F1 vs. CPU time trade-o among the soft consensus functions.

Further researc h will be orien ted towards applying the prop osed F BC function on multiclass consensus clustering problems and implemen ting other voting schemes. [1] J.-A. Aslam and M. Mon tague. Mo dels for metasearc h. [2] E. Dimitriadou, A. Weingessel, and K. Hornik. A [3] A. Fred and A. Jain. Com bining multiple clusterings [4] A. Jain, M. Murt y, and P. Flynn. Data clustering: a [5] S. Siersdorfer and S. Sizo v. Restrictiv e clustering and [6] A. Strehl and J. Ghosh. Cluster Ensem bles { A
