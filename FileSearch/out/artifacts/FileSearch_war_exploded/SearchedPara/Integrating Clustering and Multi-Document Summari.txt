 Document understanding techniques such as document clustering and multi-document summarization have been receiving much at-tention in recent years. Current document clustering methods usu-ally represent documents as a term-document matrix and perform clustering algorithms on it. Although these clustering methods can ture the meanings of the documents since there is no satisfactory interpretation for each document cluster. In this paper, we propose a new language model to simultaneously cluster and summarize the documents. By utilizing the mutual influence of the document clustering and summarization, our method makes (1) a better doc-ument clustering method with more meaningful interpretation and (2) a better document summarization method taking the document context information into consideration.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval; I.2.6 [ Artificial Intelligence ]: Learning Algorithms, Experimentation, Performance Document Clustering, Multi-Document Summarization, Nonnega-tive Matrix Factorization with Given Bases
Document clustering and multi-document summarization are two fundamental tools for understanding documents. In this paper, we propose a new language model, factorization with given bases (FGB), by making use of both the term-document and term-sentence ma-trices to simultaneously cluster and summarize the documents. The proposed FGB model translates the clustering-summarization prob-lem into minimizing the Kullback-Leibler divergence between the given documents and model reconstructed terms. The minimiza-tion process results two matrices which represent the probabilities of the documents and sentences given clusters (topics). The doc-ument clusters are generated by assigning each document to the topic with the highest probability, and the summary is formed with the sentences with the high probability in each topic.
Instead of freely choosing topic language models, we further as-sume that topic language models are mixtures of some existing base language models , i.e. where S is the set of base language models. Here, we use sen-tence language models as the base language models. One benefit of this assumption is that each topic are represented by meaningful sentences, instead of directly by keywords.

For the sake of simplicity, we use unigram language models in this paper. Thus we have denoted by B w,s . The model parameters are ( U , V ) , where Thus, p ( w i |  X  d ) = [ BUV &gt; ] w,d .

The FGB model uses the mixtures of some existing base lan-guage models as topic language models. When the base language models are language models with single word, then this model is identical to PLSI [4].

The parameter estimation is maximum likelihood estimation given occurrence of term-document. We use the empirical distribution of The task is
The algorithm for estimating FGB model is similar to NMF al-gorithms [5, 6, 3]. The computational algorithm of our model is shown in Algorithm 1.
 Algorithm 1 Model factorization given base language models Input: A : term-document matrix.
 Output: U : sentence-topic matrix; begin 1. Initialization:
Randomly initialize U and V with normalizing each column of U and each row of V to 1 2. Iteration: repeat 2.1 Compute C ij = A ij / [ BUV &gt; ] ij ; 2.3 Compute C ij = A ij / [ BUV &gt; ] ij ; until convergence 3. Return U , V end ity of our FGB approach, in this example, we use the top 10 largest
