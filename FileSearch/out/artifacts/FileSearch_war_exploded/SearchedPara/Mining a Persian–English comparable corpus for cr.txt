 1. Introduction
The researches on Cross Language Information Retrieval (CLIR) have recently received much attention, due to the fast growth of the World Wide Web and the availability of information in different languages on the Web. One of the main issues purpose which are actually available in many language pairs and extracting translation knowledge from multilingual corpora has been extensively studied using various statistical methods. They can be either in the form of parallel or comparable corpora. However, there are limitations in obtaining parallel corpora in all domains and languages while comparable corpora are much easier resources to obtain. Thus, recently, there has been considerable interest in using comparable corpora as Tao &amp; Zhai, 2005 ). In this paper we use a Persian X  X nglish comparable corpus, University of Tehran Persian X  X nglish by extracting term associations from the comparable corpus. These association terms my contain both translations of a term, the alignments. We further propose a novel way of extracting translations in different languages based on Terms Association  X 
Network (TAN) which exploits term associations in monolingual data as well as bilingual term associations to better detect translation knowledge. TAN method uses a network of terms with implicit mutual information links between terms in the same language and term association links between terms in different languages. The main contribution of this paper lies in combining these term association links as a network, which in turn improves CLIR effectiveness. Basically, we use the neigh-neighborhoods in the same language are strongly connected and vice versa they are not likely to be translations if their neighborhoods are not strongly connected. Also, in order to discard misleading translation candidates, we do translation is different from that of its neighbors X  translations, the term is considered as an outlier.

We evaluated our methods on Hamshahri and INFILE data sets by doing cross-language information retrieval using the cross-lingual term associations extracted from the comparable corpus. We use the extracted translation knowledge to con-struct a query language model in the target language corresponding to each query in the source language and rank the doc-uments based on the KL-divergence between the query language model and document language models. Experiments show promising results for extracting translation knowledge from the UTPECC by (1) translating Out Of Vocabulary (OOV) terms, such as proper nouns, which are not in our dictionaries, (2) expanding query words with their related terms and also (3) lating OOV terms and finding related terms to expand query words.

The rest of the paper is organized as follows. We first present some previous work done on exploiting comparable corpora experiment results in Section 5 and finally bring the conclusions and future work of our study in Section 6 . 2. Previous work
Using comparable corpora as a language resource has been studied extensively in the existing literature, in fields such as queries for cross-language information retrieval. In order to do CLIR, we construct query language models based on scores of extracted related terms from the comparable corpus. Trieschnigg, Hiemstra, de Jong, and Kraaij (2010) use a similar approach to train their translation model in CLIR using a corpus.

The problem of exploring a comparable corpus and its combination with a dictionary to do CLIR has been studied before in Talvensaari et al. (2007) and Sadat (2010b) . However, our methods in both extracting translation knowledge and con-lation program (Cocot) which we use as our baseline. They also combine the comparable corpus results with dictionary-based query translation (UTACLIR) and construct queries in the InQuery format, while we use a simple dictionary and con-struct query language models. Sadat (2010b) presents a two-stage corpus-based translation model which aims to find trans-
The two stages contain bi-directional extraction of bilingual terminology from comparable corpora and selection of best translation alternatives based on a morphological analyzer. She has also exploited the linear combination of comparable cor-pus results with bilingual dictionaries. In both works, the impact of comparable corpora on cross-language information re-trieval especially combining the extracted translations with dictionaries has been shown to be effectively positive. 3. Mining term associations
Inthissection,weproposeaprocessforlearningcross-lingualtermassociationsfromcomparablecorpora.Asafirststep,we extract translationknowledge using term co-occurrencesin the comparable corpus alignments. In the second step, we propose a method based on term association network which exploits term associations in monolingual data as well as bilingual term translation candidates which are detected as outliers. We will present each step in more detail in the rest of this section. 3.1. Cocot: basic translation extraction method
As the basic method, in order to extract term associations from the comparable corpus, we use the method used in Cocot, the comparable corpus query translation program which is proposed in Talvensaari, Pirkola, J X rvelin, Juhola, and
The algorithm first calculates a weight m ik for each term s where tf ik is the frequency of s i in document d k , Maxtf terms in the document. NT can be either the number of unique terms in the collection or its approximation. In our experi-ments, these frequencies are computed after stopword removal. This tf . idf modification is adopted from Sheridan and
Ballerini (1996) who also used it in similarity thesaurus calculation. The weight of a target term t documents D is calculated as: where D is the set of target documents aligned with a source document containing t weights are penalized. This penalization is achieved by ln ( r + 1) in the denominator.

Finally, the similarity weight between a source term s i and a target term t where m ik is the weight of source term s i in the source document d documents D which are aligned with the source document d k the mean of the target term vector lengths, and a is a constant between 0 and 1 (we chose a = 0.2 same as ( Talvensaari vent harsh penalization of long feature vectors (that is, words with high document frequency) ( Hashemi et al., 2010 ). 3.2. TAN: Term Association Network Model
Cocot only considers term co-occurrences in the alignments of source and target documents of the comparable corpus to extract translations, but we perceive that we can also consider term co-occurrences in the monolingual data to get better translation knowledge. Some terms in each language are about to happen in the same topics, such as  X  X ennis X  and  X  X imble-of being translations of each other if the terms in their neighborhoods in the same language are highly correlated with each other. In other terms, if their neighborhoods are not correlated, they are less likely to be translations of each other even though they are obtained as translations. Thus, we propose to construct a network of related terms that consists of all the terms in the source language and all the terms in the target language. The edges between two terms in the same language are based on their co-occurrences in that language and the edges between terms in different languages are based on the mined associations. In our experiments we use mutual information as the links between terms in the same language and the nor-are not likely to be translations.

As an example, in the extracted translations by Cocot in our experiments, the highest ranked translation for the term number of aligned documents containing the term  X  X imbledon X  in the comparable corpus. Our goal is to improve the trans-lation quality of the list of terms extracted by the basic method by reranking the translation terms using the term network.
As can be seen from Fig. 1 which is part of the network for the term  X  X imbledon X , its neighborhood in its own language is more strongly connected to the neighborhood of its correct translation compared to the neighborhood of the incorrect trans-lation. This observation leads us to rerank the list of translations based on the strength of the connection between their neighborhoods and the neighborhood of the source language term.

Using this network, we propose to update the weight of the translation link between source and target terms s and t using general function f as: where w 0 ( s , t ) is based on mined association between s and t , N s , N T ( t ) is the set of target language terms in the neighborhood of t and sim ( N borhoods, which can be computed based on the relevance scores of source language terms in N neighborhoods will be.

In our experiments, we consider one specific case of this model which consists of weights of all the paths with length one, two or three between the two specified source and target terms. The updated translation weight between source and target terms s and t is calculated as: experiments, we have set a = b = c =1. w MI is the normalized expected mutual information weight ( Manning, Raghavan, &amp;
Sch X tze, 2008 ) of co-occurring terms in the same language. We considered confidence intervals of mutual information in our estimates to select trustworthy neighbors. w 0 is the normalized weight of the correlated terms in different languages.
Using Cocot method, we calculate the similarity of source language term s to target language term t (i.e. w tion, we observe that we can also consider the similarity of target and source language terms as well (i.e. w tions of each other. So, we define w 0 ( t , s ) as the combined similarity of source and target language terms:
In our experiments, we use the sum of normalized similarity weights as function g . We examined the product of the weights as function g as well, and the results were similar.

We normalize the mutual information scores between terms in the same language and raw translation association scores between terms in different languages before combining them using our proposed method. One basic normalization method the normalized value should drop sharply as the scores become smaller. To penalize low scores, we use exponential trans-formation in our experiments. 3.3. Translation validity check
In our obtained term associations, there exist some terms whose extracted translations are not correct, which leads to decrease in the accuracy of the extracted translations. These terms may for instance be high frequency terms having high entropies in the comparable corpus or low entropy terms which have appeared in only few aligned documents. Intuitively, it sounds reasonable to detect these exceptions using outlier detection methods to enrich translations either by omitting them from the created thesaurus or by obtaining their translations from other resources such as dictionaries.
In this section, we introduce our approach to detect incorrectly translated terms (outliers) based on the distribution of term association weights in the same language and their related terms. We use a modified version of the method proposed by Papadimitriou and Faloutsos (2003) for cross-outlier detection using probabilistic criteria for automatic recognition of outliers.

We consider the problem of detecting outliers in one language ( L language ( L 2 ). Thus, our goal is to find terms in L 1 that  X  X  X rouse suspicions X  X  with respect to terms in L hood and on the other hand the term  X  X ehran X  is not detected as an outlier since it has a similar distribution with its neighborhood.

Considering term t in language L 1 , we first collect its neighborhood of radius r in L neighborhood of t , we define a locality neighborhood with radius a over its extracted translations in language L where w L 2  X  q ; a  X  is the sum of the weights of the extracted translations of term q in language L bors in radius a which are terms in language L 2 . Thus, the standard deviation, ^ r is defined as:
Finally, a term t in L 1 is considered as an outlier at radius r with respect to the extracted translations in L where k r is a constant that determines what is a significant deviation. Typically k only consider the left-hand side of Eq. (9) to sort terms based on how much they can be considered as outliers. So, consid-ering only the left-hand side of the equation, we set k r ically selected based on the neighborhoods or translations of each term (see Section 4.2 ). Our experiment results show that the average and standard deviation with respect to radius r give us useful information about the vicinity of the terms and therefore we can detect terms that could not be translated. 4. Constructing the query language models
One of our main goals in this research is to do cross-language information retrieval using the obtained cross-lingual asso-ciations from the comparable corpus. To achieve this goad, we translate queries from the source language to the target lan-guage and obtain translation knowledge from our comparable corpus using the explained methods in Section 3 . Since some terms X  translations may not be obtained from comparable corpus, we detect them as outliers and use bilingual dictionaries to obtain their translations.

In order to use the translation knowledge to do CLIR, we present three methods to translate queries by constructing their query language models in the target language. In the first two methods, we directly use the extracted translations to trans-late query words in L 1 to their corresponding terms in L knowledge along with dictionary translations to construct the query language models. In the following, we present the de-tails of the methods. 4.1. Static related term selection
Given query Q in language L 1 , for each query word, we use top k of its extracted associated terms in language L translations and construct the query language model in L 2 equally important, so they will have equal weights in the query language model. Moreover, in the constructed query lan-guage model each translation term has a weight which is based on the weights of the extracted term associations.
We use the method proposed in Shakery (2008) to construct a basic translation of the query in L
Q = q 1 , ... , q n in L 1 and t i 1 ... t i k as the top-k related terms in L
L is constructed as: where p ( t l j q j ) is the calculated weight using Eq. (5) and p ( t otherwise.

We use the same method in the case of omitting outliers. We just do not consider the query words that are detected as outlier when constructing query language model. For instance, if the query consists of three terms and one of them is an outlier, we construct the language model based only on two of query words without considering the outlier. 4.2. Dynamic related term selection
In the basic query translation method, for all the query words, we equally select the top k associated terms in L can be used in the same context. So, they would be probably useful for query expansion. Thus, if we select k (e.g. k =3) tion of the associated terms to select the number of related terms for each term dynamically.
 We propose a method to select the best associated terms based on the knee point of the diagram as follows: maximum slope as: where ^ k is the estimated cut off for the most related extracted translations. The presented method can also be used to dynamically select the most associated neighbors to a term in a same language when the scores are calculated based on mu-using the method explained in Section 4.1 . 4.3. Using dictionary along with comparable corpus
We do not expect to extract suitable translation knowledge for all the terms from the comparable corpus. For instance, since we use distribution of term frequencies, translations for the high frequency terms could not be obtained. So, we con-struct query language model for several combined CLIR approaches based on dictionary-based query translations and ex-tracted translations from comparable corpora. 4.3.1. Dictionary along with extracted translations (Dic&amp;CC)
In this combined method, in order to translate each query word, we use its dictionary translations as well as the extracted weights compared to our extracted translations from comparable corpus. The weights of the dictionary translations are set to be the highest extracted translations from the comparable corpus for that term plus a small amount which is the difference between the highest and the second highest extracted translations from comparable corpus for that term multiplied by the factor 2. Afterwards, the query language model construction is based on the method presented in Section 4.1 . 4.3.2. Dictionary then extracted translations (Dic-CC)
In this method, first the query words are translated using dictionary and then for those query words which could not be found in dictionary, we use their extracted translations from comparable corpus. Formally, let Q = q
Since each query word is translated either by dictionary or by comparable corpus, the query language model in L structed as 4.3.3. Extracted translations then dictionary (CC-Dic)
In this method, first the query words are translated using comparable corpora then those query words which are detected as outliers, are translated by dictionary. The query language model construction is the same as previous section. 5. Experimental results
In this section, we report our experiments on doing cross-language information retrieval by applying several techniques to extract translation knowledge from the comparable corpus. The presented methods to extract translation knowledge from comparable corpus are independent of the languages of the comparable corpus and they can also be applied on other lan-guages. In our experiments, we use Hamshahri corpus and its bilingual queries to do CLIR. Since our created comparable cor-pus is constructed by the Hamshahri news articles, we also apply our proposed methods on another data set with different origin (i.e. INFILE corpus). 5.1. Document collections 5.1.1. Persian X  X nglish Comparable Corpus (UTPECC) UTPECC consists of news articles in Persian and English. The English collection is composed of news articles published in
BBC News and the Persian collection includes the news articles of Hamshahri newspaper. Five years of news articles, dated from January 2002 to December 2006 have been used. The Hamshahri articles are extracted from Hamshahri collection aligned with 190,000 Persian documents resulting a comparable corpus of more than 10,300 document pairs. The details of the collections are given in Table 1 .

To construct the comparable corpus, we first extract the keywords of each document in the source language and translate the keywords to the target language. These translations are considered as queries in the target language and are run against
Hashemi et al. (2010) and tested with different document relevance score thresholds to create the document alignments and thus the comparable corpus. The main criterion for evaluating the quality of alignments was manually assessing the align-ments of one month on a five-level relevance scale. Finally, the best high quality aligned documents are chosen as our com-parable corpus which consists of 10,365 document alignments and 10% of the 53,697 source documents are aligned. Table 2 shows some statistics about our created comparable corpus. Since the source and target documents are very different, the relatively low number of alignments was expected. Moreover, the number of alignments can be increased with lowering the thresholds, but this can also affect the quality of the comparable corpus.
 5.1.2. Hamshahri corpus
Most previous work on English X  X ersian CLIR used Hamshahri test collection at CLEF-2008 uments from topics in English. The document collection for this task contains 166,774 news stories (578 MB) that appeared in overlaps with UTPECC comparable corpus in year 2002.

The CLEF-2008 task consists of 50 query descriptions in Persian and the English translations of these topics. We used these topics to compare our results to the best existing results. For the rest of the experiments, we used CLIR task of
CLEF-2008 and 2009 together which contains 100 topics. Among them we used 85 queries, the other 15 omitted queries were too specific to Persian language, such as  X  X hajarian Concert X  and  X  X econstruction of Kandovan tunnel X . The Persian que-ries are used for monolingual retrieval. 5.1.3. INFILE corpus
The INFILE (INformation, Filtering, Evaluation) corpus is used in INFILE@CLEF 2008 and 2009 track on the evaluation of cross-language adaptive filtering systems and LREC2010 Workshop on Evaluation of Information Filtering Systems in a Com-ted from our experiments. Also, queries that had very few relevant documents (less than 4) were removed. Remaining was 42 refined queries. The titles of these queries were translated by two knowledgeable speakers and the most fluent translations were picked as the final translations. The Persian translation of topics are available at our research website. 5.2. Cross-language information retrieval using comparable corpus
We have studies the effect of applying various query translation techniques based on our created Persian X  X nglish com-also as a consequence the better the performance of CLIR systems would be. In our experiments, we first extract translation knowledge from the comparable corpus and then assess the quality of the obtained associations by doing CLIR with different retrieval models. We used two different collections to evaluate the efficiency of English X  X ersian and Persian X  X nglish CLIR tasks in several experiments. We have done experiments using Okapi with pseudo relevance feedback ( Robertson &amp; Walker, generally better, we have only reported these results.
 We have used the Lemur toolkit 6 as our retrieval system. Also, we used Porter stemmer for stemming the English terms and
Inquery X  X  stopword list (418 words). The Persian collection is only preprocessed using Neuchatel X  X  stopword list
Although we applied some Persian stemmers, none of the results were promising ( Dolamic &amp; Savoy, 2009 ). Therefore, we decided not to use any stemmer for Persian language. Also, in our experiments, as a dictionary resource, we have used more common translations first. 5.2.1. Monolingual retrieval
We use monolingual Persian X  X ersian retrieval of Hamshahri collection as one of our baselines to which we compare the cross-language results. In our monolingual runs we only consider title fields. We did two monolingual runs, one using KL-divergence retrieval model without query expansion and one using query expansion with pseudo relevance feedback.
For the pseudo relevance feedback run, we used the mixture model approach implemented in the Lemur toolkit ( Lemur, 2013 ) using top 10 retrieved documents to perform feedback. As the parameters, we used 100 terms for expanding the query model and 0.8 for the feedback coefficient. Table 3 shows the mean average precision, precision at 5 documents and preci-sion at 10 documents of our monolingual runs, along with the performance of the existing monolingual Persian runs over gual results and form a reasonable baseline to which we can compare our cross-language results.

For the rest of the experiments, we have put all the queries of CLEF-2008 and 2009 together and have omitted the queries which were too language specific. Table 4 shows the mean average precision, precision at 5 documents and precision at 10 documents of our monolingual runs over the 85 remaining queries. 5.2.2. Cocot with static related term selection
In our first set of experiments, we tried to extract term associations from the comparable corpus using Cocot as our basic on top k (top-1 to top-20) related terms for each query word in the Hamshahri corpus. We used the top k mined correlated terms to construct the query language models (see Section 4.1 ). The Cocot performance for top 2 is 0.130 in mean average precision, 0.233 in precision at 5 documents and 0.205 in precision at 10 documents. It means that using only comparable corpus as the translation method and compared to the monolingual baseline, we can achieve up to 32% of mean average pre-cision, 36.2% of precision at 5 documents and 34.8% of precision at 10 documents. 5.2.3. TAN with static related term selection
In our next set of experiments, we applied our proposed translation method based on the network of terms to do CLIR. To implement this idea, we first construct the network of terms in English and Persian languages where the links in the same language represent the mutual information between terms and the links between terms in different languages show their neighborhoods are strongly connected and similar. In order to calculate the similarity of two terms X  neighborhoods, we con-sider the paths between those two terms which come across their neighborhoods. All the 100 nearest terms around each term are considered as its neighborhood. We compare two cases here: = 0, only taking into account direct paths with length one between two terms and not considering their neighborhoods, and &gt; 0 considering the neighbors of the terms and taking into account all the paths with length one, two or three between two terms. Fig. 4 shows the results with different values of top k for = 0 and &gt; 0 compared to the Cocot method.

As can be seen from the figure, there are promising improvements in the mean average precision of the proposed meth-likewise the incorrect translations have obtained lower weights and have been sent to lower ranks in the translation lists.
Also, the lower mean average precisions of = 0 compared to the case where &gt; 0 shows that the neighborhood information of terms could help to obtained more accurate related terms from comparable corpus. 5.2.4. TAN with dynamic related term selection
In this set of experiments, we use our proposed method in Section 4.2 to select dynamic number of translations for each query word based on the observation that the extracted translation quality limit for each query word is different. Table 5 shows the results of dynamic related term selection in comparison with monolingual retrieval, basic Cocot method and term selection can achieve the best results among the other CLIR methods. Using this method compared to the Cocot method, we can achieve up to 36.1% improvement of mean average precision in Hamshahri corpus.

From now on, we will use the extracted translations from the TAN method with dynamic related term selection as the comparable corpora approach of query translation. 5.2.5. Translation validity check by outlier detection
In our next set of experiments, we try to detect terms which could not be translated based on our comparable corpus methods. These mistranslated terms may be either too general terms with high entropies or too specific terms with low fre-quencies in the aligned documents. Using the presented method in Section 3.3 , we can obtain a sorted list of query words based on their outlier coefficient score. Some examples of query terms that were deemed outliers are: road, stress, celebra-tion, kidney, carpet and persian. We observed that the number of outliers with low document frequencies are 10 times high-er than number of outliers with high document frequencies.

Fig. 5 shows the performance of the method in two cases: when we omit the outlier query words from the queries, and when we translate these outlier query words using a dictionary. The increasing slope of diagram in the omit outlier run shows that even only by omitting a few number of outliers (in this run 10), we can achieve slightly better mean average precision compared to the case when we keep all the query terms. It shows improvements, although the improvement is tionary. The results show that only by translating a limited number of terms, for example 5 query words across all queries cision which is up to 45.8% of the monolingual baseline. These results show our relative success in detecting outliers. 5.2.6. Using dictionary along with comparable corpus In the next set of experiments, we use the extracted translations from comparable corpus to improve dictionary-based CLIR. We combine comparable corpus with the dictionary in two different ways. In the first approach which is called Dic-comparable corpus. Fig. 6 shows the mean average precision of dictionary-based translation CLIR based on different number of translation terms (1 X 10) for each query word.

In this figure, the results of using only dictionary and only comparable corpus is shown as Dic and CC runs respectively. In because of lack of proper nouns which are essential query keywords. Thus, one of the success reasons of comparable corpus is the ability to translate these OOV terms such as Khatami, internet and NATO. Another advantage of comparable corpus is finding good query expansion terms. The increasing slope of some parts of diagrams confirm this hypothesis. Furthermore, as the figure shows, combing dictionary with comparable corpus is beneficial and Dic&amp;CC approach outperforms the other methods. Wilcoxon signed rank test at 0.05 level of significance determines that the improvement of using comparable cor-pora over dictionary is statistically significant.
 Table 6 shows our main experimental results. The first three rows specify reference comparisons which are monolingual, all query terms for the experiments using dictionary. As the table shows, using comparable corpus with TAN method, we can improve the results over the baselines and achieve up to 43.3% of mean average precision compared to the monolingual re-trieval. In addition, perhaps not surprisingly, Dic-CC and Dic&amp;CC methods which combine two translation resources as de-scribed, appear to be our best performing methods. They expand the vocabulary of the dictionary by adding the extracted translations from comparable corpus. In the best case, we can achieve up to 52.3% of mean average precision of monolingual baseline. The CC-Dic method indicates our translation validity check using outlier detection method. The table only presents that the mean average precision of translating only five terms from dictionary and others with comparable corpus is 45.2% of monolingual baseline which is a big improvement over dictionary only translation.

The best obtained mean average precision using dictionary translations is 0.157 which is only 38% of monolingual. This shows that the poor performance of dictionary only method is not because of the specific method we used. Also, the reported results using dictionary in CLEF-2008 have shown poor performance of dictionary (For example, AleAhmad, Kamalloo, Zareh,
Rahgozar, &amp; Oroumchian (2009b) has reported MAP = 0.124 with top-1 translation of dictionary and MAP = 0.102 with top-5 translations on CLEF-2008 data). The poor performance of the dictionary only method on Persian compared to the similar methods in other languages could be because of the specific characteristics of Persian, which should be investigated more in the future. 5.2.7. Query expansion with pseudo relevance feedback
In our experiments, we observe that the improvements of precisions at top 5 and 10 documents are more considerable than the mean average precisions. Intuitively, when the precision at top documents are high, pseudo relevance feedback can help to improve the mean average precision. So, in another set of experiments, we repeat our experiments using post-translation query expansion with pseudo relevance feedback. Table 7 shows the results of query expansion of monolingual,
CC method and the best CLIR run of using dictionary with comparable corpus. As the table shows, doing query expansion and compared to the monolingual baseline, we can achieve up to 45.7% of mean average precision, 48.2% of precision at 5 doc-uments and 46.1% of precision at 10 documents using only comparable corpus as a translation resource and about 61.8% of mean average precision, 58.6% of precision at 5 documents and 61% of precision at 10 documents using dictionary and com-parable corpus.

Furthermore, in another set of experiments, we did pre-translation query expansion using blind relevance feedback 1998; McNamee &amp; Mayfield, 2002 ). We did pre-translation expansion using the top 20 documents and 5 expanded terms expanded terms using comparable corpus and/or dictionary. But, none of the results were better than our previous obtained other queries suffer from newly added unrelated terms. A deeper research over pre-translation expansion to analyze the re-sults is left as one of our future work. 5.2.8. Comparison with previous work on CLEF-2008
In order to compare our results with the best reported results at CLEF, we repeat our experiments using only the 50 topics of CLEF-2008. Table 8 shows our main experimental results over CLEF-2008 task. To the best of our knowledge, the best re-using only comparable corpus and dictionary is sufficiently better than the best reported result. Rahimi and Shakery (2011) have also done some experiments on CLEF-2008 data. Their main focus is to construct a high quality comparable corpus from which they expect to extract high quality translation knowledge. They have used the translation knowledge extracted from the comparable corpus along with two other translation resources, namely dictionary and translation knowledge extracted from Wikipedia, to do CLIR. 5.2.9. Using INFILE corpus
In our experiments, we used Hamshahri corpus and its bilingual queries to do CLIR. Since the Hamshahri news articles are also used in constructing our comparable corpus, as another interesting research direction, we try to apply our presented methods on a corpus other than Hamshahri to see how our methods perform in another cross-language information retrieval task. We chose INFILE corpus ( Besan X on et al., 2009 ) which consists of Agence France Presse (AFP) X  X  newswires as our eval-uation data set. Using INFILE corpus, our cross-language information retrieval task is to retrieve English documents in re-sponse to Persian queries. The results of applying different query translation methods on the INFILE data set are shown in
Table 9 . Since the results of monolingual query expansion with pseudo relevance feedback were not substantially better than the results without query expansion, we did not report them. As can be seen from the table, same as Hamshahri results in
Table 6 , combining comparable corpus with dictionary can achieve the best results that in the best case is about 51.5% of mean average precision compared to monolingual baseline. Translating 12 terms with dictionary in the CC-Dic method could achieve up to 50.4% of mean average precision compared to monolingual baseline.

As can be seen from Tables 6 and 9 , the relative effectiveness of Dic&amp;CC compared to Dic-CC and CC-Dic changed from one of Dic&amp;CC on Hamshahri and INFILE collections is the number of judged relevant documents in these datasets. There are some topics on INFILE collections that have very few relevant documents. So, the failure of retrieving one of them may have an enormous impact on the final results. As a brief analysis, the average of number of judged relevant documents in Hams-hahri collection is 102, but in INFILE collection is 38. 6. Conclusions and future work
In this work, we presented methods to mine translation knowledge from comparable corpora. The most notable pre-sented method was based on network of terms (TAN) which consists of correlations of terms and their neighborhoods.
Cross-language information retrieval experiments show that using TAN method and selecting dynamic number of transla-tion terms for each query word significantly outperforms the basic translation extraction method. Therefore, neighborhood information of terms could help to obtain more accurate translation terms from comparable corpus.

Furthermore, we have done translation validity check using our proposed outlier detection method and have shown that we can detect mistranslated terms by their neighborhoods. The results show that detecting good outliers and translating only a few number of them with dictionary or even omitting them from the query words improves the retrieval performance.
Also, our experiments in combining extracted translations from comparable corpus with the dictionary translations show that using only dictionary performs poorly, because of lack of proper nouns which are essential query keywords. Thus, one of the success reasons of comparable corpus is the ability to translate these OOV terms. Another advantage of compa-rable corpus is finding good query expansion terms.

In our future work, it will be interesting to use the extracted translation knowledge to improve the quality of the created corpus, by using the extracted term associations as an additional resource to translate source language keywords and also improving its quality through an iterative construction process. In this research we present the TAN method and calculate the similarity of neighborhoods by the paths of lengths one, two and three. Thus as an interesting future direction, we are going to test more complex methods to compare terms X  neighborhoods in the TAN model and run the experiments based on the new extracted translations. Also, we will tune the parameters of the created network to investigate the importance of each source of evidence for reranking the translations.
 References
