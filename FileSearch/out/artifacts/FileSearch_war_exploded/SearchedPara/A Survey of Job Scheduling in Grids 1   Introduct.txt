 Grid computing [1] is emerging as a popular way of providing high performance computing for many data intensive, scientific applications using heterogeneous computing resources. The process of mapping tasks onto grid system consists of assigning tasks to resources available in time and space, minimizing the makespan of the tasks. The available resources, system load and computing power fluctuate in grids because of the heterogeneity, dynami city, and autonomy of it. The problem of mapping tasks onto heterogeneous resources, minimizing the makespan of these tasks, this problem. However, there is no best scheduling algorithm for all grid environments. An alternative is to select a rather better scheduling algorithm to use in a given grid environment because of the heterogeneity among the tasks, machines and network connectivity. 
Although many scheduling techniques for various computing systems exist [3, 4, 5, 6, 7], traditional scheduling systems are inappropriate for scheduling tasks onto grid resources. First, grid resources are geographically distributed and organization (VO) [1], which is a group of consumers and producers united in their secure use of distributed high-end computational resources. Second, these grid resources have decentralized ownership and different local scheduling policies dependent on their VO. Third, the dynamic load and availability of the resources require mechanisms for discovering and charact erizing their status continually [1]. 
Moreover, the failure rate increases when the scale of the grid system becomes and fault-tolerant because of the dynamici ty of the grid environment. The mapping also must be load-balancing-effective to leverage the impact of imbalance caused by the heterogeneity among tasks and machines, and the arrival rate of the tasks. 
Job executions are carried out across the domain boundaries in grids. Realistic platforms for grid systems are facing security threats from network attacks and system vulnerability. To enable more effective job scheduling, the job scheduling algorithm must be security-aware and risk-resilient. Therefore, special mechanisms for security and fault-tolerance are needed [7]. 
Experiments with real applications on real resources are often performed to evaluate the scheduling algorithms. However, modern computing platforms are increasingly distributed and often span multiple administrative domains. Therefore, resource availability fluctuations make it impossible to conduct repeatable experiments for relatively long running applications. Another problem is that the these difficulties with real experimentations, most researchers have resorted to discrete-event simulation. Simulation has been used extensively as a way to evaluate and compare scheduling strategies because simulation experiments are configurable, repeatable, and generally fast [8]. 
In this paper, we discuss several aspects of job scheduling in grids. The rest of the paper is organized as follows: Section 2 presents the basic concepts of job scheduling in grids. In Section 3, some kinds of job scheduling algorithms and models are discussed. In Section 4, we describe and evaluate the fault-tolerant framework of task scheduling in grids. We present the existing study of security assurance for task scheduling in Section 5. In Section 6, we introduce some advances in simulation of grid job scheduling. Finally, we summarize the study of task scheduling in grids and make some remarks on further research in Section 7. i=1,2,3,...,n }, The expected execution time ij e of task i t on machine j m is defined as the wall-clock time at which j m completes i t . Let the arrival time of the task i t be i a , and let the time i t begins execution be i b . From the above definitions, ij i ij e b c + = . The most common objective function of task scheduling problems is makespan. However, on a computational grid, the second optimal makespan may be much longer than the optimal makespan because the computing power of a grid varies over time [9].Consequently, if the performance measure is makespan, there is no approximation proposed which is called Total Processor Cycle Consumption (TPCC), the total number of instructions the grid could compute until the completion time of the schedule. 
The following assumptions are commonly made when evaluating the job scheduling algorithms in grids: (1)A large application has been partitioned to some sub tasks, and the sub tasks are independent. This assumption is commonly made and it is prevalent in the job precedence constraints, or DA G(Directed Acyclic Graph) topologies can be found in [11,12]. (2) The tasks have no deadlines or priorities associated with them. (3) The real-time states of the resources are known. This can be achieved by using some network or grid services like NWS (Network Weather Service) [13] and MDS (Monitoring and Discovery System) [14] when scheduling. (4) The execution time of the tasks is known. These estimates can be supplied proposed a job allocation scheme in distributed systems (TAG) using the Markovian process algebra PEPA where the scheme requires no prior knowledge of job size and had been shown to be more efficient than round robin and random allocation when the job size distribution is heavy tailed and the load is not high (5) Communication delay between sender and receiver are not considered. (6) Service strategy on a host is FCFS (First Comes First Served) [16], and the hosts execute tasks one at a time (7) Sites are cooperative in the grid environments. metrics [6] are commonly used: (1) Makespan : the total running time of all jobs; (2) Scheduling success rate : the percentage of jobs successfully completed in the grid; (3) Grid utilization : defined by the percentage of processing power allocated to user jobs out of total processing power available over all grid sites; (4) Average waiting time : the average waiting time spent by a job in the grid. selected a collection of 11 heuristics and ad apted, implemented, and analyzed them under one set of common assumptions. The 11 heuristics examined are Opportunistic Load Balancing, Minimum Execution Time, Minimum Completion Time, Min-min, Max-min, Duplex, Genetic Algorithm, Simulated Annealing, Genetic Simulated Annealing, Tabu, and A*. It is shown that for the cases studied, the relatively simple Min-min heuristic performs well in comparison to the other techniques. Braun et al. [3] also proposed two types of mapping heuristics, immediate mode and batch mode heuristics. The immediate mode dynamic heuristics consider task affinity for different machines and machine ready times. The batch mode dynamic heuristics consider these factors, as well as aging of tasks waiting to execute. The simulation results revealed that the choice of which dynamic mapping heuristic to use in a given heterogeneous environment depends on parameters such as [3] (a) the structure of the heterogeneity among tasks and machines, and (b) the arrival rate of the tasks. Thus, in a real grid system, there must be several scheduling algorithms to be selected in coordination by a centralized scheduler may be unrealistic. Beaumont et al. [17] presented decentralized schedulers that use only local information at each participating resource. 
Buyya [18] identified challenges in managing resources in a grid computing environment and proposed computational economy as a metaphor for effective management of resources and application scheduling. The literature also identified distributed resource management challenges and requirements of economy-based grid systems, and discussed various representative economy-based systems, both historical and emerging, for cooperative and competitive trading of resources such as CPU cycles, storage, and network bandwidth. 
Due to the development of new applications and the increasing number of users with diverse needs, providing users with quality of service (QoS) guarantees while executing applications has become a crucial problem that needs to be addressed [5]. This problem is referred to as the QoS-based scheduling problem and proved to be NP-hard. Dogana et al. [5] investigated the problem of scheduling a set of independent tasks with multiple QoS needs, which may include timeliness, reliability, security, data accuracy, and (QSMTS_IP) which assumes time-invariant penalty functions is developed. 
In order to satisfy the QoS requirements of tasks, the status of the grid systems must be monitored and the performance data should be recorded, such as resource matching of grid sites and QoS requirements will consume a large amount of computation and communication overhead. This is not acceptable for a low-end grid. Thus, efficient monitoring and discovering technologies must be developed. However, this exceeds the scope of scheduling. 
In a real life situation, asking the grid users to fully specify their QoS requirements such as security demand is an unreasonable burden. For example, job user only need to specify a security level such as low, middle, or high when submitting a remote job rather than the numerical value of the security conditions. Thus, how to evaluate the qualitative analysis and quantitative analysis is a key factor that impacts the matching of user jobs and grid sites heavily. controlled or locally controlled environment. The failure rate grows higher when the scale of the grid becomes larger. And the whole application will fail due to some key tasks or sites failure. This is not acceptable for some small granularity, large scale and long running grid applications. Consequently, the mapping of tasks onto grid sites must be adaptive and fault-tolerant because of the dynamicity of it. 
Hwang et al. [19] presented a failure detection service (FDS) and a flexible failure handling framework (Grid-WFS) as a fault tolerance mechanism on the grid. The FDS enables the detection of both task crashes and user-defined exceptions. A notification mechanism is proposed which is based on the interpretation of notification messages being delivered from the underlying grid resources. The paper also described how to achieve the flexibility by the use of workflow structure as a high-level recovery policy specification, which enables support for multiple failure recovery techniques, the separation of failure handling strategies from the application code, and user-defined exception handlings. A resource monitoring center is always used to provide performance information. A central server collects performance information from all other sites periodically and will communicate the execution status of the job replications to the monitoring center periodically. However, the centralized monitoring can't scale well when the number of grid sites increases. One solution to this problem is to provide monitoring knowledge that correlates alarms and provides fewer, but more meaningful advisory information. Moreover, such a solution could reduce operator load further by taking automatic corrective action [20]. 
On failure there are a number of different possibilities: the entire queue may be lost from the system if they continue to be sent to a broken node. However, most existing models assume that the presence of breakdowns is immediately known by any router or scheduler directing jobs to a queue. Thus, communication latency must be tolerated in scheduling. 
Job replications [6, 10] are commonly used to provide fault-tolerant scheduling in grids. However, existing job replication algorithms use a fixed-number replication. Abawajy [10] presented a Distributed Fault-Tolerant Scheduling (DFTS) to provide fault tolerance for job execution in a grid environment. Their algorithm uses fixed number replications of jobs at multiple sites to guarantee successful job executions. Fixed-number replications in scheduling strategies may utilize excessive hosts or resources. This makes the makespan and average waiting time of tasks rather longer. Thus an adaptive replication strategy is nece ssary in a real grid with dynamic security level. In a large-scale grid, job executions are usually carried out between many virtual organizations in business applications or scientific applications for faster execution or remote interaction. However, security is a main hurdle to make the job scheduling usage, its resources may not be accessible from remote sites. Thus, the jobs scheduled Unfortunately, most of the existing proposed scheduling algorithms had ignored the security problem while scheduling jobs onto geographically distributed grid sites, with a handful of exceptions. In a real life scenario, security threats always exist and the jobs are subject to failures or delays caused by infected hardware, software vulnerability, and distrusted security policy [6]. Consequently, the assumption that the grid environments are safe and the resources are 100% reliable is no longer applicable for job scheduling in real grids. 
Arenas [22] presented an overview of the different concepts and technologies existing mechanisms for managing trust and security. 
Song et al. [6] developed three risk-resilient strategies, preemptive, replication, and delay-tolerant to provide security assurance. In addition to extending from known scheduling heuristics, they developed a new space-time genetic algorithm (STGA) based on faster searching and protected chromosome formation. Song et al. [23, 24] developed a security-binding scheme through site reputation assessment and trust integration across grid sites. They applied fuzzy theory to handle the fuzziness or uncertainties behind all trust attributes. The binding is achieved by periodic exchange of site security information and matchmaking to satisfy user job demands. 
The USC GridSec [25] project develops distributed security infrastructure and self-defense capabilities to secure wide-area networked resource sites participating in a grid application. It proposes a grid security infrastructure including trust modeling, security-binding methodology, and defense architecture against intrusions, worms, system-wide security that enables fine-grained access control within systems in which third party web services are deployed. In that paper, the OGSA-DAI grid services were secured via XACML-based access control policies. 
Matching of security demand and trust level is commonly used to provide security reputation model) for job scheduling. A too simple trust model is not reliable enough and a too complicated trust model will consume excessive computation and communication. Simulation has been used extensively as a way to evaluate and compare scheduling strategies because simulation experiments are configurable, repeatable, and generally methodology used for scheduling research: ( 1) there is no simulation standard in the scheduling research community; and (2) traditional models and assumptions about computing platforms are no longer valid for modern platforms because the simplistic network models used in most scheduling literature do not hold for modern computing platforms. Consequently, there is a need for a simulation framework designed for conducting research on distributed application scheduling. And Legrand [8] outlined the objectives that a useful framework must have: (1) good usability; (2) possibility to run fast simulations;(3) possibility to build configurable, tunable, and extensible simulations; and (4) scalability and the sustaining of simulations with tens of thousands of resources and application tasks. 
When simulation is conducted for a grid job scheduling, the corresponding modeling must be constructed, such as grid instance model, network model, hosts model and task model. Zanikolas et al.. [27] described a grid instance along the PCs, Clusters, SCs, and other special-purpose instruments; and (7) the distribution and characteristics of resource types within hosts. 
Networks are immensely complex due to diversity at all levels: end hosts with various implementations of a TCP/IP protocol stack, communication via diverse network devices over different mediums of various characteristics [8]. Moreover, the distribution of Internet connection capacity of grid sites and that of hosts among sites considerable number of hosts; thus, a uniform distribution seems appropriate. Actually, in a real grid, there are a considerably smaller number of high performance sites. 
Bolosky et al. [28] constructed an empirical model for downtime intervals of hosts in a corporate environment. The proposed model is a mixture of two uniform distributions for 14-and 64-hour downtime intervals, and a gamma distribution for the hosts that do not have a cyclical availability behavior. In general, almost all algorithms assume that jobs arrive in the queue in a negative-exponentially distributed. In a real grid, jobs may have QoS requirements, such as timeliness, priority, and security demands [6].Thus, these QoS requirements must be quantified based on some tasks model. 
Through the above we can see that the modeling of hosts and jobs (or tasks) is not accurate or precise. And the proposed model can only be used for qualitative analysis, need massive data on site B to compute and the computational results may be transferred to site C for visualization. The modeling of network connectivity is critical for efficient computation and remote interaction. In the past few years, grids have emerged as an important methodology to utilize dispersed resources over the Internet or local area network. Grids have shown global popularity and harnessed the aggregate computing power to hundreds of TFlops. Thus, scalability must be taken into account when scheduling jobs in a large-scale grid. 
Job dependency is also a key factor that must be considered for job scheduling. In a large distributed environment, some jobs must be executed sequentially. For example, in a real lift scenario, job X must be executed before the execution of job Y. How to quantitate the QoS requirements of jobs is a main challenge for job scheduling. The Summarily, the performance of job scheduling could be improved if the modeling of jobs, network connectivity, and hosts (or grid sites), gets more accurate and precise. Acknowledgments. The funding support of this work by Innovation Fund of Huaz-No. HF04012006271 is appreciated. 
