 As the volume of online documents has drastically increased, the analysis of topic bursts, topic drift or detection of topic is a practical problem attract-ing more and more attention (Allan et al., 1998; Swan and Allan, 2000; Allan, 2003; Klinken-berg, 2004; Lazarescu et al., 2004; Folino et al., 2007). The earliest known approach is the work of Klinkenberg and Joachims (Klinkenberg and Joachims, 2000). They have attempted to han-dle concept changes by focusing a window with documents sufficiently close to the target concept. Mane et. al. proposed a method to generate maps that support the identification of major re-search topics and trends (Mane and Borner, 2004). The method used Kleinberg X  X  burst detection al-gorithm, co-occurrences of words, and graph lay-out technique. Scholz et. al. have attempted to use different ensembles obtained by training sev-eral data streams to detect concept drift (Scholz, 2007). However the ensemble method itself re-mains a problem that how to manage several clas-sifiers effectively. He and Parket attempted to find bursts, periods of elevated occurrence of events as a dynamic phenomenon instead of focusing on ar-rival rates (He and Parker, 2010). However, the fact that topics are widely distributed in the stream of documents, and sometimes they frequently ap-pear in the documents, and sometimes not often hamper such attempts.

This paper proposes a method for detecting topic over time in series of documents. We rein-forced words related to a topic with low frequen-cies by collecting documents from the corpus, and applied Latent Dirichlet Allocation (LDA) (Blei et al., 2003) to these documents in order to ex-tract topic candidates. For the results of LDA, we applied Moving Average Convergence Divergence (MACD) to find topic words while He et. al. , ap-plied it to find bursts. The MACD is a technique to analyze stock market trends (Murphy, 1999). It shows the relationship between two moving av-erages of prices modeling bursts as intervals of topic dynamics, i.e. , positive acceleration. Fuku-moto et. al also applied MACD to find topics. However, they applied it only to the words with high frequencies in the documents (Fukumoto et al., 2013). In contrast, we applied it to the topic candidates obtained by LDA.

We examined our method by extrinsic evalua-tion, i.e. , we applied the results of topic detection to extractive multi-document summarization. We assume that a salient sentence includes words re-lated to the target topic, and an event of each doc-uments. Here, an event is something that occurs at a specific place and time associated with some specific actions(Allan et al., 1998). We identified event words by using the traditional tf  X  idf method applied to the results of named entities. Each sen-tence in documents is represented using a vector of frequency weighted words that can be event or topic words. We used Markov Random Walk (MRW) to compute the rank scores for the sen-tences (Page et al., 1998). Finally, we selected a certain number of sentences according to the rank score into a summary. 2.1 Extraction of Topic Candidates LDA presented by (Blei et al., 2003) models each document as a mixture of topics (we call it lda topic to discriminate our topic candidates), and generates a discrete probability distribution over words for each lda topic. The generative pro-cess for LDA can be described as follows: 1. For each topic k = 1,  X  X  X  , K , generate  X  k , 2. For each document d = 1,  X  X  X  , D , generate  X  d , 3. For each word n = 1,  X  X  X  , N d in document d ; Like much previous work on LDA, we used Gibbs sampling to estimate  X  and  X  . The sampling prob-ability for topic z i in document d is given by: z \ i refers to a topic set Z , not including the cur-rent assignment z i . n v in topic j that does not include the current assign-ment z i , and n  X  dimension. W refers to a set of documents, and T denotes the total number of unique topics. After a sufficient number of sampling iterations, the ap-proximated posterior can be used to estimate  X  and  X  by examining the counts of word assignments to topics and topic occurrences in documents. The approximated probability of topic k in the docu-ment d ,  X   X  k k ,  X 
We used documents prepared by summarization tasks, NTCIR and DUC data as each task consists of series of documents with the same topic. We applied LDA to the set consisting of all documents in the summarization tasks and documents from the corpus. We need to estimate the appropriate number of lda topic.

Let k  X  be the number of lda topics and d  X  be the number of topmost d  X  documents assigned to each lda topic. We note that the result obtained by LDA can be regarded as the two types of clus-tering result shown in Figure 1: (i) each cluster corresponds to each lda topic (topic id0, topic id1  X  X  X  in Figure 1), and each element of the clusters is the document in the summarization tasks (task1, task2,  X  X  X  in Figure 1) or from the corpus (doc in Figure 1), and (ii) each cluster corresponds to the summarization task and each element of the clus-ters is the document in the summarization tasks or the document from the corpus assigned topic id. For example, DUC2005 consists of 50 tasks. Therefore the number of different clusters is 50. We call the former lda topic cluster and the latter task cluster. We estimated k  X  and d  X  by using En-tropy measure given by: l refers to the number of clusters. P ( A i , C j ) is a probability that the elements of the cluster C j as-signed to the correct class A i . N denotes the total number of elements and N j shows the total num-ber of elements assigned to the cluster C j . The value of E ranges from 0 to 1, and the smaller value of E indicates better result. Let E topic and E task are entropy value of lda topic cluster and task cluster, respectively. We chose the parame-ters k  X  and d  X  whose value of the summation of E topic and E task is smallest. For each lda topic, we extracted words whose probabilities are larger than zero, and regarded these as topic candidates. 2.2 Topic Detection by MACD The proposed method does not simply use MACD to find bursts, but instead determines topic words in series of documents. Unlike Dynamic Topic Models (Blei and Lafferty, 2006), it does not as-sume Gaussian distribution so that it is a natural way to analyze bursts which depend on the data. We applied it to extract topic words in series of documents. MACD histogram defined by Eq. (6) shows a difference between the MACD and its moving average. MACD of a variable x t is defined by the difference of n 1 -day and n 2 -day moving averages, MACD( n 1 , n 2 ) = EMA( n 1 ) -EMA( n 2 ). Here, EMA( n i ) refers to n i -day Exponential Mov-ing Average (EMA). For a variable x = x ( t ) which has a corresponding discrete time series x = { x t | t = 0,1,  X  X  X } , the n -day EMA is defined by Eq. (5).  X  refers to a smoothing factor and it is often taken to be 2
The procedure for topic detection with MACD is illustrated in Figure 2. Let A be a series of doc-uments and w be one of the topic candidates ob-tained by LDA. Each document in A is sorted in chronological order. We set A to the documents from the summarization task. Whether or not a word w is a topic word is judged as follows: 1. Create document-based MACD histogram 2. Create term-based MACD histogram where 3. We assume that if a term w is informative 3.1 Event detection An event word is something that occurs at a spe-cific place and time associated with some spe-cific actions (Allan, 2003; Allan et al., 1998). It refers to notions of who(person), where(place), when(time) including what, why and how in a doc-ument. Therefore, we can assume that named en-tities(NE) are linguistic features for event detec-tion. An event word refers to the theme of the document itself, and frequently appears in the doc-ument but not frequently appear in other docu-ments. Therefore, we first applied NE recogni-tion to the target documents to be summarized, and then calculated tf  X  idf to the results of NE recogni-tion. We extracted words whose tf  X  idf values are larger than a certain threshold value, and regarded these as event words. 3.2 Sentence extraction We recall that our hypothesis about key sentences in multiple documents is that they include topic and event words. Each sentence in the docu-ments is represented using a vector of frequency weighted words that can be event or topic words.
Like much previous work on extractive sum-marization (Erkan and Radev, 2004; Mihalcea and Tarau, 2005; Wan and Yang, 2008), we used Markov Random Walk (MRW) model to compute the rank scores for the sentences. Given a set of documents to be summarized, G = ( S , E ) is a graph reflecting the relationships between two sentences. S is a set of vertices, and each vertex s in S is a sentence. E is a set of edges, and each edge e ij in E is associated with an affinity weight f ( i  X  j ) between sentences s i and s j ( i 6 = j ). The affinity weight is computed using cosine measure between the two sentences, s i and s j . Two ver-tices are connected if their affinity weight is larger than 0 and we let f ( i  X  i ) = 0 to avoid self tran-sition. The transition probability from s i to s j is then defined as follows: We used the row-normalized matrix U ij = ( U ij ) | S | X | S | to describe G with each entry corre-sponding to the transition probability, where U ij = p ( i  X  j ) . To make U a stochastic matrix, the rows with all zero elements are replaced by a smoothing vector with all elements set to 1 tion matrix is given by formula (8), and each score of the sentence is obtained by the principal eigen-vector of the matrix M .
 We selected a certain number of sentences accord-ing to rank score into the summary. 4.1 Experimental settings We applied the results of topic detection to ex-tractive multi-document summarization task, and examined how the results of topic detection af-fect the overall performance of the salient sen-tence selection. We used two tasks, Japanese and are (i) MRW model ( MRW ): The method ap-plies the MRW model only to the sentences con-sisted of noun words, (ii) Event detection ( Event ): The method applies the MRW model to the result of event detection, (iii) Topic Detection by LDA ( LDA ): MRW is applied to the result of topic can-didates detection by LDA and (iv) Topic Detec-tion by LDA and MACD ( LDA &amp; MACD ): MRW is applied to the result of topic detection by LDA and MACD only, i.e. , the method does not include event detection. 4.2 NTCIR data The data used in the NTCIR-3 multi-document summarization task is selected from 1998 to 1999 of Mainichi Japanese Newspaper documents. The gold standard data provided to human judges con-sists of FBFREE DryRun and FormalRun. Each data consists of 30 tasks. There are two types of correct summary according to the character length,  X  X ong X  and  X  X hort X , All series of documents were tagged by CaboCha (Kudo and Matsumoto, 2003). We used person name, organization, place and proper name extracted from NE recognition (Kudo and Matsumoto, 2003) for event detection, and noun words including named entities for topic de-tection. FBFREE DryRun data is used to tuning parameters, i.e. , the number of extracted words ac-cording to the tf  X  idf value, and the threshold value of KL-distance. The size that optimized the aver-age Rouge-1(R-1) score across 30 tasks was cho-sen. As a result, we set tf  X  idf and KL-distance to 100 and 0.104, respectively.

We used FormalRun as a test data, and another set consisted of 218,724 documents from 1998 to 1999 of Mainichi newspaper as a corpus used in Figure 3: Entropy against the # of topics and doc-uments Table 1: Sentence Extraction (NTCIR-3 test data) and d  X  in LDA, i.e. , we searched k  X  and d  X  in steps of 100 from 200 to 900. Figure 3 illustrates en-tropy value against the number of topics k  X  and documents d  X  using 30 tasks of FormalRun data. Each plot shows that at least one of the docu-ments for each summarization task is included in the cluster. We can see from Figure 3 that the value of entropy depends on the number of doc-uments rather than the number of topics. From the result shown in Figure 3, the minimum entropy value was 0.025 and the number of topics and doc-uments were 400 and 300, respectively. We used them in the experiment. The summarization re-sults are shown in Table 1.
 Table 1 shows that our approach,  X  X vent &amp; Topic X  outperforms other baselines, regardless of the summary type (long/short). Topic candidates include surplus words that are not related to the topic because the results obtained by  X  X DA X  were worse than those obtained by  X  X DA &amp; MACD X , and even worse than  X  X vent X  in both short and long summary. This shows that integration of LDA and MACD is effective for topic detection. 4.3 DUC data We used DUC2005 consisted of 50 tasks for train-ing, and 50 tasks of DUC2006 data for testing in order to estimate parameters. We set tf  X  idf and Table 2: Comparative results (DUC2007 test data) KL-distance to 80 and 0.9. The minimum en-tropy value was 0.050 and the number of topics and documents were 500 and 600, respectively. 45 tasks from DUC2007 were used to evaluate the performance of the method. All documents were tagged by Tree Tagger (Schmid, 1995) and 2005). We used person name, organization and lo-cation for event detection, and noun words includ-ing named entities for topic detection. AQUAINT are used as a corpus in LDA and MACD. Table 2 shows Rouge-1 against unigrams.

We can see from Table 2 that Rouge-1 obtained by our approach was also the best compared to the baselines. Table 2 also shows the performance of other research sites reported by (Celikylmaz and Hakkani-Tur, 2010). The top site was  X  X ybH-Sum X  by (Celikylmaz and Hakkani-Tur, 2010). However, the method is a semi-supervised tech-nique that needs a tagged training data. Our ap-proach achieves performance approaching the top-performing unsupervised method,  X  X TM X  (Ce-likylmaz and Hakkani-Tur, 2011), and is compet-itive to  X  X YTHY X  (Toutanoval et al., 2007) and  X  X PAM X  (Li and McCallum, 2006). Prior work including  X  X TM X  has demonstrated the usefulness of semantic concepts for extracting salient sen-tences. For future work, we should be able to obtain further advantages in efficacy in our topic detection and summarization approach by disam-biguating topic senses. The research described in this paper explores a method for detecting topic words over time in se-ries of documents. The results of extrinsic evalu-ation showed that integration of LDA and MACD is effective for topic detection.
