 REGULAR PAPER Imad Rahal  X  Dongmei Ren  X  Weihua Wu  X  Anne Denton  X  Christopher Besemann  X  William Perrizo Abstract Graphs are increasingly becoming a vital source of information within which a great deal of semantics is embedded. As the size of available graphs in-creases, our ability to arrive at the embedded semantics grows into a much more complicated task. One form of important hidden semantics is that which is em-bedded in the edges of directed graphs. Citation graphs serve as a good example in this context. This paper attempts to understand temporal aspects in publication trends through citation graphs, by identifying patterns in the subject matters of scientific publications using an efficient, vertical association rule mining model. Such patterns can (a) indicate subject-matter evolutionary history, (b) highlight subject-matter future extensions, and (c) give insights on the potential effects of current research on future research. We highlight our major differences with previ-ous work in the areas of graph mining, citation mining, and Web-structure mining, propose an efficient vertical data representation model, introduce a new subjective interestingness measure for evaluating patterns with a special focus on those pat-terns that signify strong associations between properties of cited papers and citing papers, and present an efficient algorithm for the purpose of discovering rules of interest followed by a detailed experimental analysis.
 Keywords Citation analysis  X  Citation graphs  X  Association rule mining  X  Frequent itemset mining  X  Data mining  X  Graph databases  X  Link analysis  X  P-trees 1 Introduction From a data mining perspective, this paper attempts a special kind of associa-tion rule mining (ARM) [ 1 , 2 ]. The standard ARM, when applied to scientific publications, would consider every publication as a transaction each subsuming a variable number of items drawn from a predefined item space. Traditionally, ARM has been applied to shopping carts or market baskets that contain combi-nations of items bought during a single visit to a store. In this context, the item space is the set of all items that are for sale in a store. Borrowing this concept over to the publications context, the presence of a property, such as a subject matter, could be viewed as an item. In this paper, we view the set of all subject matters, present in the publications represented by the citation graph under consideration, as a well-defined item space where a certain publication can have more than one subject matter (which is usually the case). The user is advised that we will use the terms  X  X roperty, X   X  X ubject, X  and  X  X ubject matter X  interchangeably hereinafter. of the citation graph, applying ARM could only highlight which subject matters are most commonly covered in conjunction with which others. To learn about the future impact of publications or their evolutionary history, we have to relate their properties to the properties of papers that are citing them. The cited and citing papers are referred to as citee and citer papers, respectively, hereinafter. problem of association rule mining. In Sects. 3 and 4 ,wegiveanoverviewof the current research literature on Web-structure mining, citation mining and graph mining, and show how our work is fundamentally different. Section 5 discusses the vertical data model proposed for citation-graph data. In Sect. 6 , we introduce a new subjective interestingness measure and compare it with other similar mea-sures in the literature; we also present the notions of intra-and inter-node rules. Section 7 gives a detailed description of our algorithm along with a performance analysis study and an investigation of the results. Finally, Sect. 8 concludes this work with potential future extensions. 2 Association rule mining Formally speaking, the problem of association rule mining can be stated as fol-lows: Let I be a set of items (collectively referred to as the item space), and D be a set of transactions (cardinality is | D | ) each containing a list of items. An itemset is a set of items from I (we refer to it as a k -itemset when it contains k items). The number of transactions containing all the items of an itemset is defined to be the support of the itemset [ 1 , 2 ]. Given an absolute support threshold, minisupp ,where 0 &lt; minisupp  X | D | , an itemset X from I is said to be frequent with respect to D and minisupp , if and only if the support of X is greater than or equal to minisupp . which is associated with a confidence measure [ 1 , 2 ] greater than or equal to a minimum pre-specified confidence threshold, miniconf , where 0  X  miniconf  X  1 (in which case we say that the rule is confident ). An association rule is a rule of the form A  X  C where A , the antecedent of the rule, and C , the consequent of the rule, are itemsets in I . The confidence of an association rule is defined as the support of the itemset formed by the union of A and C divided by the support of A (i.e., it gives the proportion of transactions in D containing A that also contain C ) . Association rule mining finds all confident rules from all frequent itemsets (i.e., all strong rules). 3 Web-structure and citation mining Our work is similar in essence to other research in the broad areas of Web-structure mining and citation mining. The area of citation analysis has been studied in in-formation retrieval (IR) for a number of purposes, one of which is the discovery of influential journals by assigning journals  X  X nfluence weights X  [ 10 ] based on the number of other influential journals citing them.
 newer context of the World Wide Web. Formally, Web-structure mining is the process of discovering influential and  X  X uthoritative X  pages over the Web. Two types of Web pages can be distinguished: authorities and hubs. Authorities are Web pages that are linked to by many other pages because they contain important information; while, hubs are Web pages that contain links to many authorities on some subject. Kleinberg [ 23 ] proposes an approach for analyzing the link structure of the World Wide Web and finding information of  X  X igh-quality X  in response to broad-topic search queries which can have thousands of potentially relevant Web-page matches. In order to provide effective searching for users, only the subset of authoritative pages on the topics of interest is returned. Chakrabati et al. [ 7 ] discuss a newer approach for the same purpose that is based on Kleinberg [ 23 ]. bines citation bibliometrics and text-mining. Their work focuses on analyzing and documenting the impacts of research on the development of real-life applications, technology, and other research over time along with the pathways through which this can be achieved; in addition, it attempts to recognize and highlight the differ-ent characteristics related to the user population. In other words, the work stud-ies the different impacts of research along with the impacted population. They represent the studied research by papers indexed by the Science Citation Index database 1 and the user population by other papers in the database citing the stud-ied research papers as well as their future extensions. Citation bibliometrics [ 30 ]is utilized to highlight the characteristics of the impacted user population by analyz-ing the user papers. The text-mining [ 40 ] component of their approach analyzes the relations among technical areas in the population papers and between them and other areas in the research papers using intelligent and feasible text-mining. [ 23 ] and Chakrabati et al. [ 7 ], has focused primarily on discerning entities such as journals, papers or Web pages that are deemed influential to or authoritative on certain topics of interest. In contrast, our work has a totally different entity of focus, the publication subject matter, which could be viewed as the input query in Web-structure mining. In addition, our work differs from Kostoff et al. X  X  [ 25 ] and other citation-mining research in that we attempt to discover research trends by understanding the semantics hidden in the edges of citation-graph data. Those edges are used to relate publications, represented by their corresponding subject matters, where edge directionality embodies an important time element. We adopt a novel approach that utilizes a popular data-mining technique to arrive at rules as-sociating subject matters from publications written at different points in time. The objective of our study is to discover rules capable of uncovering subject-matter history (i.e. the subject matters from which a given subject matter extended) and extensions or evolution over time (i.e. the subject matters which have been im-pacted by a given subject matter) as well as providing a  X  X otential X  framework for predicting likely future subject matters that could be affected by current research, where and when applicable. As will be discussed later, a vertical data represen-tation model is employed to enable the possible analysis of huge citation graphs where other approaches might fail. We also propose a new subjective interesting-ness measure to guide the search through the enormous rule space in the quest for the subset of interest. 4 Graph mining A particular interest in graph data is that of interactions. Many objects in the world have special interaction relationships where they influence one another. One area in which interactions are involved is the study of proteomics in biology [ 39 ]. Most work in a cell is not isolated; rather, it is done by interacting proteins. Some of the many techniques developed for protein interaction analysis include binary net-works [ 3 ], Bayesian networks [ 39 ], and support vector machines [ 6 ]; nevertheless, there is still room (and apparently need) for the application of a wide range of new graph-based techniques.
 action and what defines the support of an itemset depends on the utilized graph model. In some instances, a transaction can be viewed as an individual graph and the support of an itemset (which is a sub-transaction or a sub-graph) is based on the number of transactions (or graphs) in the graph database [ 20 ] supporting the itemset. The task of finding frequent patterns is then represented as frequent sub-graph or sub-transaction searching. Searching occurs by comparing sub-graphs to check for similarity, a problem defined as the sub-graph isomorphism problem which is known to be NP-complete [ 20 , 26 ]. To alleviate the complexity of match-ing sub-graphs, various heuristics and methods are utilized. Common among these is a form of canonical labeling, which allows label codes attached to graph nodes or edges to be compared rather than the actual graph structures themselves which invariably takes less time [ 20 , 26 ]; however, it is widely accepted that methods using such labeling techniques suffer when the data has few unique labels. Two popular algorithms for frequent itemset generation applied to graph data are sum-marized next. Other algorithms can be found in studies by Matsuda et al. [ 29 ], and YanandHan[ 50 ].
 substructures or sub-graphs. In AGM, graphs take the shape of adjacency matrices. Canonical matrix coding is used in order to alleviate the isomorphism problem. Candidate generation [ 1 , 2 ] occurs when two sub-graph matrices share all but the last column. These two matrices are joined to form the next level candidate, which has all the common columns in addition to the differing ones. This is in direct comparison to the way itemsets are joined in typical Apriori-based algorithms to produce larger candidate frequent itemsets. After generating a candidate sub-graph, the graph set is then scanned to determine its support.
 Apriori. FSG seeks to limit the space by only considering connected sub-graphs. A sparse graph representation is used to reduce storage and computational costs. As with AGM, FSG adds one edge at a time during candidate generation. However, it was noted more prominently that each join for a candidate does not necessarily result in a single candidate as is the case in classical ARM.
 the graph structure. Node information played a secondary role, if any, in the pro-cess. Our approach, on the other hand, is centered on the data available in the graph nodes. Graph structures are exclusively used to relate information between nodes connected by citation edges. Some work in bioinformatics have used a sim-ilar approach but with a main focus on biological aspects [ 31 , 38 ]. In addition, because we are dealing with citation data, only directed graph (digraph) structures are considered in our work where nodes represent papers and edges represent ci-tation relationships. Albeit such graphs are usually viewed as acyclic, in practice, they might contain cycles due to factors such as  X  X reprints X  and  X  X elf-citations. X  every citation, we have a directed edge connecting the citer to the citee. Figure 1 depicts part of a citation graph drawn using a modified version of the TouchGraph software [ 46 ]. The figure shows three disconnected components. In the larger component, a paper having property 12.10.  X  g 2 and another paper having prop-erty 11.25.Mj are involved in a citation relationship where the latter paper cites the former (in the figure, citation relationships go from lower to higher nodes). 5 Data representation We utilize a path-based approach to represent graph data in tabular or relational format where we consider every path in the graph as a separate transaction. In the simplest version, we can limit ourselves to paths of length one where every edge results in a single transaction. Such transactions contain information on both par-ticipating nodes. The corresponding ARM problem can then be phrased in terms of the relation that results from twice joining the relation containing the node data of the citation graph with the edge relation. If the frequency of a particular prop-erty is evaluated on this joined table, the result will differ from the frequency in the original relation containing only the node data. In this context, it is vital for the reader to recognize that the concept and existence of frequent itemsets will largely depend on the path length chosen for transactions (recall that we can represent paths of any length as transactions instead of limiting ourselves only to edges or paths of length one). Note that the resulting relation is likely to contain a lot of redundancy, since the information of a particular node is being repeated once for every incoming and outgoing edge.
 erties, P 1  X  X  n , where every publication has a property value of either 1 or 0 de-noting the existence or absence of that property in the corresponding paper, re-spectively. Figure 2 depicts an example of the framework utilized to represent a citation graph as a set of transactions. The first half of this table constitutes the citee set, while the second constitutes the citer set. In this example, the first row shows a paper with (visible) properties P 1 and P 2 being cited by a paper with (visible) property P 2 .
 model to represent the data shown in Fig. 2 . Unlike Zaki and Gouda [ 51 ]who also use bitmaps to encode the existence or absence of properties, our model uses bit-based compression and faster processing operations. Compression is exploited to help assuage problems when the number of edges in the graph (i.e., transactions in our model) grows enormously especially in cases of sparse data. The reader is referred to Sect. 7.1 for more details on how compression is achieved.
 Coatney [ 33 ] model paths in 3-D coordinate graphs, where nodes represent atoms and edges represent chemical bonds, as transactions similar to the way we model graph edges. However, because macromolecules can have an extremely large num-ber of atoms, their corresponding graphs tend to have a massive number of paths to be represented as transactions which can clearly become infeasible. The au-thors also emphasize that the expected number of atoms in interesting patterns is at least five but is usually more than that; as a result, they propose an interest-ing and intuitive approach to reduce the large number of transactions by pruning all atom-pair interactions whose length (i.e., distance between the two interacting atoms) is above a certain user-defined threshold. They justify this step by the fact that the distance between two interacting atoms is inversely proportional to the strength of the chemical bond between them.
 (i.e., edges only). Any efficiency degradation resulting from the increase in the number of transactions can be circumvented by using vertical data processing and compression, as we empirically demonstrate in our experiments particularly over large data sets. In cases where compression does not prove to be very effective, we suggest utilizing temporal constraints on the data by limiting the nodes in the graph to those published between certain dates and ignoring all other nodes along with their incoming and outgoing edges. Another suggestion would be to limit the edges in the graph to those involving nodes containing specific subjects of interest only. Due to time limitations, this paper does not attempt to analyze the effects of those suggestions; such analysis is to be considered in future extensions of this work. 6 Rule interestingness measures 6.1 On interestingness measures A number of studies in the literature have analyzed the notion of interestingness in data mining. In general, interestingness measures for the discovered patterns can fall in one of two classes: objective measures and subjective measures. Objective interestingness measures are data-centric in that they define the interestingness of a pattern in terms of the data used in the mining process. They also highly depend on the structure of the patterns. For example, in ARM, the two ubiquitous objective measures are support and confidence, both of which highlight statistical properties relating to the discovered rules. Due to the many complexities arising in the pattern discovery process, objective measures usually discover a large number of patterns and thus fall short of their purpose 3 especially when the notion of interestingness depends on additional factors such as the decision-maker. above scenario. In general, subjective measures endeavor to generate a smaller, tailored set of patterns that is potentially more interesting and useful to the pattern examiner.
 on two main factors to discover patterns, namely, actionability and unexpected-ness. Actionability states that a pattern is considered interesting to the examiner if it calls for action on his or her behalf. Unexpectedness focuses more on the surprising factor of the pattern with respect to the examiner (i.e., the degree at which the pattern surprises the examiner). In order for the unexpectedness factor to be integrated into a subjective measure, a system of beliefs [ 44 ]mustbede-fined first. Such a system would define the standard knowledge expected by the examiner. The discovery process then captures all deviations from such standards as unexpected and thus as interesting to the examiner.
 edge that the examiner is not willing to change even in the light of newly discov-ered contradictory evidence; the validity of the discovered patterns and sometimes of the original data is questioned instead. On the other hand, soft beliefs could be changed by the examiner if suggested by new patterns. A user-defined measure of strength, referred to as the degree of belief, is usually associated with every belief in the system. A number of subjective interestingness measures for association rules are presented next.
 pected rules in the form of  X  X ule-pairs. X  Their work is domain independent in that it requires no prior knowledge in the form of beliefs against which the unexpect-edness factor is measured.
 beliefs (which are also represented as rules) to arrive at the subset of interesting rules. A rule, R , is considered to be different than a belief, B ,if R and B have similar consequents but very dissimilar antecedents, or vice versa. In this context, their distance-based similarity is based on the synthetic structures of rules and beliefs.
 degree with which it  X  X hakes X  the belief system. Their work emphasizes the im-portance of rule actionability along with the complexity associated with formulat-ing and integrating it into the discovery process. To alleviate this problem, they assume that most actionable rules are unexpected and use rule unexpectedness as the focal point of their interestingness measure. A number of approaches were suggested to define a degree of belief such as the frequency approach, which is limited to beliefs in the form of rules and the conditional approach, which can be applied to more general forms of beliefs and is, thus, chosen by the authors. Their work can be applied to dynamic environments where data changes often thus affecting the degrees of belief and, consequently, the outcome of interesting patterns.
 healthcare insurance claims. The concern here is to find  X  X eviations from the norms X  which can call for corrective actions to reinstate them back to the stan-dard. The actions are pre-defined by domain experts for each class of deviations which is clearly only possible for very domain-specific applications.
 system where beliefs are encoded as rules. Degrees of belief are defined by the examiner and can be updated using a  X  X evision procedure. X  As in the study by Silberschatz and Tuzhilin [ 44 ], they also focus on the unexpectedness factor of interestingness and define a rule, R , to be unexpected with respect to a belief, B , if (1) the antecedents of R and B are logically contradicting, and (2) the number of tuples intersecting R and B (i.e., the subset of tuples where the antecedents of R and B are both true) is  X  X tatistically X  large. In order to arrive at the subset of interesting rules, the authors assume the validity of what they refer to as the  X  X onotonicity of beliefs X  which states that if a belief holds on some data with some degree then it must also hold on large subsets of that data. 6.2 Intra-and inter-node rules Definition 6.1 (Intra-node rule) An intra-node rule is a rule whose antecedent and consequent are properties drawn from either the citer or the citee set of papers but not from both simultaneously, support is greater than the minimum support threshold, and confidence is greater than the minimum confidence threshold. citation graph. Those rules would, however, differ from rules that are derived in the path-based setting described in the previous section, namely, inter-node rules. Definition 6.2 (Inter-node rule) An inter-node rule is a rule whose antecedent is drawn from the citee set of papers, consequent is drawn from the citer set of papers, support is greater than the minimum support threshold, and confidence is greater than the confidence of the corresponding intra-node rule (Definition 6.3). Definition 6.3 (Corresponding intra-node rule) The corresponding intra-node rule of an inter-node rule is a  X  X otential X  rule having the same antecedent as the inter-node rule (drawn from the citee set of papers), a consequent whose proper-ties are the same as those in the consequent of the inter-node rule but drawn from the citee set of papers, and a confidence greater than the minimum confidence threshold.
 graph structure. Here, a rule might have its antecedent drawn from the citer, and its consequent from the citee, or vice versa. Different formats of rules could be derived in this manner, but we will limit ourselves to only one form in which the antecedent is drawn from the citee, while the consequent is drawn from the citer. As we discuss later, this format of rules can give insights into research publication trends by associating properties of papers written at different points in time. thresholds; however, since it is very difficult to estimate a minimum threshold for confidence that would yield interesting rules, we consider an inter-node R to be of interest if there exists a corresponding intra-node rule, R , such that the confidence of R is larger than or equal to the confidence of R . As described in Definition 6.3, R has the same antecedent as R (both drawn for the citee set) along with a consequent having the same properties drawn from the citer set for R and from the citee set for R . Note that in Definition 6.3, we say that a corresponding intra-node rule is a  X  X otential X  rule to emphasize that we are not interested in its support; we just use its confidence, which must exceed the minimum specified threshold, for testing the inter-node rule at hand.
 Citer Prop 9 . This rule associates property 1 from the citee set with the combina-tion of properties 6 and 9 from the citer set. The corresponding intra-node rule, R , would then be R : Citee Prop 1  X  Citee Prop 6 , Citee Prop 9 .
 stronger associations than corresponding intra-node ones which deserves atten-tion. As a result, the reader is advised that the notion of the ubiquitous fixed confidence threshold is substituted with a tailored form derived dynamically and separately for each rule from the confidence of its corresponding intra-node rule. be semantically interpretable; however, surprises may arise. We consider those as a form of unexpected knowledge which could be interesting to the user. From a data-mining perspective, inter-node rules provide valuable knowledge embod-ied in associations between earlier work represented by citee properties and later work represented by citer properties. Given a subject S of interest, the derived as-sociations can be used to show S  X  X  future subject extensions (i.e., what subjects have directly or indirectly extended from S ). This could be done by matching S against the antecedents of the rules and viewing the subjects in the consequents as extensions of S . A rule such as R : Subject X Citee  X  Subject Y Citer says that a considerable number of publications with subject Y cite publications with subject X which could indicate that Y has extended from X . A more concrete example would be a rule stating that subject  X  X atabases X  implies subject  X  X ata mining, X  since the latter can be considered an extension from the former. 4 impact which largely coincides with our previous objective. In addition, the need for such knowledge arises in document categorization systems which group their documents, for navigation purposes, by subject matter, such as the popular Web portal Yahoo! [ 49 ]. The subject matter categorization scheme itself is hierarchical in that a user usually starts with a general subject matter (such as  X  X atabases X ) and from there goes into more specific ones (such as  X  X ata mining X ). The mined rules could help form such hierarchies and sometimes even pinpoint unnoticed hierarchical relationships among subject matters.
 subject matters which, in essence, could be viewed as the opposite of the first. Sev-eral writers and philosophers are trying to understand and analyze current ideas and ideologies of interest by stepping back into history and studying possible ef-fects or impacts from other ideas that were prevalent earlier. In their book, The Nature and Logic of Capitalism [ 18 ], the authors strive to analyze the factors and ideologies that had direct impacts leading to the birth and shaping of Capitalism; viewing Capitalism as a subject matter, our analysis of the published literature could highlight potentially important ideologies and ideas (also represented as subject matters) that date back to the pre-Capitalism era and that had a lot to do with its coming to existence. rules, we can view the subjects in the antecedents as the original subjects from which S extended or derived. The rationale for this is that publications, written later in time, usually follow the trend of citing the original and seminal papers that started a certain research direction. As an example, almost all papers involv-ing subject matter  X  X ssociation rule mining X  usually cite Agrawal X  X  seminal work [ 1 , 2 ] implying that their application areas could be viewed as extensions of the market-basket research (MBR) area. For instance, this is particularly true for this paper where we model citation-graph data as MBR transactions to enable the ap-plication of ARM; as a result, we can view MBR as an original subject which we (and many others) have extended.
 have on future papers. We do realize that this last use might not be applicable all the time; nevertheless, it might provide valuable knowledge when it does. For ex-ample, a rule such as R : Subject X Citee  X  Subject Y Citer , Subject Z Citer might tell us that having a set of papers, S , involving subject matter X implies that future papers involving subject matters Y and Z might cite S with a certain support and confidence ( R  X  X  support and confidence, respectively). Looking at the same issue from a different angle, using inter-node rules such as R may help us in determining what future publication subject matters might get affected by cur-rent publication subject matters. For example, from R , we could conclude that it is probable for current papers with subject matters X to be cited by future papers with subject matters Y and Z ; thus, we can say that subject matter X will probably have an effect on subject matters Y and Z . Notice that, from this last observation, we might also be able to gain more insight on future subjects given current ones. The next paragraph provides a more concrete example.
 and engineering domains; in fact, many mathematical researchers have formu-lated answers and proofs to problems derived in non-mathematical contexts. Such scenarios might lead to producing rules of the form Mathematics Subject X  X  Some Scientific Subject Y , where a mathematical subject matter X has had an im-pact on some scientific subject matter, Y . Such a rule could still be valid later in time especially in the presence of new research in subject matter X with high po-tential impact on Y ; put differently, if the new research in X improves its theory and foundations, such an improvement might reflect on subject matter Y .Forex-ample, the theory of Fourier transformations and analysis from mathematics has been heavily exploited in the signal and image processing domains; as a result, fu-ture advancements in its theory might have considerable future impact on almost all of its applications including those in signal and image processing thus stim-ulating future research in those application areas which certainly validates any matching rules.
 ations between the properties of two sets of papers written at different points in time (the citee set and the citer set) that are more confident than similar associ-ations between properties of the same set of publications. In a sense, our work is similar to Pietracaprina and Zandolin X  X  [ 37 ] in that we also discover specific pairs of rules, where each pair is composed of an inter-node rule along with its corresponding intra-node rule, and use the conditional probability of those rules to decide upon the interestingness of the inter-node rule. Another similarity with Pietracaprina and Zandolin X  X  work [ 37 ] is that we do not utilize a user-defined system of beliefs such as Liu and Hsu [ 28 ], Padmanabhan and Tuzhilin [ 32 ], Silberschatz and Tuzhilin [ 44 ], and Suzuki [ 45 ]. The corresponding intra-node rules are used to set the norms; any inter-node rule that deviates from this norm by having a confidence larger than the confidence of its corresponding intra-node rule is considered unexpected and thus interesting. The rationale for this is based on our belief that, for coexisting subject matters that are often cited together, in general, one usually expects intra-node properties to exhibit the strongest associ-ations because their nodes play the same role in the graph which happens to be the citee role here as we are only focusing on citee intra-node rules (i.e., those subject matters are often cited by the same set of subject matters). If we consider for a moment the derived relation resulting from our data representation, we can quickly realize that, in order for an intra-node rule to a have a high confidence value, the subject matters participating in the rule consequent must exist together with those in the antecedent part quite often; and for that to happen, those subjects must coexist together in the same publications. In view of the fact that we are more interested in understanding a subject matter X  X  evolutionary history, exten-sions and potential predictivity, the time element plays a crucial role in our notion of interestingness which coexisting subjects do not satisfy. As a result, we view inter-node rules that are not stronger than their corresponding intra-node rules as not interesting in our context.
 subject matter S Citer to cite a set of publications P Citee involving another subject matter S Citee where S Citer is not prominently spread in P Citee , S Citer , in its current form, ought to be rather newer than S Citee which explains its scarcity in P Citee .For example, a large number of publications on  X  X ata mining X  cite the  X  X achine learn-ing X  literature which is an older subject matter forming one of the roots of  X  X ata mining. X  Inter-node associations adhering to this justification can clearly highlight subject-matter evolutionary history as well as future extensions as discussed pre-viously.
 would give the corresponding associations an interdisciplinary value. A good clar-ifying example would be the  X  X ash Equilibrium X  work that the renowned Dr. John Nash did at Princeton University in area of game theory in 1950 which awarded him the Nobel Prize in economics in 1994. Almost all economics literature on equilibrium cites Nash X  X  work or its extensions even though economics and game theory are disciplinary unrelated fields of research.
 a  X  X ot X  research subject matter after the introduction of S Citee . In the event that this is true, a reasonable justification for the observed citation phenomenon could be that research in S Citee has lead to important advancements or findings with high applicability to S Citer . Biological research started way before the introduc-tion of computers; however, due to the advancements in the fields of data mining and machine learning, a large number of publications in biology, especially those focusing on the  X  X n-silico X  analysis of biological data (i.e., through the use of sil-icon ships or computers) [ 39 ], cite the newer data-mining and machine-learning literatures.
 sure discovers unexpected rules capable of highlighting subject matter extensions and evolutionary history which also could give insights on the potential effects of current research on future research where applicable. We focus largely on the unexpectedness factor of interestingness simply because, at this stage, we are largely interested in understanding the semantics embedded in citation graphs. This is in direct comparison with Silberschatz and Tuzhilin [ 44 ] who also limit their definition of interestingness to the unexpectedness factor but only because the actionability factor is hard to formulate and integrate into the discovery process. 7 The proposed mining approach 7.1 The P-tree technology Our implementation for this work is based on a new and efficient data structure, the P-tree 5 (Predicate or Peano tree). P-trees are tree-like data structures that store relational data in a loss-less bit-compressed column-wise format by splitting each attribute into bits, grouping together bits at each bit position, and representing each bit group by a P-tree. P-trees provide a lot of information and are structured to facilitate fast data-mining processes [ 9 , 34 ]  X  an aspect greatly appreciated when dealing with vast amounts of raw data as is the case in almost all data-mining applications.
 tional format, though we will limit ourselves to the former as this happens to be the format of the data with which we are dealing in this paper. Note that data par-titioning here is attribute-(or column-) based and not row-based as is the case in the ubiquitous relational data representation.
 binary attribute for all the transactions separately. In other words, we group all bits, for all transactions t in the table, in each binary attribute, separately. Each such group of bits is called a bit group. Figure 3 shows the process of creating P-trees from a binary relational table. Part (b) of the figure shows the creation of two bit groups, one for each attribute in (a). Parts (c) and (d) show the resulting P-trees, P 1 and P 2 , one for each of the bit groups in (b).
 Each P-tree records the total number of 1s in the corresponding bit group on the root level. The second level in the tree gives the number of 1s in each of the halves of the bit group. The first node from the left on the second level gives the number of 1s in the first half of the bit group, and similarly, the second node gives the number of 1s in the second half of the bit group. This logic is continued throughout the tree with each node giving the number of 1s in either the first or the second half (depending on whether it is the left or right node) of the bit group represented by the parent node.
 bit group. The second level of P 2 contains the 1-bit counts of the two halves sepa-rately, 4 and 2. Since the first half contains only 1s, it is considered pure (referred to as pure-1) and thus there is no need to partition it further. This last aspect is called P-tree compression [ 9 , 34 ] and is one of the most important characteristics of the P-tree technology. Similarly, the second half, 2, is pure-1 and need not be partitioned further. By the same token, nodes representing halves containing pure-0 and are not partitioned further.
 Fig. 3 ; instead, we use variants called pure-1 trees (or P1-trees). P1-trees contain nodes labeled with 0 s or 1 s. A node is labeled with a 1 if and only if the corre-sponding bit group it represents is made up entirely of 1 s (i.e., it is pure-1). Nodes labeled with 0 s can be either pure-0 nodes or mixed (i.e., neither pure-0 nor pure-1). Note that we can easily differentiate between pure-0 nodes and mixed nodes by observing that pure-0 nodes have no children (i.e., they are always leaf nodes). Figure 4 shows the P1-trees corresponding to the P-trees in Fig. 3 .
 ROOTCOUNT (the count of the number of 1s in the bit group represented by the tree) in order to query the underlying data [ 9 , 34 ]. The NOT operation is a straightforward swap of each pure node; pure-1 nodes become pure-0 nodes and vice versa, while, mixed nodes stay as they are. The AND operation logically ANDs the nodes at the same position in the operand trees, while the OR operation logically ORs them. Note that ANDing a pure-0 node with anything results in a pure-0 node, and, by the same token, ORing a pure-1 node with anything results in a pure-1 node. These observations, which can be attributed to P-tree compression, are exploited to achieve fast P-tree ANDing and ORing.
 sive deal of literature exists on the P-tree technology and its applications such as Ding et al. [ 8 ], Khan et al. [ 22 ], Perrizo et al. [ 35 ], Perrizo et al. [ 36 ], Rahal and Perrizo [ 40 ], Serazi et al. [ 41 ], and Wang et al. [ 48 ]. 7.2 The P-tree X  X ased algorithm We have devised a P-tree X  X ased ARM implementation for this work to analyze a subset of the data set available for the seventh KDD CUP competition [ 21 ], a knowledge discovery and data mining competition held in conjunction with the Ninth Annual ACM SIGKDD conference. The subset under consideration deals with citation graphs and publication subject matters represented by PACS num-bers (Physics and Astronomy Classification Scheme). PACS numbers are used to represent subject matters of publications in the physics domain. The total number of PACS numbers available in the given data set is 828. We have 828 citee PACS numbers and 828 citer PACS numbers amounting to a total of 1,656 attributes or columns used in the derived table. The total number of transactions (i.e., edges) considered is 1,448 out of a possible 352,807 edges in the original data set. The reason for this reduction is that we selected only the subset of papers participating in the citation graph (i.e., nodes in it) and having PACS numbers. As aforemen-tioned, the possible attributes in each transaction are the 828 citee PACS numbers followed by the 828 citer PACS numbers. We use item indexes 1 X 828 for citee attributes and 829 X 1,656 for citer properties. Each transaction records the item in-dexes of the PACS numbers existing in its participating nodes. The same file could also be represented in bitmap format where attribute values record the existence or absence of the corresponding PACS number in the participating nodes of every edge. In our case, we adopted the latter format to help expedite the process of creating P-trees following our description in the previous subsection.
 into four steps, the first two of which are carried out in parallel. First, we mine all frequent itemsets from the citee part of the data set. Those itemsets satisfy the minimum specified support threshold, minisupp (i.e., have support greater than or equal to minisupp ). Second, we mine all frequent itemsets from the citer part. Note that representing data in P-tree format has the advantage of speeding up the frequent itemset-mining process because, after creating the P-trees which could be easily done offline, no database scans are ever needed [ 9 , 34 ], just logical oper-ations on compressed bitmaps. To get the support of an itemset containing items X and Y , all we have to do is to AND P X and P Y and issue a ROOTCOUNT operation on the resulting tree which is obviously cheaper than a database scan [ 9 , 34 ].
 resentation to utilize memory efficiently by only materializing the P-trees related to the part of the data set we are currently dealing with. Even while mining each part separately, all P-trees for non-frequent itemsets are unloaded from memory as they will not be of any use anymore, resulting in better memory utilization. In addition, the two steps described so far are independent thus giving us the ability to perform them asynchronously (i.e., in parallel as in study by Shenoy et al. [ 42 ]). a join step on the results (described in more details in the next subsection). The reason for this is the format of the desired inter-node rules; each rule must have its antecedent and consequent drawn from the citee and citer parts, respectively. By definition, the support of a rule must be greater than or equal to minsupp ; i.e., the support of the union of the antecedent and consequent must be greater than or equal to minsupp and so must be the support of each considered separately. Thus, instead of mining all frequent itemsets across the whole data set which could result in an exponential increase in the number of itemsets that must be generated and tested (because of doubling the item space) and then pruning all itemsets that do not contain items from both parts of the data set, we adopt a divide-and-conquer approach by mining, in parallel, each part separately and then joining the results. Note that an additional advantage of this division is the ability of halting the mining process early on in case we observe that any of the parts produces no frequent itemsets (we do not even need to continue the mining of the other part in such a case). Other approaches can only detect this scenario during their post-processing phases.
 parts. Each itemset produces only one rule because all of its citee items should reside in the antecedent while its citer items reside in the consequent. As a result, producing rules is rather fast and requires almost no processing (i.e., no enumer-ation of the different rules that could be derived from an itemset) other than the confidence test.
 have to compare the confidence of each inter-node rule with the confidence of its corresponding intra-node rule (which in turn must exceed miniconf ) and mark all the inter-node rules that satisfy this criterion. As mentioned earlier, we do not care for the support of the corresponding intra-node rules; we just use their confidence for testing purposes.
 rather straight forward because of P-trees; the confidence of a rule is equal to the ROOTCOUNT of the P-tree representing the itemset in the rule antecedent union the rule consequent divided by the ROOTCOUNT of the P-tree representing the antecedent. As mentioned earlier, each such inter-node rule provides us with valu-able information as it associates subject matters of publications written at different points in time. The formal description of our P-tree X  X ased algorithm used herein is given in Fig. 6 . 7.3 Implementation details All the procedures shown in Fig. 6 are largely self-explanatory; we will high-light some important points in relation to our frequent itemset-mining procedure ( PROC Mine Frequent Itemsets ) and join step ( PROC Association Rule Mining ). item space, I I I , testing the support of an itemset only after processing all of its subsets. For example, suppose that for I I I ={ a , b , c , d , e } ,wehave Frequent 1 Itemsets ={ a , b , c , d } . The way in which the entries are tested and in-serted into the Frequent Itemsets vector is as follows (assuming everything is fre-sert a ,thenweinsert b and try joining it with everything on its left (from left to right) in Frequent Itemsets vector; so we try ba . Similarly, we insert c ,then ca , cb ,and cba , etc. Note that first we insert the new frequent item in the vector and then try to join it with all itemsets preceding it; as a result, before testing the fre-quency of any itemset, we are guaranteed that all of its subsets are frequent and exist in the Frequent Itemsets vector, thus exploiting the anti-monotonicity prop-erty of support with respect to itemset size as introduced by Agrawal et al. [ 1 ], and Agrawal and Srikant [ 2 ]. For example, we do not try the new item c with ba if either a or b is not in Frequent Itemsets vector simply because ba would cease to exist in this case; however, we still would try to join the new item d with cba even if cd is not frequent because cba is in the Frequent Itemsets vector which suggests a non-complete exploitation of the anti-monotonicity property of support. taboo list (TL), with every new frequent item, I , inserted into the vector where we save all the itemsets that produce infrequent itemsets when joined with I . Going back to the previous example, if cd is infrequent then we append itemset c to the taboo list of item d (TL d for short). In later steps, we can skip all supersets of c .In general, before computing the support of a new candidate itemset, X , containing item d , we check if any subset of X is in TL d ; if so, we discard X .Albeitthismay seem unfeasible, an efficient implementation has been devised as we shall discuss in more details in the next subsection.
 lists is very similar to a popular approach used in the AI literature and known as tabu search [ 13 ]. The idea of tabu search is to traverse the space in a more effective manner by avoiding moves that result in revisiting points in the space previously visited whose outcome are known not to be acceptable (hence the name  X  X abu X ). The fact that the union of two itemsets I and X produces an infrequent itemset implies that future joins of I with any superset of X will certainly produce an infrequent itemset; a scenario similar in essence to revisiting a point in the search space whose outcome is known to be unsatisfactory and which could have been circumvented by putting that point on a tabu list. Because of the difference in context and problem definition, we refer to our lists as taboo lists instead of tabu lists.
 whose supports, when joined with I , are less than minisupp and thus the supports of their supersets when joined with I need not be computed. Each TL I is a P-tree having a size equal to the number of itemsets in the Frequent Itemsets vector preceding I (i.e., all nodes that I needs to be joined with). A value 1 is used for itemsets which, when joined with I , result in infrequent itemsets; thus, none of their supersets need to be joined later with I . The remaining TL I entries will be 0s. For example, for item space I I I ={ a , b , c , d } , suppose that the entries in the Frequent Itemsets vector created so far are: a , b , ab , c , ac , bc , abc .Forthenew item d , we initialize a TL d having seven entries all containing 0s initially. If the union of item d with node b results in an infrequent itemset, then the second entry in TL d which corresponds to itemset b is flagged with a 1, and so are all other entries containing b (i.e., entries pointing to ab , bc ,and abc ). But how can we efficiently locate all other entries containing item b ? P-tree) that has a 1 value for every position that this item exists in. For example, item c will have the following index list (it will be stored as a P-tree but we are just listing the entries in a list for convenience): 0, 0, 0, 1, 1, 1, 1. In other words, item c occurs in the Frequent Itemsets vector in positions 4, 5, 6, and 7. Every new itemset added to the vector results in the expansion of all index P-trees by either a 1, if the corresponding item is in the added node, or a 0 otherwise. The TL of the current item is also expanded by a 0 to maintain a size equal to that of the index P-trees.
 b results in an infrequent itemset and thus node b needs to be added to TL d ,we simply OR the index P-tree of item b with TL d and store the result in TL d .In general, if we want to add itemset xy....z to some taboo list, say TL I , we AND the index P-trees for all items in the itemset (i.e., AND index P-tree of x with that of y . . . with that of z ) . The result will give us where itemset xy....z and all its supersets occur in the Frequent Itemsets vector. Then we OR the resulting P-tree index with TL I and store the result in TL I which results in appending node xy....z to TL I .
 and takes advantage of the anti-monotonicity property of support with respect to itemset size by utilizing taboo lists. We proceed by initializing the set of joined frequent itemsets to be empty at first and then insert in it all frequent itemset that result from joining every frequent itemset in one of the joined sets with all frequent itemsets in the other set using taboo lists to reduce the itemset space (similar to what we have done previously). Note that here, in addition to providing the two sets of frequent itemsets as input, the join step expects the final set of index lists for at least one of the two joined sets  X  even though this is not explicitly stated in the algorithm  X  in order to be able to use taboo lists.
 step uses a taboo list for every frequent itemset in one of the sets of frequent itemsets being joined. This has not been case previously because we used one taboo list for every item only (or frequent 1-itemset) and not for every frequent itemset.
 not grow as has been the case so far. The reason for this lies in the way we are joining the two sets of frequent itemsets; we are taking one frequent itemset from one of the sets, S 1 , and joining it with all frequent itemsets in the other set, S 2 . The size of the taboo list for each frequent itemset in S 1 and of the index list for each item in S 2 is equal to the size of frequent itemsets in S 2 . utilizes the anti-monotonicity property of support only partially because whatever is done for frequent k -itemsets is not reflected on their frequent ( k + 1)-supersets being joined. We could alleviate this problem simply by maintaining the taboo lists of all frequent k -itemsets being joined throughout the join phase. Recall that the taboo lists are initialized to contain all zeros at first. This would still hold for frequent 1-itemsets; however, for frequent k -itemsets (for k &gt; 1), we initialize their taboo lists to be the ORing of the taboo lists of all of their subset frequent ( k  X  1)-itemsets. This ensures that all itemsets I join has failed with any itemset I 2 , from the second part of the data set, are not joined with any supersets of I 2 (because supersets of I 2 will get a  X  X opy X  of I 2  X  X  taboo list). In essence, this can be viewed as a space versus time issue. Storing all taboo lists for the whole duration of the join step will surely save more time be-cause it ensures 100% utilization of the anti-monotonicity of support; meanwhile, not doing so would save memory space at the expense of time. In our experiments, we employee the latter approach.
 faster logical operations in addition to compression. Note that in the case of taboo lists, compression could speed node traversal especially in cases where there are many consecutive 1s. For example, suppose the entries in a taboo list are: 1111 1100. Figure 7 depicts the corresponding P-tree of the given taboo list. In this ex-ample, instead of going through the first four nodes sequentially and then skipping them because they are flagged with 1s, using a P-tree to represent the taboo list, we can directly skip the first four entries because they form a pure-1 node 6 on the second level of the P-tree.
 to generate candidate itemsets, namely, the join and prune steps. Apriori joins any two k -itemsets sharing the first  X  k  X  1 X  items in the join step and prunes all those ( k + 1)-itemsets that have at least one infrequent subset in the prune step. Pruning can be accomplished either by searching which incurs an execution time cost or by using special data structures to store frequent itemsets like hash trees as suggested by Agrawal and Srikant [ 2 ] which incurs space and maintenance overheads. In our approach, we combine those two steps into one step because we form a candidate frequent itemset if and only if all subsets of that itemset are frequent. of the Frequent Itemsets vector but in binary format (i.e., all the items of every itemset in the vector have a 1 in their index P-trees in the position of the itemset); consequently, for better space utilization, we need not physically store the Fre-quent Itemsets vector. All frequent itemsets can be derived from the index P-trees. In addition, each TL is only stored for the life of the processing of the correspond-ing item after which it can be discarded. Also during its life, it is stored as a P-tree and, thus, is compressed when possible. As a result, our approach has no stor-age overhead other than the temporary taboo lists each lasting for the processing duration of the corresponding item, after which it can be discarded. 7.4 Performance analysis To the best of our knowledge, there are no benchmarks (or previous work) attempt-ing to discover rules for similar analysis of citation graphs. To give the reader a clearer view of our efficiency, we developed an implementation for the work sug-gested herein using P-trees (called PARM) and compared it with four popular contemporary ARM approaches, namely, FP-Growth (FPG) [ 17 ] which utilizes a vertical as well as a horizontal compressed data representation [ 16 ] and is popular for being one of the best state-of-the-art approaches, APRIORI [ 1 , 2 ] which is the traditional bread-first approach for ARM, Depth-First Apriori (DFA) [ 24 ]which uses a depth-first traversal of the search space, and finally ECLAT [ 52 ]which uses a vertical data representation in the rule discovery process. Note that FPG and ELCAT are depth-first X  X ased traversal approaches.
 parison results are largely dependent on the implementations used; as a result, we used popular publicly available implementations known for their speed for all the approaches with which we compared. For FPG and APRIORI, we used popular implementations which have been used as benchmarks in the Frequent Itemset Mining Implementations (FIMI) repository [ 11 ] and are available for download on Goethals [ 14 ]. For ECLAT and DFA, we used Borgelt X  X  implementation [ 5 ] and the implementation described by Kosters and Pijls [ 24 ], respectively, both of which have participated in the FIMI competition and can be downloaded from the FIMI repository. All implementations are coded in C++ and executed on an In-tel Pentium-4 2.4 GHz processor workstation with 2 GB RAM running Red Hat Linux. In all experiments, we focus on mining all the frequent itemsets only by varying the minimum support threshold. The reader is warned that the shown ex-ecution times do not reflect any post-processing that is still required of all ap-proaches other than ours (and the P-APRIORI approach which is described next) for the purpose of removing all frequent itemsets that do not contain items from both parts of the data set (which obviously favors the rest of the approaches). (1) the performance of the overall algorithm which utilizes a parallel divide-and-conquer methodology and (2) the performance for mining frequent itemsets which is the dominant factor in almost all ARM algorithms. For (1), we compare with FPG, APRIORI, ELCAT, and DFA all of which mine the frequent itemsets over the whole data set and retain only those that contain both citee and citer items. For (2), we compare with another approach that uses P-trees and is based on Apriori (called P-APRIORI). P-APRIORI uses the same algorithm that we use (i.e., di-vides the problem of mining frequent itemsets into a number of smaller tasks the first two of which are carried out in parallel to be joined later) except for the part where it mines frequent itemsets from the citee and citer parts separately; a verti-cal Apriori implementation based on P-trees as by Ding et al. [ 8 ] substitutes our frequent itemset mining algorithm. Note that Ding et al. [ 8 ] has shown better re-sults than state-of-the-art approaches such as FPG especially over large data sets in the form of Remotely Sensed Images (RSI) [ 8 ].
 design methodologies. In general, the factors whose effects need to be studied are the number of transactions in the data set (cardinality), number of items in the data set (dimensionality), density of the data set, type of the data set (i.e., real or synthetic), and minimum support threshold. The only response of interest is the execution time to mine all frequent itemsets.
 values) for each of the k factors and then monitor the responses at all possible 2 k factor-level combinations each known as a design point. Applying this design in its present form would be intractable due to inherent problems related to some of our factors; consequently, we use a variant of this design that is tailored for our context. For example, to probe the effect of the cardinality factor, we had to generate synthetic data sets using IBM X  X  Quest synthetic data generator [ 19 ] because, to the best of our knowledge, such large data sets are not available on public repositories like the University of California Irvine data repository [ 47 ] or the FIMI repository. In addition, we are not aware of any available real-life citation data sets that represent publications in terms of the relevant subject matters as required in our work. Due to those reasons, we have used real-life data sets from the FIMI repository, when available, to represent design points; otherwise, synthetic data sets have been created for those design points having no matching real-life ones. Three out of the four small data sets shown later are real-life data sets, while all the large data sets are synthetic. As a result of this real-life data set unavailability, we ignore the effects of the type factor in our experimental study. difficult to choose only two levels for support, we generate the design points using the rest of the factors without involving the support factor. For every design point, we probe the behavior of the compared implementations over the corresponding data set by varying the support threshold and going to low but manageable values. only the cardinality, dimensionality and density factors to generate design points and the support factor, at each design point, to generate the performance graphs with respect to the time response for the compared approaches.
 because of its subjective nature. It has been an adopted policy of ours to be as general as possible in our choices. For the cardinality factor, we choose  X  X arge X  and  X  X mall X  as the levels taking 1 million transactions as a threshold. For the dimensionality factor, we have  X  X igh X  and  X  X ow X  levels where high dimensionality exceeds 250 items. The levels for the density factor are  X  X ense X  and  X  X parse. X  We take  X  X parse X  to mean few items (out of the total number of items in our item space) in most transactions of the data set so we set the threshold for this factor at 0.05 meaning that the average number of items per transaction in  X  X parse X  data sets should not exceed 5% of the total number of items in the data set. In total, we have eight (or 2 3 ) resulting design points each represented by a data set as shown in Table 1 where ITEMS is the number of items in the whole data set, AVG the average number of items per transaction, TRANS the number of transactions, and DESC the factor-level combination description for each data set.
 synthetically generated using IBM X  X  Quest synthetic data generator. While gen-erating those data sets, we only set the number of transactions, number of items, and average number of items per transaction and used default values for the rest of the input parameters. The  X  X MS-POS, X   X  X ccidents X  [ 12 ] and  X  X hess X  data sets are real-life data sets downloaded from the FIMI repository.
 support thresholds on the given design points each represented by a corresponding data set. Curves terminating prematurely, such as FPG in Fig. 8 (lower right part) which terminates at 15% support while other curves show results at less than 5% support, imply that the respective approaches took too much time before finishing that they had to be terminated manually.
 based on the parallel divide-and-conquer algorithm proposed herein, demonstrate superior results compared with contemporary approaches over all large data sets showing improvements up to an order of magnitude as is the case over  X  X S4 X  (es-pecially for PARM). Results on the smaller data sets were not nearly as encourag-ing, indicating that results for our vertical divide-and-conquer approach improve by increasing the level of the cardinality factor; an observation which we find to be very reasonable because our approach uses compressed data structures to ver-tically process requests for discovering frequent itemsets.
 mining, our approach reduces the time over all given data sets with an approxi-mate reduction range of 15 X 65% and an average of more than 40% regardless of the cardinality factor thus supporting our claims regarding the effectiveness of the frequent itemset mining approach integrated into this work. Given that our results are generally better than P-APRIORI and that Ding et al. [ 8 ] have been able to demonstrate better results than a number of renowned approaches including FPG over large data sets, we are inclined to believe that such improvements will be the case in other scenarios where other state-of-the-art approaches are substituted in place of our frequent itemset-mining approach. However, any other approach that is to replace our frequent itemset engine must either be capable of efficiently join-ing the results from the initial two parallelized steps or must produce the frequent itemsets in the required order, in addition to producing taboo and index lists if it were to use our join step (the third step in our approach). Another option would be to create the lists and reorder the frequent itemsets after producing them but clearly this would be computationally inefficient.
 sets is not being able to amortize the cost of processing P-trees over small data sets where even compression does not prove to be effective. To justify our claims regarding the efficacy of the vertical data processing over large data sets, the user is referred back to the figures where ECLAT, which also uses vertical data processing, shows the best performance among the selected contemporary ap-proaches over all large data sets. Our improvements over ECLAT can be at-tributed to the parallel divide-and-conquer methodology which is complemented by a compressed data representation due to P-trees and taboo lists to utilize the anti-monotonicity property of support; a characteristic which ECLAT lacks as dis-cussed by Goethals [ 15 ].
 and small data sets could be formulated by recalling how our approach works in terms of data processing: it performs precautionary logical operations on taboo lists and index lists in order to eliminate unnecessary operations on P-trees (i.e., counting support only when needed). This implicitly assumes that the precaution-ary operations are cheaper than the P-tree data operations. Since both types of operations are mostly logical operations, the dominant factor here would be the size of the operands (for simplicity, assuming they are compressed to a similar degree). For the precautionary operations, the operands have the same size as the number of produced frequent itemsets, while for the data P-trees, this would be the size of the data set. It is very clear that when the data cardinality is high enough (where the number of transactions is much larger than the number of produced frequent itemsets which does not exceed the thousands for our large data sets), the cost of operating on data P-trees exceeds that of the precautionary operations; thus, by eliminating the unnecessary data P-tree operations over the large data sets, we would be saving a lot on execution time. On the other hand, for the smaller data sets under consideration, the number of frequent itemsets is very large compared to the number of the transactions in the corresponding data set such that not only do the savings resulting from eliminating unnecessary data P-tree operations by operating on taboo lists and index lists diminish drastically, but most likely an extra processing cost is incurred.
 resulting from the dimensionality factor over low to relatively high dimensional data sets. After careful consideration, we realized that the effect and controllability of the dimensionality factor are rather dubious because the actual effect on the time response is largely controlled by the number of frequent one itemsets which, in turn, depends on the support, density, and cardinality factors making it extremely difficult to control. Note that the number of frequent one itemsets could be very small even when the number of items is extremely large.
 comparable over all data sets regardless of density. However, it is worth mention-ing that results over small data sets seem slightly more encouraging for the sparser ones where PARM ranks closely after ECLAT and FPG, followed by P-APRIORI (upper left and lower left parts of Fig. 9 ). Over the denser small data sets (up-per right and lower right parts of Fig. 9 ), the performance degrades even more for PARM and for P-APRIORI. Note that the shown figures might deceive the reader into believing the contrary; however, the time differences should confirm our point. Using P-trees, we can compress sparse data very well; in addition, the number of produced frequent itemsets tends to decrease over sparse data as it is not highly correlated. The density factor does not show evident effects on large data sets. Even with this slight variation over smaller data sets, PARM still shows better results than P-APRIORI regardless of the factor under consideration (i.e., density factor).
 to be rather large and sparse. The rationale for focusing relatively on sparse data sets is that, using our model, even though the number of transactions representing edges in the graph can grow enormously, the number of items per transaction (i.e., the number of subjects a publication can belong to) is still estimated to be rather small. As shown in our experimental analysis, those data sets can be handled quite efficiently by our approach.
 tics for the contemporary approaches with which we are comparing. For example, the performance of FPG improved drastically over smaller data sets which could be justified by the fact that FPG views transactions as ordered strings of items and proceeds by storing the transactions in a trie data structure creating an in-memory version of the data set after which it traverses the trie to derive all frequent item-sets. After creating the trie, no database scans are needed. In general, this is very feasible in case the data set is relatively small so as to fit in memory, and fairly dense where it will compress due to the use of tries (after ordering all items in transactions so as to increase the overlap). Thus, it is very intuitive to expect per-formance degradations over larger data sets. For dense data sets, we have only noticed improvements for FPG over ECLAT on small dense data sets where FPG outperforms ELCAT on  X  X ccidents X  for the only time in our experiments and shows very close performance on  X  X hess. X  DFA) are observed over sparse data sets regardless of the cardinality and dimen-sionality factors. We believe this to be very reasonable because Apriori-based ap-proaches are known to operate in a generate-and-test fashion which greatly de-grades as the number of generated candidate frequent itemsets to be tested in-creases over dense data sets. Though it is not as evident in the graphs, PARM and P-APRIORI also behave similarly as they also generate candidate frequent itemsets and then test their support. 7.5 Result analysis We ran our algorithm several times on the data set described in Sect. 7.2 using dif-ferent support threshold values, and noted the impact on the number of produced intra-and inter-node rules. Miniconf was set at zero. Figure 10 depicts a graphical representation of the number of intra-and inter-node rules produced versus the support value chosen. The figure also depicts a table showing the exact measures in both cases.
 the inter-node rules, it is interesting that whenever we have intra-node rules, at least one inter-node rule shows. Another noteworthy observation is that, in the support range of [20, 50] (absolute support is given), only one inter-node rule shows. The rule is 11 . 30 . Pb CITEE  X  11 . 30 . Er CITER , 11 . 27 . + d CITER ( conf &gt; = 0 . 294798, supp = 94 / 1448); it associates the 11 . 20 . Pb citee PACS number with the 11 . 30 . Er and 11 . 27 . + d citer PACS numbers with a confidence of approximately 29.5% and a support of 6.5%. This rule has the highest support among all produced inter-node rules. Note that no rule, neither intra-node nor inter-node, is produced when the (absolute) support exceeds 55.
 from 0.4 to 100%). The following rule has a confidence of 100% and is worth mentioning: 11 . 20 . Dj 2 CITEE  X  03 . 65 . Db 2 CITER ,03 . 80 . + r 2 CITER ( conf = 100%, supp = 0 . 35%). Figure 11 depicts some of the inter-rules that were produced at different support thresholds which we include herein just for completeness. For better readability PACS numbers drawn from the citee set and the citer set are appended by CITEE and CITER, respectively.
 the description of the PACS numbers available at the American Institute of Physics website [ 4 ]. This description associates every PACS number with its corresponding subject matter. For example, the following inter-node rule, 11 . 30 . Rd CITEE  X  12 . 39 . Fe CITER ,12 . 38 . Lg CITER ( conf &gt; = 0 . 25, supp = 15 / 1448), can now be semantically rewritten as follows. culations  X ( conf = &gt; 0 . 25, supp = 15 / 1448). According to our analysis, subject matter  X  Chiral symmetries  X  has an impact on subject matters  X  Chiral Lagrangians , X  and  X  Other non-perturbative calculations  X  where the latter subject matters have extended from the former. Figure 12 lists the rules from Fig. 11 with the PACS numbers replaced by their semantic equivalents from the AIP website. theory. One observation was that most of the rules have older and more gen-eral antecedents (citee part) than consequents (citer part) possibly indicating that physics researchers tend to cite the entire history of development of a subject, going back several decades if necessary, thereby, connoting that older and more general subjects are more likely to be cited often with time. It is clear that old and general subjects form the ground for most research subjects that come later, thus supporting our claims regarding the ability of the discovered rules to highlight subject extensions and evolutionary history over time. For example, in the rule  X  Supersymmetry  X   X   X  Charge conjugation , parity , time reversal , and other dis-crete symmetries , X   X  Extended classical solutions ; cosmic strings , domain walls , texture  X  the antecedent is very fundamental, general, and rather old, and all the consequent subjects can be viewed as its extensions.
 it could be confirmed that, in fact, consequent subject matters of the rules have extended, in some form, from antecedent subject matters. For example, the rule  X  Chiral symmetries  X   X   X  Chiral Lagrangians , X   X  Other non-perturbative calcu-lations  X  holds as it is known that, in general, symmetries are fundamental proper-ties of a system and Lagrangians are used to calculate system behavior based on its symmetries making them a valid extension. In other words, symmetries form the theory which has been extended by the application of predicting behavior, namely, Lagrangians. In addition, Lagrangians is one type of non-perturbative calculations which explains the remaining part of the rule.
 sequent of another, we expect to be able to look many hops backwards in subject evolution (or forward in subject extensions) in order to further understand how subjects form and what their future impacts might be. Thus, even though we fo-cused only on direct citations by limiting ourselves to the use of single-edge paths as the transactions in our data model, we could be capable of gaining deeper in-sights on subject evolutionary history, extensions, and future impacts by forming rule chains. All of these observations fit, to a large extent, our motivation for the format of the desired rules, subjective notion of interestingness, and claims re-garding their usability. 8 Conclusion and future work In this work, we have proposed an efficient vertical association rule mining model for representing citation-graph data and generating rules capable of associating research subject matters of publications written at different points in time. Pat-terns of interest could reveal the original subject matters from which other subject matters of interest might have extended later in time, the evolutionary history of specific subject matters, and the potential effects of current research on future research. We hope to have initiated a novel research direction which will lead to better understanding of how we ought to understand citation-graphs (and directed graphs in general).
 in the directionality of the citation-graph edges in an efficient manner. An edge from node X to node in Y in a citation graph implies that paper X cites paper Y and, more importantly, that Y was written before X (ignoring factors such as  X  X reprints X  or  X  X elf-citations X ). The time factor is perhaps one of the main reasons we have restricted our analysis to citation-graph data; nevertheless, we believe that our techniques could be generalized to other application domains.
 graphs with the aim of exploiting factors other than the time factor. We also plan to study the efficacy of the suggested temporal and subject constraints proposed to reduce the number of considered graph edges by focusing only on the nodes (along with their incoming and outgoing edges) satisfying the given constraints. In addition, we would like to analyze the usefulness of the concepts presented herein when applied to other potential domains such as citation mining and Web-structure mining.
 aspects in publication trends through citation graphs by identifying patterns in the subjects of scientific publications. Our carefully designed experiments have shown significant time improvements for our approach when compared with other opti-mized implementations for contemporary approaches especially over very large data sets regardless of their dimensionality and density, thus, demonstrating the effectiveness of the proposed vertical data presentation and processing over com-pressed data structures in addition to that of the divide-and-conquer methodology; however, results on smaller data sets were not as encouraging. In addition, the proposed frequent itemset mining approach has shown better results than vertical Apriori (P-APRIORI) over all data sets regardless of all factors which suggests the efficiency of our itemset enumeration and mining algorithm.
 References Author Biographies
