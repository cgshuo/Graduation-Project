 Gang Niu gang@sg.cs.titech.ac.jp Wittawat Jitkrittum wittawatj@gmail.com Bo Dai bdai6@gatech.edu Hirotaka Hachiya hacchan@gmail.com Masashi Sugiyama sugi@cs.titech.ac.jp Semi-supervised learning, which utilizes both labeled and unlabeled data for training, has attracted much attention over the last decade. Many semi-supervised assumptions have been made to extract information from unlabeled data. Among them, the manifold as-sumption (Belkin et al., 2006) is of vital importance. Its origin is the low-density separation principle . However, this low-density separation principle is not the only way to go. A useful alternative is the informa-tion maximization principle (IMP). IMP comes from information maximization clustering (Agakov &amp; Bar-ber, 2006; Gomes et al., 2010; Sugiyama et al., 2011), where a probabilistic classifier is trained in an unsu-pervised manner, so that a given information measure between data and cluster assignments is maximized. These clustering methods have shown IMP is reason-able and powerful.
 Following IMP, we propose an information-theoretic approach to semi-supervised learning. Specifically, the squared-loss mutual information (SMI) (Suzuki et al., 2009) is designated as the information measure to be maximized. Then, we introduce an SMI approxima-tor with no logarithm inside (Sugiyama et al., 2011), and propose the model of SMI regularization (SMIR). Unlike maximizing the mutual information, SMIR is strictly convex under mild conditions and the unique globally optimal solution is accessible. Albeit we can employ any convex loss in principle, SMIR can get rid of logarithm in the involved optimization and guar-antees the analytic expression of the globally optimal solution if we use the squared difference of two prob-abilities (Sugiyama, 2010). SMIR aims at multi-class probabilistic classifiers that possess the innate ability of multi-class classification with the probabilistic out-put, and no reduction from the multi-class case to the binary case (cf. Allwein et al., 2000) is needed. These classifiers can also naturally handle unseen data and need no explicit out-of-sample extension. To the best of our knowledge, SMIR is the only framework up to the present which leads to semi-supervised algorithms equipped with all these properties.
 Furthermore, we establish two data-dependent gener-alization error bounds for a reduced SMIR algorithm based on the theory of Rademacher averages (Bartlett &amp; Mendelson, 2002). Our error bounds can consider not only labeled data but also unlabeled data. Thus, they can reflect the properties of the particular mech-anism generating the data. Thanks to the analytical solution, our bounds also have closed-form expression even though they depend on the data in terms of the Rademacher complexity. Notice that previous bounds (Belkin et al., 2004; Cortes et al., 2008) just focus on the regression error, and none of semi-supervised algo-rithms hitherto have similar theoretical results. The rest of this paper is organized as follows. First of all, we present preliminaries, and propose the model and algorithm of SMIR in Section 2. In Section 3, we derive the generalization error bounds. The compar-isons to related works are in Section 4, and then the experiments are in Section 5. In this section, we propose the SMIR approach. 2.1. Preliminaries Let X  X  R d and Y = { 1 ,...,c } where d and c are natural numbers, ( X,Y )  X  X   X Y have an underlying p ( x ,y ) and p ( x ) &gt; 0 over X . Given i.i.d. { ( x and { x i } n i = l +1 where n = l + u and l u , we aim at estimating p ( y | x ). Then, we can classify any x  X  X to  X  y = arg max y  X  X  p ( y | x ).
 As an information measure, squared-loss mutual infor-mation (SMI) (Suzuki et al., 2009) between random variables X and Y is defined by
SMI := SMI is the Pearson divergence (Pearson, 1900) from p ( x ,y ) to p ( x ) p ( y ), while the mutual information (Shannon, 1948) is the Kullback-Leibler divergence They both belong to f -divergence (Ali &amp; Silvey, 1966; Csisz  X ar, 1967), and thus share similar properties. For instance, both of them are nonnegative, and take zero if and only if X and Y are independent.
 In Sugiyama et al. (2011), a computationally-efficient unsupervised SMI approximator was proposed. By as-suming a uniform class-prior probability p ( y ) = 1 /c , SMI becomes Then, p ( y | x ) is approximated by a kernel model: where  X  = {  X  1 ,...,  X  c } and  X  y = (  X  y, 1 ,..., X  y,n are model parameters, and k : X  X X 7 X  R is a ker-nel. After approximating the expectation w.r.t. p ( x ) in Eq. (1) by the empirical average, an SMI approxi-mator is derived as where K  X  R n  X  n is the kernel matrix. 2.2. Basic model Instead of Eq. (2), we introduce an alternative kernel model for SMIR (the reason will be explained in Re-mark 1). Let the empirical kernel map (Sch  X olkopf &amp; Smola, 2001) be the degree of x i be d i = P n j =1 k ( x i , x j ), and the de-gree matrix be D = diag( d 1 ,...,d n ). We approximate the class-posterior probability p ( y | x ) by 1 where  X  a , b  X  = P n j =1 a j b j is the inner product. Plug-ging (3) into Eq. (1) gives us an alternative SMI ap-proximator: where A = (  X  1 ,...,  X  c )  X  R n  X  c is the matrix repre-sentation of model parameters.
 Subsequently, we employ Eq. (4) to regularize a loss function  X ( p,q ) that is convex w.r.t. q . More specif-ically, we have three objectives: (i) Minimize  X ( p,q ); (ii) Maximize d SMI; (iii) Regularize  X  . Therefore, we formulate the optimization problem of SMIR as where  X , X  &gt; 0 are regularization parameters. A remarkable characteristic of optimization (5) is its convexity, as long as the kernel function k is nonnega-tive and  X  &gt;  X c/n : Theorem 1. Assume that k : X  X X 7 X  R + and  X  &gt;  X c/n . Then optimization (5) is strictly convex, and there exists a unique globally optimal solution. 2 Remark 1 . We introduced Eq. (3) due to the follow-ing reasons: (i) In principle, any kernel model linear w.r.t.  X  y may be used to approximate p ( y | x ), and maximizing d SMI alone must be non-convex. However, optimization (5) becomes convex if  X  is large enough. Hence, only  X  above a certain threshold is acceptable: The threshold of (3) is  X c/n . The threshold of (2) is k K k 2 2  X   X c/n where k K k 2 is the spectral norm of K . It depends upon all the training data thoroughly and is usually much larger than  X c/n . (ii) We found that (3) experimentally outperformed (2). 2.3. Proposed algorithm Due to limited space, we give a brief derivation here. We choose the squared difference of probabilities p and q as the loss function (Sugiyama, 2010):  X  2 ( p,q ) := It enables the analytical solution and facilitates our future theoretical analysis. Its empirical version is b  X  2 = Const .  X  Let Y  X  R l  X  c be the class indicator matrix for l la-beled data and B = ( I l ; 0 u  X  l )  X  R n  X  l . Subsequently, Eq. (6) can be expressed by  X  2 = Const .  X  Substituting Eq. (7) into optimization (5), we will get the following objective function: At last, by equating  X  X  to the zero matrix, we obtain the analytical solution to unconstrained optimization problem (5): We recommend to post-process the model parameters as where  X  y is a normalized version of  X   X  y , and  X  y is an estimate of p ( y ) based on labeled data. In addition, probability estimates should be nonnegative and thus our final solution can be expressed as follows (cf. Ya-mada et al., 2011): Although q ( y | x ;  X   X  ) might be negative or unnormal-ized, Kanamori et al. (2012) implies that minimizing  X  2 could achieve the optimal non-parametric conver-gence rate from q to p , and when we have enough data q is automatically a probability (i.e., non-negative and normalized). To elucidate the generalization capability, we reduce SMIR to binary classification. Now, a class label y is  X  1, a single vector  X   X  R n is enough to construct a discriminative model, and we classify any x  X  X  to Let us encode the information of class labels into y = ( y 1 ,...,y l ) &gt;  X  R l . The solution is then and for convenience, we define the decision function Let E and  X  E stand for the true and empirical expecta-tions, ` ( z ) = (1  X  sign( z )) / 2 be the indicator loss , and ` ( z ) = min(1 , max(0 , 1  X  z/ X  )) be the surrogate loss . We bound E ` ( yf ) using the theory of Rademacher av-erages (Bartlett &amp; Mendelson, 2002). If all labels are available for evaluation, we can evaluate  X  E `  X  ( yf ) over all training data and bound E ` ( yf ) more tightly. We state the theoretical result in Theorem 2 and prove it in Appendix B.
 Theorem 2. Assume that Let  X   X  F and f ( x ) be the optimal solution and the deci-sion function defined in Eqs. (8) and (9) respectively, and For any  X  &gt; 0 and 0 &lt;  X  &lt; 1 , with probability at least 1  X   X  , we have If the ground truth class labels y l +1 ,...,y n are also available for evaluation, with probability at least 1  X   X  , we have Theorem 2 gives the tightest upper bounds (i.e., the coefficients of 1 / given scenario) based on the inductive Rademacher complexity. The bound in Eq. (10) is asymptotically O (1 / cases, we may benefit from unlabeled data by a lower empirical error. It becomes O (1 / can access the other u labels, even though they are not used for training. Due to the smaller deviation of the empirical error and the empirical Rademacher com-plexity when they are estimated over all training data, we can improve the order from O (1 / Nevertheless, there is no free lunch: In (11), the em-pirical error is evaluated over all training data, and it may be significantly higher than that evaluated over labeled data. Basically, (10) or (11) which right-hand side is smaller reflects whether the information maxi-mization principle befits the data set or not. Information-theoretic semi-supervised approaches di-rectly constrain p ( y | x ) by unlabeled data or some p ( x ) given as the prior knowledge. Information regu-larization (IR; Szummer &amp; Jaakkola, 2002) is the pi-oneer for this purpose. Compared with later informa-tion maximization methods, IR minimizes the mutual information (MI) based on a key observation: Within a small region Q  X  X  , MI Q is low/high if the label in-formation is pure/chaotic. Subsequently, IR estimates a cover C of X from { x 1 ,..., x n } , and minimizes the maximal MI Q for Q  X  C , subject to class constraints provided by labeled data. The advantage of IR is its flexibility and convexity, while the drawback is that it is unclear how to estimate C properly. Each region should be small enough to preserve the locality of the label information in a single region; each pair of re-gions should be connected to ensure the dependence of p ( y | x ) over all regions, and this implies a great number of tiny regions.
 By employing the Shannon entropy of p ( y | x ) as a measure of class overlap, entropy regularization (ER; Grandvalet &amp; Bengio, 2004) minimizes the entropy from a viewpoint of maximum a posteriori estimation. More specifically, ER regularizes the maximum log-likelihood estimation of a logistic regression or kernel logistic regression model by an entropy term: ER favors low-density separations, since the low/high entropy means that the class overlap is mild/intensive. ER and IR seem opposite at a first glance, because MI equals the difference of the entropies of class prior and posterior. However, IR minimizes MI locally and ER minimizes the entropy globally , so both of them highly penalize the variations of the class-posterior probabil-ity in high-density regions. A recent framework called regularized information maximization (RIM; Gomes et al., 2010) follows ER and further maximizes the entropy of the class-prior probability to encourage bal-anced classes. ER and RIM do not model p ( x ) explic-itly which is a major improvement, but the disadvan-tage is the non-convexity of their optimizations. Expectation regularization (XR; Mann &amp; McCallum, 2007) goes one step further such that it does not use p ( x ) at all. Therefore, XR does not favor low-density separations and can handle highly overlapped classes. XR encourages the predictions on unlabeled data to match a designer-provided expectation by minimizing the KL-divergence between the expectations predicted by the model and provided as the prior knowledge. If there is no prior knowledge, XR will match the class prior of unlabeled data with that of labeled data: where  X  y is an estimate of p ( y ) through labeled data, and q ( y | x ;  X  ) is a logistic or kernel logistic regression model. Unlike IR and ER, XR does not prefer low-density separations. As a result, XR cannot deal with low-dimensional data with nonlinear structures (such as the famous two-moons or two-circles ), if there are not enough labeled data.
 On the other hand, there are lots of geometric meth-ods for semi-supervised learning. Please see Table 1 as a list of representative methods. Note that all ge-ometric methods in Table 1 are in the style of either large margins or similarity graphs. According to Ta-ble 1, we could know that many methods based on similarity graphs (Szummer &amp; Jaakkola, 2001; Zhou et al., 2003; Joachims, 2003; Zhu et al., 2003) are trans-ductive, while the information-theoretic methods are all inductive; only two geometric methods (Szummer &amp; Jaakkola, 2001; Zhou et al., 2003) could deal with multi-class data directly, while it is an inherent prop-erty of all information-theoretic methods. However, none of previous information-theoretic methods have analytical solutions, due to the logarithms in the en-tropy, MI or KL-divergence. Thanks to SMI, the pro-posed SMIR involves a strictly convex optimization problem with no logarithm inside and consequently it has the analytic expression of the unique globally op-timal solution.
 The similarity between ER and SMIR is intriguing. RIM followed ER historically. Nonetheless, if we start from MI maximization with the uniform p ( y ), we will get ER as Recall that SMI maximization under the assumption of the uniform p ( y ) is expressed by As a consequence, they have the similar preference as the logarithm is strictly monotonically increasing. The vital difference is the convexity and the analytical so-lution: SMIR is convex and the globally optimal solu-tion can be obtained analytically, whereas ER is non-convex so any locally optimal solution has to be found numerically. 3 In this section, we numerically evaluate SMIR. The specification of benchmark data sets is summarized in Table 2. Besides the four well-tried benchmarks in the first block (i.e., USPS, MNIST, 20Newsgroups and Iso-let), there are eight benchmarks from a book entitled Semi-Supervised Learning (Chapelle et al., 2006) 4 in the second block, and eight benchmarks from the UCI machine learning repository 5 in the third block except that Senseval-2 is from a workshop for word sense dis-ambiguation 6 . Detailed explanation of benchmarks is omitted due to lack of space. Our experiments consist of three parts: Firstly, we compare SMIR with entropy regularization (ER; Grandvalet &amp; Bengio, 2004) and expectation reg-ularization (XR; Mann &amp; McCallum, 2007). The prob-abilistic models are the logistic regression and the kernel logistic regression (Ker) where  X  X  ,  X  X  is the inner product,  X  n is the empirical kernel map for the Gaussian kernel. SMIR also ap-plies the Gaussian kernel, so there are three kernel methods which allow nonlinear decision boundaries in R . The two-fold cross-validation is performed to se-lect the hyperparameters. The kernel width is the median of all pairwise distances times the best value parameters, which is same as the third term of opti-mization (5), is included for XR and KerXR (Mann &amp; McCallum, 2007). No extra prior is added to ER or KerER, since ER itself is a prior from a viewpoint of maximum a posteriori estimation (Grandvalet &amp; Bengio, 2004). Therefore, ER/KerER has one regu-larization parameter whereas XR/KerXR and SMIR have two. The candidate list of regularization param-from  X c/n +10  X  { X  10 ,  X  8 ,  X  6 ,  X  4 ,  X  2 } for SMIR to en-sure the convexity. The minFunc 7 package for uncon-strained optimization using line-search methods (the quasi-Newton limited-memory BFGS updates, by de-fault) is utilized to solve ER/KerER and XR/KerXR. Since minimizing the entropy is non-convex, we ini-tialize ER/KerER with the globally optimal solution of its supervised part.
 We evaluated them on USPS, MNIST, 20Newsgroups and Isolet. Pearson X  X  correlation (Hall, 2000) was used to select 1000 most informative features for 20News-groups. For each data set, we prepared a multi-class task, namely, the tasks using 10 classes of USPS and MNIST, 7 classes of 20Newsgroups, and 26 classes of Isolet. In addition, extensive experiments of simple classification tasks were conducted, including 45 bi-nary tasks of USPS, 45 binary tasks of MNIST and 21 binary tasks of 20Newsgroups. Isolet may lead to too many binary tasks and these tasks are often too easy, and thus we combined 26 letters into 13 groups (e.g.,  X  X  X  with  X  X  X ,  X  X  X  with  X  X  X  etc.) and treated each group as a single class resulting in 78 simple classification tasks. For each task, we repeatedly ran all methods on 100 random samplings, where the sample size was fixed to 500. Each random sampling was partitioned into a training set and a test set with 80% and 20% data, and 10% class labels of training data were revealed to construct labeled data.
 Figure 1 reports the experimental results of the multi-class tasks, Figure 2 reports the experimental results of the simple tasks, and Table 3 summarizes the ex-perimental results. We can see from Figure 1 that SMIR outperformed others on the multi-class tasks of USPS, MNIST and Isolet. Likewise Figure 1 indicates that SMIR was the most computationally-efficient al-gorithm on all four multi-class tasks. According to Figure 2, SMIR was the best on the simple tasks of USPS, 20Newsgroups and Isolet, but was slightly in-ferior to plain ER on MNIST. Note that there were 12 highly imbalanced tasks among 21 simple tasks of 20Newsgroups, which implies that the uniform class-prior assumption will not affect the performance of SMIR essentially, if the tasks are not so complicated. The experiments of Isolet further imply that SMIR is fairly good at multi-modal data, since all classes there had two clusters. Compared with KerER and KerXR, the plain ER and XR were better on USPS, MNIST and Isolet, but worse on 20Newsgroups. Nonetheless, ER/XR always outperformed KerER/KerXR in Ta-ble 3. Even though other algorithms often converged quite quickly on the simple tasks, SMIR was still a computationally-efficient algorithm after taking these simple tasks into account.
 Secondly, we compare SMIR with two well-known ge-ometric methods: Laplacian regularized least squares (LapRLS; Belkin et al., 2006) with a multi-class exten-sion, as well as learning with local and global consis-tency (LGC; Zhou et al., 2003) with an out-of-sample extension. They represent the state-of-the-art mani-fold regularization and similarity graph transduction respectively. Similarly to SMIR, their optimizations are convex and can be solved analytically. LapRLS is extended using the one-vs-rest trick, and LGC is ex-tended via the Nadaraya-Watson estimator (Delalleau et al., 2005). The experimental setup and the candi-dates of hyperparameters for LapRLS and LGC are same as SMIR, except that the regularization param-SMIR was always best or tie in Table 4, and thus it is fairly competitive with those pure geometric methods on these benchmarks.
 Finally, we take all seven methods and compare their performance on the sixteen benchmarks listed in Ta-ble 2. The experimental results are reported in Ta-bles 5 and 6, where the experimental setup and the candidates of hyperparameters are same as previous experiments. To be clear, there are two benchmarks, BCI and Wine, whose sample size is less than 500. As a result, each of their random samplings included the whole set, and the randomness or the difference of the classification error was actually from how the training, test and cross-validation data were split and also how labeled data were selected. We can see from Table 5 that ER, LGC and SMIR were best or comparable on three benchmarks, and KerER, XR and KerXR were best or comparable on two benchmarks. Moreover, in Table 6, SMIR won or tied five times, while all other methods except XR won or tied twice. Therefore, it is reasonable and practical to maximize SMI following the information maximization principe, and SMIR is a promising information-theoretic approach to semi-supervised learning. In this paper, we proposed squared-loss mutual infor-mation regularization (SMIR). Compared with other information-theoretic regularization, SMIR is convex with no logarithm in the involved optimization prob-lem, and thus enables the analytic expression of the globally optimal solution. We established novel data-dependent generalization error bounds that even in-corporate the information of unlabeled data. We then evaluated SMIR on twenty benchmark data sets, and the results demonstrated that SMIR compared favor-ably with entropy regularization, expectation regular-ization, manifold regularization, and similarity graph transduction.
 GN was supported by the MEXT scholarship 103250, WJ was supported by the Okazaki Kaheita Interna-tional Scholarship Foundation, HH was supported by the FIRST program, and MS was supported by the MEXT KAKENHI 25700022.

