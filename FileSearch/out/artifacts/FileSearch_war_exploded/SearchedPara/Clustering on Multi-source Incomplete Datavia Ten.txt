 Due to the recent advances of data collection technologies, many application fields are facing various data with complex multiple sources ( i.e. , multi-source data), where different data sources provide different levels of necessarily detailed knowledge. For example, in a user-oriented recommendation system, data sources can be user profile database, users X  log data, users X  credit score and users X  social connections (as shown in Fig. 1 ). Different from traditional data with a single source, these multi-source data commonly have the following properties: 1. Each data source can have its own feature sets. For example, users X  X  credit (numerical features and nominal features as shown in Fig. 1a ) and the social relation (Fig. 1d ) provides graph relational features. Such data is called heterogeneous data. 2. Each data source may be incomplete. For example, in Fig. 1a , User3 does not complete her profile. From Fig. 1b , we can see that User2 and User4 do not have browsing behavior history. In Fig. 1c , only User1 and User2 have credit scores. User3 does not have any social connection information in
Fig. 1d . 3. Data can be with an arbitrary number of sources. In some applications, the number of available data sources may be small while in other applications, it may be quite large.
 The above properties raise one fundamental challenge for data mining research: the complex structure and lack of consistent and complete representation of data. Multiple sources may provide complementary data, and multi-source data fusion can produce a better understanding of the observed situation by decreasing the uncertainty related to the individual sources. An effective model for multi-source data should be able to integrate or extract information from these multiple sources in order to perform analysis or management steps. Motivated by these challenges, multi-source mining, in particular multi-source clustering, have received consid-erable attention in the last decade.
 In the literature, multi-source/multi-view clustering problem has been exten-on clustering heterogeneous data with an arbitrary number of sources. How-ever, these studies either assumed that data have full information in all sources [ 11 , 15 , 17 ], or that there exists at least one source which is complete with no missing values [ 25 ]. Although the most advanced methods [ 14 , 21 ] do not require the completeness of each source, the application of these two methods is limited to two-source situation. Multi-source clustering is still challenging on multiple heterogeneous sources in the presence of missing data of each source. samples that suffer from missing information. However, this clearly contradicts with the target of clustering which aims at distributing all samples to their corresponding cluster. In this paper, we focus on clustering the multi-source incomplete data by handling the missing information with the help of tensor algebra, which is the extension of vector and matrix algebra. More specifically, we develop a tensor-based modeling and factorization framework, called T-MIC ( T ensor based M ulti-source I ncomplete data C lustering). We first construct an initial tensor over the multi-source incomplete data based on kernel matrices. Then we formulate a joint tensor factorization process with the sparsity con-straint and use it to iteratively push the initial tensor towards a quality-driven exploration of the latent factors by taking into account missing data uncertainty. This leads to obtain the common latent features across all the data sources, as well as a common latent feature space. We can then use any standard clustering algorithm for grouping the objects in this space. As compared with previous approaches, this paper has several advantages: 1. The proposed T-MIC framework can be used in situations even when all the 2. T-MIC uses kernel matrices to form the tensor across all multiple data 3. T-MIC produces a unified, non-redundant, consistent and complete set of conduct several experiments on synthetic data and real-world data. The proposed approach outperforms the comparison algorithms in all the scenarios in both normalized mutual information and average purity.
 describe the notation and background knowledge. In Section 3 , we will describe the formulation of the problem and the proposed T-MIC framework. Experi-mental settings and result analysis are described in Section 4 . Before proceeding, we introduce some related concepts and notation of tensors used throughout of the paper. For more details about tensor algebra, please refer to [ 7 ].
 order tensors) and matrices (second-order tensors). The order of a tensor is the number of dimensions, also known as ways or modes. We denote tensors of order N  X  3 by calligraphic letters ( X , Y , Z ), matrices by boldface capital letters letters ( a , b , c ). Columns of a matrix are denoted by boldface lower letters with a subscript, e.g. , a r is the r th column of matrix A . Entries of a matrix or a tensor are denoted by lowercase letters with subscripts, i.e. , the ( mentwise) product is denoted by X X  X  and defined as for all values of the indices. Their inner product, denoted by of the products of their entries, i.e. , The Frobenius norm of a tensor X X  R I 1  X  I 2  X  X  X  X  X  I N is defined as Given a sequence of matrices A ( n )  X  R I n  X  R for n=1, 2, ..., N, the n otation [[ A , A (2) ,..., A ( N ) ]] defines an N-th order tensor of size whose elements are given by for i The outer product of two vectors a  X  R I 1 and b  X  R I 2 , denoted by a represents a matrix of size I 1  X  I 2 with the elements ( a It is worth noting that mathematically a tensor is an element of the outer product of vector spaces, each of which has its own coordinate system. The outer product of vector spaces forms an elegant algebraic structure for the theory of tensors. Such structure endows the tensor with the inherent advantage in representing real-world data, which naturally results from the interactions of multiple factors or multiple sources.
 The two most commonly used factorizations are the Tucker model and CAN-DECOMP/PARAFAC (CP) model, both of which can be regarded as higher-order generalizations of the matrix singular value decomposition (SVD). The CP model factorizes a tensor into a sum of rank-1 tensors, where each rank-1 tensor is the outer product of vector loadings in all modes, whereas in Tucker variants the factor interactions are modelled via a core tensor. This rank-1 component factorization of CP and its intrinsic axis property from parallel proportional pro-power. The Tucker model is more flexible, though, the complex interactions and non-uniqueness of solutions make its interpretation more difficult. Therefore, we adapt an underlying CP factorization for our model.
 [ 7 ] is defined by factor matrices A ( n )  X  R I n  X  R for such that where R is the rank of the tensor X , defined as the smallest number of rank-1 tensors in an exact CP factorization. The factor matrices A can be viewed as the common latent feature matrices in different modes. In this section, we discuss our approach T-MIC for multi-source incomplete clustering. The key challenge is how to effectively integrate information from multiple heterogeneous sources in the presence of missing data. Here we identify three broad goals: incorporating heterogeneous sources, extracting latent fea-tures, handling the incompleteness. In each of the following three subsections, we try to achieve one of the above goals respectively. Before we give more detailed discussions for each part, we first formulate the problem as below. feature matrices are denoted as X 1 , X 2 , ..., X K . We assume that none of the related data sources is complete. The goal is to derive a clustering solution based on all the incomplete sources. 3.1 Tensor Construction Tensor has been used to model multiple data sources for many years. There are several ways to construct a tensor from multiple data sources. One natural way is stacking the object-feature matrices derived from multiple data sources in a tensor. This construction requires that all the object-feature matrices has the same dimension. However, in real-world problems, the data sources usually exhibit heterogeneous properties, i.e. , the dimension of the feature space for different source is different. Taking the recommendation system in Section 1 as an example, the dimensions of available sources are different. Just stacking the object-feature matrices cannot form a tensor.
 In order to deal with the heterogeneity of the data sources, we construct a tensor in a way that is independent of data dimension. In [ 16 ], a tensor is constructed by stacking the object-object similarity matrices derived from all the sources. Inspired by this, we choose the homogeneous kernel as the similar-ity measure to form a shared latent tensor. Assume there are with different feature dimensions available. Each of the sources has the same objects. Although the feature dimensions of each source are different, the ker-nel matrices are all with dimension of N  X  N . A tensor can be constructed by stacking the K kernel matrices together as shown in Fig. 2 . However, in our problem, all the sources are incomplete. Thus, we fill in the missing entries with average value or majority value. We denote the feature matrices with missing value filled as  X  X 1 ,  X  X 2 , ...,  X  X K . The initial kernel matrices can be calculated and form an initial tensor T X  R N  X  N  X  in Fig. 2 . 3.2 Latent Features Extraction Using the tensor construction approach described in Section 3.1 , we can get an initial tensor T . In order to get the common factor matrices across all the sources, we need to factorize the tensor. As mentioned above, we use the CP factorization model to achieve this goal. In order to impose the sparsity constraints on the latent factor matrices to get more concise latent factors, we add latent factor matrices A and B to the objective function. The objective function for CP factorization with sparsity constraints can be stated as: where A  X  R N  X  R , B  X  R N  X  R , C  X  R K  X  R are the latent factor matrices of  X  and  X   X  are sparsity tradeoff parameters for latent factors A and B and is the l 1 norm. Here, A and B can be interpreted as the common latent factor across all the data sources. Note that the kernel matrix is symmetric, this will lead to the equality of A and B . Thus, we can apply any standard clustering algorithm ( i.e. , k-means ) on A to get a clustering solution.
 There exists quite a few algorithms to solve the optimization problems in Equation ( 4 )[ 13 , 20 ]. We will not discuss the details of the optimization in this paper. In the following, We will illustrate how to use the optimization to refine the initial tensor T and learn the common latent factor simultaneously. 3.3 Quality-Driven Integration By solving the optimization problem in Equation ( 4 ), we can get an  X  X ptimal X  common factor A and the estimated tensor  X  T : Algorithm 1. T-MIC for Clustering Multi-source Incomplete Data However, the  X  X ptimal X  common factor A may not be exact because of the inaccuracy of the initial tensor T . We use an iterative way to get a better result by taking into account the incompleteness of data sources. we define a weight tensor W X  R N  X  N  X  K : Note that the weight tensor W represents the reliability of the entries in tensor We want to retain the most reliable entries in tensor T and update those that are not reliable. Thus, we can use W to update the original tensor where  X  is Hadamard (elementwise) product and 1 is the tensor with all ones. After T is updated, A , B and C can be updated using Equation ( 4 ). This pro-cedure will repeat until convergence. The learned optimal common factor A can be used by any standard clustering algorithm ( i.e. , k -means).
 (the initial weight is 0 according to Equation ( 6 )). We update the weight tensor at the end of each iteration as follows: where  X  is a decay parameter. We set  X  to be 0 . 95 for all experiments. From Equation ( 8 ), we can see that W will converge to 1 as the number of iterations goes up. Thus, the whole framework will converge. The framework is shown in Algorithm 1 . In this section, we validate the effectiveness of the proposed T-MIC framework for multi-source incomplete clustering on both synthetic and real-world data. 4.1 Baselines and Metrics We compare the proposed T-MIC framework with a number of baselines. In particular, we compare with:  X  Feature Concatenation (Concat) : Concatenating the features of each view, and then running standard k -means clustering (after dimension reduc-tion, if needed).  X  Canonical Correlation Analysis based Feature Extraction (CCA) :
Applying CCA for feature fusion from multiple views of the data [ 6 ], and then running standard k -means clustering using these extracted features.  X  Co-regularized Spectral Clustering (Co-regSC) : Using co-regularized framework to spectral clustering [ 11 ]. We choose Gaussian kernel to build the affinity matrix for each view and set the parameter in this algorithm to be 0.01 as suggested.  X  Collective Kernel Learning (CoKL-KCCA) : Collectively learn the ker-nel matrices for each source and applying kernel canonical correlation analy-sis [ 21 ]. This algorithm does not require the completeness of the data sources.
However, CoKL-KCCA can only be applied to two sources. It also assumes two sources can cover all the instances, which may not be true in many real-world applications. This assumption makes it difficult to extend CoKL-
KCCA to more than two sources.  X  Tensor based Multi-source Incomplete data Clustering (T-MIC) :
This is the proposed framework. In the experiments, we empirically set  X   X  = 1. Considering the fact that there is no known closed-form solution to determine the rank R of a tensor a priori, and rank determination of a tensor is still an open problem [ 23 ], we set R equal to the number of clusters
In addition, Gaussian kernel is used and the width parameter is set as the median pairwise distance between instances.
 Note that Concat, CCA and Co-regSC only work for complete data sources. We first fill the incomplete sources with average values, then we apply these approaches.
 The clustering result is evaluated by comparing the clustering index of each data point with the label provided by the data sources. Two metrics, the aver-age purity (Purity) and the normalized mutual information (NMI) are used to measure the clustering performance. Since k -means is sensitive to initial seed selection, we run k -means 30 times on each parameter setting, and report the averaged NMI and purity. All the data we used in the experiments are complete, but we randomly delete some of the instances in each data source. The incom-plete percentage for each source is equal in the experiments. To generate 30% incomplete data, we randomly delete 30% of the instances from every sources. We compare the clustering results for different incomplete percentage (from 5% to 50%).
 4.2 Dataset Collection One synthetic and two real-world data are used in the experiments.  X  Synthetic data (Synthetic) : The synthetic data consists of two sources  X  Internet Advertisements (ADs) : This data represents a set of possible  X  Handwritten Dutch Digit Recognition (Digit) : This data contains 4.3 Experimental Results Fig. 3 -Fig. 8 demonstrate the clustering performance of different methods on all the three data. In order to randomize the experiments, 30 test runs on every parameter settings were conducted and the average performance are reported. From the results, we can observe that as the incomplete percentage goes up, the clustering performance goes down. This is because when the incomplete per-centage gets larger, the amount of useful information remained in the incomplete sources gets less. However, we can still observe that T-MIC outperforms all the other four comparison methods for all the settings.
 margin as large as 17% in NMI and 16.8 % in average purity. We can also observe from Fig. 3 and Fig. 4 , that when the incomplete percentage is 0 ( i.e. , both sources are complete), CoKL-KCCA is slightly better than T-MIC. However, as the incomplete percentage increases, the performance of CoKL-KCCA gets worse. It is also worth mentioning that when the incomplete percentage is large (30%-50%), CCA becomes the best baseline (closest to T-MIC).
 On the Internet advertisements data, we also observe that T-MIC achieves the best performance in both NMI and average purity compared with the base-lines. One interesting observation from Fig. 5 and Fig. 6 is that Concat performs better than other baselines in many situations. The reason behind this may be that the data itself does not contain too much useful information (with complete sources, we only get around 0.3 for NMI). For a less informative data, the efforts to find latent spectral feature (Co-regSC) or maximize the correlation (CCA and CoKL-CCA) may lead to worse performance.
 For the handwritten digit data, we cannot apply CoKL-KCCA since it can only be applied to two-source incomplete data. From Fig. 7 and Fig. 8 ,wecan observe that T-MIC still achieves the best performance in NMI and average purity compared with the baselines. It is also worth mentioning that Co-regSC remains the best baseline method for almost every settings, while for Synthetic data and Ads data, Co-regSC is not that good. The reason why Co-regSC is better than other baselines is that Co-regSC co-regularizes the latent feature matrices for all the sources towards a common consensus while the other two baselines don X  X . Thus, for a complex task (more sources and more clusters) Co-regSC can get better performance. However, T-MIC uses tensor to model the multi-source incomplete data, which can better capture the relationship between sources. So the iteratively tensor factorization with sparsity constraint can learn the common latent features more accurately.
 Our work is related to both multi-view learning and data fusion techniques. We briefly discuss both of them. Multi-view learning [ 3 , 8 , 18 , 22 ], is proposed to learn from instances which have multiple representations in different feature example, [ 1 ] developed and studied partitioning and agglomerative, hierarchical multi-view clustering algorithms for text data. [ 10 , 11 ] are among the first works proposed to solve the multi-view clustering problem via spectral projection. [ 17 ] proposed a novel approach to use mapping function to make the clusters from different pattern spaces comparable and hence an optimal cluster can be learned from the multiple patterns of multiple views. Linked Matrix Factorization [ 24 ] is proposed to explore clustering of a set of entities given multiple graphs. [ 15 ] used nonnegavite matrix factorization to integrate multi-view data. Recently, [ 25 ] proposed a kernel based approach which allows clustering algorithms to be applicable when there exists at least one complete view with no missing data. As far as we know, [ 14 , 21 ] are the only two works that do not require the completeness of any view.
 The goal of such approaches is to integrate information from multiple data sources to create a single representation that are more complete and accurate than those derived from any individual data source, i.e. , the whole is greater than the sum of its parts. So far, many integration architectures have been proposed, a thorough survey of these techniques can be found in [ 2 ]. Here we only focus on tensor based data fusion architecture. The work most closed to our method are [ 16 ] and [ 19 ]. In [ 16 ], a clustering method based on Tucker decomposition was proposed to integrate multiple view data without the sparsity constraint. In [ 19 ], a multi-view clustering method based on CP factorization and sparsity constraint was proposed for multi-view graph data. However, all of the above mentioned methods can only deal with complete multi-source data.
 First, to the best of our knowledge, we take the first step toward studying the problem of multi-source clustering, in which each data source suffers from missing information and data. Second, T-MIC has no constraints on the feature space of the datasets. T-MIC constructs a tensor by only using the kernel matrices, which is independent of the types/dimensions of the data sources. Third but not last, T-MIC does not have limitation for the number of available data sources. The high order/dimension nature of tensor makes it suitable for modeling multi-source problems. Clustering via T-MIC framework can be easily extended to (
K&gt; 2) data sources. In this paper, we study the problem of clustering on multi-source incomplete data. We proposed a tensor-based modeling and factorization framework, T-MIC, to integrate multiple incomplete data sources. We first construct an initial tensor from the multi-source incomplete data using kernel matrices. Then a joint tensor factorization process with sparsity constraint is applied to itera-tively factorize the tensor. A common latent feature space across all the sources can be learned simultaneously. The experiments on both synthetic and real-world data were conducted to evaluate the clustering performance. It can be clearly observed that the proposed approach outperforms the comparison algorithms in almost every scenario.

