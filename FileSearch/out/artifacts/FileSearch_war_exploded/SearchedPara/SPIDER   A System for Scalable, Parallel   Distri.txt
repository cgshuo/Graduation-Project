 RDF is a data model for representing labeled directed graphs, and it is used as an important building block of semantic web. Due to its flexibility and applicability, RDF has been used in applications, such as semantic web, bioinformat-ics, and social networks. In these applications, large-scale graph datasets are very common. However, existing tech-niques are not effectively managing them. In this paper, we present a scalable, efficient query processing system for RDF data, named SPIDER, based on the well-known par-allel/distributed computing framework, Hadoop. SPIDER consists of two major modules (1) the graph data loader, (2) the graph query processor. The loader analyzes and dis-sects the RDF data and places parts of data over multiple servers. The query processor parses the user query and dis-tributes sub queries to cluster nodes. Also, the results of sub queries from multiple serv ers are gathered (and refined if necessary) and delivered to the user. Both modules uti-lize the MapReduce framework of Hadoop. In addition, our system supports some features of SPARQL query language. This prototype will be foundation to develop real applica-tions with large-scale RDF graph data.
 Categories and Subject Descriptors: H.2 [DATABASE MANAGEMENT]: Physical Design, Distributed databases General Terms: Algorithms, Design, Reliability
Keywords: Distributed, RDF, Semantic Web, Triple Store
RDF is a data model for representing labeled and directed graphs and is used as important building blocks of semantic web. RDF can also be extended easily to ontologies, such as RDFS and OWL, where these ontologies provide a means to define vocabularies specified to some domain, schemas, and relations between elements of vocabulary. Recently, owing to its flexibility and applicability, RDF has been popularly used in a variety of applications, such as semantic web, bioin-formatics, and social networks, wherein a huge computation over a large data set is required.

For example, in semantic web large-scale graph datasets are required for complex inferencing; in life science many projects are accumulating DNA sequence and protein in-teraction network datasets; and the social networks area needs massive sample data sets for complex mining of re-lations between people (and groups). In these days, vol-ume of graph data generated from these applications reach tens of petabytes. However, existing systems are not suffi-cient to deal with these large-scale RDF graph datasets due to the following two reasons: limited I/O bandwidth and NP-Completeness of various graph problems (e.g. subgraph isomorphism).

As a practical approach to overcome these problems above, we use the server-cluster approach, where the large RDF data is partitioned and distributed over multiple servers, and the given complex subgraph query is dissected/delivered into multiple sub queries and processed over multiple servers in a distributed and parallel manner. The approach appears to be simple, but there are some challenges on the way of distribution: (i) how to distributed RDF data (ii) How to distribute the computation (and also gathering the results)
In this demonstration, we present a RDF subgraph pro-cessing system, which enables S calable, P arallel /D istributed E valuation of large-scale R DF data (SPIDER). It aims to store and process large-scale RDF graph datasets. In order to store and process a huge amount of RDF graph data, we use a distributed file system and employ the MapReduce framework[5] respectively.
This section gives an overview the design and implemen-tation issues of SPIDER. In Section 2.1, we briefly presents the system architecture. In Section 2.2, we present how SPIDER performs graph query processing in a distributed manner.
In order to provide extensible storage and scalable pro-cessing capability, SPIDER is based on not only distributed file system (DFS) but also distributed processing technique. In order to focus on distributed subgraph processing, we make use of Hadoop[1] and HBase[4] (i.e., a clone of Bigtable[3]) rather than implementing new distributed framework that supports DFS and distributed and parallel processing. The SPIDER system consists of master and triple servers. The master is an entry point for external applications and manages the triple servers. The triple servers keep assigned triples; a triple is a basic data element of RDF data. Besides, triple servers play either mappers or reducers in MapReduce Figure 1: Query Processing carried out with MapReduce framework while processing RDF graph queries. Each triple server stores RDF data by making use of column-oriented store[6]. In terms of semi-structured data, RDF data are very likely to be sparse because each resource has various attributes (or predicates), and it is designed for integrat-ing heterogeneous schema dataset. Therefore, RDF data is suited for column-oriented store which is effective in han-dling sparse and wide tables.

In addition, the SPIDER system has two modules, which are (1) the graph loader and (2) the graph query processor. The graph loader plays a role to import the a large amount of RDF data from distributed file system to SPIDER store system. The graph query processor plays a role to search subgraphs from their own local disks and to refines candi-dates of final results.
Overall, with map-reduce processing the graph query pro-cessor searches subgraphs matched to a given query. Al-though MapReduce paradigm makes query processing of SPIDER scalable, it incurs several challenges. One of the most crucial problems is that each triple server is not aware of entire RDF data. In other words, a triple server has only partial datasets. Although SPIDER uses a distributed file system, triple servers do not communicate one another during processing because MapReduce is based on shared-nothing architecture that does not restrict communication during processing. Hence, each triple server searches sub-graphs from data sets stored on its own local storage. As a result, it cannot discard even partial matched subgraphs be-cause each triple server can not determine if they belong to final results. For this reason, SPIDER needs further refine-ment processing to find compositions of subgraphs that form complete subgraphs. And then, SPIDER checks if these complete subgraphs are matched to the given query, and then it produces final results. Note that the refinement processing incurs severe computation and communication costs that deteriorate the performance of query processing. Therefore, we need to suppress the refinement processing. We found that preserving the locality of RDF graph data in regards to their connectivity reduces the the number of the refinement processing. For this reason, we sort stored triples in order of their URIs so as to give small locality to RDF data. Also, we remain this problem for future work about giving further locality of stored RDF data.

Fig. 1. shows a RDF graph query processing architec-ture. In the figure, a SPIDER cluster composed of three nodes (i.e., N 1 , N 2 and N 3 )isprocessingqueries( Q 1 , and Q 3 ). In the first, map operations retrieve the partial matched subgraphs. After this step, mappers pass retrieved subgraphs to reducers. And then, nodes N 1 , N 2 and N 3 per-form reduce operations for queries Q 1 , Q 2 and Q 3 respec-tively. During this step, each of them combines partially matched subgraphs and check them if combined subgraphs matched to the given query.
To illustrate how the SPIDER is scalable, we will demon-strate the SPIDER running on a cluster. In this demo, we use the 100 million triples generated by the Berlin SPARQL Benchmark [2].

SPIDER provides a graphic user interface (GUI) that fea-tures the graph loader and the graph query UI. The graph loader UI enables users to import a RDF dataset from a lo-cal file system or DFS. The graph loader UI enables users to give an instance name to the loaded graph data.
The graph query UI provides a text form to enable users to submits subgraph patterns defined by SPARQL query language. When a user submit a query, the query processing starts with map-reduce operation. After query processing, the graph query UI outputs results as a list of triples. We built a SPIDER cluster on 6 machines (consist of 2 Xeon Quad-Core, 2 Core2 duo, 2 Pentium 4). We conduct experiments for the graph loading and the graph querying with basic graph pattern in SPARQL. The graph loading takes 16 minutes to load 100 milion triples; 10k triples/s. The basic graph pattern takes from several seconds to tens of seconds in terms of the given query size. If you want fur-ther information, contact http://dbserver.korea.ac.kr/ projects/spider .
This work was supported by the Korea Science and En-gineering Foundation (KOSEF) grant funded by the Korea government (No. R01-2008-000-20564-0). [1] Apache Software F oundation. H adoop, 2006. [2] C. Bizer and A. Schultz. The berlin sparql benchmark, [3] F.Chang,J.Dean,S.Ghemawat,W.C.Hsieh,D.A.
 [4] J.-D. Cryans, B. Duxbury, J. Kellerman, A. Purtell, [5] J. Dean and S. Ghemawat. Mapreduce: Simplified data [6] M. Stonebraker, D. J. Abadi, A. Batkin, X. Chen,
