 In this paper, we try to predict which category will be less ac-curately classified compared with other categories in a clas-sification task that involves multiple categories. The cat-egories with poor predicted performance will be identified before any classifiers are trained and additional steps can be taken to address the predicted poor accuracies of these cat-egories. Inspired by the work on query performance predic-tion in ad-hoc retrieval, we propose to predict classification performance using two measures, namely, category size and category coherence . Our experiments on 20-Newsgroup and Reuters-21578 datasets show that the Spearman rank corre-lation coefficient between the predicted rank of classification performance and the expected classification accuracy is as high as 0.9.
 H.3.6 [ Information Storage and Retrieval ]: Library Au-tomation X  Large text archives ; I.5.2 [ Pattern Recogni-tion ]: Design Methodology X  Classifier design and evalua-tion Classification performance prediction, text classification Measurement, Performance
Text classification is analogous to disease diagnosis. Some diseases are more accurately diagnosed than others. In text classification, some category may be more difficult to classify accurately than other categories. To find out whether clas-sification accuracy can be predicted, we conducted experi-ments on 20-Newsgroup and Reuters-21578 datasets using three widely-adopted classifiers, i.e., Na  X   X ve Bayes,  X  -Nearest Neighbor (KNN) and Support Vector Machines (SVM). The three classifiers are significantly different in their classify-ing algorithms. Evaluated by Area under Precision-Recall Curve (AUP), the classification accuracies of 20 categories from Newsgroup and 26 categories from Reuters datasets re-spectively are plotted in Figures 1(a) and 1(b) respectively.
As shown in Figure 1(a), the three classifiers produced very similar curves on Newsgroup. Spearman correlation coefficient on the results by any pair of classifiers are larger than 0.9. All classifiers found the categories c.pc.hardware , sci.electoronics , talk.religion.misc and talk.politics.misc much more difficult to classify. Based on the category names, these 4 categories likely cover broad topics. On Reuters dataset, the three classifiers produced slightly different curves. Yet their results are largely correlated with coefficients all above 0.6 tested by Spearman correlation for any pair of classifiers. This results show that in general some categories are more difficult to classify than others.

Given training examples for a number of categories, our research problem is to predict classification accuracies on these categories without constructing classifiers. This prob-lem is interesting as its results could provide useful guide-lines on which categories require further investigation when constructing classifiers on them. Given a category with poor predicted accuracy, one may want to investigate its train-ing documents and try to uncover the possible reasons, e.g., insufficient/noisy training examples, or the category covers too many sub-topics (e.g., sports may cover many kinds of sports not necessarily related to each other).

To the best of our knowledge, the problem of classifica-tion performance prediction has not been studied before. Besides formally defining the problem, our work has made another two major contributions. First, we establish the re-lationship between classification performance prediction and query performance prediction that is heavily studied in ad-hoc retrieval. Inspired by the latter, we propose 6 category coherence measures. Second, we show that category coher-ence measure alone is not sufficient in predicting category classification performance as its effectiveness could be heav-ily affected by category size . In our experiments, the best performing coherence measure coupled with category size achieved Spearman correlation coefficient of 0.9 on News-group and above 0.6 on Reuters.
Query performance prediction is to predict the effective-ness of a query in retrieving relevance documents from a col-lection. One significant contribution in query performance prediction is clarity score [1]. Clarity score of a query is computed as the distance between the query and the collec-tion language models. Formally, let  X  be a query consisting of query words {  X   X   X   X   X  } , and  X  be the set of top- X  ranked documents retrieved by  X  from collection  X  . The value of  X  is defined to be 500 in [1]. Query language model  X  (  X   X   X  ) is estimated by Equation 1, where  X  (  X   X   X  ) is obtained through Bayes X  theorem knowing  X  (  X   X   X  ) = the relative frequency of word  X  in  X  smoothed by  X   X  X  rel-ative frequency in the collection. The collection language model,  X  (  X   X  X  X  ), is estimated by the relative frequency of word  X  in collection  X  . The clarity score of query  X  is the Kullback-Leibler (  X  X  X  )-divergence between  X  (  X   X   X  ) and  X  (  X   X  X  X  ) given in Equation 2.
In text classification, if all documents in a category  X  are on a specific topic (e.g., rec.sport.hockey ), then  X  is expected easy to classify. However, if documents in  X  cover a very broad topic (e.g., talk.politics.misc ), then it is relatively hard to learn an accurate classifier on  X  . Assume there exists a query  X   X  which retrieves exactly all documents belonging to  X  from a collection  X  . The clarity score of  X   X  gives the topical coherence of  X  with respect to  X  . Hence we believe that the clarity score can be used to predict classification performance.
Given a dataset  X  where each document  X   X  X  X  is labeled with zero, one, or more categories from a pre-defined set of categories  X  = {  X  1 ,  X  2 ,  X  X  X  X  ,  X   X  X  X  X  } . The problem of clas-sification performance prediction is to predict a score  X  (  X  ) for each category  X   X  X  X  . This predicted score indicates the likelihood of getting accurate classification on category  X  . To simplify the problem slightly, classification performance prediction may assign a rank  X  (  X  ) to a category  X  instead of a real value score. High (or small) ranks correspond to high scores (i.e., accurate predicted classification performance).
To address the above problem, we assume that the classi-fier constructed for one category is independent of the clas-sifiers learned for other categories. A classifier for category  X  takes knowledge solely from two sets of documents, i.e.,  X  and  X  . Whether or not documents in  X  belong to other categories is not part of the input in constructing classifier for  X  . From the problem definition, the only information used in classification performance prediction for a category is  X  and  X  without actually constructing the classifier.
Based on the similar intuition to query performance pre-diction, a less topically cohesive category should be more difficult to classify. We propose to measure category coher-ence with two distances: With distance function defined, the next key step is to esti-mate the language models. Three document models, namely large , small , and centrality document models are used to es-timate  X  (  X   X   X  ). Similar document models were used in [2] for blog feed search for a given query. Our work differs as there is no queries in our modeling.

The details of estimating  X  (  X   X   X  ) using the three doc-ument models are discussed below.  X  (  X   X   X  ) is estimated similarly by simply replacing  X  with  X  in the formulation. Both  X  (  X   X   X  ) and  X  (  X   X   X  ) are then smoothed using Equa-tion 3 with  X  = 0 . 99.
With large document model , each category is treated as a single virtual document, ignoring document boundaries within the category. The category language model is there-fore simply the relative word frequency in the virtual docu-ment, shown in Equation 4.
With small document model , a category is described by a set of equally important documents in the category. The category language model is estimated by the averaged relative word frequency in these documents. The small document model can be considered as a special case of estimating query language model in clarity score (see Equation 1). Assuming a query  X   X  retrieves all documents in  X  with equal relevance, then  X  (  X   X   X   X  ) = 1  X   X   X  .
With centrality document model , documents that are more close to the centroid of the category are considered more representative and contribute more to the category language model. The category language model  X   X  (  X   X   X  ) is estimated using Equation 6, where  X  (  X   X   X  ) reflects the relative closeness of the document to the category X  X  centroid defined in Equation 7. In Equation 7,  X  (  X ,  X  ) is a centrality function defining the similarity between document  X  to category  X  . Following [2],  X  (  X ,  X  ) is defined to be the weighted geometric mean of word generation probabilities in  X  shown in Equation 8. The weight of each word is its likelihood in document  X  . The centrality document model better simulates the clarity score with an estimated  X  (  X   X   X   X  ) by Equation 7. As  X   X  effectively retrieves all documents in  X  , it is reasonable to estimate  X  (  X   X   X   X  ) by  X  (  X   X   X  ) with Equation 7.

With two distance functions,  X  X  X  (  X   X  X  X  ) and  X  X  X  (  X   X   X  ), and three document models, a total of 6 coherence measures can be defined as summarized in Figure 2(c).
It is well understood that documents contain noise (or subtopics) besides their main topics. The chance of observ-ing a topic-irrelevant word increases when  X   X   X  getting larger, leading to smaller  X  X  X  (  X   X  X  X  ) and  X  X  X  (  X   X   X  ) assuming doc-uments in  X  do not focus on a particular topic. However, in text classification, larger number of training documents in a category often means more accurate classifier. This con-tradicts our earlier assumption that smaller  X  X  X  (  X   X  X  X  ) and  X  X  X  (  X   X   X  ) means the category is more difficult to classify.
To study the relationship between classification perfor-mance and category size, we take the category size in con-sideration and propose a coefficient  X  (  X  ) for each category  X  defined in Equation 9, where  X  is a parameter controlling the impact of the category size. The PDLD measure listed in Figure 2(c) then becomes:  X  (  X  )  X   X  X  X  (  X   X  X  X  ). The other measures are extended similarly with the coefficient. Our experiments were conducted on 20-Newsgroup and Reuters-21578 datasets. We used the  X  X ydate X  version of 20-Newsgroup 1 , which contains 11,293 training and 7,528 test documents. All the 20 categories were used as target categories in our experiments. For Reuters-21578,  X  X od-Lewis X  split was used with 13,625 training and 6,188 test documents. The 26 categories each having at least 50 posi-tive training documents were used in our experiments. http://web.ist.utl.pt/  X acardoso/datasets/ .
Note that for a given dataset, there is no ground truth on which category is more difficult to classify than another. To estimate the classification accuracy, we classified the test documents of each category using three different classifiers (Na  X   X ve Bayes, KNN, and SVM). The averaged AUP over the three classifiers on each category is used as the expected classification accuracy for the category. Na  X   X ve Bayes was implemented based on the Multinomial Model [3]. Weighted KNN was implemented referencing to [4] and the number of nearest neighbors  X  = 30 in our experiments. For SVM, we used  X  X  X   X   X  X  X  X  X  X  X  package 2 with linear kernel and default setting for all other parameters.

To evaluate the effectiveness of the measures proposed, we compute the Spearman rank correlation coefficient between the measures and the expected classification accuracies on the categories. We emphasize that the proposed measures predicate classification accuracy using solely training doc-uments and the expected classification accuracies are ob-tained by classifying test documents .
The only parameter in the proposed measures is  X  used to control the impact of category size. We therefore report the results with varying  X  . The Spearman correlation coefficient on Newsgroup are plotted in Figure 2(a). The following observations are made: To summarize, on Newsgroup dataset, both the choices of distance function (e.g.,  X  X  X  (  X   X   X  ) ) and document model (e.g., centrality model) had significant impact on the predic-tion. Category size had limited impact. Even when  X  = 0 , PNCD achieved 0.8 with Spearman correlation.

Figure 2(b) plots the Spearman coefficient for Reuters dataset. It is observed that (i) again  X  X  X  (  X   X   X  ) was more effective than  X  X  X  (  X   X  X  X  ); (ii) all three document models performed similarly; and (iii) more importantly, the cate-gory size had significant impact on prediction performance. When category size was not considered (i.e.,  X  = 0 ), nega-tive correlation with classification accuracy was observed by PDSD and PDLD. The prediction performance of all mea-sures increased sharply from  X  = 0 to  X  = 0 . 4 and reached their peaks when  X  = 0 . 4, followed by slight degradation with larger  X  .

Comparing the results obtained from the two datasets, it is interesting to note the following observations: http://svmlight.joachims.org/
To find out the reasons behind, we obtained data statistics on the two datasets. On Reuters dataset, the mean docu-ment length from different categories falls into the range of 50 to 150 with most categories having average document length about 100. The standard deviations on different cat-egories are also in the range of 50 to 150. However, on Newsgroup dataset, the mean document length varies from 50 to 200 for different categories. More importantly, the standard deviations of document length in most categories are significantly larger than that in Reuters.
 From the statistics, document length varies greatly in Newsgroup dataset. Because of the variety in document length, with large document model, the estimated language model is heavily biased towards the long documents in News-group dataset. With small document model, all documents are treated equally important and those very short docu-ments that are not very close to the centroid of the cate-gory introduce noise into the estimated category language model. The centrality document model treats the docu-ments that are more close to the major topic of the category more importantly and is able to estimate a more accurate category language model. Nevertheless, for Reuters dataset, the documents are news articles written by professionals. The lengths of the documents are more regular compared to UseNet messages and the documents are also believed con-taining less noise. The category language models estimated from the three document models are likely to be similar to each other.

The category size ratio from the two datasets are also ob-tained, where category size ratio is defined by the percent-age of positive training examples among all examples, i.e., evenly distributed in the 20 categories. For the same  X  , all categories receive nearly the same category size coeffi-cient. This explains why category size had minimum im-pact on Newsgroup. On Reuters dataset, the category size ratio, however, varies significantly. The largest category has size ratio about 0.25 and about 10 categories have size ra-tio around 0.005. As discussed in Section 4.2, the number of documents in a category does affect coherence measures, it is not a surprise that category size had large impact on Reuters dataset.

The category size ratio also explains why the choice of distance function (e.g.,  X  X  X  (  X   X   X  ) and  X  X  X  (  X   X  X  X  )) was less sensitive on Reuters dataset. Recall that in Reuters dataset, about 10 categories has category size ratio around 0.005. For each of these categories, because of such a small number of positive documents,  X   X   X  +  X   X   X   X   X   X   X  . The language model estimated from  X  =  X   X   X  is therefore very similar to the language model estimated from  X  alone. Hence  X  X  X  (  X   X   X  ) and  X  X  X  (  X   X  X  X  ) return similar values.
Most works in text classification target on improving the classification accuracy. Little has been done to understand the example documents themselves for their effectiveness in training accurate classifiers. In this paper, we study classifi-cation performance prediction and try to predict the classi-fication accuracies of categories based on their training ex-amples. Relating the problem to the query performance pre-diction problem studied in ad-hoc retrieval, we proposed 6 coherence measures. We have also discussed the impact of the category size in predicting classification accuracies. In our experiments, the distance between the category language model and negative category model estimated with central-ity document model showed its effectiveness in classification accuracy prediction. How to uncover the reasons behind dif-ficult categories based on the measures and further study of the impact of category size are the two major directions of our future work. [1] S. Cronen-Townsend, Y. Zhou, and W. B. Croft.
 [2] J. L. Elsas, J. Arguello, J. Callan, and J. G. Carbonell. [3] A. K. McCallum and K. Nigam. A comparison of event [4] S. Tan. Neighbor-weighted k-nearest neighbor for
