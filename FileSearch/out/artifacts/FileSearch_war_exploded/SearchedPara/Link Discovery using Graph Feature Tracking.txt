 The prediction of the future state of an evolving graph is a challenge of interest in many applications such as predicting hyperlinks of webpages [16], finding protein-protein interactions [7], studying social networks [9], as well as collaborative filtering and recommendations [6]. Link prediction can also be seen as a special case of matrix completion where the goal is to estimate the missing entries of the adjacency matrix of the graph where the entries can be only  X 0s X  and  X 1s X . Matrix completion became popular after the Netflix Challenge and has been extensively studied on both theoretical and algorithmic aspects [15]. In this paper we consider a special case of predicting the evolution of a graph, where we only predict the new edges given a fixed set of vertices of an undirected graph by using the dynamics of the graph over time.
 Most of the existing methods in matrix completion assume that weights over the entries (i.e. the edges of the graph, e.g. scores in movie recommendation applications) are observed [3]. These weights provide a richer information than the binary case (existence or absence of a link). Consider for instance the issue of link prediction in recommender systems. In that case, we consider a bipartite graph for which the vertices represent products and users, and the edges connect users with the products they have purchased in the past. The setup we consider in the present paper corresponds to the binary case where we only observe purchase data, say the presence of a link in the graph, without any score or feedback on the product for a given user. Hence, we will deal here with the situation where the components of snapshots of the adjacency matrix only consist of  X 1s X  and missing values. Moreover, link prediction methods typically use only one snapshot of the graph X  X  adjacency matrix -the most recent one -to predict its missing entries [9], or rely on latent variables providing semantic information for each vertex [11]. Since these methods do not use any information over time, they can be called static methods . Static methods are based on the heuristic that some topological features, such as the degree, the clustering coefficient, or the length of the paths, follow specific distributions. However, information about how the links of the graph and its topological features have been evolv-ing over time may also be useful to predict future links. In the example of recommender systems, knowing that a particular product has been purchased by increasingly more people in a short time window provides useful information about the type of the recommendations to be made in the next period. The main idea underlying our work lies in the observation that a few graph features can capture the dynamics of the graph evolution and provide information for predicting future links. The purpose of the paper is to present a procedure which exploits the dynamics of the evolution of the graph to find unrevealed links in the graph. The main idea is to learn over time the evolution of well-chosen local features (at the level of the vertices) of the graph and then, use the predicted value of these features on the next time period to discover the missing links. Our approach is related to two theoretical streams of research: matrix completion and diffusion models. In the latter only the dynamics over time of the degree of a particular vertex of the graph are modeled -the diffusion static matrix completion methods, only a few methods have been developed that combine static and dynamic information mainly using parametric methods  X  see [4] for a survey. For example, [13] embeds graph vertices on a latent space and use either a Markov model or a Gaussian one to track the position of the vertices in this space; [10] uses a probabilistic model of the time interval between the appearance of two edges or subgraphs to predict future edges or subgraphs. However, to the best of our knowledge, there has not been any regularization based method for this problem, which we consider in this paper.
 The setup of dynamic feature-based matrix completion is presented in Section 2. In Section 3, we develop a fast linearized algorithm for efficient link prediction. We then discuss the use and esti-mation of relevant features within this regularization approach in Section 4. Eventually, numerical experiments on synthetic and real data sets are depicted in Section 5. Setup. We consider a sequence of T undirected graphs with n vertices and n  X  n binary adjacency matrices A t ,t  X  X  1 , 2 ,...,T } where for each t the edges of the graph are also contained in the graph likely to appear at time T + 1 , that is, the most likely non-zero elements of the binary adjacency matrix A T +1 . To this purpose we want to learn an n  X  n real-valued matrix S whose elements indicate how likely it is that there is a non-zero value at the corresponding position of matrix A T +1 . The edges that we predict to be the most likely ones at time T + 1 are the ones corresponding to the largest values in S .
 We assume that certain features of matrices A t evolve over time smoothly. Such an assumption is necessary to allow learnability of the evolution of A t over time. For simplicity we consider a linear feature map f : A t 7 X  F t where F t is an n  X  k matrix of the form F t = A t  X  , with  X  an n  X  k matrix of features. Various feature maps, possibly nonlinear, can be used. We discuss an example of such features  X  and a way to predict F T +1 given past values of the feature map F 1 ,F 2 ,...,F T in Section 4  X  but other features or prediction methods can be used in combination with the main part of the proposed approach. In the proposed method discussed in Section 3 we assume for now that we already have an estimate of F T +1 .
 An optimization problem. The procedure we propose for link prediction is based on the assumption that the dynamics of graph features also drive the discovery of the location of new links. Given the last adjacency matrix A T , a set of features  X  , and an estimate b F of F T +1 based on the sequence of adjacency matrices A t ,t  X  { 1 , 2 ,...,T } , we want to find a matrix S which fulfills the following requirements: For any matrix M , we denote by k M k F = p Tr ( M 0 M ) , the Frobenius norm of M , with M 0 being the transpose of M and the trace operator Tr ( N ) computes the sum of the diagonal elements of the square matrix N . We also define k M k  X  = P n k =1  X  k ( M ) , the nuclear norm of a square matrix M of size n  X  n , where  X  k ( M ) denotes the k -th largest singular value of M . We recall that a singular value of matrix M corresponds to the square root of an eigenvalue of M 0 M ordered decreasingly. The proposed optimization problem for feature-based matrix completion is then: and where  X  and  X  are positive regularization parameters. Each term of the functional L reflects the aforementioned requirements for the desired matrix S . In the case where  X  = 0 , we do not use information about the dynamics of the graph. The minimizer of L corresponds to the singular value thresholding approach developed in [2], which is therefore a special case of (1). Note that a key difference between link prediction and matrix completion is that in (1) the training error uses all entries of the adjacency matrix while in the case of matrix completion only the known entries (in our case the  X 1s X ) are used. We now discuss an efficient optimization algorithm for (1), the main part of this work. Solving (1) is computationally slow. We adapt the fast linearization method developed in [5] to our problem, which attains an optimal iteration complexity when using only first order information. Here, the functional L ( S, X , X  ) is continuous and convex but not differentiable with respect to S . We propose to convert the minimization of the target functional L ( S, X , X  ) into a tractable problem through the following steps: where  X  B,C  X  = Tr ( B 0 C ) for two matrices B , C . The tuning of the parameter  X  will be discussed with the convergence results at the end of this section. We denote by m G ( e S ) the minimizer of G now formulate an algorithm for the fast minimization of the functional L  X  ( S, e S ) inspired by the algorithm FALM in [5] (see Algorithm 1 ). Note that, in the alternating descent for the simultaneous minimization of the two functions G  X , X  and H  X  , we use an auxiliary matrix Z k . This matrix is a linear combination of the updates for S and e S . The work by Ma and Goldfarb shows indeed that the particular choice made here leads to an optimal rate of numerical convergence. Key formulas in the case, these minimizers have explicit expressions which can be derived when solving the first-order optimality condition as Proposition 1 shows.
 Algorithm 1 -Link Discovery Algorithm Parameters :  X , X , X 
Initialization : W 0 = Z 1 = A T ,  X  1 = 0 for k = 1 , 2 ,... do end for Proposition 1 Let  X  S = e S  X   X   X  h ( e S ) and the singular value decomposition b S = b U Diag ( also consider the singular value decomposition of S denoted by S = U Diag (  X  X  ) V . We set the notation, for x &gt; 0 : We then have: The proof can be found in the Appendix.
 Validity of the approximations and rates of convergence. Our strategy replaces the non-differentiable term in L by a smooth version of it. The next result offers guarantees that minimizing the surrogate function (3) provides an approximate solution of the initial problem (1). We will say that an element x is an -optimal solution of a function  X ( x ) if it is such that  X ( x )  X  inf x  X ( x ) + . Proposition 2 The following statements hold true: The proof of this result can be derived straightforwardly from [5]. Moreover, following the proof of Theorem 4.3 in [5], one can show that the number of iterations in order to reach an -optimal solution of L  X  using Algorithm 1 is of the order O (1 / of the parameter  X  is provided as the inverse of the largest value for the Lipschitz constant of each of the gradients of g  X  and h . With our notations, we can easily derive here:  X  = min  X / X , 1 / (1 +  X  X  1 ( X )) , where  X  1 ( X ) is the largest singular value of  X  . As discussed above one can use various features  X  and methods to predict the n  X  k matrix F T +1 given past values of the feature map F 1 ,F 2 ,...,F T . We consider a particular case here to use in conjunction with the main algorithm in the previous section. In particular, we use as features  X  the first k eigenvectors of the adjacency matrix A T . Let A T =  X  X  X  X  0 be the orthonormal eigenvalue A The suggested method aims to estimate A T +1  X  that is informative for the reconstruction of A T +1 . We denote by  X  j , j  X  X  1 , 2 ,...,k } the n -dimensional feature vectors which are the columns of  X  . which describes the evolution of the j -th feature over the n vertices of the graph. We now describe the procedure for learning the evolution of this j -th graph feature over time: between m consecutive values of ( A t  X  m  X  j ,...,A t  X  1  X  j ) and the next value A t  X  j is stable over time (stationarity assumption). Clearly methods other than ridge regression or other ways of creating the training data can be used, which we leave for future work. We tested the proposed method using both simulated and real data sets. As benchmarks we use the following methods: 5.1 Synthetic Data We generate sequences of graphs as follows. We first generate a sequence of T matrices Q ( t ) of size n  X  r whose entries Q i,j ( t ) are increasing over time as a sigmoid function : synthetic model for the evolution of the graph over time. We then add noise to the time dynamics as follows. For a given noise level  X   X  [0 , 1] we replace each entry of Q i,j ( t ) with probability  X  generate the adjacency matrix A t as for a threshold  X  . We pick  X  so that the sparsity (i.e. proportion of non-zero entries) of A T reflects the sparsity of the real data used in the next section (  X  10  X  3 ). In the experiments, we simulated graphs with n = 1000 vertices. 5.2 Real Data Collaborative Filtering 1 We can see the purchase histories of e-commerce websites as graph sequences where links are established between a user and a product when the user purchases that product. We use data from 10 months music purchase history of a major e-commerce website to evaluate our method. For our test we selected a set of 10 3 users and 10 3 products that had the highest degrees (number of sales). We split the 8 . 5  X  10 3 edges of the graph (corresponding to purchases) into two parts following their occurrence time. We used the data of the 8 first months to predict the features at the end of the 10th month and use these features as well as the matrix at the end of the 8th month to discover the purchases during the 2 last months. 5.3 Results The results are shown in Figure 1 and Tables 1 and 2. The Area Under an ROC Curve (AUC) is reported. For the simulation data we report the average AUC over 10 simulation runs.
 From the simulation results we observe that for low rank underlying matrices, our method outper-forms the rivals. The same comparative results were observed for ranks as high as 100. Our method (as well as the static low rank method based on the low rank hypothesis) however fails when the rank of S ( t ) is high. However, even in this case our method outperforms the method of static matrix completion.
 The results with the real data further indicate the advantage of using information about the evolution of the graph over time. Similarly to the simulation data, the proposed method outperforms the static matrix completion one. The main contribution of this work is the formulation of a learning problem that can be used to predict the evolution of the edges of a graph over time. A regularization approach to combine both static graph information as well as information about the dynamics of the evolution of the graph over time is proposed and an optimization algorithm is developed. Despite using simple graph features Figure 1: AUC performance of the proposed algorithm with respect to the two parameters  X  and  X  on simulated data. Table 1: Simulation data. The average AUC over 10 simulation runs is reported. For each row the pair of numbers in the first column show the rank r and the noise level  X  . as well as estimation of the evolution of the feature values over time, experiments indicate that the proposed optimization method improves performance relative to benchmarks. Testing, or learning, other graph features as well as other ways to model their dynamics over time may further improve performance and is part of future work.
 We first write the optimality condition for G  X , X  ( S,  X  S ) with respect to S : Table 2: Collaborative Filtering data; AUC for different values of  X  and  X  . The AUC of preferential attachment is 0.6019, and Katz reaches 0.6670 With the notations for b S , the previous condition can be written: We now use the fact that  X  g  X  ( S ) =  X U Diag (min {  X , 1 } ) V where S/ X  = U Diag (  X  ) V (see [5]). This observation leads to the solution where S satisfies: Similarly, the optimality condition of H  X  ( S, e S ) with respect to e S is Since the function h is differentiable as the sum of two quadratic terms, we have: and we can derive the optimal value for e S : Acknowledgments This work was partially supported by D IGIT  X  EO (B  X  EMOL project), that authors greatly thank. [1] A. L. Barab  X  asi, H. Jeong, Z. Nda, A. Schubert, and T. Vicsek. Evolution of the social network [2] Emmanuel J. Cand ` es and Terence. Tao. A singular value thresholding algorithm for matrix [3] Emmanuel J. Cand ` es and Terence Tao. The power of convex relaxation: Near-optimal matrix [4] Lise Getoor and Christopher P. Diehl. Link mining: a survey. SIGKDD Explorations Newslet-[5] Donald Goldfarb and Shiqlan Ma. Fast alternating linearization methods for minimizing the [6] Yifan Hu, Yehuda Koren, and Chris Volinsky. Collaborative filtering for implicit feedback [7] Hisashi Kashima, Tsuyoshi Kato, Yoshihiro Yamanishi, Masashi Sugiyama, and Koji Tsuda. [8] Leo Katz. A new status index derived from sociometric analysis. Psychometrika , 18(1):39 X 43, [9] David Liben-Nowell and Jon Kleinberg. The link-prediction problem for social networks. [10] Tanya Y. Berger-Wolf Mayank Lahiri. Structure prediction in temporal networks using frequent [11] Kurt Miller, Thomas Griffiths, and Michael Jordan. Nonparametric latent feature models for [12] Yu Nesterov. Smooth minimization of non-smooth functions. Mathematical Programming , [13] Purnamrita Sarkar, Sajid Siddiqi, and Geoffrey J. Gordon. A latent space approach to dynamic [14] Ashish Sood, Gareth M. James, and Gerard J. Tellis. Functional regression: A new model for [15] Nathan Srebro, Jason D. M. Rennie, and Tommi S. Jaakkola. Maximum-margin matrix fac-[16] Ben Taskar, Ming-Fai Wong, Pieter Abbeel, and Daphne Koller. Link prediction in relational [17] Demetrios Vakratsas, Fred M. Feinberg, Frank M. Bass, and Gurumurthy Kalyanaram. The
