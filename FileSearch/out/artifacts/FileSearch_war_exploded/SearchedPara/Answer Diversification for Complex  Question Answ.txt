 Automatically generating a set of answers for complex questions , e.g. definition, opi-nion, and online community questions, remains a challenging task for several reasons. First, the information needs underlying this type of questions are often subjective and ill-defined [15]. Hence, it requires one or more answer passages to generate a com-plete response to complex questions. Furthermore, the answers themselves do not easily fall into predictable semantic classes [10]. So, name-entity style answer extrac-automatic response to return a set of factually diverse answers. 
Suppose that there are two sets of answers, A and B , generated by two different systems that response to a given question  X  what effect does steroid use have on ath-letes X  performance ? X  Set A consists of two answers { steroid helps boost athletic per-formance by improving muscle mass , steroids can cause many harmful effects } while set B contains { steroids enhance athletic performance , athletes use steroids to im-prove their performance }. An information seeker would find answers in set A to be more useful than those in set B . As illustrated, the two facts in the first set are relative-ly more novel than those in the second set. In other words, the factual coverage of the second set is less diverse than that of the first one. 1.1 Contributions In this research, we utilize a graph topology to rerank the answer s according to their relevance and novelty. The summary of our contributions is as follows: 1. We propose a graph ranking model called DiverseRank to extract a diverse set of answers for complex questions. Our method is motivated by the ideas that a good answer set should contain facts which are highly relevant as well as novel. The main contribution of our work is in the use of a graph topology to reduce redun-dancy among answers. We represent a set of answers as a set of vertices whose edges correspond to the similarity between answer nodes. A negative sign is as-signed to the edge weights to model the redundancy relationship between nodes. 
Then, the final ranking score for each answer node is derived from its long-term negative endorsement. 2. We conduct a comprehensive experiment on two distinct question answering data sets: a subset of Yahoo! Answers data and TREC 2006 X  X  complex questions data, to evaluate the performance of the proposed method. To measure the quality of the ex-tracted answers, we use a nugget pyramid [14] evaluation and a recall-by-length curve as the performance metrics. Specifically, we measure diversity in terms the amount of common information nuggets , a small text fragment that describes a cer-tain fact about a given question, between the extracted sets and the gold standard set. 1.2 Paper Organization The rest of the paper is organized as follows. First, we review related work in section experimental evaluation, including data sets, evaluation metrics, and procedures. Finally, we discuss about the results and conclude the paper in section 5 and 6, respectively. Several issues in web community question answering have been investigated, e.g. finding high-quality answers [9][19], maximizing the facet coverage [16], etc. How-ever, answer diversity issue has not been explored as much. Diversity in ranking has long been one of the major issues in many research areas, such as text summarization and information retrieval. Many information retrieval researchers have attempted to establish several theoretical frameworks of diversity ranking and evaluation [2][7]. There are a growing amount of works in text summarization area [6][9][12][20] which try to integrate diversity as part of the ranking function X  X  properties. For exam-ple, Zhu et al. [20] proposes a unified ranking algorithm called GRASSHOPPER which is based on random walks over an absorbing Markov chain. Their method works by iteratively transforming the top-ranking nodes into absorbing nodes, effec-tively reducing the transition probabilities to zero. The absorbing nodes will drag down the scores of the adjacent nodes as the walk gets absorbed. On the other hand, the nodes which are far away from the absorbing nodes still get visited by the random walk, thus ensuring that the novel nodes will be ranked higher. Li et al [12] approaches the diversity ranking from the optimization under constraints perspective. They propose a supervised method based on structural learning which incorporates diversity as a set of subtopic constraints. Then, they train a summarization model and enforce diversity through the optimization problem. Our method differs from other diversity ranking methods in the rank propagation. Since most graph ranking models [6] [18][20] are inspired by the PageRank algorithm [4], they rely on eigenvector centrality to measure the importance of nodes. In con-trast, our method uses the negative edges to propagate negative endorsements among nodes. High redundant nodes are those which receive a substantial amount of disap-proval votes. Furthermore, the applications of negative edges in ranking model have been explored in related domains, such as trust ranking [8], social network mining [11][12], and complex question answering [1]. On the other hand, our method consid-ers the negative edges to represent redundancy relationship among answers. Our method focuses on two key aspects to find a diverse set of answer. First, a set of answer should contain many relevant facts pertaining to an information need. Second, Each answer should be novel or contain a distinct fact with respect to the other an-swers. To achieve that, we employ a graphical model to rank the relevant answers according to the balance between relevance and novelty. Two set of relations are represented by two signed graphs. First, the relevance relation is denoted by the posi-tive edges between the question node and the answer nodes. On the other hand, the redundancy relation is represented by the negative edges between answer nodes. The negative sign is used to propagate a negative endorsement or disapproval vote be-tween the nodes. The absolute value of edge weight represents the degree of similarity between answers. A formal description of our method is described as follows. 
Given a question q and a set of n relevant answers, we first define G = ( V , E ) as an undirected graph where V V is a set of vertices representing the relevant answers, E E is a set of edges representing the similarity between vertices where . Next, we ment A ij in A is the normalized value of S ij such that and trix B from the outer product of an all-1 vector and r T such that each element transition matrix P as: where d is a damping factor with a real value from [0,1], A is the initial answer adja-non-zero probabilities which add up to 1, P is a stochastic matrix where each element P Thus, P satisfies ergodicity properties and has a unique stationary distribution . At this stage, each answer can then be ra nked according to its stationary distribution. At this point, we have derived a random walk model over the answer graph which incorporates both answer relevance and answer similarity. However, it does not take into account factual redundancy between answers. 
In order to employ the negative edges to reduce factual redundancy, we modify the swer vertices, E -is a set of negative edges where . Then, an adjacency matrix M, corresponding to edge weights in G -, is defined as an all-negative matrix of S where and . 
Next, similar to the process of deriving the transition matrix P , we define a mod-ified transition matrix Q to incorporate the negative adjacency weights defined in M and the answer relevance defined in B . To ensure that Q is still ergodic, we multiply matrix B with a scaling factor c . The value of c is determined by the conditions that all elements in Q should be non-negative and each i -th row of Q should add up to 1. That is, . Since all rows of M sum to -1 and all rows of B add up to 1, c is a function of d where . where M is an all-negative answer adjacency matrix. Since ergodicity properties still hold, Q has a unique stationary distribution . Finally, we rank each node i according to its stationary probability . From the matrix notations, the simplified equation of the DiverseRank score can be formulated as follow: Where d is a damping factor with a real value in [0,1] range. Additionally, d serves as a penalty factor of redundancy. c is a scaling factor defined as a function of d where consistently outperform other relevance models at the sentence level. 4.1 Data Sets Two question answering data sets are used in the evaluation: a subset of Yahoo! An-swers data set (YahooQA) used in Liu et al. X  X  work [15] and a complex interactive question answering test set (ciQA) used in TREC 2006 [10]. YahooQA data comprises subjective and ill-defined information needs formulated by the community members. The subjects of interests span widely from mathematics, general health, to wrestling. In contrast, ciQA data largely focus on the complex entity-relationship questions. Their information needs reflect those posed by intelligence analysts. From data quali-ty perspective, YahooQA data are much noisier than ciQA data as they contain mostly informal linguistic expressions. To prepare YahooQA data set, we randomly select 100 questions and 10,546 answers from the top 20 most frequent categories (meas-ured in terms of a number of responded answers) to use as a test set. A set of information nuggets for YahooQA is automatically created by matching relevant answers with the corresponding questions. The best answer chosen by askers for each question is marked as a vital nugget while the other answers are marked as an okay nugget. In the case of ciQA data set, 30 question topics and their free-form description are prepared by human assessors at National Institute of Standards and Technology (NIST). 4.2 Evaluation Settings We employ the nugget pyramid metric to evaluate the diversity of the answer set. Generally, we assume that the factual diversity of the extracted answers can be meas-ured in terms of a number of information nuggets the extracted sets have in common with the benchmark nuggets. The formulas to compute the pyramid F-score are de-scribed in [14]. In summary, the pyramid F-score is computed as a weighted harmonic mean (F-score) between nugget recall (NR) and nugget precision (NP). NR and NP are derived from summing the unigram co-occurrences between terms in each infor-mation nugget and terms from each extracted answer set. Following the standard procedure in TREC 2006, we set the evaluation parameters to and l = 7,000 and use Pourpre [14] script version 1.1c to automatically compute the scores. Additional-ly, we further perform a fine-grained analysis of the algorithmic performance at vary-ing sizes of answer set using a recall-by-length performance curve [13]. Better me-thods will produce curves that rise faster as more relevant and novel facts are included in the answer set. 
Starting from the preprocessing step, we ex tract word features from a collection of answers by splitting answers into unigram tokens, removing non content-bearing words, e.g., articles, conjunctions, prepositions, etc., and stemming the tokens using Porter Stemmer. After the preprocessing step , we use a vector-space model to retrieve the relevant answers. Free-form narrative field associated with each question is used as a query. The relevance scores between the answers and query are computed from relevant answers into an undirected graph with negative edges. Different edge weight-ing schemes based on inter-sentence similarity measures are employed. Next, the relevance scores of the retrieved answers and a query are calculated using various relevance models. 
We compare the effectiveness of four baseline methods against the proposed me-thod. The baselines include maximal marginal relevance (MMR) [5], SumBasic [17], topic-sensitive LexRank [18], and a backward ranking of topic-sensitive LexRank (henceforth LexRankInv). The last baseline method is used to test whether diversity can be promoted by simply reversing the ranking of eigenvector centrality method. The default cut-off level for answer length is 7,000 characters. 5.1 Pyramid F-Scores Table 1 shows the average pyramid F-scores of the baseline methods and the best DiverseRank variants. In both data sets, the proposed method significantly outper-forms all baseline methods, p-value&lt;0.05. Considering the performance between random-walk based methods (LexRank vs. DiverseRank), DiverseRank also outper-forms LexRank in both data sets although the improvements are relatively minor (6.65% and 2.69%), compared to those of other baselines. Furthermore, LexRankInv produces inferior scores to DiverseRank in both data sets. 5.2 Recall-by-Length Performance Curves Figure 1 shows recall-by-length curves of the baselines and the proposed method in each data set. In YahooQA case (figure 1a), DiverseRank starts to produce a signifi-cantly better performance than the best baseline method (LexRank) at the answer length of 1,200 characters. On the other hand, DiverseRank does not perform quite well in ciQA case (figure 2b). In this case, DiverseRank starts to outperform LexRank after the answer length of 1,800 characters. This outcome is not entirely unexpected. As a smaller answer set contains fewer number of information nuggets, therefore there are fewer items to be diversified. As the answer set continues to grow, Diverse-Rank continues to gain a better performance. Note that our results confirm the pre-vious results in [10] in which a method that produces the best F-score at a predefined answer length does not necessarily perform equally effective across all incremental lengths. We propose a graph ranking model to find a diverse set of answers based on random walks over a negative-edge graph. Our main contribution is in the utilization of a graphical model to reduce redundancy within the answer set. First, given a complex question and a set of relevant answers, we represent the relevant answers as an answer graph whose nodes correspond to answers and edges correspond to the similarity between answers. Then, we assign a negative sign to edge weights in the answer graph to model the redundancy relationship between nodes. Then, the final ranking score for each answer node is derived from its long-term negative endorsement. The evaluation results show that our method outperforms most baseline methods. The analysis of recall-by-length performance curves suggests that the best baseline me-thod performs better than DiverseRank at smaller answer lengths. This is explained by the fact that a smaller set of answers contains fewer facts, thus its content can hardly be diversified further. As the size of answer set increases, DiverseRank eventually outperforms the baseline methods. Acknowledgments. This research work is supported in part from the NSF Career grant IIS 0448023, NSF CCF 0905291, NSF IIP 0934197, NSFC 90920005 and the Program of Introducing Talents of Discipline to Universities B07042 (China). 
