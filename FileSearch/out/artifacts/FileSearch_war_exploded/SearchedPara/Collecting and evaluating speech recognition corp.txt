 Abstract We describe the Lwazi corpus for automatic speech recognition (ASR), a new telephone speech corpus which contains data from the eleven official lan-guages of South Africa. Because of practical constraints, the amount of speech per language is relatively small compared to major corpora in world languages, and we report on our investigation of the stability of the ASR models derived from the corpus. We also report on phoneme distance measures across languages, and describe initial phone recognisers that were developed using this data. We find that a surprisingly small number of speakers (fewer than 50) and around 10 to 20 h of speech per language are sufficient for the purposes of acceptable phone-based recognition.
 Keywords Speech recognition Lwazi corpus Resource-scarce languages South African languages 1 Introduction There is a widespread belief that spoken dialog systems (SDSs) will have a significant impact in the developing countries of Africa (Tucker and Shalonova 2004 ), where the availability of alternative information sources is often low. Traditional computer infrastructure is scarce in Africa, but telephone networks (especially cellular networks) are spreading rapidly. In addition, speech-based access to information may empower illiterate or semi-literate people, 98% of whom live in the developing world.

SDSs can play a useful role in a wide range of applications. Of particular importance in Africa are applications such as education, using speech-enabled learning software or kiosks and information dissemination through media such as telephone-based information systems. Significant benefits can be envisioned if information is provided in domains such as agriculture (Nasfors 2007 ), health care (Sherwani et al. 2007 ; Sharma et al. 2009 ) and government services (Barnard et al. 2003 ). Recent years have seen extensive research on the application of speech technology in the developing world X  X or a recent review, see (Patel et al. 2010 ). In order to make SDSs a reality in Africa, technology components such as text-to-speech (TTS) systems and automatic speech recognition (ASR) systems are required. The latter category of technologies is the focus of the current contribution.
Speech recognition systems exist for only a handful of African languages (Roux et al. 2000 ; Seid and Gamba  X  ck 2005 ; Abdillahi et al. 2006 ), and to our knowledge no service available to the general public currently uses ASR in an indigenous African language. A significant reason for this state of affairs is the lack of sufficient linguistic resources in the African languages. Most importantly, modern speech recognition systems use statistical models which are trained on corpora of relevant speech (i.e. appropriate for the recognition task in terms of the language used, the profile of the speakers, speaking style, etc.) This speech generally needs to be curated and transcribed prior to the development of ASR systems, and for most applications speech from a large number of speakers is required in order to achieve acceptable system performance. On the African continent, where infrastructure such as computer networks is less developed than in countries such as USA, Japan and the European countries, the development of such speech corpora is a significant hurdle to the development of ASR systems.

The complexity of speech corpus development is strongly correlated with the amount of data that is required, since the number of speakers that need to be canvassed and the amount of speech that must be curated and transcribed are major factors in determining the feasibility of such development. In order to minimise this complexity, it is important to have tools and guidelines that can be used to assist in designing the smallest corpora that will be sufficient for typical applications of ASR systems. As minimal corpora can be extended by sharing data across languages, tools are also required to indicate when data sharing will be beneficial and when detrimental.

In this paper we describe and evaluate a new speech corpus of South African languages recently developed (the Lwazi corpus) and evaluate the extent in which computational analysis tools can provide further guidelines for ASR corpus design in resource-scarce languages. 2 Project Lwazi The goal of Project Lwazi is to provide South African citizens with information and information services in their home language (that is, the language that the speaker identifies with most strongly), over the telephone, in an efficient and affordable manner. Commissioned by the South African Department of Arts and Culture, the activities of the first stage of this project (2006 X 2009) included the development of core language technology resources and components for all the official languages of South Africa, where, for the majority of these, no prior language technology components were available.

The core linguistic resources that were developed include phoneme sets, electronic pronunciation dictionaries and the speech and text corpora required to develop ASR and TTS systems for all eleven official languages of South Africa. The usability of these resources were demonstrated during a national pilot in 2009. All outputs from the project have since been released as open source software and open content (Meraka-Institute 2009 ).

Resources were developed for all eleven languages that are recognised as official languages in South Africa (SA) and contribute to the available HLT components (Grover et al. this volume). These languages are: 1. isiZulu (ISO 639-3: zul) and isiXhosa (ISO 639-3: xho), the two Nguni 2. The three Sotho languages: Sepedi (ISO 639-3: nso), Setswana (ISO 639-3: 3. Afrikaans (ISO 639-3: afr), a Germanic language, which is the home language 4. South English (ISO 639-3: eng), the home language of only 8% of the 5. The two Nguni languages less widely spoken in SA: Siswati (ISO 639-3: ssw) 6. Xitsonga (ISO 639-3: tso) and Tshivenda (ISO 639-3: ven), the home languages
For all these languages, new pronunciation dictionaries, text and speech corpora were developed. ASR speech corpora consist of approximately 200 speakers per language, producing read and elicited speech, recorded over a telephone channel. Each speaker produced approximately 30 utterances, 16 of these were randomly selected from a phonetically balanced corpus and the remainder consist of short words and phrases: answers to open questions, answers to yes/no questions, spelt words, dates and numbers. The speaker population was selected to provide a balanced profile with regard to age, gender and type of telephone (cellphone or landline). Table 1 provides a summary of the amount of speech for the different languages. 3 Related work Below, we review earlier work relevant to the development of speech recognisers for languages with limited resources. This includes both ASR system design (Sect. 1 ) and ASR corpus design (Sect. 2 ) In Sect. 3 , we also review the analytical tools that we utilise in order to investigate corpus design systematically. 3.1 ASR for resource-scarce languages The main linguistic resources required when developing ASR systems for telephone based systems are electronic pronunciation dictionaries, annotated audio corpora (used to construct acoustic models) and recognition grammars. An ASR audio corpus consists of recordings from multiple speakers, with each utterance carefully transcribed orthographically and markers used to indicate non-speech and other events important from an ASR perspective. Both the collection of appropriate speech from multiple speakers and the accurate annotation of this speech are resource-intensive processes, and therefore corpora for resource-scarce languages tend to be very small (1 X 10 h of audio) when compared to the speech corpora used to build commercial systems for world languages (hundreds to thousands of hours per language).

Different approaches have been used to best utilise limited audio resources when developing ASR systems. Bootstrapping has been shown to be a very efficient technique for the rapid development of pronunciation dictionaries, even when utilising linguistic assistants with limited phonetic training (Davel and Barnard 2004 ; Kominek and Black 2006 ; Maskey et al. 2004 ).

Small audio corpora can be used efficiently by utilising techniques that share data across languages, either by developing multilingual ASR systems (a single system that simultaneously recognises different languages), or by using additional source data to supplement the training data that exists in the target language. Various data sharing techniques for language-dependant acoustic modelling have been studied, including cross-language transfer, data pooling, language adaptation and bootstrap-ping (Wheatley et al. 1994 ; Schultz and Waibel 2001 ; Byrne et al. 2000 ). Both Wheatley et al. 1994 and Schultz and Waibel 2001 found that useful gains could be obtained by sharing data across languages with the size of the benefit dependent on the similarity of the sound systems of the languages combined. In the only cross-lingual adaptation study using African languages (Niesler 2007 ), similar gains have not yet been observed. 3.2 ASR corpus design Corpus design techniques for ASR are generally aimed at specifying or selecting the most appropriate subset of data from a larger domain in order to optimise recognition accuracy, often while explicitly minimising the size of the selected corpus. This is achieved through various techniques that aim to include as much variability in the data as possible, while simultaneously ensuring that the corpus matches the intended operating environment as accurately as possible.

Three directions are primarily employed: (1) explicit specification of phonotactic, speaker and channel variability during corpus development, (2) automated selection of informative subsets of data from larger corpora, with the smaller subset yielding comparable results, and (3) the use of active learning to optimise existing speech recognition systems. All three techniques provide a perspective on the sources of variation inherent in a speech corpus, and the effect of this variation on speech recognition accuracy.

Nagroski et al. ( 2003 ) use Principle Component Analysis (PCA) to cluster data acoustically. These clusters then serve as a starting point for selecting the optimal utterances from a training database. As a consequence of the clustering technique, it is possible to characterise some of the acoustic properties of the data being analysed, and to obtain an understanding of the major sources of variation, such as different speakers and genders. Interestingly, the effect of utterance length has also been analysed as a significant source of variation (Riccardi and Hakkani-Tur 2003 ).

Active and unsupervised learning methods can be combined to circumvent the need for transcribing massive amounts of data (Riccardi and Hakkani-Tur 2003 ). The most informative untranscribed data is selected for a human to label, based on acoustic evidence of a partially and iteratively trained ASR system. From such work, it soon becomes evident that the optimisation of the amount of variation inherent to training data is needed, since randomly selected additional data does not necessarily improve recognition accuracy. By focusing on the selection (based on existing transcriptions) of a uniform distribution across different speech units such as words and phonemes, improvements are obtained (Wu et al. 2007 ).

In our focus on resource-scarce languages, the main aim is to understand the amount of data that needs to be collected in order to achieve acceptable accuracy. This is achieved through the use of analytic measures of data variability, which we describe next. 3.3 Evaluating phoneme stability In (Badenhorst and Davel 2008 ; Badenhorst 2009 ) a technique is developed that estimates how stable a specific phoneme model is, given a specific set of training data. This statistical measure provides an indication of the effect that additional training data will have on recognition accuracy: the higher the stability, the less the benefit of additional speech data.

The model stability measure utilises the Bhattacharyya bound (Fukunaga 1990 ), a widely-used upper bound of the Bayes error. The Bayes error provides an indication of the separability between two probability distributions. If a probability distribution is calclulated for two phonemes, say /a/ and /e/, then the ease with which a new audio sample can be classified as being an /a/ or an /e/ depends on how separable ( X  X ifferent X ) the two distributions are. The more similar the distributions, the more miss-classifications are expected, and the higher the minimum expected miss-classification rate or Bayes error. When two distributions are identical, it becomes impossible to determine to which of the two classes a new sample should belong, apart from guessing. The expected miss-classification rate then becomes 0.5 (50%).

By determining how close to identical the probability distributions are of the same phoneme calculated using different sections of the training corpus, it is possible to determine whether the developed models are stable. If the probability distribution of phoneme /a/ trained on one section of the corpus is quite different to the probability distribution of the same phoneme /a/ trained on another section of the corpus, then the training subset is still too small to produce stable acoustic models. When these probability distributions are very similar (and the Bayes error approaches 0.5) then the training data were sufficient, and the acoustic models are stable: adding additional data will not influence the estimated probability density significantly.

Since the Bayes error itself cannot always be calculated analytically (depending on the complexity of the probability distributions being compared) an upper bound provides a  X  X lose-enough X  estimate of the value itself. The Bhat bound is such an estimate, and provides the assurance that the true error will never be larger than the bound calculated.

If P i and p i ( X ) denote the prior probability and class-conditional density function for class i , respectively, the Bhattacharyya bound e is calculated as: When both density functions are Gaussian with mean l i and covariance matrix R i ; integration of e leads to a closed-form expression for e : where is referred to as the Bhattacharyya distance.

In order to estimate the stability of an acoustic model, the training data for that model is separated into a number of disjoint subsets. All subsets are selected to be mutually exclusive with respect to the speakers they contain. For each subset, a separate acoustic model is trained, and the Bhattacharyya bound between each pair of models calculated. By calculating both the mean of this bound and the standard deviation of this measure across the various model pairs, a statistically sound measure of model estimation stability is obtained. 4 Computational analysis of the Lwazi corpus We now report on our analysis of the Lwazi speech corpus, using the stability measure described in Sect. 3.3 . Here, we focus on four languages (isiNdebele, Siswati, isiZulu and Tshivenda) for reasons of space; later, we shall see that the other languages behave quite similarly. 4.1 Experimental design For each phoneme in each of our target languages, we extract all the phoneme occurrences from the 150 speakers with the most utterances per phoneme. We utilise the technique described in Sect. 3.3 to estimate the Bhattacharyya bound both when evaluating phoneme variability and model distance. In both cases we separate the data for each phoneme into 5 disjoint subsets. We calculate the mean of the 10 distances obtained between the various intra-phoneme model pairs when measuring phoneme stability, and the mean of the 25 distances obtained between the various inter-phoneme model pairs when measuring phoneme distance.

In order to be able to control the number of phoneme observations used to train our acoustic models, we first train a speech recognition system and then use forced alignment to label all of the utterances using the systems described in Sect. 5.1 Mel-frequency cepstral coefficients (MFCCs) with cepstral mean and variance normalisation are used as features, as described in Sect. 5.1 . 4.2 Analysis of phoneme variability In an earlier analysis of phoneme variability of an English corpus (Badenhorst and Davel 2008 ), it was observed that similar trends are observed when utilising different numbers of mixtures in a Gaussian mixture model. (That is, a model with a limited number of mixtures is a good predictor of the behaviour of a more complex model.) Similarly, it was found that context dependent and context independent models also produced comparable behaviour. (Asymptotes occur later, but trends remain similar.) Because of the limited size of the Lwazi corpus, we therefore only report on single-mixture context-independent models in the current section and the phone model topology can thus be viewed as single-state HMMs.

As we also observe similar trends for phonemes within the same broad categories, we report on a couple of examples from several broad categories which occur in most of our target languages. Using X-SAMPA notation, the following phonemes are selected: /a/ (vowels), /m/ (nasals), /b/ and /g/ (voiced plosives) and /s/ (unvoiced fricatives), after verifying that these phonemes are indeed represen-tative of the larger groups.

Figures 1 and 2 demonstrate the effects of variable numbers of phonemes and speakers, respectively, on the value of the mean Bhattacharyya bound. This value should approach 0.5 for a model fully trained on a sufficiently representative set of data, since a value of 0.5 corresponds to indistinguishable distributions (with 50% two-class error rates). In Fig. 1 we see that the various broad categories of sounds approach the asymptotic bound in different ways. The vowels and nasals require the largest number of phoneme occurrences to reach a given level, whereas the fricatives and plosives converge quite rapidly (With 10 observations per speaker, both the fricatives and plosives achieve values of 0.48 or better for all languages, in contrast to the vowels and nasals which require 30 observations to reach similar stability). Note that we employed 30 speakers per phoneme group, since that is the largest number achievable with our protocol.

For the results in Fig. 2 , we keep the number of phoneme occurrences per speaker fixed at 20 (this ensures that we have sufficient data for all phonemes, and corresponds with reasonable convergence in Fig. 1 ). It is clear that additional speakers would still improve the modelling accuracy for especially the vowels and nasals. We observe that the voiced plosives and fricatives quickly achieve high values for the bound (close to the ideal 0.5).

Figures 1 and 2  X  X s well as similar figures for the other phoneme classes and languages we have studied X  X uggest that all phoneme categories require at least 20 training speakers to achieve reasonable levels of convergence (bound levels of 0.48 or better). The number of phoneme observations required per speaker is more variable, ranging from less than 10 for the voiceless fricatives to 30 or more for vowels, liquids and nasals. We return to these observations in Sect. 5 .

For large-vocabulary systems, requiring context-dependent modeling, the picture is unfortunately much more complicated. In that case, one also has to consider the impact of state tieing, and the trade-off between the number of clusters and the amount of training data per cluster becomes an important issue. 4.3 Distances between languages In Sect. 3.1 it was pointed out that the similarities between the same phonemes in different languages are important predictors of the benefit achievable from pooling the data from those languages. Armed with the knowledge that stable models can be estimated with 30 speakers per phoneme and between 10 and 30 phoneme occurrences per speaker, we now turn to the task of measuring distances between phonemes in various languages.

We again use the mean Bhattacharyya bound to compare phonemes, and obtain values between all possible combinations of phonemes. Results are shown for the isiNdebele phonemes /n/ and /a/ in Fig. 3 . As expected, similar phonemes from the different languages are closer to one another than different phonemes of the same language. However, the details of the distances are quite revealing: for /a/, Siswati is closest to the isiNdebele model, as would be expected given their close linguistic relationship, but for /n/, the Tshivenda model is found to be closer than either of the other Nguni languages. For comparative purposes, we have included one non-Bantu language (Afrikaans), and we see that its models are indeed significantly more dissimilar from the isiNdebele model than any of the Bantu languages. In fact, the Afrikaans /n/ is about as distant from isiNdebele /n/ as isiNdebele /l/ and isiZulu /l/ are.
 Analysis of another isiNdebele vowel /i/ and nasal /m/ are shown in Fig. 4 . Interestingly all distances are found to be diminished and we conclude that the models for /i/ are indeed very similar across the investigated language borders. For /m/ all the Bantu languages are also even more closely related than for the model /n/.

To complete the picture, an isiNdebele fricative /s/ and plosive /g/ are also investigated. It can be seen in Fig. 5 that immediately all of the closest matches are non-vowel sounds. We also find that models are no longer that dissimilar between the different plosive phonemes. For the isiNdebele /g/ it can be seen that /k/ and the Afrikaans /d/ models are actually closer than models for isiZulu /g/ or the Siswati /b/.
 The similarity of the fricative /s/ also proves interesting across language borders. Although the Siswati model is found to be closest, the model for Afrikaans is very similar. All of the fricative models are, however, distinct from the other investigated phones of the languages. 5 Recognition results In this section, we aim to confirm the measurements reported in Sect. 4 with several ASR measures. Baseline accuracies for both phone recognition and small vocabulary word recognition are established in Sects. 5.1 and 5.2 respectively. We then measure ASR accuracy with varying amounts of data, based both on the number of speakers and the number of phones in Sect. 5.3 In Sect. 5.4 , we use these results together with a heuristic relationship (Schuurmans 1997 ) to get a rough estimate of the amount of data required to achieve a particular phone recognition accuracy. 5.1 Phone recognition with the Lwazi corpus The recognisers we employ are standard HMM-based systems. We use HTK 3.4 to build a context-dependent cross-word HMM-based phone recogniser with triphone models. Each model has 3 emitting states with 7 mixtures per state. (These parameter choices were determined to be optimal for phone-recognition accuracy with the complete corpora during pilot experiments.) 39 features are used: 13 MFCCs together with their first and second order derivatives. Cepstral Mean Normalisation (CMN) as well as Cepstral Variance Normalisation (CVN) are used to perform speaker-specific normalisation. A diagonal covariance matrix is used; to partially compensate for the implicit assumption of feature independence, semi-tied transforms are applied. A flat phone-based language model is employed for phone recognition.

The optimal values of parameters such as the number of mixtures and the insertion penalty (during language modelling) will in general depend on the amount of training data available. Since our values are optimised for the full corpus, our reported accuracies for reduced corpora are underestimates. Although we have not exhaustively evaluated all parameter options, we have verified that the dependencies are quite weak, and that the overall trends reported below are also observed when the parameters are adjusted.

As the initial pronunciation dictionaries were developed to provide good coverage of each language in general, these dictionaries did not cover the entire ASR corpus. Grapheme-to-phoneme rules are therefore extracted from the general dictionaries using the Default and Refine algorithm (Davel and Barnard 2008 ) and used to generate missing pronunciations. For the reason cited above, tone is not modelled in the system.

For phone recognition, we divided the data into a test set, which consists of 30 randomly selected speakers in each language, and a training set (the remaining speakers, approximately 170 per language). The recogniser for each language was built using all the training data for that language, using the recognition architecture as described above. These recognisers were then evaluated by performing a Viterbi search (an efficient search technique (Viterbi 1967 )) with a language model that allows unrestricted transitions between any pair of phonemes. Dynamic program-ming was used to match the resulting phoneme strings against the strings that result from automatic phonemic transcription of the orthographic transcriptions of the test utterances. The resulting accuracies are summarised in Table 2 . Phone-recognition correctness refers to the percentage of correctly recognised phone labels with regard to the total number of expected phone labels, while the dynamic programming used to calculate the accuracy values take into account phone label insertions/deletions as well. The table also lists the phonotactic perplexity of each language  X  that is, the perplexity that is measured if a bigram model is used to model the phoneme sequences that occur in the training set. Lastly, the table contains word-recognition results, which are discussed in Sect. 5.2 .

Interestingly, the correctness and accuracy of all other languages are higher than that of English, despite the fact that most languages have more phonemes than English. One possible explanation for this observation is the fact that English has fewer phonotactic constraints than any of the other languages, as can be deduced from the perplexity values in Table 2 . (The Southern Bantu languages employ CV (consonant-vowel) or V syllable structures predominantly.) Overall, however, phonotactic perplexity does not correlate well with correctness or accuracy in our results, so other explanations for the relative accuracies should also be investigated. Finally, the relatively high recognition accuracies obtained with these small corpora confirm the observations summarized in Figs. 1 and 2 . 5.2 Small-vocabulary speech recognition with the Lwazi corpus Phone recognition is a useful benchmark to employ for recognition in new languages, since extensive intuition exists on phone-recognition accuracies achieved on standard corpora. However, initial applications of ASR in the developing world will in practice require accurate small-vocabulary recognition (as described in Sect. 1 ) We therefore describe experiments aimed at estimating our performance on such tasks next.

During the collection of the Lwazi ASR corpus, callers were asked several questions, of which some resulted in only a small set of responses. These included the following:  X  Are you married?  X  Are you speaking on a landline or a cellphone?  X  What is your gender?  X  What is your mother tongue?  X  Where do you live? / Where were you born?
Since these same questions were asked of all speakers across all languages, they form a suitable basis for small-vocabulary experiments. Mother tongue speakers were then asked to label all answers that were semantically equivalent. In this fashion, answers such  X  X  X goli X  X  (isiZulu name for Johannesburg, meaning  X  X  X lace of gold X  X ) and  X  X  X ohannesburg X  X  were considered equivalents.

This resulted in 10 distinct semantic concepts for each language, with approximately one to three different lexical items corresponding to the same concept in a language. Because of the similar questions, similar meanings are attached to the matching concepts in each language, except for minor variations because of cultural differences. (For example, the majority of English and Afrikaans speakers would simply answer  X  X  X es X  X  or  X  X  X o X  X  to the first question. In contrast, the majority of Xitsonga, Sepedi and Tshivenda speakers would answer the question in different ways depending on their gender. A Xitsonga man would for example say  X  X  X i tekile / a ni tekangi X  X  (I have taken / I haven X  X  taken), whereas a woman would say  X  X  X i tekiwile / a ni tekiwangi X  X  (I have been taken / I haven X  X  been taken)). Our small vocabulary task was constructed by removing all utterances that contain any of the phrases corresponding to any of these concepts from the training set, since such vocabulary-independent performance is the realistic goal for application in SDSs. For testing purposes, all utterances that contain only these phrases were employed; recognition was deemed correct if the phrase was placed into the correct semantic category. Because of the relatively small set of test utterances (five or fewer per speaker), we employed ten-fold cross validation to estimate recognition accuracy.

A vocabulary of ten words (actually, concepts) is a good test of typical recognition tasks in an SDS which is aimed at Interactive Voice Response (IVR) applications, where the dialogue is structured to contain mostly menu items and command words. Common tasks such as yes/no recognition require even smaller vocabularies, and larger tasks with highly distinctive vocabularies may in fact give comparable accuracies to those achieved with our artificially-constructed grammar.
As a baseline for comparison, we have also measured the accuracies that can be achieved with the cross-language transfer procedure described, for example, in Sherwani et al. ( 2007 , 2009 ). That procedure, which is often a starting point for resource-scarce languages, utilises a well-trained recogniser in a world language such as English. All the words in the recognition task are transcribed using the phonemes of this well-trained recogniser, mapping the phonemes in the actual target language to the closest world-language phonemes where necessary. This cross-language dictionary is then used for recognition. Three English recognisers were investigated for our baseline, namely recognisers trained on the NTIMIT and Wall Street Journal corpora (the latter band-limited and downsampled to match our telephone corpus), and one trained on the English part of the Lwazi corpus.
Table 2 contains recognition results obtained with these baseline systems, as well as with our language-specific recognisers. (We were not able to carry out this experiment for isiNdebele, for lack of access to a mother-tongue speaker who could perform the semantic mappings.)
We see that accuracies above 90% are achieved in all languages except Sepedi and Setswana. With careful dialogue design (Cohen et al. 2004 ), this should be sufficient for a usable SDS. Of the three baseline systems that use phoneme mappings, the Lwazi English model is easily the most accurate. This is to be expected, since the acoustic conditions of NTIMIT and WSJ are somewhat dissimilar to those in Lwazi; however, the magnitude of the differences in accuracy is somewhat surprising. Even this best baseline system is, however, much less accurate than the language-specific acoustic models in most cases. Excluding English, the languages with the smallest absolute difference between baseline and trained models are Afrikaans, which is linguistically quite similar to English, and Sepedi, which also performed worst in the phone-recognition experiments (Sect. 5.1 ).

Given the similarities between the semantic categories in the different languages, it is interesting to compare the accuracies achieved in this task across languages (with error rates ranging between 2.1 and 12.3%). The quality of the phone recognisers partially explains these differences X  X n particular, the relatively poor performance of Sepedi and Setswana at both phone recognition and small-vocabulary word recognition is notable. However, Sesotho (with relatively accurate word recognition) and isiZulu (relatively accurate phone recognition) point to other relevant factors, such as the acoustic confusibility of words in semantically distinct classes that happens to occur in some languages but not others. 5.3 The number of training speakers To analyse the influence of the number of training speakers on the recognition accuracy achieved, we investigate phone-recognition accuracy as a function of both the number of training speakers and the total number of phones used for training. (We use the number of phones rather than the number of words or utterances as measure of the amount of training data employed because of the significant differences in word and utterance lengths between the various languages  X  the phone count is therefore a better measure of the actual amount of speech employed.) The training sets are selected in such a way that the number of phones per speaker remains balanced.

Figure 6 shows typical results. (The empty blocks without dots in the upper right-hand corner of each figure represent experiments that could not be performed because sufficient data was not available for each individual speaker.) It is clear that the number of training speakers has little or no influence on the accuracy achieved, in the range that we have investigated. Whereas the figures show systematically increasing accuracy as the number of training phones is increased (from left to right), increasing the number of speakers contributing to a given set of training data has little effect (top to bottom). This same behaviour is observed for all eleven languages, and is confirmed by representations such as that shown in Fig. 7 which shows the phone accuracy as a function of the number of training speakers, when about a quarter of the training data is used in each language. A similar insensitivity to the number of speakers was also observed for the training of context-dependent models (Barnard et al. 2009 ). 5.4 The amount of training data In Fig. 7 b we show the trends of phone recognition accuracy as a function of the amount of training data, when all 120 speakers are used. Although the curves for some languages (especially Sepedi) are quite noisy, it seems clear that none of the languages is approaching asymptotic phone-recognition accuracy given the amount of training data available in our corpus. In order to obtain a rough estimate of the amount of training data required to approach such an asymptote, we employ a heuristic relationship that is expected to hold for a wide range of classifiers (Schuurmans 1997 ). This relationship states that the error rate will asymptotically depend on the number of training samples ( N ) through the relationship A -( B / N ), with A and B parameters corresponding to the asymptotic error rate and the number of training samples required (a) to approach within 1% of that error rate, respectively. We have empirically determined that this relationship provides a reasonable fit to our data for values of N greater than approximately 50,000; we have therefore used a linear least-squares fit to estimate A and B values for all our languages, including only measured accuracies for N &gt; 50,000 in our analysis. Table 3 summarises the results obtained, and Fig. 8 shows a typical fit obtained in this manner. We see that quite good fits are obtained for several languages ( R 2 &gt; 0.96), and that the B parameter, which is related to the number of training phones required for accurate training, ranges between approxi-mately 300,000 and 550,000 for these languages. (For N = B , phone accuracies within 1% of the asymptotic value are predicted.) In our corpus, the average phone duration is approximately 150 ms X  X ence, corpora of approximately 750 to 1,400 min per language are suggested.
 6 Conclusion Collecting appropriate speech corpora for resource-scarce languages can be a challenging task, especially when financial resources are limited and speaker populations are small or geographically remote, with limited access to information and communication infrastructure. When collecting corpora from such environ-ments, an understanding of the interplay between type and amount of data can be of great benefit, by ensuring that the collection effort is made as efficient as possible.
In this paper, we describe the Lwazi corpus for automatic speech recognition (ASR), a new telephone speech corpus for Sourth African languages. We analyse perspective: we measure the stability of ASR models derived from the corpus and evaluate phoneme recognition accuracy directly. We find that different phone classes tend to have different data requirements. Voiceless fricatives, for example, can be trained accurately with relatively few tokens per speaker, whereas nasals and vowels require more data per speaker for comparable convergence (stability) of the acoustic distributions. The number of speakers required for a given level of stability shows comparable, but not identical, trends.

Our investigation of the practical training of speech-recognition systems reveals that the number of training speakers is less of a constraint than the amount of data per speaker (under the circumstances investigated in this study). In particular, this investigation reveals that systems of this nature can be trained successfully with around 40 X 50 training speakers; the total amount of speech to approach within 1% of asymptotic accuracy should be around 750 X 1,400 min per language. Clearly, more complicated recognition systems will benefit from more speakers and larger corpora; it is therefore important that work similar investigations should be carried out on larger multilingual corpora where such are available.

Another interesting avenue for future exploration follows from our findings that different phone classes have different data requirements. The data collection process could conceivably be made more efficient by biasing the recorded material towards the more  X  X  X ata-hungry X  X  phonetic categories; it remains to be seen, however, whether that benefit can be obtained without making the recording protocol too unnatural.
 References
