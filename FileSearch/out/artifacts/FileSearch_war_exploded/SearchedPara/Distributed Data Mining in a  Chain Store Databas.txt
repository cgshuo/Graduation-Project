
In this paper, we broaden the horizon of traditional rule min-ing by introducing a new framework of causality rule min-ing in a distributed chain store database, Specifically, the causality rule explored in this paper consists of a sequence of triggering events and a set of consequential events, and is designed with the capability of mining non-sequential, inter-transaction information. Hence, the causality rule mining provides a very general framework for rule derivation. Note, however, that the procedure of causality rule mining is very costly particularly in the presence of a huge number of can-didate sets and a distributed database, and in our opin-ion, cannot be dealt with by direct extensions from existing rule mining methods. Consequently, we devise in this pa-per a series of level matching algorithms, including Level 
Matching (abbreviatedly as LM), Level Matching with Se-lective Scan (abbreviatedly as LMS), and Distributed Level 
Matching (abbreviatedly as Distibuted LM), to minimize the computing cost needed for the distributed data mining of causality rules. In addition, the phenomena of time window constraints are also taken into consideration for the develop-ment of our algorithms. As a result of properly employing the technologies of level matching and selective scan, the proposed algorithms present good efficiency and scalability in the mining of local and global causality rules. Scale-up experiments show that the proposed algorithms scale well with the number of sites and the number of customer trans-actions. 
Index Terms: knowledge discovery, distributed data min-ing, causality rules, triggering events, consequential events 
The progress in database technology has provided the foundation that made large stores of transactional data ubiq-uitous. The discovery of association relationship among a huge database has been known to be useful in selective mar-permission and/or a fee. SIGKDD '02 Edmonton, Alberta, Canada Copyright 2002 ACM 1-58113-567-X/02/0007 ...$5.00. keting, decision support, and business management [3]. A popular area of applications is the market basket analysis, which studies the buying behaviors of customers by search-ing for sets of items that are frequently purchased either together or in sequence. Since the earlier work in [1], sev-eral technologies on association rule mining have been devel-oped, including: (1) association rule mining [5, 14]; (2) in-cremental updating [8]; (3) mining of generalized [16], multi-dimensional rules [17]; (4) constraint-based rule mining [6, 10] and multiple minimum supports issues [11]; (5) temporal association rule mining [7, 9]; (6) frequent episodes discovery [13]; and (7) sequential patterns mining [2, 15]. While the discovery of association rules and sequential patterns among the transaction data in a huge commerce database has received a significant amount of research at-tention, as pointed out in [9], the existing models of rule mining might not be able to discover user preferred frequent patterns efficiently in a chain store database of short trans-actions due to the following fundamental problems. 1. The technical challenge imposed by the distributed na-2. The difficulty of mining association rules on a short 3. Lack of long patterns for sequential pattern mining. In view of this, we broaden in this paper the horizon of traditional rule mining by introducing a new framework of causality rule mining in a distributed chain store database of short transactions. Such a short transaction database is, in our opinion, common in many real applications. Explicitly, we shall further explore in this paper the mining of causal-ity rules from a transaction database, where each event ma~, belong to multiple categories and the causality rule consists quential events as introduced in [9]. Specifically, transaction patterns can be derived from collected data by observing the customer behavior in terms of cause and effect, or what will be referred to as triggering events and consequential ing causality rules is determined by the first phase, this problem can be reduced to the one of discovering all one-triggering causality rules for the same support threshold. 
For interest of space, we concentrate our presentation on mining one-triggering causality rules with the objective of distributed mining. To deal with this newly identified prob-lem in a chain store database, a series of algorithms with the technique of level matching is devised to contribute on the distributed mining of causality rules. In Section 3.1, we present algorithms Level Matching (abbreviatedly as LM) and Level Matching with Selective Scan (abbreviatedly as 
LMS). The algorithm Distributed evel Matching is devel-oped in Section 3.2. The pseudo-code for algorithm LM is outlined below. 
Algorithm LM //Input: transaction database: db, window size: wsize, hash table size: hsize, and minimum support: min_ sup //Output: all one-triggering causality rules in db 10. for each Ci in scanned list do 11. obtain Li by pruning Ci according to min_supp; 12. output L~; 13. end for 14. set Ck+l = Lk * Lk; 16. set scanned list = Ck+l; 17. set k= k+T; 18. end while cated first to obtain the 2-event candidate rule set C2. In preSean, it scans the database once to get all frequent items 
L1 (e.g., those items bought by over 25% of customers), and C2 will be n 2, which will be huge in practical applications. In view of this, a hash method similar to that of algorithm 
DHP [14] is employed to further prune out infrequent can-didates from C2. will be described in detail later. As their names imply, the function getSearehDS is used to construct, a search struc-ture of all candidate rules within scanned list and the func-tion countCandidate is utilized to recognize the patterns of the candidate rules hidden in the transactions of a cus-tomer. 
We next present the details of the functions, getSearehDS, used in algorithm LM. Function getSearehDS of algorithm LM //Predefine: a hash function, hash, which maps each con-sequential event to an integer i E [1, ]subnode[] and a limit of size of leaf node: leaf_ limit //Input: collections of candidate set: scanned_ list //Output: a search structure: an array root 1. set each element in array root to be a new leaf node; 2. for each candidate set c in scanned list do 3. for each candidate s in c do 4. set node = root[s.trigger]; 5. set level = 0; 6. while (node is not leaf node) do 7. set level = level + 1; 8. set node = node.subnode[hash(s.items[level])]; 9. end while 10. add c to node.patterns; 11. if ( Inode.listl &gt; leaf_limit) 12. for each candidate s in node.patterns do 13. if (Isl &gt;= level) 14. set idx = hash (s.items [level]); 15. add e to node.subnode[idx].list; 16. end for 17. end if 18. end for 19. end for 20. return array root; 
In this algorithm, a hash tree is employed as a key search-ing scheme. In each node of the constructed hash tree, a set "patterns" is utilized to keep all the stored candidates, and an array "subnode" is utilized to contain the child nodes of set patterns. If a candidate is dispatched into an internal node, it is dispatched to the corresponding child according to the hash function. Note that, in a node of level i, the i th consequential event in the candidate s is used to obtain the index of subnode to which s is dispatched (as shown in Step 15 of function getSearchDS). After the update of the set patterns of a node, if the number of stored candidates is greater than leaf limit, then each candidate stored is dispatched to the children in the same manner described previously, where leaf_limit is a predefined value to limit the size of pat~:erns. 
We next use the following examples to illustrate the mean-ing of a window constraint. Example 3.1: Consider the transaction database shown in Figure 4. We assume the window size in this example is 5. It is noted that without the window constraint, the rule {A} {C} appears in three customers, i.e., Customers 1, 3 and 5. With the window constraint, the occurrence count of rule {A} ~ {C} is only 1 instead of 3, because the time intervals between the triggering event A and the consequential event B in Customer 1 and Customer 3 exceed the time window limit. Therefore, rule {A} ~ {C} is only valid for Customer 
Figure 4: An illustrative example of the main func-tion in algorithm LM with time window constraints by adopting the window constraint, a set of more precise rules can be generated. time window constraints. 
Function countCandidate for algorithm LM //Input: a searching data structure: ds, an array of the transactions made by a customer: tranx, and the given win-dow size: wsize. //Output: no output (only update the state of the searching data structure ds) 10. update(uid, sid, t, result); 11. end for 12. end for outer for loop, we move the start of the window to the next transaction, i.e., tranx[i]. Then, we construct an or-dered set, results, to contain all the events occurring in the current window. Since results is an ordered set, each el-ement within it is unique and stored with a lexicographic order. In the function update at Step 10 of this algo-rithm, it searches the candidates stored in the searching data structure ds and updates the count of each candi-date rule which is a subset of results. The function up-date, in fact, calls a recursive function rec_update, i.e., The pseudo-code of function rec_update is listed below. 
Function rec_update of algorithm LM //Input: a triggered event: t, a results event set: results, a unique id used to identify customer: uid, and an integer, start, indicated the recognition position //Output: no output (only state update) 1. set length as the size of array of items; 2. for i = start to length 3. set next = node.subnode[hash(results.event[iD]; 4. update(next, items, i + 1); 5. end for 6. for each candidate c in node.patterns do 7. if (c C results) then 8. e.count = c.count + 1; 9. end for 
This function works as follows. Given a triggering event t and consequential set results, for any candidate rule c = {t} ~-~ {rl,r2 ..... ra}, where {rl,r2,...,ra} C results = hash(rt)}. According to {rl,r~ .... ,ra} C results, we as-sume ri = Sd~. In addition, since both the consequential set in rule c and results are in a lexicography order, we have i =di, we can go down to subsequent nodes along the path mentioned above. Finally, we can reach the node f and update the count of candidate rule c. 
We next utilize the selective scan technique [4] to en-hance the performance of algorithm LM. In algorithm LM, each candidate set is generated by previous frequent set, i.e., Ca = La-l*La-1. In selective scan, however, if Ca is smaller then its source, i.e., Lk-1, we continue to generate Ck+l di-rectly from Ca. This process continues until Ci+l is empty or ICi+ll &lt; ICil. According to the technique of selective scan, the database might be scanned only once for several Cas. As a result, the I/O cost needed can be significantly reduced. In addition, the occurrence of each candidate in scanned_list is counted by each scan of database. Thus, the frequent itemset La can be obtained by removing those candidates whose counts are less than the specified support. This algorithm ceases when there are no more candidates needed to be counted. With the addition of the selective scan technique to Algorithm LM, we have algorithm LMS (i.e., level matching with selective scan). We can derive algo-rithm LMS directly by inserting the following code segment after Step 17 in algorithm LM. Code segment for algorithm LMS 17.a. set last size = ILk-ll, size = [Ck[; 17.b. while(si-ze &lt; lastsize and size &gt; 0) do 17.c. set Ck+l = Ck * Ck; 17.d. set scanned list = scanned list U Ck+l; 17.e. set last size = size; 17.f. set size = [Ck+l[; 17.g. set k = k + 1; 17.h. end while 
As described before, the nature of a chain store database is distributed and horizontally partitioned. Such a retail chain store naturally has several regional data centers, each of which manages the transaction records in its own region. Especially, for security issues, those distributed data sets are usually not allowed to be transmitted or joined together (e.g., discovering unexpected information from your com-petitors' Web sites is prohibited as mentioned in [12]). In essence, one would like to explore the relationship among the local data sets, e.g., the customers' purchasing behavior in New York, and the global purchasing behavior of people at the same time. Let D be a partitioned database located at n sites S 2, D 2, ..., D'~}. In the following, we will adopt the conven-tion of attaching a superscript i on a symbol to denote the corresponding distributed symbol for site S i. In addition, I:D[ and IDi[ represent the size of 7) and that of the par-tition D/, respectively. For a given itemset X, let X.sup and X.sup ~ be the respective support counts of X in ~D and D/. Hence, X.sup represents the global support count of X and X.supp i represents the local support count of X at site S i. For a given minimum support s, we have the following definitions of global and local large itemsets. Definition 2: X is globally large if X.supp _&gt; s x ]D[. s x lOll . 
In the following, we will use Lk to denote all global large k-itemsets in D. Respectively, Lk(S i) is represented the lo-cal large k-itemsets in D i. The problem of mining causality rules in a distributed database D can be reduced to the one of finding of all local and globally large itemsets. The algo-rithm Distributed LM for distributed causality rule mining is shown below. Algorithm Distributed Level Matching (Distributed LM) 1. By utilizing algorithm LMS, each store conducts the i e [1, k]. 2. Each store sends its causality rules, L(Si), to the center. 4. The center sends the complement local candidate C' (S ~) = C -L (S i) to each store. 5. Each store re-scans the database once to count C' (S ~) and sends the result back to center. 6.The center obtains global rules L by pruning in C those candidates whose counts are less than rain_supp. 
Note that, this algorithm of distributed mining not only As wilt be evaluated in our experimental studies, this algo-rithm is very efficient with low communication cost between center and stores. 4. EXPERIMENTAL STUDIES 
We generated several transaction databases from a set of potentially frequent itemsets to evaluate the performance of the proposed algorithms. However, we show the experimen-tal results from synthetic transaction data so as to obtain results of different workload parameters. To assess the rela-tive performance of the algorithms and study their scale-up properties, we perform several experiments on a computer with a CPU clock rate of 650 MHz and 512 MB of main memory. The simulation program was coded in Java and developed by J2SDK. 
For obtaining reliable experimental results, the method to generate synthetic transactions we employed in this study is  X  ~ ~oo ~ ..-,...0.20.~,. .~" .-* ~ L -~--o.5o,/, ...."/ 
Figure 5: Scale-up experiments as the number of customers varies for algorithm LMS 12oo r ~ 120~ [ -.~.-0.,0, ...-5 t ......... , 0.5o ...... / 
Figure 6: Scale-up experiments as the number of customers varies for algorithm APS similar to the ones used in prior works [2, 14, 16]. However, we show the experimental results from synthetic transaction data so as to obtain results of different workload parameters. 4.1 Scale-Up Experiments: We first present the scale-up results as the number of cus-tomers varies. Figure 5 shows how algorithm LMS scales up as the increase of the number of clusterms from 25,000 to 2.5 million. We evaluate the execution results with three levels of min supp. The size of the dataset with 2.5 mil-lion customers was 445 MB in binary format. It can be seen from Figure 5 that the line with min supp = 0.5% is not as smooth as the other two. Note that for minsupp = 0.7%, the mining of each dataset requires three database scans, and the mining for rninsupp = 0.2% requires four database scans. However, for min_supp = 0.5%, some datasets re-quire three scans and other datasets require four scans, thus explaining the shape of the curve with min_supp = 0.5%. 
Overall, the execution time of algorithms LMS scales well as depicted in Figure 5. 4.2 Distributed Mining Experiments In this section, we conduct a series of experiments on dis-tributed mining of different values of min supp and vari-ous numbers of stores. Without loss of generality, our ex-perimental results are evaluated in accordance with the as-sumption of the same computing power and database size in each site S i. The results are summarized in Figure 7 and 
Figure 8. As shown in Figure 7, as the number of stores increases, both the communication cost and the computing cost grow. As compared to the experiment in Figure 5, it can be observed that more computing power is needed to do the same mining in a distributed system. With the growth of the number of sites, CPU and I/O overhead increase sig-nificantly. Note, however, that with the flexibility for the mining [1] R. Agrawal, T. Imielinski, and A. Swami. Mining [2] R. Agrawal and R. Srikant. Mining Sequential [3] M.-S. Chen, 3. Han, and P. S.Yu. Data Mining: An [4] M.-S. Chen, J.-S. Park, and P. S.Yu. Efficient Data [5] J. Han, J. Pei, and Y. Yin. Mining Frequent Patterns [6] L. V. S. Lakshmanan, R. Ng, J. Han, and A. Pang. [7] C.-H. Lee, C.-R. Lin, and M.-S. Chen. On Mining [8] C.-H. Lee, C.-R. Lin, and M.-S. Chen. Sliding-Window [9] C.-H. Lee, P. S. Yu, and M.-S. Chen. Causality Rules: [10] C.-R. Lin and M.-S. Chen. On the Optimal Clustering [11] B. Liu, W. Hsu, and Y. Ma. Mining Association Rules [12] B. Liu, Y. Ma, and P.S. Yu. Discovering unexpected [13] H. Mannila and D. Rusakov. Decomposition of event [14] J.-S. Park, M.-S. Chen, and P. S. Yu. Using a [15] J. Pei, J. Han, B. Mortazavi-Asl, H. Pinto, Q. Chen, [16] R. Srikant and R. Agrawal. Mining Generalized [17] K. Wang, S.Q. Zhou, and S.C. Liew. Building [18] C. Yang, U. Fayyad, and P. Bradley. Efficient 
