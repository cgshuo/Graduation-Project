 The vast amount of news data available on the Internet has urged the development of personalized news recommender systems which are capable of providing individual users with personalized articles according t o their interests. As one of the two most popular recommendation paradigms, content-based filtering has been widely used in news recommendation[1]. The process basically consists of comparing the user profile against the content of candidate articles a nd ranking them consequently according to users X  reading interests.

In most content-based systems, all words in an article, except stop words, are rep-resented in a bag-of-words vector and further embedded in a cosine similarity function to estimate the relationship between articles and user profiles. As a result, two news articles with few common words would be judged dissimilar in such a style, although they may be closely related to each other. Th ese motivate the exploitation of semantic analysis in content-based systems.

Semantic analysis allows learning the similarities between articles and user profiles beyond traditional bag-of-words format, e.g., modeling the relatedness between two concepts according to real world knowledge . For instance, a basketball fan definitely knows Michael Jordan ,a former NBA player, is regarded as one of the greatest players in the history of Chicago Bulls . But without these background in basketball and NBA ,a system would never think Michael Jordan and current Chicago Bulls stars are related, and therefore it would never recommend a Michael Jordan article to a basketball fan with Chicago Bulls and Derrick Rose (a current Chicago star) in his/her user profile. The background knowledge can be in the form of finely crafted knowledge bases for specific domains, but are costly and time-consuming to build. Or, it may be coarse grained ontological structures that may contain noises but can be obtained from the Web without much human involvement. We argue that it is crucial to introduce real world knowledge into the content-based paradigms, but it is not expected to rely on expensively built knowledge bases. We, t herefore, should first find out what background knowledge resources we can use without relying on intensive annotations, and secondly, how we can properly incorporate those background knowledge into a content-based framework, and lastly, how about the efficiency of adding semantic analysis. In content-based recommender systems, finely crafted ontologies or taxonomies have been attempted to model user profiles and estimate the similarity between two concepts. Goossen et al. propose an extension of the classic TF-IDF approach, called CF-IDF[2]. The system works on a manually built NASDAQ ontology, considers its concept words to represent the article, and uses cosine similarity to model the relationship between user profiles and articles. News@hand [3] constructs a similar framework, but uses the Jaccard similarity. IJntema et al. define the s imilarity between two given concepts as the cosine similarity of their corresponding semantic neighbourhoods [4]. Degemmis et al. represent a book or article as bag-of-synsets according to WordNet, and further use a Naive Bayesian binary classifier to judge an item as interesting or not for a given user[5].
The methods discussed above represent articles and profiles as bag of concepts ac-cording to their ontologies, and simply operate the news-user matching as cosine or Jaccard similarities without any considerati ons about their embedde d ontological struc-tures. Furthermore, these ontologies are usually built manually for specific domains, thus limited in size and coverage. Therefore even harder to make a conclusion whether these approaches are easy to adapt to other domains or applications.

In this paper, we will model articles and user profiles on larger real world ontologies harvested from the web without much human annotations, and further investigate the concept similarity and news-user matching over such an ontology by considering its naturally embedded ontological structures. Recently, there are many open domain knowl edge resources available on the Web. Ex-amples of general purposes include Wikipedia, DBPedia, Freebase, etc., in English, and Hudong encyclopedia, Baidu encyclopedia, etc., in Chinese. DBPedia is constructed from Wikipedia with more than 3.64 million things, 1.83 million of which are classified in a consistent ontology. Baidu encyclopedia and Hudong encyclopedia are similar to Wikipedia, both of which claim to host over 5 million concepts in Chinese. We thus build two world ontologies from those resources, one in English derived from DBPedia and the other in Chinese from Hudong encyclopedia.

The DBPedia release contains two files, one storing all classes in a hierarchy, and the other with all instances in the form of subject-relation-object . We build the DBPedia based ontology as: a), construct the inner nodes of the ontology from the hierarchy of the first file, and b), append all instances in the second file as leaf nodes of the ontology(see Figure 1 for illustration).

In Hudong encyclopedia, each concept app ears as a web page containing its rela-tionship with other concepts; concepts of the same category are stored in the same directory, and all directories form a hierarchy. We construct the Hudong based ontology as: a) build the skeleton of the ontology by scanning the hierarchy; b) extract instances and relations from every concept page and append them as leaf nodes of the ontology. Compared to the English one, we admit our Chinese ontology is noisy in nature, since it contains more duplicate con cepts with different paths. In this section, we describe how we take advantage of background knowledge when measuring the similarities between news articles and user profiles. Formally, we assume the ontology built in section 3 contains a set of n concepts with their relations denoted as: Ontology W = { c w 1 ,c w 2 , ..., c w n } . For a given article, we only consider its concept words appearing in the ontology concept list, represented as: News = { &lt;c n 1 ,w n 1 &gt; c (1  X  i  X  p ) ,and p the number of concepts in the article. For a given user, we construct the user profile by accumulating a ll concepts found in the articles that the where q is the total number concepts in the user X  X  reading history, w u j is the average weighting of concept c u j in the articles containing this concept and read by this user. Concept-Concept Similarity. Most existing ontology-based methods compute the sim-ilarity between two concepts sets using eith er cosine or Jaccard similarity functions, where concepts are treated is olated and measured accordin g to their presence, i.e., a concept will contribute 1 if occurring in both sets, otherwise 0, without any considera-tions about the semantic relationship between two concepts.

In this paper, the news-user similarity boils down to investigating the underlying se-mantic relatedness between two concepts. We propose to model the similarity between two concepts by taking their background ontological structures into account: where, c 1 and c 2 are two concepts, d is the shortest distance from c 1 to c 2 on the world ontological structure, while  X  is the shortest distance from their lowest common ancestor to the root node of the ontology, H the height of the ontology.

In Formula 1,  X  log 2 H d 2 H has negative correlation with d and prefers two adjacent concepts. e  X  e  X  +1 is considered as a weight of  X  log 2 H d 2 H and will prefer two more con-crete concepts at the bottom of the ontology , based on the assumption that two adjacent concrete concepts at a lower level tend to be more similar than at a higher level, since they share more common information from their ancestors.
 News-User Similarity. Now we are ready to compute the similarity between a news article and a user profile as: where w i,j is taken as a confidence of Csim ( c n i ,c u j ) , We can see that when w n i and w u j are about equal, c n i and c u j have similar importance to their corresponding concept sets, the concept similarity Csim ( c n i ,c u j ) will have a higher confidence ( w i,j ). The smoothing factor k in Formula 3 is used to control the sensitivity of the confidence factor w i,j .Larger k leads to a sensitive confidence function, which in turn penalize the concept pairs that have distinct importance in their own sets.
Now with Formula 2 at hand, we are ready to compute the similarities between news articles and user profiles, and the articles with similarities over a threshold will be rec-ommended to the users. In this section, we evaluate our approaches and several traditional ontology-based base-line models on both English and Chinese datasets on a news recommendation platform (NRS) we developed. As of today, our NRS has about 581 users.
 English Dataset. We construct the English ontology from DBPedia with 3.6 million things, and randomly select 6,000 articles from New York Times (2006-2007) for our English experiments. Chinese Dataset We construct our Chinese ontology with 5 mil-lion entries extracted from Hudong encyclopedia and around 6,000 articles are collected everyday from Sina News for our Chinese experiments.
 Experimental Setup. We use CF-IDF[2] and Jaccard[4] as our baselines. Both methods model articles and user profiles in a bag-of-concepts format, and calculate news-user similarity by either consine or Jaccard sim ilarity. We perform our English experiment and Chinese experiment, respectively, as follows: 1) 400 news articles are randomly selected by NRS at one time , and shown to the users. Every user has to read all 400 articles and indicates whether it is interes ting or not for each article. 2) Then, the 400 articles with ratings (interesting or uninteresting) are randomly split into two sets. 60% for training, 40% for testing. There are relatively equal proportions of interesting arti-cles in each set. 3) For each user, NRS constructs the profile from the interesting articles of the training set. The test set is used by different algorithms to calculate the news-user similarity. An article is marked as interesting if its similarity is higher than a thresh-old. 4) For each article in the test set, we compare the model generated ratings with the human ratings, and compute Precision, Recall and F-measure for all methods. 5) In order to obtain reliable results, we ask NRS to repeat the evaluation process for 2000 iterations by randomly sp litting the training/testing sets i n each iteration, and calculates the average performance for each algorithm. 5.1 Precision, Recall and F-Measure After the experiments are conducted, the pr ecision, recall and F-measure are calculated by NRS. Table 1 shows that in the experiment over English and Chinese data sets, OBSM outperforms other algorithms on all aspects by a large margin. This is not sur-prising: our model utilizes the structured information embedded in the ontologies, and provides more reasonable similarity estimations between concepts, which is obviously better than using these concepts in a bag-of-words format.

In order to investigate the overall perform ance of these algorithms over different thresholds, we plot the the curve of the F-measure against different thresholds from 0.00 to 1.00. As figure 2(a) and 2(b) show, OBSM scores higher than others consistently. The standard threshold used for testing is set to 0.5, where OBSM has the highest F-score. 5.2 Precision-Recall Curves When PR curves for these algorithms are plo tted in one figure, we can see the difference clearly. As figure 2(c) and 2(d) show, in the experiment over both English data set and Chinese data set, OBSM performs better than CF-IDF and Jaccard.

OBSM performs a bit lower on the Chinese dataset than the English dataset, but others perform almost the same on both datasets. The reason may be that the ontology of Hudong encyclopedia is coarsely crafted as we mentioned in section 3, while DBPedia takes a more clear and less duplicate ont ology. Recall that CF-IDF and Jaccard just use bag of concepts in the ontology and do not take any structure information of the ontology into account, while OBSM calculates the similarity between the news article and user profile by utilizing the structure of the ontologies. That is why CF-IDF and Jaccard show consistent performances no ma tter how messy the ontological structure is, while OBSM performs worse on the Chinese dataset than the English one. In this paper, we exploit semantic analysis in the content-based filtering framework to boost personalized news recommendation by employing background knowledge in the form of ontological structures automatically obtained from the web without expensive human annotations. This is novel to the best of our knowledge to make use such kind of resources in RS. We compared our model again st several traditional ontology-based ap-proaches in a news recommender system (NRS) over both English and Chinese datasets, and the results show that OBSM outperforms the baseline models on all measures over both datasets.

