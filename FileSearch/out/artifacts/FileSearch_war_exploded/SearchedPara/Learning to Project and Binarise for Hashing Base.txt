 In this paper we focus on improving the effectiveness of hashing-based approximate nearest neighbour search. Gen-erating similarity preserving hashcodes for images has been shown to be an effective and efficient method for search-ing through large datasets. Hashcode generation generally involves two steps: bucketing the input feature space with a set of hyperplanes, followed by quantising the projection of the data-points onto the normal vectors to those hyper-planes. This procedure results in the makeup of the hash-codes depending on the positions of the data-points with respect to the hyperplanes in the feature space, allowing a degree of locality to be encoded into the hashcodes. In this paper we study the effect of learning both the hyperplanes and the thresholds as part of the same model. Most previ-ous research either learn the hyperplanes assuming a fixed set of thresholds, or vice-versa. In our experiments over two standard image datasets we find statistically significant increases in retrieval effectiveness versus a host of state-of-the-art data-dependent and independent hashing models.
Nearest neighbour search is the problem of finding the most similar data-points to a query in a database, and is a fundamental operation that has found wide applicability in many fields of study, ranging from Information Retrieval (IR) to Bioinformatics. Hashing-based approximate near-est neighbour (ANN) search methods permit the nearest neighbours to a query data-point to be retrieved in constant time [5]. Hashing permits constant time search per query by condensing both the database and the query into fixed-length compact binary hashcodes or fingerprints. The hash-codes exhibit the neighbourhood preserving property that similar data-points will be assigned similar (low Hamming distance) hashcodes. To compute these hashcodes, many hashing models partition the input feature space into dis-joint regions with hyperplanes [1, 6, 9]. In the case of hy-perplanes the polytope-shaped regions formed by the inter-secting hyperplanes constitute the hashtable buckets. The hashtable key for a data-point is generated by simply de-termining which side of the hyperplanes the data-point lies. Depending on which side it falls a  X 0 X  or a  X 1 X  is appended to the hashcode for that data-point. By repeating this pro-cedure for each hyperplane we can build up a hashcode for each data-point that is the same length as the number of hyperplanes partitioning the space.

Algorithmically this hashcode generation procedure can be accomplished in two separate steps performed in a pipeline: projection followed by quantisation . Projection involves a dot product of the feature vector representation of a data-point (e.g. bag of visual words) onto the hyperplane normal vectors positioned either randomly or in data-aware posi-tions in the data-space. The hyperplanes should ideally par-tition the space in a manner that gives a higher likelihood that similar data points will fall within the same region, and therefore assigned the same hashcode. In the second step the real-valued projections are quantised into binary ( X 0 X  or  X 1 X ) by thresholding the corresponding projected di-mensions 1 typically with a single threshold placed at zero for mean centered data.

In our contribution we depart from most previous work and study the effect of learning both of the hyperplanes and the quantisation thresholds as part of the same hashing model. On image retrieval experiments over standard col-lections we demonstrate that learning both parameters can achieve a significant increase in retrieval effectiveness versus models that learn either parameter in isolation.
In the most closely related past research authors have gen-erally focused on either learning the hashing hyperplanes [1, 4, 6] or the quantisation thresholds [2, 3, 7, 8] based on the distribution of the data. Seminal approaches for data-dependent hyperplane learning either solving an eigenvalue problem to generate a set of orthogonal hyperplanes, for ex-ample using Principal Components Analysis (PCA) [9], or frame a custom objective functions that uses pairwise labels to appropriately position the hyperplanes within the feature space [1]. Many of these models for hyperplane learning use a single threshold placed directly at zero (for mean centered data) to quantise the projections into binary. This approach is commonly known as Single Bit Quantisation (SBQ). Re-cently there has been significant interest in improving on
W e define a projected dimension as the collection of the real-valued projections (dot products) of all data-points onto the normal vector to a hyperplane. SBQ by learning one or more thresholds to quantise pro-jections [2, 3, 8, 7]. These quantisation models use either an unsupervised objective such as k-means or squared error minimisation to learn a good set of thresholds for each pro-jected dimension [7, 8], or propose a semi-supervised objec-tive that takes into consideration the neighbourhood struc-ture between the data-points in the input feature space [2, 3].
We couple the graph regularised projection model (GRH) of [1] and the semi-supervised quantisation model (NPQ) of [2]. The hyperplanes are learnt using GRH and the projec-tions quantised using NPQ. We aim to answer the following research question: RQ-1: Can learning the hashing hyperplanes and multiple quantisation thresholds as part of the same model give a higher retrieval effectiveness than a model which learns ei-ther the hyperplanes or thresholds?
Initially, the model is first run for M iterations, in which the following three steps (1-3) are applied per iteration: Having learnt the projections of our data-points in steps 1-3, we then quantise the learnt projections using NPQ [2]: Quantisation step (Step 4) using the projections for hyperplane h shows the result obtained by optimising the threshold t 1 point a receives the same bits as its neighbours. Figures 1-2 provide an intuitive overview of steps 1,2 and 4. Groundtruth : We define the groundtruth nearest neigh-bours using the  X  -NN paradigm 2 , that is if a data-point is  X  i s set to be the average L 2 distance to the 50 th nearest neighbour. This parameter setting follows previous work [7, 8]. within a radius of  X  to the query then it is deemed to be true nearest neighbour for the purposes of evaluation ([2, 6, 8, 7]). To evaluate the quality of the hashcodes, database images are ranked based on the Hamming distance to the hashcodes of the queries. The resulting ranked lists are used to compute the area under the precision recall curve (AUPRC). The higher the AUPRC, the better the quality of the hashcodes.
 Parameter Optimisation : We use three thresholds ( T = 3) per projected dimension for NPQ and use the Manhat-tan hashing codebook and ranking strategy [8]. This corre-sponds to a 2-bit per projected dimension encoding, which means that K/ 2 hyperplanes are learnt for a K -bit hashcode. For GRH we use a held out validation dataset to find the number of iterations M  X  Z + , the interpolation parameter  X   X  [0 , 1] and the flexibility of margin C  X  R + for the linear SVM. Our parameter tuning strategy for the GRH model is as follows: firstly, we set C = 1 and perform a grid search setting that gives the highest validation AUPRC. To find M we stop the sweep when the validation dataset AUPRC falls for the first time, and set M to be the number of the penul-timate iteration. Finally, we then find the optimal value for the flexibility of margin C  X  X  0 . 01 , 0 . 1 , 1 . 0 , 10 , 100 } . Datasets : CIFAR-10 3 and Tiny 100K image [8] datasets both encoded with GIST features. To define the test queries we randomly sample N teq =1 , 000 data points with the re-maining points forming the database for retrieval and the training dataset for learning the hyperplanes and quantisa-tion thresholds. We set N trd =2 , 000 for learning. Reported AUPRC figures are the average over 10 independent runs. Initialisation Methods : LSH [5], Shift-invariant Kernel-based Locality-Sensitive Hashing (SIKH) [6], PCA Hashing (PCAH) [4] and Spectral Hashing (SH) [9]. h ttps://www.cs.toronto.edu/  X kriz/cifar.html Bars show standard error of the mean.
 LSH 0.3623 0.4970 0.4791 0.4796 0.5680 N N Table 1: AUPRC on the T iny 100K dataset with a hash-code length of 32 bits. NN indicates statistical significance (Wilcoxon signed rank test, p &lt; 0 . 01) when comparing GRH + NPQ to the next best model.
The Hamming ranking retrieval results are shown in Ta-bles 1-2 and Figures 3(a)-(b). We observe in Tables 1-2 that learning the hyperplanes and thresholds as part of the same combined model (GRH+NPQ) yields the highest retrieval effectiveness compared to learning the hyperplanes (GRH) or the thresholds (NPQ, VBQ) independently. For example, for LSH projections GRH+NPQ gains a relative increase in AUPRC of 60% over NPQ and 28% over GRH on CIFAR-10. Furthermore, the combination of GRH+NPQ outperforms the adaptive thresholds allocation model (VBQ) of [3] by a relative margin of 27%. Each of these increases are found to be statistically significant using a Wilcoxon signed rank test (p-value &lt; 0 . 01). We find that the superior retrieval ef-fectiveness of GRH+NPQ is maintained when the hashcode length is varied between 16-128 bits for both LSH and PCA projections (Figure 3(a)-(b)) on CIFAR-10. In most cases, significant increases in effectiveness are found for other pop-ular projection functions including SH and SKLSH across both datasets (Tables 1-2). Based on these experimental results we can answer RQ-1 in the affirmative.
In this paper we have explored the benefits of learning the hashing hyperplanes and multiple quantisation thresholds as part of the same hashing model. We introduce a simple cou-pling of two state-of-the-art models for projection function and quantisation threshold learning [1, 2] and find statisti-LSH 0.0954 0.1621 0.2023 0.2035 0.2593 N N SKLSH 0.0513 0.1063 0.1652 0.2122 0.2566 N N Table 2: AUPRC on the C IFAR-10 dataset with a hash-code length of 32 bits. cally significant increases in retrieval effectiveness. Future research will explore a tighter coupling between the learning of the hyperplanes and quantisation thresholds by creating a unified objective function encompassing both parameters that could be jointly learnt using gradient-based optimisers.
The author would like to thank Victor Lavrenko for his valuable feedback on this research.
