 Metabolomics is the omics science of biochemistry. The as-sociated data include the quantitative measurements of all small molecule metabolites in a biological sample. These datasets provide a window into dynamic biochemical net-works and conjointly with other omic data, genes and pro-teins, have great potential to unravel complex human dis-eases. The dataset used in this study has 63 individuals, normal and diseased, and the diseased are drug treated or not, so there are three classes. The goal is to classify these individuals using the observed metabolite levels for 317 mea-sured metabolites. There are a number of statistical chal-lenges: non-normal data, the number of samples is less than the number of metabolites; there are missing data and the fact that data are missing is informative (assay values be-low detection limits can point to a specific class); also, there are high correlations among the metabolites. We investi-gate support vector machines (SVM), and random forest (RF), for outlier detection, variable selection and classifica-tion. We use the variables selected with RF in SVM and visa versa. The benefit of this study is insight into interplay of variable selection and classification methods. We link our selected predictors to the biochemistry of the disease. Categories and Subject Descriptors: H.2.8 [Database Management]: Database applications -Data Mining; I.2.6 [Artificial Intelligence]: Learning General Terms: Measurement Keywords: Metabolomics, Random Forest, Support Vec-tor Machines, Missing Data
Complex data sets are becoming available in the post-genomic era; there is an increasing interest in the analysis of genetic information (genomics), and their transcription Copyright 2004 ACM 1-58113-888-1/04/0008 ... $ 5.00. (transcriptomics) and subsequent translation into protein (proteomics). The recently emerging science of metabolomics completes the primary omics ladder of DNA, RNA, proteins and metabolites. As such, it provides another window into cellular function. Metabolomics is the biochemical profil-ing of all small molecules, biochemicals or metabolites, in an organism. Even though the dynamic nature of metabo-lites makes them difficult to measure, recent advances in technology allows robust quantification of the concentra-tions of hundreds to thousands of metabolites from a bi-ological sample [11]. This new omics science offers potential insight into the dynamic interactions in metabolic pathways and an unambiguous representation of cellular physiology [2]. Furthermore, patterns of metabolites can be used to identify biomarkers of specific disease, understand patholog-ical development, and propose targets for drug intervention. We might even track metabolites returning to normal under treatment.

Data obtained through a metabolomic experiment posses a number of challenges to statistical modeling. The num-ber of metabolites measured ( p = 317), usually in the hun-dreds is much larger than the number of biological samples ( n = 63) typically available. Additionally, there may be severe distributional difficulties such as non-normal distri-butions, outliers (unusual data values), missing values, and high correlations among metabolites. Common objectives are finding  X  X atterns X  in the data, in particular, cluster-ing of the biological samples (rows) into groups with similar metabolic expression profiles; and clustering the metabolic concentrations (columns) into groups where the pattern of metabolic expression is similar in the samples.

Due to the challenges of modeling this dataset, many classification and clustering techniques may produce sub-optimal results. For example, most techniques are influ-enced by outliers and can not accommodate missing values. A disadvantage of hierarchical clustering techniques is that the dendrograms produce orderings of rows (biological sam-ples) that are not unique [8]. To address these issues, we previously used a method called robust singular value de-composition (rSVD) [8] (see also [6]) to detect the cluster. This method was compromised by another method based on recursive partitioning (RP) [1]. RP uses information on the classification of the objects whereas rSVD uses only the information on the metabolites, learning with supervision versus learning without supervision.
In this paper, we explore two more techniques on this metabolomic dataset (see Section 2). The dataset contains biological samples in which there are three different groups: diseased on medication, diseased not on medication and healthy individuals. One of the main scientific interests is to identify other groups within these major groups based on the metabolites data. Even though the samples were analyzed using a detector that is better than conventional mRNA, the dataset contains blanks where the metabolic concentrations were below detection limits. Subject to these conditions, the goal of this study is to use support vector machines and ran-dom forests in an exploratory manner to identify metabo-lites that classify samples into consistent groups that cor-respond to their known biological classification by experts. Moreover, it is of scientific interest to examine which or how many of these metabolites are important in classifying these cases.
Inthispaperweuseadatasetgeneratedfrombloodsam-ple of people with and without a particular disease. We know from the medical diagnosis which people have the dis-ease and which do not. There is evidence that some of the people with the disease self-medicate so one or more of the blood metabolites could identify those people. The data set was collected to test the analytical equipment and as a proof of concept to support additional studies. The concept is that a sample of blood will be taken from a control and a target population. The separation and mass spec process should give a profile of metabolic products that is distinct for the target population. If this can be done, then there is an op-portunity for better diagnosis. There is also the possibility of dynamically tracking either disease progression or how treatment modifies metabolites. Just as in genomic studies, there will be a need to track back to annotated data bases. The big advantage of mass spec is the very high precision it allows for the identification of metabolic products. There are many statistical challenges with data of this sort.
The data was acquired by an established global electrical chemical array profiling method. It is expected that addi-tion of massspec capabilities for assaying equivalent numbers of different classes of compounds will significantly enhance the categorical separation and mechanistic insights in such data sets [7]. The data contains the metabolic profiles of 63 samples: 32 healthy subjects, 22 subjects diagnosed with a specific disease state and not taking medication, and 9 sub-ject diagnosed with the disease who are taking medication. Each profile contains the intensity (abundance) levels of 317 compounds. Blanks in the data indicate the metabolic con-centrations were below detection limits.
As its name suggests, a random forest is a collection of identitically distributed trees. Each tree is constructed via a tree classification algorithm such as CART [4]. A ran-dom forest is formed by using bootstrap samples from the learning set. That is, each tree is grown on a bootstrap replicate of the learning sample. The simplest random for-est with random features is formed by selecting randomly, at each node, a small group of input variables to split on. The size of the group is fixed throughout the process of growing the forest. Each tree is grown by using the CART method-ology without pruning. After the forest is formed, drop a case with input x into the forest for each tree to classify The forest chooses the class for x having the majority vote. Specifically, for each case, the proportion of votes for each class is recorded. For each member of a test set (with or without class labels), these proportions are also computed. They contain useful information about the case. The margin of a case is the proportion of votes for the true class minus the maximum proportion of votes for the other classes. The size of the margin gives a measure of how confident the clas-sification is. A very useful reference to RF is [3].
Since an individual tree is unpruned, the terminal nodes will contain only a small number of instances. Run all cases in the training set down the tree. If cases i and j both land in the same terminal node, increase the proximity between i and j by one. At the end of the run, the proximities are divided by twice the number of trees in the run and proximity between a case and itself set equal to one.
Features of RF can be highlighted as follows (see also [3]):
The design of random forests is to give the user a good deal of information about the data besides an accurate pre-diction. The information includes:
Support vector machine (SVM) is a supervised learning technique for classification and regression. The key to the success of SVM is the kernel function which maps the data from the original space into a high dimensional (possibly in-finite dimensional) feature space. By constructing a linear boundary in the feature space, the SVM produces nonlinear boundaries in the original space. When the kernel function is linear, the resulting SVM is a maximum-margin hyper-plane. Given a training sample, a maximum-margin hyper-plane splits a given training sample in such a way that the distance from the closest cases (support vectors) to the hy-perplane is maximized. Typically, the number of support vectorsismuchlessthanthenumberofthetrainingsam-ple. Nonlinear kernel functions such as the polynomial ker-nel and the Gaussian (radial basis function) kernel are also commonly used in SVM. The computational complexity of the SVM depends on the training sample, thus it avoids the traditional problem of  X  X urse of dimensionality X . One of the most important advantage for the SVM is that it guarantees generalization to certain extend. Namely, the decision rules reflect the regularities of the training data rather than the incapabilities of the learning machine. Because of the many nice properties of SVM, it has been widely applied to virtu-ally every research field. More detailed discussion of SVM and kernel methods can be found in [9].
In our analysis, we use RandomForest 4.0-7 for R and the soon to be released version of the fortran code developed by Breiman and Cutler. Table 1: Confusion matrix and the error rate. The OOB estimate of error rate: 12.7%
The current version of random forests contains some mod-ifications and major additions to earlier versions. The addi-tions are:
To highlight these features, we grow a forest using 500 trees. The size of the selected group of input variables has a default value of square root of the total number of inputs (
F =[ The bootstrap training sample on which each tree is grown has about 2/3 of the cases. The remaining 1 / 3 cases are the out-of-bag (OOB) samples. By putting back into the asso-ciated tree they form a test sample that gives the ongoing OOB estimate of test set error. If an individual variable in the OOB cases is randomly permuted before being put back into the tree, then the decrease in the estimated margins is an indication of how important that variable is. In building the forest for the metabolite data, the number of variables tried at each split is 17. The OOB estimate of error rate is 12.7%. The result is summarized in Table 1. The error rate is depicted in Figure 1.
 Figure 3: Variable importance according to group (Healthy=left, Diseased-treated=middle, Diseased-untreated=right).
 What was also interesting from a scientific viewpoint was an estimate of the importance of each of the 317 metabolites. From the result of random forest, Figure 2 shows the mean decrease accuracy and the mean decrease Gini (as an indica-tion of node impurity) for all three classes, sorted from the most decrease to the least. Figure 3 contains plots of the mean decrease Gini in each class, sorted according to the magnitude of decrease. To get further insight about these variables or metabolites, we used a tool (still being devel-oped) by putting the three classes together, and distinguish them by color as in Figure 4. Useful information on vari-able importance can also be highlighted by blurring the less important ones. This indicates a significant step in feature selection without losing the accuracy in predicting the class membership. These figures suggest a set of less than 15 (out of 317) metabolites would provide a very reasonable classi-fication rule. Figure 4: Alternative plot of variable importance. Important variable are number 6, 7, and 8.
 RF allows the user to see the number of variables employed in building each tree as well as the size of the tree in the forest. A histogram of the size of the tree in our forest is given in Figure 5. It is observed that this forest has many small trees with a mean about 8. The range of the tree sizes is 4 to 12.
 From a study conducted based on the technique rSVD, it is known that there are outliers in this data set [10]. RF de-fines outliers as cases having small proximities to all other cases. Figure 6 is a graph of outlyingness for the metabolite data using the normalized measure of outlyingness described above. There are at least three large values, which are pro-found enough to be considered as outliers. The finding here is consistent with method based on rSVD in [10].
 According to the clinical information, we suspect that there are two normal subgroups and they were not easily detected Figure 7: Multidimensional scaling plot. Diseased-untreated=circle, Diseased-treated=triangle, Healthy=cross. Figure 8: MDS plot combined with orthogonal projections. Diseased-untreated=circle, Diseased-treated=triangle, Healthy=cross. There are two healthy subgroups. Table 2: Confusion matrix and the error rate for im-puted data. The OOB estimate of error rate: 14.0%. Table 3: Comparison of different SVM techniques on the metabolonmic data set.
 class.error 19.0% 23.8% 28.6% 17.4%
Features N/A N/A N/A 32 using methods such recursive partitioning or rSVD reported previously [10]. Figure 7 shows the first two multidimen-sional scaling coordinates obtained by using the proximity matrix.

RF is currently under a major revision, we used a graph-ical interface developed in JAVA to view the MDS plot through orthogonal projections or rotations. Figure 8 shows a rotated multidimensional scaling plot. The two normal subgroups are clearly displayed.
 In a previous study[10], we applied rSVD and recursive par-titioning methods to classify the outcomes, but did not look into the issue of imputing the missing values. In the current paper, RF was used to impute the missing value values. The test set error rate based on the imputed data is summarized in Table 2. The error rate is higher than that given in Ta-ble 1, which was obtained by replacing the missing values by zeros. The missing values occurred when the machine could not detect the intensity level of the metabolite, and so it may not be a good idea to treat below-detection values as missing at random.
We perform several SVM analysis on the metabolomic data set. For the classical SVM, we used the OSU SVM package available at http://www.ece.osu.edu/ maj/osu svm/. We also applied the LPSVM [5] for the SVM with feature se-lection. In this study, we divide the samples into the disease and non-disease groups and perform binary classifications. For the classical SVM, we apply both the linear kernel and the nonlinear kernel (second degree polynomial and Gaus-sian). In all these studies, leave-1-out cross validation is used to choose the optimal tuning parameters.

Table 3 describes the error rates for these methods. Ap-parently, the error rate obtained from the LPSVM: 0.174 is the best among all the five methods. Moreover, this supe-rior performance is achieved by using only 32 out of the 317 metabolites.
In this study, we observed that the SVM has the tendency to overfit the data. That is, it will be difficult to apply the tool to select important metabolites to classify cases. It also has a higher error rate. On the other hand, random forests do not overfit and the method provides many useful tools for detecting hidden structures in the data. We have used the outlyingness measure to search for the profound obser-vations, and the newly implemented graphical procedure to detect the hidden subgroups in the healthy cohort. Further-more, the variable importance feature has been effective in choosing the relevant metabolites, this is important for un-derstanding the nature and the biochemistry of the disease. The authors gratefully acknowledge Dr.s Bruce Kristal, Steve Rozen, Rima Kaddurah-Daouk, Wayne Matsen, Misha Bogdanov, Flint Beale, and Dr.s Merit Cudkowicz and Robert Brown at the Massachussetts General Hospital for their con-tributions to the organization of the original dataset and for access to the clinical samples. We also wish to thank David Banks and Leanna House for many helpful discussions and ideas.
Additional authors: Adele Cutler (Department of Mathe-matics, Utah State University, Logan UT 84332-3900, email: adele@math.usu.edu ) and S. Stanley Young (National In-stitute of Statistical Sciences, Research Triangle Park, NC 27709, email: genetree@bellsouth.net ). [1] FIRMPlus . Golden Helix Inc, Bozeman, MT, USA. [2] C. Beecher. Metabolomics: The newest of the  X  X mics X  [3] L. Breiman. Random forests. Machine Learning , [4] L.Breiman,J.Friedman,R.Olshen,andC.J.Stone.
 [5] G. Fung and O. L. Mangasarian. A feature selection [6] D. M. Hawkins, L. Liu, and S. Young. Robust singular [7] B. S. Kristal, K. Vigneau-Callahan, and W. R. [8] L. Liu, D. Hawkins, S. Ghosh, and S. Young. Robust [9] B. Scholkopf and A. Smola. Learning with Kernels . [10] S. J. Simmons, X. Lin, C. Beecher, Y. Truong, and [11] M. Stitt and A. R. Fernie. From measurements of
