 1. Introduction
In Information Retrieval (IR) one of the main problems facing users is that of expressing their informa-tional need in a query suitable for the retrieval system. Apart from the requirements of the system for for-mulating the query, the main problem lies in determining the set of words or terms that express this need semantically. The problem is aggravated owing to the effect of inconsistency in the subjective assigning of terms to concepts, or what is the same, that two people use different words for defining the same concepts ( Furnas, Landauer, Gomez, &amp; Dumais, 1987 ).

In current IR systems with complete text this approach is especially important. The content of a docu-ment is represented using the words that appear in it. The set of index terms is formed by all the words in the document collection after pre-processing the text, which usually includes the elimination of stop words, stemming, selection of terms by semantic category, etc. The user  X  s informational need must also be repre-sented using these terms. The main drawback to this approach is the well-known vocabulary mismatch prob-lem . On the one hand, there are different words that express the same concepts (synonymy) and on the other hand, the same word can have several meanings (polysemy). In these systems it is assumed that two doc-uments are similar if they contain the same words. But this is the problem: the same concept can be ex-pressed with different words, and one same word can appear in documents dealing with completely different subjects.

In this situation, depending on their experience, users will normally have to reformulate their queries until they obtain suitable results. The user begins by sending an initial query to the retrieval system which returns a set of documents ordered according to some relevance criterion of the system. After examining the documents retrieved, the user has to reformulate the query to obtain more relevant results. The greater the number of terms in the query, the smaller the problem, since it will surely include a greater number of index terms that represent relevant documents ( Xu &amp; Croft, 2000 ). In general, the construction of the new query involves expanding terms of the initial query and recalculating the importance of each term in the expanded query ( reweighting of terms ).

In this study we focus on short queries, i.e. queries with one or only a few terms. These queries are par-ticularly interesting because they are the typical ones made on the search engines of the Internet. It is well-known that queries on the web tend to be very short, between one and three terms per query ( Jansen, Spink, &amp; Saracevic, 2000 ; Wolfram, Spink, Janses, &amp; Saracevic, 2001 ). Retrieval results can be improved by expanding the initial query, i.e., including better terms in the query that provide more relevant documents than the original query. The problem lies in finding these better terms. 2. Query expansion
To expand the query, words or phrases with a similar meaning to those in the initial query must be used. A dictionary or general thesaurus could be used in this process. A thesaurus is a classification system compiled of words and/or phrases organised with the objective of facilitating the expression of ideas. They have been used in retrieval information in the process of creating or expanding queries, i.e. in the process of expressing or broadening the informational needs of a user in a query. Basically this is done by selecting the terms closest to what the user wants. Unfortunately, the use of a general thesau-rus for formulating the user  X  s need does not give good results ( Voorhees, 1994 ), mainly because the relations in a general thesaurus are not valid in the local context of user and document collection. Bet-ter results are obtained if thesauri or query expansion techniques constructed from the document col-lection on which the search is launched are used. In any case, reformulation of the query involves two major problems: the choice of the most suitable terms and the weighting of the new terms. This process can be done automatically, i.e., without the intervention of the user, or with the user  X  s collaboration. In the latter case, the user is offered a graphic interface with the results of the search. In Belkin et al. (2001) the evolution of research in the reformulation of queries in the interactive task of the famous
TREC ( Text REtrieval Conference ) is analysed and some user interfaces can be seen. With this interface the users can visualise and select the documents and terms that they consider more relevant to their initial search and the system will use them to reformulate the query.
Mainly three mechanisms are distinguished, depending on the technique used for formulating the new query ( Baeza-Yates &amp; Ribeiro-Neto, 1999 ): (i) the so-called query feedback with the intervention of the user ( user relevance feedback ); (ii) query reformulation, using exclusively information from the documents re-trieved ( local analysis ) and (iii) reformulation using overall information from the whole document collection ( global analysis ). These strategies are based on the idea that if two items are similar (with respect to any criterion) the terms that frequently co-occur in them will be related. Based on this idea, seman-tic relations between the terms are sought. The Association Hypothesis ( van Rijsbergen, 1979, p. 104 ) says:
Indeed, considering that the terms that appear in the query are good at discriminating relevant and non-relevant documents, the related terms will also be so. This would allow us to add them to the original query.
The first of the techniques indicated is user relevance feedback (RF). This is a mechanism that has been the object of much study and one of those that give the best results. In RF process the user marks the doc-uments retrieved as relevant and non-relevant, and the system reformulates the query with the terms of these documents. The main advantage is that it is a process guided by the user  X  s criteria of relevance, which is the basis for trying to increase the importance of certain terms and decrease that of others ( Salton &amp;
Buckley, 1990 ). One of the algorithms most used is that of Rocchio ( Rocchio, 1971 ). Our research group has also made some experiments on the subject ( Figuerola, Zazo, &amp; Berrocal, 2002 ). A similar technique that does not require the presence of the user is pseudo relevance feedback (PRF). In PRF the first 10 or 20 retrieved documents assume relevant and the algorithm of Rocchio is applied to obtain the reformu-lated query. It is a simple local expansion method that obtains good results ( Mitra, Singhal, &amp; Buckley, 1998 ).

The other two techniques for query expansion look for relations or associations between terms: syno-nyms, derivatives (inflectional and derivational variants) or proximal ones (which have a minimum distance in number of words). Clustering algorithms are normally used for this. These algorithms are sometimes based on counting the co-occurrences of terms in the documents, and other times on finding similarities under other criteria. Global analysis looks for relations between the terms of the whole document collec-tion, whereas local analysis only considers the documents retrieved and shown to the user for one particular query. Both seek to construct a matrix or thesaurus of relations between the terms, in a global or local sense, and use it to expand the original query with the terms considered best related. Two important strat-egies stand out in local analysis: on the one hand, what is known as local clustering , based on the study by
Attar and Fraenkel (1977) , and, on the other hand, the combination of certain techniques of local and glo-bal analysis, called local contextual analysis ( Xu &amp; Croft, 1996, 2000 ). Both obtain good results based on the documents retrieved.

The global analysis techniques extract information from the whole collection of documents and use it to expand the query. Research in this field has been developing since before the 1960s. Most of it has had as a basis the study of co-occurrences of terms. It was not until the beginning of the 1990  X  s, however, that satis-factory results were obtained ( Peat &amp; Willet, 1991 ), mainly using clustering of term, although also of docu-ments. Term clustering is used for the construction of thesauri. Based on the criteria used in its construction, some approaches are distinguished: thesauri constructed using simple measurement of term co-occurrences ( Minker, Wilson, &amp; Zimmerman, 1972 ); using clustering of documents to create a thesaurus of infrequent terms ( Crouch, 1990 ); thesauri constructed making the transposition of the document-term matrix (similarity from several sources of expansion (WordNet, general dictionaries, local thesauri, manual lexicon or parallel corpus) ( Mandala, Tokunaga, &amp; Tanaka, 1999 ; Xu, Fraser, &amp; Weischedel, 2001 ).

The main strategies carried out, both automatically and with the intervention of the user, have been based on the simple use of co-occurrence values, the classification of documents or the use of syntactical contexts, with results highly dependent on the document collection to which they were applied, or on parameters that are difficult to calculate or to adjust ( Crouch &amp; Yang, 1992 ; Grefenstette, 1992b ; Han,
Hull, &amp; Pedersen, 1995 ; Xu et al., 2001 ). In contrast with these methods there is another that is attempting to create a similarity global thesaurus as a basis for query expansion. 3. Similarity thesauri
The target here is to construct a similarity thesaurus that makes it possible to expand the complete query ( query concept ), and not only each individual term separately ( Qiu &amp; Frei, 1993 ). A similarity thesaurus is a matrix that is constructed using relations between terms. In its preparation the co-occur-rence of terms in the documents is not measured, instead a mechanism is used by which each term in the collection is characterised by the documents in which it appears. This turns the classic concept of information retrieval systems upside down (in these, documents are characterised by index terms, i.e., the terms are used to represent the documents). To construct the similarity thesaurus, the terms of the collection are considered documents, and the documents are used as index terms (they are usually called index documents ); in other words, the documents can be considered capable of representing the terms.

In order to apply this approach we shall take as a basis the vector space model. In the vector space model ( Salton, 1968 ) each document d i in the collection of N documents is represented by a vector ~ d  X  X  w i 1 ; w i 2 ; ... ; w im  X  , where w ij indicates the weight of the index term t ber of index terms in the collection. The weights are normally calculated using the tf-idf scheme ( Salton &amp;
Yang, 1973 ). The query is also represented in the vector space of terms using a vector, ~ q  X  X  q
Each element q j expresses the degree to which the term t making the query. The main objective of an information retrieval system is to provide information appro-priate to the query made by the user. This process depends enormously on the weighting scheme of the terms and subsequent calculation of similarity made by the system. In Salton and Buckley (1988) 287 dif-ferent combinations of assigning weights to terms of documents and queries were experimented with, and we have followed the indications that appear there in order to obtain the best results. The easiest way of obtaining the degree of similarity between a query and a document is to calculate the scalar product of the vectors that represent them ( van Rijsbergen, 1979 ). This is a simple process and one of the most used.
So that the similarity value will be between 0 and 1, the vectors of the documents and queries are normal-ized.

Below we shall see how the similarity thesaurus can be constructed in the vector space model. The idea is to turn around the representation model described and instead of considering the documents as rep-resented by index terms, we consider that the index terms can be defined using the documents ( index doc-uments ). Turning the vector model around, each term t i in the collection of m terms will be represented by a vector of N components in the vector space of documents, ~ t that expresses the weight of index document d j in the representation of the term t the value of p ij the tf-idf scheme is used, but the roles of terms and documents is inverted. The weights are normalized in order to have unit vectors. The calculation proposed in Qiu and Frei (1993) is the one indicated in (1) . This calculation derives from the inversion of the ann scheme (in SMART  X  s term weight triple notation ( Salton &amp; Buckley, 1988 )) for documents, plus itf factor, see below. This is the one we used in our experiments. where f ij is the number of times that the term t i appears in document d frequency values for the term t i in the whole document collection (i.e., it will be the value f the document where it appears most), itf j  X  log m j d the number of different terms there are in that document.

Calculation of the inverse term frequency shows that a short document plays a more important role than a long one. If two terms co-occur in a long document, the probability of them being similar is smaller than if they co-occur in a short one. To calculate the similarity between two terms, t also used.

Calculation for all the terms pairs in the collection produces the similarity thesaurus. It is a symmetric matrix, with values between 0 and 1. Its construction is computationally costly, although it is only done once. If documents are added to the collection the values for the terms included in the new documents must be updated. Once again we must point out that the thesaurus is constructed taking into consideration the representation of terms using index documents, and not only co-occurrence information. The importance is the weight of each document in the representation of terms. 3.1. Query expansion
The objective when using the similarity thesaurus is to expand the whole query, and not just each indi-vidual term separately. For a term to be able to be added to the query there must be a high similarity be-tween this term and all the terms in the query. In the vector space of the terms query q is represented by the vector ~ q  X  X  q 1 ; q 2 ; ... ; q m ), where q i is the weight of term t thesaurus has been constructed using the vector space of documents, we must transfer that vector to this space so as to be able to compute the similarity between the whole query and all the index terms in the col-it will have an importance of q i ~ t i . We consider that in this space the query only depends on the terms in-cluded in it.
 Once the query has been represented in the document space, we must obtain the terms most similar to it.
We use the measurement of the scalar product with each of the terms t in the collection.
The similarity values between terms are the entries in the similarity thesaurus which have already been cal-culated. Thus, all the terms of the collection can be put in order according to the value obtained from (3) .In general not all the terms are expanded, but only those which have the highest similarity values. We shall denote as r the number of terms that will be added to the original query. The weight, in the space of terms, associated with each term t e , which will be added to the query, remains to be determined. It seems natural to consider it according to the similarity:
The value of the weight of each expanded term is that given in (4) . That is, new terms are added to the initial query and the weight of the already existing terms can be modified if they appear among the first r selected for expansion. 4. Experiments
In our experiments we employed the test collection used in several CLEF
The mean number of terms was considered after eliminating stop words. The document collection came from the news agency EFE, from all the news items of 1994: 215,718 documents (513 MB of information), stored in files, one for each day of 1994. Each file contains several documents, and in these there are fields delimited with SGML labels, as indicated in the example in Fig. 1 . The relevant fields are TITLE and TEXT , since the rest contain information such as date, section of the newspaper, etc. The mean number of words per document, considering these fields, is 333.68.

Together with the documents there is a test set of 50 queries in Spanish. Each query is divided into three fields, ES-title , Es-desc (description) and ES-narr (narrative), as can be seen in Fig. 2 . Queries can be posed with one, several or all the fields.

In our experiments we considered an index term to be composed of any set of alphanumerical characters, i.e., we also included numbers as index terms. In the lexical pre-processing of the text the terms of the doc-uments and queries that can be used as index terms are obtained. This pre-processing was carried out in several stages. In the first we eliminated all the non-alphanumerical characters, did not detect proper names or acronyms and stored the index terms in small letters without considering accents.

The second action performed in pre-processing the text was the elimination of stop words. With the objec-tive of reducing the number of index terms, words that are not very significant in the information retrieval process because of their slight semantic capacity or high frequency are not included. This set of words, known as stop words, is composed of prepositions, articles, adverbs, conjunctions, possessives, demonstra-tives, pronouns, some verbs and some nouns. With the elimination of stop words, the aim is to reduce the noise that they could introduce in the retrieval. To eliminate stop words, the text was separated into words, which were then checked against the stop word list. In our experiments we used a list of 573 words.
The next step consisted of stemming. Stemming is the process whereby morphological variations of the terms are sought in order to extract the common root. The canonical form of the root represents the var-iations of the terms deriving from it. In our case, we did not apply stemming.
Next, we selected the terms or groups of terms that would be the index terms. This is normally done according to the syntactical nature of the term, since those that act grammatically as nouns usually have a greater semantic content than verbs, adjectives or adverbs. Our selection of terms was not made on the basis of any syntactical criterion. Simply, those remaining after eliminating the stop words were con-sidered as index terms.

The last step in the pre-processing of terms was the construction or application of a thesaurus that al-lowed the query to be expanded. Query expansion has already been described. The objective of this study was to show the characteristics of query expansion using similarity thesauri on the EFE  X  94 document col-lection.

Once the index terms of the test collection had been obtained, representation of documents and queries was done using the tf-idf weighting mechanism and the recommendations of Salton and Buckley (1988) .We used the scalar product to calculate similarities between documents and queries. In the expansion of each query a local similarity thesaurus was calculated for the terms included in it. The related terms as described in Section 3 were obtained and arranged in decreasing order to then take the first r . The results were eval-uated by calculating the average precision for all the queries in the collection in three values representative of recall: a low value of 25%, a medium value of 50% and a high value of 75%. Then the mean of these three values was taken.

In this study we also wished to measure the influence of the number of terms in the original query on the results. For this purpose we did several tests considering different fields of the query: test T ( ES-title field), test D ( ES-desc field) and test N ( ES-narr field). Table 2 shows the improvement in query expansion, with an expansion of 500 terms added to the original query. It can be noted that the greater the number of terms in the original query, the smaller the improvement. This was to be expected, since long queries contain greater description of the user  X  s informational need than short queries.

Fig. 3 shows the results of our experiments according to the number of terms expanded. We can see that the improvement follows the same course for all three tests. A major improvement is noted with the expan-sion of a few terms, but after a certain number (around 250) the improvement increases very slowly, and continues to increase, perhaps indefinitely. This seems to coincide with the experiments of other authors that more discriminating terms are needed in large test collections. 5. Conclusions
The technique described obtains good results in query expansion. A thesaurus of similarity between terms was constructed, taking advantage of the possibilities the documents have to represent the terms. This duality in the representation of information allowed us to construct the thesaurus taking into account a weighting scheme according to the length of the documents and not just the number of times that a term appears in that document. The main characteristic resides, however, in the fact that the expanded terms were chosen and weighted taking into consideration the terms of the whole query, and not each individual term separately.

Moreover, as opposed to other techniques of local analysis that base their expansion on the terms of a few documents with user relevance feedback, here we used a technique that related all the terms in the test collection to each other. In fact, our results are worse than obtained with user relevance feedback, mainly because the expansion we used did not incorporate any type of additional relevance information, which other methods do incorporate. It is, therefore, a mechanism that can be executed automatically a some more characteristic of the information retrieval system. The main drawback lies in the high computational cost required for the construction of the similarity thesaurus. For new documents the task is more simple, since the values of the terms appearing in them are modified.

One very interesting aspect is that the smaller the query, the more it benefits from expansion. Consider-ing that the queries on the search engines on the Internet usually contain one, two or three terms, this technique can be especially useful in this context. However, considering that the volatility of the informa-tion on the Internet is very high, and that major modification of the thesaurus would be required, since the number of documents that would disappear from the collection and the number that would be incorporated is enormous, we believe that this technique can only be used in more static situations.
 References
