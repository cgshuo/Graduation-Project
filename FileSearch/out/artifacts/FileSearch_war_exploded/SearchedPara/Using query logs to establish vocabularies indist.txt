 1. Introduction
Traditional information retrieval systems use corpus, document, and query statistics to identify likely answers to users X  queries. However, these queries can be captured in a query log, providing an additional source of evidence of relevance. In recent years, considerable attention has been devoted to the study of query logs and the way people express their information needs ( de Moura et al., 2005; Fagni, Perego, Silvestri, &amp;
Orlando, 2006; Jansen &amp; Spink, 2005 ). The query logs of commercial search engines such as Excite
Web 3 ( Jansen &amp; Spink, 2005 ) have been investigated and analysed. Query logs have been used in information 2004 ). The question we explore in this paper is how query logs can be used to guide future search, in the con-text of distributed information retrieval.

In distributed information retrieval ( DIR ) systems, the task is to search a group of separate collections and identify the most likely answers from a subset of these. Brokers receive queries from the users and send them to those collections that are deemed most likely to contain relevant answers. In a cooperative environment, col-lections inform brokers about the information they contain by providing information such as term distribu-tion statistics. In uncooperative environments, on the other hand, collections do not provide any information about their content to brokers. A technique that can be used to obtain information about collections in such environments is to send probe queries to each collection. Information gathered from the limited number of answer documents that a collection provides in response to such queries is used to construct a representation set ; this representation set guides the evaluation of user queries.

In this paper, we introduce two novel applications of query logs: sampling for improved query probing, and pruning of index information.

The first of these is sampling. Using a TREC web crawl, we show that query log terms can be used produce effective samples from uncooperative collections. We compare the performance of our strategy with the state-of-art method, and show that samples obtained using query log terms allow for more effective collection selec-tion and retrieval performance  X  improvements in average precision are often over 50%. Our method is at least as efficient as current sampling methods, and can be much more efficient for some collections.
Our second new use of query logs is a pruning strategy that uses query log terms to remove less significant terms from collection representation sets. For a DIR environment with a large number of collections, the total size of collection representation sets on the broker might become impractically large. The goal of pruning methods is to eliminate unimportant terms from the index without harming retrieval performance. In previous work  X  such as that of Carmel et al. (2001), Craswell, Hawking, and Thistlewaite (1999), de Moura et al. (2005) and Lu and Callan (2002)  X  pruning strategies have had an adverse effect on performance. The reason is that these approaches drop many terms that are necessary to future queries. We show that pruning based on query logs does not decrease search precision. In addition, our method can be applied during document index-ing, which means that it independent of term frequency statistics. We also test our method on central indexes and for different types of search tasks. We show that, by applying our pruning strategy, the same performance as a full index can be achieved, while substantially reducing index size. In practice, such pruning might not always be desirable; if a term is present, it should be searchable. However, our pruning does provide an inter-esting benchmark against which other methods can be measured, and is clearly superior to the principal alternative. 2. Distributed search
The vast volume of data on the web makes it extremely costly for a single search engine to provide com-prehensive coverage. Moreover, public search engines cannot crawl and index documents to which there are no public links, or from which crawlers are forbidden. These documents form the so-called hidden web and generally only be viewed by using custom search interfaces supplied as part of the site. Distributed information retrieval ( DIR ) aims to address this issue by passing queries to multiple servers through a central broker. Each server sends its top-ranked answers back to the broker, which produces a single ranked list of answers for pre-sentation to the user. For efficiency, the broker usually passes the query only to a subset of available servers, selecting those that are most likely to contain relevant answers. To identify the appropriate servers, the broker calculates a similarity between the query and the representation set of each server.

In cooperative environments, servers provide the broker with their representation sets ( Callan, Lu, &amp; Croft, 1995; Fuhr, 1999; Gravano, Chang, Garcia-Molina, &amp; Paepcke, 1997, 1999; Yuwono &amp; Lee, 1997 ). The bro-ker can be aware of the distribution of terms at the servers, and is therefore able to calculate weights for each server. Queries are sent to those servers that indicate the highest weight for the query terms.
In practice, servers may be uncooperative and therefore do not publish their index information. Server rep-resentation sets can be gathered using query-based sampling ( initial query is created from frequently-occurring terms found in a reference collection  X  to increase the chance of receiving an answer  X  and sent to the server. The query results provided by the server are downloaded, and another query is created using randomly-selected terms from these results. This process continues until a suf-ficient number of documents have been downloaded ( Callan &amp; Connell, 2001; Callan et al., 1999; Shokouhi,
Scholer, &amp; Zobel, 2006 ). Many queries do not return any yet-unseen answers; Ipeirotis and Gravano (2002) claim that, on average, one new document is received per two queries.
 are not highly representative for the server. 2.1. Query-based sampling
Query-based sampling QBS was introduced by Callan et al. (1999) , who suggested that even a small number of documents (such as 300) obtained by random sampling can effectively represent the collection held at a ser-ver. They tested their method on the CACM collection ( Jones &amp; Rijsbergen, 1976 ) and many other small servers artificially created from TREC newswire data ( Voorhees &amp; Harman, 2000 ). In first are selected by choosing terms from documents that have been downloaded so far ( Callan et al., 1999 ).
Various methods were explored; random selection of query terms was found to be the most effective way of choosing probe queries, and this method has since been used in other work on sampling non-cooperative serv-ers ( Craswell, Bailey, &amp; Hawking, 2000; Si &amp; Callan, 2003 ). These methods generally proceed until a fixed number of documents (usually 300) have been downloaded. However, Shokouhi et al. (2006) have shown that for more realistic, larger collections, fixed-size samples might not be suitable, as the coverage of the vocabulary of the server is poor.

An alternative technique, called Qprober ( Gravano, Ipeirotis, &amp; Sahami, 2003 ), has been proposed for auto-matic classification of servers. Here, a classification system is trained with a set of pages and judgments. Then the system suggests the classification rules and uses the rules as queries. For example, if the classification sys-tem suggests ( Madonna ! Music ), it uses  X  X  X adonna X  X  as a query and classifies the downloaded pages as music-related. Qprober differs from QBS in the way that probe queries are selected and requires a classification system in the background. 2.2. Using query logs for sampling
Terms that appear in search engine query logs are  X  by definition  X  popular in queries, and tend to refer to topics that are well-represented in the collection. We therefore hypothesis that probe queries composed of query log terms would return more answers than the random terms, leading to higher efficiency. Since query terms are aligned with actual user interests, we also believe that sampling using query log terms would better reflect user needs than random terms from downloaded documents. Hence, instead of choosing the terms from downloaded documents for probe queries, we use terms from query logs. Analysis of our method shows that it is at least as efficient as previous methods, and generates samples that produce higher overall effectiveness. 2.3. Evaluation To simulate a DIR environment, we extracted documents from the 100 largest servers in the lection ( Bailey, Craswell, &amp; Hawking, 2003 ). These sets vary in size from 26505 documents ( www9.yahoo. com ), to 2790 documents ( swax32.-swmed.edu ), with an average size of 5602 documents per server. For sam-pling queries, we used the 1000 most frequent terms in the Excite search engine query logs collected in 1997 ( Spink et al., 2001 ).

For each query, we download the top 10 answers; this is the number of results that most search interfaces return on the first page of results. Sampling stops after 300 unique documents have been downloaded or 1000 queries have been sent (whichever comes first). Although using fixed-size samples might not always be the opti-mal method ( Shokouhi et al., 2006 ), we restrict ourselves to 300 documents to ensure that our results are com-parable to the widely accepted baseline ( Callan et al., 1999 ).

For each server we gather two samples: one by query-based sampling, and the other by our query log method. For query log ( QL ) experiments, each of the 1000 most frequent terms in the Excite query logs are passed as a probe query to the collection, and the top 10 returned answers are collected. For are selected from the current downloaded documents at each time, and the top 10 results of each query are gathered.
 To evaluate the effectiveness of samples for different queries, we used topics 451 X 550 from the
TREC 2001 Web Tracks. We used only terms in the title field as queries. Since we are extracting only the largest 100 servers from WT 10 g, the number of available relevant documents is low, so the precision-recall metrics produce poor results. For this reason, many DIR experiments use the set of documents that are retrieved by a central server as an oracle. That is, all of the top-ranked pages returned by the central index are considered to be relevant, and the performance of DIR ing the documents of all 100 servers 4 as a benchmark.

For both the baseline and DIR experiments, we gathered the top 10 results for each query. Results for 100 and 1000 answers per query were found to be similar and are not presented here. We tested different cutoff (CO) points in our evaluations: for a cutoff of 1, the queries were passed to the one server with the most similar corresponding representation set; for a cutoff of 50, queries were sent to the top 50 servers. Table 1 shows that the QL method consistently produces better results. Differences that are statistically significant based on the t -test at the 0.05 and 0.01 level of significance are indicted by the and , respectively. For mean average pre-cision (MAP), which is considered to be the most reliable evaluation metric ( Sanderson &amp; Zobel, 2005 ), outperforms QBS significantly in four of five cases.

We made two key observations. First, query log ( QL ) terms did not retrieve the expected 300 documents for four servers after 1000 queries, while QBS failed to retrieve this number from only one server. Analysis showed that these servers contain documents unlikely to be of general interest to users. For example www.two-birds.com has error pages and HTML forms while www.snweb.com includes many pages with non-text characters.

Second, the QL method downloads an average of 2.43 unseen documents per query, while the corresponding average for QBS is 2.80.

Having access to the term document frequency information of any collection, it is possible to calculate the expected number of answers from the collection, for single-term queries extracted randomly from its index.
Therefore, we indexed all of the servers together as a global collection. At most 10 answers are retrieved per query. The expected number of answers per query can be calculated as which gives an expected value of 2.60, close to numbers obtained by both the However, these values contrast with those reported by Ipeirotis and Gravano (2002) , who claim that downloads an average of only one unseen document per two queries. On further investigation, we observed that the average varies for different collections, as shown in Table 2 . The first collection is extracted from TREC AP newswire data and contains newspaper articles. Collections labelled WEB are subsets of the 10 g collection ( Bailey et al., 2003 ). Finally, GOV-1 is a subset of the
Hawking, 2002 ). Note that the average values for QL are between 4.9 and 7.4 unseen documents per query, while for QBS these range from 1.2 to 4.5. In general, the gap between methods is more significant for larger collections with broad topics. Each QL probe query returns about 10 answers  X  the maximum  X  on the first page, while this number is considerably lower for QBS . 3. Pruning using query logs
In uncooperative DIR systems, the broker keeps a representation sample for each collection ( Callan &amp; Con-nell, 2001; Craswell et al., 2000; Shokouhi et al., 2006 ). These samples usually contain a small number of doc-uments downloaded by query-based sampling ( Callan et al., 1999 ) from the corresponding collections.
Pruning is the process of excluding unimportant terms (or unimportant term occurrences) from the index to reduce storage cost and, by making better use of memory, increase querying speed. The importance of a term can be calculated according to factors such as lexicon statistics ( Carmel et al., 2001 ) or position in the document ( Craswell et al., 1999 ). The major drawback with current pruning strategies is that they decrease precision, because the pruned index can miss terms that occur in user queries. In addition, in lexicon-based pruning strategies, the indexing process is slowed significantly. First, documents need to be parsed, so that term distribution statistics are available. Then, unimportant terms can be identified, and excluded, based on the lexicon statistics. For example, terms that occur in a large proportion of documents might be treated as stopwords. Finally, the index needs to be updated based on the new pruned vocabulary statistics.
 Lexicon-based pruning strategies face additional problems when dealing with broker indexes in uments are gathered from different collections, with different vocabulary statistics. A term that appears unim-portant in one collection based on its occurrences might in fact be critical in another collection. Therefore, pruning the broker X  X  index based on the global lexicon statistics does not seem reasonable.

We introduce a new pruning method that addresses these problems. Our method prunes during parsing, and is therefore faster than lexicon-based methods, as index updates are not required. Unlike other approaches, our proposed method does not harm precision, and can increase retrieval performance in some cases. Note, however, that we regard this pruning strategy as an illustration of the power of query logs rather than a method that should be deployed in practice: users for search for a term should be able to find matches if they are present in the collection. Although sampling inevitably involves some loss, that loss should be mini-mised. That said, as our experiments show the new pruning method is both effective and efficient. 3.1. Related work Pruning is widely used for efficiency, either to increase query processing speed ( Persin, Zobel, &amp; Sacks-
Davis, 1996 ), or to save disk storage space ( Carmel et al., 2001; Craswell et al., 1999; de Moura et al., 2005; Lu &amp; Callan, 2002 ).

Carmel et al. (2001) proposed a pruning strategy where each indexed term is sent in turn as a query to their search system. Index information is discarded for those documents that contain the query term, but do not appear in the top ranked results in response to the query. This strategy is computationally expensive and time consuming. The soundness of this approach is unclear; the highly ranked pages for many queries are not highly ranked for any of the individual query terms, de Moura et al. (2005) have extended Carmel X  X  method.
The apply Carmel X  X  method to extract the most important terms of each collection. Then they keep only those sentences that contain important terms, and delete the rest. For the same reason discussed previously, this approach also is not applicable in uncooperative DIR environments. Although this approach is more effective than Carmel X  X  method in most cases, the loss in average precision compared to a full-text baseline is significant.

D X  X ouza, Thorn, and Zobel (2004) discuss surrogate methods for pruning, where only the most significant words are kept for each document. In this approach, the representation set is not the collection X  X  vocabulary, but is instead a complete index for the surrogates. Such an approach requires a high level of cooperation between servers.

Craswell et al. (1999) use a pruning strategy to merge the results gathered from multiple search engines. In their work, they download the first four kilobytes of each returned document instead of extracting the whole document for result merging. They showed that in some cases, the loss in performance is negligible. They only evaluated their method for result merging.

In a comprehensive analysis of pruning in brokers, Lu and Callan (2002) divided pruning methods into var-ious groups: frequency-based methods prune documents according to lexicon statistics; location-based methods exclude terms based on the position of their appearance in the documents; single-occurrence methods set a pruning threshold based on the number of unique terms in documents, and keep one instance of each term in the document; and multiple-occurrence methods allow for multiple occurrences of terms in pruned docu-ments. Experiments evaluating the performance of nine methods demonstrated that four models can achieve similar optimal levels of performance, and do not have any significant advantage over each other.
Of these best methods, FIRSTM is the only one which does not rely on a broker X  X  vocabulary statistics. For each document, this approach stores information about the first 1600 terms. The other methods measure the importance of terms based on frequency information. As discussed, these methods are unsuitable for many ways: the frequency of a term in the broker does not indicate its importance in the original collections; the cost of pruning and re-indexing might be high; and, adding a new collection makes the current pruned index unusable, since after a new collection is added to the system, the previous information is no longer valid.
The FIRSTM approach prunes during parsing, which makes it more comparable with our approach. Therefore, we evaluate our approach by using FIRSTM as a baseline.
 Lu and Callan tested their methods on 100 collections created from their models can reduce storage costs by 54% X 93%, with less than a 10% loss in precision. We test our systems on WT 10 g and GOV 1, which are larger and consist of unmanaged data crawled from the Web; these collections are described in more detail in Section 3.2 .

Our proposed pruning method is applied during parsing, and is independent of index updates, as the addi-tion of new collections to the system does not require the re-indexing of the original documents. Moreover, our pruning method does not reduce system performance and precision, while in all of the discussed previous decrease in precision. 3.2. Using query logs for pruning
The main motivation for pruning is to omit unimportant terms from the index. That is, pruning methods are intended to exclude the terms that are less likely to appear in user queries ( de Moura et al., 2005 ). Some methods prune terms that are rare in documents ( Lu &amp; Callan, 2002 ). However, the distribution of terms in user queries is not similar to that in typical web documents.

We propose using the history of previous user queries to achieve this directly. Our hypothesis is that prun-ing those terms that do not appear in a search engine query logs will be able to reduce index sizes while main-taining retrieval performance. We test our hypothesis with experiments on distributed environments and central indexes for different types of queries. In a standard search environment, where completeness may be more important than improvements in efficiency, such pruning (or any pruning) is unappealing; but in a dis-tributed environment, where index information is incomplete and is difficult to gather, such an approach has significant promise.

For our experiments, we used a list of the 315936 unique terms in the log of about one million queries submitted to the Excite search engine on 16 September 1997 ( Spink et al., 2001 ). Larger query logs, or a com-bination of query logs from different search engines, might be useful for larger collections. Also, for highly topic-specific collections, topical query logs ( Beitzel, Jensen, Chowdhury, Grossman, &amp; Frieder, 2004 )and additional benefits.

For experiments on uncooperative DIR environments and brokers, we used the testbed described in Section 2.3 . The 100 largest servers were extracted from the TREC WT as a separate collection. Query-based sampling, as described in Section 2.1 , was used to obtain representation sets for each collection by downloading 300 documents from each server in our testbed. We do not omit stop-words in any of our experiments. For each downloaded sample, we only retained information about those terms that were present in our query log, and eliminated the other terms from the broker. We used ( Callan et al., 1995 ) for collection selection and result merging; line ( Craswell et al., 2000; Nottelmann &amp; Fuhr, 2003; Powell &amp; French, 2003; Si &amp; Callan, 2003 ).
TREC topics 451 X 550 and their corresponding relevance judgements were used to evaluate the effectiveness of our pruned representation sets. We use only the  X  title  X  field of To test our pruning method on central indexes, we used the collection ( Craswell &amp; Hawking, 2002 ) contains over a million documents crawled from the .gov domain.
TREC topics 451 X 550 were used for our experiments on WT 10 g, and topics 551 X 600 and NP01 X  X P150 were used with the GOV 1 collection. All experiments with central indexes use the OkapiBM25 similarity measure ( Robertson, Walker, Hancock-Beaulieu, Gull, &amp; Lau, 1992 ).

In addition to these topic-finding search tasks, we evaluate our pruning approach on central indexes for related to a general topic ( Craswell, Hawking, Wilkinson, &amp; Wu, 2003 ). We use the ics 551 X 600, and corresponding relevance judgements, with the known as homepage finding) the aim is to find particular web pages of named individuals or organisations. To evaluate this type of search task, we used the TREC named-page finding queries NP01-NP150 ( Craswell &amp;
Hawking, 2002 ). 3.3. Distributed retrieval results The results of our experiments using different pruning methods for each scheme, up to 1000 answers were returned per query. The cutoff (CO) values show the number of collec-last row, the top 50 collections  X  half of the available collections -are selected. Our pruning method ( original un-pruned index ( ORIG ), produce the same performance for cutoff values of 1 and 10 For larger cutoff values, the broker with pruned documents is able to select better collections, and outperforms the system that uses the original documents. The PR approach consistently outperforms the confidence are indicated with * and , respectively.

Overall, performance based on MAP is low because there are few relevant answers for the queries in the testbed. To measure system performance when all documents are available, we indexed all documents from all 100 servers using a central index. Results for all 100 collections, searched using a central index, are shown in Table 4 . 3.4. Central index pruning results
The experiments reported in the previous section were undertaken in an uncooperative distributed environ-ment. In this section, we evaluate our pruning method on central indexes, considering different search tasks. We evaluate the performance of five different pruning approaches.

Under the FIRSTM approach, only information about the first 1600 terms in each document is added to the index. For our PR scheme, we only store index information about terms that occur in our query log. We also test the effect of using fewer query log terms on performance. The occurred two or more times in the query log; all the other terms that occur in documents are pruned. This reduces the size of our query log from 315936 unique terms to 108425 terms.

We also investigate two hybrid models. The FIRSTM -PR scheme only retains information about terms that are present in the query log and also occur in the first 1600 words of each document.
 information further, adding an additional constraint whereby terms need to occur two or more times in the query log to be indexed.
 Table 5 shows the effectiveness of the various schemes for a topic-finding task on the original, full-text index is labelled ORIG . The performance of the index. The t -test does not show any significant difference between the performance of the original index, and PR (tf) at the 0.1 or 0.05 levels. The FIRSTM scheme harms performance compared to the full-text index. This reduction is significant at the 0.05 level for the MAP, P@5 and R -precision metrics. The hybrid models, approaches are able to reduce index size further, as is discussed below.

The outcomes of using the five pruning approaches for topic distillation queries are shown in Table 6 . All schemes are able to maintain retrieval performance compared to the full-text index (tf) schemes are able to increase performance above the baseline. However, this increase is not statistically significant.

The results for the named-page search task are shown in Table 7 . Many of these queries contain out-of-vocabulary terms, which are not present in the vocabulary of the index. Since our pruning strategy excludes any term which is not available in the query log, we were expecting to see a significant loss in performance. However, the t -test showed that none of the differences between the original index ( age, all systems are able to return the correct answer as the second document in the results list. Because the out-of-vocabulary problem is more prevalent for the query log schemes, the proportion of queries where no correct answer is found in the first 1000 items is slightly higher than for the other schemes.
As explained in Section 3.2 , we use the Okapi BM25 similarity function ( Robertson et al., 1992 ) for our central index effectiveness experiments. To check that the trends in our results are not an artifact of a partic-ular similarity measure, we also investigated the effect of pruning using three different ranking functions on each collection: tf  X  idf ( Salton &amp; McGill, 1986 ); INQUERY for brevity are not included here.

One of the key aims of index pruning is to reduce the amount of data that needs to be maintained, either in a central index, or in the document representation sets used by a broker in distributed IR. The effect of the various pruning schemes on the size of indexes is shown in Table 8 . For the ,and PR (tf) schemes all reduce the size of index by about 30%. The hybrid models, FIRSTM -PR (tf), are able to reduce index size by an additional 15%. As demonstrated by our effectiveness results, these schemes are differentiated by search performance, FIRSTM and PR (tf) do not alter it from the full-text baseline. Moreover, while the in similar levels of precision, the hybrid models reduce the index size by an additional 15% over fore, if search performance is to be optimised, the PR scheme is to be preferred. If efficiency is of greater con-cern, one of the hybrid models should be chosen.
 There is more variation among the efficiency gains for the various schemes when they are applied to the collection. Here, the PR and PR (tf) schemes show the least reduction in index size, reducing the space required by around 30%. The FIRSTM approach is able to give a reduction of 43%, while the hybrid models reduce the index size by about 57%. In terms of effectiveness, the schemes showed similar levels of accuracy for topic distillation and named-page finding tasks on the GOV 1 collection. For these search tasks we therefore recommend the use of the hybrid schemes, which minimise index size while retaining performance effectiveness.
 Efficiency results for our distributed testbed are shown in Table 9 . Here, size of 64%, which is substantially greater than the 22 X 28% reduction given by the ever, there is again a tradeoff with effectiveness, since the to the PR scheme. As in the central index case, the hybrid models are able to reduce index size slightly further 4. Conclusion
We have proposed two new applications of query logs for the improvement of distributed information retrieval systems. The first is a novel sampling approach for distributed collections in an uncooperative envi-ronment. Our approach focused the sampling process by using terms that are available in search engine query logs. Our experiments demonstrated that the method is no more costly than previous approaches to query-based sampling, but produces samples that allow retrieval to be significantly more effective.
The second application we propose is that query logs can be used to focus index pruning strategies towards terms that are important to users. Our pruning strategy is able to maintain system effectiveness compared to a full-text index, while being able to reduce index size by 22 X 28%. We evaluated our approach for various web search tasks, including topic distillation and named-page finding. Although many of these topics contain out-of-vocabulary terms, the pruned indexes retrieve relevant answers as effectively as the original index. We com-pared our strategies with the FIRSTM pruning approach, which can give additional savings in index size but often results in decreased search effectiveness. We also proposed two hybrid models that combine features of the FIRSTM and query-log approaches, and showed that such models can further reduce index size, while not reducing effectiveness below the FIRSTM -only scheme. For standard retrieval applications, such pruning may not be desirable, but in environments such as DIR where pruning may be essential such an approach is an effective choice.

While our results demonstrate that using query logs can be an effective mechanism to guide both collection sampling and index pruning, it is well-known that query topics shift over time. As future work, we plan to simulate and evaluate the effects of these changing patterns on the effectiveness of our schemes. We also plan to investigate the effectiveness of our approaches on larger collections, such as the
Finally, we plan to investigate the effects of combining query logs from different search engines, and whether this can further enhance the robustness of our techniques. However, our results already clearly demonstrate that query logs provide a robust basis for DIR in uncooperative environments, with significant improvements in effectiveness compared to previous methods.
 References
