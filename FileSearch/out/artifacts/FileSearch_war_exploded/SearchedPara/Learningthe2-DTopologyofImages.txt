 Machine learning has been applied to a number of tasks involving an input domain with a spe-cial topology: one-dimensional for sequences, two-dimensional for images, three-dimensional for videos and for 3-D capture. Some learning algorithms are generic, e.g., working on arbitrary un-structured vectors in d , such as ordinary SVMs, decision trees, neural netw orks, and boosting neural netw orks [6, 7], time-delay neural netw orks [5, 16].
 prior over all possible permutations.
 was probably incorrect. To answer that question we consider a hypothetical learning task involv-ing images whose pix els have been permuted in a fix ed but unkno wn way. Could we reco ver the alization? A related study performed in the conte xt of ICA can be found in [1]. The basic idea of the paper is that the two-dimensional topology of pix els can be reco vered by that nearby pix els have similar distrib utions of intensity (and possibly color) values. We explore a number of manifold techniques with this goal in mind, and explain how we have are most appropriate could probably be inferred from the data. images, i.e., with 2-dimensional structures, and our experiments have been performed with images of size 27 27 to 30 30 , i.e., with about a thousand pix els. It means that we have to look for the embedding of about a thousand points (the pix els) on a two-dimensional manifold. Metric Multi-Dimensional Scaling MDS is a linear embedding technique (analogous to PCA but starting from distances and yielding coordinates on the principal directions, of maximum variance). Non-parametric techniques such as Isomap [13 ], Local Linear Embedding (LLE) [12 ], or Semidefinite Embedding (SDE, also kno wn as MVU for Maximum Variance Unfolding) [17 ] have computation are feasible, and we experimented with MDS, Isomap, LLE, and MVU.
 Since we found Isomap to work best to reco ver the pix el topology even on small sets of images, we revie w the basic elements of Isomap. It applies the metric multidimensional scaling (MDS) algorithm to geodesic distances in the neighborhood graph . The neighborhood graph is obtained by connecting the k nearest neighbors of each point. Each arc of the graph is associated with a It first computes the dot-product (or Gram) n n matrix M using the  X  X ouble-centering X  formula, yielding entries M vectors v x between pix el intensities, and it works very well.
 The empirical correlation we assume them to be the value of a Gaussian kernel then by defining D that can be used with the manifold learning algorithms: resulting embedding, so it is unnecessary . Man y other measures of distance would probably work as well. Ho we ver, we found the absolute correlation to be simple and easy to understand while yielding nice embeddings. 3.1 Dealing With Lo w-V ariance Pixels to fold the manifold on itself.
 imum over all pix els). On the NORB dataset, which has varied backgrounds, this step does not remo ve any of the pix els (so it is unnecessary). form the data vectors back into images. For this purpose we have performed the follo wing two steps: by the connections in a graphical model, which is then trained by maximizing the approximate Let p vertical axis being in the orthogonal direction, and perform the appropriate rotation. Once we have a coordinate system that assigns a 2-dimensional position p placed at irre gular locations inside a rectangular grid, we can map the input intensities x intensities M and machine vision learning algorithms. The output image pix el intensity M is obtained through a con vex average where the weights are non-ne gati ve and sum to one, and are chosen as follo ws. with an exponential of the L where N ( i; j; k ) is true if k ( i; j ) p embedding is not perfect. Too small values result in a loss of effecti ve resolution. pro vided in an arbitrary but fix ed order .
 Input: X f Ra w input n N data matrix, one row per example, with elements in fix ed but arbitrary order g low-v ariance pix els g Input: k = 4 (def ault value) f Number of neighbors used to build Isomap neighborhood graph g Input: L = p N ; W = p N (def ault values) f Dimensions (length L , width W of output image) g Input: = 3 (def ault value) f Smoothing coef ficient to reco ver images g Output: p f N 2 matrix of embedding coordinates (one per row) for each input variable g Output: w f Con volution weights to reco ver an image from a raw input vector g n = number of examples (ro ws of X ) for all column X end for
Remo ve columns of X for which i for all column X end for f Compute the 2-D embeddings ( p k 1 ; p k 2 ) of each input variable k through Isomap g p = Isomap( D; k; 2) f Rotate the coordinates p to try to align them to a vertical-horizontal grid (see text) g f Invert the axes if L &lt; W g f Compute the con volution weights that will map raw values to output image pix el intensities g for all grid position ( i; j ) in output image ( i in 1 : : : L , j in 1 : : : W ) do end for embedding for each input variable.
 Input: x f Ra w input N -vector (in same format as a row of X abo ve) g Input: p f N 2 matrix of embedding coordinates (one per row) for each input variable g Input: w f Con volution weights to reco ver an image from a raw input vector g Output: Y f L W output image g for all grid position ( i; j ) in output image ( i in 1 : : : L , j in 1 : : : W ) do end for We performed experiments on two sets of images: MNIST digits dataset and NORB object classi-The MNIST images are particular in that the y have a white background, whereas the NORB images have more varying backgrounds. The NORB images are originally of dimension 108 108 ; we subsampled them by 4 4 averaging into 27 27 images. The experiments have been performed with k = 4 neighbors for the Isomap embedding. Smaller values of k often led to unconnected neighborhood graphs, which Isomap cannot deal with. Figure 1: Examples of embeddings disco vered by Isomap, LLE, MDS and MVU with 250 training produces coordinates with an arbitrary rotation. Isomap appears most rob ust, and MDS the worst method, for this task.
 In Figure 1 we compare four dif ferent manifold learning algorithms on the NORB images: Isomap, LLE, MDS and MVU. Figure 2 explains why Isomap is giving good results, especially in comparison with MDS. One the one hand, MDS is using the pseudo-distance defined in equation 1, whose neighborhood. On the other hand, Isomap uses the geodesic distances in the neighborhood graph, whose relationship with the real distance is really close to linear . Figure 2: (a) and (c): Pseudo-distance D (b) and (d): Geodesic distance in neighborhood graph vs. the true distance on the grid. The true distance is on the horizontal axis for all figures. (a) and (b) are for a point in the upper -left corner , (c) and (d) for a point in the center . Figure 3 sho ws the embeddings obtained on the NORB data using dif ferent numbers of examples. transformation that minimizes the Root of the Mean Squared Error (RMSE) between the coordinates and with 2000 or more a very good embedding is obtained: the RMSE for 2000 examples is 1 : 13 , meaning that in expectation, each pix el is off by slightly more than one. 10 examples 50 examples 100 examples 1000 examples 2000 examples on the original grid, third row sho ws the residual error (RMSE) after the alignment. into an embedded image and finally into a reconstructed image as per algorithms 1 and 2. Figure 4: Example of the process of transforming an MNIST image (top) from which pix el order pix el by the gre y level in a circle located at the pix el coordinates disco vered by Isomap. We also performed experiments with acoustic spectral data to see if the time-frequenc y topology second. The first 30 frequenc y bands are kept, each covering 21.51 Hz. We used examples formed samples from each recording yielded 2600 30-frames images, on which we applied our technique. Figure 5 sho ws the resulting embedding when we remo ved the 30 coordinates of lowest standard deviation ( = : 15 ). (either in a lifetime or through evolution), our results suggest otherwise. Ho w do we interpret that apparent contradiction? such a lar ge class of functions. There are approximately N = tion error and test error to be bounded [15 ] by 1 d = 400 (the number of pix els with non-ne gligible variance in MNIST images), d log d d 2000 . symmetries, rotations, and small errors in pix el placement.
 dependencies between the input random variables. If we are going to perform computations on sub-to reduce the amount of connecting hardw are), it would seem wiser that these computations com-notion that is exploited in con volutional neural netw orks.
 e.g., by ICA [9] or Products of Experts [3, 11].
 way in which we score permutations is not the way that one would score functions in an ordinary it is not surprising that it allo ws us to generalize from man y fewer examples. We pro ved here that, even with a small number of examples, we are able to reco ver almost per -algorithm performed well on sound data, even though the topology might be less obvious in that case.
 of data than the ones discussed abo ve.
 Ackno wledgements ackno wledge the support from several funding agencies: NSERC, the Canada Research Chairs, and the MIT ACS netw ork.

