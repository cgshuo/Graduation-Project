 As tensors provide a natural representation for the higher-order relations, tensor factorization techniques such as Tucker decomposition and CANDECOMP/PARAFAC decomposi-tion have been applied to many fields. Tucker decomposition has strong capacity of expression, but the time complexity is unpractical for the large-scale real problems. On the other hand, CANDECOMP/PARAFAC decomposition is linear in the feature dimensionality, but the assumption is so strong that it abandons some important information. Besides, both of TD and CP decompose a tensor into several factor ma-trices. However, the factor matrices are not natural for the representation of the higher-order relations. To overcome these problems, we propose a near linear tensor factoriza-tion approach, which decompose a tensor into factor tensors in order to model the higher-order relations, without loss of important information. In addition, to reduce the time complexity and the number of the parameters, we decom-pose each slice of the factor tensors into two smaller ma-trices. We conduct experiments on both synthetic datasets and real datasets. The experimental results on the synthetic datasets validate that our model has strong capacity of ex-pression. The results on the real datasets show that our approach outperforms the state-of-the-art tensor factoriza-tion methods.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Information filtering ; I.2.6 [ Artificial Intelligence ]: Learning Tensor decomposition; DTT  X  Corresponding author
Matrix factorization is used for modeling the second-order relations. For example, for the item recommendation prob-lem, we can construct a user-item matrix. The user-item matrix is factorized into two factor matrices, representing the latent features of the users and the items, respectively. From the users X  view, matrix factorization assumes the items are divided into D item-groups, where D denote the number of the elements in the vector. For user u , there is a corre-sponding vector of the factor matrix of the users, describing the relation between that user and the items. Each element of that vector represents the relevance between that user and a item-groups. From the items X  X iew, the situation is similar.
Tensor is a higher-order extension of matrix and tensor factorization is a higher-order extension of matrix factor-ization. Since tensor decomposition can model the higher-order relations, tensor decomposition techniques are applied to many fields, such are psychology, chemometrics, computer vision and data mining. Traditional tensor decomposition techniques like Tucker decomposition (TD) [21] and CAN-DECOMP/PARAFAC (CP) [2, 9] decomposition factorize a tensor into several factor matrices. Take the problem of personalized tag recommendation for example. Traditional tensor decomposition techniques decompose a user-item-tag tensor into three factor matrices. Just like matrix factor-ization, from the users X  view, for user u , there is a cor-responding vector of the factor matrix of the users, indi-cating the relation between that user, the items and the tag, but we can not explain the meaning of each element in that vector by the same way we analyzing matrix fac-torization, since vector is not natural for the representation of the relations between that user, the item-groups and the tag-groups. Moreover, both of Tucker decomposition and CANDECOMP/PARAFAC have some drawbacks. Tucker decomposition has strong capacity of expression but it re-quires lots of computation power, which is unpractical for large-scale real problems. The other tensor factorization technique CANDECOMP/PARAFAC decomposition is lin-ear in the feature dimensionality, but it makes a strong as-sumption that limits its capacity of expression.

To tackle all these problems, we propose a novel tensor fac-torization approach in this paper. We decompose a tensor into several factor tensors instead of factor matrices. From the users X  view, for user u , there is a corresponding matrix of the factor tensor of the users, indicating the relation be-tween that user, the items and the tag, and each element of that matrix can be seen as the relevance between that user, a item-group and a tag-group. Thus, the higher-order rela-tions can be modeled by a more natural way. In addition, to reduce the time complexity and the number of the parame-ters, we decompose each slice of the factor tensors into two smaller matrices. Our proposed model is near linear and has strong capacity of expression.

The contributions of our work are summarized as follows.
The rest of the paper is organized as follows. The related work to tensor factorization techniques is depicted in the next section. The notations and preliminaries of tensor and tensor factorization are presented in section 3. Our proposed tensor decomposition approach is described in section 4. The experimental results on the synthetic datasets and the real datasets are showed and analyzed in section 5. Finally, we conclude our work in section 6.
Since tensors provide a natural representation for the higher-order relations, they are applied to many fields [12], includ-ing chemometrics [1], neuroscience [13], graph analysis [18, 6, 5, 24], personalized recommendation [19, 24, 11, 14, 17, 7, 20, 16, 23] and so on. Tucker decomposition (TD) [21] and CANDECOMP/PARAFAC decomposition (CP) [2, 9] are the most common techniques to decompose a tensor, where Tucker decomposition has stronger capacity of expres-sion and CANDECOMP/PARAFAC decomposition can be trained in linear time.

Some researches are based on Tucker decomposition (TD) [3, 4, 19, 11, 14]. Higher-Order Singular Value Decompo-sition (HOSVD) [3, 4], a generation of the matrix Singular Value Decomposition (SVD), is introduced for computing Tucker decomposition. Sun et al. [19] construct a user-query-URL tensor from the clickthrough data in the search engines and employ HOSVD to capture the latent factors of users, queries and URLs for personalized web search. Karat-zoglou et al. [11] integrate the context information into the traditional collaborative filtering models to improve the rec-ommendation quality by making use of HOSVD as well. However, one of the drawbacks of HOSVD is that it can not deal with the missing data in the sparse tensors. It treats the values of all the missing elements in the tensors as zeros. To fix this problem, Rendle et al. [14] propose a method called RTF, which exploits Bayesian Personalized Ranking (BPR) [15] and learns from pairwise constraints, for personalized tag recommendation. Although TD has a strong capacity of expression for the higher-order relations, the time complexity of which is not feasible for large-scale real problems.

On the other hand, some studies employ CP decomposi-tion (CANDECOMP/PARAFAC decomposition) [5, 23, 17]. Dunlavy, Kolda, and Acar [5] incorporate the time informa-tion for link predictions. The first two dimensions of the tensor represent the entities and the third dimension rep-resents the time slices. They decompose the tensor by CP. Similarly, Xiong et al. [23] take the time into consideration and add a special constraint on the time dimension. Work by Rendle and Schmidt-Thieme [17] puts forward a new tensor factorization technique, Pairwise Interaction Tensor Factor-ization (PITF) for personalized tag recommendation. PITF can be seen as a special case of CP, modeling the pairwise in-teractions between the three dimensions of the user-item-tag tensor. The CP-based models can be trained in linear time, but it abandons some important information and restricts the capacity of expression.

Furthermore, both TD and CP decompose a tensor into several factor matrices, but the factor matrices are not natu-ral for the representation of the higher-order relations, since matrices can only represent the second-order relations. There-fore, we propose a novel tensor factorization approach in this paper, which decomposes a tensor into several factor tensors instead. In this section, we first introduce tensor and the notations. Then, we depict two most common tensor factorization tech-niques, Tucker decomposition and CANDECOMP/PARAFAC, and analyze the advantages and the disadvantages of them.
A tensor is a multidimensional or multi-way array and the order of a tensor is the number of the dimensions or modes. Vectors are first-order tensors, matrices are second-order tensors and tensors of order three or higher are called higher-order tensors. Matrices have column and row, a col-umn and a row of which are called mode-1 vector and mode-2 vector respectively. Similar to matrices, the third-order ten-sors have column, row and tube and a tube is a mode-3 vec-tor. Since the constructions of the higher-order tensors are similar, we focus on the third-order tensors and the third-order tensors are called tensors for short in the following paper.

Matrices and tensors are denoted by boldface capital let-ters, e.g., X . Element ( i,j ) of a matrix X is denoted by x and element ( i,j,k ) of a tensor A is denoted by a ijk .
Just like matrix factorization, tensor decomposition tech-niques factorize a tensor into several components. Tucker de-composition (TD) [21] and CANDECOMP/PARAFAC de-composition (CP) [2, 9] are the most common tensor decom-position techniques and can be seen as higher-order genera-tions of the matrix factorization, Singular Value Decompo-sition (SVD) [8].

Tensor decomposition techniques have been applied to personalized tag recommendation. For the sake of conve-nience of analysis, let X  X  take the personalized tag recom-mendation problem for example in the following paper. A user-item-tag tensor A  X  R I  X  J  X  K is constructed from the social tagging data, where I , J , K denote the number of the users, the items and the tags, respectively. Element a ijk tensor A measures the probability of the i -th user annotat-ing the j -th item with the k -th tag. We treat all of the users, the items and the tags as three different types of entities. Tucker decomposition (TD) [21] was first introduced by Tucker in 1966. It decomposes a tensor into a core tensor multiplied by a matrix along each mode. Element a tensor A  X  R I  X  J  X  K can be written as and D 3 are the numbers of the latent features. Given a factor matrix, a vector of that matrix represents the latent features of the corresponding entity. For user i , item j and tag k , c pqr of the core tensor C indicates the relevance between the p -th feature of user i , the q -th feature of item j and the r -th feature of tag k .

CANDECOMP/PARAFAC decomposition (CP) [2, 9] de-composes a tensor into a sum of component rank-one ten-sors. a i,j,k can be written as where X  X  R I  X  D , Y  X  R J  X  D , Z  X  R K  X  D are the factor matrices and D is the number of latent features. For user i , item j and tag k , CP makes a strong assumption that the p -th feature of user i , x ip is only relevant to the p -th feature of item j , y jp and the p -th feature of tag k , z kp . In other words, it assumes x ip is independent of y jq and z kr , where p 6 = q and p 6 = r . As generally D &lt;&lt; I,J,K on real problems, the assumption limits CP X  X  capacity of expression.

TD is more flexible and can model the higher-order re-lations better, but it requires lots of computation power. Therefore, TD is not practical for large-scale real problems. On the other hand, CP is linear in the feature dimension-ality, but it gives up some important information. To over-come the drawbacks, we propose a near linear tensor decom-position technique, which is practical for large-scale prob-lems, without lost of information.
In this section, we explain our proposed tensor factoriza-tion approach and how to compute our model. Besides, we compare our proposed approach with other tensor factoriza-tion techniques on the time complexity, the space complexity and the capacity of expression.
Singular Value Decomposition (SVD) factorize a matrix into two smaller factor matrices and the factor matrices present the the second-order relations. Tensor factorization is a higher-order generation of matrix factorization. Tra-ditional tensor decomposition techniques like TD and CP, decomposing a tensor A  X  R I  X  J  X  K into factor matrices, but the factor matrices are not proper for presenting the higher-order relations. Different from TD and CP, we de-compose the tensor A into factor tensors, since the represen-tation of a factor tensor is more natural for the higher-order relations, compared with the representation of a factor ma-trix. We refer our proposed tensor factorization approach to decomposing a tensor into tensors (DTT). The framework of our proposed tensor factorization model is illustrated in Figure 1. From the figure, we can see that tensor A is fac-torized into three smaller factor tensors X  X  R I  X  D D 3 are parameters of the DTT model (e.g., taking person-alized social tagging as an example, they can been seen as the numbers of the user groups, the item groups and the tag groups, respectively).

Since we decompose a tensor into factor tensors rather than factor matrices, the latent features of an entity are represented by a matrix rather than a vector. D 2  X  D 3 is the size of the factor matrix of a user, D 3  X  D 1 is the size of the factor matrix of an item and D 1  X  D 2 is the size of the factor matrix of a tag. From the users X  view, the factor matrix of user i , the i-th slice in X , which is denoted by X i :: , contains D 2 rows and D 3 columns. It can be assumed that for user i , all the items are divided into D 2 item-groups and the q -th row of X i :: represents the characters of the q -th item-group. Similarly, for user i , all the tags are divided into D 3 tag-groups and the r -th column of X i :: represents the characters of the r -th tag-group. Thus, x iqr means the relevance between user i , the items in the q -th item-group and the tags in the r -th tag-group. That is to say, matrix X i :: indicates the preference of user i under different condi-tions. The analysis from the views of the items and the tags is similar.

Our proposed approach DTT models the higher-order re-lations by modeling the relations between the user-groups, the item-groups and the tag-groups. For triplet ( i,j,k ), the way how we predict the value of that triplet is illustrated in Figure 2, where ug , ig and tg are short for the user-group, the item-group and the tag-group.
Note that Eq. (3) is equivalent to are the factor tensors. As the order of x iqr , y jrp and z has no influence on computing a ijk in Eq. (4), the order of the factor matrices of user i , item j and tag k in Figure 2 has no influence on computing the relevance score, which is a good property as follows.
 Property 1. The prediction of tensor element a ijk is well
Although DTT can capture the higher-order relations, the time complexity to calculate Eq. (4) is O ( D 1 D 2 D 3 ), which is unacceptable for the large-scale real problems. Besides, there are too many parameters in DTT that it may lead to overfitting, the space complexity of which is O ( ID 2 JD 3 D 1 + KD 1 D 2 ), where I , J and K denote the numbers of the users, the items and the tags, repectively. Note that, for each entity, there is a factor matrix describe the features of that entity. To reduce the number of parameters and the time complexity, we decompose the factor matrix of each en-tity into two smaller matrices. For user i , the factor matrix X X 3 ) O ( D 3 I ) and d 3 are the numbers of latent features of the factor ma-trices X i :: , Y j :: and Z k :: , respectively. Additionally, d and d 3 should satisfy d 1  X  min ( D 2 ,D 3 ), d 2  X  min ( D and d 3  X  min ( D 1 ,D 2 ). d 1 , d 2 and d 3 depend on the com-plexity of the pairwise relations. For example, if the relation between the items and the tags is complex for the users, d should be large so as to reconstruct the factor matrices of the users and if the relation between the items and the tags is simple, d 1 is small. Concretely, from the users X  view, for user i , matrix X ( l ) i :: indicates the relations between user i and the item-groups and matrix X ( r ) i :: indicates the relations between user i and the tag-groups. Thus, the multiplication i , the item-groups and the tag-groups.

After decomposing each factor matrix into two smaller matrices, a ijk can be rewritten as a where Note that, Eq. (8) is equivalent to where we define Eq. (9) can be calculated in O ( D 1 d 2 d 3 + D 2 d 3 d 1 and generally, d 1 ,d 2 ,d 3 &lt;&lt; D 1 ,D 2 ,D 3 . Therefore, DTT is feasible for large-scale real problems. Besides, the number of parameters is reduced from O ( ID 2 D 3 + JD 3 D 1 + KD 1 to O ( Id 1 ( D 2 + D 3 ) + Jd 2 ( D 3 + D 1 ) + Kd 3 ( D
Tensor decomposition techniques are employed to many applications. As for different applications, the objective function are different, let X  X  assume the objective function is F . Gradient-base approaches are common to learn the parameters of the models and we use gradient-based ap-proaches to optimize the objective function F . The gra-dients are where we define Thus, given a ijk in A , the gradients can be calculated in optimize the objective function F . For the simplicity of the analysis, we assume D 1 = D 2 = D 3 = D , d 1 = d 2 = d 3 = d and I = J = K , where I , J and K denote the number of the users, the items and the tags. Let N denote the number of the triplets in the training set and T denote the number of iterations for training. The comparison of the time complexity and space complexity is summarized in Table 1.

For TD, it makes the assumption that the users, the items and the tags are divided into several groups and uses a core tensor to model the higher-order relations between the user-groups, the item-groups and the tag-groups. Given user i , item j , tag k , the higher-order relation between them is or-ganized into a relation tensor. The framework of Tucker decomposition is illustrated in Figure 3(a). The three vec-tors on the left of the arrow describe the factors of the user, the item and the tag, respectively. The core tensor and the three vectors are integrated into a relation tensor, which is on the right of the tensor. Although Tucker decomposition is flexible and expressive, it takes O ( D 3 NT ) to train the data and O ( D 3 ) to predict a triplet. Besides, since the core tensor is shared by all the triplets, it is not suitable to apply parallel computing to training the data. Therefore, TD is unpractical on large-scale datasets.

On the other hand, it takes O ( DNT ) to trained a CP-based model and O ( D ) to predict a triplet. Thus, CP is efficient. Moreover, CP divides the users, the items and the tags into several groups as well. However, it assumes the users in the p -th user-group are independent of the items in the q -th item-group and the tags in the r -th tag-group, where p 6 = q and p 6 = r . In fact, D &lt;&lt; I when solving real problems and it is possible that a user in the first user-group might annotate an item in the first item-group with a tag in the first tag-group and annotate another item in the second item-group with another tag in the third tag-group. Therefore, The assumption is too strong that it reduce the capacity of expression.

For user i , item j , tag k , DTT exploits a special mul-tiplication x iqr y jrp z kpq to model the higher-order relation between them. Just like TD, it organizes the relation into a relation tensor as well, the framework of which is showed in Figure 3(b). The three matrices on the left of the arrow are the factor matrices of the user, the item and the tag and the factor matrices are integrated into a relation ten-sor, which is on the right of the arrow. Since DTT doesn X  X  make the strong independent assumption like CP, it is more expressive. Moreover, DTT requires O ( Dd 2 NT ) to train and O ( Dd 2 ) to predict, where d &lt;&lt; D . Thus, DTT is near linear. According to the experimental results on the real datasets, d = 1 is enough to capture the higher-order rela-tions. Thus, DTT is efficient for large-scale problems. Fur-thermore, compared with TD and CP, DTT exploits a more natural way to represent the features of an entity, by using a matrix to describe the features of the entity rather than a vector.
In this section, we compare our proposed model with other tensor factorization techniques on the synthetic datasets and the real datasets. The experiments on the synthetic datasets are used to compare the capacity of expression and the ex-periments on the real datasets are used to compare the ac-curacy for predicting a triplet of a tensor.
As tensor decomposition techniques are applied to per-sonalized tag recommendation, we use real tag annotation datasets to evaluate the performance of DTT. In addition, we generate synthetic datasets to compare different factor-ization models X  capacity of expression.

Let A denote the tensor constructed from the training set and P denote the observed user-item posts in the training the positive tags and the set of the negative tags for that post, respectively. We construct a candidate set for each post ( u,i ). If triplet ( u,i 0 ,t ) is observed for some item i triplet ( u 0 ,i,t ) is observed for some user u 0 , t is treated as the candidate tag for post ( u,i ). For tag t in the candidate set of post ( u,i ), if triplet ( u,i,t ) is observed, t is treated as the positive tag for post ( u,i ), otherwise, the negative tag. Following the work of [14], we optimize the pair-wise rank-ing function Bayesian Personalized Ranking (BPR) [15] for personalized tag recommendation. The objective function F is defined as where  X  is the weight of the regularization term,  X  are the model parameters and  X  denotes the mean of the parame-ters. We use Stochastic Gradient Descent (SGD) to optimize the objective function F .
We compare our approach with three categories of base-lines, CP-based, TD-based and popularity-based. PITF [17], CP are CP-based, RTF [14] and HOSVD [3] are TD-based. For popularity-based, given a post ( u,i ), method popularity ranks the tags based on the frequency item i and the tags co-occur in the training set.

For convenience, we set D 1 = D 2 = D 3 = D and d 1 = d 2 = d 3 = d . We performed all the experiments multi-ple times and tuned the hyper-parameters of all models to achieve the best performance on the first training split for each dataset. Most of the parameters of the tensor factoriza-tion models are drawn from normal distribution N (  X , 0 . 01 For all the parameters in DTT,  X  = 1  X  parameters in PITF,  X  = 0 . 0. For all the parameters in RTF,  X  = 1 D and the parameters of the core tensor in RTF are drawn from normal distribution N (0 , 0 . 1). Besides, the weight of the regularization term  X  = 0 . 0001 on the syn-thetic datasets and  X  = 0 . 002 on the real datasets for DTT and CP,  X  = 10  X  6 on all the datasets for RTF, and  X  = 0 . 001 on synthetic datasets and  X  = 0 . 01 on the real datasets for PITF.

We exploit two widely used metrics in information re-trieval, Normalized Discounted Cumulative Gain (NDCG) and Mean Average Precision (MAP), to evaluate the perfor-mance of different models.

Discounted Cumulative Gain (DCG) evaluates the gain of a recommendation list based on the positions of the tags in that list. The NDCG accumulated at a particular position p is defined as where rel ( i ) is 1 if the tag at position i i relevant and 0, oth-erwise. Normalized Discounted Cumulated Gain (NDCG) is defined as where IDCG (Ideal Discounted Cumulated Gain) is the DCG of the ideal ranked list, which is introduced to normalize the DCG.

Mean Average Precision (MAP) computes the mean of average precision (AP) over all posts in the test set, where AP is the average of precisions computed at all positions with a preferred tag and defined as where prec ( i ) is the precision of the cutoff rank list from the first tag to the i -th tag and N is the size of the corresponding candidate set.
To evaluate different tensor factorization models X  capacity of expression, we generate 10 synthetic datasets. There are 100 users, 100 items and 100 tags in each synthetic dataset. For each dataset, We first randomly divided the users into 10 user-groups, divide the items into 10 item-groups and di-vide the tags into 10 tag-groups. We assume the users in the i -th user-group always annotate the items in the j -th item-groups with the tags in the g ( i,j )-th tag-group, where g ( i,j ) is randomly generated in the range 1 -10. Thus, to sam-ple a triplet, we first randomly sample a user and an item, and then sample a tag in the g ( i,j )-th tag group, where i , j denote the IDs of the user-group that user belongs to and the item-group that item belongs to, respectively. For each dataset, we randomly sample 6000 triplets and split the triplets into a training set with 5000 triplets and a test set with 1000 triplets. By this way, we can compare differ-ent tensor factorization techniques X  capacity of expression and verify whether the independent assumption made by CP limits its capacity.
The experiments are repeated 10 times. We compare DTT to other tensor factorization models and for all tensor fac-torization models, D = 2, and for DTT d = 1. The results on the synthetic datasets are showed in Figure 4. HOSVD works poorly, since it doesn X  X  handle the missing value in the tensors. As the other TD-based model, RTF, exploits the pair-wise ranking function Bayesian Personalized Rank-ing (BPR) [15], the performance is much better. Moreover, as we expected, DTT and RTF outperforms the CP-based models, PITF and CP, because the independent assumption constraints CP-based models X  capacity. CP-based models assumes the users in the i -th user group will only annotate the items in the i -th item groups and annotate the items with the tags in the i -th tag group, which is not always the truth. Thus, DTT and TD-based models has stronger ca-pacity of expression than CP-based models. Besides, DTT and TD-based models like RTF model the higher-order re-lation by a relation tensor, but it seems the method DTT exploiting to construct the relation tensor is better.
In addition to the synthetic datasets, we evaluate the performance of DTT on real datasets Delicoius 1 , Last.fm Movielens 3 and Delicious-large 4 [22]. The first three smaller datasets come from the 2-nd International Workshop on In-formation Heterogeneity and Fusion in Recommender Sys-tems (HetRec 2011). The larger dataset Delicious-large [22] contains the complete bookmarking activity for almost 2 million users from the launch of the social bookmarking web-site in 2003 to the end of March 2011. For each real dataset, we removed the infrequent users, items and tags by using a core-based approach [10]. For the smaller datasets, we per-form the removal until every user, item and tag occurred in at least 5 triplets. For Delicious-large , as the original dataset is too large, we first randomly sample 50% of the users, the items and the tags, and then perform the core-based approach until every user, item and tag occurred in at least 10 triplets. The statistics of the datasets after re-moval are described in Table 2. Average Candidates means the average number of the candidate tags for each post in each dataset.
 We perform 10-fold cross-validation for the smaller datasets. The comparison between DTT, PITF, CP, HOSVD, RTF and popularity on the smaller real datasets are showed in Table 3. As TD-based models require a lot of computation power, we conducted experiments only on the smallest real dataset Movielens for TD-based models, HOSVD and RTF. Although TD-based models have stronger capacity of ex-pression than CP-based models, they work poorer than the CP-based models on the real datasets. Maybe the core ten-sor of TD is too complex that it leads to overfitting. Both of TD and DTT model the higher-order relations into relation tensors, but it seems DTT works much better. It can be seen http://www.delicious.com http://www.lastfm.com http://www.grouplens.org http://www.zubiaga.org/resources/socialbm0311 from the table that the MAP of DTT is significantly higher than that of the other approaches on all the real dataset and the NDCG@5 of DTT is significantly higher than that of the other approaches on the real datasets Last.fm and Movielens .

In the next experiment, we split Delicious-large into train-ing (90%) and test (10%). As PITF is the most competitive approach on the smaller datasets, we only compare DTT with PITF on Delicious-large . The results can be found in Table 4, which show that DTT outperform PITF on the larger dataset as well.

Finally, we investigate the influence of the number of fea-tures to DTT. The results with different number of features on the real datasets are showed in Figure 5. It can be seen from the figure that with the same D , the prediction quality of the models with d = 1 is comparable with the models with d = 2 on all the real datasets. d has no significant influence to the accuracy of the models on the real datasets. Thus, d = 1 is sufficient to capture the higher-order relations on the personalized tag recommendation datasets. As the time complexity of DTT to predict a triplet is O ( Dd 2 ) and at most of the time d = 1, the time complexity is reduced to O ( D ). Thus, DTT is near linear and is efficient for the large-scale real problems.
We propose a novel tensor factorization technique in this paper. Different from the traditional tensor factorization techniques which decompose a tensor into factor matrices, we decompose a tensor into factor tensors, since factor ten-sors can provide a more natural representation for the higher-order relations. Besides, we compare our proposed approach with two commonly used tensor factorization techniques, Tucker decomposition and CANDECOMP/PARAFAC. Our proposed approach overcome some of the drawbacks of Tucker decomposition and CANDECOMP/PARAFAC. It not only has strong capacity of expression, but also is near linear, which is feasible for the large-scale real problems. The ex-perimental results on the synthetic datasets validate that our approach has strong capacity of expression. The results on the real datasets show that our approach outperforms other tensor factorization techniques.

In the future, we plan to further reduce the time complex-ity and space complexity to compute our proposed approach, by investigating its special case.
 We would like to thank the many referees of the previous version of this paper for their extremely useful suggestions and comments. This work was supported by Huawei In-novation Research Program (HIRP) and National Science Foundation of China (61033010). [1] Carl J Appellof and ER Davidson. Strategies for [2] J.D. Carroll and J.J. Chang. Analysis of individual [3] Lieven De Lathauwer, Bart De Moor, and Joos [4] Lieven De Lathauwer, Bart De Moor, and Joos [5] Daniel M Dunlavy, Tamara G Kolda, and Evrim Acar. [6] Beyza Ermi  X s, Evrim Acar, and A Taylan Cemgil. Link [7] Dehong Gao, Renxian Zhang, Wenjie Li, and Yuexian [8] Gene H Golub and Christian Reinsch. Singular value [9] Richard A. Harshman. Foundations of the PARAFAC [10] Robert J  X  aschke, Leandro Marinho, Andreas Hotho, [11] Alexandros Karatzoglou, Xavier Amatriain, Linas [12] Tamara G Kolda and Brett W Bader. Tensor [13] Fumikazu Miwakeichi, Eduardo Mart X nez-Montes, [14] Steffen Rendle, Leandro Balby Marinho, Alexandros [15] Steffen Rendle, Christoph Freudenthaler, Zeno [16] Steffen Rendle, Christoph Freudenthaler, and Lars [17] Steffen Rendle and Lars Schmidt-Thieme. Pairwise [18] Stephan Spiegel, Jan Clausen, Sahin Albayrak, and [19] Jian-Tao Sun, Hua-Jun Zeng, Huan Liu, Yuchang Lu, [20] Panagiotis Symeonidis, Alexandros Nanopoulos, and [21] Ledyard R. Tucker. Some mathematical notes on [22] Robert Wetzker, Carsten Zimmermann, and Christian [23] Liang Xiong, Xi Chen, Tzu-Kuo Huang, Jeff G [24] Nan Zheng, Qiudan Li, Shengcai Liao, and Leiming
