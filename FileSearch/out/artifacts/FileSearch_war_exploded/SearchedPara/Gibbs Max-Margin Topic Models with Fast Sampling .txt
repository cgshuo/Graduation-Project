 Jun Zhu dcszj@mail.tsinghua.edu.cn Ning Chen ningchen@mail.tsinghua.edu.cn Hugh Perkins ngls11@mails.tsinghua.edu.cn Bo Zhang dcszb@mail.tsinghua.edu.cn As supervising information gets easier to obtain, de-veloping supervised latent topic models has attracted a lot of attentions. Both maximum likelihood estima-tion (MLE) and max-margin learning have been ap-plied to learn supervised topic models. Different from the MLE-based approaches ( Blei &amp; McAuliffe , 2007 ), which define a normalized likelihood model for re-sponse variables, max-margin supervised topic models, such as maximum entropy discrimination LDA (MedL-DA) ( Zhu et al. , 2009 ), directly minimize a margin-based loss derived from an expected prediction rule. Although max-margin supervised topic models have shown superior performance in various settings, such as text mining ( Zhu et al. , 2009 ) and image annota-tion ( Yang et al. , 2010 ), their learning problems are generally hard to solve. Existing methods rely on a variational approximation scheme with strict mean-field assumptions on posterior distributions, and they normally need to solve multiple latent SVM subprob-lems in an EM-type iterative procedure. The recent work ( Jiang et al. , 2012 ) developed Monte Carlo (M-C) methods for such max-margin topic models, with a weaker mean-field assumption; but they also need to solve multiple SVM problems. Thus, their efficiency could be limited as learning SVMs is normally compu-tationally demanding. Also, it is not easy to parallelize these algorithms for large-scale applications. This paper presents Gibbs MedLDA, a new formula-tion of max-margin supervised topic models with effi-cient inference algorithms. Instead of minimizing the margin loss of an expected prediction rule as adopted in MedLDA, Gibbs MedLDA minimizes the expect-ed margin loss of many latent prediction rules, each rule corresponding to a configuration of topic assign-ments and the prediction model, drawn from a post-data posterior distribution. Theoretically, the expect-ed margin loss is an upper bound of the existing margin loss of an expected prediction rule. Computationally, although the new margin loss can be hard in devel-oping variational algorithms, we can develop simple and fast collapsed Gibbs sampling algorithms without any restricting assumptions on the posterior distribu-tion, by exploiting the classical ideas of data augmen-tation ( Dempster et al. , 1977 ; Tanner &amp; Wong , 1987 ; van Dyk &amp; Meng , 2001 ) and its recent extensions to max-margin classifiers ( Polson &amp; Scott , 2011 ). We further generalize the ideas to develop a Gibbs MedL-DA regression model and its Gibbs sampling algorithm with data augmentation. Empirical results on real da-ta sets demonstrate significant improvements on time efficiency. The classification performance is also sig-nificantly improved.
 The paper is organized as follows. Sec 2 reviews MedL-DA and its EM-type algorithms. Sec 3 presents Gibbs MedLDA and its sampling algorithms for classification and regression. Sec 4 presents empirical results. Sec 5 concludes and discusses future directions. We consider binary classification with a labeled train-Y takes values from the output space Y = { X  1 , +1 } . MedLDA consists of two parts  X  an LDA model for describing input documents W = { w d } D d =1 , where w ument d , and an expected classifier for considering the supervising signal y = { y d } D d =1 . Below, we introduce each of them in turn.
 LDA : LDA is a hierarchical Bayesian model that posits each document as an admixture of K topics, where each topic k is a multinomial distribution over a V -word vocabulary. For document d , the generating process can be described as where Dir(  X  ) is a Dirichlet distribution; Mult(  X  ) is multinomial; and z the non-zero entry of z dn . The topics are random sam-ples drawn from a prior, e.g., k  X  Dir( ).
 Given a set of documents W , we let z d = { z dn } N d n =1 { z distribution p ( , Z , | W )  X  p 0 ( , Z , ) p ( W | Z , ). We can show that the posterior distribution by Bayes X  rule is the solution of an information theoretical optimization problem where KL( q || p ) is the Kullback-Leibler divergence, and P is the space of probability distributions. In fact, if we add the constant log p ( W ) to the objective, it is the minimization of KL( q ( , Z , )  X  p ( , Z , | W )). Expected Classi er : Given a training set D , an expected classifier chooses a posterior distribution q ( h |D ) over a hypothesis space H of classifiers such that the q -weighted (expected) classifier h q ( w ) = sign E q [ h ( w )] will have the smallest possible risk. MedLDA follows this principle to learn a posterior q ( , , Z , |D ) such that the expected classifier has the smallest possible risk, approximated by the training error R D ( q ) = discriminant function is defined as
F ( w ) = E q ( , z |D ) [ F ( , z ; w )] , F ( , z ; w ) = where  X  z is a vector with element  X  z k = 1 N 1), and I (  X  ) is an indicator function that equals to 1 if predicate holds otherwise 0. Note that the expect-ed classifier and the LDA likelihood are coupled via the latent topic assignments Z . The strong coupling makes it possible for MedLDA to learn a posterior dis-tribution that can describe the observed words well and make accurate predictions.
 Regularized Bayesian Inference : To integrate the above two components for hybrid learning, MedlDA regularizes the properties of the topic representations by imposing the following max-margin constraints derived from the classifier ( 2 ) to a standard LDA inference problem ( 1 ) where  X  (  X  1) is the cost of making a wrong prediction; and = {  X  d } D d =1 are non-negative slack variables for inseparable cases. Let L objective for doing standard Bayesian inference with the classifier and p 0 ( , , Z , ) = p 0 ( ) p 0 ( , Z , ). MedLDA solves the regularized Bayesian infer-ence ( Zhu et al. , 2011 ) problem where the margin constraints directly regularize the properties of the post-data distribution and c is the positive regularization parameter. Equivalently, MedLDA solves the unconstrained problem 2 where R ( q ) = loss that upper bounds the training error R D ( q ) of the expected classifier ( 2 ). Note that the factor 2 is included simply for convenience. 2.1. Existing Iterative Algorithms Since it is intractable to solve problem ( 4 ) or ( 5 ) directly, both variational and Monte Carlo method-s have been developed for approximate solutions. It can be shown that the variational method ( Zhu et al. , 2012 ) is a coordinate descent algorithm to solve prob-lem ( 5 ) with the fully-factorized assumption that q ( ,  X  , Z , ) = q ( )( while the Monte Carlo methods ( Jiang et al. , 2012 ) make a weaker assumption that q ( ,  X  , Z , ) = q ( ) q ( X  , Z , ). All these methods have a similar EM-type iterative procedure, which solves many latent SVM subproblems, as outlined below.
 Estimate q ( ): Given q ( , Z , ), this step solves When the prior p 0 ( ) is the commonly used standard normal, we have the optimum solution q ( ) = N ( ,I ), where = pliers. It can be shown that the dual problem of ( 6 ) is the dual of a standard binary linear SVM and we can solve it or its primal form efficiently using existing high-performance SVM learners. We denote the opti-mum solution of this problem by ( q  X  ( ) ,  X  ,  X  ,  X  ). Estimate q ( , Z , ): Given q ( ), this step solves Although we can solve this problem using Lagrangian methods, it would be hard to derive the dual objec-tive. An effective approximation strategy was used in ( Zhu et al. , 2012 ; Jiang et al. , 2012 ), which updates q ( , Z , ) for only one step with fixed at  X  . By fixing at  X  , we have the solution q ( , Z , )  X  p ( W , , Z , ) exp { (  X  )  X  term indicates the regularization effects due to the max-margin posterior constraints. For those data with non-zero Lagrange multipliers (i.e., support vectors), the second term will bias MedLDA towards a new pos-terior distribution that favors more discriminative rep-resentations on these  X  X ard X  data points. The Monte Carlo methods directly draw samples from the poste-rior distribution q ( , Z , ) or its collapsed form using Gibbs sampling to estimate E q [  X  z d ], the expectation-s required to learn q ( ). In contrast, the variational methods solve problem ( 7 ) using coordinate descent to estimate E q [  X  z d ] with a fully factorized assumption. Now, we present Gibbs max-margin topic models and their  X  X ugment-and-collapse X  sampling algorithms. 3.1. Learning with an Expected Margin Loss As stated above, MedLDA chooses the strategy to min-imize the hinge loss of an expected classifier. In learn-ing theory, an alternative approach to building classi-fiers with a posterior distribution of models is to mini-mize an expected loss, under the framework known as Gibbs classifiers (or stochastic classifiers) ( McAllester , 2003 ; Catoni , 2007 ; Germain et al. , 2009 ) with nice theoretical properties.
 For our case of inferring the distribution of latent topic assignments Z and the classification model , the expected margin loss is defined as follows. If we have drawn a sample of the topic assignments Z and the prediction model from a posterior distribution q ( , Z ), we can define the linear discriminant function F ( , z ; w ) =  X   X  z as before and make prediction using the latent Gibbs rule Let  X  d =  X   X  y d  X   X  z d . The hinge loss of the classifier is
R ( , Z ) = loss is Since R ( , Z )  X  ( q )  X  an upper bound of the expected training error of the Gibbs classifier ( 8 ). Thus, it is a good surrogate loss for training.
 Then, with the same goal as MedLDA to find a posterior distribution q ( , , Z , ) that on one hand describes the observed data and on the other hand predicts as well as possible on training data, we define Gibbs MedLDA as solving the new regularized Bayesian inference problem Comparing to MedLDA in problem ( 5 ), we have the following lemma by applying Jensen X  X  inequality. Lemma 1 The expected hinge loss R  X  is an upper bound of the hinge loss of the expected classi er ( 2 ): 3.2. Formulation with Data Augmentation If we directly solve problem ( 9 ), the expected hinge loss R  X  is hard to deal with because we do not have a closed-form of the expectation of the max function. Fortunately, we can develop a simple collapsed Gibbs sampling method based on a data augmentation for-mulation of the expected hinge-loss.
 Let  X  ( y d | z d , ) = exp { X  2 c max(0 , X  d ) } be the unnor-malized pseudo-likelihood of the response variable for document d . Then, problem ( 9 ) can be written as where  X  ( y | Z , ) = lem ( 10 ) with the constraint that q ( , , Z , )  X  P , we can get the normalized posterior distribution where  X  ( y , W ) is the normalization constant. Us-ing the ideas of data augmentation ( Tanner &amp; Wong , 1987 ; Polson &amp; Scott , 2011 ), we have Lemma 2 . Lemma 2 (Scale of Mixture) The unnormalized pseudo-likelihood can be expressed as Proof: Due to the fact that a max(0 ,x ) = max(0 ,ax ) if a  X  0, we have  X  2 c max(0 , X  d ) =  X  2 max(0 ,c X  d ). Then, we can follow the proof in ( Polson &amp; Scott , 2011 ) to get the results.
 Lemma 2 indicates that the posterior distribution of Gibbs MedLDA, q ( , , Z , ), can be expressed as the marginal of a higher dimensional distribution that includes the augmented variables . The complete posterior distribution is where the pseudo-joint distribution of y and is In fact, we can show that the complete posterior distribution is the solution of the data augmentation problem of Gibbs MedLDA which is again subject to the normalization constraint that q ( , , , Z , )  X  P . 3.3. Inference with Collapsed Gibbs Sampling Although we can do Gibbs sampling to infer the com-plete posterior distribution q ( , , , Z , ) and thus q ( , , Z , ) by ignoring , the mixing rate would be slow due to the large sample space. One way to effectively reduce the sample space and improve mix-ing rates is to integrate out the intermediate Dirichlet variables ( , ) and build a Markov chain whose equi-librium distribution is the resulting marginal distribu-tion q ( , , Z ). We propose to use collapsed Gibbs sampling, which has been successfully used in LDA ( Griffiths &amp; Steyvers , 2004 ). With the data augmen-tation representation, this leads to an  X  X ugment-and-collapse X  sampling algorithm for Gibbs MedLDA. For Gibbs MedLDA, the collapsed posterior distribu-tion is where  X  ( x ) = the term t being assigned to topic k over the whole that terms being associated with topic k within the d -th document and C d = { C k d } K k =1 . Then, the condi-tional distributions used in collapsed Gibbs sampling are as follows.
 For : let X  X  assume its prior is an isotropic Gaussian distribution p 0 ( ) = where the posterior mean is =  X ( c and the covariance matrix is  X  = ( 1  X  2 I + c  X  from a K -dimensional multivariate Gaussian distribu-tion. The inverse can be robustly done using Cholesky decomposition, an O ( K 3 ) procedure. Since K is nor-mally not large, the inversion can be done efficiently. For Z : The conditional distribution of Z is By canceling common factors, we can derive the conditional distribution of one variable z dn given others Z  X  as: where C  X   X  ,  X  n indicates that term n is excluded from the corresponding document or topic;  X  = 1 N  X  value without word n . We can see that the first term is from the LDA model for observed word counts and the second term is from the supervised signal y . For : Finally, the conditional distribution of the aug-mented variables is where GIG ( x ; p,a,b ) = C ( p,a,b ) x p  X  1 exp(  X  1 2 ( is a generalized inverse Gaussian distribu-tion ( Devroye , 1986 ) and C ( p,a,b ) is a normalization constant. Therefore, we can derive that  X   X  1 d follows an inverse Gaussian distribution where IG ( x ; a,b ) = and b &gt; 0.
 With the above conditional distributions, we can con-struct a Markov chain which iteratively draws samples of using Eq. ( 11 ), Z using Eq. ( 12 ) and using Eq. ( 13 ), with an initial condition. To sample from an in-verse Gaussian distribution, we apply the transforma-tion method with multiple roots ( Michael et al. , 1976 ). In our experiments, we initially set = 1 and random-ly draw Z from a uniform distribution. In training, we run this Markov chain to finish the burn-in stage with M iterations. Then, we draw a sample  X  as the Gibbs classifier to make predictions on testing data. 3.4. Prediction To apply the Gibbs classifier  X  , we need to infer the topic assignments for testing document. We take the approach in ( Zhu et al. , 2012 ; Jiang et al. , 2012 ), which uses a point estimate of topics from training data and makes prediction based on them. Specifically, we use the MAP estimate  X  to replace the probability distribution p ( ). For the collapsed Gibbs sampler, an estimate of  X  using the samples is  X   X  kt  X  C t k +  X  t . Then, given a testing document w , we infer its latent compo-nents z using  X  as p ( z k n = 1 | z  X  n )  X   X   X  kw n ( C where C k  X  n is the times that the terms in this document w assigned to topic k with the n -th term excluded. 3.5. Gibbs MedLDA Regression Model Before ending this section, we briefly discuss how to generalize the above ideas to develop a regression mod-el, where the response variable Y takes real values. Specifically, the Gibbs MedLDA regression model has the same LDA model to describe input words and a Gibbs regression model for the response variable. If a sample of the topic assignments Z and the predic-tion model is drawn from the posterior distribution q ( , Z ), we define the prediction rule as  X  y =  X  z . One widely used margin-based loss measure is the  X  -insensitive loss R  X  ( , Z ) = support vector regression ( Smola &amp; Scholkopf , 2003 ), where  X  d = y d  X   X   X  z d is the margin. Then, we define the Gibbs MedLDA regression model as solving the regularized Bayesian inference problem where R  X  = E q [ R  X  ( , Z )] = the expected  X  -insensitive loss. Similarly, we can show that R  X  is an upper bound of the  X  -insensitive loss of MedLDA X  X  expected prediction rule, by noting that max(0 , | x |  X   X  ) = max(0 ,x  X   X  ) + max(0 ,  X  x  X   X  ) . (15) Lemma 3 We have R  X   X  Proof: By using the equality ( 15 ), we have R  X  =  X  ogous to Lemma 1, we can show that E q [max(0 ,  X  d  X   X  )]  X  max(0 , E q [ X  d ]  X   X  ) and E q [max(0 ,  X   X  d  X   X  )] max(0 ,  X  E q [ X  d ]  X   X  ). Then, applying the equality ( 15 ) again, we get the results.
 We can reformulate problem ( 14 ) in the form as prob-lem ( 10 ), with the pseudo-likelihood  X  ( y d | , z d ) = exp(  X  2 c max(0 , |  X  d |  X   X  )). Then, we have the dual scale of mixture representation.
 Lemma 4 (Dual Scale of Mixture) For regres-sion, the pseudo-likelihood can be expressed as Proof: By the equality ( 15 ), we have  X  ( y d | , z d ) = exp { X  2 c max(0 ,  X  d  X   X  ) } exp { X  2 c max(0 ,  X   X  d  X  Each of the exponential terms can be formulated as a scale mixture of Gaussians due to Lemma 2.
 Then, the data augmented learning problem of the Gibbs MedLDA regression model is min where  X  ( y , , ! | Z , ) =  X  ( y d , X  d , X  d | Z , ) = Solving the augmented problem and integrating out ( , ), we can get the collapsed posterior distribution q ( , , ! , Z )  X  p 0 ( ) p ( W , Z | , )  X  ( y , , ! | Z , ) . Then, following similar derivations as in the classifica-tion model, the Gibbs sampling algorithm to infer the posterior has the following conditional distributions. For : again, with the isotropic Gaussian prior p ( ) = where  X  = ( 1  X  2 I + c 2 c  X ( We can easily draw a sample from a K -dimensional multivariate Gaussian distribution. The inverse can be robustly done using Cholesky decomposition. For Z : We can derive the conditional distribution of one variable z dn given others Z  X  as: where  X  = 1 N discriminant function value without word n . The first term is from the LDA model for observed word counts. The second term is from the supervised signal y . For : Finally, we can derive that  X   X  1 d and  X   X  1 d follow the inverse Gaussian distributions: We present empirical results to demonstrate the effi-ciency and prediction performance of Gibbs MedLDA (denoted by GibbsMedLDA) on the 20Newsgroups da-ta set for classification and a hotel review data set for regression. We also analyze its sensitivity to key pa-rameters. The 20Newsgroups data set contains about 20K postings within 20 groups. We follow the same setting as in ( Zhu et al. , 2012 ) and remove a standard list of stop words for both binary and multi-class classi-fication. For all the experiments, we use the standard normal prior p 0 ( ) (i.e.,  X  2 = 1) and the symmetric Dirichlet priors =  X  K 1 , = 0 . 01  X  1 , where 1 is a vector with all entries being 1. For each setting, we re-port the average performance and standard deviation with five randomly initialized runs. All the experi-ments are done on a standard desktop computer. 4.1. Binary classi cation The binary classification is to distinguish postings of the newsgroup alt.atheism and postings of the group talk.religion.misc . The training set contains 856 doc-uments, and the test set contains 569 documents. We compare Gibbs MedLDA with the MedLDA model that uses variational methods (denoted by vMedL-DA) ( Zhu et al. , 2012 ) and the MedLDA that uses col-lapsed Gibbs sampling algorithms (denoted by gMedL-DA) ( Jiang et al. , 2012 ). We also include the unsu-pervised LDA using collapsed Gibbs sampling as a baseline, denoted by GibbsLDA. For GibbsLDA, we learn a binary linear SVM on its topic representa-tions using SVMLight ( Joachims , 1999 ). The results of other supervised topic models, such as sLDA and DiscLDA ( Lacoste-Jullien et al. , 2009 ), were report-ed in ( Zhu et al. , 2012 ). For Gibbs MedLDA, we set  X  = 1,  X  = 164 and M = 10. As we shall see, Gibbs MedLDA is insensitive to  X  ,  X  and M in a wide range. Although tuning c (e.g., via cross-validation) can pro-duce slightly better results, we fix c = 1 for simplicity. Fig. 1 shows the accuracy, training time, and testing time of different methods with different numbers of topics. We can see that by minimizing an expected hinge-loss and not making any restricting assumptions on the posterior distributions, GibbsMedLDA achieves higher accuracy than other max-margin topic models, which make some restricting assumptions. Similarly, as gMedLDA makes a weaker mean-field assumption, it achieves slightly higher accuracy than vMedLDA, which assumes that the posterior distribution is fully factorized. For the training time, GibbsMedLDA is about two orders of magnitudes faster than vMedL-DA, and about one order of magnitude faster than gMedLDA. This is partly because both vMedLDA and gMedLDA need to solve multiple SVM problems. For the testing time, GibbsMedLDA is comparable with gMedLDA and the unsupervised GibbsLDA, but much faster than the variational algorithm used by vMedL-DA, especially when K is large.
 4.2. Regression We use the hotel review data set ( Zhu &amp; Xing , 2010 ) built by randomly crawling hotel reviews from the Tri-pAdvisor website where each review is associated with a global rating score ranging from 1 to 5. In these experiments, we focus on predicting the global rating scores for reviews using the bag-of-words features on-ly, with a vocabulary of 12,000 terms. All the reviews have character lengths between 1500 and 6000. The data set consists of 5,000 reviews, with 1000 reviews per rating. The data set is uniformly partitioned in-to training and testing sets. We compare the Gibbs MedLDA regression model with the MedLDA regres-sion model that uses variational inference and super-vised LDA (sLDA) which also uses variational infer-ence. For Gibbs MedLDA and vMedLDA, the preci-sion is set at  X  = 1 e  X  3 and c is selected via 5 fold cross-validation during training. Again, we set the Dirichlet parameter  X  = 1 and the number of burn-in M = 10. Fig. 2 shows the predictive R 2 ( Blei &amp; McAuliffe , 2007 ) of different methods. We can see that Gibb-sMedLDA achieves comparable prediction perfor-mance with vMedLDA, which is better than sLDA. Note that vMedLDA uses a full likelihood model for both words and response variables, while GibbsMedL-DA uses a simpler likelihood model for words only. For train time, GibbsMedLDA is about two orders of mag-nitudes faster than vMedLDA (as well as sLDA), again due to the fact that GibbsMedLDA doesn X  X  need to solve multiple SVM problems. For testing time, Gibb-sMedLDA is also much faster than vMedLDA and sL-DA, especially when the number of topics is large. 4.3. More discussions 4.3.1. Multi-class classification We perform multi-class classification on the 20News-groups data with all 20 categories. The test set con-sists of 7,505 documents, and the training set consists of 11,269 documents. Again, since GibbsMedLDA is insensitive to  X  and  X  , we set  X  = 1 and  X  = 64. We also fix c = 1 for simplicity. The number of burn-in iterations is set as M = 20, which is sufficiently large, as we shall see.
 Various methods exist to apply binary classifiers to do multi-class classification, including the popular  X  X ne-vs-all X  and  X  X ne-vs-one X  strategies. Here we choose the  X  X ne-vs-all X  strategy, which has been shown effec-tive ( Rifkin &amp; Klautau , 2004 ), to provide some prelim-inary analysis. Fig. 3 shows the classification accuracy and training time, where GibbsMedLDA builds 20 bi-nary GibbsMedLDA classifiers. Since there is no cou-pling among these 20 binary classifiers, we can learn them in parallel, which we denote by pGibbsMedLDA. We can see a clear improvement on the classification accuracy, which may be due to the different strategies on building the multi-class classifiers 3 . However, giv-en the performance gain on the binary classification task, we believe that the Gibbs sampling algorithm without any restricting factorization assumptions is another factor leading to the improved performance. For training time, GibbsMedLDA takes slightly less time than the variational MedLDA as well as gMedL-DA. But if we train the multiple classifiers in paral-lel, we can save a lot of training time. These results are promising since it is now not uncommon to have a desktop computer with multiple processors or a cluster with tens or hundreds of computing nodes. 4.3.2. Sensitivity analysis Burn-In : Fig. 4 shows the performance of Gibb-sMedLDA with different numbers of burn-in samples for the binary classification task. When M = 0, the model is in fact random. We can see that the classi-fication performance increases very fast and converges to the stable optimum with 5 to 10 burn-in steps. The training time increases about linearly in general when using more burn-in steps. Moreover, the training time increases linearly as K increases. In the previous ex-periments, we have chosen M = 10. Fig. 5 shows the performance of GibbsMedLDA for multi-class classification with different numbers of burn-in steps. We can see when the number of burn-in steps is larger than 20, the performance is quite stable. Again, the training time grows about linear-ly as the number of burn-in steps increases. Even if we use 40 or 60 steps of burn-in, the training time is still competitive, compared with the variational MedL-DA, especially considering that GibbsMedLDA can be naively parallelized by learning different binary classi-fiers simultaneously.
 Dirichlet prior  X  : Fig. 6 shows the classification per-formance of GibbsMedLDA on the binary task with different  X  values. For the three different topic num-bers, we can see that the performance is quite stable in a wide range of  X  values, e.g., from 0 . 1 to 10. We can also see that it generally needs a larger  X  in order to get the best results when K becomes larger. This is mainly because a large K tends to produce sparse topic representations and an appropriately large  X  is needed to smooth the representations, as the effective Dirichlet prior is  X  k =  X /K .
 Loss penalty  X  : Fig. 7 shows the classification per-formance of GibbsMedLDA on the binary classification task with different  X  values. Again, we can see that in a wide range, e.g., from 25 to 625, the performance is quite stable for all the three different K values. In the above experiments, we set  X  = 164. For the multi-class classification task, we have similar observations, and we set  X  = 64 in the previous experiments. We presented Gibbs MedLDA, a new formulation of max-margin supervised topic models, which minimizes an expected margin loss. By using the idea of da-ta augmentation, we presented simple and highly ef-ficient  X  X ugment-and-collapse X  Gibbs sampling algo-rithms without making any restricting assumptions on posterior distributions. Empirical results on real data demonstrate significant improvements over the exist-ing max-margin topic models.
 The new data augmentation formulation without any need to solve constrained subproblems has shown great promise on improving the time efficiency of max-margin topic models. For future work, we are interest-ed in developing highly scalable sampling algorithms (e.g., using a distributed architecture) ( Newman et al. , 2009 ; Smola &amp; Narayanamurthy , 2010 ) to deal with large scale data sets.
 This work is supported by National Key Foundation R&amp;D Projects (No.s 2013CB329403, 2012CB316301), Tsinghua Initiative Scientific Research Program No.20121088071, the 221 Basic Research Plan for Y-oung Faculties at Tsinghua University, and a Research Fund No. 20123000007 from Microsoft Research Asia. Blei, D.M. and McAuliffe, J.D. Supervised topic mod-els. Advances in Neural Information Processing Sys-tems (NIPS) , pp. 121 X 128, 2007.
 Catoni, O. PAC-Bayesian supervised classification:
The thermodynamics of statistical learning. Mono-graph series of the Institute of Mathematical Statis-tics , 2007.
 Dempster, A.P., Laird, N.M., and Rubin, D.B. Maxi-mum likelihood estimation from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, Ser. B , (39):1 X 38, 1977.
 Devroye, L. Non-uniform random variate generation . Springer-Verlag, 1986.
 Germain, P., Lacasse, A., Laviolette, F., and Marc-hand, M. PAC-Bayesian learning of linear classifier-s. In International Conference on Machine Learning (ICML) , pp. 353 X 360, 2009.
 Griffiths, T.L. and Steyvers, M. Finding scientific top-ics. Proceedings of National Academy of Science (P-NAS) , pp. 5228 X 5235, 2004.
 Jiang, Q., Zhu, J., Sun, M., and Xing, E.P. Monte Car-lo methods for maximum margin supervised topic models. In Advances in Neural Information Pro-cessing Systems (NIPS) , 2012.
 Joachims, T. Making large-scale SVM learning practi-cal . MIT press, 1999.
 Lacoste-Jullien, S., Sha, F., and Jordan, M.I. DiscL-
DA: Discriminative learning for dimensionality re-duction and classification. Advances in Neural In-formation Processing Systems (NIPS) , pp. 897 X 904, 2009.
 McAllester, D. PAC-Bayesian stochastic model selec-tion. Machine Learning , 51:5 X 21, 2003.
 Michael, J.R., Schucany, W.R., and Haas, R.W. Gen-erating random variates using transformations with multiple roots. The American Statistician , 30(2): 88 X 90, 1976.
 Newman, D., Asuncion, A., Smyth, P., and Welling,
M. Distributed algorithms for topic models. Journal of Machine Learning Research (JMLR) , (10):1801 X  1828, 2009.
 Polson, N.G. and Scott, S.L. Data augmentation for support vector machines. Bayesian Analysis , 6(1): 1 X 24, 2011.
 Rifkin, R. and Klautau, A. In defense of one-vs-all classification. Journal of Machine Learning Re-search (JMLR) , (5):101 X 141, 2004.
 Smola, A. and Narayanamurthy, S. An architecture for parallel topic models. Very Large Data Base (VLD-B) , 3(1-2):703 X 710, 2010.
 Smola, A. and Scholkopf, B. A tutorial on support vector regression. Statistics and Computing , 14(3): 199 X 222, 2003.
 Tanner, M.A. and Wong, W.-H. The calculation of posterior distributions by data augmentation. Jour-nal of the Americal Statistical Association (JASA) , 82(398):528 X 540, 1987. van Dyk, D. and Meng, X. The art of data augmen-tation. Journal of Computational and Graphical S-tatistics (JCGS) , 10(1):1 X 50, 2001.
 Yang, S., Bian, J., and Zha, H. Hybrid genera-tive/discriminative learning for automatic image an-notation. In Uncertainty in Arti cial Intelligence (UAI) , 2010.
 Zhu, J. and Xing, E.P. Conditional topic random field-s. In International Conference on Machine Learning (ICML) , pp. 1239 X 1246, 2010.
 Zhu, J., Ahmed, A., and Xing, E.P. MedLDA: maxi-mum margin supervised topic models for regression and classification. In International Conference on Machine Learning (ICML) , pp. 1257 X 1264, 2009. Zhu, J., Chen, N., and Xing, E.P. Infinite latent SVM for classification and multi-task learning. In
Advances in Neural Information Processing System-s (NIPS) , pp. 1620 X 1628, 2011.
 Zhu, J., Ahmed, A., and Xing, E.P. MedLDA: max-imum margin supervised topic models. Journal of Machine Learning Research (JMLR) , (13):2237 X 
