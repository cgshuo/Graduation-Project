 We develop a method for recommending products to customers with applications to both on-line and surface mail promotional offers. Our method differs from previous work in collaborative filtering [8] and imputation [18], in that we assume probabilities are conditionally independent. This assumption, which is also made in Na X ve Bayes [5], enables us to pre-compute probabilities and store them in main memory, enabling very fast performance on millions of customers. The algorithm supports a variety of tunable parameters so that the method can address different promotional objectives. We tested the algorithm at an on-line hardware retailer, with 17,400 customers divided randomly into control and experimental groups. In the experimental group, clickthrough increased by +40% (p&lt;0.01), revenue by +38% (p&lt;0.07), and units sold by +61% (p&lt;0.01). By changing the algorithm X  X  parameter settings we found that these results could be improved even further. This work demonstrates the considerable potential of automated data mining for dramatically increasing the profitability of on and off-line retail promotions. Imputation, cross-sell, collaborative filtering, recommendation Most customer recommendation algorithms can be understood as performing some kind of imputation [13]. Some of the customer X  X  interests are known because they have entered  X  X tar ratings X  or have bought a product, but most are not. The problem of deciding what product to recommend next involves finding out what the customer X  X  attitudes would be toward the missing values, by analyzing the statistical patterns of the population. For example, say that Joe purchased science&amp;nature and mystery. We can look Collaborative filtering systems [8,9,16] implement a nearest neighbor variant of the above strategy. The donor set is restricted to the k closest matching customer profiles to a candidate. Various alterations to this procedure have been proposed including weighting users, products or star ratings, and adding significance tests for measuring the reliability of recommendations [16]. In all the above methods one needs to calculate a match between the candidate and every other customer in the population, before blending the donor data to arrive at a score. In practice one needs to perform this computation quickly. One option is to calculate it when a customer visits a site. The time complexity of this operation is O( CN ) where C are the number of customers and N is the size of the profile.
 The alternative is to pre-compute probabilities and store in memory or on disk for faster lookup using an index or hash table. Unfortunately, there are usually too many match patterns to store for this to be feasible. We need to store results for each There are N possible items in the term before the bar -variable in the profile that we are estimating a probability for. The term after the bar, or pattern of conditional purchases in the customer  X  s profile -contains a string of N variables which can take the value The total number of combinations is N 2 N , which grows as an exponential function of N . The approach in this paper differs from previous work on collaborative filtering in the following respect. We do not calculate interest probabilities conditional upon meeting all of the criteria of a customer  X  s profile, as is required in conditional mean imputation and collaborative filtering. Instead, we operate under the assumption of conditional independence of the past behavior of the customer in question. Formally: Definition: Conditional independence This is somewhat unrealistic. If a customer has bought five scifi books, we would expect their probability of being interested in a new scifi book to be higher than another customer with one scifi book with ten gardening. Never the less, we will adopt the purchase profile.
 purchase relationships. The idea of lift is to promote products which have high mutual attractions to each other. For instance, an  X  air conditioning unit  X  and  X  air conditioning unit accessory  X  might be very rarely bought resulted in a +40% increase in profit for items that were moved together [11]. The formula for lift is Lift is a symmetric measure, so Lift( a , b )=Lift( b , a ). A number greater than one is interpreted as the number of times higher than random that two items occur together. A fractional number can be inverted and interpreted as the number of times lower than random that two items occur. Interestingly, lift is related to the Mutual Information Criterion (MIC) from information theory [3]. MIC is equal to log of lift. We favor the untransformed lift score because it is easier to interpret for the user. If we assume mutual independence between products, then the expected profit after buying a product a is equal to the probability of buying b given a , Pr( b | a ) multiplied by the profit  X  of b . As a result this is the formula: The idea behind incremental profit is to maximize the profit minus the profit you would expect to receive due to the natural course of a customer  X  s purchasing. For example, say a customer comes into a store and buys a hammer (product a ). You have two choices: nails, or screwdrivers. Analysis of customer purchase patterns may indicate that nails are almost certainly going to be bought in the future, since these have a 20% chance of being bought by any customer. Therefore, instead of promoting something that we know the customer will be buying anyway, we go for the purchase that has a higher incremental profit  X  the screwdriver. Incremental profit maximizes the profit of the item, minus the baseline profit associated with the item. Thus incremental profit is similar to lift, except it subtracts the base probability, rather than dividing by it. This parameter directs the algorithm to recommend at least one item from each driver, or will pool all of the recommendations together, and will select those with the highest promotional Driver recency forces the recommendation algorithm to consider more recent purchases preferentially over purchases in the past. of the customer  X  s historical purchases can be beneficial, because the largest RecommendationValue scores might come from just one product in the customer  X  s profile (eg. one which has a high baseline probability). Thus all recommendations would be based on a single purchase, when that customer  X  s profile might contain much more information, for instance, 10 purchases of scifi books.
 Figure 3 shows the overall effectiveness of the automated recommendations, compared to the control recommendations. These results show that in the experimental group revenue per customer increased by 38%, clickthrough by 40%, and quantity purchased by 61%. A t-test revealed that the clickthrough, quantity and transactions improvements were statistically significant at the p&lt;0.01 level, whilst the revenue increase was significant at the p&lt;0.07 level. As a result, the improvements in the automated system were both large and have a very low chance of being caused by random. Figures 7 and 8 show some example customers and the products they were recommended. clickthrough per customer $ per recommendation generated 6-8% more revenue than base response probability. trans or qty per recommendation Incremental profit and lift both outperformed conditional probability and profit maximization in all behavioral measurements including revenue, transactions, and quantity purchased (figure 5).
 The fact that incremental profit and lift out-performed the other methods is interesting. Lift is the conditional probability divided by the baseline probability. Now consider that incremental profit is the conditional probability minus the baseline probability. These two measures are similar in that both are discounting the baseline probability in some way. [2] also found that discounting base rating frequencies increased accuracy in predicting interest in test data. Their  X  inverse user weighting  X  scheme increased accuracy in all 24 experiments they ran on test data. Further experiments will be needed to identify (a) if this principle holds true in general, (b) the best Figure 6: Revenue resulting from different numbers of recommendations
Customer Qty Rev Responses First B 82 561.74 11 4/22/96 0 1165 Table 8.3. Customer B purchases three days after offer sent customer purchased it. Because we had access to a long period of customer history, we were also able to analyze the effect of previous responses to promotions on the likelihood of responding to this promotion. We identified a 25 factors, listed in figure 9. The best predictor for high revenue in the promotion is a high quantity purchased per catalogue received (R=0.38) followed by other lifetime revenue and quantity variables. The response probability of items recommended was correlated with customer revenue (R=0.13).
 Other researchers have reported similar results to those in our experiment. [10] reported a lift in clickthrough from 8.3% to 13.2% for market basket analysis (possibly similar to the method in this paper), and 13.96% for nearest neighbor method, in direct email campaigns (59% and 68% respectively). [15] reported a lift in revenue of 60% at a catalogue company in the United Kingdom using a nearest neighbor method. B ecause of these large improvements, we are confident that our results are typical of results achieved by implementing intelligent increase profitability of customers. We have shown in this paper that implementation of such a system can significantly increase profitability and re-visit propensity by as much as 38% and 40% respectively at a low volume retailer, and without a finely tuned system. This kind of improvement cannot be ignored, and we predict that all web sites will install systems of a type like that in this paper to increase their customer satisfaction, re-visit frequency, and most importantly, the bottom-line profitability of their web business. Thanks to Vignette Corporation for making possible this research. [1] Agrawal, R. and Srikant, R, Fast algorithms for mining [2] Breese, J., Heckerman, D., Kadie, C., Empirical Analysis of [3] Chan, P., A non-invasive learning approach to building [4] David, M., Little, R., et. al., Alternative methods for CPS [5] Elkan, C., Boosting and Na  X  ve Bayesian Learning, [6] Ford, B., An Overview of Hot-Deck Procedures, in [7] Herlocker, J., Konstan, J., Borchers, A., Riedl, J., An
