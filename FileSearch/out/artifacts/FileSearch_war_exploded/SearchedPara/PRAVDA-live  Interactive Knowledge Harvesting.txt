 Acquiring high-quality (temporal) facts for knowledge bases is a labor-intensive process. Although there has been recent progress in the area of semi-supervised fact extraction, these approaches still have limitations, including a restricted corpus, a fixed set of relations to be extracted or a lack of assessment capabilities. In this paper we introduce PRAVDA-live, a framework that overcomes these limitations and supports the entire pipeline of interactive knowl-edge harvesting. To this end, our demo exhibits fact extraction from ad-hoc corpus creation, via relation specification, labeling and assessment all the way to ready-to-use RDF exports.
 H.1.0 [ Information Systems ]: Models and Principles X  General Algorithms, Design, Experimentation Interactive Knowledge Harvesting, Label Propagation
In recent years, automated fact extraction from Web contents has seen significant progress with the emergence of freely available knowledge bases, such as DBpedia [1], YAGO [10], TextRunner [5], or ReadTheWeb [3]. These knowledge bases are constantly growing and contain currently (by example of DBpedia) several million entities and half a billion facts about them.

To expand these knowledge bases even further, extraction of fac-tual knowledge from free text is an essential ingredient, since most knowledge is not available in (semi-)structured formats. Systems like NELL [4] or Prospera [8] are tackling this task. Additionally, most knowledge bases come with a certain thematic focus and re-lations specific to that, i.e. YAGO X  X  most populated relations are about persons. This calls for the creation of even more knowledge bases in previously uncovered domains or contexts.

However developing a fact extraction system from scratch is a cumbersome endeavor, because even subproblems such as entity disambiguation or pattern mining are active research topics on their own, requiring experts to be solved. Hence, to lower the bar of customized fact extraction from free text, we introduce an interactive knowledge harvesting system called PRAVDA-live 1 . Our system is an out-of-the-box solution to address this issue by covering all subproblems within a web-based interface. This allows user of all provenance (from greenhorns to experts) to perform fact extraction on their own.
 Problem Setting. Let us consider the following sentence from Wikipedia:  X  Albert Einstein was born in Ulm, in the Kingdom of W X rttemberg in the German Empire on 14 March 1879.  X  We can nail down parts of its semantics by using the relation bornIn , which connects a subject of type person with an object of type location and maybe an additional temporal annotation. Given that, our system can be employed to automatically distill the fact bornIn ( Einstein , Ulm )@14 . 03 . 1879 from this sentence.
 State of the Art. DBpedia [1] and YAGO [10] harvest semi-structured data from Wikipedia, such as tables, infoboxes, or cate-gories, not focusing on the extraction from free text. The projects NELL/ReadTheWeb [3] and TextRunner/ReVerb [5, 6] tackle free text, have an online query interface for their stored facts, but do not allow interactive fact extraction. Freebase [2] is very related to our work, since users are encouraged to add facts. Nevertheless, there is no support for extraction from text documents. Moreover, there are numerous crowd-sourcing activities for data annotation such as KiWi [9] or Semantic Wiki [7]. However, those efforts do not aim at fact extraction for knowledge bases in terms of clarity and scale. Contributions. We present a system called PRAVDA-live, which supports fact extraction of user-defined relations from ad-hoc se-lected text documents and ready-to-use RDF exports. Further fea-tures include the support for temporally annotated relations, custom or mined extraction-patterns, and a constraint solver being able to clean extracted facts by inter-fact constraints. Framework. Our system is structured as depicted in Figure 1, where boxes with continuous lines require user-interaction. As a first step, the user uploads a corpus to be subject to the extraction process. Then, the user either selects predefined relations or defines customized relations or both. What follows are the parsing and graph construction stages performed in the backend. If customized relations are present, the next step is the seed fact selection followed by manual labeling of these. Afterwards we continue with pattern analysis and label propagation, eventually yielding extracted facts. Finally, there is the option to define and apply constraints to the extracted facts, or to download the facts immediately. Parsing. Once the corpus is uploaded and the relations of interest are known, our system proceeds with the parsing stage. Following [13] entities are recognized and disambiguated by leveraging YAGO [10]. The surface string between two entities are lifted to patterns by considering n-grams of nouns, verbs converted to present tense, and prepositions. A detailed description of patterns is beyond the scope of this paper, however we refer the interested reader to [8]. Finally, we detect temporal expressions by regular expressions. Graph Construction. As in [13] we construct an undirected graph G = ( V, E ) comprising two types of vertices V = V e  X   X  V former set V e contains one vertex per entity pair discovered in the corpus and the latter set V p has one vertex per pattern. Edges between V p and V e are added, if an entity pair occurs in a pattern. Additional edges between vertices in V p are derived from similarities among patterns. An example graph is shown in Figure 3, where oval vertices belong to V e and box-shaped vertices are members of V Seed Fact Selection. In previous works seed facts were chosen entirely manually, which requires an understanding of the label propagation procedure. To ease usability of our system, we develop a novel ranking algorithm returning the entity pairs to be labeled by the user. The algorithm is presented in Section 2.2. This stage is skipped for problem instances with only predefined relations, where labeled seed facts are available in our system.
 Pattern Analysis. In this stage, the labeled seed facts are employed to compute an initial weighting of the patterns, being derived from frequency counts of both the patterns as well as the entity pairs. A more detailed description is available in [8].
 Label Propagation. Building on [13] we utilize Label Propagation [11] to determine the relation expressed by each pattern. Here, the labeled seed facts and patterns serve as input. Label Propagation is a semi-supervised learning algorithm, where the supervision re-sults from the labeled seed facts. It passes labels on the previously described graph (see Figure 3), where each label corresponds to a relation.
 Fact Extraction. After Label Propagation has terminated, the entity pair vertices which hold a relation X  X  label weighted above a threshold form a fact.
 Constraint Solving. Given user-defined constraints and a set of extracted facts, we intend to select a maximal consistent subset of facts. The resulting optimization problem is encoded into an integer linear program as in [12].
Our seed fact selection procedure acts on the graph introduced in Section 2. By construction the graph consists of disconnected components corresponding to a different type pair each. Consider-ing a single connected component, we cluster its vertices reflecting patterns in the following manner: If two patterns have an identi-cal first verb and last preposition, they belong to the same cluster. For example, in the disconnected component of Figure 3, we clus-ter  X  die in  X  and  X  died at home in  X . More formally, a disconnected component is defined as C = ( V e  X   X  (  X  S i V p,i ) , E ) , where V the vertices standing for entity pairs, and each V p,i represents a cluster of vertices embodying patterns. With respect to Figure 3, we have V e = { ( Bohr , Copenhagen ) , ( Einstein , Princeton ) } and V p, 0 = { die at home in , die in } , V p, 1 = { decede in } , for instance. To each disconnected component we apply Algorithm 1 implementing a greedy strategy. The loop in Line 3 repeatedly cycles through all clusters of pattern vertices V p,i beginning with the largest cluster. It stops when k seed facts have been determined. For each cluster we add an entity-pair-vertex v e as seed, which has maxi-mum degree and is connected to V p,i (Line 4). In Figure 3 the largest Algorithm 1 Seed Fact Selection Require: Component C = ( V e  X   X  (  X  S i V p,i ) , E ) , number of seeds k 1: S =  X  . Seed facts to return 2: while | S | &lt; k do 3: for V p,i  X  C by decreasing | V p,i | do 4: v e := argmax v 5: S := S  X  X  v e } 6: if | S | X  k then 7: break . Break loop in Line 3 8: return S cluster is V p, 0 = { die at home in , die in } where the algorithm selects ( Einstein , Princeton ) since its degree is maximal.
PRAVDA-live is implemented in Java, where Apache Tomcat 2 acts as Webserver. While parsing text documents we employ Open-NLP 3 for part of speech tagging, converting verbs to present tense and stemming nouns. Furthermore, the integer linear program tack-ling the constraints utilizes Gurobi 4 . All data is managed by a PostgreSQL 5 database.
Figure 2 shows screenshots of PRAVDA-live X  X  user interface. 1. Corpus Upload. The user interface offers the opportunity for 2. Defining Relations. By investigating the text on the left, the 3. Seed Labeling. During the labeling process text snippets are 4. Defining Constraints. As a final feature, PRAVDA-live al-
In order to showcase the entire pipeline of our interactive knowl-edge harvesting system PRAVDA-live, we have prepared two ded-icated demo scenarios: Ad-hoc Fact Extraction for YAGO and Fact Extraction on Customized Relations . Users may freely in-teract with our system.

Ad-hoc Fact Extraction for YAGO . In the first scenario we will enable users to harvest (temporal) facts based on the relations supported by YAGO. To this end, users will be able to either upload a document collection or paste a text document in the user interface of PRAVDA-live for a subsequent fact extraction. After that, the facts can be evaluated via the assessment interface. Finally, the so created fact set can be exported as an RDF document, which is ready to be fed into YAGO.

Fact Extraction on Customized Relations . The second sce-nario allows users to harvest facts from customized relations. This use case is of particular interest for those, who want inject RDF exports from PRAVDA-live into a proprietary knowledge base. To this end, we demonstrate the specification of additional relations and the corresponding labeling of a small number seed facts. Further, we showcase the bulk processing feature of PRAVDA-live with a subsequent assessment. The demo concludes with a RDF export of the extracted facts.
 This work is supported by the 7 th Framework IST program of the European Union through the focused research project (STREP) on Longitudinal Analytics of Web Archive data (LAWA) under contract no. 258105.
