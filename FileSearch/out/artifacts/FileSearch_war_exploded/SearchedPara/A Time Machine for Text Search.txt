 Text search over temporally versioned document collections such as web archives has received little attention as a re-search problem. As a consequence, there is no scalable and principled solution to search such a collection as of a speci-fied time t . In this work, we address this shortcoming and propose an efficient solution for time-travel text search by extending the inverted file index to make it ready for tempo-ral search. We introduce approximate temporal coalescing as a tunable method to reduce the index size without signif-icantly affecting the quality of results. In order to further improve the performance of time-travel queries, we intro-duce two principled techniques to trade off index size for its performance. These techniques can be formulated as op-timization problems that can be solved to near-optimality. Finally, our approach is evaluated in a comprehensive se-ries of experiments on two large-scale real-world datasets. Results unequivocally show that our methods make it pos-sible to build an efficient  X  X ime machine X  scalable to large versioned text collections.
 H.3.1 [ Content Analysis and Indexing ]: Indexing meth-ods; H.3.3 [ Information Search and Retrieval ]: Re-trieval models, Search process Algorithms, Experimentation, Performance Temporal text indexing, time-travel text search, web archives
In this work we address time-travel text search over tem-porally versioned document collections. Given a keyword query q and a time t our goal is to identify and rank rel-evant documents as if the collection was in its state as of time t .

An increasing number of such versioned document col-lections is available today including web archives, collabo-Copyright 2007 ACM 978-1-59593-597-7/07/0007 ... $ 5.00. rative authoring environments like Wikis, or timestamped information feeds. Text search on these collections, how-ever, is mostly time-ignorant: while the searched collection changes over time, often only the most recent version of a documents is indexed, or, versions are indexed indepen-dently and treated as separate documents. Even worse, for some collections, in particular web archives like the Inter-net Archive [18], a comprehensive text-search functionality is often completely missing.

Time-travel text search, as we develop it in this paper, is a crucial tool to explore these collections and to unfold their full potential as the following example demonstrates. For a documentary about a past political scandal, a jour-nalist needs to research early opinions and statements made by the involved politicians. Sending an appropriate query to a major web search-engine, the majority of returned re-sults contains only recent coverage, since many of the early web pages have disappeared and are only preserved in web archives. If the query could be enriched with a time point, say August 20th 2003 as the day after the scandal got re-vealed, and be issued against a web archive, only pages that existed specifically at that time could be retrieved thus better satisfying the journalist X  X  information need.

Document collections like the Web or Wikipedia [32], as we target them here, are already large if only a single snap-shot is considered. Looking at their evolutionary history, we are faced with even larger data volumes. As a consequence, na  X  X ve approaches to time-travel text search fail, and viable approaches must scale-up well to such large data volumes.
This paper presents an efficient solution to time-travel text search by making the following key contributions: 1. The popular well-studied inverted file index [35] is trans-2. Temporal coalescing is introduced to avoid an index-3. We develop two sublist materialization techniques to 4. In a comprehensive experimental evaluation our ap-
The remainder of this paper is organized as follows. The presented work is put in context with related work in Sec-tion 2. We delineate our model of a temporally versioned document collection in Section 3. We present our time-travel inverted index in Section 4. Building on it, temporal coa-lescing is described in Section 5. In Section 6 we describe principled techniques to improve index performance, before presenting the results of our experimental evaluation in Sec-tion 7.

We can classify the related work mainly into the following two categories: (i) methods that deal explicitly with col-lections of versioned documents or temporal databases, and (ii) methods for reducing the index size by exploiting either the document-content overlap or by pruning portions of the index. We briefly review work under these categories here.
To the best of our knowledge, there is very little prior work dealing with historical search over temporally versioned doc-uments. Anick and Flynn [3], while pioneering this research, describe a help-desk system that supports historical queries. Access costs are optimized for accesses to the most recent versions and increase as one moves farther into the past. Burrows and Hisgen [10], in a patent description, delineate a method for indexing range-based values and mention its potential use for searching based on dates associated with documents. Recent work by N X rv  X ag and Nyb X  [25] and their earlier proposals concentrate on the relatively simpler problem of supporting text-containment queries only and neglect the relevance scoring of results. Stack [29] reports practical experiences made when adapting the open source search-engine Nutch to search web archives. This adap-tation, however, does not provide the intended time-travel text search functionality. In contrast, research in temporal databases has produced several index structures tailored for time-evolving databases; a comprehensive overview of the state-of-art is available in [28]. Unlike the inverted file in-dex, their applicability to text search is not well understood.
Moving on to the second category of related work, Broder et al. [8] describe a technique that exploits large content overlaps between documents to achieve a reduction in index size. Their technique makes strong assumptions about the structure of document overlaps rendering it inapplicable to our context. More recent approaches by Hersovici et al. [17] and Zhang and Suel [34] exploit arbitrary content overlaps between documents to reduce index size. None of the ap-proaches, however, considers time explicitly or provides the desired time-travel text search functionality. Static index-pruning techniques [11, 12] aim to reduce the effective index size, by removing portions of the index that are expected to have low impact on the query result. They also do not consider temporal aspects of documents, and thus are tech-nically quite different from our proposal despite having a shared goal of index-size reduction. It should be noted that index-pruning techniques can be adapted to work along with the temporal text index we propose here.
In the present work, we deal with a temporally versioned document collection D that is modeled as described in the following. Each document d  X  D is a sequence of its versions Each version d t i has an associated timestamp t i reflecting when the version was created. Each version is a vector of searchable terms or features. Any modification to a docu-ment version results in the insertion of a new version with corresponding timestamp. We employ a discrete definition of time, so that timestamps are non-negative integers. The deletion of a document at time t i , i.e., its disappearance from the current state of the collection, is modeled as the insertion of a special  X  X ombstone X  version  X  . The validity time-interval val ( d t i ) of a version d t i is [ t i , t version with associated timestamp t i +1 exists, and [ t i otherwise where now points to the greatest possible value of a timestamp (i.e.,  X  t : t &lt; now ).

Putting all this together, we define the state D t of the collection at time t (i.e., the set of versions valid at t that are not deletions) as
As mentioned earlier, we want to enrich a keyword query q with a timestamp t , so that q be evaluated over D t , i.e., the state of the collection at time t . The enriched time-travel query is written as q t for brevity.

As a retrieval model in this work we adopt Okapi BM25 [27], but note that the proposed techniques are not dependent on this choice and are applicable to other retrieval models like tf-idf [4] or language models [26] as well. For our considered setting, we slightly adapt Okapi BM25 as
In the above formula, the relevance w ( q t , d t i ) of a doc-ument version d t i to the time-travel query q t is defined. We reiterate that q t is evaluated over D t so that only the version d t i valid at time t is considered. The first factor w score is defined as It considers the plain term frequency tf ( v, d t i ) of term v avdl ( t i ) in the collection at time t i . The length-normalization parameter b and the tf-saturation parameter k 1 are inherited from the original Okapi BM25 and are commonly set to val-ues 1 . 2 and 0 . 75 respectively. The second factor w idf which we refer to as the idf-score in the remainder, conveys the inverse document frequency of term v in the collection at time t and is defined as gives the number of documents in the collection that contain the term v at time t . While the idf-score depends on the whole corpus as of the query time t , the tf-score is specific to each version.
The inverted file index is a standard technique for text indexing, deployed in many systems. In this section, we briefly review this technique and present our extensions to the inverted file index that make it ready for time-travel text search.
An inverted file index consists of a vocabulary , commonly organized as a B + -Tree, that maps each term to its idf-score and inverted list . The index list L v belonging to term v contains postings of the form where d is a document-identifier and p is the so-called pay-load. The payload p contains information about the term frequency of v in d , but may also include positional in-formation about where the term appears in the document. The sort-order of index lists depends on which queries are to be supported efficiently. For Boolean queries it is fa-vorable to sort index lists in document-order. Frequency-order and impact-order sorted index lists are beneficial for ranked queries and enable optimized query processing that stops early after having identified the k most relevant docu-ments [1, 2, 9, 15, 31]. A variety of compression techniques, such as encoding document identifiers more compactly, have been proposed [33, 35] to reduce the size of index lists. For an excellent recent survey about inverted file indexes we re-fer to [35].
In order to prepare an inverted file index for time travel we extend both inverted lists and the vocabulary structure by explicitly incorporating temporal information. The main idea for inverted lists is that we include a validity time-interval [ t b , t e ) in postings to denote when the payload infor-mation was valid. The postings in our time-travel inverted file index are thus of the form where d and p are defined as in the standard inverted file index above and [ t b , t e ) is the validity time-interval.
As a concrete example, in our implementation, for a ver-v , the index list L v contains the posting
Similarly, the extended vocabulary structure maintains for each term a time-series of idf-scores organized as a B Tree. Unlike the tf-score, the idf-score of every term could vary with every change in the corpus. Therefore, we take a simplified approach to idf-score maintenance, by comput-ing idf-scores for all terms in the corpus at specific (possibly periodic) times.
During processing of a time-travel query q t , for each query term the corresponding idf-score valid at time t is retrieved from the extended vocabulary. Then, index lists are sequen-tially read from disk, thereby accumulating the information contained in the postings. We transparently extend the se-quential reading, which is  X  to the best of our knowledge  X  common to all query processing techniques on inverted file indexes, thus making them suitable for time-travel query-processing. To this end, sequential reading is extended by skipping all postings whose validity time-interval does not contain t (i.e., t 6 X  [ t b , t e )). Whether a posting can be skipped can only be decided after the posting has been trans-ferred from disk into memory and therefore still incurs signif-icant I/O cost. As a remedy, we propose index organization techniques in Section 6 that aim to reduce the I/O overhead significantly.

We note that our proposed extension of the inverted file index makes no assumptions about the sort-order of in-dex lists. As a consequence, existing query-processing tech-niques and most optimizations (e.g., compression techniques) remain equally applicable.
If we employ the time-travel inverted index, as described in the previous section, to a versioned document collection, we obtain one posting per term per document version. For frequent terms and large highly-dynamic collections, this leads to extremely long index lists with very poor query-processing performance.

The approximate temporal coalescing technique that we propose in this section counters this blowup in index-list size. It builds on the observation that most changes in a versioned document collection are minor, leaving large parts of the document untouched. As a consequence, the payload of many postings belonging to temporally adjacent versions will differ only slightly or not at all. Approximate temporal coalescing reduces the number of postings in an index list by merging such a sequence of postings that have almost equal payloads, while keeping the maximal error bounded. This idea is illustrated in Figure 1, which plots non-coalesced and coalesced scores of postings belonging to a single document. Approximate temporal coalescing is greatly effective given such fluctuating payloads and reduces the number of post-ings from 9 to 3 in the example. The notion of temporal coalescing was originally introduced in temporal database research by B  X ohlen et al. [6], where the simpler problem of coalescing only equal information was considered.
We next formally state the problem dealt with in approx-imate temporal coalescing, and discuss the computation of optimal and approximate solutions. Note that the technique is applied to each index list separately, so that the following explanations assume a fixed term v and index list L v .
As an input we are given a sequence of temporally adja-cent postings Each sequence represents a contiguous time period during which the term was present in a single document d . If a term disappears from d but reappears later, we obtain multiple input sequences that are dealt with separately. We seek to generate the minimal length output sequence of postings
O =  X  ( d, p j , [ t j , t j +1 ) ) , . . . , ( d, p m  X  1 that adheres to the following constraints: First, O and I must cover the same time-range, i.e., t i = t j and t n = t Second, when coalescing a subsequence of postings of the in-put into a single posting of the output, we want the approx-imation error to be below a threshold . In other words, if ( d, p i , [ t i , t i +1 )) and ( d, p j , [ t j , t j +1 O respectively, then the following must hold for a chosen error function and a threshold :
In this paper, as an error function we employ the relative error between payloads (i.e., tf-scores) of a document in I and O , defined as:
Finding an optimal output sequence of postings can be cast into finding a piecewise-constant representation for the points ( t i , p i ) that uses a minimal number of segments while retaining the above approximation guarantee. Similar prob-lems occur in time-series segmentation [21, 30] and histogram construction [19, 20]. Typically dynamic programming is applied to obtain an optimal solution in O ( n 2 m  X  ) [20, 30] time with m  X  being the number of segments in an optimal sequence. In our setting, as a key difference, only a guaran-tee on the local error is retained  X  in contrast to a guarantee on the global error in the aforementioned settings. Exploit-ing this fact, an optimal solution is computable by means of induction [24] in O ( n 2 ) time. Details of the optimal algo-rithm are omitted here but can be found in the accompany-ing technical report [5].

The quadratic complexity of the optimal algorithm makes it inappropriate for the large datasets encountered in this work. As an alternative, we introduce a linear-time approx-imate algorithm that is based on the sliding-window algo-rithm given in [21]. This algorithm produces nearly-optimal output sequences that retain the bound on the relative error, but possibly require a few additional segments more than an optimal solution.
 Algorithm 1 Temporal Coalescing (Approximate) 1: I =  X  ( d, p i , [ t i , t i +1 ) ) , . . .  X  O =  X   X  3: for ( d, p j , [ t j , t j +1 ) )  X  I do 5: p 0 = optrep ( p 0 min , p 0 max ) 6: if err rel ( p min , p 0 )  X   X  err rel ( p max , p 0 )  X  then 8: else 9: O = O  X  ( d, p, [ t b , t e ) ) 11: end if 12: end for 13: O = O  X  ( d, p, [ t b , t e ) ) Algorithm 1 makes one pass over the input sequence I . While doing so, it coalesces sequences of postings having maximal length. The optimal representative for a sequence of postings depends only on their minimal and maximal pay-load ( p min and p max ) and can be looked up using optrep in O (1) (see [16] for details). When reading the next post-ing, the algorithm tries to add it to the current sequence of postings. It computes the hypothetical new representative p and checks whether it would retain the approximation guarantee. If this test fails, a coalesced posting bearing the old representative is added to the output sequence O and, following that, the bookkeeping is reinitialized. The time complexity of the algorithm is in O ( n ).

Note that, since we make no assumptions about the sort order of index lists, temporal-coalescing algorithms have an additional preprocessing cost in O ( | L v | log | L v | ) for sorting the index list and chopping it up into subsequences for each document.
Efficiency of processing a query q t on our time-travel in-verted index is influenced adversely by the wasted I/O due to read but skipped postings. Temporal coalescing implic-itly addresses this problem by reducing the overall index list size, but still a significant overhead remains. In this section, we tackle this problem by proposing the idea of materializing sublists each of which corresponds to a contiguous subinter-val of time spanned by the full index. Each of these sublists contains all coalesced postings that overlap with the corre-sponding time interval of the sublist. Note that all those postings whose validity time-interval spans across the tem-poral boundaries of several sublists are replicated in each of the spanned sublists. Thus, in order to process the query q it is sufficient to scan any materialized sublist whose time-interval contains t .

We illustrate the idea of sublist materialization using an example shown in Figure 2. The index list L v visualized in the figure contains a total of 10 postings from three doc-uments d1, d2, and d3. For ease of description, we have numbered boundaries of validity time-intervals, in increasing time-order, as t 1 , . . . , t 10 and numbered the postings them-selves as 1 , . . . , 10. Now, consider the processing of a query q t with t  X  [ t 1 , t 2 ) using this inverted list. Although only three postings (postings 1, 5 and 8) are valid at time t , the whole inverted list has to be read in the worst case. Suppose that we split the time axis of the list at time t 2 , forming two sublists with postings { 1, 5, 8 } and { 2, 3, 4, 5, 6, 7, 8, 9, 10 } respectively. Then, we can process the above query with optimal cost by reading only those postings that existed at this t .

At a first glance, it may seem counterintuitive to reduce index size in the first step (using temporal coalescing), and then to increase it again using the sublist materialization techniques presented in this section. However, we reiterate that our main objective is to improve the efficiency of pro-cessing queries, not to reduce the index size alone. The use of temporal coalescing improves the performance by reduc-ing the index size, while the sublist materialization improves performance by judiciously replicating entries. Further, the two techniques, can be applied separately and are indepen-dent. If applied in conjunction, though, there is a synergetic effect  X  sublists that are materialized from a temporally co-alesced index are generally smaller.

We employ the notation L v : [ t i , t j ) to refer to the mate-rialized sublist for the time interval [ t i , t j ), that is formally defined as,
L v : [ t i , t j ) = { ( d, p, [ t b , t e ) )  X  L v | t b
To aid the presentation in the rest of the paper, we first provide some definitions. Let T =  X  t 1 . . . t n  X  be the sorted sequence of all unique time-interval boundaries of an in-verted list L v . Then we define to be the set of elementary time intervals . We refer to the set of time intervals for which sublists are materialized as and demand i.e., the time intervals in M must completely cover the time interval [ t 1 , t n ), so that time-travel queries q t  X  [ t 1 , t n ) can be processed. We also assume that inter-vals in M are disjoint. We can make this assumption with-out ruling out any optimal solution with regard to space or performance defined below. The space required for the materialization of sublists in a set M is defined as i.e., the total length of all lists in M . Given a set M , we let denote the time interval that is used to process queries q with t  X  [ t i , t i +1 ). The performance of processing queries q t for t  X  [ t i , t i +1 ) inversely depends on its processing cost which is assumed to be proportional to the length of the list L v :  X  ( [ t i , t i +1 ) ). Thus, in order to optimize the per-formance of processing queries we minimize their processing costs.
One strategy to eliminate the problem of skipped post-ings is to eagerly materialize sublists for all elementary time intervals, i.e., to choose M = E . In doing so, for every query q t only postings valid at time t are read and thus the best possible performance is achieved. Therefore, we will refer to this approach as P opt in the remainder. The initial approach described above that keeps only the full list L v and thus picks M = { [ t 1 , t n ) } is referred to as S opt remainder. This approach requires minimal space, since it keeps each posting exactly once.

P opt and S opt are extremes: the former provides the best possible performance but is not space-efficient, the latter requires minimal space but does not provide good perfor-mance. The two approaches presented in the rest of this section allow mutually trading off space and performance and can thus be thought of as means to explore the config-uration spectrum between the P opt and the S opt approach.
The P opt approach clearly wastes a lot of space materi-alizing many nearly-identical sublists. In the example il-lustrated in Figure 2 materialized sublists for [ t 1 , t [ t , t 3 ) differ only by one posting. If the sublist for [ t was materialized instead, one could save significant space while incurring only an overhead of one skipped posting for all t  X  [ t 1 , t 3 ). The technique presented next is driven by the idea that significant space savings over P opt are achiev-able, if an upper-bounded loss on the performance can be tolerated, or to put it differently, if a performance guaran-tee relative to the optimum is to be retained. In detail, the technique, which we refer to as P G (Performance Guar-antee) in the remainder, finds a set M that has minimal required space, but guarantees for any elementary time in-terval [ t i , t i +1 ) (and thus for any query q t with t  X  [ t that performance is worse than optimal by at most a factor of  X   X  1. Formally, this problem can be stated as  X  [ t i , t i +1 )  X  E : P C ( [ t i , t i +1 ) )  X   X   X  | L
An optimal solution to the problem can be computed by means of induction using the recurrence C ( [ t 1 , t k +1 ) ) = where C ( [ t 1 , t j ) ) is the optimal cost (i.e., the space re-quired) for the prefix subproblem and condition stands for
Intuitively, the recurrence states that an optimal solution for [ t 1 , t k +1 ) be combined from an optimal solution to a prefix subproblem C ( [ t 1 , t j ) ) and a time interval [ t that can be materialized without violating the performance guarantee.

Pseudocode of the algorithm is omitted for space reasons, but can be found in the accompanying technical report [5]. The time complexity of the algorithm is in O ( n 2 )  X  for each prefix subproblem the above recurrence must be evaluated, which is possible in linear time if list sizes | L : [ t i precomputed. The space complexity is in O ( n 2 )  X  the cost of keeping the precomputed sublist lengths and memoizing optimal solutions to prefix subproblems.
So far we considered the problem of materializing sublists that give a guarantee on performance while requiring mini-mal space. In many situations, though, the storage space is at a premium and the aim would be to materialize a set of sublists that optimizes expected performance while not ex-ceeding a given space limit. The technique presented next, which is named SB , tackles this very problem. The space restriction is modeled by means of a user-specified parame-ter  X   X  1 that limits the maximum allowed blowup in index size from the space-optimal solution provided by S opt . The SB technique seeks to find a set M that adheres to this space limit but minimizes the expected processing cost (and thus optimizes the expected performance). In the definition of the expected processing cost, P ( [ t i , t i +1 ) ) denotes the probability of a query time-point being in [ t i , t i +1 mally, this space-bound sublist-materialization problem can be stated as argmin
The problem can be solved by using dynamic program-ming over an increasing number of time intervals: At each time interval in E the algorithms decides whether to start a new materialization time-interval, using the known best ma-terialization decision from the previous time intervals, and keeping track of the required space consumption for materi-alization. A detailed description of the algorithm is omitted here, but can be found in the accompanying technical re-port [5]. Unfortunately, the algorithm has time complexity in O ( n 3 | L v | ) and its space complexity is in O ( n is not practical for large data sets.

We obtain an approximate solution to the problem us-ing simulated annealing [22, 23]. Simulated annealing takes a fixed number R of rounds to explore the solution space. In each round a random successor of the current solution is looked at. If the successor does not adhere to the space limit, it is always rejected (i.e., the current solution is kept). A successor adhering to the space limit is always accepted if it achieves lower expected processing cost than the current solution. If it achieves higher expected processing cost, it is randomly accepted with probability e  X   X  /r where  X  is the increase in expected processing cost and R  X  r  X  1 denotes the number of remaining rounds. In addition, throughout all rounds, the method keeps track of the best solution seen so far. The solution space for the problem at hand can be efficiently explored. As we argued above, we solely have to look at sets M that completely cover the time interval [ t , t n ) and do not contain overlapping time intervals. We represent such a set M as an array of n boolean variables b . . . b n that convey the boundaries of time intervals in the set. Note that b 1 and b n are always set to  X  X rue X . Initially, all n  X  2 intermediate variables assume  X  X alse X , which cor-responds to the set M = { [ t 1 , t n ) } . A random successor can now be easily generated by switching the value of one of the n  X  2 intermediate variables. The time complexity of the method is in O ( n 2 )  X  the expected processing cost must be computed in each round. Its space complexity is in O ( n )  X  for keeping the n boolean variables.

As a side remark note that for  X  = 1 . 0 the SB method does not necessarily produce the solution that is obtained from S opt , but may produce a solution that requires the same amount of space while achieving better expected per-formance.
We conducted a comprehensive series of experiments on two real-world datasets to evaluate the techniques proposed in this paper.
The techniques described in this paper were implemented in a prototype system using Java JDK 1.5. All experi-ments described below were run on a single SUN V40z ma-chine having four AMD Opteron CPUs, 16GB RAM, a large network-attached RAID-5 disk array, and running Microsoft Windows Server 2003. All data and indexes are kept in an Oracle 10g database that runs on the same machine. For our experiments we used two different datasets.
 The English Wikipedia revision history (referred to as WIKI in the remainder) is available for free download as a single XML file. This large dataset, totaling 0.7 TBytes, contains the full editing history of the English Wikipedia from January 2001 to December 2005 (the time of our down-load). We indexed all encyclopedia articles excluding ver-sions that were marked as the result of a minor edit (e.g., the correction of spelling errors etc.). This yielded a total of 892,255 documents with 13,976,915 versions having a mean (  X  ) of 15 . 67 versions per document at standard deviation (  X  ) of 59.18. We built a time-travel query workload using the query log temporarily made available recently by AOL Re-search as follows  X  we first extracted the 300 most frequent keyword queries that yielded a result click on a Wikipedia ar-ticle (for e.g.,  X  X rench revolution X ,  X  X urricane season 2005 X ,  X  X a vinci code X  etc.). The thus extracted queries contained a total of 422 distinct terms. For each extracted query, we randomly picked a time point for each month covered by the dataset. This resulted in a total of 18 , 000 (= 300  X  60) time-travel queries.

The second dataset used in our experiments was based on a subset of the European Archive [13], containing weekly crawls of 11 .gov.uk websites throughout the years 2004 and 2005 amounting close to 2 TBytes of raw data. We filtered out documents not belonging to MIME-types text/plain and text/html , to obtain a dataset that totals 0.4 TBytes and which we refer to as UKGOV in rest of the paper. This included a total of 502,617 documents with 8,687,108 ver-sions (  X  = 17.28 and  X  = 13.79). We built a corresponding query workload as mentioned before, this time choosing key-word queries that led to a site in the .gov.uk domain (e.g.,  X  X inimum wage X ,  X  X nheritance tax X  ,  X  X itizenship ceremony dates X  etc.), and randomly sampling a time point for every month within the two year period spanned by the dataset. Thus, we obtained a total of 7,200 (= 300  X  24) time-travel queries for the UKGOV dataset. In total 522 terms appear in the extracted queries.

The collection statistics (i.e., N and avdl ) and term statis-tics (i.e., DF ) were computed at monthly granularity for both datasets.
Our first set of experiments is aimed at evaluating the ap-proximate temporal coalescing technique, described in Sec-tion 5, in terms of index-size reduction and its effect on the result quality. For both the WIKI and UKGOV datasets, we compare temporally coalesced indexes for different values of the error threshold computed using Algorithm 1 with the non-coalesced index as a baseline.
 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Table 1: Index sizes for non-coalesced index (-) and coalesced indexes for different values of
Table 1 summarizes the index sizes measured as the total number of postings. As these results demonstrate, approxi-mate temporal coalescing is highly effective in reducing in-dex size. Even a small threshold value, e.g. = 0 . 01, has a considerable effect by reducing the index size almost by an order of magnitude. Note that on the UKGOV dataset, even accurate coalescing ( = 0) manages to reduce the index size to less than 38% of the original size. Index size continues to reduce on both datasets, as we increase the value of .
How does the reduction in index size affect the query re-sults? In order to evaluate this aspect, we compared the top-k results computed using a coalesced index against the ground-truth result obtained from the original index, for dif-ferent cutoff levels k . Let G k and C k be the top-k documents from the ground-truth result and from the coalesced index respectively. We used the following two measures for com-parison: (i) Relative Recall at cutoff level k ( RR @ k ), that measures the overlap between G k and C k , which ranges in [0 , 1] and is defined as (ii) Kendall X  X   X  (see [7, 14] for a detailed definition) at cut-off level k ( KT @ k ), measuring the agreement between two results in the relative order of items in G k  X  C k , with value 1 (or -1) indicating total agreement (or disagreement). Figure 3 plots, for cutoff levels 10 and 100, the mean of RR @ k and KT @ k along with 5% and 95% percentiles, for different values of the threshold starting from 0 . 01. Note that for = 0, results coincide with those obtained by the original index, and hence are omitted from the graph.
It is reassuring to see from these results that approximate temporal coalescing induces minimal disruption to the query results, since RR @ k and KT @ k are within reasonable limits. For = 0 . 01, the smallest value of in our experiments, RR @100 for WIKI is 0 . 98 indicating that the results are almost indistinguishable from those obtained through the original index. Even the relative order of these common results is quite high, as the mean KT @100 is close to 0 . 95. For the extreme value of = 0 . 5, which results in an index size of just 2 . 35% of the original, the RR @100 and KT @100 are about 0 . 8 and 0 . 6 respectively. On the relatively less dynamic UKGOV dataset (as can be seen from the  X  values above), results were even better, with high values of RR and KT seen throughout the spectrum of values for both cutoff values.
We now turn our attention towards evaluating the sub-list materialization techniques introduced in Section 6. For both datasets, we started with the coalesced index produced by a moderate threshold setting of = 0 . 10. In order to reduce the computational effort, boundaries of elementary time intervals were rounded to day granularity before com-puting the sublist materializations. However, note that the postings in the materialized sublists still retain their origi-nal timestamps. For a comparative evaluation of the four approaches  X  P opt , S opt , P G , and SB  X  we measure space and performance as follows. The required space S ( M ), as defined earlier, is equal to the total number of postings in the materialized sublists. To assess performance we com-pute the expected processing cost (EPC) for all terms in the respective query workload assuming a uniform proba-bility distribution among query time-points. We report the mean EPC, as well as the 5%-and 95%-percentile. In other words, the mean EPC reflects the expected length of the in-dex list (in terms of index postings) that needs to be scanned for a random time point and a random term from the query workload.

The S opt and P opt approaches are, by their definition, parameter-free. For the P G approach, we varied its param-eter  X  , which limits the maximal performance degradation, between 1 . 0 and 3 . 0. Analogously, for the SB approach the parameter  X  , as an upper-bound on the allowed space blowup, was varied between 1 . 0 and 3 . 0. Solutions for the SB approach were obtained running simulated annealing for R = 50 , 000 rounds.
 Table 2 lists the obtained space and performance figures. Note that EPC values are smaller on WIKI than on UK-GOV, since terms in the query workload employed for WIKI are relatively rarer in the corpus. Based on the depicted results, we make the following key observations. i) As ex-pected, P opt achieves optimal performance at the cost of an enormous space consumption. S opt , to the contrary, while consuming an optimal amount of space, provides only poor expected processing cost. The P G and SB methods, for different values of their respective parameter, produce solu-tions whose space and performance lie in between the ex-tremes that P opt and S opt represent. ii) For the P G method we see that for an acceptable performance degradation of only 10% (i.e.,  X  = 1 . 10) the required space drops by more than one order of magnitude in comparison to P opt on both datasets. iii) The SB approach achieves close-to-optimal performance on both datasets, if allowed to consume at most three times the optimal amount of space (i.e.,  X  = 3 . 0), which on our datasets still corresponds to a space reduction over P opt by more than one order of magnitude.

We also measured wall-clock times on a sample of the queries with results indicating improvements in execution time by up to a factor of 12.
In this work we have developed an efficient solution for time-travel text search over temporally versioned document collections. Experiments on two real-world datasets showed that a combination of the proposed techniques can reduce index size by up to an order of magnitude while achieving nearly optimal performance and highly accurate results.
The present work opens up many interesting questions for future research, e.g.: How can we even further improve performance by applying (and possibly extending) encoding, compression, and skipping techniques [35]?. How can we extend the approach for queries q [ t b , t e ] specifying a time interval instead of a time point? How can the described time-travel text search functionality enable or speed up text mining along the time axis (e.g., tracking sentiment changes in customer opinions)?
We are grateful to the anonymous reviewers for their valu-able comments  X  in particular to the reviewer who pointed out the opportunity for algorithmic improvements in Sec-tion 5 and Section 6.2. [1] V. N. Anh and A. Moffat. Pruned Query Evaluation [2] V. N. Anh and A. Moffat. Pruning Strategies for [3] P. G. Anick and R. A. Flynn. Versioning a Full-Text [4] R. A. Baeza-Yates and B. Ribeiro-Neto. Modern [5] K. Berberich, S. Bedathur, T. Neumann, and [6] M. H. B  X ohlen, R. T. Snodgrass, and M. D. Soo. [7] P. Boldi, M. Santini, and S. Vigna. Do Your Worst to [8] A. Z. Broder, N. Eiron, M. Fontoura, M. Herscovici, [9] C. Buckley and A. F. Lewit. Optimization of Inverted [10] M. Burrows and A. L. Hisgen. Method and Apparatus [11] S. B  X uttcher and C. L. A. Clarke. A Document-Centric [12] D. Carmel, D. Cohen, R. Fagin, E. Farchi, [13] http://www.europarchive.org. [14] R. Fagin, R. Kumar, and D. Sivakumar. Comparing [15] R. Fagin, A. Lotem, and M. Naor. Optimal [16] S. Guha, K. Shim, and J. Woo. REHIST: Relative [17] M. Hersovici, R. Lempel, and S. Yogev. Efficient [18] http://www.archive.org. [19] Y. E. Ioannidis and V. Poosala. Balancing Histogram [20] H. V. Jagadish, N. Koudas, S. Muthukrishnan, [21] E. J. Keogh, S. Chu, D. Hart, and M. J. Pazzani. An [22] S. Kirkpatrick, D. G. Jr., and M. P. Vecchi. [23] J. Kleinberg and E. Tardos. Algorithm Design . [24] U. Manber. Introduction to Algorithms: A Creative [25] K. N X rv  X ag and A. O. N. Nyb X . DyST: Dynamic and [26] J. M. Ponte and W. B. Croft. A Language Modeling [27] S. E. Robertson and S. Walker. Okapi/Keenbow at [28] B. Salzberg and V. J. Tsotras. Comparison of Access [29] M. Stack. Full Text Search of Web Archive [30] E. Terzi and P. Tsaparas. Efficient Algorithms for [31] M. Theobald, G. Weikum, and R. Schenkel. Top-k [32] http://www.wikipedia.org. [33] I. H. Witten, A. Moffat, and T. C. Bell. Managing [34] J. Zhang and T. Suel. Efficient Search in Large [35] J. Zobel and A. Moffat. Inverted Files for Text Search
