 Feature extraction is an important preprocessing step which is encountered in many areas such as data mining, pattern recognition and scientific visualization [1]. Discov-ering intrinsic data structure embedded in high dimensional data can give a low di-mensional representation preserving essential information in the original data. While Principal Component Analysis (PCA), Lin ear Discriminant Analysis (LDA) and Multi-dimensional Scaling (MDS) are traditional linear dimension reduction methods [1, 2, 3], recently nonlinear dimension reduction methods utilizing local geometric structures have been proposed [4, 5]. Isomap first c onnects paths between each data point and its neighbors and then extends them by sear ching for the shortest paths for each pair of data points [4]. Based on the constructed distance matrix, classical MDS finds low dimensional representation to preserve ge odesic distances among data points. However, Isomap does not give optimal dimension reduc tion for classification, since it does not consider class information. Also some limitations in Isomap exist in its assumption that the data is connected well enough to define low dimensional geometry. But in many real situations, for example, if the data has separated classes, a small number of neighbors will not connect classes and a large number of neighbors would fail to capture nonlinear structure in the data. MDS at the second stage of Isomap does not give an efficient way to compute low dimensional representation for a new data point. Moreover, MDS may not give optimal dimension reduction for classification.

In this paper, we propose a new approach which combines a linear dimension reduc-tion and local manifold learning through the similarity-based sparse feature representa-tion. We learn local manifolds from the neighborhood configuration. However instead of searching for shortest paths for each pair of data points, we apply a linear dimension reduction method for the similarity matrix reflecting local manifolds. Local similarity learning gives the effects of unfolding nonlinear structures in the data and a linear di-mension reduction finds a optimal transformation which maximizes similarities within each classes and minimizes similarities between classes.

The rest of the paper is organized as follows. In Section 2, a new method for similarity-based feature extraction using local manifold learning is presented. In Section 3, based on the sparse similarity matrix a linear dimension reduction method, Minimum Squared Error Solution (MSE), is applied. Experimental results in Section 4 demonstrate the per-formance of the proposed method. Throughout the paper, we assume that the data is given with known class labels and the problem is to assign a class label to new samples, i.e., the goal is classification. First a similarity matrix based on the local geometric structure in the data is constructed. When a natural similarity measure between data points is available, the most similar k neighbors for each data objects are kept as actual neighbors and relations with the other remaining points are disregarded, i.e. their similarities are set as zeros. Also a distance measure can be converted to a sim ilarity measure. As in Isomap, the distance d ij between two points a i and a j is defined as a i  X  a j if one is among the k -nearest neighbors of the other or within the -radius neighborhood, otherwise d ij =  X  . Simi-larity is defined from the distance by a converter function f as What is required for the converter function f is The conditions in (1) imply that all similar ities are nonnegative and the infinite distance is mapped to zero similarity. Also similarity is measured in inverse order of distances. For a data set A = { a 1 ,  X  X  X  ,a n } and the similarity matrix each column s i =[ s 1 i ,  X  X  X  ,s ni ] T represents the similarities between a data point a i and the others. Similarities among near by points are emphasized while connections with points resided in the far distance are disregarded. Taking the column s i as a new feature vector gives a sparse feature representation for a i .

Nearby points a i and a j which belong to the same class will share a majority of neighbors and therefore s i and s j show similar patterns. However, nearby points can belong to different classes and some points in the same class may not be nearer than the points in the different classes as in the cases of nonlinearly structured data. Hence based on the new feature representation, we perform linear dimension reduction in order to enhance similarities among the elements in the same class and decrease them between elements belonging to different classes as discussed in Section 3.

Now we discuss several properties of the proposed method addressing detailed im-plementations for the optimal values for k or , and a converter function f .There-quirements in (1) for a converter function f impose the inverse relationship between distance measure and similarity measure. As examples, these two functions can be used for a converter function, The purpose of the parameter  X  in (3) is the normalization of distance measure. Let  X  be the average of distances from each data point to the nearest neighbor. The inverse of  X  was used for  X  in our experiments. In that case, the r emaining distances are represented as a ratio of  X  .

The optimal value for k should be chosen to be large enough so that the majority in the k -neighbors of data points is the members of the same class as the given point, and at the same time it should be small enough to capture nonlinear geometric structure in the data. In our implementation, k was chosen as follows. For each data point a i ,let t i is the number of data points which have the same class labels as a i and are nearer to a i than any data points belonging to the different classes. Then k is determined as where N i is the index set of data items in the class i and n i is the number of elements in the class i . Eq. (5) computes the average number of the nearest neighbors which has same class labels as each data point. The number k chosen by Eq. (5) increases similarities among data points within each clas s and also decreases similarities of data points belonging to different classes. Cross-validation also can be used to determine the optimal values for any parameters. For the similarity matrix constructed in Section 2, any linear dimension reduction methods can be applied. In this section, we apply Minimum Squared Error Solution (MSE) [1] for the constructed sparse feature vectors.
 Letusdenoteadataset A as and the similarity vectors constructed in Section 2 as { s 1 ,  X  X  X  ,a n } , where each class i ( 1  X  i  X  r )has n i elements { a j i | 1  X  j  X  n i } and the total number of data is n = i =1 n i . Minimum Squared Error Solution (MSE) finds a set of linear discriminant which minimize the least squares error where y ji =1 if a j belongs to the class i , and 0 otherwise [1]. The MSE solution of the problem (7) can be obtained by W = P + Y ,where P + is the pseudo-inverse 1 of z is assigned to the class i if for all j = i We call this approach as sparse MSE . Since similarities are computed in a neighbor-hood, a similarity matrix S is very sparse. With a sparse similarity matrix S , computa-tions utilizing sparsity can be used to save computational complexities [7, 8]. For the experiment, letter image recognition data was downloaded from UCI Machine Leaning Repository. From the capital alphabet letters of black-and-white rectangular pixel images, 16 integer attributes were extracted [9]. The data distribution is described in Table 1. From the 26 alphabets, three data sets were composed as shown in Table 2. Each class was randomly split to the training and test sets in the ratio of 3:2 and the mean prediction accuracies by 10 times ra ndom splittin g to the training and test sets were computed as a performance measure.

Prediction accuracies by sparse MSE as w ell as LDA and MSE are shown in Table 2. In the reduced dimensional spaces by each method, the 1-NN classifier was used for classification. For sparse MSE, the converter function in (3) was used. The value k for the k -neighbors was chosen as discussed in (5) and cross-validation was used to deter-mine the optimal values for other parameters. The percentage of nonzero components of the similarity matrix S in sparse MSE is also reported in Table 2. While the similarity matrix constructed by local manifold learning contained only nonzero components of about 4 % of the total components, sparse MSE improved classification performance greatly compared with LDA and MSE.
 Note that the similarity matrix can be learned in various ways. Instead of converting Euclidean distances to similarities, similarities between data points can be defined di-rectly without using distance measures. Hence even when the data is not represented as the vector space representation, the proposed method can be applied for any similarity measures.

