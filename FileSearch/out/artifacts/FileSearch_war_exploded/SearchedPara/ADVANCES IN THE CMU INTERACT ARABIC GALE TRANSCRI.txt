 Now with Toshiba Research Europe Ltd, Cambridge, U nited Kingdom This paper describes the CMU/InterACT effort in developing an Arabic Automatic Speech Recognition (ASR) system for broadcast news and conversations within the GALE 2006 evaluation. Through the span o f 9 month in preparation for this evaluation we impro ved our system by 40% relative compared to our legacy system. These improvements have been achieved by various steps, such as developing a vowelized syste m, combining this system with a non-vowelized one, harvesting transcripts of TV shows from the web for slightly supervised training of acoustic models, as well as language model adaptation, and finally fine-tuni ng the overall ASR system. GALE, Arabic, Slightly supervised training, web dat a. The goal of the GALE (Global Autonomous Language Exploitation) program is to develop and apply compu ter software technologies to absorb, analyze and interp ret huge volumes of speech and text in multiple languag es and make them available in English. In a long run t his requires to combine techniques from text summarization, information retrieval, machine translation, and automatic speech recognition. NIS T will perform regular evaluations and the first eval uation took place recently. This paper describes improveme nts in the CMU Modern Standard Arabic (MSA) system through the span of 9 months in preparation for thi s evaluation. One of the language characteristics and challenges of Arabic is that some vowels are omitted in the writt en form. These vowels carry grammatical case informati on and may change the meaning of a word. Modeling the vowels in the pronunciation dictionary was found to give improvements over un-vowelized pronunciations [4]. In this paper we achieved another significant improvement by combining a vowelized with a non-vowelized system. Furthermore, we got gains by collecting and utilizing web transcripts from TV sh ow, which include broadcast conversations. Our MSA speech recognition system is based on the Janus Recognition Toolkit JRTk [9] and the IBIS decoder [10]. Before decoding the audio, an automatic segmentatio n step and a speaker clustering step is performed. Th e segmentation step aims at excluding those segments that contain no speech, such as music or background nois e. The remaining segments are clustered into speaker clusters such that all adaptation and normalization steps can be processed on clusters as batches. From the incoming 16 kHz audio signal we extract fo r each segment power spectral features using a FFT wi th a 10ms frame-shift and a 16ms Hamming window. From these we compute 13 Mel-Frequency Cepstral Coefficients (MFCC) per frame and perform a cepstra l mean as well as variance normalization on a cluster basis. To incorporate dynamic features we concatena te 15 adjacent MFCC frames ( X 7) and project these 195 dimensional features into a 42 dimensional space us ing a transform found by linear discriminate analysis (LDA). We use the context-dependent codebooks as classes for finding the LDA transform [2]. On top o f the LDA we apply a single maximum likelihood trained Semi-Tied-Covariance (STC) matrix. The general decoding setup employs a first pass in which a speaker independent acoustic model without vocal tract length normalization (VTLN) and no adaptation is used. The hypotheses of a cluster fro m the first pass are then used to estimate the VTLN warpi ng factors to warp the power spectrum using the maximu m likelihood approach described in [8]. After the VTL N factors are found, the same hypotheses are consider ed to estimate a feature space adaptation (FSA) using a constrained MLLR (CMLLR) transform. Then a model space adaptation is performed using maximum likelihood linear regression with multiple regressi on classes. The regression classes are found through clustering of the Gaussians in the acoustic model. The second pass decoding uses a speaker adaptive traine d acoustic model, in which the adaptation was perform ed using a single CMLLR transform per speaker. For the non-vowelized system, we applied a grapheme -to-phoneme approach to automatically generate the pronunciation dictionary. For the vowelized system we used the same phoneme set as in the non-vowelized system but extended it with the 3 short vowels, whi ch do not appear in the writing system. Both systems a re 2-pass system as described above and employ Cepstral Mean Normalization (CMN), MLLR, Semi-tied covariance (STC), and Feature space adaptation (FSA ). For the development of context dependent acoustic models we applied an entropy-based polyphone decision tree clustering process using context ques tions of maximum width  X 2, resulting in shared quin-phone s. In addition we included word-boundary tags into the pronunciation dictionary, which can be asked for in the decision tree can ask for word-boundary tags. The n on-vowelized system uses 4000 phonetically-tied quin-phones with a total of 305,000 Gaussians. The non-vowelized system has 5000 codebooks with a total of 308,000 Gaussians. These consist of 40 hours Broadcast news (BN) from manually transcribed FBIS data, 50 hours BN LDC-TDT4 selected from 85 hours using a slightly supervised approach as described in [3], and 30 hou rs Broadcast conversation (BC) recorded from Al-jazeer a TV, and 70 hours (40hrs BN, 30hrs BC) from LDC-GALE data. For quality reasons we removed some of the most recent GALE data from acoustic model training. The Arabic Giga word corpus distributed by LDC is currently the major Arabic text resource for langua ge modeling. Since this corpus only covers broadcast n ews, we spidered the web to cover broadcast conversation al data. We found transcripts for Arabic talk shows on the Al-jazeera web site www.al-jazeera.net and collected all data available from 1998 to 2005. We excluded all material from 2006 to comply the evaluation rules which prohibit the use of any data starting Februar y 2006. In addition to the mentioned data we collecte d BN data from the following source: Al-Akhbar (Egyptian daily newspaper 08/2000 to 12/2005) and Akhbar Elyom (Egyptian weekly newspaper 08/2000 to 12/2005). Furthermore, we used unsupervised trainin g transcripts from 750 hours BN created and shared by IBM. For language modeling building we used the SRILM tool kit from SRI [5]. Since we have 2 kinds of dat a, Broadcast News and Conversation, we built various individual 4-grams language models. 11 models were then interpolated to create one language model. The interpolation weights were selected based on a held out data set from BN and BC sources. We found that the data from Al-jazeera (both BN &amp; BC) has the highest weight comparing to other sources. The resulting fi nal language model uses a total number of n-grams is 12 6M and a vocabulary of 219k words. The perplexity of t he language model is 212 on a test set containing BC a nd BN data. Most of our acoustic and language model training da ta comes from broadcast news. However, since GALE targets broadcast news as well as conversations we looked for an effective method to increase the trai ning data for Arabic BC. We made use of the fact that so me Arabic TV stations place transcripts for their prog ram on the web. These transcripts lack time stamp but include acceptable quality of the transcription. However, one challenge is that the transcriptions a re not complete in that they do not include transcripts of commercials or any news break that may interrupt th e show. In total we recorded 50 hours of Broadcast conversation shows from Al-jazeera and used them in our acoustic model and language model training by performing the following procedures:  X  We manually selected shows from Al-jazeera TV  X  We used a scheduler to automatically start the  X  We spidered the web to collect corresponding show  X  We automatically processed the transcripts to  X  We added these shows to our LM data with high  X  We aligned the reference (transcripts without time  X  We selected only the portions that are correct; we  X  Based on the above criteria we finally selected 30  X  We clustered utterances based on BIC criteria As a result, we managed to project the time stamp i n the original transcript such that it can be used for tr aining. Using these 30 hours of data resulted in a 7% relat ive improvement on RT04. Since RT04 is broadcast news, we expect even higher gains on broadcast conversational data. It is worth mentioning that we applied the same slightly supervised approach to th e TDT4 data which is a low quality quick transcriptio n. We selected 50 out of 80 hours and achieved an improvement of 12% relative. The gain was higher since at the time of these experiments we had only 40 hours of training from FBIS data, therefore more th an doubled the amount of training data by adding TDT4. Arabic spelling is mostly phonemic; there is a clos e letter-to-sound correspondence. We used a grapheme-to-phoneme approach similar to [1]. Our phoneme set contains 37 phonemes plus three special phonemes fo r silence, non-speech events, and non-verbal effects, such as hesitation. We preprocessed the text by mapping the 3 shapes o f the grapheme for glottal stops to one shape at the beginning of the word since these are frequently mi ss-transcribed. This preprocessing step leads to 20% reduction in perplexity of our language model and 0 .9% improvements in the final WER performance on RT04. Preprocessing of this kind appears to be appropriat e since the target of the project is not transcriptio n but speech translation and the translation community applies the same pre-processing. We used a vocabula ry of 220K words selected by including all words appearing in the acoustic transcripts and the most frequent words occurring in the LM. The OOV rate is 1.7% on RT04. Table 1 shows the performance of our Speaker-Independent (SI) and Speaker-Adaptive (SA) non-vowelized system on the RT04 set. Table 1: Non-vowelized System Results System WER on RT04 (%) Non-Vowelized (SI) 25.3 Non-Vowelized (SA) 20.8 Written MSA lacks vowels, thus native speakers add them during reading. Vowels are written only in children books or traditional religious books. To r estore vowels for a 129K vocabulary [4], we performed the following steps:  X  Buckwalter morphological analyzer (BMA) (found  X  If a word is not vowelized by the analyzer, we  X  If the word did not appear in any of those, we used In total 11k entries could not be resolved by eithe r the BMA or the Treebank. This vowelization step resulted in 559,035 pronunciations for the 129k words in our vocabulary , i.e. we have on average 5 pronunciations per word. To reduce the number of pronunciation variants we performed a forced alignment and excluded pronunciations which did not occur in the training corpus. This results in 407,754 pronunciations, whi ch is a relative reduction of about 27%. For system train ing we used the same vocabulary and applied the same training procedure as in the non-vowelized system f or acoustic model training. As Table 2 shows, we achieved a very good gain of 1.3% absolute on the SI pass and 1.5% on the SA pas s, both benchmarked on RT04 (compare Table 1). We envision to seeing even higher improvements after estimating and applying probability priors to multi ple pronunciation and after vowelizing the remainder 11 k words that had not been covered by BMA or the Tree-Bank. Table2: Vowelized System Results System WER on RT04 (%) Vowelized (SI) 24.0 
Vowelized (SA) 19.3 After seeing significant improvements by vowelizati on, we investigated the performance gain through cross-adapting the vowelized system with the non-vowelize d system. The vowelized system cross adapted with the SA non-vowelized gave us 1.3 over the vowelized system adapted on the SI vowelized system. We used a 3-pass decoding strategy, in which the first pass u ses the speaker independent (SI) vowelized system, the seco nd pass uses the speaker adaptive (SA) non-vowelized system, and the third, final pass, uses the speaker adaptive vowelized system. Some challenges for the cross-adaptations had to be overcomed, for instance to cross adapt the non-vowelized system on the voweliz ed system, we had to remove the vowels to have a non-vowelized transcript. Since the phoneme set of the non-vowelized system is a subset of the phoneme set of the vowelized system, we could simply exclude the vowel phonemes from the vowelized system. Furthermore, t he search vocabulary is the same and so is the languag e model. The main changes are the pronunciation dictionary a nd the decision tree. We tried different combination schemes, e.g. by starting with the non-vowelized system, then the vowelized, and then the non-voweli zed but found that none outperforms the combination reported here in terms of WER. In addition starting with the non-vowelized SI pass is much faster than the vowelized SI system (4.5RT compared to 9RT). Table 3: Non-vowelized &amp; vowelized System Combination 9. ACOUSTIC MODEL PARAMETER TUNING We started our legacy system with 40 hours and unti l it reached 90 hours we were using the same number of codebooks (3000) and same number of Gaussians (64) per codebook. With the increase of training data fr om 90 hours to 190 hours we investigated the effect of increasing the number of codebooks and Gaussians. Also, we were using merge and split training (MAS) and STC only for the adapted pass; we furthermore investigated the effect of using it for the SI pass . We found that using MAS &amp; STC on the SI pass gave us a gain of 5% relative on the SI pass. In addition we found that the ideal number of codebooks is 5000 for the non-vowelized system resulting in a gain of 5.3% relati ve on the SI pass. We expect to see further gains on the SA pass. Table 4 summarizes the system performance usi ng different parameter sizes and training schemes. Table 4: System Performance vs.Model Size Table 5 shows the gains we achieved at major milest one stages while building the system. The key improveme nts are due to adding data collected from the web, Vowelization, and combining the vowelized and non-vowelized systems. Tuning the acoustic models parameters gave us a good gain and finally the interpolation of different language model for diffe rent sources gave additional improvements. The real-time behavior of the system improved from 20RT to 10 RT while loosing only 0.2% which is in acceptable trad e-off. Recently, we gained 3.5% relative applying discriminative training (MMIE). We presented the CMU 2006 GALE ASR Arabic system. It can be seen that we achieved 40% improvements over our legacy system. Table 5: System Progress WER (%) LEGACY SYSTEM 32.7 STC+VTLN 30.1 SPEED FROM 20RT TO 10RT 30.3 FROM 3 TO 4GM+BETTER SEGMENTATION TDT4 TRANSCIPTS SELECTION REFINEMENT CLUSTERING REFINEMENT &amp; RETRAINING MORE LM DATA +INTERPOLATING 11 LMS 24.2 ADDITION Q3 OF LDC DATA 23.6 ACOUSTIC MODEL PARAMETER TUNING 20.7 MMIE 20.0 COMBINED SYSTEMS (VOW+NON-VOW) 18.3 We combined a vowelized and a non-vowelized system and achieved 4.0% relative over the vowelized syste m. Also, we managed to use TV web transcript as a meth od to cover the shortage of training data specially th e broadcast conversation. Currently, we are exploring more on the vowelized system by adding weights to different multiple pronunciations and adding vowelization to words not covered by the morphologi cal analyzer or the tree-bank. We also would like to thank Qin Jin for applying he r automatic clustering techniques to the web data. 
