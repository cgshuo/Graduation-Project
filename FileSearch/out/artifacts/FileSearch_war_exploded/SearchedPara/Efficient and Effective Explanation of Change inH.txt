 Dimension attributes in data warehouses are typically hierarchical (e.g., geographic locations in sales data, URLs in Web traffic logs). OLAP tools are used to summarize the measure attributes (e.g., to-tal sales) along a dimension hierar chy, and to characterize changes (e.g., trends and anomalies) in a hierarchical summary over time. When the number of changes identified is large (e.g., total sales in many stores differed from their expected values), a parsimonious explanation of the most significant changes is desirable. In this pa-per, we propose a natural model of parsimonious explanation, as a composition of node weights along the root-to-leaf paths in a di-mension hierarchy, which permits changes to be aggregated with maximal generalization along the dimension hierarchy. We formal-ize this model of explaining changes in hierarchical summaries and investigate the problem of identifying optimally parsimonious ex-planations on arbitrary rooted one dimensional tree hierarchies. We show that such explanations can be computed efficiently in time es-sentially proportional to the number of leaves and the depth of the hierarchy. Further, our method can produce parsimonious explana-tions from the output of any statistical model that provides predic-tions and confidence intervals, making it widely applicable. Our experiments use real data sets to demonstrate the utility and robust-ness of our proposed model for explaining significant changes, as well as its superior parsimony compared to alternatives. H.2.8 [ Database Management ]: Database Applications X  X atamining; F.2.m [ Analysis of Algorithms and Complexity ]: Miscellaneous Algorithms,Experimentation,Theory, Performance OLAP, hierarchical summary, change, parsimonious explanations, statistical model
Dimension attributes in data warehouses are typically hierarchi-cal, and a variety of OLAP applications (such as point-of-sales analysis and decision support) call for summarizing the measure attributes in fact tables along the hierarchies of these attributes. For example, the total sales at different WalMart stores can be summarized hierarchically by geographic location (e.g., state/city/ zip_code/store), by time (e.g., year/month/day/hour), or by product category (e.g., clothing/outerwear/jackets/brand). Existing OLAP tools help to summarize and navigate the data at different levels of aggregation (e.g., jackets sold in each state during December 2006) via drill-down and roll-up operators. OLAP tools are also used to characterize changes in these hierarchical summaries over time (e.g., the sales in December 2006 compared to their expecta-tions over different locations), to detect anomalies and characterize trends. When the number of changes identified is large (e.g., the total sales at many locations differed significantly from their ex-pectations), one seeks explanations.

Explanations may be verbose (e.g., a separate ad hoc explanation for the observed change at each location) or parsimonious (e.g., a single explanation for multiple observed changes, such as attribut-ing a drop in sales at a large number of locations in Louisiana in 2005 to Hurricane Katrina). Parsimonious explanations are ob-viously more desirable and more effective than (ad hoc) verbose explanations. In this paper, we are interested in parsimonious ex-planations of changes in measure attributes (e.g., total sales) ag-gregated along an associated dimension attribute hierarchy (e.g., location).

Existing work has addressed the issue of explaining change be-tween OLAP aggregates in terms of subaggregates [21] (we de-scribe in more detail how this work differs from ours in Section 2), but these changes were expressed as outliers of point-to-point sub-aggregate comparisons. We seek a more holistic explanation. We propose a natural model that makes effective use of the dimension hierarchy and describes changes at the leaf nodes of the hierarchy (e.g., individual stores in the location hierarchy) as a composition of  X  X ode weights X  along each node X  X  root-to-leaf path in the dimen-sion hierarchy  X  each node weight constitutes an explanatory term. For example, overall sales in stores in California increased by a factor of three; sales in San Jose stores further grew by a factor of two (a six-fold total increase), whereas sales in Los Angeles stores increased less than the statewide increase by half (so a total growth factor of 1.5). Formally, we assume that the dimension hierarchy remains fixed over time, and each data item (e.g., a record in a fact table) has a timestamp and is associated with a leaf node (e.g., an individual store) of the hierarchy. A hierarchical summary or snap-shot (over some time interval) then associates with each node in the dimension hierarchy (e.g., store, zip_code, city, state) the ag-gregated value of the measure attribute (e.g., total sales) of all data items (with a timestamp in that time interval) in its subtree.
If we consider two snapshots, it is clear that the changes between the trees can be expressed over the different levels of the dimension hierarchy in numerous possible ways. For example, if the sales at all California stores increased four-fold, we can model this change (among other possibilities) as a weight of four for each individual store, or a weight of four at the California state level, or as a weight of two at the California state level and a weight of two for each store. The important question is, what are the nodes in the hierarchy that explain the (most significant) changes parsimoniously.
A straightforward and intuitive attempt at identification of par-simonious explanations is a top-down approach. Starting from the roots of the two snapshots, compare aggregate values of the mea-sure attributes at corresponding nodes. If the difference between the aggregates is completely  X  X xplained X  by the composition of node weights along the path from the root to the parent of that node, no additional node weight (o r explanatory term) is needed at that node. Otherwise, the node weight is set appropriately to the differential value with respect to the composition of weights along nodes for ancestor path from the root to that node. While straight-forward and intuitive, such an explanation can be easily shown to not be optimally parsimonious. For example, if 1 out of 5 stores in Los Angeles that all used to have the same sales exhibited a 4-fold increase in sales, while the other 4 exhibited no change in sales, a top-down explanation would attribute a 1.6-fold (8/5) increase at the Los Angeles city level, and would then have to have additional explanations at each store to explain the differences with the city-level explanation -thus needing 6 explanatory terms. An optimally parsimonious explanation, on the other hand, needs only 1 explana-tory term -a 4-fold increase at the anomalous store. This explana-tion is parsimonious in the sense that changes are aggregated with maximal generalization along the dimension hierarchy.

We envision that in many practical cases the user may want to compare a snapshot of the hierarchy with the values predicted by a model. Such an operation would be particularly useful for example when validating a forecasting mode l, or to identify conditions that are not properly modeled or to provide parsimonious explanation of anomalies that are expected to be related through the hierarchi-cal structure. In this scenario, the use of statistical modeling would provide an expected value for each leaf of the hierarchy, with asso-ciated confidence intervals. Our proposed method can provide par-simonious explanation after incorporating uncertainty in the fore-casts, quantified through confidence intervals.
We summarize our contributions as follows: The rest of this paper is structured as follows. In Section 2, we discusss related work. In Section 3, we formalize the variants of our problem of parsimonious explanation. Algorithms, along with their proofs of correctness and complexity analysis, are presented in Section 4. Experimental results along with description of the statistical models used are given in Section 5.
Hierarchies on data attributes have played a significant role in data warehouses, for which database operators such as the dat-acube have been developed to summarize and navigate the data at the different levels of aggregation [6]. In the data mining litera-ture, several tools have been proposed for summarizing hierarchi-cal data at a single time instance, including GMDL regions [16], Icecubes [10], and Hierarchical Heavy Hitters (HHH) [9, 7].
With respect to detecting changes in data, recent approaches in-clude velocity density estimation [1] for visualizing change, win-dowed statistical testing [14] for detecting distributional changes, and histogram differencing [8] for identifying items which exhibit the largest changes in frequencies. However, these papers deal with flat (non-hierarchical ) data. There have been a few papers explicitly dealing with hierarchical data. Zhang et al. studied change detec-tion of (aggregated) time series corresponding to HHH IP prefixes in the IP address hierarchy [26]. Chawathe et al. studied the prob-lem of change detection on semi-structured data, but for topological changes [23].
 The problem of path explorations of hierarchies was studied in [22]. Here the user defines a set of linear constraints and the values in the datacube cells are predicted using the Maximum Entropy Principle. Given a supplied model, the technique finds the cells that are sig-nificantly different values from the expected values. Our problem is essentially the opposite: to find the best model that explains the changes. There is also some marginally related work on identify-ing bursts in hierarchical time series data, that is, the time intervals tightly capturing high arrival frequencies [27, 15].

Most related to our work is the DIFF operator for explaining dif-ferences in the datacube [21]. In their problem, a user selects two aggregates at the same level in the datacube which fixes some of the dimensions. The ratio between the selected aggregates is then ex-plained in terms of the free dimensions, and subaggregates having deviating ratios explained recursively. This puts constraints on the intermediate node ratios, whereas our solution has the freedom to explain leaf aggregate changes in terms of intermediate node ratios, and is thus more parsimonious. In the example involving the Los Angeles stores mentioned earlier, the need to additionally  X  X xplain X  the ratio of 8/5 at the Los Angeles city level internal node results in a verbose explanation using the DIFF approach. To demonstrate this verbose behavior of DIFF on real data, we shall experimentally compare against the DIFF operator in Section 5.

The problem of using compact hierarchical histograms for ap-proximating leaf-level data was studied in [20] which employed a predefined hierarchy like our approach but solved the dual prob-lem: given a bound on the size of the synopsis (i.e., the number of explanations), find the synopsis that minimizes the error. Fur-ther, the paper considered three different partitioning functions and solved via dynamic programming to reduce distributive error met-rics given a space bound. Their LPM variant is the same problem studied in [21] but solved heuristically due to the expensive cost of distribute error metrics; the other partitioning functions find in-ferior solutions to LPM. Our work is based on the initial problem formulation presented in [4].
Recently [18] investigated a problem similar to our work. The proposed solution used the Haar wavelet representation to construct dataset synopses of minimum space. The use of the wavelet rep-resentation restricts this approach to less efficient (i.e., less parsi-monious) explanations than our hierarchical parsimonious explana-tions. Another problem relevant to ours (for the case of binary hier-archies) is Haar wavelet compression with maximum-error metrics, introduced in [17]. The best current solution requires O ( n and O ( n ) space to solve the dual problem [11] and, just as in [21], constraints are imposed at all nodes rather than just at the leaves, leading to less parsimonious solutions. [12] introduced the notion of unrestricted Haar wavelets and [13] defined the Haar+ tree as an improvement, but these exploit discretization of values and there-fore are not comparable with our approach which allows for any (potentially infinite sized) domain. However [13] is equivalent to the model given by [20] when the hierarchy is restricted to binary trees. [13] presents provably good approximate algorithms to solve this problem in O ( R 2 n log n log 2 B ) or O ( R 2 n log is the size of the input, B the maximum number of coefficients in the synopsis and R the number of the examined values per coeffi-cient), for general error metrics. Interestingly, our problem (with binary hierarchies) offers an alternative to Haar wavelet compres-sion, yielding better answers with smaller complexity: O ( n log n ) for the primal problem and O ( n log n log  X  ) for the dual. An al-gorithm that solves the dual problem (as in [20] or [13]) can be modified to solve the primal problem using a binary search proce-dure on B . Thus, these algorithms would need to run an additional log B factor slower if modified to solve our problem.
In this section, we first define a natural change explanation model, which expresses the change between the leaf nodes of two hierar-chical summaries as a composition of changes top-down from the root to the leaves of the tree. 1 We then discuss the model in the context of Occam X  X  Razor to find a parsimonious explanation of change. Let S be a set of items from a domain D where the ele-ments come from a well-defined hierarchy. Each item i  X  S has an associated measure value v  X  V . The ordered pairs ( i, v ) could have been obtained by summing over the (projected) columns in a data warehouse fact tabl e containing a multiset of (itemID, value) pairs where itemID is a dimension attribute and value is a measure attribute. Or they could have been aggregated over some time series window (eg, moving window average). Let T be a rooted tree obtained by inducing the dimension hierarchy on S , where the nodes correspond to different prefixes in the dimension hierarchy. We do not assume a total ordering over the dimension hierarchy, only that it is partially ordered with maximum height h . Let denote a leaf node and m ( ) denote some value attached to the leaf node . Given values attached to leaf nodes that represent some measure of change, we define a class of hierarchical change explanation models below.

D EFINITION 3.1. Hierarchical Change Explanation: Given
Our method works for both multiplicative and additive compo-sitions by transforming the former to latter using logarithms; we illustrate using the additive scale. ahierarchy T and change values m ( ) attached to leaves ,ahier-archical change explanation model is a complete, top-down compo-sition of changes ( X  X eights X ) w ( n ) between nodes along the root-to-leaf path, for each leaf node.
 More formally, for each leaf node , where for tree nodes n where p ( n ) is the parent node of n . A solution to this system gives weights w ( n ) for each n . In fact, if denotes the ancestor path from the root down to a tree node n , then by unraveling Equations 1-3, our problem is to find weights w ( n ) of each node n in the tree subject to the constraints m ( )= P n  X  X  ( ) w ( n ) . Since this system of equations is under-specified, there are multiple solutions each of which provides a hierarchical change explanation.

In general, the change values m ( ) are obtained as some dis-crepancy measure d ( m 1 ( ) ,m 2 ( )) between two sets of values ob-served for the hierarchy T . For example, consider the Census dataset [5] where we have population counts m 1 ( . ) and m zip codes and a geographical hierarchy that defines aggregations at state, county and city levels at two different snapshots T T 2 as exemplified in Figures 1(a) and (b). Here, 3D m ( )= detection problems, m 1 ( ) is forecasted value based on some sta-tistical model that captures normative behavior and m 2 ( ) is the actual observed value with higher discrepancy being indicative of anomalous behavior.
Definition 3.1 provides a rich class of hiearchical change expla-nation models; we provide a couple of examples that are trivial to compute but sub-optimal and then provide a notion of an optimal or parsimonious hierarchical change explanation model.

One possible assignment of weights that is used in anomaly de-tection applications is the one that completely ignores the hierarchi-cal structure and assigns each leaf node in T a weight of m ( ) , and 0 to the non-leaf nodes. We call this the  X  X on-hierarchical X  model, comparison w.r.t this model helps in quantifying the gain achieved by using the hierarchy. Figure 1(c) shows a non-hierarchical assignment for the trees T 1 and T 2 shown in Figures 1(a) and (b), respectively. The leaf-level nodes encircled boldly have non-zero assigned weights. Using trees T 1 and T 2 , we construct a third tree as in Figure 1(c) such that the value associated with a leaf is log of the ratio of the correspoding leaf counts. Assuming the existence of a rollup operator that aggregates values of children to the parent, another possible assignment is top-down, which re-cursively assigns weights from the root down such that m ( )= W ( n )= Figure 2(a) provides an example where values at each snapshot are rolled up using the sum operator. Both of these assignments satisfy the equations of hierarchical cha nge explanation but are not neces-sarily parsimonious: the former ignores all opport unities to group leaves with equal differences in the same subtree whereas the latter is too greedy in that it groups unequal leaf differences.
A node weight w ( n )=0 implies no change to node n relative to p ( n ) and does not need to be reported in an explanation. Thus, the explanation size is the number of non-zero weights in the expla-nation. Applying Occam X  X  Razor, we prefer an explanation of the smallest size. Therefore, we define a parsimonious explanation of hierarchical change as one with the smallest explanation size, that is, the minimum number of weights not equal to zero. Consider Figure 2(b), which is able to explain the changes using only 2 non-zero weights compared to 3 for the non-hierarchical strategy and 7 for the top-down one; in fact, it is optimal. We describe the algo-rithms which lead to assignments in Figure 2(b)-(c) in Section 4.
However, this explanation model has certain shortcomings. Of-ten one wants to compare a snapshot with expected values; large deviations from these values can be reported as anomalies. Statis-tical forecasting models (e.g., based on moving averages) typically yield confidence intervals based on a supplied confidence level. An important shortcoming of the current model is that it does not work with such a forecasting model because its formulation does not deal with ranges of possible values. In addition, the model is sensitive to noise. Intuitively, we would like to capture similar changes among related leaves which may not have exactly equal differences but are roughly the same. For example, if two sibling leaves have differ-ences of 1.98 and 2.02, we may wish to describe this at the parent using a difference of 2. Since the deviations from this description at the leaves are small (1%), we may tolerate this error as being a good enough approximation to report only significant changes and to avoid overfitting the data. Our original description above, which only allows exact matches, does not allow this.

In order to ameliorate this, we extend the definition to allow a tol-erance parameter on the values of the leaves. We allow weights on the nodes that result in differences of at most between two leaves in the snapshots. We assume that in practice this tolerance parameter will be provided by the confidence interval of the pre-diction model, which can be different from leaf to leaf, so the model allows different tolerances ( ) at each . Specifically, we assign weights such that | m ( )  X  W ( ) | X  ( ) for each ,where W ( )=
To see the connection with a forecasting model, we assume m ( )= d ( m 1 ( ) ,m 2 ( )) = m 2 ( )  X  m 1 ( ) . In fact, rewriting this equa-tion m 2 ( )  X  ( m 1 ( )+ ( ))  X  W ( )  X  m 2 ( )  X  ( m 1 ( ) and denoting m 1 ( )+ ( ) and m 1 ( )  X  ( ) by UB ( ) and LB ( ) , respectively, clearly shows how to use output from a forecasting model in our framework. LB ( ) and UB ( ) are lower and up-per confidence bounds that are obtained from the estimated fore-casting distribution. One possibility which works for symmetric distributions is to choose m 1 ( ) as the predicted mean and ( ) to be proportional to the predicted standard deviation, the constant of proportionality depending on th e desired coverage of the con-fidence interval. For instance, a choice of 1 . 96 under a Gaussian assumption on the statistical distribution of our node values guar-antees 95% coverage. In general, our method is agnostic to the particular choice of forecasting model; the only requirement is the availability of LB ( ) and UB ( ) . This makes it a highly general purpose method with wide applicability in anomaly detection prob-lems involving hierarchical data where changes are expected to be spatially clustered in subregions of the hierarchy.
We now define our parsimonious explanation model, which al-locates a tolerance budget along each path that can be distributed among the individual path nodes in any fashion while maintaining the constraint | m ( )  X  D EFINITION 3.2. Hierarchical Parsimonious explanation: Given a set of leaf changes m ( ) and a tolerance budget ( ) on the total sum of weights along the path to for all leaves ,a hierarchical parsimonious explanation of change finds the smallest explanation size, that is, minimum number of node weights w ( n ) s.t. w ( n )  X  [  X  k, k ] ; k  X  0 .
 In definition 3.2, for positive tolerances, only k =0 is of interest to us in practice. However, to facilitate comparison with DIFF al-gorithm, extended definition which allows thresholding on positive values of k is necessary as we shall see later in section 5.
In this section, we describe an algorithm to compute optimal weight assignments (that is, minimizing the explanation size), for the problem defined in Section 3 (Definition 3.2). The algorithm presented here generalizes this problem by allowing any supplied error tolerances for the leaves as well as intermediate nodes of the hierarchy.

The problem with =0 is a special case of the following prob-lem: Given real matrix A and vector b , find x such that Ax = b minimizing the number of non-zero x i  X  X . That problem is not only NP-hard, but is not approximable within 2 log 1  X   X  n  X &gt; 0 (in polynomial time, assuming NP is not contained in quasi-polynomial time) [2, 3]. For the special case studied here we give a fast and exact algorithm. We first describe the algorithm intuitively, and then present it formally.

The algorithm makes two passes over the tree: the first bottom-up and the second top-down. In the first pass the algorithm com-putes a tentative set of  X  X est X  incoming partial sums for each node, using dynamic programming. We prove that the best partial sums for a node are those that allow the node to incur no cost for its own weight and, simultaneously, to provide best partial sums for the maximum number of children. This set of best partial sums for a node is a union of closed intervals, at most one for each leaf.
In the second pass the algorithm works down from the root to assign weights. Each node chooses its own weight so as to benefit the maximum number of its children. If the incoming partial sum for a node is one of the best for the node, it can do this without incurring a cost at the node. Otherwise, the node incurs a cost of 1 for its own weight, which it chooses to benefit the maximum number of its children. We illustrate this process in Fig. 3 for the case =1 .

Note that instead of taking k&gt; 0 , one may add k |P ( ) ( ) ,thentake k =0 . This expands the set of feasible weightings. We present the algorithm for the general case k&gt; 0 for compati-bility with [21], which we shall compare against in Section 5.
D EFINITION 4.1. For any subtree T and real value x , define cost( x, T ) to be the minimum cost of any feasible labeling of T , given that the partial sum coming into the root of T from above is x . (Formally, this is the minimum cost of any feasible labeling of the tree T in isolation, where each leaf change m ( ) has been decreased by x .)
Define bestcost( T )=min x cost( x, T ) and bestsums( T )= { x :cost( x, T ) = bestcost( T ) } .

We start with the observation that a bad incoming partial sum increases the cost of T by only 1.
 Let [ x  X  S ] denote 0 if x  X  S and 1 otherwise.

L EMMA 4.2. For any subtree T and real x , cost( x, T )= bestcost( T )+[ x  X  bestsums( T )]
P ROOF . Since the costs are integers, it X  X  enough to prove that cost( x, T )  X  bestcost( T )+[ x  X  bestsums( T )] .

Let x be a partial sum achieving bestcost( T ) ,andlet w be a corresponding min-cost weighting of T for partial sum x . Adding x  X  x to the weight of the root of T gives a feasible weighting for T with partial sum x , and increases the cost of w by at most 1.
Next we prove a recurrence which will be the basis for the algo-rithm. Let A  X  B denote { a + b : a  X  A, b  X  B } .
 T HEOREM 4.3. Let T be any subtree with immediate subtrees T ,T 2 ,...,T c .Then bestsums( T ) equals
P ROOF .Let kiddiff( z, T ) denote |{ i : z  X  bestsums( T Let bestkiddiff( T ) denote min z kiddiff( z, T ) .Fix x and T .By definition, cost( x, T ) equals ( y is the weight given to the root of T ). By lemma 4.2, this is `
X The term on the left is independent of x , while the min y the right will equal bestkiddiff( T ) for some x (e.g. when y =0 and x minimizes kiddiff( x, T ) ).

Thus, x  X  bestsums( T ) (that is, x minimizes cost( x, T ) )iff Taking z = x + y , this condition is equivalent to
Theorem (4.3) gives a recurrence relation for bestsums() .Us-ing this recurrence, computeDS ( T, d ) (Algorithm 1) uses dynamic programming to compute, for all subtrees T , bestsums( T ) and kidopt( T )= Algorithm 1 computeDS(subtree T , leaf values m ) 1: If T is a leaf (a single node ): 2: let kidopt( T )  X  X  m ( ) } X  [  X  ( ) , ( )] 3: else: 4: for each subtree T i of T : computeDS ( T i ,m ) 5: kidopt( T )  X  X  z minimizing |{ i : x  X  bestsums( T i ) 6: bestsums( T )  X  kidopt( T )  X  [  X  k, k ]
The algorithm first calls computeDS ( T, m ) to compute kidopt( T ) and bestsums( T ) for all subtrees T . It then weights T by calling weightTree (0 ,T ) (Algorithm 2).
 Algorithm 2 weightTree(partial sum x , subtree T ) 1: if x  X  bestsums( T ) : 2: pick y  X  [  X  k, k ] s.t. x + y  X  kidopt( T ) 3: else: let y  X  x  X  x for any x  X  kidopt( T ) 4: give the root of T weight y 5: for each subtree T i of T : weightTree ( x + y, T i ) L EMMA 4.4. weightTree ( x, T ) finds a feasible weighting of T (assuming incoming partial sum x ) of optimal cost cost( x, T ) .
P ROOF . From Theorem 4.3, computeDS correctly computes kidopt and bestsums . Theorem 4.3 assures that y exists in the second line of weightTree(). A standard proof by induction shows that the weighting is feasible. To finish we consider the cost. By in-spection weightTree ( x, T ) chooses a root weight y so x + y kidopt( T ) . Thus (assuming by induction that the subtrees are weighted optimally), the total weight for nodes in the subtrees is bestcost( T ) . In addition, at the root we pay [ y  X  [ inspection of weightTree(), this equals [ x  X  bestsums( T )] . Thus, the total cost of our weighting is bestcost( T )+[ x  X  bestsums( T )] . By Lemma 4.2, this is best possible.

L EMMA 4.5. The running time of the algorithm is O ( hN log N ) , where h is the height of the tree and N is the number of leaves.
P ROOF . (Sketch) The running time of the algorithm is domi-nated by the time it takes to compute the optimal shift for each node. We note that the total size of the optimal shifts for a node is bounded by the number of leaves in the subtree rooted at that node. Computing the optimal shifts of a parent node from the labeling of its children nodes requires sorting and merging of the children node labels.

For any tree, assume that at depth d there are c ( d ) nodes. A node v at depth d will have lv ( v i ) leaves in its subtree, but N where N is the number of leaves. This bounds the size of the labeling at that node. The cost of sorting and merging M nodes is M log M . So the total processing time at level d is given by Hence, total time of processing over the entire tree is
In this section, we investigate both the effectiveness and the effi-ciency of the proposed algorithms and the outputs they generate us-ing real data. We evaluate the effectiveness of our proposed change detection model according to its ability to capture interesting hier-archical changes as well as the robustness and stability of the output under small perturbations of error tolerance.
We define stability to measure the sensitivity of the set of expla-nation weights as a function of confidence level c .Let S of nodes at level l where  X  X xplanations X  occur. Then the stability of the output at level l , given a change in tolerance parameter from c the previous value of c .

We used the following two real data sets: Census, which gives population counts for a geographical hierarchy given by state/county/ city/zip_code [5]; and WorldCup, which is a Web log over a dura-tion of several months of URL accesses to files having a maximum path length of 7 [25]. Note that the hierarchy induced by the URL file paths are not homogeneous, that is, the nodes have different fanouts and the paths have different depths. The Census data has approximately 81,000 leaf nodes and 130,000 total nodes in the tree. The maximum height of the tree is 5 (including the root which stands for the whole country). The World Cup datasets have about 4300 leaf nodes and around 4500 total number of nodes. In the non-homogeneous World Cup datasets, the maximum height of the tree is 8 including the root.

All experiments were run on a Pentium(R) machine with 4 CPU and clock speed 2.66GHz.
We provide a description of the models that were used in our ex-perimental evaluation on real data. We do not claim any novelty here and use the popular exponentially weighted moving average (EWMA) for both our datasets. In our analysis, we assume a Gaus-sian distribution for the node values. Although this may not be a reasonable assumption for count data on the original scale, it is of-ten a good approximation on a transformed scale (log and squared-root are widely used for count data). For our example datasets, we consider an exponentially weighted moving average (EWMA) to model the transformed leaf counts. We use a single smoothing pa-rameter for all our leaf nodes, the value being selected to minimize the average predictive squared-error loss on a tuning set across all nodes. We assume there is no seasonality in our time series. This is the case for both the data sets analyzed in this paper. Consider a single leaf node and let  X  x t denote predicted value at time t based on data until time t  X  1 .

For EWMA,  X  x t = m t  X  1 ;and where  X   X  (0 , 1) is a smoothing constant with higher values giving less weight to historical observations. Equation 4 can be shown to be a steady state model obtained form a simple random walk model given by where x t is observed value at time t , m t may be thought of as the truth, t and  X  t are uncorrelated random variables with zero means and variances V ( ) and V (  X  ) [19]. At steady state, the confidence values 95% confidence values
Fraction of explanations denote the confidence values. optimal prediction obtained through Equation 5 reduces to Equa-tion 4 with an optimal value of  X  given by ( R = V ( ) /V (  X  ) and the predictive variance at time t based on data up to time t  X  1 is given as V = V ( ) / (1  X   X  ) . Thus, es-timators which give more weight to historical data achieve more smoothing and have lower predictive variance.

In our scenario, we are dealing with N&gt; 1 time series cor-responding to the leaf nodes giving rise to pairs (  X  i ,V estimated. For simplicity, we assume  X  i =  X  for all i and estimate the optimal value by minimizing squared-error predictive loss on a tuning set (see [24] for an example of such an estimator). For V ( ) , we test the following variations: a) separate parameter for each series; b) one parameter for a set of sibling leaf nodes (nodes sharing same parent); c) one parameter for all the time series. We select the best model as the one that minimizes average predictive log-likelihood on the tuning set; this captures both the mean and variance properties of the predictive distribution.

We analyzed two datasets: Census and WorldCup. The Census data have yearly population numbers from 2000  X  2004 .Weused 2000  X  2003 as our training period and 2004 as our test period on which we detect anomalies. We have approximately 81 K leaf nodes on the test period. Since all number are positive (a count of 0 is interpreted as missing data), we modeled the data using a EWMA of 4 time points on the log scale. days on the log scale.
We considered daily counts for the World Cup data and used 32 time points.The 31 st time point is used as a tuning set to select the smoothing parameter  X  and the variances. The last time point is used as our test set. The optimal value of  X  in this case was 0 . 8 and the model with separate variances for each node also turned out to be the best one for this data. Unlike the Census data, the World Cup hierarchy is not homogeneous, i.e., the nodes have different fanouts and paths have different depths. Also, the structure of the tree is dynamic with new nodes appearing and some old nodes be-coming inactive over time. We restrict ourselves only to nodes that occurred at least twice in the last 10 time points. This removes nodes that have a small mean and are not of interest, providing a set of approximately 5 . 5 K leaf nodes to be monitored. Since zero counts are common in these time series, a log transform to achieve algorithm on Census data # Explanations k is per-node tolerance and is in linear-scale (not log). symmetry is not an option here. Instead, we use a squared-root transformation which, for count data, is known to stabilize vari-ance, achieve approximate symmetry and makes the assumption of a Gaussian distribution reasonable.
For illustrative purposes, we present the top 5 nodes in result-ing explanations based on absolute magnitude (difference of the weights from 0). Table 1 shows the 5 nodes in the Census datasets which are explanations and whose absolute relative error is among the Top 5. We show the lists for two different prediction mod-els: a superior EWMA model, with a separate variance component for each leaf and an inferior EWMA model with a separate but fixed variance for each leaf in a cluster (nodes under same parent) which is set to the harmonic mean of the individual leaf variances in the cluster. Note that some nodes are common explanation nodes under both the models such as Illi nois/Lake/Lib ertyville/27923, Texas/Parker/ForthWorth and Taxas/Hays/Austin. Similar exam-ples are shown for World Cup datasets in Table 2.

Figure 4 considers hierarchical relationships among the explana-tion nodes. If many nodes in the explanation set have descendant nodes that are also part of the explanation, then this indicates the importance of hierarchical explanations, as descendant nodes are needed to explain trends that are different from the ancestors in the explanation; these could be stronger trends or counter-trends com-pared to the ancestor node. Thus, for each node in the explanation set, we counted how many descendants below it are also part of the explanation. Let V ( l ) be the number of explanation nodes at level l and V ( l ) B be the number of explanations nodes at level l which have at least one explanation node as descendant. Then we compute V ( l ) B /V ( l ) . In these plots, level 0 indicates the root. We observe that significant number of counties ( &gt; 25% ) have cities which have different trends in population under all prediction models.
In Figure 5 we compare the parsimonious explanations against those obtained by the naive non-hierarchical approach. We use three different prediction models in which the mean of prediction is given by the EWMA model but the variances V ( ) are different  X  EWMA with a separate variance per leaf, same variance for each element in a cluster, set to the harmonic mean of cluster variances (leaves belonging to same parent), single variance for each leaf, set to the harmonic mean of the global variances. Note that the lat-ter two estimates underestimate var iability for a larg e fraction of World Cup data. nodes; we choose them to study the effect of inferior prediction model on our algorithms.

As the confidence level increases, the precision decreases and therefore, all the curves show decreasing trends monotonically. In Figure 5(a) we observe that the parsimonious model with EWMA offers the best parsimony with the smallest number of explanations, followed by non-hierarchical on EWMA model, thus showing the advantage of parsimonious algorithm. As expected, the perfor-mance of EWMA model with global harmonic mean of the vari-ances perform worst in terms of parsimony.
 We show the parsimony of our algorithm by comparing with the DIFF operator [21]. Since the technique in [21] puts constraints on the intermediate nodes, we have to modify our algorithm so that we have a tolerance parameter k in each node, and we use this model in this comparison shown in Figure 6. We compare two different snapshots -year 2003 and 2004 from Census data; and May 26 and 27 from World Cup data. It is to be noted that x-axis in Figure 6 is per-node tolerance, k&gt; 0 and it is not in log-scale.
The improvement in the number of explanations when using our model is significant, up to two orders of magnitude. The improve-ment is more evident in the Census data, which exhibit hierarchical trends, compared to the World Cup data.

Figure 5 (b) show the average stability across all levels for both non-hierarchical and parsimonious algorithms. We observe that with increase in confidence level, the stability decreases since the set of nodes which are explanations changes. Since Census data is homogeneous with 4 levels, we observe almost monotonic change in stability with increase in confidence level except for the parsi-monious algorithm with the best EWMA model at confidence level 97. Similarly, we observe parsimony and stability on World Cup datasets in Figure 7.
In Figures 8 (a)-(b), we show the runtime of the parsimonious model on Census data (minimum time over 5 runs) as function of the number of leaves, N and confidence level respectively (using all three models). First, the increase in running time with N follows O( hN log( N ) ) growth. Second, we observe that all the algorithms show a decreasing trends (prominently in Census data) in running time with increase in confidence level (increasing error) leading to a small number of intervals in the non-leaf nodes. Third, we observe that the parsimonious algorithm with EWMA model has least running time complexity. That means, variances in individual leaves can summarize changes well whereas the algorithm which uses harmonic mean of variances cannot summarize the changes that well and thus leads to many explanations (and intervals) up in the trees. To vary the number of leaves, we sample each leaf with some probability to be included in the tree. We also show it for two different values of confidence levels.

In Figure 8(c), we show the space complexity of the 3 parsimo-nious algorithms on Census (similar trend on World Cup data but better), averaged over all nodes per level. We observe that the aver-age number of intervals per node is very close to 1 except for par-simonious algorithm using EWMA model with same variance per leaf node (estimated by the Harmonic Mean of the individual leaf Variances). Furthermore, we observe that the same parsimonious algorithm has higher running time and larger number of average intervals at different levels.
In this paper, we proposed a natural model for explaining changes in hierarchical data and formulated two problem variants for find-ing a parsimonious explanation in this model. Our model makes ef-fective use of the hierarchy and describes changes at the leaf nodes as a composition o f node weights along each path of each root-to-leaf path in the hierarchy. We designed algorithms to minimize the explanation size for both problem variants. Despite the fact that assigning node weights optimally is an under-constrained problem, we have shown that it is not NP-hard and that our algorithms re-quire time proportional to the product of the number of leaves and the depth of the dimension hierarchy.

We evaluated our approach on real data to demonstrate both its efficiency and effectiveness. In practice, the performance and space usage of our algorithms are much less than the worst-case bounds. On population census data, the explanations discovered (counter) trends, mainly at the city-level. We made similar observations when we analyzed HTTP traffic logs from the FIFA World Cup hosting site. Our approach can also be used to reveal  X  X nteresting X  anomalies in hierarchical data when used in conjunction with a sta-tistically sound predictive model that forecasts values within con-fidence intervals. These anomalies were explained more parsimo-niously using our algorithm compared to the leaf-level anomalies that the predictive model detects.

We are currently extending our approach to multiple dimensions, which presents several non-trivial challenges due to the existence of multiple parents in the hierarchy. Another natural extension we have considered for future work is where there is a global budget on error tolerance for the entire tree. Although we have found a poly-nomial solution, its complexity appears to be significantly higher than the problems studied in this paper, and its feasibility on mas-sive data sets remains to be shown.

We thank the anonymous reviewers for excellent comments on the paper. The work of the third author is supported by NSF awards 0330481 and 0534781. [1] On change diagnosis in evolving data streams. IEEE TKDE , [2] E. Amaldi and V. Kann. On the Approximability of [3] Sanjeev Arora, L X szl X ; Babai, Jacques Stern, and [4] Dhiman Barman, Flip Korn, Divesh Srivastava, Dimitris [5] Census (population vs. location), 2000-2004. http: [6] Surajit Chaudhuri and Umeshwar Dayal. An overview of [7] Graham Cormode, Flip Korn, S. Muthukrishnan, and Divesh [8] Graham Cormode and S. Muthukrishnan. What X  X  new: [9] Cristian Estan, Stefan Savage, and George Varghese. [10] Min Fang, Narayanan Shivakumar, Hector Garcia-Molina, [11] Sudipto Guha. Space Efficiency in Synopsis Construction [12] Sudipto Guha and Boulos Harb. Approximation algorithms [13] Panagiotis Karras and Nikos Mamoulis. The Haar+ Tree: a [14] Daniel Kifer, Shai Ben-David, and Johannes Gehrke. [15] Jon Kleinberg. Bursty and hierarchical structure in streams. [16] Laks V. S. Lakshmanan, Raymond T. Ng, Christine Xing [17] Yossi Matias, J.S. Vitter, and M. Wang. Wavelet-Based [18] S. Muthukrishnan. Subquadratic algorithms for [19] P.J.Harrison. Exponential smoothing and short-term sales [20] Frederick Reiss, Minos Garafalakis, and Joseph Hellerstein. [21] Sunita Sarawagi. Explaining differences in multidimensional [22] Sunita Sarawagi, Rakesh Agrawal, and Nimrod Megiddo. [23] Sudarshan S.Chawathe. Differencing data streams. In Proc. [24] S.Hill, D.Agarwal, R.Bell, and C.Volinsky. Building an [25] WorldCup 1998. http://ita.ee.lbl.gov/html/ [26] Yin Zhang, Sumeet Singh, Subhabrata Sen, Nick Duffield, [27] Yunyue Zhu and Dennis Shasha. Efficient elastic burst
