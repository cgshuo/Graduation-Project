 Learning to rank studies have mostly focused on query-dep-endent and query-independent document features, which en-able the learning of ranking models of increased effective-ness. Modern learning to rank techniques based on regres-sion trees can support query features , which are document-independent, and hence have the same values for all docu-ments being ranked for a query. In doing so, such techniques are able to learn sub-trees that are specific to certain types of query. However, it is unclear which classes of features are useful for learning to rank, as previous studies leveraged anonymised features. In this work, we examine the useful-ness of four classes of query features, based on topic classifi-cation, the history of the query in a query log, the predicted performance of the query, and the presence of concepts such as persons and organisations in the query. Through ex-periments on the ClueWeb09 collection, our results using a state-of-the-art learning to rank technique based on regres-sion trees show that all four classes of query features can significantly improve upon an effective learned model that does not use any query feature.
 Categories &amp; Subject Descriptors: H.3.3 [Information Storage &amp; Retrieval]: Information Search &amp; Retrieval Keywords: Learning to Rank, Query Features
Learning to rank deployments leverage various query-dep-endent (e.g. term weighting models, proximity) and query-independent (e.g. URL length, PageRank, content quality) features, with which to re-rank in an effective manner a sample of documents obtained from a single feature (usu-ally BM25 [12]). Such features are combined into a learned model , obtained by minimising the loss function of a learning to rank technique (see [12] for an overview).

While learning to rank techniques are often differentiated as pointwise, pairwise, and listwise, depending on their loss function [12], we focus on a different viewpoint. In particu-lar, we separate techniques that learn a linear combination of features (possibly after a kernel transformation) from tech-niques based on regression trees, such as gradient boosted regression trees [21] and LambdaMART [23]. For such tree-based learners , the partial score for a given document is ob-tained by following a traversal of a regression tree. At each node, a particular feature value is compared with a learned threshold, to define a path towards the partial score for that document. Typically, an ensemble consisting of multiple trees is used to obtain the final score for each document [7].
In addition to the classical query-dependent and query-independent document features, a third class of features has seen comparatively less research in the learning to rank lit-erature. In particular, a query feature is a quantifier for some aspect of the query, and  X  in contrast to document features  X  has the same value across all documents in the sample. Query features can be used by tree-based learners, serving as decision points between the various branches of a learned ranking model, thereby permitting the learner to  X  X ustomise X  the learned model with respect to an aspect of the query [21]. An example query feature might be the num-ber of terms in the query [21], which may trigger branches of the learned model X  X  trees that are better suited for long queries. Chapelle et al. [7] mention the  X  X dult score X  of the top retrieved documents as another example query feature.
Research encapsulating query features within learning to rank is in its infancy. For instance, none of the standard LETOR datasets for learning to rank [12] deploy query fea-tures. This problem was addressed by the Yahoo! Learning to Rank Challenge, whose dataset contains 36 query fea-tures [7]. However, to the best of our knowledge, no pre-vious work has analysed the usefulness of different types of query features. As a matter of fact, the features included in the Yahoo! Learning to Rank Challenge are anonymised, which precludes any analysis on the effectiveness of individ-ual query features. On the other hand, query features have been used for customising rankings on a per-query basis. For instance, Geng et al. [11] used query features to clus-ter similar queries before learning common ranking models. Santos et al. [20] used query features to learn the appropri-ate amount of diversification to apply for a query. However, in all of these works, query features were used to improve the understanding of the query before the ranking stage, as op-posed to being leveraged as an integral part of the learning to rank process.

To further the understanding of the usefulness of query features for learning to rank, we conduct the first empirical analysis of query features in a state-of-the-art learning to rank deployment. In the remainder of this paper, Section 2 describes the experimental methodology underlying our in-vestigation. Our findings are discussed in Section 3, and Section 4 presents our final remarks.
In the following experiments, our goal is to assess the effec-tiveness of a series of query features when added to an effec-tive machine learned ranking model that does not employ any query features. Our experiments use the ClueWeb09 (category B) collection, which comprises 50 million English Web documents, and is aimed to represent the first tier in-dex of a commercial search engine. We index this collection weak Porter stemmer. To form a sizable query set for our in-vestigation, we use 98 topics and relevance assessments from the TREC 2009-2010 Web tracks, and 140 from the TREC 2009 Million Query track. For each query, we use the Diver-gence from Randomness DPH weighting model [2] to pro-duce an initial sample of 5000 documents. For documents in the sample, we calculate a total of 47 standard query-dependent (e.g. term weighting models, proximity features) and query-independent document features (e.g. link analy-sis, URL length, content quality), similar to those deployed within the LETOR datasets [12]. Table 1 lists the 47 query-dependent (QD) and query-independent (QI) document fea-tures used within our experiments.

To re-rank the documents in the sample, we use the Lamb-daMART learning to rank technique [23] (as implemented by h ttp://terrier.org of-the-art learning to rank techniques, as per its top-class performance in the 2011 Yahoo! Learning to Rank Chal-lenge [7]. A ranking model learned by LambdaMART based on a loss function targeting NDCG, and using the afore-mentioned 47 document features, forms the baseline for the investigation in this paper. In particular, we investigate whether query features can enhance this baseline, and anal-yse the usefulness of different classes of query features.
Following Santos et al. [20], who investigated query fea-tures for selective diversification, in addition to the 47 doc-ument features, we calculate 178 query features, which are classified according to the tasks that inspired them: query concept identification (QCI); query performance prediction (QPP); query log mining (QLM); and query topic classifi-cation (QTC). More generally, these features examine the statistics of the query within the corpus, how many times (persons, products, organisations or locations) occur in the query or the corresponding top-ranked sample documents. Table 2 details the used query features for each class. This table also lists the main evidence source for each feature, namely the query itself, user click behaviour in the query log h ttp://code.google.com/p/jforests/ http://tiny.cc/wscd09 for that query, or a number of top-ranked sample documents f or the query. In particular, query features that examine the sample documents are instantiated multiple times, ex-amining different numbers of top-ranked sample documents. For example, a query feature named DocEntityCount-org-50 considers the mean number of organisation entities identi-fied in the top 50 retrieved sample documents. The distribu-tion of entities across the top retrieved documents results in further summary features, such as average, maximum, and standard deviation.

Finally, our experiments are conducted using a 5-fold cross validation across the 238 TREC topics (98 Web track + 140 Million Query track topics), where each fold has separate training, validation and test query sets. Our experimental results report NDCG@20 across the test sets from each fold.
For each query feature, we add it to the 47 document fea-tures of the baseline ranking, and learn a ranking model for each fold. Across the 5 folds, we found that all but 3 query features exhibited improvements over the LambdaMART baseline that has no query features. Of the improving fea-tures, 46 brought significant improvements in NDCG@20 (paired t-test, p &lt; 0 . 05). Table 3 shows the effectiveness of the four best features for each class (QCI, QPP, QLM and QTC) of query features, as well as the mean NDCG@20 of all query features in each class, and the number of query fea-tures exhibiting significant improvements compared to the baseline ( p &lt; 0 . 05). On analysing the table, we observe that the most useful query features result in statistically signif-icant increases over the baseline that does not deploy any query features.

In particular, the best query topic classification (QTC) feature results in an 8% increase in NDCG@20 (0.2832  X  0.3109). This query feature examines the number of organ-isations mentioned in the top 100 sample documents [20], suggesting that QTC query features are useful for triggering sub-trees that promote entity homepages.

Of the other query feature classes, query performance pre-dictors (QPP) show promise in increasing effectiveness, with the Gamma1 pre-retrieval predictor [20] being the most ef-fective. By deploying such a query performance predictor as a query feature, the learner is able to customise differ-ent tree branches for easy and difficult queries. The sec-ond most useful query performance predictor is simply the query length, in tokens. This feature likely helps the learner to decide the importance of proximity document weighting features (e.g. MRF [14] or pBiL [16]). The last two features concern the importance of 1-grams and 4-grams within an anchor text index, and hence are likely to differentiate be-tween navigational and informational queries, resulting in different learned models for queries of different intents.
For query concept identification (QCI), the deployed query features include the number of acronyms and the number of entities (locations, organisations, products, and persons) in the query. Three of the four highest performing features con-sider the ambiguity of the query: for instance, the highest-performing query feature counts the number of disambigua-tion pages ranked in the top 3 Wikipedia pages for the query. The fourth feature denotes the presence of known person en-tities in the query, again suggesting that queries for entities require adapted ranking models.

Of our query log mining (QLM) features, three of the four most useful query features considered the complexity of the user sessions for such a query. For instance, longer mean user session duration for a query may indicate a more diffi-cult query, with a similar interpretation if more results are viewed. Moreover, a query whose sessions exhibit a high variance in durations may be ambiguous. Hence, both these query features and the QCI Wikipedia disambiguation query features suggest that different sub-trees for queries with dif-ferent levels of ambiguity can be learned. Finally, the 1-gram score for the query is an indication of query popular-ity, suggesting that more popular (head) queries can have customised ranking models  X  indeed, head queries are likely to be more navigational in nature.

Comparing the mean performance across all features in a class (see Mean rows in Table 3), we find that all classes of query features exhibit similar improvements above the baseline, suggesting that each could bring further evidence to the LambdaMART learning to rank technique. However, the QTC and QLM classes exhibited a higher fraction of features leading to significant effectiveness improvements. Overall, the results in Table 3 provide concrete examples of several types of features that can be successfully integrated with document features to result in learned models with sig-nificantly improved effectiveness.

Finally, we combine the best feature from each class with the 47 document features. The performance of the com-bined learned model with 47+4 features is shown in Table 3. As the performance does not improve over the 47-feature baseline, we conclude that LambdaMART can easily over-fit for multiple query features. This is explainable in that the learner has far less queries than documents (144 training queries vs 720,000 training documents). While more queries should prevent this overfitting, feature selection approaches (e.g. adapting [10] to query features) may also be beneficial -we leave this to future work.
While query features are mentioned in the recent learning to rank literature, there has been no empirical investigation into which types of query feature are useful to improving learned models. In this paper, we investigated the benefit of several classes of query features, when integrated with document features using the state-of-the-art LambdaMART learning to rank technique. We found that almost all query features could improve performance, with over a quarter of the 178 features exhibiting significant improvements. In par-ticular, our results show that query features can be success-fully employed to customise ranking models for queries with different popularity, length, difficulty, ambiguity, and related entities. Future work will examine other possible classes of query features, and ways in which multiple query features can be combined. [1] G. Amati. Probability models for information retrieval [2] G. Amati, E. Ambrosi, M. Bianchi, C. Gaibisso, and [3] M. Bendersky, W. B. Croft, and Y. Diao. Quality-[4] D. Carmel and E. Yom-Tov. Estimating the query [5] P. Clough, M. Sanderson, M. Abouammoh, [6] G. V. Cormack, M. D. Smucker, and C. L. A. Clarke. [7] O. Chapelle, Y. Chang. Yahoo! learning to rank [8] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. [9] Y. Ganjisaffar, R. Caruana, and C. Lopes. Bagging [10] X.-B. Geng, T.-Y. Liu, T. Qin, H. Li. Feature [11] X.-B. Geng, T.-Y. Liu, T. Qin, A. Arnold, H. Li, [12] T.-Y. Liu. Learning to rank for information retrieval. [13] C. Macdonald, V. Plachouras, B. He, C. Lioma, and [14] D. Metzler and W. B. Croft. A Markov random field [15] I. Ounis, G. Amati, V. Plachouras, B. He, [16] J. Peng, C. Macdonald, B. He, V. Plachouras, and [17] V. Plachouras, I. Ounis, and G. Amati. The static [18] S. E. Robertson, S. Walker, S. Jones, [19] M. Sanderson. Ambiguous queries: Test collections [20] R. L. T. Santos, C. Macdonald, and I. Ounis. Selectively [21] S. Tyree, K. Weinberger, [22] Y. Wang and E. Agichtein. Query ambiguity revisited: [23] Q. Wu, C. J. C. Burges, K. M. Svore,
