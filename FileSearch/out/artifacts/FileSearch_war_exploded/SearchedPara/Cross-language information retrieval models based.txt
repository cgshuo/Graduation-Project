 Abstract In this paper, we study different applications of cross-language latent topic models trained on comparable corpora. The first focus lies on the task of cross-language information retrieval (CLIR). The Bilingual Latent Dirichlet allocation model (BiLDA) allows us to create an interlingual, language-independent representation of both queries and documents. We construct several BiLDA-based document models for CLIR, where no additional translation resources are used. The second focus lies on the methods for extracting translation candidates and semantically related words using only per-topic word distributions of the cross-language latent topic model. As the main contribution, we combine the two former steps, blending the evidences from the per-document topic dis-tributions and the per-topic word distributions of the topic model with the knowledge from the extracted lexicon. We design and evaluate the novel evidence-rich statistical model for CLIR, and prove that such a model, which combines various (only internal) evidences, obtains the best scores for experiments performed on the standard test collections of the CLEF 2001 X 2003 campaigns. We confirm these findings in an alternative evaluation, where we automatically generate queries and perform the known-item search on a test subset of Wikipedia articles. The main importance of this work lies in the fact that we train translation resources from comparable document-aligned corpora and provide novel CLIR statistical models that exhaustively exploit as many cross-lingual clues as possible in the quest for better CLIR results, without use of any additional external resources such as parallel corpora or machine-readable dictionaries.
 Keywords Cross-language information retrieval Unsupervised cross-language lexicon extraction Probabilistic latent topic models Evidence-rich retrieval models 1 Introduction The ongoing growth of the World Wide Web and its ubiquity lead to its further locali-zation, where users tend to abandon English as the lingua franca of the global network, since more and more content is available in their native languages. However, the avail-ability of content in a huge spectrum of different natural languages, many of which with scarce translation resources, creates a need to efficiently bridge the gap between the languages, using language-independent and generic approaches for miscellaneous prob-lems involving multilingualism.

Machine-readable translation dictionaries do not exist for all language pairs or all domains, as they are usually trained on large parallel corpora or are hand-built. Compiling such lexicons manually is often an expensive and time-consuming task, whereas the methods for mining the lexicons from parallel corpora are not applicable for language pairs and domains where such corpora is unavailable or missing. In a parallel corpus, each text in the source language has an exact translation in the target language. In a comparable corpus on the other hand, documents in different languages are paired when they contain partially overlapping or similar content. Thus, it is much easier to build a high-volume comparable corpus. 1 For instance, news stories and encyclopedia entries often discuss the same event or topics in different languages, but with different focuses. A representative example of such a comparable text collection is Wikipedia, where one may observe articles discussing the same topic, but strongly varying in style, length and even vocabulary, while still sharing a certain amount of main concepts (or topics).

We tackle and combine two different problems: (1) cross-language information retrieval and (2) cross-language lexicon extraction, both in a hard setting, where no external parallel corpora are absent, and only document alignments for comparable training corpora are known. We accomplish both tasks utilizing the potential of a cross-language generative model, i.e., bilingual Latent Dirichlet allocation (BiLDA), which is an extension of the standard LDA model (Blei et al. 2003 ), operating in a cross-language setting. Probabilistic topic models are a powerful tool for discovering and analyzing topic patterns in text. They are based upon the idea that there exist latent variables which determine how words in documents might be generated. Fitting a generative model means finding the best set of those latent variables in order to explain the observed data. Within that setting, documents are observed as mixtures of latent topics, where topics are probability distributions over words. In a cross-language setting, we assume that the topic patterns are shared across languages. Another important assumption is that by collecting as many various comparable data (e.g. Wikipedia articles) as possible, the BiLDA model will be able to correctly learn many important topics shared between languages. Such a broad coverage of topics, together with word distributions over topics, will serve as a sound foundation for solving various cross-language problems.

Cross-language information retrieval (CLIR) deals with documents written in a language different from the language of the user X  X  query. At the time of retrieval the query in the source language is typically translated into the target language of the documents with the help of a machine-readable dictionary or a machine translation system. Once a user has retrieved relevant documents for a particular query, they can be translated to the language of the user, possibly by means of manual translation in case resources for automatic translation are unavailable.

This paper addresses the question whether suitable cross-language retrieval models can be built in case machine-readable translation dictionaries or systems that are hand-built or extracted from large parallel sentence-aligned corpora are absent. A number of words might appear with the same meaning in different languages (especially when dealing with languages from the same family). However, when only using a monolingual retrieval model for CLIR, we will miss many relevant documents. Moreover, a word might exhibit the same orthography in different languages, but actually mean something different. Consequently, we need some kind of translation resource, preferably built automatically from comparable corpora.

The transfer of the query into other languages can be accomplished by means of a cross-language probabilistic latent topic model. The language models for retrieval have a sound statistical foundation and can leverage statistical estimation to optimize the retrieval parameters. They can be easily adapted to complex retrieval tasks and have already shown their value in cross-language retrieval settings, incorporating translation probabilities obtained from a translation dictionary in the retrieval model. Our attempt is to exploit the probability distributions over interlingual topics as a translation resource, since they pro-vide an interlingual content representation of the documents.

Cross-language lexicon extraction (CLE) is the task of automatically acquiring translation candidates from parallel, comparable or unrelated texts. We focus on the lex-icon extraction from comparable, document-aligned texts, where no seed lexicons are identify potential translations from document-aligned comparable text collections such as Wikipedia. State-of-the-art generative models, most commonly used for obtaining word translation probabilities from parallel corpora, such as IBM Models (Och and Ney 2003 ) are unusable and computationally intractable within this setting, and cannot be employed on comparable corpora aligned only at the document level.

We try to establish a connection between cross-language latent topics and an idea known as the distributional hypothesis (Harris 1954 ) X  X ords with a similar meaning are often used in similar contexts. Besides the obvious context of direct co-occurrence, we believe that topic models constitute an additional source of knowledge which might be used to improve results in the quest for translation candidates extracted without the availability of a translation dictionary and linguistic knowledge. We designed several methods, all derived from the core idea of using word distributions over topics as an extra source of contextual knowledge. Two words are potential translation candidates if they are often present in the same cross-language topics and not observed in other cross-language topics. In short, a word w 2 from a target language is a potential translation candidate for a word w 1 from a source language, if the distribution of w 2 over the target language topics is similar to the distribution of w 1 over the source language topics.

In the remainder of the paper, the two problems ( CLIR and CLE ) will be first observed, described and evaluated independently, still sharing the latent topic model underpinning them. The most important step of this work will combine the two in a coherent evidence-rich document model for CLIR which blends translation probabilities from the translation lexicon extracted from per-topic word distributions learned by the BiLDA model with our retrieval model that relies solely on per-document topic distributions and per-topic word distributions connected through the shared space of interlingual topics.

The contributions of the paper are as follows. First, we show the validity and the potential of training bilingual LDA model on bilingual comparable corpora that are available in abundance (e.g. Wikipedia, news). Second, per-topic word distributions learned during training may be used for automatic cross-language lexicon extraction which can be utilized in other cross-language applications. We demonstrate the applicability and use-fulness of the BiLDA-induced lexicon in the novel framework of cross-language infor-mation retrieval. Third, we successfully integrate the knowledge from the lexicons and the knowledge from probability distributions of the BiLDA model into a novel evidence-rich cross-language statistical retrieval model which uses only internal evidence, and perform a full-fledged evaluation and comparison of all our retrieval models for: (1) the simpler task of English-Dutch and Dutch-English known-item search performed on Wikipedia articles, and (2) English-Dutch and Dutch-English CLIR on the standard CLEF test collections. We also show that the results obtained by our retrieval models, which do not exploit any linguistic knowledge from an external translation dictionary, but exploit all the evidences from probability distributions of the BiLDA model to the fullest, are competitive with and sometimes display a better performance than dictionary-based models for CLIR.

The paper is structured as follows. Section 2 describes related work in cross-language information retrieval, drawing parallels with LDA-based methods for the monolingual setting and listing several approaches which have been trying to develop retrieval models based on latent classes and concepts. This section also discusses different methods of automatic cross-language lexicon extraction, focusing mainly on previous attempts to use topic models to recognize potential translations. Section 3 provides an overview of the cross-language BiLDA model used in all our experiments. Section 4 describes the first set of BiLDA-based statistical models for CLIR, while Sect. 5 gives a complete insight in the methods we used for generating a general lexicon from topical knowledge. We continue the development of retrieval models in Sect. 6 , with a model that uses only entries from the obtained lexicons, and another model which combines lexicon entries with the LDA-only model from Sect. 4 . In Sect. 7 , we present our experimental setup, training and test collections, and used queries. In Sect. 8 we test and evaluate our CLIR models on different collections and within different settings, and discuss the obtained results. Finally, Sect. 9 lists conclusions and future work. 2 Related work Probabilistic topic models such as probabilistic Latent Semantic Indexing (pLSI) (Hof-mann 1999 ) and Latent Dirichlet allocation (LDA) (Blei et al. 2003 ) are a popular means to represent the content of a document. Although designed as generative models for the monolingual setting, their extension to multilingual domains follows naturally. Platt et al. ( 2010 ) propose several variant models to project documents from multiple languages into a single interlingual vector space, based on the pLSI and LDA models. They use discrimi-document retrieval for Wikipedia and Europarl documents, and cross-lingual text classi-fication on Reuters. Cimiano et al. ( 2009 ) use standard monolingual LDA, but trained on concatenated parallel and comparable documents in a document comparison task. Roth and Klakow ( 2010 ) try to use standard monolingual LDA trained on concatenated Wikipedia articles for cross-language information retrieval, but they do not obtain decent results without additional usage of a machine translation system. They use the standard Moses machine translation toolkit (Koehn et al. 2007 ) trained on a parallel sentence-aligned corpus to translate queries and perform monolingual retrieval afterwards.

Recently, the bilingual or multilingual LDA model was independently proposed by different authors (Ni et al. 2009 ; Mimno et al. 2009 ; De Smet and Moens 2009 ; Boyd-Graber and Blei 2009 ) who identify interlingual topics of different languages. These authors train the bilingual LDA model on a parallel corpus. Jagarlamudi and Daume  X  III ( 2010 ) extract interlingual topics from comparable corpora, but use information from existing hand-built translation dictionaries. Their work follows the opposite direction; while we utilize learned cross-language topics to mine potential translations, they employ knowledge from a dictionary to learn cross-language topics. None of these works apply the bilingual LDA model in a cross-language information retrieval setting.
 Cross-language information retrieval is a broad and well-studied research topic (e.g., Grefenstette 1998 ; Nie et al. 1999 ; Savoy 2004 ; Nie 2010 ). As mentioned, existing methods rely on a translation dictionary to bridge documents of different languages. In a typical setting, cross-language information is learned based on parallel corpora and cor-relations found in the paired documents (Mathieu et al. 2004 ), or are based on Latent Semantic Analysis (LSA) applied on a parallel corpus. In the latter case, a singular value decomposition is applied on the term-by-document matrix, where a document is composed of the concatenated text in the two languages, and after rank reduction, the document and the query are projected in a lower dimensional space (Dumais et al. 1996 ; Littman et al. 1998 ; Chew et al. 2007 ; Xue et al. 2008 ). The term-by-document matrix formed by con-catenated parallel documents was used to generate probabilistic term translations with a standard pLSI model and used in cross-language information retrieval (Muramatsu and Mori 2004 ). Our work follows this line of thinking, but uses generative LDA models trained on a comparable document-aligned corpus, which might be different from the document collection used for retrieval. In addition, our models are trained on the individual documents in different languages, but paired by their joint interlingual topics and, due to that fact, we expect our models to lead to better results than CLIR relying on the cross-language LSI model. Relevance models (Lavrenko et al. 2002 ) have also been applied for CLIR, but they still need either a parallel corpus or a translation dictionary for estimation. LDA-based monolingual retrieval has been described by Wei and Croft ( 2006 ).

Transfer learning techniques, where knowledge is transfered from one source to Transfer learning bridged by probabilistic topics obtained via pLSI was proposed by Xue et al. ( 2008 ) for the task of cross-domain text categorization. Recently, knowledge transfer for cross-domain learning to rank the answer list of a retrieval task was described by Chen et al. ( 2010 ). Takasu ( 2010 ) proposes cross-language keyword recommendation using latent topics. Cross-language text clustering and categorization based on the multilingual LDA model was recently proposed by the authors (De Smet and Moens 2009 ; De Smet factory, and relies solely on 30 documents and 7 queries, none of the above works use LDA-based cross-language topics in novel cross-language retrieval models.

The idea to acquire translation candidates based on comparable and unrelated corpora is first tackled in (Rapp 1995 , 1999 ). Over the years, other similar approaches have emerged (Fung and Yee 1998 ; Diab and Finch 2000 ;De  X  jean et al. 2002 ; Chiao and Zweigenbaum 2002 ; Gaussier et al. 2004 ; Fung and Cheung 2004 ; Morin et al. 2007 ; Shezaf and Rap-poport 2010 ; Laroche and Langlais 2010 ). All these methods have examined different representations of word contexts and different methods for matching words across lan-guages, but they all need an initial lexicon of translations, cognates or similar words which are then used to acquire additional translations of the context words, and a bootstrapping procedure is often employed. In contrast, our cross-language extraction methods do not bootstrap on language pairs that share morphology, cognates or similar words and do not unrelated corpora using a generative model with latent concept spaces and orthographic and contextual features. However, orthographic features imply that their method works required.

Some attempts of obtaining translations using cross-language topic models have been made in the last few years, but they are model-dependent and do not provide a general environment to adapt and apply other topic models for the task of finding translation correspondences. Ni et al. ( 2009 ) have designed a probabilistic topic model that fits Wikipedia data, but they did not use their models to obtain potential translations. Mimno et al. ( 2009 ) retrieve a list of potential translations simply by selecting a small number N of the most probable words for topics in both languages and then add the Cartesian product of straightforward, but it does not catch the structure of the latent topic space completely and is unable to provide semantically related words for low-frequency words. Another model, proposed by Boyd-Graber and Blei ( 2009 ), builds topics as distributions over bilingual matchings where matching priors may come from different initial evidences such as a machine-readable dictionary, the edit distance, or the point-wise mutual information sta-external knowledge for matching priors and uses a restricted vocabulary. The authors also admit that their multilingual model suffers from overfitting, since it tends to learn matchings between unrelated words.
 None of these works apply the extracted lexicons in a real-life problem such as CLIR. To our knowledge, our work is the first real application of any lexicon derived from a latent topic model, and we show its usefulness for CLIR. The usage of multiple semantically related words from the lexicon entries may be observed as a query expansion technique, constructed to improve the effectiveness of a CLIR model. Query expansion techniques relying on a statistical similarity measure among terms stored in an automatically gener-ated thesaurus/lexicon are described by Adriani and Rijsbergen ( 1999 ) and Sheridan and Ballerini ( 1996 ), but their work differs from ours in both construction of the lexicon and its usage in the CLIR model. 3 Bilingual LDA 3.1 Description of the model The topic model we use is a bilingual extension of a standard LDA model, called bilingual LDA (BiLDA) (Ni et al. 2009 ; Mimno et al. 2009 ; De Smet and Moens 2009 ; Boyd-Graber and Blei 2009 ). As the name suggests, it is an extension of the basic LDA model, taking performance on a collection of comparable texts where related documents are paired, and therefore share their topics to some extent. Unlike, for instance, Wikipedia articles, where document alignment is established via interlingual links, in some cases it is necessary to perform document alignment as the initial step. Our work mainly focuses on Wikipedia data and, by the nature of the Wikipedia structure, articles about the same subject, but in different languages are linked. The cross-lingual pairing of documents is not the subject of the research reported here, but we refer the interested reader to (Utiyama and Isahara 2003 ; Resnik and Smith 2003 ; Vu et al. 2009 ).

BiLDA takes advantage of the document alignment by using a single variable that contains the topic distribution h . This variable is language-independent, because it is shared by each of the paired bilingual comparable documents. Topics for each document are sampled from h , from which the words are sampled in conjugation with the vocabulary distribution / (for language S) and w (for language T). a and b are the parameters of the uniform conjugate Dirichlet priors 4 for the per-document topic distribution h and the per-topic word distributions / and w , respectively. 5 Algorithm 3.1 summarizes the generative story, while Fig. 1 shows the plate model.

Having one common h for both of the related documents implies parallelism between the texts, which might not always be the case. Still, we later show that the BiLDA model can provide satisfactory results when trained on a comparable corpus such as Wikipedia.
As in the standard monolingual LDA model, we need to set K , i.e., the number of cross-language topics a priori, before training with actual data takes place. 3.2 Output of the model The described BiLDA model serves as a framework for modeling our statistical retrieval models and methods for automatic lexicon extraction. After the training using Gibbs sampling (Geman and Geman 1984 ; Steyvers and Griffiths 2007 ), two sets of probability probability distributions and another set consists of per-document topic probability dis-tributions. For the per-topic word probability distributions / , associated with a source vocabulary W S , the probability of sampling a new source token w i 2 W S for an interlingual topic z k from K different topics can be obtained as follows: z is assigned to the word w i from the source vocabulary W S . The sum words in the source vocabulary. The formula for a set of per-topic word probability distributions w for the target side of a corpus is computed in an analogical manner.
The second set of probability distributions gives us the distribution of topics for a document. For the BiLDA model, it can be calculated as follows: where for a document D J and a topic z k , n J ( k ) denotes the number of times a word in the document D J is assigned to the topic z k .

Due to the fact that the model possesses a fully generative semantics, it is possible to train it on any document-aligned comparable corpus, and later infer it on previously unseen corpora, where inferring a model means calculating the per-document topic distributions h for the new documents. Once trained on a document-aligned corpus, it is possible to use the topic models for CLIR on any monolingual test collection, even on those which do not have a document-aligned counterpart in the query language. 4 LDA-based Cross-language information retrieval (Act I) This section provides a theoretical insight into statistical cross-language information retrieval models relying on per-topic word distributions and per-document topic distribu-tions from Sect. 3.2 . More retrieval models, which additionally make use of our BiLDA-induced bilingual lexicon, are described in Sect. 6 . 4.1 LDA-only CLIR model Given the set D 1 ; D 2 ; ... ; D L fg of documents in a target language T , and a query Q in a source language S , the task is to rank the documents according to their relevance to the query. We follow the basic approach for using language models in monolingual infor-mation retrieval. In the query likelihood model, the bag-of-words assumption holds, that is, the terms are independent given the documents and the score of each document is the likelihood of its model generating the query. The probability P ( Q | D J ) that the query Q is generated from the document model D J , is calculated based on the unigram language model:
The main difference between monolingual IR and CLIR is that documents are not in the same language as the query. Thus, one needs to find a way to efficiently bridge the gap between languages. The common approach is to apply machine-readable translation dic-tionaries, translate the query and perform monolingual retrieval on the translated query. If a translation resource is absent, one needs to find another solution. In lack of any translation resource, we propose to use sets of per-topic word distributions and per-document topic distributions, assuming the shared space of latent topics. Combining (1) and (2), we can related probability distributions: d 1 is the interpolation parameter, while P ( q i | Ref ) is the maximum likelihood estimate of the query word q i in a monolingual source language reference collection Ref . It gives a non-zero probability for words unobserved during the training of the topic model in case it occurs in the query. Here, we use the observation that latent topics constitute a language-independent space shared between the languages. If that observation holds, it is justified to use the per-topic word distributions for the source language to predict the probability that the word q i from the query Q will be sampled from the topic z k S , and it is justified to use the per-document topic distributions for the target language to predict the probability that the same topic z k T (but now in the other language 6 ) is assigned to a token in the target document D . As mentioned, we may infer the model (learn per-document topic distributions) on any monolingual collection in the source or the target language.
 We can now merge all the steps into one coherent process to calculate the probability a document in the target language. We name this model the LDA-only model : 2. For each word q 1 ... q m in the query, do: 3. Compute the whole probability score for the given query and the current document D J : This gives the score for one target language document D J . Finally, documents are ranked based on their respective scores. If we train a bilingual (or a multilingual) model and wish formed in an analogical manner after the model is inferred on a desired corpus. 4.2 LDA-unigram CLIR model The LDA-only CLIR model from Sect. 4.1 can be efficiently combined with other models for estimating P ( w | D ). If we assume that a certain amount of words from the query does not change across languages (e.g. some personal names) and thus could be used as an evidence the source language, and D J a document model for a target language document) may be specified by a document model with the Dirichlet smoothing. We adopt smoothing tech-niques according to evaluations and findings from Zhai and Lafferty ( 2004 ). The Dirichlet smoothing acts as a length normalization parameter and penalizes long documents. The model is then:
P where P mle ( q i | D J ) denotes the maximum likelihood estimate of the word q i in the document D language, l is the Dirichlet prior, and N d the number of words in the document D J . d 2 is another interpolation parameter, and P ( q i | Ref ) is the background probability of q i , calcu-lated over a large corpus. It gives a non-zero probability for words that have zero occurrences in test collections. We name this model the simple unigram model .

We can now combine this document model with the LDA-only model using linear interpolation and the Jelinek-Mercer smoothing: P  X  q i j D j  X  X  k  X  1 d 2  X  where P lda is the LDA-only model given by ( 4 ), P lex the simple unigram model given by ( 6 ), and k is the interpolation parameter. We call this model the LDA-unigram model .
The combined model presented here is straightforward, since it directly uses words shared across a language pair. One might also use cognates (orthographically similar words) identified, for instance, with the edit distance (Navarro 2001 ) instead of the shared words only. However, both approaches improve retrieval results only for closely related language pairs, where enough shared words and cognates are observed. In the absence of such words and any translation resources, we need to turn to a more general and language-independent method. The next section presents and discusses different methods for auto-matic cross-language lexicon extraction from the already obtained / and w per-topic word guage pairs. The lexicon entries will be used to remodel the P lex ( q i | D J ) part of Eq. ( 7 ). 5 LDA-based Cross-language lexicon extraction (Intermezzo) This chapter shows the potential of a cross-language latent topic model to successfully identify translation candidates and semantically related words based on language-specific per-topic word distributions (Eq. 1 ) and a shared topic space learned during training of the model. The models for identifying translation candidates from probability distributions of a cross-language latent topic model are thoroughly presented and evaluated by Vulic  X  et al. ( 2011 ). Based on those results, for the cross-language lexicon extraction within the CLIR task, we opt for the combined TI 1 Cue method, which is computationally feasible since it uses a limited topic space while extracting lexicon entries. The core methods underlying the combined method will be presented shortly. 5.1 Cue method A straightforward approach (called the Cue method) tries to express similarity between two words emphasizing the associative relation between the two words in a natural way. It as a response to a cue source word w 1 . For the BiLDA model we can write:
Probability P ( w 2 | z j ) follows directly from the per-topic word distributions, while we still need to find a way to compute conditional topic distributions P ( z j | w 1 ), which describe a probability that a given word is assigned to a particular topic. If we apply Bayes X  rule, we respectively. We assume P ( Z ) to be a uniform distribution (Griffiths et al. 2007 ) 7 . P ( w )is given by P ( w ) = where Norm / denotes the normalization factor for the currently observed source language word w i .
 We can then calculate the similarity between two words w 1 and w 2 as follows:
The conditioning from Eq. ( 9 ) automatically compromises between word frequency and semantic relatedness (Griffiths et al. 2007 ), since higher frequency words tend to have semantically related topics dominate the sum. 5.2 TI method Another approach borrows an idea from information retrieval and constructs word vectors over a shared latent topic space. Values within vectors are the TF-ITF (term frequency X  inverse topic frequency) scores which are calculated in a completely analogical manner as the TF-IDF scores for the original word-document space (Manning and Schu  X  tze 1999 ). If with a source topic z k and refers to the absolute non-smoothed counts after the Gibbs sampling (see  X  X  Appendix  X  X ).
 Term frequency (TF) of the source word w i for the source topic z k is given as: Inverse topical frequency (ITF) measures the general importance of the source word w i across all source topics. Rare words are given a higher importance and thus they tend to be more descriptive for a specific topic. The inverse topical frequency for the source word w i is calculated as 8 : The final TF-ITF score for the source word w i and the topic z k is given by TF ITF i ; k  X  TF i ; k ITF i . We calculate the TF-ITF scores for target words associated with target topics in an analogical manner. Source and target words share the same K -dimen-sional topical space, where K -dimensional vectors consisting of the TF-ITF scores are built for all words. The standard cosine similarity metric is then used to find the most similar word vectors from the target vocabulary for a source word vector. We name this method the TI method. For instance, given a source word w 1 represented by a K -dimensional vector SV 1 and a target word w 2 represented by a K -dimensional vector TV 2 , the similarity between the two words is calculated as follows: 5.3 Properties of the methods Topic models have the ability to build clusters of words which might not always co-occur together in the same textual units and therefore add extra information of potential relat-methods for automatic extraction of a cross-language lexicon interpret and exploit per-topic word distributions in different ways. Hence, by combining the methods and capturing different evidences, we are able to boost overall scores. The two methods are linearly combined, where the overall score is given by: The value of c is empirically set to 0.1. We have used this combined TI 1 Cue method in all our experiments.

When parallel corpora are not available, standard models for lexicon extraction (i.e., learning translation probabilities) from parallel corpora such as IBM Models (Och and Ney 2003 ) are then unusable. In that case, extracting the lexicon from comparable corpora using topic models proves to be very useful, since we have detected (Vulic  X  et al. 2011 ) that our TI ? Cue method for CLE significantly outperforms similarity-based methods such as cosine similarity with TF-IDF word vectors. 9
The proposed methods from Sects. 5.1 and 5.2 (and their combination) possess another desirable property. They all generate lists of semantically related words, where synonymy is not the only semantic relation observed. Such lists provide comprehensible and useful contextual information in the target language for the source word, even when the correct translation candidate is missing, as presented in Table 1 . We believe that such cross-language semantic relatedness might serve as a useful aid for CLIR, even when the exact translation is absent.

Since all the methods provide ranked lists with scores that measure the strength of cross-language similarity between two words, it is straightforward to convert these ranked lists into a probabilistic lexicon. Each item in the ranked list holds its TI 1 Cue score as shown in Fig. 2 . In order to reduce the complexity of calculations and storage, we decided to use only the subset of top V words from the complete ranked list. Probability P ( w i | e j ), which models the degree of association between a source word w i and a target word e j found in follows: Probabilities P ( w i | e j ) might change depending on the value of V . 6 LDA-based cross-language information retrieval (Act II) This chapter describes several retrieval models that additionally exploit the knowledge from a BiLDA-induced cross-language lexicon. The construction of the lexicon is described in Sect. 5 .

Recall that one lexicon entry for a word w 1 in the source language is in fact a ranked list of words in the target language with their associated scores. The ranked list is a collection of words that are semantically related to the word w 1 , based on their respective distribu-tions over cross-lingual topics. While the quality of the lexicon might prove to be inferior for direct translation tasks, it will still be useful for the CLIR task. Since the lexicons are unrelated to features of test collections, we evaluate all the lexicons we obtain (with TI 1 Cue ) after we train the topic models with different parameters, and simply use the best ones for all the experiments.

Each lexicon entry is simply transformed into a probabilistic lexicon by normalizing the scores (Eq. 16 ). 10 6.1 Lex-only model The simplest model which uses the knowledge from the lexicon relies on Eq. ( 6 ). In case a source word q i exists in the target language vocabulary 11 , Eq. ( 6 ) is applied directly. If the word q i does not exist in the target vocabulary, we need to reach out for the probabilistic lexicon. We closely follow the translation model as presented in Berger and Lafferty ( 1999 ) and Xu et al. ( 2001 ). If top V words from the lexicon entry are taken into account for retrieval, the probability P lex ( q i | D J ) is then given by: The summation goes over the top V target words from the ranked list of the lexicon entry calculated by Eq. ( 16 ) when only top V words are taken into account, while P ( e v | D J ) can be background probability, needed in case there is no lexicon entry for the query word q i .We call this model the lex-only model . It uses only evidences from the lexicon combined with the evidence of shared words. 6.2 LDA-lex model The next model combines the knowledge from the lexicon (the lex-only model from the previous section) with the LDA-only model given by Eq. ( 4 ). The model follows Eq. ( 7 ), vocabularies, and by ( 17 ) for all other words. This model has been named the LDA-lex model .

The model is underpinned by several a priori assumptions: (i) if a word occurs in both source and target vocabularies, it is reasonable to assume that the word speaks for itself more than its translations do (for instance, if someone searches for documents related to Barack Obama , no translation is needed 12 ), (ii) if a word is not shared, one may use a list of semantically related words in the target language, from the lexicon obtained from the per-topic word distributions learned during the training of the topic model. It is convenient, since it uses the same infrastructure as the topic model and does not require any additional resource nor dictionary, (iii) the  X  X  X DA-part X  X  13 of the retrieval model introduces additional topical knowledge, since it connects words in the source language with documents in the target language through the shared space of interlingual topics and groups together words appearing in similar contexts. The modeling of probability P ( q i | D J ) follows these steps:
P where P ( q i | e v ) is calculated using Eq. ( 16 ), and P ( e v | D J ) using Eq. ( 18 ). 2. Calculate the probability P lda ( q i | D J ): 3. Combine the calculated probabilities P lex ( q i | D J ) and P lda ( q i | D J ):
If we deal with distant languages or languages that do not share the same alphabet, we can treat each word from the query in the source language as the word unobserved in the target vocabulary, and use only Eq. ( 17 ) to model the lexical part of the retrieval model. 7 Experimental setup 7.1 Training collections The data used for training of the models is collected from various sources and varies strongly in theme, style and its  X  X  X omparableness X  X . The only constraint on the training data is the need for an initial document alignment (Sect. 3.1 ), and it is the only assumption our BiLDA model utilizes during training.
The first subset of our training data is the Europarl corpus (Koehn 2005 ), extracted from proceedings of the European Parliament and consisting of 6,206 documents in English and Dutch. We use only the evidence of document alignment during the training and do not benefit from the  X  X  X arallelness X  X  of the sentences in the corpus.

Another training subset is collected from Wikipedia dumps 14 and consists of paired documents in English and Dutch. Since the articles are written independently and by different authors, rather than being direct translations of each other, there is a considerable amount of divergence between aligned documents. The aligned articles often have a dif-ferent focus on a subject, which results in different subtopics being addressed. Our Wikipedia training sub-corpus consists of 7,612 documents which vary in length, theme and style, discussing many different subjects including medicine, science, geographic locations, historical figures, industry, political issues, etc.

We removed stop words (429 in English and 110 in Dutch). Our final vocabularies consist of 76,555 words in English, and 71,168 words in Dutch. 7.2 Test collections and queries We have carried out two conceptually different sets of experiments to evaluate our retrieval models. The first set of experiments tests the performance of the retrieval models on a less difficult task, where a subset of the training collections is used for testing. Another set of experiments has been conducted on test collections that were not used for training beforehand. Here, we deal with a more complex problem; we want to retrieve documents from a monolingual collection, which might be completely topically unrelated to our training collections (e.g. we train the BiLDA model on Wikipedia articles and Europarl documents, infer the BiLDA model on a newswire corpus, and use the BiLDA-based retrieval models on that newswire corpus). Despite the obvious topical disparity, we believe that by having enough training data to cover many different topics, we will be able to learn per-topic word document distributions and infer per-document topic distributions that will lead to quality CLIR models, even for topically unrelated monolingual corpora.
Parameters a and b for the BiLDA training are set to values 50/ K and 0.01 respectively, where K denotes the number of topics (Steyvers and Griffiths 2007 ). The Dirichlet parameter l is set to 2,000 in all models where it is used. Parameters d 1 , d 2 and d 3 are all set to negligible values. 15 , while we set the interpolation parameter k = 0.3 for all experiments, which assigns more weight to the topic model. 7.2.1 Wikipedia as a test collection for the known-item search Being document-aligned, Wikipedia data might serve as a framework for the initial evaluation of our models in the less difficult task, where test articles have already been observed during the BiLDA training. The idea was to simulate the cross-language known-item search , since it provides a precise semantics and thus removes potential issues with defining an exact information need and assigning relevance judgments. The known-item known-item search in the cross-lingual setting might refer to finding a correct Wikipedia article in the target language with a query provided in the source language.

We did not have the ground truth nor existing queries for this task, so we decided to construct it by adapting the approach from Azzopardi et al. ( 2007 ) to the cross-language setting. Their approach has already proven useful for automatic generation of queries for a monolingual known-item search. As the first step, we randomly sample 101 pairs of Wikipedia articles from our training collection. The sampled articles will be regarded as known items we want to retrieve. After that, we generate a known-item query by selecting a document (in this case, the known-item) and constructing a query for that known item. For example, if we have a Wikipedia article pair ( A i E , A i D ), where A i E denotes the English article and A i D its Dutch counterpart, we are able to generate a known-item query from the For producing the automatic known-item queries (for instance, a query in English to retrieve a Dutch article), we have followed these steps: 1. Pick a Dutch article A i D for which an English query Q will be generated. 2. Initialize an empty English query Q = {} for the current article A i D . Query words are 3. Choose the query length L with probability P ( L ). The query length is drawn from a Poisson 4. For each word w e in the article A i E , calculate probability P  X  w e j M A E Quality of the query is influenced by the d 4 parameter which models noise in the sampling process. As d 4 decreases to zero, the user is able to recall the content of the article in its entirety. Following the same line of thinking, as d 4 increases to 1, the user knows that the article exists in the collection, but is not able to recollect any of the words relevant to the article. According to Azzopardi et al. ( 2007 ), setting d 4 = 0.2 reflects the average amount of lihood of selecting the word w e from the document A i E , we have opted for the Popular ? Discrimination Selection strategy which tries to compromise between popular words in a document (we assume that the user tends to use more frequent words as query words) and discriminative words for a document (the user considers information outside the scope of a document, and tries to construct a query from such query words that discriminate the par-ticular document from the rest of the collection). The strategy is summarized in the following probability distribution: M is the number of documents in the entire collection, n ( w e , A i E ) denotes the number of occurrences of w e in the article A i E , and df ( w e ) is the document frequency of w e . 5. Rank all words from the document A i E based on the scores obtained after employing 6. Take the top L words from the ranked list as the query words of the known-item query
We perform this automatic query generation for all 101 article pairs in both directions, designing 101 Dutch queries to retrieve English documents and vice versa. For instance, for a Dutch article discussing halfwaardebreedte (full width at half maximum) , a query in English is Q = { width , hyperbolic , variable , deviation }. 7.2.2 CLEF test collections Our experiments have been carried out on three data sets taken from the CLEF 2001-2003 CLIR campaigns: the LA Times 1994 ( LAT ), the LA Times 1994 and the Glasgow Herald 1995 ( LAT 1 GH ) in English, and the NRC Handelsblad 94-95 and the Algemeen Dagblad 94-95 ( NC 1 AD ) in Dutch.

We extracted queries from the title and description fields of all CLEF themes 18 for each year. Queries without relevant documents were removed from the query sets. Table 2 a used for testing. 8 Results and discussion This section reports our experimental results for two main tasks: (i) retrieval models have been tested within a lenient experimental setup, where the goal is to perform a cross-language known-item search over Wikipedia articles (English queries, Dutch articles and vice versa) that have already been used to train the topic model, (ii) retrieval models have been tested with CLEF test collections for the tasks of English-Dutch and Dutch-English cross-language information retrieval. The cross-language topic model is trained just once on a large bilingual training corpus. It may then be used for both tasks 19 .

First, we describe our training settings and evaluate lexicons obtained from per-topic word distributions of the BiLDA model trained with different parameters in order to choose the best lexicon for CLIR models. Second, we test our retrieval models from Sects. 4 and 6 in the known-item search of Wikipedia articles and report our findings. As the next step, we carry out different experiments for English-Dutch and Dutch-English cross-lan-guage information retrieval: (1) we compare our LDA-only to one baseline that has also tried to exploit latent topic spaces for CLIR (standard monolingual LDA trained on con-catenated paired documents as described by Roth and Klakow 2010 ), and we also compare it to the simple unigram model from Sect. 4.2 . We want to prove the soundness and the utility of the LDA-only model and, consequently, other models that later build upon the foundation established by the LDA-only model ( LDA-unigram and LDA-lex ), (2) we pro-vide an extensive evaluation over all CLEF test collections with all BiLDA-based models ( LDA-only , LDA-unigram and LDA-lex ), (3) we compare our LDA-based models with similar models for monolingual retrieval (queries and documents in the same language) and a model that uses Google Translate tool to translate query words and then performs monolingual retrieval, and measure the decrease of performance for CLIR, (4) we also compare the best scoring combined LDA-lex model with the lex-only model that uses only evidences of the shared words and knowledge from the extracted lexicon, and, as the final step, (5) we compare results for all test collections when the BiLDA model is trained on different types of training data (parallel, comparable and combined) and show that com-parable data boost retrieval performance.

We have trained our BiLDA model with different number of topics (400, 1,000 and 2,200) on the combined EP 1 Wiki corpus. Additionally, for the purpose of comparing retrieval performance when the BiLDA model is trained on different corpora, we have also trained the BiLDA model with K = 1,000 topics 20 on two different subsets of training corpora: (1) the parallel Europarl corpus ( EP ) 21 , and (2) the comparable Wikipedia corpus ( Wiki ).

We have also empirically detected that the optimal value for V is 10, 22 so we have used the top 10 items from the ranked list for each lexicon entry in all experiments with the lex-only and the LDA-lex model. 8.1 Lexicon extraction and evaluation 8.1.1 Evaluation settings and results Following the intuition that more data lead to better per-topic word distributions, we have decided to compare our lexicons extracted from the BiLDA model trained on the EP ? Wiki corpus. Following the results from Vulic  X  et al. ( 2011 ), we have used the TI 1 Cue lexicon extraction method.

As test sets, we use the set of words appearing in English and Dutch queries extracted from the CLEF themes. These sets form a true representation of a general vocabulary, since they contain high-frequency, medium-frequency and low-frequency words. Our test sets consist of 711 English words and 724 Dutch words. Our evaluation relies on Recall@1 scores 23 (the percentage of words where the first word from the list of translations is the correct one) and Mean Reciprocal Rank (MRR) scores as given by: where E V denotes the top V words from the ranked list of a lexicon entry. V is set to 10 for all lexicons, since we use the same V in further evaluations of our CLIR models. We compare our candidates against translation candidates acquired by the Google Translate tool, which serves as the ground truth for evaluations, although one should be aware that Google Translate does not always necessarily return the best or even correct translation (Dolamic and Savoy 2010 ). We also provide the percentage of translations detected in the ranked lists (Recall@10) 23 . Results are presented in Table 3 . 8.1.2 Discussion As we can see in Table 3 , lexicon entries are far from perfect, but we believe that lexicons will be useful for CLIR since, even when a correct translation is not found, other words from the ranked list carry enough semantics of the input word. By expanding a word from the query with the words from the list, we should be able to retrieve documents that cannot be entirely captured with the simple unigram or the LDA-only model. For instance, a correct translation for a Dutch word zender (transmitter, sender) is not found, but the first five words in the list of related English words are (radio, broadcast, broadcasting, tele-vision, broadcaster) . This group of words can certainly help retrieving the correct docu-ments associated with the query word zender , even when the correct translation is missing. For the next experiments we have decided to use the best scoring English-Dutch and Dutch-English lexicons extracted from a model with 2,200 topics, based on the results from Table 3 . 24 8.2 Cross-language known-item search for Wikipedia articles 8.2.1 Experimental setup and results The cross-language known-item search has been carried out for 101 pairs of Wikipedia articles randomly sampled from 7, 612 pairs of the English-Dutch Wikipedia training corpus. Experiments have been conducted for both possible retrieval directions (English to Dutch and Dutch to English). The BiLDA model was trained on the EP ? Wiki corpus. To make the search a bit more difficult, we have also included the Europarl documents in the search space. Our search space then consisted of 13, 818 documents from all training document pairs. Scores for the simple unigram model and the lex-only model are given in Table 4 . We report Recall@1 (the only relevant document is retrieved as the first in the list) and Recall@5 (the only relevant document is retrieved among the top 5 retrieved documents) scores of our BiLDA-based models for both search directions in Tables 5 and 6 . 8.2.2 Discussion We have drawn several conclusions based on the results presented in Sect. 8.2.1 :  X  Table 4 reveals that adding lexicon entries significantly helps in improving overall  X  The LDA-only model is outperformed by the LDA-unigram and the LDA-lex model  X  LDA-unigram and LDA-lex display comparable results, with a slight advantage for the  X  We have successfully applied a method from Azzopardi et al. ( 2007 ) to automatically 8.3 Comparison of the LDA-only model with baseline systems From now on, all experiments will be conducted on standard CLEF test collections. The main evaluation measure we use for all further experiments is the mean average precision (MAP).

The LDA-only model serves as the backbone of the two more advanced BiLDA-based document models ( LDA-unigram and LDA-lex ). Since we want to make sure that the LDA-only model constructs a firm and sound language-independent foundation for building more complex retrieval models, we compare it to another system which tries to build a CLIR system based around the idea of latent concept topics: the standard LDA model trained on the merged document pairs. We also compare the LDA-only model to the simple unigram model to make sure that our complex models do not draw its performance mainly from the shared words.

We have trained the standard LDA model on the combined EP ? Wiki corpus with 400 and 1,000 topics and compared the retrieval scores with our LDA-only model which uses the BiLDA model with the same number of topics. The LDA-only model outscores this model by a huge margin. The MAP scores for standard LDA are very low, and vary between the MAP of 0.01 and 0.03 for all experiments, which is significantly worse than the results of the LDA-only model as seen in Table 7 . A problem with this baseline method might be in concatenation of document pairs, since one language might dominate the merged document. On the other hand, BiLDA keeps the structure of the original document space intact.

The MAP scores of the simple unigram model for NL 2001, NL 2002, and NL 2003 are 0.0274, 0.0343, and 0.0292, respectively, while the MAP scores for EN 2001, EN 2002, and EN 2003 are 0.0643, 0.1030, and 0.0827, 25 respectively. Comparison of precision-recall curves for the LDA-only model and the simple unigram model is presented in Figs. 5 a, b.

Following these results, we are justified to use BiLDA in other retrieval models. 8.4 Comparison of LDA-only, LDA-unigram and LDA-lex In this subsection, the idea was to compare three retrieval models that rely on per-docu-ment topic distributions of the BiLDA model, once it is inferred on test corpora. Besides that, we wanted to test whether the knowledge from shared words (as in the LDA-unigram model, and the knowledge of the shared words combined with the knowledge from BiL-DA-induced lexicons (as in the LDA-lex model) positively affect retrieval. 8.4.1 Comparison of models with a fixed number of topics (K = 1,000) The LDA-only model, the LDA-unigram model and the LDA-lex model have been evalu-shows the precision-recall values obtained by applying all three models to English test collections with Dutch queries, while Fig. 3 b shows the precision-recall values for Dutch test collections and English queries.

As the corresponding figures show, the LDA-only model seems to be too coarse to be used as the only component of an IR model (e.g., due to its limited number of topics, words in queries unobserved during training). However, combining it with words shared across languages and lexicon entries from BiLDA-induced lexicons leads to a drastic increase in results. Results of the LDA-lex model which scores better than the LDA-unigram model seem especially promising. The LDA-unigram relies solely on shared words, which clearly makes it language-biased, since its performance relies heavily on the amount of shared words (or the degree of closeness between two languages). On the other hand, the LDA-lex has been envisioned for CLIR between distant language pairs. 8.4.2 Varying the number of topics The main goal of the next set of experiments was to test the importance of the lexicon, and the behavior of our two best models if we vary the number of topics in BiLDA training. We have carried out experiments with the CLIR models relying on BiLDA trained with dif-ferent numbers of topics (400, 1,000 and 2,200). The MAP scores of the LDA-unigram and the LDA-lex model for all campaigns are presented in Table 7 , while Fig. 4 shows the associated precision-recall values. 8.4.3 Discussion We observe several interesting phenomena from Table 7 and Fig. 4 :  X  The LDA-lex model obtains the best scores for all test collections which proves the  X  The margins between scores of the LDA-unigram and the LDA-lex model are generally  X  Due to a high percentage of shared words, especially per English query (see the  X  The margins between scores of the LDA-unigram and the LDA-lex model are generally  X  Although a larger number of topics should intuitively lead to a more fine-grained model 8.5 Comparison with monolingual LDA-based models and LDA-based models that use With this set of experiments, we investigated how efficient our LDA-based translation process actually is. Thus, we decided to compare our LDA-based models already evaluated in Sect. 8.4 with another four models: (1) a model that performs monolingual retrieval in the same fashion as our CLIR LDA-only model ( MLDA-only ), (2) a model that performs monolingual retrieval in the same fashion as our CLIR LDA-unigram model, as presented by Wei and Croft ( 2006 )( MLDA-unigram ), (3) a model that uses Google Translate to perform word-to-word translation of query words, and then performs monolingual retrieval using MLDA-only ( GT 1 MLDA-only ), (4) a model that uses Google Translate in the same way, and then employs the monolingual MLDA-unigram ( GT 1 MLDA-unigram ). In order to use these models, we have trained standard monolingual LDA with K = 1,000 topics for both English and Dutch side of our training corpora. MAP scores for these models are presented in Table 8 , 28 while MAP scores for our CLIR models have already been pre-sented in Table 7 .

By examining the results in Tables 7 and 8 , we derive several conclusions:  X  As expected, the monolingual MLDA-unigram model outperforms our CLIR models,  X  Low results for MLDA-only for monolingual Dutch retrieval when we train standard  X  A significant drop in performance for both retrieval directions is marked when we use  X  Our combined CLIR models outperform GT ? MLDA-unigram for English queries and  X  For almost all CLEF campaigns, our LDA-unigram and LDA-lex models display 8.6 Comparison of Lex-only and LDA-lex 8.6.1 Motivation for comparison and results We also want to compare our best scoring LDA-lex model that blends evidences from lexicons and shared words, and evidences from probability distributions of BiLDA, with the lex-only model which uses only the shared words and the lexicon knowledge as evidences. We have already proved that the combined, evidence-rich model yields better scores than the LDA-only that uses only evidences in the form of per-topic word and per-document topic distributions. We now want to prove that it also scores better than the more straightforward lex-only model that uses only lexical evidences. MAP scores for the lex-only model are 0.1998, 0.1810 and 0.1513 for NL 2001, NL 2002 and NL 2003 (Dutch queries, English documents), respectively, and 0.1412, 0.1378 and 0.1196 for EN 2001, EN 2002 and EN 2003 (English queries, Dutch documents). The best MAP scores for LDA-lex are given in Table 7 . Figure 6 a shows the comparison of the associated precision-recall diagrams for all English collections (with queries in Dutch), and Fig. 6 b shows the comparison for all Dutch collections (with queries in English). 8.6.2 Discussion Figure 3 a and b have already shown the superiority of the LDA-lex model over the LDA-only model. The results in this subsection again show the superiority of the LDA-lex model over the Lex-only model. The fact that the LDA-lex model combines the evidences from the other two models makes it the strongest model. The other two models utilize only subsets of the available evidences which makes them more error-prone. For instance, if the semantics of a word from a query is not captured by the  X  X  X DA-part X  X  (as in the LDA-only model), that model is unable to retrieve any documents strongly related to that word. On the other hand, if the same problem occurs for the LDA-lex model, it still has a possibility to look up for an aid in the lexicon. Additionally, if a document scores good for more than one evidence, it strengthens the belief that the document might be relevant for the query. 8.7 Training with different types of corpora 8.7.1 Motivation for comparison and results In the final set of experiments with CLEF data, we measure the performance of our topic models trained on three different types of corpora (EP, Wiki, EP ? Wiki) with K = 1,000 topics. We wanted to find out if and how Wikipedia training data help the retrieval. Moreover, we wanted to test our  X  X  X he more the merrier X  X  assumption that more training data lead to better probability distributions in the BiLDA model and, following that, better retrieval models. The LDA-unigram model and the LDA-lex model have been used for all the experiments. Table 9 shows the MAP scores over all CLEF test collections.
Figure 7 a shows precision-recall values of the LDA-lex for Dutch queries and English documents, and Fig. 7 b for English queries. 8.7.2 Discussion The results lead us to several conclusions:  X  They show that the comparable out-of-domain Wikipedia data can be used to train the  X  Table 9 also reveals that accumulating more training data by adding comparable  X  More training data lead to better per-topic word and per-document topic distributions  X  These initial experiments also reveal a clear advantage of using our automatically-9 Conclusions and future work We have proposed and constructed a novel language-independent framework for cross-language information retrieval, built upon the idea of cross-language topic models trained on document-aligned corpora, which does not use any type of an external translation resource such as a machine translation system or a dictionary that is hand-built or extracted from parallel data. The models employ translation dictionaries extracted directly from per-topic word distributions of the trained topic model instead. It makes the framework lan-guage-independent and applicable to any language pair. We have successfully integrated the automatically generated BiLDA-based cross-language lexicon into novel CLIR models. We have proved that models that exploit more different evidences (not necessarily dis-junct) yield better retrieval results. Naturally, the cross-language lexicon proves to be of greater importance for source language queries where query words are not observed in target language test collections. We have also shown that adding out-of-domain compa-rable data to the training data boosts the quality of the topic model and, consequently, the proposed CLIR models that perform better even on topically unrelated test collections.
We have thoroughly evaluated all our models using our manually constructed Wiki-pedia test set and standard test collections from the CLEF 2001 X 2003 CLIR campaigns, presenting and explaining their key advantages and shortcomings. We have shown that our combined models, which fuse different evidences (probability distributions from the BiLDA model, unigrams shared across languages, knowledge from the BiLDA-induced lexicon) generally obtain the best scores.

Estimation of the cross-lingual BiLDA model is done offline (following the  X  X  learn once, use many  X  X  principle), so there are no restrictions in utilizing the proposed framework in real-world applications. We have shown that a large amount of Wikipedia articles paired through the interlingual links constitutes a quality dataset to train a cross-language topic model, which can later be used to cross-lingually retrieve documents in monolingual data collections. The BiLDA model can be easily expanded to cover more than two languages, while the CLIR framework and the methods for CLE underpinned by it remain completely unchanged and follow the same steps.

The BiLDA model was originally designated for parallel corpora, where an a priori assumption of a shared topical space is clearly valid. However, we have proved its applicability on comparable document-aligned data such as Wikipedia, where a greater divergence exists between topics and subtopics being addressed in different languages. Hence, in future work, we plan to expand the standard BiLDA or construct a novel cross-language model similar to the work presented by De Smet et al. ( 2011 ) which will be able to learn the number of topics dynamically during training. Those models should fit more divergent comparable training datasets. By using potentially novel models more suitable to comparable document-aligned corpora, we hope to learn better per-topic word and per-document topic distributions which will lead to retrieval models of higher quality. We also plan to combine our models with relevance models. Finally, we plan to apply the con-structed framework to other cross-lingual tasks, such as document summarization and classification.
 Appendix: Gibbs sampling for BiLDA The goal of training the bilingual LDA (BiLDA) model is double; given a document-aligned corpus, i.e., a collection of aligned document pairs: 1. Discover which words together form vocabulary topics (for both source and target 2. Discover which topics appear in each document pair, i.e., per-document topic
The most likely values of h , / , and w that explain the corpus are thus sought. The topics will then contain meaningful words that share a semantic meaning, relevant to the training collection of words. We will present how to learn those most likely values for BiLDA using Gibbs sampling (Geman and Geman 1984 ).

We will show the derivation and explain the notation for the source side of a bilingual corpus only (denoted by S , also in the superscripts of the variables involved in the deri-analogously. h and / will not be calculated directly, but rather inferred afterwards. As a result, they are integrated out of the calculations. The only hidden variable that is left then is z . Gibbs sampling then dictates that each z is cyclically updated, by being sampled from its posterior given all other variables (including all other z ji S -s). For the S part of each document pair d j and each word position i , the probability is calculated that z ji S assumes, as its new value, one of the K possible topic indices. This new value is indicated with the variable k * : Both h and / have a prior Dirichlet distribution and their posterior distributions are updated with the counter variable n (which counts the number of assigned topics in a document) and the counter variable v (which counts the number of assigned topics in the corpus) respectively (see the explanations of the symbols after the derivation). So, the expected values  X  which, explicitly written in function of the formula for an expected value of a Dirichlet distribution gives: So the formulas for Gibbs sampling used in the BiLDA training are and The last two formulas use important counter variables. The counter n S j ; k denotes the number of times a source word w ji S occurs with a source topic k * in the source document of (i.e., n S j ; k 1). The same is true for the target side T .

When a  X  X   X  X  appears in the subscript of a counter variable, this means that the counts range over all values of the variable whose index the  X  X   X  X  takes. So, while n S j ; k counts the number of values of w ji S over one topic k * in d j , n S j ; does so over all topics in d j .
The second counter variable, v S k ; w k on the source side of the corpus, but not counting the current w ji S (i.e., v S k ; w source vocabulary that can be found on position i in a source document from the document pair d j , and z ji S is the source topic index associated with the source word w ji S .
Additionally, z S denotes all source topic indices for the document pair d j ; z S : ji denotes all source topic indices in d j excluding w ji S . w S denotes all source words in a corpus, and | W
S | is the number of source words in the corpus. K is the number of topics set a priori before training, and a and b are the parameters of the uniform conjugate Dirichlet priors (see Griffiths et al. 2007 ; Heinrich 2008 ). v S k ; ; : counts the total number of source words associated with source topic k * in the whole corpus, as it is the sum over all possible source words (a  X  X   X  X  appears instead of the w ). Again, because of the : symbol in the superscript, the current w ji S is not counted (i.e., v
With formulas ( 26 ) and ( 27 ), each z ji S (and z ji T ) of each document pair is sampled and updated in turn. After a random initialization (usually a uniform distribution of proba-bution of h , / and w , after a time called the burn-in period . The estimations for h j , / k and w k can then be calculated from these burned-in samples.

As can be seen from the first term of Eqs. ( 26 ) and ( 27 ), the document pairs are linked vocabulary count variables operate only within the language of the term currently being considered.

Finally, one of the main advantages of the BiLDA model is an efficient procedure for inference. By taking the vocabulary distributions and the prior on the per-document topic distributions outside of the corpus, the per-document topic distribution of new documents can be inferred using the same Gibbs sampling formulas used for training.
 References
