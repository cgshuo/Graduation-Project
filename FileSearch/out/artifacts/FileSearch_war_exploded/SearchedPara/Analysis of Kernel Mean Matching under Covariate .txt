 Yao-Liang Yu yaoliang@cs.ualberta.ca Csaba Szepesv  X ari szepesva@cs.ualberta.ca In traditional supervised learning, the training and test sample are usually assumed to be drawn from the same probability distribution, however, in prac-tice, this assumption can be easily violated for a vari-ety of reasons, for instance, due to the sampling bias or the nonstationarity of the environment. It is there-fore highly desirable to devise algorithms that remain effective under such distribution shifts.
 Needless to say the problem is hopeless if the training and test distribution share nothing in common. On the other hand, if the two distributions are indeed related in a nontrivial manner, then it is a quite remarkable fact that effective adaptation is possible. Under rea-sonable assumptions, this problem has been attacked by researchers from statistics (Heckman, 1979; Shi-modaira, 2000) and more recently by many researchers from machine learning, see for instance, Zadrozny (2004); Huang et al. (2007); Bickel et al. (2009); Ben-David et al. (2007); Blitzer et al. (2008); Cortes et al. (2008); Sugiyama et al. (2008); Kanamori et al. (2009). We focus in this paper on the covariate shift assump-tion which was first formulated by Shimodaira (2000) and has been followed by many others.
 The assumption that the conditional probability dis-tribution of the output variable given the input vari-able remains fixed in both the training and test set is termed covariate shift , i.e. the shift happens only for the marginal probability distributions of the covari-ates. It is well-known that under this setting, the key to correct the sampling bias caused by covariate shift is to estimate the Radon-Nikodym derivative (RND), also called the importance weight or density ratio. A number of methods have been proposed to estimate the RND from finite samples, including kernel mean matching (KMM) (Huang et al., 2007), logistic re-gression (Bickel et al., 2009), Kullback-Leibler impor-tance estimation (Sugiyama et al., 2008), least-squares (Kanamori et al., 2009), and possibly some others. Despite of the many algorithms, our current under-standing of covariate shift still seems to be limited. From the analyses we are aware of, such as (Gretton et al., 2009) on the confidence bound of the RND by KMM, (Kanamori et al., 2012) on the convergence rate of the least-squares estimate of the RND, and (Cortes et al., 2008) on the distributional stability, they all assume that certain functions lie in the reproducing kernel Hilbert space (RKHS) induced by some user se-lected kernel. Since this assumption is impossible to verify (even worse, almost certainly violated in prac-tice), one naturally wonders if we can replace it with something more reasonable. Such goal is pursued in this paper and constitutes our main contribution. We consider the following simple problem: Given the E
Y te , provided covariate shift has happened? Note that we do not observe the output Y te i on the test sample. This problem, at a first glance, ought to be  X  X asy X , after all we are humbly asking for estimat-ing a scalar. Indeed, under usual assumptions, plus the nearly impossible assumption that the regression function lies in the RKHS, we prove a parametric rate, orem 1 below (to fix ideas, we focus exclusively on KMM in this paper). For a more realistic assump-tion on the regression function that we borrow from learning theory (Cucker &amp; Zhou, 2007), the conver-gence rate, proved in Theorem 2, degrades gracefully parameter measuring certain regularity of the regres-sion function (in terms of the kernel). Observe that in the limit when  X   X   X  , the regression function even-tually lies in the RKHS and we recover the previous parametric rate. In this regard our bound in Theo-rem 2 is asymptotically optimal. A very nice feature we discovered for the KMM estimator is that it does not require knowledge of the smoothness parameter  X  , thus, it is in some sense adaptive .
 On the negative side, we show that, if the cho-sen kernel does not interact very well with the un-known regression function, the convergence rate of the KMM estimator could be exceedingly slow, roughly regularity of the regression function. This unfortunate result should draw attention to the importance of se-lecting which kernel to be used in practice. A thorough comparison between the KMM estimator and the nat-ural plug-in estimator, conducted in Section 4.3, also reveals the superiority of the former.
 We point out that our results are far from giving a complete picture even for the simple problem we con-sider here, for instance, it is unclear to us whether or not the rate in Theorem 2 can be improved, eventually, to the parametric rate in Theorem 1? Nevertheless, we hope that our paper will convince others about the im-portance and possibility to work with more reasonable assumptions under covariate shift , and as an example, suggest relevant tools which can be used to achieve that goal. In this section we formally state the covariate shift problem under our consideration, followed by some rel-evant discussions. 2.1. Problem Setup Consider the familiar supervised learning setting, where we are given independent and identically dis-tributed ( i.i.d. ) training samples { ( X tr i ,Y tr i ) } the joint (Borel) probability measure P tr (d x, d y ) on the (topological) domain X  X Y , and i.i.d. test sam-ples { X te i } n te i =1 from the joint probability measure P te (d x, d y ) on the same domain. Notice that we do not observe the output Y te i on the test sample, and more importantly, we do not necessarily assume that the training and test sample are drawn from the same probability measure. The problem we consider in this paper is to estimate the expected value E Y te from the how fast, say, the 1  X   X  confidence interval for our es-timate shrinks to 0 when the sample sizes n tr and n te increase to infinity.
 This problem, in its full generality, cannot be solved simply because the training probability measure can be completely irrelevant to the test probability mea-sure that we are interested in. However, if the two probability measures are indeed related in a nontriv-ial way, our problem becomes solvable. One particular example, which we focus on hereafter, is known in the literature as covariate shift (Shimodaira, 2000): Assumption 1 (Covariate shift assumption) We use the same notation for the joint, conditional and marginal probability measures, which should cause no confusion as the arguments would reveal which mea-sure is being referred to. Note that the equality the conditional probability measure, whose existence can be confirmed under very mild assumptions.
 Under the covariate shift assumption, the difficulty of our problem, of course, lies entirely on the potential mismatch between the marginal probability measures P tr (d x ) and P te (d x ). But the Bayes rule already sug-gests a straightforward approach: P where the three quantities on the right-hand side can all be estimated from the given samples. However, in order for the above equation to make sense, we need Assumption 2 (Continuity assumption) The Radon-Nikodym derivative  X  ( x ) := dP te dP defined and bounded from above by B &lt;  X  .
 Note that B  X  1 due to the normalization constraint R
X  X  ( x )P tr (d x ) = 1. The Radon-Nikodym derivative (RND) is also called the importance weight or the not well-defined, i.e. , there exists some measurable in general we cannot infer P te (d x, d y ) from merely P ate shift assumption. The bounded from above as-sumption is more artificial. Recently, in a different setting, (Cortes et al., 2010) managed to replace this assumption with a bounded second moment assump-tion, at the expense of sacrificing the rate a bit. For us, since the domain X will be assumed to be compact, the bounded from above assumption is not too restrictive (automatically holds when  X  ( x ) is, say, continuous). Once we have the RND  X  ( x ), it becomes easy to cor-rect the sampling bias caused by the mismatch be-tween P tr (d x ) and P te (d x ), hence solving our problem. Formally, let be the regression function, then By the i.i.d. assumption, a reasonable estimator for E
Y te would then be 1 n similarly to most publications on covariate shift, our problem boils down to estimating the RND  X  ( x ). 2.2. A Naive Estimator? An immediate solution for estimating  X  ( x ) is to es-timate the two marginal measures from the train-ing sample { X tr i } and the test sample { X te i } , respec-tively. For instance, if we know a third (Borel) measure Q(d x ) (usually the Lebesgue measure on R d ) such that dard density estimators to estimate them and then set  X  is known to be inferior since density estimation in high dimensions is hard, and moreover, small estimation er-knowledge, there is little theoretical analysis on this seemingly naive approach. 2.3. A Better Estimator? It seems more appealing to directly estimate the RND  X  ( x ). Indeed, a large body of work has been de-voted to this line of research (Zadrozny, 2004; Huang et al., 2007; Sugiyama et al., 2008; Cortes et al., 2008; Bickel et al., 2009; Kanamori et al., 2009). From the many references, we single out the kernel mean match-ing (KMM) algorithm, first proposed by Huang et al. (2007) and is also the basis of this paper.
 KMM tries to match the mean elements in a feature space induced by a kernel k (  X  ,  X  ) on the domain X X X : min where  X  : X 7 X  X  denotes the canonical feature map, H is the reproducing kernel Hilbert space 1 (RKHS) in-duced by the kernel k and k X k H stands for the norm in H . To simplify later analysis, we have chosen to omit the normalization constraint 1 n where is a small positive number, mainly to re-flect the fluctuation caused by random samples. It is not hard to verify that (3) is in fact an instance of quadratic programming, hence can be efficiently solved. More details can be found in the paper of Gretton et al. (2009).
 A finite sample 1  X   X  confidence bound for  X  L (  X  ) (similar as (10) below) is established in Gretton et al. (2009). This bound is further transferred into a confidence bound for the generalization error of some family of loss minimization algorithms in Cortes et al. (2008), under the notion of distributional stability. However, neither results can provide a direct answer to our prob-lem: a finite sample confidence bound on the estimate of 2.4. Plug-in Estimator Another natural approach is to estimate the regression function from the training sample and then plug into the test set. We postpone the discussion and compar-ison with respect to this estimator until section 4.3. We motivate the relevance of our problem in this sec-tion.
 Suppose we have an ensemble of classifiers, say, { f j } N j =1 , all trained on the training sample hence rank, the classifiers by their generalization errors. This is usually done by assessing the classifiers on some hold out test sample { ( X te i ,Y te i ) } n not uncommon that the test sample is drawn from some different probability measure than the training sample, i.e. covariate shift has happened. Since it could be too costly to re-train the classifiers when the test sample is available, we nevertheless still like to have a principled way to rank the classifiers. Let ` (  X  ,  X  ) be the user X  X  favourite loss function, and set Z ij = ` ( f j ( X f . But what if we do not have access to Y te i hence consequently Z te ij ? Can we still accomplish the ranking job? The answer is yes, and it is precisely the covariate shift problem under our consideration. To see that, the covariate shift assumption, that is P tr (d y | x ) = P P te (d z | x ), hence the covariate shift assumption holds for the ranking problem, therefore the confidence bounds derived in the next section provide an effec-tive solution.
 We do not report numerical experiments in this paper for two reasons: 1). Our main interest is on theoretical analysis; 2). Exhaustive experimental results on KMM can already be found in Gretton et al. (2009). This section contains our main contribution, i.e. , a theoretical analysis of the KMM estimator for E Y te . 4.1. The population version Let us first take a look at the population version of KMM 2 , which is much easier to analyze and provides valuable insights:  X   X   X   X  arg min The minimum value is 0 since the true RND  X  ( x ) is apparently feasible, hence at optimum we always have The question is whether the natural estimator R is Z where recall that m ( x ) is the regression function de-fined in (2) and  X  ( x ) is the true RND.
 The equality in (5) indeed holds under at least two con-ditions (respectively). First, if the regression function m  X  X  , then taking inner products with m in (4) and applying the reproducing property we get (5). Second, if the kernel k is characteristic (Sriperumbudur et al., 2010), meaning that the map R X  X ( x )P(d x ) from the space of probability measures to the RKHS H is injec-tive, then we conclude  X   X   X  =  X  from (4) hence follows (5).
 The above two cases suggest the possibility of solv-ing our problem by KMM. Of course, in reality one only has finite samples from the underlying probability measures, thus calls for a thorough study of the empir-ical KMM, i.e. (3). Interestingly, our analysis reveals that in the first case above, we indeed can have a para-metric rate while in the second case the rate becomes nonparametric, hence inferior (but does not seem to rely on the characteristic property of the kernel). 4.2. The empirical version In this subsection we analyze KMM in details. The following assumption will be needed: Assumption 3 (Compactness assumption) X is a compact metrizable space, Y  X  [0 , 1] , and the ker-nel k is continuous, whence k k k  X   X  C 2 &lt;  X  . We use k X k  X  for the supremum norm. Under the above assumption, the feature map  X  is continuous hence measurable (with respect to the Borel  X  -fields), and the RKHS is separable, therefore the Bochner integrals in the previous subsection are well-defined. Moreover, the conditional probability measure indeed exists un-der our assumption.
 We are now ready to derive a finite sample confidence bound for our estimate | 1 n  X   X  is a minimizer of (3). We start by splitting the sum: 1 n where  X  i :=  X  ( X tr i ) and h  X  X  is to be specified later. We bound each term individually. For the last term in (6), we can apply Hoeffding X  X  inequality (Hoeffding, 1963) to conclude that with probability at least 1  X   X  , The first term in (6) can be bounded similarly. Con-ditioned on { X tr i } and { X te i } , we apply again Ho-effding X  X  inequality. Note that  X   X  i ( Y tr i  X  m ( X tr [  X   X  size  X   X  i . With probability at least 1  X   X  , 1 n The second and third terms in (6) require more work. Consider first the third term: 1 n where  X  1: n tr denotes the restriction of  X  to the training is because h  X  H (and the reproducing property of the canonical feature map), the first inequality is by the Cauchy-Schwarz inequality, the second inequality is due to the triangle inequality, and the last inequality is by the optimality of  X   X  and the feasibility of  X  1: n problem (3). Next, we bound  X  L (  X  1: n tr ):  X  with probability at least 1  X   X  , where the inequality follows from the Hilbert space valued Hoeffding in-equality in (Pinelis, 1994, Theorem 3.5). Note that Pinelis proved his inequality for martingales in any 2-smooth separable Banach space (Hilbert spaces are bona fide 2-smooth). We remark that another way, see for instance (Gretton et al., 2009, Lemma 1.5), is to use McDiarmid X  X  inequality to bound  X  L (  X  1: n tr ) by its expectation, and then bound the expectation straight-forwardly. In general, Pinelis X  X  inequality will lead to (slightly) tighter bounds due to its known optimality (in certain sense).
 Finally, we come to the second term left in (6), which is roughly the approximation error in learning the-ory (Cucker &amp; Zhou, 2007). Note that all confidence bounds we have derived so far shrink at the paramet-ric rate O ( p 1 /n tr + 1 /n te ). However, from here on we will have to tolerate nonparametric rates. Since we are going to apply different approximation error bounds to the second term in (6), it seems more con-venient to collect the results separately. We start with an encouraging result: Theorem 1 Under Assumptions 1-3, if the regression function m  X  H (the RKHS induced by the kernel k ), then with probability at least 3 1  X   X  , 1 where M := 1 + 2 C k m k H and  X   X  i is computed from (3) . Proof: By assumption, setting h = m zeros out the second term in (6). A standard union bound combin-ing (7)-(10) completes the proof (and we simplified the bound by slightly worsening the constant).
 The confidence bound shrinks at the parametric rate, although the constant depends on k m k H , which in gen-eral is not computable, but can be estimated from the metric. Since this estimate inevitably introduces other uncomputable quantities, we omit the relevant discus-sion. On the other hand, our bound suggests that if a priori information about m is indeed available, one should choose a kernel that minimizes its induced norm on m .
 The case when m 6 X  X  is less satisfactory, despite of its practicality. We point out that a denseness argument cannot resolve this difficulty. To be more precise, let us assume for a moment m  X  C ( X ) (the space of con-tinuous functions on X ) and k be a universal kernel (Steinwart, 2002), meaning that the RKHS induced by k is dense in ( C ( X ) , k X k  X  ). By the assumed univer-sal property of the kernel, there exists suitable h  X  X  that makes the second term in (6) arbitrarily small (in fact, can be made vanishing), however, on the other hand, recall that the bound (9) on the third term in (6) depends on k h k H hence could blow up. If we trade off the two terms appropriately, we might get a rate that is acceptable (but worse than parametric). The next theorem concretizes this idea.
 Theorem 2 Under Assumptions 1-3, if A 2 ( m,R ) := inf constant C 2  X  0 , then with probability at least 1  X   X  , where D 2 := 2 C C  X  := (1 + 2 / X  )  X  2 Proof: By the triangle inequality, 1 n Not surprisingly, we apply yet again Hoeffding X  X  in-equality to relate the last term above to its expecta-tion. Since we have with probability at least 1  X   X  , 1 n
X where R := k h k H . Combining this bound with (7)-(10) and applying our assumption on A 2 ( m,R ): In Theorem 2 we do not even assume m  X  C ( X ); all we need is m  X  L 2 P grable functions. The latter condition always holds since 0  X  m  X  1 by Assumption 3. The quan-tity A 2 ( m,R ) is called the approximation error in learning theory and its polynomial decay is known to be (almost) equivalent to m  X  Range( T  X  2  X  +4 k ), see for instance Theorem 4.1 of Cucker &amp; Zhou (2007). Here T k is the integral operator ( T k f )( x 0 R
X k ( x rameter  X  &gt; 0 measures the regularity of the regres-sion function, and as it increases, the range space of k becomes smaller, hence our decay assumption on A 2 ( m,R ) becomes more stringent. Note that the exponent  X  2  X  +4 is necessarily smaller than 1 / 2 (but ap-proaches 1 / 2 when  X   X  X  X  ) because by Mercer X  X  theo-rem T 1 2 k is onto H (in which case the range assumption would bring us back to Theorem 1).
 Theorem 2 shows that the confidence bound now shrinks at a slower rate, roughly O ( n  X   X  2(  X  +2) n te ), which, as  X   X  X  X  , approaches the paramet-we assume m  X  H . We point out that the source of this slower rate comes from the irregular nature of the regression function (in the eye of the kernel k ). The polynomial decay assumption on A 2 ( m,R ) is not always satisfied, for instance, it is shown in Theorem 6.2 of Cucker &amp; Zhou (2007) that for C  X  (indefinite times differentiable) kernels (such as the popular Gaus-sian kernel), polynomial decay implies that the regres-sion function m  X  C  X  ( X ) (under mild assumptions on X and P tr (d x )). Therefore, as long as one works with smooth kernels but nonsmooth regression functions, the approximation error has to decay logarithmically slowly. We give a logarithmic bound for such cases. Theorem 3 Under Assumptions 1-3, if A  X  ( m,R ) := inf constant C  X   X  0 (assuming R  X  1 ), then (for n tr and n te larger than some constant), 1 n holds with probability at least 1  X   X  , where D  X  = 2 C The proof is similar as that of Theorem 2 except that Theorem 3 shows that in such unfavourable cases, the confidence bound shrinks at an exceedingly slow is due to the slow decay of the approximation error A  X  ( m,R ). It is proved in Theorem 6.1 of Cucker &amp; Zhou (2007) that for the Gaussian kernel k ( x 0 ,x ) = exp(  X  X  x  X  x 0 k 2 2 / X  2 ), if X  X  R d has smooth bound-ary and the regression function m  X  H s ( X ) with in-dex s &gt; d/ 2, then the logarithmic decay assumed in Theorem 3 holds. Here H s ( X ) is the Sobolev space (the completion of C  X  ( X ) under the inner product lar bounds also hold for the inverse multiquadrics ker-remark that in this regard Theorem 3 disrespects the popular Gaussian kernel used ubiquitously in practice and should draw the attention of researchers. 4.3. Discussion It seems worthwhile to devote a subsection to dis-cussing a very natural question that the reader might already have: why not estimate the regression function m on the training set and then plug into the test set, after all m does not change under the covariate shift assumption? Algorithmically, this is perfectly doable, perhaps conceptually even simpler since the algorithm does not need to see the test data beforehand. We note that estimating the regression function from i.i.d. sam-ples has been well studied in the learning theory lit-erature, see for instance, Chapter 8 of Cucker &amp; Zhou (2007) and the many references therein.
 The difficulty, though, lies in the appropriate error metric on the estimate. Recall that when estimating the regression function from i.i.d. training samples, one usually measures the progress ( i.e. the discrep-ancy between the estimate  X  m and m ) by the L 2 norm under the training probability measure P tr (d x ), while what we really want is a confidence bound on the term Since P tr 6 = P te , there is evidently a probability mea-sure mismatch between the bound we have from es-timating m and the true interested quantity. Indeed, the triangle inequality we can bound (11) by : The first term above can be bounded again through Hoeffding X  X  inequality, while the second term is close to what we usually have from estimat-ing m : the only difference being that the L 2 norm is now under the test probability measure P te (d x ). Fortunately, since the norm of the identity map id : ([  X  1 , 1] X , k X k L 2 bounded by a bound for (11) based upon results from estimating m , though less appealingly, a much looser bound than the one given in Theorem 2. We record such a result for the purpose of comparison: Theorem 4 Under Assumptions 1-3, if the regression function m  X  Range( T  X  2  X  +4 k ) for some  X  &gt; 0 , then with probability at least 1  X   X  , 1 n where C 1 is some constant that does not depend on n tr ,n te , and  X  m is the (regularized least-squares) esti-mate of m in Smale &amp; Zhou (2007).
 The theorem follows from the bound on k  X  m  X  m k L 2 in Corollary 3.2 of Sun &amp; Wu (2009), which is an im-provement over Smale &amp; Zhou (2007).
 Carefully comparing the current theorem with Theo-rem 2, we observe: 1). Theorem 4, which is based on the regularized least-squares estimate of the regres-sion function, needs to know in advance the parameter  X  (in order to tune the regularization constant) while Theorem 2, derived for KMM, does not require any such information, hence in some sense KMM is  X  X dap-tive X ; 2). Theorem 4 has much worse dependence on the training sample size n tr ; it does not recover the parametric rate even when the smoothness parameter other hand, Theorem 4 has better dependence on the test sample size n te , which is, however, probably not so important since usually one has much more test sam-ples than training samples because the lack of labels make the former much easier to acquire; 3). Theorem 4 seems to have better dependence on the parameter B ; 4). Given the fact that KMM utilizes both the training data and the test data in the learning phase, it is not entirely a surprise that KMM wins in terms of convergence rate, nevertheless, we find it quite stun-ning that by sacrificing the rate slightly on n te , KMM is able to improve the rate on n tr so significantly. For estimating the expected value of the output on the test set where covariate shift has happened, we have derived high probability confidence bounds for the kernel mean matching (KMM) estimator, which sion function lies in the RKHS, and more generally exhibits certain regularity measured by  X  . An ex-provided, calling attention of choosing the right ker-nel. From the comparison of the bounds, KMM proves to be much more superior than the plug-in estima-tor hence provides concrete evidence/understanding to the effectiveness of KMM under covariate shift . Although it is unclear to us if it is possible to avoid approximating the regression function, we suspect the bound in Theorem 2 is in some sense optimal and we are currently investigating it. We also plan to general-ize our results to the least-squares estimation problem. This work was supported by Alberta Innovates Tech-nology Futures and NSERC.
 Aronszajn, Nachman. Theory of reproducing kernels.
Transactions of the American Mathematical Sociery , 68:337 X 404, 1950.
 Ben-David, Shai, Blitzer, John, Crammer, Koby, and
Pereira, Fernando. Analysis of representations for domain adaptation. In NIPS , pp. 137 X 144. 2007. Bickel, Steffen, Br  X uckner, Michael, and Scheffer, To-bias. Discriminative learning under covariate shift. JMLR , 10:2137 X 2155, 2009.
 Blitzer, John, Crammer, Koby, Kulesza, Alex, Pereira,
Fernando, and Wortman, Jennifer. Learning bounds for domain adaptation. In NIPS , pp. 129 X 136. 2008. Cortes, Corinna, Mohri, Mehryar, Riley, Michael, and
Rostamizadeh, Afshin. Sample selection bias correc-tion theory. In ALT , pp. 38 X 53. 2008.
 Cortes, Corinna, Mansour, Yishay, and Mohri,
Mehryar. Learning bounds for importance weight-ing. In NIPS , pp. 442 X 450. 2010.
 Cucker, Felipe and Zhou, Ding-Xuan. Learning theory: an approximation theory viewpoint . Cambridge Uni-versity Press, 2007.
 Gretton, Arthur, Smola, Alexander J., Huang, Ji-ayuan, Schmittfull, Marcel, Borgwardt, Karsten M., and Sch  X olkopf, Bernhard. Covariate Shift by Kernel Mean Matching , pp. 131 X 160. MIT Press, 2009. Heckman, James J. Sample selection bias as a specifi-cation error. Econometrica , 47(1):153 X 161, 1979. Hoeffding, Wassily. Probability inequalities for sums of bounded random variables. Journal of the American Statistical Association , 58(301):13 X 30, 1963. Huang, Jiayuan, Smola, Alexander J., Gretton, Arthur, Borgwardt, Karsten M., and Sch  X olkopf,
Bernhard. Correcting sample selection bias by un-labeled data. In NIPS , pp. 601 X 608. 2007.
 Kanamori, Takafumi, Hido, Shohei, and Sugiyama,
Masashi. A least-squares approach to direct impor-tance estimation. JMLR , 10:1391 X 1445, 2009.
 Kanamori, Takafumi, Suzuki, Taiji, and Sugiyama,
Masashi. Statistical analysis of kernel-based least-squares density-ratio estimation. Machine Learning , 86:335 X 367, 2012.
 Pinelis, Iosif. Optimum bounds for the distributions of martingales in Banach spaces. The Annals of Prob-ability , 22(4):1679 X 1706, 1994.
 Shimodaira, Hidetoshi. Improving predictive inference under covariate shift by weighting the log-likelihood function. Journal of Statistical Planning and Infer-ence , 90(2):227 X 244, 2000.
 Smale, Steve and Zhou, Ding-Xuan. Learning theory estimates via integral operators and their approxi-mations. Constructive Approximation , 26:153 X 172, 2007.
 Sriperumbudur, Bharath K., Gretton, Arthur, Fuku-mizu, Kenji, Sch  X olkopf, Bernhard, and Lanckriet,
Gert R. G. Hilbert space embeddings and metrics on probability measures. JMLR , 11:1517 X 1561, 2010. Steinwart, Ingo. On the influence of the kernel on the consistency of support vector machines. JMLR , 2: 67 X 93, 2002.
 Sugiyama, Masashi, Nakajima, Shinichi, Kashima,
Hisashi, Buenau, Paul Von, and Kawanabe, Mo-toaki. Direct importance estimation with model se-lection and its application to covariate shift adapta-tion. In NIPS , pp. 1433 X 1440. 2008.
 Sun, Hongwei and Wu, Qiang. A note on applica-tion of integral operator in learning theory. Applied and Computational Harmonic Analysis , 26:416 X 421, 2009.
 Yosida, K X osaku. Functional Analysis . Springer, 6th edition, 1980.
 Zadrozny, Bianca. Learning and evaluating classifiers
