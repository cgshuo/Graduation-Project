 Building a common representation for several related data sets is an important problem in multi-view learning. CCA and its extensions have shown that they are effective in find-ing the shared variation among all data sets. However, these models generally fail to exploit the common structure of the data when the views are with private information. Recent-ly, methods explicitly modeling the information into shared part and private parts have been proposed, but they pre-sume to know the prior knowledge about the latent space, which is usually impossible to obtain. In this paper, we propose a probabilistic model, which could simultaneously learn the structure of the latent space whilst factorize the information correctly, therefore the prior knowledge of the latent space is unnecessary. Furthermore, as a probabilistic model, our method is able to deal with missing data problem in a natural way. We show that our approach attains the performance of state-of-art methods on the task of human pose estimation when the motion capture view is complete-ly missing, and significantly improves the inference accuracy with only a few observed data.
 I.2.6 [ Artificial Intelligence ]: Learning; I.5.1 [ Pattern Recognition ]: Models X  Statistical Algorithms, Experimentation, Performance Multi-view Learning, Factorized Latent Space, Sparse Struc-tured Projections
In machine learning, we usually meet cases where there are two or more disparate but related data sets. For exam-ple, the human pose data is composed of video information and motion capture information; in image retrieval appli-cations, the data is consisted of various image features and surrounding texts. Effective consolidation of these data sets has been proved to be beneficial for many computer vision tasks. Particularly in this paper, we define data consolida-tion as to find a common latent representation for all data sets. Furthermore, this problem appears naturally in the multi-view learning problem, where the multiple data sets are refereed as different views of one object.

Canonical Correlation Analysis (CCA) aims to find lin-ear transformations which maximize two views X  correlation. And there has been work to put CCA into the more flexible probabilistic framework [2]. Other extensions include learn-ing nonlinear transformations [9] in the RKHS, and learning a sparse projection matrix for easy interpretation [6]. Be-side models based on CCA, there have been several nonlinear methods proposed [15, 4, 13, 17], which may be more ro-bust than Kernel CCA [9] in noisy cases. However, they do not capture the view-dependent information, and therefore either totally fail to represent them, or mix them with the information shared by all views.

Some methods explicitly accounting the dependencies and independencies have been proposed. They factorize the la-tent space into shared part across all views and private part of each view. Among these approaches, some could be char-acterized as deterministic type [14, 7], and are proposed for predicting missing view based on observed views. One lim-itation of these methods is that they presume to know at least one complete view. Furthermore, these approaches all fail to utilize the partial observed information in views.
On the other hand, methods on factorizing the latent s-pace into shared part and private parts in a probabilistic framework have been proposed [3, 1, 8]. Particularly, [8] firstly extracts the shared information across views by C-CA, then learns the private information from the residuals. And the work of [3] can be regarded as its kennel exten-sion. Since these methods use CCA or NCCA to capture the shared information, they inevitably inherit the defats of CCA. Another approach [1] imposes a sparse structure on the projection matrices for better interpretation, but this assumption may not work for missing view prediction task. What X  X  more, these methods generally assume to know the The corresponding graphical model for the latent variable learning of data x n . The data in each view x ( v ) n is a mix of two independent continuous components, the shared one z n and the private one z ( v ) n .
 dimension of the latent space, which is impractical in real-world applications.

In this paper, we build our model in the framework of the probabilistic interpretation of CCA, which is outlined by [2] and further extended by [8, 1]. Compared with previous approaches towards shared/private factorization, our model has three main contributions: (1) . while previous extensions of CCA are generally sensitive to highly correlated noise, our model is more robust; (2) . our model could learn the dimension of the latent space automatically without prior knowledge; (3) . as a probabilistic model, our approach has an innate mechanism to deal with missing data problem. Detailed model will discussed in the later sections.
The remainder of this paper is organized as follows. In the next section, our model will be introduced. A parame-ter estimation and inference procedure will be provided in section 3 . In Section 4, we will give a through review of re-lated methods. Section 5 is our experimental part. Finally, we will give the conclusion and discuss the future work in Section 6.
The graphical model of our model is shown in Fig. 1. It is capable of dealing with the multi-view case, and explicit-ly considers the effects of private information in each view. Each data x n has V representations as x (1) n , x (2) n ,..., x Furthermore, the data x n in each view x ( v ) n  X  R P v is gen-erated as a mix of a shared latent continuous vector z (0) R D 0 , and a view-dependent (private) continuous latent vec-tor z ( v ) n  X  R D v ,v  X  X  1 , 2 ,...,V } , such that: where A ( v ) and B ( v ) are projection matrices,  X  ( v ) white noise in view v .Here 0 p  X  q / 1 p  X  q is the all-zero/all-one matrix in R p  X  q ,and I p is the identity matrix in R p  X  p respectively.

To obtain a low dimensional latent variable, we choose a simple yet effective way as to impose a column sparsi-ty structure on the projection matrices. In this way, with sufficient number of columns, the model can automatical-ly determine the number of nonzero columns of A ( v ) , B and then adapt them to the data. To achieve the column sparsity, we impose an Automatic Relevance Determination (ARD) [18] prior on the columns of A ( v ) , B ( v ) as follows:
On the other hand, we directly impose the ARD prior on the latent variables z ( v ) to make them low dimension-al. Therefore, the covariance matrices  X  (0) ,  X  (1) ,...,  X  are diagonals. Note that if we restrict them to be nonzero diagonal, then the model reduces to Factor analysis. Fur-thermore, if we set  X  ( V ) as identity, then the model reduces to Probabilistic PCA[19].

If the initial dimension of latent variables is set to be large, the model tends to over-fit since the residual variance  X  will approach zero. As a remedy to this problem, we impose a conjugate gamma prior on the inverse residual variance as (  X  ( v ) )  X  1  X  X  ( a, b ). Here ( a, b ) could be regarded as regular-ization parameters.
Before we give a detailed parameter estimation and infer-ence procedure in section 3, we would like to first rewrite the above model in a more compact form. We denote the data in each view, the projection matrices and the latent variables as follows: where N is the number of observations. The generative mod-el can then be reformulated as follows: where D = V v =0 D v and P = V v =1 P v . Note that we incorporate the column variance  X  and  X  into  X  . Addition-ally, we abuse the notation that if C ( v ) j =0,then  X  ( v ) The reason for this notation is that, C ( v ) is initially dense, and we wish to learn a sparse column projection by setting columns not fitting to data to zero.
In this section we first derive an algorithm to estimate the parameters and latent variables. After that we give a infer-ence procedure in case that some data of the measurements are missing.
A traditional approach for parameter estimation is the renowned Expectation-Maximization (EM) algorithm, in which we treat Z as the missing variables, and other random vari-ables as parameters to be estimated. We however, choose to directly take the Maximum Posteriori (MAP) of both Z ,the projection matrices C ( v ) and the residual variance  X  ( v )1 Now we write the log-likelihood function as follows:
L =log P ( X , Z , C ,  X  , X , X  ) where Z i, : denotes the i -th row of Z . And each item expand as:  X  1 log P ( Z i, : )=  X  1 log P (  X  ( v ) )=  X  ( a  X  1) log  X  ( v ) + a log ( b )  X  We directly use maximum likelihood to estimate  X  and  X  . By setting the derivative to be zero, we have: Note that the above variance is directly related to the mag-nitude norm of rows of Z or columns of C respectively, and the ones not fitting to the data well will be driven to ze-ro. This is the internal mechanics why our model could do automatic dimension determination.

By substituting the  X  and  X  in Eq.(3) with Eq.(4) and (5), and further discarding the constants, we then formulate it as a minimization problem:
Since max Z log P ( X , Z )  X  E (log P ( X , Z ) | X ), therefore taking the MAP of Z is a reasonable alternate to EM We can directly solve problem Eq. (6) by the Majorization-Minimization (MM) algorithm [10]. For numerical stability, we can slightly modify the objective function of Eq. (6) by replacing the last two terms: where  X  is a regularization parameter. We denote the so-lution obtained in the t -th iteration as Z ( t ) , C ( v,t ) ( t + 1)-th iteration, due to the concavity property, we can bound the last two terms in Eq. (eq:minreg) by the following inequation: Thus in the ( t + 1)-th iteration, what we need is to solve a weighted version of the Eq. (7):
According to [10], the MM algorithm is guaranteed to con-verge to a local optimum. In each iteration, the object func-tion is not convex. Here we simply optimize one variable while fixing the other variables and the sub-optimization problem turns out to be convex. Moreover, note that al-though the optimization is not convex w.r.t.  X  ( v ) ,itiscon-vex w.r.t. its inverse.
An important application of multi-view learning is to in-fer unobserved views based on observed ones [7, 14]. And very frequently we may have partial information about the missing views. Unfortunately, many deterministic method-s, such as [5, 14, 7] fail to utilize the partial observed data since their models can only deal with sound data. Here we treat it as a missing data problem and wish to utilize the incomplete information in inference.

We treat the latent variables Z and  X  ( v ) as parameters, and the missing data X mis as latent variables. The EM algorithm is then applied. Denoting X =( X ob , X mis ), the corresponding object function is: Note that E X mis ( X ( v ) mis | Z , X ob ) can be calculated accord-from C ( v ) ZZ T C ( v ) . We omit the detailed computations here.
There has been some work taking emphasis on the struc-ture of the latent space [7, 14, 3, 1, 8, 12]. That is, they all explicitly model the shared information across all views and private information for each view. Those models can all be formulated in a general generative model as where f , g ( v ) could either be linear [7, 1, 8], or nonlinear functions, such as the functions in RKHS [3, 14, 12]. Exist-ing work in the literature can also be characterized in two categories, namely probabilistic framework [1, 8, 12], or de-terministic framework [7, 14, 3].

Both linear and kernel extensions on CCA to model the private information of views have been proposed in [3]. The key idea is that as CCA incorporates private information into the residuals, it explicitly extracts the view-dependent effects from the residuals. To be specific, it first applies CCA or kernel CCA to the data to calculate f and z (0) n ,andthen optimize g ( v ) and z ( v ) n from the residuals. While it seems to be a good idea, this model inevitably inherits the drawbacks of CCA, i.e., sensitive to highly correlated noise.
Different from optimizing f, z (0) and g ( v ) , z ( v ) step by step, a model iteratively optimizing the linear projections and la-tent variables has been proposed in [8]. In each inner itera-tion, it first models the shared information f and z (0) ,and then extract the private information from the residuals. Un-fortunately, this model essentially inheritress the drawbacks of CCA just as [3]. Their relationship is similar to the re-lationship between Maximum Likelihood (ML) and the EM optimization procedure for probabilistic CCA, therefore [8] behaves sensitively to correlated-noise as CCA.

In the model proposed by [1], the linear projection func-tions f and g ( v ) are simultaneously optimized. The key con-tribution of their model is to impose priors on the elements of projection matrices to incur sparsity, i.e. the ARD pri-or, which results in a model easy to interpret. However, their model only works well when the projection matrices are really sparse in the generative model, which is scarce-ly the case in practice. Furthermore, this model, together with the above two models all require to know the latent dimension beforehand, which is usually not possible in real applications.

The model introduced in [12] is quite similar to the kernel version proposed in [3], while it actually uses an ARD poly-nomial kernel function as the covariance function. Although the prior knowledge of the latent dimension is unnecessary here, the model still inherits limitations of CCA as [3, 8] do.
The model proposed by [14] encourages the private-shared factorization to be non-redundant by explicitly adding a penalty term to sKIE [17] and sGPLVM [4]. Although the resulting model has been shown to yield more accurate re-sults in the context of human pose estimation, the optimiza-tion is computationally expensive. Furthermore, extension from two views to multiple views is non-trivial since the number of of shared/private latent spaces that need to be explicitly modeled grows exponentially with the number of views. [7] casts the latent variable learning problem as a matrix decomposition task, and uses recent advances in the sparse coding in helping learning their representation. They in-troduces p norm on the column of dictionary matrix for each view, and the p norm also impose structured sparsity on the rows of the coefficient matrix. While this seems a good idea, their method lacks mechanism to deal with views with missing data, and thus implicitly require the training views to be complete. Furthermore, in case there is auxiliary information of target view, i.e. data motion view is partly available in human pose estimation, it is not straightforward to incorporate them into the inference process and thus the result is doomed to degrade.

Compared with the above models, our method avoids the noise sensitiveness of CCA. Furthermore, it can automati-cally detect the latent dimension without prior information. Finally, our method could deal with missing data in a natu-ral way, which is either critical or beneficial in many multi-view learning applications.
In this section, we conduct experiments to evaluate the effectiveness of our approach. We first give synthetic exam-ples for illustration of the properties of our method, and then apply it to the real application of human pose estimation.
To study the property of our method, we conducted the same toy experiment in [14, 7]. We show that our method can also correctly factorize a latent space into shared part and private part, even when the noise is highly correlated. We generated 100 points of two data streams containing one shared and one private information per view. Specifically, we used sinusoidal signals at different frequencies such that: z (0) =sin(2  X t ) , z (1) =cos(  X  2 t ) , z (2) =cos(2 with t from the same interval (  X  1 , 1). Therefore, the ground-truth latent space is composed of 3 dimensions, 1 shared and 2 private. We then randomly projected the joint shared-(a) Ground-truth of latent s-paces (c) Dimensions recovered by C-
CA spaces respectively. Then both correlated noise 0 . 02 sin(3 . 6  X t ) and zero-mean Gaussian noise of variance 0 . 01 are added.
The ground-truth latent spaces together with correlated noise are depicted in Fig. 2(a), and the input views are depicted in Fig. 2(b). Fig. 2(c) shows the result obtained by CCA when the latent space is set by a priori to be 3-dimensional. We see that CCA recovers the shared signal, but mixes correlated noise as well. This phenomenon com-plies with the theoretical claim that approaches based on C-CA would tend to be correlated-noise sensitive [8]. Fig. 2(d) depicts the reconstructed latent spaces for both views with our method, which clearly demonstrates the shared-private factorization.
We then apply our method to the problem of the human pose estimation on HumanEva dataset [16], which consists of Videos recorded by 7 cameras and motion capture data describing the 3 D locations of joints of a human skeleton. The video and the motion capture data have been synchro-nized and thus can be perceived as different representations for an object. Therefore, the task of pose estimation, i.e, infer the 3D poses from the 2D images, can be naturally formulated as a multi-view learning problem.

In our experiments, several methods that directly perform a regression from the images features to 3D poses were com-pared, namely linear regression (Lin-Reg), Gaussian Process regression with a linear kernel (GP-Lin). And we also im-plemented the Gaussian Process regression with an RBF kernel (GP-Rbf) as a competition method. Besides, we also implemented the method proposed in [7], which has been shown to outperform most multi-view learning methods for the task of human pose estimation on HumanEva. For clar-ity, we name their model FLSSP in short. By directly com-paring our model to FLSSP, redundant comparisons with other existing algorithms [14, 11, 3, 4] are avoided. We use cross-validation to determine the regularization parameter for each algorithm.

We consider the walking and jogging video sequences of the first and second subject seen from the BW1 and BW2 camera. Since the objects move in circles, we used the first loop for training, and the remaining for testing. Each im-age is represented using a 100 dimensional integral HOG descriptoror 84 dimensional PHOG descriptor. In each test case, we have all together three views: two from the video recorded by BW1 and BW2, and one from the motion cap-ture data. The standardized mean squared error (SMSE) is chosen as the metric function, since it X  X  scale-invariant and thus easy for reproduction.

We treat the inference task as a missing data problem, in which we uniformly chose some features of the data points to be missing. In each trial, we varied the missing rate  X  , (the number of missing elements over the total number of elements in the data matrix), from 1 to 0 . 1. For each value, we sampled 20 different testing sets on the original inference framework. Fig. 3 and 4 show the mean error of the 20 trials.
We see that when the motion data is completely missing (  X  = 1), our method is among the most promising methods. Since there is high ambiguity across the multiple views [3], the outstanding performance demonstrates the power of our model to factorize the intrinsic structure of the latent space.
Furthermore, our method effectively captured the struc-tured information from the observed data, i.e. the error dropped dramatically as the missing rate decreased. For example, the estimation error reduces to about 1 / 2with around 10% visible motion capture data in walking people with HOG features.
We have proposed a probabilistic model aiming at factor-izing the latent space into shared part across views and pri-vate part for each view. By imposing ARD priors, we learn column sparse projection matrices. We have demonstrated the effectiveness of our approach on both synthetic data and the human pose estimation task. Currently, we are making two extensions. Since many multi-view learning data sets such as HumanEva are temporal, we are trying to capture the embedded time evolution in analogous to Kalman Filter. On the other hand, we are trying to develop a full Bayesian parameter estimation procedure. [1] C. Archambeau and F. Bach. Sparse probabilistic [2] F. Bach and M. Jordan. A probabilistic interpretation [3] C. Ek, J. Rihan, P. Torr, G. Rogez, and N. Lawrence. (a) S1 Walking+HOG (a) S1 Jogging+HOG [4] C. Ek, P. Torr, and N. Lawrence. Gaussian process [5] J. Fan, W. G. Aref, A. K. Elmagarmid, M.-S. Hacid, [6] D. Hardoon and J. Shawe-Taylor. Sparse canonical [7] Y. Jia, M. Salzmann, and T. Darrell. Factorized [8] A. Klami and S. Kaski. Probabilistic approach to [9] M. Kuss and T. Graepel. The geometry of kernel [10] K. Lange, D. Hunter, and I. Yang. Optimization [11] N. Lawrence. Gaussian process latent variable models [12] G. Leen. Context assisted information extraction. PhD [13] R. Navaratnam, A. Fitzgibbon, and R. Cipolla. The [14] M. Salzmann, C. Ek, R. Urtasun, and T. Darrell. [15] A. Shon, K. Grochow, A. Hertzmann, and R. Rao. [16] L. Sigal and M. Black. Humaneva: Synchronized video [17] L. Sigal, R. Memisevic, and D. Fleet. Shared kernel [18] A. Tipping. Analysis of sparse Bayesian learning. In [19] M. Tipping and C. Bishop. Probabilistic principal
