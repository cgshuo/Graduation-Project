 We study a new task, proactive information retrieval by combining implicit relevance feedback and collaborative fil-tering. We have constructed a controlled experimental set-ting, a prototype application, in which the users try to find interesting scientific articles by browsing their titles. Im-plicit feedback is inferred from eye movement signals, with discriminative hidden Markov models estimated from exist-ing data in which explicit relevance feedback is available. Collaborative filtering is carried out using the User Rating Profile model, a state-of-the-art probabilistic latent variable model, computed using Markov Chain Monte Carlo tech-niques. For new document titles the prediction accuracy with eye movements, collaborative filtering, and their com-bination was significantly better than by chance. The best prediction accuracy still leaves room for improvement but shows that proactive information retrieval and combination of many sources of relevance feedback is feasible. H.3.3 [ Information Systems ]: Information Storage and Retrieval X  Information Search and Retrieval ; I.5.1 [ Com-puting Methodologies ]: Pattern Recognition X  Models ; H.1.2 [ Information Systems ]: Models and Principles X  User/Machine Systems Copyright 2005 ACM 1-59593-034-5/05/0008 ... $ 5.00. Algorithms, Experimentation, Theory Collaborative filtering, eye movements, hidden Markov mod-el, latent variable model, mixture model, proactive informa-tion retrieval, relevance feedback
In a typical information retrieval setup users formulate queries that express their interests. The task of an infor-mation retrieval system then is to identify documents that best match the query terms, based on the contents of the documents. Alternatively, documents can be used as very complex queries to find other relevant documents, when sim-ilarity measures have been defined between documents. The systems may additionally collect explicit relevance feedback from the user, by asking which of the retrieved documents were relevant, and combine the results to a new search.
Information retrieval systems would be much more user-friendly if the number of successive explicit queries and ex-plicit relevance evaluations could be reduced X  X r eliminated altogether. Our work is a feasibility study on how far we can go by measuring implicit feedback signals from the user, and by combining them with existing data about preferences of a group of similar-minded users. The task is to predict rel-evance; if the predictions are successful they can be used in a variety of proactive applications, including proactive information retrieval.

We infer user interest from eye movements with proba-bilistic models that predict whether a user finds a text rel-evant, given her eye movement trajectory while reading the text. The key assumption motivating the use of eye move-ments is that attention patterns correlate with relevance, and that attention patterns are reflected in eye movements (see [19]). At the simplest, people tend to pay more atten-tion to objects they find relevant or interesting.
Gaze direction is an indicator of the focus of attention, since accurate viewing is possible only in the central fovea area (only 1 X 2 degrees of visual angle) where the density of photoreceptor cells is highly concentrated. A detailed in-spection of a scene is carried out in a sequence of saccades (rapid eye movements) and fixations (during which the eye is fairly motionless). Information about the environment is mostly gathered during fixations. The physiology suggests that eye movements can provide a rich source of informa-tion about the attention and interest patterns of the user. Indeed, psychologists have studied eye movements as an in-dicator of different cognitive processes for decades [18], and a recent feasibility study [19] showed that relevance can be inferred from eye movements, at least to a certain degree.
Our key contribution is that we do not assume anything about the details of this relationship between the attention and eye movement patterns; we infer everything we need from data, using machine learning methods.

Collaborative filtering is another, complementary source of relevance information. The goal of collaborative filtering is to predict the relevance of a document to a given user, based on a database of explicit or implicit relevance ratings from a large population of users. In this work we comple-ment the rich but noisy eye movement-based relevance feed-back with collaborative filtering, using a probabilistic latent variable model.

Finally, we combine the predictions from eye movements and collaborative filtering, again with a probabilistic model. The system is modular in the sense that new better compo-nents can easily be plugged in later, to replace the ones we use in this feasibility study.

In our prototype application the users browse titles of scientific articles and their eye movements are measured. We then combine the relevance predictions of the collaborative filtering model with the model that predicts the relevance from implicit feedback information.

The main research questions of this paper are: 1. How does the eye movement model perform in inferring 2. How does the collaborative filtering model perform in 3. How do the models compare against each other? 4. Is it feasible to combine relevance predictions from
To our knowledge, the combination of using implicit feed-back from eye movements and relevance prediction from a collaborative filtering model is new. However, earlier work exists in several separate fields: inferring relevance implic-itly from eye movements [19], extending queries or modeling user preferences by estimating relevance from implicit feed-back [7], and using user modeling to determine documents that may be relevant to a group of users [4, 12]. There have also been various studies on combining collaborative filtering and content-based filtering in general (e.g. [1, 14]).
Eye movements have earlier been utilized as alternative input modalities for either pointing at icons or typing text in human-computer interfaces (the most recent application being [25]). The first application where user interest was in-ferred from eye movements was an interactive story teller [23]. The story teller concentrated more on items that the user was gazing at on a display. Rudimentary relevance deter-mination is needed also in [5], where a proactive translator is activated if the reader encounters a word which she has difficulties in understanding. These difficulties are inferred from eye movements.

Traditionally implicit feedback in IR has been derived from document reading time, or by monitoring user behav-ior: saving, printing, and selecting of documents (see [7] for a good overview on different approaches). Use of eye move-ments as a source of implicit feedback for IR is a relatively new concept. A prototype attentive agent application (Sim-ple User Interest Tracker, Suitor) is introduced in [10, 11]. The agent monitors eye movements while the user views web pages, in order to determine whether the user is reading or just browsing. If reading is detected, the document is de-fined relevant, and more information on the topic is sought and displayed. The feasibility of the application was not ex-perimentally verified, however. To our knowledge the only study with statistically tested significance is [19, 20], which is a simple feasibility study. The experimental setup is close to ours, but the task is much simpler. The user is presented a question and a list of possible answers, some of which are relevant to the question and one provides the answer. The eye movements are then used to infer the relevant lines, as well as the correct answer.

Traditionally collaborative filtering has been performed by memory-based techniques, in which one first identifies users similar to a given user and then gives predictions based on interests of those users (see e.g. GroupLens [8], or Ringo [22]). However, the time and memory requirements of the memory-based techniques do not scale well as the number of users and documents increases.

We propose a model-based approach, which is based on the User Rating Profile model (URP) [12]. We have ex-tended the previous work by optimizing the URP by using Markov Chain Monte Carlo (MCMC) integration instead of the variational approximation used earlier. The model structure of URP is closely related to a probabilistic latent variable model introduced by Pritchard et al. [16], and La-tent Dirichlet Allocation (LDA) [2] which is also known as Multinomial PCA (mPCA) [3].
Eye movements were modeled using hidden Markov mod-els (HMMs); they are simple yet reliable models for sequen-tial data. We use two kinds of HMMs: ordinary and dis-criminative, both modeling word-level eye movement data. In eye movement research, HMMs have earlier been used for segmenting the low-level eye movement signal to detect focus of attention and for implementing (fixed) models of cognitive processing [21]. Discriminative HMMs have been previously applied to eye movement data in [20].

Prediction of known classes with machine learning meth-ods is based on a labeled data set from which the predictive model is learned. We collected such a set by measuring eye Figure 1: The topology of the discriminative hidden Markov model. The first level models transitions between sentences having relevance r  X  X  I,R } and the second level (within the boxes) models transi-tions between the words in a sentence. movements in a setting where relevance was known: explicit feedback for presented sentences (titles of scientific docu-ments) was collected from the user during the recording ses-sion. The experimental setup is described in more detail in Section 4.1.
 The simplest model that takes the sequential nature of eye movement data into account is a two-state HMM. We opti-mized one model individually for each of the two classes, in our case relevant ( R ) and irrelevant ( I ) sentences. In a pre-diction task a maximum a posteriori (MAP) estimate was computed. The two HMMs were fitted to data by the Baum-Welch (BW) algorithm that maximizes the log-likelihood of the data Y given the model and its parameters  X  , that is, log p ( Y |  X  ) [17]. The model is described in more detail in [20].
 In discriminative modeling we want to predict the relevance r = { I,R } of a sentence, given the observed eye movements Y . Formally, we optimize In speech recognition, where HMMs have been extensively used for decades, the current state-of-the-art HMMs are dis-criminative. The parameters of the discriminative HMM can be optimized with an Extended Baum-Welch (EBW) algo-rithm [15], which is a modification of the original BW.
Eye movements are modeled with a two-level discrimina-tive HMM, where the first level models transitions between sentences whereas the second level models transitions be-tween words within a sentence. The topology of the model is shown in Figure 1.

In our implementation, the first level Markov model has two states, each modeling one sentence class ( I or R ). Each state has the following exponential family emission distri-butions: (1) A multinomial distribution emitting the rele-vance of the line, r . This distribution is fixed; for each state one of the probabilities is one and the other is zero. (2) A Viterbi distribution emitting the probability of the sequence of words in a sentence. The Viterbi distribution is defined by the probability of a Viterbi path [24] trough a two-state Markov model forming the second level in our model. The two states of the second level model emit the exponential observation distributions. The modeled eye movement fea-tures are described in Section 4.1.

When optimizing the model the most likely path through the second level model is sought by the Viterbi approxima-tion [24]. The discriminative Extended Baum-Welch algo-rithm optimizes the full model, keeping the Viterbi path in the second level model fixed.
Collaborative filtering was carried out with a state-of-the-art latent topic model, the User Rating Profile model (URP) [12]. This model was used because in [12] it outper-formed several other latent topic models. It was originally optimized with variational Bayesian methods ( variational URP ). We also implemented a potentially more accurate Markov Chain Monte Carlo (MCMC) integration method to compute the predictions from the model, using Gibbs sampling ( Gibbs URP ).
 URP is a generative model which generates a binary rating r for a given (user, document) pair. 1 Our notation is sum-marized in Table 1. We estimate the posterior distribution P ( r | u,d, D ) by Gibbs sampling where D denotes the train-ing data that consists of observations ( u,d,r ). The model assumes that there are a number of latent user groups whose preferences on the documents vary, and the users belong to these groups probabilistically. Alternatively, the groups can be interpreted as different  X  X ttitudes X  of the user, and the attitude may be different for different documents.
The generative process proceeds according to the following steps (see also Figure 2):
Note that the model allows also multiple-valued ratings if the binomial is replaced with a multinomial.
We denote distributions with capitalized words followed by their parameters in parentheses. Figure 2: A graphical model representation of URP. The grey circle indicates an observed value. The boxes are  X  X lates X  representing replicates and the index at the bottom right corner of each plate in-dicates the number of replicates. The lowest plate, labeled with N U , represents users. The plate labeled with N D represents the repeated choice of user group and document. The plate labeled with K U repre-sents the multinomial models of relevance for the different user groups. In the URP model each user is assigned a distribution of multinomial parameters  X  and the latent user group ( X  X opic X  in text modeling) Z is sampled repeatedly for each docu-ment. A user can therefore belong to many groups with varying degrees. In URP, the multinomial parameters  X  are marginalized out from the maximum likelihood cost func-tion. In the well-known latent topic model called Probabilis-tic Latent Semantic Analysis [4], the number of parameters grows with the number of users, since each user is given a fixed set of multinomial parameters  X  .

URP is closely related to Pritchard X  X  latent variable model [16] and Latent Dirichlet Allocation (LDA) [2] (also known as multinomial PCA). URP can be seen as an extension to LDA with one extra dimension in the parameter matrix  X  to represent the possible different rating values. In our case we only have two values.
 In Gibbs URP a five-fold cross-validation within the training set was first carried out to determine the optimal number of user groups in the range { 1 , 2 ,...,N U } . In our experiments the optimal number of user groups was found to be two, which was later used when computing the predictions for the final test set.

The duration of the burn-in period was determined by running three MCMC chains in parallel and monitoring the convergence of predictions.
 We introduced two simple models to give baseline results. The dumb model classifies all documents to the largest class, P ( r = 0) = 1. The document frequency model does not take into account differences between users or user groups. It Figure 3: A graphical model representation of the discriminative Dirichlet mixture model. X is the in-dex of the model that predicts relevance, in our case X  X  X  eye,urp } . The grey circles indicate observed values. In our model we observe triplets ( r,P eye ,P urp for each user-document pair. simply models the probability of a document being relevant as the frequency of r = 1 in the training data for the docu-ment,
We started by examining the prediction performance of each of the models separately. Since the models use different sources of information, the natural extension is to combine their predictions.

Both models produce a probability of relevance for each given (user, document) pair. The simplest way to combine the models is to train the models independently and combine the predicted probabilities to produce the final prediction. This approach has the advantage of being modular and eas-ily extensible.
 We formulated a generative model for combining probabili-ties. Let us denote the prediction of the collaborative filter-ing model by P urp and the prediction of the eye movement model by P eye .

We first define a model that generates the observed rel-evances r  X  X  0 , 1 } and the (noisy) predictions P urp and P eye . Our goal is to find an expression for P ( r | P urp where  X  denotes all parameters of the model.

The generative process of the discriminative dirichlet mix-ture model is (see Figure 3) as follows:
The observed variables of our model are the binary rele-vances r n and the prediction probabilities P X,u,d , where the indices u,d denote all (user, document) pairs. The param-eters of the model are given by  X  = {  X , X  r urp , X  r eye } . We have ignored the priors of the parameters, since we assume the prior to be flat, i.e., P ( r,P X |  X  ) = P ( r,P X , X  ), up to a normalization factor.

We optimize the model by maximum likelihood, and since the task is to predict relevances we build a discriminative model by maximizing the conditional log-likelihood of the relevances, Values of the parameters  X  can be found using standard optimization methods, for instance gradient ascent.
Besides giving predictions of relevance, the Dirichlet mix-ture reveals how useful the different sources of relevance in-formation are relative to each other. Some of the feedback channels may produce smaller prediction probabilities P X than others for the observed relevances r . Some of the rele-vance feedback channels may additionally be noisy, that is, the prediction probabilities P X for a given relevance r have a large variance. After optimization, the mixture parameters  X 
X will contain information about magnitude and noisiness of the probability predictions. The magnitude of the predic-tion is contained in the relative magnitudes of the Dirichlet components. The information of the noisiness is contained in the sum of the Dirichlet parameters: if the sum of Dirich-let parameters is large, P i  X  X  0 , 1 }  X  r Xi 1, the prediction probabilities P X have smaller variance, and vice versa. To serve as a baseline, we constructed a linear mixture model in which the final probability is a linear combination of the predictions of the various models (here of two models), The parameter q is optimized by maximizing the conditional log-likelihood (Equation (1)) using standard optimization methods.
The test subjects were shown 80 pages, each containing titles of scientific articles. On each page the subject was instructed to choose the two most interesting titles in the order of preference.

The subjects participating in the experiment were research-ers in vision research, artificial intelligence, and machine learning. The stimuli consisted of titles of scientific arti-cles published during autumn 2004 in major journals in the fields of vision research (VR), artificial intelligence (AI), ma-chine learning (ML), and general science (see Appendix A). On each page there was a randomly selected list of titles always containing two VR titles and one title from a general science journal. Half of the lists contained additionally one AI title and two ML titles, and half vice versa. Each list consisted of six titles, resulting in a total of 480 titles. The lists were shown in a randomized order to each subject, but the pages themselves were identical to all subjects.
Data was gathered in two different modalities. 22 of the subjects were asked to give their feedback explicitly via a web form, and three of the subjects participated in an eye movement experiment (Figure 4). In this paper we refer to these subjects as the web-subjects and eye-subjects , re-spectively. The web-subjects were given the full publication information of the most interesting paper as a reward to encourage them to find the truly most interesting titles.
Three of the subjects (the eye-subjects) were shown the same stimuli in a controlled setting where eye movements were recorded. In the experiment, the subject was instructed in a similar manner to choose the two most interesting titles from the list of six titles (the eye movements were measured during this part), then press  X  X nter X  to proceed to another display, and finally to type in the numbers corresponding to the interesting titles. Hence both explicit ratings and eye movement trajectories were available for these subjects. Eye movements were measured with a Tobii 1750 eye tracker with a screen resolution of 1280x1024.
Randomly chosen 21 of the lists (26 %) and the corre-sponding ratings from the eye-subjects formed a common test data set for all the models. Seven of the titles in the test set were not read by the eye-subjects. They were dis-carded, leaving us a total of 371 titles in the test set. Test data set was not touched before computing the final results.
Eye movement models were trained with the remaining feedback data from the three eye-subjects. 3 For training the URP model we used the eye-subjects X  explicit ratings that were not included in the test data set, and all the explicit feedback data from the web-subjects.
 For the eye-subjects, nine of the measured lists had to be discarded from the data sets for technical reasons, thus leav-ing a set of 71 lists where both explicit and implicit feedback was available. The explicit ratings were, however, not dis-carded.

The raw eye movement data (consisting of x and y co-ordinates of the gaze direction, measured with a sampling rate of 50 Hz) was segmented into a sequence of fixations and saccades by a window-based algorithm (software from Tobii), with a 20-pixel window size and a minimum duration of 80 ms, used for defining fixations. An example of an eye movement trajectory in a case where relevance can easily be determined is shown in Figure 5.

Feature extraction from the fixation-level data was then carried out as in [20]. Each fixation was first assigned to the nearest word, which segmented the eye movement trajec-tory into words. The following features were then computed from the segmented data to be modeled with hidden Markov models: (1) One or many fixations within the word (mod-eled with a binomial distribution). (2) Logarithm of the total fixation duration on the word (assumed to be Gaus-sian). (3) Reading behavior (multinomial): skip next word, go back to already read words, read next word, jump to an unread line, or the last fixation in an assignment. In the explicit feedback data all the selected titles were as-sumed to be relevant ( r = 1) for the user, resulting in one third of all the ratings being  X  X elevant. X  In other words, we did not model the users X  preference order for the titles.
The number of eye-subjects was chosen to be three for practical reasons. Three subjects were sufficient to train the HMM, to form the test set, and to obtain a statistically significant result. eye tracker (right).
 Figure 5: A reconstruction of the eye movement trajectory of a test subject during one of the as-signments. Fixations are denoted by circles. The relevant sentences ( R ) are on lines 1 and 4. The tra-jectories naturally varied across the users and ex-periments.
For all the models we used perplexity and prediction ac-curacy in the test data set as measures of performance. Per-plexity measures the probabilistic quality of the prediction, Here  X  denotes the parameters of the model under evalua-tion, the sum is taken over the test set, and N is the size of the test set. We further computed the accuracy, that is,
The best possible performance yields perplexity = 1 and random guessing (coin flipping) yields perplexity = 2. If perplexity is greater than 2 the model is doing worse than random guessing. Theoretically, it can grow without a limit if the model predicts zero probability for some item in the test data set. However, we actully clipped the probabilities to the range [ e  X  10 , 1] implying maximum perplexity of e 22 , 000 the fraction of the items in the test data set for which the prediction was correct for all the models, and the precision and recall measures. Precision is defined as the fraction of relevance predictions that were correct. Recall is defined as the fraction of relevant items correctly predicted. The results are shown in Table 2.

The discriminative HMM produced a reasonable, though rather noisy, prediction of the relevance 5 . The difference in classification accuracy versus the dumb model was statisti-cally significant (McNemar X  X  test, P 0 . 01). The perfor-mances of the Document Frequency Model and URP cannot be directly compared to the HMM, since the HMM predic-tion is only based on the eye movements from the three eye-subjects, whereas the other models utilize the explicit feedback given by the 22 web-subjects. Consequently, it is not surprising that the URP outperforms the pure HMM in terms of perplexity and accuracy measures.

As expected, URP was able to distinguish two different user groups and provide a reasonable 83 % accuracy. The accuracies of the variational version and the Gibbs version were practically equal. However, the perplexity of the Gibbs URP is better. The reason is that the variational URP finds a maximum likelihood point estimate of the model param-eters, whereas the Gibbs URP integrates properly over all model parameters, resulting in a more robust probability prediction. The difference of Gibbs URP to Document Fre-quency Model and HMM was tested by the Wilcoxon signed rank test, applied to the negative log-likelihoods given by the models for individual test samples. The differences were significant ( P 0 . 01).

The discriminative Dirichlet mixture model did succeed in combining the predictions of the different models. The difference of Dirichlet Mixture Model (HMM+Gibbs URP) to Gibbs URP was statistically significant ( P 0 . 01) us-ing the Wilcoxon signed rank test. The linear mixture of predictions performed poorly, placing all the weight on the prediction of the URP and ignoring the more noisy HMM al-
The performance of simple two-state HMMs optimized for each class was similar to discriminative HMM. together. The precision of the best mixture (83.7 %) is quite good, taking into account that the content of documents was not modeled in any way.

Finally, we wish to point out that, for relatively small data sets, the classification accuracy is a noisy measure, as compared to perplexity. However, the difference between the accuracies of the Mixing Model (HMM+variational URP) and Gibbs URP is significant even for this noisy measure, at a moderate P -value of 0.04 (with McNemar X  X  test). The difference between the accuracies of variational URP and the combination is not statistically significant ( P = 0 . 06).
We have set up a controlled experimental framework where the test subjects rated the relevance of titles of scientific ar-ticles. Eye movements were measured from a subset of the test subjects. The experimental setup was designed to re-semble closely a real-world information retrieval scenario, where the user browses the output of, e.g., a web search engine in an attempt to find interesting documents. In our scenario a database of user preferences is combined with the measured implicit relevance feedback, resulting in more accurate relevance predictions. Collaborative filtering and implicit feedback can be used alone, or to complement stan-dard textual content-based filtering.

We applied a discriminative time series model that pro-duced a reasonable, though rather noisy, prediction of docu-ment relevance based on eye movement measurements. We also applied a probabilistic collaborative filtering model that produced a quite robust document relevance prediction. Thirdly, we introduced a probabilistic mixture model that can be used to combine the predictions. The mixture model clearly outperformed a simple linear method and was found necessary for making use of several information sources, the quality of which varied.

Our work provides the next step towards proactive infor-mation retrieval systems. The obvious extension is to incor-porate the textual content of the documents to the models; in this work we do not utilize it at all. The second ex-tension is to supplement or replace the eye movements by other sources of implicit feedback, such as measurements by biopotential sensors, e.g., from autonomic nervous system signals [9] or electromyographic activity [13], and respective probabilistic models.

The models for inferring relevance could also be developed further. A good opportunity is provided by our Pascal EU Network of Excellence challenge (Inferring Relevance from Eye Movements, [6]), a competition where participants are invited to develop methods that best predict relevance from eye movement data. The authors would like to thank the people at Tampere Unit for Computer-Human Interaction, mainly Aulikki Hyrs-kykari, P  X aivi Majaranta, and Kari-Jouko R  X aih  X a for useful discussions and for providing us with the measurement time for the eye movement experiments. We would also like to thank all the persons that (more or less) voluntarily took part in our experiment.

This work was supported by the Academy of Finland, de-cision no. 79017, and by the IST Programme of the Eu-ropean Community, under the PASCAL Network of Excel-lence, IST-2002-506778. This publication only reflects the authors X  views. The authors acknowledge that access rights to the data sets and other materials produced in the PRIMA project are restricted due to other commitments. [1] J. Basilico and T. Hofmann. Unifying collaborative [2] D. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet [3] W. Buntine. Variational extensions to EM and [4] T. Hofmann. Latent semantic models for collaborative [5] A. Hyrskykari, P. Majaranta, and K.-J. R  X aih  X a. [6] http://www.pascal-network.org/Challenges/IREM/ . [7] D. Kelly and J. Teevan. Implicit feedback for inferring [8] J. Konstan, B. Miller, D. Maltz, and J. Herlocker. [9] C. L. Lisetti and F. Nasoz. Maui: a multimodal [10] P. P. Maglio, R. Barrett, C. S. Campbell, and [11] P. P. Maglio and C. S. Campbell. Attentive agents. [12] B. Marlin. Modeling user rating profiles for [13] T. Partala, V. Surakka, and T. Vanhala.
 [14] A. Popescul, L. Ungar, D. Pennock, and S. Lawrence. [15] D. Povey, P. Woodland, and M. Gales. Discriminative [16] J. K. Pritchard, M. Stephens, and P. Donnelly. [17] L. R. Rabiner. A tutorial on hidden Markov models [18] K. Rayner. Eye movements in reading and information [19] J. Saloj  X arvi, I. Kojo, J. Simola, and S. Kaski. Can [20] J. Saloj  X arvi, K. Puolam  X aki, and S. Kaski. Relevance [21] D. D. Salvucci and J. R. Anderson. Automated [22] U. Shardanand and P. Maes. Social information [23] I. Starker and R. A. Bolt. A gaze-responsive [24] A. Viterbi. Error bounds for convolutional codes and [25] D. J. Ward and D. J. MacKay. Fast hands-free writing Pool 1: Machine learning journals: Pool 2: Vision research journals: Pool 3: Artificial intelligence journals: Pool 4: General scientific journals:
