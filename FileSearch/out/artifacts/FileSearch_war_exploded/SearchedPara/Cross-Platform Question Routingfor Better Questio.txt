 The last two decades have made the Internet a major source for knowledge seekers to ful ll their needs. Several tools and platforms were developed to nd answers to one's questions such as search engines and online encyclopedias. The wide adoption of social media pushed the possibilities even further by giving people the opportunity to stimulate the genera-tion of answers that are not already present on the Internet. Some of these are primarily community question answering (CQA) services, while the others have a more general audi-ence but can also be used to ask and answer questions.
Assessing which platform is best suitable for certain infor-mation need of a particular user, and subject to some spe-ci c constraints, is a problem that has seen little attention (e.g. [3]). Hence, we consider cross-platform question rout-ing to automatically suggest where an input question should be asked or redirected to. Several stages are required to ad-dress this problem. First, we need to analyze the di erences between the questions asked on di erent platforms. In fact, on a search engine, a user would provide the least context to the query. On a CQA website, she would summarize the question in a title and provide more context in a descrip-tion. On the other hand, the (online) identity of the asker in a microblogging service is revealed to her friends, leading to a higher exposure of context. For this stage, we gather questions from di erent platforms to train a classi er on the distribution of question types [4] over these platforms. Ser-vices that are not restricted to questions and answers (e.g., Twitter) need special mining to nd content that contains questions. We will extend the work of [2] to nd out which posts are answer seeking questions.

Second, we need to characterize the users, as not all de-mographics have the same needs. For instance, the types of information seeked by a journalist are arguably di erent from those of a teenager. Thus, we build some classi ers that di erentiate users based on the questions they might ask. We apply some algorithms that we developed to re-trieve the Twitter accounts of Arab journalists, and adapt the taxonomy of the more general question types to this group of professionals.

Routing questions needs new evaluation measures that take in consideration various aspects beyond the traditional dimension of relevance. For instance, for a user who is seek-ing opinions of \ordinary" people about non-factoid ques-tions, informality of the answers is a constraint that might be added to the evaluation [1]. The cost of wrong answers is an additional dimension to the evaluation that is often traded-o with response time. For example, a journalist cannot accept an unveri ed report and might spend a long time verifying it. On another hand, someone who is trying to get recommendations for a nearby restaurant might trade the correctness for a timely answer. Thus, we need to model the likelihood of whether and when a question will get a response, if a response contains an answer to that question and to which extent that answer is correct and satis es the expectations of the asker.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Question Answering; Question Routing; Social Media This work was made possible by NPRP grant# NPRP 6-1377-1-257 from the Qatar National Research Fund (a mem-ber of Qatar Foundation). The statements made herein are solely the responsibility of the author.
 [1] M. Bagdouri, D. W. Oard, and V. Castelli. CLIR for [2] M. Hasanain, T. Elsayed, and W. Magdy. Identi cation [3] A. Oeldorf-Hirsch, B. Hecht, M. R. Morris, J. Teevan, [4] Z. Zhao and Q. Mei. Questions about questions: An
