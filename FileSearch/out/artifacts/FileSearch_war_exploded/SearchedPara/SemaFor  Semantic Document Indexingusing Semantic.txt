 Traditional document indexing techniques store documents using easily accessible representations, such as inverted indices, which can efficiently scale for large document sets. These structures of-fer scalable and efficient solutions in text document management tasks, though, they omit the cornerstone of the documents X  purpose: meaning . They also neglect semantic relations that bind terms into coherent fragments of text that convey messages. When seman-tic representations are employed, the documents are mapped to the space of concepts and the similarity measures are adapted appro-priately to better fit the retrieval tasks. However, these methods can be slow both at indexing and retrieval time. In this paper we propose SemaFor , an indexing algorithm for text documents, which uses semantic spanning forests constructed from lexical resources, like Wikipedia ,and Wo rd N e t , and spectral graph theory in order to represent documents for further processing.
 H.3.3 [ Information Search and Retrieval ]: [Retrieval models, Selection process]; H.3.1 [ Content Analysis and Indexing ]: [Lin-guistic processing, Thesauruses] Algorithms, Experimentation, Theory Document Indexing, Semantic Graphs, Text Representation
Document indexing has been traditionally conducted with the use of a term to document mapping and its inverse, which takes into account only the frequency of occurrence of terms in the in-dexed documents, neglecting semantic relatedness between terms, uments in the form of semantic spanning forests. Similarity com-putation is performed using spectral graph algebra , as explained in detail in Section 3.
The basic hypothesis behind our approach is that the use of se-mantic information for the representation of documents may im-prove the performance of the text clustering and retrieval tasks, both in precision and recall. The hypothesis is based on previously published scientific indications, e.g., [10].

In one direction, several approaches attempt to capture seman-tic relatedness between terms using statistical analysis of corpora. They attempt to group the terms of a document into subsets (top-ics) that contain statistically  X  X elated" terms, in order to represent documents as combinations of one or more topics [4, 5]. How-ever, such approaches require extensive analysis of large text cor-pora, and the produced models cannot be easily transferred across domains. In another direction, the use of lexical and other knowl-edge resources is employed in order for the documents to be rep-resented as graphs of terms [14]. Recent studies, e.g., [19] have shown that linguistic and crowdsource-based knowledge sources, for example Wo rd N e t and Wikipedia respectively, can be used com-plementary in this task. The processing of document semantics in SemaFor also results in a graph, which contains the document terms only. Though SemaFor does not perform topic detection literally, the SST s of each indexed document can be seen as the document topics. Taking one step further to the aforementioned approaches, SemaFor indexes the document graph using a mechanism that facil-itates storage and fast processing, and incorporates semantic infor-mation inside the indexing data structures. For the task of the graph creation it uses both Wo rd N e t and Wikipedia , combining the  X  wis-dom of linguists "and X  wisdom of crowds ". Close to our approach are also the works that embed senses and semantic information for text document management, like for example Generalized Vector Space Models (GVSM) [12] and semantic kernels [2].

An important point in existing approaches is the consideration of word sense disambiguation methods ( WSD ) which can potentially offer the transit from terms to senses. In this paper we address word disambiguation by employing a very simple WSD algorithm that provides state of the art performance and is used as a very competitive baseline for WSD methods; the first sense heuristic , which selects the most frequently appearing sense of each word [9]. 1 .

Finally, with regards to semantic indexing methodologies, exist-ing approaches map documents to graphs, yet they do not consider the semantic information at indexing level. In [7], each document is mapped to a graph with terms as vertices and 4 types of edges (based on Wo rd N e t relations). The graph structure is neither in-dexed, nor employed in the computation of similarity between doc-uments. In [15] documents are mapped to semantic forests using the co-occurrence of terms (actually stems) and their semantic re-lations (as given by Wo rd N e t ) in order to draw semantic relations between terms. During the indexing and document similarity com-putation phases, the graph information is neglected and each forest is perceived as a set of terms. In contrast to the aforementioned ap-proaches, SemaFor introduces a lightweight representation of the document graph that keeps only the strongest edges and employs spectral graph theory in order to convert the spanning trees into an indexable format.
In our implementation we are using Wo rd N e t as the main dictio-nary, or Wikipedia definitions if the term is ambiguous and does not appear in Wordnet n term-POS pairs, namely T = tp 1 ,tp 2 , ..., tp n , in the remain-ing of this section we describe how a semantic spanning forest is constructed from this set. Primarily, note that for any given pair ( tp i ,tp j ) with i = j and both i, j  X  [1 ..n ] the t part of the term-pair tp i might be identical with the t part of the tp j term pair, but then the p part in the two term pairs must differ (i.e, we keep the set of all distinct term-POS pairs for D ).

Initially, we compute the semantic relatedness S between every term-pair combination in T . In our implementation, for pairs of terms that exist in Wo rd N e t we are using Omiotis [13]; for the rest we are using WLM ,a Wikipedia -based measure [8]. Omio-tis , which has been shown to outperform WLM in case both terms exist in Wo rd N e t [13]. Note that the suggested methodology is gen-eral enough to allow for the use of any other measure of semantic relatedness or similarity. Both used measures are in the range of [0 , 1] , with 1 meaning totally related and 0 meaning totally unre-lated, they are publicly available and their performance is state-of-the-art in their category of measures [19]. Since for both mea-putations of semantic relatedness. Based on the above we define S ( WLM ( tp i ,tp j ) .

Next, we construct a semantic graph which initially contains all the elements of T as nodes. Each node represents a term-POS el-ement of D . We add an edge e tp i ,tp j between every pair of nodes ( tp i ,tp j ) for which S ( tp i ,tp j ) &gt; 0 , with weight w tp i ,tp j = 1
Once all the edges have been added, the semantic graph contains terms as nodes and reverse semantic relatedness values between them as edges. For each connected component of the graph, i.e., as this is defined by traditional graph theory, we apply the computa-tion of the minimum spanning tree algorithm of Kruskal. D is now asetof minimum semantic spanning trees . We define this set as the Semantic Spanning Forest representing document D ( SSF ( D )), and each i -th semantic tree of D ( SST i ( D )) as one of its topics.
Having the documents in the form of semantic spanning forests ( SSF ), we now proceed in representing them to a metric space where we can compute similarity between documents. For their similarity, we are based on the spectra of the normalized Laplacian of the two bipartite graphs, following the basis of spectral graph theory [3]. The similarity between two semantic forests is even-tually based on the computation of the Hausdorff distance [1] be-tween the two SSF s, which considers the spectral properties of the two graphs, and more specifically the sectional curvatures of their edges. The Hausdorff distance has been shown to perform very well in the application of graph clustering in the field of computer vision [6]. Thus, the following procedure, though not new, it consti-tutes a novel embedding in our case, since it is for the first time, to the best of our knowledge, that it is applied in graphs representing documents, as a means of SSF s. We give details on the Hausdorff distance in the following section.

In the following we explain the details of the first application of this technique in text processing and more specifically we show how SSF s are transformed to facilitate spectral similarity computa-tion. Initially, let G ( V,E ) be a graph, which in our case represents a document as a means of a SSF ,where V is the set of its vertices, and E the set of its edges. For reasons of simplicity, let us also assume that G is connected, forming a spanning tree. Primarily, for every such graph in our document collection, we compute the Algorithm 2 SemaFor ( SSF , t ) 1: INPUT: A semantic spanning forest SSF , the parameter t of 2: OUTPUT: The indexing of SSF as a set of ordered lists of real 3:  X  L,  X  ,  X  ,K : Initially empty matrices 4: K set : An initially empty set of ordered real values 5: L : An initially empty list of K set 6: SST : An initially empty set of trees 7: for all s  X  SST do 8:  X  L := NormalizedLaplacian(s) 9:  X  ,  X  := EigenValueDecomposition(  X  L ) 10: for all ( u, v ) pairs  X  s do 11: d 2 E ( u, v ) := | V | i =1 exp [  X   X  i t ](  X  i ( u )  X   X  i ( v )) 2 13: K [ u, v ] := 2 14: end for 15: K set := OrderValuesOf( K ) 16: L := AddToList( K set ) 17: end for 18: Store SSF as L
Eventually, the sectional curvature of the edge ( u, v ) can be com-puted as follows (the proof can be found in [16]): The sectional curvatures of the SSF are the only information that we index for our tree structures. Essentially, the sectional curva-tures capture the topological structure of SSF and allows us to con-struct a low-dimensional feature space in which these values reside. Ultimately, we only need to index those values instead of the full SSF structure. Algorithm 2 describes the SemaFor document in-dexing algorithm. It assumes that the SSF of a given document D has already been computed (using Alg 1). Eventually, a list of sets of ordered real values are indexed for the SSF . Each set represents each SST of the SSF , and the values are the respective sectional curvatures of the SST edges. Given two graphs G 1 ( V 1 ,E 1 ,K 1 ) and G 2 ( V 2 ,E 2 ,K 2 ) ,where V ,V 2 are the respective sets of their vertices, E 1 ,E 2 are the re-spective sets of their edges, and K 1 ,K 2 are the respective matrices with the sectional curvatures of their edges (e.g., K 1 ( u, v ) is the sectional curvature of edge ( u, v ) in G 1 ) we are using the Haus-dorff distance [6] to compute the distance between G 1 and G 2 as follows:
Hausdorff ( G 1 ,G 2 )= max The Hausdorff distance in our case is a maximin function between the sectional curvature matrices. Since we have assumed that the SSF s we are examining are connected, we will generalize Equation 10 to capture all the cases, i.e., cases that SSF may contain several semantic spanning trees ( SST ). The generalization takes place in a similar manner that the average-link works during the agglomera-tion step in the hierarchical agglomerative clustering ( HAC ). The reason is simple: given two sets (i.e., the SSF s) of elements (their SST s), we estimate the distance between sets based on the Haus-dorff distance between elements. This is exactly the problem faced by the HAC algorithm.
 ment similarity approach [15]. In order to be compatible with the results presented in [15], we are using the same document subsets, produced as described in their respective work: (1) C 1 , comprising 50 documents in total from the Oil and Nat-Gas categories ( 25 doc-uments from each category), (2) C 2 , comprising 100 documents in total from the Coffee and Sugar categories ( 50 documents from each category), and (3) C 3 , comprising 200 documents from the Grain , Wheat , Ship and Crude categories ( 50 documents in each category). For our evaluation, we compute precision, recall, and F-Measure (or F 1 score) for each category in every case ( C 1 ,C 2 , and C 3 ), as well as their macro-averages, and overall accuracy. The accuracy results, that are directly comparable with the results reported in [15] are shown in Table 1. Table 2 contains the detailed results of SemaFor for each category, in each subset, where MP , MR ,and MF1 are macro-averaged precision, recall and F1-score respectively. For the text retrieval evaluation of SemaFor we are using the TREC2 document collection, and more specifically the Wall Street Journal articles from 1990 , so that we can directly compare with the semantic indexing approach proposed by Kang and Lee [7]. This document set comprises 21 , 705 articles, and the 50 query top-ics 101  X  150 from the respective collection are used.

Figure 2 shows the average precision results of top N documents over all queries for SemaFor ,the SW-IDF semantic indexing ap-proach introduced in [7] and the standard baseline. The SW-IDF and TF-IDF VSM results are taken from [7].

The results show that the precision of SemaFor is higher than that of SW-IDF ([7]) in the top-10 ( k =10 ) and top-20 ( k =20 ) documents, which is the typical amount of retrieval results that a user examines in a search. Precision is always higher than that of the baseline method. The top results of SemaFor are better than that of its competitors.
In this work, we presented SemaFor , a novel document indexing algorithm that is based on the spectra of the documents X  seman-tic graphs to represent and index documents. SemaFor uses the-sauri ( Wo rd N e t and Wikipedia ) in order to extract the semantic rela-tions between documents X  terms. The indexing algorithm employs algebraic transformations from spectral graph theory in order to provide a reduced and compact representation for each document. The Hausdorff distance is used to define the distance between two documents. Evaluation in text clustering experiments shows that the spectrum-based graph representation of the documents can im-prove significantly the performance of the text clustering process, and has satisfactory performance in the retrieval task. Our next
