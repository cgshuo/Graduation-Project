 While the physiological response of humans to emotional events or stimuli is well-investigated for many modalities (like EEG, skin resistance, ...), surprisingly little is known about the exhalation of so-called Volatile Organic Com-pounds (VOCs) at quite low concentrations in response to such stimuli. VOCs are molecules of relatively small mass that quickly evaporate or sublimate and can be detected in the air that surrounds us. The paper introduces a new field of application for data mining, where trace gas responses of people reacting on-line to films shown in cinemas (or movie theaters) are related to the semantic content of the films themselves. To do so, we measured the VOCs from a movie theater over a whole month in intervals of thirty seconds, and annotated the screened films by a controlled vocabu-lary compiled from multiple sources. To gain a better un-derstanding of the data and to reveal unknown relationships, we have built prediction models for so-called forward predic-tion (the prediction of future VOCs from the past), back-ward prediction (the prediction of past scene labels from future VOCs), which is some form of abductive reasoning, and Granger causality. Experimental results show that some VOCs and some labels can be predicted with relatively low error, and that hints for causality with low p-values can be detected in the data. The data set is publicly available at: https://github.com/jorro/smelloffear.
 G.3 [ Probability and Statistics ]: Time series analysis Atmospheric Chemistry, Movie Analysis, Emotional Re-sponse Analysis, Breath Analysis, Data Mining, Application
Modeling, analyzing, and understanding human emotions, their expressions, and effects, has become a fascinating re-search topic over the past couple of years in computer sci-ence, as witnessed quite prominently by a recent special issue of Communications of the ACM (December 2014). While the physiological response of humans to emotional events or stimuli is well-investigated for many modalities (like EEG, skin resistance, ...), surprisingly little is known about the ex-halation of so-called Volatile Organic Compounds (VOCs) at quite low concentrations in response to such stimuli. VOCs are molecules of relatively small mass that quickly evaporate or sublimate and can be detected in the air that surrounds us.

VOCs are continuously generated within the human body [8], and can enter the atmosphere following emission in the breath or from the skin. While breath VOC concentrations can be large (ca. 100 ppb  X  p arts p er b illion) relative to typ-ical ambient values (ca. 1 ppb), human beings do not rep-resent a significant source of VOC on the global scale [25]. An active area of current breath VOC research is the assess-ment of metabolites and biomarkers as an aid in diagnos-ing disease [2]. While experiments have been performed to characterize the emissions as a function of age, gender, diet, and exercise [2], little attention has been paid as to whether breath emissions reflect a person X  X  current emotional status. This is an important question for medicinal breath measure-ments as the patient X  X  disposition to the analysis has the potential to affect the result itself. A related, broader, and highly intriguing question is whether breath-borne chemical signals convey unseen and unheard messages of love or fear.
In this work, we analyze the trace gas responses of peo-ple reacting on-line to films within a cinema/movie theater. Using a cinema as a laboratory is an elegant way to sam-ple large numbers of people simultaneously, in a relatively controlled but real-world environment. The subjects remain largely static while being exposed to the same stimuli at the same time and under stable flow conditions. Eliciting emotion through film has been used before to investigate possible human fear related scents in underarm secretions with samples being collected (one per person per film) for subsequent laboratory analysis [7, 1]. The data is a com-bination of two aspects. First, the content of the movies, i.e., the cause of the reactions of the visitors, and second the concentrations of VOCs in the cinema, caused by the reactions of the visitors in that cinema.

This combined data set of VOC concentrations and movie content provides multiple research goals and a playground for method-oriented data mining research. The most im-portant question, from an application point of view, may be what humans exhale or evaporate when confronted with certain stimuli. Hence, the goal is to find reactions in the VOC concentration, given certain events in the movies. To systematically analyze this, we annotated the movie con-tent with a set of approximately 40 labels, giving informa-tion on the plot of the movies. If there are certain types of scenes that cause an emotional response in the viewer, the viewer will emit certain VOCs. E.g., we could try to find out which VOC concentrations increase when the viewers feel fear, caused by a certain type of scene in the movies. To achieve this, we adapt algorithms to find hints about causalities in data, that use regression learners to calculate potential causalities between the concentrations of VOCs in the air and the content of the movie.

To sum up, there are four main contributions of this pa-per. First, we present a unique combination of online atmo-spheric chemistry measurement, movie analysis, emotional response analysis, breath analysis, and data mining, that has not been studied before. Second, we introduce a data set giving the concentrations of VOCs in the air in a cinema combined with scene annotations of the movies. This offers the intriguing possibility to relate the world of movies, nar-ratives, and the semantics and dramaturgy of movie scenes to the world of physiological response in the form of exhaled VOC concentrations. Third, we investigate the concept of abductive reasoning in a non-logical setting. Finally, we em-ploy a variant of Granger causality generalized to arbitrary regression models.

The remainder of the paper is organized as follows: First, we introduce the data set and describe how the measure-ments were done and the movies were labeled. Next, we de-scribe our methods to identify relationships between VOC concentrations and events in the movie using a variant of Granger causality and abductive reasoning. After this, we explain the evaluation process and give the results of our experiments. Finally, we give an overview of related work on the covered fields and give a conclusion.
The data set consists of two parts: First, the data on the concentrations of VOCs in the cinema. Second, the data on the content of the movie at each measurement. The concen-trations were measured using mass spectrometry in a cinema in Mainz, Germany in December 2013 and January 2014. Data on the movie scenes were generated by introducing a labeling system created to describe the movies in the data set the best. The labeling was done by volunteers from the Institute of Computer Science in Mainz. The scenes were Table 1: Data set statistics on movie screenings. All marked movies were labeled and used in the further analysis. labeled multiple times, and a consensus of the annotators was calculated.

We selected six movies from the data set and used them in the further analysis. From these six movies, we have 45 screenings (see Table 1). The cinema provided us with the times of the screenings and the number of visitors. One measurement was done every 30 seconds, hence, we have a data set of approximately 8000 instances. Buddy , The Se-cret Life of Walter Mitty , and Hunger Games 2  X  Catching Fire were selected as they provided the largest collection of screenings, and hence increased the size of the data set. Walking with Dinosaurs was not selected as the audience was rather small at the screenings and, as mainly children watched this movie, did not represent the average audience in the other screenings. The Hobbit  X  The Desolation of Smaug was chosen due to its thematic similarity to Hunger Games 2  X  Catching Fire in terms of target audience and production style. On the other hand, we chose Machete Kills and Paranormal Activity: The Marked Ones due to their contrasting styles. Both provoke a stronger reaction in the audience, being constructed to repeatedly shock the au-dience, which should cause stronger feelings than the other, calmer movies. Hence this should lead to a stronger signal in the data set. All data were recorded at the Cinestar Cinema in Mainz, Germany between December 1st, 2013 and January 14th, 2014. Of the 14 screen multiplex, two separate screen rooms were used (Kino 2, capacity 230, and Kino 7, capacity 230). During a film, the entrance doors were closed and ambient air was circulated from outside into the room through vents under the banked seating and out via ceiling mounted open-ings such that the screening room was flushed entirely circa 6 times per hour. The measurement instruments (PTR-MS-ToF and the CO2 detector) were located outside the screen-ing room (to avoid possible noise) and next to the outgoing air vent. An inlet was inserted into the midpoint of the exit flow and a 2 L / min was drawn through 1 4 00 Teflon line con-tinuously. From this line the PTR-MS-ToF and CO2 instru-ments sampled at 100 ml / min and 1 . 5 L / min respectively. A schema of the setup can be seen in Figure 2. (c) CO2 concentration 26.12.2013 12:30  X  17:00 F igure 2: Measurement in the cinema. Air gets drawn off out of the screening room. Via the ventilation system it is transported to the mass spectrometer in the ventilation room.

Note that while the mass spectrometry part of the data set covers almost a whole month of measurement, we were not able to use all of the data. There are gaps where mea-surements did not work, whole days where there was no measurement due to technical problems or calibration (this can be seen in Figure 1a). Certain time slots were not usable due to a systematic problem with the cinema air condition-ing system, which reset every midnight, changing the airflow, and thus making measurements at that time unusable. For the final computational experiments, we extracted a subset of the data that contains only VOC concentrations with at least 50% non-missing values. 1
VOCs were measured using a commercial PTR-TOF-MS (Proton Transfer Reaction Time of Flight Mass Spectrome-ter, Ionicon Analytik GmbH, Innsbruck, Austria). The mea-
It should be noted that the mass spectrometry instruments measure concentrations of certain masses in the air. The next step in identifying the VOCs related to the masses is to calculate the masses of VOCs and match them with the measured masses. surement technique is based on the protonation of molecules with a proton affinity higher than water by H3O+ ions (691 kJ / mol ) that are generated in a hollow cathode dis-charge. All protonated molecular ions are accelerated by an electrical field to the same kinetic energy such that the re-sultant velocity of the ions depends on the mass-to-charge ratio, which is determined by a time of flight mass spectrom-eter. Mass spectra were collected ranging from 10  X  500 m / z with a TOF acquisition sampling time per channel of 0 . 1 ns. The mass resolution was approximately 3700 m /  X  X . The instrument was operated with a drift pressure of 2 . 20 hPa (E / N140 Td) and a drift voltage of 600 V. For mass calibra-tion, 1,3,5-trichlorobenzene was used as internal standard. The PTR-TOF-MS was calibrated with a custom mixture of compounds calibrated with pressurized gas standards, which typically have an overall uncertainty of about 5%. The cal-culated detection limit (3  X  of the noise at background) was typically 15 ppt well below the measured concentrations. CO2 was measured on a one second timescale using a Li-COR Li-7000 system. The Li-7000 monitor was calibrated using standards ranging from 2 ppmv to 500 ppmv (Air Liq-uide, Germany). This instrument is easily capable of quan-tifying the changes detected within the cinema environment.
While there has been some work on scene annotations [14, 10, 27, 24, 26], there is, quite surprisingly, so far no stan-dard way of annotating movie scenes. Hence, for this data set, we suggest a combination of previously used scene an-notation schemes, and labels transferred from movie genres, as well as labels which are used in psychology [5, 15]. We selected intentionally labels that are objective (except the labels adapted from psychology), describing the content of the movie, not the reaction to it (as in action or chase). To use a more subjective labeling, describing the reactions in the audience, a larger set of annotators would be needed. Otherwise, the labels could not be transferred from the la-beling to the measurements in the cinema, as the audience might be too different from the annotators in terms of the reaction to the movies.

A common method to describe the movie content is the distribution of shots over the movie [20]. 2 Scenes which are faster and contain more action tend to have more shots than calmer scenes, e.g., romantic scenes. 3 We counted the num-ber of shots over the time period of 30 seconds, as this is the time period we have for every VOC concentration mea-surement. We used the Video Shot and Scene Segmentation tool by Mezaris et al. [3, 21] for the estimation of numbers of shots.

Additionally, we used the time in the movie, in absolute terms  X  giving the exact time since the movie started  X  and relative  X  giving the percentage, how much of the movie already passed  X  for every time stamp. This gives some in-formation, as movies tend to have similar dramaturgy, hence
A database and tool to manually count the shots is avail-able at http://www.cinemetrics.lv/ .
The number of shots and the shot length has a strong rela-tion to the year the movie was produced, and varies a lot in different decades. Nevertheless, this does not concern this data set, as all movies have a similar style and production year. similar content at similar times. The concentration of VOCs depends strongly on the number of visitors, the more people are in the room, the higher the concentrations of the VOCs, hence we included the number of visitors in the data. The more visitors are in the cinema, the higher the concentra-tions, and also the stronger the reactions to the movie as the visitors influence each other.

Nevertheless, the main information on the scenes is stored in the scene labels. A detailed list is given at https://github.com/jorro/smelloffear/wiki/ Scene-Annotations . There are three types of labels. First, all labels in the genre category are adapted from IMDB movie genres, like suspense or comedy . We selected a sub-set suitable for the movies in our data set and suitable to describe scenes. 4
The content category holds labels that describe what is happening in the scene, similar to the labels by Zhai et al. [26]. Examples are conversation , laughter , or injury . Some of the labels have sub-labels, like suspense , which can be split into chase , hidden threat , or hiding . If a sub-label is true, the corresponding label is true too.
Additionally, we have a set of ordinal labels indicating the valence and the level of arousal in the scene. They go from positive to negative and from exciting to calm in five levels. These categories are adapted from psychological ex-periments, where pictures were labeled according to that schema [15].
 This makes the data set clearly a multi-label data set. The labels have dependencies among them, e.g., action and suspense . Additionally, there are dependencies among the labels over time, because certain scenes are more likely to follow certain scenes. A recovery scene, for example, might more likely follow an action scene than a conversation scene.

The scene labels were generated manually by multiple peo-ple, such that each movie was labeled three times, with the exception of The Hobbit  X  The Desolation of Smaug and Paranormal Activity: The Marked Ones , where only two labelings were generated. Then, we averaged over the label-ings and only used labels, on which at least two third of the annotators agreed on. 5 The agreement was calculated over a time window of 30 seconds for each time point in the data set.
The combination of the presented two data sets provides several interesting learning tasks. Nevertheless, the main goal is to identify relations between the movie content, rep-resented by the labels, and the reactions of the viewers, given by their emitted VOCs, and hence, measured by the VOC concentrations in the air. This could give an indication what the human body uses to potentially broadcast the current emotions.

To find these effects, we can use a certain time frame in the data sets and examine the interactions among the
Suitable labels in terms of being used in the movies in the data set. Genres like documentary simply do not apply to the movies.
In this case, this means two of the three annotators needed to agree. In the case of two movies, we only had two anno-tations, in these both annotators had to agree on the label. data. To examine a certain time point in the movie, we set a frame of five minutes before and five minutes after the time point (resulting in 10 measurements each, as the mass spectrometry data was measured every 30 seconds). Five minutes seem to be a reasonable time frame, as the reactions of the audience in terms of emitted VOCs have enough time to reach the mass spectrometer (which takes up to two minutes), and we have a certain buffer, as we cannot align the data sets perfectly. Additionally, reactions might get stronger with the time as the audience influences itself, a visitor might react stronger after he or she feels the neighbor is also scared. In terms of movie length, 5 minutes is a long enough period to cover the current events and not interfere too much with unrelated events in the movie (too long ago in the past).

By considering the past, we can investigate the influence of the past labels and the emitted VOCs of the visitors on the current concentrations in the cinema. This is an inter-esting question, as we can see if a certain VOC increases some reaction in the audience and whether some effects get stronger. Considering the future time frame, we can ex-amine from the perspective of a movie label whether some VOC concentration increases after this label has occurred. In other words, if this scene appeared in the movie, what is the reaction in the audience, e.g., if the scene is scary, what is the  X  X mell of fear X  in the cinema. We introduce four approaches to analyze the data set. The first one, called forward prediction, uses regression to predict VOC concentrations from the past VOC concentra-tions and events in the movie. As the primary goal is to gain insight into the data, we used trees, so that we can exam-ine the tree structure to discover dependencies in the data set, e.g., which scene was needed in the past to make this concentration go up. The second approach, called backward prediction, predicts labels of the movies from future VOC concentrations. 6 We use the concept of abductive reasoning, in a non-logical setting, for this purpose. This is another way of identifying relationships. Using trees, again, we can see in the structure which concentrations need to go up to have a certain scene before that. We validate the predic-tions using the third method. By using the predictions of the forward learners as input to the backward learners, we can evaluate the prediction without knowing the actual val-ues of the future. Finally, the fourth method is a variant of Granger causality [11], adapted for our purposes. Leav-ing out single time series in the learner and comparing the performance with and without that time series, we can find potential causal relations between them. E.g., we can try to predict the label action with the concentration of CO2, and without it; if the prediction with it is significantly better with than without it, there might be a causal relationship.
This method targets one of the main goals in time series analysis, the prediction of future events from past events. In our case, we predict the next concentration of a VOC given a time frame in the past including all other data and the current events in the movie. The past data consists of all Hence the name backward, predicting the past.
 VOC concentrations, as well as all scene annotations in a time frame before the target time point. The current data known to the learner are only the scene annotations. One learner is learned independently from all the others.
The algorithm (see Algorithm 1) transforms the data set such that one set of instances is generated for each target VOC. Each instance in each set represents one time point with the features consisting of scene information and other VOCs. The scene information is stored for every time point from 30 seconds ago until 5 minutes in the past, each 30 second step as a feature. Information about all VOCs is given from the point in time before this point in time until 5 minutes in the past, again with each 30 second step as a feature. The target value of an instance is the corresponding VOC concentration to this point in time.

Algorithm 1: Data Set Generation for Forward Pre-diction of VOCs. In the pseudo-code,  X  denotes the concatenation of two or more vectors.

Hence, the learning task is: Given the past concentrations of VOCs in the air and the recent scenes in the movie, what will be the next concentration of this VOC. In this way, the model predicts how the audience will react to these scenes.
This model can also be used to identify dependencies among the VOCs. E.g., when using a regression or model tree, the dependencies can be identified by inspecting the structure of the tree.
To further identify the dependencies between VOCs and movie content, this algorithm trains models to predict the scene labels based on the VOCs. This can give further in-sight to discover which labels are influenced by which VOC and, hence, which product of human metabolism.

Abductive reasoning argues that, given an observation a , and knowing that the cause of a is b , then b must occur too. This can be adapted to our case. Hence the backward prediction can be understood as abductive reasoning, as we use the future concentrations to predict the current event in the movie.

As the audience reacts on the scenes, the reactions of the audience are in the future of the scenes. Hence, if we want to predict the labels, we have to use the information of the VOCs from the time steps in the future compared to the current time step (backward prediction). Thus, the data set needs to be assembled in a different way compared to the previous problem. The procedure is the same, but not the past steps are included in the data set, but the future time steps (see Algorithm 2).

Algorithm 2: Data Set Generation for Backward Pre-diction of Scene Labels. In the pseudo-code,  X  denotes the concatenation of two or more vectors.

As the resulting data set is similar to the one from the for-ward prediction, we will not give a separate example. The only difference would be the abundance of the scene annota-tions in the features and the time frame, which would not be in the past, but the future. While it does not seem intuitive to predict the past from future events, in this case it has a practical use. We can confirm that there are relationships in the data set between labels and VOCs. And as for appli-cations, this method can be used to label unlabeled scenes or pictures based on the reaction of the audience.
Given these two algorithms, we can combine them (a vi-sualization of the approach is given in Figure 3). We use the forward models to predict the future concentrations of VOCs, then use models on the predicted concentrations to predict the labels, which were used as features in the forward classifiers.

This approach 7 can be used in the prediction of future events in time series to verify and improve the models. A model can be trained to predict future events, and another model to predict the features from the target values. Then, the future can be predicted in a certain time frame, and, using the other model, the present features can be predicted from the future. This can be evaluated and, e.g., used to improve the parameters of the model.

In our case, we use this process to evaluate the overall approach of predicting future and past events in time se-ries. While we can use the individual algorithms to identify dependencies and potential causalities in the data, this com-bined algorithm can evaluate the overall process (e.g., the coherence of the forward and backward prediction models).
The pseudo-code is quite bulky and omitted due to space constraints. r omance Acet on
Iso pren t = -00:30 t = -05:00 F igure 3: An overview of the algorithms used in this paper.
The final step is the identification of possible causal re-lations between the VOC measurements. To do this, we adapted the concept of Granger causality [11] to our case, for the previously learned models (see Algorithm 3). Granger causality uses the p-value of (typically linear) autoregressive models to determine if two time series have a causal relation or not. To deal with our quite complex scenario, we suggest to consider the statistically significant difference measured in the error of the predictions of arbitrary regression models. Algorithm 3: Causality Calculation
This method uses the significance calculated by using a t-test to compare the performance of a classifier on the whole data set, compared to the performance of the learner on the data set without the future information of another time series. The significance level hints at a potential causality between these time series. We used the corrected resampled t-test to establish this statistical significance. One impor-tant step is the transformation of the time series into the data set for the learner. Each instance in a data set consists of the future of all time series, and the current value of the first target label. Then, the performance of a classifier is calculated on this data. The target VOC concentration is removed from the data set and another performance of the learner is calculated on the new data set. The performance is compared using a significance test, returning the signif-icance level, which can hint at a potential causality of the two targets (scene label and VOC concentration).
We evaluated the data set and methods using multiple approaches. The evaluations were carried out using a 15 times repeated holdout validation, where 1 / 3 of the screen-ings in data set were used as a test set, and the remainder as a training set. Additionally, we used a  X  X ard X  evaluation setting, where we left out all screenings of one movie and used this set as a test set. This validation procedure might work better for certain movies in the test set easier, as they are similar to others (being in the training set), like Hunger Games 2  X  Catching Fire and The Hobbit  X  The Desola-tion of Smaug , both being big Hollywood productions with a similar target audience and screenplay. Additionally, we evaluated the performance of the classifiers over the time. Predicting in both directions time step by time step, we used trained models to further fill the data set.

Forward-backward prediction was used as described pre-viously to evaluate the performance of the combination of forward and backward learning. First, we trained models on the training sets. Then we predicted the values on the test set in different window sizes step by step. Subsequently, we used the backward models to predict the movie labels from the just predicted VOC concentrations. This was done until we reached the starting point again, where we evaluated the predictions using the known values.

Finally, to calculate the causality, we evaluated the back-ward classifiers on the whole data set using holdout valida-tion. Leaving out single VOC concentrations as described above, we evaluated the new performance. This was again repeated 15 times. From these two results, we calculated the corrected resampled t-test.

Model training was carried out using several classifiers and regression learners. For most of the steps, except where we aimed for a visual representation of the dependencies, we used random forests. To inspect the trained dependencies, we used J48, all implemented in WEKA [12].
In the following section, we give an overview on the ex-perimental results on the data set. Due to space limitations, we cannot give all result tables and figures in this paper. More detailed results are available at https://github.com/ jorro/smelloffear/wiki/Experimental-Results . The results on the forward and backward prediction (see Figure 4) show that meaningful models can be trained on the data set. Clearly, good models cannot be trained for all VOC concentrations, many of them are completely independent from emitted VOCs of cinema visitors. Additionally, the results depend on the harder evaluation case, i.e., leaving out all screenings of a movie gives a worse performance than on the  X  X oft X  evaluation when completely randomly leaving out screenings.

What can be seen is that good models can be trained on blood (violence) , death , and conversation -main ac-tor . blood (violence) seems plausible, as in movies, this is usually used to cause a strong reaction in the audience, some Figure 4: Performance of forward and backward prediction evaluated using holdout validation. The top row shows the best (Figure 4a) performance in terms of area under ROC curve on labels when using holdout, where 1 / 3 of the screen-ings were left out for testing. The bottom row gives the best (Figure 4b) performance in terms of Pearson correla-tion coefficient between prediction and real value on VOC concentrations when using holdout. role in the movie is hurt, which is a good thing (if a villain is hurt), or a bad thing (if a main role is hurt). Both using strong reactions which should be found also in the emitted VOCs. The same can be said about death , which might many times appear together with the blood (violence) la-bel and cause similar reactions. On the other hand, this one does not occur too often in these movies. Conversations of the main actor on the other hand occur comparably often in the movies. Additionally, this usually contains important information, the main role is the main focus of the movie. Hence, this might catch the attention of the audience and hence cause stronger reactions than other scenes where, e.g., a landscape is shown.

If we consider the class distribution and consider labels with a relatively high percentage of positive values, we can see that suspense , comedy and conversation -main ac-tor can be predicted from the data. Figure 5: Performance of backward prediction evaluated us-ing holdout validation, leaving one movie out at a time. It should be noted that we get the performance of fewer la-bels as many labels are exclusive to certain movies or do not appear in other movies. E.g., blood (violence) does not occur at all in The Secret Life of Walter Mitty , hence we can not evaluate it when using this movie in the test set.
In the  X  X ard X  evaluation scenario, we left out all screen-ings of a movie for the training and used them as a test set. Figure 5 gives the performance of single scene annota-tions depending on the movie that was used in the test set. Unfortunately, the density of the labels is relatively sparse, and leaving one movie out completely increases the risk of certain labels to not occur in the test or training set at all. Many labels are exclusive to certain movies or genres. Hence they cannot be predicted at all. This reduces the possibility to compare the performance in this case.
For the case of forward-backward, we give the performance of this approach in dependency of the starting point in Table 2. When starting at the beginning of the movies, only the first 10 time points were set as known, the remaining ones were first predicted using forward prediction, then backward prediction was used to predict all labels from the predicted VOC concentrations. When set in the middle of the movie, 10 time points before the middle were set to be known, the remainder of the movies was predicted. At the end of the movie, only the last 10 time points were predicted.
Each task has its own challenges, making it hard for the classifiers to perform well. If set in the beginning, the algo-rithm has to predict all values to the end and back, hence adding a lot of uncertainty to the prediction. Hence, only suspense and conversation -main actor achieves a per-formance above random. This might also be attributed to the fact that these labels have a high chance of appearing at the beginning, introducing the main actor with a conver-sation, or drawing the viewers attention to the movie.
The prediction of the middle starting point is similarly challenging, still, half of the movie needs to be predicted, and again, only conversation -main actor can be predicted well. Nevertheless, this can be done remarkably well. This might be attributed to it being one of the most frequent Table 2: Performance of forward-backward prediction in re-lation to the window size. The forward prediction was per-formed over the whole movie (Table 2a), half of the movie (Table 2b, and the last part of the movie (meaning only one step) (Table 2c). It should be noted that only few labels can be actually evaluated since they do not necessarily oc-cur exactly at this time point. It should be noted that conv. m.a. refers to a conversation including the main actor, while conversation in Figure 5 refers to any conversation. labels, and hence, providing enough information to train meaningful models.

Finally, when evaluating starting from the end, the biggest challenge might be that there is no big chance of correcting an once incorrectly made prediction, as it is only done in 20 steps. This, and the sparsity of labels shortly before the end of the movie lead to a random prediction in this case.
In the experiments, we were able to find hints for causal-ities between scene annotations and VOC concentrations. Low significance levels, indicating a high causality were achieved for multiple annotation  X  VOC pairs.

In Table 3 the causalities of four scene annotations are given. The masses of the VOCs with the highest causal relations to each annotation are given. Table 3b gives the relations to the suspense annotation. This seems to be an interesting case, as only two VOCs seem to be important for this label. So far, it is unknown which VOCs these are, as only the mass is known, but we will investigate these in the future. Table 3c gives a broader list of VOCs, and there seem to be many VOCs related to the crying annotation. This could potentially cause a strong reaction in the audience and we will look into these in the future. Table 3a gives the relations to the annotation blood (violence) , meaning there is some blood seen on the screen, and the bleeding is caused by violence. The first mass in the list, 18 . 0338 most probably belongs to protenated ammonia, a component of sweat. This could mean that the audience sweated at the sight of violence and blood, which appears quite plausible.
Nevertheless, these are only a small fraction of the cal-culated results. It is ongoing work to identify the VOCs belonging to these mass points and relate them to known mechanisms of human physiology.
The related work of this paper covers a wide range of top-ics: atmospheric chemistry, movie analysis, emotional re-sponse analysis, breath analysis and data mining in these areas. As this combination has not been done before, there is no work that covers the same combination. The closest work might be from Soleymani et al. [22], who analyzed a data set collected in psychological experiments, where the physical response of viewers of movie clips was measured in multiple ways and analyzed. Nevertheless, they only mea-sure responses of single persons, not on a larger scale as the experiments in the cinema. Additionally, they did not measure the emitted VOCs but the direct body reactions and tried to discover correlations to features of the scenes. Similar work to this has been presented, mostly focusing on the response of body signals to emotional stimuli. E.g., data mining was used by Frantzidis et al. [9] to predict the valence and arousal dimension from body signals.

On the other hand, data mining is well established on movie or movie rating analysis. E.g., the ratings which are provided by the Netflix challenge [4]. Different to the work presented here, they relate movies and their content to the users rating the movie, and not a measured response directly to the scenes.

Atmospheric chemistry has been connected before to hu-man emitted VOCs. Veres et al. [25] measured VOC con-centrations in a stadium during a football game. It was possible to identify certain VOCs directly related to the vis-itors of the game like human respiration/breath, ozonolysis of skin oils, and cigarette smoke/combustion. Nevertheless, the experiments in the cinema provide a much more con-trolled environment, the visitors being relatively still, and minimizing influence from the outside.
 Table 3: The suggested causalities for selected scene la-bels. A lower significance level indicates a higher predicted causal relation to the scene annotations. We give causalities for the scene labels blood (violence) , suspense , crying , and comedy . As it is not always clear which compound be-longs to which mass, we can only give suggestions for some of the masses, which is given in brackets under the corre-sponding mass. Note that these are only suggestions, given with our experience from atmospheric chemistry, as multiple molecules can have a similar mass, or some masses might be fragments of compounds, or combinations of multiple com-pounds overlay into one signal.
Van Berkel et al. [23] used SVMs to analyze VOCs in the exhaled air of probands. This study tried to distinguish smoking from non-smoking probands by the VOC concen-trations, unlike this work, which aims to identify signals of emotional responses in the air.

Rignello et al. [19] aimed to find connections between the heart rate and emotional stimuli caused by music. They used rule learners to find correlations between the heart rate and music samples, previously manually classified into happy or sad samples. In the publication, they found indications that there is an anatomic response to emotional stimuli.
Prediction of future events in time series, or forecasting has been widely studied in data mining and machine learn-ing. A wide range of methods exist, an overview can be found in Chatfield [6] or Hamilton [13]. Nevertheless, to the best of our knowledge a backward prediction, a prediction of the past from future events to analyze dependencies, is not well-investigated in this area.

The notion of abduction in a philosophical sense was intro-duced by Peirce [18] and meanwhile adapted by several com-puter science applications, e.g., bioinformatics [16]. Never-theless, it was always used in a logical setting, our approach is one of the few in a non-logical data mining setting.
The most widely used causality calculation on time series is the Granger causality [11]. Granger causality calculates the p-values of autoregressive models on time series and com-pares them to linear regression learners on a subset of these time series, where certain series are left out. This approach is very close to the one presented here, we simply extend it to non-standard autoregressive models.
In this paper, we presented the analysis of a new data set generated from the measurements of VOC concentrations in a cinema and the annotation of the corresponding movies. We introduced new approaches to identify hints for poten-tial causalities among different time series, in this case, the annotations of the movies and the measured VOC concen-trations. Additionally, we used the concept of abductive reasoning to validate the prediction of future events in time series. This was done using an additional backward predic-tion of current events in the time series from future events in another series.

The experimental results validated the approach and show the potential of the used methods. Currently, we are eval-uating the results in detail and try to identify metabolic products that cause the change in the VOC concentrations. Additionally, we are carrying out further measurements in an even more controlled environment, where the reactions of single probands to standardized visual stimuli are measured. We will apply the presented methods to these new data sets.
Nevertheless, this data set has a high potential for more research questions, besides the identification of relations be-tween the content of the movies and the emitted VOCs. It can be examined what influence the emitted VOCs have to other visitors, does it change their behavior, i.e., do they react stronger or less to something, given the presence of other VOCs in the air? Another possibility would be to focus on the movies and identify patterns in the movies in terms of annotations and reactions in the audience to find certain parts of the story or scenes, designed to cause some reactions in the audience. Of interest for the movie indus-try might be the reactions in terms of what the audience consumes. It can be identified if and when alcohol was con-sumed in the screening room, soft drinks emit certain VOCs, so the question could be, are there scenes in the movie that cause the audience to consume certain products?
In summary, we presented an application along with methodological approaches with a high potential for discov-eries regarding important research questions, e.g., do hu-mans communicate their feelings via emitted VOCs? The data set was generated and analyzed using a unique combi-nation of atmospheric chemistry, breath analysis, emotional response analysis, movie analysis, and data mining. [1] K. Ackerl, M. Atzmueller, and K. Grammer. The scent [2] A. Amann and D. Smith. Volatile biomarkers: [3] E. Apostolidis and V. Mezaris. Fast shot segmentation [4] J. Bennett and S. Lanning. The netflix prize. In [5] M. M. Bradley and P. J. Lang. Measuring emotion: [6] C. Chatfield. Time-series forecasting . CRC Press, [7] D. Chen and J. Haviland-Jones. Human olfactory [8] B. de Lacy Costello, A. Amann, H. Al-Kateb, [9] C. A. Frantzidis, C. Bratsas, M. A. Klados, [10] Y. Gong, W. Wang, S. Jiang, Q. Huang, and W. Gao. [11] C. W. Granger. Investigating causal relations by [12] M. Hall, E. Frank, G. Holmes, B. Pfahringer, [13] J. D. Hamilton. Time series analysis , volume 2. [14] S. Moncrieff, S. Venkatesh, and C. Dorai. Horror film [15] J. D. Morris. Observations: Sam: the self-assessment [16] H. Nabeshima, K. Iwanuma, K. Inoue, and O. Ray. [17] J. Pearl. Causality: models, reasoning and inference , [18] C. S. Peirce. Abduction and induction. Philosophical [19] F. Riganello, A. Candelieri, M. Quintieri, D. Conforti, [20] B. Salt. Statistical style analysis of motion pictures. [21] P. Sidiropoulos, V. Mezaris, I. Kompatsiaris, [22] M. Soleymani, G. Chanel, J. J. Kierkels, and T. Pun. [23] J. Van Berkel, J. Dallinga, G. M  X  oller, R. Godschalk, [24] J. C. van Gemert, J.-M. Geusebroek, C. J. Veenman, [25] P. R. Veres, P. Faber, F. Drewnick, J. Lelieveld, and [26] Y. Zhai, Z. Rasheed, and M. Shah. Semantic [27] H. Zhou, T. Hermans, A. V. Karandikar, and J. M.
