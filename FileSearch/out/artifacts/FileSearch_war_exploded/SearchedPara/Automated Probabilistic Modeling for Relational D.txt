 Probabilistic graphical model representations of relational data provide a number of desired features, such as inference of missing values, detection of errors, visualization of data, and probabilistic answers to relational queries. However, adoption has been slow due to the high level of expertise expected both in probability and in the domain from the user. Instead of requiring a domain expert to specify the probabilis-tic dependencies of the data, we present an approach that uses the relational DB schema to automatically construct a Bayesian graphical model for a database. This resulting model contains customized distributions for the attributes, latent variables that cluster the records, and factors that reflect and represent the foreign key links, whilst allowing efficient inference. Experiments demonstrate the accuracy of the model and scalability of inference on synthetic and real-world data.
 H.2 [ Database Management ]: Miscellaneous Bayesian models;Probabilistic Modeling;Databases
Relational databases have been the primary choice for man-agement of structured knowledge for a majority of scientific and commercial applications, such as medicine, bioinformat-ics (protein interactions), commercial transactions, paper citation records, product ratings, and many more. These databases are often large, noisy, and contain missing values for many cells. Incorporating machine learning techniques into such databases can help in a number of ways: inference of missing values, detection of errors in the database (out-liers), probabilistic responses to queries (taking into account the noisy and missing values), and visualization of the data. (a) Table without Links Figure 1: Building blocks of the graphical model: red the rows of each table in a manner that is aware of the clusterings of linked tables; these clusterings can be used for data exploration and visualization. Inference is used to learn the parameters of the model, allowing predicted distributions over the missing values and error-checking via outlier detec-tion (low-probability observations). Most importantly, since the approach is fully-automated, we enable the construction of probabilistic models automatically for a large number of existing databases without any manual intervention. In this section we describe how we automatically create a Bayesian graphical model from a database schema.
 Schema Description : Formally, a schema S consists of linked tables A,B,C, ... , where each table A contains n A These attributes are typed by the standard data types (inte-ger, double, categorical, string, etc.), and may be missing or observed. Each table A may also contain m A foreign links to other tables B 1 ,B 2 , ... ,B m A ; the p th link in row a in A to a row in B p is represented by f AB p a . The links in the schema, when represented as edges A  X  B p , should form an acyclic graph 3 . Further, we filter out the attributes that should not be modeled, for example we automatically exclude string attributes that contain many unique values.
 Single Table : We begin the description of the model by examining a single table A with attributes x A . We employ a mixture model for each table, wherein a mixture component is used to generate all the attributes x A a of row a . Specifically, the model for each table consists of J A components, and each component j represents the distributions  X  A j 1 , X  A j 2 , ... , X  A jn A that correspond to each attribute of A . Each row also con-tains a latent variable z A a  X  { 1 ... J A } that indicates the component row a belongs to. We use this latent variable to select the distribution to generate the attributes of the row, i.e. we use  X  A ji to generate x A ai where j = z A a . The type of the latent distributions  X  A ji depends on the data type of the attribute (Gaussian for real-valued, Discrete for categorical-valued, and Bernoulli for Boolean-valued attributes), and each distribution is generated from its observed, conjugate prior  X  A ji (Gaussian for the mean, Gamma for the precision, Dirichlet for Discrete, and Beta for Bernoulli). If the table
The violation to this ( circular reference ) is often discour-aged, if not disallowed, in most DBMS. Further, such refer-ences can often be normalized by including additional tables. Model Hyper-Parameters : A number of priors in the models need to be specified. We set these priors to be unin-formative 5 , however specifying the number of components for each table is crucial. Too many components result in slower inference, while too few components produce inaccu-rate models. We set these components heuristically, and will explore non-parametric approaches in future work. The model is generated as source code for Infer.NET [ 8 ], however other graphical model toolkits may also be used. Inference is performed using variational message passing [ 11 ]. Training and Predictions : During training, we learn the parameters of the model and use it to predict missing values. The complexity of each iteration of message passing is linear in the number of rows when all foreign keys are observed, since estimating the priors for foreign links is equivalent to counting. When not all of the links are observed, the complexity remains linear in the number of rows in tables that contain links, which are often much bigger than the tables they point to. Although the complexity is proportional to the product of the number of components in tables that are linked to (dimensionality of  X  B i in Figure 1(b)), in practice it is rare to find more than 3 tables linked from a single table. Marginal distributions computed during inference for each cell can be directly used to predict missing values (and confidence) and detect outliers in the observed cells. Further, latent variables z assign a component/cluster to each row; predictions include probabilistic assignments of each row to these clusters.
 Querying the Model : The approach also supports a re-stricted set of queries over the trained model. Queries take the form of a set of rows for each table with missing values or foreign links. Using the learned distributions, inference estimates marginal posteriors over the query rows that are used to predict missing values and detect outliers in the query. Inference also provides clusters for each query row that may be used to discover similar rows in the existing data (such as other users that rate similarly). Query inference is efficient: linear in query size if links to the existing data are observed. In this section, we present experiments to evaluate the accu-racy, clustering quality, and scalability of the approach. Synthetic User-Movies-Ratings : One typical approach to modeling values in a database is to use a single-table mixture model on the result of a join over all the tables. Unlike in our relational model, the dependencies across rows are lost in the join operation 6 . To evaluate its effect on accuracy, we compare these models by hiding a proportion of cells before performing the join. We create synthetic data for the schema in Figure 2(a), and perform inference to predict the values of the hidden cells. The prediction error for real-valued attributes is shown in Figure 3, demonstrating that the schema-based model is consistently more accurate and robust in the presence of missing cells. In particular, the We use (  X  = 0 , X  = 10 8 ) for Gaussian, ( k = 1 , X  = 10) for Gamma,  X  = 1 for Discrete, and (  X  = 1 , X  = 1) for Beta.
For example, age of a user may appear in multiple cells in the joined table, and thus the estimates may be different. Relational Latent Variable Models : Our underlying model is most similar to the latent variable models for rela-tional data [ 12 , 7 ]. These approaches construct a generative model over entities and relations , where each member of these classes is associated with attributes and latent com-ponent indicators. A similar approach was also proposed for graphical data [ 1 ]. Although we have been inspired by these models, the relations that they support are restricted to presence/absence, while our approach, by representing uncertainty at the (finer) level of foreign key relationships, is not restricted to Boolean links. Even though these ap-proaches are non-parametric, inference speed is impractical; inference for our model scales linearly in the size of the data. Statistical Relational Learning (SRL) : SRL has made significant strides in representing uncertain entity-relation data [ 4 ]. Friedman et al. [3] and Heckerman et al. [5] intro-duce Bayesian models for SRL, allowing informed domain experts to create probabilistic models. Taskar et al. [10] in-stead use undirected models, for which Neville and Jensen [9] introduce tractable learning. Our work differs in a number of aspects. First, we directly represent the schema and foreign keys, providing the flexibility to represent data that existing approaches cannot. Second, we create a joint model and use a well-understood inference algorithm that is accompa-nied by certain guarantees. We prefer Bayesian generative modeling due to its elegant adaptation to different domains; undirected models are often unreliable on small and/or sparse data. Third, since the schema captures much of the depen-dency structure, by using latent foreign links we create a customized model without making any other assumptions. Probabilistic Databases : In recent years, there has been significant progress in probabilistic databases , i.e. tools to manage imprecise and uncertain data scalably, and to sup-port efficient probabilistic inference for arbitrary SQL-like queries [ 2 ]. To attain this goal, many of these approaches make strong independence assumptions in the probabilistic model, resulting in lower accuracy. Others require the user to specify the underlying model, and perform efficient in-ference and learning over this model. We instead focus on automatically creating a joint, expressive model over all the data, and perform queries on a subset of SQL. Although filling missing values is related to uncertainty representa-tion (as both require an underlying probabilistic model), by restricting our queries to a reasonable subset (that we feel is important), we are able to use the power of Bayesian
