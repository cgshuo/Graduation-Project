 As the scale of available on-line data grows ever larger, individuals and businesses must cope with increasing complexity in decision-making processes which utilize large volumes of unstructured, semi-structured and/or structured data to satisfy multiple, interrelated in-formation needs which contribute to an overall decision. Tradi-tional decision support systems (DSSs) have been developed to ad-dress this need, but such systems are typically expensive to build, and are purpose-built for a particular decision-making scenario, making them difficult to extend or adapt to new decision scenarios. In this paper, we propose a novel decision representation which al-lows decision makers to formulate and organize natural language questions or assertions into an analytic hierarchy, which can be evaluated as part of an ad hoc decision process or as a documented, repeatable analytic process. We then introduce a new decision support framework, QUADS, which takes advantage of automatic question answering (QA) technologies to automatically understand and process a decision representation, producing a final decision by gathering and weighting answers to individual questions using a Bayesian learning and inference process. An open source frame-work implementation is presented and applied to two real world ap-plications: target validation , a fundamental decision-making task for the pharmaceutical industry, and product recommendation from review texts , an everyday decision-making situation faced by on-line consumers. In both applications, we implemented and com-pared a number of decision synthesis algorithms, and present exper-imental results which demonstrate the performance of the QUADS approach versus other baseline approaches.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval; H.4.2 [ Information Systems Applications ]: Types of Systems X  Decision support ; J.3 [ Computer Applications ]: Life and Medical Sciences question answering; decision support; target validation; product recommendation
As the scale of available on-line data grows ever larger, indi-viduals and businesses must cope with increasing complexity in decision-making processes which utilize large volumes of unstruc-tured, semi-structured and/or structured data to satisfy multiple, interrelated information needs which contribute to an overall de-cision. The field of decision support systems (DSS) [33, 4] has focused on how to help decision makers to: a) structure a decision-making process interactively, b) automatically access and retrieve decision-supporting information, c) adapt decision models or ana-lytic techniques for evidence fusion, and d) produce a final report [19, 35, 4]. However, traditional DSSs mostly suffer from inflexi-bility in adapting to new decision scenarios. First, a well-tailored DSS is usually developed for one specific decision problem, such that users are unable to dynamically input new decision scenarios the way that users of an information retrieval system can formu-late novel queries. Second, data retrieval and processing typically follows a fixed flow within a DSS, which does not allow decision makers to easily leverage additional decision support information, integrate data analysis algorithms or tools, or further reconfigure the decision support pipeline for better performance. A large team of developers who have some acquaintance with the decision prob-lem and the application area are typically involved in development and deployment of DSSs [33]. As a result, DSSs are normally very expensive to build and maintain.

Defining a formal decision representation that accurately reflects a decision maker X  X  information need is a challenging problem [34]. To support a more direct and effective communication between de-cision makers and the underlying analytic models and tools, and to provide a more flexible solution, we propose a novel decision rep-resentation that allows decision makers to formulate and organize natural language questions or assertions in an analytic hierarchy [31], which has been shown to be the most natural and least am-biguous way to express an information need [28]. We then intro-duce a new decision support framework, QUADS, which takes ad-vantage of question answering (QA) technologies to automatically understand and process the decision scenario in real time, providing an overall weighted decision by combining evidence from answers to individual questions [19, 2].

Question answering and decision support systems have been in-dependently developed for decades. With the recent development of high-performance question answering (QA) systems, which combine natural language processing and information retrieval, users can directly interact with an information system to evaluate evidence gathered automatically; IBM X  X  Watson [7] is a prominent example. Only recently has automatic question answering been ap-plied in support of complex human decision making for specific domains, e.g. cancer diagnosis [20].
To date, a general approach for decision support using QA tech-nologies has not been thoroughly and systematically studied. Our goal in this work is to outline a formal procedure for represent-ing any decision-making process as a complex question answering scenario. Following the desiderata expressed by other decision the-orists [9, 6], we focus on tackling the following challenges in this paper:
In this paper, we introduce a directed acyclic graph (DAG) rep-resentation for decision processes, and formalize the decision mak-ing problem in the context of natural language question answering. The decision process is then mapped to a Bayesian decision repre-sentation, to jointly model the entire decision scenario (including the users X  preferences, assertions from QA systems, and hierarchi-cal decision factors) in a unified model. We present a message-passing algorithm that is used to predict decisions and learn deci-sion models from labeled examples. The QUADS framework was implemented as an open source software package, with wrappers for Apache UIMA 1 and the CSE framework [37] to support: a) in-tegration of any QA system deployed as a UIMA Aggregate Analy-sis Engine, and b) configuration, optimization and extension of the QA system with various decision synthesis strategies. Finally, we describe how the QUADS framework was leveraged to help vali-date potential gene targets for diseases, one of the most fundamen-tal tasks in the life sciences, using a real-world dataset containing nearly 6K diseases and over 6K known gene candidates. We also describe another application where QUADS was used to implement a decision process that recommends products based on on-line re-views. Figure 1 shows the overview architecture diagram of the QUADS framework.
Two research areas related to this work  X  decision analysis and question answering  X  have been extensively studied for decades. In this section, we first survey prior work in the field of decision analysis, focusing on comparison of decision analysis models and decision making approaches, and present a few decision support systems. We then focus on reviewing related work in the question answering domain, especially question decomposition methods and prior work on scenario or contextual question answering.
Decision Analysis Models. Decisions are usually represented by a hierarchy or network of sub-criteria or sub-problems that are more easily comprehended or evaluated in isolation. Example rep-resentations include analytic hierarchy process (AHP), or analytic network process (ANP) representation [31], decision tree, or more compact influence diagram (ID, or decision diagram, a represen-tation of Bayesian decision problems) [10], etc. AHP focuses on one single uncertainty with multiple alternatives, and decision makers are not allowed to explicitly assign priority judgments to non-primitive criteria . In contrast, an ID can more flexibly model complex decision situations with multiple alternatives from multi-ple uncertainties, and allows the decision makers to specify their preference at any node in the diagram. In this paper, we introduce a directed acyclic graph (DAG) representation for decision processes, and provide a solution framework that automatically finds answers to each question and gathers evidence from all decision factors.
Decision Support Systems. Many DSSs have been developed in support of various decision scenarios during the past few decades [19, 35, 4]. Although most DSSs are built for predetermined tasks, which requires domain knowledge obtained from humans and stored in a structured data store with a predefined schema, a few DSSs have explored how to understand decision needs from natural language input. For example, the Structured Evidential Ar-gumentation System (SEAS) [19] provides decision references for national security crisis warnings, and Intelligent Decision System (IDS) [35] analyzes business innovation self-assessments written in free-text. However, neither approach is general or extensible enough to cover the cases we implemented with our proposed so-lution and open-source implementation.

Question Decomposition. Complex factoid questions usually contain more than one factor or assertion about the answers [12], and the question answering systems need to correctly identify each factor, reconstruct subquestions, and merge evidence from sub-questions. Most approaches leverage syntactic or semantic prop-erties of the question, e.g. lexical cues [12, 32], coreference [12, 16], relation triples [13], semantic dependencies [16] to identify subquestions, and then deploy discourse [12] or semantic decom-position such as knowledge templates [13, 29], textual entailment [16], constraint networks [29]. Compared with question decom-position in open-domain QA (such as Watson [12, 7]), where the entire information need might be expressed in a single, paragraph-length question, QUADS assumes that the decision maker has al-ready decomposed the overall information need into individual, atomic subquestions that can be understood (and answered) inde-pendently, based on best practices and templates developed for do-main decision-making.

Scenario and Contextual QA. Our work is directly related to prior work on scenario QA, where the user X  X  input can include background information, questions with multiple parts, and even follow-up questions, requiring a more complex representation of the information need [3]. However, little research has been done to systematically study how to support global decision making using a scenario QA system. Contextual QA is a related research area which focuses on how systems can automatically track the infor-mation context through a series of questions in dialogs with real users [36]; however, this research has focused primarily on context identification using discourse analysis to improve answering of in-dividual questions, rather than hierarchical decision-making.
In this section, we formally define the concepts related to the decision process and provide a formal definition of the decision making problem in the context of question answering.

For complex multi-criteria decision problems, decision analy-sis usually employs the general divide-and-conquer approach, via problem reduction and solution synthesis phases. In the problem reduction phase, decision situations are often formalized by a re-cursive procedure that decomposes the decision goal into a hierar-chy or network of sub-criteria or sub-problems that are more easily comprehended and/or evaluated independently [23]. In the solution synthesis phase, the global priority or probability of each option for each uncertainty is estimated by synthesizing the local priorities that are estimated against primitive criteria. Following the conven-tional analytic approach, we formally define decision factor and decision process respectively.
 Definition 1 (Decision factor, dependency) . Given a decision goal f with alternatives a = f a k g k , a decision factor f is an aspect that may impact decision making. A preference variable y i sociated with each decision factor f i with possible outcomes iden-tical to the alternative set a . A conditional dependency p ( y established between factors f j and f i if the aspect of the problem that f j implies can directly contribute to the decision of f
Both probabilities f p ( y i ) g i and f p ( y i j y j ) g to reflect a decision maker X  X  personal preference or prior knowl-edge of how to prioritize the options for each factor, and how to make a decision based on dependent factors. If users do not have any preferences, we may always assume that they follow uniform distributions assumption.
 Definition 2 (Decision process) . A decision process is described by a directed acyclic graph S = ( Y; E ) with the decision goal y being the only sink, the node set Y = f y i g i representing the decision factors and the edge set E representing conditional de-pendencies between factors. Given a decision process, we use F to denote the index set of all dependent factors of y i . If F then we call f i a primitive factor.

A simple decision process example is shown in Figure 2a, which consists of three random variables y 0 , y 1 , and y 2 , representing fac-tors f 0 , f 1 , and f 2 . Factor f 0 is analytically decomposed into two dependent factors f 1 and f 2 . We note that different directions or depths of analytic thinking may lead to different decision processes with distinct decision factors and/or dependencies even if the deci-sion goal is the same. For example, a primitive factor in one deci-sion process can be further decomposed into sub-factors in a more in-depth decision process. We will experiment with different levels of process of the same decision goal in Section 5 to demonstrate how decision problem solving is different from traditional single question answering.

Traditional decision analysis approaches such as analytic hier-archy process (AHP) [10] can help decision makers to outline the decision process in a similar fashion, but cannot automate the eval-uation process for each factor to make any overall conclusion. An automatic question answering system is able to overcome this prob-lem by evaluating all possible assertions or input questions associ-ated with each factor to produce an estimation, for multiple types of questions (e.g. factoid questions, yes/no questions, Jeopardy! clues[7], etc.), and evidence gathering and evidence-based answer reranking have become important research topics in QA [24]. To enrich the evidence, various question decomposition approaches have been proposed to analyze factors within the question [12, 32, 16, 13, 29, 29]. However, the additional information gained from the decomposed questions must be combined effectively to support overall decision-making, which can be improved by incorporating human intuitions and reasoning strategies from decision analysis (e.g. weights for sub-factors and rules for combining evidence). In this paper, we study how to leverage the mutual benefit of decision analysis and question answering for better overall decision support. Before we give the definition of the problem, we first define the question answering process as a probabilistic event, as follows: Definition 3 (Question answering process) . Given a factor de-scribed by a natural language question in our context, a question answering (QA) process produces assertions assigned with prob-abilities, and thus we associate the QA process for factor f a random variable x i representing the result estimated by the pro-cess and outcomes o i = f o ik g k being all possible assertions or candidate answers.

For example, in a decision problem to choose a cell phone, the decision maker may create two question answering processes:  X  X s the PHONE light X  and  X  X hat is the brand of the PHONE X . Then the outcomes for the first factor o 1 can be within f light, heavy g and the outcomes for the second factor o 2 can be the set of all phone brands. We note that a mixture of factoid questions and yes/no questions or assertions can co-exist in our setting. We now give the definition of decision making problem in the QA context . Definition 4 (Decision making problem in the QA context) . Given a decision process S representing a decision maker X  X  preferences and question answering pipelines producing probabilistic asser-tions for the decision factors in S , the decision making problem in the QA context aims to make the optimal decision for S by pri-oritizing alternatives f a k g k for the decision goal f 0 erations of decision maker X  X  preferences, assertions produced from QA processes, and augmented evidence from dependent decisions.
Unlike simple question answering, the decision making prob-lem requires an analytic decision process instead of a single natural language question as its input. A user can specify a handcrafted decision process specific to a particular decision situation, which is referred to as an ad hoc decision process , whereas instantiat-ing a generic decision process template (DPT) for specific decision problems is an alternative way to create a decision process. In con-trast to a decision process, assertions in a DPT may contain vari-ables. For example, users can create a DPT a priori if repeated decision support is required for similar decision situations, or in another case, novices may want to use DPTs created by more ex-perienced users as a starting point. Furthermore, a DPT can also group decision processes to alleviate the data sparsity problem, by d y allowing the system to learn a better evidence synthesis strategy to achieve higher prediction performance. DPTs have existed in many domains that require formalized decision-making expertise, although they are often referred to as best practices or business rules . In Section 5 and 6, we will present in detail DPTs converted from the standard principles for disease target validation and in the context of user product recommendation.
In this section, we present the proposed QUADS framework for solving the decision making problem defined in Section 3. We first model the decision process by introducing additional auxiliary ran-dom variables to represent the internal assertion synthesis process in Section 4.1, where we also establish a connection to the tradi-tional decision support problem outside of the QA context. We then derive algorithms for making decisions and learning decision strategies based on the model in Sections 4.2 and 4.3, respectively.
The QUADS framework is comprised of two major types of components: QA-based decision factor estimation components and factor synthesis components. We first model them individually, and then obtain the joint probability to represent the overall QUADS framework by multiplying these factors.

Modeling QA based decision factor estimation. We here make two assumptions on the variable x i . First, given the fact that the existing external QA pipeline is executed to answer each question individually, we assume x i and x j are independent. Second, we assume x i is conditioned on y i and its value is determined by a estimation policy We make no assumption on the outcome-alternative mapping or QA system accuracy, i.e. p i ( x i = o il j y i = a k ) can be any num-ber between 0 and 1, although a higher QA performance is always preferred. Intuitively, certain assertions often imply a deterministic tendency of prioritizing the corresponding alternatives. For exam-ple, an assertion that  X  X he item is light X  is an indicator that it tends to be preferred in the decision process, which corresponds to a higher conditional probability p i .

This assumption allows us to collectively model decision making and answer rectifying in a unified way. In Section 5, we compare the solution proposed in next section with a baseline method which does not consider estimation policy. In Figure 2b, the decision pro-cess is augmented with QA estimation variables x 0 , x 1 , and x accordingly.

Modeling decision factor synthesis. Synthesizing factor-level assertions into a comprehensive decision is non-trivial. Intuitively, not only does it need to evaluate the importance of each factor, i.e. the weight or priority in traditional decision analysis, but it is also more important to map combinations of assertions from sub-factors into assertions of their super-factor. In the  X  X hone recommenda-tion" example, the synthesizer needs to understand how to combine two assertions light and Samsung from two QA processes and how they jointly influence a super-decision in the hierarchy.
We introduce a random variable d i for each decision factor f to represent the decision synthesized from the predecessors and the assertion x i made from a local QA estimation. The outcomes from d are identical to those of x i , i.e. o i . The value of d by a synthesis policy
We make a simple assumption that it depends on only immediate predecessors, i.e. F i . Analogously, we further create a dependency on y i from each d i , which captures the intuition that, similar to x , the synthesized estimation also observes the latent variable but from a more comprehensive perspective. Formally, the synthesizer needs to estimate the probability p i ( d i j y i ; d F i
We combine the decision maker X  X  preference p ( y i j y F i based decision factor estimation components p i ( x i j y represent the overall QUADS framework as follows where Y , D , X ,  X  , and represent f y i g i , f d i g i and f i g i respectively. A strategy refers to the collection of  X  The graphical model representation of modeling decision process is shown in Figure 2c for the example used in Figures 2a and 2b. Given a general complex decision, we may use the compact plate notation to represent the nodes and dependencies equivalently as shown in Figure 2d.
Comparison with other decision analysis formulations. Sim-ilar to traditional multiple criteria decision analysis such as AHP [31], the decision factors are represented in a hierarchy. In contrast to AHP [31], an influence diagram (ID) or decision network [10], an extension of a Bayesian network of chance nodes augmented with decision nodes , utility functions specifying the preferences of the decision maker, and a precedence ordering, can represent any structured decision problem under uncertainty in a more flexible way. Limited memory influence diagrams (LIMIDs) [17, 21] relax the regularity and no forgetting assumptions of ID to allow a de-cision to be conditioned on a limited number of relevant previous observations and decisions. The representation in our work can be considered as a special case of LIMID, where rather than allow-ing chance nodes and decision nodes to be arbitrarily connected, we explicitly create two hierarchies (or DAGs) individually for an-alytic thinking ( Y ) and decision synthesis ( D ) and then establish necessary connections between the nodes in the two hierarchies. In the QUADS framework, users do not need to specify an additional utility function to represent their preference besides the priors Y .
In Section 4.2, we present how to make optimal decisions in the QUADS framework with a bottom-up dynamic programming if policies  X  and are predefined by users, similar to the Viterbi algorithm or the max-sum algorithm [25]. Otherwise, similar to LIMIDs, finding optimal decision rules can be difficult [18]. We leverage an iterative approach similar to maximizing expected util-ity (MEU) problem [14, 18, 26], which is discussed in Section 4.3.
Given policies  X  and and QA-produced assertions X , deci-sion making corresponds to finding the values of Y and D that maximize the conditional probability, or formally as follows,
Loopy belief propagation algorithms such as max-sum algorithm [25] can be utilized to maximize Y and D on a general Bayesian network with undirected loops. In our case, we make an additional assumption that each decision factor only belongs to a single super-factor, i.e. F i \ F j =  X  for any i and j . If a factor is included in the same decision process more than once, we will treat them as different factors. Although the Bayesian network still contains undirected loops with the additional assumption, we could find that optimal solution of Eq. 1 can be efficiently obtained with a bottom-up message passing approach 2 . Specifically, we start by calculating a local message vector u i containing information about the known assertion x i for each decision factor f i ,
Then we calculate messages v i and w i corresponding to y d for each primitive factor f i ,
The messages v j and w j corresponding to internal decision fac-tor f j are ready to be calculated once calculation for messages v and w F v have been done, w j ( d j ) = max y
Finally, Y and D correspond to the maximal elements in each message v j and w j , i.e. y j = arg max y j v j ( y j ) and d arg max d j w j ( d j ) . We can see from Eqs. 2 to 6 that user pref-erence, assertions made by QA processes and decisions from sub-factors are jointly considered.
If the policies  X  and are unknown to the decision maker, but past decision processes and values to the random variables are available (all Y , and D are known) or partially available (in the ex-treme case, we only know the final decision a k for the decision goal y of each decision process), then we can learn the policies by solv-ing a similar MEU problem. Many algorithms have been proposed to solve MEU problem, such as the single policy update algorithm [17], EFBP [26], or belief propagation based algorithm [18]. Here based on the efficient message passing algorithm described in Sec-tion 4.2, we utilize an iterative belief propagation approach [18].
Specifically, we start by assigning random discrete distributions (e.g. uniform distributions) to all policies in  X  and , and calculate marginalization messages if some variables are unavailable, which are similar to maximization messages (Eqs. 2 to 6), except that the max-sum calculation is replaced by sum-product calculation. Then, we find a local optimal policy for each i 2  X  and i 2 . Since utility function is degenerated, the local MEU problem becomes maximum likelihood estimation, i.e., where n ( ) is the number of times all the conditions are satisfied in the training set. We may also smooth p i and p i by adding pseudo-counts. Recall that if we have a decision process template, then we assume the policy should exhibit similar behavior across different instances of the template. With the updated  X  and , if true values of D and Y are partially unknown, they will also be updated ac-cording to Eqs. 2 to 6, which completes one iteration. The learning will stop if convergence or a certain stop condition is satisfied.
We implemented an open source software package 3 to support the QUADS solution framework described in this section, which is highly compatible with UIMA and CSE framework [37] to allow users to integrate existing UIMA based QA pipelines and decision synthesis components. In this subsection, we highlight some of the main features of the implementation.

Pluggable decision factors . A YAML 4 file is created to rep-resent a single decision factor ( f i ) including the question additionally a list of factors ( F i and p ( f i j F i ) ) by referencing all YAML files corresponding to the factors and optionally giving preference, unless primitive. We chose decision factors as basic building blocks of the decision process, which allows users to reuse previously created factors or existing decision processes to expand and create similar or more complex decision processes. Declarative descriptors for templates and decision reports . The concept of decision process template (DPT) introduced in Sec-tion 3 is also supported in the QUADS implementation. The list of variables ( vars ) is explicitly specified in each YAML file if the factor is used as part of a template. For example, in the decision factor template that is used in Section 5 to seek genes for targeting diseases, one can specify the vars as [GENE, DISEASE] a YAML-based decision report is generated by the QUADS imple-mentation, which highlights the prioritization of all alternatives and a detailed explanation based on QA pipeline outputs and reasoning process done by the decision synthesis algorithm.

Configurable QA pipelines and extendable decision synthesis interface . The two major components in the QUADS implementa-tion (QA pipelines and factor synthesis components) can be config-ured specifically for each decision factor at each level. In addition, the CSE framework allows multiple QA pipelines or factor synthe-sis components to be specified as alternatives for the same decision factor. Each configuration combination in the configuration space will be executed separately to enable the decision makers to deter-mine the optimal QA pipeline(s) and synthesis component(s) for each decision factor.
Developing a new drug is a complex and costly process that pro-ceeds from the identification of a potential therapeutic candidate to marketing a drug product, usually taking more than a decade and costing in excess of 1 billion US dollars [11, 30]. At each phase of the process, decision makers need to consider a series of predeter-mined criteria about each drug candidate to reduce risk to human subjects and to increase the chance of picking a winning therapeu-tic molecule [30]. In the earliest stage, target identification and target validation (or target assessment ) aim to select and prioritize a number of disease targets (agents with a particular biological ac-tion that are anticipated to have therapeutic utility), and estimate the  X  X ruggability X  of each target influenced by a complex balance of scientific, medical and strategic considerations, including effi-cacy, safety, commercial profits, etc. [15, 11].

In this experiment, we focus on leveraging QUADS to support the target validation problem. The goal is not only to identify tar-gets but also to emphasize a deeper understanding of targets in the full disease context. We implemented a target validation DPT to jointly consider multiple scientific and medical criteria, and applied it to support target validation on a dataset of 5.6K human diseases and 6.2K genes. We first describe how the experiment was set up, and then present the experimental results and analysis, which are further followed by a specific case study on breast cancer.
In this section, we describe how we implemented the target val-idation DPT, integrated the biomedical QA system, and how we collected the dataset and prepared the baseline methods.
Dataset preparation . We created a Lucene 5 index from a cor-pus containing more than 22 million PubMED 6 abstracts, with only minimal preprocessing for each document (tokenizing and down-casing the text, and removing common English stopwords). The index was then integrated into the QA system to support evidence gathering. Several databases have been developed to store associ-ations between genes and diseases, each of which focuses on dif-ferent relational aspects. We used the DisGeNET 2.0 dataset [1], which contains 6,029 diseases and 9,313 genes integrated from a number of gene-disease association databases, including OMIM UniProt 8 , PharmGKB 9 , and CTD 10 .

The DisGeNET dataset was used for QUADS model training and evaluation, and we therefore focus on the manually-created items and exclude the LHGDN subset, which contains gene-disease rela-tions automatically extracted by text mining algorithms. We split the manually created dataset into 5,605 subsets, each of which cor-responds to one of the remaining diseases, and 6,158 genes were used as candidates for target validation. We ran 10-fold cross-validation over the dataset, where nine folds were used to learn the policies  X  and in the model and one fold was used to test the model. At most 100 iterations were executed for each training run when optimizing the policies.

Target validation DPT . Following the DPT guidelines, we cre-ated a simple target validation DPT to test our hypothesis, based on literature review as well as personal knowledge and experi-ence from a group of professional biochemists and bioinformati-cians working at a pharmaceutical company. We first defined and DISEASE as the variables in the target validation DPT could then be instantiated with actual GENE and DISEASE names from the relations in the DisGeNET dataset. We expanded the tem-plate containing questions in the form of Jeopardy! clues in a top-down fashion. Specifically, we started with defining the decision goal, which summarizes the target identification task from a high level:  X  X his GENE is directly involved in DISEASE and can be a suitable target. X  As one can easily see that necessary detailed cri-teria for a  X  X ruggable X  target cannot be thoroughly depicted in the decision goal, the template was then expanded to a second level, which contains six important decision factors such as gene expres-sion, gene mutation, pathway, clinical trials, etc., that was further decomposed as necessary.

For this experiment, we used a target validation DPT with a total of 16 factors, which is shown in Figure 3. We can see from Figure 3 that some strong indicators can hardly be satisfied by a single gene, whereas some other weak factors only contribute to the decision if some gene satisfies all the criteria. We also pay special attention to the criteria 1.6 and 1.6.1, which intuitively are negative indicators. We did not specify any preferences to the decision processes as in-put, i.e. a uniform prior was used by default, and we see in Section 5.2 that the QUADS framework could automatically learn decision policies taking all these considerations into account.

Supporting evidence based QA pipeline . A supporting evi-dence based QA system such as Watson [7] commonly incorpo-rates parsing, interpretation, retrieval, answer extraction, evidenc-ing and ranking phases. We integrated a few simple biomedical QA components 12 , which were designed to answer questions such as those in TREC Genomics tasks [8] or CLEF QA4MRE tasks [27]. We list the key components of the QA pipeline in Table 1. After a question analysis phase which introduces NLP annotations, different queries are constructed from all or part of the extracted focus terms, and other nouns, verbs, and named entities (with syn-onyms) are also combined via an OR operator. A Lucene index was used to retrieve candidate passages based on various queries, and document frequencies for these queries were then combined with weights learned using logistic regression. Table 1: Summary of integrated components for QA pipeline Category Components Parsing Interpretation Retrieval Lucene search engine
Evidencing and ranking
Since the types of questions in the DPT are all yes/no questions, either y or n will be assigned to label each assertion at the end of the pipeline. Compared with arbitrary nominal assertions, yes/no assertions can generalize the learned decision model 13 . We can see that the chosen QA pipeline lacks the algorithmic sophistication (i.e. inference capability) required to automatically interpret and match the term  X  X uitable target X  by filling in necessary information related to the specific factors.

Evaluation methods. Although the  X  X ruggability X  of genes is sometimes subjective and difficult to measure until later phases are conducted in the drug development process, this experiment, as the first attempt to support target validation, demonstrates how to rediscover known targets for the diseases purely and automat-ically from the literature by considering various decision factors involved in the decision process. In particular, for each subset cor-responding to a particular disease, QUADS evaluates each gene in the list and finally decides an ordered list of genes prioritized by the global confidence p ( d 0 = y ) for each candidate (or equiva-lently w 0 ( d 0 = y ) w 0 ( d 0 = n ) ), which were compared against the unordered known target list. We then focus on measuring pre-cision, recall, and F-1 averaged over all subsets when the order is ignored, similar to list question answering evaluation [5], as well as mean average precision (MAP) for retrieval tasks when the or-der is considered. Detailed performance evaluation at each factor level is unavailable for the entire dataset, which is actually one of the disadvantages of existing databases that only give an incompre-hensive view of the problem. In Section 5.3, we manually labeled a number of genes from the top of the returned list at the factor level to support deeper analysis.

Baseline methods . To motivate the QUADS framework, we compare it with several baseline methods. We first simplified the target validation DPT by leaving only the first level and the first two levels (denoted by 1 Level and 2 Levels ) to demonstrate how additional decision factors can impact the decision results, and in addition, we flattened the template hierarchy but still kept all the factors to construct an unstructured decision process (denoted by Flattened ) similar to a scenario question [3], which is used to mo-tivate the importance of analytic thinking (decision synthesis) in decision making. We also compare with two different decision syn-thesis methods. One is an unsupervised method that simply syn-thesizes the decision by majority voting from all factors, which is denoted by Voting . The other baseline method simplifies the pro-posed QUADS solution by removing the extra latent variable layer Y and thus , i.e. policies  X  directly map d F i and x i to d noted by Simplified ). The learning process is then degenerated to a linear programming problem and no inference is required. In addition, we also simplified the QA pipeline by removing the syn-onym expansion components (denoted by No Synonym ), which have been shown to play a crucial role in biomedical information retrieval tasks in previous research [8]. We present the performance results in terms of Precision, Recall, F-1, and MAP averaged across all 10 test folds in Table 2, with the significance test (t-test) results of each baseline method com-pared against the proposed QUADS framework within each run; scores are labeled with different significance levels ( y and z for p &lt; 0 : 005 , and no difference was observed in signifi-cance levels across runs). We can see that the proposed QUADS solution framework significantly outperformed all baseline meth-ods in all the metrics. In general, MAP scores are higher than pre-cision scores, which indicates the most extensively studied genes contained in the DisGeNET dataset can be successfully retrieved and prioritized at the top of the list, however some other genes that QUADS and other baseline methods also consider likely to be tar-get candidates are missing in the DisGeNET.

Compared with QUADS, 1 Level and 2 Levels achieved worse recall and could not properly prioritize the candidates, due to miss-ing important factors that relate to the target validation goal. The only information that was utilized in prioritization in 1 Level is the QA evidence score, which was unfortunately dominated by Table 2: Experimental result for target validation decision support f 1 1.1 1.1.1 1.1.2 1.1.3 1.2 1.2.1 1.2.2 p .4743 .5005 .6577 .6791 .4699 .8864 .8895 .8131 f 1.2.3 1.3 1.3.1 1.3.2 1.4 1.5 1.6 1.6.1 p .5442 .8840 .8873 .9044 .9107 .5497 .0118 .0088 f 1 1.1 1.1.1 1.1.2 1.1.3 1.2 1.2.1 1.2.2 p .9736 .9999 .9998 .9996 .9998 .9987 .9986 .9994 f 1.2.3 1.3 1.3.1 1.3.2 1.4 1.5 1.6 1.6.1 p .9999 .9988 .9989 .9987 .9984 .9998 .1767 .2875 the popularity or history of research for each gene, since the QA pipeline couldn X  X  capture any information about the original deci-sion goal beyond the two named entities GENE and DISEASE . Flattened could achieve a higher precision and recall than 1 Level and 2 Levels , but the MAP is unsatisfactory compared with QUADS. Since most biomedical QA systems are not tuned to an-swer scenario questions that involve complex discourse structures, this isn X  X  surprising. Therefore, even when all necessary pieces of information are retrieved by the QA pipeline, without QUADS, a single QA pipeline can hardly assign a correct weight to each fac-tor, nor can it properly merge evidence like any decision policy which can explicitly represent various combination strategies in-cluding AND , OR , NOT , etc.

Voting performed very similarly to Flattened , due to the simi-lar setting that factors were undistinguished when the decision was made at d 0 . The only difference is that in Voting each factor is associated with a weight, which is determined by its position in the hierarchy. Specifically, the deeper it is in the hierarchy or the more sibling factors it has, the lower it is weighted.

Among the baseline methods, the performance of Simplified was closest to that of QUADS in all metrics, which first motivates the importance of decision processes when complex information needs are required, and second, suggests that modeling the generation of QA assertions as a probabilistic event helps to make QUADS more sensitive to the different levels of uncertainty inside each QA pro-cess. In fact, if we look at the final trained model, we could see in Table 3 that most can learn a stochastic matrix being nearly an identity matrix, indicating a relatively trustworthy QA process with exceptions regarding factors 1.6 and 1.6.1.

Finally, we see that No Synonym unsurprisingly did the worst job in identifying the correct targets, which again suggests that syn-onym expansion is necessary for effective biomedical information retrieval in this context.

To further satisfy our curiosity, we sorted the input combinations of the decision policies  X  in descending order of the probability of making a y decision, i.e. p ( d = y j y; d F ) , which is partially shown in Table 4. We observe that among the top input combinations for all decision policies, the final decision f 1 is obviously more diffi-cult than others, and the local QA estimation y 1 can positively im-f Rk y , d F p Rk y , d F p 1 1 y 1 = y , d 1.* = y .9710 2 y 1 = n , d 1.* = y .5015 Figure 4: Evidence collected for all factors (y-axis) in terms of averaged document frequency for the subsets corresponding to true positive (a), false positive (b) and true negative. Queries of different complexity (x-axis) in terms of number of query terms were issued. pact the final decision, whereas most other decision policies tend to ignore the local estimation, e.g. y 1.1 and y 1.3 . We used breast cancer as a case study to demonstrate how QUADS can help understand different ways of finding support for disease targets, focusing on the factor estimation part. We discov-ered that the system can achieve a recall of 100% for this particular disease, and then we selected and manually checked the correctness of nine genes that were ranked at top of the list to understand the loss of precision, which are AKT1 , BRCA1 , BRCA2 , BRIP1 FGFR1 , KDR1 , mTOR , and NBN . We found that all the nine genes are known to be good targets, and three genes among them ( FGFR1 KDR1 , mTOR ) were not included in the DisGeNET dataset.
Evidence scores collected from QA processes that relate to the factors for the genes are compared in a heat map in Figure 4, and are further grouped by (a) true positive, (b) false positive, and (c) true negative. The x-axis corresponds to the number of additional OR -combined query terms to represent queries of different com-plexity, and the value in the heatmap corresponds to the document frequency; these were used as features for learning the assertion generation. The Y-axis corresponds to the factors in Figure 3.
First, we can clearly see that more evidence was gathered for the true positive subset than the false positive and true negative sub-sets. Second,  X  X nswerability X  of questions differs across factors. Some questions with narrow but clear concept, e.g.  X  X uman in vivo experiments X ,  X  X xpression alteration X ,  X  X n disease tissues or cells X  can be answered properly for all genes, were hence trusted by cor-responding policies. A difficult-to-answer question can lead to low evidence, e.g. factors 1.1.3 and 1.2.3, 1.5, which correspond to rel-atively lower weights in Table 3. In order to maximize the benefit from these two observations, we may also consider the real-valued confidence score, in addition to the yes/no answer returned from the QA pipelines, in order to improve the decision synthesis process.
In this section, we consider a common decision process faced by on-line consumers: product recommendation from review texts . Unlike the target validation problem, algorithmic methods for the product recommendation problem (as well as the related rating problem) have been extensively investigated [22]. In this section, rather than propose any new algorithm for this relatively well-studied problem, we adapt the QUADS framework (built origi-nally for target validation) to product recommendation with min-imal change, in order to test its flexibility and generalizability.
We first describe the experimental settings and focus on what changes have been done to enable the adaptation of QUADS to the problem of product recommendation from review texts.
 Accessories subset of an Amazon product review corpus [22] which contains 78,931 reviews for 7,438 distinct products. Similar to the preparation procedure for PubMED abstract corpus, we here focused on the review/text field in each review and tagged with productId and ignored other fields such as userId or time though they provide supplementary information for the recommen-dation task, incorporating structured data with a different schema often requires extra effort to adapt existing QA pipelines).
Since this task is more subjective than target validation, we can hardly rely on a manually-created gold-standard product recom-mendation list. Thus we use the average rating of each product as its overall recommendation weight, which is then converted from a 5-point rating scale to a binary recommendation. Specifically, ratings equal to or smaller than 3 are assigned no , otherwise and then we obtained a list of 4,583  X  X ecommended X  products. We conducted a 10-fold cross validation experiment on 7,439 items. Various product reviews give insights about the same product from different perspectives, in the same way that different publications in PubMED discuss different perspectives of the same gene/disease.
DPT for buying a cell phone. Unlike the target validation task, where a few second-level aspects can cover most considerations for a majority of diseases, different products have different decision factors, which motivated us to focus on a particular product type (cell phones). We allowed QA pipelines to return nominal answer texts (e.g. Apple , Sumsung , Nokia , Google , etc. from a finite set of brands ). Compared with the number of cell phone products and reviews available in the data set, this restriction did not cause any issues with data sparsity.

Most of the adaptation effort was spent in creating a DPT for cell phone recommendation. First, we defined the decision goal as  X  X uying this PHONE is recommended X  where PHONE is the vari-able in the template. Then, we considered the most important high-level features, including design and usability functionality , carrier , and operating system ated natural language questions corresponding to each feature, e.g.  X  X his PHONE has good usability X , or  X  X his PHONE is made by Apple X , etc. For broad questions such as the one asking about usability , we further decomposed the question down to two sub-factors:  X  X his PHONE is light X  and  X  X his PHONE has a good inter-face design X . Finally, we created a DPT with 17 decision factors
Although many of the questions about each carrier or brand can preferably be answered by searching structured product informa-tion databases or parsing a semi-structured product specification page instead of raw review texts, in this experiment we focused only on unstructured texts (which could be searched after much simpler domain adaptation).

QA pipeline. We applied almost the exact same set of QA com-ponents for this task. However, some changes were required. First, we replaced the POS tagging model and NER model to models trained on standard English news corpora. Second, we replaced the biomedical synonym expansion component with a sentiment word dictionary 16 . We also integrated the index for the reviews to replace the biomedical corpus. The rest of the pipeline remains the same.
The evaluation methods and baseline methods are also identical to those used for the target validation task, except that the Unsuper-vised condition is unavailable (without careful tuning, it is difficult to combine different nominal assertions from different factors).
In Table 5, we list the evaluation results for the phone recom-mendation problem. The scores are generally lower than those in the target validation evaluation, which surprisingly indicates that an everyday decision problem that seems easy is actually harder than a scientific decision based on technical publications. Never-theless, the result still supports the original motivation for QUADS and illustrates the relative effectiveness of the proposed QUADS framework compared to baseline methods.

In fact, the two decision problems face different types of infor-mation overload. The biggest challenge in the target validation problem is to navigate through millions of publications to select the most relevant ones. It is always the case that, among the thousands of candidate genes, only a few of them have been studied exten-sively and documented in relevant papers; once the relevant docu-ments are identified, the evidence is relatively easy to estimate. In product recommendation from review texts problem, two new chal-lenges should be solved: informal texts and mixed reviews. There are many texts about each product, but they use widely varying language (degrees of formality/informality) and often mix positive / negative evidence within the same review.
In this paper, we first introduced a novel decision representa-tion which allows decision makers to define a decision-making best practice with DPT , or to structure an ad hoc decision process by formulating and organizing natural language questions or asser-tions in an analytic hierarchy. We then introduced a new decision support framework, QUADS, which takes advantage of question answering technologies to automatically understand the decision situation and make a decision in real time, using Bayesian learning and inference processes. An open source framework implemen-tation was used in two real-world applications: target validation , one of the most fundamental tasks for the pharmaceutical industry, and product recommendation from reviews , an important decision process for on-line consumers. We implemented and compared a number of baseline methods that partially accomplish some of the QUADS functions, and show that the QUADS framework outper-formed these baseline methods for both datasets.

QUADS demonstrates a new way to provide structured decision support from unstructured text by incorporating QA technology, but many important (and interesting) research questions remain to be addressed. First, the QA pipelines used in the experiments are far from perfect, and can be improved by adding evidence from structured knowledge bases as another decision sub-factor in addi-tion to unstructured text. On the other hand, since the estimation policy reveals the trustworthiness or comformity of each factor, we can also identify the hardest questions for the QA components. A joint optimization of question answering and decision making may be a good way to approach this combined problem in new domains. Another question is how to discover the hidden factors that influ-ence a decision, in order to enable automatic decision process con-struction. Beyond existing facet extraction methods, one can also integrate websites that have already collected and organized  X  X ow-to X  manuals and allow community content development, such as wikiHow. Moreover, the current framework requires a static deci-sion process specified at the beginning of the system X  X  execution, which can be effective for a decision problem with tens of factors and thousands of alternatives like the two applications we investi-gated; nevertheless, there are limits to how far one can scale the manual creation of decision process descriptions. An interesting idea for future research is to allow the decision process to emerge dynamically, with selective pruning of alternatives based on the partial evidence that has been aggregated thus far.
 Acknowledgement. The authors would like to thank the Roche BioQA project team for their helpful discussion and suggestions.
