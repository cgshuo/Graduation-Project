 Graphs or networks can be used to model complex systems. Detecting community structures from large network data is a classic and challenging task. In this paper, we propose a novel community detection algorithm, which utilizes a dy-namic process by contradicting the network topology and the topology-based propinquity, where the propinquity is a measure of the probability for a pair of nodes involved in a coherent community structure. Through several rounds of mutual reinforcement between topology and propinquity, the community structures are expected to naturally emerge. The overlapping vertices shared between communities can also be easily identified by an additional simple postpro-cessing. To achieve better efficiency, the propinquity is in-crementally calculated. We implement the algorithm on a vertex-oriented bulk synchronous parallel(BSP) model so that the mining load can be distributed on thousands of machines. We obtained interesting experimental results on several real network data.
 H.2.8 [ DATABASE MANAGEMENT ]: Database Ap-plications X  Data mining Algorithms, Performance
This work was supported in part by National Natural Sci-ence Foundati on of China under grant No . 60833003 and 60873171, 973 Program under Grant No . 2006CB303103, a research award from Google, Inc., Basic Research Founda-tion of Tsinghua National Laboratory for Information Sci-ence and Technology (abbr. TNList), and the Program of State Education Ministry of China for New Century Excel-lent Talents in Universit y under Grant No . NCET-07-0491.
Graphs(or Networks, preferred by physicists) can be used to model a variety of complex systems, such as social com-munication network, biological interaction network, paper citation and coauthor network, and web linkage graph. A distinguishing property of real-life graphs other than ran-dom graphs is the ubiquitousness of community structures, which are intuitively highly intra connected subgraphs with relatively sparse connections to the leaving parts. The in-terpretation of community structures varies with the appli-cation domain. For example, community structures in so-cial network may imply a group of people who share com-mon interests or simply live near, depending on the context. Web pages included within a community structure discov-ered from web linkage graph deal with related topics. Com-munity structures in biological network help identify func-tional module[14]. Emerging Internet services have brought abundant traditionally unattainable graph data online. For instance, the social network service(SNS, e.g. Orkut, MyS-pace, and Facebook) saves the sociologist from questionnaire survey, and generates large online social network. The hy-perlinks between Wikipedia pages form an encyclopedia net-work with millions of articles as vertices. Detecting and in-terpreting community structur es from various newly avail-able large online graph data is of great significance and chal-lenge.

Although the community detection problem can be intu-itively and simply described as discovering coherent parts from network, there is no widely accepted formalized defini-tion. The most strict community structure definition is by clique, which means the fully connected subgraph. Certainly the clique is too strict to be realistic. There are several relaxed clique definitions, like quasi-clique, in which each vertex is connected with at least a minimum proportion of other vertices[20]. Some other community definitions require that the inner community edges exceed the inter commu-nity edges[17]. Regardless of the community overlapping, in its simplest form the community detection means divid-ing the graph into disjoint sets. Therefore some researchers do not directly define what is a community structure but define a quality function to quantitatively evaluate a graph division scheme. The community detection problem is then transformed to optimizing the quality function with proper graph division[11, 9, 5, 12]. The mostly referred quality function is the modularity [13]. Other algorithms even have no explicit definition or objective function, but can natu-rally generate community division scheme through reason-able heuristics[18, 1]. The algorithm proposed in this paper belongs to this class.

Other aspects deserving attention is community overlap and hierarchy. In some real networks, it is not easy to clearly divide the graph into disjoint sets. There are swing ver-tices lying at the boundary between communities, causing the difficulty to decide which community they belong to. A considerate algorithm should be able to identify the commu-nity overlaps[14]. Our algorithm can meet this requirement through simple postprocessing. Another natural phenomena related to the community structure is their hierarchical orga-nization, which means a community obtained under coarse granularity can be further divided into more compact ones.
There have been many interesting community detection algorithms, why do we propose another? The primary mo-tivation of this work is we need a more efficient algorithm to discover community information from web scale graph data. Few existing algorithms can finish within O ( | V | 2 )onsparse graph, which is unacceptable on very large and even dense graph. The complexity of our algorithm on sparse graph is O ( k  X | V | ), where k is the iteration count. Another differ-ence from the algorithms proposed mainly by physicists is that we emphasize on the scalability aspects without loss of community quality. The basic idea of our algorithm is that, the community structure can naturally emerge by a self-organizing dynamic process, which is easily comprehended. Incremental techniques are adopted for further efficiency im-provement. We implement the algorithm on special vertex-oriented parallel model, which helps distribute the compu-tation to thousands of machines. The experimental results indicate that our algorithm is competitive with respect to both community quality and mining efficiency.
A historical problem related to community detection is graph partitioning[10], the problem with which is the num-ber of communities must be specified before hand. To avoid trivial communities the approximate size of the expected community also has to be given as input parameter. Both are supposed to be parts of running results for a successful community detection algorithm. There is a class of algo-rithms by iteratively removing the edges that lie between communities[6]. The key is defining the edge centrality, which indicates the possibility for an edge lying between communities. Several centrality definitions were proposed: edge betweenness[8], current-flow betweenness[8], random-walk betweenness[8], information centrality[7], edge cluster-ing coefficient[17]. The algorithm in [17] is the more efficient oneinthethreewithtimecomplexityof O ( | E | 4 / | V | 2 ). Since Newman et al proposed the null model based modularity[13], there emerged a class of community detection algorithms with the target of maximizing modularity. The modularity optimization techniques proposed by the literature include greedy algorithm[11], simulated annealing[9], extremal opti-mization[5], spectral optimization[12]. [3] improves the ef-ficiency of the greedy optimization in [11] by achieving the time complexity of O ( | V | log 2 | V | ), which is very efficient. But the problem with the greedy modularity optimization is its ineffectiveness on finding accurate community. The ex-tremal and spectral optimization algorithm can both scale to O ( | V | 2 log | V | ) on sparse graphs. Another class of com-munity detection algorithms exploit the spectral property of the Laplacian matrix[4] or normal matrix[2]. [18] maps the graph onto a q-Potts model with nearest-neighbors interac-tion. In [16], the authors utilized the heuristic that random walk mostly happens within the community. [1] takes use of the synchronization dynamics to reveal the hierarchical community structure.
The necessity of a community detection algorithm is be-cause of the belief that there are abundant community struc-tures hidden in real networks. Now that the community structures are right there, why do we work so hard to mine them? Why not just let the communities claim themselves? In the real social network, the implicit interpersonal com-munities are progressively and spontaneously formed by the collaborative local decision of each individual. If this sponta-neous process continues with more aggressive local criterion, the community structures are expected to emerge as natu-ral results. Based on this intuition, in this section we first describe the dynamic process which we term as propinquity dynamics and its incremental form. Then we give one con-crete definition of propinquity and show how to calculate it efficiently.
In some clustering algorithms, it is a elementary compo-nent to define the similarity or distance between a pair of objects. With respect to the graph or network, we need a quantity to evaluate the probability that a pair of ver-tices are involved in a coherent community. We name this quantity as propinquity , which is a term borrowed from soci-ologist and refers to the physical or psychological proximity between people. Certainly there are a number of alterna-tive heuristics to concretely define the propinquity. Here we simply assume we had one, denoted by P G ( v 1 ,v 2 ), in which subscript G will be dropped if there is no ambiguity. Note that, the propinquity must be calculated purely from net-work topology and be sensitive to the topology changes.
For a graph containing disjoint densely connected com-ponents, one can easily identify the community structures. However for real networks, the topology is not that clear. An intuitive way for identifying the community structures may be by purposely increasing the graph contrast. In order to maximize the community contrast, we can utilize the contra-diction between propinquity and topology. Specifically, the graph topology should be updated to keep consistent with the propinquity calculated from itself.
 For conceptual clarity, this paper focuses on undirected non-weighted graph without multi edge and self loop. The graph is represented by G ( V, E ), where V is the vertex set and E is edge set. From local perspective, updating topology accord-ing to propinquity means cutting existing edges or inserting new edges. The updating criteria is shown in Equation 1, where  X  is the minimum propinquity that can support the survival of an edge and is called the cutting threshold .  X  is the emerging threshold that controls whether a new edge should be inserted in order to guarantee the consistency be-tween local topology and propinquity.

From the global perspective, given a graph topology a set of propinquity values can be calculated. Let P ( T )denotethe overall propinquity set of topology T . According to the local topology updating criteria, we use C  X , X  ( T,P )todenotethe overall process that contrasting topology T and propinquity P . It returns a new topology, say P ,whichshouldbemore consistent with propinquity than the previous one. Then the propinquity of the new topology can be further calculated by P ( T ). This process goes on iteratively until we have a satisfactory consistency between the topology and propin-quity. The final topology should be a good material for the next steps of community detection. Let T 0  X  G , then the process can be represented by Equation 2. We term the it-erative process in Equation 2 as propinquity dynamics .The termination condition of the propinquity dynamic process should be | T n +1  X  T n | &lt; ,where is a nonnegative integer. While the propinquity dynamics goes on, the topology dif-ference,  X  T , between successive iterations will become rel-atively small compared with the whole graph. Similar case happens on the propinquity. In order to avoid the recalcu-lation of all the propinquity values in each iteration, we can incrementally update the existing propinquity calculated in the previous iteration by comparing the current topology with the previous one. Then the propinquity dynamics in Equation 2 can be rewritten in its incremental form: Different from the non-incremental one, at the beginning of an incremental iteration besides the current topology T n have the propinquity P n corresponding to it. Their initial values come from T 0  X  G and P 0  X  P ( T 0 ). By contra-dicting them( C I  X , X  ( T n ,P n )), the updated part of the new topology( X  T n +1 ) can be generated. Then the propinquity update( X  P n +1 ) can be calculated purely from old topology( T and the incremental part( X  T n +1 ). This is the most compu-tationally significant step therefore we will elaborate it in a separate section. The last two steps are easily compre-hensive without explanation. Note that, we must assume  X  P  X   X  T , which is the prerequisite of the efficiency of incremental propinquity update over the initial one, and de-pends on the concrete propinquity definition. Equation 3 is just a high level description. We will elaborate on the incre-mental propinquity update in Section 4, before that, we will first give a concrete propinquity definition in Section 3.3.
In both version of propinquity dynamics we assumed that the topology can converge, which means lim n  X  X  X  T n =  X  . We have not yet proved the convergency of the dynamic propinquity and the necessary condition, however we did observe the convergency in all the real graph data we use.
Through the edge redistribution of the propinquity dy-namics, the resulting topology is supposed to be clear with Figure 1: Coherent neighborhood propinquity and its calcu-lation. high density contrast. Then extracting community struc-tures from it can be trivially done by discovering the con-nected components. Before that we have a chance to iden-tify the community overlaps. Besides the topology, we have its corresponding propinquity as a byproduct, which can be reutilized to carry out a micro clustering on each vertex X  X  neighbor vertices. The connected components extraction can still be done by breath-first search. The difference is that, the search cannot pass through the boundary between different micro clusters. Overly split micro clusters which are supposed to be in the same community will probably still be connected by micro clusters at other vertices. The robustness is based on the stable structure obtained from propinquity dynamics.
Now that we have the propinquity dynamics framework, it is time to specifically define the propinquity. Under the framework, the propinquity definition must possess several properties. First, the propinquity should reflect the prob-ability that a pair of vertices are involved in a same com-munity structure. Second, it can be efficiently calculated purely from topology. In addition, the definition should fa-cilitate the incremental update. As mentioned in Section 1, the quasi-clique guarantees both density and balance of edge distribution, therefore can act as a hint for defining the community structure. In [20], a gamma -quasi-clique(where  X   X  0.5) is said to be coherent . In [15], the author proved that the diameter of a coherent graph is no greater than 2. If we assume the resulting community structures are coherent, the topological distance for each pair of vertices in a community should be within 2. It suggests that the propinquity defi-nition should consider only the local neighborhood within two hops. The propinquity between a pair of vertices with distance great than 2 should be zero. Then we can use the number of common neighbor vertices as part of the propin-quity evaluation. Comparing back with the coherent graph, sharing large number of common neighbor does not guaran-tee the balance of the local neighborhood as a community. Therefore, the overall connectivity of the local neighborhood must be taken into consideration for propinquity evaluation. Let N G ( v ) represent the neighbor vertex set of vertex v in G . The subscript G will be omitted if there is no danger of confusion. Let G [ S ] represent the induced subgraph of vertex set S  X  V ( G )withrespectto G . Then the propin-quity between v 1 ,v 2  X  V ( G ) can be defined by Equation 4. The first sum term is trivially the number of direct connect-ing edges. The second term is the common neighbor vertices count. The last part is the number of edges connecting com-mon neighboring vertices of v 1 and v 2 . Wenametheedge set in the third part the conjugate between v 1 and v 2 . This propinquity definition reflects the connectivity of the maximum coherent subgraph involving a vertex pair to a certain degree, therefore we term it as coherent neighborhood propinquity . Figure 1(A) shows an example of the coherent neighborhood between v 1 and v 2 . The four shaded circles are common neighbors contributing 4 to the coherent neigh-borhood propinquity. The three edges in red are conjugates which contribute 3. Added up with the direct connection, the total propinquity between v 1 and v 2 is 8. The local-ity of the coherent neighborhood propinquity restricts the propinquity calculation and updates in a controllable scope.
A straightforward way to calculate all the neighborhood propinquity value in a graph is by the literal definition. That is, for each pair of vertices, intersecting their neigh-bor vertex set to get the common neighboring set, then counting the edges connecting the common neighbor ver-tices. The complexity of this na  X   X ve calculation is approxi-mately O (( | V | + | E | )  X | E | ), which is unacceptable for a large graph with millions of edges(e.g., Wikipedia linkage graph). Note that in the complexity analysis, we use | E | / | V | proximate each vertex degree. However in a real network the distribution of the vertex degree is highly skewed with a long tail in most of the cases, which makes the na  X   X ve computation even more inefficient. So we need a better one. In Figure 1(B), v a is a common neighbor of v 1 and v 2 , therefore can be gotten by set intersection according to propinquity defini-tion. From the view point of v a , it contributes a propinquity unit to v 1 and v 2 . Actually, v a contributes propinquity unit to each pair of its neighbor vertices, if | N ( v a ) | X  2. As in Figure 1(C), v a contributes propinquity to three pair of ver-tices. This type of propinquity components are produced in-tuitively by an angle in graph, therefore we name it as angle propinquity . The angle propinquity calculation can be for-mulated as:  X  v  X  V ( G ) ,  X  v i ,v j  X  N ( v )( i = j ) ,P ( v P ( v i ,v j ) + 1. In Figure 1(D), ( v a ,v b ) is a conjugate of v and v 2 , therefore can be directly calculated according the propinquity definition. From the view point of conjugate edge ( v a ,v b ), it contributes propinquity unit to v 1 Actually, it contributes to each pair of common neighbors of v a and v b , e.g., the three pairs formed by v 1 , v 2 and v the Figure 1(E). This type of propinquity components come from the conjugate edges, therefore we name it as conjugate propinquity . The conjugate propinquity calculation can be formulated as:  X  ( v s ,v d )  X  E ( G ) ,  X  v i ,v j  X  N ( v j ) ,P ( v i ,v j )  X  P ( v i ,v j ) + 1. The advantage of the later al-gorithm is that the computation can be done in a manner of for-each-vertex and for-each-edge, while the former na  X   X ve algorithm is in a for-each-vertex-pair manner. The overall complexity is therefore reduced to O (( | V | + | E | )  X | For sparse networks, the complexity is simply O ( | V | ).
In the previous section, we described the incremental propin-quity dynamics framework from a global view. Now that we also have the concrete coherent neighborhood propin-quity definition and its efficient calculation algorithm, in this section we will focus on how to incrementally update the propinquity in order to reflect the graph topology up-dates.

The topology update involves many edge deleting and in-serting. Let us first consider the simplest case of single edge update. If a single edge is deleted from a graph, most of the propinquity values will remain unchanged, and we only need to update limited number of propinquity around the neigh-borhood. Assume the edge to be deleted is ( v 1 ,v 2 ). The angle propinquity involving ( v 1 ,v 2 ) as an angle edge should be reduced. That is, for each v  X  N ( v 1 )  X  X  v 2 } should be reduced by one propinquity unit. Symmetrically, for each v  X  N ( v 2 )  X  X  v 1 } , P ( v 1 ,v ) should also be reduced by one unit. As for the conjugate propinquity update brought by single deleted edge, it is just an inverse process of its initial calculation. The single edge inserting case can be easily derived in a similar way. While all the deleting and inserting are taken into consideration as a whole, it is a little more complex than the single edge case to conduct efficient propinquity update.
 Let N n ( v ) be the neighboring vertex set of v in topology T . The local neighboring topology update of v in the n -th iteration can be represented as Equation 5a, where N I n ( v ) represents the set of vertices with which connection will be established, and N D n ( v ) is the vertices from which vertex v will disconnect.
 Let N R n ( v ) be the set of neighbors remaining unchanged after the ( n +1)-th iteration, as shown in Equation 5b. We will use n ( v ), N I n ( v )and N D n ( v ) at each local vertex to integrally express N n ( v )and X  N n +1 ( v ). All the following formulations will be mapped to operations on the three set. The subscript n or target vertex v will be dropped for simplicity if the context allows.

In order to facilitate the description, we need to introduce several convenient symbolic shortcuts. Given two disjoint vertex sets, S 1 ,S 2 ( S 1 S 2 =  X  ), we use S 1 + S 2 to represent the operation that, for each v i  X  S 1 and each v j  X  S 2 crease P ( v i ,v j ) by a unit propinquity. Accordingly we define the binary operator  X  as the pairwise unit propinquity de-creasing on the cartesian product of two operand vertex sets. In the case of self-join, two unary operators, (  X  ) + and ( will be used. ( S ) + means, for each v i ,v j  X  S ( v i = v a unit propinquity to P ( v 1 ,v 2 ). The ( S )  X  can be easily comprehended. Recall that the original angle propinquity can be calculated by N ( v ) + for each v  X  V ( G ). Let N n +1 and N n denote the neighboring vertices of the target vertex in two consec-utive iterations. Then the incremental angle propinquity update process can be deduced from Equation 6. Note that, each operand in Equation 6 represents a set of unit propin-quity updates. Equation 6a can be deduced from Equa-tion 5. Given two disjoint sets, S 1 and S 2 , according to the definition of cartesian product, we have ( S 1 + S 2 ) + = 1 + S + 2 + S 1 + S 2 . SowehaveEquation6b. Twoneg-atives make a positive, that is,  X  S  X  = S + and  X  ( S 1  X  S )= S 1 + S 2 , so we can have Equation 6d.
 Incremental conjugate propinquity update seems rather sim-ple. If a graph edge, say ( v 1 ,v 2 ), is cut in a topology up-date, we only need to erase the conjugate propinquity it contributed in the previous ite ration. The erasing operation can be done with Equation 7. In the case of edge emerg-ing, a complemental formulation can be easily derived as in Equation 8.
 So far, it seems that we have covered all the propinquity update cases, including angle and conjugate propinquity, edge inserting and deleting. However there is another case that is easily neglected. An edge ( v 1 ,v 2 )mayremainafter the topology update, but the conjugate propinquity it con-tributed in the previous iteration may partly change because of the coherent neighborhood update involving v 1 and v 2 We can analyze this case by analogy of the incremental an-gle propinquity update. Similar to N R n ( v ), N C,R n ( v be defined as the set of common neighbor vertices of v 1 and v that remain unchanged from the n -th algorithm iteration to the ( n + 1)-th iteration. N C,I n ( v 1 ,v 2 )and N C,D are also defined analogically from N I n ( v )and N D n ( v )respec-tively. This part of conjugate propinquity update can be calculate by Equation 9, which is derived in the same way as in Equation 6.
 The current problem with R-conjugate update is how to cal-culate the three parts of common neighbor vertices, that is rive N C,R n ( v 1 ,v 2 ) by Equation 10. Equation 10a and 10b are derived by corresponding definitions. Equation 10c is derived by Equation 5. From the disjointedness between N R ( v ), N I ( v )and N D ( v ), 10c can be simplified to 10d by polynomial reduction.
 With the help of Equation 10, the newly emerging part of the common neighbor vertices set between v 1 and v 2 can be deduced in Equation 11.
 In a similar way, the disappeared part of the common neigh-bor vertices set between v 1 and v 2 can be deduced in Equa-tion 12.

We put the overall incremental propinquity update process in Figure 2. The update can be divided into two cases of angle and conjugate propinquity, as the coherent neighbor-hood propinquity was defined. The angle propinquity up-date is relatively simpler and can be calculated with Equa-tion 6. The conjugate propinquity update is divided into three subcases. The conjugate propinquity update brought by cut and emerged edges are shown in Equations 7 and 8 respectively( I-Conjugate Update and D-Conjugate Update in Figure 2). The third part of conjugate propinquity update( R-Conjugate Update in Figure 2, Equation 9) is somewhat analogy to the angle propinquity. The three elements of Equation 9 should be further calculated with Equations 10, 11 and 12. Note that, all the operations are mapped to set intersection( ), concatenation(+) and cartesian product( + or  X  ) operations on three simple disjoint set, that is N N I ( v )and N D ( v ). In other words, the entire algorithm can be accomplished by a limited number of simple set opera-tions.
The correctness of the incremental angle propinquity up-date is obvious by the deduction in Equation 6. Now we focus on the conjugate propinquity. Let  X  G ( E )denotethe conjugate propinquity value set calculated from each edge in E ( G ), that is: We define E R , E I and E D by E n +1 = E n + E I  X  E D and N R = E n + E D . Then the overall conjugate propinquity difference between two successive iterations can be deduced by Equation 14.
 By expanding the three parts of Equation 14 with Equation 13, we can have precisely the first lines of Equations 9, 8, and 7 as the sum terms. Adding up the na  X   X ve direct con-nection propinquity, by now we have successfully proved the completeness of incremental propinquity update.
Without the assumption of graph sparsity, the complexity of propinquity dynamics is actually O (( | V | + | E | )( where we use the average to approximate each vertex degree. However real networks are always dense, and the degree dis-tribution is highly skewed. The Wikipedia linkage graph data we adopted in the experiment is a typical dense and skewed one. Besides, the intermediate propinquity values on dense graphs can be very large, and requires large memory space. All these facts make community structure discovery from such large, dense and skewed graph data a challeng-ing task. Therefore, we implemented our algorithm under a parallel framework, and expect that the work load can be distributed on thousands of machines.
The most intuitive way to parallelize a graph algorithm is by data parallelism, in which all vertices are evenly as-signed onto multiple machines. Our model basically belongs to this class. Each vertex is regarded as a virtual process, and keeps its own programming logic and local data struc-tures. A physical machine sequentially executes the vertices assigned to it by calling their logic and accessing their lo-cal data structures. Under this parallel framework we can implement the propinquity dynamics from the vertices X  per-spective. The vertices(virtual processes) have no idea about their physical location, and can exchange information from each other purely by message passing.

The propinquity dynamics is an iterative process and re-quires all vertices keeping synchronous from each other. So we also adopt the bulk synchronous parallel (BSP) model[19], which perfectly satisfies this requirement. In the BSP model, the overall computation proceeds in consecutive superstep s. Each participating processor can do three kinds of jobs in a single superstep: (1)Accessing the messages sent to it in the previous superstep; (2)Carrying out local computation and accessing local memory; (3)Send other processors mes-sages, which will be available to the destination processor later in the next superstep. There is a barrier between two successive supersteps for the synchronization purpose. The barrier is also beneficial to the fault recovery. Combining the virtual-process technique and the BSP model by regard-ing graph vertices as the processors of the BSP model, we can have our parallel framework. The physical machines will execute each contained vertices once in each BSP superstep.
Since we have the vertex oriented parallel framework, in this section we will describe the implementation of our al-gorithm from the vertex X  X  perspective. Each vertex uses three vertex id sets: N R , N I and N D to record its neigh-boring topology and topology update. In Section 4, we have mapped the entire incremental propinquity update process to operations on these three sets. As for the propinquity, each vertex keeps a hash map structure P for recording the pairwise propinquity values, which involve itself and are greater than 0. Note that each propinquity map entry is double duplicated at another vertex.

Besides the local data structures, we have to define two main types of messages before hand. The first message tells the receiving vertex to update its local propinquity map: where v d is the destination vertex id, and V u is a vertex id set. For each vertex v i  X  V u , the receiving vertex should increase/decrease the propinquity map entry P [ v i ]byone unit. The second message is used to send parts of the local neighbor vertex sets to another vertex: where S R , S I and S D are used to carry N R , N I and N respectively, all the three can be null. At the very begin-Input : N R Output : P
Superstep 0 :
Superstep 1 :
Superstep 2 :
Superstep 3: ning, a vertex, say v s ,has N R storing its initial neighbor vertex set. Before stepping into the cruising incremental propinquity dynamic iterations, the propinquity map P ac-cording to the current topology should be first constructed by Algorithm 1. After that each vertex can run into the logic in Algorithm 2, where the input and output are both N R and P therefore can be executed iteratively. In Algo-rithm 2 we omit the propinquity update message processing logic as it is exactly the same as in Algorithm 1. The h ( in the pseudo code is a function that helps decide which vertex should initiate the neighborhood donation. For the sake of compactness, we put the lengthy set intersection and concatenation operation involving neighboring sets from dif-ferent vertices in Table 1. In the pseudo code we use C XY to refer the resulting set calculated from operation at X col-umn and Y row in Table 1, where subscripts 1 and 2 are used to differentiate the source of the neighborhood sets. Besides, the original formulation numbers are marked next to the formula in Table 1. Input : N R , P ,  X  ,  X  Output : N R , P Superstep 0 : Superstep 1:
Superstep 2: Superstep 3:
While mining very large network data, we found that the overall messages sent in a single superstep can easily exceed the memory quota. In order to guarantee the memory res-idence rather than relying on I/O, in real implementation, one single supersteps in the pseudo code description is ac-tually broken into multiple supersteps. In the pseudo code, each message sending statement is preceded by a for-each , therefore bookmark variables can be introduced to resume the message sending progress in the next superstep. The question is how the vertices decide when to cease message sending, which directly influences the load balance. After tried several message bounding schemes, we adopt a message size estimate technique, in which before a macro superstep begins, all vertices collaboratively estimate the overall mes-sage size. Then by comparing with the physically available memory, a least required number of supersteps can be cal-culated. Each vertex can then break its local message into this number of parts. In this way, the overall load balance can be guaranteed.

Another performance improvement we made is to buffer the propinquity messages with a second map. Before leav-ing the iteration in Algorithm 2, each vertex merges the two propinquity maps. While the update part of the propin-quity value list is relatively small, the buffering technique can save many map search operations on the original propin-quity map.
In order to evaluate the efficiency and scalability of our algorithm on large graphs we mainly used the Wikipedia linkage graph dataset dumped on July 24, 2008. In this graph, vertices are normal wikipedia pages, and hyperlinks in Wikipedia pages pointing to other normal Wikipedia pages are extracted as undirected graph edges. There are some redirect pages that will be redirected to another normal page if accessed, e.g. Mongol is a redirect page pointing to Mon-gols . We utilized several passes of MapReduce to prepro-cess and clear the graph data including merging the redirect pages and removing self loop and multi edges. The statistics of the final Wikipedia linkage graph are shown in Table 2.
We also used other three neutral-sized graph datasets. Ed-inburgh Associative Thesaurus(EAT) is a word association network, which is composed by a large number of word norm stimulus and responds. Erdos02 is a co-authorship network, in which Paul Erdos is a prolific mathematician lying at the core of the entire network. hep-th-new is a citation graph data from KDD Cup 2003. It contains the citation tree of many articles related to the topic of high energy physics(hep) since 1974. The statistics of these three datasets are also in shown Table 2. Figure 3: Overlapping community structures mined from word association network eatRS.
We first tested effectiveness of the propinquity dynamics based community detection on the neutral-sized eatRS data. Figure 3 shows part of the discovered community structures, which are differentiated by colors. The blue, pink, yellow, and green communities contain words related to forest land-scape, stiff material, stationery, and manufacturing respec-tively. Besides, the red nodes represent the community over-lapping, which may mean polysemous words, e.g., wood is a kind of hard material(pink) and is also essential as part of the forest(blue). As another overlapping example, the factory(green) and stationery(yellow) are both related with work . This experiment was done with  X  =5 and  X  =180 on 10 machines(1G CPU, 1G Mem) 1 . The computation can be finished by 2 iterations and 363 supersteps within 2 minutes.
Running on the Erdos02 co-authorship network, our algo-rithm also generates interesting results as partly shown in Figure 4. It contains the projection of 5 selected resulting community on the entire network. The communities and overlaps are also marked with different colors. Famous Paul Erdos is certainly the most obvious community overlap. Be-sides, there is another productive mathematician, Tarlok N. Shorey, in Figure 4 who has collaborated with two loosely in-terconnected groups of mathematicians, therefore can been seen as a community overlap. This experiment was done with  X  =2 and  X  =20 on 10 machines(1G CPU, 1G Mem). The computation was complete by 2 iterations and 169 su-persteps within 1 minute. Through these two experimental results we can see that the propinquity dynamics can effec-tively identify the community structures and the overlapping vertices between them.
 Figure 4: Overlapping community structures mined from the Erdos02 co-authorship network.

Compared with the previous two neutral-sized graph data, detecting community structures from the original Wikipedia linkage graph is rather challenging. The difficulty comes from the skewness of the vertex degree distribution. For ex-ample, the page of United States is pointed by 40,9511 other pages, and the page with title List of endangered animal species contains hyperlinks pointing to other 5,549 pages. The running time for a single vertex depends on the square of its degree, therefore the overall efficiency can be under-mined by these abnormal vertices. By the principle of TF-IDF, vertices with very high degree contribute few infor-mation for the community detection. So we filter out part of edges by setting an upper bound on the in-degree and out-degree. We used 300 as a typical degree bound for guar-
All our experiments are carried out on Google data centers, from which computational resources like CPU and memory can be accurately allocated. anteeing both the community quality and running efficiency. By setting  X  =400 and  X  =1000, we obtained the community structures hidden in the Wikipedia. Like the skewness of the degree distribution, the community structure size distri-bution is also unbalanced. Table 3 lists three neutral-size ones.
 Table 3: Selected community structures mined from Wikipedia linkage graph.

Our community detection algorithm is implemented on a parallel platform, so it is time to verify the efficiency of the parallelization. We will use two group of experiments to test the speedups on datasets with different scale. The first group of experiments were conducted on the neutral-sized hep-th-new graph, with parameter setting:  X  =20,  X  =300. The termination condition was set to 1000, which con-strains the running to 5 iterations and 655 BSP supersteps. We allocated 1G CPU and 1G memory for each machine. From Figure 5 we can see that the speedups on dozens of machines are very impressive. The reason we did not use more machines is because of the neutral size of the dataset. Over partitioning the graph vertices on too many machines will bring unaffordable overload. Figure 5: Speedups while running on neural-sized hep-th-new paper citation network.

Figure 6 presents the speedup results on large Wikipedia dataset while running on up to 1K machines. The param-eter setting is:  X  =400,  X  =1000. For a reasonable waiting time, the running is constrained to the first 4 algorithmic iterations. Note that, when 62 and 125 machines are used, the memory quota has to be increased for the purpose that the overall memory space req uirement can be satisfied. Figure 6: Speedups while running on large scale Wikipedia linkage graph.
In the final part, we will reveal the improvement brought by the incremental propinquity update technique. Under the configuration of  X  =400 and  X  =1000, the r unning time comparison between the incremental and non-incremental versions of propinquity calculation on 1000 standard ma-chines with 1G CPU and 1G memory is drawn in Figure 7. Iteration 0 means the initial propinquity calculation from input graph topology. We can explain the efficiency im-Figure 7: The effectiveness of the incremental propinquity update(Wikipedia linkage graph). provement of incremental propinquity update over the non-incremental one by Figure 8, which presents the size evolving of the topology and propinquity maps. The size of N I and the precondition of the justification of our incremental tech-nique. The size difference between propinquity map and the updated part proved the double propinquity map technique makes sense. Figure 8: Topology and propinquity evolves with iteration.
In this paper, we proposed an intuitive community de-tection algorithm, which is based on the mutual update between the topology and propinquity. Under the defini-tion of coherent neighborhood propinquity, a series of effi-cient calculation algorithms and incremental techniques are proposed. The entire algorithm was implemented under a virtual-vertex oriented bulk synchronous parallel frame-work, which can distribute the work load to thousands of machines. The experimental results show that the coher-ent neighborhood based propinquity dynamics can correctly identify overlapping community structures from real graph data, and the incremental propinquity update techniques are very effective. The speedups on various datasets and different numbers of machines are satisfactory.

In the future, we should prove the conditional convergency of propinquity dynamics. Besides, each vertex may be able to decide its local cutting and emerging threshold through referring its local neighborhood. Two threshold parameters control the granularity of the community detection, and may be changed in the middle of the propinquity dynamics for the purpose of exploring the community hierarchy.
