 With the popularization and application of E-commerce extending, more and more people go shopping online rather than shopping out. Searching for product information and buying commodities online have become popular activities [1]. Empirical research shows that, nowadays, many individuals tend to start their shopping process with an information search on the Internet before they go to the store [2]. Many online shopping websites become popular consequently, such as Taobao, 360buy, Amazon. It is convenient to buy commodities on Internet, and customers can buy almost everything without going out, furthermore the commodities which customer have bought can even be delivered to them. Thus, E-shopping could lift the time and space constraints of shopping process and bring more flexibility [3]. 
As a commodity has different prices in different online stores, customers need to select an approving store for buying when they see a favorite commodity. Customers even actively seek commodities they want to buy. Most search engines of online shopping websites provide search based on text words currently. However, the search can not meet customers X  requirement as: (1) Sometimes, text words are more difficult to express customers X  requirements than images. For example, if a customer wants to clearly than text description about it. (2) Text search is based on tags of commodities, many irrelevant results or loss many relevant results. 
Obviously, search based on images is required. Nowadays some search engines have shopping websites. We have noted that commodities of online shopping websites have a character that there are about five images and a paragraph of text to describe a commodity. The images can be divided into three kinds, i.e.  X  X ig image X ,  X  X iddle image X , and  X  X mall image X . Customers achieve a general impression of the commodity further uses SIFT features for accurate search in second stage. Moreover, we develop a prototype system for multi-modal search including text-based and image-based search. Using the prototype, by submitting words, phrases, images or their combination, customers can retrieve what they want to buy. The experiments of comparing with modal search prototype system are effective, and the retrieval results can satisfy customers X  requirements well for online shopping. 
The remainder of the paper is structured as follows. Section 2 introduces the extract commodities X  features. Section 5 describes online multi-modal search process. Section 6 shows our experiment results. Section 7 concludes our work and gives directions for future studies. In this paper, our purpose is to provide a multi-modal search prototype system about commodity for online shopping. In this field, researchers have done some pioneering work. Davis [4] proposed a multi-modal shopping assistant which provides users with occurs during this activity. It helps consumers to find commodities quickly at shopping mall in reality. Anil [5] proposed an algorithm to search commodities with trademark. Some companies have developed some image retrieval systems like QBIC of IBM [6, 7], Virage of company Virage [8], and MARS of Illinois University of United States [9]. All of them are great prototype systems for image retrieval. 
However, these studies above have some obvious shortcomings. Firstly, few of into online shopping websites. Secondly, although some of the works take multi-modal into consideration, they do not mention the impact of other factors such as time. To overcome these shortcomings, we propose a multi-modal searching for online shopping websites in this paper. In our work, we will utilize HSV [10], GLCM respectively. About these algorithms, researches have done related work. 
Color histogram is one of the most commonly used methods of image retrieval based on color feature, and HSV is an approximately-uniform color space: Hue, Saturation, and Value [10]. One of the reasons inhibiting these spaces from being transformations involved [11]. 
Texture is a visual feature which is produced by spatial distribution of tonal variations linear relationship with another gray tone, within the area under investigation [12]. 
SIFT descriptors are computed for normalized image patches with the code provided by Lowe [14]. The resulting descriptor is of dimension 128, and it is distortion, addition of noise, and change in illumination [14]. 
Locality-sensitive hashing (LSH) was introduced as an approximate high-dimensional similarity search scheme with provably sublinear dependence on the data size [15, 16]. Instead of using tree-like space partitioning, the key idea is to hash the points using several hash functions so as to ensure that. For each function, the probability of collision is much higher for objects which are close to each other than for those which are far apart [17]. 
Based characteristics of HSV, GLCM, and SIFT algorithm above, in our work, we extract visual features using them, and search related image with the merged features roughly by LSH, then use SIFT features to search accurately. commodity without ambiguity) consisting of texts and images of all n commodities. and description, and I -Set means c  X  X  image data such as big image, middle image, and small image. Moreover, we give more detailed descriptions as follows. obtained from T -Set of c by natural language process techniques. Let VF ={ VFC , VFT , and SIFT features, respectively. 
For the shopping website, we will extract TF and VF from T -Set and I -Set of every RF = f 2 ( VFS ). Fig.1 gives an example of generating BF and RF . CQ be text set and image set submitted by the customer for query, respectively,  X  be the relation of T -CQ and I -CQ , and  X  ={and, or}.  X  can be obtained from customers X  feedback or other approaches. 
For multi-modal search in shopping website, TF , VF ={ VFC , VFT , VFS }, BF = f ( VFC , VFT ) and RF = f 2 ( VFS ) need to be extracted and generated from T -CQ and I -textual feature set TF , we use two-stage strategy for image search. images (presenting commodities), i.e. compare BF from I -CQ of customer X  X  query CQ with one from I -Set of every commodity c  X  C , and the result is C  X   X  C . 
Stage 2: Based on the results of Stage 1, use RF = f 2 ( VFS ) for image search accurately, i.e. in the commodities set C  X  after filtering in Stage 1, compare RF from I -CQ of customer X  X  query CQ with one from I -Set of every commodity c  X  C  X . 
In this paper, we build a multi-modal search prototype system with above functions. It includes offline and online process. 
The offline process includes: (1) Download data C ={ c 1 , c 2 , ..., c n } from shopping respectively; (5) Update these sets regularly according to new commodities. and visual features from I -CQ , and generate BF and RF ; (3) Execute text search based on TF for result TR (text results) and two-stage image search based on BF and RF for result IR (image results); (4) Return R = TR  X  IR to customers. 
We will give a detailed description of above work in Section 4 and Section 5. After downloading commodity data from online shopping websites, the commodity data include two kinds of data, texts and images, i.e. T -Set and I -Set in Section 3. We apply different algorithms to extract different features from different kinds of data. 
To obtain textual features, we build two lexicons, Stop Lexicon and Shop Lexicon to help us analyze the textual description of commodities more effectively. 
To deal with images, we extract three kinds of features from the images, color feature, textural feature, and SIFT feature, using HSV [10], GLCM [13], and SIFT [14] algorithm, respectively. Due to the complexity of the SIFT, though this kind of features is invariant to image scaling and rotation, and partially invariant to changes in illumination and viewpoint [18], the high dimension and time-consuming problems make it unsuitable for complete image search, so we utilize other two kinds of features to filter first and then further utilize SIFT features to search smaller image set. 4.1 Construction of Stop Lexicon and Shop Lexicon for Text Partition In our prototype system, we apply JE [19] to split T -Set into words. Because there are many onomasticon in shopping websites such as  X   X   X   X (fleece),  X   X  X  X  X   X (mohair),  X   X   X  X  X  X   X (discount), JE can X  X  split them correctly. For solving the problem, we build two lexicons, the Stop Lexicon and the Shop Lexicon. (1) Shop Lexicon. For T -Set of a commodity, JE will split it to a string of words. However, JE often get too fine-grained words, so we build the Shop Lexicon to help JE to split appropriately. Our Shop Lexicon has 59 common words such as  X   X  X  X  X   X ,  X   X  X  X  X   X , and 10 onomasticons such as  X   X  X  X  X  X  X   X ,  X   X  X  X  X  X  X   X . (2) Stop Lexicon. When we calculate the frequency to sort the result set, some words are unnecessary to the ranking, so we build the Stop Lexicon to filter the result. Our Stop Lexicon has 45 words, such as  X   X  X  X  X   X ,  X   X  X   X ,  X  2012  X . With two lexicons, we apply Lucene [19] to create index of text and implement text search. Moreover, the two lexicons can be updated with new commodity data. 4.2 Generation of Basic Feature Set from Commodity Image poor, but using SIFT features will be much time-consuming. To solve the problem, we propose a two-stage image search approach combining color, textural, and SIFT features, which utilizes basic features consisting of color and textural features to filter mismatching image in first stage, and further uses SIFT features for accurate search in second stage. For this purpose, we offline extract these features from commodity images, generate basic feature set BF and refined feature set RF . 
We first use HSV algorithm to extract color features and GLCM algorithm to commodity and images is one-to-many. Thus, we merge the color features and textural features extracted from images belonging to the same commodity and generate BF of the commodity. 
It is notable that background of commodity image can make big noise to the image search, so we preprocess it firstly. In preprocessing, background interference is removed to get interest region, and merge the images features which describe one commodity. We describe the algorithm as Algorithm 1. Algorithm 1: Basic Feature Generation Input: I -Set; // I -Set is image set belongs to one commodity; 
Output: BF ; // BF is Basic Features of the commodity; 1) call Algorithm 2 for getting interest region I -Set X   X  I -Set of each image; 2) for every image I  X  I -Set X  3) {extract color features of I using HSV algorithm and get VFC ; 4) extract textural features of I using GLCM algorithm and get VFT ; 5) generate BF = f 1 ( VFC , VFT ) and append BF into BF -Base;} In the Algorithm, line 4) is for getting textural features using GLCM. In three kinds of images of a commodity, because small images describe details of the commodity, we extract textural features of small image as the commodity X  X  image textural features. It is a vector of 5 dimensions. Because the value in this vector is very small, we amplify we compare the area of interest region to judge which image is  X  X mall image X . Next, line 5) is for getting basic feature set BF using f 1 function. In detail, for a commodity c , which has m images such as big images, middle images, and small images, we as VFC i , VFT i , respectively. Here f 1 function is shown as Formula (1). maximal area), BF is an union features finally. Here VFC is a 128 dimensions vector, H component is divided into 16 levels and S component is divided into 8 levels. 
In line 1) of Algorithm 1, Algorithm 2 is called to get interest region. Algorithm 2 is described as follows. Algorithm 2: Interest Region Getting Input: I -Set; // I -Set is image set belongs to one commodity; 
Output: I -Set X   X  I -Set; // I -Set X  is interest region set of each images; 1) for every image I  X  I -Set 2) {detect Canny operators CI of I ; 3) compute CI  X  by  X  4) detect Contours CCI  X  X f I ; 5) compute I  X  by  X  In Algorithm 2, line 2) is for image edge extraction using Canny edge detection operator, because the operator is sensitive, many edge inside of the interest region can Line 4) is to get the contours of the CI  X . In line 5),  X  is erosion operation to eliminate the noise. At last, we get the produced image set I -Set X  which has interest areas only. 4.3 Generation of Refined Feature Set from Commodity Image Generating refined feature set is for seco nd stage image search. We apply SIFT algorithm to extract SIFT feature set VFT from interest region of every commodity X  X  images. According to SIFT al gorithm, for an image, its VFT is a 128 dimensions of m  X  128. Obviously, f 2 function is constructing a matrix with m vectors. Algorithm 3 gives the process of generating RF . Algorithm3: Generation of Refined Feature Set Input: I -Set X ; // I -Set X  is interest region set of all images; 
Output: RF ; // RF is refined feature set from I -Set X ; 1) for every image I  X  I -Set X  extract SIFT feature set VFS ; 2) RF = f 2 (all VFS ) and save RF into RF -Base; //from vector to matrix; We can execute accurate search based on RF because of its invariant to image scaling and rotation. Because the search is executed in smaller set, the time cost is acceptable. Our multi-modal search prototype system is a middleware between shopping websites and customers. In this prototype system, customer can submit phrases, sentences, or images for searching. We use multi-modal s earch algorithm to implement the process. Algorithm 4 gives the process of multi-modal search. Algorithm 4: Multi-modal Online Search 
Output: R = TR  X  IR ; // TR and IR are returned results for text and image search; 1) split T -CQ into TF '=( tf 1 , tf 2 , ..., tf m ) with Shop Lexicon; 2) delete stop words from TF with Stop Lexicon; 4) extract VFC , VFT , and VFS from I -CQ using HSV, GLCM, and SIFT method; 5) generate BF using f 1 function, i.e. BF = f 1 ( VFC , VFT ); 6) call Algorithm 5 based on BF for getting filtered image set C  X ; 7) generate RF using f 2 function, i.e. RF = f 2 ( VFS ); 8) call Algorithm 6 based on RF and C  X  for getting exact result IR ; 9) if  X  = X  X r X  return R = TR  X  IR ; In Algorithm 4, line 1)~line 3) is for text search, where Shop Lexicon and Stop features from the image submitted by customers, the approach is the same with line 3) and line 4) in Algorithm 1, and line 1) in Algorithm 3. Line 5) is for generating basic have been introduced in Section 4.2 and Section 4.3, respectively. Line 6) is for and TSearch ( IR , n ) is for searching the commodities by text which contains the top n of the word frequency from high to low of the TR. Algorithm 5 and Algorithm 6 show the two-stage search processes. Algorithm 5: First Stage Image Search Input: BF ={ bf 1 , bf 2 , ...}; // BF is basic features from customers X  query I -CQ ; 
Output: C  X   X  C ; // C  X  X s search result set with basic features 1) for BF -Base of all commodities // BF -Base is generated in Algorithm 1; 3) transform every bf  X  BF into a binary vector q -B H also using Formula (2); // c -B H is from BF -Base and q -B H is from BF 5) compute the MD5 value I i of c-B H  X  and q-B H  X  using Formula (4);} 6) save all Mc-B H  X  into c -bucket and get hashtableNum buckets; // Mc-B H  X  is the key value of the hashmap 7) save the hashmap and hashfamily which include all hash function g i ; 8) for i =1 to hashtableNum ; 9) find the same key c in i th-c -bucket with Mq-B H  X  and add c to C  X ; hashmap and hashfamily and saving them. It will be executed only if the database has randomly. The smaller  X  is, the greater the ability of approximate searching is, but the false positive corresponding is bigger too. When  X  equals to an appropriate size, the result will be best. Line 8) to line 9) is to search the similar data in every hash table. 
In the first stage of image search, BF is high dimensional, we can X  X  use traditional index technology like R-tree because of dimension curse. Hence, we choose LSH index technology to search based on BF . We put similar BF into one hash bucket by hashtableNum hash tables using related series of hash functions. When we query a commodity by BF , the hash value is calculated by related hash functions and the data in corresponding hash bucket is the candidate set which is the result of first stage. Moreover, Algorithm 6 gives second stage image search process. Algorithm 6: Second Stage Image Search Input: C  X , RF ; // C  X  is search result set of first stage; RF is refined feature set 
Output: IR ; // IR is final image search result set 1) find RF  X   X  RF -Base; // RF  X  is RF of C  X  2) find IR by comparing RF with every rf  X   X  RF  X ; RF -Base, i.e. refined SIFT features. Line 2) uses RF from I -CQ to search in RF  X , and final result IR is image set found by refined SIFT features. In order to test the algorithm we proposed in this paper, we download the data from the hottest online website in China, Taobao (http://www.taobao.com/). We get about 9632 images in 309 stores, and searching data in the database. Some images are shown as Fig.2, and the category distribution of the data is shown in Fig.3. 6.1 The Parameters in Search and Effectivity Comparison In line 4) of Algorithm 5,  X  is the unknown numbers of the vector in hash function g i , precision, recall and F-score vary with  X  respectively. We can see when the value of  X  and every value is the average of ten experiments. 
In line 6) of Algorithm 5, hashtableNum is another parameter. As Fig.5 shows, the three charts present precision, recall and F-score varies with hashtableNum . When hashtableNum increases, F-score increases too. But when hashtableNum equals 10, the F-score is almost invariant, thus we choose 10 to be the value of hashtableNum . highest frequency. With the variety of n , F-score is changing. In Fig.6, the first figure is result numbers with n , and second is F-score with n . As it shows, we can see, as the value of n is equal to 4, the F-score has the maximum. Fig.7(a) shows F-score decr easing with commodity number increasing. In Fig.7(a), GF is our method, means image features merged by HSV, GLCM and SIFT. Contour means using color features from interest area, HSV and GLCM represent using simple features only. As the curve shows, if we use single simple features, the accuracy will be very low, and even more, with the numbe r of images increase, it decrease rapidly. GF , because we merge the SIFT features in it, the accuracy is high and stable. Fig.7(b) shows the comparison of time between SIFT and GF (using Base and SIFT features). 6.2 Multi-modal Retrieve Results As Fig.8 shows, Fig.8(a), we search with text  X   X  X  X  X   X  X  X  X  X  X  X  X  X  X  X  X   X (skirt choose first 9 commodities to show. The textual description of the commodities has not only  X   X  X  X  X   X   X (ink flower), but also  X   X  X  X  X   X   X (blooming ceiba flower) because of the sort of the frequency. The same as Fig.8(a), we search with a sentence  X   X  X   X  X  X   X  X  X  X  X   X  X  X  X   X (new style dress) in Fig.8(b), and get the results as below. In both parts, the parameter  X  is  X  X nd X . The results in Fig.8(c) and Fig.8(d) are in the same condition except the parameter  X  is  X  X r X . In this paper, we focus on searching commodities by both image and text from online shopping websites. For text search, we use Lucene to construct index. For image search, we propose two-stage search. The first stage is to filter obvious mismatching images by color and textural features using algorithm LSH, and the second stage is to match with SIFT features. For online search, customers may submit texts and (or) images for obtaining exact or extensive commodities X  information they want to buy. 
Although this paper puts forward many actual effective methods, there are still some to be improved such as applying and improving more image process methods, including more data modalities. All of them are our future research topics. 
