
A new supervised learning problem, the multiclass pre-diction with bandit setting model, is first proposed by [1]. Unlike the conventional supervised learning paradigm, it focuses on applications in which only partial feedback, instead of full label information, is received by the learner itself.  X  X artial feedback X  as mentioned, means that the learner receives only  X  X ight X  or  X  X rong X  feedback about its prediction results from some independent oracles, such as people.

Naturally, this kind of paradigm is online. And for most real world internet usages, full label information is hardly revealed. On one hand, as we all know, the internet surfers are too  X  X azy X  to enter true labels even if they have already known them; On the other hand, even the surfer does not know what exactly he or she wants. For example, people put some requiring features to the search engine, but they cannot figure out the definite item type they request. However, a simple click of the mouse responses an approximate incli-nation of the user, representing kind of  X  X artial feedback X .
The rising usages of internet raise new challenges to the traditional learning fields and impose a powerful encourage-for future prediction, [1] proposed the Banditron Algorithm. Based on multiclass perceptron algorithm, the Banditron uses an exploitation-explore scheme to handle the difficulties of utilizing the negative feedback. In some rounds where the algorithm explores, it makes a prediction uniformly with probability  X  from the full label set instead of the most probable one the learner believes.

Another approach is Offset Tree reduction algorithm[2], a recent research work for learning with partial labels, which deals with a more general problem than what we consider. However, since  X  X hen solving a given problem, try to avoid solving a more general problem as an intermediate step X  X 3], our method focuses on the specific problem instead of the general one. When the partial label learning problem requires an interactive setting, the reward for one prediction choice is restricted in 1 and 0 and it X  X  always possible to choose the best action, the partial label learning problem degenerates to the online multiclass prediction with bandit setting problem and the corresponding algorithm is called Realizable Offset Tree Reduction algorithm. And Costing algorithm[4] is applied to the updating rule according to the partial feedback.

Some other works is related to  X  X ulti-armed bandit X  problem[5], [6]. Though both dealing with side information, the online multiclass prediction with bandit setting model focuses more on the classification problem and finding effi-cient algorithms, while those consider more on the abstract hypothesis spaces[1].

This paper provides a new perspective for online mul-ticlass prediction with bandit setting model. Our algorithm, called Conservative OVA(one-versus-all) Reduction with On-line Passive-Aggressive Algorithm , enjoys a pleasing theo-retical result of cumulative error rate as well as updating time cost linear to the sample size. First, we reduce the multiclass problem to binary according to Margin Based Reduction from Multiclass to Binary [7]; then a conservative scheme is applied for dealing with the bandit setting ; finally Online Passive-Aggressive algorithm[8] is embedded to handle binary problems. More details about our algorithm will be discussed later. Further experiments on several multiclass prediction dataset verify our theoretical bound and demonstrate that our algorithm performs far better than the Banditron Algorithm and the Realizable Offset Tree Reduction Algorithm.

In this section, we will define the problem of online multiclass prediction with bandit setting model in details, as well as the concept of Online Binary Linear Predictor , which will be used in the following sections.
 A. Online Multiclass Prediction with Bandit Setting Model
In Online multiclass prediction with bandit setting prob-lem, the learner observes instances in a consecutive manner. whenever a mistake is made then l t  X  1 and l 2 t  X  1 , as well as the obvious fact that l t  X  0 , which is noted in Eq. (1). A. Overview In this section, we will introduce our Conservative OVA(one-versus-all others) Reduction with Online Passive-Aggressive Algorithm in details, which is novel and concen-trated on the idea of  X  X onservative X .

In contrast to the Banditron Algorithm [1], our method doesn X  X  take much efforts to deal with the balance between exploitation and exploration, since we abandon the Kesler X  X  construction [9] for multiclass setting. Instead, we bring in the method of Margin Based Reduction from Multiclass to Binary [7] as an oracle framework for multiclass prediction problem with bandit setting. The simplicity of this method as well brings a lot of benefits for fully theoretical analysis.
What we mean by calling our algorithm  X  X onservative X  is that we choose to take advantage of maximum information each sample discloses fully, and not to explore greedily to obtain more information. More about  X  X onservative X  would be clearly explained in subsection III-C.

Furthermore, we embed Online Passive-Aggressive Al-gorithms [8] for online binary learners as base learning algorithm to accomplish our method for online multiclass prediction with bandit setting problem. Noting that other on-line binary learning algorithms can be suitable for our oracle Conservative OVA Reduction scheme, we adopt this choice to make a comparable part against Banditron Algorithm ,as well as further theoretical analysis.
 B. Margin based Reduction from Multiclass to Binary
Reducing multiclass prediction problem to binary attracts much attention during the years, which includes two of the most famous methods, one-versus-all others and all-pairs approaches[10]. Usually these kinds of reduction for multiclass prediction enjoy acceptable performance as good as some direct algorithms.

In this paper, we adopt the Margin based Output Cod-ing [7] as the basic scheme for our problem. First, a coding matrix is given, where k is the number of classes and l is the number of binary classifiers. Any binary classifier learning algorithm is provided with labeled examples in the form of ( x 1 means that the sample ( x i ,y i ) is a positive example for s th binary classifier, while M ( y i ,s )= is a negative example and M ( y i ,s )=0 means this sample is omitted by the s th binary classifier.

For instance, for all-pair reduction, M is a k  X  k 2 matrix with each column corresponding to a label pair ( k 1 ,k 2 ) .In
At first, we reduce the multiclass problem to binary by one-versus-all others reduction. As mentioned in III-B, now the coding matrix M is k  X  k ,
Generally speaking, we maintain k binary classifiers to distinguish each class from all other classes. An example ( x, y ) is a positive example only for the y th classifier, and a negative example for the other k  X  1 classifiers.
Then based on one-versus-all others reduction, we will talk about our Conservative Scheme . In bandit setting, if a learner receives a positive feedback of its prediction, then it immediately reveals what the actual label of this sample is. Thus an example with full label information is obtained and all the k binary classifier can be updated by the sample ( x t ,y t ) at this step; Otherwise, if a negative feedback is accepted, the learner will only know that this example doesn X  X  belong to a certain class and has no further information about which class it belongs to, denoted by  X  y t . Thus only the binary classifier corresponding to y t with x t as a negative example is updated, and with other binary classifiers unchanged.

Finally, the learner outputs the predicted label based on its former knowledge by minimizing the total loss as Eq. (6), where M is shown as Eq. (7).

In this Conservative scheme, when an input instance x t is misclassified, only the corresponding binary classifier is updated in order to put on more loss to assign such example as this misclassified class. So if a similar instance x t comes, then the learner is more likely to predict it as another class. Since other binary classifiers are not changed, the combined learner X  X  preference of other labels will only root in the knowledge obtained from former steps. Thus we try not to make any prior knowledge to influence the learner, and let the learner exploit the knowledge by itself.

This updating scheme is conservative . We don X  X  take any efforts to explore what the real label the example x t is when the predicted result is incorrect. Instead, we try to make use of the partial information that which class it does not belong to as entirely as possible. Thanks to the one-versus-all others reduction, these partial feedbacks can be easily handled by only updating corresponding binary classifiers.

In addition, an online algorithm is called conservative if it updates its prediction rule only in rounds in which it makes a prediction error[11]. Nevertheless in our work, the definition of conservative is not quite the same. We call our algorithm conservative because that when the algorithm gets a negative feedback, it only updates the corresponding binary classifier, while leaving others unchanged.
 Eq. (8) forces the updated weight vector w t + 1 to satisfy l ( w ;( x
Since problem (8) is a convex optimization problem, it can be easily solved. According to Karush-Khun-Tucker conditions[12], the closed form of solution for (8) is
It seems too brutal to force the weight vector w t + 1 to satisfy the constraint imposed by the current example ( x t ,  X  y t ) because of the common phenomenon of the noisy label information. Thus a nonnegative slack variable  X  is brought in to achieve a soft-margin classifier. When the objective function scales linearly with  X  , we call it PA-I model: On the other hand, when the objective function scales quadratically with  X  , we call it PA-II model: where C is a parameter that adjusts how much the soft-margin  X  affects the objective function.

Eq. (10) and Eq. (11) are also convex optimization problems, the solutions to which also take the closed form w
Embedding above online binary prediction algorithm into our Conservative OVA Reduction scheme, we can conclude our algorithm as Algorithm 1.

In this section, we will carefully derive some relative bounds for our Conservative OVA with PA algorithm and verify the effectiveness of our algorithm.

First, as [7], we define the distance between two rows of Let S = { s : M ( r, s ) = M ( y t ,s ) ,s =1 ,...,l } .An inequality can be attained:  X  z st , thus L ( z st )+ L ( z st ) one of z st and z st equals 0 , which indicates that L ( z st )+ L ( z st )  X  L (0) = 1 . So by the definition  X  and  X  ( u, v ) ,we can derive that: Therefore, by taking summation of Eq.(18) according to round t and considering the basic character of loss function L (  X  ) that L (  X  )  X  0 , we obtain the following bound of the number of cumulative mistakes E : Simply switching the two summations completes the proof.
Theorem 1 differs from the training error bound of [7] since we focus on deriving a bound for online algorithms, while [7] deals with its batch counterpart.
 For the one-versus-all others reduction, the following Corollary holds: Corollary 1. If the condition is the same to Theorem 1, for one-versus-all others reduction, the cumulative number of mistakes made by the loss-based decoding scheme of an online algorithm satisfies, E = reduction, Eq.(19) is simply verified.

In [8], Crammer etc. have proven bounds for PA algo-rithm. And PA-I and PA-II updating method approximate the basic PA updating when aggressive parameter C goes to infinity. The following lemma for binary classifiers has been mentioned and carefully discussed in [8].
 Lemma 1. Assuming that for a sequence of samples, ( x { +1 ,  X  1 } . Let u be an arbitrary fixed predictor and define Then by setting  X  t be as in Algorithm 2, we hold the bound for any u  X  R d , Then with definition of  X  t of PA-I, Eq.(26) is derived.
Finally, plugging Corollary 2, 3 and Lemma 4 into The-orem 1, we provide a bound on the cumulative number of mistakes, E , which the Conservative OVA with PA algorithm makes.
 Theorem 2. Assuming that for a sequence of samples, ( x and || x t || X  R , in each round, we maintain a classifier f , where f t =( f 1 t ,f 2 t ,...,f lt ) . And with a coding matrix M  X  X  X  1 , 0 , +1 } k  X  l as in Eq.(7) and loss function L , let  X  be as (13), the cumulative number of mistakes made by the Conservative OVA(one-versus-all others) Reduction with Online Passive-Aggressive Algorithm satisfies: for basic PA model, for PA-I model, for PA-II model,
In linearly separable cases, by the definition of u s and l , we find that there exist u s , s =1 ,...l which makes the corresponding l  X  st equals 0 . Thus the cumulative mistakes made by the Conservative OVA with PA Algorithm would only be a function of u s . Therefore, the cumulative error rate would approach to 0 as the number of samples T goes to infinity. Here we see a very pleasing theoretical results of our algorithm.

In our method, we deal with samples in a consecutive manner. For each sample, k binary classifiers are maintained, so our method would update at most k binary classifiers. With regard to each binary classifier, the updating procedure takes a closed form and takes time linear to the feature dimension d , no matter which Online Passive-Aggressive Al-gorithm is employed. Thus our method takes at most O ( kd ) time for each sample, which leads to a time complexity linear to sample size n .
 we choose data samples with only one of the highest four topic codes (CCAT, ECAT, GCAT and MCAT) in the  X  X opic Codes X  hierarchy in the data set. This process constructs data size of 704877 from the original 804414 samples, vectors of which are cosine-normalized, log TF-IDF vectors.
MNIST : This is a database of handwritten digits, which had been size-normalized and centered in a 28  X  28 image. The formal dataset maintains 60000 examples for training and 10000 for testing. For our usage, we simply combine these two into onedataset consisting of total 70000 examples, vectors of which are the gray value of pixels and scaled to [0 , 1] . 20 Newsgroups : This dataset collects nearly 20000 news-group documents categorized by 20 different newsgroups. We use the whole dataset including 19928 examples, vectors of which are scaled to binary encoding.

USPS : This is an optical character recognition dataset with 9298 samples. Samples are digits of 10 different classes, vectors of which are of 256 dimensions.
 C. Results and Discussion
Figure 1, 2, 3 and 4 show the experimental results on four real world datasets mentioned above. The parameter  X  of Banditron for RCV1-v2 is set as 0 . 03 , for MNIST as 0 . 15 , 20 Newsgroups as 0 . 3 , USPS as 0 . 3 . For each figure, the left subgraph represents the cumulative error rate of four algorithms, Multiclass Perceptron , Banditron , Conservative OVA with PA-I and Realizable Offset Tree with PA-I , on consecutive samples of each data set; the middle one describes the cumulative error rate of Conservative OVA Reduction scheme with three kinds of PA algorithms as base classifiers; the right subgraph depicts the cumulative error rate of Realizable Offset Tree reduction scheme whose base learners are achieved by three kinds of PA algorithms.
Table I shows the final cumulative error rate of each algorithm on each dataset.

From the experimental results on four datasets, we can see that 1) For online multiclass prediction with bandit setting 2) Our algorithm performs far better in contrast to Ban-3) Though on RCV1-v2 dataset, the result of Realizable
