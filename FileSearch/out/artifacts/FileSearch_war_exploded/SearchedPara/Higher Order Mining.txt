 The value of knowledge obtainable by analysing large quan-tities of data is widely acknowledged. However, so-called primary or raw data may not always be available for knowl-edge discovery for several reasons. First, cooperating insti-tutions that are interested in sharing knowledge may not be willing (or allowed) to disclose their primary data. Second, data in the form of streams are only temporarily available for processing. If stored at all, stream data are maintained in the form of synopses or derived, abstract representations of the original data. Finally, even for non-stream data, there are limits on the computation speed to be achieved  X  such limits are set by hardware and firmware technologies. This problem can only be partially solved through parallelization and increased processing power. Ultimately, in many cases data must be summarized to be processed efficiently. In the light of these observations, we anticipate the need for defining and practising data mining without the luxury of primary data. To that end, we formally introduce the paradigm of Higher Order Mining as a form of data mining that is applied over non-primary, derived data or patterns. Although Higher Order Mining is a new paradigm, there are already research advances on knowledge discovery methods from patterns rather than data. We discuss them and orga-nize them under the light of the new paradigm. We show that the HOM paradigm reveals further potential for knowl-edge discovery, including the delivery of rules and patterns with semantics that are closer to human intuition and are thus more appropriate for human inspection.
 Higher Order Mining, Mining Patterns, Mining Data Syn-opses, Mining Derived Data, Rule Semantics. Traditional data mining involves the analysis of data stored in files and data warehouses. The Higher Order Mining (HOM) paradigm extends this to the analysis of patterns rather than just from primary data. We are motivated by the fact that in many applications primary data are either not available at all or for only a limited time period. HOM generalizes the idea of data summarization, which is broadly exploited in real-time stream mining.
 In this paper, we discuss the potential of HOM for knowledge discovery, enumerate the challenges associated with HOM and elaborate in more detail on the issues of interestingness and performance. HOM encompasses data mining methods that operate upon derived, i.e. non-primary, forms of infor-mation. These may be: HOM subsumes the notion of meta-learning, as suggested by, among others, Prodromidis et al. [59], where a classifier is trained upon the output of classifiers rather than upon the primary data. The ultimate goal of data mining is to deliver, in a timely manner, results that support scientific insight and contribute to well-informed decision taking. The data used for data mining are typically assumed to be primary data captured by some application, cleaned and prepared according to the demands of the mining algorithm, and occasionally enriched with further, imported data, which may be alien to the orig-inal application. For example, consider the extraction of preference profiles from customer transaction data. Such primary data may be enriched with demographic data im-ported from a statistics agency. These imported data will be most likely aggregated, for example at the level of city block. To ascribe them to the primary data, some mapping function is needed, which can be used to infer, for instance, the profession of a customer, given the distribution of pro-fessions in her city block. Data mining is then performed upon the primary data, not the aggregated data.
 In some applications that demand data mining, no primary data are available. This situation emerges whenever insti-tutions recognize that data sharing will lead to better min-ing results and thus to better insights, but are not willing or not allowed to share their primary data. Such institu-tions may be companies that participate in product or mar-ket alliances, governmental agencies that derive profiles to deal with national security threats, research institutions and health departments that aim to gain insights on diseases and treatments and to maximise health outcomes while min-imising costs. One concrete example, fraud detection, is discussed by Prodromidis et al. [59] who point out that if multiple banks share information, their composite predic-tor is likely to be better than any one built from local data only. Since privacy and confidentiality considerations pro-hibit the disclosure of the data owned by each institution, the challenge is tackled by training a classifier upon the out-put of local, black-box classifiers. This is an early example of Higher Order Mining, i.e. data mining over classifiers rather than raw data.
 In other applications, primary data are only available for a short time. A characteristic example is stream data, which are not stored at all  X  they are encountered, processed in real time and deleted [26]. Guha et al. [32] describe the stream paradigm for mining demonstrating the gradual replacement of primary data with aggregates, in their case summarized data. In their k -means stream clustering algorithm, data are buffered for clustering, and primary data are gradually and continuously replaced by the centroids of the clusters they belong to, so that buffer space is freed for new incoming data. As soon as primary data are replaced by a cluster centroid, data mining is performed on data + patterns, an example of Higher Order Mining.
 Even if the data are not presented as a stream, they can-not be stored indefinitely. The removal of primary data may be due to legal obligations, which determine how long a data record may be retained, self-obligations (such as a sensitivity to Freedom of Information requests) or internal regulations about storage management. While data sum-marization techniques are usually applied upon streams, the summarization of transactions as part of a long-term data maintenance process is less usual. In all cases, long-term maintenance and an a posteriori analysis of the aggregated information is rarely foreseen  X  it would require algorithms that perform data mining upon aggregated rather than pri-mary data. This paper is structured as follows. In the next section, we introduce the concept of Higher Order Mining in a more formal way and provide a taxonomy of related literature. A discussion of advances in HOM from different perspec-tives, including spatiotemporal mining, stream mining, pat-tern change detection and meta-learning is presented in Sec-tions 3 and 4 with the first providing a discussion on HOM from a static, non-temporal perspective and the second from a temporal viewpoint. In Section 5, we explore how the HOM concept can contribute to some outstanding problems of conventional data mining and discuss the potential for HOM in creating rules with higher, in some cases more use-ful, semantics. We also elaborate on the interplay of HOM and of privacy-preserving data mining , where knowledge dis-covery without access to primary data is also necessary. Sec-tion 6 concludes with an agenda of open issues. We define Higher Order Mining as knowledge discovery upon previously induced patterns and models. To some degree, any kind of derived information, such as the average value of an attribute or a histogram over an attribute X  X  values, can be perceived as a simplistic pattern. We opt for a tighter definition as follows: More formally, let  X  = {  X  i | i = 1 . . . n, n  X  1 } be a set of models or patterns, such that  X  i has been extracted from a dataset D i . The datasets D 1 , . . . , D n may be distinct or identical, may correspond to a vertical or horizontal parti-tioning of a distributed dataset or may be snapshots of a data stream. Higher Order Mining encompasses the discov-ery of any new pattern or model b  X  from the set  X  through the use of data mining methods.
 Using this definition, we can describe some of the current research as follows: While most data mining research has assumed primary data, there have been a number of research advances that ad-here to the HOM paradigm. We present two taxonomies and identify research advances for the individual taxa. An algorithm-oriented taxonomy is presented in Table 1. The vertical axis encompasses the mining techniques that are applied on the primary data, delivering different kinds of patterns/models. In the horizontal axis we list the mining algorithm applied upon the patterns or models. For each cell, we provide some example research contributions that indicate the wide spectrum of HOM perspectives.
 In Table 1, we do not distinguish between conventional data and stream data, nor among different data types of data, such as text, spatial data or clickstreams. In Table 2, we draw a fundamental distinction between temporal and non-temporal data. The temporal category encompasses both algorithms designed explicitly for the demands of streams and algorithms that process accummulating (and possibly ageing) data in predefined snapshots. The algorithms of the non-temporal category compare models/patterns upon datasets drawn in an independent process. Quite naturally, this last category of algorithm concentrates on pairwise com-parisons for dissimilarity detection.
 In the category Spatiotemporal data in Table 2, we have as-signed algorithms that make explicit use of the metric space, such as by studying moving clusters [36] or by referring ex-plicitly to geographic objects, such as cities [55]. In Section 3, we elaborate on research related to Higher Or-der Mining for arbitrary static data. Then, in Section 4 we discuss advances in temporal Higher Order Mining. Studies in spatiotemporal data, as depicted in the middle column of Table 2 are also discussed in this section. In this section we discuss the discovery of higher order static patterns while Section 4 concentrates on methods that mon-itor and identify changes in evolving patterns. A number of problems lend themselves to using association mining techniques over non-primary data. Many of these relate to the non-discreteness of either the data or their se-mantics. We focus here on the use of clustering as a prepro-cessing step to association rule discovery and its objective of formulating appropriate data intervals or object groups, upon which association rules of adequate frequency can be built. A significant challenge for association mining is to generate rules from data that contains quantitative or time-series at-tributes [61]. An example of a quantitative association rule is . . . if a customer buys two, three, or four toothbrushes, then he/she also buys between three and six tubes of tooth-paste [72], which can be represented by the rule: Unless attribute values are aggregated there may be a sig-nificantly increase in the number of rules, which can in turn both increase the difficulty of interpretation and re-duce the chance of individual rules reaching the required support threshold [52].
 Attempts at aggregating values during the mining phase have suffered from a range of difficulties. The predefined static allocation of ranges can be beneficial for data sets where the user has previous knowledge of what will prove to be meaningful. However, in some cases the distribution of data can favour the border between two predefined ranges and static groups can effectively hide an interesting distri-bution. For example, given age ranges of: a disease such as meningococcal (which is most prevalent in children under 5 and young adults 15-24) may go undetected in the latter range.
 There are complexity issues in discovering quantitative rules, with conventional algorithms often sacrificing precision to make the problem tractable [72]. As stated by Fu et al. [25], the idea of semantic distance cannot be accurately described by the imposition of static ranges. There are three possible solutions to this problem: 1. Aggregate the data as a preprocessing task, 2. Combine the clustering of values with the production 3. Cluster the association rules in the result set based on Aggregating values into groups effectively .. .transforms the quantitative problem into a boolean one [25], from which as-sociation rules can be elicited.
 Since clustering algorithms are intended to find dense dis-tributions of data, they are capable of discovering partitions that can provide an improved fit to the distribution of data and thus reduce the chance of missing important associa-tions [25]. The interpretation of intervals is a difficult issue. Miller and Yang [52] contend that partial completeness mea-sures may allow for neighbouring partitions to merge based on ordering and support, without indicating any kind of se-mantic distance between the values contained in those par-titions. The level of measurable semantic distance required determines the nature of the clustering solution possible. Both Miller and Yang [52] and Fu et al. [25] are able to use the resulting cluster information to generate a set of intervals which can then be used in association rule mining. Fu et al. [25] solve discretisation issues pertaining to values that are more ordinal in nature. They use CLARANS to dis-cover a set of medoids that determine the mid-range values from which partitions are extracted. As partitions extend from a mid-point they are expected to overlap. This use of fuzzy sets allows for items to be members of two partitions based on partial membership.
 Compactness and separation can be calculated not only be-tween clusters, but can also be combined to form a single metric to compare different clustering runs over a data set. Gyenesei [34] allows the user to specify a range of possible values that are iteratively used as parameters to a c -means algorithm, after which the results are compared to assess the number of clusters that will provide the highest quality clus-ters. A consequence of this is the inherent time complexity in running the clustering algorithm repeatedly with differ-ent parameters. Due to the relative complexity of mining multi-dimensional quantitative attributes through previous algorithms which do not use clustering, it is difficult to mea-sure whether the trade off in time complexity would prove more effective than if a different preprocessing scheme were used [72]. The input of the possible range of clusters belies the necessary user requirement to determine some measure of distance, or in a more general sense, what possible values could provide the most meaningful numerical ranges. Other work includes the SemGrAM algorithms [60] that utilise semantic graphs during association mining to form disjunc-tive itemgroups (consisting of semantically similar items) within conjunctive itemsets, such as: In contrast to discovering numerical ranges for ordinal val-ues, Miller and Yang [52] ensure that the semantics of con-tinuous interval data are preserved. Using BIRCH , clusters are capable of being identified over a single pass, which is sig-nificantly more efficient than previous clustering algorithms [77]. This advantage is significant when the clusters are be-ing generated as a preprocessing task.
 The distance-based association rules created by Miller and Yang [52] can be directly elicited from the clusters gener-ated. Instead of using a set of medoids to determine ranges, the clustering features produced by BIRCH provide sufficient information to produce association rules that are the com-bination of multiple dimensions. Moreover, instead of the more simplistic measures of support and confidence to deter-mine the degree of association, Miller and Yang [52] suggest a measure based on the distance between two clusters as a means of determining how strong an implication is  X  the larger a distance between two clusters C x , C y , the weaker the implication of C x  X  C y .
 In studies over both ordinal and distance-based data it is clear that user influence over parameters is required to pre-serve semantic representation  X  for ordinal data it is the de-termination of how many ranges will be informative, whereas for intervals it is the maximum allowable relative distance. The formulation of groups via clustering is a powerful com-plement to traditional model learning or pattern discovery. We elaborate here on clustering of patterns and on clustering inside patterns. In some cases a set of association rules can be difficult to manage or understand [66]. A set of rules of the form might be better described as Lent et al. [44] formalise the issue of clustering association rules using a technique that is similar to the adjacent parti-tion (or bin) merging algorithm of Agrawal and Srikant [3], however, the Lent et al. algorithm is also capable of merg-ing consecutive values or bins into larger entities. As the algorithm can handle either intervals or values it is possi-ble to input either into the binning algorithm. Each bin in the matrix represents an association rule in which the consequent is bound to an individual cell.
 Gupta et al. [33] extend this work by looking at distance based clustering of association rules while Perrizo and Den-ton [57] outline a framework based on partitions to unify various forms of data mining algorithm.
 This method can be contrasted against clustering as a pre-processing task, as the clustered representations are required to be frequent, therefore an iterative process is undertaken to ensure that the discovered clusters are sufficiently fre-quent enough to allow for subsequent association rules to be derived from them. One of the most useful domains for clus-tering association rules has been for explorative mining [67], in which a user requires on-demand expressive rules that are easily readable  X  in many cases it would be infeasible to do this as a preprocessing task. Partition-based clustering algorithms aim to produce clus-ters that have high intracluster similarity and low interclus-ter similarity. They are iterative in that their centres are based on their average item membership distance, which will continue to shift until it reaches convergence, according to some criterion. A set of initial points are required at which to allow the mean to be initially calculated, and allow the iterative process to begin.
 As illustrated by the work of Bradley and Fayyad [11], if the starting point is sub-optimal, clustering algorithms which use iterative refinement will in many cases reach sub-optimal solutions. They contend that an improved local minima can be discovered through performing clustering over subsam-ples and then through clustering the combination of solu-tions arrive at an improved starting point. This form of re-finement allows for even a reasonably large subsample (10% of overall data set) to be run in a fraction of the time taken to cluster the entire set of data. If the clustering process has a sub-optimal starting point, it will take longer to reach convergence. The smoothing process reduces the outlier sub-sample solutions and gives a a greater chance that the sub-sequent clustering algorithm will reach a good solution [11]. Associative classification uses association rules to build a classifier [48]. The association rules must have the conse-quent bound to the class label, hence are considered class association rules (CARs). Once a set of CARs are gathered, they are ranked, with the highest ranked CARs used to cre-ate classifiers. Associative classification has two primary benefits: 1. Many preprocessing solutions used within the associ-2. Freitas [24] points out that association rule mining It can be argued that CARs are more understandable to the domain expert who is required to verify the reasoning be-hind a classifier X  X  choices, which has been a significant short-coming of previous rule elicitation techniques when building classifiers [56].
 Associative classification can be separated into two areas, the first involves the generation and ranking of association rules, with the other performing rule selection and classifier construction. Conventional association rules use a variety of methods to determine which rules are interesting and statistically sig-nificant within the chosen data set [31].
 Clearly, rules must be pruned and ranked during the CAR generation process to reduce the volume of rules generated, and to allow for an informed selection procedure to build the classifiers from the most highly ranked rules. One procedure to evaluate CARs is to rank them in the order of confidence, support, and precedence, respectively [48], also termed the most-confident-first (MCF) principle [70]. Given a prioritised set of CARs, a classifier is created based on the set of rules that cover the training set [48]. The classifier is iteratively built where, if a rule covers some set of examples in the training set, it is added and those pertinent examples are removed from further rule evaluations. Each rule, therefore, only covers a particular set of examples, and hence only rules that increase the accuracy of the classifier are included. The default class is determined as the rule which covers the largest number of examples.
 Due to the CAR set being sorted, it is often the first rule to cover the set of examples that will be the most accurate. It should be noted that the best judge of a rule is not sim-ply the set of examples it successfully classifies, but also the proportion that are classified incorrectly. Pessimistic prun-ing may remove some rules with a high error rate during the CAR generation phase, however a lower ranked rule may, upon comparison with other higher precedence rules, offer an improvement to accuracy in the form of a reduced er-ror rate. This error checking is important to avoid issues relating to over and under fitting.
 It is difficult to determine which will be the most appropri-ate rule to determine a class based on precedence and error rate. In contrast to a single rule determining the label, it is possible to use a group of high precedence rules to build a classifier [47]. This has the benefit of reducing the error rate due to the possible subset of examples identified incorrectly by a single high precedence rule being covered correctly by a lower precedence rule if it were in the same group, on the proviso that the correct rule from the group is selected to cover each set of examples. When a new object is classified, the label is determined by the most representative rule. There is an inherent limitation in using only positive associa-tion rules to build classifiers  X  items not positively correlated with a particular class label may prove to be the deciding factor in respect of which class label an object should be considered [6].
 ARC-PAN (Association Rule Classification with Positive And Negative) uses both positive association rules and nega-tive association rules in the same manner, allowing mul-tiple ranked rules to be used much in the same form as the algorithm presented by Li et al. [47] with the strength of a group of rules being determined by average confidence. There are two complementary ideas presented in an effort to reduce the error rate through multiple rules: 1. Additional information provided through negative rules 2. A new measurement with which to assess the best Using multiple rules for the purposes of prediction provides more accurate classifiers. Yin and Han [74] identify the use of multiple rules as crucial due to the lack of full accuracy possible in determining the effectiveness of a rule. Further-more, as with the strength-based measurements, a single rule will cover the majority class only. The temporal perspective of higher order mining is man-ifested in frameworks that detect changes upon data that come from a non-stationary distribution, in pattern man-agement suites and, obviously, in temporal mining methods. Many approaches in this area assume that the data change but their schema does not. However, there are also advances on dealing with pattern evolution when the schema changes as well. Ganti et al. [27, 28, 29] proposed an early framework for the detection of changes in patterns to identify and quan-tify the differences between patterns drawn from different snapshots of the dataset. These snapshots may refer to data selected at different timepoints or at different locations (spatial data). The framework encompasses three modules, FOCUS, DEMON and CACTUS.
 FOCUS [28] compares two models and computes an inter-pretable qualifiable deviation measure between them. For the comparison, Ganti et al assume that a model consists of a structure component and a measure component . For example, the structure component of a decision tree is a representation of its nested nodes, while the measure com-ponent captures the distribution of classes in each node. To compare two patterns, the patterns are decomposed and in-teresting regions are identified. These are regions where the two patterns disagree in the class distribution. The subset of data belonging to each region are then summarized by the measure component.
 FOCUS can be coupled with DEMON [29], the module which monitors data change across the time axis. DEMON distin-guishes betweeen systematic and non-systematic changes in the data and identifies the data blocks that must be pro-cessed to build new patterns. Finally, CACTUS summa-rizes clusters into synopses of their statistical properties [27]. Thus, instead of comparing data subsets, pattern compari-son can be performed on their summarized representations. Another framework for pattern comparison and the detec-tion of differences has been proposed by Bartolini et al. [9] for the PANDA framework. Similarly to FOCUS, com-plex objects (patterns) are decomposed into simpler ones, for which similarity/distance functions are available. The decomposition of complex objects, the computation of dis-similarity scores for their components and the aggregation of these scores into scores for complex objects are governed by some aggregation logic . Aggregation logics can be speci-fied by the PANDA user, albeit some built-in logics are also available within the framework. The mining of association rules over changing attribute val-ues is a complex challenge due to the dimensionality and semantics of the data involved. Wang et al. [71] define an evolution as the temporal changes of attribute values of some object .
 To discover temporal associations the temporal dimension must first be divided into subsequences (snapshots in this model) much like the algorithm presented by Das et al. [20]. Due to the large number of dimensions that an object may have, it is sometimes necessary to mine a subset of the pos-sible dimensions. This idea of subspace clustering is covered by Agrawal et al. [4], who describe the discovery of clus-ters over dimensions that are derived from only a subset of the possible dimensions based on the identification of which dimensions primarily contribute to noise, and those dimen-sions in which there may be particular interest.
 Wang et al. [71] discretise the quantitative attributes into small intervals. It is then possible to cluster these intervals to discover the ranges for each subsequence. To determine if the occurrence of an object is frequent enough, we must use a density metric to ensure that sufficient objects follow this evolution, which can be expressed as a ratio of the number of objects that contain values within the interval. Support and strength measures are used to ensure that a significant num-ber of objects follow the evolution across the subsequences, so that a rule such as: represents the evolution of a significant proportion of objects within the data set. It is evident that through the combina-tion of many first order concepts we can achieve these higher order relationships. Wang et al. [71] introduce a measure of information loss by initially discretising intervals with equal distances to reduce the inherent complexity of combining multiple algorithms.
 Harms et al. [35] offer a similar solution. Upon the discovery of frequent episodes (as opposed to shapes), sequential rules can be built of the form: with both  X ,  X  being frequent within their respective window win , and  X  occuring for lag after  X  . While support for these rules follows the calculation as for normal association rules, confidence is measured by the conditional probability that  X  occurs, given that  X  occurs, under the time constraints specified by the rule . Window width is specified by the user, as the duration as to what will be an interesting time metric is variable based on the environment.
 Au and Chan [7] propose a different implementation. Asso-ciation rules are still elicited from sequence data, however a decision tree is built over the collected association rules to allow for prediction based on the present set of rules discov-ered. The confidence measure used is more complex than Harms et al. [35], with confidence being determined as: with D j being the set of transactions for time period t j One of the important issues with trend prediction raised by Au and Chan [7] is the fact that rules that do not reach the necessary support and confidence measures will not be returned by an association rule miner. Fuzzy meta-rules are elicited from these association rules by measuring the change of support and confidence between contiguous sequences and quantifying them through simple linguistic terms such as fairly decrease . The fuzzy meta-rules are built using a fuzzy decision tree which effectively establishes branches based on information gain. New occurrences can then be classified based on traversing the tree in a manner that offers the path of best fit (meets the conditions of that leaf). The concept of fuzziness is required due to the lack of a crisp sense of membership; the linguistic terms used to quantify rules allow for this degradation in precision. To discover the meta-rules that make up a decision tree the tree is traversed from the root to the respective leaves.
 A pendant to trend prediction with classification algorithms is the use of clustering upon a temporal sequence to iden-tify segments that show the same trend. Das et al. [20] partition a temporal sequence into contiguous subsequences of equal length. Once this subsequence size is specified, subsequences can be clustered into basic shapes such as increase (1), decrease (1). Once this collection of shapes has been derived, it is possible to discover associations that are linked to these basic shapes. The adaptation of the associa-tion rule algorithm uses the following notion of confidence: where F is the frequency of A following B within T . Harms et al. [35] follow a similar methodology in separating a se-quence into subsequences based on a sliding window, over which episodes may be derived.
 Das et al. [20] elaborate on the importance of selecting proper clustering parameters to ensure that the shapes identified do not contain too many or two few subshapes, as they are required to be applied repeatedly to detect time-based associations. One of the shortcomings of this approach is that tuning clustering algorithms based on previous results is based on the assumption that previous information was correct [39]. A parameterless algorithm may prove more ac-curate due to the possible range of errors that may arise through a user X  X  prejudices, expectations, and presumptions . Similarly to association rules, clusters may also exhibit trends of different types. A cluster may shrink or expand, it may split into further clusters or merge with another. Cluster transition detection on the time axis is performed by the MONIC framework [64]. Next to a cluster transition model , which lists that types of change that may occur on a cluster, MONIC also encompasses an algorithm that detects external transitions  X  transitions involving multiple clusters, and a set of heuristics that detect and quantify internal transitions inside a cluster, such a change in size or homogeneity. Aggarwal adopts a different perspective upon cluster change [1]. A cluster is a densification of data in a multidimensional space, where a dimension is a feature. Intuitive examples for this type of cluster definition can be obviously found in Geographical Information Systems. A town is an area where houses are more dense than in the surrounding areas. The evolution of the town (e.g. growth or shrinkage) can be modeled as a density change. Aggarwal approximates clus-ters with kernel functions and then computes kernel den-sity changes at each spatial location. Density changes at neighbouring spatial locations are aggregated, so that one can detect areas which change at different speed than their surroundings. For each location at timepoint t , the forward density change captures kernel density changes in the future of t , while backward density change captures density changes in the past of t . The difference between forward and back-ward density change constitute the velocity of change [1]. Kalnis et al propose a special type of cluster change, the moving cluster , whose contents may change while its density function remains the same during its lifetime [36]. They find moving clusters by tracing common data records between clusters of consecutive timepoints.
 The work of Aggarwal [1] and Kalnis et al. [36] refer to clus-ters as spatial objects. For many applications, this cluster representation is not intuitive. Particularly for text clus-tering, one is inclined to derive cluster labels as summaries for the cluster contents. Aggarwal and Yu [2] introduce the notion of a droplet as a summary of a cluster over a text stream. A droplet consists of two vectors, one containing all words appearing in the underlying cluster and one contain-ing all pairs of co-occuring words. The words in the first vector are weighted  X  older records of the stream have lower weights.
 Aggarwal and Yu [2] use the notion of droplet to monitor cluster evolution. A new document is assigned to the cluster to which the droplet is most similar. If a cluster does not receive documents for some time, it becomes inactive . If a document cannot be assigned to a new cluster, it becomes itself a cluster, thereby replacing the oldest inactive cluster. This approach assumes that the number of droplets (top-ics in a stream) are fixed and that old ones decay and are replaced by newer ones.
 Cluster trends over a text stream are also addressed by Mei and Zhai [50]. Again, the focus is on detecting topics and studying their evolution. Mei and Zhai perform probabilis-tic clustering, in which each document contributes to each cluster with a probability that increases with its similarity to the cluster X  X  mean. A topic is the mean of a cluster and consists of the words characterizing the cluster X  X  members. For trend monitoring, Mei and Zhai connect clusters that appear in the time axis and have similar topics. This re-sults to a topic evolution graph , where a sequence of clusters constitutes a (persisting) theme . Pattern management is an orthogonal task to data mining and to higher order mining. However, for the higher order mining paradigm, we should keep in mind that the analysis of patterns presupposes pattern maintenance.
 Some systems store previously mined rules for the purpose of performance improvement through incremental mining [42, 65]. It is computationally expensive to continuously rescan the entire database for rules when at a small cost in terms of accuracy, adequate results can be obtained through sampling [43], or by reanalysing only when the difference be-tween the rules stored and the actual information held by the data differs significantly [42].
 Pattern maintenance frameworks pay much attention to the management of association rules. Spiliopoulou and Roddick [63] provide a framework for modelling higher order associ-ation rules as temporal sequences of conventional rules ob-tained from different mining sessions. Mining sessions are defined as a 6-tuple, providing a signature to which higher order reasoning algorithms can refer. Higher order mining routines are then able to operate over temporal sequences of rulesets. The followup system PAM [8] studies these se-quences to detect changes in the statistics of the rules in the ruleset under observation.
 Association rules X  management is also offered in PANDA [9] and in PSYCHO [49], where some emphasis is also paid in the maintenance of decision trees and of clusters. A special-ized system for Web pattern monitoring is proposed in [17]. The system NaviMoz is designed for navigation patterns and pays much emphasis on how the habitual behaviour of users can be reconstructed through pattern querying and inspec-tion. A generalization effort for arbitrary patterns has been proposed by Meo and Psaila [51]. The pattern management model XDM captures data and patterns and builds upon XML as basis for a unifying framework. Data mining techniques have been successfully applied in a number of diverse application domains including health and medicine, biotechnology, natural sciences, telecommu-nications, commerce, financing and marketing, defence and national security. The diversity of applications and current trends in data management have exacerbated or even given raise to several still unresolved issues. As pointed out in Section 1, some modern applications require data analysis without primary data or without the possibility of main-taining and scanning the primary data more than once. In this section, we discuss how Higher Order Mining can con-tribute to solutions for conventional but complex applica-tions of knowledge discovery and we elaborate on the asso-ciated challenges that are yet to be solved. While advances in data mining encompass very powerful al-gorithms, there are fewer advances on driving the knowl-edge discovery process towards results appropriate for hu-man consumption. For example, association rules X  discov-ery methods transform a very large dataset into a very large ruleset that is human understandable but still brings a con-siderable cognitive load for the human expert. The ap-proaches proposed to this challenge can be categorized into: The aforementioned methods refer to the organisation of the results. Some methods or measures are incorporated to the mining algorithm to ensure the generation of a human-consumable set of results, while others are rather prepro-cessing filters upon already derived results. However, the study of patterns reveals that experts are often interested in specific higher order patterns and need means for describing them in advance. For example, an analyst that designs a product portfolio may be interested in competitor products or products that churn with each other. An analyst that studies customer profiles is likely to care about shrinking profiles that require special attention and growing profiles that indicate a new exploitable market segment. The first example refers to patterns that have some special property, while the latter refers to patterns whose evolution exhibits a pattern  X  i.e. to Higher Order Mining patterns. Example of such patterns are given in Appendix A.
 A promising approach towards specifying the meta-patterns to be exhibited by the resulting patterns comes from the paradigm of database mining. Advances in inductive databases and in (multi-)relational database mining include query lan-guages and engines with which the knowledge discovery pro-cess can be driven by the human expert. The query language of PANDA allows the juxtaposition of patterns with data [9], the query language of PSYCHO supports the detection of trends in patterns [49], the ConQueSt system supports the incorporation of constraints to database mining [12], while the system of Bonchi et al. [10] integrates mining algorithms into the query optimiser. Although much of the research on Higher Order Mining is performed on data that are not stored in a database and without exploiting the potential of database querying facilities, the recent research on pat-tern comparison and pattern trend discovery in a database context indicates that powerful query languages are already being made available to human experts for Higher Order Mining. Another open area is the development of pattern languages that can describe data mining patterns. Such pattern languages have been used successfully in other areas such as in describing software patterns [19]. Conventional set theory generally only considers a crisp set methodology, whereas in many real world domains that level of precision is not available [41]. Cases such as first-order association rules not meeting thresholds for second-order mining [7], or partitions of interval data not having clear boundaries [52], imply the need for some form of support for fuzziness that cannot be adequately handled based on the rigid constraints of set theory.
 Since the accuracy and completeness of information cannot be assumed, due in part to the human element involved in all data handling and the propensity to assume that data stored in a database is accurate, it is evident that higher order mining may be particularly susceptible to the constraints imposed by classical (crisp) sets.
 One of the primary methods of dealing with fuzzy sets in a data mining context is to use more generalised terms to de-scribe attribute values. In terms of human readability and comprehension the use of general linguistic terms is ben-eficial in many contexts where precision is not paramount. Fuzzy sets are also beneficial for the purposes of spatial or in-terval data, where hard partitions of quantitative data may imply a significant difference. Privacy is a non-trivial issue if data mining is to gain general acceptance and a variety of approaches have been suggested. While the issue is too large to discuss in detail here (qv. [5, 18, 21, 30, 38, 69, and many others]), it is worth noting that rulesets have a level of anonymity built in as a natural consequence of the mining process.
 The demand for privacy-preservation in data mining emerges in two different, though often related contexts. First, per-sonal data must be protected from disclosure towards ev-eryone . Second, confidential data must be protected from disclosure towards partners . The first case encompasses all kinds of sensitive data, the analysis of which can lead to useful insights, such as understanding the progress of ill-nesses, the effects and adversary effects of drugs, the pref-erences for products and the anticipated lifetime value of customers. While there are agents that legitimately express those needs, this does not imply a legitimation for disclo-sure of the primary data themselves. The second case refers to data sharing among institutions, whose internal data are confidential (and occasionally subject to the constraints of the first case), but who recognize that integrated data min-ing is of benefit to all participating institutions. There are a number of situations in which rulesets can be made publicly available but the source datasets cannot. For example, in the case of horizontal partitioned data, Kantar-cioglu and Clifton [37] point out that rules mined from a particular source may put the provider at a disadvantage if they are unique to that provider and are sensitive in na-ture. Using their method of encryption and rule passing the algorithm collects the set of rules and passes them on to the next repository in a secure fashion. Similarly, Vaidya and Clifton [68] create an association rule mining algorithm which can operate over vertically partitioned data in a pri-vacy preserving way. Rozenberg and Gudes [62] offer an improved algorithm which reduces the chance of disclosure through the use of fake transactions, together with a cost-benefit algorithm which assesses the risk of information dis-closure during the confidence measuring process.
 Two groups of privacy-preserving approaches have emerged in the literature to deal with these challenges. The first group encompasses anonymization methods, which (a) dis-tort the primary data and (b) aggregate them into derived data, from which private information cannot be reconstructed any more. These anonymized, derived data are published -occasionally in the form of publicly accessible files, but also in the form of queryable databases. The published data can then be processed with conventional mining methods. Advances of this category deal with the first case of privacy-preservation.
 The second group encompasses interaction and processing protocols that allow the partners to build a shared data mining model without sharing the primary data. This cor-responds to the second case above. In a nutshell, privacy-preserving algorithms of this category specify how the data used to build the model should be encrypted and shared among the partners. The output model is based on the original data rather than a distorted version. However, sim-ilarly to the first group, the data being encrypted are not primary but rather aggregates, mainly in the form of a sum of a set of values.
 Our Higher Order Mining paradigm is orthogonal and com-plementary to the privacy-preservation advances of the sec-ond group, but contributes to the same problem. As Pro-dromidis et al. [59] have shown, a common model can be es-tablished by sharing and mining patterns rather than data. For some applications, this level of sharing may be appro-priate, more so if the patterns themselves can be treated as black boxes [59] -the confidential data are not shared but the model is built. For other applications, a privacy-preservation protocol may be needed. As Kantarcioglu and Clifton [37] point out, the association rules contributed by one partner may disclose sensitive information if they are unique to that partner. The same authors have provided a privacy-preserving solution to this problem [37]. Hence, Higher Order Mining for shared patterns can be enhanced with a privacy-preserving protocol if the application de-mands so. Li et al. [45] put forward a method to mine data across multiple databases, each with its own schema. Rules may be generated across these heterogeneous schemas on the proviso that there is a unique key for each dataset that may join these complex sets of data together. Previous work in the field has focused on horizontally or vertically distributed databases, where the global schema has been known [37, 62, 68]. Horizontal partitions involve multiple sites containing a set of the tuples which make up the global set. Vertical partitions contain relations which can be joined to provide information or to create a global schema, or that contain some elements of a transaction [68]. In either case each of these relations or tuple sets can be owned by a separate entity, or may have been distributed for the purposes of performance improvements.
 Previous work assumes some level of global schema knowl-edge, however in some cases this may not be possible. Zhang et al. [76] consider the schema differences between a set of distributed databases as one of the largest challenges facing traditional data mining methods. Heterogeneous records are records stored at different sites which each have data for different set of features possibly with some common fea-tures among sites [46]. The level of record heterogeneity and privacy required for a particular database will deter-mine the nature of the implementation. In an environment where privacy is not a significant issue, they use a similarity-based distributed data mining framework (SBDDM) to in-tegrate the tables into a set of virtual tables based on simi-larity. Distributed data mining techniques are then applied to these virtual tables. One of the significant limitations of this virtual table method is that it may result in a sig-nificant communications overhead for databases with large number of transactions or that the merging process may simply prove infeasible for systems with extremely hetero-geneous data sets [75].
 Whether it is possible to use higher order semantics to find relationships between highly heterogeneous data sets [45, 58] is an open question. In many cases mining routines are beginning to encounter problems as the volume of data requiring analysis grows disproportionately with the comparatively slower improve-ments in I/O channel speeds. That is, many data mining routines are becoming heavily I/O bound and this is lim-iting many of the benefits of the technology. Methods to reduce the amount of data have been presented in the lit-erature including statistical methods, such as sampling or stratification, dimension reduction by, for instance, ignoring selected attributes, and incremental maintenance methods, that analyse the changes to data only [16].
 The I/O channel speed problem has meant that in many practical situations, even an algorithm of linear scalability up can show inadequate performance. A large retail, web search or telecommunications company can easily produce multi-GB transaction files each day. Annually, transaction files can exceed many TB. A single scan through such data can take two or more hours, even on high end servers. By comparison, even with support levels set low, the combined size of 365-odd itemsets (i.e. one for each trading day of the year) can be in the MB range. While higher order mining has already been used in a variety of ways, the identification of HOM as a distinct subfield would be useful as a mechanism for comparing approaches and identifying open areas for research. Our taxonomy in Table 1 shows that there are areas where little or no overt research has been done to date which might prompt further investigation.
 Higher order mining requires a change in perspective for knowledge discovery, from the analysis of data to the anal-ysis of patterns. As we have discussed, this brings new po-tential, among else in driving the mining process towards human-consumable results and in fostering distributed data analysis among institutions that are not disposed or allowed to share their raw data. However, this potential also re-quires further research work in expressing the semantics of higher order patterns according to human perception and in line with privacy-preservation requirements.
 Higher order mining can contribute to the challenge of ap-plying high-complexity algorithms on enormous data vol-umes. Data could be partitioned across the time axis or across some other dimensions of the feature space, allowing for conventional mining within each partition and for higher order mining over the patterns extracted from the partitions. This would not only reveal similarities and interesting dif-ferences among the partitions but also contribute to a more tractable process. For time-based partitions, stream mining methods and frameworks for change detection can be used, while methods for pattern comparison and spatiotemporal methods are appropriate for other types of partitioning. The development of HOM techniques also reasks the ques-tion of whether plug-compatibility for data mining algo-rithms might be possible. There is a wealth of conventional data mining algorithms to serve as a basis for higher order mining. Any pattern specification model, be it intended for a mining language or for pattern presentation and exchange, must be compatible with existing advances and make full use of them. The same is desirable for the higher order mining algorithms themselves. Many of the algorithms that oper-ate upon patterns are tightly coupled to their specifications and particularities, calling for a more seamless integration between conventional algorithms and HOM algorithms. In summary, as discussed in Section 1.1, higher order mining is the sole approach for knowledge discovery in the absence of primary data. In this study, we have shown that there are many advances that adhere to this paradigm but that the overall potential of HOM is still largely unexploited and worthy of further research.
 [1] C. C. Aggarwal. On change diagnosis in evolving data [2] C. C. Aggarwal and P. S. Yu. A Framework for Clus-[3] R. Agrawal and R. Srikant. Mining sequential patterns. [4] R. Agrawal, J. Gehrke, D. Gunopulos, and P. Ragha-[5] S. Agrawal and J. Haritsa. A framework for high-[6] M. L. Antonie and O. R. Za  X  X ane. An associative clas-[7] W.-H. Au and K. C. Chan. Mining changes in associa-[8] S. Baron, M. Spiliopoulou, and O. G  X unther. Efficient [9] I. Bartolini, P. Ciaccia, I. Ntoutsi, M. Patella, and [10] F. Bonchi, F. Giannotti, C. Lucchese, S. Orlando, [11] P. S. Bradley and U. M. Fayyad. Refining initial points [12] T. Calders, B. Goethals, and A. Prado. Integrating [13] M. Ceci, A. Appice, and D. Malerba. Spatial associa-[14] A. Ceglar and J. F. Roddick. Association mining. ACM [15] S. Chakrabarti, S. Sarawagi, and B. Dom. Mining sur-[16] D. Cheung, J. Han, V. Ng, and C. Wong. Mainte-[17] E. Christodoulou, T. Dalamagas, and T. Sellis. Navi-[18] C. Clifton and V. Estivill-Castro, editors. Privacy, [19] J. O. Coplien and D. C. Schmidt. Pattern languages of [20] G. Das, K.-I. Lin, H. Mannila, G. Renganathan, and [21] V. Estivill-Castro, L. Brankovic, and D. Dowe. Privacy [22] W. Fan. Systematic data selection to mine concept-[23] D. Fisher. Knowledge acquisition via incremental con-[24] A. A. Freitas. Understanding the crucial differences be-[25] A. Fu, M. H. Wong, S. C. Sze, W. C. Wong, W. L. [26] M. M. Gaber, A. Zaslavsky, and S. Krishnaswamy. Min-[27] V. Ganti, J. Gehrke, and R. Ramakrishnan. CAC-[28] V. Ganti, J. Gehrke, and R. Ramakrishnan. A frame-[29] V. Ganti, J. Gehrke, and R. Ramakrishnan. DEMON: [30] J. Gehrke, editor. Special Issue on Privacy and Secu-[31] L. Geng and H. J. Hamilton. Interestingness measures [32] S. Guha, A. Meyerson, N. Mishra, R. Motwani, and [33] G. K. Gupta, A. Strehl, and J. Ghosh. Distance based [34] A. Gyenesei. Fuzzy partitioning of quantitative at-[35] S. K. Harms, J. S. Deogun, and T. Tadesse. Discov-[36] P. Kalnis, N. Mamoulis, and S. Bakiras. On discovering [37] M. Kantarcioglu and C. Clifton. Privacy-preserving dis-[38] M. Kantarcioglu, J. Jiashun, and C. Clifton. When [39] E. Keogh, S. Lonardi, and C. A. Ratanamahatana. To-[40] M. Klemettinen, H. Mannila, P. Ronkainen, H. Toivo-[41] G. J. Klir and B. Yuan. Fuzzy sets and fuzzy logic: the-[42] S. Lee and D. Cheung. Maintenance of discovered as-[43] S. D. Lee, D. W. Cheung, and B. Kao. Is sampling [44] B. Lent, A. Swami, and J. Widom. Clustering associa-[45] S. Li, T. Wu, and W. M. Pottenger. Distributed higher [46] T. Li, S. Zhu, and M. Ogihara. A new distributed data [47] W. Li, J. Han, and J. Pei. CMAR: Accurate and ef-[48] B. Liu, W. Hsu, and Y. Ma. Intergrating classifica-[49] A. Maddalena and B. Catania. Towards an interop-[50] Q. Mei and C. Zhai. Discovering evolutionary theme [51] R. Meo and G. Psaila. An xml-based database for [52] R. Miller and Y. Yang. Association rules over interval [53] D. Mladenic, W. F. Eddy, and S. Ziolko. Exploratory [54] C. H. Mooney and J. F. Roddick. Mining itemsets -an [55] D. Neill, A. Moore, M. Sabhnani, and K. Daniel. De-[56] M. Pazzani, S. Mani, and W. R. Shankle. Beyond con-[57] W. Perrizo and A. Denton. Framework unifying asso-[58] W. M. Pottenger, S. Li, and C. D. Janneck. Distributed [59] A. Prodromidis, P. Chan, and S. Stolfo. Meta-learning [60] J. F. Roddick and P. Fule. SemGrAM -integrating se-[61] J. F. Roddick and M. Spiliopoulou. A survey of tempo-[62] B. Rozenberg and E. Gudes. Association rules mining in [63] M. Spiliopoulou and J. F. Roddick. Higher order min-[64] M. Spiliopoulou, I. Ntoutsi, Y. Theodoridis, and [65] S. Thomas, S. Bodagala, K. Alsabti, and S. Ranka. [66] H. Toivonen, M. Klemettinen, P. Ronkainen, K. Ha-[67] A. Tuzhilin and A. Gediminas. Handling very large [68] J. Vaidya and C. Clifton. Privacy preserving associa-[69] K. Wahlstrom, J. F. Roddick, R. Sarre, V. Estivill-[70] K. Wang, S. Zhou, and Y. He. Growing decision trees [71] W. Wang, J. Yang, and R. R. Muntz. Tar: Temporal as-[72] J. Wijsen and R. Meersman. On the complexity of [73] H. Yang, S. Parthasarathy, and S. Mehta. A gener-[74] X. Yin and J. Han. CPAR: Classification based on [75] C. Zhang, M. Liu, W. Nie, and S. Zhang. Identifying [76] S. Zhang, X. Wu, and C. Zhang. Multi-database min-[77] T. Zhang, R. Ramakrishnan, and M. Livny. BIRCH: An While tightly specifying higher order patterns precisely can be difficult, the advantage of such higher-order descriptors is that they can be closer to the semantics we use when de-scribing objects or object behaviour. Indicative (and loosely described) examples of the sort of higher order pattern that might be found for association rules include: Similarly, based on the cluster transitions we have discussed in Section 4.3, we can also derive descriptions for clusters that regularly exhibit a particular behaviour. Such descrip-tions may include: The specification of such patterns is an open area for re-search.

