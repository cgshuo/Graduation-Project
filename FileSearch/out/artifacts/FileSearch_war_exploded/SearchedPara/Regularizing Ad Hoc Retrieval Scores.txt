 The cluster hypothesis states: closely related documents tend to be relevant to the same request. We exploit this hypothesis directly by adjusting ad hoc retrieval scores from an initial retrieval so that topically related documents re-ceive similar scores. We refer to this process as score regu-larization. Score regularization can be presented as an op-timization problem, allowing the use of results from semi-supervised learning. We demonstrate that regularized scores consistently and significantly rank documents better than un-regularized scores, given a variety of initial retrieval al-gorithms. We evaluate our method on two large corpora across a substantial number of topics.
 H.3.3 [ Information Systems ]: Information Search and Re-trieval X  clustering, retrieval models Algorithms, Performance, Experimentation regularization, manifold learning, pseudo-relevance feedback, clustering
In ad hoc retrieval, the clustering hypothesis states: closely related documents tend to be relevant to the same request [7]. Many information retrieval techniques have adopted the clustering hypothesis as a core assumption. A number of methods explicitly attempt to partition the corpus into clusters. Some examples of this approach include cluster-based retrieval, latent semantic indexing, and aspect mod-els. Other methods build clusters on the fly in response to a query. These methods include pseudo-relevance feedback and query-dependent clustering.

In score-based retrieval, the clustering hypothesis implies the following: closely related documents should have simi-lar scores, given the same request . We propose expressing this implication as an optimization problem of balancing the score from some initial retrieval with the scores of related documents. When viewed as a process of smoothing docu-ment scores with those of related documents, this problem can be solved with methodologies from machine learning. We refer to this process as score regularization .
We will begin by describing the general regularization framework in Section 2. This regularization framework relies on having a data structure to encode document relatedness. In Section 3, we present a method for computing related-ness when explicit information is absent. The clustering hypothesis underlies many important information retrieval techniques. In Section 4, we reduce several well-known tech-niques to score regularization. We present results for regu-larizing ad hoc retrieval scores in Section 5. We conclude in Section 6 by placing our work in context of previous results in machine learning and information retrieval.
In previous work, regularization has been posed as an op-timization problem [27]. We will review relevant results in the context of information retrieval. More thorough deriva-tions can be found in referenced publications.

Let n be the number of document scores to regularize. In our case, this is always the top n documents of an initial re-trieval. Given the initial scores in the length n vector, y , we would like to compute a set of regularized scores, f , also a length n vector. We have two contending objectives: score consistency with related documents and score consistency with the initial retrieval. Let S ( f ) be a cost function asso-ciated with the inter-document consistency of the scores, f ; if related documents have very inconsistent scores, then the value of this function will be high. Let E ( f ) be a cost func-tion measuring the consistency with the original scores; if documents have scores very inconsistent with their original scores, then the value of this function will be high. We use a linear combination of these objectives for our composite objective function, where  X  is a regularization parameter allowing us to control how much weight to place on inter-document smoothing ver-sus consistency with the original score.
Inter-document relatedness is represented by an n  X  n affinity matrix, W , where W ij represents the affinity be-tween documents i and j and W ii = 0. At the moment, we will leave the notion of affinity abstract, allowing any number of possible measures; we will outline one way to build this matrix in Section 3. A set of scores is considered smooth if related documents tend to have similar scores. To this end, we define the cost function, S ( f ), which penalizes inconsistency between related documents, where D is a diagonal normalizing matrix such that D ii = P j =1 W ij . This matrix, D , allows us to normalize the affin-ity between two documents; the diagonal elements of this matrix represent how related a document is to all other documents. We can then weight the affinity between some document d i and d j relative to its affinity with all other doc-uments. Using such a normalization has been shown to have superior convergence properties than unnormalized affinities for tasks such as spectral clustering [23].
Obviously, we would like to select our regularized scores, f , such that S ( f ) is minimized. Unconstrained, however, minimizing this objective function would yield a flat set of scores. Therefore, we consider a second objective so that the regularization does not stray from the initial scores, The regularized scores, f , minimizing this function would be completely consistent with the original scores, y ; that is, if we only minimize this objective, then the solution is f = y .
We would like to find the optimal set of regularized scores, f , such that, We can compute the solution to this problem by iteratively smoothing the scores. This computation can be formulated as, where  X  = 1 1+  X  is a simple function of our regularization parameter [27]. We can initialize the regularized scores such that f 0 = y . In the limit, the regularized scores, f t , con-verge on the optimal scores, f  X  . Notice here that, for a sin-gle iteration, a candidate document score is smoothed with the scores of its related neighbors weighted by the relative affinity with individual neighbors.

Alternatively, the optimal regularized scores can be for-mulated as the solution of matrix operations, In our experiments, we will use the closed form solution in Equation 6 to compute f  X  .
The affinity matrix, W , defines the behavior of the regu-larization. A poor affinity matrix will result in the smooth-ing of scores between unrelated documents. Oftentimes, the affinity is explicit and suitable for topical relationships. For example, hyperlinks can provide evidence that two docu-ments share a topic. When such information is not available, affinity can be computed using any number of measures of document similarity.

The majority of results presented here use language mod-eling baseline systems. In order to maintain consistency be-tween our affinity measure and our retrieval model, we will focus on distributional affinity of document language mod-els. 1 One popular distributional affinity measure in the in-formation retrieval community is the Kullback-Leibler diver-gence. However, this measure is asymmetric and has demon-strated mixed results when made symmetric. Therefore, we use the multinomial diffusion kernel [12]. This affinity mea-sure between two distributions,  X  i and  X  j , is motivated by Fisher information metric and defined as, where | V | is the size of the vocabulary and t is a parame-ter controlling the decay of the affinity with respect to the arc cosine of the component-wise square root of the distri-butions. In information retrieval, | V | can be quite large. Therefore, for practical reasons, we ignore the first term, (4  X t )  X  | V | 2 . The diffusion kernel has been shown to be a good affinity metric for tasks such as text classification.
Although we compute the complete n  X  n affinity matrix, there are several reasons to consider a sparse affinity ma-trix instead. For example, we may be more confident about the affinity between very related documents than distant documents. In this situation, the space is often better ap-proximated by the geodesic distances between documents; that is, using the piecewise affinity over local relationships as a measure of non-neighboring affinity rather than ambient affinity. Such a sparse representation can be reasoned about as a weighted graph capturing potential lower-dimensional structure in the data. For example, the matrix may only include the affinities to the k -nearest documents and zero otherwise. A growing body of work has demonstrated that constructing these document affinity graphs accurately cap-tures the lower-dimensional manifold [3]. Our preliminary work confirmed that using the complete affinity matrix was not as successful as sparser representations.

We should, at this point, list a few caveats about assum-ing the presence of an underlying, lower-dimensional mani-fold. First, there is no explicit evidence that the documents from the initial retrieval lie on a lower-dimensional manifold. However, the success of cluster-based retrieval methods sug-gest that there probably exists some topical substructure [14, 26]. Second, the use of these manifold methods nor-mally assumes a uniform sampling on the manifold. We know, though, that topics are neither similarly sized nor uniformly sampled in the initial retrieval. We therefore note
We should note that there is no reason why we could not use cosine similarity in computing W . Preliminary experi-ments have shown that the retrieval power of language mod-els combined with the well-studied cosine similarity measure leverage the strengths of both. 1. compute n  X  n affinity matrix 2. add the k nearest neighbors for each document to W n number of document scores to regularize y top n initial retrieval scores k number of neighbors to consider  X  parameter favoring inter-document consistency f  X  regularized scores Figure 1: Local Score Regularization Algorithm. In-puts are n , y , k , and  X  . The output is the a length n vector of regularized scores, f  X  . that our performance can be further improved by addressing some of these issues in future work.
 Our final score regularization algorithm is presented in Figure 1. Note that the affinity matrix computed in Step 1 is used for adding elements to W in Step 2 and does not define W itself unless k = n .
Several classic retrieval models can be posed as instances of score regularization. In this section, we will be focus-ing on the relationship between these models and a single iteration of score regularization. To this end, we define a general version of Equation 5. Given an initial score func-can be computed as, where the bolded d is our candidate document and  X  E and  X 
S are our regularization weights. The index  X  is some item with which we are comparing the candidate document. These could be other documents or X  X n some cases X  X lusters. We will also see circumstances where s 0 behaves one way for d and another way for  X  .

Given this definition, Equation 5 can be writtern as,
Pseudo-relevance feedback refers to the technique of build-ing a model out of the top n documents retrieved by the original query. The system then performs a second retrieval using combination of this model and the original query.
The classic Rocchio pseudo-relevance feedback algorithm assumes some number of the top documents from an initial retrieval to be relevant. Let this set be R . We then linearly combine these document vectors with the original query vec-tor [20]. Using normalized document and query vectors, the modified query can be computed by, where  X  R is the weight placed on the pseudo-relevant doc-uments. We can then use this new representation to score documents by their cosine similarity to Q 0 . This allows us to derive the regularization version of Rocchio, Notice here that the first factor in the sum is merely the original cosine similarity between the document and query. The second factor in the sum represents the similarity to the pseudo-relevant documents. In terms of Equation 8, the  X  indexes over only the pseudo-relevant documents.

We can also look at a version of Rocchio where we as-sume a pseudo-non-relevant document set, N , usually sam-pled from the tail of an initial ranking. A similar derivation results in, cos ( d , Q ) = cos( d , Q ) +  X  where we now consider two weight parameters. Notice that s (  X , Q ) = { X  1 , 1 } behaves very differently than s ( d, Q ) = cos( d, Q ). Effectively, the regularization only propagates scores of documents in R  X  N ; and for these documents, the scores are 1 or  X  1.
A far more interesting case arises with the language model version of pseudo-relevance feedback [13]. In this case, the original scores are used as weights for the estimated rele-vance model. This relevance model, P ( w |  X  R ), is formally constructed by interpolating the maximum likelihood query model, P ( w |  X  Q ), and document models, P ( w |  X  d ), P ( w |  X  R ) =  X P ( w |  X  Q ) + (1  X   X  ) X where Z = P D  X  R P ( Q |  X  d ). Theoretically, the summation goes over the entire collection so that R includes every doc-ument.

We can use the cross entropy between language models as a scoring measure. Defined as the dot product of language model vectors, the cross entropy is, where  X  is a vector of term probabilities. After some alge-braic manipulation, 2 we can rewrite the ranking based on
Credit is due to Victor Lavrenko for suggesting this deriva-tion.  X   X  log  X  c P ( d | c ) (1  X   X  )  X  c  X  in Equation 8.
 Equation 12 as, using the fact that P ( Q |  X  d ) = | Q | (  X  Q  X  log  X  d Rocchio method assume the top n to have score 1 . 0, this
As mentioned earlier, several techniques attempt to ex-plicitly cluster documents and use this information in re-trieval. We will investigate a recently proposed language model version of cluster-based retrieval [9]. We can extend derivations from this work to demonstrate that cluster-based retrieval is an instance of regularization,
P ( Q | d ) = X all documents. Notice the similarity of Equations 14 and 15. The first factor in the sum is the same in both. The sec-ond factor corresponds to our smoothing process. Instead of smoothing against the scores of all documents in the col-lection, we weight against the scores of clusters. The value of P ( d | c ) can be computed in various ways. Previous work has used an exponential function of the negative Kullback-Leibler divergence [9]. This results in a non-symmetric mea-sure whose behavior is very similar to our diffusion kernel.
We present the results of these reductions in Table 1. It is worthwhile to make some observations. First, only Rocchio and local score regularization use symmetric affinity mea-sures. Symmetry lets us make certain assumptions about the affinity matrix, allowing us to use the results presented in Section 2. Second, some algorithms handle the normal-ization of s 0 ( d , Q ) and s 0 (  X , Q ) differently. Normalization presumably puts scores on similar scales. Both relevance models and cluster-based retrieval use unnormalized scores for the s 0 ( d , Q ); only relevance models normalizes s In our experiments, we normalize both s 0 ( d , Q ) and s in a consistent manner. Third, only local score regulariza-tion normalizes the affinity. As mentioned earlier, normal-izing the affinity has nice theoretical properties. In our ex-periments, we found affinity normalization to be critical for good performance.
We performed all experiments on two data sets. The first data set, trec12, consists of the 150 TREC ad hoc topics 51-200. We used all collections on Tipster disks 1 and 2 [6]. The second data set, robust, consists of the 250 TREC 2004 Robust topics [24]. These topics are considered to be difficult and have been constructed to focus on topics which systems usually perform poorly on. For both data sets, we use only the topic title field. We indexed collections using the Lemur toolkit, the Rainbow stop word list, and Krovetz stemming [1, 16, 8].
We performed exhaustive grid search to train our two free parameters:  X  and t . The regularization parameter was swept over values  X  = [0 . 1 , 0 . 9] with a step size of 0 . 1. The kernel spread parameter was swept over values t  X  1 = [0 . 1 , 0 . 9] with a step size of 0 . 1. We considered 10 nearest neighbors in accordance with previous document classification results.

We selected parameters to optimize mean average preci-sion. We present mean average precision results as well as interpolated precision at the standard 11 recall points.
We performed 10-fold cross-validation by randomly parti-tioning the topics described in Section 5.1.1. For each par-tition, i , the algorithm is trained on all but that partition and is evaluated using that partition, i . For example, if the training phase considers the topics and judgments in parti-tions 1-9, then the testing phase uses the optimal parame-ters for partitions 1-9 to perform retrieval using the topics in partition 10. Performing this procedure for each of the ten partitions results in 150 ranked lists for trec12 or 250 for robust. Evaluation was performed using the concatenation of these ranked lists.
We believe that the top of the initial retrieval tends to be more topically consistent than the full ranked list. This belief follows from an investigation of the distances between known relevant and known non-relevant documents in the initial retrieval. Figure 2 displays the distribution of log-cosine similarities between relevant documents and non-relevant documents; a similar graph is presented in the original clus-tering hypothesis work. As can be seen, relevant documents X  those near the top of the ranked list X  X end to be more sim-ilar than non-relevant documents. This indicates that the relevant documents exist in some small core of the initial retrieval and the non-relevant documents spread out from this core without relationship to each other. We therefore present results for regularizing the top 100, 250, 500, and 1000 results. This presentation also lets us note the im-provement we can achieve if we do not have the resources to compute the full 1000  X  1000 affinity matrix.
Our experiments considered two retrieval systems: lan-guage models and Okapi.
As baselines we use query-likelihood retrieval [4] and rel-evance models [13]. Both of these algorithms are imple-mented in the Lemur language modeling toolkit [1]. The query-likelihood is a standard language model scoring tech-nique and ranks documents according to the probability of the document having generated the query, where  X  d is the document language model. We use the log of this score in our experiments. As mentioned earlier, rele-vance models are the language model equivalent of pseudo-relevance feedback and use an initial ranking generated by Equation 16. Using the classic formulation, we first estimate a relevance model, where R is the set of top 50 documents. We then rank the documents according to the Kullback-Leibler divergence between documents and the relevance model,
D KL ( P (  X |  X  R ) || P (  X |  X  d )) = X where the summation is taken over the top 50 terms in the relevance model. We used Dirichlet smoothing of document models with  X  = 1000 for all experiments. These param-eters demonstrate expected behavior and performance on our collection. Results using query-likelihood and relevance models will be indicated as QL and RM, respectively.
Although the majority of our experiments focus on reg-ularizing language modeling scores, this framework can be applied to regularize scores from arbitrary retrieval meth-ods, given some affinity matrix. We therefore conducted ex-periments studying the regularization of Okapi BM25 scores [19]. We use the implementation in the Lemur toolkit with default parameter settings.

We use the simple cosine similarity between documents to define our affinity matrix. We could further improve per-formance by applying a dampening function on top of the cosine measure; adapting the cosine measure in this way has been demonstrated to improve results by acting as a soft nearest-neighbor threshold [28, pages 9-19].
This constitutes a total of six regularization experiments: trec12/QL, trec12/RM, trec12/okapi, robust/QL, robust/RM, robust/okapi. For each of these experiments, we will eval-uate various regularization pool sizes. We normalized all scores using a shift-and-scale process.
Table 2 presents the results for trec12 and Table 3 presents the results for robust.

We first note that regularizing all 1000 documents in the initial retrieval in all cases but one results in a significant im-provement in mean average precision. At 1000 documents, these improvements occur at most recall points for QL. The one data set (robust/RM) for which regularizing 1000 doc-uments does not improve mean average precision only sees improvements at larger recall points. This result may be ex-plained by the difficulty of these queries. Notice that base-line RM for the robust data set under-performs regularized QL scores for low recall levels. We believe that if we regular-ized QL scores over a larger pool, this improvement would become more pronounced.

In general regularizing RM scores only affects higher recall points, indicating that predicted non-relevant documents are being boosted up by related predicted relevant documents. At the same time, these predicted relevant documents are only slightly affected by the process.

We found that  X  was stable between partitions in our cross-validation but sensitive to retrieval algorithm. For ex-ample, the runs using trec12/QL usually had  X  = 0 . 60 while trec12/RM tended toward  X  = 0 . 30. The other collection demonstrated similar behavior. A higher  X  value indicates a more aggressive regularization. The reason for lower val-ues with RM may be in redundancy with pseudo-relevance feedback.

As expected, varying pool size affects only the perfor-mance of the subset being regularized. This result implies that there is a tradeoff between the amount of improvement in mean average precision and the computation required for the regularization. We stopped at 1000 documents for prac-tical reasons but believe that performance will improve fur-ther when considering more documents.
In Table 4, we demonstrate the regularization of Okapi retrieval scores. The trends observed for regularization of language models generalize to Okapi scores. This is sur-prising given that the system only tuned the regularization parameter,  X  , in training.
In the preparation of the final draft of this paper, we be-came aware of unpublished work presenting many ideas re-lated to our algorithm [15]. Although the algorithms are similar, our experiments cover a wider range of topics and collections. We have also placed regularization in the con-text of well-known information retrieval algorithms.
The majority of techniques in this document are related to work in graph-based methods for semi-supervised learning [3]. Most of these methods are applied to classification tasks where the starting vector y is composed of values in { -1,1,0 } for negative, positive, or unlabeled documents, respectively. While there have been methods proposed for incorporating external classifications [28] and strictly positive labels [27], our experiments demonstrate the use of regularization in a true retrieval scenario with highly skewed class distributions and no training instances.

As mentioned earlier, our work is also closely related to cluster-based information retrieval [9, 14]. These techniques construct a lower-dimensional structure of the corpus or ini-tial retrieval. These clusters are then scored according to the query. The original document score and the cluster score are usually interpolated to get a new score. These techniques assume the existence of a very specific lower-dimensional spaces when computing the new score. Our technique re-laxes the impact of this assumption by focusing on the local structure; the global behavior is a product of this local anal-ysis.

When viewed as a graph algorithm, our work is also re-lated to the many spreading activation [2, 11, 21, 25, 5] and inference network [22, 17] retrieval methods. In these sys-tems, terms and documents are handled in the same graph framework and usually only direct relationships such as au-thors or sources allow inter-document links. By focusing on the document manifold, we pay attention to accurately modeling the data rather than on the scoring documents; the score regularization is a product of the document modeling.
A growing body of work focuses on the exploitation of corpus structure for re-ranking documents [9, 10, 18]. These algorithms usually can be interpreted in the generalized reg-ularization framework presented in Section 4. Although re-lated, this work does not explicitly use solutions presented in Section 2. Furthermore, many of these algorithms, inspired by PageRank, operate on directed graphs while our affinity measure is symmetric. The study of graph directedness on such algorithms remains an open research area.
We have presented a framework for improving document retrieval scores under a regularization framework. We are considering several extensions. Our experiments exclusively deal with regularizing scores of some relatively small set of retrieved documents. We believe that the improvement in mean average precision with growing regularization pools indicates that doing regularization of even larger X  X erhaps corpus-level X  X ools will provide even greater improvements.
We would like to thank the various reviewers for helpful feedback. Additionally, many of the experiments in this pa-per would not have been tractable without Andre Gauthier X  X  technical assistance. This work was supported in part by the Center for Intelligent Information Retrieval and in part by SPAWARSYSCEN-SD grant number N66001-02-1-8903. Any opinions, findings and conclusions or recommendations expressed in this material are of the author and do not nec-essarily reflect those of the sponsor. 0.00 0.7561 0.7550 0.7548 0.7555 0.7492 0.10 0.5347 0.5342 0.5337 0.5314 0.5313 0.20 0.4559 0.4552 0.4551 0.4575 0.4585 0.30 0.3889 0.3884 0.3900 0.3921 0.3931 0.40 0.3350 0.3357 0.3371 0.3378 0.3414 0.50 0.2793 0.2791 0.2805 0.2824 0.2862 0.60 0.2286 0.2286 0.2293 0.2315 0.2362 0.70 0.1664 0.1665 0.1664 0.1679 0.1726 0.80 0.1048 0.1051 0.1053 0.1061 0.1085 0.90 0.0517 0.0517 0.0520 0.0528 0.0543 1.00 0.0019 0.0019 0.0021 0.0023 0.0021 map 0.2836 0.2835 0.2836 0.2845 0.2866 test ( p &lt; 0 . 05 ). using the Wilcoxon test ( p &lt; 0 . 05 ). 0.00 0.7167 0.7351 0.7249 0.7173 0.7042 0.10 0.5056 0.5245 0.5271 0.5242 0.5250 0.20 0.4027 0.4290 0.4302 0.4295 0.4330 0.30 0.3234 0.3453 0.3512 0.3544 0.3574 0.40 0.2483 0.2646 0.2797 0.2834 0.2867 0.50 0.2084 0.2161 0.2274 0.2300 0.2345 0.60 0.1641 0.1690 0.1718 0.1743 0.1824 0.70 0.1191 0.1235 0.1285 0.1307 0.1360 0.80 0.0741 0.0747 0.0779 0.0811 0.0861 0.90 0.0425 0.0441 0.0459 0.0478 0.0512 1.00 0.0221 0.0234 0.0236 0.0242 0.0249 map 0.2333 0.2462 0.2496 0.2506 0.2532 ments in performance using the Wilcoxon test ( p &lt; 0 . 05 ).
