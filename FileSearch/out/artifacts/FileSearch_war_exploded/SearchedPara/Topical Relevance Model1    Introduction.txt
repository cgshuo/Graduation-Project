 An information need expressed in a short query can encompass a wide range of more focused sub-information need s. For instance, the query Poliomyelitis and Post-Polio may seek relevant information on polio disease, its outbreaks, on medical protection against the disease, and on post-polio problems. This multi-faceted nature of the in-formation need expressed in a query is manifested in the retrieved documents, as they tend to form clusters of topics [1]. The model proposed in this paper estimates multiple relevance models, each pertaining to a singl e aspect of the overall information need ex-pressed in a query, as opposed to estimating only one relevance model [2]. Thus, for the example query Poliomyelitis , our model would estimate multiple relevance models, one catering for the disease information, one asso ciated with the prevention of the disease, one pertaining to the post-polio problems and so on. Sometimes, the expression of mul-tiple aspects of an information need can be explicit, such as in queries of associative document search [3], where full documents are used as queries to retrieve related doc-uments from the collection, e.g. patent prior art search [4], where each claim field of a patent query expresses an individual information need for prior art related to a particu-lar claim. These very long queries describe diverse, sometimes orthogonal information needs, in contrast to short queries. To cater for the different characteristics of the two types of queries, i.e. short queries with implicit multi-topical information needs, and ex-plicitly multi-faceted long queries, we propose two variants of our model: one with the assumption that terms in a query are gener ated by sampling from a number of relevance models each pertaining to a specific aspect of the information need; and the other with the assumption that each relevance model generates a subset of query terms. We call the two variants unifaceted topical relevance model (uTRLM) and multifaceted topical relevance model (mTRLM), respectively. We provide a formal description for the two variants of TRLM and evaluate both on standard datasets. The remainder of this paper is organized as follows. In Section 2, we describe related work in PRF and topic models. Section 3 introduces the topical relevance model and provides estimation details for the model. Section 4 describes the experimental setup, followed by Section 5 presenting the evaluation results. Section 6 conclude s the paper with directions for future work. Pseudo-Relevance Feedback. Pseudo-relevance feedback (PRF) is a standard auto-matic technique in IR which seeks to improve retrieval effectiveness in the absence of explicit user feedback [5]. PRF assumes that top ranked initially retrieved documents are relevant, which are then used to iden tify terms that can be added to the original query performing an additional retrieval run with the expanded query [5]. PRF can also involve re-weighting of query terms [5,6] and re-ranking initially retrieved documents by recomputing similarity scores, such as the relevance model estimation [2]. Relevance Model (RLM). RLM is a statistical generative model utilizing the co-occurrence between terms in the query and pseudo-relevant documents to improve retrieval quality [2]. However, a limitation of RLM is that it uses whole documents for co-occurrence statistics. This shortcoming of the RLM was addressed by the positional relevance model (PRLM) [7], which assigns higher weights to co-occurrences within close proximity to better estimate the relevance model. We hypothesize that proximity with query terms alone do not adequately identify relevant topics in a document, and that it would be better to apply techni ques of topic modelling on the set of pseudo-relevant documents. To this effect, we propose the topical relevance model (TRLM) as an extension to the RLM.
 allocation (LDA) which treats every document as a mixture of multinomial distribu-tions with Dirichlet priors [8]. IR with LDA based document models (LBDM) involves estimating LDA model for the whole collection by Gibbs sampling and then linearly combining the standard LM term weighting with LDA-based term weighting [9]. Lin-ear combination was done because LDA itself may be too coarse to be used as the only representation for IR. In fact they report that optimal results are obtained by setting the proportion of LDA to 0 . 3 as a complementary proportion of 0 . 7 for standard LM weighting. Our proposed method overcomes the coarseness of the topic representation limitation by restricting LDA to only the top ranked pseudo-relevant set of documents. This also makes the estimation a lot faster. Another major difference to [9] is that we do not linearly combine document language model scores and the KL divergence scores. We simply calculate the KL divergence between the estimated topical relevance model and the document language model to re-ra nk each document. Thus, our model does not require an extra parameter for a linear combin ation, which makes optimization easier. Overview of the Relevance Model. The key idea in RLM-based retrieval is that both relevant documents and query terms are assumed to be sampled from an underlying hypothetical model of relevance R pertaining to the information need expressed in the query. In the absence of training data for the relevant set of documents, the only observ-able variables are the query terms assumed to be generated from the relevance model. Thus, the estimation of the probability of a word w being generated from the relevance model is approximated by the conditional probability of observing w given the observed query terms. This is illustrated in Figure 1a.

Given the query Q = { q i } n i =1 of n independent terms, the probability of generating aword w from an underlying relevance model R is thus estimated as follows. An assumption that the query terms are conditionally sampled from multinomial docu-ment models { D j } R j =1 ,where R is the number of top ranked documents obtained after initial retrieval as shown in Figure 1b, leads to Equation (2).
 The last step of Equation (2) has been obtained by discarding the uniform prior for has an intuitive explanation in the sens e that the likelihood of generating a word w from in other words if w co-occurs frequently with a query term q i in a pseudo-relevant document D j . RLM thus utilizes co-occurrence of a non-query term with the given query terms to boost the retrieval scores of documents, which otherwise would get a lower language model similarity score due to vocabulary mismatch. For more details on the RLM, the reader is referred to [2].
 Motivation for TRLM. Since co-occurrences in the R LM are computed at the level of whole documents, the co-occurrence of a word belonging to a topic different from the query topics, is not down-weighted as it should be. Thus, it is potentially helpful to compute co-occurrence evidences at the sub-document level, as done in the PRLM using proximity [7]. However, instead of relying on proximity we generalize the RLM by introducing the notion of topics. The RLM has an oversimplified assumption that all relevant documents are generated from a single generative model. A query typically encompasses multiple aspects of the overall information need expressed in it. Thus in a more general case, it would be reasonable to assume that the query terms are sampled from a number of relevance models instead of one. This is illustrated in Figure 2a, where it is assumed that the query words are sampled from three different RLMs R 1 , R 2 and R 3 , and that each RLM R i generates its own set of relevant documents. Broadly speaking, the sub-relevant models can be t hought of addressing each separate topic of the overall information need shown by the encompassing RLM R . We call this model unifaceted , because the query terms themselves a re assumed to belong to a single topic, whereas the underlying information need might be broad and pertain to different topics. The prefix  X  X ni X  in the name thus relates to the query characteristic.

Queries can also be explicitly multifaceted, i.e. structured into diverse information needs, e.g. patent applications in patent prior art search are structured into claims and the requirement is to retrieve prior articles for each such claim. In such a case, we can hypothesize that a query essentially is comprised of a set of sub-queries each of which is sampled from a separate relevance model, as shown in Figure 2b.
 TRLM Description. Let R represent the underlying relevance model that we are trying to estimate. In the standard RLM, it is assumed that words, both from the relevant doc-uments and the query, are sampled from R as shown in Figure 1a. In contrast to this, the unifaceted topical relevance model (uTRLM) assumes that a query expresses a single overall information need, which in turn encapsulates a set of sub-information needs. This is shown in Figure 2a where R 1 , R 2 and R 3 are specific sub-information needs en-capsulated within the more global and general information need R . This is particularly true when the query is broad and is comprised of a wide range of underspecified infor-mation needs. The uTRLM thus assumes that the relevance model is a mixture model, where each model R i generates the words in relevant documents addressing a particular topic, and in addition the query terms as well.

Another generalization which can be made to the RLM is for the case where a query explicitly conveys a set of largely different information needs. Queries in the associa-tive document search domain fall under this category. Segmenting a query into a set of non-overlapping bl ocks of text and then using each block as a separate query has successfully been applied for associative document search [3], which illustrates that such long queries are comprised of multiple information needs. Speaking in terms of the TRLM, it is reasonable to assume that each topical relevance model thus generates its own set of relevant documents and its own subset of query terms. This is illustrated in Figure 2b, which shows that R 1 generates its own set of relevant documents with the subsets of query terms, and leads to our definition of the multifaceted TRLM (mTRLM). Estimation of the TRLMs. The only observable variables in a TRLM are the query terms. Hence one needs to approximate th e probability of generating a non-query term w from the RLM R , by the probability of generating w given that the model has already approximated probability of generating a term from the RLM R , similar to Equation (1). Let us assume that a word w can be generated from a finite universe of topics z = { z 1 ,...,z K sub-relevance model R i , as shown in Figure 2. Assuming z  X  R K follows a multinomial distribution  X   X  R K , with Dirichlet prior  X  for each  X  i , each document d  X  X  D j } R j =1 in turn comprises of a number of topics, where it is assumed that a topic z  X  X  z k } K k =1 is chosen by a multinomial distribution  X   X  R K with the Dirichlet prior  X  . With this terminology, we derive the estimation equations for the two variants of TRLM.
The dependence graph of a unifaceted TR LM is shown in Figure 3a. Let us assume that the query terms { q i } n i =1 are conditionally sampled from multinomial unigram doc-ument models { D j } R j =1 ,where R is the number of top ranked documents obtained after an initial retrieval step. Every query term q i is generated from a document D j with P ( q i | D j ) . Each P ( w | q i ) in turn is given by Due to the addition of a layer of latent topi c nodes, there is no longer a direct depen-dency of w on D j , as in the RLM (see Figure 1b and Equation (2)). Hence to estimate P ( w | D j ) , we need to marginalize this probability over the latent topic variables z k . Thus, we have Substituting Equation (4) in Equation (3) and applying Bayes rule, we obtain P ( w | q i )=  X  1 The last step in Equation (5) is obtained by discarding the uniform prior P ( q i ) and replacing the inner summation with the LDA document model. This is shown by the box labelled  X  X DA X  in the dependence graph of Figure 3a. P ( q i | D j ) is the standard probability of generating a term q i from a smoothed unigram multinomial document model D j . Equation (5) has a very simple interpretation in the sense that a word w is more likely to belong to the TRLM if it i) co-occurs frequently with a query term q i in the top ranked documents; and ii) w has a consistent topical class across the set of pseudo-relevant documents. It is also seen from Equation (5) that the uTRLM uses a document model P LDA ( w | D ) , different to the standard unigram LM document prob-ability P LM ( w,D ) for a document D . This may be interpreted as smoothing of word distributions over topics, somewhat similar to [9], the difference being that the smooth-ing is done over the set of top ranked documents instead of the whole collection. Using marginalized probabilities P ( w | z k ) in Equation (4) leads to a different maximum like-lihood estimate in comparison to P ( w | D ) , which is the standard maximum likelihood of a word w as computed over the whole document D . It also ensures that each topic is
The difference between the mTRLM and the uTRLM is the way in which query terms are sampled from document models. While in the uTRLM, a query term is directly gen-erated from a document model, in the mTRLM the query term generation probability is marginalized over the latent topic models, as shown in Figure 3b. Thus it models the fact that not only the pseudo-relevant documents but also a query comprises multiple topics. This is shown by the additional layer of latent topic nodes inserted between the document nodes and the query term nodes. Ta king into account the latent topics in a query, P ( q i | D j ) of Equation (5) has to be marginalized over the topic nodes as shown in Equation (6).
 Substituting Equation (6) in Equatio n (5) and ignoring the denominator P ( D j ) by as-suming uniform priors, leads to the modified TRLM equation for the mTRLM. Equation (7) thus involves two levels of LDA estimated term generation probabilities, one for the words in pseudo-relevant documents and the other for the query terms. This is shown by the two boxes LDA w and LDA q respectively in Figure 3b. Equation (7) ensures that it assigns higher probability to a term being generated from the relevance model, if the term co-occurs with a query term in pseudo-relevant documents and is also likely to belong to the same topic as that of the query term. Dataset. We evaluate the uTRLM on the TREC 6, 7, 8 and Robust adhoc test collections using the title field of these queries typically comprising of a few keywords. In addi-tion to testing it on these queries, we also use longer queries in the form of the TREC Robust TDN (Title, Description, Narrative) topics to test both uTRLM and mTRLM. The rationale behind using these longer queries is to examine how the two variants of the model perform for queries which have an intermediate length between the two ex-tremes of either being very short comprising of a few keywords, or being very long as in associative document search. For evaluating TRLM on very long queries we use the CLEF-IP 1 2010 dataset, which comprises of a collection of patents from the European patent office, where the queries are themselves full patent documents.
 Selecting Baselines. Since the evaluation objective is to examine whether the TRLM improves on the RLM, we used the RLM as one of our baselines. Additional term-based query expansion with query re-weighting on top of RLM estimation (denoted as RLM+QE) has been found to improve its effectiveness further [10]. We thus use RLM+QE as a stronger baseline for comparison with the TRLM. To compare RLM and TRLM on the same platform, we implemented both in SMART 2 . GibbsLDA++ 3 was used for Gibbs sampling for LDA inference in TRLM. The reason for not using the LBDM approach as a baseline for our experime nts is that according to the experiments described in [9], it could not outperform RLM, which in turn implies that our choice of RLM and RLM+QE as baselines is stronger than LBDM.
 Parameters. The reported results for our experiments were obtained after tuning the parameters through a series of initial retriev al experiments. The s moothing parameter of initial retrieval (LM) i.e.  X  , was optimized empirically to 0 . 4 and 0 . 6 respectively for the TREC and CLEF-IP collections. The hyper-parameters  X  and  X  which control the Dirichlet distributions for TRLM, were set to 50 K and 0 . 1 respectively as suggested in [11]. The number of iterations for Gibbs sampling i.e. N , was set to 1000 for all TRLM experiments. We tuned the common parameter R , i.e. the number of top ranked documents used for pseudo-relevance, within the range of [5 , 50] so as to obtain the best settings for both the RLM and the TRLM. We did not split up the topic sets into separate training and test sets, but rather t he parameters were tuned separately for each individual dataset. Short Queries. The results in Table 1 show that the uTRLM significantly 4 outperforms the RLM for three query sets viz. TREC-6, 8 and Robust. The uTRLM also outperforms RLM+QE, i.e. the RLM with explicit term-based query expansion, even though the latter performs a second retrieval run with additional expansion terms. The limitation of RLM can particularly be seen on TREC-8 where re-ranking documents by RLM in fact decreases MAP with respect to the initial retrieval, whereas RLM+QE increases MAP significantly. By outperforming RLM+QE, the TRLM, which relies only on re-ranking, provides empirical evidence to a mor e accurate and more robust estimation of the relevance model compared to the RLM.
 Very Long Queries. It can be seen from the last row of Table 1, that the mTRLM performs better than the RLM on CLEF-IP dataset. The mTRLM achieves significantly higher MAP over the initial retrieval result LM result, whereas the RLM X  X  improvement over LM is not significant. RLM+QE which performed well for short queries, gives poor results for these long queries. This conforms to previous findings that query expansion is of little or no use for patent search, due to the fact that expansion terms tend to add more noise to the already very long and noisy queries [12]. The mTRLM overcomes the necessity to add expansion terms, thus outperforming RLM+QE, and also marginalizes co-occurrence computation over individual topics in a query instead of the whole query, thus outperforming the RLM.
 Sensitivity to the Number of Topics. An important parameter of the TRLM is the number of topics K , which was optimized empirically in the range of [2, 50] for the values reported in Table 1. Figure 4 shows the effect of variations in the number of topics on retrieval as measured by MAP. It can be seen from the figures that the re-trieval effectiveness is relatively insens itive to the choice of the number of topics. The justification of using a much smaller value range of K in comparison to the global LDA based approach [9], which used much higher values of K in the range of 100 to 1500, is that LDA estimation in the TRLM is done on only a small number of docu-ments in contrast to the full corpus. To see the effect of the parameter K on individ-ual queries, we looked at the MAP values for TREC 6, 7, 8 and Robust queries for different K values in the range of [2 , 50] and found that only 24 of 250 queries reg-ister a standard deviation higher than 0 . 02 in MAP, which suggests that the MAP is fairly insensitive to the choice of K and performance is stable for a majority of queries. Figure 5 highlights the observations for three queries with the highest variances in MAP values. Three patterns of MAP variations for different values of K can be observed in Figure 5: i) a sharp in-crease, ii) a peak, and iii) a sharp de-crease, with increasing K . The first case is illustrated by query Gulf War Syn-drome , where we note a sharp increase in the MAP with an increase of K ,which intuitively suggests that this query is of a very generic nature and the pseudo-relevant documents are associated with a high number of diverse topics. A wide range of symptoms occurring in different individuals tend to form separate topics, as a result of which the model is optimized for a high value of K . The case of a distinct peak in MAP is illustrated by the query tax evasion indicted . The peak is suggestive of the ideal number of relevant topics for this particular query. This query encapsulates expresses two broad information needs: firstly about tax evasion, and secondly about the people who lost money. Both of these can in turn address individual sub-topics, e.g. there can be many different types of or-ganizations involved in tax evasion. The third case is shown by the query supercritical fluids , which is suggestive of a very specific and precise information need. The TRLM for this query thus yields the optimal result with only 2 topics, and the MAP decreases with an increase in the number of topics. This paper has presented the TRLM, a novel framework for exploiting the topical as-sociation of terms in pseudo-relevant documents. The key contributions of this paper are: i) a theoretical justification of the use of topic models in local context analysis thus addressing aspects of relevance; ii) investigating the use of LDA smoothed document and query models for relevance model es timation; iii) proposing an effective technique for associative document retrieval in a single retrieval step; and iv) outperforming the standard RLM, RLM+QE on queries of diverse types and lengths.The work presented in this paper treats an entire pseudo-relevant document as a unit in the LDA estimation. A possible extension to this approach, which will be investigated as part of our future work, is to use smaller textual units, i.e. sentences or paragraphs as document units in the LDA estimation. This would naturally take into account proximity evidence as well, in addition to the topical distribution of terms.
 In the web age, publishing information and opinions online is very easy and fast. Since the web is reachable by a huge number of grassroots people, the number and scale of social networking sites 1 are growing at a tremendous speed. It is an interesting thing to find out information, news &amp; events, opinions, etc., exchanged in these sites. Thus quite a few researchers focus on this and some related issues. One major characteristic of these social networking sites is their dynamic nature. When new things or themes appear, they are discussed in quirk, and then forgotten very quickly. It is also true that the thriving and decline of such sites may happen very quickly. How t o cope with this dynamic environment is a challenging issue for the information/opinion search services.

Data fusion has been demonstrated as a useful technique to improving re-trieval effectiveness. Different data fusion methods, such as CombSum [8,9], CombMNZ [8,9], the linear combination method [2,13,14,16], the correlation method [17], the Borda count [1], the Condercet fusion [11], the probabilistic fusion method [10], Markov chain-based methods [6,12], the multiple criteria approach [7], and others, have been proposed and investigated.

Among all the data fusion methods, the linear combination method is very flexible since different weights can be assigned to different component systems. When choosing component systems, it is not as picky as other data fusion meth-ods such as CombSum, CombMNZ, and Borda count, since poor component systems are not harmful to the linear combination method (if weights are as-signed properly) but can be disastrous t o some others. Therefore, the linear combination method is very good when component systems vary in effective-ness. A recent investigation [15] finds that if weights can be assigned by proper training using such as multiple linear regression, then the linear combination method is superior to other data fusion methods involved and is able to beat the best component systems by a large margin. For the weights obtained by train-ing, they do not need to change if the condition (such as document collection, query topics, and component retrieval systems involved) is not changed. How-ever, this may cause serious effectiveness deterioration in a dynamic environment in which some or all above three aspects change rapidly. Rather than using a static weighting scheme, we consider th at an incremental weighting scheme is more desirable. Therefor e, we try to find out effective weighting methods which are applicable in a dynamic environment.

Up to now, there is not much research on dynamic data fusion methods. A few papers such as [3,5] investigated qu ery-specific data fusion methods. Such methods may be useful for routing tasks in which the underneath document collection is updated regularly but the same group of queries are used again and again. In this paper, we focus on the aspect of upgrading of component retrieval systems, which is an important dimension of dynamic retrieval environment. To our knowledge, this has not been explored before. There are two classes of data fusion methods: equally-treated and biased meth-ods. Equally-treated methods treat all component results equally, while biased methods treat different component results in different ways. CombSum, Comb-MNZ, Borda, Condorcet belong to the first category; while the linear combina-tion, weighted Borda, weighted Condorcet go to the second category. Comparing these two types of methods, equally-tre ated methods are usually more efficient, but may not perform well when the performance of all component results or the similarity between different pairs of component results vary considerably. On the other hand, biased methods are more able to deal with different situations, but training is needed so as to obtain proper weights for all the results involved.
In the real world, things keep changing. This is especially true for information retrieval/web search. First, the document collection may always under change, for example, in the case of web search. It is also true for many different types of digital libraries, online information services, blogs, and so on. Second, queries issued by users vary over time. Third, the information retrieval system (search engine) may be upgraded regularly. Therefore, the information retrieval environ-ment can be very dynamic if all or some of the above three aspects change over time considerably.

Although quite a few information retrieval events such as TREC, NTCIR ,CLEF 3 have been held annually for some time, it is difficult for them to take dynamic search environment into much consideration. In these events, a task consists of a group of queries. However, when running queries one by one, the same collection of documents is used and the information retrieval system involved does not evolve over those different queries. That is to say, only one of the three aspects (query) is dynamic. Therefore, the data sets being used in these events are not ideal for testing adaptive data fusion methods. In the first place, it is desirable to generate bench marks for the testing of adaptive data fusion methods.

Considering the huge cost of generating new data sets, it is reasonable to reuse data sets in TREC or other events with some necessary change. One good candidate for this is the data set of the TREC 2008 blog opinion task. In most TREC tasks, 50 queries are used. However, in the TREC 2008 blog opinion task, it includes 150 queries (queries 851-950 and queries 1001-1050). Later in this paper we re-number them from 1 to 150. As a matter of fact, it has the second largest number of queries in all p ast years X  TREC tasks, only after the TREC 2004 robust track in which 250 queries are used. When more queries are involved, it is more likely to generate som e runs with various level of performance over different queries. It is also possible to use more queries for training and/or testing. Thus the experimental results should be more reliable.

What we would do is to make some individual runs, each of which performs quite differently across different block of queries. This can be done by generating some  X  X rtificial X  runs, and any of which is a mixture of several different original runs. We use the following procedure: first of all, we divide all 150 queries into 3 groups and each comprises 50 queries (1-50, 51-100, or 101-150), where 3 is chosen arbitrarily. 2, 4, 5, or 6 might be reasonable options as well. Then from all 191 original runs, we randomly pick up three different runs r 1 , r 2 ,and r 3 .We take the result for the first 50 queries from r 1 , result for the second 50 queries from r 2 , and result for the third 50 queries from r 3 , and mix them together to generate a new run r m . The above process is repeated until we obtain 191 generated runs, the same number as that of the original ones.

The generated runs have in creased the dynamic property of the component results to some degree, mainly in the res pect of information r etrieval systems. There is not much difference between th e generated and original runs on the aspect of average performance. In such a scenario, we can expect that those data fusion methods that work well in the standard TREC setting can still work reasonably well. Suppose that we have a group of component systems, and for any query issued each of them will provide a ranked list of documents. These ranked lists are fused by some data fusion methods. We also assume that the result from any component system for a single query will be evaluated and its effectiveness will be known immediately. Thus we are able to apply adaptive data fusion methods and to adjust weights dynamically. It may be argued that such a condition is quite difficult to satisfy. Anyway, it is possible for us to use some form of feedback information provided by users or just some click-through data from users as a kind of pseudo-relevance feedback. The n we can reckon the performance of the information retrieval system approximately. Another situation is that the user interaction is included in the search process.

The adaptive data fusion methods work in the following way: at the very be-ginning, since no knowledge about any of the component systems/results is avail-able, we just treat all component systems equally. Afterwards, When the queries are processed, we have more knowledge about the effectiveness of those results involved. Thus we can update the weight for the linear combination method accordingly.
 In this study, we propose and investigate two methods of updating weights. One is the simple performance-square updating (referred to as PSU later in this paper), the other is a mixture of performance-square updating and linear regression analysis updating (referred to as the mixed updating method). Both methods update the weights of component systems per query.

PSU is related to the performance-squa re weighting, which is investigated in [16] for the linear combination method. PSU uses the following equation to update the weight of any component system where w i and w i are the weights before and after the updating, respectively. p is the performance (measured by average precision over all relevant document levels, and 0  X  p  X  1) of the system in question on the current query, and c is a parameter that needs to be set in the range of 0-1. Two extreme situations are c =0and c =1.If c =0,then w i is only decided by p 2 of the current query; if c =1,then w i is only decided by w i , and no adaptive updating is allowed.
Multiple linear regression is found to be an effective technique for determining the weights of component systems for the linear combination method [15]. In this study, we apply multiple linear regression to the data from a single query, not from a large number of queries as in [15]. This decision mainly based on the following two considerations: first, because adaptive data fusion methods are used in a dynamic environment and weights assignment and fusion need to be done in running time, storing and processing data generated from a large number of queries may be too costly; second, th e search environment may change very rapidly, then it is not good to use too many old historical data. 4
Suppose for one query q , n documents d 1 ,..., d n are retrieved by all m com-ponent systems. Every component system IRS i assigns a score s ij to document d . The multiple linear regression tries to minimize u in the following equation where y j is the judged score of document d j to the query. If binary relevance judgment is used, it is 1 for a relevant document and 0 for an irrelevant document. When y j for (1  X  j  X  n )and s ij for (1  X  i  X  m ,1  X  j  X  n )areknown,  X  1 ,  X  ,...,  X  m can be calculated out for a group of training data.
Further those coefficients (  X  ) can be normalized by where m is the number of systems involved. After normalization, the average of all  X  sis1.

Finally, for the mixed method, we may use the following equation to update weight for every information retrieval system involved: where c and c 1 are two parameters that need to be set empirically. As in Equa-tion 1, w i and w i are the weights before and after the updating, respectively. c represents the rate of weight inherited from previous queries and 1  X  c the rate of weight updated by the current query. c 1 is another parameter which is used to normalize the value from multiple linear regression so as to make it comparable with the value from performance-square updating.

Theoretically, the multiple linear regression can be used alone. However, when running the multiple linear regression, quite often (about 14% of all the cases tested in this study) no valid coefficients can be found. We guess the reason for this is the data records used in this study are too few (only from one query). This phenomenon never happen if using the data records for 25 queries or more. If no valid coefficients can be found, we just assign 1 to every system involved. The advantage of the mixed method is, when the multiple linear regression does not work, we can still use the performance-square weighting to perform the task. It has been found that the logistic model is good for score normalization of information retrieval results [4,15]. The logistic model is also used in this study. In Equation 5, s ( t ) is the normalized score of the document at rank t .After using the data in all generated 191 runs, we obtain the values of the two coeffi-cients: a =0.718, and b =-2.183. Note that the logistic model used here is far from optimum because we treat all the runs and all the queries equally. No matter in which run and for which query, a document X  X  score is only decided by its ranking position.

From all available (generated) runs, we randomly choose 3, 4, 5, 6, 7 ,8 ,9, or 10 of them to perform the data fusion experiment. Apart from the two adaptive methods, CombSum is also tested for comparison. For both adaptive methods, every result is given a equal weight of 0.2 initially. The consideration for using such a value is because the average of all 191 runs is about 0.4. After a few steps, the average weight for all the systems will be close to 0.2 (0.4*0.4=0.16). For both methods, c is set to 0.05. Thus at each step, the weight generated takes 95% of the old value and takes 5% of the update. c can be set to different values so as to meet different application requirements. The general principle is: the more dynamic the search environment is, the larger value we should set c . For the mixed method, c 1 is set to 0.2. Note in Equation 4, because the average p 2 is about 0.2 and the average of  X  i is 1, we set c 1 to 0.2 so as to let both components affect th e final result equally. The purpose of such treatment is to make the weights of all the systems as stable as possible throughout the whole process. 4.1 Average Performance of Data Fusion Methods For each given number (3-10), 200 rando mly selected combinations are tested. The experimental results are shown in Fi gures 1-2, in which each data point is the average of 200 combinations and 150 queries in each combination.

From Figures 1-2, we can see that PSU and the mixed method are a little better (about 1% on AP and 0.5%-0.6% on P@10) than CombSum, no matter which measure is used. Although the difference is small, it is significant at a level of 1% (two-tailed T test). PSU and the mixed method are very close, the dif-ference between them are som etimes statistically significant, but sometimes not. Note that all component runs are close in performance is a favorable condition for CombSum to achieve good fusion performance, this is a major reason that is why in this experiment, all three data fu sion methods are close in performance. We also compare all three data fusion methods with the best component result. We find that all three data fusion methods are better than the best component result. This is very consistent across d ifferent number of component results and different measures (AP or P@10). 6 Considering AP, CombSum, PSU, and the mixed method are better than the best component result by 42.60%, 43.68%, and 43.74%, respectively. As to P@10, the corresponding figures are 33.64%, 34.09%, and 34.25%. These figures are very impressive. 4.2 Further Discussion Next let us have a close look at the fusion result per query. We are interested to see how adaptive data fusion methods react when some or all component systems change significantly in performance. Figure 3 shows the average performance of all component results in all combinations. Each data point is the average of 1600 combinations (200 combinations for a given number multiplied by 8 different numbers). In Figure 3 we can see the saw teeth-like curve of it. This shows that the average performance of all component r esults varies considerably from query to query. Such uncertain and rapid performance change from query to query can not be very helpful in any way to data fusion methods, even to adaptive data fusion methods. If there is a more systematic change, then it is possible for adaptive data fusion methods to make certain changes to the fusion model so as to achieve better performance.

Because each generated run is assembled b y three equal-sized blocks (compris-ing 50 queries) from three different original runs, those queries at the beginning of the second and third blocks are worth attention. Such an effect is just like that the information retrieval system has been changed considerably after 50 queries and after 100 queries again. It is interesting to see how data fusion methods per-form when such a considerable change has been made to information retrieval systems. Figure 4 shows the performan ce of the three data fusion methods per query (query numbers 52-61). The result is very similar for another group of queries 102-111. Here we do not distinguish the number of component results involved. Each data point in Figure 4 is the average of 1600 combinations (200 combinations for a given number multiplied by 8 different numbers). The average performance of all componen t results are also shown.

From Figure 4, we can see that both PSU and the mixed method are bet-ter than CombSum, and all three data fusion methods are better than the av-erage of all component results. For queries 52-61, both PSU and the mixed method are better than CombSum by a little over 2%; for queries 102-111, the improvement rate is 1.5%. In both cases, the difference is significant at a level of 1% (two-tailed T test). It shows that both adaptive methods are do-ing well when there is a radical change on the implementation of the infor-mation retrieval system. Comparing two sub-groups of queries (52-61, 102-111) with all the queries , we can see that adaptive methods do better for queries 52-61 and queries 102-111 than the average of all 150 queries. The corresponding figures are 2%, 1.5%, and 1% respectively. This shows that the two adaptive data fusion methods are able to cope with systematic changes from component retrieval systems. The purpose of this paper is to investigate adaptive data fusion methods in a dynamic environment. In order to test them, a benchmark has been generated from 191 runs submitted to the 2008 opinion retrieval task. In this benchmark, any run is a mixture of partial results from three original runs submitted. The effect is very much like that the informat ion retrieval systems have been made radical changes twice when running those 150 queries. It happens for the first time after 50 queries have been performed, and happen for the second time after another 50 queries have been performed.

Two adaptive methods, PSU and the mixed method, are presented. Exper-iments on the benchmark show that both PSU and the mixed method are better than CombSum. Although on average the improvement is small, the difference is statistically significant. More importantly, the proposed methods do better when there is radical changes to some or all of the information retrieval systems.

