 To receive personalized web services, the user has to provide personal information and preferen ces, in addition to the query itself, to the web service. Howe ver, detailed personal information could identify the sender of sensitive queries, thus compromise user privacy. We propose the notion of online anonymity to enable users to issue personalized queries to an untrusted web service while with their anonymity preserved. The challenge for providing online anonymity is dealing with unknown and dynamic web users who can get online and offline at any time. We define this problem, discuss its implications and differences from the problems in the literature, and propose a solution. H.3.5 [ Online Information Services ]: Web-based services; K.4.1 [ Public Policy Issues ]: Privacy Algorithms, Performance, Design, Experimentation, Theory Online anonymity, web services, privacy, personalization The annual personalization survey conducted by Choicestream (http://www.choicestream.com/news/) from 2004 to 2006 consistently shows that 80% of consumers were interested in personalized web service and cont ents. On one hand, personalized web service offers users tailored services according to their personal preference, and thus, is far effective to meet users X  need; on the other hand, personalized web service entails gathering considerable amounts of personal in formation from its users, thus, raise much of privacy concern. Often, data collected by web services, such as query logs, are excellent candidates for various data mining applications. However, the use of such data raises privacy concerns if the data is not made anonymous enough. An example is the release of AOL query logs (New York Times, Aug 9 2006), where the searcher No. 4417749 was traced back to Ms Thelma Arnold. Recently, Google were ordered by a federal judge to turn over YouTube user log to Viacom in a lawsuit (New York Times, July 4 2008). If the user log is not made anonymous enough, video viewing habits of tens of millions of YouTube users may be under the risk of exposure. To better understand how a user may be identified in a personalized web service, let us consider a toy but concrete example. Suppose that the user Albert submits a query q ={diabetes, symptoms} to a vertical search engine like Healthline 1 , which provides specialized search on health and medical information. Wishing to get personalized results, Albert registered his personal information d on date of birth, gender, zip code as required in the online regi stration form. Each query leaves time . We assume that the query log does not contain explicit identifying information of searchers . As a secondary use, the query log is published to a health care company for data mining research, or to the public as in the AOL case. In the following discussion, the attacker refers to a party that has access to the query log and seeks to re-identify the (sensitive) queries of Albert, called the target . Usually, the attacker has some sort of relationship with Albert, e.g., colleagues, neighbors, friends, enemies, etc. Consider the following two ways of re-identification.  X  Re-identification through personal information Suppose that the attacker knows that Albert has searched the above site. 
Also suppose that the attacker knows Albert X  X  date of birth, gender, zip code, which can be acquired from a public source such as a voter list [1]. The attacker then could narrow down the queries issued by Albert by matching this knowledge against the personal information d in the query log. As reported in [1], with as little knowledge as {date of birth, gender, zip code}, 87% of population in US is uniquely identifiable.  X  Re-identification through approximate query time If the attacker also acquires an approximate query time such as  X  X lbert used medsite two days ago X , say from an office conversation, the attacker can further narrow down the candidate queries by excluding the entries in the query log that are not within this time interval. In the above search scenario, a personalized query has two parts &lt; d,q &gt;. The query q contains query terms on which the user wants to get results. This part is unstruc tured (i.e., free text) and contains sensitive information, meaning that the user does not want to be identified as the sender of the query. The personal information d http://www.healthline.com/ contains demographic data of the user (such as age, gender, zip code) and other preference informa tion, and is used to guide the search of results tailored to the use r X  X  taste. This part is structured with pre-defined semantics and is usually obtained via a user sign-up interface. Note that this s cenario is distinguished from the personalization based on informati on on the web service side such as query histories, which is beyond users X  control. The flow of a re-identificati on attack described above is 
Web User ( u, d ) Attacker X  X  Knowledge: &lt; d , T&gt; time t to a web service. Over time, the web service collects a query log containing all entries &lt; d i , q i , t query log is used or published for data mining research. One recipient of the query log (possibly the web service itself), the that is, the attacker knows that th e target user with the personal information d has issued some queries at the approximate query time T. The attacker X  X  goal is to identify the target user X  X  queries from the query log. The anonymity of the target user is compromised if a small number of entries &lt; d i , q match the prior knowledge &lt; d , T&gt;. Starting from end users X  point of view, a key assumption in this work is that the web service is untrusted due to the collection of potentially identifying personal information d . Specifically, we adapt the semi-honest model [6] for the web service: the web service will follow the specified co mputation, but may seek to use the collected query log to identify the query of a target user. This assumption ensures a stronger privacy guarantee on the query log so that it can be used by the web service or published to third parties without causing end users X  privacy concerns. Our approach Under the assumption of the untrusted web service, there is a privacy threat because the web service owns both the personal information d and the query q . A detailed d may link a unique user or a small number of users to q . To break the link between the two, we introduce an untrusted third party called user pool , which also follows the semi-honest model. Instead of anonymizes d through the user pool and then sends &lt; d X ,q &gt; to the web service, where d X  is some generalization of d . The goal is to ensure that the generalized personal information d X  cannot be linked to the user. We assume that all communications between a user and the web service/user pool are anonymous [2]. As a result, the user pool possesses the raw personal information d , but has no knowledge about the query q that u may send. The web service possess the query q and the generalized personal information d X , but cannot identify u from d X  because d X  has been generalized. Our contribution is summarized below. Contribution I We introduce the notion of o nline anonymity to ensure that each query entry &lt; d X  , q , t &gt; in the query log cannot be anonymity if at least k distinct users have issued a query using the generalized personal information d X  and within the w proximity of the query time t . Therefore, if the attacker X  X  knowledge T about query time is not more accurate than w , all of these users are possible candidates for the sender of &lt; d X  , q , t &gt;. We will show that online anonymity provides defense against the web service. Contribution II We propose an algorithm that achieves online anonymity through the user pool. A significant challenge comes from the assumption of untrusted web service and user pool, and dealing with the dynamic sets of online users. Specifically, to provide online anonymity, the user pool must track the online users who issued queries during a certain time interval and anonymize their personal information d in an online fashion. This tracking also entails some interaction between the user pool and web users. We propose a protocol for this interaction to guarantee that the additional information collected by the user pool cannot be used to compromise user anonymity. Contribution III Although we focus on anonymizing the personal information d that is separately provided for the personalization purpose, in the same spirit, our approach can be extended to deal with personally identifying information that may be contained in the query d . In this sense, our work is also applicable to general web services where there is a need to anonymize the query, with or without personalization. We will discuss this extension in Section 3.3. In privacy preserving data publishing [1], a trusted party, called publisher, collects all data (i.e., query logs) first and anonymizes the data for publishing. This scenar io is not applicable to our web setting where personal informati on is held by individual web users and there is no trusted data publisher. To put our scenario into the context of data publishing, the query entries in the query log are data records, but they must be generalized before they are submitted to the web service. This distributed setting of data is similar to [3]. However, [3] cons iders a pre-defined set of users and cannot deal with the open web scenario where there is no pre-defined set users because users get online and offline arbitrarily. In anonymous communication [2], systems aim to provide a communication channel for users to interact with the web service anonymously. Similarly, privacy-preserving data collection [7] addresses respondents X  anonymity in a data collection process. All of these works do not address the re -identification of data subjects from the content of data transmitted. In contrast, our work assumes communication anonymity as the infrastructure and focuses on re-identification attacks arising from examining the content of data. Another body of work makes use of an alias, including anonymous user accounts [4], digital pseudonyms [5], and anonymous web browsing (http://www.anonymizer.com). In the scenario of personalized web se rvice, personal information is required for personalization and it is such information that links the queries to their senders. This threat exists independently of communication channels, pseudonyms, and user accounts used. This section describes our personalization framework, the assumptions and privacy notion used in this paper. We consider a timeline labeled by a sequence of time units denoted by 1, 2, 3, .... A time un it could be a second, a minute, an hour, or a fraction of such units. A time interval or window is a sequence of consecutive time units. The window size refers to the number of time units in the wi ndow. Figure 2 depicts the basic components and flow of information in our framework. Web service and web users We consider a web service and a attached with his personal information d . Unlike database queries, a web query q consists of several query terms and is unstructured, unedited, and lack of pre-defined semantics. On receiving &lt; d, q &gt;, the web service returns personalized services (or results) to the user. Over time, the web service collects and maintains a query log that contains all entries &lt; d,q,t &gt; ordered by the query time t . The web service follows the semi-honest model [6]: it will follow the specified computation but may attempt to identify the queries sent by a user. We assume that there is an anonymous communication channel [2] between each user and the web service. This means that queries and results will be transmitted between users and the web service as expected, but the web service has no way to know the user behind a query and has no trace of different queries from the same user by observing where a query comes from.
 User pool The function of user pool is to pull all web users together and determine for each web user the disclosure of personal information d in order to access personalized web services anonymously. We assume the semi-honest model for the user pool and an anonymous communication channel between each web user and the user pool. By choosing the semi-honest model for the user pool, we cover the case that the user pool may also seek to identify the sender of a query or even collude with the web service to do so. See more discussions on attackers below. In implementation, the user pool can be hosted by either some third party as a public service, or by web services to offer their users anonymity so as to gain a competitive advantage over competitors. We envisage that the adoption process of the user pool might be similar to that of OpenID (http://openid.net), a shared identity service that allows Internet users to log on different web services using a single digital identity. It initially arose from open source community as an open and free service, but later gained its popularity among large sites with large organizations such as Google and Microsoft as providers. The information flow is described in Figure 2. (1) Prior to sending a query q to the web service, a web user must first register with the user pool his personal information d . (2) The user pool returns to the web user the generalized personal information d X  . d X  contains less, but semantically consistent information, than d . For example, if d contains  X  X ge=25 X , d X  may contain  X  X ge in [20,30] X . (3) Subsequently the we b user submits the generalized personalized query &lt; d X ,q &gt; to the web service. (4) Upon receiving &lt; d X ,q &gt;, the web service returns the result to the user. As a result, the query log contains generalized query entries &lt; d X ,q,t &gt;, instead of the raw query entries &lt; d,q,t &gt;. Attacker An attacker is a party that seeks to identify a query sent by a target user u from the query log. To do so, the attacker has the following information:  X  Query log . We assume that the web service has published the query log (for research purpose) and the attacker is one of the recipients.  X  Personal information d of u . The attacker has obtained the personal information d of u as public knowledge.  X  Approximate time interval T q during which u has sent a query q . The attacker knows that u sent a query q within a time interval T q (but does not know the content of q ). Often, T is an interval containing the actual query time because the attacker knows an approximate query time, but not the exact query time. The size of the interval T q indicates the  X  X ower X  of the attacker. that our semi-honest model for the web service and user pool covers the case that these paritie s may be the attacker if they obtain the above information. For example, the user pool could be one of the recipients of the published query log. In fact, the web service and the user pool may even collude to identify a user X  X  query. The temporal accuracy of the attacker X  X  knowledge T query time can be m odeled as follows. Definition 1 Consider the attacker with prior knowledge &lt; d , T on a query q , we say that the attacker is w-oblivious if [ t-w, t+w ]  X  T q , where t is the actual query time of a query q .  X  In other words, a w -oblivious attacker has at least  X  w error around the actual query time. The smaller the w is, the more precise the attacker X  X  knowledge T q is about the query time, therefore, the more powerful the a ttacker is. Our goal is to provide users anonymity against the w -oblivious attacker for a given w . Given the query log and the prior knowledge &lt; d, T q &gt; on a target user u , the attacker tries to narrow down the candidate entries &lt; d X ,q, t &gt; (in the query log) originating from u . Such entries must match &lt; d, T q &gt;, denoted d  X  d X  and t  X  T not all matched entries originated from u because other users have similar generalized personal information d X  and have issued a query within T q . The more matched queries were sent by different users, the less certain the attacker is about whether a matched query was actually sent by u . Definition 2 ( k -identification) We say that the target user is k-sent by less than k distinct users.  X  To prevent k -identification, we require that, for each query &lt; d X ,q , distinct users: these queries share the same generalized personal information d  X  and were sent within time proximity from t that is not distinguishable by the attack er. This motivates the following privacy notion. Definition 3 (( k , w )-online-anonymity) A query &lt; d X , q , t &gt; in the query log is said to have ( k , w )-online-anonymity if there are at t &gt; was sent by a distinct user, d X  i = d X  , and |t-t i is said to have ( k , w )-online-anonymity if all queries in the log have ( k,w )-online-anonymity.  X  Theorem 1 If a query log has ( k , w )-online-anonymity, for any query in the log, the sender of the query is not k -identified by any w -oblivious attacker. online-anonymity. Suppose that the attacker has the prior knowledge &lt; d, T q &gt; about u and q . Since the attacker is w -oblivious (Definition 1), we have [ t-w, t+w ]  X  T within the interval [ t-w, t+w ] by distinct users and those users share d X  (Definition 3). All these queries match &lt; d, T [ t-w, t+w ]  X  T q . Since these queries were sent by k distinct users, from Definition 2, the sender of &lt; d X , q , t &gt; is not k -identified.  X  So far, we consider only attacks based on the personal information d provided for personalizati on. Sometime, the query q itself may contain personally identifying information. We can extend the notion of online anonymity to cover personally identifying information contained in the query q . Suppose that the query q can be divided into two parts. The the user wants to submit as it is and does not want to be identified as the sender. Such terms typically refer to financial information, health information, religion and political beliefs. The public sub-query of q p refers to the set of public terms in q that may potentially identify the user and th e user is willing to modify. For example, for a query q ={ X  X tripper club, Redmond WA X  X , q be  X  X tripper club X  and q p could be  X  X edmond WA X . We assume that public/private terms can be specified by the user. With the above partition &lt; q p , q s &gt; of q , we can treat the public sub-query q p as an extension of the personal information d and generalize both d and q p using our method. Unlike d , q unstructured (i.e., free text). However, the issue of how to anonymize d and q p is orthogonal to our approach in that it is entirely local to the user pool and any generalization algorithm can be plugged into our approach. Our focus is on the challenge of providing online anonymity in the open and dynamic web setting without assuming a trusted we b service. The implication of this extension is that our appro ach is applicable to general web services, with or without pers onalization. In the absence of personalization, d and d  X  are empty, and anonymization focuses on the personally identifying information in the query q , i.e., q Due to the space limit, we are unable to describe how to achieve online anonymity in detail. The core idea is to group web users with similar characteristics, both on personal information and query time, so that their query entries in the query log provide ( k,w )-online-anonymity for each other. The algorithms and evaluation will be presented in th e full version of this paper. This paper was motivated by two emerging trends: web users want personalized services and web users want privacy. One challenge is that personal inform ation must be made anonymous under the assumption that the participating parties, including the web service, are not completely trusted, due to systematic collection of personal information in addition to queries. Another challenge is the online and dynamic nature of web users. We proposed the notion of online anonymity to protect web users and we proposed an approach to maintain online anonymity through time. Our approach makes use of a third party called the user pool and we do not require the user pool to be trusted. The simulation study on real US demographics showed promising results: it is feasible to achieve personalizati on for reasonable privacy settings. [1] L. Sweeney. K-Anonymity: a m odel for protecting privacy. [2] L. Ahn, A. Bortz, and N. J. Hopper. K-anonymous message [3] S. Zhong, Z. Yang and R. N. Wright. Privacy-enhancing k-[4] E. Gabber, P. B. Gibbons, A. Mayer, Y. Matias. How to [5] A. Kobsa, and J. Schreck. Pr ivacy through pseudonymity in [6] O. Goldreich. Foundations of Cryptography , Volume 1. [7] Z. Yang, S. Zhong, R. N. Wright, Anonymity-preserving 
