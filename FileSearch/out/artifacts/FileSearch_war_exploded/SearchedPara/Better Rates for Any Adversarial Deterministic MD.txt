 Ofer Dekel oferd@microsoft.com Microsoft Research, 1 Microsoft Way, Redmond, WA 98052, USA Elad Hazan ehazan@ie.technion.ac.il Technion -Israel Inst. of Tech., Haifa 32000, Israel A sequential decision making problem deals with a de-cision maker X  X  extended interaction with his environ-ment. The decision maker can take different actions that influence his state in the environment, and each action can have both short and long term effects on the decision maker X  X  utility. For example, imagine a robotic vacuum cleaner that travels around the house and dynamically makes turn-by-turn decisions about its route. The robot is the decision maker, the house is the environment, the utility is the amount of dirt collected, and the robot X  X  state in this example is his location. The robot knows its state and can take ac-tion to move to an adjacent state. Not all actions are available in all states and the consequence of each ac-tion depends on the current state.
 Sequential decision making problems can be formu-lated in various ways, with different assumptions on the dynamics of the environment and on the informa-tion available to the decision maker. In this paper, we focus on one such setting: the adversarial determinis-tic Markov decision process with bandit feedback , ab-breviated by ADMDP . Namely, we assume that the en-vironment is adversarial, the state transition dynamics of the environment are deterministic, and the feedback observed by the decision maker is bandit feedback (all of these terms are explained below).
 We model the sequential decision making problem as a game between a randomized game player (the de-cision maker) and a deterministic adversary (the en-vironment). First, the player and the adversary are told the total number of steps in the game, T . Then, they are given a directed graph G = ( V , E ) called the state transition graph , where V is a finite set of ver-tices or states , and E is a set of directed edges, with at least one outgoing edge per state. Moreover, one of the states in V is designated as the initial state , and denoted by v 0 .
 Next, the adversary defines a sequence of T loss func-tions , f 1 ,...,f T , where each f t takes the form f E 7 X  [0 , 1]. In other words, the adversary assigns a loss value to each edge at each step in the game. The loss functions are not revealed to the player. Finally, the player takes T steps along the edges of the graph, starting from the initial state v 0 . Specifically, at each step t  X  1 ,...,T , the player begins in state V t  X  1  X  V , chooses one of V t  X  1  X  X  outgoing edges, and traverses that edge to a new state V t  X  V . The cho-sen edge is called the player X  X  action . Since the player has the power of randomization, he chooses his action by defining a distribution over V t  X  1  X  X  outgoing edges and sampling a concrete edge from that distribution. Therefore, the player X  X  state after step t is a random variable. Some of the edges in E may be self-loops, in which case V t can equal V t  X  1 . While traversing the edge ( V t  X  1 ,V t ), the player suffers a loss of f t ( V The player observes this loss value, but he does not observe the loss that he would have suffered had he chosen a different action or had he been in a differ-ent state; this feedback model is commonly called the bandit feedback model.
 We assume that the adversary has unlimited compu-tational power and may even know the player X  X  algo-rithm. The adversary may use random bits if he so desires, but since he already has unlimited computa-tional power, random bits do not give him an addi-tional advantage. Despite the adversary X  X  power, he must choose the entire sequence of loss functions be-fore the player makes a move and without knowing the player X  X  random choices (in other words, the adversary knows the player X  X  algorithm but he doesn X  X  know the player X  X  random bits). This type of adversary is called an oblivious adversary or a non-adaptive adversary . Although the adversary is oblivious, the player X  X  loss at time t still depends on his past actions, since the loss depends on the player X  X  state, which is determined by his past actions. Therefore, from the player X  X  point of view, the environment does seem to react to his past actions.
 The player X  X  goal is to accumulate the smallest pos-sible loss as he performs his T -step path over the graph. Although the loss functions are deterministic, the player X  X  loss depends on his randomized actions. Therefore, our analysis focuses on the player X  X  expected cumulative loss. Since the loss functions are adversar-ial, the player X  X  loss is only meaningful when compared to a baseline (since the adversary could always assign the maximal loss to all actions at all states and at all steps); in this paper we compare the player X  X  loss to the loss of the best fixed policy in hindsight. Formally, let  X  be the set of all fixed (deterministic) policies, where each  X   X   X  is a mapping from V to itself that maps each state v to one of its outgoing neighbors. A deterministic policy  X  starts in the initial state v 0 , transitions to  X  ( v 0 ), then to  X  (  X  ( v 0 )), and so on. This motivates us to define the shorthand Using this notation, the loss accumulated by the fixed policy  X  equals P T t =1 f t (  X  t  X  1 ( v 0 ) , X  t ( v 0 define the player X  X  (undiscounted) regret , denoted by R
ADMDP ( T ), as the difference between his expected cumulative loss and the loss of the best fixed policy in hindsight. Formally, R ADMDP ( T ) is defined as E We say that the player is learning if R ADMDP ( T ) is upper-bounded by a sub-linear function of T , uni-formly for all sequences of loss functions. A sub-linear regret implies that the average expected loss suffered by the player on each individual step tends to zero, so the player gets better with time. On the other hand, the player isn X  X  learning if R ADMDP ( T ) keeps growing linearly, even as T tends to infinity. Our technical goal is to prove bounds of the form R ADMDP ( T ) = O ( T q ) where q  X  [0 , 1); a smaller value of q implies faster learning.
 Any fixed policy in  X  takes at most |V| steps before re-turning to a state that it has previously visited. From that point on, the fixed policy repeats the same cy-cle of states over and over again. However, note that two policies in  X  that lead to the same cycle can still accumulate different losses if they enter the cycle at different times or at different states. To demonstrate this concept, consider the complete graph over three states, V = { v 0 ,v 1 ,v 2 } . Notice that there are two poli-cies that lead to the 2-cycle v 1 ,v 2 : one that traverses the edge ( v 1 ,v 2 ) on odd steps and one that does so on even steps. Now, define for all t While the policy that traverses the edge ( v 1 ,v 2 odd steps suffers a total loss of  X  ( T ), the policy that traverses the edge ( v 1 ,v 2 ) on even steps suffers a total loss of  X  (1). Therefore, it is insufficient to merely dis-cover low-loss cycles in G , and we must also think of a policy X  X  phase within a cycle.
 We make some assumptions on the structure of G , but we prove that they are necessary. Specifically, we as-sume that G contains exactly one strongly connected component (see definition below), and we prove that without this assumption, there does not exist any al-gorithm that guarantees a sub-linear regret for all loss sequences. With this assumption alone, we present a new algorithm that guarantees a regret of O ( T 2 / 3 ) against any sequence of loss functions. This proves that the assumption above is also a sufficient condi-tion for learning in the ADMDP setting. To the best of our knowledge, our algorithm is the first to make do with the necessary condition on G , and all previous work on this topic requires more restrictive assump-tions (see the related work section below for details). To simplify our presentation, we first solve the prob-lem with the additional assumption that G is aperiodic (see definition below), but then we relax this assump-tion and default back to the necessary condition. A more common way of formulating sequential deci-sion making problems uses (stochastic) Markov deci-sion processes (MDP). The classic MDP formulation assumes that the losses (or rewards) are stochastic (rather than adversarial) and the state transitions are noisy (rather than deterministically controlled by the player). Adversarial losses are strictly more general than stochastic ones, as the adversary is free to use random bits if he so chooses. Adversarial environments also generalize time-varying stochastic environments, strategic environments (like the ones encountered in multiplayer video games and online bidding systems), and malicious environments (like the ones encountered in spam and fraud detection systems). In this sense, our setting is strictly more powerful than the MDP set-ting. On the other hand, the deterministic state transi-tions in the ADMDP model are weaker than the noisy transitions in the MDP model. In some cases, this may be a limitation of the ADMDP setting. However, in general, a stochastic system is often well approximated by a deterministic system (Bertsekas, 2005, Chap. 2). Overall, the relatve strength of the ADMDP model versus the more traditional MDP model is not well understood.
 Our assumption that the entire graph is known to the player beforehand is simply a matter of conve-nience. This assumption can be easily relaxed using ideas from Ortner (2010). Also, we chose to formulate the problem in terms of loss functions (rather then reward functions) for notational convenience; moving from rewards to losses and vice versa can be done by replacing each f t with 1  X  f t . 1.1. Related Work and Our Contribution Our work is most closely related to the recent work in Arora et al. (2012), which addresses the same problem (with the minor technical difference that it formulates the game using rewards rather than losses). Arora et al. (2012) presents an algorithm called MarcoPolo , with a regret bound of O ( T 3 / 4 ), which holds for any strongly connected aperiodic graph. In contrast, our new algorithm guarantees a better regret of O ( T 2 / 3 ), without the restriction to aperiodic graphs. We note that the slow learning rate attained by MarcoPolo is not due to a loose analysis, but is an inherent limita-tion of the algorithm: MarcoPolo is built of a top-layer multi-armed bandit algorithm and a bottom layer on-line linear bandit algorithm; the latter is reset times during the T steps, and must learn everything from scratch each time; this algorithmic construction implies that the O ( T 3 / 4 ) regret guarantee is the best possible for MarcoPolo.
 More generally, there is an abundance of previous re-search on MDPs with stochastic transitions and adver-sarial rewards (Even-Dar et al., 2009; Yu et al., 2009; Yu &amp; Mannor, 2009; Neu et al., 2010). Moreover, the typical regret rates in this setting are O ( T Since stochastic transitions are strictly more general than deterministic transitions, it would seem that these papers solve a more general problem with a better regret guarantee. However, all of these pa-pers make an additional assumption that turns out to be extremely restrictive when the transitions are deterministic. Specifically, the results for stochastic MDPs require that the state transition dynamics have a unichain structure, which means that every policy must lead to a Markov chain with a single recurrent class. In other words, they assume that the underly-ing Markov chain is uniformly ergodic under any pol-icy. In the special case of deterministic transitions, the unichain assumption implies that any two cycles in the graph share a common vertex (Feinberg &amp; Yang, 2008). Clearly, this assumption excludes most of the deterministic state transition graphs that one could imagine; for example, it excludes any graph with two self-loops. Therefore, these results are typically inap-plicable in our deterministic setting. For more details on the limitations imposed by the unichain assumption in the ADMDP setting, see (Arora et al., 2012). As noted above, our new algorithm applies to any state transition graph that satisfies the necessary condition. More generally, there is relevant previous work on the connections between reinforcement learning and individual sequence prediction in information theory (Farias et al., 2010), as well as work on regret mini-mization in stochastic and deterministic MDPs when the rewards are stochastic, rather than adversarial (see Szepesvari (2010) and the references therein, as well as (Ortner, 2010)). We begin with some mathematical background on di-rected graphs. Let G = ( V , E ) be a directed graph. A set of vertices V 0  X  V is strongly connected if G con-tains a directed path between any two vertices in V 0 . We say that G is a strongly connected graph if its ver-tices V form a strongly connected set. A cycle in G from v to itself is a path in G that starts and ends at v . If G is strongly connected, it contains at least one cy-cle from any vertex v . A strongly connected component of G is a maximal strongly connected set of vertices. That is, if we add any vertex to a strongly connected component, it will no longer be strongly connected. Assume that G is strongly-connected and consider the set of cycles of length at most |V| from v to itself. Let  X  ( v ) be the greatest common divisor (gcd) of the lengths of these cycles; this quantity is called the period of the vertex v . It can be shown that all of the vertices of a strongly connected graph have the same period (Bremaud, 1999, Chap. 2, Thm. 4.2), so period is actually a property of G rather than of v . For brevity, we denote the period of G by  X  . If  X  = 1, we say that G is an aperiodic graph.
 A strongly connected graph G with period  X  can al-ways be arranged in a cyclic structure (Bremaud, 1999, Chap. 2, Thm. 4.1) of length  X  . That is, the graph vertices can be uniquely partitioned into  X  non-empty sets, called cyclic classes and denoted by C 0 ,..., C  X   X  1 such that the outgoing edges from the vertices in C all lead to vertices in C i +1 (for all i , where i + 1 is computed modulo  X  ). If G is aperiodic, then all of its vertices belong to a single cyclic class and the above holds trivially. In other words, if the vertices of G cannot be partitioned into multiple cyclic classes, as described above, then its period is necessarily  X  = 1. From the above, we can conclude that the length of every cycle in a strongly connected graph with period  X  is an integer multiple of  X  .
 If G is strongly connected and aperiodic (namely,  X  = 1), there exists a critical length d such that for any s  X  d there are paths in G of length s between any pair of vertices (Denardo, 1977). This result is a consequence of the Frobenius coin exchange problem in combinatorics, and specifically of Schur X  X  theorem (Al-fonsin, 2005). Moreover, the critical length d is never too big: letting n = |V| , it is known that d  X  n ( n  X  1) (Denardo, 1977).
 If G has a period of  X  &gt; 1, the upper bound on d can be generalized as follows: there exists a critical value d such that for any integer s  X  d there is a path in G of length s X  from any state v to any other state in the same cyclic class . To prove this generalization, let C denote the cyclic class to which v belongs. Construct a new graph G 0 = ( V 0 , E 0 ), where V 0 = C i and where E 0 contains the edge ( u,v ) if and only if there exists a path of length  X  from u to v in G . Now note that G 0 is aperiodic: the greatest common divisor of cycles in G is  X  so the greatest common divisor of the cycles in G 0 is 1. The generalized theorem applied to G follows from applying the original theorem to G 0 . As previously mentioned in the introduction, we make some assumptions on the topology of G . First, recall our assumption that every state has at least one out-going edge -clearly this assumption is required for the ADMDP game to be well defined. Next, we assume that all of the states are reachable from v 0 . This as-sumption is made without loss of generality, since oth-erwise we can simply remove the unreachable states without changing the sequential decision problem. These two assumptions already imply that G con-taints at least one strongly connected component that is reachable from v 0 . The reasoning is straightforward: there exists a path of length |V| that starts at v 0 ; this path must return to a state that it previously visited; this creates a cycle, which is a strongly connected set. We add a third assumption on the topology of G : we assume that G contains exactly one strongly connected component, and no more. In contrast to the pre-vious assumptions, which did not restrict generality, this assumption limits the graph topologies that we can handle. However, we prove this assumption is a necessary condition for any learning algorithm in this setting. Namely, if G contains two or more strongly connected components, a sub-linear upper bound on regret is unattainable by any algorithm.
 Theorem 3.1. If G has two (or more) strongly con-nected components, for any algorithm used by the player, there exists an oblivious sequence of loss func-tions that inflicts a regret of  X ( T ) .
 Proof of this theorem is deferred to the full version of this paper due to space restrictions.
 Fig. 1 shows an example of a graph that satisfies the conditions of our algorithm. It contains a single strongly connected component and has a period of 3. Theorem 3.1 proves that having a single strongly con-nected component is a necessary condition. With this assumption in place, we can further simplify our pre-sentation by assuming that all of the vertices in G are strongly connected. Given the previous assumptions, this assumption can be made without further loss of generality. To see why, assume that G is not strongly connected but contains exactly one strongly connected component. Note that any vertex that does not belong to the strongly connected component is unreachable after the first n steps (recall that n = |V| ). A precise proof of this is straightforward: a vertex that does not belong to a strongly connected component can only be visited once; therefore, any path of length n must end inside the strongly connected component; once the path enters the strongly connected component it can never leave. Therefore, the player can begin the game by performing n arbitrary actions (at worst, adding a constant to the cumulative loss). After that, he never has to worry about the vertices that are not part of the connected component. In other words, we assume, without any additional loss of generality, that G is strongly connected to begin with.
 In this section, we add the simplifying assumption that the state transition graph G is aperiodic. We will relax this assumption in a later section. As explained in Sec. 2, this assumption implies that there exists a path in G of length d = n ( n  X  1) between any pair of states. We solve the ADMDP problem by reducing it to the bandit shortest path (BSP) problem. BSP is also mod-eled as a repeated game between an adversary and a player. The problem is defined by a directed graph G ? = ( V ? , E ? ) and two special vertices, a source vertex s  X  X  ? , and a target vertex t  X  X  ? . We assume that G contains at least one path from s to t .
 The game is played for J rounds (the move from T to J is deliberate, and hints that our reduction will change the time scale of the original T -step ADMDP game). As in the ADMDP game, the adversary defines the entire sequence of loss functions before the player takes any action. To facilitate the presentation of our reduction, we denote the sequence of loss functions for the BSP problem by g 1 ,...,g J , where each g j : E 7 X  R + . On round j of the game, the player chooses a path P j from s to t and suffers a loss equal to the sum of the losses on the edges in P j . We overload our notation and for any path p we define As its name implies, the bandit shortest path game is played with bandit feedback, so the player observes his loss but not the loss that he would have suffered had he chosen a different path. We note that our reduc-tion actually results in an instance of the semi-bandit shortest path (SBSP) problem, where the player ob-serves the loss along each edge of his path, but for simplicity, we present our results in terms of the BSP problem.
 We define the player X  X  regret by comparing his loss to the loss of the best fixed path from s to t . Formally, let P be the set of all paths from s to t , and define the player X  X  regret as
R BSP ( J ) = E Note that the BSP problem is a stateless online deci-sion problem, which means that the player X  X  choices on round j are not constrained by his choices in the past. In other words, by reducing ADMDP to BSP, we re-move one of the most problematic aspects of ADMDP  X  the player X  X  state. 4.1. The Reduction We are now ready to construct the reduction from AD-MDP to BSP. We use [ i ] k to denote the integer in { 1 ,...,k } that satisfies [ i ] k  X  1 = ( i  X  1) modulo k . In other words, We take the state transition graph G = ( V , E ) from the definition of the ADMDP problem and use it to define n 2 new graphs (where, again, n = |V| ): for each state v  X  V and each integer k  X  { 1 ,...,n } we define the graph G v,k = ( V v,k , E v,k ). The vertex set V v,k includes 2 + ( k  X  1) n vertices: two copies of v and k  X  1 copies of the entire set V . The two copies of v are denoted by v v,k, 0 and v v,k,k . The i -th copy of u  X  V is denoted by u v,k,i . Note that each vertex has three superscripts: the first two identify the graph, while the third distinguishes between the different copies of the original state.
 Next, we construct the set of edges E v,k . For each of v  X  X  outgoing edges in G , ( v,u )  X  E , we add the edge ( v v,k, 0 ,u v,k, 1 ) to E v,k . For each of v  X  X  incoming edges in G , ( u,v )  X  E , we add the edge ( u v,k,k  X  1 ,v v,k,k E v,k . Finally, for each edge ( u,w )  X  E (including the incoming and outgoing edges of v ), we add the set of Our notation is unavoidably tedious, but the construc-tion itself is quite straightforward. We illustrate our construction with a simple example in Fig. 2.
 Finally, we unify the n 2 induced graphs into one big graph G ? = ( V ? , E ? ). Specifically, we add a source node s and a target node t , and for each v  X  V and k  X  { 1 ,...,n } we add an edge from s to v v,k, 0 and another edge from v v,k,k to t . More formally, we define
V
E The important thing to note is that each cycle of length k in the original graph G induces k disjoint paths in G ? from s to t , one for each phase (i.e., each starting point) of the cycle. Specifically, the cycle in G that includes the states u 1 ,...,u k induces a path from s to t through each of the subgraphs G u 1 ,k ,...,G u k On the other hand, every path in G ? from s to t cor-responds to a cycle and a phase in the original graph G .
 Next, we describe how a ADMDP player uses an on-line algorithm for BSP to determine his actions in the ADMDP game. First, the player splits the T steps of the ADMDP game into epochs of length  X  , where  X  is a positive integer that we will specify later on. As-sume, without loss of generality, that  X  divides T , and let J = T/ X  be the number of epochs. Note that epoch j starts on step ( j  X  1)  X  + 1 and ends on step j X  . The player invokes the BSP algorithm once per epoch on the graph G ? ; at the beginning of epoch j , the BSP algorithm chooses a path P j from s to t . By construction, there must be a state u 1  X  V such that P j starts with the edge ( s,u u 1 ,k, 0 and ends with the edge ( u u 1 ,k,k 1 ,t ). Therefore, let sequence of vertices in P j . The player erases the su-perscripts from this sequence, as well as the source and target nodes, and is left with the state sequence u ,...,u k . By construction, this sequence forms a fea-sible cycle in the state transition graph G , in one of the k possible phases.
 Recall that any state in G can be reached from any other state in exactly d steps. The player spends the first d steps in the epoch moving from his current state towards the cycle u 1 ,...,u k . Specifically, on step ( j  X  1)  X  + d the player needs to arrive in state u [ d ] though the player suffers a loss on these d initial steps, and this loss is accounted for in the regret analysis, the player simply ignores it. Since there are J epochs in the entire game, the player ignores the loss on at most Jd steps.
 The player spends the rest of the epoch traversing the cycle u 1 ,...,u k over and over again. On these steps, the player keeps track of the sum of losses. At the end of the epoch, the BSP algorithm expects a feed-back from the environment, which represents the loss assigned to its chosen path P j . The player provides the accumulated sum of losses as the feedback. The BSP algorithm uses this feedback to update itself and then chooses the next cycle P j +1 .
 Our first technical goal is to prove that, from the point of view of the BSP algorithm, it is playing a standard BSP game against a predefined sequence of loss func-tions g 1 ,...,g J . To this end, for each j  X  { 1 ,...,J } , v  X  V , and k  X  { 1 ,...,n } , define g j ( s,v v,k, 0 ) = 0 and g j ( v v,k,k ,t ) = 0. Also, for each j,v,k as above, ( u,w )  X  X  , and i  X  X  2 ,...,k  X  1 } , define The above is a complete definition of g j , for every edge in E ? . Note that this definition is independent of the player X  X  actions, and can therefore be specified at the beginning of the game (obliviously). The following lemma proves that the feedback provided to the BSP algorithm equals g j ( P j ).
 Lemma 4.1. Let P j be the path chosen by the BSP al-gorithm on epoch j and let g j be as defined in (1) . As-sume that the player uses P j to transition through the ADMDP graph G and accumulates losses, as described in the reduction above. Then the loss accumulated by the player and reported to the BSP algorithm at the end of epoch j equals g j ( P j ) = P ( u,w )  X  P Proof. Recall that the actual loss of the player, aside from the first d steps in the epoch in which the player is moving to the start vertex, is given by where V t is the state at time t . Let the sequence of vertices in P j that correspond to the phased cycle u 1 ,...,u k in the original graph. Thus, for the time periods in this epoch we have ( V 4.2. Regret Upper Bound After specifying the reduction to BSP, we state and prove our main result. Let R ADMDP ( T ) be the regret incurred by the ADMDP player that uses the tech-nique defined in the previous section and let R BSP be the regret of the underlying BSP algorithm. The next theorem bounds R ADMDP ( T ) in terms of R BSP . After stating and proving this theorem, we derive concrete bounds on R ADMDP ( T ) using known bounds on R BSP . Theorem 4.1. Let A be a given algorithm for the BSP (or SBSP) problem with a regret bound of: Then, the regret of our ADMDP algorithm is bounded by Proof. Recall that there exists a path of length d be-tween any two states in G , and that d &lt; n 2 . Assume w.l.o.g. that n 2  X   X   X  T , otherwise the theorem holds trivially.
 Recall the definition of R ADMDP ( T ) as
E As detailed previously, the optimal policy  X  is, up to an initialization phase of length at most | V | 2 = n 2 , a phased cycle in the graph which we denote as P  X  = ( v 1 ,v 2 ,...,v k ). Thus, we can write the loss of  X  as In the last inequality we used the fact that the losses are nonnegative. Here J = T  X  is the partition of time into epochs of length  X   X  d . We assume that T is an integer multiple of  X  , else incur an addition  X  regret by ignoring the last at most  X  game steps.
 By construction, the loss incurred by our algorithm is exactly equal to the loss incurred by the paths chosen by the BSP algorithm with the addition of the first d steps of each epoch, in which the ADMDP algorithm incurs an additional loss of at most d . Thus, Combining the previous two observations, we have where the first equality holds since the edge costs of the BSP problem are now bounded by  X  = T J rather than one, as the costs per edge are summed up along the epoch. By the additive definition of regret, this increases the regret of the BSP algorithm by a multi-plicative factor of at most T J .
 The final regret bound we obtain depends on the un-derlying BSP (or SBSP) algorithm used in our con-struction. Possible choices are the BSP algorithms of Abernethy et al. (2012); Dani et al. (2007) or the SBSP algorithm of Audibert et al. (2011).
 Theorem 4.2 (Audibert et al. (2011)) . For the SBSP problem on graphs with n vertices and m edges, the algorithm presented in Audibert et al. (2011) runs in polynomial time and guarantees a regret of R
SBSP ( J ) = O ( m Thus, we obtain Corollary 4.1. The ADMDP algorithm attains R Proof. The graph G ? constructed in the reduction has O ( n 2 m ) edges, where n,m are the number of vertices and edges in G . Thus, applying Theorem 4.1, we get R The corollary is obtained by taking J = T 2 / 3 . In the previous section, we focused only on strongly connected aperiodic graphs. In this section, we deal with graphs with arbitrary period. Recall our as-sumption (made without loss of generality) that G is strongly connected and that our goal is to com-pete with the best fixed policy in hindsight. When a strongly connected graph has a period of  X   X  1, we know that the length of every cycle in the graph is a multiple of  X  (see Sec. 2). This already gives us some useful information about the path induced by the best fixed policy. Moreover, we know that G has a cyclic structure. Let C 0 be the cyclic class that contains the initial vertex v 0 . The cyclic structure of G implies that the best fixed policy starts in C 0 and necessarily returns to C 0 every  X  steps.
 These two facts allow us to refine our reduction. In-stead of defining a graph G v,k for each state v  X  X  and each cycle length k  X  X  1 ,...,n } , we only need G v,k for each cycle length k  X  X   X , 2  X ,..., b n/ X  c  X  } and for each v  X  C 0 (for a total of b n/ X  c| C 0 |  X  n 2 / X  graphs). As in the previous section, we connect a common source s and target t to each graph, and name the resulting graph G ? . As before, each path in G ? from s to t cor-responds to a phased cycle in G that is attainable by a fixed policy, and vice versa.
 As before, we split the T steps into epochs, making sure that the epoch length  X  is a multiple of  X  (in-creasing  X  to a multiple of  X  does not effect the regret rate). Additionally, recall that we begin each epoch by taking d steps toward the chosen cycle; now we must take d X  steps. These choices guarantee that we start each epoch at a vertex in C 0 and that we finish the initial d X  steps of the epoch in a vertex of C 0 . We can now apply the reduction exactly as before. We have modeled the sequential decision making prob-lem as an ADMDP -an MDP with deterministic state transitions, adversarial losses, and bandit feed-back. We presented a new algorithm that significantly improves on the state-of-the-art in terms of regret bounds. Moreover, it applies to the most general class of state transition graphs, whereas all previous algo-rithms relied on generality-limiting assumptions. Several interesting questions remain open. Is a regret bound of O ( T 1 / 2 ) possible in the ADMDP setting or is O ( T 2 / 3 ) the best bound possible? Can our algorithm and analysis be extended to the more general case of stochastic state transitions, without reintroducing the restrictive unichain assumption? Finally, can we allow the adversary to influence the state transition dynam-ics? We leave these questions for future research. We thank Alexander Holroyd for directing us to the coin exchange problem. Part of this research was conducted when the second author was a visiting researcher at Microsoft Research, Redmond. Elad Hazan is supported by the Technion-Microsoft Elec-tronic Commerce Research Center.
 Abernethy, J., Hazan, E., and Rakhlin, A. Interior-point methods for full-information and bandit on-line learning. IEEE Transactions on Information Theory , 58(7):4164 X 4175, 2012.
 Alfonsin, J. Ramirez. The Diophantine Frobenius prob-lem . Oxford University Press, 2005.
 Arora, R., Dekel, O., and Tewari, A. Deterministic
MDPs with adversarial rewards and bandit feed-back. In Proceedings of the 28th Conference on Un-certainty in Artificial Intelligence , pp. 93 X 101, 2012. Audibert, Jean-Yves, Bubeck, S  X ebastien, and Lugosi,
G  X abor. Minimax policies for combinatorial predic-tion games. Journal of Machine Learning Research -Proceedings Track , 19:107 X 132, 2011.
 Bertsekas, D. P. Dynamic Programming and Optimal Control . Athena Scientific, Third edition, 2005. Bremaud, P. Markov chains : Gibbs fields, Monte Carlo simulation and queues . Springer, 1999. Dani, Varsha, Hayes, Thomas P., and Kakade, Sham.
The price of bandit information for online optimiza-tion. In Advances in Neural Information Processing Systems 20 , 2007.
 Denardo, E. V. Periods of connected networks and powers of nonnegative matrices. Mathematics of Op-erations Research , 2(1):20 X 24, 1977.
 Even-Dar, E., Kakade, S., and Mansour, Y. Online markov decision processes. Mathematics of Opera-tions Research , 34(3):726 X 736, 2009.
 Farias, V. F., Moallemi, C. C., Roy, B. Van, and Weiss-man, T. Universal reinforcement learning. IEEE
Transactions on Information Theory , 56(5):2441 X  2454, 2010.
 Feinberg, E. A. and Yang, F. On polynomial cases of the unichain classification problem for Markov deci-sion processes. Operations Research Letters , 36(5): 527 X 530, 2008.
 Neu, G., Gy  X orgy, A., Szepesv  X ari, C., and Antos, A.
Online Markov decision processes under bandit feed-back. In Advances in Neural Information Processing Systems 23 , pp. 1804 X 1812, 2010.
 Ortner, R. Online regret bounds for Markov decision processes with deterministic transitions. Theoretical Computer Science , 411(29-30):2684 X 2695, 2010. Szepesvari, C. Algorithms for reinforcement learning.
Synthesis Lectures on Artificial Intelligence and Ma-chine Learning , 4(1), 2010.
 Takimoto, Eiji and Warmuth, Manfred K. Path ker-nels and multiplicative updates. Journal of Machine Learning Research , 4:773 X 818, 2003.
 Yu, J. Y., Mannor, S., and Shimkin, N. Markov decision processes with arbitrary reward processes.
Mathematics of Operations Research , 34(3):737 X 757, 2009.
 Yu, Jia Yuan and Mannor, Shie. Arbitrarily modu-lated markov decision processes. In Proceedings of the 48th IEEE Conference on Decision and Control ,
