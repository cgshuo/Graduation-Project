 In this paper we investigate the problem of clustering sets of measurements Y which are measured as a 2 variable represents time and we have data on M different individuals, where for each individual we have dimensional) over time. Figure 1 shows an example of such data for a set of 6 different y measurements. The x-axis represents time and the y-axis is the vertical location in pixel coordinates (relative to a fixed coordinate frame), giving the centroid of a person X  X  hand as estimated from a sequence of images. Each Figure 1: Trajectories of the estimated vertical position 6 different video sequences. a particular individual performing a particular simple hand movement. 
In this paper we will use the term trajectory data as a general term to refer to this type of data, emphasizing the notion that the data for each individual is assumed to be a smooth trajectory in y space as a function of an independent variable x. We are interested in being given trajectory data and determining if the data can be naturally clustered into groups. Trajectory data can arise in a variety of applications where repeated measurements are available on individual  X  X bjects X  over time, e.g., response curves in drug therapy monitoring, experimental gene expression data in protein modeling, individual responses to stimuli in animal behavior experiments, and so forth. We are currently investigating the problem of clustering storm trajectories in the atmosphere over the Pacific and Atlantic oceans. Identification of such clusters is important to atmospheric scientists who model and predict storm behavior. Storms are currently clustered using vector-based clustering methods such as K-means which (as we discuss below) require that all trajectories be of the same length. 
An obvious way that one might go about clustering trajectory data is to take all of the nj measurements for an object or individual and form a vector yj of dimension nj . Assume for the moment that each individual has the same number of measurements (i.e., nj = n for all individuals j) and these measurements were all taken at exactly the same 2 values. We can then treat the set of yj trajectories as a set of n-dimensional vectors in an n-dimensional space and use any of a variety of the many clustering methods which operate in vector-spaces. 
While this may be a reasonable approach in some applications, it will not always be applicable or appro-priate. For many data sets, the trajectories will be of different lengths and may be measured at different time points. In addition, the y measurements may be multi-dimensional (e.g., 3d position estimates in tracking the dynamics of a moving object), in which case there is no natural vector representation. One could resort to various ad hoc techniques to try to force the data into a particular representation (e.g., concatenate each of the dimensions together in one long string), but a better ap-proach would handle the natural dimensionality of the data directly. 
By converting the data to a vector representation there is a fundamental loss of information. If we believe from the underlying physics of the data-generating pro-cess that the y X  X  are a smooth function of the x X  X , then this smoothness information is lost when we convert a sequence of n numbers to an n-dimensional vector of numbers. Thus, intuitively, retaining the notion of tra-jectory smoothness in our clustering procedure, should generate better data models compared to throwing away this information. Thus, we investigate model-based clus-tering of trajectories, where each cluster will be modeled as a prototype function with some variability around that prototype. A distinct feature of this model-based approach to clustering is the fact that it produces a de-scriptive interpretable model for each cluster. Since we are estimating smooth functions from noisy data it will be natural to use a probabilistic framework. What we describe in this paper as trajectory data is quite similar to what is known in statistics as longitudinal data or repeated measures data (e.g., IS]). In a slightly more general context the term functional data [lo] is also used to describe these data sets, where in this case x need not necessarily be a time index and it is explicitly assumed that y X  X  are a smooth function of the x X  X . Although such data could be thought of as time-series data for each individual, it is typically the case that the data records per individual are too short to be amenable to conventional time series modeling techniques, thus requiring specialized approaches. Typically repeated measures and functional data analysis methods focus on problems where group (cluster) memberships of the trajectories are known a priori. In contrast we focus here on discovering the groups directly from the data, i.e., clustering. 
Probabilistic clustering using vector representations is well established (e.g., [8], [4], and [12]). However, none of this work extends directly to trajectory cluster-ing unless one uses a vector representation for the data. The work of Stanford and Raftery [14] on mixture mod-els for finding clusters in two-dimensional spatial point patterns is similar in spirit to the approach proposed here. However, the problem of clustering trajectory data is somewhat different to that of two-dimensional curves, since there is an explicit dependence on an in-dependent variable x present in the trajectory case. 
The  X  X ixtures of experts X  models popular in neural networks research, as originally proposed by Jordan and Jacobs [7], is also mathematically quite similar to our cluster models. However, in mixtures of experts, the emphasis is on prediction rather than clustering and there is no explicit focus on obtaining clusters. Furthermore, there is no notion of having sets of trajectories, of possibly different lengths and measured at different points. Thus, although mathematically similar, the focus of the work presented here and the focus of mixtures of experts models are quite different. In probabilistic clustering we assume that the data are being produced in the following  X  X enerative X  manner: 1. An individual is drawn randomly from the popula-2. The individual has been assigned to cluster k with 3. Given that an individual belongs to cluster k, there 
From this generative model, it follows that the observed density on the y X  X  must be a mixture model, i.e., a linear combination of the component models: 
Thus, if we observe the yj X  X , and we assume a particular functional form for the j X  X  components, we can try to estimate from the data what the most likely values of the parameters tik and the weights Wk are. 
The probabilistic framework allows one to address issues such as finding the best number of clusters in a principled and relatively objective manner. For example, one can use penalized likelihood [4]) or cross-validated likelihood [13] to determine the most likely value for the number of components in the mixture model given the data. 
We can straightforwardly generalize the multivariate mixture model above (Eq. (3)) to define mixtures of regression models, where we have measurements y which are a function of some known 2. Each component now is a conditional density function of the form fk(y]~, ok). Assume for now that y and 2 are each l-dimensional. Typically we will assume a standard regression relationship between y and 2, e.g., the mixing weights, and ok is the set of parameters for component k . 
Conditional independence between trajectories, given the model, amounts to assuming that our individuals constitute a random sample from a population of individuals, and allows the full joint density to be written as: The log-likelihood of the parameters 0 given the data bk, and gk (z) is a deterministic linear function of z (linear in the parameters). Thus, in the case of Gaussian noise we have that the conditional density fk(y]x,@k), given that y belongs to the kth group, has mean gk (z) and standard deviation gk. Here ok includes both the parameters of the model gk(Z) and the noise deviation Uk. 
We are now ready to define a probabilistic cluster model for sets of trajectories. Let our data set S consist of nj measurements for each of M individuals, 1 &lt; j 5 M. We will refer to these measurements as beKg a function of time, although this is not strictly necessary. Let the trajectory of measurements for the jth individual be denoted as yj, with the ith measurement of yj denoted as yj(i). Furthermore, suppose that the trajectory of measurements yj were taken at the times in xj. fined as .fk[Yj(i)lxj(i),~k), and is assumed to be a con-ditional regression model as discussed above. We can then define the density of a complete trajectory, given a particular component model k as 
Here we m,ake the standard regression assumption that, conditioned on the model and the x values, the noise is independent at different x points along the trajectory. 
Dependent noise could be modeled if appropriate for a particular application. The task at hand is to pull the mixture components out of the joint density, using S as a guide, so that the underlying group behavior can be discovered. The problem would be simple if it was known to which group each trajectory belonged. A common approach for dealing with hidden data is to employ the EM algorithm [l], [9]. Realizing that if we knew the hidden data, the problem usually becomes a set of much simpler problems, it makes sense to estimate the hidden data, work out the answers to the simpler problems, and then re-estimate the hidden data using our newly found answers. The EM framework gives us a consistent way to estimate the hidden data so that ,C(e]S) is guaranteed to never decrease. This process is then repeated, in theory, until we reach a local maximum in the log-likelihood. However, when used in practice, one normally applies some stopping criterion to halt the iterations (e.g., when the marginal change in log-likelihood falls below some threshold). 
In Eq. (4), the hidden data corresponds to the un-known group membership for each of the M trajec-tories. Let Z be a matrix of indicator vectors zj = said to be generated from the kth mixture component. The joint density of Y and Z given X can be defined as follows. 
This follows from our previous conditional indepen-dence assumptions on yj and yj(i), and the fact that our zj X  X  are independent. The augmented log-likelihood function (also referred to as the complete-data likeli-hood) follows directly from Eq. (5): 
The EM algorithm consists of two steps: (1) the expected value of Eq. (6) is taken with respect to ~(ziy, x, et-l), where tit-r is a current set of param-eters, and (2) this expectation is maximized over the parameters 0 to yield the new parameters et. For the particular form of Z chosen here, the expectation of gels, Z) is 
E[~(elS,Z)1=$1:~hjklo@;Wr where The hjk can be thought of as soft (0 5 hjk 5 1) indicator variables, and the .%jk can be thought of as hard (%jk E (0, 1)) indicator variables. That is, hjk corresponds to the posterior probability that trajectory yj was generated by component k. Note that all of the measurements for trajectory j share this membership probability. 
In Eq. (l), we defined the regression equation for y such that the expected value of y was equal to g(z), a linear function of x. We adapt our notation to fit our data S into this framework by defining the regression equation as follows. with Yj = [l yj (1) * . yj (nj)] X  , and 
In other words, Yj is a column vector formed from the measurements of the jth trajectory, and Xj is an nj by p + 1 matrix whose second column contains the times corresponding to the measurements in Yj (p gives the order of the regression model). We assume that ok is a vector of size nj consisting of zero-mean Gaussians with variance ai, and that pk = [&amp;s @ki . .  X  &amp;] X  is a vector of regression coefficients. By specifying our mixture components as regression models defined by 
Eq. (g), we are setting fk(yj IXj, ok) equal to a Gaussian with mean XjPk and covariance matrix $1. 
The maximization of Eq. (7) with respect to the parameters ok = {wk, Pk, ai} is straightforward. In fact, the solutions for Pk and ai are exactly those obtained from the well known weighted least squares problem [2]. 
The solutions for the regression coefficients @k, the variance terms c$, and the mixing weights tik are given below. 
Above, we let Hk = diag([hik hzk . . . hbk]), with hjk = [hji) hj2,) . . . hjij)]. That is, hj*k is a row vector consisting of nj copies of the membership probability hjk to be shared amongst all the measure-ments of the jth trajectory. 
If we let N = Ey nj, then I&amp; is an N by N diagonal matrix whose elements on its main diagonal represent the weights to be applied to Y and X during regression. 
The weights, in this case, are the membership probabil-ities for each of the trajectories. Thus, they determine how much of an impact the jth trajectory has on the kth regression. We also have Y = [Yi Yk . . . Y X  X ] X , and X = [Xi XL . . . X X ,] X . In other words, Y is an N by 1 matrix containing all the yj(i) measure-ments, one trajectory after another, and X is an N by p + 1 matrix whose second column gives the time points where the corresponding Y values were measured. 
Intuitively, the estimate ai is a form of weighted residual resulting from the regression in the transformed weighted-space, and &amp; is the average proportion of trajectories contributing to the kth regression. These equations yield the following EM algorithm for mixtures of linear regression models: 4.1 Extensions to Non-Parametric 
A useful extension to the framework developed in Sec-as non-parametric regression models. These types of models can be used to relax the assumptions placed on the form of the regression function. This approach is inherently more data-driven. Non-parametric function estimation has been studied in a number of different settings, for example, kernel smoothing [15], local poly-nomial modelling [3], and density estimation [ll]. as non-parametric regression models, we can cluster trajectory data for which the general relationship between y and x is uncertain, or for when we do not wish to make any such assumptions on our regression functions. In this paper, we experiment with the use of kernel regression model components for our densities can approximate any arbitrary function with a series of simple locally-weighted functions, such as linear regression functions. Therefore, we will approximate the unknown function at a point x0, by running a locally-weighted linear regression (of order p) about the point x0, and report the prediction 6 as the height of this fit. The weights are produced by a symmetric kernel (e.g., standard Gaussian density) centered about the point x0, whose purpose is to down-weight points far away from x0. When the random component for the locally fit regression model is Gaussian, the solution for the regression coefficients can be calculated using weighted least squares. 
For our purposes, this means that if we include kernel regression model components into our mixtures of regressions framework, then all we need to modify in our previous algorithm specification is step 2. Instead of requiring that we calculate @k and ii, we require the calculation of the mean yj(i), or predicted value, and variance &amp;, at every point xj (i) , by solving a locally-weighted least squares problem (the weights become the posterior probabilities multiplied by the kernel weights at each point). With these values (and, of course tik), we can proceed to step 3, to calculate our new membership probabilities. 
The only other consideration, here, is the bandwidth for the kernels. The bandwidth determines how spread out the density for a kernel will be. Much has been written on the subject of learning this parameter from the data, for example [3]. In this paper we will just assume a known fixed bandwidth but clearly one could generalize our algorithms to include a  X  X ata-adaptive bandwidth X  component. The complexity of our EM algorithm is O(NKl), where we have N data points in total, K clusters, and perform I iterations (I is often less than 10 or so). When one makes use of kernel regression models, however, the complexity can scale as 0( LNKI) , where L is the number of unique x values in the data set, since we must perform a separate weighted-least squares regression at each of the L z points using (potentially) all of the N y points. 
One further extension to the above framework that we have developed so far, is to include the handling for multivariate yj measurements (or outputs). For exam-ple, suppose we have trajectory data that measures the movement of a particle in two-dimensional space, over time. In this case, our regression equation will look like the following. [Y! X (i) Yt2 X (i)] = [l Xj(i)] f 3 where ok is zero-mean multivariate Gaussian with co-variance matrix &amp;. In other words, now Yj is a multidi-mensional trajectory, and correspondingly, the density 
The steps in our EM algorithm do not change for the multidimensional measurements case, except that in Eq. (8), we replace the univariate density with the above multivariate density, and we calculate full covariance matrices during Step 2. 
Figure 3: Mean log-likelihood and classification error rate performance on test data as the noise level increases from (T = 10 to d = 35. (I = 10,n = 15). clusters/classes). 
Figure 2 shows one such generated data set and a few iterations of our developed EM algorithm applied to this data. The data set shown in Figure 2 was sampled from the three underlying polynomials (three clusters): y = 120 + 4x, y = 10 + 2x + 0.1x2, and y = 250 -0.75x. The number of trajectories sampled from each underlying true function is 1 = 4, and the length of each trajectory is n = 10. In the upper left graph of Figure 2 the plotting symbols (square, circle, and ex) for each trajectory represent its class label (i.e., which polynomial it was generated from), and the line styles are used to differentiate between trajectories within a single class. The upper-right graph shows the same data (with the class labels and lines removed for clarity) along with the representative random starting points (solid lines) for each of the three true models. 
Initially, a proportion of each sequence is randomly assigned to each cluster and weighted least squares is run to get the three initial estimates. Each estimate, represented by a regression line, was obtained assuming a second-order polynomial regression model. The lower-left graph shows the regression lines obtained after iteration 1, and the lower-right graph shows the final regression lines as output by our algorithm (iteration 4). In addition, the lower-right graph shows the true models as dotted lines as well as the learned clustering for the trajectory data, shown by the plotting symbols. 
The clustering/classification is perfect in this case, i.e., all trajectories are assigned to the true clusters which generated them. 
In order to demonstrate the effectiveness of the linear regression mixture model, several different comparison tests are presented here. The experimental results de-scribed below were collected as follows. Two polyno-mials were selected to represent the mean behavior for two different clusters of Gaussian perturbed trajecto-ries: y = 200 + 1.72, and y = 200 + 0.72. Fifty differ-ent randomly generated training sets and test sets were created by randomly choosing some x-values and adding 
Gaussian noise to the function values evaluated at these points. Using these data sets, each of the three cluster-ing techniques (K-means, Gaussian mixtures, and lin-ear regression mixtures) were applied to the sets in or-der to assess performance based on log-likelihood scores and classification error rate tests (note that K-means is not a probabilistic model, and thus will not have log-likelihood scores). Finally, these tests were repeated over eight different noise levels, and the mean values for the attained log-likelihood scores, and the classifica-tion error rates on the test data are reported. 
In the top graph of Figure 3, we see the log-likelihood scores for both linear regression mixtures, and Gaussian mixtures on test data. The training data in these tests each contained 1 = 10 trajectory samples from each class (i.e., from each polynomial), with each trajectory having length of n = 15. The noise level was increased from u = 10 to u = 35 along the x-axis. In the graph, we see that the linear regression mixture model attains a higher likelihood score, on the test sets, at every noise level. 
The bottom graph of Figure 3 compares the cluster-ing (or classification) effectiveness for each of the three clustering algorithms on the same data as above. In the graph, we see that the linear regression mixture model classifies (clusters) the test data with less error, at every noise level, than the other approaches. 
In most tests, the Gaussian mixture model returns a higher log-likelihood on the training data than the regression mixture model does. This appears to be an overfitting effect. The Gaussian mixtures model exhibits this behavior on this type of data because it treats the trajectory data as if it were simple vector data, and thus cannot use the trajectory (or smoothness) information to guide it in the learning process. The natural incorporation of this trajectory information into the regression mixture model is one of its distinct advantages over vector-based clustering approaches. In [5] we describe further experiments in which the number of trajectories and noise-level per class was varied: once again, the mixtures of regression models universally outperformed the vector-based approaches. 
In this section, we apply both the linear regression mix-ture model, and the kernel regression mixture model, to the problem of clustering sequences of images, and show that the extension to multidimensional trajecto-ries is useful. Our data set consists of 20 video streams depicting 5 different hand movements made by an ac-tor: (1) an upward movement (Up), (2) a downward movement (Down), (3) a left-to-right movement (Left-
Right), (4) a right-to-left movement (Right-Left), and (5) a diagonal movement acting from the bottom-left to the top-right of the frame (bLeft-tRight). All movement directions are from the perspective of the actor. There are 4 samples (video streams) for each movement. Fig-ure 5 displays some sample images from these streams. sional trajectory, in pixel coordinates, measured over time (or frames). The trajectory crudely follows the movement of the hand by tracking the centroid of the image difference between frames. Since each of the tra-jectories are of different lengths (each video may contain a different number of frames) and are multidimensional, vector-based clustering such as K-means or Gaussian mixtures is difficult to apply here without some further (non-obvious) processing of the data. groups, based on our estimated trajectory data. The data is clustered using both a linear regression mixture model, and a kernel regression mixture model (fitting locally-weighted linear regressions of order 1). The trajectory data is input to our algorithms without scaling or registering them in any way. Since the trajectory data is two dimensional, we will regress two dimensional output vectors yj(i) = [yj (i)(l) yj (i) X  X )] on a univariate xj (i) representing time (frame number), and our density on yj will be the multivariate Gaussian with mean gk(Xj) and covariance matrix &amp; (note that, in this case, ck is a 2 by 2 matrix). four example trajectories for each of the Down, Up, and Right-Left movements with the solid, dotted, and dashed lines, respectively. These lines depict the movement of the hand in the horizontal direction. The Figure 4: The top graph shows example trajectories for three movements. The bottom graph superimposes cluster lines returned from kernel regression mixtures onto some trajectories from above. bottom graph of the figure shows the kernel regression lines returned for these three movements, superimposed on a single example trajectory (for each movement) from the above graph. Here, we see that our algorithm is able to find and describe the different movements in an unsupervised manner. 
The picture in Figure 6 gives the resulting clustering from running linear regression mixtures on the video data. The line marked True, in the picture, represents the true  X  X attern X  of clustering for the video data. It shows that trajectories l-4 are in group one, 5-8 are in group two, 9-12 are in group three, 13-16 are in group four, and 17-20 are in group five. The lines marked 
Vertical and Horizontal show the patterns of clustering when our algorithm is only allowed to look at one of the dimensions of the data at a time. It is clear that these patterns do not match the True clustering. However, movement. Figure 6: The graph shows clustering/classification comparisons based on regressing only on the vertical trajectories, the horizontal trajectories, or on both trajectories (20). The line labeled Trve gives the pattern of the true clustering. Only the 20 regression achieves the correct clustering. the line marked 20 shows the pattern of clustering if our algorithm is allowed to regress the full 2-dimensional output trajectories yj on the univariate xj. In this case, the true pattern of clustering is found, and we find value in the multi-dimensional extension to the problem. The same sort of caricature can be seen when kernel regression components are inserted into the EM algorithm. 
The graphs in Figure 7 show the horizontal regression curves returned for each of the clusters using both kernel (top) and linear (bottom) regression mixtures in 2D. 
The curves describe the mean-movement behavior in the horizontal direction. Note (for example) that the 
Left-Right and Right-Left curves appear as inverses in the horizontal direction (as they should, since they are inverse movements in this direction), but they appear similar in the vertical direction (not shown). We can also see that the bleft-tRight curve appears inverse to the Right-Left curve in the horizontal direction. This nicely fits with our understanding of these movements in the real world. In terms of the difference between the kernel (top) and linear (bottom) models, one can clearly see that the linear means in the bottom plot are simple summaries of the kernel means on the top. 
The probabilistic framework allows for a variety of extensions which were not discussed in this paper due to space limitations. In particular, the number of clusters and the functional form of the component models can in principle be determined automatically using penalized likelihood or cross-validated likelihood. 
Another direction for generalization is to allow linear shifts and scaling of the trajectories such as replacing g(z) by g (aa: + b) where a and b are scaling and translation parameters specific to each trajectory which are estimated from the data. 8 Conclusions 
In this paper we investigated the problem of cluster-ing trajectory data. Traditional vector-based cluster-ing algorithms are inadequate in many cases for these types of data sets. We introduced a probabilistic mix-ture regression model for such data and showed how the EM algorithm could be used to cluster trajectories. 
The model-based assumption can be relaxed to allow Figure 7: Estimated component model lines as returned from our EM algorithm for mixtures of kernel regression models (top) and linear regression models (below), horizontal component. for non-parametric regression components and the tech-nique can also be easily extended to handle multivariate trajectories. We demonstrated that the proposed ap-proach outperforms the more traditional vector-based approaches in simulation experiments and illustrated the utility of the method on video data sequences. The work described in this paper was supported by the National Science Foundation under Grant IRI-9703120. The authors gratefully acknowledge the contributions of Steve Cody for providing the video data. [l] Dempster, A.P., Laird, N.M., &amp; Rubin, D.B. [31 [51 [71 P31 [14] Stanford, D., and Raftery, A.E. Principal curve [15] Wand, M.P., and Jones, M.C. Kernel Smoothing. 
