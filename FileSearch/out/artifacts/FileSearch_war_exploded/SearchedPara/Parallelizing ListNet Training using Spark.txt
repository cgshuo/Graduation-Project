 As ever-larger training sets for learning to rank are cre-ated, scalability of learning has become increasingly impor-tant to achieving continuing improvements in ranking accu-racy [2]. Exploiting independence of  X  X ummation form X  com-putations [3], we show how each iteration in ListNet [1] gra-dient descent can benefit from parallel execution. We seek to draw the attention of the IR community to use Spark [7], a newly introduced distributed cluster computing system, for reducing training time of iterative learning to rank al-gorithms. Unlike MapReduce [4], Spark is especially suited for iterative and interactive algorithms. Our results show near linear reduction in ListNet training time using Spark on Amazon EC2 clusters.
 H.3.3 [ Information Search and Retrieval ] Learning to Rank, Distributed Computing
A potentially easy way to improve learning to rank ac-curacy is to simply train on more data. The challenge, of course, is scalability [2]. Prior work showed that many ma-chine learning algorithms, such as those based on a  X  X um-mation form X , can be easily parallelized [3]. While summa-tion terms can be independently computed via MapReduce, MapReduce is not well-suited to computing iterative learn-ing algorithms and relatively inefficient for such usage.
Instead, we adopt the recently developed Spark 1 cluster computing system [6]. Spark is not only well-suited to such iterative (and interactive) algorithms such as gradient de-scent, but it runs on existing Hadoop 2 clusters. Spark sup-ports reuse of a working set of data across multiple parallel operations via a distributed memory abstraction called Re-silient Distributed Datasets (RDDs), which support parallel, in-memory computations on large clusters while retaining similar fault tolerance as MapReduce to reconstruct a lost partition whenever faults occur. As with Hadoop, the same (a) Relative speedups on the 10K train-ing set as more CPUs are used. range 10  X  4 to 10  X  2 and chose the largest one that did not lead to divergence. Standard MSLR dataset partitioning of queries facilitates computation of NDCG accuracy using five-fold cross-validation. Average NDCG@10 ranking ac-curacy achieved over all five test folds of MSLR-WEB10K is 0 . 252; greater NDCG accuracy might be achieved by us-ing more sophisticated line search procedures in the gradient descent algorithm shown in Figure 1.

Figure 2(a) shows that as the number of iterations in-creases, the overhead of using Spark reduces, achieving lin-ear speedup in training time. Figure 2(b) shows that with a parallelism of 10 we achieve roughly 11x speedup on the MSLR-WEB10K dataset as compared to serial implementa-tion. When the degree of parallelism is 1 and the cache is insufficient, some recomputations might become inevitable leading to slightly super-linear speedup. Figure 2(c) shows that training times for MSLR-WEB30K, which has 3x as many queries as MSLR-WEB10K, is actually less than 3x the corresponding times for MSLR-WEB10K, showing greater benefit from parallelization as the training size increases. [1] Cao, Z., Qin, T., Liu T.-Y., Tsai M.-F., and Li, H.:
