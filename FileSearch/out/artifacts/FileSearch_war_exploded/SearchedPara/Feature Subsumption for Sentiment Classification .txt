 With the growing availability and popularity of online user-generated information, including reviews, forum discussions, and blogs, sentiment analysis and opinion mining ( X  X entiment analysis X  and  X  X pinion mining X  denote the same field of study [1]) have become one of the key technologies for handling and analyzing the text data from internet. One of the most widely-studied sub-problems of opinion mining is sentiment classification, which classifies evaluative documents, sentences or words as people automatically identify the viewpoints underlying the online user-generated text classification techniques [4]. 
Up to this date, machine learning-based methods have been commonly adopted for sentiment classification due to their outstanding performance [3, 4]. An open problem in machine learning-based sentiment classification is how to extract complex features that outperform simple features; figuring out which types of features are most valuable is another [5]. Most of the existing research focus on simple features, been considered in sentiment classification. In fact, the substring-group features based allowing sub-word features and super-word features to be exploited automatically. With such approaches, the messy and rather artificial problem of defining word boundaries in some Asian languages can be avoided, and non-alphabetical features can be taken into account. Furthermore, different types of documents can be dealt with in a uniform way. 
In this study, the substring-group features are extracted and selected for sentiment substring-group features are extracted from the suffix tree constructed by the training documents and unlabeled test documents, the structural information of unlabeled test documents is incorporated to feature extraction. Secondly , the extracted substring-SVM is adopted to classify the unlabeled test documents based on the selected languages, Chinese, English and Spanish, and the experimental results demonstrate the effectiveness of the proposed algorithm. 
The rest of this paper is organized as follows. Section 2 reviews the learning paradigms and the related work. The proposed algorithm is described in detail in Section 3. The experimental setup is illustrated in Section 4 and the results are given and analyzed in Section 5. Finally, this paper is summarized in Section 6. Learning paradigms: Given an example x and a class label y , the standard statistical Y X predictions. Y test are completely hidden during training time. In the case of semi-supervised inductive learning [10-12], the learner is also provided with auxiliary unlabeled learning as a special case of semi-supervised learning in which X auxiliary = X test . Related work: Sentiment classification can be performed on word level, sentence generally classified into two categories, unsupervised approaches and supervised approaches. 
The unsupervised approaches focus on identifying semantic orientation of individual words or phrases, and then classifying each document in terms of the number of these words or phrases contained in each document. Turney determines semantic orientation by phrase Pointwise Mutual Information (PMI) based on pre-defined seed words [15] and rates reviews as thumbs up or down [16]. Kim and Hovy [17] build three models to assign a sentiment category to a given sentence by customer reviews using a holistic lexicon [18, 19]. Kennedy and Inkpen determine the sentiment of customer reviews by counting positive and negative terms and taking into account contextual valence shifters, such as negations and intensifiers [20]. Devitt and Ahmad explore a computable metric of positive or negative polarity in financial news text [21]. Wan uses bilingual knowledge and ensemble techniques for unsupervised Chinese sentiment analysis [22]. 
The supervised approaches focus on training a sentiment classifier using labeled corpus. Since the work of Pang et al. [4], various classification models and linguistic features have been proposed. Dave et al. use machine learning based methods to classify reviews on several kinds of products [23]. Pang and Lee report 86.4% accuracy rate of sentiment classification of movie reviews by using word unigrams features for SVMs [3]. Mullen and Collier also employ SVMs to bring together diverse sources of potentially pertinent information, including several favorability measures for phrases and adjectives and knowledge of the topic of the text [24]. Most recently, Li and Sun compare the performance of four machine learning methods for investigate domain adaptation for sentimen t classifier [25]. Songbo et al. combine learn-based and lexicon-based techniques for sentiment detection without using labeled examples [26]. 
To the best of our knowledge, though substring-group features have been used for documents is used in this paper by transductive learning , which has not been studied feature extracting and feature selecting has been reflected in this study. According to the conclusions from the learning paradigms in subsection 2.1 , not only the training documents, but also the unlabeled test documents can be used at training time by the transductive learning-based algorithm [10]. The proposed algorithm takes the training documents ( both text and class labels ) and the unlabeled test documents documents. The framework of the proposed algorithm is shown in Figure 1 , including four stages: substring-group feature extracting , term weighting, feature selecting and classifying. 3.1 Substring-Group Feature Extracting The unique substring-group features are extracted by the following steps. documents and unlabeled test documents. The suffix tree is constructed by Ukkonen X  X  algorithm with O(n) time complexity, where n is the number of characters in the text corpus [27]. This step shows the incorporation of transductive learning . 
In step ( b ), the key-nodes are extracted from the constructed suffix tree. For an m-internal nodes [28]. The text corpus X  X  length m is usually a very large number, so it X  X  criteria proposed in [9] is used in this paper, and the recommended values are adopted: L=20, H=8000, B=8, P=0.8 and Q=0.8. The meanings of the parameters are listed in Table 1. In step ( c ), every suffix of each document is matched with the suffix tree, and all the IDs of the matched key-nodes are taken as the content of the corresponding document. 
In step ( d ), from the training part of the converted documents, all the unique key-that the evaluation of the following experiments is open test . using the unique feature table produced by step ( d ). 
According to the definition of suffix tree [28], each node of the suffix tree represents a substring-group of the text co rpus. Therefore, the extracted key-node IDs are also called substring-group features in this paper. Moreover, the time complexity of the computing steps a, b and c is linear, which has been proved in [9]. 3.2 Term Weighting Consequently, both term presence ( X  X ool X ,  X  X hree X ) and term frequency ( X  X f X  and  X  X fidf-c X ) are used in this paper. The  X  X fidf-c X  is the variants of standard  X  X fidf X , and it is widely used in text classification [29, 30]. The four adopted term weighting approaches are defined as formulas 1, 2, 3, and 4. documents the term  X   X  occurs in; N is the total number of training documents. 3.3 Feature Selecting the simplest criterion for feature selection and can be easily scaled to a large dataset with linear computation complexity. It is a simple but effective feature selection method for text categorization [31]. In this study, DF is used to pick out the discriminating substring-group features for training and classification. 
For DF (Document Frequency) calculation, we compute the document frequency highest scores. The basic assumption is that the rare features are either non-informative for class prediction, or not influential in global performance. 3.4 Classifying predict the classifications of the unlabeled test documents. Due to SVMs X  outstanding performance [3, 4, 6, 8, 24, 32], SVMs are adopted in this paper. The SVM light package is used for training and testing with default parameters. 4.1 Datasets The proposed algorithm has been tested on three open datasets in three different languages: Chinese , English and Spanish . Table 2 gives a short summary of these open datasets. http://www.ctrip.com/ , which is one of the most well-known websites in China for hotel and flight reservation. The  X  English _1400 X  is most commonly used reviews on cars, hotels, washing machines, books, cell phones, music, computers, and movies. Each category contains 50 positive and 50 negative reviews, defined as positive or negative based on the number of stars given by the reviewers. fold , 3-fold and 3-fold cross validation are used respectively in the following experiments. 4.2 Evaluation Metrics To evaluate the performance of the proposed algorithm for sentiment classification, categorization [30]. In addition, microF1 and macroPrecision are also computed to compare with the related work. 5.1 Comparisons typical methods on each dataset are listed in Table 3, respectively. The column  X #Features X  is the number of features when the best performance is achieved. 
As illustrated in Table 3, although the proposed algorithm (shown in gray background) does not use any preprocessing steps, such as word segmentation and stemming, it outperforms the character or word Ngrams based methods on three different language datasets. Note that all these datasets are processed by the proposed observation is that the number of features (#Features) used in the proposed algorithm of the proposed algorithm is at the cost of high feature dimension. 5.2 Multilingual Characteristics Since the proposed algorithm treats the input documents as character sequences regardless of their syntax or semantic structures, no word segmentation technology is needed. Consequently, the proposed algorithm can deal with any language in any encoding, which has been demonstrated by the experiments in Table 3 . 
Furthermore, the proposed algorithm is capable of handling text corpus containing both English and Chinese words at the same time. We conduct an experiment on the mixed-language dataset, including the  X  X nglish_1400 X  corpus and 1,400 Chinese reviews (700 pos + 700 neg) randomly selected from the  X  X hinese_16000 X  corpus. Three-fold cross validation is adopted. The experimental results are shown in Figure 2. As is shown in Figure 2, the proposed algorithm achieves promising performance (shown in dark blue curve) on the mixed-language dataset, which is even better than the performance obtained by using only the English corpus. 5.3 Feature Frequency vs. Feature Presence The performance of sentiment classifier is highly affected by the text representation. To show the impact of the term weighting approaches, we conduct a series of experiments. 
As demonstrated in Figure 3, different term weighting approaches lead to different classifying performances. Among all the term weighting methods, the  X  X fidf-c X  while the  X  X f X  performs the worst. The  X  X ool X  always achieves better performance than the  X  X hree X . 
This observation agrees with Pang X  X  finding: the better performance is achieved by accounting only for feature presence ( X  X ool X ), not feature frequency ( X  X f X ) [4]. However, the advanced feature frequency ( X  X fidf-c X ) is superior to the feature every experiment in the following subsections. 5.4 Influence of Feature Selecting Figure 3 also displays the effectiveness of feature selecting to sentiment classification. As illustrated in Figure 3, the DF-based feature selection method can eliminate up to 50% or more of the unique substring-group features with either an improvement or no addition, Table 3 shows that all the best performances achieved by the proposed algorithm have used DF-based feature selection methods. 
Based on above observations, we draw the following conclusions: the extracted substring-group features in step 1 are redundant and the feature selecting methods should be further used to eliminate the redundancy among the extracted substring-group features. 5.5 Transductive Learning vs. Inductive Learning The following experiments show the effectiveness of using the transductive learning-based algorithm instead of inductive methods. Figure 4 gives the experimental results on the three open datasets in three different languages. 
As demonstrated in Figure 4, the transductive learning (shown in green and red curves) based algorithm is well situated for sentiment classification. With the growth performances of transductive learning based algorithms improve significantly. 
Seen from the data shown in white background in Table 3 and the dark blue curves algorithms in inductive learning setting is in ferior to the algorithms using character or word Ngrams features, which illustrate transductive learning X  X  importance to sentiment classification from another perspective. 
The reason for the improvement by transductive learning is that the more unlabeled test documents are added to the construction of the suffix tree, the more complete the substring-group features from the suffix tree. Result is the converted documents being more representative of the original text documents. So the unlabeled test documents X  subsumption for sentiment classification. In this study, both feature extracting and feature selecting are incorporated into proposed algorithm combines the substring-group features with transductive learning. 
Experiments have been conducted on three open datasets in three different proposed algorithm achieves better performance than the existing algorithms, without any preprocessing steps (word segmentation, stemming, etc.). Furthermore, the proposed algorithm proves to be multilingual , and it can be directly used for sentiment classification with any language in any encoding. In terms of term can significantly improve the classifiers X  performance by incorporating the structural information of unlabeled test documents . improve the classifier. In addition, more feature extracting methods will be explored to improve the overall performance of sentiment classification. This work is supported by National Natural Science Foundation of China (Grant No: 60405011, 60575057 and 60875073). 
