
A pervasive problem in large relational databases is identity uncertainty which occurs when multiple entries in a database refer to the same underlying entity in the world. Relational databases exhibit rich graphical structure and are naturally modeled as graphs whose nodes represent en-tities and whose typed-edges represent relations between them. We propose using random walk models for resolv-ing identity uncertainty since they have proven effective for finding points which are proximately located in a network. Because not all types of relations are equally helpful in al-leviating identity uncertainty, we develop a supervised ap-proach to learning the usefulness of different database re-lations from a training set of database entries whose true identities are known. When tested on the task of resolving uncertainty of ambiguously named authors in bibliograph-ical data, the learned random walk models yield perfor-mance superior to support vector machines, and to a related spectral clustering method.
Many authors have proposed using random walk models for inducing similarities over relational data represented as a network or graph [26, 21, 17]. Such models capture an intuitive notion of graph similarity such that the similarity of nodes grows as a function of the number of short short between them. Consequently, two nodes are similar if they lie in nearby, well-connected regions of the network.
Here we propose using random walk models for re-solving identity uncertainty in bibliographic data. Iden-tity uncertainty is a problem which occurs when names in a database are not in one-to-one correspondence with the entities they refer to [27]. Random walk models are well suited to resolving identity uncertainty since coreferent database entries X  X .e. entries which refer to the same entity in the world X  X ill possess similar or identical relationships to other coreferent entries. Hence there will be many chains of commonality linking them, or linking the items to which they are linked. For example, duplicate entries of an article in a bibliographic database might possess fields indicating that their titles are similar, that they share some coauthors in common, and that they were published in the same jour-nal. Consequently, when viewed as nodes in a graph, these duplicates will have many edges which connect them.
While other authors have proposed using random walks for resolving identity uncertainty [21, 26], we use machine learning to learn the importance of different database re-lations in terms of how helpful they are at resolving iden-tity uncertainty. Such learning is critical in feature rich databases which contain many different relationships, each indicating potential similarities between database entries.
Identity uncertainty is a problem faced when trying to extract even the simplest information from many real-world, bibliographic databases. For example, it is common to measure the impact of an author by the number of articles that he or she has published. But as discussed by Han et al., highly prolific authors often turn out to be amalgams of sev-eral individuals, all with the same or similar names [15]. A related problem occurs in biomedicine where, due to the un-coordinated efforts of different research communities, dis-tinct genes are given the same name or abbreviation. Chen et al. found that in a sample of nearly 150,000 official gene symbols collected from 21 species, identical symbols ex-isted in one of the other 20 species roughly 14% of the time [7]. The converse problem, in which a distinct gene possesses multiple names is also widespread; it is common for genes to have both long-form and abbreviated names, as well as to be called by different names by different re-search communities. Bioinformatic analysis is slowed by difficulties in determining which genes are meant by which names [32].

The task of resolving name ambiguity has itself been called by many different names, including identity uncer-tainty, reference reconciliation, record linkage, the merge-purge problem, and entity resolution [11, 13, 16, 5]. Early work on entity resolution treated the problem as a binary classification task such that matching decisions were con-sidered in isolation or in sequence [13, 6, 3]. More recent work has framed the problem as a collective classification problem where a global cost is defined over the entire set of match decisions [4, 8, 29, 10]. In richly relational data, a collective approach is appropriate since different matching decisions need to be considered simultaneously in order to preserve the relational semantics of the data. Unfortunately, minimizing the global cost of these collective models is NP-complete, so optimization must be done in a series of local refinement steps.

The method we propose shares features of both the col-lective and the independent approach. Similar to collective approaches, we model the relational structure of the data using graphs whose nodes represent entity references and whose edges represent typed relations between them. How-ever, rather than performing classification directly on this graph, as the collective approaches do, we instead use the graph to induce a random-walk based similarity, or distance, on the entity references. We then use this similarity to make independent classification decisions. The similarity that we define between references is the probability that a random walk started from one reference ends at another.

As mentioned above, we propose a method for learning the weight of each relation type. Specifically, each relation type represents a type of transition in an absorbing Markov chain and, given training data, we show how to learn the im-portance of these transition types so that random walks will connect references which refer to the same entity. Due to the parametric form of our model, the similarity we learn is symmetric and induces a distance on the graph known as the resistance distance [33]. We note that while some authors have used random walk models or related spectral cluster-ing methods for resolving identity uncertainty [15, 21, 26], our work addresses the problem of learning the importance of different transition types, a problem which has received less attention in the literature. When tested on the task of resolving uncertainty among ambiguously named authors in bibliographical data, our method of learned random walks gives superior performance to support vector machines and to a related spectral clustering method [22].
In what follows, we assume we are given an undirected graph G = ( V, E ) whose vertices are entity references and whose edges represent similarities between those references in terms of how likely they are to corefer. The edges of the graph have weights which reflect the amount of similar-ity. Greater weight means greater similarity while a weight of zero means no similarity. The graph X  X  edge weights are stored in a matrix W whose entry W uv denotes the weight
Figure 1. The graph is extended such that a sink node (gray) is attached to each transient node (white). S denotes the set of sink nodes and T denotes the set of transient nodes. of the edge between references u and v .

In the problem of author-name disambiguation, each ver-tex represents a reference to an author on a particular pa-per. So if u refers to the i  X  X h author on the p  X  X h paper of the citation database, and v refers to the j  X  X h author of the q  X  X h paper of the database, then W uv reflects the degree to which we believe that u and v refer to the same person in the world. Factors which would determine the value of W uv might include the string edit-distance between the i  X  X h name on the p  X  X h paper and the j  X  X h name on the q  X  X h paper (i.e., the string edit distance between the names associated with u and v ), the number of words shared between the titles of pa-pers p and q , whether p cited q (or vice versa), and whether or not p and q were published in the same journal.
Given an undirected, weighted graph with weight ma-trix W , we can construct a transitive similarity score be-tween author references by considering random walks of unbounded length on the graph. We define this similar-ity score in terms of an absorbing Markov chain [12], or equivalently, a lazy random walk on the graph. To do this, we construct a probability transition matrix P by nor-malizing the rows of W so that they sum to one: P ij = W ij / P k W ik . The entry P ij = P ( j | i ) denotes the prob-ability that a random walk at vertex i will be at vertex j in the next time-step. The transitive similarity score that we wish to induce from P is the probability that a random walk started from vertex i ends at vertex j after a sequence of steps. However, to make this definition precise, we need to impose some stopping conditions so that walks actually terminate.

We do this by attaching a terminal node, or  X  X ink, X  to each vertex and requiring that a walk stops once it hits a sink. That is, for each vertex i , we create a sink node i and link i to i sink with an edge of weight one. A walk at a non-sink vertex i is allowed to transition to another non-sink node j with probability P ( j | i ) = W ij / (1 + P k W ik can exit to its sink node i sink with probability P ( i sink | 1 / (1 + P j W ij ) and terminate. Nodes which are not sinks are called  X  X ransient X  nodes. Figure 1 depicts the construc-tion.

With this construction in hand, we now define the transitive/mulit-step similarity between two nodes to be the probability that a random walk started from one node ends at the other node X  X  sink. That is, for two nodes i and j , we define the transitive similarity between i and j to be We call  X  A  X  the absorbing probability matrix and its entries A ij give the probability of landing in sink node j sink when from transient node i .
We now show how to compute A analytically. For nota-tional convenience, it is easiest to assume that the vertex set V , edge set E , and weight matrix W , have all been extended to contain the added sink nodes and the transitions to them. Partitioning V into the set of sinks, S , and transient nodes, T , and ordering the sink nodes ahead of the transients, W can be written in block form as
Because the nodes of S are sinks, they have no outgo-ing transitions, or equivalently, they only transition back to themselves. Thus W SS = I , the identity matrix, and W
ST = 0 , the zero matrix. Additionally, if we assume an ordering of the sink nodes such that the i  X  X h vertex of S is the sink for the i  X  X h vertex of T , the matrix W T S can also be written as the identity matrix, I , because we assumed that the edges connecting transients to sink nodes had weight one. Finally, we can analogously decompose P into P SS , P
ST , P T S and P T T by normalizing the rows of W as it is written in equation (2). This allows us to compute the ab-sorbing probability matrix, A , as Here we have used the fact that ( P k ) ij is the probability that a random walk starting at node i lands in node j at k time-steps in the future.

Intuitively, equation (3) says that A ij is the probabil-ity that a random walk started at transient node i bounces around the transient nodes for some number of steps, lands at transient node j and then exits to sink j sink . The matrix inverse in equation (3) arises from the sum of a convergent geometric series and is well defined because the rows of P
T T sum to less than one, making ( I  X  P T T ) diagonally dominant. Since a random walk on the graph must eventu-ally terminate at some sink node 1 , the matrix A is itself a probability transition matrix.

Surprisingly, A is also symmetric. If we define D to be the diagonal degree matrix where D ii = P j W ij and rewrite P as D  X  1 W , it is easy to show that where we have written D as D = D S 0 0 D ( D T  X  W T T ) is symmetric and therefore so is its inverse. W
T S is equal to the identity matrix which yields the desired result.

As discussed in [33], the matrix A is an inner product on the vertices of the graph and therefore has an associated distance called the resistance distance: Consequently, either the similarities given by A , or the asso-ciated distances, can be used to resolve identity uncertainty.
In practice, we are not simply given a graph correspond-ing to an entity resolution task but instead must construct one from the relational database. A typical database con-tains many types of relations, only some of which pro-vide relevant information for solving the entity resolution problem. We make the assumption that these relations are represented as a set of symmetric similarity matrices measures between the entity references of the database. To take our running example of ambiguous author references in a bibliographic database, one similarity matrix, W ( r )  X  W will contain the string edit-distances between the names as-sociated with the author references, while another similarity matrix, W ( s )  X  W , will be the binary-valued matrix whose entries W ( s ) ij are zero or one depending on whether the arti-cle associated with author reference i cited the article asso-ciated with author reference j (or vice versa).

From the collection of similarity matrices, W , we con-struct the weight matrix W T T so that its entries, ( i, j ) , are a pointwise log-linear combination of the ( i, j )  X  X h entries of the matrices in W . That is, The parameter  X   X  R R is a vector of mixing coefficients which determines how much emphasis is placed on each similarity matrix, W ( r ) . This parametric form for W T T be viewed as a set of experts who vote independently on which type of transition to take and has been used exten-sively in the spectral clustering literature [1, 22, 34]. Ad-ditionally, it later allows us to perform unconstrained op-timization in order to learn the coefficient vector,  X  , since it ensures that the entries of W T T are never negative. We similarly define where exp(  X  sink ) can be interpreted as a bias weight which determines how likely a random walk is to exit to a sink-node at each step. The mixing coefficients  X  and  X  sink can either be set by hand using domain expertise, or as we show next, they can be optimized through supervised learning.
For databases containing a small number of entity rela-tions, reasonable values for the mixing coefficients can be chosen by hand. However, when there are many attributes, manually choosing optimal values for  X  and  X  sink becomes arduous. Thus, we give a supervised learning procedure for choosing these values when the true identities are known for a small set of entity references in the database.
We assume that this knowledge is given to the learning procedure as a training set  X  V L , K  X  , where V L  X  V is the subset of nodes in the graph whose true identities are known and K : V  X  V  X  { X  1 , 0 , +1 } is the labeling function which labels pairs of nodes as being coreferent or not: K ( i, j ) = We define K ( i, i ) = 0 because we know that a node is coreferent with itself and do not need to learn this infor-mation in training.

We denote the set of pairs of nodes which are known to be coreferent as K + = { ( i, j ) | K ( i, j ) = +1 } and similarly, the set of nodes known not to be coreferent as K  X  = { ( i, j ) | K ( i, j ) =  X  1 } . Additionally, we define K i = { j | ( i, j )  X  K
Given the set of weight matrices W and a training set  X  V
L , K  X  , the learning task is to find a vector  X  which in-creases the probabilities A ij for all ( i, j )  X  K + and de-creases the probabilities A ij for ( i, j )  X  K  X  . To do this, we take the straight-forward approach of maximizing the sum of the log-probabilities P ( i,j )  X  K + log A ij . However, because walks can exit at unlabeled nodes, maximizing the probabilities for pairs in K + is not guaranteed to mini-mize the probabilities of pairs in K  X  . Consequently, it is necessary to add an explicit penalty on the entries A ij for ( i, j )  X  K  X  .

We give two related objectives for doing this. The first is analogous to way we handled pairs in K + . For all ( i, j )  X  K  X  , we try to maximize the quantity, log(1  X  A which is large when A ij is small. Incorporating this into our objective gives us the following function to maximize:
J 1 ( V L , K,  X  ) = Here we have included the ridge penalty k  X  k 2 = P r  X  2 to prevent the entries of  X  from getting too big X  X  common problem in training log-linear models. The hyper-parameter  X  controls how much the objective focuses on K + versus K  X  , and the hyper-parameter  X  controls the contribution of the ridge penalty to the overall objective.

In our second version of the objective function, rather than minimize the individual entries A ij for ( i, j )  X  K we minimize the overall probability of ending up at a non-coreferent node. That is, for all nodes i we minimize ing a walk at node i and ending up any node j which is known not to be coreferent with i . This objective therefore controls for the overall likelihood of a bad event rather than controlling for individual bad events. Under this alternate objective, we try to maximize the function:
J 2 ( V L , K,  X  ) = As in equation (8), maximizing log(1  X  P ( K  X  i | i )) makes P ( K  X  i | i ) small. The second summation is normalized by | V L | because it is a sum over | V L | terms.

In our experiments, we maximize J 1 or J 2 using gradient ascent. The gradient is relatively straight-forward to calcu-late using the chain rule. The only tricky part is the partial derivative,  X  X / X  X  , which is found to be where N = ( D T  X  W T T )  X  1 is the inverse of the graph Laplacian, but where the rows of W T T sum to less than the corresponding rows of D T , due to the fact that each tran-sient node has an additional edge connecting it to its sink.
When the input similarity matrices W ( r ) are dense, equation (10) scales as O ( R | T | 3 ) since the matrix multipli-cations are O ( | T | 3 ) and the derivative needs to be computed with respect to the R = |W| matrices. In practice, most of the similarity matrices are extremely sparse so the cost of equation (10) is closer to O ( R | T | 2 ) . However, inverting the matrix ( D T  X  W T T ) is O ( | T | 3 ) and thus approximations must be used for graphs containing more than a few thou-sand nodes. The datasets in our experiments were small enough to compute the matrix inverse directly.
Identity uncertainty in bibliographic databases is a well-documented problem, especially for author names [20]. Be-cause each author of an article is represented only by a text string spelling his or her name, confusion arises when two or more authors publish under the same name or when a single author publishes under slightly different names. For example, there are over 25 authors in the DBLP publishing under the name  X  X ei Li, X  while the publications of the sin-gle author  X  X ernando C. N. Pereira X  are attributed to three separate individuals:  X  X ernando C. N. Pereira, X   X  X ernando C. Pereira, X  and  X  X ernando Pereira. X  Additionally, there are at least four other individuals named  X  X ernando Pereira X  in the DBLP. Such ambiguity makes it difficult to search for articles by a specific author, properly assess an author X  X  im-pact, or discover research communities [14].

We formulate the problem of resolving author identity uncertainty as learning the weight matrix W so that cita-tions containing mentions of the same author will have high random-walk similarity. Each citation is represented by a transient node in the graph and is linked to its corresponding sink via the edges in W T S (see section 2.1). The adjacency matrices in W correspond to particular relations between citations such as the number of coauthor names that two ci-tations have in common, the TF-IDF similarity of document titles, and the TF-IDF similarity between venue names. We also extract adjacency matrices corresponding to individual words in document titles and to similarities between doc-ument titles and journal names which have been projected into a low-dimensional space.

We learn and evaluate our random-walks approach on eight of the citation data sets used by Han et al. (see ta-ble 1) [15]. Each dataset contains citations for authors pos-sessing the same first initial and last name. To create further ambiguity, Han et al. replaced first and middle name infor-mation with only the first initial of the first name. The size of the data sets in terms of number of citations, number of actual authors, and number of similarity matrices in W is presented in table 1.

For all datasets, we extracted adjacency matrices repre-senting TF-IDF similarities between coauthor names, be-tween document titles, and between venue names. For the four smallest datasets, we also extracted adjacency matrices for each word that occurred in at least five document titles or venue names. For example, in the  X  X . Kumar X  dataset, the word  X  X etwork X  occurs in six document titles. Thus the ad-jacency matrix for the word  X  X etwork X  contains edges con-necting all six of these citations. Words in all three of these citation fields were tokenized on non-alphabetical charac-ters, lower-cased, and stripped of accents. Stop-words were removed.

For the four large datasets, creating edges for each word in the vocabulary created too many adjacency matrices, and as shown in section 3, the gradient scales with the number of input similarity matrices, R . Therefore, to prevent this feature blow-up, we reduced the dimensionality of the vo-cabulary to 10 dimensions by applying non-negative matrix factorization [19] to a Word  X  Venue matrix which we ex-tracted from the DBLP database. In this matrix, each of the 2,500 venues is treated as a long document consisting of the words appearing in the titles of articles published in it. Non-negative matrix factorization was applied to this matrix to obtain a 10-dimensional vector representation of each word. Document titles and venue names were then obtained by the summing the 10-dimension vectors for the words they contained. By projecting titles and venue names into this space, we were able to replace the many vocabulary adjacency matrices with only two adjacency matrices: one consisting of the dot-products between the 10-dimensional title vectors and one consisting of the dot-products between the 10-dimensional venue vectors.
We trained and evaluated random walk models on the eight datasets described above. For each dataset, we learned and computed the similarity matrix A and gener-ated precision-recall curves by plotting precision and recall at each level of similarity. That is, at each threshold of sim-ilarity t , if two citations i and j have similarity A ij we predict that i and j contain mentions of the same author. Precision is computed as the number of such predictions that are correct divided by the total number of predictions. Recall is computed as the number of correct predictions di-vided by the number of pairs of citations wherein both cita-tions refer to the same author. 4.1.1 Learned versus Unlearned Random Walks We first tested how much benefit is obtained from learn-ing the weights of the different link types. To do this, we split the citations of the small datasets into training and test-ing sets and compared the performance of models learned on the training sets to  X  X nlearned X  models whose feature weights were all set equal to the same constant  X 1. X 
Parameters for the random walk models were opti-mized via conjugate gradient with line search. The hyper-parameters  X  and  X  in equations (8) and (9) were chosen using a grid search over  X ,  X   X  [0 , 1] and picking the com-bination which yielded the highest average precision on the training set. Results for the four smaller datasets are av-erages over five-fold cross-validation. Training plus grid search took roughly half an hour per fold on a machine with an Intel Xeon 3GHz processor. On the four larger data sets, five-fold cross-validation was impractical so we evaluated on a single train-test split wherein 500 citations were sam-pled for training and the rest were used for testing. Training on the larger data sets took between and 9 and 23 hours on the same machine. Typical values chosen for  X  were be-tween 0 . 1 and 0 . 5 while  X  ranged between 0 . 01 and 0 . 1 .
Precision and recall curves are given in figure 2 compar-ing the learned and unlearned models. The results shown were obtained using objective function J 2 which consis-tently out-performed J 1 . In every case learning the rela-tive importance of different link types greatly improves both precision and recall. In particular since link types were cre-ated for many of the words found in the document titles and venue names, learning is critical for detecting the important words which distinguish authors. 4.1.2 Necessity of Penalizing Probabilities for K  X  Next, to assess the benefit obtained by penalizing the set of dissimilar citations, K  X  , we compared our full model to a model which lacked this penalty. The results shown in table 2 demonstrate that including the penalty on K  X  improves the performance of our model. The results here are reported in terms of average precision, i.e. the average of precision scores at each level of recall. 4.1.3 Comparisons against Support Vector Machines To compare against an external baseline, we trained sup-port vector machines (SVMs) on the entity resolution task,
A. K UMAR X 0.86 0.09
M. J ONES X 0.74 0.05
K. T ANAKA X 0.91 0.03
Table 2. Average precision with  X  X  X  and with-out  X   X  the penalty on pairs in K  X  . treating it as a binary classification problem in which the object is to predict whether two citations refer to the same author [6]. Since SVMs have smaller time complexity than our method with respect to the number of features used, all features were used in training the support vector machines. This can only help the support vector methods since the number of features (similarity matrices) used is too small relative to the number of training pairs to create overfitting (the number of features is given in table 1).

We used two methods to generate precision and recall curves for the SVM output. In the first, each prediction was taken independently and precision and recall curves were generated as above using the SVM prediction score as the similarity. In the second method, the transitive closure of predictions was computed at each threshold [24]. That is, if citations i and j were predicted to corefer and citations j and k were also predicted to corefer, then citations i and k would be predicted to corefer. Such a strategy adds transi-tivity to the SVM predictions. Note that this strategy is not needed for random walks since they are inherently transi-tive. In figure 3, the two strategies are called  X  X ndep X  and  X  X losure. X  We used SVMLight [18] and trained it with a polynomial kernel of degree two. Linear and higher order polynomials were also tried, as were Gaussian kernels, but none consistently out-performed the chosen kernel. A range of values were tried for the slack-variable penalty, C , all of which yielded similar performance. Therefore we conser-vatively picked C to be the inverse of the 90-th percentile of the inner-product values,  X  x i , x j  X  for all i and j . 4.1.4 Comparison with Learned Spectral Clustering We also compare our method against the method for learn-ing random walks proposed by Meila and Shi [22], which learns a transition matrix P from random walks consist-ing of only one step X  X .e., walks are required to stop af-ter one step. In their method, the smallest eigenvectors of the learned graph are used to cluster the graph X  X  nodes. In-stead of clustering, we use these eigenvectors to form a low-dimensional embedding of the nodes and compute a simi-larity from the embedding coordinates. Let W ( ms ) be the weight matrix learned from their method and let L ( ms ) be its associated Laplacian matrix. From L ( ms ) , we compute the ten smallest eigenvectors of the generalized eigenvec-tor/value problem, L ( ms ) v =  X  v (see [28] and [33] for de-tails), and use them to find the embedding coordinate vec-tors, y i  X  R 10 for each node i in the graph. The similar-ity between nodes i and j is taken to be the inner product,  X  y i , y j  X  . Results for the spectral learning method are given in figure 3.

The results of figure 3 show that on the smaller datasets, random walks perform comparably to, or better than, the SVM and spectral methods. The spectral method, on the whole, performs worse than either the random walk models or the SVMs. On the larger datasets where dimensional-ity reduction is used to limit the number of input similarity matrices, the learned random walk models still perform as well as, or better than, the SVMs trained with all features. Finally we note that taking the transitive closure seems to improve the performance of SVMs, but it is not clear how great the benefit is due to the resulting jaggedness of the precision-recall curves.
There has been extensive work on entity resolution in-volving both supervised and unsupervised learning of prob-abilistic relational models [4, 10, 27, 29]. These approaches provide powerful frameworks for resolving identity uncer-tainty. However, they differ from the work presented here in that they are not based on random walks.

Malin et al. do use random walks to disambiguate actor names within the Internet Movie Database (IMDB) but they do not learn the link weights [21]. Minkov et al. use random walks to disambiguate names in email messages. Instead of learning link weights, they use absorbing probabilities as a feature in a linear classifier [26]. More recent work by Minkov and Cohen uses error back-propagation to learn the connection strengths of different links in the graph [25]. Additionally, they minimize squared error rather than log-loss. It remains an area of future research to determine which approach is more appropriate for entity resolution. Toutanova et al. learn transition probabilities in a random walk model for prepositional phrase attachment [30]. Their loss function is tailored to the particular natural language task and is quite different from the objective function we propose here.

Zhu et al. apply absorbing random walks to the prob-lem of semi-supervised learning on a partially labeled graph [34]. They work in a classification setting where the number of classes is known at training time. This is unlike the entity resolution task where the true number of entities (classes) is unknown. Zhu et al. do learn link weights, but their objective minimizes the label entropy of nodes in the test set rather than minimizing a loss computed on nodes and SVMs run on the four large datasets. of the training set. The work of Tsuda et al. [31] is similar to that of Zhu et al. in that they perform semi-supervised classification on graphs. The authors propose an objective function for learning link weights which minimizes predic-tion errors on the training set and maximizes the smoothness of the prediction function over the entire graph. Again, their objective function is quite different from ours.

Finally, several authors have incorporated learning into spectral clustering with a loss on the eigenvectors and eigen-values of the normalized graph Laplacian [2, 9, 23]. We differ in taking a probabilistic approach which avoids some of the difficulties involved in differentiating with respect to eigenvectors and eigenvalues.
We have proposed a method based on absorbing ran-dom walks for entity resolution. Unlike other random-walk based approaches to this problem, we give a supervised learning procedure for learning the transition probabilities from training data. Large numbers of features can be a bottleneck for our model but we show how dimensional-ity reduction can be used to circumvent this problem. The learned random walk models out-perform support vector machines and a related spectral clustering method on the task of resolving author ambiguities in bibliographic data. Finally, the learned random walk models achieve greater classification accuracy than the unlearned models and in-duce a distance on the nodes of the graph.

