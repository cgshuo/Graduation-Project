 Suggestions from a machine translation system can increase the speed and quality of professional hu-man translators (Guerberof, 2009; Plitt and Mas-selot, 2010; Green et al., 2013a, inter alia ). How-ever, querying a single fixed model for all different documents fails to incorporate contextual informa-tion that can potentially improve suggestion quality. We describe a model architecture that adapts simul-taneously to multiple genres and individual docu-ments, so that translation suggestions are informed by two levels of contextual information.

Our primary technical contribution is a hierarchi-cal adaptation technique for a post-editing scenario with incremental adaptation, in which users request translations of sentences in corpus order and pro-vide corrected translations of each sentence back to the system (Ortiz-Mart X nez et al., 2010). Our learning approach resembles Hierarchical Bayesian Domain Adaptation (Finkel and Manning, 2009), but updates both the model weights and translation rules in real time based on these corrected transla-tions (Mathur et al., 2013; Denkowski et al., 2014). Our adapted system can provide on-demand trans-lations for any genre and document to which it has ever been exposed, using weights and rules for do-mains associated with each translation request.
Our weight adaptation is performed using a hier-archical extension to fast and adaptive online train-ing (Green et al., 2013b), a technique based on Ada-Grad (Duchi et al., 2011) and forward-backward splitting (Duchi and Singer, 2009) that can accu-rately set weights for both dense and sparse fea-tures (Green et al., 2014b). Rather than adjusting all weights based on each example, our extension adjusts offsets to a fixed baseline system. In this way, the system can adapt to multiple genres while preventing cross-genre contamination.

In large-scale experiments, we adapt a multi-genre baseline system to patents, lectures, and news articles. Our experiments show that sparse mod-els, hierarchical updates, and rule adaptation all contribute consistent improvements. We observe quality gains in all genres, validating our hypothe-sis that document and genre context are important additional inputs to a machine translation system used for post-editing. The log-linear appoach to statistical machine trans-lation models the predictive translation distribution p ( e | f ; w ) directly in log-linear form (Och and Ney, 2004): p ( e | f ; w ) = where f  X  F is a string in the set of all source language strings F , e  X  E is a string in the set of all target language strings E , r is a phrasal deriva-tion with source and target projections src ( r ) and tgt ( r ) , w  X  R d is the vector of model parameters,  X  (  X  )  X  R d is a feature map computed using corpus c , and Z ( f ) is an appropriate normalizing constant. During search, the maximum approximation is ap-plied rather than summing over the derivations r . Model . We extend a phrase-based system for which  X  ( r ; c ) includes 16 dense features:  X  Two phrasal channel models and two lexical  X  Six orientation models that score ordering con- X  A linear distortion penalty that promotes  X  An n -gram language model score, p ( e ) , which  X  Fixed-value phrase and word penalties.
 The elements of  X  ( r ; c ) may also include sparse features that have non-zero values for only a subset of rules, but typically do not depend on c (Liang et al., 2006). In this paper, we use four types of sparse features: rule indicators, discriminative lexi-calized reordering indicators, rule shape indicators and alignment features (Green et al., 2014b).
The model parameters w are chosen to maximize translation quality on a tuning set.
 Adaptation . Domain adaptation for machine trans-lation has improved quality using a variety of ap-proaches, including data selection (Ceau X fu et al., 2011), regularized online learning (Simianer et al., 2012; Green et al., 2013b), and input classification (Xu et al., 2007; Banerjee et al., 2010; Wang et al., 2012) and has also been investigated for multi-domain tasks (Sennrich et al., 2013; Cui et al., 2013; Simianer and Riezler, 2013). Even without domain labels at either training or test time, multi-task learn-ing can boost translation quality in a batch setting (Duh et al., 2010; Song et al., 2011).

Post-editing with incremental adaptation de-scribes a particular mixed-initiative setting (Ortiz-Mart X nez et al., 2010; Hardt and Elming, 2010). For each f in a corpus, the machine generates a hypothe-sis e , then a human provides a corrected translation Figure 1: The weights used to translate a document in the patent genre include three domains. model weights w and corpus c used for rule extrac-i th sentence f i , the system uses weights w i  X  1 and corpus c i  X  1 . The new corpus c i results from adding ( f ,e  X  i ) to c i  X  1 . For incremental adaptation, speed is essential, and so w i is typically computed with a single online update from w i  X  1 using ( f i ,e  X  i ) as the tuning example.

To alleviate the need for human intervention in the experiment cycle, simulated post-editing (Hardt and Elming, 2010; Denkowski et al., 2014) replaces each e  X  with a reference that is not a corrected vari-ant of e . Thus, a standard test corpus can be used as an adaptation corpus. Prior work on online learn-ing from post-edits has demonstrated the benefit of adjusting only c (Ortiz-Mart X nez et al., 2010; Hardt and Elming, 2010) and further benefit from adjust-ing both c and w (Mathur et al., 2013; Denkowski et al., 2014). Incremental adaptation of both c and the weights w for sparse features is reported to yield Our hierachical approach to incremental adaptation uses document and genre information to adapt ap-propriately to multiple contexts. We assume that each sentence f i has a known set D i of domains, which identify the genre and individual document origin of the sentence. This set could be extended to include topics, individual translators, etc.
Figure 1 shows the domains that we apply in experiments. All sentences in the baseline training corpus, the tuning corpus, and the adaptation corpus share a root domain.
Our adaptation is conceptually similar to hier-archical Bayesian domain adaptation (Finkel and Manning, 2009), but both weights and feature val-ues depend on D i , and we use L 1 regularization. Weight Updates . Model tuning and adaptation are performed with AdaGrad, an online subgradient method with an adaptive learning rate that comes with good theoretical guarantees. AdaGrad makes the following update:
The loss function ` reflects the pairwise ordering between hypotheses. For feature selection, we ap-ply an L 1 penalty via forward-backward splitting (Duchi and Singer, 2009).  X  is the initial learning rate. See (Green et al., 2013b) for details.
Our adaptation schema is an extension of frustrat-ingly easy domain adaptation (FEDA) (Daum X  III, 2007) to multiple domains with different regular-ization parameters, similar to (Finkel and Manning, 2009). Each feature value is replicated for each do-main. Let D denote the set of all domains present in the adaptation set. Given an original feature vector  X  ( r ; c ) for derivation r of sentence f i with D i  X  X  , the replicated feature vector includes |D| copies of  X  ( r ; c ) , one for each d  X  X D| , such that The weights of this replicated feature space are ini-tialized using the weights w tuned for the baseline  X  ( r ; c ) . In this way, the root domain corresponds to the un-adapted baseline weights, denoted as  X   X  in (Finkel and Manning, 2009). The idea is that we simultane-ously maintain a generic set of weights that applies to all domains as well as their domain-specific  X  X ff-sets X , describing how a domain differs from the generic case. Model updates during adaptation are performed according to the same procedure as tun-ing updates, but now in the replicated space.
Different from (Finkel and Manning, 2009), this generalized FEDA model does not restrict the do-mains to be strictly hierarchically structured. We could, for example, include a domain for each trans-lator that crossed different genres. However, all of our experimental evaluations maintain a hierarchi-cal domain structure, leaving more general setups to future work.
 Rules and Feature Values . A derivation r of sen-tence f i has features that are computed from the combination of the baseline training corpus c 0 and a genre-specific corpus that includes all sentence pairs from the tuning corpus as well as from the adaptation corpus ( f j ,e  X  j ) with j &lt; i sharing f i genre. We refer to this combined corpus as c i . The tuning corpus is the same that is used for parameter tuning in the baseline system. The adaptation cor-pus is our test set. Note that in our evaluation, each sentence is translated before it is used for adaptation, so that there is no contamination of results.
In order to extend the model efficiently within a streaming data environment, we make use of a suffix-array implementation for our phrase table (Levenberg et al., 2010).

Rather than combining corpus counts across these different sources, separate rules extracted from the baseline corpus and the genre-specific corpus exist independently in the derivation space, and features of each are computed only with one corpus. In this configuration, a large amount of out-of-domain evidence from the baseline model will not dampen the feature value adaptation effects of adding new sentence pairs from the adaptation cor-pus. The genre-specific phrases are distinguished by an additional binary provenance feature.
In order to extract features from the genre-specific corpus, a word-level alignment must be computed for each ( f j ,e  X  j ) . We force decode using the adapted translation model for f j . In order to avoid decoding failures, we insert high-cost single-word translation rules that allow any word in f j to Sparse Features . Applying a large number of sparse features would compromise responsiveness of our translation system and is thus a poor fit for real-time adaptive computer-assisted transla-tion. However, features that can be learned on a single document are limited in number and can be discarded after the document has been processed. Therefore, document-level sparse features are a powerful means to fit our model to local context with a comparatively small impact on efficiency. We performed two sets of German  X  English exper-iments; Table 1 contains the results for both. Our first set of experiments was performed on the PatTR corpus (W X schle and Riezler, 2012). We divided the corpus into training and development data by date and selected 2.4M parallel segments dated be-fore 2000 from the  X  X laims X  section as bilingual training data, taking equal parts from each of the eight patent types A X  X  as classified by the Cooper-ative Patent Classification (CPC). From each type we further drew separate test sets and a single tune set, selecting documents with at least 10 segments and a maximum of 150 source words per segment, with around 2,100 sentences per test set and 400 sentences per type for the tune set. The  X  X laims X  section of this corpus is highly repetitive, which makes it ideal for observing the effects of incremen-tal adaptation techniques.

To train the language and translation model we additionally leveraged all available bilingual and monolingual data provided for the EMNLP 2015 tal size of the bitext used for rule extraction and feature estimation was 6.4M sentence pairs. We trained a standard 5-gram language model with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998) using the KenLM toolkit (Heafield et al., 2013) on 4 billion running words. The bitext was word-aligned with mgiza (Och and Ney, 2003), and we used the phrasal de-coder (Green et al., 2014a) with standard German-English settings for experimentation.

Our second set of experiments was performed on a mixed-genre corpus containing lectures, patents, and news articles. The standard dev and test sets the lecture genre. Each document corresponded to an entire lecture. For the news genre, we used newstest2012 for tuning, newstest2013 for meta-parameter optimization, and newstest2014 for test-ing. The tune set for the patent genre is identical to the first set of experiments, while the test set consists of the first 300 sentence pairs of each of the patent type specific test sets of the previous ex-periment. The documents in the news and patent genres contain around 20 segments on average.
Our evaluation proceeded in multiple stages. We first trained a set of background weights on the Table 1: Results in uncased Bleu [%]. Each com-ponent is added on top of the previous line. All results in line + genre TM and below are statisti-cally significant improvements over the baseline with 95% confidence. We also report the repetition rate of the test corpora as propsed by Bertoldi et al. (2013). concatenated tune sets ( baseline ). Keeping these weights fixed, we performed an additional tun-ing run to estimate genre-level weights ( + genre weights ). 5 In the patent-only setup, we used patent CPC type as genre. Next, we trained a genre-specific translation model for each genre by first feeding the tune set and then the test set into our incremental adaptation learning method as a contin-uous stream of simulated post edits ( + genre TM ). After each sentence, we performed an update on the genre-specific weights. In separate experiments, we also included document-level weights as an addi-tional domain ( + doc. weights ) and included sparse features at the document level ( + sparse features ). 6
Table 1 demonstrates that each component of this approach offered consistent incremental qual-ity gains, but with varying magnitudes. For the patent experiments we report the average over our eight test sets (A-H) due to lack of space, but to-tal improvement varied from +4.92 to +6.46 Bleu . In the mixed-genre experiments, Bleu increased by +2.27 on lectures , +0.97 on news , and +5.33 on patents . On all tasks, we observed statistically significant improvements over the baseline (95% confidence level) in the + genre TM , + doc. weights and + sparse features experiments using bootstrap resampling (Koehn, 2004).

These results demonstrate the efficacy of hierar-chical incremental adaptation, although we would like to stress that the patent data was selected specif-ically for its high level of repetitiveness, and the Figure 2: Bleu difference between baseline + genre weights and our incremental adaptation approach, computed on a single segment from each document according to their order, i.e. the first segment from each document, then the second segment from each document, etc. large improvement in this genre would only be ex-pected to arise in similarly structured domains. This property is quantified by the repetition rate mea-sure (RR) (Bertoldi et al., 2013) reported in Table 1, which confirms the finding by Cettolo et al. (2014) that RR correlates with the effectiveness of adapta-tion.
 Analysis . Figure 2 shows Bleu score differences to the baseline + genre weights system for different subsets of the news and patent test sets. Each point is computed by document slicing, i.e. on a single segment from each document. The rightmost data point is the Bleu score we obtain by evaluating on the 20th segment of each document, grouped into a pseudo-corpus. Note that this group does not cor-respond to any number in Table 1, which reports Bleu on the entire test sets. Thus, we evaluate on all sentences that have learned from exactly ( i  X  1) seg-ments of the same document, with i = 1 ,..., 19 . Although the graph is naturally very noisy (each score is computed on roughly 150 segments), we can clearly see that incremental adaptation learns on the document level: on average, the improve-ment over the baseline increases when proceeding further into the document.
 Decoding speed . In our real-time computer-assisted translation scenario, a certain translation speed is required to allow for responsive user in-teraction. Table 2 reports the speed in words per second on the lecture data. Adding a genre-specific translation model results in a speed reduction by a factor of 12.6 due to the additional (forced) decod-Table 2: Decoding speed on the lecture data. ing run and weight updates. Sparse features slows the system down further by a factor of 2.4. However, the largest part of the computation time incurs only when the user has finalized collaborative translation of one sentence and is busy reading the next source sentence. Further, the speed/quality tradeoff can be adjusted with pruning parameters. We have presented an incremental learning ap-proach for MT that maintains a flexible hierarchical domain structure within a single consistent model. In our experiments, we define a three-level hierar-chy with a global root domain as well as genre-and document-level domains. Further, we perform in-cremental adaptation by training a genre-specific translation model on the stream of incoming post-edits and adding document-level sparse features that do not significantly compromise efficiency. Our re-sults show consistent contributions from each level of adaptation across multiple genres.

