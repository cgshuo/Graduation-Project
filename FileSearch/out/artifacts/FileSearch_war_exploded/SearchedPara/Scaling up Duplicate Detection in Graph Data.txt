 Duplicate detection determines different representations of real-world objects in a database. Recent research has considered the use of relationships among object representations to impro ve du-plicate detection. In the general case where relationships form a graph, research has mainly focused on duplicate detection q ual-ity/effectiveness. Scalability has been neglected so far, even though it is crucial for large real-world duplicate detection task s.
We scale up duplicate detection in graph data (DDG) to large amounts of data using the support of a relational database sy stem. We first generalize the process of DDG and then present how to scale DDG in space (amount of data processed with limited mai n memory) and in time. Finally, we explore how complex similar -ity computation can be performed efficiently. Experiments o n data an order of magnitude larger than data considered so far in DD G clearly show that our methods scale to large amounts of data. H.4 [ Information Systems Applications ]: Miscellaneous Algorithms, Performance Data quality, duplicate detection, entity resolution, sca lability
Duplicate detection has been addressed in a large body of work [3]. We classify duplicate detection research along th ree di-mensions, namely data , approach , and algorithm focus . For data, we distinguish (i) data in a single table , without multi-valued at-tributes, (ii) tree data, such as data warehouse hierarchies or XML data, and (iii) data represented as a graph , e.g., data for personal information management. The second dimension discerns bet ween three approaches used for duplicate detection: (i) machine learn-ing , where models and similarity measures are learned, (ii) the use of clustering techniques, and (iii) iterative algorithms, which clas-sify one pair of candidates at every iteration. Considering the algo-rithm focus dimension, we observe that most articles focus o n ei-ther effectiveness, efficiency, or scalability. Research o n effective-ness is concerned with improving precision and recall, for i nstance  X  A full version of this paper is available. See [8] for details . by developing sophisticated similarity measures [2] or by c onsid-ering relationships [6]. Research on efficiency assumes a gi ven similarity measure and develops algorithms that avoid appl ying the measure to all pairs of objects [4]. To apply methods to very large data sets, it is essential to scale not only in time but also to scale in space, for which relational databases are commonly used [4] .
We observe that for duplicate detection in graph data, no met h-ods for scalable iterative duplicate detection have been pr oposed, a shortcoming we address in this paper.
All iterative DDG algorithms proposed so far adhere to the fo l-lowing framework: During initialization , a graph representation of the considered data is created, precomputations are perf ormed, and a priority queue of pairs of candidates (object represen tations among which duplicates should be detected) is set up. During the iterative phase , pairs of candidates are retrieved from a priority queue, are then classified as duplicates or non-duplicates, and po-tentially trigger some update in the priority queue. The mos t sig-nificant update is to sort the priority queue when a duplicate was classified. This is done to reduce the number of classificatio ns per-formed for a candidate pair (which can be more than one in DDG) . Scaling up initialization is discussed in the extended vers ion of this paper [8] and we only summarize scaling up the iterative phas e here, which is summarized in Fig. 1. We show how a pair of actor candidates ( a1 , a1 X  ) is retrieved from the priority queue residing in main memory. As it is classified as a duplicate, the movie cand idate pair ( m1 , m1 X  ) rises in the order of the priority queue, because its similarity increases based on the fact that actor a1 playing in m1 is the same as actor a1 X  playing in m1 X  .

To scale up the iterative phase of DDG, we scale up the individ -ual steps. To this end, we assume that all necessary data stru ctures, in particular the graph and the priority queue are stored in a rela-tional database. Scalable DDG is summarized in Fig. 2. To scale up retrieval and update, we propose our R E CUS /B algorithm. It uses an in-memory buffer B sorting the priority queue each time a duplicate is found. Th e in-tuition behind R E CUS /B UFF is that although ranks of several pairs may change after a duplicate has been found, sorting the prio rity queue immediately after finding the duplicate is not always n eces-sary and may actually occur several times before an affected pair is retrieved. For instance, when a1 and a1 X  are detected to be du-plicates, the pair consisting of the movies they respective ly play in, i.e., ( m1 , m1 X  ) rises in the priority queue, e.g., from position 7 to position 5. Although it is now closer to the head of the priori ty queue, any pair at position 4 or less will be compared first. He nce, sorting the priority queue does not immediately affect the c ompar-ison order and should therefore be avoided. To this end, we us e to temporarily store candidate pairs whose position in the p riority queue decreased. At each retrieval step, we check if the pair coming from the in-database priority queue has a lower or equal posi tion to the pair in the buffer. Depending on the result, either the pa ir from the database or the pair from the buffer is compared first. Whe n the buffer overflows, we update the in-database priority queue a nd sort it again. Until this happens, using the internal buffer avoi ds sorting the potentially large priority queue, which significantly i mproves runtime while scaling DDG to large amounts of data.
To classify pairs as duplicates or non-duplicates, a simila rity-based approach is often used: If the similarity is above a spe cified threshold, the pair is classified as a duplicate, otherwise i t is classi-fied as a non-duplicate. We investigate three variants of effi ciently computing the similarity of a pair.
 SQL. As the data is stored in a database, we can in principal com-pute the similarity of a pair using SQL. However, the type of s im-ilarity function is then limited by the expressive power, wh ich is especially a problem when the similarity measures use other aggre-gate functions to aggregate attribute similarities than th ose speci-fied in SQL.
 Hybrid/Complete. To overcome the limit of expressive power of the SQL variant, we define the Hybrid/Complete variant that e ssen-tially retrieves all necessary data for the computation of t he similar-ity from the database, which is then processed outside the da tabase to obtain the final similarity.
 Hybrid/Optimized. Having control over the processing of the data, we can further optimize the similarity computation by retriev-ing and processing the data in such a way that we can abort retr ieval and computation as soon as it is mathematically impossible t hat the similarity exceeds the predefined threshold. We call this te chnique early classification, because it classifies a pair as non-dup licate be-fore the actual similarity is computed.
Table 1(a) summarizes how the different phases of DDG scale in time depending on the size of the data s , the duplicate ratio and the connectivity c , which in total amounts to a linear behavior. Table 1(b) summarizes results reported for other DDG algorithms. Tab. 1(b) reports on the data set size, runtime (without init ialization time) and the parameters for which the algorithms do not scale lin-early ( s , dr , and c are considered). We observe that R takes comparably long, but this comes as no surprise as DB com -munication overhead and network latency add to the runtime. More interestingly, none of the DDG algorithms except R scales linearly in time with all three parameters s , dr , and deed, all algorithms but R E CUS /B UFF do not scale linearly in time with the data set size s , which compromises scaling up DDG to large amounts of data. Note that further experiments are rep orted in [8].
 duplicate ratio dr &lt; 0 . 8 linear constant
This paper is the first to consider scalability of duplicate d etec-tion in graphs (DDG). We showed that using R E CUS a suited classification strategy such as Hybrid/Optimized, we can scale up DDG to large amounts of data not fitting in main memory . Part of the research presented here was successfully applie d to an industry project [7].
