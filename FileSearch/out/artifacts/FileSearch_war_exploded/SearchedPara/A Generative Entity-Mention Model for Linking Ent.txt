 In recent years, due to t he proliferation of knowledge -sharing communities like W ikip e dia 1 and the many research efforts for the automated knowledge base population from Web like the R ead the Web 2 project , more and more large -scale knowledge bases are available. These knowledge bases contain rich knowledge about the world  X  s entities, their semantic properties, and the semantic rel a tions between each other. One of the most notorious exam ples is Wikipedia : its 2010 En g lish version contains more than 3 million entities and 20 million semantic relations. Bridging these knowledge bases with the textual data can facil i tate many different tasks such as entity search, information extra c tion and text classification. For example, as shown in Figure 1, knowing the word Jordan in the document refers to a baske t ball player and the word Bulls refers to a NBA team would be helpful in classif y ing this document into the Sport/Basketball class.

A key issue in bridg ing the knowledge base with the textual data is link ing the entities in a document with their referents in a knowledge base , which is usually referred to as the Entity Linking t ask . Given a set of name me n tions M = { m 1 , m , ... , m k } contained in documents and a kno w ledge base KB containing a set of entities E = { e function : ME  X   X  which links these name mentions to their referent entities in KB . For e x ample, in Figure 1 an entity linking system should link the name mention Jordan to the ent i ty Michael Jef f rey Jordan and the name mention Bulls to the entity Chic a go Bulls .

The entity linking task , however, is not tr i vial due to the name variation problem and the name ambiguity problem . Name variation means that an entity can be mentioned in different ways such as full name , aliases , acronyms and mis s pellings . For example, the entity Michael Jeffrey J ordan can be me n tioned using more than 10 names, such as Michael Jordan , MJ and Jo r dan . The name ambiguity problem is related to the fact that a name may refer to different entities in different co n texts. For example, the name Bulls can refer to more than 20 entities in Wik i pedia, such as the NBA team Chicago B ulls , the foo t ball team Belfast Bulls and the cricket team Queensland Bulls .

C omplicated by the name variation problem and the name ambiguity problem , the entity lin k ing decisions are critically depending on the knowledge of enti ties (Li et al., 2004; Bune scu &amp; Pa s ca , 2006; Cucerzan , 2007; Milne &amp; Witten , 2008 and Fader et al., 2009 ). Based on the pr e vious work , we found that the following three types of ent i ty knowledge can provide critical evidence for the entity lin k ing decisions:  X  Popularity Knowledge . T he popularity knowledge of entities tells us the lik e lihood of an entity appearing in a document. In en t ity linking, the entity popularity kno w ledge can provide a priori information to the pos s ible referent entities of a name mention. For example, without any other information , the popularity knowledge can tell that in a Web page the name  X  Michael Jordan  X  will more likely refer to the notorious basketball pla y er Michael Jeffrey Jordan , rather than the less popular Berkeley professor Michael I. Jordan .  X  Name Knowledge . The name knowledge tells us the possible names of an entity and the likelihood of a name referring to a sp e cific entity. For example, we would expect the name knowledge tells that both the  X  MJ  X  and  X  Michael Jordan  X  are possible names of the bask etball player Michael Jef f rey Jordan , but the  X  Michael Jordan  X  has a larger lik e lihood. The name knowledge plays the ce n tral role in resolving the name variation problem, and is also helpful in r e solving the name ambiguity problem.  X  Context Knowledge . The c ontext knowledge tells us the likelihood of an entity a p pearing in a specific context. For example, given the context  X  __wins NBA MVP  X  , the name  X  Michael Jordan  X  should more likely refer to the basketball player Michael Jef f rey Jordan than the Berkeley pro fessor M i chael I. Jordan . Context knowledge is cr u cial in solving the name ambiguities.

Unfortunately, in entity linking system, the modeling and exploitation of these types of ent i ty knowledge is not straightforward. As shown above, these types of knowled ge are heteroge n ous, making it diff i cult to be incorporated in the same model. Furthermore, in most cases the knowledge of entities is not explicitly given , making it challenging to extract the entity kno w ledge from data.

To resolve the above problems, thi s paper proposes a generative probabilistic model, called entity -mention model , which can leverage the heterogeneous entity knowledge (including popularity knowledge, name knowledge and context kno w ledge) for the entity linking task. In our model, each nam e mention is modeled as a sample ge n erated through a three -step generative story , where the entity kno w ledge is encoded in three distributions: the entity popularity kno w ledge is encoded in the distribution of entities in document P(e) , the entity name kno w ledge is encoded in the distribution of possible names of a specific entity P(s|e) , and the entity context knowledge is encoded in the distribution of pos s ible contexts of a specific entity P(c|e) . The P(e) , P(s|e) and P(c|e) are respectively called the e ntity popularity model , the entity name mo d el and the entity context model . To find the referent en t ity of a name mention, our method combines the evidences from all the three distributions P(e) , P(s|e) and P(c|e) . We evaluate our method on both Wikipedia a r ticles and general newswire documents. Exper i mental results show that our method can significantly improve the entity lin k ing accuracy.
 Our Contributions. Specifically, the main co n tributions of this paper are as follows: 1) We propose a new generative mode l, the entity -mention model , which can leverage heterogenous entity knowledge (including pop u larity knowledge, name knowledge and context kno w ledge) for the entity linking task; 2) By modeling the entity knowledge as probabilistic distributions, our model has a statistical foundation, making it different from most pr e vious ad hoc approaches.

This paper is organized as follows. The en t ity -mention model is described in Section 2 . The model estimation is described in Section 3 . The exp e rimental results are presen ted and discussed in Section 4 . The related work is reviewed in Section 5 . Finally we conclude th is paper in Se c tion 6. In this section we describe the generative entity -mention model. W e first describ e the generative st o ry of our model, then formulate the model and show how to apply it to the entity lin k ing task. 2.1 The Generative Story In the entity mention model , e ach name me n tion is modeled as a generated sample. For demo n stration, Figure 2 shows two e xamples of name mention generation. As shown in Fi g ure 2, the generative story of a name mention is composed of three steps, which are detailed as fo l lows: (i) Firstly, the model chooses the referent entity e of the name mention from the given knowledge base, according to the distribution of entities in document P(e) . In Figure 2, the model chooses the entity  X  Michael Jeffrey Jo r dan  X  for the first name mention, and the entity  X  Michael I. Jordan  X  for the second name me n tion; (ii) Secondly, the model outputs the name s of the name mention according to the distr i bution of possible names of the referent entity P(s|e) . In Figure 2, the model outputs  X  Jordan  X  as the name of the entity  X  Michael Jeffrey Jo r dan  X  , and the  X  Michael Jo r dan  X  as the name of the entity  X  Michael I. Jo r dan  X  ; (iii) Finally, the model outputs the context c of the name mention according to the distr i bution of possible contexts of the referent en t ity P(c|e) . In Figure 2, the model outputs the context  X  join s Bulls in 1984  X  for the first name mention, and the co n text  X  is a professor in UC Berkeley  X  for the second name mention. 2.2 Mo d el Based on the above generative story, the prob a bility of a name mention m (its context is c and its name is s ) referring to a specific entity e can be expressed as the fo l lowing formula (here we assume that s and c are independent given e ): This model incorporates the three types of entity knowledge we explained earlier: P(e) corre s ponds to the popularity knowledge, P(s|e) co r responds to the name knowledge and P(c|e) co r responds to the context knowledge.
 linking, we need to find the entity e which maximizes the probability P(e|m) . Then we can resolve the entity lin k in g task as follows: e arg max arg max ( ) ( | ) ( | ) Therefore, the main problem of entity linking is to e s timate the three distributions P(e) , P(s|e) and P(c|e) , i.e., to extract the entity knowledge from data. In Section 3, we will show how to estimate these t hree distr i butions.
 Candi d ate Selection . Because a knowledge base usually contains millions of entities, it is time -consuming to compute all P(m,e) scores between a name mention and all the ent i ties c ontained in a knowledge base. To reduce the time require d, the entity linking system employ s a candidate selection process to fi l ter out the impossible referent candidates of a name mention. In this paper, we adopt the cand i date selection method of NLPR_KBP system ( Han and Zhao, 2009 ), the main idea of which is first building a name -to -entity dicti o nary using the redirect l inks , disambiguation pages , anchor texts of Wikipedia, then the ca n didate entities of a name mention are selected by finding its name  X  s corresponding entry in the di c tionary. Section 2 shows that the entity mention model can decompose the entity linking task into the estimation of three distr i butions P ( e ), P ( s | e ) and P ( c | e ). In this section, we describe the details of the estimation of the se three distributions. We first intro duce the training data, then describe the estimation m e thods. 3.1 Training Data In this paper, the training data of our model is a set of annotated name me n tions M = { m 1 , m 2 , ... , m Each annotated name mention is a triple m={s, e , c } , where s is the name, e i s the referent entity and c is the context. For example, two a n notated name mentions are as fo l lows:
In this paper, we focus on the task of linking entities with Wikipedia, even though the pr o posed method can be applied to other resources. We will only show how to get the training data from Wikipedia. In Wikipedia, a hyperlink b e tween two arti cles is an annotated name me n tion ( Milne &amp; Witten, 2008 ): its an c hor text is the name and its target article is the referent entity. For exa m ple, in following hype r link (in Wiki syntax), the NBA is the name and the National Basketball Association is the re ferent ent i ty. Therefore, we can get the training data by collecting all annotated name me n tions from the hyperlink data of Wikipedia. In total, we co l lect ed more tha n 23,000,000 annotated name mentions. 3.2 Entity Popularity Model The distribution P(e) encodes the popularity knowledge as a distribution of entities, i.e., the P(e 1 ) should be larger than P(e 2 ) if e 1 is more popular than e 2 . For example, on the Web the P( Mic hael Jeffrey Jo r dan ) should be higher than the P( Michael I. Jordan ) . In this section, we estimate the distr i bution P(e) using a model called entity popular i ty model .
 entities, in its simplest form, we can assume t hat all entities have equal popularity, and the distr i bution P(e) can be estimated as: However, this does not reflect well the real situ a tion because some entities are obviously more popular than others. To get a more precise es t i mation, we observed that a more popular entity usually a p pears more times than a less popular entity in a large text corpus, i.e., more name mentions refer to this entity. For example, in W i kipedia the NBA player Michael Jeffrey Jordan appears more than 10 times than the Berkeley professor Michael I. Jordan . Based on the above observation, our en t ity popularity model uses the entity frequencies in the name mention data set M to estimate the distribution P(e) as fo l lows: where C ount(e) is the count of the name me n tions whose referent entity is e , and the | M | is the total name mention size. The estimation is fu r ther smoothed using the simple a dd -one smoot h ing method for the zero probability pro b lem. For illustration, Table 1 shows three selected entities  X  popularity .
 3.3 Entity Name Mode l The distribution P(s|e) encodes the name kno w ledge of entities, i.e., for a specific entity e , its more frequently used name should be assigned a higher P(s|e) value than the less fr e quently used name, and a zero P(s|e) value should be a s signed to those never used names. For instance, we would expect the P ( Michael Jordan | Michael Jeffrey Jordan ) to be high, P ( MJ | Michael Jeff rey Jordan ) to be relative high and P ( Michael I. Jo r dan | Michael Jeffrey Jordan ) to be zero.
Intuitively, the name model can be estimat ed by first collecting all (entity, name) pairs from the name mention data set, then using the max i mum likelihood estim a tion: where the Count(e,s) is the count of the name mentions whose referent entity is e and name is s . Howev er, this method does not work well b e cause it cannot correctly deal with an unseen en t ity or an unseen name. For example, because the name  X  MJ  X  doesn  X  t refer to the Michael Jeffrey Jordan in Wikipedia, the name model will not be able to identify  X  MJ  X  as a name of him, even  X  MJ  X  is a popular name of Michael Jeffrey Jo r dan on Web. paper proposes a much more generic model, called entity name model , which can capture the variations (including full name , aliases , acr o nyms and misspellings ) of an entity's name using a statistical translation model. Given an ent i ty  X  s name s , our model assumes that it is a transl a tion of this entity  X  s full name f using the IBM model 1 ( Brown , et al., 1993 ). Let  X  be the vocabulary containing all words may be used in the name of entities, the entity name model assumes that a word in  X  can be translated through the follo w ing four ways: 1) It is retained (translated into itself); 2) It is translated into its a cronym ; 3) It i s omitted(translated into the word NULL ); 4) It is translated into another word (misspe l ling In this way, all name variations of an entity are captured as the possible translations of its full name. To illustrate, Figure 3 shows how the full name  X  Michael Jeffrey Jordan  X  can be transalted into its mis s pelling name  X  Micheal Jordan  X  . Based on the translation model, P(s|e) can be written as: where  X  is a normalization factor, f is the full name of entity e , l f is the length of f , l s is the length of the t(s i |f j ) is the lexical translation proba bility which ind i cates the probability of a word f j in the full name will be written as s i in the output name. translation prob a bility t(s i |f j ) . In this paper, we first collect the ( name , entity full name ) pa irs from all annotated name mentions, then get the lexical translation probability by feeding this d a ta set into an IBM model 1 training system (we use the G I ZA++ Toolkit 3 ).
 tran s lation probabilities through the abov e process. We can see that the entity name model can ca p ture the different name variations, such as the a cronym ( Michael  X  M ), the misspelling ( Michael  X  M icheal ) and the o mission ( St.  X  NULL ).
 Table 2. Several lexical translation probabil i ties 3.4 Entity Context Model The distribution P(c|e) encodes the context knowledge of entities, i.e., it will assign a high P(c|e) value if the entity e frequently appea r s in the context c , and will assign a low P(c|e) va l ue if the entity e rarely appears in the context c . For example, given the fo l lowing two contexts: T hen P(C1|Michael Jeffrey Jordan) should be high because the NBA player Michael Jeffrey Jordan often appears in C1 and the P(C2|Michael Jeffrey Jordan) should be e x tremely low because he rarely appears in C2.
To estimate the distribution P(c|e) , we propose a method based on language modeling, called entity context model . In our model, the context of each name mention m is the word window su r rounding m , an d the window size is set to 50 a c cording to the experiments in ( Pedersen et al., 2005). Specifically, the context knowledge of an entity e is encoded in an unigram language mo d el: where P e (t) is the probability of the term t a p pearing in the context of e . In our model, t he term may indicate a word, a named entity (extracted using the Stanford Named Entity Reco g nizer 4 ) or a Wikipedia concept (extracted using the m e thod described in (Han and Zhao, 2010)). Figure 4 shows two entity context mo d els and the contexts generated u sing them.

Now, given a context c containing n terms t t 2 ... t n , the entity context model estimates the probability P(c|e) as: So the main problem is to estimate P e (t) , the probability of a term t appearing in the context of the en tity e .
 we can get the maximum likelihood estim a tion of P (t) as follows: where Count e (t) is the frequency of occurrences of a term t in the contexts of the name mentions whose ref e re nt entity is e .
 Because an entity e  X  s name mentions are usually not enough to support a robust estimation of P e (t) due to the sparse data problem (Chen and Goo d man, 1999), we further smooth P e (t) using the Jelinek -Mercer smoothing method (J e linek and Merce r, 1980): where P g (t) is a general language model which is est i mated using the whole Wikipedia data, and t he optimal value of  X  is set to 0. 2 through a learning process shown in Section 4. 3.5 The NIL Entity Problem By estimating P(e) , P(s|e) and P(c|e) , our m e thod can effectively link a name me n tion to its referent entity contained in a knowledge base. Unfortunately, there is still the NIL entity pro b lem (McNamee and Dang, 2009), i.e., the ref e rent entity may not be contained in the given knowledge base. In this situation, the name mention should be linked to the NIL entity. Traditional methods usually r e solve this problem with an ad ditional classification step (Zheng et al. 2010) : a classif i er is trained to identify whether a name mention should be linked to the NIL entity.
Rather than employing an additional step, our entity mention model seamlessly takes into a c count the NIL entity problem. The start assum p tion of our solution is that  X  If a name me n tion refers to a specific entity, then the probabil i ty of this name mention is generated by the specific entity  X  s model should be significantly higher than the probability it is generated by a general language model  X  . Based on the above assum p tion, we first add a pseudo entity , the NIL entity, into the knowledge base and assume that the NIL entity generates a name mention accor d ing to the general language model P g , without using any entity knowledge; then we treat the NIL en t ity in the same way as other entities: if the probabil i ty of a name mention is generated by the NIL entity is higher than all other entities in Kno w ledge base, we link the name mention to the NIL entity. Based on the ab ove discussion, we compute the three probabilities of the NIL entity: P(e) , P(s|e) and P(c|e) as fo l lows: In this section, we assess the performance of our meth od and compare it with the traditional methods. In following, we first explain the exper i mental settings in Section 4.1, 4.2 and 4.3, then evaluate and discuss the results in Se c tion 4.4. 4.1 Knowledge Base In our experiments, we use the Jan . 30 , 20 10 English version of Wikipedia as the kno w ledge base , which contains ov er 3 million distinc t entitie s. 4.2 Data Sets To evaluate the entity linking performance, we adopted two data sets: the first is WikiAmbi , which is used to evaluate the performance on Wikipedia artic les; the second is TAC_KBP , which is used to evaluate the pe r formance on general newswire documents. In following, we describe these two data sets in d e tail.

WikiAmbi : The WikiAmbi data set contains 1000 annotated name mentions which are ra n domly selected from Wikipedia hyperlinks data set (as shown in Section 3.1, the hyperlinks b e tween Wikipedia a r ticles are manually annotated name mentions). In WikiAmbi , there were 207 distinct names and each name contains at least two possible referent entities (on aver age 6.7 candidate referent entities for each name) 5 . In our experiments, the name mentions contained in the WikiAmbi are r e moved from the training data.

TAC _ KBP : The TAC_KBP is the standard d a ta set used in the Entity Linking task of the TAC 2009 ( McNamee and Dang, 2009 ) . The TAC_KBP contains 3904 name mentions which are s e lected from English newswire articles. For each name mention, its referent entity in Wikip e dia is manually annotated. Overall, 57% (2229 of 3904) name mentions  X  s referent entities are mis s ing in Wikipedia, so TAC_KBP is also suitable to evaluate the NIL entity detection perfo r mance.
The above two data sets can provide a sta n dard testbed for the entity linking task. However, there were still some limitations of these data sets: First, these data sets only annotate the s a lient name mentions in a document, meanwhile many NLP applications need all name mentions are linked. Second, these data sets only contain well -formed documents, but in many real -world a p plications the entity linking often be applied to noisy documents such as product reviews and micr o blog messages. In future, we want to develop a data set which can reflect these real -world se t tings. 4.3 Evaluation Criteria We adopt ed the standard performance me tric s used in the Entity Linking tas k of the TAC 2009 ( McNamee and Dang, 2009 ) . These me tric s are: Accuracy ): measures entity linking accur a cy averaged over all the name mentions; Accuracy) : measures entity linking accur a cy averag ed over all the target entities.
 As in TAC 2009, we used M i cro -Accuracy as the primary performance m e tric . 4.4 Experimental Results We compared our method with three baselines: (1) The first is the traditional Bag of Words based method ( Cucerzan , 2007): a name me n tion  X  s referent entity is the entity which has the highest cosine similarity with its context  X  we denoted it as BoW ; (2) The second is the method d e scribed in ( Medelyan et al., 2008 ), where a name mention  X  s referent entity is the entity which has the largest average semantic relate d ness with the name mention  X  s unambiguous context entities  X  we d e noted it as TopicIndex . (3) The third one is the same as the method d e scribed in ( Milne &amp; Witten , 2008 ), which uses learning techniques to balance the semantic rel a tedness, commoness and context quality  X  we denoted it as Lear n ing2Link . 4.4.1 Overall Performance We conduct experiments on both W i kiAmbi and TAC_KBP datasets with several methods: the baseline BoW ; the baseline TopicIndex ; the bas e line Learning2Link ; the proposed method using only popularity knowledge ( Popu ), i.e., the P(m,e)=P(e) ; the proposed method with one component of the model is ablated(this is used to evaluate the independent contributions of the three components), correspondingly P o pu+Name ( i.e., the P(m,e)=P(e) P(s|e) ) , Name+Context ( i.e., the P(m,e)=P( c| e) P(s|e) ) and Popu+Context ( i.e., the P(m,e)=P(e) P(c|e) ) ; and the full entity me n tion model ( Full Model ). For all methods, the parameters were configured through 10 -fold cross validation. The overal l pe r formance results are shown in Table 3 and 4.
 Tab le 3. The overall results on WikiAmbi d a taset Table 4 . The overall results on TAC -KBP dat a set From the results in Table 3 and 4, we can make the following observations: our entity mention model can achieve a signif i cant performance improvement: In WikiAmbi and TAC_KBP d atasets, compared with the BoW baseline, our method respectively gets 20% and 14% micro -accuracy improvement; compared with the TopicIndex baseline, our method respe c tively gets 14% and 6% micro -accuracy i m provement; compared with the Learning2Link baselin e, our method respectively gets 10% and 3% micro -accuracy improv e ment. our method can significantly improve the entity linking performance: When only using the pop u larity knowledge, our method can only achieve 49.5% micro -accuracy. By adding the name knowledge, our method can achieve 56.5% micro -accuracy, a 7% improvement over the P o pu . By fu r ther adding the context knowledge, our method can achieve 83% micro -accuracy, a 33.5% improvement over Popu and a 26.5% i m prove ment over P o pu+Name . co n tribute to the final performance improvement, and the context knowledge contributes the most: By respectively ablating the popularity kno w ledge, the name knowledge and the context knowledge, the p erformance of our model correspondingly r e duces 7.5%, 5% and 26.5%.

NIL Entity Detection Performance . To compare the performances of resolving the NIL ent i ty problem, Table 5 shows the micro -accuracies of di f ferent systems on the TAC_KBP data set (where Al l is the whole data set, NIL only contains the name me n tions whose referent entity is NIL, InKB only contains the name me n tions whose referent entity is contained in the kno w ledge base). From Table 5 we can see that our method can effectively detect the NI L entity meanwhile retain ing the high InKB acc u racy.
Table 5. The NIL entity detection perfo r mance on 4.4.2 Optimizing Pa rameters Our model needs to tune one parameter: the Jel i nek -Mercer smoothing parameter  X  used in the entity co n text model. Intuitively, a smaller  X  means that the general language model plays a more important role. Figure 5 plots the tradeoff. In both Wik iAmbi and TAC_KBP data sets, Fi g ure 5 shows that a  X  value 0.2 will result in the best perfo r mance.
 4.4.3 Detailed Analysis To better understand the reasons why and how the proposed method works well, in this Section we anal yze our m e thod in detail. The Effect of Incorporating Heterogenous Entity Knowledge . The first advantage of our method is the entity mention model can incorp o rate heterogeneous entity knowledge. The Table 3 and 4 have shown that, by incorporating heteroge nous entity knowledge (i n cluding the name knowledge, the popularity knowledge and the context knowledge), the entity linking performance can obtain a signif i cant improvement. The Ef fect of Better Entity Knowledge Extraction. The second advantage of our m e thod is that, by representing the entity knowledge as probabilistic distributions, our model has a stati s tical foundation and can better extract the entity knowledge using more train ing data through the en t ity popularity model , the entity name model and the entity context model . For instance, we can train a better entity context model P(c|e) u s ing more name mentions. To find whether a be t ter entity knowledge extraction will result in a better performance, Fi g ure 6 plots the micro -accuray along with the size of the training data on name mentions for P(c|e) of each entity e . From Figure 6, we can see that when more trai n ing data is used, the performance i n creases. 4.4.4 Comparision with State -of -the -Art We also compared our method with the state -of -the -art entity linking systems in the TAC 2009 KBP track (McNamee and Dang, 2009). Figure 7 plots the comparison with the top five perfo r mances in TAC 2009 KBP track. From Figure 7, we c an see that our method can outperform the state -of -the -art approaches: compared with the best ran k ing system, our method can achieve a 4% performance i m provement.
 Figure 7. A comparison with top 5 TAC 2009 In this section, we br iefly review the related work. To the date, most entity linking systems e m ployed the context similarity based methods. The essential idea was to extract the discrimin a tive features of an entity from its description, then link a name mention to the entity w hich has the largest context similarity with it. Cucerzan (2007) proposed a Bag of Words based method, which represents each target entity as a vector of terms, then the similarity between a name me n tion and an entity was computed using the cosine similar i ty measure. Mihalcea &amp; Csomai (2007), Bunescu &amp; Pasca (2006), Fader et al. (2009) e x tended the BoW model by incorporating more entity knowledge such as popularity kno w ledge, entity category knowledge, etc. Zheng et al. (2010), Dredze et al. (2010), Zhang et al. (2010) and Zhou et al. (2010) employed the learning to rank techniques which can further take the rel a tions between candidate entities into account. B e cause the context similarity based methods can only represent the entity knowledge as fe a tures, th e main drawback of it was the difficulty to incorporate heterogenous entity kno w ledge.

Recently there were also some entity linking methods based on inter -dependency . These methods assumed that the entities in the same document are related to each other, t hus the ref e rent entity of a name mention is the entity which is most related to its contextual entities. Med e lyan et al. (2008) found the referent entity of a name mention by computing the weighted ave r age of semantic relatedness between the candidate en t i ty and its unambiguous contextual entities. Milne and Witten (2008) extended Medelyan et al. (2008) by adopting learning -based technique s to ba l ance the semantic relatedness, commoness and context quality. Kulkarni et al. (2009) pr o posed a method which c ollectively resolves the entity linking tasks in a document as an optimization pro b lem. The drawback of the inter -dependency based methods is that they are usua l ly specially designed to the leverage of semantic rel a tions, doesn  X  t take the other types of en tity knowledge into consider a tion. This paper proposes a generative probabili s tic model, the entity -mention model , for the ent i ty linking task. The main advantage of our model is it can incorporate multiple types of heterogenou s entity knowledge. Furthermore, our model has a statistical foundation, making the entity kno w ledge extraction approach different from most previous ad hoc approaches. Exp e rimental results show that our method can achieve competitive perfo r mance.
In our m ethod, we did not take into account the dependence between entities in the same document. This aspect could be compl e mentary to those we considered in this paper. For our f u ture work, we can integrate such depe n dencies in our model. The wor k is supported by the National Natural Science Foundation of China under Grants no. 60773027 , 60736044 , 90920010 , 61070106 and 61003117 , and the National High Technology Development 863 Program of China under Grants no. 2008AA01Z145 . Moreover, we si n cerely thank the reviewers for their valuable comments.
