
Maoqiang Xie 1 , Jinli Liu 1 , Nan Zheng 1 , Dong Li 2 , Yalou Huang 1 , and Yang Wang 1 documents searched from a large amount of corpus based on their relevance to the query submitted by user. Recently, many researches have been developed on it. In unsupervised ranking which makes use of the link relations among the web pages to construct graph, such as PageRank [3] or HITS [8]. Based on the link relation graph, the influence of web pages can be propagated through the path of graph and the final ranking scores can be obtained from the stable graph. The other one is supervised ranking (also called  X  X earning to rank X ), which trains ranking models by learning from the labeled query-document pairs, such as Ranking SVM [6] or Prank [5]. Recently, proposed cost-sensitive Ranking SVM that gives different penalties for the errors occurring at different positions [4] [13]. Xu and Li directly optimize ranking performance measure by Boosting method [14]. Furthermore, in SIGIR 2007,  X  X earning to rank X  workshop is held and supervised ranking has gained increasing attention. 
From the above work, it can be found that unsupervised ranking has been used for link relations of documents cannot be obtained. Moreover,  X  X earning to rank X  needs a lot of labeled query-document pairs which require expensive human labeling and much time. At the same time, large numbers of unlabeled data are far easier to obtain. Therefore, semi-supervised ranking for text retrieval should be deeply studied. 
To address the above issues, a novel semi-supervised graph ranking (SSG-Rank) method is proposed in this paper, which re-weights the affinity matrix by using pairs of labeled documents, so that similarities between documents in the same class are enhanced, and vice versa. With the constrained graph, the ranking scores are propagated more precisely and efficiently. 
The rest of paper is organized as follows: Section 2 introduces some basic notions on graph, and then a semi-supervised graph ranking method called SSG-Rank is concluded in Section 4. An important problem for text retrieval is the insufficiency of labeled instances, since generation of documents is much faster than the manual labeling on them. Hence it is the large amount of unlabeled documents may help supervised ranking. Major research on semi-supervised learning is focused on classification and regression. For example, Blum and Mitchell exploit co-training method that splits the features into labeled data with these two different feature sets predict the unlabeled data, and teach the other one with predicted labels until most of predictions are agreed by these two independent classifiers[2]. Joachims applies SVM to semi-supervised classification by adding the unlabeled data constraint to the loss function[7]. Zhou, Z. employs co-training to semi-supervised regression[17]. Zhu adopts graph to semi-supervised learning[18]. Zhou, D. et al. and Agarwal propose the unsupervised graph ranking algorithms respectively[16][1]. And Wan et al. employ unsupervised graph ranking for document search[11]. Unfortunately, few semi-supervised ranking methods for text retrieval are proposed. To deal with above problems, we propose a novel semi-supervised graph-ranking algorithm by re-weighting the affinity graph on the basis of labeled document pairs. 2.1 Construction of Text Ranking Matrix dense region; (2) points in the same dense connective region (formally called a manifold or a cluster) are likely to have the same label. With these two assumptions, semi-supervised learning methods can spread the label information to their neighbors through the global structure, so that the model trained by labeled data can be used to find credible predictions of unlabeled data, and then the training dataset is expanded by inserting predicted unlabeled data with high confidence. Motivated by it, the labeled pairs can be used as constraint when the affinity matrix is constructed on the basis of data features, which will make the ranking information spread more precisely and efficiently. EVV  X  X  , and corresponding edge weights : wE + 6\ . The goal of graph ranking is to learn a ranking function : fV 6\ , The graph G represents the similarity between documents by assigning weight to each edge. The degree of similarity often uses RBF from the same class are to be ranked with highest score ( r 1 level), followed by pairs in with hierarchical classes, and the pair from the same father class can be ranked as  X  r h  X   X  function r ( ,  X  X  X  ) denotes the ranking level of document pairs. After being re-weighted relationships and will be more suitable for the graph ranking. 2.2 Semi-Supervised Graph Ranking A good ranking function can be considered as a ranking model that can minimize a suitable combination of the penalty of the model X  X  complexity and the empirical ranking error. And the optimization function is: where F can be considered as a vectorial function that assigns a ranking value vector Fi to each point xi, and di is given by equation (2). good ranking model should ma ke little difference between adjacent documents, which is also referred to as smoothness constraint. The second term is the empirical risk used to punish the ranking error on labeled data as well as the great change from the initial ranking score. The positive regularization parameter  X  is the trade-off between these two terms. In order to minimize the Q ( F ), Q ( F ) should be differentiated about F as follows: and the optimum F can be obtained. 
In practice, F* is often solved approximately by the iterative propagation, and the algorithm is illustrated as follows: Algorithm : SSG-Rank (Semi-Supervised Graph Ranking) Input: Output : The vectorial / discrete ranking function 
When the iteration converges, the ranking score between query x i and document x j should be high if both of them belong to the same cluster or quite relevant, and vise versa. In our experiments, we made use of 20-newsgroups dataset to test the performance of SSG-Rank  X  which consists of 20 hierarchical classes about 1000 documents per class. We selected 6 classes, including  X  X omp.graphics X ,  X  X omp.os.ms-windows.misc X ,  X  X ec.motorcycles X , X  X ec.sport.baseball X ,  X  X ci.space X , and  X  X alk.politics.mideast X (Fig. 2). Since Ranking SVM cannot handle large scale training dataset effectively and the count of document pairs is ( n -1)/2 times of document, 360 documents are sampled from these 6 classes. There are 3 rank levels in our experiments, comprising r 1 (query and document are in the same leaf class), r 2 (query and document are in the same father  X  ( r 3 ) = 0. To test the average performance, 12 datasets are sampled randomly. The name and the size of them are listed in Table 1, where L i represents labeled document set and U j represents unlabeled document set. 3.1 SSG-Rank v.s. Supervised Ranking Methods The first experiment is performed for comparing with SSG-Rank and selected baselines, such as supervised Ranking SVM and PRank. Additionally, BM25 Experiments on SSG-Rank and these baselines were conducted respectively on L 0 U i of results ranked by different methods. It is clear that SSG-Rank outperforms supervised ranking models and BM25 particularly in terms of MAP and NDCG. Additionally, even the least NDCG accuracy is above 90%. unlabeled data together with the constraint of labeled data. As the relationships between labeled data are credible, the ranking scores propagated from neighbors via the graph will be more credible. Furthermore, since SSG-Rank ran under the setting ranking score, while the supervised ranking methods never make any use of unlabeled data 3.2 Properties of SSG-Rank The second experiment is conducted to test the properties of SSG-Rank by using with incremental size. It shows that NDCGs increase until the count of unlabeled data up to 162, and then decrease gradually, which indicates that SSG-Rank has an ranking performance (MAP) on the same above datasets. 
Figure 5 presents the improvements of NDCGs and MAP when the unlabeled observed that the performance is improved significantly at first, and then become stable with the incremental labeled data, which implies that too many labeled data are not needed when the affinity graph is re-weighted well. 
Finally, experiment is executed with different weighting schemas, and the results are illustrated in Figure 6. From the 12 results, it is evident that  X  1 schema (  X  (r1) = 4,  X  (r2) = 1, and  X  (r3) = 0) reveals the best performance for the reason that the ranking score propagated via definitely relevant pairs will be enhanced, and more scores can be revised by these paths. In addition, unsupervised graph ranking is the special case supervised graph ranking benefiting from re-weighting of affinity graph on the basis of labeled pairs is much more effective than unsupervised graph ranking. assumptions and graph notions, SSG-Rank can modify the intrinsic manifold by using propagating the ranking scores to their neighbors via the graph. The experimental results show that SSG-Rank executed on 20-newsgroups dataset outperforms supervised ranking (Ranking SVM and PRank) and unsupervised graph ranking significantly. Finally, we also analyze different weighting schemas for improving the ranking performance of top documents. 
Future work includes theoretical analysis on the generalization error and other properties of the SSG-Rank algorithm, and further empirical evaluations of the algorithm including comparisons with other algorithms should be studied. Acknowledgments. This work is supported by National Science Foundation of China under the grant 60673009 and the key project of the Tianjin Science and Technology Research Program under the grant 05YFGZGX24000. We would like to thank Rui Kuang for helpful discussion. 
