 With the overwhelming amounts of visual contents on the In-ternet nowadays, it is very important to generate meaningful and succinct descriptions of multimedia contents including images and videos. Although human taggings and annota-tions can partially label some of the images or videos, it is impossible to exhaustively describe all the multimedia data due to its huge scale. Therefore, the key to this important task is to develop an effective algorithm that can automati-cally generate a description of an image or a frame. In this paper, we propose a multimodal feature fusion framework which can model any given image-description pair using se-mantically meaningful features. This framework is trained as a combination of multi-modal deep networks having two integral components: An ensemble of image descriptors and a recursive bigram encoder with fixed length output feature vector. These two components are then integrated into a joint model characterizing the correlations between images and texts. The proposed framework can not only model the unique characteristics of images or texts, but also take into account their correlations at the semantic level. Exper-iments on real image-text data sets show that the proposed framework is effective and efficient in indexing and retriev-ing semantically similar pairs, which will be very useful to help people locate interesting images or videos in large-scale databases.
 H.2.8 [ [Information Systems ]: [Data Mining] ; H.3.1 [ [Information Systems ]: [Content Analysis and Indexing]; I.4.7 [ Feature Measurement ]: [Feature repre-sentation] Multimodal Framework, Feature Fusion, Restricted Boltz-mann Machine.
Nowadays, a vast ocean of multimedia data has been ac-cumulated and shared among the world, and the multimedia volume keeps increasing every day. Most of the multimedia contents are accompanied by very few or even no text de-scriptions. This hinders people from understanding, search-ing and utilizing the multimedia data. Therefore, people have been striving to generate automatic descriptions for multimedia data such as images or videos. Such text de-scriptions will be very useful in numerous applications: Se-curity authorities can conduct more efficient management by utilizing large amount of monitoring data; image search en-gines can benefit from image description in supporting more accurate query results; and automatically generated image descriptions can help visually disable people understand vi-sual content as normal people.

However, automatic description generation is an extremely challenging task. First, existing analytic tools are still far from fully understanding image and language to automati-cally generate meaningful descriptions. Second, images and texts have completely different structures and statistical char-acteristics which are hard to be modeled together. More-over, there is a big gap in information quality provided by an image and its description. Image contents provide huge amounts of information but not all of the information is use-ful and many features could be redundant and noisy. On the other hand, descriptions can be concise but it restricts the amount of semantic information we can obtain from them. Third, description is not a simple annotation using discon-nected words, instead, a description should be more semanti-cally informative in which spatial relationships, actions and sentiments must be revealed.

Over the past years, many efforts have been devoted to develop systems or tools to generate image descriptions au-tomatically based on annotated image datasets. Existing work roughly falls into two categories: Detection-based or retrieval-based approaches. Detection-based approaches mainly focus on modeling image content [10], [11], [16], in which visual words are extracted using classifiers and object de-tectors. Such visual words can then be concatenated as de-scriptions. These methods require a large collection of accu-rate detectors trained on annotated images. Unfortunately, existing detectors are still unable to provide accurate detec-tion results, which will in turn degrade the performance of description generation. Moreover, in this approach, visual words are limited to a set of predefined categories, which significantly reduce the expressive power of words. Usually, the concatenation of these visual words is only a collection of disconnected words instead of a meaningful and plausible sentence that is needed. Retrieval-based approaches try to solve the problem from another perspective. They extract words or phrases retrieved from the annotations of visual similar images in the image database as the description of the target image [12], [8]. The use of a huge database of image and associated descriptions can circumvent the diffi-culty of image understanding, and thus retrieval-based ap-proaches have the potential to generate natural and semanti-cally meaningful descriptions. However, several crucial fac-tors have to be considered to make these approaches success-ful. If visual similarity is wrongly computed, then irrelevant descriptions will be generated. Moreover, many heuristic and hand-crafted features have been used to represent text or images. Each of these features is just capable of charac-terizing a partial view of the data and a global view can be obtained only by integrating these features. However, exist-ing work combines features by simply concatenating them into a long feature vector, which is not an efficient solution especially for high dimensional data.

In this paper, we propose a novel multimodal Feature Fu-sion Network (FFN) that fuses representative features from both images and texts to facilitate retrieval-based descrip-tion generation. It has the following properties. First, im-age and description are modeled separately based on their unique characteristics and their correlations are captured at the semantic level. Second, the model does not require accurate classifiers trained on sufficient annotated data, in-stead, it can be trained using low-level features generated by unsupervised feature extractors. Third, it is robust in the presence of missing data. Last but not least, it provides a more reasonable and flexible solution by fusing different types of features into a unified space in which different char-acteristics are nicely preserved.

Recently, deep architectures achieved impressive results in application domains such as computer vision and natural language processing [2], [15] etc. The intuition and extensive learning power of these models are suitable for this task. In FFN, different modalities (views) are modeled in different pathways but eventually fused into a unified representation. Text and image X  X  difference at low levels are preserved and modeled while their correlations at the higher levels are cap-tured. This framework can be used to generate efficient hash indexes for fast image retrieval and description generation because it retains the semantic uniqueness of each image-description pair.

In summary, we make the following contributions.
Data with multiple modals usually provide complemen-tary information to support each other, where each modal describes one aspect of the objects. Therefore, in this sec-tion, we introduce the methodology of FFN as follows. First, we introduce the layer-wised methodology for FFN. Second, we introduce our proposed method about how textual and image features are fused in their own pathways. Finally, we show the training and querying of our across-modality fusion framework. For the purpose of better illustration, we show our framework in Figure 1, in which the left branch trans-forms visual inputs into encodings f v , and the right branch transforms textual inputs into encodings f t .
Our framework can be represented as layers of hidden vari-ables in which every two adjacent layers forms a fully con-nected bipartite graph. This structure is a special case of Markov Random Field, named Restricted Boltzmann Ma-chine [4]. The model defines the following energy function: E : { 0 , 1 } V + H  X  R : E ( v , h ;  X  ) =  X  where V and H are the number of visible units v and hid-den units h respectively. W  X  R V  X  H is the weight matrix, b  X  R V and a  X  R H are the bias terms for visible layer and hidden layer. If we denote  X  = { a,b,W } as the model pa-rameters, the joint distribution over the visible and hidden units can be defined as: And the conditional distribution is defined as: Where Rect (  X  ) is the Rectifier function. The definition and approximation function are defined as: Since there is no inter-connection within a layer, the con-ditional independent property enables efficient Gibbs Sam-pling for each layer given the other. Training an RBM by maximum likelihood would require performing mini-batch gradient descent on the training set negative log-likelihood with respect to  X  : and the gradient with respect to any parameter  X  of the negative log-likelihood can be calculated as: The maximum likelihood gradient is intractable to compute because of the exponential sum over both the visible and hidden units in the second expectation. A well known strat-egy to tackle this problem is to use Contrastive Divergence (CD) [4], which consists of replacing the expectation over v by a point estimate at a sample  X  v obtained by running limited steps on Markov chain initialized by the input data. The samples  X  v are often referred to as Reconstruction or Negative Data. A trained RBM will give low reconstruction error (distance between v and  X  v ) for the input which is closer to the training data, and give higher reconstruction error when the input is far away from knowledge.

The learning strategy of RBM will be changed when mod-eling word vectors due to the dimensionality. If we denote K as the number of unique words in the dictionary, then a word will be represented as a binary vector of length K in which one value is 1 and the others are 0s (sometimes called one-hot representation or K -ary). If we denote e one-hot vector with its k th component set to 1, then the conditional distribution of the i th word becomes
Running Gibbs Sampling on p ( v i = e k | h ) is still problem-atic when K is large. Instead of sampling from p ( v | h ), we can sample from a proposed distribution q ( v i ) by running Metropolis Hastings directly on the i th group of the visi-ble layer. And this proposal distribution can be efficiently drawn in constant time by running Alias method [7]. This RBM variant is named Word Representation RBM (WR-RBM) by Dahl et al [1]. We propose to train the first layer of textual branch as WRRBM because it could naturally pro-duce a real value feature vector for an entire n-gram which is required for the framework.
We first apply a preprocess algorithm to textual descrip-tions before the fusion stage. Since we are learning a joint representation, the keyword frequency becomes a major con-cern, i.e. rare words (e.g. names) are hard to be learned due to the lack of training samples. Moreover, the description of an image should be clear and simple (e.g. transitional complex sentence is not straightforward to understand even for human). Moreover, It is not helpful to elaborate over all the objects in an image.

To leverage this challenge, we propose a heuristic Seman-tic Down-Casting (SDC) algorithm. Specifically, to marginal-ize the negative effects caused by variant nouns, we train a Stanford NER [3] using CoNLL dataset and substitute all words which are detected as [PERSON] by term  X  X ERSON X . Then we apply Stanford POS tagger [13] on the new sentence and substitute all the words by word  X  X BJECT X  when they are labeled as [NNP] and have term frequency less than a certain threshold.

This transformation on both textual side and query side might reduce the precision but result in a large increase on recall. This is particularly helpful for this task, because rare terms are down-casted into a broader class which has more training samples.
We propose Bigram Encoder (BE) as base structure which takes encodings of two words (or phrases) and output a sin-gle representation. Specifically, if we denote WRRBM for unigram as  X  1 (  X  ), then the encoding of word i can be defined as c i =  X  1 ( e k ). So, given two adjacent words X  representa-tions e p and e q , we can generate a concatenated represen-tation [ c i ; c i +1 ] and feed it to the joint RBM layer denoted as  X . The training of BE can be done in an effective way: Given a dataset, we first train  X  1 using sliding window of size 1, then we perform a second pass using window of size 2 and encode two words individually using the trained  X  1 into a concatenated representation to train  X .
 After BE is trained, we can perform a recursive Greedy Pair-wised Fusion (GPF) procedure on all single word en-codings generated by  X  1 and recursively fuses a pair of ad-jacent encodings by  X  which gives the lowest reconstruction error. This encoding algorithm can be viewed as unsuper-vised structure learning, since the  X  X usion X  procedure even-tually forms a tree structure.

GPF procedure will eventually transform a sentence into a fixed length representation as desired.
To further improve the performance of BE, we propose to incorporate n-gram statistic information as prior knowledge as shown in Algorithm 1. The intuition is that n-gram will introduce a phrase level semantic which would help BE to generate more meaningful encodings.
 As the example shown in Figure 2, Hierarchical N-gram Encoding (HNE) will transform a sentence into a list of fea-ture vectors. Then each feature vector will be fused into the unified representation by applying GPF procedure. Algorithm 1 Hierarchical N-gram Encoding Input: A query sentence L ;  X  1 ,  X  2 ,  X  3 trained from dataset Output: Encoding E of L 1: n = 3, E = list() 2: run sliding window of size n and pick the n-gram N with 3: encode N using  X  n and insert the encoding into E with 4: L 1 , L 2 = ( L exclude N ) 5: run step 2 on L 1 , L 2 to get more segments, and run step 6: stop when length( segments i ) &lt; n or the picked n-gram X  X  7: n -= 1 and repeat step 2-6 We choose to use Pyramid Histogram of Words (PHOW) [9], Color Histogram and Convolutional Neural Net (CNN) to extract features. The reason is that we hope to have a di-verse set of views that provide complimentary information about the image. If we treat Color Histogram and PHOW as global feature extractors, then CNN will be responsible for extracting local features. Since extracted features are usu-ally continuous values, we propose to use Gaussian units on concatenated feature. The energy function is thus changed to: E ( v , h ;  X  ) =  X  Under the modified energy function, the conditional prob-abilities for visible and hidden units are therefore changed to: where N (  X |  X , X  2 i ) denotes the Gaussian probability density function with mean  X  and variance  X  . Using Gaussian units will not change the bipartite structure, so that the updating rule stays the same with the original RBM.
After handling each input modality separately, we propose the overall framework  X  Feature Fusion Network, showed in Figure 1. It consists of a textual branch B t which accept sentence as input and output feature vector f t , and a vi-sual branch B v which accepts raw image (color/greyscale) as input and output feature vector f v . We then get the con-catenated representation [ f t ; f v ] and train an extra RBM layer to model their joint probability. We can then use the joint encoding getting from the conditional distribution p ( f joint | f t ,f v ) to measure similarity of any image-description pairs.

The overall training of FFN needs to perform the following procedures:
When the framework has been trained, we can run a sec-ond pass through all the data to get three sets of encodings: f for text, f v for images and f joint for the input pairs. Then we associate each one of them with the file identifiers as val-ues and store the hash table into any single or distributed database.

When answering unimodal queries(i.e. p ( f t | f v , X  ) for de-scription generation and p ( f v | f t , X  ) for image retrieval), we can encode the image query into f query v and then sample f t from the joint layer and find the nearest neighbor of f query t and return as the answer. Alternatively, we can search for the nearest neighbor of f query v and directly return the associated description as the result. The process is simi-lar to the textual query. For the multimodal queries, we feed each side with corresponding input to get the encodings and find their nearest neighbors in f joint .
In this section, we verify our framework on three sets of experiments. First two sets are for textual branch and vi-sual branch and the last set is for FFN. We use Im2Text 1 dataset, which contains one million image-description pairs refined from Flickr. The provided baseline method can be found in [12]. We split the dataset into two parts: First part contains 990K samples used for training and the remaining 10K samples are used for testing. We use Mini-batch Gra-dient Descent for all the training with mini batch size set to 20. Moreover, in order to use Vector Space Model (VSM), we construct inverse index with TF-IDF score associated for all textual datasets using Apache Lucene. The dataset we use to train BE is the LDC-2005 English Gigaword corpus. The corpus is tokenized following the Penn Treebank tokenization. After tokenizing and remov-ing infrequent terms, we build a vocabulary with unique unigram, bigram and trigram dictionaries of size (number of unique items) 91K, 1.8M and 4M. We denote this larger corpus as D L .

We preprocess the data as the followings. For each sen-tence, we first eliminate all the non-alphabetical terms and perform SDC to down cast this purified sentence. Then we tokenize this down-casted corpus and get unigram, bigram and trigram dictionaries of size 51K, 495K, and 1.3M. We denote this smaller corpus as D S .

For the above two corpus, we split them into large barrels which contain 1K samples for each and randomly pick one barrel as test set and another one as validation set. BE is constructed as follows: number of hidden units is set to be 1024 for each WRRBM as well as the number of hidden units for the joint layer. Note that the number of visible units of WRRBM for D L and D S is 91K and 51K respectively. We evaluate BE on validation set and fix the number of M-H iteration to 100 and the learning rate to 0.005 for WRRBM and 0.01 for the joint layer RBM. The performances of BEs are shown in Figure 3.

We hope to observe the following properties of GPF. The sentences of similar meaning should be encoded closer and http://dsl1.cewit.stonybrook.edu/ vicente/sbucaptions/ the sentences that are abnormal in statistic perspective should be encoded far away.

We collect 1K synonymous sentences from low level En-glish language qualification test 2 . They are simple sentences which match the requirements for our task, e.g.
We can see from Figure 4-left that GPF has similar per-formance with Recursive Autoencoder (RAE) [5] because GPF uses similar idea that recursively fuse two encodings into one. However, since RAE introduces domain knowledge (parse tree) as a prior to guide the learning, we can expect that if the training set is small, or the sentence has com-plex grammar, RAE will outperform GPF. Both RAE and GPF outperform VSM. Because VSM assumes that terms are statistically independent, the sentences with similar con-text but different term vocabulary will not be associated.
We conduct another experiments to show the GPF per-formance on sentences which contain statistically abnormal words. From the algorithm perspective, if two sentences contain words that are statistically  X  X ar away X , then their en-codings should be less similar. We made a synthetic dataset from D S which contains 1K preprocessed sentences for this purpose.

Figure 4-right shows that GPF is more effective in find-ing abnormal sentences. Because GPF is trained as a pure statistical model and should exhibit a more sensitive reac-tion on abnormal combinations. We also observe that VSM performs poorly on this test. Because the exact matching mechanism ensures that when a majority of words remains mostly from quia.com and english-test.us the same, the cosine score will not change dramatically. The sensitivity to abnormal sentences is very important for the joint layer. From the visual point of view, we cannot allow an image of  X  X at X  to be indexed together with a sentence which contains abnormal words like  X  X hobia X . Im2Text contains 1M variable-resolution images from Flickr. To simplify the training, we down-sample the images to a fixed resolution of 256  X  256. Given a rectangular image, we first rescale the image such that the shorter side is of length 256, and then crop out the central 256  X  256 patch from the resulting image.

We train a CNN on ImageNet LSVRC-2010 3 using cuda-convnet 4 with the same architecture and parameter setting in Krizhevsky X  X  work [6], then we run the trained CNN on all the images of Im2Text dataset and get a matrix M 1 of size 1M  X  4096. We use VLFeat [14] for extracting PHOW fea-tures, and Python Image Library for the Color Histogram. The extracted features are stored in two matrices: M size 1M  X  781 and M 3 of size 1M  X  256. We train the Gaus-sianRBM of image side which contains 1024 hidden units by the concatenated matrix [ M 1 ; M 2 ; M 3 ].

For testing, we pick 1K samples and randomly zero-out 20% of pixels from each image. For the second query set, we pick 100 images as query and manually collect the most similar results (in our own judgment) from Google Image Search.

We can clearly observe in Figure 5-right that using image encoding results in a closer distance than using raw features. Our method is also robust in missing values when some key components are blacked out. In this situation, feature ex-tractors may extract different set of features so that raw feature curve shows severe perturbations (abrupt increases) as shown in Figure 5-left. However, our method damps this problem. In other words, our method is less sensitive to the noise. We can explain this phenomenon from the algo-rithm perspective that since we jointly learn three sets of image features in a statistical way, our model  X  X emorized X  the pattern so that even if one set of features is missing or distorted, we can still reconstruct it by the most possible distribution.
We first get f L t and f S t from the original Im2Text train-ing set D L and the down-casted version D S as described in Section 2.4. Then for the image side we get raw features f raw and the encoding f v from Im2Text training set. For http://www.image-net.org/challenges/LSVRC/2010/ https://code.google.com/p/cuda-convnet/ comparison, we train two separate top layer RBMs which contain 2048 hidden units using [ f L t ; f v ] and [ f resulting models are thus  X  X FN without SDC X  and  X  X FN with SDC X  respectively. The learning rate for both are set to 0.01. Although the description generation procedure has been described in Section 2.4, we give further clarifications  X  there are two schemes for getting the description: Given a query image, we force our model to The symmetric procedure is also applied for retrieving image given a textual query.
We can see from Figure 6-left that  X  -FFN and  X  -FFN con-stantly outperform baseline methods which are based on re-trieving visually similar images on raw features. Also, since SDC algorithm purifies the dataset, all three methods show around 20% performance boost. We observe that the perfor-mance gap between  X  -FFN and  X  -FFN are subtle. However we can expect a higher performance of  X  -FFN when the query images are noisy.

For textual queries we use VSM as baseline to retrieve similar description and get its associated image. As shown in Figure 6-right, the performance gap between  X  -FNN and VSM is not that obvious. It implies that, although descrip-tions may be similar, their associated image may had a sig-nificant different look. However,  X  -FFN is still in the leading position which significantly outperforms VSM. The reason for this is that the joint RBM layer of FFN would generate a more statistically plausible encoding for the image side given the textual side encoding. This experiment shows a non-symmetric property of FFN, i.e.  X  -FFN shows a weaker performance when answering textual queries, which high-lights the importance of the joint RBM layer.
In this paper, we proposed a multimodal feature fusion framework which is able to transform large image-description collection into better spaces in which image-only, text-only and image-text queries can be answered more accurately. We found an effective way of modeling semantic unique-ness from arbitrary length descriptions and fused in n-gram knowledge as prior. Besides, we proposed an effective eval-uation algorithm to measure textual side performance. The experiments showed that FFN can effectively describe the similarity between unimodal and multimodal inputs and give reasonable results without using any high-level image con-tent detectors or classifiers. Our model also shows a superior performance than other methods on Im2Text dataset. [1] G. E. Dahl, R. P. Adams, and H. Larochelle. Training [2] D. Erhan, A. Courville, Y. Bengio, and P. Vincent. [3] J. R. Finkel, T. Grenager, and C. Manning.
 [4] G. E. Hinton and R. R. Salakhutdinov. Reducing the [5] E. Huang. Paraphrase detection using recursive [6] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet [7] R. A. Kronmal and A. V. Peterson Jr. On the alias [8] P. Kuznetsova, V. Ordonez, A. C. Berg, T. L. Berg, [9] S. Lazebnik, C. Schmid, and J. Ponce. Beyond bags of [10] S. Li, G. Kulkarni, T. L. Berg, A. C. Berg, and [11] M. Mitchell, J. Dodge, A. Goyal, K. Yamaguchi, [12] V. Ordonez, G. Kulkarni, and T. L. Berg. Im2text: [13] K. Toutanova, D. Klein, C. D. Manning, and [14] A. Vedaldi and B. Fulkerson. VLFeat: An open and [15] B. Wang, J. Jiang, W. Wang, Z.-H. Zhou, and Z. Tu. [16] Y. Yang, C. L. Teo, H. Daum  X e, III, and Y. Aloimonos.
