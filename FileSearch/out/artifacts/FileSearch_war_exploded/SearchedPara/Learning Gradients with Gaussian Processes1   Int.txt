 Analyzing data sets associated with ma ny variables or coordinates has become increasingly challenging in many circumstances, especially in biological and phys-ical sciences [1]. A wide range of machine learning algorithms based on the regularization theory such as suppor t vector machines (SVMs) [2] have been proposed to solve the predictive problems in the past two decades. Although these approaches demonstrate quite a cceptable and robust performances in a lot of experiments and applications, sometimes one also wants to get an insight into the relationships between the coordinates and the influence of the coor-dinates/attributes/features on the outputs. For example, it is very interesting to investigate which covariant is most significant for prediction and how the variables vary with respect to each other in estimation.

The gradient of the target function provides a valuable measure to charac-terize the relationships [3,4,5,1] and it has been used in many approaches and applications. For example, the minimum average variance estimation (MAVE) method and the outer product of gradients (OPG) estimation approach pro-posed by [3] focus on finding the effective dimension reduction (e.d.r.) space explicitly, respectively. These model s show better performance in the estima-tion of e.d.r. space than others, but learning gradient information would fail in the  X  X arge m (dimension), small n (size of the dataset) X  paradigm [6]. Re-cently, [4] and [5] proposed a method to learn the gradient of a target function directly from a given data set based on the Tikhonov regularization method, which avoided the overfitting problem in the  X  X arge m ,small n  X  settings. The most significant statistical measu re we can get by those nonparametric kernel based models is the gradient outer product (GOP) matrix, which can interpret the importance of the coordinates for the prediction and the covari-ation with respect to each other. In addition, with the assistance of spectral decomposition of the gradient outer product matrix, the e.d.r. directions can be directly estimated [1]. F urthermore [7] extended gradient learning algorithm from the Euclidean space to the manifolds setting, and provided the conver-gence rate dependent on the intrinsic dimension of the manifold rather than the dimension of the ambient space. This is very important in the  X  X arge m , small n  X  settings. Except for the application examples proposed in the avail-able literature, gradient learning from scattered data sets is particularly im-portant for surfaces reconstructi on in computer graphics where, when visually scaled geometric surfaces constructed from scattered data, analytical expression (or rules) of gradients was highly desirable in calculating the nor-mals at any given point needed in most surface reconstruction algorithms (see [8]).

However, these direct gradient learning methods cannot offer any reasonable error bars for the estimated gradients because essentially the task of estimating gradients is the problem of point estimation. In many application scenarios, a confidence interval on the estimates is very important, such as found in computer graphics.

In this paper, we propose a new gradient learning approach under the Bayesian framework based on Gaussian Processes (GPs) [9]. Compared to the learning gradients method in [4], not only can our algorithm apply in the  X  X arge m ,small n  X  cases and achieve higher accuracy, but it can also return the error bars of the estimated gradients, which provide us with an estimate of the uncertainty of stability. We will verify these features in Sections 5.

The rest of this paper is organized as follows. In Section 2, we introduce the statistical foundation for learning gradients. The gradients learning method with Gaussian Processes will be proposed in Section 3, which includes a brief introduction of the Gaussian Processes regression. The algorithm derivation is illustrated in Section 4. Then, simulated data are used to verify our algo-rithm in Section 5. Finally, closing remarks and comments will be given in Section 6.
 2.1 Notations metric subspace X X  R m and y i  X  R p is a vector too. Without loss of generality, we will assume that p = 1. Our approach can be easily extended to the case of vectorial value outputs y . Typically we assume that the data are drawn i.i.d. we want to model the regression function F defined by the conditional mean of Y |
X , i.e., F = E Y [ Y | X ]. The gradient of F is a vectorial value function with m components, if all the partial derivatives exist,
The gradient and the issues of variable se lection and coordinate covariation are relevant because the gradient can provide following information [4]: 1. Variable selection: the norm of the partial derivative 2. Coordinate covariation: the inner product of the partial derivatives with A central concept in all gradients learning approaches, called the gradient outer product (GOP) matrix, is defined by The GOP has a deep relation with the so-called effective dimension reduction (e.d.r.) space and the relationship was exploited in several gradient regression methods such as MAVE and OPG [3] and [4,5]. 2.2 Learning Gradients To propose our approach for gradients learning, let us focus on the introduction of those available algorithms of learning the gradients from the data. Recall that the MAVE and OPG suffer from the problem of the overfitting in the  X  X arge m ,small n  X  paradigm, so the so-called regularized framework has been used to overcome the overfitting based on the kern el representer theorem. Actually the kernel representer theorem is also the motivation for our algorithm in the next section. The algorithms based on the ker nel representer theorem show better performance than the MAVE and OPG [1].

Our goal is to design a model for the gradient estimate directly from data. The conventional methods usually take a two-steps procedure by first learning a re-gression function F and then calculating the gradients of the F . However a direct gradients learning algorithm may have more advantages than the conventional ways, as demonstrated in [4,1].

Essentially, all of those kinds of models are motivated by the Taylor expansion of the target function: A model for gradients leaning f rom an observation dataset D is defined as where w ij is called weights and defined as w ij = 1  X  m +2 exp  X  2 is set to the median of the input data. When x j is far away from x i ,the Taylor expansion of the function F ( x j )at x i makes less contribution to the regression objective.

According to the representer theorem [10], the optimal solution to (4) is the linear combination of kernel function defined on the data points, thus the prob-lem is actually transformed to solving a linear systems problem, see [4].
Due to regularization, this model can prevent overfitting in the  X  X arge m ,small n  X  paradigm and obtain fairly remarkable performance. However, sometimes it is also important that we want to know the error bars of the point estimation for the gradient, which can not be provided by those kinds of models.
An alternative method is to define the model under the Bayesian learning and inference framework. We aim to use the Gaussian Processes (GPs) model which is also based on the kernel and can be viewed as the exhibition of the representer theorem. So motivated by th e model in [4] and associated it with the GPs, we will show how to improve the accuracy and compute the error bars of the estimated gradient in the following section. 3.1 Gaussian Processes Regression Given the data set which consists of the i.i.d. samples from unknown distribu-regression is concerned with the case when p = 1. The goal is to estimate the p ( y | x  X  )foratestdata x  X  . In the standard Gaussian processes (GPs) regression model, a latent variable f is introduced in the model defined by model is nonparametric because the latent variable f is random function which follows the Gaussian Process with zero mean and covaiance function k (  X  ,  X  ). Also the likelihood follows a Gaussian distribution with zero mean and covariance  X  2 t .
Denote by X = { x i } n i =1 . Due to the independence of the samples D ,its likelihood under the model is the product of p ( y i | f ( x i )) which is a Gaussian too. Given a test point x  X  , it is easy to check that the joint distribution of the latent function is given by, see [9], where K are matrix of the kernel function values at the corresponding points and N (  X ,  X  ) denotes the Gaussian distribution with mean  X  and covariance  X  .
Under the Gaussian likelihood assumption, we can simply add the covariance of the noise to the GP prior due to the independence assumption of the noise. So the predictive distribution on the observation is f where the variance of the conditional distribution illustrates the uncertainty of can be represented in terms of a number of basis function is one feature of the representer theorem. 3.2 Gradients Estimation Model with Gaussian Processes To apply the Gaussian Process model in the case of gradients learning, we have to overcome two hurdles. First, the regression model (4) shows that we are dealing with a multi-task regression problem as the gradient f is a vectorial function, so we have to generalize the standard Gaussian Process regression to multi-task case. This has been done in the recent works such as [11]. Second, the i.i.d. assumption for the data set can not be used to produce a joint likelihood which is the product of individual likelihood at each data point. In fact, when we transform (4) into probabilistic formulation, we see that the coupling between data makes learning and inference more complicated. However, we can still define a likelihood for the whole data set D rather than a likelihood for each data pair.
Under the above modification, we can formulate the gradients learning model in the Bayesian framework based on the GPs, named Gaussian Process Gradient Learning (GPGL) model, and we will show the advantages in Section 5.
Based on the analysis we have just given, a new likelihood formulation by extending the datum-based likelihood to dataset-based likelihood is defined as m  X  m matrices B weights w ij as [4] for comparison. Furthermore define B = U T diag( B 1 ,B 2 ,  X  X  X  , B n ) U and a column vector of dimension mn h = U T [ h T 1 ,h T 2 , is a permutation matrix. Similarly define the column vector (of dimension mn )
Under the above notation, it is easy to validate that the likelihood (7) of the observation dataset D can be written as where M is the normali zed constant.

The variable f collects the information of m partial derivatives over the given input data X . In our model formulation, the variable f is assumed to be a Gaussian Processes while the covariance function is  X  = K ff  X  K XX .Sothe Gaussian processes prior is where K ff  X  R m  X  m is the coordinate-similarity matrix, K XX  X  R n  X  n is the covariance matrix of the samples X ,and  X  is the parameters of the covariance function.

By using the Bayesian formulation, the posterior of f given the dataset is As all the densities in the above relation are Gaussian, it is easy to derive, see Appendix A of [9], the posterior of f where E =( B +  X   X  1 )  X  1 .

For a new data x  X  , we want to estimate f  X  = f ( x  X  ) based on the observation data. According to the predictive distribution of Gaussian processes, we have f | f , x  X  ,X, X   X  X  (( K f  X  K  X  f according to the posterior ( 11), we can get the gradients predictive distribution That is, the gradients predictive distribution is a Gaussian with the mean P and the covariance Q . Thus the gradient estimate is given by and the error bar is given by To develop an approach for learning coordinate-similarity matrix K ff and the kernel hyperparameters, we use gradient-based optimization of the marginal like-Since K ff controls the correlations between m dimensions of the gradients, the simplicity means that we are assuming the independence of different coordinates. Actually the optimization with respect to the parameters in K ff can be dealt with in the same way as follows [11].
 Then the log marginal likelihood log p ( Y | X,  X  )isgivenby where C is a constant independent of the parameter  X  , which can be ignored in optimizing L with respect to  X  . To work out a formula for the derivatives of L with respect to  X  , we refer to the matrix reference manual for the notation [12]. Denote by F 1 =  X  log | B  X  1 +  X  | ,then dF 1 =  X  ( B  X  1 +  X  )  X  1 : T d (  X  ):. Similarly, we have dF 2 = F
According to the derivative formula for the Kronecker product of matrices, where T m,n , called the vectorized transpose matrix, is the mn  X  mn permutation matrix whose ( i, j )-th elements is 1 if j =1+ m ( i  X  1)  X  ( mn  X  1) i  X  1 n or 0 otherwise.

So the derivatives of L with respect to  X  is In our experiments, we learn the param eters of the models so as to maximize the marginal likelihood using gradient-based search. The code is based on Neil D. Lawrence X  X  MATLAB packages Kern and Optimi 1 .

We have seen that ( B  X  1 +  X  )  X  1 needs to be inverted for both making pre-dictions and learning the hyperparameters in time O ( m 3 n 3 ). This can lead to computational problems if mn is large. Although we only use cholesky decompo-sition and singular value decompositio n to accelerate computation, the efficient approximation method in [11] can be directly used in our GPGL algorithm to reduce the computational complexity. In this section we will verify our GPGL algorithm in two simulated data sets to show the higher accuracy of the estimation and the credible intervals that the gradient learning methods in [4], named Mukherjee X  X  algorithm in the following, can not gain. In the first data set, we generate some samples from four simple functions which can compute the real gradients for comparison. Another high-dimensional data set is used to test that our algorithm can be applied to show the variable selection and coordinate covariance like Mukherjee X  X  algorithm. 5.1 Error Bar Estimation We illustrate how GPGL can be used to estimate the credible intervals of the estimated gradient and compare Mukherjee X  X  algorithm with GPGL to show higher accuracy that GPGL demonstrates.

Given four representative elementary regression models y =exp( x ); y = our experiment, we sampled 100 points from the Gaussian distribution. The true derivatives are given by y =exp( x ); y =1 /x ; y =2  X  x ; y =cos( x ), respectively. The comparison of the re sults between proposed GPGL algorithm and Mukherjee X  X  algorithm is shown in F igures 1 to 4. We use the mean squared error between the true derivative and learned derivative to measure the quality of learning algorithm. The smaller MSE means that a better performance of the algorithm. All the MSEs for those four functions with different algorithms are collected in Table 1. It can be seen that the proposed GPGL algorithm gives better performance in terms of lower MSEs for three out of the four functions.
Although for the functions y =exp( x )and y = x 2 , Mukherjee X  X  algorithm gives slightly better results, the proposed GPGL algorithm outperforms Mukher-jee X  X  Algorithm in other cases. However , in the experiment, we find that Mukher-jee X  X  algorithm is sensitive to the value of regularization parameter and the percentage of the eigenvalues parame ters (see the code in [4]) that need to be chosen manually, especia lly the regularization parameter. Sometimes, it is hard to choose them optimally, although a standard cross validation can be applied. However, the proposed GPGL method does not suffer from this prob-lem and is more stable with ability to automatically adapt parameters. In ad-dition, the error bars can be obtained from our algorithm along with gradient estimation. 5.2 High-Dimensional Data Set Definition 1. The empirical gradient matrix (EGM), F z ,isthe m  X  n matrix whose columns are f ( x j )with j =1 ,  X  X  X  ,n . The empirical covariance matrix (ECM), is the m  X  m matrix of inner products of the directional derivative of two coordinates, which can be denoted as Cov( f ):=[ ( f ) p , ( f ) q K ] m p,q =1 .
The ECM gives us the covariance between the coordinates while the EGM provides us information about how the variables differ over different sections of the space.

For a fair comparison, we construct the same artificial data as those used in [4]. By creating a function in an m = 80 dimensional space which consists of three linear functions over different partitions of the space, we generate n =30 samples as follows: 1. For samples { x i } 10 i =1 , 2. For samples { x i } 20 i =11 , 3. For samples { x i } 30 i =21 Arepresentationofthis X matrix is shown in Figure 5(a). Three vectors with support over different dimension s were constructed as follows: Then the function is defined by 1. For samples { y i } 10 i =1 y i = x i w 1 + N (0 , X  y ) , 2. For samples { y i } 20 i =11 y i = x i w 2 + N (0 , X  y ) , 3. For samples { y i } 30 i =21 y i = x i w 3 + N (0 , X  y ) .
 Adrawofthe y values is shown in Figure 5(b). In Figure 5(c), we plot the norm of each component of the estimate of th e gradient using the GPGL algorithm. The norm of each component gives an indication of the importance of a variable and variables with small norms can be eliminated. Note that the coordinates with nonzero norm are the ones we expect, l =1 ,  X  X  X  , 20 , 41 ,  X  X  X  , 50. In Figure 5(d) we plot the EGM, while the ECM is displayed in Figure 5(e). The blocking structure of the ECM indicates the coordinates that covary. The similar result can be found in [4]. In this paper we have proposed a direct gradient learning algorithm from sample dataset in the Bayesian framework. The Gaussian Processes Gradient Learning (GPGL) model we propose can be seen as th e manifestation of the representer theorem which is the basis of Mukherj ee X  X  algorithm. However, only the GPGL model can provide the error bars of the estimated gradients which characterize the uncertainty of the estimation. Besides, the GPGL model is stable and shows higher accuracy than Mukherjee X  X  al gorithm in terms of MSE in some circum-stances. Another advantage is that GPGL model is more stable with automatical parameter adapting while the result from Mukherjee X  X  algorithm heavily depends on the better tuning of the regularization parameters. In future work we plan to extend GPGL to sparse model to improve the generalization capability that is especially useful in the  X  X arge m ,small n  X  setting.

