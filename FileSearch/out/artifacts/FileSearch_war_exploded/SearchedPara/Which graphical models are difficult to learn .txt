 G is the pairwise Markov random field over binary variables x = ( x parameters  X  G .) x defined as where P we let  X  The general problem is therefore to characterize the functi ons n ticular for an optimal choice of the algorithm. General boun ds on n such a general characterization, in this paper we estimate n p , with typically  X  1.1 A toy example: the thresholding algorithm G by thresholding the empirical correlations 1: Compute the empirical correlations { b C 2: For each ( i, j )  X  V  X  V 3: If b C of the empirical correlations, i.e.  X   X  =  X  (  X  ) such that Further, the choice  X  (  X  ) = (tanh  X  + (1 / 2 X )) / 2 achieves this bound. thresholding algorithm always fails with high probability . apart variables x bounded degree graphs with long-range correlations. 1.2 More sophisticated algorithms In this section we characterize  X  lenging result, namely Theorem 1.6.
 to be bounded: |  X  X  | X   X  . 1.2.1 Local Independence Test ture encoded by the graph [1, 4, 5, 6]. the value of x of vertices, thus motivating the following algorithm. 1: Select a node r  X  V ; 2: Set as its neighborhood the largest candidate neighbor U of 3: Repeat for all nodes r  X  V ; b
P b P computation of the S CORE ( U ) and the computation of b P Both theorems that follow are consequences of the analysis o f [4]. numerical constant K , such that More specifically, one can take  X  = 1 makes the above approach impractical.
 thresholding: that C this is true at small  X  .
 enough constant K . Then there exists  X ,  X ,  X  such that n More specifically, we can take  X  = tanh  X  ,  X  = 1 1.2.2 Regularized Pseudo-Likelihoods As a specific low complexity implementation of this idea, we c onsider the  X  where x from the following extension of (1), where  X  = {  X  and  X  The function L (  X  ; { x (  X  ) } ) depends only on  X  borhood of each node by the following algorithm, Rlr (  X  ) , 1: Select a node r  X  V ; 2: Calculate  X   X  3: If  X   X  Theorem 1.5. There exists numerical constants K be a graph with degree bounded by  X   X  3 . If  X   X  K Further, the above holds with  X  = K This theorem is proved by noting that for  X   X  K n graph of regular degree  X  .
 of the model parameter  X  and of the regularization parameter  X  for different  X  , and should correspond to optimal regularization. specific graph families.
 n to the Lasso in [14] is also relevant for the Ising model. for  X   X   X  or in the weakly coupled regime  X  &lt;  X  structural learning is also intrinsically easier.
 ization parameter as  X  = 2  X  is empirically the most satisfactory), and searched over  X  grid  X 
P analytically, cf. Lemma 3.3 below, and get  X  the data. notations. If M is a matrix and R, P are index sets then M parameters and write  X  as a shorthand of  X  of samples is n . Note that, since we assumed  X   X   X  0 ,  X  z  X  Hessian of L (  X  ; { x (  X  ) } ) and Q (  X  ) = lim the Hessian of E of a symmetric matrix M by  X  Throughout this section G is a graph of maximum degree  X  . 3.1 Proof of Theorem 1.6 Our first auxiliary results establishes that, if  X  is small, then || Q  X  condition for the failure of Rlr (  X  ) .
 Lemma 3.1. Assume [ Q  X  C where  X  n X  graph with only one edge between nodes r and i and n X  2  X  K , then Finally, our key result shows that the condition || Q  X  result for ferromagnetic Ising models on random graphs prov ed in [17]. small. Then, there exists  X  probability converging to 1 as p  X  X  X  .
 Furthermore, for large  X  ,  X  C 1 as p  X  X  X  .
 forward and we omit its proof for space reasons.
  X  ), and  X , C a some i  X  V \ r .
 In particular P 3.2 Proofs of auxiliary lemmas (  X   X 
S , 0) for any  X   X  { x (  X  ) } in all the expression derived from L .
 theorem yields where W n =  X  X  X  L (  X   X  ) and [ R n ] Breaking this expression into its S and S c components and since  X   X   X   X 
S  X   X  Now notice that Q n where  X  and E From E it follows that  X  bound the absolute value of the i th component of  X  z where the subscript i denotes the i -th row of a matrix. assumption, this implies |  X  z has components of magnitude at most 1 .
 manipulations imply (See Lemma 7 from [7]) For t  X  N , let T ( t ) = ( V Ising measure as  X  X xternal field X  that non-trivial local expectations with r espect to More precisely, let B we are trying to reconstruct). For any fixed t , the probability that B goes to 0 as p  X   X  . Let g ( x g (  X  x B p The proof consists in considering [ Q  X  ( Q [ Q calculus and we omit them from this outline.
 The lower bound on  X  Acknowledgments and the NSF grant DMS-0806211 and by a Portuguese Doctoral FC T fellowship.
