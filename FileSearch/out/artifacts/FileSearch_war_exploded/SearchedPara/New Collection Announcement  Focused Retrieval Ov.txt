 Focused retrieval (a.k.a., passage retrieval) is important at its own right and as an intermediate step in question an-swering systems. We present a new Web-based collection for focused retrieval. The document corpus is the Category A of the ClueWeb12 collection. Forty-nine queries from the educational domain were created. The 100 documents most highly ranked for each query by a highly effective learning-to-rank method were judged for relevance using crowdsourc-ing. All sentences in the relevant documents were judged for relevance.

Many retrieval applications and tasks rely on passage re-trieval ; that is, retrieving document parts (passages), rather than whole documents, in response to expressions of infor-mation needs. Question answering systems, for example, often apply passage retrieval in response to the question at hand [7, 1]. Then, an answer is compiled from the top re-trieved passages. In focused retrieval systems, the result list retrieved for a query is composed of sentences [10, 25, 24] or more generally passages [2, 3, 4, 6, 9].

To advance the development of passage retrieval methods  X  e.g., in light of the recent resurgence of interest in Web question answering [1]  X  collections for evaluating passage retrieval effectiveness are called for. In this paper we de-scribe a new such Web-based collection.
 The document corpus is category A of the English Clue-Web12 collection which contains about 733 million docu-ments. Forty-nine short keyword queries, accompanied by descriptions of the information need, were created based on questions posted on community question answering sites and questionnaires. The queries are from the education domain and are of topical nature. They represent various informa-tion needs of parents (henceforth, the target group). Fur-thermore, educational topics are of interest to a wide range of additional users, such as education experts, journalists, policy makers, and students.

For each query, a document list was retrieved using a highly effective learning-to-rank method. All sentences in documents in the list were judged for relevance using crowd-sourcing. The final collection as well as the data processing pipeline are publicly available. 1
The Novelty tracks of TREC [10, 25, 24] used relevance judgments for sentences and the HARD (High Accuracy Retrieval from Documents) tracks [2, 3, 4] used relevance judgments for passages. These tracks rely on old (mainly) newswire TREC document corpora and are rarely used nowa-days. In contrast, our dataset is based on the newest Web collection of TREC (ClueWeb12).

The task in the TREC Question Answering (QA) track was to provide a short, concise answer to a factoid question [26]. In contrast, our queries are opinion-seeking and cannot necessarily be answered by a short string. The annotators in the TREC QA track evaluated the short answer string, while our annotators were asked to determine the relevance to a query of each sentence in the top retrieved documents.
The dataset can be downloaded at: http://ie.technion. ac.il/  X kurland/dip2016corpus/; the code used to process the data can be found at: https://github.com/UKPLab/ sigir2016-collection-for-focused-retrieval.
There is a Wikipedia-based INEX (Initiative for the Eval-uation of XML Retrieval) collection with relevance judg-ments provided for parts of documents with respect to queries [6, 9]. The relevance judgment regime that was found to be the most robust with respect to inter-annotator agreement rates was highlighting all and only relevant text in each doc-ument [21]. We adopt a similar (binary) relevance judgment regime, but use crowdsourcing to produce judgments rather than trained annotators as was the case in INEX. Further-more, we use a noisy Web collection (ClueWeb12) rather than the well edited Wikipedia collection.

A recently introduced collection provides relevance judg-ments for  X  X nswer passages X  [13]: passages in the docu-ments most highly ranked in response to a query are judged with respect to TREC topics (specifically, their descriptions) that are of a question form/type; the document collection is TREC X  X  GOV2. In contrast, the queries we have devel-oped are not necessarily of question form (e.g., many of the queries are of topical nature), and we use the much larger and noisier ClueWeb12 collection.
To ensure high variability of queries, as well as their per-tinence to the target group (parents), we combined two ap-proaches for query compilation. The first approach relied on exploiting existing Question-Answering Web portals and the second approach utilized a user questionnaire.
 To sample users X  information needs from QA sites, we used Yahoo L6 Question Answering Collection 2 [23] and selected about 150k questions from the Education section which con-tains nine categories. Questions from each category were projected onto a latent 300-dimensional semantic space us-ing word2vec [18] and independently clustered into k/ 10 clusters (where k is a number of question in the category) using the CLUTO software package 3 with the Repeated Bi-section clustering method [27]. We randomly sampled 10 questions from each of the 5 largest clusters, and conducted a survey with eight participants whose task was to formulate a query/queries that would best represent the information needs reflected by that cluster. This process yielded roughly 200 queries in total, from which 39 queries were randomly selected. The selected queries were enriched with a specifi-cation of what relevant information is to be expected from the system; these specifications are similar to the narratives used in TREC.
 Although the Yahoo QA set is very rich, it usually covers US-specific topics. We thus created an additional Google Forms based online questionnaire of 20 parental issues and distributed it among 51 non-US parents of children aged 1-21. The instructions were to rank each issue according to the level of interest that causes participants to search for online information, on a scale from 1 (not interested at all) to 5 (very interested). For the 10 top ranked queries, a description was added.

The final query set (examples of query titles are presented in Table 3) consists of 49 queries combined from the two
Provided by Yahoo! Webscope, http://research.yahoo. com/Academic Relations http://www.cs.umn.edu/  X karypis/cluto aforementioned approaches. 4 Following common practice, a query is composed of a short title and information need de-scription. To make the annotation task easier for non-expert judges, we presented the information need description in a table. The table provides context that cannot be inferred from the title alone, and examples of relevant and irrele-vant information that the annotator may encounter (see an example in Table 1).
The full ClueWeb12 dataset (category A), which contains about 733 million documents, was used for creating the col-lection. For the retrieval stage, the documents and the queries were stemmed using the Krovetz stemmer via the Indri toolkit 5 which was also used for retrieval. The 100 documents most highly ranked with respect to a query were annotated. (Details of the retrieval method are provided below.) Some of the (Web) documents are noisy and hard to read. Thus, we pre-processed all documents before the annotation. Specifically, we applied a state-of-the-art boil-erplate removal tool [22] and a sentence splitter [16]. As a result of applying the boilerplate, a few documents became empty and were removed from the lists to be annotated.
We first ranked all the documents in the dataset with re-spect to a query using the negative cross entropy between the unsmoothed unigram language model induced from the query and the Dirichlet-smoothed unigram language model induced from each of the documents [15]. Following common practice [8], documents assigned with a score below 70 by Waterloo X  X  spam classifier were filtered out from this initial ranking top down until 1000 presumably non-spam docu-ments were accumulated. The remaining documents were then re-ranked to produce the final document ranking.
To re-rank the documents, a learning-to-rank approach was applied with 130 features. Most of these features were used in Microsoft X  X  learning-to-rank datasets 6 with the fol-lowing exceptions. Instead of the two quality features (Qual-ityScore and QualityScore2), which are not available for the ClueWeb12 dataset, we used query-independent document quality measures that were shown to be highly effective
We removed one query from the 50 selected queries as it asked for a site containing only links, which has a very differ-ent nature from the topical queries in the rest of the corpus. www.lemurproject.org/indri www.research.microsoft.com/en-us/projects/mslr Documents 4,820 Sentences 628,026 Workers 2,041 Sentences/HIT 1...40 41...80 81...120 HITs 2199 1800 4,130 Average duration (sec) 67 87 150 for spam classification [20] and Web retrieval [5]. Specif-ically, we used as features the ratio between the number of stopwords and non-stopwords in a document, the per-centage of stopwords in a stopwords list that appear in the document and the entropy of the term distribution in a document. As is the case with the features used in Mi-crosoft X  X  datasets, these quality measures were computed for the entire document, its body, title, URL and anchor text. We used the score assigned to a document by Wa-terloo X  X  spam classifier [8] as an additional quality measure. Hence, Waterloo X  X  spam classifier served both for filtering out documents from the initial document ranking and as a feature in the learning-to-rank model. Additional features used in Microsoft X  X  datasets that were not considered here are the Boolean Model, Vector Space Model, LMIR.ABS, Outlink number, SiteRank, Query-URL click count, URL click count, and URL dwell time.

To integrate the features we used SVM rank [12] applied with a linear kernel and default free-parameter values. The titles of topics 201-250 from TREC 2013 were used as queries to train the model. The Dirichlet smoothing parameter in LMIR.DIR, which served both for creating the initial rank-ing and as a feature in the learning-to-rank model, was set to  X  = 1000. We used LMIR.JM with  X  = 0 . 1; for BM25, we set k 1 = 1 and b = 0 . 5. The INQUERY list was used for computing the two stopword-based quality measures.
Although the majority of documents (  X  74%) are no longer than 120 sentences, the document length distribution is heavy-tailed: 73% of the sentences are in documents containing over 120 sentences (e.g., the longest document contains over 4500 sentences). It has been frequently pointed out in pre-vious work on crowdsourcing that unlike traditional annota-tion approaches, a so-called  X  X un factor X  can strongly affect the annotation quality and workers X  response. In some cases it played even a more important role than the size of the re-ward [19, 14]. Thus, during a pilot study, three experts were instructed to provide their feedback on how many sentences they can annotate without significant loss of concentration. Based on their observations, the length of a single Human Intelligence Task (HIT) was limited to 120 sentences. If a document contained more than 120 sentences, it was split on paragraph borders so that each split segment would have no more than 120 sentences. If a document or a split seg-ment contained less than 80 or less than 40 sentences, it was grouped in medium and short HIT groups, respectively. Table 2 sums up the annotation setup details.

We performed crowdsourcing using the Amazon Mechan-ical Turk platform. The task was designed as follows: the workers were invited to read a document from top to bottom. Each document was split into sentences and no paragraph marking was preserved. The workers were to judge each sen-tence individually as relevant or not to a given query. The instructions asked workers to base their decision on two lists of relevant and irrelevant examples (Table 1). If a worker de-cided that a sentence or several sentences were relevant then they should highlight it by either clicking on it or dragging the mouse over the relevant sentences. 7 We also provided a link to guidelines 8 with an extended definition of what should be considered relevant. According to the guidelines, a sentence can be either: Workers were asked to highlight only sentences that are clearly relevant and relevant in context.

In order to be allowed to work on a HIT, workers were required to have two Amazon Mechanical Turk Qualifica-tions: 1. They must be based in the US. 2. They must have acceptance rate higher than 95%. Rather than gener-ating the gold data through a majority vote over five work-ers X  decisions, we integrated MACE, an unsupervised tool for quality prediction of non-expert annotations [11], in our publicly available pipeline and extended the guideline by a warning about automatic cheater detection. Later on, 64 workers were blocked from working on the HITs based on the competence scores assigned by MACE.

Table 2 provides a summary of the resulting dataset. The dataset includes 4820 annotated documents and over 600k annotated sentences (5 assignments per sentence) for 49 queries. The total cost of the annotation was 3880 US Dol-lars. The workers X  response varied depending on the length of the documents. The best response was observed for short HITs with an average annotation speed of 288 HITs a day, while long HITs were annotated with an average speed of 174 HITs a day. Although we did not explicitly instruct work-ers to submit any qualitative user feedback, the annotation interface had a comment field which was mostly designed to provide a convenient way for workers to report tech-nical problems. Interestingly, we received over 2000 com-mented HITs (10% of the total) with multiple positive feed-back about the content of the annotated documents. Many workers pointed out that they found articles useful and ed-ucational and, hence, enjoyed working on the task despite a modest reward. This demonstrates, again, the importance of the entertainment component for the success of a crowd-sourcing annotation task, as it is very likely that the high agreement between annotators (see Section 3.4) is a direct http://tinyurl.com/jdxuyyl http://tinyurl.com/zvwjm2p consequence of the fact that many workers were attentively reading documents because of their personal interest in the documents X  content.
For each annotated document, we computed observed an-notation agreement using the DKPro Agreement package [17]. 9 The minimal units for agreement computation were sentences, each with two categories per annotator (relevant or non-relevant). Average agreement over all documents judged is 0 . 725 and standard deviation is 0 . 175 (where agree-ment ranges between 0 and 1). Interestingly, we found that the average (over sentences) agreement on non-relevant doc-uments is statistically significantly higher than that on rele-vant documents: 0 . 880 vs 0 . 706 respectively 10 . Presumably, when the document is relevant the annotators may compre-hend it differently as the task of marking relevant sentences is challenging; in contrast, it is easier to agree on irrelevant documents, especially if these are off topic.

Overall, the documents retrieved for 49 queries were an-notated. Per query, about 98 documents on average were annotated on a sentence level. On average, about 87 docu-ments and about 4618 sentences per query were judged rel-evant. Overall, about 89% of the annotated documents are relevant and about 36% of the sentences are relevant. The Normalized Discounted Cumulative Gain (NDCG) at top 30 documents is 0 . 924 and the precision at top 5 ranks is 0 . 943; these performance numbers attest to the high effectiveness of the retrieval.

We presented a novel Web-based collection for query-based focused retrieval (a.k.a. passage retrieval) with sentence-level relevance judgments. The document corpus is the Cat-egory A of the ClueWeb12 collection; forty-nine queries from the educational domain are used.
 Acknowledgments. We thank the reviewers for their com-ments. This paper is based upon work supported in part by the German Research Foundation (DFG) via the German-Israeli Project Cooperation (DIP, grant DA 1600/1-1), the Volkswagen Foundation as part of the Lichtenberg-Profes-sorship Program under grant N o I/82806, and the Technion-Microsoft Electronic Commerce Research Center.
The dynamic nature of assigning HITs to workers in crowd-sourcing as well as splitting long documents into several HITs do not allow us to compute traditional inter-annotator statistics like Cohen X  X   X  , Krippendorff X  X   X  or Fleiss X   X  , as these measures expect a fixed set of the same annotators over the entire data.
We used two tailed permutation test at 95% confidence level to test the difference.
