 As the volumes of AI problems involving human knowledge are likely to soar, crowdsourcing has become essential in a wide range of world-wide-web applications. One of the biggest challenges of crowdsourcing is aggregating the answers collected from crowd workers; and thus, many aggregate techniques have been proposed. However, given a new application, it is difficult for users to choose the best-suited technique as well as appropriate parameter values since each of these techniques has distinct performance character-istics depending on various factors (e.g. worker expertise, question difficulty). In this paper, we develop a benchmarking tool that al-lows to (i) simulate the crowd and (ii) evaluate aggregate techniques in different aspects (accuracy, sensitivity to spammers, etc.). We believe that this tool will be able to serve as a practical guideline for both researchers and software developers. While researchers can use our tool to assess existing or new techniques, developers can reuse its components to reduce the development complexity. Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Selection process General Terms: Algorithms, Design, Experimentation.
 Keywords: benchmark, crowdsourcing, aggregate technique.
Today, crowdsourcing becomes a promising methodology to over-come various problems that require human knowledge such as im-age labeling, text annotation, and product recommendation [1]. A wide range of applications (e.g. ESP game, reCaptcha, and Free-base [2]) have been developed on top of more than 70 crowdsourc-ing platforms 1 such as Amazon Mechanical Turk and CloudCrowd. The rapid growth of such crowdsourcing applications opens up a variety of technical and social challenges [2].

One of the most critical issues of crowdsourcing is to aggregate different answers given by crowd workers . This is a challenging task because of two reasons: (i) the workers might have wide rang-ing levels of expertise and (ii) the questions may vary in different levels of difficulty. While the former leads to high contradiction http://www.crowdsourcing.org and uncertainty in the answer set, the latter renders some difficulties in distinguishing between truthful workers and malicious workers. To fully tackle this challenge, a rich body of research on answer aggregation has developed different techniques.

However, each work often reported its superior performance gen-erally using a limited variety of data sets of evaluation method-ologies. As a result, understanding the performance implications of these techniques, for a given type of application, is difficult to comprehend. Therefore, we present the Benchmark for Aggregate Techniques in Crowdsourcing (BATC) with three functionalities:
To support these functionalities, we design our tool with three main features: (i) simulate the crowd, (ii) re-implement state-of-the-art aggregate techniques within a common framework, and (iii) evaluate these techniques with different metrics. To the best of our understanding, BATC is the first system to provide these attractive features. In the following, we first describe the system overview and implementation details in Section 2. Next, Section 3 presents some demonstrations. Finally, Section 4 summarizes the paper.
Figure 1 illustrates the simplified architecture of our framework X  which is built upon three layers: data access layer, computing layer and application layer . The data access layer abstracts underlying data objects, which could be synthetic or real data. The application layer provides an interactive GUI to users. The computing layer consists of two important modules: Our benchmarking tool is developed as an Eclipse Rich Client Platform (RCP) application. The runnable file and the demo video of this tool are publicly available at our website 2 .
We will demonstrate the benchmarking capabilities of BATC as described in Section 1. Users are able to simulate crowdsourcing process that involve different types of workers, questions, and ag-gregate techniques. They can also choose real datasets. To provide in-depth analysis, we characterize the aggregate techniques evalu-ated in the benchmark using five measures: https://code.google.com/p/benchmarkcrowd
BATC visualizes benchmarking results in several views, allow-ing users to compare multiple settings and choose the best-suited technique for their applications. Figure 2 depicts the interactive GUI of BATC that supports several operations such as zooming, panning, and dragging&amp;dropping. In the BATC X  X  interface, users may explore different parameter configurations and choose appro-priate values for their application requirements.
We have developed a benchmarking tool that focuses on pro-viding in-depth analyses and practical guidelines. The target users (researchers and developers) can use BATC to select and configure well-suited aggregate techniques for a potential application. This tool is built upon a component-based architecture, in which new techniques and new measurements can be easily plugged. As the source code as well as the demonstrations are publicly available, we expect that our reusable framework will be refined and improved by the research community, in particular when more data become available, more experiments are performed, and more techniques are integrated into the framework in the future.
 This research has received funding from the NisB -European project (FP7/2007-2013) under grant agreement number 256955.
 [1] L. von Ahn.  X  X uman computation X . In: Design Automation [2] A. Doan et al.  X  X rowdsourcing systems on the World-Wide [3] P. G. Ipeirotis et al.  X  X uality management on Amazon Me-[4] D. Karger et al.  X  X terative learning for reliable crowdsourcing [5] F. Khattak et al.  X  X uality Control of Crowd Labeling through [6] K. Lee et al.  X  X he social honeypot project: protecting online [7] V. Raykar et al.  X  X upervised learning from multiple experts: [8] J. Vuurens et al.  X  X ow much spam can you take? an analy-[9] J. Whitehill et al.  X  X hose vote should count more: Optimal
