 The Internet has enabled the creation of a growing num-ber of large-scale knowledge bases in a variety of domains containing complementary information. Tools for automati-cally aligning these knowledge bases would make it possible to unify many sources of structured knowledge and answer complex queries. However, the efficient alignment of large-scale knowledge bases still poses a considerable challenge. Here, we present Simple Greedy Matching ( SiGMa ), a simple algorithm for aligning knowledge bases with millions of enti-ties and facts. SiGMa is an iterative propagation algorithm that leverages both the structural information from the re-lationship graph and flexible similarity measures between entity properties in a greedy local search, which makes it scalable. Despite its greedy nature, our experiments indicate that SiGMa can efficiently match some of the world X  X  largest knowledge bases with high accuracy. We provide additional experiments on benchmark datasets which demonstrate that SiGMa can outperform state-of-the-art approaches both in accuracy and efficiency.
 D.2.12 [ Software Engineering ]: Interoperability X  Data map-ping ; H.3.4 [ Information Storage and Retrieval ]: Sys-tems and Software X  Information networks ; I.2.4 [ Artificial Intelligence ]: Knowledge Representation Formalisms and Methods X  Semantic networks knowledge base; alignment; large-scale; entity; relationship; greedy algorithm
In the last decade, a growing number of large-scale knowl-edge bases have been created online. Examples of domains include music, movies, publications, and biological data As these knowledge bases sometimes contain both overlap-ping and complementary information, there has been grow-ing interest in attempting to merge them by aligning their common elements. This alignment could have important uses for information retrieval and question answering. For example, one could be interested in finding a scientist with expertise on certain related protein functions  X  information which could be obtained by aligning a biological database with a publication one. Unfortunately, this task is challeng-ing to automate as different knowledge bases generally use different terms to represent their entities, and the space of possible matchings grows exponentially with the number of entities.

A significant amount of research has been done in this area  X  particularly under the umbrella term of ontology match-ing [7, 16 , 12 ]. An ontology is a formal collection of world knowledge and can take different structured representations. In this paper, we will use the term knowledge base to empha-size that we assume very little structure about the ontology (to be specified in Section 2). Despite the large body of lit-erature in this area, most of the work on ontology matching has been demonstrated only on fairly small datasets of the order of a few hundred entities. In particular, Shvaiko and Euzenat [26 ] identified large-scale evaluation as one of the ten challenges for the field of ontology matching.

In this paper, we consider the problem of aligning the in-stances in large knowledge bases, of the order of millions of entities and facts, where aligning means automatically iden-tifying corresponding entities and interlinking them. Our starting point was the challenging task of aligning the movie database IMDb to the Wikipedia-based YAGO [28]. This provides another step towards the Semantic Web vision of interlinking different sources of knowledge as exemplified by the Linking Open Data Initiative 2 [4]. Initial attempts to match IMDb entities to YAGO entities by naively exploit-ing string and neighborhood information failed; we thus de-signed SiGMa ( Simple Greedy Matching ), a scalable greedy iterative algorithm that is able to exploit previous match-ing decisions as well as the relationship graph information between entities.

The design decisions behind SiGMa were both to be able to take advantage of the combinatorial structure of the match-
Such as MusicBrainz , IMDb , DBLP , and UnitProt. http://linkeddata.org/ ing problem (in contrast to traditional scalable database record linkage approaches that make more independent de-cisions), as well as to focus on a simple approach that could be scalable. SiGMa works in two stages: it first starts with a small seed matching assumed to be of good quality. Then the algorithm incrementally augments the matching by us-ing both structural information and properties of entities such as their string representation to define a modular score function. Some key aspects of the algorithm are that (1) it uses the current matching to obtain structural information, thereby harnessing information from previous decisions; (2) it proposes candidate matches in a local manner, from the structural information; and (3) it makes greedy decisions, enabling a scalable implementation. A surprising result is that we obtained accurate large-scale matchings in our ex-periments despite the greediness of the algorithm.

Contributions. The contributions of the present work are the following: 1. We present SiGMa , a knowledge base alignment algo-2. In the context of testing the algorithm, we constructed 3. We provide a detailed experimental comparison illus-
The remainder of the paper is organized as follows. Sec-tion 2 presents the knowledge base alignment problem with a real-world example as motivation for our assumptions. We describe the algorithm SiGMa in Section 3. We evaluate it on benchmark and on real-world datasets in Section 4, and situate it in the context of related work in Section 5.
Consider merging the information in the following two knowledge bases: 1. YAGO [28 ], a large semantic knowledge base derived 2. IMDb , a large popular online database that stores in-The information in YAGO is available as a long list of triples (called facts ) that we formalize as  X  e,r,e 0  X  , which means
The code (in Python) and datasets can be downloaded from http://mlg.eng.cam.ac.uk/slacoste/sigma . that the directed relationship r holds from entity e to en-tity e 0 , such as  X  John Travolta , ActedIn , Grease  X  . The infor-mation from IMDb was originally available as several files that we merged into a similar list of triples. We call these two databases knowledge bases to emphasize that we are not assuming a richer representation like RDFS [ 29 ] (that dis-tinguishes between classes and instances). In the language of ontology matching, our setup is the less studied instance matching problem, discussed by Castano et al. [ 6]. Here, the goal is to match concrete instantiations of concepts like specific actors and movies rather than the general actor or movie class. YAGO comes with an RDFS representation, but IMDb does not; therefore, we focus on methods that do not assume or require a class structure or rich hierarchy in order to find a one-to-one matching of instances between YAGO and IMDb . However, we assume that the relations between the two knowledge bases can be manually aligned. This is straightforward for these types of knowledge bases (e.g. column 1 and 3 of Table 2).

Relationships vs. properties. In this work, we decided to exploit the powerful assumption that the alignment is in-jective (1 X 1)  X  as we will see in our experiments, this covers most of the cases for the YAGO -IMDb setup, as well as other datasets. Given our 1 X 1 assumption, we need to distinguish between two types of objects present in the list of triples: entities vs. literals . By our definition, the entities are the only objects that we try to align  X  they are objects like spe-cific actors or movies that have a clear identity. The literals , on the other hand, correspond to a value related to an entity through a special kind of relationship that we call property . The defining characteristic of literals is that it would not make sense to try to align them between the two knowl-edge bases in a 1 X 1 fashion  X  examples are numerical values, dates, strings, etc. We use the literals to define a similarity score between entities of the two knowledge bases. For exam-ple, the YAGO triple  X  m1 , wasCreatedOnDate , 1999-12-11  X  forms an entity-property-literal triple. Figure 1 provides a concrete example of information within the two knowledge bases that we will keep re-using in this paper. Table 2 gives the properties and relationships for our large-scale datasets. We now define formally the problem that we address.
Definition: A knowledge base KB is a tuple tities, literals, relationships, and properties (respectively); F R  X  X  X R X E is a set of relationship-facts whereas F P  X  E X P X L is a set of property-facts (both can be represented as a simple list of triples). To simplify the notation, we as-sume that all inverse relations are added in F R  X  that is, if  X  e,r,e 0  X  is in F R , we also have  X  e 0 ,r  X  1 ,e  X  in F
Problem: one-to-one alignment of instances be-tween two knowledge bases. Given two knowledge bases KB 1 and KB 2 as well as a partial mapping between their cor-responding relationships and properties, we want to output a 1 X 1 partial mapping m from E 1 to E 2 that represents the semantically equivalent entities in the two knowledge bases (by partial mapping, we mean that the domain of m does not have to be the whole of E 1 ).
Standard approaches for the ontology matching problem, such as RiMOM [20 ], could be used to align small knowledge
This allows us to cover all possibilities while only mention-ing one standard direction of facts (like in ( 3) for instance). Figure 1: Example of neighborhood to match in YAGO and IMDb . Even though entities i and j have no words in common, the fact that several of their respective neighbors are matched together is a strong signal that i and j should be matched together. This is a real example from the dataset used in the experiments and SiGMa was able to correctly match all these pairs ( i and j are actually the same movie despite their different stored titles in each KB). bases. However, they do not scale to millions of entities (as needed for our task) given that they usually consider all pairs of entities and thus suffers from a quadratic scaling cost. On the other hand, the related problem of identifying duplicate entities known as record linkage or duplicate de-tection in the database field, and co-reference resolution in the natural language processing field, do have scalable solu-tions using indexing techniques [9, 13 ]. However, these do not typically exploit the 1 X 1 matching combinatorial struc-ture present in our task, thereby reducing their accuracy. A notable exception is the work on collective entity resolu-tion by Bhattacharya and Getoor [3 ], solved using a greedy agglomerative clustering algorithm. The algorithm SiGMa that we present in Section 3 can actually be seen as an ef-ficient specialization of their work to the task of knowledge base alignment.

Another approach to alignment arises from the word align-ment problem in natural language processing [23 ], which has been formulated as a quadratic assignment problem [ 18 ]. This formulation encourages neighboring entities in one graph to align with neighboring entities in the other graph while exploiting the 1 X 1 matching structure. This enables align-ment decisions to depend on each other (see the caption of Figure 1 for an example of this in our setup). However, the quadratic assignment formulation [19 ], which can be solved as an integer linear program, is NP-hard in general, and these approaches were only used to align at most one hundred entities. In the algorithm SiGMa that we propose, we are interested in exploiting both the 1 X 1 matching con-straint, as well as building on previous decisions, like these word alignment approaches, but in a scalable manner which would handle millions of entities. SiGMa does this by greed-ily optimizing the quadratic assignment objective, as we de-scribe in Section 3.1 . Finally, Suchanek et al. [27 ] recently proposed an ontology matching approach called PARIS that they have succeeded to apply on the alignment of YAGO to IMDb as well. But the scalability of their approach is not as clear, as we explain in Section 5. We provide a detailed comparison with PARIS in the experiments section.
The SiGMa algorithm can be seen as the greedy opti-mization of an objective function that globally scores the suitability of a particular matching m for a pair of given KBs. This objective function uses two sources of informa-tion useful for choosing matches: a similarity function be-tween pairs of entities defined from their properties; and a graph neighborhood contribution making use of neighbor pairs being matched (see Figure 1 for a motivation). Let us encode the matching m : E 1  X  E 2 by a matrix y with entries indexed by the entities in each KB, with y if m ( i ) = j , meaning that i  X  E 1 is matched to j  X  E and y ij = 0 otherwise. The space of possible 1 X 1 partial mappings is thus represented by the following set of binary matrices: M . = { y  X  { 0 , 1 } E 1  X E 2 : P l y il  X  1  X  i  X  E P k y kj  X  1  X  j  X  E 2 } . We define the following quadratic objective function that globally scores the suitability of a matching y :
The objective contains linear coefficients s ij which encode a similarity between entity i and j , as well as quadratic co-efficients w ij,kl which control the algorithm X  X  tendency to match i with j given that k was matched to l 5 . N ij is a local neighborhood around ( i,j ) that we define later in (3 ) and which depends on the graph information from the KBs. g ( y ) is basically counting (in a weighted fashion) the num-ber of matched pairs ( k,l ) which are in the neighborhood of i and j .  X   X  [0 , 1] is a tradeoff parameter between the linear and quadratic contributions. Our approach is motivated by the maximization problem: where the norm k y k 1 . = P ij y ij represents the number of elements matched and R is an unknown upper-bound which represents the size of the best partial mapping which can be made from KB 1 to KB 2 . We note that if the coeffi-cients are all positive (as will be the case in our formulation  X  we are only encoding similarities and not repulsions be-tween entities), then the maximizer y  X  satisfies k y  X  k Problem ( 2) is thus related to one of the variations of the quadratic assignment problem, a well-known NP-complete problem in operations research [19 ]. Even though one could approximate the solution to ( 2) using a linear program relax-ation (see Lacoste-Julien et al. [ 18]), the number of variables is quadratic in the number of entities, so this is obviously not scalable. Our approach is instead to greedily optimize (2) by adding the match element y ij = 1 at each iteration which in-creases the objective the most and selected amongst a small set of possibilities. In other words, the high-level operational definition of the SiGMa algorithm is as follows: 1. Start with an initial good quality partial match y 0 .
In the rest of this paper, we use the convention that i and k are always entities in KB 1 ; whereas j and l are in KB could be in either KB. 2. At each iteration t , augment the previous matching 3. Stop when the bound k y k 1 = R is reached (and never
Having outlined the general framework, we now describe our choice of neighborhood N ij , candidate set S t , and stop-ping criterion R . We describe methods for choosing the similarity coefficients s ij and w ij,kl so that they guide the algorithm towards good matchings in Section 3.3 . These choices influence both the speed and accuracy of the algo-rithm.

Compatible-neighbors. N ij should be chosen so as to respect the graph structure defined by the KB facts. Its contribution in the objective crucially encodes the fact that a neighbor k of i being matched to a  X  X ompatible X  neighbor l of j should encourage i to be matched to j  X  see the caption of Figure 1 for an example. Here, compatibility means that they are related by the same relationship (they have the same color in Figure 1). Formally, we define:
N ij = compatible-neighbors ( i,j ) . = Note that a property of this neighborhood is that ( k,l )  X  X  iff ( i,j )  X  X  kl , as we have that the relationship r is matched to s iff r  X  1 is matched to s  X  1 as well. This means that the increase in the objective obtained by adding ( i,j ) to the current matching y defines the following context depen-dent similarity score function that is used to pick the next matched pair in the step 2 of the algorithm: score ( i,j ; y ) = (1  X   X  ) s ij +  X  X g ij ( y )
Information propagation on the graph. The compati-ble-neighbors concept that we just defined is one of the most crucial characteristics of SiGMa . It allows the informa-tion of a new matched pair to propagate amongst its neigh-bors. It also defines a powerful heuristic to suggest new can-didate pairs to include in a small set S t of matches to choose from: after matching i to j , SiGMa adds all the pairs ( k,l ) from compatible-neighbors ( i,j ) as new candidates. This yields a fire propagation analogy for the algorithm: start-ing from an initial matching (fire), the algorithm starts to match their neighbors, letting the fire propagate through the graph. If the graph in each KB is well-connected in a simi-lar fashion, it can visit most nodes this way. This heuristic enables SiGMa to avoid the potential quadratic number of pairs to consider by only focussing its attention on the neigh-borhoods of current matches. In the language of the record linkage literature [ 8], this amounts to an iterative blocking technique.

Stopping criterion. R is implicitly chosen by the fol-lowing heuristic motivated from standard optimization al-gorithms: SiGMa terminates when the variation in the ob-jective value, score ( i,j ; y ), of the latest added match ( i,j ) falls below a threshold (or the queue becomes empty). The threshold in effect controls the precision/recall tradeoff of the algorithm. By ensuring that the s ij and g ij ( y ) terms 1: Initialize matching m = m 0 . 2: Initialize priority queue S of suggested candidate pairs 3: while priority queue S is not empty do 4: Extract  X  score ,i,j  X  from queue S 5: if score  X  threshold then stop 6: if i or j is already matched to some entity then 7: skip them and continue loop 8: else 9: Set m ( i ) = j . 10: for ( k,l ) in N ij and not already matched do 11: Add  X  score ( k,l ; m ) ,k,l  X  to queue S .
 are normalized between 0 and 1, we can standardize the scale of the threshold for different score functions. We used a threshold of 0.25 in our experiments. It appeared to cor-relate well with a transition point at which the F-measure stops increasing and the precision starts to decrease signifi-cantly on a wide variety of datasets.
We present the pseudo-code for SiGMa in Table 1. We now elaborate on the algorithm design as well as its imple-mentation aspects. We note that the score defined in ( 4) to greedily select the next matched pair is composed of a static term s ij , which does not depend on the evolving matching y , and a dynamic term  X g ij ( y ), which depends on y , though only through the local neighborhood N ij . We call the  X g component of the score function the graph contribution; its local dependence means that it can be updated efficiently after a new match has been added. We describe the choice of similarity measures for these components in Section 3.3 .
Initial match structure m 0 . The algorithm can take any initial matching seed assumed of good quality. In our current implementation, this is done by looking for entities with the same string representation (with minimal standard-ization like removing capitalization and punctuation) with an unambiguous 1 X 1 match  X  that is, we do not include an exact matched pair when more than two entities have this same string representation, thereby increasing precision.
Optional static list of candidates S 0 . Optionally, we can initialize S with a static list S 0 which only needs to be scored once as any score update will come from neighbors already covered by step 11 of the algorithm. S 0 has the purpose to increase the possible exploration of the graph when another strong source of information (which is not from the graph) can be used, and corresponds to the stan-dard blocking (or indexing ) heuristics in record linkage [ 9]. In our implementation, we use an inverted index built on words to efficiently suggest entities which have at least two words in common in their string representation as potential candidates. More powerful indexing heuristics could also be used.

Data-structures. We use a binary heap for the priority queue implementation, allowing insertions in O (log n ) where n is the size of the queue. Because the score function can only increase as we add new matches, we do not need to keep track of stale nodes in the priority queue in order to update their scores, yielding a significant speed-up.
An important factor for any matching algorithm is the similarity function between pairs of elements to match. De-signing good similarity functions has been the focus of much of the literature on record linkage, entity resolution, etc., and because SiGMa uses the score function in a modular fashion, SiGMa is free to use most of them for the term s ij as long as they can be computed efficiently. We provide in this sec-tion our implementation choices (which were motivated by simplicity), but we note that the algorithm can easily han-dle more powerful similarity measures. The generic score function used by SiGMa was given in (4). In the current implementation, the static part s ij is defined through the properties of entities only. The graph part  X g ij ( y ) depends on the relationships between entities (as this is what deter-mines the graph), as well as the previous matching y . We also make sure that s ij and g ij stay normalized so that the score of different pairs are on the same scale.
The static property similarity measure is further decom-posed in two parts: we single out a contribution coming from the string representation property of entities (as it is such a strong signal for our datasets), and we consider the other properties together in a second term: where  X   X  [0 , 1] is a tradeoff coefficient between the two contributions set to 0.25 during the experiments.

String similarity measure. For the string similarity measure, we primarily consider the number of words that two strings have in common, albeit weighted by their in-formation content. In order to handle the varying lengths of strings, we use the Jaccard similarity coefficient between the sets of words, with a smoothing term. To capture the information that some words are more informative than oth-ers, we use the IDF (inverse-document-frequency) weight for each word in a weighted Jaccard measure, a commonly used feature in information retrieval. The weight for word v in KB o is w o v . = log 10 |E o | / | E o v | , where E o word v in its string representation } . Combining these ele-ments, we get the following string similarity measure: string ( i,j ) = where W e is the set of words in the string representation of entity e and smoothing is the scalar smoothing constant (we try different values in the experiments). While this measure is robust to word re-ordering, it is not robust to variations of spelling for words. This problem could be addressed by using more involved string similarity measures as described in [10 ], though our current implementation only uses (6 ) for simplicity. We also explore the effect of different scoring functions in our experiments in Section 4.5 .

Property similarity measure. We recall that we as-sume that the user provides a partial matching between properties of both databases. This enables us to use them in a property similarity measure. In order to elegantly handle missing values of properties, a varying number of property values present, etc., we also use a smoothed weighted Jac-card similarity measure between the sets of properties. The detailed formulation is given in Appendix A of the longer technical report [17 ] for completeness, but we note that it can make use of a similarity measure between literals like a normalized distance on numbers (for dates, years, and so on) or an edit distance on strings.
We now introduce the part of the score function which en-ables SiGMa to build on previous decisions and exploit the relationship graph information. We need to determine w ij,kl the weight of the contribution of a neighboring matched pair ( k,l ) for the score of the candidate pair ( i,j ). The gen-eral idea of the graph score function is to count the num-ber of compatible neighbors which are currently matched together for a pair of candidates (this is the g ij ( y ) contribu-tion in ( 1)). Going back to the example in Figure 1, there were three compatible matched pairs shown in the neighbor-hood of i and j . We would like to normalize this count by dividing by the number of possible neighbors, and we may want to weight each neighbor differently. We again use a smoothed weighted Jaccard measure to summarize this in-formation, averaging the contribution from each KB. This can be obtained by defining w ij,kl =  X  i w ik +  X  j w jl  X  i and  X  j are normalization factors specific to i and j in each database and w ik is the weight of the contribution of k to i in KB 1 (and similarly for w jl in KB 2 ). The graph contribution thus becomes: Let N i be the set of neighbors of entity i in KB 1 , i.e. N { k :  X  r s.t. ( i,r,k )  X  F R 1 } (and similarly for N remembering that P k y kl  X  1 for a valid partial match-ing y  X  M , the following normalizations  X  i and  X  j yield the average of two smoothed weighted Jaccard measures for g ( y ):  X  i . = 1 We thus have g ij ( y )  X  1 for y  X  X  , keeping the contribution of each possible matched pair ( i,j ) on the same scale in obj in (1 ).

The graph part of the score in ( 4) then takes the form:  X g ij ( y ) = X The summation over the first two terms yields g ij ( y ) and so is bounded by 1, but the summation over the last two terms could be greater than 1 in the case that ( i,j ) is filling a  X  X ole X  in the graph (thus increasing the contribution of many neighbors ( k,l ) in obj in ( 1)). Finally, we use unit weight for w ik . See Section 4.5 and Appendix B of the longer technical report [17 ] for alternatives (which did not perform better in experiments).
We made a prototype implementation of SiGMa in Python 6 and compared its performance on benchmark datasets as
The code and datasets can be downloaded from http://mlg.eng.cam.ac.uk/slacoste/sigma . well as on large-scale knowledge bases. All experiments were run on a cluster node Hexacore Intel Xeon E5650 2.66GHz with 46GB of RAM running Linux. Each knowledge base is represented as two text files containing a list of triples of relationships-facts and property-facts. The input to SiGMa is a pair of such KBs as well as a partial mapping between the relationships and properties of each KB which is used in the computation of the score in (4), and the definition of compatible-neighbors (3). The output of SiGMa is a list of matched pairs ( e 1 ,e 2 ) with their score information and the iteration number at which they were added to the so-lution. We evaluate the final alignment (after reaching the stopping threshold) by comparing it to a ground truth us-ing the standard metrics of precision, recall and F-measure on the number of correctly matched entities in the first KB with ground truth information . The benchmark datasets are available together with corresponding ground truth data; for the large-scale knowledge bases, we built their ground truth using web URL information as described in Section 4.2 .
We found reasonable values for the parameters of SiGMa by exploring its performance on the YAGO to IMDb pair (the methodology is described in Section 4.5 ), and then kept them fixed for all the other experimental comparisons (Sec-tion 4.3 and 4.4 ). This reflects the situation where one would like to apply SiGMa to a new dataset without ground truth or to minimize parameter adaptation. The standard pa-rameters that we used in these experiments are given in Appendix D of [17 ] for reproducibility.
Our experiments were done both on several large-scale datasets and on some standard benchmark datasets from the ontology alignment evaluation initiative (OAEI) (Table 3). We describe these datasets below.

Large-scale datasets. As mentioned throughout this paper so far, we used the dataset pair YAGO -IMDb as the main motivating example for developing and testing SiGMa . We also test SiGMa on the pair Freebase -IMDb , for which we could obtain a sizable ground truth. We describe here their construction. Both YAGO and Freebase are available as lists of triples from their respective websites. 7 IMDb , on the other hand, is given as a list of text files. different files for different categories, e.g.: actors, produc-ers, etc. We use these categories to construct a list of triples containing facts about movies and people. Because SiGMa ignores relationships and properties that are not matched between the KBs, we could reduce the size of YAGO and Freebase by keeping only those facts for which their rela-tionship had a 1 X 1 mapping with IMDb as presented in Ta-ble 2, and the entities appearing in these facts. To facilitate the comparison of SiGMa with PARIS , the authors of PARIS kindly provided us their own version of IMDb that we re-fer to as IMDb PARIS  X  this version actually has a richer structure in terms of properties. We also kept in YAGO the relationships and properties that were aligned with those of IMDb PARIS (Table 2). Table 3 presents the number of unique entities and relationship-facts included in the rele-vant reduced datasets. We constructed the ground truth for YAGO -IMDb by scraping the relevant Wikipedia pages of entities to extract their link to the corresponding IMDb
YAGO2 core was downloaded from: http://www.mpi-inf.mpg.de/yago-naga/yago/downloads.html and Freebase from: http://wiki.freebase.com/wiki/Data dumps . http://www.imdb.com/interfaces#plain Table 2: Manually aligned movie related relation-ships and properties in large-scale KBs.
 page, which often appears in the  X  X xternal links X  section. We then obtained the entity name by scraping the correspond-ing IMDb page and matched it to our constructed database by using string matching (and some manual cleaning). We obtained 54k ground truth pairs this way. We used a simi-lar process for Freebase -IMDb by accessing the IMDb URLs which were actually stored in the database. This yielded 293k pairs, probably one of the largest knowledge base align-ment ground truth sets to date.

Benchmark datasets. We also tested SiGMa on three benchmark dataset pairs provided by the ontology align-ment evaluation initiative (OAEI), which allowed us to com-pare the performance of SiGMa to some previously published methods [ 20 , 14 ]. From the OAEI 2009 edition, 9 we use the Rexa -DBLP instance matching benchmark from the domain of scientific publications. Rexa contains publications and authors as entities extracted from the search results of the Rexa search server. DBLP is a version of the DBLP dataset listing publications from the computer science domain. The pair has one matched relationship, author , as well several matched properties such as year , volume , journal name , pages , etc. Our goal was to align publications and authors. The other two datasets come from the Person-Restaurants (PR) task from the OAEI 2010 edition, 10 containing data about people and restaurants. In particular, there are per-son11 -person12 pairs where the second entity is a copy of the first with one property field corrupted, and restaurant1 -restaurants2 pairs coming from two different online databases that were manually aligned. All datasets were downloaded from the corresponding OAEI webpages, with dataset sizes given in Table 3.
In this experiment, we test the performance of SiGMa on the three pairs of large-scale KBs and compare it with PARIS [27], which is described in more details in the re-h ttp://oaei.ontologymatching.org/2009/instances/ http://oaei.ontologymatching.org/2010/im/index.html Table 4: Exp. 1: Results (precision, recall, F-measure) on large-scale datasets for SiGMa in com-parison to a simple exact-matching phase on strings as well as PARIS [27]. The  X  X T Size X  column gives the number entities with ground truth information. Time is total running time, including loading the dataset (quoted from [27 ] for PARIS ). lated work Section 5. We also compare SiGMa and PARIS with the simple baseline of doing the unambiguous exact string matching step described in Section 3.2 which is used to obtain an initial match m 0 (called Exact-string ). Table 4 presents the results. Despite its simple greedy nature which never goes back to correct a mistake, SiGMa obtains an im-pressive F-measure above 90% for all datasets, significantly improving over the Exact-string baseline. We tried running PARIS [27] on a smaller subset of YAGO -IMDb , using the code available from its author X  X  website. It did not com-plete its first iteration after a week of computation and so we halted it (we did not have the SSD drive which seems crucial to reasonable running times). The results for PARIS in Ta-ble 4 are thus computed using the prediction files provided to us by its authors on the YAGO -IMDb PARIS dataset. In or-der to better relate the YAGO -IMDb PARIS results with the YAGO -IMDb ones, we also constructed a larger ground truth reference on YAGO -IMDb PARIS by using the same process described in Section 4.2 . On both ground truth evaluations, SiGMa obtains a similar F-measure to PARIS , but in 50x less time. On the other hand, we note that PARIS is solving the more general problem of instance and schema alignment, and was not provided any manual alignment between rela-tionships. The large difference of recall between PARIS and SiGMa on the ground truth from [ 27] can be explained by the fact that more than a third of its entities had no neigh-bor, whereas the process used to construct the new larger ground truth included only entities participating in movie facts and thus having at least one neighbor. The recall of SiGMa actually increases for entities with increasing num-ber of neighbors (going from 70% for entities in the ground truth from [ 27 ] with 0 neighbors to 95% for entities with 4+ neighbors).
 About 2% of the predicted matched pairs from SiGMa on YAGO -IMDb have no word in common and thus zero string similarity  X  difficult pairs to match without any graph in-formation. Examples of these pairs came from spelling vari-ations of names, movie titles in different languages, foreign characters in names which are not handled uniformly, or multiple titles for movies (such as the  X  X lood In, Blood Out X  example of Figure 1).

Error analysis. Examining the few errors made by SiGMa , we observed the following types of matching errors: 1) errors in the ground truth (either coming from the scraping scheme used; or from Wikipedia ( YAGO ) which had incorrect infor-mation); 2) errors with multiple very similar entities (e.g. mistaking the  X  X aking of X  the movie vs. the movie itself); 3) errors with pairs of entities that shared exactly the same D ataset System Prec Rec F GT size P erson SiGMa 100 100 100 500
Re staurant SiGMa -linear 100 100 100
R exa -DBLP SiGMa 99 94 96 Table 5: Exp. 2: Results on the benchmark datasets for SiGMa , compared with PARIS [27] and RiMOM [20]. SiGMa-linear and Exact-string are also included on the interesting datasets as further com-parison points. neighbors (e.g. two different movies with exactly the same actors) but without other discriminating information. Fi-nally, we note that going through the predictions of SiGMa that had a low property score revealed a significant num-ber of errors in the databases (e.g. wildly inconsistent birth dates for people), indicating that SiGMa could be used to highlight data inconsistencies between databases.
In this experiment, we test the performance of SiGMa on the three benchmark datasets and compare them with the best published results so far that we are aware of: PARIS [27] for the Person-Restaurants datasets (which compared favor-ably over ObjectCoref [ 14 ]); and RiMoM [20] for Rexa-DBPL . Table 5 presents the results. We also include the results for Exact-string as a simple baseline as well as SiGMa-linear , which is the SiGMa algorithm without using the graph infor-mation at all, 11 to give an idea of how important the graph information is in these cases.

Interestingly, SiGMa significantly improved the previous results without needing any parameter tweaking. The Person-Restaurants datasets did not have a rich relationship struc-ture to exploit: each entity (a person or a restaurant) was linked to exactly one another in a 1 X 1 bipartite fashion (their address). This is perhaps why SiGMa-linear is surprisingly able to perfectly match both these datasets.

For the Rexa-DBLP dataset, we note that the organizers of the OAEI 2009 edition built the augmented ground truth set that we have used by inspection of the predictions of the sub-mitted systems (and probably by using exact string match-ing heuristics as well). SiGMa discovered about a thousand new matches that were not present in this ground truth but that appeared to be mostly correct. The recall for Exact-string and RiMOM is thus probably overestimated compared to the one of SiGMa . This benchmark which has a medium size also highlights the nice scalability of SiGMa : despite using the interpreted language Python, our implementation runs in less than 10 minutes on this dataset. By comparison, RiMOM took 36 hours on a 8-core server in 2009 [ 20].
In this section, we explore the role of different configura-tions for SiGMa on the YAGO -IMDb pair, as well as deter-
SiGMa-linear is not using the graph score component (  X  is set to 0) and is not using the neighbors in N ij to suggest candidates (it only uses the inverted index S 0 ). precision Figure 2: Exp. 3: Precision/recall curves for SiGMa on YAGO -IMDb with different scoring configurations. The filled circles indicate the maximum F-measure position on each curve, with the corresponding diamond giving the F-measure value on the y-axis at this recall point. mine which parameters to use for the other experiments. We recall that SiGMa with the final parameters yields a 95% F-measure on this dataset (second section of Table 4). Experi-ments 5 and 6, which explore the optimal weighting schemes as well as the correct stopping threshold, are described for completeness in Appendix E of [17 ].
In this experiment, we explore the importance of each part of the score function by running SiGMa with some parts turned off (this can be done by setting the  X  and  X  tradeoffs to 0 or 1). The resulting precision/recall curves are plot-ted in Figure 2. By comparing SiGMa with SiGMa-linear , we see that including the graph information moves the F-measure from a bit below 85% to over 95%, a significant gain, indicating that the graph structure is more important on this challenging dataset than the easier OAEI benchmark datasets and it was crucial to exploit it.
In this experiment, we test how important the size of the matching seed m 0 is for the performance of SiGMa . We report the following notable results. We ran SiGMa with no exact seed matching at all: we initialized it with a ran-dom exact match pair and let it explore the graph greedily (with the inverted index still making suggestions). This ob-tained an even better score than the standard setup: 99% of precision, 94% recall and 96% F-measure, demonstrating that a good initial seed is actually not needed for this setup , and illustrating the power of the graph information for this dataset. We observed a similar improvement for the Free-base and YAGO -IMDb PARIS datasets (with the new ground truth).
We contrast here SiGMa with the work already mentioned in Section 2.2 and provide further links. In the ontology matching literature, the only approach which was applied to datasets of the size that we considered in this paper is the recently proposed PARIS [27 ], which solves the more general problem of matching instances, relationships and classes. The PARIS framework defines a normalized score between pairs of instances that represents how likely they are to be matched, 12 and that depends on the matching scores of their compatible neighbors. The final scores are obtained by first initializing (and fixing) the scores on pairs of literals, and then propagating the updates through the relationship graph using a fixed point iteration, yielding an analogous fire propagation of information as SiGMa , though it works with soft [0-1]-valued assignments whereas SiGMa works with hard { 0,1 } -valued ones. The authors handle the scalability issue of maintaining scores for all pairs by using a sparse representation with various pruning heuristics (in particular, keeping only the maximal assignment for each entity at each step, thus making the same 1 X 1 assumption that we did ). An advantage of PARIS over SiGMa is that it is able to include property values in its neighborhood graph (it uses soft-assignments between them) whereas SiGMa only uses relationships given that a 1 X 1 matching of property values is not appropriate. We conjecture that this could ex-plain the higher recall that PARIS obtained on entities that had no relationship neighbors on the YAGO -PARIS IMDB dataset. On the other hand, PARIS was limited to use a 0-1 similarity measure between property values for the large-scale experiments in [ 27 ], as it is unclear how one could apply the same sparsity optimization in a scalable fashion with more involved similarity measures (such as the IDF one that SiGMa is using). The use of a 0-1 similarity measure on strings could explain the lower performance of PARIS on the Restaurants dataset in comparison to SiGMa . We stress that SiGMa is able in contrast to use sophisticated similarity measures in a scalable fashion, had a 50x speed improvement over PARIS on the large-scale datasets, and yields a signifi-cantly simpler implementation.

The SiGMa algorithm is related to the collective entity res-olution approach of Bhattacharya and Getoor [3 ], who pro-posed a greedy agglomerative clustering algorithm to cluster entities based on previous decisions. Their approach could handle constraints on the clustering, including a 1 X 1 match-ing constraint in theory, though it was not implemented. We think a contribution of our work is to demonstrate the effec-tiveness of using the 1 X 1 matching constraint for knowledge base alignment. Some scalable solutions for collective entity resolution were proposed recently [ 1, 25 ], though they did not implement a 1 X 1 matching constraint, and their imple-mentation can be a complex software engineering endeavor in contrast to the simplicity of our approach.

The idea to propagate information on a relationship graph has been used in several other approaches for ontology match-ing [ 15 , 21 ], though none were scalable for the size of knowl-edge bases that we considered. An analogous  X  X ire propaga-tion X  algorithm has been used to align social network graphs in [ 22 ], though with a very different objective function (they define weights in each graph and want to align edges which
The authors call these  X  X arginal probabilities X  as they were motivated from probabilistic arguments, but these do not sum to one. have similar weights). The heuristic of propagating infor-mation on a relationship graph is related to a well-known heuristic for solving constraint satisfaction problems known as constraint propagation [2 ]. Ehrig and Staab [11 ] men-tioned several heuristics to reduce the number of candidates to consider in ontology alignment, including a similar one to compatible-neighbors , though they tested their approach only on a few hundred instances. Finally, we mention that Peralta [ 24] aligned the movie database MovieLens to IMDb through a combination of steps of manual cleaning with some automation. SiGMa could be considered as an alternative which does not require manual intervention apart from spec-ifying the score function to use.
 B  X  ohm et al. have recently proposed a similar algorithm to SiGMA called LINDA [5 ], independently to our prior techni-cal report [17 ]. LINDA is also an iterative greedy algorithm for a quadratic assignment objective applied to instance matching, but with slightly different scoring functions. We think that their work complements ours: they have demon-strated how to scale a SiGMa -like algorithm to billions of triples using the MapReduce framework. On the other hand, we provide a significantly simpler implementation for single machines with experiments demonstrating that higher ac-curacy can be achieved (they reported only 80% F-measure on the Restaurants dataset, as well as 66% sampled preci-sion for their large-scale experiments, whereas the sampled precision we measured was actually always above 90%).
We have presented SiGMa , a simple and scalable algo-rithm for the alignment of large-scale knowledge bases. De-spite making greedy decisions and never backtracking to cor-rect decisions, SiGMa obtains a higher F-measure than the previously best published results on the OAEI benchmark datasets, and matches the performance of the more involved algorithm PARIS while being 50x faster on large-scale knowl-edge bases of millions of entities. Our experiments indicate that SiGMa can obtain good performance over a range of datasets with the same parameter setting. On the other hand, SiGMa is easily extensible to more powerful scoring functions between entities, as long as they can be efficiently computed.

Some apparent limitations of SiGMa are that it a) can-not correct previous mistakes and b) cannot handle align-ments other than 1 X 1. Addressing these in a scalable fash-ion which preserves high accuracy are open questions for future work. We note though that the non-corrective nature of the algorithm didn X  X  seem to be an issue in our experi-ments. Moreover, pre-processing each knowledge base with a de-duplication method can help make the 1 X 1 assumption, which is a powerful feature to exploit in an alignment al-gorithm, more reasonable. Another interesting direction for future work would be to use machine learning methods to learn the parameters of more powerful scoring function. In particular, the  X  X earning to rank X  model seems suitable to learn a score function which would rank the correctly la-beled matched pairs above the other ones. The current level of performance of SiGMa already makes it suitable though as a powerful generic alignment tool for knowledge bases and hence takes us closer to the vision of Linked Open Data and the Semantic Web.

Acknowledgments: We thank Fabian Suchanek and Pier-re Senellart for sharing their code and answering our ques-tions about PARIS . We thank Guillaume Obozinski for help-ful discussions. This research was supported by a grant from Microsoft Research Ltd. and a Research in Paris fellowship.
