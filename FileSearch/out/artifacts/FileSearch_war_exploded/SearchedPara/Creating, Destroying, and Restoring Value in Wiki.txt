 Wikipedia X  X  brilliance and curse is that any user can edit any of the encyclopedia entries. We introduce the notion of the impact of an edit, measured by the number of times the edited version is viewed. Using several datasets, including recent logs of all article views, we show that an overwhelm-ing majority of the viewed words were written by frequent editors and that this majority is increasing. Similarly, using the same impact measure, we show that the probability of a typical article view being damaged is small but increasing, and we present empirically grounded classes of damage. Fi-nally, we make policy recommendations for Wikipedia and other wikis in light of these findings.
 H.5.3 [ Group and Organization Interfaces ]: Computer-supported cooperative work, web-based interaction Human factors Wiki, Wikipedia, collaboration, vandalism, damage
Wikipedia is the great success story of collective action on the Web. It is also a source of wonder: its essential idea  X  that a useful encyclopedia can be created by allowing anyone to create and edit articles  X  seems absurd. Some people are ignorant, some are malicious, and some are just bad writers. Yet, Wikipedia seems to work. As of this writing, it contains nearly two million articles and ranks among the top ten most visited sites on the Web. Further, some research has found that the accuracy of Wikipedia and Encyclopaedia Britannica are roughly equivalent [9].

How does Wikipedia work? As a wiki, every one of its ar-ticles can be edited by anyone  X  there is no credential check-ing. Changes are visible to everyone immediately, without any review cycle. However, there is intense, ongoing review of articles. Wikipedia has attracted a community of deeply commited editors: for example, Kittur et al. found that by mid-2006 there wer e hundreds of users who had made over 10,000 edits and well over 10,000 editors who had made more than 100 edits [13]. Editing is made easier by various mech-anisms: a recent changes page and IRC channel show every edit made to every article, watch lists help users monitor articles they care about, and version histories help editors quickly roll back objectionable changes.

That Wikipedia works may be amazing, but how it works is a research question, one that previous work has addressed in various forms. We build on prior work by developing a new approach to estimating the value of Wikipedia, based on how many people are affected by a change to an article. We pose three specific research questions: 1. Creating value : Who contributes Wikipedia X  X  value? 2. Impact of damage : What is the impact of damage 3. Types of damage : What types of damage occur, and
Regardless of the value of Wikipedia itself, these questions matter, as do our results. Many other popular online com-munities produce artifacts of lasting value [7] through the collective action of their users. Sites such as slashdot.org , reddit.com ,and digg.com all rely on user opinions to filter and order the vast volume of online news stories. Users of freedb.org (CDs) and imdb.com (movies) have created large databases that provide value to anyone interested in those topics. These examples, and others, show that the issues of who creates value, and the types and impacts of damaging behavior, are of general interest.

The rest of the paper is organized as follows. First, we survey related work, showing how ours builds upon and ex-tends it. Most notably, our work is the first to compute value of edits and the impact of damage in terms of how many user views they receive. Estimating this is hard, so we detail how it is done. We then describe the methods and present the results for each of our three research questions and close with a brief summary.
The past few decades have seen the emergence of nu-merous Internet-based social media, including email lists, Usenet, MUDs and MOOs, chat, blogs, wikis, and social networking systems. Researchers have taken advantage of the opportunities created by the large user bases these me-dia have attracted, creating a wide variety of advanced soft-ware (e.g., visualization and analysis tools [19, 16], social agents [11], and social navigation aids [23]) and conducting a broad range of empirical research (e.g., on conversational patterns [3], social interaction [5], gender [10], and group-wide patterns [24]).
The areas we address  X  who contributes value, and how does a community maintain itself against antisocial behavior  X  have been widely researched in different social media.
Creating Value . It has been observed widely that a small minority of participants in an online group produce most of the content, while the vast majority of users produce little or no content. The distribution of activity typically takes the form of a power law . Whittaker et al. [24] observed this for Usenet postings and Marks [15] for blog links.
Antisocial behavior (damage) . Much work has been de-voted to characterizing, detecting, and managing behavior like flaming (personal attacks) and spam (irrelevant content designed for financial or other gain). For example, Slash-dot uses a socially-based moderation system that is effective in making it easy to ignore uninteresting or offensive con-tent [14]. More generally, Cosley et al. [7] presented a model and empirical results suggesting that a policy of reviewing content after it is  X  X ublished X  (as in Wikipedia) eventually results in quality equivalent to that obtained under a pre-review policy (as in traditional peer-reviewed journals).
As Wikipedia has grown, it has attracted research on a variety of issues, including predicting article quality [17], comparing article topics to those in traditional encyclope-dias [8], contrasting participation in Wikipedia and in open source software projects [17], and analyzing the development of contributors from an activity-theoretic perspective [4]. The issue of who (i.e., what types of editors) contributes Wikipedia X  X  content is a matter of some dispute. Jimmy Wales, one of the founders of Wikipedia, has stated that  X 2% of the users do 75% of the work X  [22], while Swartz [18] has argued that the work is more distributed. Voss [21] provided data on this question by counting number of edits: unsurprisingly, the data showed a power law distribution.
Kittur et al. [13] analyzed the amount of content con-tributed by different classes of editors, finding that elite users (10,000 or more edits) accounted for over 50% of edits in 2002 but only 20% by mid-2006, due to increasing par-ticipation by users with less than 100 edits. Furthermore, Kittur and his colleagues explored whether elite and low-edit groups accounted for different amounts of content. By measuring the total number of words added and deleted in edits, they found that elite editors accounted for a higher proportion of content changes (around 30%) and were more likely than low-edit users to add (rather than delete) words. Adler and Alfaro [1] developed a reputation system for Wikipedia editors. Their reputation metric futher developed the notion of measuring editors X  value contributions by ac-counting for whether changes introduced by an edit persisted over time. Defining short-lived edits and short-lived text as changes that were at least 80% undone (by edit distance) within a few subsequent edits, they showed that these met-rics could be used to compute reputations for authors, and that these reputations predicted well whether an author X  X  text would persist.

We share the concern of these efforts to understand who produces Wikipedia X  X  valuable content. Kittur et al. took an important step by examining content within edits, and Adler and Alfaro took another by considering whether an edit X  X  changes persist. Our work, however, produces two significant advances.
 First, we use a more general n otion of persistence than Adler and Alfaro, measuring how words persist over time rather than just detecting short-lived changes. Second, we compute how much each word is viewed over time. There is no real value in content that no one views, even if there is a lot of it; conversely, content that is viewed frequently has high value, regardless of how much of it there is. Thus, our metric matches the notion of the value of content in Wikipedia better than previous metrics.
Over the past few years, the issue of vandalism in Wik-ipedia, e.g. deliberate and malicious editing of a destruc-tive nature, has received much attention. There have been a number of high-profile cases of vandalism in Wikipedia. For example, Adam Curry allegedly altered pages about the history of podcasting in order to promote his role and diminish that of others [30], Jeffrey Seigenthaler X  X  article stated falsely for months that he was involved in the John F. Kennedy assassination [31], and the comedian Stephen Col-bert has even conducted humorous tutorials on how to van-dalize Wikipedia [33].

In 2004, Vi  X  egas et al. published a seminal paper [20] that popularized the study of Wikipedia in the HCI and CSCW fields, introducing a novel visualization technique for explor-ing the history of edits to Wikipedia articles. Most vividly, they used this to identify cases of conflict and vandalism. Focusing on one particular class of vandalism, mass delete , where all or nearly all of an article X  X  content is deleted, they found that this vandalism was repaired in a median time of 2.8 minutes.

In more recent work, Kittur et al. [12] investigated the occurrence of the key mechanism for repairing vandalism, the revert, and found that its use is growing.

Our work significantly extends that of Vi  X  egas. First, we systematically categorize types of damage in Wikipedia and provide estimates on how common they are. Second, build-ing on Kittur et al. X  X  use of reverts, we propose and evaluate a new vandalism-detecting metric and use it to analyze three orders of magnitude more instances of vandalism than Vi  X  e-gas. Third, we analyze not only how long articles remained in a damaged state, but also how many times they were viewed while in this state. Finally, we use a larger, cur-rent, and comprehensive corpus: nearly three more years of Figure 1: Datasets used in this paper. We consider the period September 1, 2002 to October 31, 2006. Wikipedia article history (ten times as many revisions) plus additional datasets to estimate the number of views each revision of an article received. Thus, we can better approx-imate the impact of damage on readers.
We define a few critical terms as follows. The act of mak-ing and saving changes to an article is an edit ,andthehis-tory of an article forms a sequence of content states called revisions  X  i.e., edits are transitions between revisions. Fur-ther, there is a special kind of edit, called a revert : revert-ing an article means restoring its content to some previous revision, removing the effects of intervening edits.
We measure the value of contributions or the impact of damage in terms of number of views .Importantly,this information is required for every point in time during the period of Wikipedia X  X  history under study. It X  X  not enough to know how many views an article receives  X  X ow X . Instead, we need to know how many views it received (for example) between 9:17 and 9:52 on July 8, 2005  X  perhaps because it was in a damaged state.

There is a reason past analyses haven X  X  used view data: it is not available, because the relevant logs no longer ex-ist. However, we have access to several datasets that let us estimate view data. 1
A word of caution . We assume that one serving of an article by a Wikipedia server is a reasonable proxy for one view of that article by a user. While a human may request an article but not read all of it, or web caching schemes may cause one Wikipedia serving of an article to correspond to many user views of that article, we believe that these factors do not materially affect the accuracy of our view estimates, in particular since we are most interested in comparisons between articles. We use five datasets in this work, illustrated in Figure 1. The first four are used to estimate article views, while the fifth is used to estimate the secondary metrics that Sections 4 and 5 focus on. It is included here for completeness. 1. Total number of articles in Wikipedia over time [32]
Our estimation tool is available online at http://www.cs. umn.edu/  X reid/views.tar.bz2 . 2. Total views per month is also provided by Wik-3. Alexa views per million over time is compiled by 4. Request logs from Wikipedia server activity. The 5. History of articles . Wikipedia provides a historical
A tempting proxy for article views is article edits. How-ever, we found essentially no correlation between views and edits in the request logs. Therefore, we must turn to a more elaborate estimation procedure.
We need to compute the number of article views during a specific interval  X  how many times was article X viewed during the interval t 1 to t 2 ? We do this by computing the article X  X  view rate  X  e.g., how many views per day did arti-cle X have at time t  X  from which it is straightforward to compute views. Specifically, we compute: where r ( X,t ) is the view rate of article X at time t , is the view rate of Wikipedia as a whole at time t (i.e., R ( t )= of articles in Wikipedia at time t .

Intuitively, to compute the historical view rate of an ar-ticle, we take its current view rate, shrink it to compensate for shrinkage of Wikipedia X  X  view rate as a whole, and then expand it to compensate for the smaller number of articles. This computation is based on the assumption that the view rates of articles, relative to each other, are fairly stable.
The values of the five terms in the above formula are them-selves computed as follows. r ( X, now) and R (now) are 1.5% of the log data were lost between Wikimedia and us due to dropped packets and interruptions of our collector process. Our estimates compensate for this small loss. average values from the request logs (data set 4).  X  X ow X  is the center of the request log data period, i.e. April 24, 2007, while Z ( t ) and Z (now) are interpolated from the total number of articles (data set 1).

R ( t ) is the most complex to compute. If t falls within the period covered by the total views-per-month data from Wikipedia (data set 2), it is interpolated from those data. If not, we interpolate it from scaled Alexa data (data set 3). Intuitively, what we are after is a scaling of the Alexa data so that it matches well both the old views-per-month data and the new R (now) computed from request logs. This is done by taking the linear regressions of the log of the Alexa data and the log of the views-per-month data during the period of overlap, then scaling up the Alexa data until the linear regressions intersect at the center of this period. This also results in a close match between the scaled Alexa data and R (now): 97 and 104 million views per day, respectively. This scaled Alexa data is what we use to interpolate R ( t
If an article had aliases at time t , r ( a,t ) is computed for each alias a as well, and the final view rate is the sum of the view rates of the article and each of its aliases. Historical view rates for articles or aliases which no longer exist will be zero, but we believe this does not materially affect our results because few articles and aliases are deleted, and those that are are obscure.

For example, suppose that we wish to calculate the num-ber of views of article Y , which has no aliases, from June 9 to June 14, 2004. Suppose that r ( Y, now) is 1,000 views per day, and recall that R (now) is 104 million views per day. We first compute the views per day of article Y on June 12, the center of the period. 1. Z (now) = 1,752,524, interpolated from the values for 2. Z ( June 12, 2004 ) = 284,240, interpolated from the 3. R ( June 12, 2004 ) = 5,019,355, interpolated from the Applying the formula, r ( Y, August 12, 2004 ) = 298. Be-cause the period is six days long, we estimate the number of views as six times this value, or 1,785 views. The calcula-tion would proceed similarly for the period November 9 to November 14, 2004, except R ( November 12, 2004 )would be interpolated from scaled Alexa data, because the last Wikipedia views-per-month datum is for October 15, 2004. 4
As a proxy for the value contributed by an edit, we use the persistent word view (PWV), the number of times any given word introduced by an edit is viewed. PWV builds on the notion of an article view: each time an article is viewed,
The time period between Wikipedia X  X  founding and the be-ginning of the views-per-month data is excluded from anal-ysis in the present work.
In actuality, these computations are done in floating-point seconds, but we simplify here for clarity. each of its words is also viewed. When a word written by editor X is viewed, he or she is credited with one PWV.
Two key insights drive this metric. First, authors who write content that is read often are empirically providing value to the community. Second, if a contribution is viewed many times without being changed or deleted, it is likely to be a valuable. Of course, this metric is not perfect: the concept of value is dependent on the needs and mental state of the reader. One might imagine a single fact, expressed in only a few words, that provides enormous value to the one reader who really needs that fact. In a large pseudonymous reading community like Wikipedia, capturing a notion of value that depends on the specific information needs of the readersisoutsidethescopeofthiswork.

For example, see Figure 2. Assuming that each page is viewed 100 times after each edit, Carol has accrued at least 1,200 PWVs: 400 from bravo (because she wrote it and it was present for 4 edits), 300 from charlie , and 500 from delta (eventhoughitwasmovedseveraltimes).
 The case of alpha is problematic because it is ambiguous. When Bob deleted an alpha , whose did he delete: Carol X  X  or Denise X  X ? Words carry no identifier, so it is impossible to tell for certain without understanding the text of the two edits. In cases of ambiguity, we choose randomly. Carol could have 1,300 or 1,700 PWVs depending on whether or not her alpha was chosen. Over the trillions of PWVs analyzed, these random choices have little effect on the results.
PWVs are calculated per-article, and the final score for each editor is the sum of his or her scores over all articles. The  X  X wner X  of each PWV is determined by comparing the text of subsequent article edits, data contained in the history of articles (data set 5 above); specifically, we: 1. Remove punctuation (except hyphens) and wiki markup 2. Eliminate letter case. 3. Remove stopwords, because very common words carry 4. Sort each list of words, because we analyze the appear-5. Compare the new and old word sequences to determine 6. The editor who made the new edit begins accruing Figure 3: Percentage of PWVs according to the decile of the editor who wrote them.

Our software does not track persistent words if text is  X  X ut-and-pasted X  from one article to another. If an editor moves a block of text from one article to another, PWVs after the move will be credited to the moving editor, not to the original editors. This problem is challenging, because edits are per-article, making it difficult to detect where the text moved to, or even if it moved to only one place.
An editor can work either anonymously, causing edits to be associated with the IP address of his or her computer, or while logged in to a pseudonymous user account, causing edits to be associated with that pseudonym. We exclude anonymous editors from some analyses, because IPs are not stable: multiple edits by the same human might be recorded under different IPs, and multiple humans can share an IP.
Editors who revert do not earn PWV credit for the words that they restore, because they are not adding value, only restoring it; rather, the edit ors whose words they restore regain credit for those words.

Reverts take two forms: identity revert , where the post-revert revision is identical to a previous revision, and ef-fective revert , where the effects of prior edits are removed (perhaps only partially), but the new text is not identical to any prior revision. Identity reverts are unusually common, because Wikipedia includes a special mechanism through which any editor can easily revert a page to a previous edit, and because the official Wikipedia guide to resolving van-dalism recommends using this mechanism. Kittur et al. [12] report that of identity reverts and effective reverts which could be identified by examining edit comments, 94% are identity reverts. There are probably other effective reverts, because some editors do not clearly label their edits, but de-tecting these is challenging because it requires understand-ing the intent of the editor. In this paper, we consider only identity reverts.
We analyzed 4.2 million editors and 58 million edits. The total number of persistent word views was 34 trillion; or, excluding anonymous editors, 25 trillion. 300 billion PWVs were due to edits before the start of our analysis and were excluded. 330 billion PWVs were due to bots  X  autonomous or semi-autonomous programs that edit Wikipedia. 56
Figure 3 shows the relative PWV contributions of editors divided by edit count decile. From January 2003 to Febru-ary 2004, the 10% of editors with the most edits contributed about 91% of the PWVs. Then, until February 2006, Wik-ipedia slowly became more egalitarian, but around Febru-ary 2006, the top 10% re-stabilized at about 86% of PWVs. Growth of PWV share increases super-exponentially by edit count rank; in other words, elite editors (those who edit the most times) account for more value than they would given a power-law relationship. Figure 4 zooms in; editors with the top 0.1% of edits (about 4,200 users) have contributed over 40% of Wikipedia X  X  value. Collectively, the ten editors with the most PWVs contributed 2.6% of all the PWVs.
Editors who edit many times dominate what people see when they visit Wikipedia. The top 10% of editors by num-ber of edits contributed 86% of the PWVs, and top 0.1% contributed 44%  X  nearly half! The domination of these very top contributors is increasing over time.

Of the top 10 contributors of PWVs, nine had made well over 10,000 edits. However, only three of these users were also in the top 50 ranked by number of edits. The num-ber one PWV contributor, Maveric149, contributed 0.5% of all PWVs, having edited 41,000 times on 18,000 articles. Among the top PWV contributors, WhisperToMe (#8) is highest ranked by number of edits: he is #13 on that list, having edited 74,000 times on 27,000 articles.

Exploring the list of top editors by edit count, we notice something interesting: the list is filled with bots. They oc-cupy the top 4 slots, 9 of the top 10, and at least 20 of the top 50. One the other hand, the list of top editors by PWV
We identified bots by taking the union of (a) editors with usernames ending in  X  X ot X , followed by an optional digit, that had at least 100 edits and (b) users listed on Wiki-pedia X  X  list of approved bots [28].
Some damage-reverting bots had a bug causing a few re-verts to become non-identity reverts. Because our software could not detect these reverts, it treated the situations as removal of all text and replacement with entirely new text. Effectively, these bots  X  X tole X  PWVs from their rightful own-ers. Our measurements show that these bugs resulted in only about 0.5% of PWVs being stolen. is filled with humans: only 2 bots appear in the top 50, and none in the top 10. This suggests, perhaps reassuringly, that people still matter.
We interpret damage differently from value, because in some cases only a few changed words can result in a worse-than-useless article. For instance, adding or removing the word  X  X ot X  or changing a key date can mislead a reader dan-gerously. Therefore, we classify a revision simply as  X  X am-aged X  or  X  X ot damaged X  rather than counting the number of damaged words. The key metric in this analysis is the damaged article view (DAV), which measures the number of times an article was viewed while damaged.
We use the opinions of the Wikipedia community in de-ciding which revisions are in a damaged state. Note that we focus on revisions that are damaged, not the edits which repair the damage, because these are the revisions which lead to damaged article views. Revisions that are subse-quently reverted (using an identity revert) are considered as candidates for the  X  X amaged X  label. To distinguish damage repair from disagreement or other non-damaging behavior, we look for edit comments on the reverts that suggest intent to repair damage.

We assume that for the purpose of damage analysis, most incidents of damage are repaired by identity reverts. This assumption is motivated by two factors. First instructions on Wikipedia itself have, since the beginning of our analysis period, recommended the use of identity reverts for dam-age repair [26]. Second, repairing damage using the wiki interface to make an identity revert is easier than manually editing away the damage.

It is important to note that our method is not foolproof, as editors sometimes make mistakes or place overheated rhetoric into the comments of reverts, labeling each other vandals when a neutral reader would consider the situation simply a content dispute. We also cannot discover damage which was not yet repaired by the end of the article his-tory. Nonetheless, as our results below show, our method is essentially sound.

At any given instant, a revision is in zero or more of the following states. State membership is determined by how future editors react to the revision, so it can only be de-termined in retrospect. The edit comments and timestamps necessary for these computations are available in the history of articles (data set 5). Figure 5 shows the relationships of the various states informally. Figure 5: Classes of revisions: Damaged-Truth is the conjectured true set of damaged revisions, while Damaged-Loose and Damaged-Strict are in-creasingly restrictive subsets of Will-Be-Reverted designed to approximate Damaged-True. For example, consider the revision history in Figure 6. Revision 5 reverts back to revision 2  X  we know this because the MD5 checksums are the same  X  discarding the effects of revisions 3 and 4. Therefore, revisions 3 and 4 are in state WBR; additionally, because Revision 5 X  X  edit comment matches the criteria for D-Strict, these two revisions are also in D-Loose and D-Strict. Finally, if each revision were viewed 10 times, there would be 20 DAVs generated by this sequence.

Both D-Loose and D-Strict limit the distance between the first damaged revision and the repairing revert to 15 revi-sions. This is to avoid false positives due to a form of damage where someone reverts an article to a long-obsolete revision and then marks this (damaging) revert as vandalism repair.
Our classification software, which includes these expres-sions, is available by emailing the authors. We believe it is reasonable to assume that essentially all damage is repaired within 15 revisions.

The purpose of states D-Loose and D-Strict is to be prox-ies for the difficult-to-determine state D-True. To evaluate the two metrics for this purpose, three human judges in-dependently classified 676 WBR revisions, in 493 WBR se-quences selected randomly from all WBR sequences. Clas-sification included a best effort to figure out what was going on, which often included a minute or two of research to ver-ify information or clarify unfamiliar topics, words, or links. The edit comment of the final revert was hidden in order to avoid biasing the judges.

Judges classified revisions into the following three classes: Determining whether a revision was V-Human or just D-Human is difficult because it requires assessing the intent of the editor. Indeed, despite written guidelines and cal-ibration by judging together a smaller independent set of WBR revisions, there were considerable differences between the judges. From the reader X  X  perspective, the intent behind damage is irrelevant, so we consider further only D-Human.
We use these judgements to evalute the effectiveness of the classes D-Loose and D-Strict as proxies for D-True. Of the 676 revisions judged, all three judges agreed on 437 (60%), while the class of the remaining 239 (35%) was deter-mined by 2-1 majority. We assumed that revisions judged D-Human by a majority of judges, and no others, were in class D-True.

By this measure, 403 revisions (60%) were in D-True. The automatic D-Strict classifier had a precision of 0.80 but a recall of only 0.17, i.e., within the judged revisions, 80% of D-Strict revisions were in D-True, but only 17% of D-True revisions were in D-Strict; D-Strict is therefore not a reasonable proxy for D-True.

On the other hand, the precision and recall of D-Loose were 0.77 and 0.62 respectively. Clearly, D-Loose suffers from both false negatives and false positives. The former arise when editors revert damage but do not label their ac-tions clearly, while the latter can be seen in content disputes, as described previously. While imperfect, D-Loose is a rea-sonable proxy for D-True. The remainder of this section will consider D-Loose only.
We found 2,100,828 damage incidents (i.e., D-Loose se-quences). 1,294 overlapped the end of our study period, so there were 2,099,534 damage-repair reverts. No incidents overlapped the beginning of the study period. These inci-dents comprised 2,955,698 damaged revisions, i.e. an aver-age sequence comprised 1.4 damaged revisions before repair. The study period contained 57,601,644 revisions overall, so about 5% of revisions were damaged. Figure 7: Probability of a typical view returning a damaged article. Both our data and a fitted ex-ponential curve (with boundary lines representing the 95% confidence interval) are shown. We fit-ted through June 2006, when widespread use of au-tonomous vandalism-repair bots began. Figure 8: Rapidity of damage repair: 42% of damage incidents are repaired within one estimated view, meaning that they have essentially no impact.

During the study period, we estimate that Wikipedia had 51 billion total views. Of these, 188 million were damaged  X  139 million by anonymous users  X  meaning that the over-all probability of a typical view encountering damage was 0.0037. In October 2006, the last month we analyzed, it was 0.0067. Figure 7 illustrates the growth of this probability over time. In particular, the data through June 2006 fits
Figure 8 illustrates the rapidity of damage repair. 42% of damage incidents are repaired essentially immediately (i.e., within one estimated view). This result is roughly consistent with the work of Vi  X  egas et al. [20], which showed that the median persistence of certain types of damage was 2.8 min-utes. However, 11% of incidents persist beyond 100 views, 0.75%  X  15,756 incidents  X  beyond 1000 views, and 0.06%  X  1,260 incidents  X  beyond 10,000 views. There were 9 out-liers beyond 100,000 views and 2 beyond 500,000; of these, 8 were false positives (the other was the  X  X essage X  incident discussed below). The persistence of individual incidents of damage has been relatively stable since 2004, so the increas-ing probability of damaged views indicates a higher rate of damage.

A possible cause for this increase is that users may be writ-ing edit comments differently, increasing the number of edit comments that fit the D-Loose pattern. To test this hypoth-esis, we judged the precision and recall of D-Loose for a sam-ple of 100 WBR sequences (containing 115 revisions) from 2003 and earlier, using threee judges and majority opinion as above. We found that the recall of D-Loose over this sam-ple was 0.49, compared to 0.64 for a sample of 100 sequences (134 revisions) from 2006. Thus, commenting behavior has changed, and this change explains about one-third of the in-crease in probability of a damaged view. Damage was only 14 times more impactful in 2006 than 2003, not 18 times.
While the overall impact of damage in Wikipedia is low, it is rising. The appearance of vandalism-repair bots in early 2006 seems to have halted the exponential growth (note the dramatic drop in view probability after June 2006), but it is too early to tell what the lasting impact will be.
Many of the editors who performed the reverts we an-alyzed appear to have treated vandalism and other dam-age the same way. For instance, it is common to find edit comments asserting vandalism repair on reverts of revisions which are apparently users practicing editing on a regular article, something which is not welcome but is (by policy) explicitly not vandalism. This makes sense, as from the reader X  X  perspective, damage is damage regardless of intent.
The most viewed instance of damage was deletion of the entire text of the article  X  X essage X , which lasted for 35 hours from January 30 to February 1, 2006 and was viewed 120,000 times. The popularity of this article is partly explained by the fact that it is (at the time of this writing) the top Google search result for  X  X essage X . It may seem odd that such a high traffic article was not fixed more quickly. However, it is not edited very frequently, only about once every 19 days. (This reminds us that there is no correlation between view rate and edit rate.) Put another way, the tens of thousands of people who viewed the article while it was damaged sim-ply may not have included any Wikipedia editors. Further, maybe this type of damage is not as inviting of a fix as others, such as obscenities.

One example of a high-traffic article which suffered greatly from damage was  X  X iki X  in October 2006. Of the 3.1 million estimated views during that month, 330,000 were damaged, over 10%. This page was bombarded with damaging edits having no apparent pattern, most of which were repaired rapidly within minutes or hours but which had dramatic impact due to their sheer numbers. Another interesting ex-ample is  X  X aniel Baldwin, X  an article about an American actor, damaged by a single incident which lasted for three months. In October 2005, someone vandalized the article by deleting the introduction and replacing it with with text explaining (incorrectly) that Baldwin was a college student and frequent liar. The next day, someone removed the bo-gus text but failed to restore the introduction; this situation persisted through five more revisions until someone finally made a complete repair in February 2006.

We have two policy suggestions for combating damage, both based on distributing repair work to humans. The first is to ensure that revi sions are reviewed by n humans within a few seconds of being saved. The second is to ensure that each article is on at least n users X  watch lists. Assuming an edit rate of 280,000 edits per day (the average rate we observed in our log analysis), and assuming it takes 30 seconds to determine if an average revision is damaged, schemes like these would require about n  X  28,000 reviewers averaging five minutes of daily work.
We have mentioned that for each damaged edit, the level of damage depends on what exactly the damage is: for ex-ample, a reader might consider deleting all of an article X  X  content more damaging than adding nonsense, and false in-formation more damaging still. Consequently, to understand the impact of different damages, it is meaningful to define different types of damage from the reader X  X  perspective and provide estimates of how often each type of damage occurs.
Based on the experience of judging edits for RQ2 and de-veloping the tools for doing so, we present a list of features exhibited by Wikipedia damage, aiming for comprehensive-ness. These features, with comparisons to the anecdotal categories of Vi  X  egas et al. [20], are as follows:
Vi  X  egas X  phony redirection is not included above because we observed only one instance (and it was better described as Offensive), and we believe that idiosyncratic copy ( X  t e x t that is clearly one-sided, not of general interest, or inflam-matory X ) better describes disputed content, not damage. In reflecting on the results for RQ2, we observed that most D-Human sequences consisted of one or more related edits that formed a coherent single incident. Therefore, to focus the effort of our judges, we used the D-Human sequence rather than individual revisions as the unit of analysis. Of Figure 9: Distribution of damage features. % is the percentage of D-Human sequences where the feature applies (determined by majority vote), while the Agreement columns list how many times all ( 3v0 ), two of the three ( 2v1 ), and only one of the judges ( 1v2 ) believed the feature applied. (Percentages do not sum to 100 because features are not mutually ex-clusive.) PF ( proportion full ) gives the proportion assigned unanimously (i.e. PF = 3v0 / ( 3v0 + 2v1 ) ), while Ja gives the Jacquard statistic: the number of times all judges assigned the feature divided by the number of times any assigned the feature, i.e. Ja = 3v0 / ( 3v0 + 2v1 + 1v2 ) . the 493 WBR sequences analyzed in RQ2, 308 were classified as D-Human. These 308 sequences form the basis of this section X  X  analysis.

After calibration on a different sample of D-Human edit sequences, three judges independently classified the se-quences, applying as many of the damage features as were appropriate. As in RQ2 above, we used a  X  X ajority vote X  procedure, i.e. a feature applied to a sequence if at least two of the three judges believe that it does.
Figure 9 summarizes our results. It is not surprising that agreement was highest for Spam, Mass Delete, and Partial Delete, since these features do not require much judgement. On the other hand, what X  X  offensive or nonsense is somewhat subjective, and misinformation may be subtle. Finally, the low number of D-Human sequences labeled Other indicate that our categories are relatively comprehensive.
From the perspective of Wikipedia, all damage is seri-ous because it affects the credibility of Wikipedia X  X  content. However, there are specific factors that we can use to assess more precisely the implications of our results. First, how common is a given type of damage? If a particular type is infrequent, we need not worry as much. Second, what is the potential impact on readers? If there is little harm, we need not worry as much even if occurence is frequent. Finally, how easy is it to detect automatically? Even if damage is not automatically repaired, automatic notification of human editors can speed repair.

With this in mind, Mass Delete and Nonsense are low-impact types of damage. The former is relatively uncom-mon and trivial to detect automatically. The latter, while common, damages only presentation, not content, except in cases where its sheer bulk overwhelms content. For exam-ple, one incident consisted of the insertion of thousands of repetitions of a string of Korean characters into the article  X  X apan X . (Interestingly, the characters formed hate speech, but we classified the incident as Nonsense because few read-ers of the English Wikipedia understand Korean.) Spam and Partial Delete are somewhat higher impact, because they are tricky to detect automatically (useful edits introduce links and remove text all the time); also, Spam wastes readers X  time and Partial Delete may cause the omission of impor-tant information.

Offensive damage is troublesome because it is common (28% of incidents) and potentially highly impactful  X  of-fensive content damages the reputation of Wikipedia and drives away readers. Automatic detection of offensive dam-age is plausible in some cases (e.g., detecting obscenties) but harder in the general case due to the complexity of offensive speech and the difficulty of analyzing images automatically.
Misinformation may be the most pernicious form of dam-age. It is both common (20% of incidents) and difficult to detect. Automatic detection is essentially impossible be-cause it requires understanding the content of the page, and people who visit a page are typically there to learn about its topic, not because they understand it well. An intriguing and subtle example is that of the  X  X chi-soto X  article, which discusses a specific facet of Japanese language and social custom. A (presumably well-meaning) editor changed the translation of the word uchi from inside to house  X  X oth are correct, but inside is the one appropriate for this ar-ticle. This error could only be detected by a reader with sophisticated knowledge of Japanese.
Wikipedia matters . It is widely used and immensely in-fluential in contemporary discourse. It is the definitive ex-emplar of collective action on the Web, producing a large, successful resource of great value.

Our work has set the scientific study of Wikipedia  X  and, by extension, study of other online collective action com-munities  X  on a much firmer basis than ever before. Most fundamentally, we offer a better way to measure the phe-nomena people care about. Others have used author-based measures, counting edits to approximate the value of contri-butions and measuring repair time to approximate impact of damage. We use reader-based measures. We approximate both the value of contributions and the impact of damage by estimating the number of times they were viewed.
Our view-based metrics let us both sharpen previous re-sults and go beyond them. Others have shown that 1% of Wikipedia editors contributed about half of edits [6]. We show that 1/10th of 1% of editors contributed nearly half of the value , measured by words read. Others have shown that one type of damage was repaired quickly [20]. We show this for all types of damage. We also show what this re-sult means for readers: 42% of damage is repaired almost immediately, i.e., before it can confuse, offend, or mislead anyone. Nonetheless, there a re still h undreds of m illions of damaged views. We categorize the types of damage that oc-cured, show how often they occured, describe their potential impact on readers, and discuss how hard (or easy) they are to detect automatically. We give examples of especially im-pactful damage to illustrate these points. Finally, we show that the probability of encountering damage increased ex-ponentially from January 2003 to June 2006.

What are the implications of our results? First, because a very small proportion of Wikipedia editors account for most of its value, it is important to keep them happy, for example by ensuring that they gain appropriate visibility and status. However, turnover is inevitable in any online community. Wikipedia should also develop policies, tools, and user inter-faces to bring in newcomers, teach them community norms, and help them become effective editors.

Second, we speculate that the exponential increase in the probability of encountering damage was stopped by the widespread use of anti-vandalism bots. It is likely that van-dals will continue working to defeat the bots, leading to an arms race. Thus, continued work on automatic detection of damage is important. Our results suggest types of damage to focus on; the good news is that the results show little subtlety among most vandals. We also generally believe in augmentation, not automation. That is, we prefer intelli-gent task routing [7] approaches, where automation directs humans to potential damage incidents, but humans make the final decision.
We thank Tim Starling and his colleagues at the Wiki-media Foundation for making request log data available to us and providing technical assistance in its processing. We also thank the members of wikitech-l for their technical help. Aaron Swartz suggested the ideas that developed into Sec-tion 4. Finally, we gratefully acknowledge the support and feedback of members of our research group. This work has been supported by the National Science Foundation under grants IIS 03-24851 and IIS 05-34420. [1] Adler, B. and Alfaro, L. A content-driven repu-tation [2] Alexa Internet, Inc. Quick tour. [3] Arguello, J. et al. Talk to me: Foundations of [4] Bryant, S. L. et al. Becoming Wikipedian: [5] Cherny, L. Talk to me: Foundations of successful [6] Chi, E. Long tail of user participation in Wikipedia. [7] Cosley, D. et al. Using Intelligent Task Routing and [8] Emigh, W. and Herring, S. C. Collaborative authoring [9] Giles, J. Internet encyclopedias go head to head. [10] Herring, S. C. Gender and democracy in computer-[11] Isbell, Jr., C. L. et al. Cobot in LambdaMOO: A [12] Kittur, A. et al. He says, she says: Conflict and [13] Kittur, A. et al. Power of the few vs. wisdom of the [14] Lampe, C. and Resnick, P. Slash (dot) and Burn: [15] Marks, K. Power laws and blogs. [16] Smith, M. A. and Fiore, A. T. Visualization [17] Stivilia, B. et al. Assessing information quality of a [18] Swartz, A. Who writes wikipedia? [19] Vi  X  egas,F.B.andDonath,J.S.Chatcircles.In Proc. [20] Vi  X  egas, F. B. et al. Studying cooperation and conflict [21] Voss, J. Measuring Wikipedia. In Proc. Scientometrics [22] Wales, J. Jimmy Wales talks Wikipedia. [23] Wexelblat, A. and Maes, P. Footprints: History-Rich [24] Whittaker, S. et al. The dynamics of mass interaction. [25] Wikimedia Foundation. Stop word list. [26] Wikipedia. Vandalism in progress. [27] Wikipedia. Page requests per day. http: [28] Wikipedia. Bots/Status. [29] Wikipedia. Awareness statistics. [30] Wikipedia. Podcast. http://en.wikipedia.org/w/index. [31] Wikipedia. Seigenthaler Controversy. [32] Wikipedia. Size of Wikipedia. [33] Wikipedia. Wikipedia in culture.
