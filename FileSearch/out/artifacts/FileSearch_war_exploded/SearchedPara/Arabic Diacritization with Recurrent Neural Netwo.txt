 Arabic, Hebrew, and similar languages are typi-cally written without diacritics, leading to ambigu-ity and posing a major challenge for core language processing tasks like speech recognition. Previous approaches to automatic diacritization employed a variety of machine learning techniques. However, they typically rely on existing tools like morpho-logical analyzers and therefore cannot be easily extended to new genres and languages. We de-velop a recurrent neural network with long short-term memory layers for predicting diacritics in Arabic text. Our language-independent approach is trained solely from diacritized text without re-lying on external tools. We show experimentally that our model can rival state-of-the-art methods that have access to additional resources. Hebrew, Arabic, and other languages based on the Arabic script usually represent only consonants in writing and do not mark vowels. In such writ-ing systems, diacritics are used for marking short vowels, gemination, and other phonetic units. In practice, diacritics are usually restricted to specific settings such as language teaching or to religious texts. Faced with a non-diacritized word, readers infer missing diacritics based on their prior knowl-edge and the context of the word in order to re-solve ambiguities. For example, Maamouri et al. (2006) mention several types of ambiguity for the Arabic string  X  X  across part-of-speech tags, and at a grammatical
Table 1: Possible diacritized forms for  X  X  level. In practice, a morphological analyzer like MADA (Habash et al., 2009) produces at least 13 different diacritized forms for this word, a subset
The ambiguity in Arabic orthography presents a problem for many language processing tasks, in-cluding acoustic modeling for speech recognition, language modeling, text-to-speech, and morpho-logical analysis. Automatic methods for diacriti-zation aim to restore diacritics in a non-diacritized text. While earlier work used rule-based meth-ods, more recent studies attempted to learn a di-acritization model from diacritized text. A vari-ety of methods have been used, including hidden Markov models, finite-state transducers, and max-imum entropy  X  see the review in (Zitouni and Sarikaya, 2009)  X  and more recently, deep neu-ral networks (Al Sallab et al., 2014). In addi-tion to learning from diacritized text, these meth-ods typically rely on external resources such as part-of-speech taggers and morphological analyz-ers like the MADA tool (Habash and Rambow, 2007). However, building such resources is a labor-intensive task and cannot be easily extended to new languages, dialects, and domains.
In this work, we propose a diacritization method based solely on diacritized text. We treat the prob-lem as a sequence classification task, where each character has a corresponding diacritic label. The sequence is modeled with a recurrent neural net-work whose input is a sequence of characters and whose output is a probability distribution over the diacritics. Any RNN architecture can be used in this framework; here we focus on long short-term memory (LSTM) networks, which have shown re-cent success in a number of NLP tasks. We exper-iment with several architectures and show that we can achieve state-of-the-art results, without rely-ing on external resources. Error analysis demon-strates the benefit of using LSTM over simpler neural networks. Languages based on the Arabic script typically employ an abjad writing system, where each sym-bol represents a consonant while vowels and other phonetic units, commonly known as diacritics, are usually omitted in writing. In modern standard and classical Arabic, these include the short vow-els a , u , and i , the case endings F , N , and K , the gemination marker ~ , and the silence marker o . 2 Table 2, modified from (Habash et al., 2007), lists the diacritics. Importantly, the gemination marker ~ can combine with short vowels and case endings (e.g. Table 1, row 3). We define the following sequence classification task, similarly to (Zitouni and Sarikaya, 2009). Figure 1: An illustration of our network topology. Let w = ( w 1 ,...,w T ) denote a sequence of char-acters, where each character w t is associated with a label l t . A label may represent 0, 1, or more di-acritics, depending on the language. Assume fur-ther that each character w in the alphabet is rep-resented as a real-valued vector x w . This charac-ter embedding may be learned during training or fixed.

Our neural network has the following structure, illustrated in Figure 1:
During training, each sequence is fed into this network to create a prediction for each character. As errors are back-propagated down the network, the weights at each layer are updated. During test-ing, the learned weights are used in a forward step to compute a prediction over the labels. We always take the best predicted label for evaluation. Hidden layer Our main system relies on long short-term memory networks (LSTM) (Hochre-iter and Schmidhuber, 1997; Graves et al., 2013). Here we describe a single LSTM layer and refer to Graves et al. (2013) for the extension to bidi-rectional LSTM (B-LSTM) and to multiple layers. The LSTM computes the hidden representation for
Table 3: Arabic diacritization corpus statistics. input x t with the following iterative process: h t = o t tanh( c t ) where  X  is the sigmoid function, is element-wise multiplication, and i , f , o , and c are input, forget, output, and memory cell activation vectors. The crucial element is the memory cell c that is able to store and reuse long term dependencies over the sequence. The W matrices and b bias vec-tors are learned during training.
 Implementation details The input layer maps the character sequence to a sequence of letter vec-tors, initialized randomly. We also tried initializ-ing with letter vectors trained from raw text with word2vec (Mikolov et al., 2013a; Mikolov et al., 2013b), but did not notice any improvement, prob-ably due to the small letter vocabulary size. The input layer also stacks previous and future letter vectors, enabling the model to learn contextual in-formation. We use a letter embedding size of 10 and a window size of 5 characters, so the input size is 110.

We experiment with several types of hidden lay-ers, ranging from one feed-forward layer to mul-tiple B-LSTM layers. We also add a linear pro-jection after the input layer. This has the effect of learning a new representation for the letter embed-dings. The output layer is a Softmax over labels: where y t = W hy h t + b y and y t [ l ] is the l th element of y t .

Training is done with stochastic gradient de-scent with momentum, optimizing the cross-entropy objective function. Layer sizes and other hyper-parameters are tuned on the Dev set. Our implementation is based on Currennt (Weninger et al., 2015).
 Model All End # params Feed-forward 11.76 22.90 63K Feed-forward (large) 11.55 23.40 908K LSTM 6.98 10.36 838K
B-LSTM 6.16 9.85 518K 2-layer B-LSTM 5.77 9.18 916K 3-layer B-LSTM 5.08 8.14 1,498K Table 4: Diacrtic error rates (DERs) on the Dev set, over all diacritics and only at word ending. Table 5: Results (DER) on the Test set. MaxEnt results from (Zitouni and Sarikaya, 2009) Data We extract diacritizied and non-diacritized texts from the Arabic treebank, following the Train/Dev/Test split in (Zitouni and Sarikaya, 2009). Table 3 provides statistics for the corpus.
Every character in our corpus has a label cor-responding to 0, 1, or 2 diacritics, in the case of the gemination marker combining with another di-acritic. Thus the label set almost doubles. We opted for this formulation due to its simplicity and generalizability to other languages, even though previous work reported improved results by first predicting gemination and then all other diacrit-ics (Zitouni and Sarikaya, 2009).
 Results Table 4 shows the results of our models on the Dev set in terms of the diacritic error rate (DER). Clearly, LSTM models perform much bet-ter than simple feed-forward networks. To make the comparison fair, we increased the number of parameters in the feed-forward model to match that of the LSTM. In this setting, the LSTM is still much better, indicating that it is far more success-ful at exploiting the larger parameter set. Interest-ingly, the bidirectional LSTM works better than a unidirectional one, despite having less parameters. Finally, deeper models achieve the best results.
On the Test set (Table 5), our 3-layer B-LSTM model beats the lexical variant of Zitouni and Sarikaya (2009) by 3.25% DER, a 40% error reduction. Moreover, we outperform their best model, which also used a segmenter and part-of-Figure 2: A confusion matrix of errors made by our system.  X # X  marks word boundary. Best viewed in color. speech tagger. This shows that our model can ef-fectively learn to diacritize without relying on any resources other than diacritized text.
 Finally, some studies report work on a Train/Test data split, without a dedicated Dev set (Zitouni et al., 2006; Habash and Rambow, 2007; Rashwan et al., 2011; Al Sallab et al., 2014). We were reluctant to follow this setting so we per-formed all development on the Dev set of (Zi-touni and Sarikaya, 2009). Still, we ran our best model on the Train/Test split and achieved a DER of 5.39% on all diacritics and 8.74% on case end-ings. The first result is behind the state-of-the-art (Al Sallab et al., 2014) by 2% but the second one is better by 3%. Given that we did not tune the system for this data set, this result is encouraging. Error Analysis A quantitative analysis of the er-rors produced by one of our models on the Dev set is shown in Figure 2. The heat map denotes the number of errors produced. The major source of errors comes from confusing the short vowels a , i , and u , among themselves and with no diacritic. This is expected due to the high rate of short vow-els in Arabic compared to other diacritics. It also explains why methods that take the confusion ma-trix into account in their classification algorithm do quite well (Al Sallab et al., 2014).

We also analyzed some errors qualitatively. Fig-ure 3 shows the errors produced by several of our diacritization models on a sample sentence. In-Figure 3: Sample errors by selected diacritization models. Wrong predicted diacritics are underlined and in red; missing diacritics are noted by under-score. Translation:  X  X he editor of An Nahar, Ge-bran Tueni, thought that the judicial formations came to dilute the issue of MTV station X . terestingly, the simple feed-forward model fails to predict the correct case ending on the word AlqaDA } iy~ap ( X  X udicial X ), while both LSTM models succeed. This may indicate that LSTM in-deed captures the kind of long-distance dependen-cies that are responsible for case marking. Other errors are more difficult to explain, but note that all models struggle with the proper name tuwayoniy~ ( X  X ueini X ), which is difficult to solve without ex-ternal resources. In this work, we develop a recurrent neural net-work that predicts diacritics in non-diacritized texts. Our model is language agnostic: it is trained solely from diacritized text without relying on additional resources. Using LSTM units, we demonstrate that our model can effectively learn to diacritize Arabic texts and rivals state-of-the-art methods that rely on language-specific tools.
In future work, we intend to incorporate our di-acritization system in a speech recognizer. Recent work has shown improvements in Arabic speech recognition by diacritizing with MADA (Al Hanai and Glass, 2014). Since creating such tools is a labor-intensive task, we expect our diacritization approach to promote the development of speech recognizers for other languages and dialects. This research was supported by the Qatar Comput-ing Research Institute (QCRI).

