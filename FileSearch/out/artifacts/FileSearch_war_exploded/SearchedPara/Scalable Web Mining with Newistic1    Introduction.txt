 In the last decade, more and more content has made the transition from print to on-line. As a result, more people are starting to turn to news aggregator websites to help them manage information overload. These systems continuously monitor Internet news sources and analyze news using text clustering, categorization, personalization and news ranking, which are research topics w ith deep roots in the academia. However, little is known about th e internals of any of the commerc ial news gathering platforms, as there are no papers that describe their implementation in detail. Moreover, the num-ber of academic projects that describe integ rated systems combining two or more of the mentioned topics is limited. We try to address these problems by detailing the architec-ture of a web mining system in its entirety.
 Newistic is a modular web mining platform that collects and analyzes news from Internet websites. This system was built to serve as a foundation for our data mining projects. A distributed and scalable design was needed to analyze the hundreds of thou-sand of news items published each day. First, a basic infrastructure was built, with the role of downloading, extracting and storing news and articles. On top of this were added other modules, such as clustering, cate gorization, or named entity extraction.
In addition to the modular architecture, the s ystem presented in this paper brings two contributions: the implementation of an extraction algorithm that uses heuristic methods to extract news from HTML pages, and the use of the Quality Threshold clustering algorithm [4] for documents. This algorithms, borrowed from gene expression analysis, is superior in quality to the k-means algorith m. Although it has quadratic complexity, this issues was addressed by optimizing it.

The remainder of the paper is organized as follows: In Section 2 we present other work related to this subject, Section 3 describ es the architecture of the system which consists of the Crawler, Analyzer, Clustering, Storage and Web Server components, Section 4 presents the text mining functionalities, Section 5 discusses clustering, Sec-tion 6 details the results, and we wrap up with future work in Section 7. As we stated previously, there is almost no research published describing any of the commercial news aggregator engines. One e xception is a paper on the personalization feature of Google News [14], which is only one module of the platform.

A number of news aggregation platforms have emerged in the academic world, but few of them are presented as a whole in research papers. Among the most recent systems is ComeToMyHead. The architecture and components of the system are described in [11] (here, the system is named Velthune). Its novel ranking algorithm, which takes into account the freshness of news articles and the rank of their source, is the first news ranking algorithm documented in a paper [12]. It gathers, categorizes and clusters news from more than 2000 sources using RSS and Atom. NewsBlaster [7] covers only 17 sources, but they provide summarization of text articles, along with hierarchical clustering. It was built by integrating the Topic Detection and Tracking (TDT) and summarization modules, which were researched independently before.

NewsInEssence [5], [6] established a framework for finding related articles for an input story, in real-time. They use in-site search provided by the source web sites to find possible candidates for a story, thus their crawling component is greatly simplified. The articles found are summarized. QCS [8] is a modular platform that provides querying, clustering and summarization of articles from an input set. Its modular structure allows combinations between different algorithms for the three types of operations available. It has no crawling component. NewsJunkie [9] fi lters news stories by taking into account their novelty and by tracking news previously viewed by the user. Thus, it is able to provide personalized news feeds.

European Media Monitor (EMM) is the most compelling research project to date. It gathers news in 35 languages, classifies them, extracts named entities and then generates cross-lingual clusters. The clustering is p erformed over multiple languages. NewsEx-plorer, the analysis component of EMM, is described in [13]. The architecture of Newistic consists of several loosely coupled modules. These components can each have several instances running on multiple machines. Instances auto-detect each other and collaborate on the task at hand.
Each document goes through a processing pipeline that connects all the components of the system. Figure 1 shows the flow of data through the modules. The Hub Crawler periodically scans each Hub for new links. A Hub is a specific page from a website that links to the latest news. Any new link is possibly a fresh news story, so it is sent to the News Crawler. This module is responsible for downloading new links provided by the Hub Crawler and sending them further down the pipeline, to the Analyzer. The latter tries to extract the article from the HTML c ode. If successful, it detects the category of the news article and extr acts Named Entities. If links to images are found in the HTML page, the links, together with the news story, are sent to the Image Crawler. Otherwise, the story is sent directly to the Storage module. The task of the Image Crawler is to download the images it receives, and store them locally. The Storage module indexes and stores the text. From the Storage module, the data is passed over to the Clustering module, which periodically builds an in-memory structure of the clusters.

The modular architecture of Newistic makes it very easy to distribute and, conse-quently, scale out. Almost all the backend m odules can have multiples instances that run in parallel. The exceptions are the Storage component, which manages the document index, and the Clustering module, which cannot be run in parallel without rewriting its algorithm. However, the Analyzer, which is an essential and CPU-intensive component, was designed to be distributable from the start. Almost all the data mining functions are performed by the Analyzer. The sole exception, clustering, is presented in a separate section. The first mining function is extracting news articles out of HTML pages received as input. I f successful, the output consists of the title and text of the article, along with metadata like category, named entities, or images related to it. After extracting a news article, the Analyzer extracts the named entities, which are divided in 3 categories: places, p eople and organizati ons. They can be used to influence the weights of words when performing text operations like categorization, clustering or search. For this task, the ANNIE component of the GATE framework [15] was used.
 News Extraction. The News Extraction module is one of the most important parts of the system, as there are very few other systems, commercial or academic, that have this feature implemented.

Many news aggregators use RSS feeds as their source of information. Although this might be a better choice than developing a crawler and an extraction algorithm from scratch, it has two disadvantages: 1. Not all the news websites provide RSS feeds for their stories. 2. Some RSS feeds don X  X  provide the whole text of the news.
 Our algorithm extracts news directly from web pages, overcoming the disadvantages of using RSS feeds. It tries to determine the title and body, as well as links to images located in the article by analyzing the HT ML of each candidate page. In order to come up with an efficient algorithm to extract news stories from raw HTML, an heuristic approach was used. Although it is impossible to cover all the different layouts that exist for web pages, the module manages to succe ssfully analyze a fair share of them.
After cleaning out irrelevant parts such as JavaScript blocks or forms, contiguous blocks of text are extracted. Next, these blocks are analyzed to see if they are part of a possible news story. If a block has a high percentage of text within link anchors, it is discarded. Finally, each block of text is given a score that represents how likely it is to be the title of the article. This score is calculated taking into account 16 factors, including the position and length of the block in the text, and what HTML tags it is enclosed in. If the highest score passes a certain threshold, the block of text is taken as the title, and the following blocks of text are taken as the body of the article. The HTML extraction algorithm has some issues that need to be addressed in the future. One of them, for example, is the inability to treat a news story spanning multiple web pages as a single item. Another one is atypical HTML layouts that need to be analyzed individually to see if the algorithm can be particularized for each case.
 Categorization. The algorithm that we used for categorization is K-Nearest Neighbor, or kNN [2]. In [3], Yang compared it with other methods: Support Vector Machines (SVM), Linear Least Squares Fit (LLSF), N eural Networks (NNet) and Naive Bayes (NB). kNN was the second most efficient only to SVM, which is more computation-intensive. The Clustering module groups related article s. It receives all the news items extracted by the Anazlyer, and clusters them periodically. Each new set of clusters is sent to the Web Server for processing. Additionally, this module is also responsible for detecting duplicate articles.

The clustering function is run at a fixed interval, currently set at 10 minutes. It clus-ters the news gathered by the system in the last 24 hours. This amounts to between 100,000 and 200,000 news.
 The clustering algorithm that we implemented is called QT (Quality Threshold). It was proposed by Heyer et al. in [4] for gene clustering. From what we know, our implementation is the first one used for text clustering.
The following steps explain the QT clustering algorithm: 1. For a document D that we want to cluster, compute the cosine similarity between 2. All the documents that are closer than a certain threshold to D are added to a pos-3. Repeat steps 1 and 2 for all the documen ts in the dataset. This results in a set 4. Choose the cluster with the highest number of documents as an actual cluster, and 5. Repeat steps 1 -4 on the remaining dataset until there are no more documents to We added duplicate detection at the first step of the QT algorithm. At that point, docu-ments that have a high similarity with D are considered duplicates, and marked accord-ingly.

QT derives its name from the threshold that was introduced at step 2. This threshold ensures that all the documents in one cluster have a certain similarity between them, allowing the clusters to satisfy a quality criteria. A low threshold results in clusters with less documents, but closely related. Increasin g the threshold implies larger clusters, but less homogenous.

QT clustering has several advantages over the most widely used clustering algorithm, k-means [1]. First, all the clusters returned by QT pass the quality threshold. QT leaves the documents that don X  X  fit in any cluster unclassified, while k-means must cluster all the documents in the dataset. Second, QT produces the same clusters every time it is run, while k-means may yield different re sults at each run. Third, QT does not need to know the number of clusters to be produced a priori.
 The major disadvantage of QT clustering is its high processing capacity requirement. Having O( n 2 ) complexity, QT is more computationally intensive than other clustering algorithms, specifically k-means. This makes it rather unfeasible for large input data.
However, our implementation of the algorithm includes some optimizations that makes it significantly less time consuming. Instead of computing the cosines between each document, only a small percentage of th em are actually calculated. This is done by examining the words that any two documents have in common, and their weights. Additionally, we have managed to make a part of the algorithm incremental. The cosine similarity between two documents is not co mputed at each run, but instead is computed once and stored in a cache.

These optimizations allow QT clustering to scale to the number of news items that we currently process. But, in the future, if we want to increase the number of sources or the size of the covered time span, this im plementation might reach its limits. A subject of future research is designing and developing a distributed algorithm for QT clustering. Although successful tests have been made w ith multiple instances of the same com-ponent run on different machines, the tests presented in this section were made using one instance of each component. This decision was made due to the reduced number of machines available for testing. 6.1 General Results The current implementation of the system uses 5000 English news websites as the Hub list. The system has been running for approximately 2 months. In this time, more than 8 million articles were gathered. Figure 2 shows the number of news articles found each day. It can be seen that the chart has a certain cyclicity, caused by the fact that less news are crawled in weekends than during weekdays. The average amount of news crawled each day is around 110,000, but in certain days it peaks at over 150,000.

From the chart, it can be observed that the system gathers between 4000 and 5000 news/hour, or an average of 1.25 items/sec. We will show that this value is limited by the number of sources available, and not by the throughput capacity of Newistic.
To obtain the throughput of the system, we conducted another test, in lab condi-tions, as opposed to the previous one, which was performed in real-life conditions. A random news article was chosen from a website and stored locally on the computer where the Hub Crawler component is installed. Afterwards, 5000 hubs were created artificially, each containing URLs that were different, but linked to the same article downloaded previously. This way, bandwidth and number of sources don X  X  represent an issue. The system gathered around 18,000 news each hour, resulting in an average speed of 5 items/sec. This proves that currently the system is limited by the number of news sources, and not by its processing speed.

Table 1 shows some statistics for the news gathered by Newistic over a period of 26 days, between June 21st and July 17th 2008. It shows the total number of new links found, the number of HTML pages actually downloaded, the number of pages that were considered news articles, and the number of pages that were found as not being in the English language. It can be seen that less than 50. 6.2 Clustering results For clustering, we have developed a series of tests performed on the Reuters RCV1 [10] corpus. We have run the process several times, varying the number of news articles to be clustered. The tests have been ex ecuted on a machine with a 2.4 GHz quad-core processor and 4 GB RAM. Results are shown in Table 3. The table shows the duration of the clustering starting from the moment when all the documents have been read, and the Term Frequency matrix has been constructed. We have decided to exclude the reading process because it depends on factors that h ave nothing in common with the clustering process itself. An important aspect that can be observed from Table 3 is that the number of cosines computed by our optimized implementation is significant less than the number of cosines that should otherwise be computed, by a factor of around 260. Newistic is designed to be a basic platform on which more advanced data mining re-search can be carried out. After solving rem aining issues such as index sharding and distributed clustering, we want to tackle more high level functions like disambiguation, personalization and sentiment analysis.

The only component that we need to scale out is the text clustering module. The algorithm implemented by us is difficult to fully parallelize and distribute but we are currently working on this issue.

