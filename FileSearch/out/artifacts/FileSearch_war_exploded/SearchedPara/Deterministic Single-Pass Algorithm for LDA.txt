 Huge quantities of text data such as news articles and blog posts arrives in a continuous stream. Online learning has at-tracted a great deal of attention as a useful method for han-dling this growing quantity of streaming data because it pro-cesses data one at a time, whereas batch algorithms are not feasible in these settings because they need all the data at the same time. This paper focus on online learning for La-tent Dirichlet allocation (LDA) (Blei et al., 2003), which is a widely used probabilistic model for text data.
 Online learning for LDA has been already developed (Baner-jee and Basu, 2007; Alsumait et al., 2008; Canini et al., 2009; Yao et al., 2009). Existing studies were based on sampling methods such as the incremental Gibbs sampler and particle filter. Sampling methods seem to be inappropriate for stream-ing data because sampling methods have to represent a pos-terior by using a lot of samples, which basically needs much time. Moreover, sampling algorithms often need a resampling V and m , so the required memory size is O ( m T V ) .
 for LDA.
 rithm for online learning. Section 4 presents the experimental results. V is the vocabulary size. N document j . z is a Dirichlet distribution.  X  multinomial distribution, and represents the topic distribution of document j .  X  parameter a V -dimensional probability where  X  1 , , M ) .
 LDA assumes the following generative process. For each of the T topics t , draw  X   X  For each of the N p ( w j z j,i ,  X  ) where p ( w = v j z = t,  X  ) =  X  t,v . That is to say, the complete-data likelihood of a document w 2.1 Variational Bayes Inference for LDA over z = f z where  X  and  X  are variational parameters,  X  is topic t , and  X  i.e., q (  X  The log-likelihood of documents is lower bounded introducing q ( z ,  X  ) by The parameters are updated as where n the gamma prior G (  X  where  X  Algorithm 1 has the VB inference scheme of LDA. 2.2 Collapsed Variational Bayes Inference for LDA given by where  X -j,i X  denotes subtracting  X  algorithm. 3.1 Incremental Learning general unsupervised-learning, we estimate sufficient statistics s sufficient statistics  X  (= learning, for each data i , we estimate s algorithm processes data i by subtracting old s old The incremental algorithm needs to store old statistics f s old incremental algorithm sometimes converge faster than batch algorithms. 3.2 Incremental Learning for LDA VB-LDA. Statistics f n using the reverse EM (REM) algorithm (Minka, 2001) as follows: p ( w j j  X  ,  X  ) = Equation (9) is derived from Jensen X  X  inequality as follows. log x q ( x ) log Therefore, the lower bound for the log-likelihood is given by The maximum of ^ F [ q ( z )] with respect to q ( z  X  , however, we do not use  X  1 to avoid  X  1 + Eq.(12) due to marginalizing out of  X  updates for  X  and  X  , and incremental updates for  X  statistics n optimizes the lower bound in Eq.(11). 3.3 Single-Pass Algorithm for LDA x  X  f  X  + 4 iterations where  X  (0) the document for the next document as follows, and finally discard the document. Since the updates are repeated within a document, we need to store statistics f  X  in a document, but not for all words in all documents.  X  to process a streaming text. However, unlike parameter  X  We consider Eq.(5) to be the expectation of  X  prior G (  X  We regard a actually update are ~ a prior parameters a by where ~ a (0) ~ a 3.4 Analysis We eventually update parameters  X  ( j ) and  X  ( j ) given document j as where n indicate that  X   X  In our update of  X  , the appearance rate of word v in topic t in document j , n to old parameter  X  ( j 1) future work.
 Theorem 1. If  X  and  X  exist satisfying 0 &lt;  X  &lt; S satisfies Note that  X   X  to the test set. Stop words were eliminated in datasets. the particle filter for LDA used in Canini et al. (2009). We set  X  2 set.
 terms of perplexities, however, the performance of sREM is close to that of iREM sREM is O ( L are 1.2 hours in AP and 1.3 hours in WSJ. perplexity and the number of iterations within a document, i.e., l .
Conference on Data Mining , 0:3 X 12, 2008. ISSN 1550-4786. unsupervised learning. In SIAM International Conference on Data Mining , 2007. Research , 3:993 X 1022, 2003.
 algorithm for multinomial models, 2004.
 Statistics , 2009.
 pages 400 X 407, 1951.
 2000. URL http://research.microsoft.com/ minka/papers/dirichlet/ minka-dirichlet.pdf .

Microsoft, 2001. URL http://research.microsoft.com/en-us/um/people/ minka/papers/rem.html .
 //citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.33.2557 .
Neural Computation , 12(2):407 X 432, 2000. URL http://citeseerx.ist.psu.edu/ viewdoc/summary?doi=10.1.1.37.3704 .
 2007.
 Neural Information Processing Systems 22 , pages 1973 X 1981. 2009.
USA, 2009. ACM. ISBN 978-1-60558-495-9.
