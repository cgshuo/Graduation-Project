 Mohammad Ghavamzadeh, Alessandro Lazaric, Odalric-Ambrym Maillard, R  X  emi Munos Least-squares temporal difference (LSTD) learning [3, 2] is a widely used reinforcement learning (RL) algorithm for learning the value function V  X  of a given policy  X  . LSTD has been successfully applied to a number of problems especially after the development of the least-squares policy iteration (LSPI) algorithm [9], which extends LSTD to control problems by using it in the policy evaluation step of policy iteration. More precisely, LSTD computes the fixed point of the operator  X  T  X  , where T  X  is the Bellman operator of policy  X  and  X  is the projection operator onto a linear function space. The choice of the linear function space has a major impact on the accuracy of the value function estimated by LSTD, and thus, on the quality of the policy learned by LSPI. The problem of finding the right space, or in other words the problems of feature selection and discovery, is an important challenge in many areas of machine learning including RL, or more specifically, linear value function approximation in RL.
 To address this issue in RL, many researchers have focused on feature extraction and learning. Mahadevan [13] proposed a constructive method for generating features based on the eigenfunctions of the Laplace-Beltrami operator of the graph built from observed system trajectories. Menache et al. [16] presented a method that starts with a set of features and then tunes both features and the weights using either gradient descent or the cross-entropy method. Keller et al. [7] proposed an algorithm in which the state space is repeatedly projected onto a lower dimensional space based on the Bellman error and then states are aggregated in this space to define new features. Finally, Parr et al. [17] presented a method that iteratively adds features to a linear approximation architecture such that each new feature is derived from the Bellman error of the existing set of features. A more recent approach to feature selection and discovery in value function approximation in RL is to solve RL in high-dimensional feature spaces . The basic idea here is to use a large number of fea-tures and then exploit the regularities in the problem to solve it efficiently in this high-dimensional space. Theoretically speaking, increasing the size of the function space can reduce the approxima-tion error (the distance between the target function and the space) at the cost of a growth in the estimation error. In practice, in the typical high-dimensional learning scenario when the number of features is larger than the number of samples, this often leads to the overfitting problem and poor prediction performance. To overcome this problem, several approaches have been proposed includ-ing regularization. Both ` 1 and ` 2 regularizations have been studied in value function approximation in RL. Farahmand et al. presented several ` 2 -regularized RL algorithms by adding ` 2 -regularization to LSTD and modified Bellman residual minimization [4] as well as fitted value iteration [5], and proved finite-sample performance bounds for their algorithms. There have also been algorithmic work on adding ` 1 -penalties to the TD [12], LSTD [8], and linear programming [18] algorithms. In this paper, we follow a different approach based on random projections [21]. In particular, we study the performance of LSTD with random projections (LSTD-RP). Given a high-dimensional linear space F , LSTD-RP learns the value function of a given policy from a small (relative to the dimension of F ) number of samples in a space G of lower dimension obtained by linear random projection of the features of F . We prove that solving the problem in the low dimensional random space instead of the original high-dimensional space reduces the estimation error at the price of a  X  X ontrolled X  increase in the approximation error of the original space F . We present the LSTD-RP algorithm and discuss its computational complexity in Section 3. In Section 4, we provide the finite-sample analysis of the algorithm. Finally in Section 5, we show how the error of LSTD-RP is propagated through the iterations of LSPI. For a measurable space with domain X , we let S ( X ) and B ( X ; L ) denote the set of probability measures over X and the space of measurable functions with domain X and bounded in absolute value by 0 &lt; L &lt;  X  , respectively. For a measure  X   X  S ( X ) and a measurable function f : X  X  R , we define the ` 2 (  X  ) -norm of f as || f || 2  X  = R f ( x ) 2  X  ( dx ) , the supremum norm of f as || f ||  X  = sup x  X  X  | f ( x ) | , and for a set of n states X 1 ,...,X n  X  X the empirical norm of f as We consider the standard RL framework [20] in which a learning agent interacts with a stochastic environment and this interaction is modeled as a discrete-time discounted Markov decision process (MDP). A discount MDP is a tuple M =  X  X  , A ,r,P, X   X  where the state space X is a bounded closed subset of a Euclidean space, A is a finite ( |A| &lt;  X  ) action space, the reward function r : X X A X  R is uniformly bounded by R max , the transition kernel P is such that for all x  X  X and a  X  A , P (  X | x,a ) is a distribution over X , and  X   X  (0 , 1) is a discount factor. A deterministic policy  X  : X  X  X  is a mapping from states to actions. Under a policy  X  , the MDP M is reduced to a Markov chain M  X  =  X  X  ,R  X  ,P  X  , X   X  with reward R  X  ( x ) = r x, X  ( x ) , transition kernel P  X  (  X | x ) = P  X  | x, X  ( x ) , and stationary distribution  X   X  (if it admits one). The value function of a policy  X  , V  X  , is the unique fixed-point of the Bellman operator T  X  : B ( X ; V max = R max 1  X   X  )  X  B ( X ; V max ) defined the unique fixed-point of the optimal Bellman operator T  X  : B ( X ; V max )  X  B ( X ; V max ) defined operator at threshold V max , i.e., if | f ( x ) | &gt; V max then T ( f )( x ) = sgn f ( x ) V max . To approximate a value function V  X  X  ( X ; V max ) , we first define a linear function space F spanned where  X  (  X  ) =  X  1 (  X  ) ,..., X  D (  X  ) &gt; is the feature vector. We define the orthogonal projection of V onto the space F w.r.t. norm  X  as  X  F V = arg min f  X  X  || V  X  f ||  X  . From F we can gener-be a random matrix whose elements are drawn i.i.d. from a suitable distribution, e.g., Gaussian N (0 , 1 /d ) . Similar to the space F , we define the orthogonal projection of V onto the space G w.r.t. norm  X  as  X  G V = arg min g  X  X  || V  X  g ||  X  . Finally, for any function f  X   X  F , we define m ( f  X  ) = ||  X  || 2 sup x  X  X  ||  X  ( x ) || 2 . The objective of LSTD with random projections (LSTD-RP) is to learn the value function of a given policy from a small (relative to the dimension of the original space) number of samples in a low-dimensional linear space defined by a random projection of the high-dimensional space. We show that solving the problem in the low dimensional space instead of the original high-dimensional space reduces the estimation error at the price of a  X  X ontrolled X  increase in the approximation error. In this section, we introduce the notations and the resulting algorithm, and discuss its computational complexity. In Section 4, we provide the finite-sample analysis of the algorithm.
 We use the linear spaces F and G with dimensions D and d ( d &lt; D ) as defined in Section 2. Since in the following the policy is fixed, we drop the dependency of R  X  , P  X  , V  X  , and T  X  on  X  and simply use R , P , V , and T . Let { X t } n t =1 be a sample path (or trajectory) of size n generated by the Markov chain M  X  , and let v  X  R n and r  X  R n , defined as v t = V ( X t ) and r t = R ( X t ) , be the value and defined at these n states and G n = {  X   X  |  X   X  R d }  X  R n be the corresponding vector space. We denote by b  X  G : R n  X  X  n the orthogonal projection onto G n , defined by b  X  G y = arg min z  X  X  z || n , where || y || 2 n = 1 n P {  X   X  |  X   X  R D } as b  X  F y = arg min z  X  X  b  X  F y exist and are unique.
 We consider the pathwise-LSTD algorithm introduced in [11]. Pathwise-LSTD takes a single tra-jectory { X t } n t =1 of size n generated by the Markov chain as input and returns the fixed point of the empirical operator b  X  G b T , where b T is the pathwise Bellman operator defined as b T y = r +  X  b Py . The operator b P : R n  X  R n is defined as ( b Py ) t = y t +1 for 1  X  t &lt; n and ( b Py ) n = 0 . As shown in [11], b T is a  X  -contraction in ` 2 -norm, thus together with the non-expansive property of b  X  G , it guarantees the existence and uniqueness of the pathwise-LSTD fixed point  X  v  X  R n ,  X  v = b  X  G b T  X  v . Note that the uniqueness of  X  v does not imply the uniqueness of the parameter  X   X  such that  X  v =  X   X   X  . Figure 1 contains the pseudo-code and the computational cost of the LSTD-RP algorithm. The total computational cost of LSTD-RP is O ( d 3 + ndD ) , while the computational cost of LSTD in the high-dimensional space F is O ( D 3 + nD 2 ) . As we will see, the analysis of Section 4 suggests that the value of d should be set to O ( O ( n 3 / 2 D ) , which is better than O ( D 3 ) , the cost of LSTD in F when n &lt; D (the case considered in this paper). Note that the cost of making a prediction is D in LSTD in F and dD in LSTD-RP. In this section, we report the main theoretical results of the paper. In particular, we derive a per-formance bound for LSTD-RP in the Markov design setting, i.e., when the LSTD-RP solution is compared to the true value function only at the states belonging to the trajectory used by the al-gorithm (see Section 4 in [11] for a more detailed discussion). We then derive a condition on the number of samples to guarantee the uniqueness of the LSTD-RP solution. Finally, from the Markov design bound we obtain generalization bounds when the Markov chain has a stationary distribution. 4.1 Markov Design Bound Theorem 1. Let F and G be linear spaces with dimensions D and d ( d &lt; D ) as defined in Section 2. Let { X t } n t =1 be a sample path generated by the Markov chain M  X  , and v,  X  v  X  R n be the vectors whose components are the value function and the LSTD-RP solution at { X t } n t =1 . Then for any  X  &gt; 0 , whenever d  X  15 log(8 n/ X  ) , with probability 1  X   X  (the randomness is w.r.t. both the random sample path and the random projection),  X  v satisfies where the random variable  X  n is the smallest strictly positive eigenvalue of the sample-based Gram ( b  X  F v ) t for 1  X  t  X  n .
 Before stating the proof of Theorem 1, we need to prove the following lemma.
 Lemma 1. Let F and G be linear spaces with dimensions D and d ( d &lt; D ) as defined in Section 2. Let { X i } n i =1 be n states and f  X   X  F . Then for any  X  &gt; 0 , whenever d  X  15 log(4 n/ X  ) , with probability 1  X   X  (the randomness is w.r.t. the random projection), we have Proof. The proof relies on the application of a variant of Johnson-Lindenstrauss (JL) lemma which states that the inner-products are approximately preserved by the application of the random matrix m ( f  X  ) with high probability. From this result, we deduce that with probability 1  X   X  Proof of Theorem 1. For any fixed space G , the performance of the LSTD-RP solution can be bounded according to Theorem 1 in [10] as with probability 1  X   X  0 (w.r.t. the random sample path). From the triangle inequality, we have The equality in Eq. 4 comes from the fact that for any vector g  X  G , we can write || v  X  g || 2 n = probability 1  X   X  00 (w.r.t. the choice of A ), we have We conclude from a union bound argument that Eqs. 3 and 5 hold simultaneously with probability at least 1  X   X  0  X   X  00 . The claim follows by combining Eqs. 3 X 5, and setting  X  0 =  X  00 =  X / 2 . Remark 1. Using Theorem 1, we can compare the performance of LSTD-RP with the performance of LSTD directly applied in the high-dimensional space F . Let  X  v be the LSTD solution in F , then up to constants, logarithmic, and dominated factors, with high probability,  X  v satisfies By comparing Eqs. 1 and 6, we notice that 1) the estimation error of  X  v is of order O ( p d/n ) , and thus, is smaller than the estimation error of  X  v , which is of order O ( p D/n ) , and 2) the approximation m ( b  X  F v ) and decreases with d , the dimensionality of G , with the rate O ( p 1 /d ) . Hence, LSTD-RP may have a better performance than solving LSTD in F whenever this additional term is smaller than the gain achieved in the estimation error. Note that m ( b  X  F v ) highly depends on the value function V that is being approximated and the features of the space F . It is important to carefully tune the value of d as both the estimation error and the additional approximation error in Eq. 1 depend on d . For instance, while a small value of d significantly reduces the estimation error (and the need for samples), it may amplify the additional approximation error term, and thus, reduce the advantage of LSTD-RP over LSTD. We may get an idea on how to select the value of d by optimizing the bound Therefore, when n samples are available the optimal value for d is of the order O ( value of d in Eq. 7, we can rewrite the bound of Eq. 1 as (up to the dominated term 1 /n ) Using Eqs. 6 and 8, it would be easier to compare the performance of LSTD-RP and LSTD in space for the case of D =  X  to Section 4.3 of this paper.
 Remark 2. As discussed in the introduction, when the dimensionality D of F is much bigger than the number of samples n , the learning algorithms are likely to overfit the data. In this case, it is reasonable to assume that the target vector v itself belongs to the vector space F n . We state this condition using the following assumption: Assumption 1. (Overfitting). For any set of n points { X i } n i =1 , there exists a function f  X  F such that f ( X i ) = V ( X i ) , 1  X  i  X  n .
 Assumption 1 is equivalent to require that the rank of the empirical Gram matrices 1 n  X  &gt;  X  to be bigger than n . Note that Assumption 1 is likely to hold whenever D n , because in this case we bigger than n (e.g., if the features are linearly independent on the samples, it is sufficient to have D  X  n ). Under Assumption 1 we can remove the empirical approximation error term in Theorem 1 and deduce the following result.
 Corollary 1. Under Assumption 1 and the conditions of Theorem 1, with probability 1  X   X  (w.r.t. the random sample path and the random space),  X  v satisfies 4.2 Uniqueness of the LSTD-RP Solution While the results in the previous section hold for any Markov chain, in this section we assume that the Markov chain M  X  admits a stationary distribution  X  and is exponentially fast  X  -mixing with and 8.3 in [10] for a more detailed definition of  X  -mixing processes). As shown in [11, 10], if  X  exists, it would be possible to derive a condition for the existence and uniqueness of the LSTD solution depending on the number of samples and the smallest eigenvalue of the Gram matrix defined now discuss the existence and uniqueness of the LSTD-RP solution. Note that as D increases, the smallest eigenvalue of G is likely to become smaller and smaller. In fact, the more the features in F , the higher the chance for some of them to be correlated under  X  , thus leading to an ill-conditioned matrix G . On the other hand, since d &lt; D , the probability that d independent random combinations of  X  i lead to highly correlated features  X  j is relatively small. In the following we prove that the smallest eigenvalue of the Gram matrix H  X  R d  X  d , H ij = R  X  i ( x )  X  j ( x )  X  ( dx ) in the random space G is indeed bigger than the smallest eigenvalue of G with high probability.
 Lemma 2. Let  X  &gt; 0 and F and G be linear spaces with dimensions D and d ( d &lt; D ) as defined in Section 2 with D &gt; d + 2 p 2 d log(2 / X  ) + 2 log(2 / X  ) . Let the elements of the projection matrix A be Gaussian random variables drawn from N (0 , 1 /d ) . Let the Markov chain M  X  admit a stationary distribution  X  . Let G and H be the Gram matrices according to  X  for the spaces F and G , and  X  and  X  be their smallest eigenvalues. We have with probability 1  X   X  (w.r.t. the random space) Proof. Let  X   X  R d be the eigenvector associated to the smallest eigenvalue  X  of H , from the definition of the features  X  of G ( H = AGA &gt; ) and linear algebra, we obtain where  X  is the smallest eigenvalue of the random matrix AA &gt; , or in other words, singular value of the D  X  d random matrix A &gt; , i.e., s min ( A &gt; ) = Note that if the elements of A are drawn from the Gaussian distribution N (0 , 1 /d ) , the elements of B are standard Gaussian random variables, and thus, the smallest eigenvalue of AA &gt; ,  X  , can be written as  X  = s 2 min ( B &gt; ) /d . There has been extensive work on extreme singular values of random matrices (see e.g., [19]). For a D  X  d random matrix with independent standard normal random variables, such as B &gt; , we have with probability 1  X   X  (see [19] for more details) From Eq. 11 and the relation between  X  and s min ( B &gt; ) , we obtain with probability 1  X   X  . The claim follows by replacing the bound for  X  from Eq. 12 in Eq. 10. The result of Lemma 2 is for Gaussian random matrices. However, it would be possible to extend this result using non-asymptotic bounds for the extreme singular values of more general random matrices [19]. Note that in Eq. 9, D/d is always greater than 1 and the term in the parenthesis approaches 1 for large values of D . Thus, we can conclude that with high probability the smallest eigenvalue  X  of the Gram matrix H of the randomly generated low-dimensional space G is bigger than the smallest eigenvalue  X  of the Gram matrix G of the high-dimensional space F .
 Lemma 3. Let  X  &gt; 0 and F and G be linear spaces with dimensions D and d ( d &lt; D ) as defined in Section 2 with D &gt; d + 2 p 2 d log(2 / X  ) + 2 log(2 / X  ) . Let the elements of the projection matrix A be Gaussian random variables drawn from N (0 , 1 /d ) . Let the Markov chain M  X  admit a stationary distribution  X  . Let G be the Gram matrix according to  X  for space F and  X  be its smallest eigenvalue. Let { X t } n t =1 be a trajectory of length n generated by a stationary  X  -mixing process with stationary distribution  X  . If the number of samples n satisfies where  X ( n,d, X  ) = 2( d + 1) log n + log e  X  + log + max { 18(6 e ) 2( d +1) ,  X   X  } , then with probability implies  X  = 0 , and the smallest eigenvalue  X  n of the sample-based Gram matrix 1 n  X  &gt;  X  satisifies Proof. The proof follows similar steps as in Lemma 4 in [10]. A sketch of the proof is available in [6].
 By comparing Eq. 13 with Eq. 13 in [10], we can see that the number of samples needed for the empirical Gram matrix 1 n  X  &gt;  X  in G to be invertible with high probability is less than that for its counterpart 1 n  X  &gt;  X  in the high-dimensional space F . 4.3 Generalization Bound In this section, we show how Theorem 1 can be generalized to the entire state space X when the Markov chain M  X  has a stationary distribution  X  . We consider the case in which the samples { X t } n t =1 are obtained by following a single trajectory in the stationary regime of M  X  , i.e., when X 1 is drawn from  X  . As discussed in Remark 2 of Section 4.1, it is reasonable to assume that the high-dimensional space F contains functions that are able to perfectly fit the value function V in any finite number n ( n &lt; D ) of states { X t } n t =1 , thus we state the following theorem under Assumption 1. Theorem 2. Let  X  &gt; 0 and F and G be linear spaces with dimensions D and d ( d &lt; D ) as defined in Section 2 with d  X  15 log(8 n/ X  ) . Let { X t } n t =1 be a path generated by a stationary  X  -mixing process with stationary distribution  X  . Let  X  V be the LSTD-RP solution in the random space G . Then under Assumption 1, with probability 1  X   X  (w.r.t. the random sample path and the random space), where  X  is a lower bound on the eigenvalues of the Gram matrix 1 n  X  &gt;  X  defined by Eq. 14 and with  X ( n,d, X  ) defined as in Lemma 3. Note that T in Eq. 15 is the truncation operator defined in Section 2.
 Proof. The proof is a consequence of applying concentration of measures inequalities for  X  -mixing || V  X  T (  X  V ) || n  X  || V  X   X  V || n , and using the bound of Corollary 1. The bound of Corollary 1 and the lower bound on  X  , each one holding with probability 1  X   X  0 , thus, the statement of the theorem (Eq. 15) holds with probability 1  X   X  by setting  X  = 3  X  0 .
 Remark 1. An interesting property of the bound in Theorem 2 is that the approximation error of V in space F , || V  X   X  F V ||  X  , does not appear and the error of the LSTD solution in the randomly projected space only depends on the dimensionality d of G and the number of samples n . However this property is valid only when Assumption 1 holds, i.e., at most for n  X  D . An interesting case here is when the dimension of F is infinite ( D =  X  ), so that the bound is valid for any number of samples n . In [15], two approximation spaces F of infinite dimension were constructed based function. In the case that the mother function is a wavelet, the resulting features, called scrambled wavelets, are linear combinations of wavelets at all scales weighted by Gaussian coefficients. As a results, the corresponding approximation space is a Sobolev space H s ( X ) with smoothness of order s &gt; p/ 2 , where p is the dimension of the state space X . In this case, for a function f  X   X  H s ( X ) , it is proved that the ` 2 -norm of the parameter  X  is equal to the norm of the function in H s ( X ) , i.e., What is important about the results of [15] is that it shows that it is possible to consider infinite norm of f  X  in F . In such cases, m ( X  F V ) is finite and the bound of Theorem 2, which does not contain any approximation error of V in F , holds for any n . Nonetheless, further investigation is needed to better understand the role of || f  X  || H s ( X ) in the final bound.
 Remark 2. As discussed in the introduction, regularization methods have been studied in solving high-dimensional RL problems. Therefore, it is interesting to compare our results for LSTD-RP with those reported in [4] for ` 2 -regularized LSTD. Under Assumption 1, when D =  X  , by selecting the features as described in the previous remark and optimizing the value of d as in Eq. 7, we obtain Although the setting considered in [4] is different than ours (e.g., the samples are i.i.d.), a quali-tative comparison of Eq. 16 with the bound in Theorem 2 of [4] shows a striking similarity in the performance of the two algorithms. In fact, they both contain the Sobolev norm of the target func-tion and have a similar dependency on the number of samples with a convergence rate of O ( n  X  1 / 4 ) (when the smoothness of the Sobolev space in [4] is chosen to be half of the dimensionality of X ). This similarity asks for further investigation on the difference between ` 2 -regularized methods and random projections in terms of prediction performance and computational complexity. In this section, we move from policy evaluation to policy iteration and provide a performance bound for LSPI with random projections (LSPI-RP), i.e., a policy iteration algorithm that uses LSTD-RP at each iteration. LSPI-RP starts with an arbitrary initial value function V  X  1  X  B ( X ; V max ) and its corresponding greedy policy  X  0 . At the first iteration, it approximates V  X  0 using LSTD-RP and Note that in general, the measure  X   X  S ( X ) used to evaluate the final performance of the LSPI-RP algorithm might be different from the distribution used to generate samples at each iteration. Moreover, the LSTD-RP performance bounds require the samples to be collected by following the policy under evaluation. Thus, we need Assumptions 1-3 in [10] in order to 1) define a lower-bounding distribution  X  with constant C &lt;  X  , 2) guarantee that with high probability a unique LSTD-RP solution exists at each iteration, and 3) define the slowest  X  -mixing process among all the mixing processes M  X  k with 0  X  k &lt; K .
 Theorem 3. Let  X  &gt; 0 and F and G be linear spaces with dimensions D and d ( d &lt; D ) as defined in Section 2 with d  X  15 log(8 Kn/ X  ) . At each iteration k , we generate a path of size n from the (  X 
V 0 ,..., in [10], with probability 1  X   X  (w.r.t. the random samples and the random spaces), we have where C  X , X  is the concentrability term from Definition 2 in [1],  X   X  is the smallest eigenvalue of the Gram matrix of space F w.r.t.  X  ,  X   X  is  X  from Eq. 14 in which  X  is replaced by  X   X  , and E is from Theorem 2 written for the slowest  X  -mixing process.
 Proof. The proof follows similar lines as in the proof of Thm. 8 in [10] and is available in [6]. Remark . The most critical issue about Theorem 3 is the validity of Assumptions 1-3 in [10]. It is important to note that Assumption 1 is needed to bound the performance of LSPI independent from the use of random projections (see [10]). On the other hand, Assumption 2 is explicitly related to random projections and allows us to bound the term m ( X  F V ) . In order for this assumption to hold, the features {  X  j } D j =1 of the high-dimensional space F should be carefully chosen so as to be linearly independent w.r.t.  X  . Learning in high-dimensional linear spaces is particularly appealing in RL because it allows to have a very accurate approximation of value functions. Nonetheless, the larger the space, the higher the need of samples and the risk of overfitting. In this paper, we introduced an algorithm, called LSTD-RP, in which LSTD is run in a low-dimensional space obtained by a random projection of the original high-dimensional space. We theoretically analyzed the performance of LSTD-RP and showed that it solves the problem of overfitting (i.e., the estimation error depends on the value of the low dimension) at the cost of a slight worsening in the approximation accuracy compared to the high-dimensional space. We also analyzed the performance of LSPI-RP, a policy iteration algorithm that uses LSTD-RP for policy evaluation. The analysis reported in the paper opens a number of inter-esting research directions such as: 1) comparison of LSTD-RP to ` 2 and ` 1 regularized approaches, and 2) a thorough analysis of the case when D =  X  and the role of || f  X  || H s ( X ) in the bound. Acknowledgments This work was supported by French National Research Agency through the projects EXPLO-RA n  X  ANR-08-COSI-004 and LAMPADA n  X  ANR-09-EMER-007, by Ministry of Higher Education and Research, Nord-Pas de Calais Regional Council and FEDER through the  X  X ontrat de projets  X  etat region 2007 X 2013 X , and by PASCAL2 European Network of Excellence. [1] A. Antos, Cs. Szepesvari, and R. Munos. Learning near-optimal policies with Bellman-residual [2] J. Boyan. Least-squares temporal difference learning. Proceedings of the 16th International [3] S. Bradtke and A. Barto. Linear least-squares algorithms for temporal difference learning. [4] A. M. Farahmand, M. Ghavamzadeh, Cs. Szepesv  X  ari, and S. Mannor. Regularized policy [5] A. M. Farahmand, M. Ghavamzadeh, Cs. Szepesv  X  ari, and S. Mannor. Regularized fitted Q-[6] M. Ghavamzadeh, A. Lazaric, O. Maillard, and R. Munos. LSPI with random projections. [7] P. Keller, S. Mannor, and D. Precup. Automatic basis function construction for approximate [8] Z. Kolter and A. Ng. Regularization and feature selection in least-squares temporal difference [9] M. Lagoudakis and R. Parr. Least-squares policy iteration. Journal of Machine Learning [10] A. Lazaric, M. Ghavamzadeh, and R. Munos. Finite-sample analysis of least-squares policy [11] A. Lazaric, M. Ghavamzadeh, and R. Munos. Finite-sample analysis of LSTD. In Proceedings [12] M. Loth, M. Davy, and P. Preux. Sparse temporal difference learning using lasso. In IEEE [13] S. Mahadevan. Representation policy iteration. In Proceedings of the Twenty-First Conference [14] O. Maillard and R. Munos. Compressed least-squares regression. In Proceedings of Advances [15] O. Maillard and R. Munos. Brownian motions and scrambled wavelets for least-squares re-[16] I. Menache, S. Mannor, and N. Shimkin. Basis function adaptation in temporal difference [17] R. Parr, C. Painter-Wakefield, L. Li, and M. Littman. Analyzing feature generation for value-[19] M. Rudelson and R. Vershynin. Non-asymptotic theory of random matrices: extreme singular [20] R. Sutton and A. Barto. Reinforcement Learning: An Introduction . MIP Press, 1998. [21] S. Vempala. The Random Projection Method . American Mathematical Society, 2004.
 Setting: RL in high-dimensional spaces  X  the number of features is bigger than the number of samples.
 Problem: Overfitting and poor prediction performance. Solutions Regularization: adding  X  1 and  X  2 regularizations to value-function approximation algorithms.
 Random projections: this work .
 LSTD: A RL alg. for learning the value function of a given policy. Algorithm  X  LSTD-RP: LSTD in a low-dim space generated by a random projection from the high-dim space.
 We present the LSTD-RP algorithm and discuss its computational complexity. less expensive than performing LSTD in the high-dim space . We provide finite-sample performance bounds for LSTD-RP. avoid overfitting  X  better prediction: smaller estimation error and not much larger approximation error .
 Uniqueness of the LSTD-RP solution.
 LSTD-RP is more stable and needs less samples to have a unique solution, compared to LSTD in the high-dim space . We show how the error of LSTD-RP is propagated through the iterations of LSPI and present a bound for LSPI-RP .
