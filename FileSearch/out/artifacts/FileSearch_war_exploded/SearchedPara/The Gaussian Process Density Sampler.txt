 University of Cambridge
Cambridge CB3 0HE, UK We present the Gaussian Process Density Sampler (GPDS), a ge nerative model for probability den-sity functions, based on a Gaussian process. We are able to dr aw exact and exchangeable data from a fixed density drawn from the prior. Given data, this generat ive prior allows us to perform infer-ence of the unnormalized density. We perform this inference by expressing the generative process in tory. The central idea of the GPDS is to allow nonparametric B ayesian density estimation where the data should have similar probabilities. X  define a distribution over the weights of the components in an infinite mixture model, using a simple introducing a spatial component to achieve an exchangeable prior on discrete or continuous density transform a simple latent distribution through a nonlinear map, as in the Density Network [2] and the Gaussian Process Latent Variable Model [3]. Here we use t he Gaussian process to define a prior on the density function itself. We consider densities on an input space X that we will call the data space . In this paper, we assume process prior with the data space X as its input and the one-dimensional real space R as its output. The Gaussian process defines a distribution over functions f rom X to R . We define a mean function m (  X  ) : X  X  R and a positive definite covariance function K (  X  ,  X  ) : X  X  X  X  R . We assume that these functions are together parameterized by a set of hyperparameters  X  . Given these two functions and their hyperparameters, for any finite subs et of X with cardinality N there is a multivariate Gaussian distribution on R N [4]. We will take the mean function to be zero. Probability density functions must be everywhere nonnegat ive and must integrate to unity. We define a map from a function g ( x ) : X  X  R , x  X  X  , to a proper density f ( x ) via normal distribution function. We use the bold notation g to refer to the function g ( x ) compactly functional of g ( x ) : Through the map defined by Equation 1, a Gaussian process prio r becomes a prior distribution over normalized probability density functions on X . Figure 2 shows several sample densities from this prior, along with sample data. We can use rejection sampling to generate samples from a comm on density drawn from the the prior described in Section 2. A rejection sampler requires a proposal density that provides an upper unnormalized density of interest is  X ( g ( x ))  X  ( x ) .
 If from the base measure  X  ( x ) . The proposal  X  x from (0 , 1) was less than  X ( g (  X  x a random function drawn from a Gaussian process prior. We can nevertheless use rejection sampling by  X  X iscovering X  g ( x ) as we proceed at just the places we need to know it, by sampling from the prior distribution of the latent function. As it is necessar y only to know g ( x ) at the { x or reject these proposals, the samples are still exact. This retrospective sampling trick has been used in a variety of other MCMC algorithms for infinite-dimen sional models [5, 6]. The generative procedure is shown graphically in Figure 2.
 In practice, we generate the samples sequentially, as in Alg orithm 1, so that we may be assured of having as many accepted samples as we require. In each loop , a proposal is drawn from the base measure  X  ( x ) and the function g ( x ) is sampled from the Gaussian process at this proposed function values G . After the function is sampled, a uniform variate is drawn fr om beneath the bound and compared to the  X  -squashed function at the proposal location.
 The sequential procedure is exchangeable, which means that the probability of the data is identical under reordering. First, the base measure draws are i.i.d.. Second, conditioned on the proposals from the base measure, the Gaussian process is a simple multi variate Gaussian distribution, which is exchangeable in its components. Finally, conditioned on the draw from the Gaussian process, the acceptance/rejection steps are independent Bernoulli samples, and the overall procedure is ex-changeable. This property is important because it ensures t hat the sequential procedure generates data from the same distribution as the simultaneous procedu re described above. More broadly, ex-changeable priors are useful in Bayesian modeling because w e may consider the data conditionally independent, given the latent density.
 Algorithm 1 Generate P exact samples from the prior We have N data D = { x ing to the unknown density. We may also wish to generate sampl es from the predictive distribution or perform hierarchical inference of the prior hyperparame ters.
 By using the GPDS prior to model the data, we are asserting tha t the data can be explained as the result of the procedure described in Section 3. We do not, how ever, know what rejections were made down with pegs is just as important as putting up poles. In den sity modeling, defining regions with little probability mass is just as important as defining the a reas with significant mass. Although the rejections are not known, the generative proce dure provides a probabilistic model that If we define a Markov chain whose equilibrium distribution is the posterior distribution over latent Such samples capture all the information available about th e unknown density, and with them we may ask additional questions about g ( x ) or run the generative procedure further to draw predictive samples. This approach is related to that described by Murra y [7], who performed inference on an exactly-coalesced Markov chain [8], and by Beskos et al. [5] .
 noted G denoted M = { x denoted G We perform Gibbs-like sampling of the latent history by alte rnating between modification of the number of rejections M and block updating of the rejection locations M and latent function val-ues G function, i.e.  X ( z ) = (1 + exp { X  z } )  X  1 . 4.1 Modifying the number of latent rejections q (  X 
M  X  M ) . If  X  M is greater than M , we must also propose new rejections to add to the la-tent state. We take advantage of the exchangeability of the p rocess to generate the new rejections: we imagine these proposals were made after the last observed datum was accepted, and our pro-opposite by proposing to move some rejections to after the la st acceptance.
 latent history. There are  X  M + N  X  1 latent history, such that the sampler terminates after the N th acceptance. When removing rejections, simplification, the proposal ratios for both addition and re moval of rejections are identical: When inserting rejections, we propose the locations of the ad ditional proposals, denoted M + , and the corresponding values of the latent function, denoted G + independent draws from the base measure. We draw G + conditioned on all of the current latent state, i.e. ( M , G state is p ( D , M , M + , G N , G M , G + M ) = The joint in Equation 3 expresses the probability of all the b ase measure draws, the values of the function draws from the Gaussian process, and the acceptanc e or rejection probabilities of the pro-posals excluding the newly generated points. When we make an insertion proposa l, exchangeability allows us to shuffle the ordering without changing the probab ility; the only change is that now we the  X  X abeling probability X  cancel. The reverse proposal is similar, however we denote the removed proposal locations as M  X  and the corresponding function values as G  X  ratios for insertions or removals are 4.2 Modifying rejection locations and function values Given the number of latent rejections M , we propose modifying their locations M , their latent func-tion values G as  X  make simple perturbative proposals of M via a proposal density q (  X  M  X  M ) . For the latent func-tion values, however, perturbative proposals will be poor, as the Gaussian process typically defines a narrow mass. To avoid this, we propose modifications to the l atent function that leave the prior invariant.
 We make joint proposals of  X  M ,  X  G from q (  X  M  X  M ) . Second, we draw a set of M intermediate function values from the Gaussian the function values at the data. Third, we propose new functi on values at  X  M and the data D via an underrelaxation proposal of the form overrelaxed MCMC method discussed by Neal [9]. This procedu re leaves the Gaussian process prior invariant, but makes conservative proposals if  X  is near one. After making a proposal, we accept or reject via the ratio of the joint distributions: 4.3 Hyperparameter inference Given a sample from the posterior on the latent history, we ca n also perform a Metropolis X  X asting step in the space of hyperparameters. Parameters  X  , governing the covariance function and mean function of the Gaussian process provide common examples of hyperparameters, but we might also introduce parameters  X  that control the behavior of the base measure  X  ( x ) . We denote the proposal and p (  X  ) , the acceptance ratio for a Metropolis X  X astings step is a = 4.4 Prediction The predictive distribution is the one that arises on the spa ce X when the posterior on the latent next datum, given the ones we have seen and taking into accoun t our uncertainty. In the GPDS we the current latent history sample from the Metropolis X  X ast ings procedure described above. Chib and Jeliazkov [10], and observe by detailed balance of a Metropolis X  X astings move: We find the expectation of each side under the posterior of g and the hyperparameters  X  and  X  :
Z This gives an expression for the predictive density: Both the numerator and the denominator in Equation 5 are expe ctations that can be estimated by averaging over the output from the GPDS Metropolis X  X asting sampler. The denominator requires sampling from the posterior distribution with the data augm ented by x . We examined the GPDS prior and the latent history inference p rocedure on a toy data set and on a skull reconstruction task. We compared the approach descr ibed in this paper to a kernel density estimate (Parzen windows), an infinite mixture of Gaussians (iMoG), and Dirichlet diffusion trees (DFT). The kernel density estimator used a spherical Gaussi an with the bandwidth set via ten-fold cross validation. Neal X  X  Flexible Bayesian Modeling (FBM) Software [1] was used for the imple-mentation of both iMoG and DFT.
 The toy data problem consisted of 100 uniform draws from a two -dimensional ring with radius 1.5, improved on the Parzen window estimate by two or more nats, wi th the DFT approach being the most successful. A bar plot of these results is shown in Figur e 5.
 We also compared the methods on a real-data task. We modeled t he the joint density of ten measure-ments of linear distances between anatomical landmarks on 2 28 rhesus macaque ( Macaca mulatta ) skulls. These linear distances were generated from three-d imensional coordinate data of anatomical landmarks taken by a single observer from dried skulls using a digitizer [11]. Linear distances are commonly used in morphological studies as they are invarian t under rotation and translation of the objects being compared [12]. Figure 5 shows a computed tomog raphy (CT) scan reconstruction of a macaque skull, along with the ten linear distances used. Ea ch skull was measured three times in different trials, and these were modeled separately. 200 ra ndomly-selected skulls were used as a transformed and whitened as a preprocessing step, to have ze ro sample mean and spherical sample covariance. Each of the Bayesian approaches outperformed t he Parzen window technique in mean flexible nonparametric Bayesian models should have roughly similar expressive capabilities. These results are shown in Figure 5.

Figure 4: The macaque skull data are linear dis-Valid MCMC algorithms for fully Bayesian kernel regression methods are well-established. This work introduces the first such prior that enables tractable d ensity estimation, complementing alter-natives such as Dirichlet Diffusion Trees [1] and infinite mi xture models.
 implemented on single-dimensional toy problems. The GPDS c onstruction we have presented here not only avoids numerical estimation of the normalization c onstant, but allows infinite-dimensional inference both in theory and in practice. 6.1 Computational complexity The inference method for the GPDS prior is  X  X ractical X  in the sense that it can be implemented without approximations, but it has potentially-steep comp utational costs. To compare two latent histories in a Metropolis X  X astings step we must evaluate th e marginal likelihood of the Gaussian process. This requires a matrix decomposition whose cost is O (( N + M ) 3 ) . The model explicitly allows M to be any nonnegative integer and so this cost is unbounded. T he expected cost of an M X  X  step is determined by the expected number of rejections M . For a given g ( x ) , the expected proportion of the mass of  X  ( x ) contained by  X ( g ( x ))  X  ( x ) .
 constant-factor performance gains over the basic Metropol is X  X asting scheme presented here, with-out compromising the correctness of the equilibrium distri bution. Sparse approaches to Gaussian process regression that improve the asymptotically cubic b ehavior may also be relevant to the GPDS, but it is unclear that these will be an improvement over other approximate GP-based schemes for density modeling. 6.2 Alternative inference methods In developing inference methods for the GPDS prior, we have a lso explored the use of exchange sampling [17, 7]. Exchange sampling is an MCMC technique explicitly d eveloped for the situation where there is an intractable normalization constant that p revents exact likelihood evaluation, but exact samples may be generated for any particular parameter setting. Undirected graphical models such as the Ising and Potts models provide common examples of cases where exchange sampling it is applicable to the GPDS as well. Exchange sampling for th e GPDS, however, requires more approach of Section 4 does perform better.
 Acknowledgements The authors wish to thank Radford Neal and Zoubin Ghahramani for valuable comments. Ryan Adams X  research is supported by the Gates Cambridge Trust. I ain Murray X  X  research is supported by the government of Canada. The authors thank the Caribbean Pr imate Research Center, the University of Puerto Rico, Medical Sciences Campus, Laboratory of Prim ate Morphology and Genetics, and the National Institutes of Health (Grant RR03640 to CPRC) fo r support.
 References
