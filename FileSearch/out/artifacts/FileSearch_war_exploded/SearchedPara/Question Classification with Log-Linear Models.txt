 Question classification has become a crucial step in modern ques-tion answering systems. Previous work has demonstrated the effec-tiveness of statistical machine learning approaches to this problem. This paper presents a new approach to building a question classi-fier using log-linear models. Evidence from a rich and diverse set of syntactic and semantic features is evaluated, as well as approaches which exploit the hierarchical structure of the question classes. Categories and Subject Descriptors: H.3.3 [ Information Search and Retrieval ].
 General Terms: Algorithms, Experimentation Keywords: Maximum entropy, Question Classification, Question Answering, Machine Learning
Research in Question Answering ( QA ) seeks to move beyond the existing keyword-based Information Retrieval ( IR ) approaches by providing one or more exact answers to a question from a large document collection. The syntactic and semantic interpretation of a question is crucial in a QA system. The most common approach to semantic interpretation is to classify the question into a closed set of question types ( qtype ) which describe the expected semantic category of the answer to the question.

Maximum Entropy ( ME ) or log-linear models [5] have been suc-cessfully applied to many Natural Language Processing ( NLP ) prob-lems which require complex and overlapping features. Here we make use of this ability to incorporate syntactic and semantic in-formation extracted from the questions. The result is a question classifier which significantly outperforms the state-of-the-art sys-tems on the standard question classification test set [4].
Conditional log-linear models, also known as Maximum Entropy models, produce a probability distribution over multiple classes and have the advantage of handling large numbers of complex overlap-ping features. These models have the following form: where the f k are feature functions of the observation x and the class label y .  X  k are the model parameters, or feature weights, and Z ( x |  X  ) is the normalisation function.
 vided unevenly into 50 fine-grained categories. The data set 1 con-sists of approximately 5,500 annotated questions for training and 500 annotated questions from TREC 10 for testing. The training questions were collected from four sources: 4,500 English ques-tions collected by Hovy et al. [2], plus 500 manually created ques-tions for rare qtypes and 894 questions from TREC 8and TREC 9. We use the data in exactly the same manner as Li and Roth [4] in their original experiments.

We conducted two sets of experiments to investigate different aspects of the QC task. The first experiments aim to evaluate the contribution of each of our proposed feature types using a stan-dard log-linear classification model, while the second experiments investigate whether the incorporation of hierarchical label informa-tion can assist the classification. Table 1 lists the feature types used by our classifier.

In evaluating our experiments we have used precision over the top n labels returned from the classifier. In this case P 1 refers to the true precision of the classifier when it is only allowed to predict one qtype for each test instance. P n refers to the precision when the classifier is allowed to return the n most probable qtypes for each instance and if the correct qtype is in these n qtypes it is counted as a correct prediction.

Table 2 shows the fine-grained results for including all features, as well as the contribution of particular groups of features: NGRAM is just the UNIGRAM , BIGRAM and TRIGRAMS , NO SEMANTIC is all the features except those that have a semantic content (any that use WordNet, named entities and the gazetteer), and NO TARGET is all the features except those that refer to the target.
From these results we can see that, in addition to the ngram fea-tures being important for fine classification, the target features also contribute significantly to the end results, while the semantic fea-tures have a more marginal impact.
As the labels employed in the current QC scheme actually encode a semantic hierarchy over answer types it makes sense to attempt to use this additional information in our classifiers. Here we propose two hierarchical classification schemes: the first is an integrated ap-proach using feature functions defined over the coarse labels, while the second is a two-stage approach employing an initial coarse clas-sifier to feed a distribution over coarse labels to a second classifier.
The integrated hierarchical classifier builds upon the standard log-linear model described in Section 2 by adding feature functions that are conditioned on only the coarse component of a label. http://l2r.cs.uiuc.edu/  X  cogcomp/Data/QA/QC/
