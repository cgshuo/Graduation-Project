 sample size and how different the aggregated users are.
 of decide which subset of the samples S dependent approaches such as Rademacher complexity.
 H observed value x . We assume that the target model f induces some underlying distribution P observations z . In the case of classification or regression, P noise). In the case of density estimation f simply defines a distribution P a model g In our multiple source model, we are presented with K distinct samples or piles of data S and a symmetric K  X  K matrix D . Each pile S fixed and unknown model f which piles S While we are interested in accomplishing this goal for each f examine the problem from the perspective of a fixed f suppose that we are given piles S  X  simply taken the problem in the preceding paragraph, focuse d on the problem for f the other models according to their proximity to f f analyze, for any k  X  K , the hypothesis  X  h S , . . . , S k , i.e. where n first k piles of data as recommended number of piles to incorporate when learning f = f provided for a variety of natural learning problems.
 expected loss function e if for all g The parameter  X   X  1 is a constant that depends on F and e .
 Our results will require only that the unknown source models f simply leave F as a parameter of the definition.
 where  X  e ( h ) = 1 distributions P function of the number of observations n and the confidence  X  , and depends on H and L . generalizing the standard uniform convergence results to t his setting is straightforward. We are now ready to present our general bound.
 which there is a uniform convergence bound  X  for L . Let K  X  N , f = f { n for any k  X  X  1 , . . . , K } models whose data is used with respect to the target model f = f and suggest a natural choice for the number of piles k  X  to use to estimate the target f : Summing over all i  X  X  1 , . . . , k } , we find e ( f, h )  X  summation involving the f first summation is a weighted average of the expected loss of e ach f high probability e Putting these pieces together, we find that with high probabi lity 4.1 Binary classification f and 1 otherwise, and the corresponding expected loss is e ( g Pr x  X  P [ g 1 ( x ) 6 = g 2 ( x )] of Theorem 1 are thus easily satisfied, yielding the followin g. dimension of H  X  F . Let e be the expected 0/1 loss. Let K  X  N , f = f {  X  that 0 &lt;  X  &lt; 1 , with probability at least 1  X   X  , for any k  X  X  1 , . . . , K } classification problem. 4.2 Regression expected loss is thus e ( g Lemma 1 Given any three functions g the inputs X , and the expected loss e ( g (Technically we ask whether {h h ( x the following. A sketch of the proof appears in Appendix A.
 F , {  X  e ( f,  X  h k )  X  6 4.3 Density estimation The expected loss is then e ( P source setting which we illustrate using the example of mult inomial distributions. P For all distributions we consider it will hold that E of the same family (which use the same functions F and  X  ) and obtain tors F and its first-order Taylor expansion about divergences and to [9] for more information about exponenti al families. Lemma 2 Let e be the expected log loss, i.e. e ( P probability distributions P some  X   X  1 then e ( P Lemma 3 Let P function F . Then where  X  = 2 sup and lowest eigenvalues of a given matrix, and H ( ) is the Hessian matrix. dimension of H under log loss, and let e be the expected log loss. Let K  X  N , f = f F , {  X  n 1  X  d/ 16e analogs can be derived in a similar manner for other learning problems. If plexity of H on a fixed set of observations x where the expectation is taken over independent uniform { X  1 } -valued random variables  X  The Rademacher complexity for n observations is then defined as R expectation is over x Theorem 5 Let F be the set of all functions from an input set X into { -1,1 } and let  X  R loss. Let K  X  N , f = f source learning model. Assume that n at least 1  X   X  , for any k  X  X  1 , . . . , K } References
