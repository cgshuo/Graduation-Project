 Topic Detection and Tracking (TDT) [1] refers to a variety of automatic techniques for discovering and threading together topically related materials in streams of data such as newswire or broadcast news. As a core of TDT, story link detection is the task of determining whether two stories are about the same topic. TDT defines  X  X opic X  to mean a specific event or activity plus directly related events or activities. An event is "something that happens at some specific time and place". 
A number of works have been developed on story link detection, which can be classified into two categories: vector-based methods and probabilistic-based methods. 
As the vector-based approaches are widely used in IR and Text Classification research, cosine similarity between documents vectors with tf * idf term weighting[2,3] has also been one of the best methods for link detection. We have also examined a number of similarity measures in the link detection task, including weighted sum, language modeling, Kullback-Leibler divergence, Hellinger and Tanimoto, and find that cosine similarity produces the most outstanding results. Furthermore, [4] also confirms this conclusion in its story link detection research. Probabilistic-based methods have been proved to be very effective in several IR applications. One of their attractive features is that it is firmly rooted in the theory of probability, thereby allows the researcher to explore more sophisticated m odels guided by the theoretical framework. [5,6] both apply probability-based models (language model or relevance model ) to story link detection, and the experimental results indicate that the performances are comparable with those using traditional vector space models, if not better. The story models are all vector-based in this paper. We have concluded in our previous research that the multi-vector model is superior to the single-vector model for news stories. So a multi-vector model is used as the baseline model. However, we know that a story usually describes an event mainly constructed with time, location, person, organization, etc. So a new event model is proposed to represent the news story. The experimental results show that the event model is more proper for stories in TDT. 
The paper is organized as follows: Section 2 simply describes the procedure for story link detection; Section 3 describes the baseline multi-vector model and the event model, which share preprocessing, feature weighting, similarity computation and multi-similarity integration except model construction. The experimental results and analysis are given in Section 4; finally, Section 5 concludes the whole paper. In story link detection, a system is given by a sequence of time-ordered news source 1  X  j  X  k  X  n . The link system is required to make decisions on each story pair to judge if follows. For current story pair P i = ( s i 1 , s i 2 ): 1. Get background corpus B i of P i . Normally, according to the supposed application 3. Choose a similarity function F and compute the similarity between two models. First of all, a preprocessing is provided fo r all the stories. For each story we tokenize the text, tag the tokens, remove stop words, and get a candidate set of features for its tokens with same spelling are tagged with different tags, they will be taken as different features. The segmenter and tagger are completed by ICTCLAS 1 . The stop word list contains 507 words. 3.1 Model Construction In the baseline model, we divide the feature set into disjoint subsets according to the picked out to represent the story. The ten corresponding tags are person name, location name, organization name, number, time, noun (including noun and proper noun), verb (including verb, associate verb and nounness verb), adverb, adjective (including adjective, associate adjective and nounness adjective), idiom (including idiom and phraseology). We think that those tokens, which are tagged with any of these ten tags, have comparably more information. So the baseline multi-vector model is actually a ten-vector model. 
However, we know that the research objects in TDT are news stories, not usual documents. Stories have their own features besides the usual characters. For example, almost each story describes an event. The time, location, person, and so on, compose the framework of an event. And also the first few sentences often summarize the event and the rest sentences explain what the event is about in detail. Story link based. Therefore the representation model for stories should reflect the events described in them from both content and structure. The event frame maybe a more proper and natural partition standard for a multi-vector model to represent a story. In this paper we build a new multi-vector model for a story according to the event framework. We call it event model. The later experimental results also verify that just gets much improvement to the performance of story link detection and has also a larger potentiality for improvement than the baseline model. 
The main difference between the event model and the baseline model is the partition standard of the feature set. The baseline model is built according to the tags while the event model is built according to the event framework. The primary component elements are time, number, pers on, location, organization, abstract and content description in our event framework. The first five will be the same as those in the baseline model. The features in the abstract vector and the content description vector are tokens with their tags individually occurring in the first two sentences and the rest sentences in a story, while they do not belong to the former five kinds of tokens. It is because we find that the first two sentences in a story usually summarize the story and the rest sentences explain what happened in detail. According to this splitting standard, the feature set will be split into seven subsets. 3.2 Other Common System Components Firstly, the weights of the features in above two models are all based on the tf*idf form. Furthermore, it is dynamic, which lies in the dynamic computation of some since more story pairs are processed, more source files could be seen, and the story background corpus is bigger. Whenever the size of the story background has changed, the values of some certain parameters will update accordingly. We call this incremental tf*idf weighting. 
Secondly, another important issue is to determine the right function to measure similarity between two corresponding sub-vectors. The critical property of the similarity function is its ability to separate vectors describing the same information from vectors on different information. We consider the classical cosine function in this paper. This measure is simply an inner product of two vectors, where each vector is normalized to unit length. It represents cosine of the angle between two vectors. 
The last important step for the baseline model and the event model is to integrate multiple similarities into a single value to decide whether two stories are topically linked. We do this with a machine learning classifier SVM in this paper. It is because SVM has a good generalization property and has been shown to be a competitive vector where the features are similarities between two corresponding subvectors and the topically link label in each vector. The generated model is then used to automatically decide whether two stories in a new pair are linked. We use the experiments. In addition to making a decision for whether two stories are linked, SVM also produces a value as the measure of confidence, which is served as input to the evaluation software. 
We also notice that the numbers of positive and negative examples in the training data are very different. But the usual SVM treats positive and negative data equally, which may result in poor performance when applying to very unbalanced detection problems. [8] presents a method to solve this problem, where the cost factor for positive examples is distinguished from the cost factor for negative examples to adjust the cost of false positive vs. false negative. This approach is implemented by the SVMlight, in which an optional parameter j (= C + / C -) is provided to control different weightings of training errors on positive examples to errors on negative examples. Therefore, we also integrate a set of similarities with uneven SVMlight in this paper. We use the Chinese subset of TDT4 corpus in this paper. There are totally 12334 story pairs extracted for our experiments. The answers for these pairs are based on 28 topics in TDT 2003 evaluation. The first 9000 pairs are used for training. The rest 3334 pairs are used for testing. The goal of link detection is to minimize the detection cost ( C det ) due to errors caused by the system, which is a function of the miss less than one. The detailed explanation can refer to [1]. 4.1 Experimental Results To verify the effectiveness of the event model and the uneven SVM strategy, we have designed four story link detection systems: System1 uses the baseline model with even SVMlight, System2 uses the baseline model with uneven SVMlight, System3 uses the event model with even SVMlight, and System4 uses the event model with uneven SVMlight. All the systems are implemented according to the procedure introduced in the section of problem definition. When the generated SVM classify model is used to automatically decide whether two stories in a new pair are linked, the default optimum threshold is zero. The following table shows the topic-weighted experimental results of these systems. 
From the table we can see that story link detection using the event model has a lower detection cost than that using the baseline model whenever using even or uneven SVM. This may be because the event model does no discriminate those features which are not named entities (person, location, organization, time, number) and just splits them into abstract and content description vectors. On one hand it avoids the cost loss caused by exact matching when comparing two sub vectors. On the other hand it reflects the character that the first two sentences in a story are usually summary of what happened and the rest are explanation of what happened. 
Relative to even SVM, uneven SVM has made a notable decrement in P miss while a little increment in P fa . This is because uneven SVM is completed under the principle have verified through experiments that the event model is always superior to the baseline model no matter what the optional parameter j is. So we think that the event model is more appropriate to represent news stories in TDT. 
Although the event model has got a comparable lower detection cost, we should also notice that the information of relation between features in event model has not yet been abstracted and used. Only using information of tokens and tags may be insufficient. If we could get the relation between the event in a story and the seminal event of its corresponding topic, we should be able to make the right decision at a larger confidence. We will try to exploit relation information and use them properly in our future work. Story link detection is a key technology in TDT research. Though many models have been used, few of them are specific to stor ies in TDT. After analyzing the characters of stories in TDT corpora, this paper proposes a new event model. It represents the events in a story according to the event framework. The experimental results indicate that story link detection using this model can achieve a better performance than that using the baseline model, especially when using the uneven SVM integration strategy. So we think the event model is more proper for TDT stories. However, we realize that the event model is only used in a simple way in this paper, just looking it as a multi-vector model. How to mine and utilize the relation information between features in the event model will be our future work. This research is supported by the National Natural Science Foundation of China (60403050), Program for New Century Excellent Talents in University (NCET-06-0926) and the National Grand Fundamental Research Program of China (2005CB321802). 
