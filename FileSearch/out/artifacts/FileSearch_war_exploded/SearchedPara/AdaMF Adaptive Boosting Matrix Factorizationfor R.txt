 Recommender systems are widely adopted by online websites to discover the im-plicit needs of consumers. In recent yea rs, existing approaches have focused on the rating prediction for users on unobserved items, in which Root Mean Square Error (RMSE) is usually chosen as the per formance evaluation metric. We refer these approaches as rating-oriented recommender systems. One of the popular approaches for solving this problem is Matrix Factorization (MF) [1] which can also be called Latent Factor Model. However in real recommender systems, in-stead of rating predictio n performance, top-N reco mmendation precision may attract more attention as it generates a ran ked list which is directly presented to users. The uncertainty of the predictio n in rating-oriented recommender system brings instability in the ranking performance. Moreover, other factors such as the position information that measure the ranking performance are not taken in to consideration in rating-oriented recommender systems. Paolo et.al show that lower RMSE does not always translate into a better ranking result in [2]. Thus, instead of achieving better rating predict ion, directly optimizing the relevance of the ranking list can offer user better exp erience. In recent years, Learning to Rank (LTR) [3] has becoming a powerful technique to solve ranking problem by using machine learning. For existing LTR-based recommender systems [4,5], in practice, the single model cannot be adjusted adaptively to the shifty real world data and the ranking performance evaluation metrics, such as NDCG, are not usually taken into the account when building the objective function.
In this paper, we propose AdaMF, an adaptive boosting approach for ranking-oriented recommender systems, by combining a state-of-the-art ranking model AdaRank [6] with MF. AdaRank creates a strong ranker by composing weak rankers linearly and the coefficient of each weak ranker is calculated from the weight function of a certain performance metric. Different with the traditional LTR problem, recommendation problem usually does not have explicit features in dataset to build model-based weak rankers. However Matrix Factorization can allocate latent factors for each user and item only based on user history data. Therefore, we choose Matrix Factorization to build the component recommender and component recommenders are combined to the ensemble recommender lin-early according to the reco mmendation performance. In our approach we choose Normalized Discounted Cumulative Ga in (NDCG) as the parameter for the co-efficient calculation, which can measure the effect of the overall relevance of the recommendation lists.

In summary, the contributions of this work are three folded:  X  We introduce a ranking-oriented approach for recommender system AdaMF,  X  We propose a gradient descend algorithm for training component recom- X  We conduct a validation towards the efficiency and performance of AdaMF.
This paper is organized as follows: I n Section 2, we introduce some related works for our work. In Section 3 we present our AdaMF algorithm. Some ex-periment and comparison are described in Section 4. We conclude our study of AdaMF at Section 5. 2.1 Matrix Factorization Matrix Factorization model uses low dimen sional vectors to rep resent the feature for users and items. The MF model has high scalability and accuracy, and gets high success in Netflix challenge [1]. Dur ing the competition, more elements are considered to enhance the factor vector. For example, traditional neighbor based collaborative filtering is combined with MF model [7]. Temporal dynamic of users X  taste and items X  timeliness is also important in modeling. In temporal approaches of MF model, function based temporal dynamic factors have proven its effectiveness by presenting the late nt factor vector as linear function of the time stamp [8]. MF can be viewed as graphical models in some probabilistic approaches of recommender system [9]. Latent Dirichlet Allocation [10] has been used to build hidden topic feature variables of users and items, thus Probabilistic Matrix Factorization model can be combined to improve performance and solve cold start problem [11]. 2.2 LearningtoRank Traditional ranking algorithm generates a list by sorting the relevance function of query and document. With the development of linguistics model and more ac-cumulation of labeled data from search engine, researchers try to make machines  X  X earn X  the ranking model.

LTR is a series of supervised learning algorithm to learn better ranking model from the labeled documents and feedbacks of users. LTR methods are categorized as Point-wise, Pair-wise and List-wise [3 ] models. Point-wise approaches predict the score of a document to the query by regression methods in machine learning. Pair-wise ranking models use classification to learn the relative preference of each item pair, classification methods like SVM [12] can be adapted into solving the problem. In recent years List-wise algor ithms have drawing much attentions. The key point of List-wise LTR is to define loss function based on the whole result list for each query. The simple intuition is to use performance metrics in information retrieval such as MAP, ND CG, and so on, but these metrics are associated with the rank of documents thus they are discontinuous with model X  X  parameters, therefore different smoothing technology for metrics lead to various approaches [13].

Based on the direct optimization of the p erformance metric, AdaRank [6] uses boosting technique to solve the problem. The basic idea of AdaRank is to employ an exponential loss function of performance metrics. It generates a strong ranker which linearly combines weak rankers, where the coefficients are computed by the performance of each ranker. So that d ifferent weak rankers are correlated according to each one X  X  performance. To create different weak rankers, AdaRank maintains a distribution of weights over the queries in the training data. The distribution is determined according to the performance of strong ranker on each query. AdaRank increases the weights on the queries that are not ranked well by the strong ranker. For each round in the training, AdaRank trains a new weak ranker according to the weight distribution, and the newly trained weak ranker is appended to the strong ranker. 2.3 Ranking-Oriented Recommender System Learning to Rank has made contribution to a wide variety of applications. One of the mainly used areas is in recommender system [4,5]. In Ranking-Oriented recommender systems, sever al ranking-based model have been built. In [4], Bal-akrishnan et.al. propose to take the trained latent factor of each user and item as feature vector. The extra model parameters are added for building point-wise and pair-wise model. They denote as Collaborative Ranking. List-wise LTR al-gorithm was also used to build MF model. Shi et.al. propose ListRank-MF [5], which optimize the loss function which is conducted based on the top one prob-ability of item when given a predicted score. However, Collaborative Ranking proposes an EM-like training process, which cost more time on model training. Ranking based loss function in ListRank-MF also increases the complexity in gradient descent algorithm. In this section we present our model Ad aMF which combines Matrix Factor-ization with AdaRank, that provides a boosting technique for conducting the ranking-oriented recommender system. Firstly, we present our problem defini-tion. Secondly, we introduce Matrix Factorization for recommender system and the framework of our AdaMF model. Finally, we discuss the advantages of com-bining Matrix Factorization with AdaRank. 3.1 Problem Definition The objective of the recommender system is to learn the preference model from users X  rating history to generate a top-N recommendation list. Suppose that there are N ratings given by U users to I items. For each user u , R u is the set of to n . The rating given by user u to item i is denoted as R ui . The goal of our algorithm is to cr eate a recommender f ( u, i ) to rank the unrated items for users. 3.2 Matrix Factorization In the MF model [1], the prediction of the unobserved rating of item i given by user u is formulated as: where P u is the latent factor vector of user u to reflect the interests of user u to each latent factor, similarly Q i shows the relevance of item i with the latent factors, and K is the dimension of the latent factor.

The traditional approach of MF model is built for the prediction of missing ratings. To measure the performance of the accuracy of prediction, RMSE is one of the most commonly used evaluation metric. The RMSE-based loss function is defined as: where  X  r ui denotes the predicted rating given by (1). In the loss function, || X || F is Frobenius norm, which is used as the normalized term to generalize the model. Once the latent vectors have been learne d, the unobserved ratings of each user can be estimated by (1). Intuitively, the recommendation list is generated by sorting  X  r ui for unrated items of each user. 3.3 AdaMF Similar to AdaBoost, the AdaMF maintains a weight distribution on each user in the training set. The training process contains T rounds in all. In the t th round of training, the algorithm builds a component recommender P t u ,Q t u by MF according to the weight distribution on users. The distribution reflects the focal point of the component recommender. During the learning, weights on the users with low training performance are increased so that the component recommender of next round would be forced to focus on users with bad fitting. Acoefficient  X  t of the component recommender is calculated according to the performance metric. Then the newly tr ained recommender is integrated to the ensemble recommender f ( u, i ). New distribution for each user is updated after testing the performance of the ensemble recommender f ( u, i ) on the training set.
In recommender system, the user X  X  rating on items can be labeled by different level, such as one to five star scale. A metric that measures both multi-level relevance and position information is required to measure the ranking perfor-mance. For the reason of better represe nting the ranking performance, we use NDCG [14] measurement to measure th e performance of the recommenders. Specifically, NDCG is defined as: where r u,j is the real rating data given by user u to the item at the j th position of recommendation list. Z u is the normalized parameter for the user u ,sothat the perfect ranker will get NDCG of 1. I n the information retrieval domain, r u,j stands for the relevance of document on j th position to a certain query. For our recommender problem, the rating of user given to the item can be viewed as the relevance. From the definition of NDCG, we can summarize that both various relevance level and position information is taken into consideration. Thus, we use NDCG for the calculation of the coefficient  X  t . The AdaMF algorithm is shown in Algorithm 1.

AdaMF builds a strong recommender f ( u, i )= T t =1  X  t P tT u Q t i . The recom-mender list can be created by sorting f ( u, i ) of the unrated items for each Algorithm 1. AdaMF user. Theoretical analysis shows that the adaptive boosting approaches can con-tinuously improve the performance on the training set until reach the upper bound [6]. 3.4 Component Recommender In the original AdaRank, similar to AdaBoost, the weak ranker is conducted with a simple approach, only one feature is chosen to build the weak ranker. This approach can build the weak models which generate relatively low performances. However as mentioned by Li et.al. in [15], AdaBoost can get better generalization result with correlated strong classifiers . They present AdaBoostSVM by taken Support Vector Machine as the component classifier. We note that SVM is viewed as a strong classifier. In collaborative filtering, we utilize Matrix Factorization as the component recommender model. Matrix Factorization can easily achieve a good performance in rating prediction and recommender ranking. We can view MF as a relatively strong recommender .

One of the problem when building recommender model is the generalization problem. For matrix factorization, the single model would meet overfitting easily when the training gets deeper, as in usual , the latent factor vector X  X  dimension would be close to the real user X  X  rating number. The number of model parame-ter is larger than traditional model-based approaches. The common solution for solving generalization problem is to add the normalized term on the loss func-tion, as illustrated in (2). This would solve the generalization problem to some extent. However, user X  X  multi-faceted in terest cannot be easily modeled by single model with normalization. AdaMF provides an adaptive voting mechanism for different component recommenders to determine the final result together. Dif-ferent faceted models are correlated. The generalization is maintained by the combination of different c omponent recommenders.

Similar with AdaBoost X  X  robustness toward overfitting, AdaMF can rarely meet dilemma in generalization when more component recommenders are com-bined. Moreover, if the component reco mmender is generalized by the additional normalized term, the component recomme nder cannot reflect the preference over the adjusted user weight distribution. Therefore the component recommenders need to prune the normalized term.

AdaMF gives weight distribution on use rs which depends on the performance of current component recommender, the next component recommender model must fit the distribution. The current model should fit users with higher weights, i.e. minimize the errors on higher weighted users.
 In summary we define the loss function of component recommender as:
From this loss function, we can generate derivatives of each P u and Q i ,then the parameters can be learned by Sto chastic Gradient Descend (SGD). The training algorithm of component recommender is shown in Algorithm 2.  X  stands for the learning rate during the training, we set  X  =0 . 01 in our study. Notice that if D(u) equals to 1 U for every user u, the component recommender equals to the basic MF model.
 Algorithm 2. ComponentRecommender 3.5 Discussion AdaMF is easy to be implemented and can be trained fast with high accuracy. Since the complexity of training component recommender is O ( K  X  T  X  ( I  X  N + U  X  m  X  nlogn )). The algorithm X  X  cost linearly increases with the scale of case. One of the advantage of AdaMF is that it combines component recommenders with various focal point to prevent the overfitting problem. We think that in real world data with more rating history, different aspects of users X  preference are covered, in this occasion our model may show its effectiveness. In this section, we conduct experiments to evaluate the effectiveness of our method and make comparison with two baseline methods. 4.1 Experimental Setup and Datasets We use MovieLens-100K 1 dataset to conduct our experiment. MovieLens-100K dataset contains 100K ratings(scale 1-5) given by 943 users to 1682 movies. In our conduction, 10, 20, 50 ratings are randomly chosen from each user to build a training set, and the remaining ratings are gathered for the testing set. For each condition, the chosen user must have 20, 30, 60 ratings so that we can test NDCG@10 on the testing set. In addition, we build a mixed length dataset. In reality, users X  rating number varies in a large range, and only a few active users would contribute most of the rating feedbacks. Therefore, to involve the different activeness from different user, we select 50, 20, 10 ratings for users with more than 60, 30, 20 ratings then put the remain ratings into testing sets. In this way we can evaluate the adaptiveness of our method to various users X  activeness and stickiness. In the following sections, we use 10-set, 20-set, 50-set, mix-set to denote these datasets representatively.
 4.2 Improvement by Iteration Since AdaMF combines different weak recommenders, the overall performance would get improved with the increasing rounds of combination. In this exper-iment we test our AdaMF on four training sets to validate the improvement of NDCG@10 when increasing the iteration rounds number T . In specially, we choose T in 1, 2, 3, 4, 5, 7, 10, 15, 20. For each weak recommender we set the latent factor dimension K to be 10, learning rate as 0.01 and iteration upper bound T w to be 5 rounds. For each condition, we run the algorithm for 10 times and compare the mean and variance of NDCG@10.
Fig.1 shows the performance of AdaMF. It can be observed that after the com-bination of weak recommenders, the original MF model is enhanced in terms of accuracy. It is obvious from the curve th at AdaMF reaches its best performance after about 10 rounds of iteration.
In traditional MF model, the regularization coefficient  X  controls the degree of overfitting on the training set. For the single MF model, good performance could be achieved by selecting parameters f rom Cross-Validation, and this may take a great quantity of works. However as expressed in Fig.1, when setting different parameters in a certain range,, AdaMF would converge to the best performance, thus we do not need to pay too much attentions to the parameter selection.
Not only does AdaMF improve the accuracy of recommendation, it also de-creases the uncertainty ca used by randomly initialized factor value. We use the variance of the results in 10 time running to express the uncertainty. From Fig.2, it is evident to observe that the varianc e of NDCGs decreases with the increasing of iteration number T . The result confirms the expectation that AdaMF reduces the uncertainty from the initial value of the model when increasing the round of iterations. 4.3 Ranking Result Comparison In this subsection we evaluate the ranking performance by making a comparison for AdaMF with two baseline methods. We choose ListRank-MF [5] and the basic MF model to be the baseline. ListRank-MF is a state-of-the-art ranking-oriented recommendation model based on List-wise learning to rank approach, which uses top one probability for the items to build the loss function. We choose the param-eters of baselines by reach their best performance. For each algorithm, we run for 10 times on different dataset to compare the average NDCG@10 metric.
From the results presented in Table 3, we observe that AdaMF performs better on all of four datasets. Especially for the mix-set, AdaMF shows a gratifying improvement in comparison to the ListRank-MF model and the basic MF model.
It can be manifested from Table 3 that L istRank-MF model performs better on 10-set and 20-set. MF shows its disadvantage when meeting a sparse rating matrix, thus the model would easily get overfitted. However AdaMF delivers a 1.43% increase to ListRank-MF. Such res ult reflects the superiority of AdaMF when facing the sparsity. The composition of weak MF models can prevent over-fitting on the training set.

With the grow of matrix density, higher RMSE can somehow transform into better ranking result, therefore basic MF presents a similar result with ListRank-MF on the 50-set and mix-set. AdaMF can take the advantage of different MF models which fit different users better, so that AdaMF can enhance the perfor-mance of single MF model. For the 50-set and the mix-set, AdaMF improves the basic MF model by 1.73%. In this paper, we have introduced the AdaMF, a boosting algorithm for ranking-oriented recommender system. Optimization for NDCG is done by the boosting of the component recommenders which trained by different weight distribution for each user. The model offers several ad vantages: ease in implementation, low complexity in training and high performance in ranking. Experimental results demonstrate the effectiven ess and accuracy of our mod el. In comparison with ListRank-MF and basic MF model, we show the superiority of AdaMF. In the future, we are going to focus on the the distributed training algorithm by adap-tively composing component recommenders that are trained on different pro-cessers in parallel, so that the training process can be easily extended to the multi-processer environment.
 Acknowledgments. This work was supported partly by National Natural Sci-ence Foundation of China (No. 61300070, No. 61103031), partly by China 863 program (No.2013AA01A213), China 973 program (No.2014CB340305), partly by the State Key Lab for Software Development Environment (SKLSDE-2013ZX-16), partly by A Foundation for the Author of National Excellent Doctoral Dis-sertation of PR China(No. 201159), partly by Beijing Nova Program(2011022) and partly by Program for New Century Excellent Talents in University.
