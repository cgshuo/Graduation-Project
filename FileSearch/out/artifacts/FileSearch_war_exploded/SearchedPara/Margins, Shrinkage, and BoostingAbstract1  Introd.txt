 Matus Telgarsky mtelgars@cs.ucsd.edu AdaBoost and related boosting algorithms greedily ag-gregate many simple predictors into a single accurate predictor (Freund &amp; Schapire, 1997). One explanation for the efficacy of boosting is that it not only seeks ag-gregates with low empirical risk, but moreover that it prefers good margins, which leads to improved gener-alization (Schapire et al., 1997). Since AdaBoost does not attain maximum margins on general instances, a push was made to develop methods which carry such a guarantee (R  X atsch &amp; Warmuth, 2005; Shalev-Shwartz &amp; Singer, 2008; Rudin et al., 2007).
 This work shows that margin maximization may be achieved by scaling back the step size. The intuition for this result is simple (cf. Figure 1): when (equiv-alently) considered as steps in a coordinate descent procedure, the iterates, depicted as a path, approx-imate the path of constrained optima (for all possi-ble choices of constraint). By scaling back the step size, the optimal path is more finely approximated. As there have been many proposed step sizes for these methods, this manuscript will study four sepa-rate choices, deriving improved bounds for the more regularized choices. While it has been shown be-fore that regularized step sizes have good generaliza-tion and asymptotically good margins (Zhang &amp; Yu, 2005), this manuscript shows that straightforward step choices achieve these margins at rates matching explic-itly margin-maximizing boosting methods.
 This practice of scaling back weights was proposed by Friedman (2000, Section 5), who referred to it as a shrinkage scheme (Copas, 1983). This scheme is effec-tive, and adopted in practice (see for instance Bradski (2000, Class CvGBTrees ) and Pedregosa et al. (2011, Class GradientBoostingClassifier )); the purpose of this manuscript is to provide theoretical guarantees. 1.1. Outline After summarizing the main content, this introduc-tion closes with connections to related work; there-after, Section 2 recalls the core algorithm, defines the class of loss functions, and provides the four step sizes. As boosting is generally studied under the weak learn-ing assumption (a separability condition), the domi-nant study in this manuscript is also under the con-dition of separability, and appears in Section 3. The first step is to show that shrinkage does not drasti-cally change the rate of convergence of the empirical risk under these methods. The more involved study is on the topic of margins, and the final subsection compares these bounds to those of other methods. General (potentially nonseparable) instances are dis-cussed in Section 4. Once again, the first step is a convergence rate guarantee, which again matches those without shrinkage. This section also demon-strates that, under a certain decomposition of boosting problems, the algorithm is still achieving margins on a separable sub-component of the problem.
 The manuscript closes with some discussion in Sec-tion 5. All proofs are relegated to appendices (in the supplementary material). 1.2. Related Work Three close works proposed regularized line searches for boosting. First, Friedman (2000) gave the same scheme as is considered here (albeit with only the op-timal line search); follow-up work has been mainly em-pirical, and the questions of convergence rates and margin guarantees do not appear in the literature. Second, Zhang &amp; Yu (2005) also considered regular-ized line searches, but with a goal of proving consis-tency; margin maximization is proved as a byproduct, and the analogous results here hold under fewer con-ditions, and come with rates for the more stringent step sizes. A third work, due to R  X atsch et al. (2001), also proves margin maximizing properties of regular-ized line searches, but again without rates.
 As mentioned in the introduction, margin maximiza-tion properties of AdaBoost have received extensive study; an excellent survey of results with pointers to other literature is provided by Schapire &amp; Freund (2012, Chapter 5). Amongst these, a crucial result, due to Rudin et al. (2004), provides a concrete input to AdaBoost which yields suboptimal margins (which is used in Section 3.3); that work also studies the evo-lution of these margins as a dynamical system, a topic which will reappear in Section 5.
 The primary contribution of this manuscript is to ex-hibit margin maximization, thus a natural comparison is to other algorithms with this same guarantee, for in-stance the works of R  X atsch &amp; Warmuth (2005), Shalev-Shwartz &amp; Singer (2008), and Rudin et al. (2007) (or again refer to Schapire &amp; Freund (2012, Chapter 5, Bibliographic Notes) for a more extensive summary). This manuscript will briefly compare with the meth-ods of Shalev-Shwartz &amp; Singer (2008), which subsume some earlier results and match the best guarantees, along with giving a simple, general, greedy scheme. The key distinction between previous work and the present work is firstly that the algorithmic modifica-tions here are minor (in particular, the form of unregu-larized empirical risk minimization is unchanged), and that properties of an existing, widely used method are discerned (namely, the shrinkage procedure presented by Friedman (2000)).
 As is standard in the above works, this manuscript is only concerned with convergence of empirical quanti-ties.
 In order to prove convergence rates, this work relies heavily on techniques due to Telgarsky (2012). In particular, the scheme to prove convergence rates of empirical risk, detailed properties of splitting out a hard core from a boosting instance (cf. Section 4), and the notion of relative curvature (cf. Section 2.1) are all due to Telgarsky (2012). The intent of the present manuscript is to establish margin properties, and in this regard it departs from Telgarsky (2012); by contrast, the convergence rates of empirical risk pre-sented here are thus trivial, but included since they did not appear explicitly in the literature. It is worth mentioning that these methods produce bad constants when applied to the logistic loss; unfortunately, pre-vious work also suffers in this case (for instance, the work of Collins et al. (2002) provided only convergence of empirical risk, and not rates). First some basic notation. Let { ( x i ,y i ) } m i =1 { X  1 , +1 } denote an m -point sample. Take H 0 to de-note the collection of weak learners; it is assumed that h  X  H 0 satisfies h ( X )  X  [  X  1 , +1], and that H 0 has some form of bounded complexity, meaning specifically that the set of vectors { ( h ( x 1 ) ,...,h ( x m )) : h  X  H is finite; this for instance holds if there is a fixed finite set of outputs from H 0 , e.g., each h is binary. Conse-of hypothesis, and collect the responses on the sample into a matrix A  X  [  X  1 , +1] m  X  n with A ij =  X  y i h j Boosting finds a weighting  X   X  R n of H , which corre-sponds to a regressor x 7 X  P n j =1  X  j h j ( x ), and thus a binary classification rule after thresholding. The corre-sponding ( l 1 minimum) margin M ( A X  ) over the sam-ple with respect to  X  is
M ( A X  ) := min Let  X  denote the best (largest) achievable margin; equivalently (Shalev-Shwartz &amp; Singer, 2008),  X  is the weak learning rate (which justifies the choice of l 1 mar-gins):  X  := max When  X  &gt; 0, the instance is considered separable; clas-sically, this condition is termed the weak learning as-sumption (Kearns &amp; Valiant, 1989; Freund &amp; Schapire, 1997). 2.1. The Family of Loss Functions The class L will effectively be  X  X unctions similar to the exponential loss X . Some of this is for analytic conve-nience, but some of this appears to be essential, and thus a bit of motivation is appropriate.
 Optimization problems typically take advantage of curvature (e.g., strong convexity) to establish a con-vergence rate. The analysis here instead uses a relative form of curvature: it suffices for, say, the Hessian to not be too small relative to the gap between the cur-rent primal objective value and the primal optimum. fixed point of the differentiation operator.
 Definition 2.1. Given a loss ` : R  X  R ++ (where R ++ denotes positive reals), let C ` ( z )  X  1 (with po-tentially C ` ( z ) =  X  ) be the tightest positive constant C derivatives).  X  follows that y  X  z implies C ` ( y )  X  C ` ( z ). From here, the class of loss functions may be defined. Definition 2.2. Let L contain all functions ` : R  X  R + which are twice continuously differentiable, strictly convex, and have C ` ( z ) &lt;  X  for all z  X  R . Addition-ally, if lim z  X  X  X  X  C ` ( z ) = 1, then `  X  L  X  .  X  Crucially, the two classes L and L  X  both contain the exponential and logistic losses.
 Proposition 2.3. { x 7 X  exp( x ) ,x 7 X  ln(1 + exp( x )) } X  L  X  .
 Algorithm 1 boost .
 Input: loss ` , matrix A  X  [  X  1 , +1] m  X  n .
 Output: Weighting sequence {  X  t }  X  t =0 .

Initialize  X  0 := 0. for t = 1 , 2 ,... : do end for One way to interpret this is to say  X  X n the limit, logistic loss is the same as exponential loss X . Unfortunately, this treatment of the logistic loss ends up being quite unfair, in the sense that the bounds are not accurately representative of the behavior of the algorithm (see Section 3.3). It is, however, unclear how to better deal with the logistic loss.
 Lastly, the relevant primal objective function may be defined.
 Definition 2.4. Given `  X  L and vector z  X  R m , de-timization problem for boosting is to minimize L ( A X  ) over the domain R n . For convenience, define  X  L A 2.2. Algorithm The algorithm appears in Algorithm 1. Before defining the various step sizes, two more definitions are in order. Definition 2.5. For every t , define  X  t := 1  X   X  t  X   X  .)  X  Additionally, rather than depending on parameter C ( z ) for a carefully chosen z , the following definition suffices.
 Definition 2.6. For t  X  1, define C t := C The significance of C t is as follows. Since the al-gorithm itself is coordinate descent, and moreover since every line search will be shown to guarantee de-scent, every candidate  X  considered in round t will ` ( e &gt; i A X  )  X  m L ( A X  )  X  m L ( A X  t  X  1 ), and so e ` since ` is a bijection between R and R ++ by defini-tion of L (otherwise C ` ( z ) =  X  ).
 The collection of step sizes considered here are as fol-lows, in order of least to most aggressive. Throughout these step sizes,  X   X  (0 , 1] will denote a shrinkage pa-rameter.
 Quadratic upper bound. Rather than performing Wolfe. The Wolfe line search is a standard tool from AdaBoost. Following the scheme of AdaBoost, de-Optimal. Let  X  O t (1) be a minimizer to  X  7 X  relationship.
 Proposition 2.9. If A  X  [  X  1 , +1] m  X  n and `  X  L , then  X  Q t (  X  )  X   X  O t (  X  ) . meaning the weak learning assumption is satisfied (  X  &gt; 0). The three subsections respectively provide convergence rates in empirical risk, basic margin guar-antees, and close with some discussion. 3.1. Convergence of Empirical Risk The basic guarantee is that all of these line search methods, for any loss in L and with arbitrary shrink-age, exhibit the same basic convergence rate as Ad-aBoost.
 Theorem 3.1. Let boosting matrix A with correspond-ing  X  &gt; 0 and shrinkage parameter  X   X  (0 , 1] be given. Given any `  X  L , any &gt; 0 , and iterates {  X  t } t  X  0 ` = exp , then O ( 1 L ( A X  t )  X  , where the O (  X  ) suppresses terms depend-ing on C 1 and  X  .
 The proof is in the appendix, but a basic discussion will appear here for each step size. The proofs are straightforward, as they should be: convergence anal-yses typically prove a bound for one step, and then iterate the bound. As such, taking 1 / X  steps which are  X  -factor as long as the original should do at least as well as the original (which is indeed the exhibited trade-off).
 First is the quadratic upper bound, which implicitly gives an upper bound for the optimal step as well. The proof follows a standard scheme from convex optimiza-tion of lower and upper bounding a potential function based on the gradient; the specifics use the relative curvature properties of L , and follow the analysis of Telgarsky (2012, Section 6.1, Appendix D).
 Lemma 3.2. Consider the setting of Theorem 3.1, but with each step size  X  t satisfying  X  Q t (  X  )  X   X  t  X   X  O Then for any t,t 0 with t  X  t 0 ,
L ( A X  t +1 )  X L ( A X  t The reason for the parameter t 0 is to mitigate the horrendous dependence on C t 0 , which is potentially very large. In particular, consider `  X  L  X  , mean-ing lim z  X  X  X  X  C ` ( z ) = 1. C 1 may be quite bad, but convergence still happens. It follows that C t  X  1, and thus, by choosing some large t 0 , the bound pro-vides that perhaps there is an initially slow conver-gence phase, but eventually it is very fast. That is to stay, Lemma 3.2 may be applied multiple times to give a more refined picture of the convergence, partic-ularly in the case that `  X  L  X  , which guarantees the constants are eventually near 1.
 Next, the Wolfe step size has a similar guarantee (and the analysis once again heavily relies on techniques due to Telgarsky (2012, 6.1, Appendix D)).
 Lemma 3.3. Consider the setting of Theorem 3.1, but with  X  t  X   X  W t (  X  ) . Then for any t,t 0 with t  X  t 0
L ( A X  t +1 )  X L ( A X  t (The denominator blows up by a factor 4 due to extra halves introduced into the Wolfe conditions, specifi-cally to adjust around the natural Wolfe parameters being within (0 , 1) and not (0 , 1].) Lastly, consider  X  A t (  X  ). As in the statement of The-orem 3.1, this step size is only shown to work with the exponential loss. This may be an artifact of the analysis, however, which perhaps follows too closely the treatment of Schapire &amp; Singer (1999), which only considers the exponential loss; for instance, a slightly modified step size can be used to show convergence with the logistic loss (Collins et al., 2002).
 Lemma 3.4. Consider the setting of Theorem 3.1, but with  X  t  X   X  A t (  X  ) Then for any t,t 0 with t  X  t 0 , 3.2. Margin Maximization The margin rates here follow a simple pattern: the more regularized the step size, the faster the conver-gence to a good margin. While no lower bounds are presented, this is an interesting and intuitive corre-spondence (in particular, consistent with Figure 1). Unfortunately, the unconstrained step sizes only have asymptotic convergence (no rates), so the umbrella theorem for this subsection is also asymptotic. Theorem 3.5. Let boosting matrix A with correspond-ing  X  &gt; 0 and shrinkage parameter  X   X  (0 , 1] be given. Given any `  X  L  X  , any &gt; 0 , and iterates {  X  t } t (  X  ) with binary A  X  X  X  1 , +1 } T so that for all M ( A X  t )  X   X   X  for all t  X  T . In contrast with the convergence rates of empirical risk (e.g., Theorem 3.1), the condition `  X  L  X  is made, rather than simply `  X  L (with improved constants when `  X  L  X  ). This can be interpreted to say: the analysis depends heavily upon the structure of the ex-ponential loss. While this condition is likely unneces-sary, on the other extreme it is important for the loss to be strictly convex; if for instance the hinge loss is used, then minimization can stop at any point achiev-ing zero error, in particular at one with poor margin properties.
 Returning to task, the quadratic upper bound comes first.
 Lemma 3.6. Suppose the setting of Theorem 3.5, but Lemma 3.2). Then where c := max To interpret this bound, first consider the simplifying case that ` = exp, whereby C t = 1 for all t . Addi-tionally taking t 0 = 0, it follows that c 0 = m , and the bound is simply in particular, M ( A X  t )  X   X  as  X   X  0 and t X   X  X  X  . For some other `  X  L  X  , the denominator term C 6 t presents an obstacle to establishing margin maximiza-tion; but note that t 0  X  X  X  suffices, since it combines with `  X  L  X  via Theorem 3.1 to grant C t 0  X  1. The proof of Lemma 3.6 does not have to work too hard, as the step size appears prominently in the con-vergence rate bound (cf. Lemma 3.2). As will be dis-cussed in Section 3.3, the rate is nearly ideal. The Wolfe search exhibits a similar rate.
 Lemma 3.7. Suppose the setting of Theorem 3.5, but Lemma 3.3). Then where c := max explicit regularization: the first stops as soon as the steepest matching quadratic turns upward, and the second refuses to go beyond a boundary (cf. eq. (2.7)). On the other hand, the choices  X  A t (  X  ) and  X  O t (  X  ) are only constrained by the data. Recall that one way to derive  X  A t (  X  ) is in the case of binary A  X  X  X  1 , +1 } and ` = exp, where it is crucial that each weak learner is wrong on at least one example: this prevents steps from being too large. The techniques in the following proof follow those used in the margin bounds for reg-ular AdaBoost (and are asymptotic there as well). It is worth noting that not only is this bound the worst, but the analysis is the trickiest.
 Lemma 3.8. Consider the setting of Theorem 3.5, but now ` = exp and  X  t =  X  A t (  X  ) . Then for any  X  (0 , X  ] , there exists T so that M ( A X  t )  X   X   X  for all t  X  T . condition that A  X  { X  1 , +1 } m  X  n prevents the nega-tive, constraining examples from having too little in-fluence.
 Lemma 3.9. Consider the setting of Theorem 3.5, but now ` = exp , the matrix A is binary, and  X  t =  X  O t (  X  ) . Then for any &gt; 0 , there exists T so that M ( A X  t )  X   X   X  for all t  X  T .
 The above lemmas together provide the proof of Theo-rem 3.5. But before closing, note that while the results for the unconstrained step sizes were only asymptotic, it is possible to derive a rate for the more modest goal of margins closer to  X / 3.
 Proposition 3.10. Consider the setting of Theo-rem 3.5, but specialized with ` = exp and  X  t =  X  A t (  X  ) . Let a target margin value  X  &lt;  X  be given. If  X  &lt;  X / (1 +  X  ) (e.g., it suffices that  X  &lt;  X / 2 ), then 1 m M ( A X  t )  X   X  .
 Note, of course, that this bound has the severe analytic artifact of demonstrating no benefit of shrinkage! 3.3. Discussion To get a sense of these margin bounds, first recall Fre-und X  X  lower bound on boosting methods in the separa-ble case, which states that  X ( 1  X  2 ln( 1  X  )) iterations are necessary to achieve classification error  X  &gt; 0 (Fre-und, 1995, Section 2). Setting  X  = 1 /m , it follows any nonnegative margin. By comparison, with  X  W t (  X  ) and ` = exp, just 12 ln( m ) / X  2 iterations with choice  X  = 1 / 2 suffice to reach margin  X / 2 (by Lemma 3.7). More generally,  X  W t (  X  ) reaches margin  X  (1  X   X  ) with then 2 ln( m ) / (  X  X  ) 2 iterations suffice by Lemma 3.6). The explicit margin-maximizing method of Shalev-Shwartz &amp; Singer (2008) requires t  X  32 ln( m ) / 2 it-erations to achieve margin  X   X  , where  X  (0 , X  ). By comparison, converting the above multiplicative bound into an additive bound, step size  X  W t ( / X  ) requires 8 ln( m ) / 2 iterations. While this bound is slightly better, the comparison is not fair, since t ( / X  ) requires knowledge of  X  in the choice of shrinkage parameter  X  . (Pessimistically taking  X  = gives an additive guarantee, but with a poor rate.) Consequently, it can be reasoned that shrinkage meth-ods achieve excellent margins, but are best suited for multiplicative guarantees.
 Another question is how accurately the bounds pre-sented here depict the methods provided. As a brief sanity check, the methods may be run on a prob-lem instance where AdaBoost demonstrably does not achieve maximum margins. The particular instance tested here is a binary matrix A  X  X  X  1 , +1 } 8  X  8 due to Rudin et al. (2004, Theorem 7); recall that AdaBoost, in the present notation (with A binary), corresponds to ` = exp and step size  X  A t (1) =  X  O t (1) (no shrinkage). Two plots are provided. 1. Figure 2 is a sanity check, showing that ` = exp 2. Figure 3 demonstrates that the Wolfe search (with These plots will be discussed further in Section 5. Ad-ditional tests with this matrix demonstrated that the method of Shalev-Shwartz &amp; Singer (2008) indeed per-forms a tiny bit worse than the Wolfe search, but of course one example is not terribly indicative. Perhaps most importantly, a test with the logistic loss showed that the bound is loose: the logistic loss performs well, and does not suffer a startup cost as indicated by the bounds. The last technical contribution of this manuscript is to briefly consider the general case (which is potentially nonseparable). Similarly to the separable case, this section will establish convergence rates for empirical risk, margin guarantees, and briefly discuss the con-nection to existing margin maximizing methods. But first, it is necessary to discuss the structure of the gen-eral case, and in particular to develop what margins mean without separability.
 This section hinges upon the following decomposi-tion of a boosting instance. This decomposition par-titions a boosting instance, specifically its examples subset H ( A ) c . The easy subset alone is separable, and thus margins will be measured there. Although the analysis will rely heavily on properties of this decom-position due to Telgarsky (2012), the decomposition itself has appeared, with various guarantees, in nu-merous places (Goldreich &amp; Levin, 1989; Impagliazzo, 1995; Mukherjee et al., 2011). The notation H ( A ) re-flects the fact that this structure has no relation to the choice of `  X  L .
 Definition 4.1. (Cf. Telgarsky (2012, Definition 5.1, 5.7).) Given a boosting problem encoded in a matrix A  X  R m  X  n , a set of examples (rows) H ( A )  X  [ m ] is a hard core for A (and the corresponding boosting problem) if it satisfies the following properties.  X  Every weighting  X   X  R n with e &gt; i A X  &lt; 0 for some Additionally, define a row-wise partition of A into ma-trices A 0 ,A + , where A + has the examples in H ( A ), and A 0 has the examples in H ( A ) c .  X  The second property provides that H ( A ) is difficult: positive margins on some examples force negative mar-gins on others. On the other hand, the complement H ( A ) c is easy, and moreover can be solved without affecting H ( A ).
 Proposition 4.2. (Cf. Telgarsky (2012, Proposition 5.8, Theorem 5.9).) For any A  X  R m  X  n , a hard core H ( A ) always exists, and is unique.
 With the decomposition in place, the aforementioned guarantees may be stated. The first, as in the sepa-rable case, is convergence of empirical risk. There is hardly anything to do here; the groundwork from Sec-tion 3 can be plugged directly into existing techniques to generate this theorem (Telgarsky, 2012, Section 6). Theorem 4.3. Let general boosting matrix A be given (i.e., potentially  X  = 0 ), along with shrinkage param-eter  X   X  (0 , 1] , any `  X  L , and target suboptimality  X  t (  X  ) ,  X  binary. Then O ( 1 ) iterations suffice to reach subopti-mality &gt; 0 .
 If the instance is either separable (i.e.,  X  &gt; 0 as in Sec-tion 3) or attains its minimizer (i.e., | H ( A ) | = m (Tel-garsky, 2012, Theorem 5.5)), then the rate improves to O (ln( 1 )) .
 Lastly come the margin guarantees. As stated above, more that the definition of hard core provides the ex-istence of a weighting  X   X  . which has positive margins sequently, an approximate minimizer to L ( A  X  ) can al-ways add in a scaling of  X   X  and improve its empiri-cal risk while simultaneously improving margins over H ( A ) c . Consequently, it is natural to expect the meth-ods here to achieve positive margins over H ( A ) c . Note that the following result only shows that some positive margins are attained, and neither assert some sense under which they are maximal, nor does it provide rates.
 Theorem 4.4. Let general boosting matrix A be given with 1  X  | H ( A ) |  X  m  X  1 (i.e., the problem is nei-ther separable, nor is the minimizer attainable). Let shrinkage parameter  X   X  (0 , 1] and any `  X  L  X  be  X  t (  X  ) ,  X  t (  X  ) with ` = exp and binary A ., Then there exists i  X  H ( A ) c ) has margin at least  X   X  for all large t . To close, consider once again the comparison to ex-plicit margin maximizing boosting methods as pre-sented by Shalev-Shwartz &amp; Singer (2008). There is no point in discussing the specific method discussed in Section 3.3, whose optimal objective value is exactly  X  , which in this case is zero, and the method may hap-pily quit without iterating. Indeed, a primary contri-bution of Shalev-Shwartz &amp; Singer (2008) is not only to address this issue, but show how the same general boosting scheme can be instantiated for the aforemen-tioned method, as well as methods with tolerance to nonseparability.
 Indeed, consider the  X  X oft-margin X  boosting method (Shalev-Shwartz &amp; Singer, 2008), originally due to Warmuth et al. (2006), which, roughly speaking, has a parameter controlling how many examples to give up on. This is in contrast to the methods here, which not only have a fixed data-dependant structure they try less hard on (the hard core H ( A )), but moreover the particular margins achieved over the hard core are determined by the loss function `  X  L . It is of course worth mentioning that the margin analysis in the non-separable case here is by comparison very incomplete, providing no rates and not even identifying exactly what positive margins are attained. This manuscript immediately raises a number of ques-tions. Perhaps foremost is the general question of the impact of margins on the efficacy of boosting. Al-though margins certainly provide an intuitive theory, it is still unclear how much they directly correlate with good algorithms (Reyzin &amp; Schapire, 2006).
 Next, the bounds for the logistic loss are not tight. As there do not appear to be any more forgiving analy-ses of the logistic loss, the natural question is whether there are new techniques which provide a better char-acterization.
 Lastly, Figure 2 shows a threshold effect: shrinkage 1 does not lead to the right margin, but 1 / 2 and smaller suffices to reach the maximum margin. (Indeed, exper-imentation reveals the threshold to be roughly 0.92.) It should be possible to clarify this behavior from the perspective of dynamical systems: smaller steps dodge bad attractors (Rudin et al., 2004; 2007).
 Acknowledgements The author thanks Daniel Hsu and the ICML review-ers for helpful comments and discussions. The author is also deeply indebted to Robert Schapire for numer-ous discussions, insight, and for suggesting study of the unconstrained step size (at the time, guarantees were only in place for the other choices!). This work was graciously supported by the NSF under grant IIS-0713540.
 Bradski, G. The OpenCV Library. Dr. Dobb X  X  Journal of Software Tools , 2000.
 Collins, Michael, Schapire, Robert E., and Singer,
Yoram. Logistic regression, AdaBoost and Bregman distances. Machine Learning , 48(1-3):253 X 285, 2002. Copas, J. B. Regression, prediction and shrinkage.
Journal of the Royal Statistical Society, Series B (Methodological) , 45(3):311 X 354, 1983.
 Freund, Yoav. Boosting a weak learning algorithm by majority. Information and Computation , 121(2): 256 X 285, 1995.
 Freund, Yoav and Schapire, Robert E. A decision-theoretic generalization of on-line learning and an application to boosting. J. Comput. Syst. Sci. , 55 (1):119 X 139, 1997.
 Friedman, Jerome H. Greedy function approximation:
A gradient boosting machine. Annals of Statistics , 29:1189 X 1232, 2000.
 Goldreich, Oded and Levin, Leonid. A hard-core pred-icate for all one-way functions. STOC, pp. 25 X 32, 1989.
 Impagliazzo, Russell. Hard-core distributions for somewhat hard problems. In FOCS , pp. 538 X 545, 1995.
 Kearns, Michael and Valiant, Leslie. Cryptographic limitations on learning finite automata and boolean formulae. STOC, pp. 433 X 444, 1989.
 Mukherjee, Indraneel, Rudin, Cynthia, and Schapire, Robert. The convergence rate of AdaBoost. In COLT , 2011.
 Nocedal, Jorge and Wright, Stephen J. Numerical op-timization . Springer, 2 edition, 2006.
 Pedregosa, F., Varoquaux, G., Gramfort, A., Michel,
V., Thirion, B., Grisel, O., Blondel, M., Pretten-hofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot,
M., and Duchesnay, E. Scikit-learn: Machine Learn-ing in Python . Journal of Machine Learning Re-search , 12:2825 X 2830, 2011.
 R  X atsch, G., Onoda, T., and M  X uller, K.-R. Soft margins for adaboost. Machine Learning , 42:287 X 320, 2001. R  X atsch, Gunnar and Warmuth, Manfred. Efficient margin maximizing with boosting. Journal of Ma-chine Learning Research , 6:2153 X 2175, 2005.
 Reyzin, Lev and Schapire, Robert E. How boosting the margin can also boost classifier complexity. In
In Proceedings of the 23rd International Conference on Machine Learning , pp. 753 X 760, 2006.
 Rudin, Cynthia, Daubechies, Ingrid, and Schapire,
Robert E. The dynamics of AdaBoost: cyclic behav-ior and convergence of margins. Journal of Machine Learning Research , 5:1557 X 1595, 2004.
 Rudin, Cynthia, Schapire, Robert E., and Daubechies,
Ingrid. Analysis of boosting algorithms using the smooth margin function. Annals of Statistics , 35 (6):2723 X 2768, 2007.
 Schapire, Robert E. and Freund, Yoav. Boosting: Foundations and Algorithms . MIT Press, 2012.
 Schapire, Robert E. and Singer, Yoram. Improved boosting algorithms using confidence-rated predic-tions. Machine Learning , 37(3):297 X 336, 1999.
 Schapire, Robert E., Freund, Yoav, Barlett, Peter, and
Lee, Wee Sun. Boosting the margin: A new expla-nation for the effectiveness of voting methods. In ICML , pp. 322 X 330, 1997.
 Shalev-Shwartz, Shai and Singer, Yoram. On the equivalence of weak learnability and linear separa-bility: New relaxations and efficient boosting algo-rithms. In COLT , pp. 311 X 322, 2008.
 Steele, J. Michael. The Cauchy-Schwarz Master Class . Cambridge University Press, 2004.
 Telgarsky, Matus. A primal-dual convergence analysis of boosting. 2012. arXiv:1101.4752v3 [cs.LG] . Warmuth, Manfred K., Liao, Jun, and R  X atsch, Gun-nar. Totally corrective boosting algorithms that maximize the margin. In ICML , pp. 1001 X 1008, 2006.
 Zhang, Tong and Yu, Bin. Boosting with early stop-ping: Convergence and consistency. The Annals of Statistics , 33:1538 X 1579, 2005.
 Proof of Proposition 2.3. There is nothing to show for exp, so consider ` ( x ) = ln(1 + exp( x )), let z  X  R be given, and let x  X  z be arbitrary.
 Concavity grants ln(1 + exp( x ))  X  exp( x ). The lower bound can be checked in two stages. First, if x  X  min { X  1 ,z } , a Taylor expansion gives On the other hand, if  X  1  X  x  X  z , then e x  X  e z ln(1 + e  X  1 ) / ln(1 + e  X  1 )  X  e z ln(1 + e x ) / ln(1 + e  X  1 ). Next, ` 0 ( x ) = e x / (1 + e x ), so ` 0 ( x )  X  e x  X  ` 0 e z ). Similarly, ` 00 ( x ) = e x / (1 + e x ) 2 , so ` ` ( x )(1 + e z ) 2 .
 The following lemma (and its proof) derive  X  Q t (  X  ), es-Lemma A.1. Let boosting matrix A , shrinkage pa-rameter  X   X  (0 , 1] , and any `  X  L be given. For any more, any step  X   X  [  X  Q t +1 (  X  ) , X  O t +1 (  X  )] satisfies
L ( A (  X  t +  X v t +1 ))  X L ( A X  t ) exp  X  Proof. This analysis follows a scheme laid out by Tel-garsky (2012, Appendix D.3). Let t denote any fixed iteration, and I denote the (possibly unbounded) in-terval by continuity of L and choice of v t +1 , I is nonempty, with nonempty interior. By second order Taylor ex-pansion, every  X   X  I satisfies L ( A (  X  t +  X v t +1 ))  X L ( A X  t )  X   X  k A &gt;  X  X  ( A X  t ) k  X   X L ( A X  t )  X   X  k A &gt;  X  X  ( A X  t ) k  X  = L ( A X  t )  X   X  k A &gt;  X  X  ( A X  t ) k  X  +  X L ( A X  t )  X   X  k A &gt;  X  X  ( A X  t ) k  X  + which made use of ` 00  X  C t +1 exp  X  C 2 t +1 ` along I , `  X  C t +1 exp  X  C 2 t +1 ` 0 along I , | A ij |  X  1 (since elements of H are bounded in this way), and the definition of I (specifically r = 0 is the worst choice for r  X  I ). This final expression is a quadratic, whose minimizer must lie within I (since its second derivative exceeds that of L along this interval). Differentiating and setting to zero, the minimizer is in the above quadratic upper bound, L ( A (  X  t +  X v t +1 ))  X L ( A X  t )  X  Proof of Proposition 2.9. This is the first part of Lemma A.1.
 B.1. Deferred Material from Section 3.1 Proof of Lemma 3.2. By Lemma A.1, for any t ,
L ( A (  X  t +  X v t +1 ))  X L ( A X  t ) exp  X  Now let t 0  X  t be given as in the desired statement, apply this bound t  X  t 0 times, and use the fact that C Proof of Lemma 3.3. Let t denote any fixed iteration. Substituting c 1 = 1  X   X / 2, c 2 = 1  X   X / 4, and  X  = t +1 in a nearly identical guarantee for the Wolfe line search (Telgarsky, 2012, Proposition D.6) (where  X  is simply the biggest ratio between ` and ` 00 in the current sublevel set) provides Given t 0  X  t , applying this bound t  X  t 0 times and using C t +1  X  C t gives the result.
 Next, instead of directly proving Lemma 3.4, a more general lemma is given first, which will be useful later. Lemma B.1. Consider the setting of Theorem 3.1, except now each step size  X  i satisfies for some  X  &gt; 0 . Then, given t  X  t 0 ,  X L ( A X  t  X L ( A X  t Proof. Fix an iteration t , and set w i = ` 0 ( e &gt; i A X  W = P i w i  X  mC 2 t +1 L ( A X  t ). By convexity of exp(  X  ),  X   X   X   X  To simplify this expression, note that (  X  ) 1  X   X  is a con-cave function, and thus To finish, given t  X  t 0 , the result follows by t  X  t applications of these bounds.
 Proof of Lemma 3.4. This follows by taking the sec-ond bound in Lemma B.1 with the choice  X  = 0.
 Proof of Theorem 3.1. The result follows from Lemma 3.2, Lemma 3.3, and Lemma 3.4 with the choice t 0 = 0 and using C t +1  X  C t .
 B.2. Deferred Material from Section 3.2 Proof of Lemma 3.6. To start, note that k  X  By the form of L and the optimization guarantee in Lemma 3.2, where c 0 is as in the statement. Since ln(  X  ) is increas-ing, it follows that Using the above bound on k  X  t k 1 , since t X   X  P t +1 i =1 and  X  e &gt; k A X  t +1 is nonnegative by the lower bound on t , Proof of Lemma 3.7. Any step size  X  t +1 satisfying the Wolfe conditions will have lower bound indeed this expression appears in proofs demonstrat-ing the improvement due to a single step of the Wolfe search, see for instance Telgarsky (2012, Proof of Proposition D.6, second to last line).
 Additionally, note Direct from the first Wolfe condition (eq. (2.7)), Now let t  X  t 0 be given as in the statement. Applying the above inequality t  X  t 0 times, where c 0 is as in the statement. Since ln(  X  ) is increas-ing, it follows that Using the above lower bound on  X  i in terms of  X  i , and since all margins are nonnegative by the lower bound on t , The remainder of this subsection provides proofs for t (  X  ) and  X  specifically the quantity  X   X  .
 Lemma B.2. Consider the setting of Theorem 3.1, except now each step size  X  i satisfies for some  X  &gt; 0 . Let  X   X  [0 , X  ) be given. Then, given t  X  t 0 , X  X  mC t +1 exp(  X  k  X  t  X  mC t +1 exp(  X  k  X  t Proof. To start, Next, note Combining these facts with the convergence bound from Lemma B.1, X  X  mC t +1 exp(  X  k  X  t the vaguely simpler bound X  X  mC t +1 exp(  X  k  X  t Proof of Lemma 3.8. Set  X  :=  X   X  , whereby  X   X  [0 , X  ). Invoking Lemma B.2 and simplifying terms via t = 0, C i = 1, and  X  = 0, then for any t , where the replacement of  X  i by  X  made use of the first part of Lemma B.6. Now, by the second part of By Theorem B.5, since  X  &lt;  X  , there exists a  X  suffi-ciently small that  X   X  (  X  ) &gt;  X  . Consequently, there ex-ists a T so that this product is less than 1 /m whenever t  X  T , and the result follows.
 Proof of Lemma 3.9. Set  X  :=  X   X  , whereby  X   X  [0 , X  ). Since `  X  L  X  , choose t 0 large enough so that By Lemma B.7, it follows that the optimal step size satisfies with  X  =  X  2 ln( C 4 t ). Combining this with the bound on C 0 above, Plugging this into the general margin bound in Lemma B.2 and additionally replacing  X  i with  X  thanks to the first part of Lemma B.6, and finally set-ting  X  0 :=  X  + (  X   X   X  ) / 2 = (  X  +  X  ) / 2 &lt;  X  ,
X  X  mC t +1 exp(  X  k  X  t  X  mC t +1 exp(  X  k  X  t  X  mC t +1 exp(  X  k  X  t By the second part of Lemma B.6, the term within the product is less than one, and thus for all large t , this entire bound is less than 1, which gives the result. Proof of Proposition 3.10. To start, note that, for any t  X  0, Next, since then  X   X   X / (1 +  X  ) implies In particular, the expression  X   X  2 t +  X  X  t (2 +  X  t ) is de-creasing in  X  , and thus  X  t  X   X  implies and consequently, combined with the bound in (B.3), Plugging this into the simplified generic bound in Lemma B.2 with the specialization ` = exp,  X  = 0, C i = 1, and t 0 = 0, it follows that The rest of the result follows by noting  X  &lt;  X / (2 +  X  ) implies  X   X  2 +  X  X  (2 +  X  ) &lt; 0, whereby choices exist, and plugging this all in to the above bound grants that M ( A X  t )  X   X  .
 Proof of Theorem 3.5. For  X  A t (  X  ) and  X  O t (  X  ), Lemma 3.8 and Lemma 3.9 already state the results in the desired asymptotic form.
 For the other two, since `  X  L  X  , t 0 can be chosen sufficiently large so that C t 0 is arbitrarily close to 1, whereby the bounds in Lemma 3.6 and Lemma 3.7 become sufficiently tight by taking  X  small and t X  large.
 B.2.1. The Quantity  X   X  Definition B.4. Define  X   X  (  X  ) := ln(2)  X  2  X  ln((1 +  X  ) 1  X   X  + (1  X   X  ) 1  X   X  )  X  ln(1  X   X  in the case that  X  = 1, this quantity has been exten-sively studied in the context of AdaBoost X  X  margins (R  X atsch &amp; Warmuth, 2005; Rudin et al., 2004; Schapire &amp; Freund, 2012)  X  The basic properties of  X   X  are as follows.
 Theorem B.5. Suppose  X   X  (0 , 1) . 1.  X / 2  X   X   X  (  X  )  X   X  . 2. lim  X   X  0  X   X  (  X  ) =  X  .
 The bounds  X / 2  X   X  1 (  X  )  X   X  were known in the case that  X  = 1 (cf. R  X atsch &amp; Warmuth (2005) and Schapire &amp; Freund (2012, Bibliographic Notes, Chap-ter 5)).
 Proof. (Item 1, subcase  X   X  (  X  )  X   X / 2.) To start, note that (  X  ) 1  X   X  is a concave function, whereby It follows that Next recall the series expansion (when | z | &lt; 1). Plugging this in to the simplified form of  X  1 (  X  ) and paying attention to cancellations in the numerator and denominator (odd and even terms, re-spectively),
 X  1 (  X  ) = To finish, note that n  X  1 implies 1 / (4 n  X  2)  X  1 / (2 n )  X  1 / (2 n  X  1), and thus That is to say,  X / 2  X   X  1 (  X  )  X   X  , which combined with the above also gives  X   X  (  X  )  X   X  1 (  X  )  X   X / 2. (Item 1, subcase  X   X  (  X  )  X   X  .) By the power mean inequality (Steele, 2004, Equation 8.12), It follows that As such, (Item 2.) Consider the (halved, negated) first term By l X  X  X opital X  X  rule, lim = lim =  X  Consequently (recalling that this term was both halved and negated) lim The usefulness of  X   X  is captured in the following lemma.
 Lemma B.6. Let  X   X  (0 , 1] and  X   X  [0 , 1] be given. The map is nonincreasing over [  X , 1] . Additionally, now taking  X  to be fixed,  X  &lt;  X   X  (  X  ) iff Proof. Let f (  X  ) be the prescribed map. To establish f is nonincreasing, it will be shown that each element of the product f (  X  ) = g (  X  ) h (  X  ) is nonincreasing, where First, set  X  0 :=  X / 2, and note g 0 (  X  ) = d where this last term is nonpositive since  X   X   X  . Con-sequently, g (  X  ) is nonincreasing.
 For h (  X  ), note similarly that Together f (  X  ) = g (  X  ) h (  X  ) is nonincreasing in  X  . For the second statement, note that is equivalent to is equivalent to  X  &lt; ln(2)  X   X  2 ln(1  X   X  2 )  X  ln((1 +  X  ) 1  X   X  + (1  X   X  ) 1  X   X  where the last expression can be written  X  &lt;  X   X  (  X  ). B.2.2. Miscellaneous Technical Material Lemma B.7. Suppose A  X  { X  1 , +1 } m  X  n is binary and `  X  L . Then 1 2 ln More simply, Proof. Choose s  X  { X  1 } so that v t +1 = e j s for some e . Then, by first order conditions on the optimal step size, and adopting shorthand notation where the summations take j fixed according to the preceding text, but i  X  [ m ] may vary, 0 = X which can be rearranged to yield To simplify further, note that which can be added and subtracted to yield whereby Repeating the steps above to prove a lower bound on  X  t +1 , it also follows that To finish the first part of the result, it suffices to con-sider the cases s = +1 and s =  X  1 separately, which both lead to the desired pair of inequalities.
 For the second guarantee, first note that  X  O t (  X  ) = t (1) and  X  of  X  A t (1) and scaling the first guarantee by  X  , it follows that Proof sketch of Theorem 4.3. All the convergence rates developed by Telgarsky (2012, Section 6) stem from an inequality
L ( A X  t +1 )  X   X  L A  X  ( L ( A X  t )  X   X  L A ) 1  X  where c &gt; 0 is some constant independent of t (or improving with t , in which case the bound may be worsened by taking the choice for t = 0) (Telgarsky, 2012, Proposition 6.2, Proposition D.6). Exactly such a bound was provided for each line search in the proof of its respective optimization guarantee in the sepa-rable case (cf. Lemma 3.2, Lemma 3.3; no need to adjust Lemma 3.4, since ` = exp and A binary causes t (  X  ) =  X  Replacing c with the particulars for each step size will only impact the final rates in Theorems 6.3, 6.6, and 6.12 by these constants. The only other thing to check is that `  X  G , the class of losses considered by Telgar-sky (2012, Section 6); it can be checked directly that L  X  G .
 In order to establish the margin properties, the follow-ing lemma is essential.
 Lemma C.1. Consider the setting of Theorem 4.4. Then there exists T and  X   X  so that, for all t  X  T , Proof sketch. As discussed in the proof of Theo-rem 4.3, the results of Telgarsky (2012), which are superficially specialized to the Wolfe line search, carry over for the other line searches here with only a change of constants; consequently, those results carry over wholesale.
 To start, let S be a compact cube containing all iter-ates, and let  X  ( A,S ) be the corresponding generalized weak learning rate Telgarsky (2012, Definition 4.3). By (Telgarsky, 2012, Theorem 5.9), L +  X  im( A the function which is L ( y ) when y = A +  X  for some  X   X  R n , and  X  otherwise) has compact level sets, and thus strict convexity of L grants a modulus of strong convexity c &gt; 0 over S ; furthermore, it holds for every t that L ( A +  X  t )  X   X  L A (nullspace) of A &gt; + (Telgarsky, 2012, Lemma 6.8). Now choose T so that, for every t  X  T , which is possible by the convergence of {  X  t }  X  t =1 Theorem 4.3 or (Telgarsky, 2012, Theorem 6.12)). Using these facts, the definition of  X  ( A,S ), the choice  X  = exp, and the fact inf  X  L ( A +  X  ) = inf  X  L ( A X  ) = (Telgarsky, 2012, Theorem 5.9),  X   X  ( A,S ) =  X  ( A,S )  X   X  = To finish, set  X   X  :=  X  ( A,S ) /C 2 T .
 Another technical lemma is helpful.
 Lemma C.2. Consider the setting of Theorem 4.4. For each step size choice and B &gt; 0 , there exists T so that for all t  X  T B , k  X  t k 1  X  B .
 Proof sketch. This follows from Theorem 4.3 and | H ( A ) | &lt; m . In particular, choose any example i  X  H ( A ) c ; there exists &gt; 0 so that 1 by combining this with H  X older X  X  inequality, namely guarantee provides that this holds for all large t . split into two cases, one being the Wolfe step sizes, the other being a generalization of the quadratic upper bound step sizes.
 Lemma C.3. Consider the setting of Theorem 4.4, but with step sizes 0 . 5  X  Q i (  X  )  X   X  i  X  1 . 5  X  Q margins (over H ( A ) c ) exceed  X   X  .
 Proof sketch. Consider the quadratic upper bound line search in Lemma 3.2 and its proof. It is unclear to the term  X  . However, since  X  Q i (1) is the minimizer, a worse choice than anything in the specified interval. As such, plugging this in to the quadratic upper bound yields for some constant c 0 &gt; 0 depending on C 1 and not on t .
 Now choose T 1 according to Lemma C.1; by the above and Lemma C.1, for any t  X  T 1 ,
L ( A X  t +1 )  X   X  L A which, after recursive application, provides L ( A X  t +1 )  X   X  L A  X  ( L ( A X  T Since it follows that
L ( A X  t +1 )  X   X  L A  X  ( L ( A X  T ample in [ m ] \ H ( A ) which achieves the worst margin (amongst elements off the hard core) for this iteration. Since the optimal error on this example is 0 (Telgar-sky, 2012, Theorem 5.9), for any t &gt; T 1 , = exp( b &gt; t A X  t )  X  0  X  mC T  X  mC T = exp (  X   X   X c 0 k  X  t k 1 / (3  X  )) Applying ln and rearranging, To finish, by Lemma C.2, there exists T 2 so that for every i  X  T 2 , and setting T := max { T 1 ,T 2 } gives the desired result.
 Lemma C.4. Consider the setting of Theorem 4.4,  X   X  &gt; 0 and T so that, for all t  X  T , all margins exceed  X   X  .
 Proof sketch. Choose T 1 according to Lemma C.1, and let t  X  T 1 be arbitrary. Using Lemma C.1, and using the first Wolfe condition (eq. (2.7)) just as in the proof of Lemma 3.7, L ( A X  t +1 )  X   X  L A  X L ( A X  t )  X   X  L A  X   X  t +1 (1  X   X / 2) k A &gt;  X  X  ( A X  = ( L ( A X  t )  X   X  L A ) 1  X   X  ( L ( A X  t )  X   X  L A ) (1  X   X  t +1 (1  X   X / 2) X   X  )  X  ( L ( A X  t )  X   X  L A ) exp (  X   X  t +1 (1  X   X / 2) X   X  ) Applying this inequality recursively, Note next, for any t 0 , that whereby and the remainder of the proof proceeds just as for the quadratic upper bound (cf. Lemma C.3).
 Proof sketch of Theorem 4.4. The case of  X  Q t (  X  ) and t (  X  ) are handled by Lemma C.3 and Lemma C.4. Now consider the case of  X  A t (  X  ). Since  X  = 0, Lemma C.6 grants the existence of a large T so that, for all t  X  T ,  X  t  X  0 . 1. Thus, by Lemma C.5, and considering t sufficiently large that C t is almost 1, the problem reduces to the consideration of  X  Q t (  X  ); in par-ticular, the conditions to apply Lemma C.3, but now for the step  X  A t (  X  ), are satisfied. Note that this also handles the case  X  O t (  X  ), since, for  X  O t (  X  ) and  X  was assumed that A is binary and ` = exp.
 C.1. Miscellaneous Technical Material Lemma C.5. For any r  X  [0 , 1) , Proof. Set g ( r ) := 1 2 ln((1 + r ) / (1  X  r )). Note that g 0 ( r ) = (1  X  r 2 )  X  1 and g 00 ( r ) = As such, g is convex (along [0 , 1)) and g 0 (0) = 1, thus g ( r )  X  r along [0 , 1). The second part follows from concavity of ln(  X  ): 1 2 Lemma C.6. Under the conditions of Theorem 4.4, Proof sketch. As discussed in the proof of Theo-rem 4.3, every step size provides a guarantee of the type for some c &gt; 0 (independent of t ). The re-sult follows by rearranging this expression and us-ing L ( A X  t )  X   X  L A &gt; 0 (i.e., nonseparability) and L ( A X  t  X  X  ( A X  t +1 )  X  0 (i.e., the convergence result,
