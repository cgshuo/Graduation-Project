 Yu-Xiang Wang yuxiangwang@nus.edu.sg Huan Xu mpexuh@nus.edu.sg Subspace clustering is a problem motivated by many real applications. It is now widely known that many high dimensional data including motion trajec-tories (Costeira &amp; Kanade, 1998), face images (Basri &amp; Jacobs, 2003), network hop counts (Eriksson et al., 2012), movie ratings (Zhang et al., 2012) and social graphs (Jalali et al., 2011) can be modelled as samples drawn from the union of multiple low-dimensional sub-spaces (illustrated in Figure 1). Subspace clustering, arguably the most crucial step to understand such da-ta, refers to the task of clustering the data into their original subspaces and uncovers the underlying struc-ture of the data. The partitions correspond to different rigid objects for motion trajectories, different people for face data, subnets for network data, like-minded users in movie database and latent communities for social graph.
 Subspace clustering has drawn significant attention in the last decade and a great number of algorithms have been proposed, including K-plane (Bradley &amp; Man-gasarian, 2000), GPCA (Vidal et al., 2005), Spectral Curvature Clustering (Chen &amp; Lerman, 2009), Low Rank Representation (LRR) (Liu et al., 2013) and S-parse Subspace Clustering (SSC) (Elhamifar &amp; Vidal, 2009). Among them, SSC is known to enjoy superb empirical performance, even for noisy data . For ex-ample, it is the state-of-the-art algorithm for motion segmentation on Hopkins155 benchmark (Tron &amp; Vi-dal, 2007). For a comprehensive survey and compar-isons, we refer the readers to the tutorial(Vidal, 2011). Effort has been made to explain the practical success of SSC. Elhamifar &amp; Vidal (2010) show that under cer-tain conditions, disjoint subspaces (i.e., they are not overlapping) can be exactly recovered. Similar guar-antee, under stronger  X  X ndependent subspace X  condi-tion, was provided for LRR in a much earlier anal-ysis(Kanatani, 2001). The recent geometric analysis of SSC (Soltanolkotabi &amp; Candes, 2012) broadens the scope of the results significantly to the case when sub-spaces can be overlapping. However, while these anal-yses advanced our understanding of SSC, one common drawback is that data points are assumed to be lying exactly in the subspace. This assumption can hardly be satisfied in practice. For example, motion trajec-tories data are only approximately rank-4 due to per-spective distortion of camera.
 In this paper, we address this problem and provide the first theoretical analysis of SSC with noisy or corrupt-ed data. Our main result shows that a modified version of SSC (see (2.2)) when the magnitude of noise does not exceed a threshold determined by a geometric gap between inradius and subspace incoherence (see below for precise definitions). This complements the result of Soltanolkotabi &amp; Candes (2012) that shows the same geometric gap determines whether SSC succeeds for the noiseless case. Indeed, our results reduce to the noiseless results of Soltanolkotabi &amp; Candes when the noise magnitude diminishes.
 While our analysis is based upon the geometric analy-sis of Soltanolkotabi &amp; Candes (2012), the analysis is much more involved: In SSC, sample points are used as the dictionary for sparse recovery, and therefore noisy SSC requires analyzing noisy dictionary. This is a hard problem and we are not aware of any previous study that proposed guarantee in the case of noisy dictio-nary except Loh &amp; Wainwright (2012) in the high-dimensional regression problem. We also remark that our results on noisy SSC are exact , i.e., as long as the noise magnitude is smaller than the threshold, the ob-tained subspace recovery is correct . This is in sharp contrast to the majority of previous work on structure recovery for noisy data where stability/perturbation bounds are given  X  i.e., the obtained solution is ap-proximately correct, and the approximation gap goes to zero only when the noise diminishes. Notations: We denote the uncorrupted data matrix by Y  X  R n  X  N , where each column of Y (normalized to unit vector) belongs to a union of L subspaces Each subspace S ` is of dimension d ` and contains N ` data samples with N 1 + N 2 + ... + N L = N . We ob-serve the noisy data matrix X = Y + Z , where Z is some arbitrary noise matrix. Let Y ( ` )  X  R n  X  N ` denote the selection of columns in Y that belongs to S ` , and let the corresponding columns in X and Z be denot-X = [ X (1) ,X (2) ,...,X ( L ) ] be ordered. In addition, we use subscript  X   X  i  X  to represent a matrix that excludes ligraphic letters such as X , Y ` represent the set con-taining all columns of the corresponding matrix (e.g., For any matrix X , P ( X ) represents the symmetrized convex hull of its columns, i.e., P ( X ) = conv(  X X ). P S and Proj S denote respectively the projection ma-trix and projection operator (acting on a set) to sub-space S . Throughout the paper, k  X  k represents 2-norm for vectors and operator norm for matrices; other norms will be explicitly specified (e.g., k X k 1 , k X k  X  ). Method: Original SSC solves the linear program for each data point x i . Solutions are arranged into matrix C = [ c 1 ,...,c N ], then spectral clustering tech-niques such as Ng et al. (2002) are applied on the affin-ity matrix W = | C | + | C | T . Note that when Z 6 = 0, this method breaks down: indeed (2.1) may even be infeasible.
 To handle noisy X , a natural extension is to relax the equality constraint in (2.1) and solve the following un-constrained minimization problem instead (Elhamifar &amp; Vidal, 2012): We will focus on Formulation (2.2) in this paper. No-tice that (2.2) coincide with standard LASSO. Yet, since our task is subspace clustering, the analysis of LASSO (mainly for the task of support recovery) does not extend to SSC. In particular, existing literature for LASSO to succeed requires the dictionary X  X  i to satisfy RIP (Cand`es, 2008) or the Null-space proper-ty (Donoho et al., 2006), but neither of them is satis-fied in the subspace clustering setup. 1 In the subspace clustering task, there is no single  X  X round-truth X  C to compare the solution against. Instead, the algorithm succeeds if each sample is ex-pressed as a linear combination of samples belonging to the same subspace, as the following definition states. Definition 1 (LASSO Subspace Detection Property) . We say subspaces {S ` } k ` =1 and noisy sample points X from these subspaces obey LASSO subspace detection property with  X  , if and only if it holds that for all i , the optimal solution c i to (2.2) with parameter  X  satisfies: (1) c i is not a zero vector, (2) Nonzero entries of c correspond to only columns of X sampled from the same subspace as x i .
 This property ensures that output matrix C and (nat-urally) affinity matrix W are exactly block diagonal with each subspace cluster represented by a disjoint block. 2 The property is illustrated in Figure 2. For convenience, we will refer to the second requirement alone as  X  Self-Expressiveness Property  X  (SEP), as de-fined in Elhamifar &amp; Vidal (2012).
 Models of analysis: Our objective here is to pro-vide sufficient conditions upon which the LASSO sub-space detection properties hold in the following four models. Precise definition of the noise models will be given in Section 3.  X  fully deterministic model  X  deterministic data+random noise  X  semi-random data+random noise  X  fully random model. 3.1. Deterministic model We start by defining two concepts adapted from Soltanolkotabi &amp; Candes X  X  original proposal. Definition 2 (Projected Dual Direction 3 ) . Let  X  be the optimal solution to and S is a low-dimensional subspace. The projected dual direction v is defined as Definition 3 (Projected Subspace Incoherence Prop-erty) . Compactly denote projected dual direction v ( ` ) that vector set X ` is  X  -incoherent to other points if Here,  X  measures the incoherence between corrupted subspace samples X ` and clean data points in other subspaces. As k y k = 1 by definition, the range of  X  is [0 , 1]. In case of random subspaces in high dimen-sion,  X  is close to zero. Moreover, as we will see later, for deterministic subspaces and random data points,  X  is proportional to their expected angular distance (measured by cosine of canonical angles).
 Definition 2 and 3 are different from their original ver-sions proposed in Soltanolkotabi &amp; Candes (2012) in that we require a projection to a particular subspace to cater to the analysis of the noise case.
 Definition 4 (inradius) . The inradius of a convex the largest Euclidean ball inscribed in P .
 the data points. Well-dispersed data lead to larger in-radius and skewed/concentrated distribution of data have small inradius. An illustration is given in Fig-ure 3.
 Definition 5 (Deterministic noise model) . Consid-er arbitrary additive noise Z to Y , each column z i characterized by the three quantities below:  X  := max Theorem 1. Under deterministic noise model, com-pactly denote  X  If  X  ` &lt; r ` for each ` = 1 ,...,L , furthermore then LASSO subspace detection property holds for all weighting parameter  X  in the range  X   X   X   X   X   X   X  which is guaranteed to be non-empty.
 Remark 1 (Noiseless case) . When  X  = 0 , i.e., there is no noise, the condition reduces to  X  ` &lt; r ` , precisely the form in Soltanolkotabi &amp; Candes (2012). Howev-er, the latter only works for the the exact LP formula-tion (2.1) , our result works for the (more robust) un-constrained LASSO formulation (2.2) for any  X  &gt; 1 r . Remark 2 (Signal-to-Noise Ratio) . Condition  X   X  3 r 2 +8 r +2 can be interpreted as the breaking point under increasing magnitude of attack. This suggests that SS-C by (2.2) is provably robust to arbitrary noise having signal-to-noise ratio (SNR) greater than  X  1 r ( r  X   X  ) (Notice that 0 &lt; r &lt; 1 , we have 3 r 2 + 8 r + 2 =  X (1) .) Remark 3 (Geometric Interpretation) . The geomet-ric interpretation of our results is give in Figure 4. On the left, Theorem 2.5 of Soltanolkotabi &amp; Candes (2012) suggests that the projection of external data points must fall inside the solid blue polygon, which is the intersection of halfspaces defined by dual direc-tions (blue dots) that are tangent planes of the red in-scribing sphere. On the right, the guarantee of The-orem 1 means that the whole red sphere (analogous to uncertainty set in Robust Optimization (Ben-Tal &amp; Nemirovski, 1998; Bertsimas &amp; Sim, 2004)) of each external data point must fall inside the dashed red poly-gon, which is smaller than the blue polygon by a factor related to the noise level.
 Remark 4 (Matrix version of the algorithm) . The theorem suggests there X  X  a single  X  that works for all x , X  X  i in (2.2) . This makes it possible to extend the results to the compact matrix algorithm below which can be solved numerically using alternating di-rection method of multipliers (ADMM) (Boyd et al., 2011). See the supplementary material for the details of the algorithm. 3.2. Randomized models We analyze three randomized models with increasing level of randomness.  X  Determinitic+Random Noise. Subspaces and  X  Semi-random+Random Noise. Subspace is de- X  Fully random. Both subspace and samples are Definition 6 (Random noise model) . Our random noise model is defined to be any additive Z that is (1) columnwise iid; (2) spherical symmetric; and (3) k z i k X   X  with high probability.
 Example 1 (Gaussian noise) . A good example of our random noise model is iid Gaussian noise. Let each entry Z ij  X  N (0 , X / with probability at least 1  X  C/N 2 for some constant C (by Lemma B.2).
 Theorem 2 (Deterministic+Random Noise) . Under random noise model, compactly denote r ` , r and  X  ` as in Theorem 1, furthermore let If r &gt; 3 / (1  X  6 ) and  X  ` &lt; r ` for all ` = 1 ,...,k , furthermore then with probability at least 1  X  7 /N , LASSO subspace detection property holds for all weighting parameter  X  in the range  X   X   X   X   X   X   X  which is guaranteed to be non-empty.
 Remark 5 (Margin of error) . Compared to Theo-rem 1, Theorem 2 considers a more benign noise which leads to a much stronger result. Observe that in the random noise case, the magnitude of noise that SSC can tolerate is proportional to r `  X   X  `  X  the difference of inradius and incoherence  X  which is the fundamental geometric gap that appears in the noiseless guarantee of Soltanolkotabi &amp; Candes (2012). We call this gap the Margin of error .
 We now analyze this margin of error. We start from the semi-random model, where the distance between two subspaces is measured as follows.
 Definition 7. The affinity between two subspaces is defined by: subspaces. Let U k and U ` be a set of orthonormal bases of each subspace, then aff( S k , S ` ) = k U T k U ` k F When data points are randomly sampled from each subspace, the geometric entity  X  ( X ` ) can be expressed using this (more intuitive) subspace affinity, which leads to the following theorem.
 Theorem 3 (Semi-random+random noise) . Suppose N ` =  X  ` d ` +1 data points are randomly chosen on each S , 1  X  `  X  L . Use as in Theorem 2 and let c (  X  ) be a positive constant that takes value 1 / greater than some numerical constant  X  o . If max thermore then LASSO subspace detection proper-ty holds for some  X  4 with probability at least 1  X  7 N  X  P L ` =1 N ` exp(  X  p d ` ( N `  X  1))  X  Remark 6 (Overlapping subspaces) . Similar to Soltanolkotabi &amp; Candes (2012), SSC can handle over-lapping subspaces with noisy samples, as subspace affinity can take small positive value while still keeping the margin of error positive. Theorem 4 (Fully random model) . Suppose there are L subspaces each with dimension d , chosen indepen-dently and uniformly at random. For each subspace, there are  X d + 1 points chosen independently and uni-formly at random. Furthermore, each measurements are corrupted by iid Gaussian noise  X  N (0 , X / Then for some absolute constant C , the LASSO sub-space detection property holds for some  X  with proba-bility at least 1  X  C N  X  Ne  X  and Remark 7 (Trade-off between d and the margin of error) . Theorem 4 extends our results to the paradigm where the subspace dimension grows linearly with the ambient dimension. Interestingly, it shows that the between d and robustness to noise. Fortunately, most interesting applications indeed have very low subspace-rank, as summarized in Table 1.
 Remark 8 (Robustness in the many-cluster setting) . Another interesting observation is that the margin of error scales logarithmically with respect to L , the num-ber of clusters (in both log  X  and log N since N = L (  X d + 1) ). This suggests that SSC is robust even if there are many clusters, and Ld n .
 Remark 9 (Range of valid  X  in the random setting) . Substitute the bound of inradius r and subspace inco-herence  X  of fully random setting into the  X   X  X  range of Theorem 3, we have the the valid range of  X  is for some constant C 1 , C 2 . This again illustrates that the robustness is sensitive to d but not L . In this section, we lay out the roadmap of the proof for Theorem 1 to 4. Instead of analyzing (2.2) direct-ly, we consider an equivalent constrained version by introducing slack variables: The constraint can be rewritten as The dual program of (4.1) is: Recall that we want to establish the conditions on noise magnitude  X  , structure of the data (  X  and r in deterministic model and affinity in semi-random mod-el) and ranges of valid  X  such that by Definition 1, the solution c i is non-trivial and has support indices inside the column set X ( ` )  X  i (i.e., satisfies SEP). We focus on the proof of Theorem 1 and 2 and briefly explain the randomized models. Indeed, Theorem 3 and 4 follow directly by plugging to Theorem 2 the bound of r and  X  from Soltanolkotabi &amp; Candes (2012) (with some modifications). The proof of Theo-rem 1 and 2 constitutes three main steps: (1) proving SEP, (2) proving non-trivialness, and (3) showing ex-istence of proper  X  . 4.1. Self-Expressiveness Property We prove SEP by duality. First we establish a set of conditions on the optimal dual variable of D 0 corre-sponding to all primal solutions satisfying SEP. Then we construct such a dual variable  X  as a certificate of proof. 4.1.1. Optimality Condition Define general convex optimization: We state Lemma 1, which extends Lemma 7.1 in Soltanolkotabi &amp; Candes (2012). The proof is deferred to the supplementary material.
 Lemma 1. Consider a vector y  X  R d and a matrix A  X  R d  X  N . If there exists a triplet ( c,e, X  ) obeying y = Ac + e and c has support S  X  T , furthermore the dual certificate vector  X  satisfies then any optimal solution ( c  X  ,e  X  ) to (4.4) obeys c  X  0 .
 The next step is to apply Lemma 1 with x = x ( ` ) i and A = X  X  i and then construct a triplet ( c,e, X  ) such that dual certificate  X  satisfying all conditions and c satisfies SEP. Then we can conclude that all optimal solutions of (4.1) satisfy SEP. 4.1.2. Construction of Dual Certificate To construct the dual certificate, we consider the fol-lowing fictitious optimization problem that explicitly requires that all feasible solutions satisfy SEP 5 (note that one can not solve such problem in practice with-out knowing the subspace clusters).
 This problem is feasible. Moreover, it turns out that the dual solution of this fictitious problem v is a good candidate as our dual certificate. Observe that v au-tomatically satisfies the first three conditions in Lem-ma 1 and we are left to show that for all data point x  X  X  \X ` , Let  X  1 and  X  2 be the projection of  X  to subspace S ` and its complement respectively. The strategy is to provide an upper bound of | X  x, X   X  X  then impose the inequality on the upper bound. | X  x, X   X  X  = | X  y + z, X   X  X  X | X  y, X  1  X  X  + | X  y, X  2  X  X  + | X  z, X   X  X  To complete the proof, we need to bound k  X  1 k and k  X  2 k and the two cosine terms (for random noise mod-el). The proof makes use of the geometric properties of symmetric convex polytope and optimality of solution. See the supplementary material for the details. 4.2. Non-trivialness and existence of  X  The idea is that when  X  is large enough, trivial solution c by setting
OptVal( D 0 ) =  X  x ( ` ) i , X   X  X  X  Notice that (4.8) essentially requires that  X  &gt; A and (4.7) requires  X  &lt; B for some A and B . Hence, ex-istence of a valid  X  requires A &lt; B , which leads to the condition on the error magnitude  X  &lt; C and com-pletes the proof. While conceptually straightforward, the details of the proof are involved and left in the supplementary material due to space constraints. 4.3. Randomization Our randomized results consider two types of random-ization: random noise and random data .
 Random noise model improves the deterministic guar-antee by exploiting the fact that the directions of the noise are random. By the well-known bound on the area of spherical cap (Lemma B.1), the cosine terms in (4.7) diminishes when the ambient dimension grows. Similar advantage also appears in the bound of k  X  1 k and k  X  2 k and the existence of  X  .
 Randomization of data provides probabilistic bound-s of inradius r and incoherence  X  . The lower bound of inradius r follows from a lemma in the study of isotropy constant of symmetric convex body (Alonso-more effort. It involves showing that projected dual di-rections v ( ` ) i (see Definition 2) distributes uniformly on the subspace projection of the unit n-sphere, then ap-plying the spherical cap lemma for all pairs of ( v ( ` ) We defer the full proof in the supplementary material. To demonstrate the practical implications of our ro-bustness guarantee for LASSO-SSC, we conduct three numerical experiments to test the effects of noise mag-nitude  X  , subspace rank d and number of subspace L . To make it invariant to parameter, we scan through an exponential grid of  X  ranging from to n = 100, relative sampling  X  = 5, subspace and data are drawn uniformly at random from unit sphere and then corrupted by Gaussian noise Z ij  X  N (0 , X / We measure the success of the algorithm by the rela-tive violation of Self-Expressiveness Property defined below.
 where M is the ground truth mask containing al-l ( i,j ) such that x i ,x j  X  X ( ` ) for some ` . Note that RelViolation ( C, M ) = 0 implies that SEP is satisfied. We also check that there is no all-zero columns in C , and the solution is considered trivial otherwise. The simulation results confirm our theoretical findings. In particular, Figure 5 shows that LASSO subspace de-tection property is possible for a very large range of  X  and the dependence on noise magnitude is roughly 1 / X  as remarked in (3.3). In addition, the sharp contrast of Figure 6 and 7 demonstrates precisely our observations on the sensitivity of d and L in Remark 7 and 8. A remark on numerical algorithms: For fast com-putation, we use ADMM implementation of LASSO solver 6 . It has complexity proportional to problem size and convergence guarantee (Boyd et al., 2011). We also implement a simple solver for the matrix ver-sion SSC (3.1) which is consistently faster than the column-by-column LASSO version. Details of the al-gorithm and its favorable empirical comparisons are given in the supplementary materials.
 We presented the first theoretical analysis for noisy subspace segmentation problem that is of great prac-tical interests. We showed that the popular SSC al-gorithm exactly (not approximately) succeeds even in the noisy case, which justified its empirical success on real problems. In addition, we discovered a funda-mental trade-off between robustness to noise and the subspace dimension, and we found that robustness is insensitivity to the number of subspaces. Our analysis hence reveals fundamental relationships of robustness, number of samples and dimension of the subspace. These results lead to new theoretical understanding of SSC, as well as provides guidelines for practition-ers and application-level researchers to judge whether SSC could possibly work well for their respective ap-plications.
 Open problems for subspace clustering include the graph connectivity problem raised by Nasihatkon &amp; Hartley (2011), missing data problem (a first attempt by Eriksson et al. (2012), but requires an unrealistic number of data), sparse corruptions on data and oth-ers. One direction closely related to this paper is to introduce a more practical metric of success. As we illustrated in the paper, subspace detection property is not necessary for perfect clustering. In fact from a pragmatic point of view, even perfect clustering is not necessary. Typical applications allow for a small num-ber of misclassifications. It would be interesting to see whether stronger robustness results can be obtained for a more practical metric of success.
 H. Xu was supported by the Ministry of Education of Singapore through National University of Singapore startup Grant R-265-000-384-133.

