 data cases. Each data case consists of a subset of items in I. An association rule is an implication of the form X + Y, whereXcI,YcI,andXnY=O.TheruleX+Yholds in D with confidence c if c% of data cases in D that support X also support Y. The rule has support s in D if s% of the data case in D contains X v Y. The problem of mining association rules is to generate all association rules that have support and confidence greater than the user-specified Copyright ACM 1999 I-581 13-143-7/99/08...$5.00 A typical association rule mining algorithm works in two steps. The first step finds all large itemsets (a set of items) that meet the minimum support constraint. The second step generates rules from all large itemsets that satisfy the minimum confidence constraint. 
The key strength of association rule mining is that it can efficiently discover the complete set of associations that exist in data. These associations provide a complete picture of the underlying regularities in the domain. However, this strength comes with a major drawback. The number of discovered associations can be huge, easily in the thousands or tens of thousands (see Section 8). Clearly, such a large number of associations are very difficult, if not impossible, to be analyzed by a human user. This problem is particularly bad with those data sets whose items are highly correlated. arbitrary small subset of the rules to the user if there are indeed a large number of them that exist in the data because this small subset can only give a partial picture of the domain. The question then is:  X  X an we preserve the full power of association rule mining (i.e., its completeness) without overwhelming the user? X  This paper shows that it is possible. A novel technique is proposed to prune those insignificant rules and to find a special subset of association rules that represent the essential underlying relationships in the data. We call this subset of associations the direction setting (DS) rules. The DS rules give a summary of the behavior of the discovered associations. They represent the essential relationships or structure (or skeleton) of the domain. The non-DS rules simply give additional details. Using the DS rules as a summary, the user can interactively focus on the key aspects of the domain and selectively view the relevant details (non-DS rules). relational table, which consists of a set of records described by a number of attributes. An item is an attribute value pair, i.e., (attribute = value) (numeric attributes are discretized). Association rule mining in such data is typically targeted at a specific attribute because the user normally wants to know how other attributes are related to this target attribute (which can have many values) [13, 41. With a target attribute, we can express an itemset as follows (instead of a set of items as in [2]): where y is an item (or a value) of the target attribute, and X is a set of items from the rest of the attributes. For simplicity, we call this itemset a rule hereafter, regardless of whether it is significant or not. We also say a rule is large if it meets the minimum support. framework, although we can also use it (see Section 6). 
Minimum confidence does not reflect the underlying relationship of the domain represented by the data [4]. 
Instead we use statistical correlation as the basis for finding rules that represent the fundamental relations of the domain. technique X , which consists of two steps, pruning and summarization. Below, we introduce them briefly. 
It is well known that many discovered associations are redundant or minor variations of others. Their existence may simply be due to chance rather than true correlation. 
Thus, those spurious and insignificant rules should be removed. This is similar to pruning of overfitting rules in classification [21]. Rules that are very specific (with many conditions) tend to overfit the data and have little predictive power. Although association rules are not normally used for prediction, rules that only capture the irregularities and idiosyncrasies of the data have no value and should be removed. An example of such a rule is shown below. Example 1: We have the following two rules, 
If we know Rl, then R2 is insignificant because it gives little extra information. Its slightly higher confidence is more likely due to chance than to true correlation. It thus should be pruned. Rl is more general and simple. General and simple rules are preferred. chi-square test (x2) for correlation from statistics [ 171. Pruning can reduce the number of rules substantially. However, the number of rules left can still be very large. 
This step finds a subset of the rules, called direction setting rules (or DS rules), to summarize the unpruned rules. 
Essentially, DS rules are significant association rules that set the directions for non-DS rules to follow. The direction of a rule is the type of correlation it has, i.e., positive correlation or negative correlation or independence, which is also computed using x2 test. Let us see an example. Example 2: We have the following discovered rules: x2 analysis shows that having a job is positively correlated to the grant of a loan, and owning a house is also positively correlated to obtaining a loan. Then, the following association is not so surprising to us: because it intuitively follows Rl and R2. We can use Rl and R2 to provide a summary of the three rules. Rl and R2 are DS rules as they set the direction (positive correlation) that is followed by R3. In real-life data sets, a large number of associations are like R3. essential relationships of the domain. The non-DS rule is not surprising if we already know the DS rules. However, this, by no means, says that non-DS rules are not interesting. Non-DS rules can provide further details about the domain. For example, the non-DS rule above (R3) gives a higher confidence, which may be of interest to the user. 
Using DS rules to form a summary is analogous to summarization of a text article. From the summary, we know the essence of the article. If we are interested in the details of a particular aspect, the summary can point us to them in the article. In the same way, the DS rules give the essence of the domain and points the user to those related non-DS rules. Non-DS rules are basically combinations of DS rules. In the above Example 2, R3 is a combination of Rl and R2. show that although the number of discovered rules can be huge, the number of DS rules is very small (see Section 8). 
They can be analyzed manually by the human user to obtain the essential relationships in the data. He/she can then focus his/her attention on those interesting aspects of the relationships, and to see the relevant non-DS rules. This process can be easily facilitated by an interactive user interface (Section 7). The proposed technique makes association rule mining effective and practical for data sets whose items are highly correlated. The user can now obtain a complete picture of the domain without being overwhelmed by a huge number of rules. 
The problem of too many rules has been studied by many researchers. [8] proposed an approach to allow the user to specify what he/she wants to see using templates. The system then retrieves those match rules from the set of discovered rules. This method, however, does not prune those insignificant rules and does not provide a summary of the discovered rules. [22, 11, 12, 191 proposed a number of methods for finding unexpected rules. Instead of asking the user to specify what he/she wants to see as in [8], these approaches ask the user to specify his/her existing knowledge about the domain. 
The system then finds those unexpected rules by comparing the user X  X  knowledge with the discovered rules. Again, these methods do not prune and do not attempt to summarize the association rules. specified by the user in rule mining to generate only the relevant rules. Essentially, the constraints restrict the items or combination of items allowed to participate in mined rules. This approach also does not prune those insignificant rules and does not summarize the unpruned rules, pruning association rules. A cover is basically a subset of the discovered associations that can cover the database. The number of rules in a cover can be quite small. A greedy algorithm is proposed to find a good cover and the remaining rules are pruned. The problem with this method is that the advantage of association rules, its completeness, is lost. Clearly, a better approach is to summarize the discovered rules. From this summary, the user can obtain an overall picture of the domain. improvement, which is the difference between the confidence of a rule and the confidence of any proper sub-rule with the same consequent. Those rules that do not meet this minimum improvement in confidence are pruned. [24] also proposed a related technique. Our pruning method is similar. However, we use chi-square test as the basis for pruning (minimum improvement can be easily incorporated in our framework). We will see in Section 8 that even after pruning the number of rules left can still be very large. Summarization is thus important. The methods in [4, 241 do not perform summarization. classification research for rule pruning such as pessimistic error rate [21] and minimum description length based pruning [ 151. In our work, we choose the widely used chi-square test statistics for rule pruning. redundant rules, i.e., simple and strict redundancy. Essentially, a rule is redundant with respect to another rule if the support and confidence of the redundant rule are always at least as large as the support and confidence of the latter. Simple redundancy tries to remove those rules that are derived from the same itemset. For example, AB =&gt; C is redundant with respect to the rule A =&gt; BC. This, however, does not happen in our situation because in our case one itemset represents only one rule. Strict redundancy applies to two itemsets and one is a subset of the other, e.g., X =&gt; Y is redundant with respect to X =&gt; YZ. This situation also does not apply in our situation because we focus on association rules that use only one fixed attribute on the right hand side. Also our proposed technique does not use minimum confidence for rule generation (see the problems with minimum confidence in [4]), but statistical correlation (or significance). [S]. It uses chi-square test to measure the correlation. A correlation rule is a set of correlated items. It does not perform pruning or summarization. [IO] introduces a technique for clustering association rules. It mainly deals with generation of numeric associations, i.e., how to join the adjacent intervals to produce more general rules and fewer rules. Clearly, both works are different from ours. Chi-square test statistics &amp;) is a widely used method for testing independence and/or correlation [ 171. In our proposed technique, it is used in pruning as well as in finding direction setting rules. Below, we give an introduction to chi-square test. observed frequencies with the corresponding expected frequencies. The closer the observed frequencies are to the expected frequencies, the greater is the weight of evidence in favor of independence. Example 3: In a loan application domain, we have 500 :::I: pgqq ;:: Column Total: 280 220 500 Our question is  X  X s loan approval dependent on whether one has a job or not? X  In dealing with this problem we set up the hypothesis that the two attributes are independent. We then compute the expected frequency for each cell as follows: Of the 500 people included in the sample, 300 (60% of the total) had a job, while 200 (40% of the total) had no job. If the two attributes are truly independent, we would expect the 280 approved cases to be divided between job = yes and job = no in the same ratio (60% and 40%); similarly, we would expect the 220 not-approved cases to be divided in the same fashion. 2 is used to test the significance of the deviation from the expected values. Let f. be an observed frequency, and be an expected frequency. The 2 value is defined as: A 2 value of 0 implies the attributes are statistically independent. If it is higher than a certain threshold value (e.g. 3.84 at the 95% significance level [17]), we reject the independence assumption. For our example, we obtain 2 = 34.63. This value is much larger than 3.84. The independence assumption is rejected. Thus, we say that the loan approval is correlated to (or dependent on ) whether one has a job. observed frequency is 200 (i.e., the support count of the rule) and its expected frequency is only 168 (= 300 * 280 / 500). e.g., whether having a job is positively or negatively correlated to the approval of a loan. Below, we give the definitions in the context of association rule mining. Definition 1 (correlated): Let s be a minimum support and c be a significance level. X and y of a rule, X + y, are said to be (s, c) correlated (hereafter, merely correlated) if the following two conditions are met: 1. The rule X  X  support exceeds s. 2. The 2 value for the rule with respect to the whole Definition 2 (uncorrelated or independent): Let s be a minimum support and c be a significance level. X and y of a rule, X 4 y, are said to be (s, c) uncorrelated (hereafter, merely uncorrelated or independent) if the following two conditions are met: 1. The rule X  X  support exceeds s. 2. The x2 value for the rule with respect to the whole direction) of an association rule r, X + y, is to compare the rule with the whole population or the whole data set. Or more specifically, it is to compare with the rule that has the same conclusion as r but no condition, i.e., + y. The generic contingency table used is shown in Figure 3. rather than frequency to conform to association rule mining. All support counts are available from the two rules (i.e., X -+ y and + y), after running a mining algorithm. For example, to test the rule in Example 3 (RI), we compare it with R2: Clearly, these two rules specify completely the contingency table in Figure 2 (or Figure 3). R2, which has no condition, gives the column total for approved (280) and the total number of cases (500) in the data. Rl, which represents the first cell in the table, also has the number 300 for the row total (which is simply the support count of Job = yes). With all this information, the rest of the cells can be computed. Similar to those in [5], we define three types of correlation of a rule. For this work, we also call them the directions of a rule. Definition 3 (type of correlation or direction): 
Positive correlation: if X and y of a rule r, X -+ y, are Direction setting (DS) rules are the positively correlated association rules that set the directions for non-direction setting (non-DS) rules to follow. We give some definitions. Definition 4 (direction setting rule): A rule r is a direction setting (DS) rule, if it satisfies the following conditions: 1. It has the direction of 1 (positive correlation). 2. Its direction is not an element of the set of expected directions of a rule r is defined as follows: In this paper, we are only interested in the positively expected directions of r is (Ei}, i = 1, , . ., k. Lemma: All positively correlated l-condition rules are direction setting rules. Proof: It follows directly from Definition 4 and 5. Notes about the above two definitions: l In Definition 5, the assumptions (a) to (d) are l The second condition (2) of Definition 4 basically To further explain the definitions, we list all possible direction combinations of rl, rres, and r (Figure 4) using the notation: D (1)-1,-l z-1 E (l)-I,0 z-1 F (1) -1, 1 := 0 A(l), B(1) and C(1) conform to our expectations (a), (b) and (c) in Definition 5. That is, given the directions of rl and rresr on the left-hand-side of  X := X , if r has the direction 
We can also view the rule r as a combination of more on the right-hand-side of  X := X , we say r X  X  direction is expected. In fact, D(l), E( 1) and F(1) can also be seen as expected. However, they are not interesting because we are only interested in the positively correlated rules. The interesting situations occur in C(2), D(2), E(2) and F(2) (shaded in Figure 4) because the direction of r is 1, but the expected direction of the rl and r,,,, combination is 0 or unknown, which are our cases (c) and (d) in Definition 5. When such situations occur, we say that r sets a new direction. We call r a potential DS rule. r will be called a DS rule if for all possible combinations of rl and r,.,,,, the direction of r is different from the expected directions. Since we are only interested in positive correlations, the remaining situations in Figure 4 are not relevant (see also Section 6). Definition 6 (non-direction setting rules): A non-direction setting (non-DS) rule is a positively correlated rule that is not a DS rule. This section presents the basic ideas of pruning and finding direction setting rules. The next section gives the detailed algorithm, which performs both tasks. 5.1. Pruning of association rules Section 3 shows that to test for correlation between the condition and the consequent of a rule r, X -+ y, we compare it with the rule R, + y, to see whether r is significant with respect to R. Those rules that are not positively correlated are removed. However, we can do reproduced here as Example 4). Example 4: We have the following two rules, 
If we know R, then r is of limited use because it gives little extra information. Its slightly higher confidence is more likely due to chance than to true correlation. We say r can be pruned with respect to R because within the subset of data cases covered by R, r is not significant. (A rule covers a set of data cases, if the data cases satisfy the conditions of the rule.) The pruning proceeds as that in Section 3. However, instead of using the whole data set, here we test the correlation of r with respect to R as r only covers a subset of the data cases that are covered by R. If r does not show a positive correlation with respect to R, it should be pruned. In general, pruning is done as follows: l Given a rule r, we try to prune r using each ancestor 
The process for finding DS rules is as follows: x2 test is first used to evaluate each l-condition rule to determine its direction status, i.e., 1 (positive correlation), -1 (negative correlation), or 0 (independence). Then, it proceeds level-by-level to analyze each rule and decide whether it follows the direction that has already been set by rules at previous levels, or whether it sets a new direction. That is, at level 2 we analyze only 2-condition rules, at level 3, we only analyze 3-condition rules and so on. (For easy discussion, from now on we will use the level number and the number of conditions of a rule interchangeably). The analysis proceeds as follows: At level k (k &gt;l), for each k-condition rule r, we first determine its direction. We then examine each combination of l-condition rule ri and (k-1)-condition rule r,,,, of r to determine whether r follows the expected direction set by rI and rres,. If r follows the direction set by at least one such combination, we say r is not a DS rule. If r does not follow the direction set by any combination, we say r sets a new direction, and it is a DS rule. illustrated using the following if-statement (Figure 5) (the corresponding situations in Figure 4 are also indicated): 
Theorem: Using the above procedure to identify DS 
Proof: See [ 141 for the full proof. possible combination of some DS rules and/or independence rules (0 direction) is a non-DS rule. The reason is that such a combination may not meet the minimum support, or may not show positive correlation. can derive three important points: l Every DS rule r is unexpected with respect to all r, l After seeing the DS rules, the directions of non-DS l DS rules can guide the user to see the related non-DS These points enable us to build a simple user interface that allows the user to focus on the essential aspects (DS rules) of the domain and selectively view the relevant details (non-DS rules). See Section 7. Figure 6 gives the algorithm (called P-DS) for both pruning and finding DS rules. The input parameters are F and T. F is the set of discovered large rules. T is the 2 value at a particular significance level. Two points to be noted: level (line 1) from level-l to level-n (where n is the highest level). For each rule r, X+ y, at a particular level the algorithm works as follows: The procedure compDir computes the type of correlation (or direction) of r (in line 2), which is given to r.dir.  X 3 y X  is a rule without any condition. In line 3 and 4, if r is a level-l rule and its direction is 1, then r is a DS rule (DSR contains the set of all DS rules). If r X  X  direction is not 1, we record that r is pruned by  X + y X  by assigning  X + y X  to r.prune (line 5). 
This saved information is important for subsequent pruning (see procedure evalPrune in Figure 8). (r.prune is initialized to 0, indicating that r is not pruned.) Line 6 7, if r is pruned and r cannot be a DS rule, we can exit the for-loop. Anyone of the two conditions would not be sufficient for the exit. Evaluation of pruning is done in line 8 using the procedure evaZPrune. Line 9 checks to see whether it has been determined that r cannot be a DS rule. 
If so, there is no need to proceed. From line 10-21, the algorithm analyzes r by considering the four cases. This part has been discussed in Section 5.2. r.justify is used to then exit-for; record all rl and r,,,, combinations that justify r to be a potential DS rule (line 18 and 20). This information is helpful to the user in understanding the DS rules (see Section 7). When we know that r is not a DS rule (line 11, 14 and 17), no recording is needed (we set r.justify = 0). justified to be a DS rule. However, if r can be pruned cannot be pruned (r.prune = 0), then it is a true DS rule (line 25), and it is included in DSR. All unpruned rules are in unprnRules (line 28), and all non-DS rules are in non-DSR (line 29). evalPrune. Procedure compDir (Figure 7) uses the 2 test to compute the correlation or direction of r. R is an ancestor rule of r and both have the same consequent. 
Figure 7. Computing the direction or correlation of a rule r assumption, i.e., the conditions and the consequent are correlated. Line 2-5 determine the type of correlation or direction (Definition 3).  X  X . cover*(R.suplR. cover) X  is the expected frequency. xcover is the number of data cases that satisfy the conditions of rule x. prune r using rres,. In line 1, if r,,,, itself has been pruned previously (i.e., r,,,,.prune # 0). Then, the algorithm needs to find the rule that prunes rrest This is shown in line 2. In is pruned. We set r.prune = rrpst to provide a link for pruning of higher level rules than r. In line 5, if r is a positive correlation, then we try to compare it with r,,,Y, using chi-square test. If r does not show a positive pruned by r,,,, (line 6). 
Figure 8. Evaluating pruning of a rule r with respect to its Complexity of the algorithm: Let M be the number of Additional features: The above algorithm can be enhanced As mentioned earlier, DS rules form the summary of the regularities in the data and non-DS rules can provide further details. A user interface has been built to allow the user to interactively focus on the essential aspects (DS rules) of the domain and selectively view the relevant details (non-DS rules). The interface has many functions for interactive exploration of DS and non-DS rules. Due to space limitations, we only give three main functions here. We study the effectiveness of the algorithm for pruning and finding DS rules. We used 30 data sets in our experiments. 25 of them are obtained from the UC1 Machine Learning Repository [ 161, and 5 are our real-life application data sets. All these data sets contain huge numbers of associations, which present a major problem for using association rule mining to give a complete picture of the underlying domains. Currently, the 25 public domain data sets are mainly used for classification research. However, classification can only give the user a partial picture of the data, i.e., many interesting/useful rules are not discovered [20, 131. With our proposed technique, we show that association rule mining can now be effectively and practically applied to these data sets to give a complete picture of the underlying relationships in the data. sets. In all the experiments, we use a fixed attribute (target attribute) on the right hand side of association rules. The target attribute is a categorical attribute and can have a number of values (called rut-get items). For the 25 UC1 data used for classification. For our 5 real-life data sets, the target attributes are suggested by our users. For all these data sets, even with a target attribute, the numbers of associations discovered are huge (note that our modified association rule miner is able to use the target attribute [ 131). Many data sets cause combinatorial explosion. Due to this reason, we set a hard limit of 80,000 on the total number of large rules processed in memory. Even with such a large limit, mining cannot be completed for many data sets. Using a hard limit is justified because proceeding further only generates rules with many conditions that are hard to understand and difficult to use. discretize these attributes into intervals using the target attribute. There are a number of discretization algorithms in machine learning literature for the purpose. We use the entropy-based method given in [6]. The code is taken from MLC++ [9]. significance levels for the 2 test (95% and 90%) and different minimum support (minsup) values (2% and 1%). The 95% significance level for the x2 test is commonly used on the results. We used the minimum supports of 2% and with these support thresholds are sufficiently predictive. 
Table 1 gave the results obtained using the significance level of 95% for the x2 test and minsup = 1%. The other results can be found in [ 143, but they are summarized here. 
Below, we explain each column in Table 1. The final row gives the average value for each column. 
Column 1: It gives the name of the data set (the last 5 are our real-life data sets). The number of records (or cases) in these data sets range from a few hundreds to tens of thousands. Column 2: It shows the number of items in each data set. 
Recall, an item is an attribute and a value (or an interval) pair, i.e., (CUttributei&gt; = &lt;valuej&gt;). 
Column 3: It shows the number of items (or values) in the target attribute of each data set. Column 4: It gives the number of large rules generated from each data set. We can see that the number of large rules generated from an association rule miner is huge for each data set. Almost half of the data sets cannot be completed even under the hard limit of 80,000. Column 5: It gives the number of positively correlated (PC) rules found in each data set. The number is much smaller. However, on average, there are still more than 20,000 of them. Column 6: It gives the number of positively correlated (PC) rules after pruning. The number is reduced drastically. More than 96% of the PC rules are pruned. 
However, on average, the number of unpruned rules is hard to be analyzed by a human user. Column 7: It gives the number of 1 -condition DS rules, 
Column 8: It gives the total number of DS rules found in each data set. The number of DS rules is manageable and can be analyzed manually. On average, only 131 rules are DS rules. Many of them are l-condition DS rules. Column 9: It gives the average number of conditions in the 
DS rules. We can see that the DS rules are mostly short rules. Column 10: It gives the running time in second (running on 512MB Spare 2) for each data set. This time includes rule generation (data reside on disk), pruning and finding direction setting rules. We can see that the proposed technique is very efficient. We could not log any time for pruning and finding DS rules alone due to the efficiency. From the summary of the other results (below Table l), we make the following observation: . Data mining is to find patterns or regularities to summarize the data. If it also produces a huge number of patterns, it will be of limited use because a human user does not have the ability to analyze these patterns. However, if such a huge number of patterns do exist in the data, it will not be appropriate to arbitrarily discard any of them or to generate only a small subset of them. It is much more desirable if we can summarize them. This paper proposes such a technique. This technique first prunes off those rules that contain little extra information as compared to their ancestors, and then identifies the direction setting rules to give a global picture of the underlying relationships in the domain. Although the number of discovered associations can be very large, experimental results and real-life applications have shown that the number of direction setting rules is typically very small and with very few conditions. They can be manually inspected by a human user without too much effort. technique that may potentially produce a large number of patterns should provide a technique to summarize the findings or the generated patterns. In this way, the user will be able to obtain an overall picture of the domain without being overwhelmed by a large number of detailed patterns. From this summarized information, he/she can then find some interesting aspects to focus on. The proposed technique represents a major step towards this direction. [l] Aggarwal, C., and Yu, P.  X  X nline generation of [2] Agrawal, R., Imielinski, T., Swami, A.  X  X ining [31 [41 [61 [71 @I [I31 1141 H71 [I91 ml [241 
