 the raw DP algorithm on real datasets.
 algorithm to estimate the change-point locations.
 also give a non asymptotic inequality to upper-bound the ` 2.1 Framework problem. Off-line estimation of change-point locations wi thin a signal ( Y the  X  ? where  X  as follows. Let us consider: where Y elements equal to one and  X  estimation problem (1) can thus be tackled as a variable sele ction one: where k u k and k u k 2 following counterpart objective in model (1): which consists in imposing an ` is the sparsity-enforcing property of the ` of [21], used for efficient variable selection.
 in mean, and we provide a non asymptotic upper bound for the ` function with a probability tending to one. 2.2 Main results entry to zero. Let where C n index in J . The vector  X  n see Propositions 1 and 2 below.
 In the following, we shall assume that the number of break poi nts is equal to K ? . locations are close to the true change-points.
 Proposition 1. Assume that the observations ( Y If  X  =  X  n is such that  X  n  X   X   X  following minimization:  X  = Arg min Since f (  X  )  X  f (0) , we get We thus obtain using the Cauchy-Schwarz inequality the foll owing upper bound Using that  X  0 C n  X   X  n  X  1 P n piecewise function is close to the true piecewise constant f unction. Proposition 2. Assume that the observations ( Y Gaussian random variables with variance  X  2 &gt; 0 . Assume also that (  X  n  X  n = A X  p log n/n Proof. By definition of  X   X  n (  X  ) in (5) as a minimizer of a criterion, we have Using (2), we get Thus, Observe that Let us define the event E = T n zero-mean Gaussian random variables, we obtain Thus, if  X  =  X  We thus obtain with a probability larger than 1  X  n 1  X  A 2 / 2 the following upper bound described hereafter.
 Estimation with a Lasso penalty We compute the first K max non-null coefficients  X   X  Since P refine the set of change-points caught by the Lasso by perform ing a post-selection. Reduced Dynamic Programming algorithm One can consider several strategies to remove ir-the following objective for each K in { 1 , . . . , K max } : S ( X   X  1 , . . . ,  X   X  K ) in running the DP algorithm over all the n observations. change-points. As n  X  X  X  , according to [15], the ratio  X  In particular,  X  ing the number of change-points  X  K :  X  K = Min Cachalot Algorithm Input Processing Output Change-point locations estimates  X   X  To illustrate our algorithm, we consider observations ( Y (  X  30 ,  X  50 ,  X  70 ,  X  90 ) = (5 ,  X  3 , 4 ,  X  2) corresponding ( X   X  possible number of change-points K The different values of the ratio  X  are (30 , 50 , 70 , 90) , thanks to the results obtained in Table 1. cal point of view, retrieving the set of change-point locati ons {  X  ? the number of change-points is usually performed thanks to a Schwarz-like penalty  X   X  a considered as an outlier detection method. 5.1 Synthetic data online at http://www.math.u-psud.fr/  X lavielle/programs/index. html . signal divided by the number of true change-points.
 minimum magnitude jump between two contiguous segments, i. e.  X  = m Min being the level of the k th segment. The number of noise replications was set to 10. provides better results than method B in terms of precision and false alarm rate. 5.2 Real data information about rock structure and especially its strati fication. (P. 206) who used Bayesian techniques to perform change-poi nts detection. well-log data processed with a median filter multiple change-point estimation method, paving the way fo r processing large datasets.
