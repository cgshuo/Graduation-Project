 The knowledge workers who strive to keep up with the latest news and trends in the field have to frequently revisit specific Web pages containing list-oriented informa-tion such as headlines, "what's new", job vacancies and event announcements. The above information can certainly help enterprises and individuals track competitions and opportunities, and understand markets and trends, however it becomes not easy for workers to keep current when information sources exceed a handful. 
RSS (Rich Site Summary/RDF Site Summary/Really Simple Syndication), a ma-chine-readable XML format for content syndi cation [1], allows users to subscribe to the desired information and receive notification when new information is available. RSS feeds send information only to the parties that are truly interested, thereby reliev-ing the pressure on email systems suffering from spam [2]. Since virtually almost any list-oriented content could be presented in RSS format, RSS demonstrates a promising solution to track and personalize the flow of new Web information. Furthermore, enterprises can take advantage of the simp licity of the RSS specification to feed in-formation inside and outside of a firewall. Today RSS has become perhaps the most widely used XML format on the Web. However, much of the current Web content is not yet enabled by RSS feeds. In order to evangelize RSS application and leverage the Web X  X  valuable contents, the avail-ability of convenient approaches to  X  X SSify X  suitable Web content has become a stringent necessity. The point is to translate existing semi-structured HTML pages into structured RSS feeds. The simplest way is to observe HTML pages and code extraction rules manually [3, 4, 5]. However, writing rules is time-consuming, error-prone and not scalable. Therefore, we need more efficient approaches for RSS feed generation, which should be automated to the largest extent possible, in order to allow for large scale extraction tasks even in presence of changes in the underlying sites. Since core contents of different RSS versions are similar in general structure and consistent in concept [1], related RSS tags are presented only in RSS 2.0 format in this paper. At the basic level, a RSS feed consists of a channel with its own metadata RSS channel can be easily extracted from the content of the title in the HTML head . metadata of the HTML head contain description or keywords , we can convert them to contents of the description in the RSS channel . If the HTML page is static, we can convert the last-modified time in the HTTP head to pubDate in RSS channel . The language of RSS channel can be extracted from the content-language or charset metadata of the HTML head . The primary contents of the information items in list-oriented pages are the title , url and release time which are the counterparts of the title , link and pubDate in the item of RSS specification. The url of a news item in HTML tags and texts in HTML pages. However, Web pages often contain multiple topics and a number of irrelevant pieces from navigation, decoration, and inter action parts [6]. It is not easy for the machine to automatically locate and convert target items since HTML is designed for presentation instead of content description [7, 8]. This paper introduces approaches to solve this problem based on different features of list-oriented information in HTML pages. 2.1 Automatic Approach Based on Time Pattern Discovery In news or  X  X hat X  X  new X  pages, the news item is often published with the correspond-ing release time. This feature is a prominent and useful clue for locating and extract-ing the target news items. Fig. 1 shows a Fujitsu press release page. Since the formats of date and time are simple and limited, the release time is easily identified and we can easily construct a database of time patterns represented in regular expressions. In our current experiment, only about 20 time patterns are required to cover almost all the time and date formats we have met on Japanese and Chinese sites. In Fig. 2, there are some typical date and time formats. 
Firstly, we create a DOM tree for the HTML page and use the number to represent the address of nodes in DOM tree. The address consists of numbers joined by a  X . X , starting with  X 0 X , and followed by the order (index of the node in its parent X  X  children nodes list) of the nodes in the tree that are ancestors of current node, starting from the top. As a bit of a special case, the address of the root is simply  X 0 X . 
Secondly, we need to extract all text nodes containing the release time of news items in DOM tree. By pre-order traversing of the DOM tree, each text node matching the time pattern in the database is named a time node TN , and its address and corre-sponding time pattern are recorded in an address list AL and a time pattern list TPL respectively. In some cases, there are multiple time patterns in a Web page, and we can output the time nodes of all time patterns, or just time nodes belonging to specific patterns selected by a heuristic rule, or just time nodes matching patterns designated by the user. It is dependent on the concrete application requirement. 
Since the syntax structure of a HTML page is usually consistent with its semantic structure, based on the DOM tree structure, AL can be segmented into sections in each of which time nodes keep spatial locality. Each address in AL can be split into a 1-dimension array based on the separator  X . X , and AL finally is converted to a 2-dimension array M . Fig. 3 shows the M corresponding to the release time listed in Fig. 1. We can segment AL by partitioning M . A triple &lt;r, c, n&gt; defines a section in M . r and c are the row number and the column number, respectively, of the top left element row of M and corresponds to a TN in DOM tree, and C[j] represent the jth column of M . M[i, j] is said to be the element in the ith row and the jth column of M and also corre-section of M as &lt;0, 0, TR&gt; . Fig. 4 shows the recursive segmentation algorithm. checkValueInArray(C[j]) checks if all the values in the jth column of the M are section &lt;r, j, n&gt; into k sub-sections in each which the values in the jth column are the same. When each sub-section contains only 1 row, the segmentation process will be stopped and we can extract the information items in the current section. 
Although HTML pages containing the time pattern have diverse contents and structures, they can be classified into two types in terms of the layout. In the first type, each news item has an individual release time. For the second type every release time is followed by multiple news items. For the list-oriented information, each item is usually displayed in an individual line, and this is an important feature for layout analysis. The line presentation relies on the DOM tree structure and specific tags such The heuristic rules are applied to select the href attribute of a suitable &lt;a&gt; node and a proper title text in the current line as the title and link in RSS feeds [9]. 
After recognition of all the items in a section, we can decide the complete border of this section. In some pages, such as the page in Fig. 1, each section has a category title for summarizing content in the section, which corresponds to the category in the RSS item of the section, and contained in continuous text nodes on the left part in the line. If category is presented in an image, we can use a similar method to check the alt attribute of the appropriate &lt;img&gt; node. If necessary, we can also extract this infor-mation automatically. 2.2 Automatic Approach Based on Repeated Tag Pattern Mining Although the approach based on time pattern discovery can generate RSS feeds with high performance, there are still some pages containing no time pattern. In HTML pages containing list-oriented information, information items are usually arranged orderly and compactly in a coherent region, with the same style of presentation and a similar pattern of HTML syntax. We call this kind of coherent region InfoBlock . In-formation items in an InfoBlock usually share a repetitive tag pattern and have a same parent node. Fig. 5 shows a repeated tag pattern and its corresponding instances (oc-currence of the pattern) in a music news page. Therefore mining the repeated tag patterns in HTML pages provides guidance for the effective extraction of information items and generation of RSS feed. 
Since it is more convenient to discover repetitive patterns by token stream, we gen-erate tag token stream by pre-order traversing DOM tree. We also create a mapping table between each tag token in the stream and the corresponding node in the DOM structed for the tag token stream and applied to induce repetitive patterns. We apply "non-overlap" (The occurrences of a repeated pattern cannot overlap each other) and "left diverse" (The tags at the left side of each occurrence of a repeated pattern belong to different tag types.) rules to filter out the improper patterns and generate suitable candidate patterns and associated instance sets [10]. For RSS feed generation, the &lt;a&gt; and &lt;text&gt; will also be removed. Finally more than 90% of the repeated patterns are discarded. 
By a method similar to that used to segment AL in section 2.1, we can partition the instance set of each repeated tag pattern into sub-sets based on structure of DOM tree. Here the basic unit is a series of nodes belonging to a repeated pattern instance instead of one time node. After the partition, the instances in each sub-set will present spatial locality. For the instances in a sub-set, we can find corresponding nodes in DOM tree, and the root node of the smallest sub-tree containing all these nodes is called RST (the root of the smallest sub-tree) node, which represent a page region, i.e. InfoB-lock . 
Since sometimes a RST node associated with a specific information item format may correspond to multiple instance sets belonging to different patterns discovered previously, each of which represents the information item format wholly or partly, we need to assess and select the best qualified set for identifying the correct border of information items under the current RST node. We create a series of criteria such as the frequency of occurrences, length, regular ity and coverage of the repeated pattern for the assessment. Regularity of a repeated instance set is measured by computing the standard deviation of the interval between two adjacent occurrences. It is equal to the standard deviation of the intervals divided by the mean of the intervals. Coverage is measured by the ratio of the volume of the contents contained by repeated instance threshold that can be adjusted by the user. An assessment usually applies one or more of above criteria, either separately or in combination [12]. Since the each news item and information items and their borders. 
The desired part i.e. list-oriented information for the RSS feed generation, usually occupies notable regions in a HTML page. Therefore, we can select the pattern whose instance set contains the maximum contents or occupies the maximum area in the HTML page. We also can list candidate patterns and show their corresponding re-gions in the page, and let the user to select the pattern compatible with his require-ments. After selecting the right pattern and identifying the border of each information item, it is easy to extract the title and link from target items due to the simple structure of news items. If necessary, we also can employ the similar method used in section 2.1 to extract the category information based on the border of each InfoBlock . 2.3 Semi-automatic Approach Based on Visual Labelling No automatic approach can process all list-oriented HTML pages well, and there are always some exceptions for a fraction of irregular or complicated pages during auto-matic RSS feed generation. Sometimes a HTML page contains several suitable re-gions, but user wants to select only one specific section to generate the RSS feed. In order to solve above problems, we design a semi-automatic labelling GUI tool to process pages with unsatisfying result in automatic approaches. 
As shown in Fig. 6, the GUI tool contains two part of labelling interfaces: a DOM tree in the left side and a browser in the right side. The user can label RSS metadata on appropriate parts of HTML page directly and intuitively in the browser interface. When the user clicking the hyperlinks or selecting the texts displayed in the browser interface, the tool can help the user to locate the corresponding nodes in DOM tree automatically and associate RSS metadata with the nodes conveniently. The user can also select and mark the nodes in the DOM tree interface to define a region in the Web page or associate the nodes with corresponding RSS metadata. When a DOM tree node is selected, the corresponding region in the HTML page can be located and displayed in the browser at the same time. As we mentioned before, the information items in HTML pages, as discerned in their rendered views, often exhibit spatial locality in the DOM tree, and we also exploit this feature to optimize the labelling operations. After we label an item in a list, the tool can automatically deduce other items in this list based on the structure of the current item in the DOM tree. After we finish the labelling on an item list of first category, the tool can automatically deduce the lists of other categories similarly. During the deducing process, the user can si-multaneously adjust labelling process and ra nge according to the result displayed in a visual interface. 
After labelling the page and verifying the converting result, we can induce an ex-traction rule automatically. The rule is represented in a simple format similar to XPath and can be reused to process the new contents of current page in the future. Fig. 7 shows a rule example generated from the asahi.com. 
But for some irregular pages whose semantic structure are not consistent with the syntax structure, above automatic deducing process will fail, and we have to mark the items or lists manually one by one, however, even in this poor situation the tool is still useful especially for the non-technical, because the user just need to click mouse instead of writing complicated extraction programs. 
Actually, for the above two automatic feed generation approaches, it is also possi-ble to induce the reused rule from extraction result, and reduce the computing work of the RSS feed generation in the future. The system has been tested on a wide range of Japanese and Chinese Web sites con-taining the news or other list-oriented information, including country wide news paper sites, local news paper sites, TV sites, por tal sites, enterprise sites and i-mode (the most popular mobile Internet service in Japan) sites for cellular phones. We measure two performance metrics: precision and recall, which are based on the number of extracted objects and actual number of ta rget objects checked by manual work. 
Firstly, we investigated about 200 Japanese and Chinese sites and found that about 70% of news sites and almost all  X  X hat X  X  new X  or press release pages in the enterprise sites contain the release time of news items. We also checked lots of intranet sites in our company and found 90% of news inform ation list are provided with the release time. We selected 217 typical pages with time pattern from various sites as the repre-sentative examples. Fig. 8 presents the experimental result based on time pattern dis-covery. Since the time pattern has the distinct feature for the recognition, the extrac-tion of the pubDate in target items has very high performance. The time pattern is a useful and accurate clue for locating the target item, therefore, as shown in the Fig. 8 the extraction result of other data is also very good. The errors in pubDate extraction occur in only very few conditions, for example, there are multiple occurrences of current time pattern in one target item. We will solve this problem by checking the global structure of the item list in the future. The category extraction depends on the partitioning item list into the appropriate sections, however, in some irregular cases the syntax structure of the page is not consistent with its semantic structure and con-sequently the partition will be mislead. In some other cases, the partition result is correct, but there are some advertisements or recommendations information between the category title and news items, and the extraction also fails. Therefore the extrac-tion result of the category is not as good as title , link and pubDate . 
Furthermore we tested another automatic approach based on repeated tag pattern mining. Since most of news-like pages in big sites we investigated contain time pat-terns, we selected test pages without time pattern from the some small local news paper sites. We also found that some sites such as nifty.com (one of the top portal sites in Japan) have many pages containing list-oriented information without time pattern, so test pages also selected from them. Many i-mode pages have no time pat-tern associated with target items, so they are also good test candidates. Fig. 9 shows the experimental result on 54 test pages. Compared with the time pattern based ap-proach the complexity of this approach is much bigger and the performance is also lower due to the complicated repeated pattern mining. In some cases, some irrelevant InfoBlock s share the same repeated pattern with target items, so the precision de-creases. In the future we plan to analyze the display position of each section of the HTML page in the browser, which can help us to locate data-rich regions more cor-rectly. Since most of the data-rich sections are usually displayed in the centre part of the page, and top, bottom, left and right side of the page are the noise information such as navigation, menu or advertisement [13]. We can remove the redundant InfoBlock s containing the same time pattern according to the display position. Be-cause i-mode page structure is very succinct and contains the evident repeated pattern, the corresponding extraction result is very good. 
According to the above experiments, we know the automatic extraction of category is not easy due to its irregularity. If the target section is small or displayed in a special position, the automatic approaches do not work too. Therefore we need complement our system with a semi-automatic interactive tool. Since the tool is based on the man-ual labelling, the generation result can be under control and the result is always cor-rect. The point is the complexity of the operation which is dependent on the regularity of the target page. Currently, we need 4-10 clicks to label common pages, but the operation highly depends on concrete requirements. Since RSS feeds have great potential to he lp knowledge workers gather information more efficiently and present a promising solution for information integration, recently more and more attentions are paid to appr oaches for translating legacy Web contents authored in HTML into the RSS feeds. Ther e has been some existing services or sys-tems to  X  X SSify X  HTML pages. FeedFire.com [14] provides an automatic  X  X ite-To-RSS X  feed creation that allows the user to generate RSS feed for Web sites. But the FeedFire is only extracting all hyperlinks and corresponding anchor texts in the page and does not identify the data-rich regions and desired information items for RSS generation. Therefore, the results of the RSS feed are often full of noises such as links in the regions for navigation, menu and advertisement. MyRSS.JP [15] also provides an automatic RSS feed generation service similar to FeedFire.com, which is based on monitoring the difference between the current contents and previous contents of a Web page. The new hyperlinks emerge in current contents are extracted with corre-sponding anchor texts. This approach can reduce part of the noise, but the results are not good enough due to complexity of Web pages. The above two services cannot extract the release time of the information items. xpath2rss [16] is a scraper convert-ing HTML pages to RSS feeds and uses XPath instead of using regular expressions. However its converting rule in XPath has to be coded manually. Blogwatcher [17, 18] contains an automatic system to generate RSS feeds from HTML pages based on date expression and page structure, which is similar to our work on the time pattern dis-covery, and provides the title selection by simple NLP technology. But its structure analysis is not flexible enough to apply on HTML pages with complicated layout, such as the pages in which every release time is followed by multiple news items. 
Compared with existing work, our work focuses on the efficient information ex-traction for RSS feed generation and provid es adaptive approaches based on the dis-tinct features of the list-oriented information in HTML pages, consequently reaching a better result. Based on above  X  X SSifying X  approaches we implement a feed generation server to offer a series of practical services, and following are some typical cases. 
Fujitsu EIP maintains a daily updated list of Japanese IT news, which is collected from 253 Japanese news sites and enterprise sites. The list was gathered and updated by manual work before, so it took much time to keep the list current. Recently the feed generation server is applied on these news sources to produce RSS feeds, and then news list is updated automatically by aggregating feeds. When the service is launched, 204 sites can be well processed by fully automatic approaches and the user only needs to register the url and assign its update frequency. Semi-automatic label-ling approach can induce the extraction rule from 38 other sites and they are also successfully loaded to server with rules. Totally more than 95% of sites can be proc-essed by feed generation server, and only 11 sites containing the content dynamically generated by Javascript can not be converted successfully. In the future we will inte-grate a Javascript interpreter such as Rhino into our system and solve this problem. Inside Fujitsu hundreds of Web sites belonging to different departments are equipped with various old systems which are rigid to update for supporting RSS feed. It would be cumbersome and cost prohibitive to replace or reconstruct all these legacy service systems. Even for the some sites providing RSS feeds, only a small fraction of suit-able content is RSS-enabled. The feed ge neration server makes suitable contents on these sites RSS-enabled without modifying legacy system and contents can be easily integrated into EKP. Currently more 2600 pages in our intranet are translated into RSS feeds successfully and that number keeps increasing fast and constantly. 
We also provide feed generation ASP service to a financial and securities consult-ing company, and they subscribe the RSS feeds generated from dozens of financial sites. Since the stock market changes very quickly, they need to revisit related sites frequently. With our help the manual intervention has been greatly reduced and the fresh contents can be aggregated into their information system conveniently. 
In the next step we will continue to im prove automatic approaches and optimize in-teractive labeling operation, and main contents extraction of HTML pages [6] will integrated into our system to provide description or content for the item in RSS feed. We will also try to apply our system on more and more practical applications for further aggregation and analysis. 
