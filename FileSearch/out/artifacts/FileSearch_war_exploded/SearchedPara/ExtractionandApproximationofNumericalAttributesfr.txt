 Information on various numerical properties of physical objects, such as length, width and weight is fundamental in question answering frame works and for answering search engine queries. While in some cases manual annotation of objects with numerical properties is possible, it is a hard and labor intensi ve task, and is impractical for dealing with the vast amount of objects of interest. Hence, there is a need for automated semantic acquisition algorithms tar geting such properties.

In addition to answering direct questions, the ability to mak e a crude comparison or estimation of object attrib utes is important as well. For ex-ample, it allo ws to disambiguate relationships be-tween objects such as X part-of Y or X inside Y . Thus, a coarse approximation of the height of a house and a windo w is suf ficient to decide that in the  X  X ouse windo w X  nominal compound,  X  X in-dow X  is very lik ely to be a part of house and not vice versa. Such relationship information can, in turn, help summarization, machine translation or textual entailment tasks.

Due to the importance of relationship and at-trib ute acquisition in NLP , numerous methods were proposed for extraction of various lexical re-lationships and attrib utes from text. Some of these methods can be successfully used for extracting numerical attrib utes. Ho we ver, numerical attrib ute extraction is substantially dif ferent in two aspects, verification and approximation.

First, unlik e most general lexical attrib utes, nu-merical attrib ute values are comparable. It usually mak es no sense to compare the names of two ac-tors, but it is meaningful to compare their ages. The ability to compare values of dif ferent objects allo ws to impro ve attrib ute extraction precision by verifying consistenc y with attrib utes of other sim-ilar objects. For example, suppose that for Toy-ota Corolla width we found two dif ferent values, 1 . 695 m and 27 cm. The second value can be either an extraction error or a length of a toy car . Ex-tracting and looking at width values for dif ferent car brands and for  X  X ars X  in general we find:  X  Boundaries: Maximal car width is 2 . 195 m,  X  Average: Estimated avg. car width is 1 . 7 m.  X  Direct/indirect comparisons: Toyota Corolla  X  Distrib ution: Car width is distrib uted nor -Usage of all this kno wledge allo ws us to select the correct value of 1 . 695 m and reject other values. Thus we can increase the precision of value ex-traction by finding and analyzing an entire group of comparable objects.

Second, while it is usually meaningless and im-possible to approximate general lexical attrib ute values lik e an actor X  s name, numerical attrib utes can be estimated even if the y are not explicitly mentioned in the text.

In general, attrib ute extraction frame works usu-ally attempt to disco ver a single correct value (e.g., capital city of a country) or a set of distinct correct values (e.g., actors of a mo vie). So there is es-sentially nothing to do when there is no explicit information present in the text for a given object and an attrib ute. In contrast, in numerical attrib ute extraction it is possible to pro vide an approxima-tion even when no explicit information is present in the text, by using values of comparable objects for which information is pro vided.

In this paper we present a pattern-based frame-work that tak es adv antage of the properties of sim-ilar objects to impro ve extraction precision and allo w approximation of requested numerical ob-ject properties. Our frame work comprises three main stages. First, given an object name we uti-lize WordNet and pattern-based extraction to find a list of similar objects and their cate gory labels. Second, we utilize a predefined set of lexical pat-terns in order to extract attrib ute values of these objects and available comparison/boundary infor -mation. Finally , we analyze the obtained informa-tion and select or approximate the attrib ute value for the given (object, attrib ute) pair .

We performed a thorough evaluation using three dif ferent applications: Question Answering (QA), WordNet (WN) enrichment, and comparison with Wikipedia and answers pro vided by leading search engines. QA evaluation was based on a designed dataset of 1250 questions on size, height, width, weight, and depth, for which we created a gold standard and compared against it automatically 1 .
For WN enrichment evaluation, our frame work disco vered size and weight values for 300 WN physical objects, and the quality of results was evaluated by human judges. For interacti ve search, we compared our results to information obtained through Wikipedia, Google and Wolfram Alpha. Utilization of information about comparable ob-jects pro vided a significant boost to numerical at-trib ute extraction quality , and allo wed a meaning-ful approximation of missing attrib ute values.
Section 2 discusses related work, Section 3 de-tails the algorithmic frame work, Section 4 de-scribes the experimental setup, and Section 5 presents our results. Numerous methods have been developed for ex-traction of diverse semantic relationships from text. While several studies propose relationship identification methods using distrib utional analy-sis of feature vectors (Turne y, 2005), the major -ity of the proposed open-domain relations extrac-tion frame works utilize lexical patterns connect-ing a pair of related terms. (Hearst, 1992) man-ually designed lexico-syntactic patterns for ex-tracting hypern ymy relations. (Berland and Char -niak, 1999; Girju et al, 2006) proposed a set of patterns for meron ymy relations. Da vido v and Rappoport (2008a) used pattern clusters to disam-biguate nominal compound relations. Extensi ve frame works were proposed for iterati ve disco v-ery of any pre-specified (e.g., (Rilof f and Jones, 1999; Chklo vski and Pantel, 2004)) and unspec-ified (e.g., (Bank o et al., 2007; Rosenfeld and Feldman, 2007; Da vido v and Rappoport, 2008b)) relation types.

The majority of the abo ve methods utilize the follo wing basic strate gy. Given (or disco vering automatically) a set of patterns or relationship-representing term pairs, these methods mine the web for these patterns and pairs, iterati vely obtain-ing more instances. The proposed strate gies gen-erally include some weighting/frequenc y/conte xt-based algorithms (e.g. (Pantel and Pennacchiotti, 2006)) to reduce noise. Some of the methods are suitable for retrie val of numerical attrib utes. Ho w-ever, most of them do not exploit the numerical nature of the attrib ute data.

Our research is related to a sub-domain of ques-tion answering (Prager , 2006), since one of the applications of our frame work is answering ques-tions on numerical values. The majority of the proposed QA frame works rely on pattern-based relationship acquisition (Ra vichandran and Ho vy, 2009). Ho we ver, most QA studies focus on dif-ferent types of problems than our paper , including question classification, paraphrasing, etc.
Several recent studies directly tar get the acqui-sition of numerical attrib utes from the Web and attempt to deal with ambiguity and noise of the retrie ved attrib ute values. (Aramaki et al., 2007) utilize a small set of patterns to extract physical object sizes and use the averages of the obtained values for a noun compound classification task. (Banerjee et al, 2009) developed a method for dealing with quantity consensus queries (QCQs) where there is uncertainty about the answer quan-tity (e.g.  X  X ri ving time from Paris to Nice X ). The y utilize a textual snippet feature and snippet quan-tity in order to select and rank interv als of the requested values. This approach is particularly useful when it is possible to obtain a substantial amount of a desired attrib ute values for the re-quested query . (Moriceau, 2006) proposed a rule-based system which analyzes the variation of the extracted numerical attrib ute values using infor -mation in the textual conte xt of these values.
A significant body of recent research deals with extraction of various data from web tables and lists (e.g., (Caf arella et al., 2008; Crestan and Pantel, 2010)). While in the current research we do not utilize this type of information, incorpo-ration of the numerical data extracted from semi-structured web pages can be extremely beneficial for our frame work.

All of the abo ve numerical attrib ute extraction systems utilize only direct information available in the disco vered object-attrib ute co-occurrences and their conte xts. Ho we ver, as we sho w, indirect information available for comparable objects can contrib ute significantly to the selection of the ob-tained values. Using such indirect information is particularly important when only a modest amount of values can be obtained for the desired object. Also, since the abo ve studies utilize only explic-itly available information the y were unable to ap-proximate object values in cases where no explicit information was found. Our algorithm is given an object and an attrib ute. In the WN enrichment scenario, it is also given the object X  s synset. The algorithm comprises three main stages: (1) mining for similar objects and determination of a class label; (2) mining for at-trib ute values and comparison statements; (3) pro-cessing the results. 3.1 Similar objects and class label To verify and estimate attrib ute values for the given object we utilize similar objects (co-hypon yms) and the object X  s class label (hyper -nym). In the WN enrichment scenario we can eas-ily obtain these, since we get the object X  s synset as input. Ho we ver, in Question Answering (QA) sce-narios we do not have such information. To obtain it we emplo y a strate gy which uses WordNet along with pattern-based web mining.

Our web mining part follo ws common pattern-based retrie val practice (Da vido v et al., 2007). We utilize Yahoo! Boss API to perform search engine queries. For an object name Obj we query the Web using a small set of pre-defined co-h ypon ymy patterns lik e  X  X s * and/or [Obj] X  2 . In the WN en-richment scenario, we can add the WN class la-bel to each query in order to restrict results to the desired word sense. In the QA scenario, if we are given the full question and not just the (ob-ject, attrib ute) pair we can add terms appearing in the question and having a strong PMI with the ob-ject (this can be estimated using any fix ed corpus). Ho we ver, this is not essential.

We then extract new terms from the retrie ved web snippets and use these terms iterati vely to re-trie ve more terms from the Web. For example, when searching for an object  X  X oyota X , we execute a search engine query [  X  X s * and Toyota X  X  and we might retrie ve a text snippet containing  X . . . as Honda and Toyota . . .  X  . We then extract from this snippet the additional word  X  X onda X  and use it for iterati ve retrie val of additional similar terms. We attempt to avoid runa way feedback loop by requir -ing each newly detected term to co-appear with the original term in at least a single co-h ypon ymy pat-tern.

WN class labels are used later for the retrie val of boundary values, and here for expansion of the similar object set. In the WN enrichment scenario, we already have the class label of the object. In the QA scenario, we automatically find class labels as follo ws. We compute for each WN subtree a cov-erage value, the number of retrie ved terms found in the subtree divided by the number of subtree terms, and select the subtree having the highest coverage. In all scenarios, we add all terms found in this subtree to the retrie ved term list. If no WN subtree with significant ( &gt; 0 . 1) coverage is found, we retrie ve a set of cate gory labels from the Web using hypern ymy detection patterns lik e  X * suc h as [Obj] X  (Hearst, 1992). If several label candi-dates were found, we select the most frequent.
Note that we perform this stage only once for each object and do not need to repeat it for dif fer -ent attrib ute types. 3.2 Querying for values, bounds and No w we would lik e to extract the attrib ute values for the given object and its similar objects. We will also extract bounds and comparison informa-tion in order to verify the extracted values and to approximate the missing ones.

To allo w us to extract attrib ute-specific informa-tion, we pro vided the system with a seed set of ex-traction patterns for each attrib ute type. There are three kinds of patterns: value extraction, bounds and comparison patterns. We used up to 10 pat-terns of each kind. These patterns are the only attrib ute-specific resource in our frame work. Value extraction. The first pattern group, P from the Web. All seed patterns of this group contain a measurement unit name, attrib ute name, and some additional anchoring words, e.g.,  X  X bj is * [height unit] tall X  or  X  X bj width is * [width unit] X  . As in Section 3.1, we execute search en-gine queries and collect a set of numerical val-ues for each pattern. We extend this group it-erati vely from the given seed as commonly done in pattern-based acquisition methods. To do this we re-query the Web with the obtained (object, at-trib ute value, attrib ute name) triplets (e.g.,  X  X T oy-ota width 1.695m] X  ). We then extract new pat-terns from the retrie ved search engine snippets and re-query the Web with the new patterns to obtain more attrib ute values.

We pro vided the frame work with unit names and with an appropriate con version table which allo ws to con vert between dif ferent measurement systems and scales. The pro vided names include common abbre viations lik e cm/centimeter . All value acquisition patterns include unit names, so we kno w the units of each extracted value. At the end of the value extraction stage, we con vert all values to a single unit format for comparison. Boundary extraction. The second group, P lik e  X  X he widest [label] is * [width unit] X  . These patterns incorporate the class labels disco vered in the pre vious stage. The y allo w us to find maximal and minimal values for the object cate gory defined by labels. If we get several lower bounds and several upper bounds, we select the highest upper bound and the lowest lower bound.
 Extraction of comparison inf ormation. The third group, P terns. The y allo w to compare objects directly even when no attrib ute values are mentioned. This group includes attrib ute equality patterns such as  X  X Object1] has the same width as [Object2] X  , and attrib ute inequality ones such as  X  X Object1] is wider than [Object2] X  . We execute search queries for each of these patterns, and extract a set of or-dered term pairs, keeping track of the relationships encoded by the pairs.

We use these pairs to build a directed graph (W iddo ws and Doro w, 2002; Da vido v and Rap-poport, 2006) in which nodes are objects (not nec-essarily with assigned values) and edges corre-spond to extracted co-appearances of objects in-side the comparison patterns. The directions of edges are determined by the comparison sign. If two objects co-appear inside an equality pattern we put a bidirectional edge between them. 3.3 Pr ocessing the collected data As a result of the information collection stage, for each object and attrib ute type we get:  X  A set of attrib ute values for the requested ob- X  A set of objects similar or comparable to  X  Upper and lowed bounds on attrib ute values  X  A comparison graph connecting some of the Ob viously , some of these components may be missing or noisy . No w we combine these informa-tion sources to select a single attrib ute value for the requested object or to approximate this value. First we apply bounds, remo ving out-of-range val-ues, then we use comparisons to remo ve inconsis-tent comparisons. Finally we examine the remain-ing values and the comparison graph.
 Pr ocessing bounds. First we verify that indeed most (  X  50%) of the retrie ved values fit the re-trie ved bounds. If the lower and/or upper bound contradicts more than half of the data, we reject the bound. Otherwise we remo ve all values which do not satisfy one or both of the accepted bounds. If no bounds are found or if we disable the bound retrie val (see Section 4.1), we assign the maximal and minimal observ ed values as bounds.

Since our goal is to obtain a value for the single requested object, if at the end of this stage we re-main with a single value, no further processing is needed. Ho we ver, if we obtain a set of values or no values at all, we have to utilize comparison data to select one of the retrie ved values or to approx-imate the value in case we do not have an exact answer .
 Pr ocessing comparisons. First we simplify the comparison graph. We drop all graph components that are not connected (when vie wing the graph as undirected) to the desired object.

No w we refine the graph. Note that each graph node may have a single value, man y assigned val-ues, or no assigned values. We define assigned nodes as nodes that have at least one value. For each directed edge E ( A  X  B ) , if both A and B are assigned nodes, we check if Avg ( A )  X  Avg ( B ) 3 . If the average values violate the equa-tion, we gradually remo ve up to half of the highest values for A and up to half of the lowest values for B till the equation is satisfied. If this cannot be done, we drop the edge. We repeat this process until every edge that connects two assigned nodes satisfies the inequality .
 Selecting an exact attrib ute value. The goal now is to select an attrib ute value for the given object. During the first stage it is possible that we directly extract from the text a set of values for the requested object. The bounds processing step rejects some of these values, and the com-parisons step may reject some more. If we still have several values remaining, we choose the most frequent value based on the number of web snip-pets retrie ved during the value acquisition stage. If there are several values with the same frequenc y we select the median of these values.
 Appr oximating the attrib ute value. In the case when we do not have any values remaining after the bounds processing step, the object node will remain unassigned after construction of the com-parison graph, and we would lik e to estimate its value. Here we present an algorithm which allo ws us to set the values of all unassigned nodes, includ-ing the node of the requested object.

In the algorithm belo w we treat all node groups connected by bidirectional (equality) edges as a same-v alue group, i.e., if a value is assigned to one node in the group, the same value is immediately assigned to the rest of the nodes in the same group.
We start with some preprocessing. We create dummy lower and upper bound nodes L and U with corresponding upper/lo wer bound values ob-tained during the pre vious stage. These dummy nodes will be used when we encounter a graph which ends with one or more nodes with no avail-able numerical information. We then connect them to the graph as follo ws: (1) if A has no in-coming edges, we add an edge L  X  A ; (2) if A has no outgoing edges, we add an edge A  X  U .
We define a legal unassigned path as a di-rected path A where A Avg ( A 0 )  X  Avg ( A n +1 ) and A 1 . . . A n are unassigned. We would lik e to use dummy bound nodes only in cases when no other information is available. Hence we consider paths L  X  . . .  X  U connecting both bounds are ille gal. First we assign values for all unassigned nodes that belong to a single legal unassigned path, using a simple linear combination:
Then, for all unassigned nodes that belong to multiple legal unassigned paths, we compute node value as abo ve for each path separately and assign to the node the average of the computed values.
Finally we assign the average of all extracted values within bounds to all the remaining unas-signed nodes. Note that if we have no compari-son information and no value information for the requested object, the requested object will recei ve the average of the extracted values of the whole set of the retrie ved comparable objects and the com-parison step will be essentially empty . We performed automated question answering (QA) evaluation, human-based WN enrichment evaluation, and human-based comparison of our results to data available through Wikipedia and to the top results of leading search engines. 4.1 Experimental conditions In order to test the main system components, we ran our frame work under five dif ferent conditions:  X  FULL: All system components were used.  X  DIRECT : Only direct pattern-based acqui- X  NOCB: No boundary and no comparison  X  NOB: As in FULL but no boundary data was  X  NOC: As in FULL but no comparison data 4.2 Automated QA Ev aluation We created two QA datasets, Web and TREC based.
 Web-based QA dataset. We created QA datasets for size, height, width, weight, and depth attrib utes. For each attrib ute we extracted from the Web 250 questions in the follo wing way. First, we collected several thousand questions, querying for the follo wing patterns:  X  X o w long/tall/wide/hea vy/deep/high is X , X  X hat is the size/width/height/depth/weight of X . Then we manually filtered out non-questions and hea vily conte xt-specific questions, e.g.,  X  X hat is the width of the triangle X  . Ne xt, we retained only a single question for each entity by remo ving duplicates.
For each of the extracted questions we manu-ally assigned a gold standard answer using trusted resources including books and reliable Web data. For some questions, the exact answer is the only possible one (e.g., the height of a person), while for others it is only the center of a distrib ution (e.g., the weight of a cof fee cup). Questions with no trusted and exact answers were eliminated. From the remaining questions we randomly se-lected 250 questions for each attrib ute.
 TREC-based QA dataset. As a small comple-mentary dataset we used rele vant questions from the TREC Question Answering Track 1999-2007. From 4355 questions found in this set we collected 55 (17 size, 2 weight, 3 width, 3 depth and 30 height) questions.
 Examples. Some example questions from our datasets are (correct answers are in parentheses): Ho w tall is Michelle Obama? (180cm); Ho w tall is the tallest penguin? (122cm); What is the height of a tennis net? (92cm); What is the depth of the Nile river? (1000cm = 10 meters); Ho w hea vy is a cup of cof fee? (360gr); Ho w hea vy is a gi-raf fe? (1360000gr = 1360kg); What is the width of a DN A molecule? (2e-7cm); What is the width of a cow? (65cm).
 Ev aluation protocol. Ev aluation against the datasets was done automatically . For each ques-tion and each condition our frame work returned a numerical value mark ed as either an exact an-swer or as an approximation. In cases where no data was found for an approximation (no similar objects with values were found), our frame work returned no answer .

We computed precision 4 , comparing results to the gold standard. Approximate answers are con-sidered to be correct if the approximation is within 10% of the gold standard value. While a choice of 10% may be too strict for some applications and too generous for others, it still allo ws to estimate the quality of our frame work. 4.3 WN enrichment evaluation We manually selected 300 WN entities from about 1000 randomly selected objects belo w the object tree in WN, by filtering out entities that clearly do not possess any of the addressed numerical at-trib utes.

Ev aluation was done using human subjects. It is dif ficult to do an automated evaluation, since the nature of the data is dif ferent from that of the QA dataset. Most of the questions ask ed over the Web tar get named entities lik e specific car brands, places and actors. There is usually little or no vari-ability in attrib ute values of such objects, and the major source of extraction errors is name ambigu-ity of the requested objects.

WordNet physical objects, in contrast, are much less specific and their attrib utes such as size and weight rarely have a single correct value, but usu-ally possess an acceptable numerical range. For example, the majority of the selected objects lik e  X  X pple X  are too general to assign an exact size. Also, it is unclear how to define acceptable val-ues and an approximation range. Crudeness of desired approximation depends both on potential applications and on object type. Some objects sho w much greater variability in size (and hence a greater range of acceptable approximations) than others. This property of the dataset mak es it dif fi-cult to pro vide a meaningful gold standard for the evaluation. Hence in order to estimate the quality of our results we turn to an evaluation based on human judges.

In this evaluation we use only appr oximate re-trie ved values, keeping out the small amount of returned exact values 5 .

We have mix ed (Object, Attrib ute name, At-trib ute value) triplets obtained through each of the conditions, and ask ed human subjects to assign these to one of the follo wing cate gories:  X  The attrib ute value is reasonable for the given  X  The value is a very crude approximation of  X  The value is incorrect or clearly misleading.  X  The object is not familiar enough to me so I Each evaluator was pro vided with a random sam-ple of 40 triplets. In addition we mix ed in 5 manu-ally created clearly correct triplets and 5 clearly in-correct ones. We used five subjects, and the agree-ment (inter -annotator Kappa) on shared evaluated triplets was 0.72. 4.4 Comparisons to sear ch engine output Recently there has been a significant impro vement both in the quality of search engine results and in the creation of manual well-or ganized and anno-tated databases such as Wikipedia.

Google and Yahoo! queries frequently pro vide attrib ute values in the top snippets or in search result web pages. Man y Wikipedia articles in-clude infobox es with well-or ganized attrib ute val-ues. Recently , the Wolfram Alpha computational kno wledge engine presented the computation of attrib ute values from a given query text.
Hence it is important to test how well our frame-work can complement the manual extraction of at-trib utes from resources such as Wikipedia and top Google snippets. In order to test this, we randomly selected 100 object-attrib ute pairs from our Web QA and WordNet datasets and used human sub-jects to test the follo wing: 1. Go1 : Querying Google for [object-name 2. Go2 : Querying Google for [object-name 3. Wi : There is a Wikipedia page for the given 4. Wf : A Wolfram Alpha query for [object-5.1 QA results We applied our frame work to the abo ve QA datasets. Table 1 sho ws the precision and the per -centage of approximations and exact answers.
Looking at %Exact+%Approx, we can see that for all datasets only 1-9% of the questions re-main unanswered, while correct exact answers are found for 65%/87% of the questions for Web/TREC (% Exact and Prec(Exact) in the ta-ble). Thus approximation allo ws us to answer 13-24% of the requested values which are either sim-ply missing from the retrie ved text or cannot be de-tected using the current pattern-based frame work. Comparing performance of FULL to DIRECT , we see that our frame work not only allo ws an approx-imation when no exact answer can be found, but also significantly increases the precision of exact answers using the comparison and the boundary information. It is also apparent that both bound-ary and comparison features are needed to achie ve good performance and that using both of them achie ves substantially better results than each of them separately .
Comparing results for dif ferent question types we can see substantial performance dif ferences be-tween the attrib ute types. Thus depth sho ws much better overall results than width. This is lik ely due to a lesser dif ficulty of depth questions or to a more exact nature of available depth information com-pared to width or size. 5.2 WN enrichment As sho wn in Table 2, for the majority of examined WN objects, the algorithm returned an approxi-mate value, and only for 13-15% of the objects (vs. 70-80% in QA data) the algorithm could retrie ve exact answers.

Note that the common pattern-based acquisition frame work, presented as the DIRECT condition, could only extract attrib ute values for 15% of the objects since it does not allo w approximations and may only extract values from the text where the y explicitly appear .

Table 3 sho ws human evaluation results. We see that the majority of approximate values were clearly accepted by human subjects, and only 6-8% were found to be incorrect. We also observ e that both boundary and comparison data signifi-cantly impro ve the approximation results. Note that DIRECT is missing from this table since no approximations are possible in this condition.
Some examples for WN objects and approx-imate values disco vered by the algorithm are: Sandfish, 15gr; skull, 1100gr; pilot, 80.25kg. The latter value is amusing due to the high variabil-ity of the value. Ho we ver, even this value is valu-able, as a sanity check measure for automated in-ference systems and for various NLP tasks (e.g.,  X  X ilot jack et X  lik ely refers to a jack et used by pi-lots and not vice versa). 5.3 Comparison with sear ch engines and Table 4 sho ws results for the abo ve datasets in comparison to the proportion of correct results and the approximations returned by our frame work un-der the FULL condition (correct exact values and approximations are tak en together).

We can see that our frame work, due to its ap-proximation capability , currently sho ws signifi-cantly greater coverage than manual extraction of data from Wikipedia infobox es or from the first search engine results. We presented a novel frame work which allo ws an automated extraction and approximation of nu-merical attrib utes from the Web, even when no ex-plicit attrib ute values can be found in the text for the given object. Our frame work retrie ves simi-larity , boundary and comparison information for objects similar to the desired object, and com-bines this information to approximate the desired attrib ute.

While in this study we explored only several specific numerical attrib utes lik e size and weight, our frame work can be easily augmented to work with any other consistent and comparable attrib ute type. The only change required for incorpora-tion of a new attrib ute type is the development of attrib ute-specific P pattern groups; the rest of the system remains un-changed.

In our evaluation we sho wed that our frame-work achie ves good results and significantly out-performs the baseline commonly used for general lexical attrib ute retrie val 7 .

While there is a gro wing justification to rely on extensi ve manually created resources such as Wikipedia, we have sho wn that in our case auto-mated numerical attrib ute acquisition could be a preferable option and pro vides excellent coverage in comparison to handcrafted resources or man-ual examination of the leading search engine re-sults. Hence a promising direction would be to use our approach in combination with Wikipedia data and with additional manually created attrib ute rich sources such as Web tables, to achie ve the best possible performance and coverage.

We would also lik e to explore the incorpora-tion of approximate disco vered numerical attrib ute data into existing NLP tasks such as noun com-pound classification and textual entailment.
