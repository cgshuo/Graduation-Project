 Ev ery day , a lar ge number of ne ws articles are cre-ated and reported, man y of which are unique. But certain types of e v ents, such as hurricanes or mur -ders, are reported ag ain and ag ain throughout a year . The goal of Information Extraction, or IE, is to re-trie v e a certain type of ne ws e v ent from past articles and present the e v ents as a table whose columns are filled with a name of a person or compan y , accord-ing to its role in the e v ent. Ho we v er , e xisting IE techniques require a lot of human labor . First, you ha v e to specify the type of information you w ant and collect articles that include this information. Then, you ha v e to analyze the articles and manually craft a set of patterns to capture these e v ents. Most e xist-ing IE research focuses on reducing this b urden by helping people create such patterns. But e ach time you w ant to e xtract a dif ferent kind of information, you need to repeat the whole process: specify arti-cles and adjust its patterns, either manually or semi-automatically . There is a bit of a dangerous pitf all here. First, it is hard to estimate ho w good the sys-tem can be after months of w ork. Furthermore, you might not kno w if the task is e v en doable in the first place. Kno wing what kind of information is easily obtained in adv ance w ould help reduce this risk.
An IE task can be defined as finding a relation among se v eral entities in v olv ed in a certain type of e v ent. F or e xample, in the MUC-6 management succession scenario, one seeks a relation between COMP ANY , PERSON and POST in v olv ed with hir -ing/firing e v ents. F or each ro w of an e xtracted ta-ble, you can al w ays read it as  X  X OMP ANY hired (or fired) PERSON for POST .  X  The relation between these entities is retained throughout the table. There are man y e xisting w orks on obtaining e xtraction pat-terns for pre-defined relations (Rilof f, 1996; Y ang ar -ber et al., 2000; Agichtein and Gra v ano, 2000; Sudo et al., 2003).

Unrestricted Relation Disco v ery is a technique to automatically disco v er such relations that repeatedly appear in a corpus and present them as a table, with absolutely no human interv ention. Unlik e most e x-isting IE research, a user does not specify the type of articles or information w anted. Instead, a system tries to find all the kinds of relations that are reported multiple times and can be reported in tab ular form. This technique will open up the possibility of try-ing ne w IE scenarios. Furthermore, the system itself can be used as an IE system, since an obtained re-lation is already presented as a table. If this system w orks to a certain e xtent, tuning an IE system be-comes a search problem: all the tables are already b uilt  X  X reempti v ely .  X  A user only needs to search for a rele v ant table.
W e implemented a preliminary system for this technique and obtained reasonably good perfor -mance. T able 1 is a sample relation that w as e x-tracted as a table by our system. The columns of the table sho w article dates, names of hurricanes and the places the y af fected respecti v ely . The headers of the table and its k e yw ords were also e xtracted automat-ically . In Unrestricted Relation Disco v ery , the disco v ery process (i.e. creating ne w tables) can be formulated as a clustering task. The k e y idea is to cluster a set of articles that contain entities bearing a similar rela-tion to each other in such a w ay that we can construct a table where the entities that play the same role are placed in the same column.

Suppose that there are tw o articles A and B , and both report hurricane-related ne ws. Article A contains tw o entities  X  X atrina X  and  X  X e w Orleans X , and article B contains  X  X ongw ang X  and  X  X  aiw an X . These entities are recognized by a Named Entity (NE) tagger . W e w ant to disco v er a relation among them. First, we introduce a notion called  X  X asic pattern X  to form a relation. A basic pattern is a part of the te xt that is syntactically connected to an entity . Some e xamples are  X  X is hit  X  or  X  Y X  X  residents  X . Figure 1 sho ws se v eral basic patterns connected to the entities  X  X atrina X  and  X  X e w Or -leans X  in article A . Similarly , we obtain the basic patterns for article B . No w , in Figure 2, both enti-ties  X  X atrina X  and  X  X ongw ang X  ha v e the basic pat-tern  X  headed  X  in common. In this case, we connect these tw o entities to each other . Furthermore, there is also a common basic pattern  X  was-hit  X  shared by  X  X e w Orleans X  and  X  X  aiw an X . No w , we found tw o sets of entities that can be placed in correspondence at the same time. What does this mean? W e can infer that both entity sets ( X  X atrina X - X  X e w Orleans X  and  X  X ongw ang X - X  X  aiw an X ) represent a certain relation that has something in common: a hurricane name and the place it af fected . By finding multiple par -allel correspondences between tw o articles, we can estimate the similarity of their relations.

Generally , in a clustering task, one groups items by finding similar pairs. After finding a pair of arti-cles that ha v e a similar relation, we can bring them into the same cluster . In this case, we cluster articles by using their basic patterns as features. Ho we v er , each basic pattern is still connected to its entity so that we can e xtract the name from it. W e can con-sider a basic pattern to represent something lik e the  X  X ole X  of its entity . In this e xample, the entities that had  X  headed  X  as a basic pattern are hurricanes , and the entities that had  X  was-hit  X  as a basic pattern are the places it af fected . By using basic patterns, we can align the entities into the corresponding column that represents a certain role in the relation. From this e xample, we create a tw o-by-tw o table, where each column represents the roles of the entities, and each ro w represents a dif ferent article, as sho wn in the bottom of Figure 2.

W e can e xtend this table by finding another article in the same manner . In this w ay , we gradually e xtend a table while retaining a relation among its columns. In this e xample, the obtained table is just what an IE system (whose task is to find a hurricane name and the af fected place) w ould create.

Ho we v er , these articles might also include other things, which could represent dif ferent relations. F or e xample, the go v ernments might call for help or some casualties might ha v e been reported. T o ob-tain such relations, we need to choose dif ferent en-tities from the articles. Se v eral e xisting w orks ha v e tried to e xtract a certain type of relation by manu-ally choosing dif ferent pairs of entities (Brin, 1998; Ra vichandran and Ho vy , 2002). Hase g a w a et al. (2004) tried to e xtract multiple relations by choos-ing entity types. W e assume that we can find such relations by trying all possible combinations from a set of entities we ha v e chosen in adv ance; some combinations might represent a hurricane and go v-ernment relation, and others might represent a place and its casualties. T o ensure that an article can ha v e se v eral dif ferent relations, we let each article belong to se v eral dif ferent c lusters.

In a real-w orld situation, only using basic patterns sometimes gi v es undesired results. F or e xample,  X  (Pr esident) Bush fle w to T e xas  X  and  X  (Hurricane) Katrina fle w to Ne w Orleans  X  both ha v e a basic pat-tern  X  flew to  X  in common, so  X  X ush X  and  X  X at-rina X  w ould be put into the same column. But we w ant to separate them in dif ferent tables. T o alle vi-ate this problem, we put an additional restriction on clustering. W e use a bag-of-w ords approach to dis-criminate tw o articles: if the w ord-based similarity between tw o articles is too small, we do not bring them together into the same cluster (i.e. table). W e e xclude names from the similarity calculation at this stage because we w ant to link articles about the same type of e v ent, not the same instance. In addition, we use the frequenc y of each basic pattern to compute the similarity of relations, since basic patterns lik e  X  X ay X  or  X  X a v e X  appear in almost e v ery article and it is dangerous to rely on such e xpressions.
 Incr easing Basic P atter ns In the abo v e e xplanation, we ha v e assumed that we can obtain enough basic patterns from an article. Ho we v er , the actual number of basic patterns that one can find from a single article is usually not enough, because the number of sentences is rather small in comparison to the v ariation of e xpressions. So ha ving tw o articles that ha v e multiple basic pat-terns in common is v ery unlik ely . W e e xtend the number of articles for obtaining basic patterns by using a cluster of comparable articles that report the same e v ent instead of a single article. W e call this cluster of articles a  X  X asic cluster .  X  Using basic clus-ters instead of single articles also helps to increase the redundanc y of data. W e can gi v e more confi-dence to repeated basic patterns.

Note that the notion of  X  X asic cluster X  is dif ferent from the clusters used for creating tables e xplained abo v e. In the follo wing sections, a cluster for creat-ing a table is called a  X  X etacluster ,  X  because this is a cluster of basic clusters. A basic cluster consists of a set of articles that report the same e v ent which happens at a certain time, and a metacluster consists of a set of e v ents that contain the same relation o v er a certain period.

W e try to increase the number of articles in a basic cluster by looking at multiple ne ws sources simulta-neously . W e use a clustering algorithm that uses a v ector -space-model to obtain basic clusters. Then we apply cross-document coreference resolution to connect entities of dif ferent articles within a basic cluster . This w ay , we can increase the number of ba-sic patterns connected to each entity . Also, it allo ws us to gi v e a weight to entities. W e calculate their weights using the number of occurrences within a cluster and their position within an article. These entities are used to obtain basic patterns later .
W e also use a parser and tree normalizer to gen-erate basic patterns. The format of basic patterns is crucial to performance. W e think a basic pat-tern should be some what specific, since each pat-tern should capture an entity with some rele v ant con-te xt. But at the same time a basic pattern should be general enough to reduce data sparseness. W e choose a predicate-ar gument structure as a natural solution for this problem. Compared to traditional constituent trees, a predicate-ar gument structure is a higher -le v el representation of sentences that has g ained wide acceptance from the natural language community recently . In this paper we used a logical feature structure called GLARF proposed by Me y-ers et al. (2001a). A GLARF con v erter tak es a syn-tactic tree as an input and augments it with se v eral Figure 3: GLARF structure of the sentence  X  Katrina hit Louisiana X  s coast.  X  features. Figure 3 sho ws a sample GLARF structure obtained from the sentence  X  Katrina hit Louisiana X  s coast.  X  W e used GLARF for tw o reasons: first, unlik e traditional constituent parsers, GLARF has an ability to re gularize se v eral linguistic phenom-ena such as participial constructions and coordina-tion. This allo ws us to handle this syntactic v ariety in a uniform w ay . Second, an output structure can be easily con v erted into a directed graph that rep-resents the relationship between each w ord, without losing significant information from the original sen-tence. Compared to an ordinary constituent tree, it is easier to e xtract syntactic relationships. In the ne xt section, we discuss ho w we used this structure to generate basic patterns. The o v erall process to generate basic patterns and disco v er relations from unannotated ne ws articles is sho wn in Figure 4. Theoretically this could be a straight pipeline, b ut due to the nature of the im-plementation we process some stages separately and combine them in the later stage. In the follo wing subsection, we e xplain each component. 3.1 W eb Crawling and Basic Clustering First of all, we need a lot of ne ws articles from mul-tiple ne ws sources. W e created a simple web cra wler that e xtract the main te xts from web pages. W e ob-serv ed that the cra wler can correctly tak e the main te xts from about 90% of the pages from each ne ws site. W e ran the cra wler e v ery day on se v eral ne ws sites. Then we applied a simple clustering algorithm to the obtained articles in order to find a set of arti-cles that talk about e xactly the same ne ws and form a basic cluster .

W e eliminate stop w ords and stem all the other w ords, then compute the similarity between tw o ar -ticles by using a bag-of-w ords approach. In ne ws articles, a sentence that appears in the be ginning of an article is usually more important than the others. So we preserv ed the w ord order to tak e into account the location of each sentence. First we computed a w ord v ector from each article:
V w ( A ) = IDF ( w ) where V w ( A ) is a v ector element of w ord w in article A , I D F ( w ) is the in v erse document frequenc y of w ord w , and P O S ( w , A ) is a list of w  X  s positions in the article. av g w or ds is the a v erage number of w ords for all articles. Then we calculated the cosine v alue of each pair of v ectors: W e computed the similarity of all possible pairs of articles from the same day , and selected the pairs whose similarity e xceeded a certain threshold ( 0 . 65 in this e xperiment) to form a basic cluster . 3.2 P arsing and GLARFing After getting a set of basic clusters, we pass them to an e xisting statistical parser (Charniak, 2000) and rule-based tree normalizer to obtain a GLARF struc-ture for each sentence in e v ery article. The current implementation of a GLARF con v erter gi v es about 75% F-score using parser output. F or the details of GLARF representation and its con v ersion, see Me y-ers et al. (2001b). 3.3 NE T agging a nd Cor efer ence Resolution In parallel with parsing and GLARFing, we also ap-ply NE tagging and coreference resolution for each article in a basic cluster . W e used an HMM-based NE tagger whose performance is about 85% in F-score. This NE tagger produces A CE-type Named Entities 1 : PERSON, ORGANIZA TION, GPE, LO-CA TION and F A CILITY 2 . After applying single-document coreference resolution for each article, we connect the entities among dif ferent articles in the same basic cluster to obtain cross-document coref-erence entities with simple string matching. 3.4 Basic P atter n Generation After getting a GLARF structure for each sentence and a set of documents whose entities are tagged and connected to each other , we mer ge the tw o out-puts and create a big netw ork of GLARF structures whose nodes are interconnected across dif ferent sen-tences/articles. No w we can generate basic patterns for each entity . First, we compute the weight for each cross-document entity E in a certain basic clus-ter as follo ws: W where e  X  E is an entity within one article and mentions ( e ) and f ir stsent ( e ) are the number of mentions of entity e in a document and the position Figure 5: Basic patterns obtained from the sentence  X  Katrina hit Louisiana X  s coast.  X  of the sentence where entity e first appeared, respec-ti v ely . C is a constant v alue which w as 0 . 5 in this e x-periment. T o reduce combinatorial comple xity , we took only the fi v e most highly weighted entities from each basic cluster to generate basic patterns. W e ob-serv ed these fi v e entities can co v er major relations that are reported in a basic cluster .

Ne xt, we obtain basic patterns from the GLARF structures. W e used only the first ten sentences in each article for getting basic patterns, as most important f acts are usually written in the first fe w sentences of a ne ws article. Figure 5 sho ws all the basic patterns obtained from the sentence  X  Ka-trina hit Louisiana X  s coast.  X  The shaded nodes  X  X atrina X  and  X  X ouisiana X  are entities from which each basic pattern originates. W e tak e a path of GLARF nodes from each entity node until it reaches an y predicati v e node: noun, v erb, or ad-jecti v e in this case. Since the nodes  X  X it X  and  X  X oast X  can be predicates in this e xample, we ob-tain three unique paths  X  Louisiana +T-POS:coast ( Louisiana X  s coast ) X ,  X  Katrina +SBJ:hit ( Katrina hit something ) X , and  X  Katrina +SBJ:hit-OBJ:coast ( Katrina hit some coast ) X .

T o increase the specificity of patterns, we generate e xtra basic patterns by adding a node that is imme-diately connected to a predicati v e node. (From this e xample, we generate tw o basic patterns:  X  X it X  and  X  X it-coast X  from the  X  X atrina X  node.)
Notice that in a GLARF structure, the type of each ar gument such as subject or object is preserv ed in an edge e v en if we e xtract a sin-gle path of a graph. No w , we replace both entities  X  X atrina X  and  X  X ouisiana X  with v ariables based on their NE tags and obtain parameter -ized patterns:  X  GPE +T-POS:coast ( Louisiana X  s coast ) X ,  X  PER +SBJ:hit ( Katrina hit something ) X , and  X  PER +SBJ:hit-OBJ:coast ( Katrina hit some coast ) X .

After taking all the basic patterns from e v ery basic cluster , we compute the In v erse Cluster Frequenc y (ICF) of each unique basic pattern. ICF is similar to the In v erse Document Frequenc y (IDF) of w ords, which is used to calculate the weight of each basic pattern for metaclustering. 3.5 Metaclustering Finally , we can perform metaclustering to obtain ta-bles. W e compute the similarity between each basic cluster pair , as seen in Figure 6. X A and X B are the set of cross-document entities from basic clusters c
A and c B , respecti v ely . W e e xamine all possible mappings of relations (parallel mappings of multi-ple entities) from both basic clusters, and find all the mappings M whose similarity score e xceeds a cer -tain threshold. w ordsim ( c A , c B ) is the bag-of-w ords similarity of tw o clusters. As a weighting function we used ICF:
W e then sort the similarities of all possible pairs of basic clusters, and try to b uild a metacluster by taking the most strongly connected pair first. Note that in this process we may assign one basic clus-ter to se v eral dif ferent metaclusters. When a link is found between tw o basic clusters that were already assigned to a metacluster , we try to put them into all the e xisting metaclusters it belongs to. Ho we v er , we allo w a basic cluster to be added only if it can fill all the columns in that table. In other w ords, the first tw o basic clusters (i.e. an initial tw o-ro w table) determines its columns and therefore define the re-lation of that table. W e used twelv e ne wspapers published mainly in the U.S. W e collected their articles o v er tw o months (from Sep. 21, 2005 -No v . 27, 2005). W e obtained 643,767 basic patterns and 7,990 unique types. Then we applied metaclustering to these basic clusters and obtained 302 metaclusters (tables). W e then re-mo v ed duplicated ro ws and took only the tables that had 3 or more ro ws. Finally we had 101 tables. The total number the of articles and clusters we used are sho wn in T able 2. 4.1 Ev aluation Method W e e v aluated the obtained tables as follo ws. F or each ro w in a table, we added a summary of the source articles that were used to e xtract the rela-tion. Then for each table, an e v aluator looks into e v ery ro w and its source article, and tries to come up with a sentence that e xplains the relation among its columns. The description should be as specific as possible. If at least half of the ro ws can fit the e x-planation, the table is considered  X  X onsistent.  X  F or each consistent table, the e v aluator wrote do wn the sentence using v ariable names ($1, $2, ...) to refer to its columns. Finally , we counted the number of consistent tables. W e also counted ho w man y ro ws in each table can fit the e xplanation. 4.2 Results W e e v aluated 48 randomly chosen tables. Among these tables, we found that 36 tables were consis-tent. W e also counted the total number of ro ws that fit each description, sho wn in T able 3. T able 4 sho ws the descriptions of the selected tables. The lar gest consistent table w as about hurricanes (T able 5). Al-though we cannot e xactly measure the recall of each table, we tried to estimate the recall by comparing this hurricane table to a manually created one (T able 6). W e found 6 out of 9 hurricanes 3 . It is w orth noting that most of these hurricane names were au-tomatically disambiguated although our NE tagger didn X  t distinguish a hurricane name from a person T able 4: Description of obtained tables and the num-ber of fitted/total ro ws. name. The second lar gest table (about nominations of of ficials) is sho wn in T able 7.

W e re vie wed 10 incorrect ro ws from v arious ta-bles and found 4 of them were due to coreference er -rors and one error w as due to a parse error . The other 4 errors were due to multiple basic patterns distant from each other that happened to refer to a dif ferent e v ent reported in the same cluster . The causes of the one remaining error w as obscure. Most inconsistent tables were a mixture of multiple relations and some of their ro ws still look ed consistent.

W e ha v e a couple of open questions. First, the o v erall recall of our system might be lo wer than e x-isting IE systems, as we are relying on a cluster of comparable articles rather than a single document to disco v er an e v ent. W e might be able to impro v e this in the future by adjusting the basic clustering algo-rithm or weighting schema of basic patterns. Sec-ondly , some combinations of basic patterns look ed inherently v ague. F or e xample, we used the tw o ba-sic patterns  X  pitched  X  and  X   X  X -series  X  in the fol-lo wing sentence (the patterns are underlined):
It is not clear whether this set of patterns can yield an y meaningful relation. W e are not sure ho w much this sort of table can af fect o v erall IE performance. In this paper we proposed Preempti v e Information Extraction as a ne w direction of IE research. As its k e y technique, we presented Unrestricted Rela-tion Disco v ery that tries to find parallel correspon-dences between multiple entities in a document, and perform clustering using basic patterns as features. T o increase the number of basic patterns, we used a cluster of comparable articles instead of a single document. W e presented the implementation of our preliminary system and its outputs. W e obtained dozens of usable tables. T able 5: Hurricane table ( X  X torm $1(PER) probably af f ected $2(GPE).  X ) and the actual e xpressions we used for e xtraction.
 T able 6: Hurricanes in North America between mid-Sep. and No v . (from W ikipedia). Ro ws with a star (*) were actually e xtracted. The number of the source articles that contained a mention of the hurri-cane is sho wn in the right column.
 T able 7: Nomination table ( X  X ominee $2(PER) must be confirmed by $1(ORG).  X ) This research w as supported by the National Science F oundation under Grant IIS-00325657. This paper does not necessarily reflect the position of the U.S. Go v ernment. W e w ould lik e to thank Prof. Ralph Grishman who pro vided useful suggestions and dis-cussions.

