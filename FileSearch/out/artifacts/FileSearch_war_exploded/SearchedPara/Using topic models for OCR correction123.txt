 ORIGINAL PAPER Faisal Farooq  X  Anurag Bhardwaj  X  Venu Govindaraju Abstract Despite several decades of research in document analysis, recognition of unconstrained handwritten doc-uments is still considered a challenging task. Previous research in this area has shown that word recognizers perform adequatelyonconstrainedhandwrittendocumentswhichtyp-ically use a restricted vocabulary (lexicon). But in the case of unconstrained handwritten documents, state-of-the-art word recognition accuracy is still below the acceptable limits. The objective of this research is to improve word recognition accuracy on unconstrained handwritten documents by apply-ing a post-processing or OCR correction technique to the word recognition output. In this paper, we present two different methods for this purpose. First, we describe a lexicon reduction-based method by topic categorization of handwritten documents which is used to generate smaller topic-specific lexicons for improving the recognition accu-racy. Second, we describe a method which uses topic-specific language models and a maximum-entropy based topic cate-gorization model to refine the recognition output. We present the relative merits of each of these methods and report results on the publicly available IAM database.
 Keywords OCR correction  X  Topic models  X  Lexicon reduction  X  Language models  X  Document categorization  X  Handwritten documents  X  Unconstrained handwriting 1 Introduction Handwritten document analysis and recognition continues to remain a challenging task. Diversity in writing styles, incon-sistent spacing between words and lines, and uncertainty of the number of lines in a page as well as the number of words in a line, all contribute to towards the difficulty of this prob-lem [ 1 , 2 ]. However, the single most important contributor is the subtask of handwritten word recognition (Table 1 ) which relies heavily upon a lexicon. Successful applications involv-ing handwriting recognizers (e.g., postal address interpreta-tion [ 3 ], bank check reading [ 4 ], and form reading [ 5 ]) owe their success primarily to the availability of domain knowl-edge which translates to constraints on the size of the lexicon. When the lexicon size is fairly large (  X  10,000 words) the state-of-the-art recognizers perform at a poor accuracy level of  X  40% [ 6 ]. Accuracy usually improves as the lexicon size decreases. Recent research has thus focused on techniques of lexicon reduction even when a context is not available.
We have developed a methodology to achieve lexicon reduction by first estimating the topic of the document from a predefined set. The output of the recognizer is cast as the features for the topic classes and the full lexicon is viewed as a probabilistic distribution over the set of topics. We use statistical topic categorization techniques such as Maximum Entropy to automatically infer the topic of a document from the error-full text returned by the recognizer during train-ing. A desirable property of our approach is that the training procedure does not consider the internal working of the rec-ognizer: rather it treats the recognizer as a blackbox which returns top-n choices at each word position. The idea is to run the (same or different) recognizer a second time with a dynamically generated lexicon where all the words are present but have also associated with them an additional weight corresponding to the likelihood of encountering the word under a particular topic. Thus our modeling is sand-wiched between the two runs of the recognizer and therefore it can be considered to be a paradigm for OCR correction of the output of the first run as well as a paradigm for lexi-con reduction (weighting) before the second run. This is an innovative departure from lexicon reduction methods which actually prune the lexicon and suffer from the problem of out-of-vocabulary words at subsequent stages.

Figure 1 illustrates the results obtained by the same doc-ument analysis techniques on a page from Newton X  X  notes. Word recognition for this example would involve the entire English dictionary. Perhaps, if the context were known, a limited but still large dictionary could be used. For exam-ple, if the page is taken from Newton X  X  notes on Physics, the glossary of terms from a Physics textbook can be used. We propose to use this context knowledge to improve recognition accuracies.

Prior work on topic categorization is not directly applica-ble because of extreme noisy output generated by the hand-writing recognizers when presented with large raw lexicons. We have adapted the commonly used Naive Bayes and MaximumEntropymodelstobeabletopipethetop-n choices as features. The intuition underlying our approach is that sim-ilar word images tend to generate a similarly ranked list of lexicon entries which can be used as the correction model for the recognizer. We have tested our approach on the publicly available IAM handwritten document images dataset with 13 categories. Results are encouraging with a relative improvement of 25% observed by the correction model pro-posed in this paper.

Following is the outline of the rest of the paper. In Sect. 2 , we provide a brief survey of techniques for OCR correction. Our proposed method for lexicon reduction is presented in Sect. 3 . Section 4 describes the other variant of our method viz topic-specific language model. In Sect. 5 , we discuss the models used for topic categorization which is the underlying foundation for both systems. We also describe the features extracted from the handwritten documents for training the categorization models. In Sect. 6 we describe the dataset that is used for experiments followed by experimental results. We finally conclude in Sect. 7 . 2 Background This section entails some significant related work in this area. We organize the related work in two sub-sections. Section 2.1 explains some post-processing methods which have been applied previously for OCR correction. Since our method-ology involves reduction of the effective vocabularies, in Sect. 2.2 we describe some lexicon reduction-based strat-egies present in the literature, which have been employed for eliminating spurious choices from the word recognition outputs. 2.1 Post-processing strategies A general survey of the research in OCR post-processing can be found in [ 7 ]. Perez-Cortes et al. [ 8 ] describes a sys-tem which uses a stochastic finite-state machine that accepts the smallest k -testable language consistent with a represen-tative language sample. The set of strings accepted such a FSM is equivalent to a k -gram language model. Depending on the value of k , the model may behave deterministically or non-deterministically. They report reducing error rate from 33% to 2% on OCR output of handwritten Spanish names from forms. Pal et al. [ 9 ] describes a method for OCR error correction of an inflectional Indian language Bangla. Their technique is based on morphological parsing where gram-matical agreement between all candidate root X  X uffix pairs of each input string are tested. This allows the detection of the root/suffix part in which the error has occurred. The correc-tion is made by means of a fast dictionary access technique. They report correcting 84% of the words with a single char-acter error. Taghva et al. [ 10 ] proposes an interactive spelling correction system specifically designed for OCR error cor-rection. The system uses multiple information resources to propose correction candidates and lets the user review the candidates and make corrections. In a previous work [ 11 ], we describe a phrase-based direct model for OCR correc-tion. Correction is modeled as a simplified translation task. Noisy OCR output is taken as the source language and clean output (ground truth) is taken as the target language. The alignment is obtained using a simple dynamic programming-based minimum edit distance alignment. The other important research done by Wick et al. [ 12 ] used the concept of topic-based newsgroup training corpus to correct lexical errors and showed a decrease in the error rate of approximately 7%. Even though this work has similar motivation, it is based on an invalid assumption that the incorrect words are known a priori. The correction is performed for non-lexicon words to topic-specific lexicon words. This is not possible in word-model-based handwriting recognition systems where the out-puts (correct or otherwise) are always valid lexical entries.
Some of the above-mentioned techniques are highly dependent on the language since they use language-specific features. Therefore, such techniques are not applicable to a different language. Second, some of these techniques fail to use multiple recognition choices from the OCR and their per-formance is limited to top choice OCR output. The proposed methods in this paper is able to use top-n choices from the OCRineliminatingthespurious choices. Third, noneof these techniques uses domain knowledge or topic information of the document to reject noisy words from the recognition out-put. In finite-state machine-based methods, transition prob-abilities are obtained using a generic language model which is assumed to be independent of the topic of the document in question. In this work, we relax this assumption of a global language model by incorporating knowledge of the context (topic). 2.2 Lexicon reduction-based strategies The task of word recognition is a pattern recognition problem that can be simply stated as  X  X iven a word image what is the best possible corresponding word in a given lexicon X . One of the elements that contributes more to the complexity of the recognition task is the size of the lexicon. Table 2 describes how the increase in the size of the lexicon reduces the rec-ognition performance of a handwriting recognition system developed at CEDAR [ 13 ]. The problem of large lexicons is the number of times that the observation sequence extracted from the input image has to be matched against the words in the lexicon. So, a more intuitive approach attempts to limit the number of words to be compared during the recognition.
Whereas this is straightforward in a task that is geared towards a particular application such as postal automation or bank check recognition, it is non-trivial in large vocabulary and unconstrained systems. A general survey of the research in this area can be found in [ 14 ]. Kaufmann et al. [ 15 ]use the length of the word image as a simple criteria for lexicon reduction. A length-based model is trained for every word image. The test word image is classified into one of these models.Wordsbelongingtothisselectedmodelarepreserved in the lexicon and rest of the words are removed, thereby creating a reduced lexicon for the word image. Powalka et al. [ 16 ] estimate the length of cursive words based on the number of times an imaginary horizontal line drawn through the middle of the word intersects the trace of the pen in its densest area. Guillevic et al. [ 17 ] also adapt a similar approach of estimating word length and reducing the lexicon size. They estimate the number of characters by counting the number of strokes crossing within the main body of a word. Madhvanathetal.[ 18 , 19 ]useholisticfeaturebasedoncoarse representation of the word shape for pruning large lexicons. Most of these techniques rely on features extracted from the word image which are susceptible to errors. In addition to this the coverage of the lexicon (percentage of test words in lexi-con) tends to reduce drastically using these techniques. Most of the features extracted for word-shape based reduction are non-trivial to extract and prone to errors. Also, in large vocab-ulary systems the number of words that have same the length or global shape tend to be large thus leading to ineffective reduction in the size of the lexicon. We present a method that automaticallycategorizesadocumentintoapre-definedtopic and reduces the size of the lexicon based on this information. 3 Lexicon reduction using topic categorization Lexicon reduction cannot be achieved trivially. It is either a painstaking manual effort or can be automated if we can con-strain the lexicon by the domain we are interested in. This is not only important for indexing of documents; it can be exploitedinthereductionoftheinputlexicontoawordrecog-nizer as well. Milewski and Govindaraju [ 20 ] have described an initial work in this area where the domain is restricted to medical forms. Each medical form is associated with a  X  X oncept X , e.g., leg injury, heart attack, etc. They have shown that given this information, restricting the lexicon by con-cepts increases the efficiency of the recognizer and hence the retrieval significantly.

The rationale behind our approach is that the choice of words in a document is characteristic of the topic under dis-cussion. For example, the presence of words like { sensory, brain, cortex, nerve, ... } immediately leads us to believe that the document relates to the medical literature. Similarly, words like { commerce, export, import, bank, ... } suggest a trade document. However, the knowledge of the topic of dis-course is not available a-priori. This leads to the necessity of automatically categorizing documents into topics. Figure 2 shows the schematic of our system that targets this problem. We explore automatically creating small and representative lexicons Lex i for topics of interest T i from a large vocabulary V (Fig. 3 ) where | Lex i | &lt;&lt; | V | . Given the automatically detected topic from Fig. 2 , we can then use this smaller lex-icon in the second phase of recognition instead of the large vocabulary in order to procure better recognition outputs.
Document categorization is a well-researched area in the field of information retrieval when it comes to text categorization. Various topic models have been proposed that treat documents as bag of word models (SVD decomposition followed by cosine similarity measures [ 21 ], Naive Bayes [ 22 ], etc.). For a given handwritten document to be catego-rized, the raw output of a recognizer is extremely noisy. In the presence of noise, accuracies of all methods suffer badly. Work presented in [ 23 ] demonstrated how the accuracy of document categorization reduces drastically as the accuracy of OCR reduces. This problem is magnified in the case of handwriting recognition. For a large lexicon the word error rate for the top choice is generally very high. However, the error rates for the same lexicon when taking top N outputs of the recognizer into consideration are significantly lower than in the first case (see Table 3 ). We propose to utilize this fact and will present novel features to categorize a doc-ument based on n -best hypotheses for a word using a word recognizer.

Assuming that a document is related to a single topic (an assumption, which we could relate later, however, valid for handwritten documents at this point) and is composed of a set of words W ={ w i ,w 2 ,...,w n } . Let us assume that we fix the documents to a set T ={ t 1 , t 2 ,..., t n } . Then the prob-ability of the topic given the words in the document can be estimated as P ( T = t i | W ) =
Whereas, in a system where we are dealing with text, the term P ( W | T = t i ) could be estimated from the number of occurrences of the word w i given a topic labeled document, in our case each w i is a set of n-best hypotheses for each word image instead of being an atomic unit. The model can be thought of as a bag of words where each word instead of being one entry is a bunch of hypotheses with their poster-ior probabilities. Thus, our model would have to consider a document as a bag of as many bags as there are words in the document. The size of each smaller bag would be the value of  X  X  X  as in the n-best hypotheses. Figure 2 is a pictorial rep-resentation of the task of categorization of the handwritten document.
 Figure 4 shows an illustration of topic categorization. Figure 5 illustrates an example of constructing smaller topic-specific lexicons from an initial lexicon after topic categorization. This translates to extracting high-frequency words from the topic-specific distribution of words in the vocabulary. As we note, this method solves the problem of creatinglexiconsbeforehandsincethelexiconsaresecondary outputs of the topic categorization algorithm. However, this method still suffers from the problem of out-of-vocabulary words since pruning lexicons based on highly probable words can remove some words from the tail of the distribution that may still occur in the test documents. This leads to the neces-sity of the second variant of our proposed method where instead of pruning, the whole distribution is considered. 4 Topic-based language models Word recognition can be understood as obtaining a maximum likelihood (ML) estimate between the input word image and the entries of the lexicon. Each estimate in this case repre-sents the likelihood of the word image having corresponding lexicon term as its correct transcription. These likelihoods are generally represented as probabilities and are obtained from the word recognizers. In this paper, we propose refining this process by finding maximum a posteriori (MAP) value of term in the lexicon corresponding to the input word image as showninEq. 2 . Previous approaches may also be explained identically using this MAP approach, with the main differ-ence being that they assume a uniform prior probability of all lexicon terms which reduces this approach to maximum likelihood (ML)-based approach.
 P ( t = t k | w) = arg max P ( t | w) = P ( t | w)  X  P (w | t ). P ( t ) (4) where P (w | t ) represents the likelihood of word image given the lexicon entry which is obtained from an OCR and P ( t represents the prior probability of observing the term in the lexicon. Most of the previous approaches assume a uniform prior probability of the terms in the lexicon. A non-uniform prior probability of lexicon entries can be learnt using a n -gram language model and such methods have also been previously used in the information retrieval community [ 24 ].
Typically, language model-based approaches use a single training corpus. A unigram term probability is obtained by counting the frequency of every term in the corpus and then normalizing it by the total number of words present in the document. However, the major drawback of this approach is that it assumes the term count to be independent of the docu-ment topic which is not true. In practice, it has been observed that documents consist of topics, and each topic is well repre-sented by a set of terms. Therefore, a single non-topic-based language model will not allow a better estimate of the term probability P ( t ) since the term probability is assumed to be independent of the document topic (Fig. 6 ).

We propose a topic-based language model for estimating prior term probability P ( t ) of every term in the lexicon. First of all, training data are created where documents belonging to different topics are manually categorized. Separate topic-based language models are generated for each of these topic with an assumption that each language model is consistent within a given topic. Let us assume that the training set con-sists of n topics. Each topic-based language is represented by LM i where i = ( 1 , 2 ,..., n ) . The global language model is now replaced by a collection of individual topic-based language models LM i . Given an input test document, the distribution of all the trained topics is computed. Let d resent the topic distribution of a document d , which is basi-cally the probability of document d belonging to topic i . Word likelihood scores P (w | t ) for every word w in docu-ment d is obtained from the word recognizer. For every word w  X  d , the posterior term probability P ( t | w) is calculated for all the terms t in the lexicon using Eqs. 4 and 5 (used for computing new value of P ( t ) ). Finally, the term t lexicon having maximum posterior probability is output as the corrected recognizer choice for the word w . The overall system architecture is also shown in Fig. 7 .
 P ( t ) = where P ( LM i )  X  P ( d i ) in this case. This probability is again computed using topic categorization methods and from among the methods described in this paper, we will use the Maximum Entropy model in this setting. We will touch upon this later.

This methodology can also be understood from the per-spective of a dynamic lexicon construction. The proposed method is equivalent to constructing a dynamic lexicon for every word image in the document. Each dynamic lexicon consists of all the entries of the complete lexicon but con-tains an additional weight P ( t ) associated with every term t which is learnt using Eq. 5 . The word recognition likelihood P (w | t ) obtained from the word recognizer is multiplied with P ( t ) to obtain a corrected posterior probability (Fig. 8 ).
The foundation of both systems described above is the topic categorization technique for the noisy handwriting rec-ognition output. In the following section, we now describe this most crucial part of the solution. 5 Models for topic categorization Researchers have used techniques like Naive Bayes [ 22 ], MaximumEntropyModels[ 25 ],ConditionalRandomFields, Hidden Markov Models and more recently, Latent Dirichlet Allocation for text classification. As described in Sect. 3 , our task can be compared to classification with noisy data. We cannot directly apply these methods due to low accuracies and hence in this paper we describe how we adapted Naive Bayes model and also how we used the top-n information as a feature in the Maximum Entropy Model. 5.1 Naive bayes Naive Bayes classifier is a simple model for text classification that assumes independence amongst words in a give context. Because of the independence assumption the task of learning is greatly simplified. A document d i is assigned to a category c under a model  X  using the formula j = arg max = arg max The independence assumption dictates P ( d i | c j , X ) = where | d i | is the length of the document d i .

If | D | denotes the total number of documents, then the class prior is given by P ( c j |  X ) = McCallum and Nigam [ 22 ] describe how two variants of the Naive Bayes could be utilized for text classification. The first variant only depends on the presence or absence of words and thus can be characterized by a distribution that is based on a multi-variate Bernoulli . In this case, the generative model is P (w where B
As we note, this model only captures the existence of a word given a topic and fails to capture the word counts and occurrences which is an important feature in the document topic. Also, in our case since the accuracies are low and we utilize top-n ( n = 10 in our case) outputs from the recog-nizer, this model is not suited for our task. On the other hand, the second variant of Naive Bayes where the document is rep-resented by the set of word occurrences captures this infor-mation. Here it can be characterized by a distribution that is a multinomial in which a document is an ordered sequence of word events, drawn from the same vocabulary V .Thisis similar to what can be referred to as the  X  X nigram language model X . The generative model is thus modified to P (w where N Note that in the equation of N it , we have replaced the count of the word by the sum of the probabilities P ( im = w t ) is the probability from the word recognizer for the image im being word w t . This is what we will refer to as the partial count of the word. Thus, we were easily able to introduce the top-n choices from the word recognizer by replacing the count for that choice by its partial count (probability score). Figure 9 shows an example of the partial word count feature. 5.2 Maximum entropy The Naive Bayes, however, suffers from the independence assumption which may not be valid. This is often referred to as the bag-of-words model where the words are exchange-able and the topic does not depend on the mutual dependence of the words. To account for this we modified the Maxi-mum Entropy Model. The motivating idea behind maximum entropy is that the most uniform model should be preferred when no information is present and whatever information is present should constrain that uniform model. Thus, the Maximum Entropy model prefers the most uniform model satisfying the constraint P ( c | d ) =  X   X  X  represent parameters that are estimated during the course of the Maximum Entropy training. The f i s are any real-val-ued functions describing features of the (document, class) relationship, d represents the document, and c represents a category. In order to include partial counts as in the Sect. 5.1 , we use a real-valued feature function f w, c ( d , c ) = where N ( d ,w) is the  X  X artial X  count (probability from rec-ognizer) of the word w in document d . We also experiment with extracting other features from the document such as chunk distribution probability. Given a word image and its top-n recognition hypothesis, we divide the top-n results into 10 chunks each of size 10. For example, top-20 results are divided into 2 chunks where the first chunk contains choices from rank 1 X 10, and the second chunk consists of choices from rank 11 X 20. We maintain a count of each chunk from the OCR results of the training documents and convert them into probability scores by normalizing each chunk count by the sum of all chunk counts. The primary motivation behind using this feature is the observation that similar word images tend to generate a similar ranked list of lexicon entries which can be used to represent the noise model or the cor-rection model of the OCR. Figure 10 illustrates an example of extracting chunks from the OCR results.
 As is mentioned in Nigam et al. [ 25 ] since the Maximum Likelihood training overfits, we use a Maximum A posteri-ori training with Gaussian priors over feature functions. The prior probability of the model is the product of Gaussians of each feature value  X  i with variance  X  2 i . The training has been implemented using the Improved Iter-ative Scaling (IIS) algorithm which is of the Quasi-Newton family of numerical algorithms. We used the Mallet (http://mallet.cs.umass.edu) package from University of Massachusetts at Amherst to train the model. The technical report [ 26 ] provides further details of the Maximum Entropy model used in the context of natural language processing. 6 Experiments For the purpose of our experiments we have used the publicly available dataset called the IAM database [ 27 ].

The IAM database has the following characteristics:  X  657 writers contributed samples of their handwriting  X  1,539 pages of scanned text  X  5,685 isolated and labeled sentences  X  13,353 isolated and labeled text lines  X  115,320 isolated and labeled words  X   X  15k lexicon entries There are 13 topic categories ranging from Press to Religion as listed in Table 4 . More details on the database can be found in [ 27 ]. This dataset is unconstrained English and the pages are classified according to the topics. Figure 11 shows an image from the dataset. 6.1 Topic categorization results The test set (40%) of the data was used to evaluate the topic classification performance. In order to prove statistical sig-nificance, we conducted these experiments in an n -fold cross validation setting ( n = 100 in our case). We split the data randomly into 60% for training and 40% for testing and repeated it a hundred times. We also experimented with the 80 X 20 split, but we report results on the former configura-tion. During each iteration, we stored the information about the data that were used for testing as this would be required for the experiment of lexicon reduction. Figure 12 shows the average performance of the topic classification algorithms. After classification, we selected the iteration number with topic classification performance approximately equal to the overallaverageclassification.Asmentionedearlier,theinfor-mation about the test data was stored in each iteration and thus we extracted the portion of the data used for testing in this particular iteration. Table 5 shows the result of fold selec-tion method for 100-fold validation. 0 . 8 denotes that 80% of data is randomly sampled as training data and 0 . 6 denotes that 60% represents the training data. The fold value denotes the iteration number for which the classification accuracy was approximately equal to the overall average classification accuracy. 6.2 Lexicon reduction results After topic classification, the lexicon was reduced based on the highest mutual information of the words with the topic and then recognition was performed again with the new set of words. As we note, the immediate advantage of the method is the availability of the lexicon since it is a by-product of the classification model. On an average, the size of the lexicon lies between 1.5 X 2k which is an average reduction of about 85%. The recognition accuracies before and after lexicon reduction by topic classification are reported in Table 6 . 6.3 Dynamic lexicon results As noted in Fig. 12 , the Maximum Entropy model outper-forms the others. So, for this experiment we have used this method in conjunction with the topic-specific language models.

Table 7 shows the accuracy of the recognizer using the topic-specific language models. Raw accuracy here repre-sents the initial word recognition accuracy obtained from the recognizer. Corrected refers to the accuracy obtained after applying topic-based language models to each word. Since the output in this case is a single decoded sequence, we do not report top-10 accuracy in this case. As shown in the table, the proposed method significantly improves the raw accuracy of the word recognizer by  X  25%. Moreover, the method is completely trainable and flexible since it does not assume any internal details about the word recognition. 7 Conclusion We have presented a post processing method to improve unconstrained handwriting recognition accuracies by reduc-ing lexicon sizes using topic categorization of noisy word recognizer output. The method is novel in the aspect that it uses a topic-based lexicons or language models to refine the likelihood scores obtained from the recognizer instead of using a global language model. The proposed method is trainable and statistical. It is also very flexible given it does not assume any internal information about the word recog-nizer. Moreover, we believe this method can also be extended to other languages since no language-specific information is assumed here. We modified state-of-the-art statistical meth-ods of topic categorization (Naive Bayes, maximum entropy) to account for top-n choices and seamlessly include proba-bility scores of the handwriting recognizers. Without this, the methods are close to ineffective on the erroneous data produced by the recognizers. These features are also generic and can find application in similar systems like speech recog-nition, word-based machine print OCR of cursive languages like Arabic, etc. where language models and vocabularies are used. Our current work is focused on improving the topic cat-egorization results of the noisy OCRed documents which are crucial to overall performance of the system. Currently, the topic categorization is independent of the underlying noise level of the word recognition output. We are working on automatically identifying noise levels in the test documents and then associate a confidence level to the document. This confidence level can then be integrated with the topic cate-gorization score to enable a more robust estimation of topic distribution of the documents.
 References
