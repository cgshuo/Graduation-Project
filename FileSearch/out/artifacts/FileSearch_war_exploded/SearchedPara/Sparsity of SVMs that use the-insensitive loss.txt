 Given a reproducing kernel Hilbert space (RKHS) of a kernel k : X  X  X  X  R and training set D := (( x 1 ,y 1 ) ,..., ( x n ,y n ))  X  ( X  X  R ) n , the -insensitive SVM proposed by Vapnik and his co-workers [10, 11] for regression tasks finds the unique minimizer f D , X   X  H of the regularized empirical risk where L denotes the -insensitive loss defined by L ( y,t ) := max { 0 , | y  X  t | X  } for all y,t  X  R and some fixed  X  0 . It is well known, see e.g. [2, Proposition 6.21], that the solution is of the form where the coefficients  X   X  i are a solution of the optimization problem vectors of f D , X  . Furthermore, we write # for the counting measure, and hence # SV ( f D , X  ) denotes the number of support vectors of f D , X  .
 f
D , X  ( x ) . Due to this fact, the -insensitive loss was originally motivated by the goal to achieve cally it is well-known that the -insensitive SVM achieves this sparsity, there is, so far, no theo-retical explanation in the sense of [5]. The goal of this work is to provide such an explanation by establishing asymptotically tight lower and upper bounds for the number of support vectors. Based on these bounds we then investigate the trade-off between sparsity and estimation accuracy of the -insensitive SVM. Before we can formulate our main results we need to introduce some more notations. To this end, let P be a probability measure on X  X  R , where X is some measurable space. Given a measurable that P can be split into the marginal distribution P X on X and the regular conditional probability P(  X | x ) . Given a RKHS H of a bounded kernel k , [1] then showed that measure at some ( x,y )  X  X  X  R . By considering the empirical measure D := 1 n P n i =1  X  ( x the solution of (1). Finally, we need to introduce the sets where f : X  X  R is an arbitrary function and  X   X  R . Moreover, we use the short forms A low ( f ) := A low ( f ) and A up ( f ) := A Theorem 2.1 Let P be a probability measure on X  X  R and H be a separable RKHS with bounded  X  X   X  4 , we have and Before we present our second main result, we briefly illustrate Theorem 2.1 for the case where we fix the regularization parameter  X  and let n  X  X  X  .
 Corollary 2.2 Let P be a probability measure on X  X  R and H be a separable RKHS with bounded measurable kernel satisfying k k k  X   X  1 . Then, for all  X  &gt; 0 and  X  &gt; 0 , we have Note that the above corollary exactly describes the asymptotic behavior of the fraction of support vectors modulo the probability of the set above corollary gives an exact description.
 Instead, it is well-known, see [1], that the regularization parameter should vanish in order to achieve consistency. To investigate this case, we need to introduce some additional notations from [6] that where P is a distribution and the infimum is taken over all measurable functions f : X  X  R . In addition, given a distribution Q on R , [6] and [7, Chapter 3] defined the inner L -risks by and [6, Lemma 2.5], see also [7, Lemma 3.4], further established the intuitive formula R  X  L , P = R The following lemma collects some useful properties of these sets.
 Lemma 2.3 Let P be a probability measure on X  X  R with R  X  L , P &lt;  X  . Then M  X  ( x ) is a non-empty and compact interval for P X -almost all x  X  X .
 Given a function f : X  X  R , Lemma 2.3 shows that for P X -almost all x  X  X there exists a unique t ( x )  X  X   X  ( x ) such that t ( x ) on  X  . With the help of these elements, we finally introduce the sets M up ( f ) . Now we can formulate our second main result.
 Theorem 2.4 Let P be a probability measure on X  X  R and H be a separable RKHS with bounded measurable kernel satisfying k k k  X   X  1 . Assume that R L , P (0) &lt;  X  and that H is dense in L n  X  1 we have P n D  X  ( X  X  R ) n : P M low ( f P , X  )  X   X   X  # SV ( f D , X  ) If we choose a sequence of regularization parameters  X  n such that  X  n  X  0 and  X  2 n n  X  X  X  , then the resulting SVM is L -risk consistent under the assumptions of Theorem 2.4, see [1]. For this case, the following obvious corollary of Theorem 2.4 establishes lower and upper bounds on the number of support vectors.
 Corollary 2.5 Let P be a probability measure on X  X  R and H be a separable RKHS with bounded measurable kernel satisfying k k k  X   X  1 . Assume that R L , P (0) &lt;  X  and that H is dense in L (P X ) . Furthermore, let (  X  n )  X  (0 ,  X  ) be a sequence with  X  n  X  0 and  X  2 n n  X  X  X  . Then, for all  X  &gt; 0 , the probability P n of D  X  ( X  X  R ) n satisfying converges to 1 for n  X  X  X  .
 for fixed x  X  X and  X   X  0 it seems difficult to show that f P , X  ( x ) is not  X  X lipping X  from the left M P whose conditional distributions P(  X | x ) , x  X  X , have no discrete components, we always have However, there are situations in which this equality can easily be established. For example, assume for which y is not contained in the closed or open -tube around M  X  ( x ) , respectively. Consequently, (7) holds provided that the conditional distributions P(  X | x ) , x  X  X , have no discrete components, and hence Corollary 2.5 gives a tight bound on the number of support vectors. Moreover, if in this case we additionally assume = 0 , i.e., we consider the absolute loss, then we easily find M  X  ( x ) equals the median of P(  X | x ) , and hence M  X  ( x ) is a singleton whenever the median of P(  X | x ) is unique.
 Let us now illustrate Corollary 2.5 for &gt; 0 . To this end, we assume in the following that the conditional distributions P(  X | x ) are symmetric , i.e., for P X -almost all x  X  X there exists a con-more, the assumption R L , P (0) &lt;  X  imposed in the results above ensures that the conditional c ( x ) = f  X  P ( x ) for P X -almost all x  X  X . Moreover, from [8, Proposition 3.2 and Lemma 3.3] we immediately obtain the following lemma.
 Lemma 2.6 Let P be a probability measure on X  X  R such that R L , P (0) &lt;  X  . Assume that the conditional distributions P(  X | x ) , x  X  X , are symmetric and that for P X -almost all x  X  X there exists a  X  ( x ) &gt; 0 such that for all  X   X  (0 , X  ( x )] we have the unique median of P(  X | x ) .
 Obviously, condition (8) means that the conditional distributions have some mass around their me-dian f  X  P , whereas (9) means that the conditional distributions have some mass around f  X  P  X  . More-over, [8] showed that under the assumptions of Lemma 2.6, the corresponding -insensitive SVM can be used to estimate the conditional median. Let us now illustrate how the value of influences both the accuracy of this estimate and the sparsity. To this end, let us assume for the sake of simplicity By the symmetry of the conditional distributions it is then easy to see that these densities are sym-the conditional distributions are equal modulo translations. In other words, we assume that there exists a continuous Lebesgue density q : R  X  [0 ,  X  ) which is symmetric around 0 such that for P
X -almost all x  X  X we have Note that this assumption is essentially identical to a classical  X  X ignal plus noise X  assumption. In the following we further assume that q is unimodal, i.e., q has its only local and global maximum 2.6 and the discussion around (7) we then conclude that under the assumptions of Corollary 2.5 the fraction of support vectors asymptotically approaches 2Q([ ,  X  )) , where Q is the probability measure defined by q . This confirms the intuition that larger values of lead to sparser decision functions. In particular, if Q([ ,  X  )) = 0 , the corresponding SVM produces super sparse decision functions, i.e., decision functions whose number of support vectors does not grow linearly in the Lemma 3.3] indicates that the size of q ( ) has a direct influence on the ability of f D , X  to estimate [8, Lemma 3.3] and the convexity of t 7 X  X  L , Q ( t ) that By a literal repetition of the proof of [8, Theorem 2.5] we then find the self-calibration inequality which holds for all f : X  X  R with R L , P ( f )  X  X   X  L , P  X  2 / 2 . Now, if we are in the situation However, the guarantee for this approximation becomes worse the smaller q ( ) becomes, i.e., the larger is. In other words, the sparsity of the decision functions may be paid by less accurate estimates of the conditional median. On the other hand, our results also show that moderate values for can lead to both reasonable estimates of the conditional median and relatively sparse decision functions. In this regard we further note that one can also use [8, Lemma 3.3] to establish self-is obvious that such self-calibration inequalities are worse the larger is, and hence the informal conclusions above remain unchanged.
 tions, then the situation may become more involved. In particular, if we are in a situation with calibration inequalities of the form (10) are in general impossible, and weaker self-calibration in-equalities require additional assumptions on P . We refer to [8] where the case = 0 is considered. Setting C := 1 2  X n and introducing slack variables, we can restate the optimization problem (1) as f  X  = f D , X  . It is well-known, see e.g. [2, p. 117], that the dual optimization problem of (11) is (12), then we can recover the primal solution ( f  X  , X   X  ,  X   X   X  ) by for all i = 1 ,...,n . Moreover, the Karush-Kuhn-Tucker conditions of (12) are lemma provides lower and upper bounds for the set of support vectors.
 Lemma 3.1 Using the above notations we have Proof: Let us first prove the inclusion on the left hand side. To this end, we begin by fixing an index  X  i = C . From (21) we conclude  X   X  y  X  f D , X  ( x i ) &gt; can be shown analogously, and hence we obtain the first inclusion. In order to The second case  X   X   X  i = 0 can be shown analogously.
 We further need the following Hilbert space version of Hoeffding X  X  inequality from [12, Chapter 3], see also [7, Chapter 6.2] for a slightly sharper inequality.
 Theorem 3.2 Let ( X  , A , P) be a probability space and H be a separable Hilbert space. Moreover, let  X  1 ,..., X  n :  X   X  H be independent random variables satisfying E P  X  i = 0 and k  X  i k  X   X  1 for all i = 1 ,...,n . Then, for all  X   X  1 and all n  X   X  , we have Finally, we need the following theorem, see [7, Corollary 5.10], which was essentially shown by [13, 5, 3] .
 Theorem 3.3 Let P be a probability measure on X  X  R and H be a separable RKHS with bounded measurable kernel satisfying k k k  X   X  1 . We write  X  : X  X  H for the canonical feature map of H , that for all n  X  1 and all D  X  ( X  X  R ) n we have where E D denotes the empirical average with respect to D .
 Proof of of Theorem 2.1: In order to show the first estimate we fix a  X  &gt; 0 and a  X  &gt; 0 such that  X  X   X  4 . Let  X  :=  X  2  X  2 n/ 16 which implies n  X   X  . Combining Theorems 3.2 and 3.3 we then obtain a pair ( x,y )  X  A  X  low ( f P , X  ) , we then have by the triangle inequality and k k k  X   X  1 which implies k X k  X   X  k X k H . In other words, we have A low ( f P , X  )  X  A low ( f D , X  ) . Consequently, Lemma 3.1 yields Combining this estimate with (22) we then obtain Moreover, Hoeffding X  X  inequality, see, e.g. [4, Theorem 8.1], shows for all  X  &gt; 0 and n  X  1 . From these estimates and a union bound we conclude the first inequality. In order to show the second estimate we first observe that for training sets D  X  ( X  X  R ) n with k f and hence (22) yields Using Hoeffding X  X  inequality analogously to the proof of the first estimate we then obtain the second estimate.
 Let us show Obviously, the inclusion  X   X   X  directly follows from the above monotonicity. Conversely, for ( x,y )  X  A shown ( x,y )  X  A  X  low ( f P , X  ) . From (23) we now conclude from the above monotonicity of the sets A up . From (25) we then conclude Let us now fix a decreasing sequence (  X  n )  X  (0 , 1) with  X  n  X  0 and  X  2 n n  X   X  . Combining (24) and (26) with the estimates of Theorem 2.1, we then obtain the assertion.
 Proof of Lemma 2.3: Since the loss function L is Lipschitz continuous and convex in t , it is easy hence M  X  ( x ) is a closed interval. In order to prove the remaining assertions it suffices to show R
L , P &lt;  X  implies C and a sequence ( t n )  X  R with t n  X   X  X  X  . By the shape of L , there then exists an r 0 &gt; 0 such that L ( y,t )  X  2 B for all y,t  X  R with | y  X  t |  X  r 0 . Furthermore, there exists an M &gt; 0 with P([  X  M,M ] | x )  X  1 / 2 , and since t n  X  X  X  X  there further exists an n 0  X  1 such that t n  X  X  X  M  X  r 0 for all n  X  n 0 . For y  X  [  X  M,M ] we thus have y  X  t n  X  r 0 , and hence we finally find for all n  X  n 0 . The case t n  X  X  X  can be shown analogously.
 For the proof of Theorem 2.4 we need the following two intermediate results.
 Theorem 3.4 Let P be a probability measure on X  X  R and H be a separable RKHS with bounded measurable kernel satisfying k k k  X   X  1 . Assume that R L , P (0) &lt;  X  and that H is dense in L hence lim  X   X  0 R L , P ( f P , X  ) = R  X  L , P . Now we obtain the assertion from [6, Theorem 3.16]. Lemma 3.5 Let P be a probability measure on X  X  R and H be a separable RKHS with bounded measurable kernel satisfying k k k  X   X  1 . Assume that R L , P (0) &lt;  X  and that H is dense in L then yields the first assertion. In order to prove the second estimate we first observe that i.e., we have ( x,y )  X  M 2  X  up ( f P , X  ) . Again, the assertion now follows from Theorem 3.4 . Proof of Theorem 2.4: Analogously to the proofs of (24) and (26), we find Combining these equations with Theorem 2.1 and Lemma 3.5, we then obtain the assertion.
