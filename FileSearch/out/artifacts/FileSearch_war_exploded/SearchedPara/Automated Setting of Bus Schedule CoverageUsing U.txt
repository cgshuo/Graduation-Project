 Jihed Khiari 1 , Luis Moreira-Matias 1( Public Transport (PT) reliability is a major issue in modern cities. A good oper-ational planning is necessary to deliver such service quality requirements while maintaining a balanced relationship between resource usage and obtained rev-enues. Nowadays, major PT operators have their fleets equipped with Global Positioning System (GPS) antennas, communicational devices (e.g. 3G) and Radio-frequency Identification readers able communicate the vehicle X  X  position-ing (i.e., Automatic Vehicle Location (AVL)) and its ridership (i.e., Automatic Passenger Counting (APC)) to a central server [ 1 ].
 To mine this novel source of data is a massive challenge. It contains infor-mation about the patterns of human behavior while traveling (as drivers or pas-sengers) on an urban environment. Such patterns can provide useful insights to improve the operational planning of mass transit agencies -namely, its Sched-ule Plan (SP) . Such improvement may bring multiple benefits by providing ways of reducing costs (e.g. fleet (re)sizing or fuel saving due to a decrease of the necessary number of trips) and/or improving the passenger experience. [ 2 ]: (1) the first step is to define the number k of schedules and their individual coverage, S i . Consequently, this first step defines different schedules for days that are characterized by different traffic and demand patterns due to seasonal variations, for instance. Secondly, (2) the timetables are assigned for each route schedule containing the time the buses pass at each schedule time point (per trip). This process is done for all routes. While the timetables are defined route-wise (e.g. high/low frequency routes), the number of schedules (i.e. k ) and their coverage ( S i ,  X  i  X  X  1 , .., k } ) must be defined networkwise . Such definition is key to ease PT operations (e.g. maintenance tasks) and, most of all, to facilitate the SP memorization by the passengers.
 monly focused on timetabling tasks, thus skipping the coverage definition. Some of the most well-known approaches include finding the optimal slack time and round-trip time to put into the schedule using Genetic/Ant Colony Algorithms [ 3 , 4 ], mining distribution rules able to discover feature subspaces (i.e. scenarios) for an increased travel time uncertainty [ 5 ], or clustering trips based on APC data regarding their frequency setting, i.e. high/low [ 6 ]. However, the coverage definition can easily constrain the timetable construction (e.g. two days with dis-tinct demand peak periods should have different timetables). At the best of our knowledge, only Mendes-Moreira et al. [ 2 ] covers the improvement of Schedule Coverage: a Consensual Clustering framework groups days with similar behavior (using AVL data standalone) given a predefined number of schedules k . alize this framework X  X  usage for every scenario that fully exploits the information available on the data repository while still minimizing the required human input to reach a decision. The contributions are threefold: 1. a novel ad-hoc domain-oriented metric to select the most adequate number of 2. a hybrid computation of the daily profiles using APC/AVL data simultane-3. the application of a Gaussian Mixture Model (GMM) [ 8 ] to perform the nec-The proposed framework was evaluated using data acquired from a large bus operator in Sweden throughout a period of six months. Numerical experi-ments suggested a change to the agency X  X  original coverage. The impact of such change was measured by assigning a theoretical timetable to the affected period. A before-and-after schedule reliability study was conducted. The results are promising.
 The remainder of the paper is structured as follows: methodology is described in Sect. 2 , by doing an analysis of the previous work and a formal explanation of our contributions. The case study is presented in Sect. 3 , along with some summary statistics of the used datasets. The results are presented in the Sect. 4 , followed by a brief discussion. Finally, conclusions are drawn. A stepwise methodology is hereby proposed to automatically set both the number of schedules and their daily coverage. This description follows closely the one proposed in Sect. 4 of [ 2 ]. It elaborates on the principle that days where the route trips have a similar behavior (e.g. round-trip times) throughout the day should be assigned to the same schedule. Let L = { r 1 , ..., r n interest. Firstly, for each r  X  L , the running times and the boardings/alightings at each stop (if existing) are extracted from its original AVL/APC dataset. Secondly, the daily profiles are generated. If there is no APC data available for a specific route, the procedure originally suggested in [ 9 ] is used. Otherwise, a biased dwell time model is generated based on APC data to account demand peaks/valleys. Its output is added to the link travel times computed through the AVL data -as described in Sect. 2.1 .
 The next two steps generate a distance matrix between the days (using their daily profiles) and cluster them. The first task is conducted using a Euclidean-flavoured Dynamic Time Warping, while the latter is addressed using a GMM. Conversely to previous works, the clustering is made for a user-defined set of admissible number of schedules K  X  N , i.e.  X  k  X  K instead of a single predefined k value. The above mentioned steps are repeated for all routes.
 Step 5 selects the best possible k  X  K to define the best number of schedules to put in place. This is made using a two-stage process, where an ad-hoc metric is devised to evaluate the clustering result for each pair ( r, k ) , a consensual k , i.e. K is found through a domain-oriented weighted mean of the previously computed metrics -as described in Sect. 2.3 . Finally, a Consensual Clustering procedure is devised using the clustering pieces obtained for k = K to compute the suggested Schedule Coverage, following the original procedure proposed in [ 2 ]. An illustration of our methodology is presented in Fig. 1 .The remainder of this Section describes our contributions. 2.1 Modeling the Daily Profiles Let L = { L 1 ,L 2 , ..., L n } be a set of the available AVL datasets for n consid-ered routes, and C = { C 1 ,C 2 , .., C n } a set of the corresponding APC datasets. If C i =  X  , the round-trip time for every trip is obtained by adding the dwell times at stops and the link travel times as they are described in the AVL data. as slight increases/decreases of the computed round-trip time. Let r be a route of interest with the associated datasets ( L i ,C i ) where t is the number of trips and s is its number of stops. This procedure starts by modeling the dwell time at stop through a decomposition in multiple factors. It can be computed as follows: where  X  and  X  are constants that denote the alighting and boarding time per passenger, respectively, and doc denotes the time allocated for operations that take place on every stop, e.g. the opening and closing of doors. On the other hand, a o,j and b o,j are the number of passengers that alight/board on a stop j during a trip o , respectively, where o  X  X  1 , 2 , ..., t and b o,j (APC), we perform a linear regression procedure to estimate the val-ues of  X  ,  X  and doc . It consists of three steps: firstly, we isolate the samples (i.e. boardings/alightings and dwell times for every pair of [trips/stops] avail-able) where a o,j =0and b o,j = 0 into two different partitions. This allows to transform Eq. 1 into a linear one. Secondly, we estimate values for  X ,  X  and two possible values for doc , i.e. doc a ,doc b . Finally, the doc value is computed as doc = doc a + doc b 2 . Then, we use the resulting constants to compose a novel func-tion for the dwell time (i.e.  X   X  o,j ). This function is used with the original APC data to compute novel dwell time estimations, which are summed up to the link travel times observed in the original AVL data.
 dure is a modified version of the well-known least squares, where we replace its typical loss-function (a sum of the squared residuals) for the mean absolute devi-ation (MAD) (i.e. which results in a simple sum of the residuals). This change increases the framework X  X  tolerance to large errors (i.e. demand peak/valleys), which will result in an under/overestimation of the dwell times under such con-ditions. This effect aims to model the demand peaks/valleys inside the daily profiles of round trip times typically used by [ 2 ]. By producing a daily profile based on heterogeneous sources of data, we aim to adequately express the dif-ferences between the route behavior -both in terms of cruising time and in its demand -on the schedule coverage definition. 2.2 Expectation-Maximization (EM) for Clustering Analysis [ 2 ] proposed k-Means algorithm to perform the routewise clustering in the con-text of this application. This approach assumes a deterministic clustering step where the model is only given by the Euclidean Distance to the incrementally computed centroids (i.e. spherical clusters, parametric). Such characteristics may easily lead to an undesired overfitting, where the samples are erroneously ini-tially assigned to a non-homogeneous cluster, potentially increasing the variance within. To overcome this limitation, we propose a GMM (a general version of k-Means), which (briefly) operates as follows: firstly, it (a) softly assigns a sam-ple to a cluster, i.e., computing the probability of any point belonging to every centroid; then, it (b) estimates the parameters of the probability distribution, taking the sample-based covariances into account. 2.3 Automated Selection of Number of Schedules The selection of the best number of clusters is a complex problem in data analy-sis. One of the most well-known metrics to do it so is the Bayesian Information Criterion (BIC) [ 10 ], which computes an entropy-based probabilistic score that, when maximized over a set of values, i.e. K , aims to return the optimal k by minimizing the entropy between samples of the same cluster and maximizing the one between samples of different ones. However, such optimization problem may not lead to a good solution for a real-world context, given the constraints that each application domain encloses. Consequently, ad-hoc metrics are often devised to address such issues (e.g. market segmentation in [ 11 ]). In this context, we depart from BIC to set up an ad-hoc metric, i.e. m for this problem as a linear combination of multiple factors. These factors were con-sidered in light of two main constraints: (1) the cost of increasing the number of defined schedules (which reduces the schedule X  X  interpretability as well as its easy memorization, the operators X  ability to easily put it in place, and conse-quently, the route X  X  riderships) must be necessarily balanced by a gain on the punctuality of the offered service, by reducing significantly the entropy on the produced clusters; (2) the cluster X  X  output must model a frequent pattern (e.g. the Saturdays should be grouped with the Sundays throughout five months of an year). Such factors can be expressed as follows: where nbic ( k, r ) is the normalized 2 value of BIC. (1) The first term of Eq. 2 addresses the number of clusters. High values of nbic will bring a gain on the punctuality of a suitable timetable defined for such partitioning. On the other hand, the increase of the number of schedules to maximize such punctuality must be done if and only if such gain is significant . Consequently, we need to model a trade-off between an eventual gain given by increasing the number of schedules and the associated cost of decreasing its interpretability. We do it so by introducing a penalty term f ( k, r ) 2 that favors lower values of k , where f ( k, r )= k/max ( K ) ,  X  r  X  L .
 titioning for a number of schedules k . Empirically, we know that a SP in PT should cover a static set of daytypes (e.g. Mondays) throughout a relatively long set of weeks. Consequently, a suitable cluster would be one that provides such frequent pattern . The suitability of each cluster is given by an ad-hoc quality metric, i.e. q ( k, r ). It is computed in two stages: (2a) frequent itemset mining and (2b) compatible pattern merging. This procedure is detailed as follows. Cluster Quality Computation. A frequent pattern in this problem can be modeled through a sequence mining problem to find frequent itemsets of daytypes among the weeks (i.e. transactions ) covered by the input data (e.g. Mondays to Fridays). Let  X ,  X   X  [0 , 1] denote two user-defined parameters for the minimum support to consider a given itemset as frequent (i.e. the minimum amount of weeks to define a schedule) and for the minimum cluster X  X  mass ratio to be covered by it, respectively. The PrefixSpan algorithm [ 7 ] is hereby adopted to find such frequent itemsets, i.e. FI i among the daytype X  X  transactions obtained from each partition S i .Let N denote the number of weeks in the input data. The frequent pattern of each cluster, i.e. FP is then selected as follows: where  X  ( FI i ) is the support of the frequent itemset FI quality of each cluster is then computed as q ( k, r )= k this domain, it is very common to find complementary schedules (e.g. work-days for all year and workdays during summer vacations, with a support of 0 . 9 and 0 . 1, respectively). Together , these complementary clusters would present a very meaningful frequent pattern which is penalized by the q ( k, r ) computation formula introduced above. Consequently, we introduced a merging step which aims to find such clusters and to merge them in order to obtain the overall qual-ity of the coverage proposed by a given value of k . This merging step aims to find clusters which have frequent itemsets complementary to a given FP at most, one of the two constraints imposed in Eq. 3 . The algorithm to do it so is introduced by Fig. 2 . Note that two clusters are considered as complementary if they overlap, at most, 10 % of the weeks of the input data. Given the resulting clusters after the merging procedure (with a number of k clusters), we can compute the final cluster X  X  quality as where FPM i denotes the support of the frequent itemset of a merged clus-ter. Obviously, the resulting clusters may also contain other samples regarding daytypes not included in the frequent itemset (e.g. a cluster modeling the week-ends which have two Mondays within). These samples are referred to as noise in this context. Such noise naturally decreases the adequacy of the frequent pattern is calculated based on the standard deviation between the relative frequencies of every day within a particular cluster. It can be computed as: where fr k,S i , where a relative frequency of a daytype d within a cluster S number of days of daytype d divided by the cluster X  X  mass.
 Given such metric computation for all pairs ( r, k ), we can now compute a consensual number of clusters K .Let  X  ( r ) denote the normalized (see Footnote 2) number of trips for the route r . The consensual number of clusters K is defined by a weighted average of k  X  K . We can express K  X  N as follows: Our case study was a large urban bus operator in Sweden. We used data from four high-frequency (maximum planned headway of 10 min between 7:00 X 19:00) routes A1/A2/B1/B2, i.e. two bus lines A/B. Line A links residential areas to a PT hub as well as major shopping areas. B connects the southern parts of the city to the city center, traversing by a PT hub, major hospitals as well as a logistic center. This study covers six months between August 2011 and January 2012. The coverages in place are relative to two time periods: Summer, from 19 June till 14 December and Winter: from 15 December till 18 June. Two schedules are defined for each period: workdays and weekends/holidays.
 than 80 % of missing link travel times. Reversely, we performed data imputation on the remaining samples by following the interpolation procedure suggested in [ 2 ]. The dwell times were also pruned by using the 99 % percentile to remove erroneous measurements. APC data was used as is.
 contains the (i) total number of trips (NT), (ii) its number of stops, (iii) the Daily Trips (DT), (iv) the Round Trip Times (RTT) and (v) the loads (i.e. total number of boarding passengers). Both have a similar NT, while line A has a larger RTT than B. The experiments were conducted using the R language [ 12 ]. The model-based clustering was performed using the GMM implementation of mclust package [ 13 ]. To compute the frequent itemsets used in the cluster X  X  quality computation, a C++ implementation of PrefixSpan [ 14 ] was employed. This framework has three parameters: K , X  and  X  . Their values were set to 2 0 . 25 and 0 . 4, respectively. The first used the range suggested by the original experimental setup in [ 2 ]. The value of  X  was empirically set such that a schedule can only be set for a period of, at least, four weeks; on the other hand,  X  was selected out of three possible values 0 . 4 , 0 . 5 , 0 . 6 through an iterative parameter tuning setting conducted on a small subset of the training data.
 gested a novel SP -as detailed further in this Section. Its impact on the agency X  X  operations in terms of schedule reliability was assessed through a simulation procedure, described in the next section. 4.1 Impact Evaluation Through a Data-Driven Simulation Any change of the schedule coverage will result in one of two scenarios: (i) a group of days B changes from one coverage to another among the ones that were already in place or (ii) it will take a completely novel timetable. The procedure that we describe hereby is focused on the type-i Scenarios. Let A and Z be two groups of days with different coverages and, consequently, distinct timetables assigned where B  X  A . Our goal is to test whether the time period B would benefit from having the same timetable of Z instead of its original one (i.e. from B ). This procedure is done in three steps: firstly, we need to assign a timetable to B -which will change from the one in place in A to the one used in Z we need to simulate which would be the (a) link travel times and (b) the dwell times generated by such timetable given the available AVL/APC data. The (a) link travel times are generated through a k -Nearest Neighbors regres-sion [ 16 ]( k = 1), where the departure time of each stop is used as an independent variable. The demand on each stop is generated by using the headways computed through (a). These headways correspond to the idle time on a given bus stop bs  X  . The passenger arrivals at stops are modeled by iteratively sampling passenger arrival times pav i from an exponential distribution, i.e. pav the number of boardings on each stop is computed as follows: bo = arg max where  X  i is computed as time-dependent Poisson process for every specific pair ( r, bs ) by considering averages of boardings on one hour periods of the days with similar daytypes (e.g. the number of passengers boarded on a given route between 8am and 9am of every Monday) -which are linearly normalized according to the amount of idle time available to compute each bo i x . The alightings are then computed based on an assumption that the passengers traverse up to 25 % of the route. The resulting dwell times are computed using the Eq. 1 and the constant values obtained through the procedure described in Sect. 2.1 .
 The impact evaluation study is conducted on a before-and-after fashion, where schedule reliability metrics are firstly computed for the current case study (using the original AVL/APC data, as well as the SP in place). Then, the same metrics are also computed for the simulated data obtained through the abovementioned procedure. Four schedule reliability metrics were employed: O n-T ime P erformance, R un-T ime V ariation, H eadway V ariation and E xcess W aiting T ime. Details about these metrics can be found in Sect. 4 of the Survey in [ 1 ]. 4.2 Results This framework typically runs in linear time, where a single-core CPU processed the 16 k trips of our case study in  X  600 s. Figure 3 illustrates the computed val-ues for the ad-hoc metric hereby devised to assess the quality of the partitioning provided by each value of k . These values resulted in a consensual K = 3. Figure 4 shows an example of the clustering results obtained for a particular route using its best value of k , i.e. k = 5. The consensual clustering results are exhibited in Fig. 5 . Finally, Fig. 6 presents the schedule reliability evaluation metrics of the before-and-after study performed through the simulation described in the above Section. 4.3 Discussion Figure 3 clearly exhibits the penalty effects of the term f ( k, r ) trend of reducing the computed score with the increase of k . Yet, the weighted voting schema proposed in Eq. 6 ends up by finding a consensus around K =3-and not 2 as the charts may empirically suggest. As it is detailed by Fig. 4 , this happens mainly due to a particular merge between the S illustrates the obtained coverage. It differs largely from the one in place by sug-gesting that the winter schedule should be in place four weeks earlier than it is (i.e. a change from mid-December to mid-November). The affected period was used as case study to conduct the simulation-based impact study described along Sect. 4.1 . The obtained results (exhibited by Fig. 6 ) clearly outline high potential gains of performing such change. However, such gains are mainly theo-retical boundaries. They may be biased by the multiple constraints of daily PT operations, as well as by the oversimplification of the dwell time X  X  computation (i.e. used the constants computed as described in Sect. 2.1 ). Consequently, an on-field deployment of this new coverage would be necessary to determine the exact impact of the suggested changes. This paper introduces a novel procedure to improve schedule coverage on PT networks. It is based solely on AVL/APC data. The final goal is to improve PT reliability and, consequently, their ridership and cost efficiency. Our main contribution is an ad-hoc metric to select the best number of schedules to put in place giving four decision factors -punctuality, adequacy, interpretability and reliability -modeled throughout sequence mining and probabilistic reasoning. To the best of our knowledge, this is first data driven framework to automatically select the number of schedules to be put in place using real-world data from a PT operator. Experimental results uncovered the potential gains introduced by this framework. As future work, the authors intend to evaluate it on a real-world testbed. Moreover, we also expect to create adequate exceptions on the concept of frequent itemset to relevant outliers on this domain (e.g. a schedule for the Christmas week) and identify when changes in round-trip times require introducing a novel schedule. This is still an open research question.
