 Handling high-dimensional data represents one of the most challenging problems in areas of data mining and knowledge discovery. Dimensionality reduction has been shown effective in dealing with high-dimensional data for learning, which refers to the study of methods for reducing the number of dimensions describing data [ 1 ]. It can bring many potential benefits: alleviating the curse of dimen-sionality, speeding up the learning process, and improving the generalization capability of a learning model.
 In general, they can be broadly classified into two categories: feature extraction and feature selection [ 3 , 5 ]. Feature extraction constructs new features with a linear or nonlinear transformation by projecting the original feature space to a lower dimensional one. Unlike feature extraction methods, feature selection methods preserve the original meaning of the features after reduction, which can be broadly categorized into wrapper and filter methods [ 2 , 4 ]. The wrapper method uses the predictive accuracy of a predetermined learning algorithm to determine the quality of selected features. One drawback of the wrapper method, however, is that it is very expensive to run for data with numbers of features. The filter method separates feature selection from classifier learning so that the bias of a learning algorithm does not interact with feature selection algorithms. It relies on many feature measures. Much attention has been paid to filter fea-ture selection. Rough set theory offers a formal methodology for filter feature selection. The main advantage of rough set theory is that no additional infor-mation about the data is required for data analysis such as thresholds or expert knowledge on a particular domain. It provides a mathematical tool to handle uncertainty in many data analysis tasks [ 6 ]. From the perspective of evaluation functions in rough set theory, the feature selection algorithms encountered in the literature can be mainly categorized into three representative types: consistency-in the metrics that are used to evaluate the quality of candidate features to find optimal solutions.
 It should be noticed that the classical feature selection algorithms established on the rough set theory are developed for a single type of features. However, there often coexist some heterogeneous features, such as missing, categorical and numerical ones, in data sets of most practical problems. For example, in a clinic ficulty for some unconventional clinical measurements. In addition, most feature selection techniques assume the data are already stored in data sets and available without cost. But data are not free in real-world applications. In general, there not deal with the heterogeneous data. Thus, it is meaningful and challenging to effectively perform feature selection for heterogeneous data. In the literature, a simple method to deal with heterogeneous data is to preprocess them into single-ones, and missing features can be filled with known ones. This transformation of feature types might cause the loss of information. Recently, the methods found that deal with heterogeneous data [ 15 , 16 ]. However, they have not taken into the cost-sensitive data consideration. In most real-world applications, a user is not only interested in removing irrelevant and redundant features, but also in reducing costs that may be associated to features. The methods found that deal deal with the minimal test cost feature selection problem on complete data with feature selection using histograms. Bolon-Canedo et al. [ 13 ] proposed a new gen-Based on the above observations, it is challenging to effectively perform the fea-ture selection issue from both of heterogeneous features and costs intrinsic to ture subset with a minimal total cost as well as preserving a particular property of the heterogeneous data.
 concepts involved in the paper. In Section 3, a multi-criteria based evaluation function for selecting features is proposed, and a cost-sensitive feature selec-tion algorithm on heterogeneous data is developed. In Section 4, comparison experiments are made to show the efficiency and effectiveness of the proposed algorithm. Finally, the conclusions are presented in Section 5. and introduce a hybrid decision system with costs to represent the cost-sensitive heterogeneous data.
 tion system [ 4  X  11 ]. More formally, an information system is a 4-tuple U, A, V, f &gt; , where U is a set of nonempty and finite objects; of features characterizing the objects; V is the union of feature domains, i.e., V =  X  and f : U  X  A  X  V is an information function, which assigns feature values from domains of feature to objects such as  X  a  X  A , x  X  U V , where f ( x, a ) denotes the value of feature a for object subset B  X  A determines an indiscernibility relation, which is { ( x, y )  X  U  X  U : f ( x, a )= f ( y, a ) ,  X  a  X  B } , the relation tions U into some equivalence classes given by U/IND ( B )= where [ x ] B denotes the equivalence class determined by [ x ] feature set C and decision feature set D , the system is called a decision system. decision system. A hybrid decision system can be written as ical and missing feature values, where C c is a categorical feature set and numerical feature set, respectively. To simplify this, we denote the or categorical feature in C as c n i or c c i . In addition, if there exist such that f ( x, c ) is equal to a missing value (a null or unknown value, denoted as  X  ), i.e.,  X  X  X  V C .
 account in data sets. The cost-sensitive heterogeneous data can be expressed as a hybrid decision system with test and misclassification costs (HDS-TMC), it is represented as the 6-tuple S = &lt;U,C  X  D, V, f, tc, mc &gt; f have the same meanings as HDS, tc : C  X  R +  X  X  0 } is the test cost func-tion of feature set, R + is the set of positive real numbers. Test costs are inde-pendent of one another, that is, tc ( B )= b  X  B tc ( b )for is the test cost of single feature b . Central to the test cost is a vector, which can be easily be represented by tc =[ tc ( c 1 ) ,tc ( c 2 mc : k  X  k  X  R +  X  X  0 } is the misclassification cost function, which can be represented by a matrix MC = { mc k  X  k } , where k = | V misclassification cost is the concept of the cost matrix. The cost matrix can be considered as a numerical representation of the penalty of classifying objects from one class to another. measuring the uncertainty for heterogeneous data. On this basis, we present a multi-criteria based evaluation function for selecting candidate features that achieve the balance between the informative power and computational cost on cost-sensitive heterogeneous data. Finally, based on the proposed multi-criteria evaluation function, we develop a cost-sensitive feature selection algorithm on heterogeneous data. 3.1 Evaluation Function on Heterogeneous Data erogeneous data, we first propose the hybrid relation to handle the heterogeneous data, including categorical, numerical and missing feature values. Definition 1. Given a hybrid decision system HDS = &lt;U,C  X  D, V, f &gt; B =
B c  X  B n  X  C ,let HR ( B ) denote the hybrid relation between objects that are possibly indiscernible in terms of B , HR ( B )= B ,f ( x, b 1 )= f ( y, b 1 )  X  f ( x, b 1 )=  X  X  X  f ( y, b 1 )= B , | f ( x, b The hybrid relation can be used to deal with the heterogeneous data, which greatly enhances the application scope of rough sets. In this sense, it natu-numerical, categorical and missing feature values without resorting to the dis-cretization process. Obviously, HR ( B ) is reflexive and symmetric but not tran-sitive. It can be easily shown that HR ( B )=  X  b  X  B HR ( The neighborhood information granule of object x with reference to a hybrid feature set B is denoted as  X  B ( x )= { y  X  U | ( x, y ) B .For X  X  U , the lower and upper approximations of X in B { x  X  U |  X   X  /  X  V and upper approximations with respect to the decision D are and B ( D )= { x  X  U |  X  of feature selection with rough sets, we introduce the entropy as feature measure to evaluate the uncertainty of heterogeneous data.
 ground, uncertainty measures evaluate the roughness and accuracy of knowledge, which can provide us with principled methodologies to analyze uncertain data and unveil the substantive characteristics of the data sets. As common mea-sures of uncertainty, various entropies as well as their conditional ones, and the extended one have been widely applied to devise feature selection algorithms. The conditional entropy can be used as a reasonable information measure in incomplete decision systems [ 17 ], and it is quite representative among other entropies. We extend the entropy to the hybrid data environment.
 Definition 2. Let HDS = &lt;U,C  X  D, V, f &gt; be a hybrid decision system,  X  /  X  V entropy of D with respect to B is defined by Lemma 1. Given a hybrid decision system HDS = &lt;U,C  X  D, V, f &gt; B hoods, we have the following relation H ( D | B 2 )  X  H ( entropy is important for constructing a greedy search algorithm. In the following, an evaluation function for measuring the significance of candidate features is defined as follows.
 Definition 3. Let HDS = &lt;U,C  X  D, V, f &gt; be a hybrid decision system, for B  X  C ,  X  b  X  B , the evaluation function for measuring the feature denoted by E ( b )= H ( D | B )  X  H ( D | B  X  X  b } ).
 can sort all the features according to the evaluation values in descending order and select the top feature in the feature selection process.
 heterogeneous data, a new term is added to the above evaluation function so that the cost is taken into account. The multi-criteria based evaluation function is given as follows.
 Definition 4. Given a hybrid decision system with costs (HDS-TMC) the feature b is defined by M ( b )= E ( b )+  X . ( tc ( b introduced to weight the influence of the cost in the evaluation function, feature b . 3.2 Cost-Sensitive Feature Selection Algorithm on Heterogeneous Based on the proposed multi-criteria evaluation function, we can develop a fea-ture selection algorithm to find a feature subset with cost minimization and the same information power as the whole feature set. However, to quicken the feature selection process, we will introduce a strategy to speed up this pro-cess in this subsection. Given a hybrid decision system with costs (HDS-TMC) S = &lt;U,C  X  D, V, f, tc, mc &gt; ,for B  X  C , B is the selected feature subset, the objects in U have been partitioned into two parts with respect to nized objects U R  X  U and unrecognized objects U R  X  U , where and U For the objects U R  X  U recognized by B ,if b  X  C  X  B is a candidate feature, then b is redundant to the objects U From above, it indicates that any candidate feature b has no impact on the objects that have been recognized by the selected feature subset the process of feature selection, the recognized objects U the whole object set U , and the remaining objects get fewer as the selection of candidate features goes on. Incorporated into this strategy, we will develop the feature selection algorithm with the forward greedy on the hybrid decision system with costs (HDS-TMC) as follows.
 Algorithm 1. Cost-sensitive feature selection algorithm on heterogeneous data (Algorithm CSFS ) Input: A hybrid decision system with costs S = &lt;U,C  X  D, V, f, tc, mc &gt; Output: A feature subset Red .
 Begin 1. Initialize Red  X   X  ; U R  X   X  ; U R  X  U ; 2. For  X  c i  X  C (1  X  i  X | C | ) do 3. compute E ( C  X  X  c i } )and E ( C )on U R ; 4. if E ( C  X  X  c i } ) = E ( C ) then Red  X  Red  X  X  c i } ; 5. End for 6. Compute the recognized objects U R from U R induced by U 7. Sort the features in C  X  Red by M ( c ) in a descending order, and record the results by { c 1 ,c 2 ,  X  X  X  ,c | C  X  Red | } ; 8. While ( U R =  X  ) do 9. for i =1to | C  X  Red | do 10. let Red  X  Red  X  X  c i } ; 11. compute the recognized objects U R from U R induced by U 12. End while 13. For  X  p  X  Red do 14. if E ( Red  X  X  p } )= E ( Red ) then Red  X  Red  X  X  p } ; 15. End for 16. Return Red .
 End rithm CSFS uses a greedy forward strategy combined with the multi-criteria evaluation function to estimate the merit of candidate features. Steps 2-5 are to select the indispensable features from the whole feature set. Step 6 is to delete the objects recognized by the selected features. Steps 7-12 are to add the current best feature c i (1  X  i  X | C  X  Red | )to Red by the multi-criteria based evalua-tion function until satisfying the unrecognized object set is empty. In addition, the objects recognized by the selected features are deleted from the object set, this process is the key step of this framework. Steps 13-15 are to delete some redundant features from the selection result Red . algorithm, we conduct some experiments on a PC with Windows 7, Intel (R) Core(TM) Duo CPU 2.93 GHz and 4GB memory. Algorithms are coded in C++ and the software being used is Microsoft Visual 2010. We perform the experi-ments on six real UCI data sets [ 18 ]. The characteristics of six heterogeneous data sets are described in Table 1. The data sets have no intrinsic test and mis-classification costs associated. In order to study the performance of the feature selection algorithm, we will generate the random costs for their input features for experiments. For each feature, the costs are generated as a random number between 0 and 1.
 algorithm, three feature selection algorithms are used for comparisons. One is entropy-based feature selection algorithm [ 9 ], and another is the discernibility selection algorithms are denoted as PRFS, REFS and DMFS, respectively. 4.1 Optimum Value of the Parameter  X  To determine the optimum value of the parameter  X  in the multi-criteria evalu-ation function, extensive experiments are done on the six data sets. In general, different values of  X  may lead to different feature subsets selected. The proposed algorithm CSFS runs 10 times with different  X  settings on each data set. The values of  X  investigated are from 0 to 0.5 with step 0.05. With different on the data set, there are different test and misclassification costs, we use the minimal total cost to evaluate the quality of a particular feature subset. Table 2 records the optimal value  X   X  on the eight data sets for selecting feature subset with minimal costs.
 Dataset Adult Annealing Arrhythmia Dermatology Hepatitis Stat Credit  X   X  0.10 0.15 0.25 0.10 0.35 0.20 From Table 2, we can observe that the optimal value  X   X  varies for different data sets. There does not exist a rational setting of  X   X  optimum value is hard to specify for all data sets. 4.2 Feature Subset Size and Total Costs For the data sets shown in Table 1, to compare the size of the selected feature subsets, Table 3 records the results of the four feature selection algorithms in terms of the feature subset size.
 From Table 3, we can observe that the number of the selected features has largely been reduced by the four feature selection algorithms. However, the size of selected features by CSFS is fewer than other three algorithms at most cases. The detailed observations can be seen as follows. Take the data set Annealing as an example, CSFS selects 11 features, while PRFS, REFS and DMFS select 13, 13 and 15 features, respectively. The reason why the number of feature subset selected by the proposed algorithm CSFS is fewer than other three algorithms attributed to a redundancy-removing step, some redundant features are removed from the selected feature subset.
 To compare the total costs of the selected feature subsets, Fig.1 records the results of the four feature selection algorithms in terms of the total costs. For this figure, the x -axis represents the six different data sets, for instance, the coordinate value  X 1 X  correspond to the Adult data set. And the sents the total costs of the selected feature subset. Also, we can see that the cost values of the selected feature subset by CSFS are lower than that of other three algorithms in most of the data sets. Take the data set Arrhythmia as an example, the cost value of the feature subset selected by CSFS is 12.35, while the cost value of the feature subset selected by PRFS, REFS and DMFS is 15.19, 16.83 and 17.46, respectively. This may happen due to the multi-criteria based evaluation function, the proposed evaluation function considers the test and mis-CSFS, however, remains expensive in the Annealing date set. This phenomenon happens attributed to the costs have a greater effect than information power in this data set. 4.3 Computational Time A summary of computational time for the four feature selection algorithms is given in Table 4. The computational time is expressed in seconds.
 The experimental results on Table 4 show that the computational time cost of CSFS is much less than other three algorithms. The advantage of CSFS over as another example, the time cost of CSFS is about 1/5, 1/6 and 1/9 than that of PRFS, REFS, and DMFS, respectively. The results can be caused because the objects that have been recognized by the selected features in CSFS are deleted from the object set, such that the selection of features is in a compact search space. In contrast, the objects in other three algorithms always keep the same in the process of selecting candidate features. 4.4 Classification Accuracy In the process of classification, ten-fold cross validation is applied on the data sets shown in Table 1. We randomly divide the objects into ten parts, and nine of them are used as the training data and the rest one as test data. The process is repeated for 10 times, after ten rounds, the results achieved on each time are recorded and averaged to obtain the final performance. The four feature selection algorithms are performed on each partition. To evaluate the subsets of support vector machine (SVM) classifier, is trained on these subsets. The results achieved on each partition in terms of classification accuracy are recorded and averaged to obtain the final results. The recorded accuracy values are presented as a percentage.
 Data sets Raw PRFS REFS DMFS CSFS
Adult 83.65  X  3.17 83.27  X  3.95 84.01  X  3.68 82.49  X  4.22 84.71
Annealing 99.15  X  0.41 100.00  X  0.0 100.00  X  0.0 100.00
Arrhythmia 71.60  X  7.53 74.68  X  8.26 72.15  X  6.97 72.78 Dermatology 90.35  X  2.64 88.50  X  2.91 89.31  X  3.40 86.53
Hepatitis 82.97  X  4.21 84.16  X  3.10 84.27  X  4.78 85.21
Stat Credit 62.18  X  5.95 67.72  X  6.79 67.03  X  5.92 66.24 Average 81.65 83.06 82.79 82.21 83.85 The results shown in Table 5 indicate that the four feature selection algo-rithms produces better classification performances after feature selection, when compared with the raw data sets. Considering the average results in the last low, we see that there are no big differences among these four algorithms. How-CSFS outperforms the other three comparative algorithms slightly as to SVM. To features, we choose two data sets Adult and Hepatitis to make an experimental comparison. Fig.2 shows the plots of the classification accuracies for the four feature selection algorithms.
 of the four algorithms increases rapidly at first, then the classification perfor-mance of them is relatively stable as the number of features increases. However, CSFS yields superior classification accuracy comparing with other three algo-rithms in most cases. Take the data set Adult as an example, when selecting 5 features, the classification accuracy of CSFS is 79.58%, which is higher than that of PRFS, REFS and DMFS 15.46%, 7.13% and 10.29%, respectively. The same phenomenon occurs as to the Hepatitis data set. With different number of selected features, CSFS always gives feature selection that is better than or comparable to that of others. The better performance of the algorithm CSFS is achieved in most cases, due to the fact that the multi-criteria can identify relevant and significant features from the data sets more efficiently than other algorithms. From the experimental results, we confirm that the proposed algo-rithm can find a feature subset without losing the classification performance. In this paper, we introduce a multi-criteria based evaluation function to develop a feature selection algorithm with forward greedy search on cost-sensitive het-erogeneous data. Two main conclusions are drawn as follows. On the one hand, compared with the existing feature selection algorithms, the proposed algorithm can find a feature subset with costs minimization. The main reason attributed to the multi-criteria based evaluation function that not only describes the informa-cost of candidate features. On the other hand, compared with other algorithms, the proposed algorithm can find a feature subset in a much shorter time without losing the classification performance.The main reason attributed to the multi-criteria based evaluation function implemented in dwindling object set in the fea-ture selection process.Therefore, the proposed algorithm supplies a reasonable solution for feature selection on cost-sensitive heterogeneous data.
