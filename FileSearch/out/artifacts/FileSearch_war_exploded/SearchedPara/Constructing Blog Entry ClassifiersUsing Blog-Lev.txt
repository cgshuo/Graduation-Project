 Weblogs (blogs) are now considered as an attractive information source. It is generally understood that they are personal web pages authored by a single individual and made up of a sequence of dated entries of the author X  X  thoughts, a sort of short-term journal, that are arranged chronologically. Blogs tend to be frequently updated and include links to others X  blogs. The content and purposes of blogs varies greatly from links and commentary about other web sites, to news about a company/person, to diaries, photos, and so on 1 . It is said that blogs date back to 1996, but they exploded in popularity during 1999 with the emergence of blogger( http://www.blogger.com ) and other easy-to-use publishing tools[5].
Recently, the study on analyzing the sp ace of weblogs has become a hot topic, as the blogspace has started to exhibit explosive growthenlarge of its size [5]. Identifying the bloggers X  characteristics, such as age, gender[4], interest[8,11], emotion, etc., has attracted much attentio n in blog analysis recently. Identifying the bloggers X  interest provides many applications, such as to investigate the distribution of bloggers X  interest, and monitor their change over time; and to link bloggers by their interest and form a community.
Identification of a blogger X  X  interest is usually solved as a classification problem of a sequence of his/her blog entries. Cla ssifying blog entries into class labels, we can guess the blogger X  X  interest by investigating a sequence of class labels on entries. This is based on the fact that people usually write some things in which they are interested. Consider a blogger with high interest in computers. He/She will tend to write entries on comput ers to represent his/her interest.
Text classifiers are usually built with supervised learning where inputting manually constructed, rather large training data to a machine learning frame-work can produce a text classifier. Therefor e, in constructing a blog entry classi-fier, we need as training data a rather large set of blog entries that are manually labeled with a class label. However, manual labeling is quite a time consuming and costly task. In contrast, we can easily obtain a set of blog sites with class labels. Web directories contain a manually classified set of web sites, including a set of blog sites. Furthermore, there exist web directories that contain only blog sites, such as http://www.blogmura.com/ .

In this paper, we present a method of constructing a blog entry classifier by using only a set of blog sites with class l abels. Our method is based on the Naive Bayes classifier coupled with the EM algorithm. The remainder of this paper is organized as follows. Section 2 describe s related work. Section 3 describes the proposed method. Section 4 reports the results of experiments on blog data. Automatic identification of bloggers X  interests has been addressed as a text clas-sification task [11,8]. Teng and Chen [11] presented a method of the detection of bloggers X  interest with three kinds of features in blogs: textual, temporal, and interactive features. Ni et al. [8] used a technique of combining heterogeneous classifiers and a technique of hierarchical classification. Their methods are both based on the traditional supervised approach for text classification.
Automatically tagging blog entries with social tags is a similar task to blog entry classification, though the tags are not well organized class labels. In au-tomatic tagging of blog entries, unsupervised and supervised machine learning framework have been used [1,7,10], since a rather large set of training data (tagged blog entries) is readily available.

In the area of machine learning for text classification, using labeled and unla-beled examples together has often been found effective [9]. Nigam et al. showed that semi-supervised learning, with adding a huge amount of unlabeled examples to a small set of labeled examples, improves the performance of text classifica-tion, in cases of difficult availability of a large set of labeled examples. In this paper we adopted the combination of the EM algorithm and the Naive Bayes classifier, because Nigam et al. [9] already showed that the combination shows better performance in the text cla ssification. Their method is not appli-cable to the situation we are interested in, that is, when only blog-level topic labels are available. We are going to modify their idea so that we can construct entry classifiers from blog-level topic labels. Usually, each training instance is given a training label in classification tasks. However, we suppose that labels are not given to each instance, but to each blog consisting of multiple entries. More precisely, each blog in the training data has one or more labels, and the label of each entry in the blog is often one of those labels, but not always. We construct our entry classifier by imposing the relaxed version of the assumption that the label of each entry of a blog is one of the labels of the blogs. 3.1 Introduction of the Model and Q-Function In order to construct a classifier from the blog-level class labels, we use the Expectation-Maximization (EM) algorithm [2], which is used to estimate the parameters when some variables are unobserved in the training data.

Nigam et al.,[9] used the EM algorithm to incorporate unlabeled documents into the training of a text classifier. The current situation is different from Nigam et al. X  X . Our training instances (entries) are not unlabeled, but incompletely labeled; only blog-level labels are given. By assuming that each label of the entry is selected from the blog-level l abels, we can use the EM algorithm to construct a classifier for entries.

Let B denote a set of blogs and their topic labels. We estimate parameters  X  by means of maximum a posteriori estimation, i.e., by maximizing The likelihood log P ( B |  X  ) can be decomposed as (for simplicity, we do not write  X  explicitly on the right hand side) and e ij is an entry in blog b i .

We then use the above assumption that each label of the entry is selected do not know t n , we regard the selected label as latent variable, and apply the EM algorithm with the following Q-function (the expected value of Equation (1) over the latent variable): We also assume that P ( T i ) is constant and redefine Q-function: We use the multinomial model of Naive Bay es classifiers [6] for the generative probability of an entry given the label : where w is a word type, n ij ( w ) is the frequency of w in entry e ij ,and | e ij | is the number of word tokens in e ij . Therefore, we obtain P ( e ij | t n )  X  w P ( w | t ) n ij ( w ) . We use the Dirichlet prior for P (  X  )  X  t where  X  is a hyper-parameter. Hence, the Q-f unction is expressed as follows : 3.2 The EM Algorithm The EM algorithm consists of two steps: E-step and M-step. These two steps will be performed iteratively in turn until it converges.
 E-step calculated: We assume that, when t is given, e ij is independent of T i , resulting in where Then, we obtain M-step At M-step, we obtain the parameters  X  (i.e., P ( w | t n )and P ( t n )) that maxi-mize Q -function. The formula for M-step can be derived through the standard Lagrangian method. We are going to maximize the Q-function (Equation (2)) under the constraints : w P ( w | t n )=1, t is defined to be where  X  t n and  X  are Lagrange multipliers. By differentiating L (  X  )witheach parameter and setting the derivative to be 0, we obtain the following equations : From these equations, we obtain the following update formula in M-step : where | C | is the number of categories.
 We set the initial value of the posteriors to be : 3.3 The Tempered EM A number of variants to the EM algorithm have been proposed. Among them, the tempered EM[3] has a good property that it can control the smoothness of the posterior probability P ( t n | e ij ,T i ) used during the training. The tempered EM is implemented by slightly modifying the E-step : where  X  is a user-given hyper-parameter. 4.1 Experimental Setting The dataset we used in experiments is written in Japanese and was originally collected by blogWat cher corporation 2 . It consists of 634 blogs (75,161 entries). 532 blogs (64,463 entries) out of the 634 blogs above were used as training data. There are 33 topic labels such as computers, life, outdoor, art, sports, travel. 2,415 entries that were randomly chosen from the remaining entries were manually labeled with one of those 33 topics by one of the authors and used as test data. Blog-level labels were given by users of the blog site. Note that 396 blogs out of the 532 training blogs have only one topic label. The other 136 blogs have multiple topic labels. In the model construction in Section 3.1, we assumed that each label of the entry is selected from the blog-level labels. However, this assumption was only used for model construction and we should be aware that the actual dataset does not follow this assumption in both the training and the test datasets. As a result of the morphological analysis with MeCab, 3 61,149 word types were found in the training data and used in training and classification. As evaluation measure, we use accuracy, which is defined to be the number of correctly classified entries divided by the number of all the entries in the test data.

We set two baseline methods.  X  X B0: the Naive Bayes classifier constructed with only the blogs with a single  X  NB-init: the Naive Bayes classifier at the initial condition of the proposed As a reference, we also add the result of the Naive Bayes classifier constructed with labeled entries. This result was obtained through 10-fold cross-validation on the test data. 4.2 Results Experiment 1 We are going to examine the classification ability of the proposed method in the general situation where blog-level topics are not available. The topic labels t n are predicted by means of the following formula : The result is shown in Table 1. The table shows that the baseline2 (NB-init) outperforms the baseline1 (NB0), which is unable to use multiple-label blogs. It also shows that NB+EM yields a be tter accuracy than two baselines (NB0 and NB-init). Since the only difference between NB+EM and NB-init is that the EM algorithm is applied on NB+EM, the result suggests that the proposed method, NB+EM, succeeded in making good use of blog-level topic labels. The difference in accuracy between NB-init a nd NB+EM was statistically significant in the sign test with 5% significance level.
 Figure 1 shows how the accuracy of NB+EM changes during the iterations. The accuracy becomes stable after 15 iter ations. The best number of iterations is around 5. An appropriate early stopping would still increase the final accuracy.
As mentioned above, the actual dataset does not follow the assumption that each label of the entry is selected from the b log-level labels. Our model stipulates that this assumption should be true. We come up with a natural question of what would happen if we relax this assumption. We therefore tested a slightly modified method in which the value of the delta function in Equation (6) for ( t/  X  T i )is changed from 0 to 0 . 1, 0 . 5or1 . 0. With this setting, we executed NB+EM and suggests that, in our experimental setting, the strict use of the above-mentioned assumption is effective, though the a ssumption is not completely true. Experiment 2 In Experiment 1, we evaluated the proposed method in a general situation where blog-level topic labels are not given. In Experiment 2, we evaluate the proposed method in a situation where blog-level topic labels are available. In this setting, the model predicts the topic label of an entry e ij as arg max t Thus we can choose a topic label from the set of topic labels assigned to the blog that e ij belongs to. In this setting, we compare the proposed method with the above-mentioned baseline 2 (NB-init) and also with the random classification which randomly selects a topic label from the blog-level topic labels. The accu-racy of the random classification was averaged over 5 trials. The result was shown in Table 3. This table shows that, although NB+EM significantly outperforms the random classification, it shows only a slight (non-significant) improvement over the baseline 2 (NB-init). We proposed a method for constructing an entry classifier using blog-level topic labels. We used the Naive Bayes classifier enhanced with the EM algorithm for this purpose. The proposed method outperformed the Naive Bayes classifier without the EM algorithm in a general situation where blog-level topic labels are not available. In future work, we will investigate the proposed method in more details, especially in a situation where there are more blogs that have multiple topic labels. We also plan to incorporate the reliability of blog-level topic labels into the model, because some blogs are assigned with unreliable topic labels, which probably would degrade the classification performance.

