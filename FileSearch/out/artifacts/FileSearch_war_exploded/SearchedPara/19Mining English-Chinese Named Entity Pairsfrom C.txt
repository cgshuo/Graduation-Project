 LISHUANG LI, PENG WANG, DEGEN HUANG, and LIAN ZHAO, Bilingual NE (including PERSON, LOCATION, ORGANIZATION, PRODUCTION, etc.) pairs are valuable resources for many NLP applications such as machine trans-lation and cross language information retrieval. It is more attractive than ever that the mutual translation pairs can be automatically mined from the huge Web resource. Several approaches have tried to automatically acquire bilingual lexicons from the Web. Jiang et al. [2007] proposed an improved method using Web mining to generate the translation candidates in order to enhance the transliteration results with Web information. Gao et al. [2007] mined English-to-Chinese translation pairs from mono-lingual Chinese Web pages based on the observation that many Chinese terms are accompanied by their English translations in the Chinese Web pages. Candidate trans-lations are extracted using predefined templates, while transliterations and transla-tion pairs are then identified using statistical learning methods.

Most related work has focused on using parallel corpora to learn bilingual lexicons and has reached a high accuracy. But parallel corpora are scarce resources and parallel corpora are very expensive and time consuming. Compared with parallel corpora, com-parable corpora are more abundant, more up-to-date, and more accessible; therefore, much recent research has concentrated on mining bilingual lexicons using compara-ble corpora. Comparable corpora refer to texts that are not direct translations but are about the same topic, for example, various news agencies reporting major world events in different languages. Such corpora can be collected easily by downloading electronic copies of newspapers, journals, articles, etc., from the Web.

Since the comparable corpora were first described by Pascale [1995a] to extract bilingual translation pairs, lots of extending methodologies have been explored. Klementiev and Roth [2006] presented a method to automatically discover paralleled NE transliteration pairs between English and Russian from article-aligned bilingual comparable corpora. The results showed that this approach successfully discovered the NE transliteration pairs between English and Russian. Lam et al. [2007] also devel-oped a novel NE matching model that considers both semantic and phonetic informa-tion. Experimental results showed that name translations that couldn X  X  be found in the dictionary were effectively discovered.

Some methods mined the context information around the NEs in the comparable corpora. Shao and Ng [2004] proposed an approach for the task of mining new word translations from comparable corpora by combining both context and transliteration information. Lu and Zhao [2006] gave a multi-feature-based method to extract bilin-gual NEs from comparable corpora. The multi-features included phonetic feature, se-mantic feature, and context feature. Finally all the above features were combined by a weighted sum of the scores. Tao et al. [2006] used the temporal distribution of can-didate pairs. The NE that occurs in different languages often has correlated frequency patterns due to a major event. Sproat et al. [2006] exploit frequency correlation as the feature in their method, because the NE that occurs in different languages often has correlated frequency patterns due to a major event.

From the above approaches we can find that mining NE translation using the infor-mation of NE is the basic method and combination strategies are all better than the individual methods. Some methods combined context information into a feature set. However, the context information is dependent on the types of comparable corpora. Furthermore, the previous approaches have simply combined these features without considering the type-dependence of NE translation, and the performance would de-scend since some features may be not suitable for the excavation of some kinds of NE pairs. Because of the high type-dependence of NE translation, we need to integrate different features considering the different types of NEs. The features include the transliteration model, semantic (English-Chinese matching, Chinese-English match-ing), translation, model length, and context vector. We experiment with the above in-dividual features or integrated ones to mine PN, LN, and ON pairs. The general architecture of English NE pairs mining outlined in Figure 1 mainly con-sists of three components: Recognition &amp; Extension (Part A), Excavation (Part B), and Filtration (Part C). The others are the input and the output. We use a Web crawler to collect Chinese and English news respectively from XINHUA Net. Then the compa-rable corpora construction produces the comparable corpora [Huang et al. 2010]. The NE types are classified by the initial NE recognition because PN, LN, and ON have different features and feature templates. The Chinese corpora are segmented with the tool from Luo and Huang [2009]. We extract the English NEs by means of Stanford English NE Recognition system (Stanford NER) [Finkel et al. 2005]. Simultaneously, the Chinese NEs are extracted from the Chinese corpora [Luo and Huang 2009]. Then the English NE candidates are generated by the process of English NE extension de-scribed in Section 3. Considering the fact that the translations of different NE pairs are highly type-dependent, different features described in Section 4 are collected to extract PN, LN, and ON pairs. The features are integrated into one model with lin-ear combination and the MSR algorithm [Gao et al. 2005; Yuan et al. 2007] is used to fix the weights. Lastly the candidate pairs whose scores are below the threshold are abandoned.
 For example, consider Chinese corpora CH corpora and English Corpora EN corpora . Part A recognizes and extends the NEs from corpora. The Chinese PN set is {  X   X ,  X   X  } . The Chinese LN set is {  X   X ,  X   X  } . The Chinese ON set is {  X   X ,  X   X  } . The English PN set is {  X  X arack Obama X ,  X  X ilvio Berlusconi X  } . The English LN set is {  X  X raq X ,  X  X hilippines X  } . The English ON set is {  X  X olitical Bureau X ,  X  X hilippine national disaster coordinating council X ,  X  CPC  X  } . The new NE  X  Standing Committee of the Political Bureau of the CPC  X  is generated after extending  X  Political Bureau  X .

Part B includes six features. Each feature is used to compute one similarity score between one Chinese NE and one English NE. With six features we can get a score vector between the English NE and the Chinese NE. For example, after computing the similarity scores between  X   X  X nd X  Silvio Berlusconi  X , the score vector (S) is as shown below:
The weight vector (W) is as shown below:
Part C is used to filter score vector. S*W is the sum score between  X   X  X nd  X  Silvio Berlusconi  X . It is 0.66358. The threshold is 0.394 and this candidate NE pair is not abandoned. Among all the scores between  X   X  and other English PNs, the score between  X   X  X nd X  Silvio Berlusconi  X  is the highest. So this NE pair is the mined result.
 The mining process is searching the translation of one Chinese NE in the English NEs which are extracted from the English corpora. Suppose Chinese NEs are right and we search their translations in English NEs. Sometimes only part of the translation is in the English NEs since the length and the structure of Chinese and English NEs are different. But the other parts are preceding or following English NEs. So English NE Extension is used to search the other parts. Some examples are like  X   X (Barack Obama) and  X   X (Standing Committee of the Political Bureau of the CPC) extracted from Chinese corpora.  X   X  is extracted from Chinese corpora and  X  Barack Obama  X  is extracted from English corpora. Maybe there is only the surname or the given name in the Chinese corpora; however, the English name usually appears in the English corpora in the form of the full name. The translation of Chinese PN matches only part of the English NE in the corresponding English corpora. The other part of the English NE is regarded as the noise to the Chinese NE. Therefore the English NE should be split into the surname and the given name. Each part of them should be compared respectively.
Compared with PN and LN, ON is much complex in structure.  X  extracted from English corpora. But  X  Standing Committee of the Political Bureau of the CPC  X  exists in the corpora. So  X  Political Bureau  X  X nd X  CPC  X  need to be extended in order to find the completed translation.

ON belongs to a compound noun with the  X  X ttribute + keyword X  type. From the analysis [Chen and Zong 2008] a typical structure of Chinese ON is shown as Backus-Naur Form (BNF).

ON::= { [location name][suborganization name][ordinal | cardinal number][person name][other modifiers] } * &lt; organization keyword &gt;
Consider a Chinese NE ( Ch )  X   X  andanEnglishNE ( En )  X  X olitical Bureau X  . There are a word sequence Ch i j that ranges from i to j in Ch and a word sequence En i j that ranges from i to j in En.
 For example: { [ (CPC)][ (Political Bureau)] } * &lt; (Standing Committee) &gt;
As the example shows,  X  (CPC)  X  X nd  X  (Political Bureau)  X  are the mod-ifiers and  X  (Standing Committee)  X  is the keyword.
 Generally, the length and the structure of ONs may vary in different languages. The Chinese NEs recognized with our instrument are long and consist of a certain number of modifiers. The English NEs recognized with Stanford NER are relatively short and only have the keyword or the modifiers. But these modifiers which belong to these English NEs still exist in the corpora. A retrieval method is employed to recognize these modifiers in order to hunt these English NEs.

The most relevant English translation for each Chinese NE is found in the process of mining English-Chinese NE pairs. We analyze the translation rules between Chinese ONs and their English equivalences. Because the words contained in Chinese ONs are mostly content words, each of them corresponds to some words in the English part. We list three cases as follows.

Case 1. For any sequence En i j ranging from positions from i to j , its corresponding equivalence ranges from i to j in Ch . So the corresponding equivalences of some se-quence Ch i j do not exist in En . But they may exist in the context of En . Consider set T which contains these sequences. They may be the modifiers which are not recognized with Stanford NER. So we need to recognize these modifiers and merge them with En . Firstly, the translations of the sequences in T are the candidate words of modifiers. If one word proceeding or following En is in this set, it is merged into En . Repeat this process until there is no word matching. Finally, the extended NE is the candi-date. As the example shows, for any sequence in  X  Political Bureau  X , its corresponding equivalence exists in  X   X , but the translation of sequence  X  The translations of each term in T are used as the candidate set { standing committee, CPC } . These translations are preceding or following En. So they are merged into En . Finally  X  Standing Committee of the Political Bureau of the CPC  X  is the candidate NE.
Case 2. For any sequence Ch i j ranging from positions i to j its corresponding equiva-lence ranges from i to j in En . And the corresponding equivalences of some sequence En i j do not exist in Ch . These sequences need to be removed. For all sequence Ch i j ,the corresponding equivalences range from i to j in En are recorded in order to determine the min position i min and the max position j max . The sequence ranging from i min to j max is the candidate.

Case 3. For some sequences Ch i j and En i j , their corresponding equivalences do not exist in En and Ch . The keyword or some modifiers may be common in the NE pair. It is impossible to translate them mutually. Given an English NE, denoted as E , we firstly syllabicate it into a  X  X yllable X  sequence SE = { e 1 , e 2 ... e n } with the linguistic rules defined as follows. (1) a , e , i , o , u , are defined as vowels. y is defined as a vowel when it is not followed (2) Duplicate the nasals m and n whenever they are surrounded by vowels. However, (3) Consecutive consonants are separated. (4) Consecutive vowels are treated as a single vowel. (5) A consonant and a following vowel are treated as a syllable. (6) Each isolated vowel or consonant is regarded as an individual syllable. (7) ch , sh , zh , th , ph ,and wh are defined as the consonants. (8) If the letter following r is a consonant, er is a vowel. If the letter following r is a (9) If the last letter is y , y belongs to the pre-syllable. If the letter following y is a (10) If the syllable following ay is a vowel, y belongs to the back syllable. If the syllable
For example,  X  X apolitano X  is split into  X  X a/po/li/tan/no X  .  X  X chifani X  is split into  X  X /chi/fan/ni X  .  X  X ianfranco X  is split into  X  X ian/ f/ran/co X  .  X  X erlusconi X  is split into  X  X er/lu/s/co/ni X  . English names can be split with the linguistic rules. Chinese names are translated to PINYIN in English. And the Pinyin  X  X yllable X  sequence of the Chi-nese name fully matches its English translation. If its translation exists, we can find it easily.  X  X yllable X  is used as translation unit, because the phonetic translation is based on the syllable of an English name and an English syllable has far less ambiguity in finding the corresponding Chinese Pinyin character.

Given a Chinese NE, denoted as C , we first transform each Chinese character into a Pinyin  X  X yllable X  sequence CE = { c 1 , c 2 ...c n } . For Example,  X   X (ChenJianGuo) The transliteration probability dictionary can be trained with GIZA++ on parallel English NEs and the Pinyin representation of their translations. The training corpora contain 3,432 pairs of mutual translation PNs. These names are from the appendix of an English-Chinese dictionary.
 Then the dictionary is used to compute the similarity score between the syllabicated English name and the Chinese character string. Specifically, for the generated  X  X ylla-ble X  sequence SE , we seek a Chinese Character sequence CE that maximizes the total score described as Equation (4.1).
 ne c , ne e is an English syllable sequence translating e j into c i . Lam et al. [2007] proposed the matching model analyzing both semantic and pho-netic similarities within two NEs in Chinese and English. We use an English-Chinese LDC dictionary which contains 1,000,000 items to analyze the semantic similarities mapping. The phonetic similarity is determined by the transliteration model which is described in Section 4.1.

Consider a Chinese NE ( Ch ) that is divided as words C { c 1 , c 2 ...c n } . For exam-ple, the Chinese NE  X   X  is divided as { (CPC), (Political Bureau), (Standing Committee) } . Its translation ( En )is  X  X tand-ing Committee of the Political Bureau of the CPC X  .
 Some words of En can be picked up in the bilingual dictionary. Consider a word in En which has a translation set T { t 1 , t 2 ...t n } . We use the set T to compare with the Chinese NE. We can see that some translations can fully match certain position of the Chinese NE, while some translations only partially match certain characters of the Chinese NE.

Case 1. One translation may match one Chinese word in the Chinese NE. The Chi-nese word is semantically matched. Let the semantically matched word be represented as the scope ( b , e ), where b and e separately denote the start and end positions of the translation in the Chinese NE. As for the degree of matching, d is defined as the num-ber of matched Chinese characters divided by the total number of characters in the translation. For example,  X  X ommittee X  can be translated as  X   X  .So  X  X ommittee X  can be represented as (10 , 12) and d =1.

Case 2. One translation partially matches one word in the Chinese NE. Some of the same characters appear in both the Chinese word and the translation. For example,  X  X PC X  can be translated as  X   X  .  X   X  and  X   X  have two same characters  X   X  X nd X   X . Moreover, they have the same meaning. Returning to the above example, the term  X  X PC X  matches the term  X   X  with the scope (1 , 2) and d =0 . 4.

Sometimes one Chinese translation matches more than one Chinese word. One of the Chinese words is the most similar. The others are different meanings of the Chinese character. The weight d for each of the possible Chinese word is calculated as the definition. Only one of the Chinese words could be possibly translated as the English term.

Case 3. Some English terms in the NE cannot be picked up in the bilingual dictio-nary. We use the transliteration model to search the best suited position. We compare this English word with each Chinese word which is not fully matched and select the transliteration model to calculate the similarity as the weights.

Then the exhaustive search is used to find the best matched Chinese segment for each of the possible translation in order to maximize the whole score.
 For example,  X  /Liberation Tigers of Tamil Eelam X  .
 Table I shows the weights and the scopes.  X  liberation  X  can be translated as  X   X  X r  X   X .  X  Tiger  X  can be translated as  X   X ,  X   X ,  X   X  X r X   X .  X  X amil X  can be trans-lated as  X   X  or  X   X  . There are no translations of  X   X  in dictionary.
 The translation  X   X  of the English term  X  X iberation X  semantically matches the seg-ment  X   X  (9,10) in the Chinese NE and the weight ( d ) is 1.0. The translation  X   X  of the English  X  X iberation X  partially matches the segment  X   X  (10, 10) and the weight ( d ) is 0.5. So the translation  X   X  is the best fit of the English term  X  X iberation X  . The translation  X   X  X ftheEnglishterm  X  X igers X  fully semantically matches the Chinese word  X   X  (8,8) and the weight ( d ) is 1.0. The translation  X   X  of the English term  X  X iger X  partially corresponds to this word  X   X  (8,8) and the weight ( d )is0.5. Sothe translation of  X   X  is the best fit and the position is (8,8). Both the translation  X   X   X  (2,3) in the Chinese NE. And the weight ( d ) is 0.5. The word  X  X elam X  cannot be found in the English-Chinese bilingual dictionary. We need to use the transliter-ation model to match the best fit position. The Chinese NE has been split into five words by the segmentation component. We compute the similarity score between the English term  X  X elam X  and each Chinese word in the Chinese NE with transliteration model described in Section 4.1 and use this similarity score as the weight ( d ). The similarity score between  X  X elam X  and the word  X   X  is 0.115048 and the others are 0. We summarize the max weight of each Chinese segment as the total weight and it is normalized by the average number of segments between the English and Chinese NEs. In Section 4.2, the translations of each word in the English NE are used to match some characters of the Chinese NE. Another method that the translations of each word in the Chinese NE are used to match some words of the English NE is described in this section. Consider a Chinese NE ( Ch ) and an English NE ( En ).

First, Ch is segmented into several words. Some words can be translated through the Chinese-English LDC dictionary which contains 1,700,000 items in order to get the translation sets. Before comparing these translations with the English NE, the stop words (for example adverb preposition, and article) should be removed from them as preprocessing. Consider a term S in Ch which has a translation set T { t 1 , t 2 ...t n } .
Second, we use each translation in T to compare with En . We can see that part of En can fully match some translations. And some translations in T only partially match certain words of En . As for the degree of matching, d is defined as the number of matched English words divided by the total number of words in the translation.
Third, some Chinese words cannot be searched in the bilingual dictionary. They are the neologism which should be recognized as the phonetic words. Then they should be compared with these words which do not match any translation of the Chinese word.
Last, each Chinese word has matched some positions of the English NE. So the exhaustive search is used to find the best matched English word for each of the possible translation in order to maximize the whole score.

Consider a Chinese NE Ch divided as words C { c 1 , c 2 ... c n } . For example,  X  aster), (coordinating), (council) } . Its English translation ( En) is  X  X hilippine national disaster coordinating council X  .  X   X  can be translated as  X  X he Philippines X  . We removed the stop word  X  X he X  from this translation. Then  X  Philippines  X  matches the first word of En .So  X   X  match the position (1 , 1) of En and its weight is 1.0.  X   X  can be translated as  X  X ntire country X  ,  X  X ationwide X  and  X  X ational X  . We search ev-ery word of each translation in En . Only the translation  X  X ational X  can be found. Then it is the best suited translation and fully matches the English word. The position is (2 , 2) and its weight is 1.0.  X   X  can be translated as  X  X elieve disaster X  ,  X  X elp disaster victims X  . We can see that no translation in the set can fully match any segment of En . But the word  X  X isaster X  of the translations can match the third word of En . The length of  X  X elieve disaster X  is the shortest. And this translation partly matches the position (3 , 3) of En . Then we use the length of the matched word  X  X isaster X  divided by the length of translation  X  relieve disaster  X  as the weight (0.5).  X   X  can be translated as  X  X oordinate X  and  X  X oordination X  . The translation  X  X o coordinate X  should remove the  X  X o X  . Then  X  X oordinate X  can match the position (4 , 4) and its weight is 1.0.  X   X  can be translated as  X  X ommittee X  ,  X  X oard X  ,  X  X ommission X  and  X  X ouncil X  . The translation  X  X ouncil X  fully matches the position (5 , 5) of En and its weight is 1.0. Lu and Zhao [2006] find that the lengths of the NEs in the mutual translation pair are closely related. So the length of NE could be used as the feature. Table II shows some examples of length. First PN should be syllabicated into a  X  X yllable X  sequence with the method mentioned in Section 4.1. The length of each syllable in the sequence is summed up as the length of a PN or a LN. A Chinese ON or a LN should be segmented as the word sequence. We should remove the stop words from the English ON and LN and divide it as the word sequence. The length of the sequence is used as the value. Then Equation (4.2) is used to compute the score. L sho rter is the shorter length between English and Chinese sequence and L longer is the longer length.

The average length of the English PNs is longer than that of the Chinese PNs, be-cause the customs of using person names in Chinese and English are different. In the English documents, the writer usually uses the full name, such as  X  X bdulahi Hassan Barise X   X  X bhisit Vejjajiva X  , whereas in the Chinese documents the writer sometimes uses part of the name such as  X   X (Abdullah) X   X (Abhisit) . However, the average length of the English organization names is shorter than that of the Chinese organization. Some organizations not only contain the semantic part but also the phonetic part. We can use translation model to translate the NE.  X  /Philippine National Disaster Coordinating Council X  is a good example. The translation of some words in Chinese NE can fully or partly match some words in English NE. So we can use Statistic Machine Translation Model to compute the similarity between Chinese and English NEs. In this approach we use the probability of translation p(e | c) in IBM Statistic Translation Model.

Ten thousand parallel sentences are used to train the dictionary of translation prob-ability. We define Equation (4.3) to compute the score. ne c is the Chinese NE, and ne e is the English NE. m is the word number of Chinese NE and n is the word number of English NE. Context information is used to extract the bilingual lexicon. It assumes that the words in the source and target language are likely to be mutual translations if their context is similar. So this approach builds a context vector respectively for the source and target NE. We can compare the source vector with the target context vector and a similarity score between the NEs is calculated.

First, consider a Chinese NE (Ch) . The terms which are in a specified window around Ch in the whole Chinese document are collected. The length of the specified window are defined as n . The wider windows could introduce noise caused by far apart context words and narrower windows could lose important context information. So the n is confirmed by the experiments described in Section 6. Then we construct a Chinese context vector VEC ch for Ch . Consider an English NE En and construct the English context vector VEC en with the same method.

Second, we map VEC ch and VEC en using a bilingual lexicon. The translations of some terms in the context vector VEC ch are in the context vector VEC en . Consider a term s in VEC ch which has a translation set T { t 1 , t 2 ...t n } . We use the set T to compare with the context vector VEC en . If one translation ( t )inset T is contained by VEC en , this translation ( t ) is extracted as the member of the vector Filter en and the term s is extracted as the member of Filter ch . s is removed from VEC ch and t is removed from VEC en . Repeat this process until there are no mutual translations. So the dimension of Filter ch is the same as Filter en . And each term in the context vector Filter ch corresponds to one term in the context vector Filter en . Sometimes more than one translation in set T is contained by VEC en because of translations are many-to-many. So we can use this method to search all paths. It may generate many different Filter ch and Filter en pairs. We search the pair which has the max similarity score as the context vector. The pseudo-code is presented as Algorithm 1.
 Third, the weight of the term denotes the importance of the term to the NE. Equation (4.4): where tf i is the frequency that t i appears as a context word of Ch , the frequency of every term in the context vector Filter ch . After calculating the weight of each term, we can transform the context vector Filter ch and Filter en to the vector Value cn and Value en which contain the weight of each term respectively. Last, calculate the similarity between them with the two vectors Filter ch and Filter en . The Equation (4.5) is defined as: Features (transliteration model, Chinese-English matching, English-Chinese match-ing, translation model, length, context vector) are integrated into one model with sim-ple linear combination for each type of NE. The weight of each feature will be fixed with MSR algorithm.

MSR is a discriminative training method of estimating parameters of language mod-els. MSR minimizes the training error directly using a heuristic training procedure. MSR can handle a large number of features and training samples; it significantly out-performs a regular trigram model trained using maximum likelihood estimation, and it also outperforms the two widely applied discriminative methods, the boosting and the perceptron algorithms, by a small but statistically significant margin [Gao et al. 2005; Yuan et al. 2007].

In each model, the initial weight (  X  ) of feature is 1. But maybe some weight is more important. So we use MSR algorithm to confirm the weight (  X  ).
The training data is defined as Equation (5.1). S i (i=1,...,M) is a Chinese NE and M is the size of the Chinese NE set in the training data. T R i is the English NE translation of S i in the comparable corpora. Gen(S i ) is the candidate set of translations of S i and T i is defined as one candidate translation in Gen(S i ). Consider D features which are used to mine NE pairs. Then each candidate contains a score vector [score 1 . . . score D ] T . computed based on the features mentioned above.
Equation (5.2) describes the score vector of a candidate. The integrated model con-tains the vector of parameter  X  = {  X  1 ... X  D } . Each parameter is defined as the weight of each feature. So the score of one candidate translation T j i is computed by Equation (5.3).
 Equation (5.4) is used to decide the English translation ( T i )ofoneChineseNE( S i ). We introduce a loss function Lossf(T i , T R i ) to measure the Edit Distance (Levenshtein Distance) between the correct translation T R i and the mined English translation T i . Then the weight of each feature is adjusted to minimize the loss function described as Equation (5.5). Here, the evaluations methodologies are precision ( P ) recall ( R )and F -score ( F )which are defined as follows: where N cor is the number of the correct mutual translated NE pairs mined, N mined is the total number of mined NE pairs and N to l is the total number of mutual translated NE pairs in the corpora.

The previous approaches use different corpora which are not released. So we cannot conduct our experiments on some publicly available dataset. As mentioned above, a large number of English-Chinese documents extracted from the political board of XINHUA Web are used in our approach. The comparable corpora are constructed from the collected documents. Braschler and Schauble [1998] made five kinds of levels used for judgment of alignment document pairs. With the judgment we manually selected 500 pairs of English-Chinese documents from the constructed comparable corpora and identified corresponding Chinese-English NE pairs to serve as our test data. Each pair consists of one English document and one Chinese document. They are the same event and the time of their publication is in a period of one week. Table III shows the document set of the corpora. A five-fold cross-validation is applied using the above corpora.

The Chinese corpora are segmented. We extract the English NEs by means of Stan-ford English NE Recognition system. Simultaneously, the Chinese NEs are extracted from the Chinese corpora. Nine hundred thirty-seven PNs, 588 LNs, and 832 ONs are extracted from the Chinese corpora. One thousand one hundred seventy PNs, 612 LNs, and 1186 ONs are extracted from the English corpora. Table IV shows the amount of NEs in the corpora. Table V shows the performance of Chinese segmentation and NE Recognition Results. Experientially there is only the surname or the given name in the Chinese corpora; however, the English names usually appear in the English corpora in the form of the full name. The translation of Chinese PN only matches part of the English NE in the corresponding English corpora. The other part of the English NE is regarded as the noise to the Chinese NE. Therefore the English NE should be split into the surname and the given name and the Chinese NE should be compared with each part of the English NE.
 Generally, the length and the structure of NEs may vary in different languages. The Chinese NEs recognized by our instrument are long and consist of a certain amount of modifiers. The English NEs recognized by Stanford NER are relatively short and only have the stems. But those modifiers which belong to these English NEs still exist in the corpora. A retrieval method is employed to recognize the modifiers in order to hunt these English NEs.

Table VI shows the performance of NE extension. We use the combination of transliteration and length as the features to mine PN pairs. The combination includ-ing English-Chinese matching, Chinese-English matching, length model, translitera-tion model and translation model is used to mine LN pairs. ON pairs are mined with the combination of English-Chinese matching and Chinese-English matching. After the NE extension as the preprocessing, the F -score of mining PN pairs increases by 11.7% and the F -score of mining LN pairs increases by 9%. The recall increases by 33.8% and F -score increases by 34.8% in mining ON pairs. The translation of different NE pairs is highly type-dependent. We have introduced some features including transliteration, English-Chinese matching, Chinese-English matching, translation Model length, and context vector. We use different combinations of features in these experiments in order to measure these features. All features are integrated into one model with a simple linear combination and we use MSR to fix abandoned. We get the threshold with the training data by experiments.

Table VII shows the performance of different features combinations in PN pairs mining. The weights are equally distributed in each experiment. PNs are mainly translated by the transliteration and the F -score using the single transliteration fea-ture is 75%. The best performance is obtained by the combination of transliteration and length as the features and its F -score is 79%.

Table VIII shows the performance of different features combinations in LN pairs mining. The weights are equally distributed in each experiment. Transliteration, En-Ch match and Ch-En match are the basic feature for LN pairs mining. After adding length and translation to the combination of features, its F -score is 81% and it achieves the best performance.

Table IX shows the performance of different features combinations in ON pairs min-ing. The weights are equally distributed in each experiment. Lam et al. [2007] devel-oped an En-Ch NE matching model that considers both semantic and phonetic infor-mation. We use this feature to translate ONs and the F -score is 67%. When we use the combination of En-Ch matching and Ch-En matching as the features, the F -score increases by 17% and it achieves the best performance.

Table X shows the influence of different features in mining different types of NE pairs. The symbol + indicates the feature enhances the performance in mining that type of NE. The symbol  X  indicates the feature declines the performance in mining that type of NE. PN pairs can mainly be mined with transliteration. The feature of length can improve the performance; however, En-Ch matching, Ch-En matching and translation model decrease the precision and the recall. These features use the semantic information and thus they are not suitable for mining PN pairs. LN pairs are mainly mined with transliteration, translation, length, English-Chinese matching, and Chinese-English matching. Translation model is not used in mining PN and ON pairs, but it is suitable for mining LN pairs. All the PNs and most LNs containing one word can be translated by transliteration model. Other LNs containing more than two words can be split into two parts. One part is translated with transliteration model and another part can be translated by translation model correctly. But it is not suitable for mining ON pairs.

ON pairs are mainly mined with English-Chinese matching and Chinese-English matching. Transliteration, translation model, and length are not suitable. Some words in ONs can be translated by transliteration. But the others can only translated by semantic feature. The features of English-Chinese matching and Chinese-English matching analyze both semantic and phonetic similarities between different tokens within two NEs in Chinese and English.

The length of the specified window is defined as n . The wider windows could in-troduce noise caused by far apart context words and the narrower windows could lose important context information. So n is confirmed by experiments. We use different n ranging from 3 to 10 as the length of the specified window. But no NE pairs are mined. So we conclude that the feature of context vector is not suitable for mining NE pairs in this article. After analyzing the data of context vector, we find that there are no mu-tual translations in the context vectors of Chinese and English NE. But this method is suitable for mining NE pairs from the parallel corpora. Because of the contexts of NEs in the parallel corpora are mutually translated. Though the comparable corpora are about the same event, the contexts of NEs in the comparable corpora are not mutually translated. Transliteration, translation model, length, En-Ch matching, Ch-En matching, and con-text vector are integrated into one model and all the weights of each feature are 1. Some features weaken the results. Some features enhance the performance. So we use MSR to fix the weight of each feature.

Table XI shows the weights, the thresholds, and the performance of the combination of the features with MSR in PN pairs mining. All correct NE pairs in the corpora are used to train the weights and the thresholds. After using MSR to fix the weights in the second experiment, F -score increases by 5.6%.

Table XII shows the weights, the thresholds, and the performance of the combina-tion of the features with MSR in LN pairs mining. All correct NE pairs in the corpora are used to train the weights and the thresholds. After using MSR to fix the weights in the second experiment, F -score increases by 2.1%.

Table XIII shows the weights, the thresholds, and the performance of the combina-tion of the features with MSR in ON pairs mining. All correct NE pairs in the corpora are used to train the weights and the thresholds. After using MSR to fix the weights in the second experiment, F -score decreases by 0.2%.
 The translation of each Chinese NE is searched from the candidate set of English NEs. The Chinese NEs with translations are identified by going through each pair of doc-uments. The performance can be measured by comparing the output of our approach with the manually identified mutual translations above.

Table XIV shows the performance of NE pairs mining. Transliteration and length are used to mine the PN pairs. The F -score is 84.9%. But the precision is only 88.9%. Because there are approximately 33% of the names without translations in the En-glish documents transliteration, length, Chinese-English matching, English-Chinese matching, and translation are used to mine the LN pairs and the F -score achieves 83.4%. The features of Chinese-English matching and English-Chinese matching are used to mine the ON pairs and the F -score is 83.9%.

Table XV shows the amount of NE pairs mined and the amount of correct NE pairs mined from the corpora. The duplicate NEs have already been removed. There are 1,170 PNs, 612 LNs, and 1,186 ONs in English corpora; and 937 PNs, 588 LNs, and 832 ONs are extracted from Chinese corpora. After the NE pairs are manually identified, there are 493 PN, 424 LN, and 349 ON pairs in the corpora. With our method, 361 PN pairs, 364 LN pairs, and 248 ON pairs are mined from the corpora correctly.
Based on the above results, our analysis is as follows. (1) The precision is low. We analyzed the incorrectly mined NE pairs and most NEs X  (2) The recall of PN and LN pairs mining is low. Because there are some names pro-(3) The experimental results are influenced by many factors including the type of The previous approaches use different corpora which are not released. In this article, the corpora described in Section 5 are used in the experiments. To compare with them for person and ON pairs mining, three experiments are conducted.
 Exp I (Based on Lam et al. [2007]): Uses En-Ch matching which contains transliter-Exp II (Based on Lu and Zhao [2006]): Uses the combination of transliteration, con-Exp III (Our approach): Uses transliteration and length to mine PN pairs and uses
Table XVI shows the best results of each experiment. The corpora used in each experiment are different and the results of mining NE pairs are influenced by the cor-pora. So we test those methods with our corpora (The results of different combinations can refer to Tables VII, VIII, and IX).
 Experiment I mines person and organization together. Lam et al. [2007] only used En-Ch matching in their system and obtained the precision of 86%, the recall of 33% and the F -score of 48% with the threshold of 0.71. Our performance is better. But the corpora are different. We test those methods with our corpora. A new feature Ch-En matching is proposed in this article. After adding this new feature, the F -score of min-ing ON pairs increases by 17%. This feature can enhance the results. The combination of transliteration and length used to mine PN pairs has the F -score 84.9%. Length can enhance the results.

Lu and Zhao [2006] use transliteration length and translation to mine the informa-tion on NEs and use context to mine corpora information. But they do not use different combination to mine different NE types and only use fix the weights of features in one model for different NE types. In their approach, transliteration probability is used as the cost function to acquire the phonetic distance. But we use the transliteration prob-ability to compute the similarity. It does not reach the best performance with the com-bination of all features. The performance of mining ON pairs is better in our approach. The combination of En-Ch matching and Ch-En matching is better than translation. This article investigates an approach about mining English-Chinese NE pairs based on multi-feature integrated models from comparable corpora. The features include transliteration, English-Chinese matching, Chinese-English matching, translation model, length. From the experiments results, we can conclude the following points:
The translation of different NE pairs is highly type-dependent. The experimental results show that the PN pairs mining achieving the F -score of 84.9% combines of transliteration with length. Some features such as transliteration and length are ben-eficial to the mutual translation of PNs, while the semantic features such as English-Chinese Matching and translation model weaken it.

The LN pairs mining with the features of transliteration model, length, translation model, English-Chinese matching, and Chinese-English matching can reach the best performance.
 The ON pairs mining with the features of English-Chinese matching and Chinese-English matching can reach the best performance of 84.1% ( F -score). However, Length and transliteration features only enhance the correct results meanwhile produce some noises to lower the precision.
 Chinese-English matching combining semantic with phonetic information in mining ON pairs using the Chinese-English dictionary can increase the precision and recall in mining ON pairs.

Our integrated model can increase the performance. Combination of transliteration with length to mine PN pairs increases the recall and the precision. And the perfor-mance reaches the best when using transliteration model, length, translation model, English-Chinese matching and Chinese-English matching to mine LN pairs. But after using MSR to fix the weight in mining ON pairs, the precision is increased and the recall is decreased.

For further work, we have the following considerations. (1) We have discussed six features including transliteration, length, En-Ch match-(2) Bilingual PN, LN and ON pairs are mined in our system. Other types of NE pairs (3) One syllable in the English NE may be translated into many different Pinyin syl-(4) The features are integrated into one model with the simple linear combination. (5) We will expand the size of the comparable corpora and construct different types of
