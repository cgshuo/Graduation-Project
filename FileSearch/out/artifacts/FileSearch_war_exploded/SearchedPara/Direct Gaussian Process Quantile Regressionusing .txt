 Alexis Boukouvalas boukouva@aston.ac.uk Remi Barillec r.barillec@aston.ac.uk Dan Cornford d.cornford@aston.ac.uk Aston University, Birmingham B4 7ET, UK Quantile regression has been applied in a variety of domains and for different purposes (Yu et al., 2003). Applications include medical reference charts, survival analysis, economics and the detection of heteroscedas-ticity. Quantile regression allows a comprehensive analysis of the relationships between a response, y , and input variables, x . In traditional regression anal-ysis there is often an implicit assumption that any un-certainty in the learned model is a result of incomplete knowledge of an underlying deterministic function due to incomplete, noisy observations. However, quantile regression is most relevant when the response is likely to be subject to variability or intrinsic randomness, such as might occur in population or meta-studies, re-gression modelling where not all relevant inputs are available or considered, or when modelling the output of a stochastic simulation.
 Two main approaches to quantile regression have been described in the literature: the Estimating Equation (EE) approach and inverting a Cumulative Distribu-tion Function (CDF). The EE approach is based on directly modelling the quantile function, learning the parameters by minimising an appropriate loss func-tion. The CDF approach is based on estimation of the CDF of the response and inverting this to obtain the desired quantiles. This differentiation is akin to the difference between discriminatory (EE) and gener-ative (CDF) models in classification.
 In the quantile regression setting, the generative case corresponds to estimating the full conditional CDF F ( y | x ) and then inverting it to obtain specific quan-tiles. This model-based approach allows for a natural Bayesian formulation. Taddy &amp; Kottas (2010) pro-vide a clear overview of options for quantile regres-sion, and propose a model based on a non-parametric Dirichlet process prior to construct a flexible joint model for the response and inputs, conditioning this on inputs to obtain the required conditional response distribution. Chen &amp; Muller (2012) develop a non-parametric CDF approach based on computing indica-tor functions which are smoothed using a kernel spec-tral decomposition to form the full conditional density model, and thus determine the quantile functions. A related approach in the field of Geostatistics is known as indicator cokriging (Pardoiguzquiza &amp; Dowd, 2005) where a Gaussian process is used to estimate a discre-tised approximation to the CDF. All of these meth-ods require carefully designed Markov Chain Monte Carlo (MCMC) inference methods, and require signif-icant computational effort, making their application to problems with many inputs infeasible. Alterna-tively for a Gaussian posterior model the inversion of the CDF can be done analytically to retrieve quantile functions as demonstrated in Quadrianto et al. (2009). The major advantage of the CDF approach is that an appropriate likelihood function is defined and the joint estimation of the quantiles means order violations (quantile crossings) are not possible by construction. However in scenarios where the interest is in the spec-ification of one, or a small set of quantiles, intuitively it seems unnecessary to attempt to describe the entire conditional distribution. The direct EE approach may also be more appropriate in application domains such as real-time systems where inference time is critical as it allows for faster computation using simpler models than the CDF approach. The EE approach (Koenker, 2005) can be seen as akin to directly constructing a decision boundary to sep-arate the classes. In the quantile regression case a loss function is minimised to obtain the quantiles of interest directly. Various EE approaches to quantile regression exist. The frequentist interpretation min-imises an empirical risk function, related to the tilted loss function (Figure 1) given by which has been shown to consistently estimate the  X   X  X h quantile. The tilted loss is also known as the pinball loss (Takeuchi et al., 2006).
 Many approaches use linear in parameter, or spline models (Koenker, 2005). These approaches are consis-tent and produce classical estimates for the quantile functions, which can also incorporate the use of sim-plex methods or post processing to ensure no order vi-olations for multiple quantiles (Koenker, 2005). Many papers have also attempted to provide a  X  X ayesian X  ver-sion of the EE approach to quantile regression. These exploit the association between the tilted loss function and the Asymmetric Laplace Distribution (ALD), as explained in more detail in Section 2.
 Yu &amp; Moyeed (2001) develop a  X  X ayesian X  linear model for quantile regression assuming an ALD likelihood, but care must be taken in the interpretation of the posterior distribution in the typical case that one does not believe the errors on the response actually follow an ALD. The use of the ALD remains common (Yue &amp; Rue, 2011; Lum &amp; Gelfand, 2012), and while the mode of the solution can be shown to be consistent with the true quantile, the uncertainty on the quan-tiles has no clear interpretation. In Lum &amp; Gelfand (2012) a conditional Gaussian representation of the ALD is used to incorporate spatially dependent errors. Inference in this model is accomplished via MCMC. Their approach is quite similar to what is proposed in this paper in that spatial dependence is modelled via a stochastic process. Yue &amp; Rue (2011) propose a similar model where a Gauss-Markov random field model is used to address spatial (input) dependency and both iterated nested Laplace approximations and MCMC are used for inference.
 Our focus in this paper is on quantile regression where the conditional quantile functions are of interest. Our contribution consists of presenting a novel method for quantile regression which uses approximate inference methods to improve efficiency. We place a Gaussian Process (GP) prior on the quantile regression func-tion similarly to Lum &amp; Gelfand (2012) and directly minimise the expected tilted loss using an Expectation Propagation (EP) approach (Minka, 2001). Further we clarify the justification of the EE approach which has been used by a variety of authors and show that although not truly a Bayesian approach, a decision theoretic grounding is possible.
 The paper is structured as follows. In Section 2 we present our approach. A simulation study on both synthetic and real data is presented in Section 3. A discussion of the results and future extensions is given in Section 4. Our model, which we term QGP-EP, places a GP prior on the space of quantile regression functions 1 . The training of the model proceeds by directly minimising the expected loss or maximising an equivalent utility function. The latter is found to correspond to the ALD which has been widely used in direct quantile estima-tion. The expected utility turns out to not be ana-lytically tractable and we employ the EP algorithm to perform the integration. A high level description of the algorithm is given in Algorithm 1. We con-clude by discussing how hyper-parameter estimation and prediction are accomplished.
 Minimising the expected tilted loss ferred to as the expected quantile risk in Takeuchi et al. (2006). For more details on the optimality conditions for different loss functions see Berger (1985). If we take the exponent of the negative of the tilted loss and normalise, we have the ALD (Yu &amp; Zhang, 2005). The density function is:
L ( t |  X , X , X  ) = The parameter  X   X  [0 , 1] controls the skewness of the distribution. For  X  = 0 . 5 we retrieve the Laplace dis-tribution. The mean  X  can take any real value and the standard deviation has to be positive  X  &gt; 0. The indicator function I ( t  X   X  ) is 1 if the condition is true, 0 otherwise. We can therefore define a utility: where q is the predicted value of the  X  quantile, y the observations and Z the normalisation. If we take the common assumption that the utilities are independent for each observation, we have: Lastly we place a GP prior on the quantile regression function: For brevity, we have omitted the conditioning on the inputs X . We propose to train the model by directly maximising the expected utility, also known as the gain or reward: where  X  = {  X , X  K } , that is the ALD scale parameter  X  , and the GP kernel hyper-parameters  X  K . This integral is not analytically tractable. However because of the independence assumption of the utility, we can employ a message-passing algorithm that locally approximates each site given the effect, known as the context, of all other sites. EP is such an algorithm and is discussed in the next section. The maximisation of the expected utility with respect to  X  is also done numerically. 2.1. Expectation Propagation In EP, the posterior is approximated using an exponential-family distribution (Minka, 2001). This is usually a Gaussian. A local approximation is made where each factor is approximated separately in an it-erative algorithm until convergence. Our motivation for using EP stems from the computational burden of sampling methods which would preclude the use of the method in time critical application domains or where numerous model training evaluations are required such as in experimental design. Also note that simple ap-proximations that use the Hessian to obtain an approx-imate Gaussian posterior centred on the mode are not applicable as the Laplace distribution is not differen-tiable at the mode (Seeger, 2008).
 The algorithm proceeds in two steps: first compute the expected utility (EP step), then maximise it (Section 2.2). This is performed repeatedly until convergence. Prediction of the quantile is done using a plug-in value for the parameters  X  -see Section 2.2.
 As the utility factorises, we approximate each factor with a local Gaussian approximation. The expected utility is approximated with the factorised Gaussian R p ( q ) Q N i =1  X   X  i where we have introduced a shorthand notation for each factor of the utility (Equation (4)). The exact utility is  X  i = U  X  ( y i ,q i ) and the approximate utility is Gaussian where  X  Z i is the normalisation,  X   X  i the mean and  X   X  variance.
 The other quantity we will need before describing the algorithm is the context, also known as the cavity field. It is the product of all factors except the i th , q except for site i .
 We also define the projection operator where Proj[ p ( x )] =  X  p ( x ) matches the moments (mean and variance) of the Gaussian  X  p ( x ) to p ( x ). A high level description of the algorithm is given in Algorithm 1. The projection step is the only problem-dependent step in the EP algorithm (Section 3.6 of Rasmussen &amp; Williams (2006)). We need to find the un-normalised Gaussian marginal which best approximates the prod-uct of the cavity distribution and the exact (local) util-ity: Because it is un-normalised, we match the zero-th mo-ment in addition to the mean and variance. All three expectations have been computed analytically and are given in Appendix A. The expressions for the new ap-proximate  X   X  i are given in Section 3.6 of Rasmussen &amp; Williams (2006).
 Algorithm 1 QGP-EP Training Algorithm.

Input: Training data D = { x i ,y i } N , size N repeat until Convergence. 2.2. Hyperparameter learning and prediction Hyperparameter learning and QGP-EP prediction pro-ceed in a similar fashion to ordinary GP regression. Values for the hyperparameters are obtained via the maximisation of the expected utility (Eq. (6)). EP provides a direct estimate of the expected utility: Z the vector of all  X   X  i . As in ordinary GP regression, in practice we minimise the negative log of the expected utility. The predictive mean and variance at a new point x  X  for the quantile q is: where D is the training data, k  X  X  X  and k  X  the test-test and test-train set covariances respectively. We note here that the prediction is on the latent variables for the quantile q and not for the noisy observations y . In our framework, it would be meaningless to discuss the latter as we have not defined a likelihood for y . 3.1. Synthetic Data To illustrate the method and provide some valida-tion in a context where the true quantiles of p ( y | x ) are known, we look at a stochastic processes with heteroscedastic (i.e. input-dependent) variance men-tioned in Quadrianto et al. (2009). The process is of the form: where  X  ( x ) is the mean component and  X  is a Chi-squared, X 2 , noise process scaled by some input-dependent factor  X  ( x ). Realisations from the process are observed at randomly sampled inputs x . Table 1 summarises the setup. We fit, independently, one QGP-EP to the data for each of the following quantiles: fit is sensitive to the realisation, thus the experiment is repeated 30 times to assess robustness. Figure 2 shows the regression curves (solid lines) obtained for a given realisation sample (dots). The true quantiles are shown as dashed lines.
 Figure 3 (a) shows the absolute error between the true and estimated quantile for 30 different realisations of the underlying stochastic process, sorted by increasing mean error for each quantile. The estimated quantile curves provide a reasonable fit to the true quantile. Lower quantiles are better estimated due to the skew-ness of the stochastic process. It is worth noting that the method remains robust with respect to outlying data points (Figure 2 top third).
 The same heteroscedastic scenario has been utilised in Quadrianto et al. (2009) allowing us to compare the performance of the QGP method to the meth-ods examined therein. Our method performed, on average, on par with the Gaussian posterior method of Quadrianto et al. (2009) and the Quantile SVM (QSVM) method of Takeuchi et al. (2006) although it is hard to compare as the results are given for a single experimental realisation only (see Table 1 in Quadrianto et al. (2009)).
 For comparison with a well established quantile re-gression method, Figure 3 (b) shows the same infor-mation as Figure 3 (a) for spline quantile regression (Koenker, 2005). The order of the splines was set to 5 (orders between 3 and 15 were considered, the used val-ues seemed to give the best results). With a strongly skewed underlying process, the higher quantiles are typically more difficult to estimate due to the limited number of data points emanating from the tail of the distribution. Samples with particularly poor spread can even lead to numerical issues in the EP algorithm. 3.2. Benchmark Data In this section we compare the QGP method to QSVM on a set of benchmarks data sets. We do not compare against the Gaussian posterior method of Quadrianto et al. (2009) since that method relies on a Gaussian error model unlike the QGP and the QSVM methods. As in Takeuchi et al. (2006), we perform 10-fold cross validation on the data sets and transform the data to have zero mean, unit variance. The datasets used are the caution dataset which has 2 regressors and 100 points, the ftcollinssnow set which has 93 points and 1 regressor and the motorcycle set which has 1 regressor and 133 points. We utilised a zero mean GP with a squared exponential kernel with independent lengths scales for each input dimension.
 The average pinball loss is shown in Table 2. The pinball losses are similar for the two models although we note that the deviations are typically much higher for the QGP model. We would caution on the in-terpretation of the standard deviations however. We implemented the unconditional quantile model as in Takeuchi et al. (2006) and the standard deviation val-ues we obtained were higher than those shown in the paper (although the mean values were very close). Dif-ferences in the partitioning of the datasets for cross validation may give rise to the larger deviations. The cross validation was done using completely random partitions in our experiments. Just examining the mean values, the performances of the two models are similar although in some cases the QSVM is better (  X  = 0 . 9 ftcollinssnow) while in others the QGP is bet-ter (  X  = 0 . 9 caution). We discuss the main benefits of QGP in Section 4.
 3.3. English Longitudinal Study of Ageing In this section we apply the QGP-EP to the English Longitudinal Study of Ageing (ELSA) dataset. ELSA 2 is a multi-purpose large study which follows individu-als aged 50 years or older (Banks et al., 2006). Factors include clinical, physical, financial and general well-being. One of the primary interests in examining age-ing populations is the effect of the different factors on Quality of Life (QoL). There exist various measures to estimate the latter and we have selected to use the CASP-19 measure following (Blane et al., 2008), which is a compound measure of several health and socio-economical indicators.
 We investigate the effect of lung function, obesity, blood pressure and age on the CASP-19 QoL mea-sure. This analysis was done for the mean response in Blane et al. (2008) using structural equation modelling and our aim is to investigate whether their conclusions extend to the response quantiles as well. Our analy-sis is done cross-sectionally on the second ELSA wave dataset as in Blane et al. (2008). Therein it was found that lung function and obesity, but not blood pressure, were directly associated with QoL.
 We examine the effect of these factors on the 25th, 50th and 75th quantiles. These quantiles were se-lected to reflect worse than typical, typical and bet-ter than typical QoL outcomes. We utilise the Au-tomatic Relevance Determination (ARD) method to estimate the effect of each factor on the QoL output. In the ARD approach we use a separate length-scale parameter in the kernel for each input. The input do-mains are linearly rescaled to equal ranges ensuring the length-scale parameters can be interpreted as impor-tance measures. The intuition is that the length scales tell us how far along a particular axis one needs to move for the values to be uncorrelated ((Rasmussen &amp; Williams, 2006) Section 5.1). To implement the ARD approach we utilise a zero mean GP with a squared exponential kernel.
 A 1500 point training set is used to train a QGP-EP for each quantile. This is accomplished by generating 1000 random designs and selecting the design which max-imises the minimum distance between any 2 training inputs. In this fashion, we achieve a reasonable cov-erage of the input space whilst including a variety of inter-point distances in the training set to help identify the length scale parameters. The rest of the Wave 2 ELSA data set (3364 points) is utilised for validation. As the true or sample quantiles are not available for this dataset, we follow the approach of Chen &amp; Muller (2012) to assess the quality of the model pre-dictions  X  Q i (  X  ) for a quantile  X  . For a given test set { X i ,Y i } N i =1 , the expectation of the indicator function I therefore utilise the mean of the indicator function, to the true quantile, as a diagnostic. The I (  X  ) mea-sure is calculated for each quantile and is shown in Fig-ure 4. We see that for all three quantiles, the QGP-EP achieves the optimal value. For comparison we have included the measure achieved by a linear-Gaussian model which is linear in the inputs with i.i.d Gaussian noise and is estimated using ordinary least squares on the same training data as the QGP-EP. The quantile estimates are obtained by inverting the Gaussian CDF. For the 25th quantile, both models achieve the opti-mal measure whilst for the median and 75th quantile the QGP-EP achieves a better score.
 The length-scales for each input are shown in Table 3. The inputs consist of a measurement of lung function, Body Mass Index (BMI), higher values of which are indicative of obesity, diastolic blood pressure and age 3 BMI is ranked highly and diastolic blood pressure low for all quantiles in agreement with the findings of Blane et al. (2008) on the mean QoL. Lung function is found to be most critical for the 25th quantile whereas it is found less relevant for the other quantiles. Age is ranked highly for the median and 75th quantile but less so for the 25th quantile. We therefore conclude that in terms of predicting quality of life as measured by the CASP-19 measure, the findings of Blane et al. (2008) on the mean hold for all quantiles considered in terms of BMI and diastolic blood pressure. The former is found to be a good predictor of QoL whilst the latter is not and consideration may be given to omitting this variable from future measurement and analysis. On the other hand, lung function seems to be a good predictor for low QoL. We hypothesize that as lung function deteriorates past a threshold, QoL seems to be drastically affected.
 We have presented a framework for quantile regres-sion. The framework relies on the maximisation of the expected ALD utility under a GP prior and uses EP to compute the intractable integrals. The method has been validated on a non-trivial synthetic example of a strongly skewed heteroscedastic process. The method performs well, providing a good fit to the true un-derlying quantile function. As one would expect, the performance of the method is linked quite strongly to the identifiability of a specific quantile, leading to bet-ter estimation of the lower quantiles for a positively skewed process.
 When comparing the method against the QSVM of Takeuchi et al. (2006) in Section 3.2, the perfor-mance of the algorithms was similar. As the QSVM was shown to outperform several other methods in Takeuchi et al. (2006), we believe the QGP offers state of the art performance as well. However, the main ben-efit of the QGP lies in the ability to leverage the full probabilistic GP framework for quantile estimation in a computationally efficient framework. For instance, the QGP framework could be extended to handle very large data sets by using well known sparse approxima-tions (Quinonero-Candela &amp; Rasmussen, 2005). The modeller can also easily incorporate prior information by setting different mean and covariance functions for the QGP. Another example of the benefits of the GP framework is to perform variable selection by using ARD as was demonstrated in the ELSA study in Sec-tion 3.3. Finally unlike fully Bayesian approaches where the entire conditional CDF is estimated, the QGP method, like other direct quantile estimation methods, can be applied to higher dimensional prob-lems and does not require computationally expensive MCMC methods for parameter estimation.
 The second main contribution of this paper is to offer an alternative perspective on the approach of Yu &amp; Moyeed (2001) and subsequent follow up papers (e.g. (Yue &amp; Rue, 2011; Lum &amp; Gelfand, 2012)). The use of the asymmetric Laplace as a likelihood is only in-formally justified in these papers. We have described how this approach can be interpreted as a minimisa-tion of the expected tilted loss and can therefore be well grounded in decision theoretic terms.
 There are limitations to quantile estimation using QGP-EP, some inherent to the method, some to quan-tile estimation itself. Estimating quantiles in the tails of a distribution can be problematic if the distribution is very skewed, due to the smaller number of informa-tive data points. In such cases, the EP algorithm can become numerically unstable. Fractional EP (Minka, 2004) may be used to ease these problems as Seeger (2008) has noted, by reducing the impact of individual EP updates. It is also possible to set priors on the GP parameters to enforce smoother regression functions for those less identifiable quantiles, in accordance with one X  X  beliefs. In the QGP-EP framework, these would appear as regularisation terms in the maximisation of the log expected utility.
 One possible criticism of QGP-EP (and several other quantile regression methods) is the lack of information about the uncertainty in the quantile. While variance estimates are easily obtained for Bayesian regression models of the mean, these are not easily interpreted for quantile regression models. The difficulty lies in the fact that we estimate quantiles to avoid having to spec-ify a particular shape for the underlying distribution of the response. This is more akin to likelihood free methods and makes a full Bayesian treatment, yielding a posterior distribution, impossible. Alternative ap-proaches have been considered to try and estimate the quantile variance, typically using resampling methods such as the bootstrap (Koenker, 2005).
 In this work, QGP-EP only allows us to estimate a single quantile function at a time. While several quan-tiles can easily be learned independently, there is no guarantee that these will respect order constraints, al-though a stochastic ordering was established in Lum &amp; Gelfand (2012). While in the presence of sufficient training data the problem is relatively minor, further work is needed to address order constraint violation for smaller datasets. In particular, this could be done within a framework allowing several quantiles to be jointly estimated by introducing an order constraint via a step function in EP.
 The analogy with classification problems was first sug-gested by Neil Lawrence. We thank the anonymous reviewer for suggesting how to extend the approach to multiple quantiles. Thanks to Ian Nabney for useful discussions on this work. This work was funded as part of the Managing Uncertainty in Complex Models project (EPSRC grant D048893/1).
 We provide the expressions for the normali-sation, mean and variance of the Projection operator of the ALD and cavity field. The latter is a normal distribution with mean  X  and variance v . The normalisation constant is  X  Z K
A = exp ing h B and K B are obtained by replacing  X  with  X   X  1 in h A and K A respectively. The complementary error function is defined as erfc( x ) = 2  X   X  R +  X  x e  X  t 2 dt . The mean is E [ q moment is E [ q i 2 ] = 1  X  I
B = v exp  X  I
A = M h I
B = M h
